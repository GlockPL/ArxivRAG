{"title": "Musical Agent Systems: MACAT and MACataRT", "authors": ["Keon Ju M. Lee", "Philippe Pasquier"], "abstract": "Our research explores the development and application of musical agents, human-\nin-the-loop generative AI systems designed to support music performance and\nimprovisation within co-creative spaces. We introduce MACAT and MACataRT,\ntwo distinct musical agent systems crafted to enhance interactive music-making\nbetween human musicians and AI. MACAT is optimized for agent-led performance,\nemploying real-time synthesis and self-listening to shape its output autonomously,\nwhile MACataRT provides a flexible environment for collaborative improvisation\nthrough audio mosaicing and sequence-based learning. Both systems emphasize\ntraining on personalized, small datasets, fostering ethical and transparent AI en-\ngagement that respects artistic integrity. This research highlights how interactive,\nartist-centred generative AI can expand creative possibilities, empowering musi-\ncians to explore new forms of artistic expression in real-time, performance-driven\nand music improvisation contexts.", "sections": [{"title": "1 Introduction", "content": "A musical agent [1] is an artificial system designed to automate creative musical tasks within the field\nof musical metacreation\u2014an area focused on the computational simulation of musical creativity\n[2]. In this domain, musical agents perform a variety of crucial tasks that deepen and broaden\nhuman-AI collaboration in music-making. These tasks encompass generative composition, where\nagents autonomously produce original material based on learned styles; interactive performance,\nwhich allows agents to adapt in real-time to live inputs from human musicians; and accompaniment,\nin which agents provide dynamic, context-sensitive support that complements primary musical\nperformances. Further, style adaptation enables agents to align their outputs with specific genres\nor artist preferences. At the same time, real-time improvisation allows for spontaneous, unscripted\nmusical responses, fostering a creative and responsive interaction with human musicians. These tasks\nposition musical agents as versatile, adaptive collaborators, functioning as co-creators, responsive\npartners, and innovative contributors across diverse musical contexts.\nDrawing on principles from Artificial Intelligence (AI [3]) and Multi-Agent Systems (MAS [4]),\nmusical agents operate autonomously, making decisions and performing actions in response to\nreal-time musical contexts. These systems range from simple rule-based models to advanced frame-\nworks capable of learning and adapting through interaction. Often implemented using MAX/MSP\nprogramming [5], they facilitate collaboration with human musicians or other agents, showcasing\nattributes such as reactivity, adaptability, and coordination. Complementing these agents, Corpus-\nBased Concatenative Synthesis (CBCS)\u2014pioneered in IRCAM's CataRT system [6] and widely\nused in electroacoustic music [7,8,9]-utilizes small, personalized datasets of recordings to generate\nnew content. CBCS achieves this by selecting and transforming audio segments from a pre-existing\ncorpus, enabling real-time control of the generative process while preserving the expressive nuances\nof human performance."}, {"title": "1.2 Research Motivation", "content": "Our research is motivated by the aim of developing interactive, artist-in-the-loop generative AI\nsystems designed for musicians and sound artists. The primary objective is to enable musicians to\nexplore novel creative possibilities and expand their musical and artistic practices, particularly within\nreal-time scenarios and interactive environments.\nOur musical agent systems, MACAT and MACataRT, are based on the mindset of using small data\nin music [10] and the principles of model crafting in visual arts with generative AI [11], as well as\nfostering music-making in co-creative spaces between human musicians and musical agents [12],\nincluding machine listening [13].\nTraining on a small, high-quality audio corpus enables our musical agent systems to closely align\nwith the specific musical nuances and stylistic preferences of collaborating musicians. In contrast to\nmodels that broadly reproduce generalized patterns derived from large-scale audio or MIDI datasets,\nour model is optimized to interpret and respond to the unique attributes embedded within a musician's\nwork, thereby facilitating the personalization of music systems. This focused approach enhances\nthe agent's capacity to function as a genuine creative collaborator, fostering stylistic coherence and\nadaptability that could otherwise be diluted in models trained on more generic, large-scale audio or\nMIDI corpora.\nOur musical agent systems are highly flexible, enabling artists to train the models using curated\ndatasets tailored specifically to their unique stylistic preferences. This approach, which we term\n\"model crafting,\" allows artists to incorporate their own recorded data into the training process,\ncreating an agent that is not bound by predefined genres or styles but is instead highly personalized\nto their artistic identity. By selecting and shaping the training corpus, artists can ensure that the\nmusical agent aligns closely with their aesthetic vision, resulting in outputs reflecting their individual\nvoices and creative nuances. This flexibility makes the system adaptable to a wide range of musical\nexpressions, empowering artists to use AI not as a generic accompaniment tool but as a customizable\nco-creator that can evolve with their artistic practice."}, {"title": "2 Workflow of Musical Agent Systems", "content": ""}, {"title": "2.1 Workflow of MASOM and MACAT", "content": "Musical Agent based on Self-Organizing Maps (MASOM [14]) is a machine improvisation system\nfor live performances, particularly suited for experimental music and free improvisation. It integrates\nself-organizing maps (SOM [15]) as a sound memory, Variable Markov Models (VMM [16]) to\nrecognize and generate musical structures, and affective computing for real-time audio analysis."}, {"title": "2.2 Workflow of CataRT and MACataRT", "content": "The IRCAM's CataRT system [6] is a real-time corpus-based concatenative synthesis tool designed\nfor interactive sound exploration by selecting sound units from a database based on audio descriptors.\nImplemented in Max/MSP, the system organizes sounds within an audio descriptor space, enabling\nusers to target specific audio features, such as pitch or timbre, as illustrated in the workflow of Figure 1\n(c). Building on traditional granular synthesis, the system utilizes a curated corpus of segmented audio\ncrafted by artists, enabling precise control over sound characteristics through concatenative sound\nsynthesis. Its versatile applications span interactive sound synthesis, gesture-controlled synthesis,\nlive audio resynthesis, and expressive speech synthesis, offering a flexible and powerful interface for\ncreative sound exploration.\nMACataRT, as depicted in the workflow in Figure 1 (d), is an enhanced version of the CataRT system,\nincorporating a temporal model based on the factor oracle and offering a more intuitive interface for\nsound synthesis and resampling. The original CataRT system, while capable of reactive improvisation\nthrough real-time machine listening, clusters audio segments using K-Nearest Neighbors (KNN [18])\nand targets specific audio features for audio mosaicing. However, it lacks a temporal model to manage\ntime-based musical structures. To address this limitation, MACataRT integrates the factor oracle to\nautomate the generation process, building on CataRT's audio mosaicing capability. Audio mosaicing\n[19] assembles audio segments into new audio pieces by selecting segments that match target audio\nfeatures specified by musicians, who then refine the output to replicate the desired characteristics.\nMACataRT enhances this process with interactive audio mosaicing that functions in both real-time\nand offline modes. In real-time, the musical agent facilitates reactive improvisation, responding to\nlive inputs based on machine listening and targeting audio features without using the factor oracle. In\nits proactive improvisation mode, the musical agent system learns sequences of audio segment indices\nduring offline training, enabling the factor oracle to generate music based on these learned sequences.\nThis dual capability allows musicians to interactively play alongside MACataRT, fostering dynamic,\ncreative exchanges. Details on the interface of MACAT and MACataRT systems are provided in\nAppendix C."}, {"title": "3 Research-Creation and Musical Practice", "content": "The research-creation methodology in musical practice [20], especially in performances with musical\nAI agents, combines scholarly inquiry with creative experimentation, focusing on co-creation between\nhumans and AI systems. Musicians interact with AI agents as collaborative partners, capable of\nresponding to real-time inputs and aligning with the performer's expressive intent. This relationship\nfosters a dynamic musical dialogue where AI agents generate, adapt, and influence live performances,\nbroadening the possibilities for improvisation, stylistic adaptation, and spontaneous composition.\nThis approach merges artistic exploration with technical research, offering insights into AI's creative\npotential in music and redefining the musician's role in AI-augmented performances."}, {"title": "3.2 Musical Application and Artistic Practice", "content": "The application of each musical agent varies, offering musical affordances that support the research-\ncreation in real-world musical performance scenarios. MACAT, for instance, is particularly effective\nin solo performance settings, where its improvisational output adapts through a self-listening process\nthat allows it to take the lead in most musical contexts. MACAT was showcased at the MusicAcoustica\nFestival in Hangzhou, China, by the artist collective K-Phi-A, featuring Philippe Pasquier on live\nambient electronics, Keon Ju Maverick Lee on percussions, and VJ Amagi providing audiovisuals.\nMACataRT expands creative possibilities through audio mosaicing, supporting both reactive and\nproactive improvisation and proving its adaptability across diverse collaborative settings. Its practical\neffectiveness was demonstrated in a live performance, where it was used by a percussionist (Keon)\nand a guitarist (Sara) to co-create music. This collaboration earned significant recognition when\ntheir piece, Echoes of Synthetic Forest by the music duo KeRa, was selected as one of the Top 10\nfinalists in the 2024 AI Music Song Contest [21] and performed in Z\u00fcrich, Switzerland [22]. These\nachievements highlight the value of research-creation, showcasing how musical agents can enhance\nreal-time artistic expression and foster collaborative exploration. Our musical agent systems and\nshowcases are available for public access on the Metacreation Lab's GitHub repository2."}, {"title": "4 Conclusion and Future Work", "content": "Our research highlights the potential of the musical agents MACAT and MACataRT to enable creative\ncollaboration between AI and human musicians, offering a novel approach to interactive music-\nmaking through real-time generative AI. These systems prioritize the preservation of expressive\nnuances by employing corpus-based concatenative synthesis and small-data training methods, allow-\ning the musical agents to act as responsive co-creators in diverse performance contexts. MACAT and\nMACataRT demonstrate how artist-in-the-loop AI agents can significantly broaden the creative op-\ntions for musicians, providing practical tools that integrate into live performances and improvisational\nsettings. Ethical considerations related to our musical agents are discussed in Appendix A.\nIn future work, we aim to enhance both the temporality and explainability of our musical agent\nsystems. Currently, these systems utilize conventional audio mosaicing techniques and the factor\noracle algorithm for temporal modelling to identify and generate musical patterns. To improve\ntemporality, we plan to integrate deep learning architectures that enable agents to learn and retain\nlonger sequences of musical patterns. For enhanced explainability, we intend to incorporate a module\nthat records the history of past musical patterns, thereby advancing comprehension from a note-level\nto a bar-level understanding. Additionally, we aim to advance the machine listening module to deepen\nthe agents' musical comprehension. A feedback loop using reinforcement learning may also be\nintroduced to further enhance the adaptability of musical agents in real-time performance contexts."}, {"title": "A Ethical Implication", "content": "Our musical agent systems present multiple advantages over large-scale generative AI models, particularly\nregarding ethical considerations and data transparency. In contrast to big data models, which often lack\ntraceability and explainability regarding how their training data influence generated outputs-thereby risking the\nincorporation of stylistic elements from other musicians without consent our systems utilize small, personalized\ndatasets of music recordings created by specific musicians. This strategy not only promotes more ethical use of\nAI in generative music by respecting the intellectual property rights and artistic contributions of other composers\nbut also enhances the transparency and accountability of the creative process.\nBy employing small, personalized datasets and well-documented training data, our systems facilitate straight-\nforward tracking of how specific pieces of music contribute to and shape the generated content. This approach\nprovides a clearer explanation of the AI's creative influences, ensuring that the resulting music is original and\nethically produced. Additionally, using curated, smaller datasets reduces the likelihood of inadvertently mimick-\ning the styles or techniques of musicians who have not explicitly agreed to contribute to the training process,\nthereby maintaining artistic integrity and reducing the potential for ethical breaches related to appropriation or\nunintentional plagiarism.\nMoreover, the reliance on small, personalized datasets enables more precise control over the creative direction\nof the Al's outputs. It allows musicians and composers to tailor the generative process to align with their\nartistic vision and preferences, resulting in a more meaningful collaboration between human creativity and\nAI technology. This level of customization is often unattainable in large-scale generative models, where the\nvastness and anonymity of the data pool obscure the specific influences that guide the AI's creative decisions.\nThe transparency afforded by our approach enhances the accountability of AI-generated music. It allows for a\nclear audit trail, where the contributions of individual pieces to the final output can be explicitly identified and\nverified. This feature is crucial in contexts where the ethical implications of AI usage are scrutinized, such as in\nacademic research, commercial music production, or public performances. By maintaining a transparent and\nethical framework, our musical agent systems not only adhere to high standards of creative responsibility but\nalso foster trust and confidence among users, collaborators, and audiences.\nWe acknowledge the potential connection between our musical agent systems and music copyright in case\nusers may be involved in training on existing corpora created by other musicians. However, even in this\nscenario, our approach closely aligns with the typical practices of DJs who sample, mix, and manipulate musical\nworks, ensuring proper attribution through track identification for each song, as well as with the approaches of\nelectroacoustic musicians working in musique concr\u00e8te, which heavily incorporate sound design and sampling\ntechniques. As a result, the ethical considerations surrounding our system are far distant and different from\nrecent ongoing cases of alleged copyright infringement and exploitation, such as those involving Sono and Udio\nin 2024, without acknowledging musicians with the training big data for their AI models. We are committed\nto opposing any form of copyright infringement or exploitation of musicians. Furthermore, most users of our\nsystem are techno-fluent musicians, composers, and sound artists who collaborate with Al systems to expand\nand explore their artistic and creative potential by training their composition and corpora.\nAside from the above points, training AI models on smaller datasets is more environmentally friendly than\ntraining on large datasets because it requires fewer computational resources and consumes less energy, resulting\nin a smaller carbon footprint. Our musical agent systems do not require external GPUs for training; they have\nbeen effectively trained using only the Apple M2 CPU found in a MacBook Pro or even older MacBook models\nequipped with an Intel core i7 CPU. Small data models lead to quicker, more efficient training without GPUs or\ncloud computing systems, reducing energy consumption and aligning with efforts to combat climate change.\nAdditionally, this approach encourages the development of optimized, energy-efficient algorithms and minimizes\nthe need for frequent retraining. Focusing on small, high-quality datasets also promotes targeted AI development\nand addresses ethical concerns related to data privacy. Overall, using small datasets supports sustainable AI\ndevelopment by reducing environmental impact and fostering responsible technological innovation.\nIn summary, our musical agent systems leverage the ethical and creative benefits of small, personalized datasets,\noffering a more transparent, sustainable, and customizable approach to generative music AI systems. This method\nnot only safeguards the rights and contributions of individual musicians but also ensures that the generated music\nis innovative, respectful, and aligned with the ethical standards of contemporary artistic and musical practice."}, {"title": "B Detailed Explanation of Musical Agent MASOM", "content": "We provide a comprehensive explanation of the latest version of the MASOM musical agent we extended,\ndrawing on references to the closely related MACAT and MACataRT agents, which share similar architecture\nand concepts."}, {"title": "B.1 Interface of MASOM", "content": ""}, {"title": "B.2 Analysis for MASOM Architecture", "content": "Figure 3 shows the architecture of MASOM from the original paper. It is important to note from the Figure\nthat the current MASOM architecture is not equipped with the VOMM (Variable-Order Markov Model) but\nwith Factor Oracle (FO). FO in generative music is an algorithmic tool that leverages pattern recognition and a\ndirected acyclic graph structure to identify, store, and utilize recurring musical patterns. It contributes to the\ngeneration of music by allowing for the exploration and manipulation of learned patterns to create new and\ncoherent musical sequences.\nMASOM is analyzed for acting in an environment based on the conventional AI agent framework:\nPrior knowledge: The audio recordings (chosen by users) are used to train by applying AI algorithms,\nand the audio would be input data for this model. So, the musical agent should be pre-trained first\n(offline training) to use it during live performance.\nPast actions: Automatic music generation in the agent is based on the training data (in audio recordings),\nand the recordings are converted to multiple audio segments using the multi-granular segmentation\nbased on the Fast Fourier Transform algorithm. The musical agent learns to generate music based\non musical memory (from self-organizing map nodes) and statistical learning (learning through the\nconnectivity of each SOM node). The training process is based on past actions, in this case, past audio\nsegments generated by the agent, and it would generate the next node based on past played nodes\n(non-episodic).\nGoal & values: The goal of the agent is automatic music composition and free improvisation for\noriginal music content based on the training data's style of music. SOM is an artificial neural network\nthat represents, visualizes, and clusters high-dimensional input data in a 2-dimensional topology based\non each calculated feature vector. For each SOM node value, SOM clusters input data using the Best\nMatching Unit (BMU) node, and each square node displays calculated feature vectors. The SOM\ntopology is normalized between -1.0 (black) and 1.0 (white).\nObservations: The agent can efficiently listen to a massive amount of music by extracting and analyzing\naudio features as part of the machine listening module. The agent partially sees the environment\nbecause it can perceive the current status (SOM nodes are visualized in real-time) and can control\nmusical parameters to generate different outputs. However, the musical agent cannot control the\nstochastic nature of the machine learning environment, so the output would be different even if the\nmusical parameters are the same.\nInternal structure:\n(a) Architecture and program: MASOM uses a music programming environment called\nMAX/MSP/Jitter. The musical agent uses a cognitive architecture based on the multi-agent\nsystem. Moreover, the agent is considered a sonic software agent rather than a physical agent,\nsuch as a musical robot.\n(b) Knowledge of the environment: MASOM's environment is based on training data (in audio\nrecordings) and extracted audio features from the training data.\n(c) Reflexes: The musical agent has non-reflex actions because it executes an action based on\ntraining data (fixed audio corpus by a user), and the agent should be pre-trained before the\nperformance.\n(d) Goals: The goal is automatic music composition and free improvisation for original music\ncontent based on the training data's style of music. However, knowing a goal is challenging,\nconsidering MASOM is a musical (creative) agent, so it is difficult to define a problem, unlike\nmost problem-solving agents. Moreover, music tends to be subjective for evaluation, so there is\nno universal method for evaluating musical metacreation tasks. However, there are use cases in\nwhich the agent can improvise during live performances.\n(e) Utility functions: The agent's architecture consists of self-organizing maps (musical memory,\nvisualization, unsupervised learning), Variable Markov models and Factor Oracle for learning a\nmusical structure and affective computing for machine listening based on the Circumplex model\nof affect.\nDetails of the environment:\n(a) Accessibility & Determinism: MASOM environment is accessible only for training data based\non a specific format of audio recordings (.wav, 16-bit, and 44.1 kHz), not fully accessible for the\nother data formats, such as MIDI, symbolic sheet music, and non-waveform audio formats. The\nenvironment is non-deterministic because the agent cannot recognize the exact state of the world\nafter the agent's action. Moreover, the generated audio output by the agent would be different in\nevery iteration because of the stochastic nature of the machine learning algorithms.\n(b) Episodes: In the environment, the training process uses statistical learning based on past actions\n(the composition of training data). In this case, all audio segments should be analyzed and learned"}, {"title": "B.3 Machine Listening System in MASOM", "content": "The machine listening module incorporates audio feature extraction, affect estimation and statistics calculation.\nThe affect estimation algorithm in the generation module is an online version. MASOM computes audio feature\nstatistics during the sample duration played by the musical action module. Upon triggering a new sample, the\nmachine listening module resets all statistics. It produces a 31-dimensional vector (audio and affective features)\nfor the musical action module.\nThe musical action module determines distances between the machine listening module's vector and the agent's\nSOM nodes to identify the Best Matching Unit (BMU), representing the current perceived musical state. The\nBMU is then sent to FO, where a BMU history is stored to establish context. Using this context, FO predicts the\nnext SOM node to be played. Each SOM node signifies a cluster of audio segments. When the preceding sample\nplayback concludes, the musical agent utilizes the predicted SOM node to select a cluster of audio segments. In\nthe final step, the agent randomly selects a sample within the cluster node to generate the audio output.\nThe current version of the machine listening system extracts five low-level audio features, and two affective\nfeatures (Valence and Arousal) are calculated based on the extracted audio features based on the automatic\nsoundscape affect recognition. All audio features are computed with a window size of 1024 samples (duration:\n23ms) and a hop size of 512 samples (duration: 12ms).\nThe extracted audio features are as follows:\n\u2022 Mel-Frequency Cepstral Coefficients (MFCC): The computation of MFFC involves merging the Mel-\nfrequency scale with a specific frequency spectrum calculation known as cepstrum. The Mel-frequency\nscale corresponds to the critical bands of human hearing. At the same time, the cepstrum represents\nthe discrete cosine transform (DCT) of the logarithm of the spectrum obtained through the Fast Fourier\nTransform (FFT). Our calculation yields 13 MFCCs, excluding the zero coefficient. The removal of\nMFCC0, which represents energy or DC offset, is part of the process.\n\u2022 Loudness: From a psychoacoustical perspective in music, loudness is the subjective perception of a\nsound's intensity, considering factors like frequency, duration, and sound pressure level. It accounts\nfor the human auditory system's sensitivity to different frequencies, with lower frequencies potentially\nperceived differently than higher ones, even with equal physical intensity. Psychoacoustic models aim\nto quantify the nuanced relationship between physical sound characteristics and how humans perceive\nloudness.\n\u2022 Spectral flatness: Spectral flatness, denoted as the ratio between the geometric mean and the arithmetic\nmean of the energy spectrum, indicates the noisiness relative to the sinusoidality of the spectrum.\nTo assess this, we calculate the spectral flatness in four frequency bands: 250-500Hz, 500-1000Hz,\n1000-2000Hz, and 2000-4000Hz. In Equation 2a, the Spectral-Flatness-1-mean represents the moving\naverage of the computed spectral flatness over the 250-500Hz band.\n\u2022 Perceptual Spectral Decrease: Perceptual spectral decrease in the psychoacoustic context of music\nrefers to the phenomenon where the human auditory system becomes less sensitive to high-frequency\nspectral components over time. This perceptual shift influences how we perceive and process the\nfrequency content of a musical sound or phrase as it progresses. The concept is crucial in understanding\nour subjective music experience, impacting factors like timbre perception, sound quality, and tonal\nbalance. In practical terms, it guides audio engineering and music production considerations to align\nwith human auditory perception.\n\u2022 Perceptual Tristimulus: Perceptual tristimulus in psychoacoustical context denotes three perceptual\nattributes-loudness, pitch, and timbre. This concept aims to capture and represent how the human\nauditory system perceives and distinguishes various musical sounds, providing a framework for\nquantifying and analyzing the perceptual dimensions of sound in a manner aligned with human\nauditory experiences. These parameters contribute to understanding how different musical elements\nshape the overall perception of a piece of music."}, {"title": "C Interface for Musical Agent Systems", "content": ""}, {"title": "C.1 Interface of the MACAT System", "content": ""}, {"title": "The valence and arousal are computed based on the equations (1) and (2) (STD= standard deviation):", "content": "\\documentclass{article}\n\\usepackage{amsmath}\n\n\\begin{document}\n\n\\begin{align}\n\\text{Valence} = &-0.169 + (0.061 \\times \\text{Loudness\\_Mean}) + \\\\\n&(0.588 \\times \\text{Spectral\\_Flatness\\_1\\_Mean}) + \\\\\n&(0.302 \\times \\text{MFCC\\_1\\_STD}) + \\\\\n&(0.361 \\times \\text{MFCC\\_5\\_STD}) - \\\\\n&(0.229 \\times \\text{Perceptual\\_Spectral\\_Decrease\\_STD}) \\\\\n&\\\\\\\\\\\\\n\\text{Arousal} = &-1.551 + (0.060 \\times \\text{Loudness\\_Mean}) + \\\\\n&(0.087 \\times \\text{Loudness\\_STD}) + (1.905 \\times \\text{Perceptual\\_Tristimulus\\_2\\_STD}) + \\\\\n&(0.698 \\times \\text{Perceptual\\_Tristimulus\\_3\\_Mean}) + (0.560 \\times \\text{MFCC\\_3\\_STD}) - \\\\\n&(0.421 \\times \\text{MFCC\\_5\\_STD}) + (1.164 \\times \\text{MFCC\\_11\\_STD})\n\\end{align}\n\n\\end{document}"}, {"title": "C.2 Interface of the MACataRT System", "content": ""}]}