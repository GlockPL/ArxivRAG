{"title": "Peak-Controlled Logits Poisoning Attack in Federated Distillation", "authors": ["Yuhan Tang", "Aoxu Zhang", "Zhiyuan Wu", "Bo Gao", "Tian Wen", "Yuwei Wang", "Sheng Sun"], "abstract": "Federated Distillation (FD) offers an innovative approach to distributed machine learning, leveraging\nknowledge distillation for efficient and flexible cross-device knowledge transfer without necessitating the\nupload of extensive model parameters to a central server. While FD has gained popularity, its vulnerability to\npoisoning attacks remains underexplored. To address this gap, we previously introduced FDLA (Federated\nDistillation Logits Attack), a method that manipulates logits communication to mislead and degrade the\nperformance of client models. However, the impact of FDLA on participants with different identities and the\neffects of malicious modifications at various stages of knowledge transfer remain unexplored. To this end,\nwe present PCFDLA (Peak-Controlled Federated Distillation Logits Attack), an advanced and more stealthy\nlogits poisoning attack method for FD. PCFDLA enhances the effectiveness of FDLA by carefully controlling\nthe peak values of logits to create highly misleading yet inconspicuous modifications. Furthermore, we\nintroduce a novel metric for better evaluating attack efficacy, demonstrating that PCFDLA maintains stealth\nwhile being significantly more disruptive to victim models compared to its predecessors. Experimental\nresults across various datasets confirm the superior impact of PCFDLA on model accuracy, solidifying its\npotential threat in federated distillation systems.", "sections": [{"title": "INTRODUCTION", "content": "Federated Learning (FL) has emerged as a prominent distributed machine learning paradigm that fully utilizes local data and\ncomputation power of mobile clients for training deep learning models in a privacy-preserving manner. Despite its significant\nadvantages over traditional machine learning methods, FL encounters several challenges, including low communication\nefficiency and device heterogeneity . Federated Distillation (FD) , a variant of FL, addresses these issues by\nexchanging only model outputs (knowledge) during training, avoiding the cumbersome transmission of model parameters.\nFD effectively combines FL and Knowledge Distillation (KD) to meet the demands of efficient communication and\nheterogeneous device environments, showcasing considerable advantages and applications in various emerging fields.\nDespite the remarkable success of FD, previous research efforts have focused on performance improvement , commu-\nnication optimization , or system compatibility , while overlooking security challenges. Specifically, FD is\nvulnerable to logits poisoning attacks, where the knowledge uploaded can be easily modified without adequate protection .\nThis vulnerability arises because the central server cannot directly monitor the local knowledge generation during FD training,\nallowing malicious participants to interfere with the model training process. Although pollution attacks in FL have been exten-\nsively studied , the field of poisoning attacks against FD systems remains underexplored. Furthermore, FD's unique"}, {"title": "", "content": "characteristics distinguish it from traditional FL, making previous FL attack methods unsuitable for FD. Consequently, there is a\npressing need for targeted research on attacks specific to FD.\nTo fill this gap, we propose FDLA (Federated Distillation Logits Attack) in our previous work, which is a logits manipulation\nattack method specifically designed for Federated Distillation (FD). FDLA strategically modifies the transmitted logits sent\nto the server to mislead the optimization process of the local client models. The proposed manipulation goal is to generate\nfalse logits with higher confidence, aligning the model output with incorrect classification results. This method not only causes\nthe model to make erroneous predictions but also injects a high degree of confidence into these incorrect predictions. FDLA\nsignificantly alters the transmission of logits in the FD process, subtly and effectively intervening in model training. Furthermore,\nthe FDLA attack method is executed without the need to modify any private data, paving a new path for interfering with the\nmodel training process. However, FDLA has some notable limitations: 1. Due to the direct FDLA attack at the model output\nstage, the model itself receives incorrect knowledge, causing it to lose the ability to identify correct results after multiple training\nrounds. Ultimately, without the attacker having the capability to predict correct answers in advance, the model can only modify\nthe output each time without independently determining the correct answer for each test. 2. FDLA has not explored the precision\nof the attacker's and honest participants' models, so the performance analysis of these two types of models before and after the\nattack remains unknown.\nIn this paper, we propose a new logits attack method to address the shortcomings of FDLA, called Peak-Controlled Federated\nDistillation Logits Attack (PCFDLA). PCFDLA adjusts the confidence of the logits closest to the correct answer target to a\npreset higher value, while setting the confidence of other non-target types to lower values. For instance, when interfering with\nthe training process of a dataset containing images of cats, the attacker adjusts the confidence of the label \"deer\" to make it\nthe most credible output result, thereby misleading other models into thinking that the cat images are of deer. PCFDLA only\nmodifies the uploaded knowledge incorrectly, ensuring that the knowledge used for its local training remains correct. It inherits\nall the advantages of FDLA and addresses the shortcoming of FDLA's inability to retain the capability to judge correct results.\nBy modifying the knowledge during the upload phase, it gains the ability to predict the correct answer for each sample. To better\nevaluate the performance of PCFDLA in attacking the precision of the models of both the attacker and honest participants, we\nalso propose a new evaluation metric based on FDLA's metrics. This new metric can test the impact of the attack on the accuracy\nof both the attacker's and non-attacker's models and assess the extent of damage caused by all attack methods. This allows for a\nmore detailed understanding of the effectiveness of the attack method.\nIn general, our contributions can be summarized as follows:\nWe propose a novel method, PCFDLA, for logits poisoning attack in Federated Distillation (FD). PCFDLA manipulates the\nconfidence levels of logits during training, enableing the model to more accurately select incorrect but seemingly reasonable\nresults to achieve the purpose of deception.\nWe propose a new metric to evaluate the effectiveness of different attack methods by recording the model accuracy of both\nthe victims and malicious attackers before and after the attack. Our proposed metric can more clearly and comprehensively\nshow the extent of damage caused to the victims by different attacks.\nWe evaluated our proposed method with multiple settings on the CINIC-10, CIFAR-10, and SVHN datasets. The results\nshow that PCFDLA achieves a more significant reduction in model accuracy compared to FDLA and baseline algorithms.\nAdditionally, we proposed more detailed and extensive experiments than FDLA, including testing the recognition accuracy\nof both the attackers and honest participants before and after poisoning, as well as testing the attack intensity of PCFDLA."}, {"title": "RELATED WORK", "content": ""}, {"title": "Poisoning Attack in Federated Learning", "content": "In recent years, researchers have extensively focused on poisoning attack strategies that compromise the performance of federated\nlearning models, proposing various countermeasures. These strategies include implicit gradient manipulation to attack sample\ndata , unlabeled attacks , backdoor attacks by replacing components of the model , and the introduction of fake clients to\ndistort the global model's learning process . Additionally, a technique called VagueGAN employs GAN models to generate\nblurry data for training toxic local models, which are then uploaded to servers to degrade the performance of other local models .\nThese attack methods have been well-studied and have influenced traditional defense mechanisms. While these methods show\nexcellent performance in terms of attack efficiency and covert behavior within classical federated learning frameworks, they are"}, {"title": "Federated Distillation", "content": "As an emerging paradigm in federated learning, Federated Distillation (FD) applies knowledge distillation for model optimization\nand has garnered increasing attention in recent years . Specifically, FD effectively overcomes system heterogeneity through\ncollaborative optimization and knowledge sharing at the client level . By guiding local models towards different learning\nobjectives, FD meets the personalized needs of clients . It reduces the communication bandwidth required for training by\ntransmitting only lightweight model outputs . Through model-agnostic interaction protocols, it enables the training of\nlarger models on the server . Furthermore, by conducting self-distillation on the client side, FD mitigates client drift\ncaused by data heterogeneity or incremental characteristics . However, existing methods primarily focus on performance\nimprovements of FD algorithms and lack exploration of potential security threats in FD . Although FDLA investigates\nthe impact of logits poisoning attacks in FD, participants with different identities and the effects of malicious modifications at\nvarious stages of knowledge transfer remain unexplored."}, {"title": "APPROACH", "content": ""}, {"title": "General Process of Federated Distillation", "content": "In the context of FD, each client k \u2208 {1, 2, ..., K} optimizes its model parameters by combining the cross-entropy loss function\n$L_{CE}$() on local data $D_k$ and the distillation loss function $L_{KD}$ on the global knowledge $Z_S$ provided by the server. The server\ncoordinates the collaborative learning process by aggregating the logits (softmax outputs) from all clients. This iterative training\nprocess involves many communication rounds, each consisting of a local optimization phase and a global aggregation phase,\nultimately updating the local model parameters. For more details on the main notations used in this paper, see Table 1.\nRegarding the local optimization phase, each client k optimizes the following objectives:\n$\\min\\limits_{W_k} J_k \\triangleq [J_{CE} + \\beta \\cdot J_{KD}], \\\\(X_k, y_k) \\sim D_k$\nwhere \u03b2 is the distillation weighting factor, and $(X_k, y_k)$ corresponds to the samples and their associated labels of the local dataset\n$D_k$. The optimization components $J_{KD}$ and $J_{CE}$ can be expressed using the following equations:\n$J_{CE} = L_{CE}(f(W_k; X_k), y_k)$,\n$J_{KD} = L_{KD}(f(W_k; X_k), Z_S)$,\nwhere $W_k$ denotes the model parameters of client k, and $f(W_k; \\cdot)$ is the prediction function on client k. The server performs the\naggregation operation over clients' extracted knowledge $Z_k$, and can be typically formulated as knowledge averaging:\n$A(Z_k) = \\frac{1}{K}\\sum_{k \\in \\{1,2,...,K\\}} Z_k$\nUpon acquiring global knowledge on the server, it is disseminated to all clients. In response, each client proceeds to update\ntheir local models with learning rate $lr$ to align with this shared knowledge, with the process formulated as follows:\n$W_k = W_k - lr \\cdot \\triangledown_{W_k}J_k(W_k)$."}, {"title": "FDLA", "content": ""}, {"title": "PCFDLA", "content": "FDLA is a malicious method specifically designed for Federated Distillation (FD), where malicious clients interfere with the\njoint learning process of the model by altering the confidence levels of the logits output on local data. \"Confidence\" refers to the\nmodel's certainty in its predictions, indicating the reliability of the model's predictions for a given input. When attackers modify\ntheir own model's confidence values, it can lead to decision biases, thereby affecting the final results. Since the training process\nof federated distillation heavily relies on the output knowledge of the models, with confidence being an important component of\nthis knowledge, modifying the distribution of confidence levels can lead to changes in the model's final output. Intentionally\nmanipulating the rearrangement of confidence values can result in performance changes for both the attacker's model and other\nmodels.\nSpecifically, the client convinces itself to accept the incorrect answer first and uses this incorrect answer for learning, uploading\ntheir erroneous knowledge $Z_k$ to the server. This process involves sorting and transforming the logits values $C$ for each sample,\nwhere $C = [c_1, c_2, ..., c_n]$ contains confidence levels corresponding to each label, with $c_i$ representing the confidence of the i-th\nelement, assuming n is the total number of corresponding labels.\nThe transformation process of FDLA is illustrated in Figure 1: First, the confidence values $c$ are sorted to obtain the sorted\nindices $I$. Then, a transformation mapping $t$ is defined, mapping the element with the highest confidence to the position of the\nlowest confidence, while the remaining elements are shifted up one position.When the manipulated knowledge $Z_k$ from the\nmalicious client is uploaded to the aggregation server, the server performs the aggregation operation $A$. The aggregated global\nknowledge is then distributed to each client, which updates their local models based on this information. However, because the\nattacker first accepts the incorrect answer, their own model will prioritize this incorrect knowledge, leading to a degradation of\ntheir own recognition ability. Although the logits stored on the server are polluted after aggregation and ultimately affect the\nlogits sent to each client, resulting in errors throughout the training process, this is not a smart or efficient approach.\nTo be more specific, FDLA execution process is formulated into three steps:\n(1) Sort the confidences c to obtain the sorted indices I.\n(2) Define a transformation mapping t, where t[I[1]] = I[n], and for $2 \\le k \\le n$, $t[I[k]] = I[k - 1]$, where I[k] is the index of the\nk-th highest confidence.\n(3) Apply the transformation mapping t to the original confidence vector c to obtain the transformed vector c'.\nWe express the operation $C_L(c)$ as follows:\n$c'_i =\\begin{cases} c_{t[i]}, & \\text{if } r(c_i) > 1 \\\\c_{t[i]}, & \\text{if } r(c_i) = 1 \\end{cases}$\nwhere $c'_i$ is the i-th element in the transformed confidence vector c', $r(c_i)$ is the rank of $c_i$ in $c$, and t[i] is the transformed index."}, {"title": "PCFDLA", "content": "Despite the success of FDLA, it did not consider the importance of maintaining the attacker's own ability to accurately judge\nsamples. PCFDLA is an enhanced and controllable malicious attack method specifically designed for FD, and it is smarter than\nFDLA. Although malicious clients also disrupt the joint learning process of the model by transforming the confidence levels of\nlogits on local data, attackers only modify the uploaded data and do not modify the local training results. More importantly,\nwhen modifying confidence values, the attackers are aware of which incorrect answers are most misleading and make the most\nimpactful changes to affect the final result. When this modified answer is used as a reference answer by other models, it may\ncause the decline of their performance. Specifically, clients do not share their correct answers with others but upload their\nerroneous knowledge $Z_k$ to the server. This manipulation involves transforming the logits values $C$ for each sample, where\n$C = [c_1, c_2, ..., c_n]$ contains confidence levels corresponding to each label, with $c_i$ representing the confidence of the i-th element,\nand n representing the total number of corresponding labels.\nThe transformation process of PCFDLA is illustrated in Figure 2: First, the confidence values $c$ are sorted to obtain the sorted\nindices $I$. Then, a transformation mapping $t$ is defined, mapping the element with the highest confidence to a set value $S$, while\nthe remaining elements are mapped to the opposite of the set value. When the manipulated knowledge $Z_k$ from the malicious\nclient is uploaded to the aggregation server, the server performs the aggregation operation $A$. The aggregated global knowledge\nis then distributed to each client, which updates their local models based on this information. Since the attacker's model does\nnot first accept and learn this incorrect answer but instead tells everyone the incorrect answer first to gain their acceptance, the\nattacker will know which incorrect answer is the most misleading each time they encounter a new test and upload the answer."}, {"title": "", "content": "Once all the tests are passed, meaning the logits stored on the server after aggregation are already polluted, and the attacker\nand other models have accepted the incorrect answer, the attacker then convinces themselves to accept the incorrect answer\nlike the other models, completing the brainwashing of all models and ending the attack, ultimately causing erroneous guidance\nthroughout the training process.\nWhen uploading local data $D_k$ at the $t$-th time, the malicious client k modifies the confidence of the knowledge generated\nfor the i-th sample $X_i$ in the current communication round, denoted as $Z^t_i$. We define a confidence ranking function $r()$ and a\nconfidence tampering function $x()$. The ranking function $r(c_i)$ maps $c_i$ to its rank in the confidence vector (the highest confidence\nis ranked 1, the second highest is 2, and so on). Then, the transformation function $x(c_2)$ tampers $c_2$ to a new confidence $S$, where\n$S$ is a preset hyperparameter value. Besides mapping the second highest confidence to the preset confidence, other confidences $c_i$\nis a preset hyperparameter value. Besides mapping the second highest confidence to the preset confidence, other confidences $-S$.\nThis transformation can predict the most similar result as the correct result and completely negate the correctness of other labels\nto further reduce recognition accuracy.\nOur proposed execution process is formulated into three steps:\n(1) Sort the confidences c to obtain the sorted indices I.\n(2) Define a transformation function x, where $x(c[I[2]], S)$, and for $1 \\le k \\le n$ except $k = 2$, $x(c[I[k], -S)$, where I[k] is the index\nof the k-th highest confidence.\n(3) Apply the transformation function x to the original confidence vector c to obtain the transformed vector c'.\nWe express the operation $C_L(c)$ as follows:\n$c'_i =\\begin{cases} S, & \\text{if } i = 2 \\\\-S, & \\text{if } i \\neq 2 \\end{cases}$\nAmong them, $c'_i$ is the i-th element in the transformed confidence vector c', i is the index of the confidence vector, and S is the\npreset confidence value.\nOriginally, the value $c_2$, which was identified as merely the closest to the correct result, is considered the most correct result,\nwhile other possibilities $c_i$, including the correct answer $c_1$, are firmly regarded as incorrect. When the malicious attacker's $Z_k$ is\nuploaded to the aggregation server, the server performs the aggregation operation $A$ as shown in Algorithm 2.\nAfter obtaining the aggregated global knowledge, it is distributed to each client. Subsequently, the clients update their local\nmodels based on this newly acquired information, as shown in Algorithm 1. During this process, since the logits stored on the\nserver after aggregation have already been polluted, the logits ultimately sent to each client are affected, leading to a misled\ntraining process."}, {"title": "", "content": "Algorithm 1 FDLA or PCFDLA on Client k\nrepeat\nfor client k in K do\n$W_k \\leftarrow W_k-lr \\nabla_{W_k}J_k(W_k)$; according to Eq.(5)\n$Z_k \\leftarrow f(W_k,X_k)$;\nif client k is venomous client then\nif algorithm = FDLA then\n$Z_k\\leftarrow C_L (X_k)$; according to Eq.(6)\nend if\nif algorithm = PCFDLA then\n$Z_k\\leftarrow C_L(c)$; according to Eq.(7)\nend if\nend if\nUpload $Z_k$ to the server;\nend for\nuntil Training stop ;\nAlgorithm 2 FDLA or PCFDLA on the Server\nrepeat\nrepeat\nReceive $Z_k$\nuntil Receive all $Z_k$ from K clients;\n// aggregation process\n$Z_S \\leftarrow A(Z_k)$ according to Eq(4);\ndistribute $SZ$ to all clients;\nuntil Training stop;"}, {"title": "Victims Oriented Metric in FD", "content": "To evaluate the effectiveness of PCFDLA on FD systems, a rigorous assessment of its performance using standard and well-known\nmetrics is required for fair comparison. The evaluation should cover multiple aspects to determine the system's effectiveness and\npracticality, including the average recognition accuracy of the models. Given the way FDLA attacks FD systems, certain metrics\nmay be more important than others, such as the change in accuracy of the victims as a whole before and after the attack. To\nensure scientific rigor, it is crucial to provide detailed metrics for effectively evaluating the proposed method.\nFDLA has proposed several evaluation metrics to measure the accuracy and robustness of various misleading attacks. These\nmetrics typically focused on the average accuracy of the entire model. We report the current metrics used to evaluate the\nperformance of misleading attack methods on the FD system and introduce a new evaluation metric based on this: the change in\naccuracy of the victim group.\nSpecifically, the previous research work metrics mainly calculate the average model accuracy across various scenarios:\ndifferent proportions of malicious attackers on various datasets, different data distributions, different numbers of clients, and\ndifferent model structures."}, {"title": "", "content": "The aim is minimize the the following objective function:\n$TolAvgAcc = \\frac{1}{K} \\sum_{k=1}^{K} Acc$\nwhere TolAvgAcc represents the average accuracy of all models, K denotes the total number of client devices, and Acc represents\nthe accuracy of each model.\nHowever, these metrics do not directly show the impact on the victim group. In PCFDLA, by analyzing and comparing the\naccuracy changes of all victim models, we can more directly observe the interference of the attack on the performance of the\nvictim models. Additionally, since PCFDLA has controllable attack intensity, we have added an evaluation of the impact of\ndifferent attack intensities on both the original and new metrics in this paper. Specifically, we introduce the following five\nnew metrics: the average model accuracy of all models and the average model accuracy of all victim models under different\nproportions of malicious attackers on various datasets, different data distributions, different numbers of clients, different model\nstructures, and different attack intensities.\nThe aim is minimize the the following objective function:\n$VctmAvgAcc = \\frac{1}{K_V} \\sum_{k_v=1}^{K_V} Acc$\nwhere VctmAvgAcc represents the average accuracy of models from all victims, $K_V$ denotes the total number of attacked client\ndevices, and Acc represents the accuracy of each model.\nBy introducing the above new metrics, we can more clearly show the impact of different attack methods on the target models."}, {"title": "EXPERIMENTS", "content": ""}, {"title": "Experimental Setup", "content": "We conducted experiments on three common datasets: CINIC-10, CIFAR-10, and SVHN. Each dataset was divided\ninto 50 non-i.i.d. (non-independent and identically distributed) parts, using the hyperparameter \u03b1 to control the degree of data\nheterogeneity, set to 1.0. We ensured that all models reached convergence. We evaluated our method under two FD frameworks,\nnamely FedCache and FD. We adopted the $A_f$ client model architecture from . Other hyperparameters were consistent with\nthose used in ."}, {"title": "Baselines", "content": "We compared our proposed PCFDLA algorithm with three baseline algorithms from FDLA that modify logits during training:\nRandom poisoning randomly modifies logits, turning their values into random numbers between 0 and 1; Zero poisoning sets\nall logits to zero before uploading; Misleading attack (FDLA) uses a transformation mapping t that maps the element with the\nhighest confidence to the position of the lowest confidence, while the remaining elements move up one position in the ranking."}, {"title": "Results", "content": "Based on the main experimental setup, the final results are shown in Tables 2, 3, and 4. The data indicate that under both FD and\nFedCache algorithms, whether on the SVHN, CIFAR-10, or CINIC-10 datasets, the PCFDLA attack has the most significant\nimpact on model accuracy in all scenarios, causing an accuracy loss of up to 5% - 20%. In contrast, the attack strategies proposed\nin this paper (random poisoning, zero poisoning, and misleading attacks) do not significantly affect the model. In some cases,\nthey can even slightly improve the model's recognition rate. As the proportion of malicious attackers increases, the effectiveness\nof PCFDLA further strengthens, reducing the model recognition accuracy to around 20%. In contrast, the other three baseline\nalgorithms show no noticeable change in model accuracy interference. In the FD algorithm, with 30% malicious attackers,\nPCFDLA proves to be the most effective, achieving the lowest recognition accuracy of 31.22%. Similarly, in the FedCache"}, {"title": "Ablation Study", "content": "The ablation study is divided into three parts, aiming to explore the relationship between hyperparameters and performance:\nThe experimental default configuration is the same as the main experiment, but only the SVHN dataset is used. The basic FD\nalgorithm used is FedCache, and the poisoning ratio is set to 20%.\nThe first part examines the robustness of PCFDLA concerning data distribution:\nThis experiment compares the effects of three different poisoning methods by setting \u03b1 to three different values: 0.5, 1.0, and\n3. This allows us to explore how the effectiveness of PCFDLA is influenced by the distribution of data among clients.\nThe second part investigates the robustness of PCFDLA concerning the number of participating clients:\nThis experiment sets up scenarios with different numbers of participants, specifically 10, 50, and 200, to compare the effects\nof three different poisoning methods. This investigation aims to understand how the effectiveness of FDLA is influenced by\nthe number of clients in the federated learning network.\nThe third part of the study explores the robustness of PCFDLA concerning model architecture:\nThis experiment evaluates the performance of three different poisoning methods when configured with homogeneous\nor heterogeneous models to further study the robustness of PCFDLA to model structure. The experiment considers the\nhomogeneity and heterogeneity of client models. Specifically, in the homogeneous model experiment, all clients use the\nsame model architecture, denoted as $A_f$. In the heterogeneous model case, FedCache assigns different model architectures to\nclients based on the client index modulo 3. When the index modulo 3 equals 0, 1, and 2, the assigned model architectures are\n$A^c_1$, $A^c_2$, and $A^c_3$, respectively. That is:\n$A_f = A^c_{(i \\bmod 3)+1}$"}, {"title": "CONCLUSION", "content": "This paper introduces a novel logits poisoning attack method, PCFDLA, for federated distillation. PCFDLA modifies the logits\nsent from the client to the server, guiding the model to generate high-confidence incorrect predictions. It can control the strength\nof the attack, thereby interfering with model training at different intensities. During Federated Distillation, PCFDLA subtly\ninfluences model training by redesign the knowledge-sharing mechanism. Experimental results demonstrate the strong attack\neffect of PCFDLA across various settings."}, {"title": "CONFLICT OF INTEREST", "content": "The authors declare no potential conflict of interests."}]}