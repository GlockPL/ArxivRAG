{"title": "Peak-Controlled Logits Poisoning Attack in Federated Distillation", "authors": ["Yuhan Tang", "Aoxu Zhang", "Zhiyuan Wu", "Bo Gao", "Tian Wen", "Yuwei Wang", "Sheng Sun"], "abstract": "Federated Distillation (FD) offers an innovative approach to distributed machine learning, leveraging knowledge distillation for efficient and flexible cross-device knowledge transfer without necessitating the upload of extensive model parameters to a central server. While FD has gained popularity, its vulnerability to poisoning attacks remains underexplored. To address this gap, we previously introduced FDLA (Federated Distillation Logits Attack), a method that manipulates logits communication to mislead and degrade the performance of client models. However, the impact of FDLA on participants with different identities and the effects of malicious modifications at various stages of knowledge transfer remain unexplored. To this end, we present PCFDLA (Peak-Controlled Federated Distillation Logits Attack), an advanced and more stealthy logits poisoning attack method for FD. PCFDLA enhances the effectiveness of FDLA by carefully controlling the peak values of logits to create highly misleading yet inconspicuous modifications. Furthermore, we introduce a novel metric for better evaluating attack efficacy, demonstrating that PCFDLA maintains stealth while being significantly more disruptive to victim models compared to its predecessors. Experimental results across various datasets confirm the superior impact of PCFDLA on model accuracy, solidifying its potential threat in federated distillation systems.", "sections": [{"title": "INTRODUCTION", "content": "Federated Learning (FL) has emerged as a prominent distributed machine learning paradigm that fully utilizes local data and computation power of mobile clients for training deep learning models in a privacy-preserving manner. Despite its significant advantages over traditional machine learning methods, FL encounters several challenges, including low communication efficiency and device heterogeneity . Federated Distillation (FD), a variant of FL, addresses these issues by exchanging only model outputs (knowledge) during training, avoiding the cumbersome transmission of model parameters. FD effectively combines FL and Knowledge Distillation (KD) to meet the demands of efficient communication and heterogeneous device environments, showcasing considerable advantages and applications in various emerging fields.\nDespite the remarkable success of FD, previous research efforts have focused on performance improvement , communication optimization , or system compatibility , while overlooking security challenges. Specifically, FD is vulnerable to logits poisoning attacks, where the knowledge uploaded can be easily modified without adequate protection . This vulnerability arises because the central server cannot directly monitor the local knowledge generation during FD training, allowing malicious participants to interfere with the model training process. Although pollution attacks in FL have been extensively studied , the field of poisoning attacks against FD systems remains underexplored. Furthermore, FD's unique characteristics distinguish it from traditional FL, making previous FL attack methods unsuitable for FD. Consequently, there is a pressing need for targeted research on attacks specific to FD.\nTo fill this gap, we propose FDLA (Federated Distillation Logits Attack) in our previous work, which is a logits manipulation attack method specifically designed for Federated Distillation (FD). FDLA strategically modifies the transmitted logits sent to the server to mislead the optimization process of the local client models. The proposed manipulation goal is to generate false logits with higher confidence, aligning the model output with incorrect classification results. This method not only causes the model to make erroneous predictions but also injects a high degree of confidence into these incorrect predictions. FDLA significantly alters the transmission of logits in the FD process, subtly and effectively intervening in model training. Furthermore, the FDLA attack method is executed without the need to modify any private data, paving a new path for interfering with the model training process. However, FDLA has some notable limitations: 1. Due to the direct FDLA attack at the model output stage, the model itself receives incorrect knowledge, causing it to lose the ability to identify correct results after multiple training rounds. Ultimately, without the attacker having the capability to predict correct answers in advance, the model can only modify the output each time without independently determining the correct answer for each test. 2. FDLA has not explored the precision of the attacker's and honest participants' models, so the performance analysis of these two types of models before and after the attack remains unknown.\nIn this paper, we propose a new logits attack method to address the shortcomings of FDLA, called Peak-Controlled Federated Distillation Logits Attack (PCFDLA). PCFDLA adjusts the confidence of the logits closest to the correct answer target to a preset higher value, while setting the confidence of other non-target types to lower values. For instance, when interfering with the training process of a dataset containing images of cats, the attacker adjusts the confidence of the label \"deer\" to make it the most credible output result, thereby misleading other models into thinking that the cat images are of deer. PCFDLA only modifies the uploaded knowledge incorrectly, ensuring that the knowledge used for its local training remains correct. It inherits all the advantages of FDLA and addresses the shortcoming of FDLA's inability to retain the capability to judge correct results. By modifying the knowledge during the upload phase, it gains the ability to predict the correct answer for each sample. To better evaluate the performance of PCFDLA in attacking the precision of the models of both the attacker and honest participants, we also propose a new evaluation metric based on FDLA's metrics. This new metric can test the impact of the attack on the accuracy of both the attacker's and non-attacker's models and assess the extent of damage caused by all attack methods. This allows for a more detailed understanding of the effectiveness of the attack method.\nIn general, our contributions can be summarized as follows:\n\u2022\nWe propose a novel method, PCFDLA, for logits poisoning attack in Federated Distillation (FD). PCFDLA manipulates the confidence levels of logits during training, enableing the model to more accurately select incorrect but seemingly reasonable results to achieve the purpose of deception.\n\u2022\nWe propose a new metric to evaluate the effectiveness of different attack methods by recording the model accuracy of both the victims and malicious attackers before and after the attack. Our proposed metric can more clearly and comprehensively show the extent of damage caused to the victims by different attacks.\n\u2022\nWe evaluated our proposed method with multiple settings on the CINIC-10, CIFAR-10, and SVHN datasets. The results show that PCFDLA achieves a more significant reduction in model accuracy compared to FDLA and baseline algorithms. Additionally, we proposed more detailed and extensive experiments than FDLA, including testing the recognition accuracy of both the attackers and honest participants before and after poisoning, as well as testing the attack intensity of PCFDLA."}, {"title": "RELATED WORK", "content": ""}, {"title": "Poisoning Attack in Federated Learning", "content": "In recent years, researchers have extensively focused on poisoning attack strategies that compromise the performance of federated learning models, proposing various countermeasures. These strategies include implicit gradient manipulation to attack sample data , unlabeled attacks , backdoor attacks by replacing components of the model , and the introduction of fake clients to distort the global model's learning process . Additionally, a technique called VagueGAN employs GAN models to generate blurry data for training toxic local models, which are then uploaded to servers to degrade the performance of other local models . These attack methods have been well-studied and have influenced traditional defense mechanisms. While these methods show excellent performance in terms of attack efficiency and covert behavior within classical federated learning frameworks, they are"}, {"title": "Federated Distillation", "content": "As an emerging paradigm in federated learning, Federated Distillation (FD) applies knowledge distillation for model optimization and has garnered increasing attention in recent years . Specifically, FD effectively overcomes system heterogeneity through collaborative optimization and knowledge sharing at the client level . By guiding local models towards different learning objectives, FD meets the personalized needs of clients . It reduces the communication bandwidth required for training by transmitting only lightweight model outputs . Through model-agnostic interaction protocols, it enables the training of larger models on the server . Furthermore, by conducting self-distillation on the client side, FD mitigates client drift caused by data heterogeneity or incremental characteristics . However, existing methods primarily focus on performance improvements of FD algorithms and lack exploration of potential security threats in FD . Although FDLA investigates the impact of logits poisoning attacks in FD, participants with different identities and the effects of malicious modifications at various stages of knowledge transfer remain unexplored."}, {"title": "APPROACH", "content": ""}, {"title": "General Process of Federated Distillation", "content": "In the context of FD, each client k \u2208 {1, 2, ..., K} optimizes its model parameters by combining the cross-entropy loss function LCE() on local data Dk and the distillation loss function LKD on the global knowledge ZS provided by the server. The server coordinates the collaborative learning process by aggregating the logits (softmax outputs) from all clients. This iterative training process involves many communication rounds, each consisting of a local optimization phase and a global aggregation phase, ultimately updating the local model parameters. For more details on the main notations used in this paper, see Table 1.\nRegarding the local optimization phase, each client k optimizes the following objectives:\nmin Jk [JCE + B \u00b7 JKD], Wk (X)~Dk\nwhere \u1e9e is the distillation weighting factor, and (Xk, yk) corresponds to the samples and their associated labels of the local dataset Dk. The optimization components JKD and JCE can be expressed using the following equations:\nJCE = LCE(f(Wk; Xk), yk),\nJKD = LKD(f(Wk; Xk), ZS),\nwhere Wk denotes the model parameters of client k, and f(Wk; \u00b7) is the prediction function on client k. The server performs the aggregation operation over clients' extracted knowledge Zk, and can be typically formulated as knowledge averaging:\nA(Zk) =1K\u03a3k\u2208 {1,2,...,K}Zk\nUpon acquiring global knowledge on the server, it is disseminated to all clients. In response, each client proceeds to update their local models with learning rate lr to align with this shared knowledge, with the process formulated as follows:\nWk = Wk - lr \u00b7 \u25bdWkJk(Wk)."}, {"title": "FDLA", "content": "FDLA is a malicious method specifically designed for Federated Distillation (FD), where malicious clients interfere with the joint learning process of the model by altering the confidence levels of the logits output on local data. \"Confidence\" refers to the model's certainty in its predictions, indicating the reliability of the model's predictions for a given input. When attackers modify their own model's confidence values, it can lead to decision biases, thereby affecting the final results. Since the training process of federated distillation heavily relies on the output knowledge of the models, with confidence being an important component of this knowledge, modifying the distribution of confidence levels can lead to changes in the model's final output. Intentionally manipulating the rearrangement of confidence values can result in performance changes for both the attacker's model and other models.\nSpecifically, the client convinces itself to accept the incorrect answer first and uses this incorrect answer for learning, uploading their erroneous knowledge Zk to the server. This process involves sorting and transforming the logits values C for each sample, where C = [C1, C2, ..., Cn] contains confidence levels corresponding to each label, with ci representing the confidence of the i-th element, assuming n is the total number of corresponding labels.\nThe transformation process of FDLA is illustrated in Figure 1: First, the confidence values c are sorted to obtain the sorted indices I. Then, a transformation mapping t is defined, mapping the element with the highest confidence to the position of the lowest confidence, while the remaining elements are shifted up one position.When the manipulated knowledge Zk from the malicious client is uploaded to the aggregation server, the server performs the aggregation operation A. The aggregated global knowledge is then distributed to each client, which updates their local models based on this information. However, because the attacker first accepts the incorrect answer, their own model will prioritize this incorrect knowledge, leading to a degradation of their own recognition ability. Although the logits stored on the server are polluted after aggregation and ultimately affect the logits sent to each client, resulting in errors throughout the training process, this is not a smart or efficient approach.\nTo be more specific, FDLA execution process is formulated into three steps:\n(1) Sort the confidences c to obtain the sorted indices I.\n(2) Define a transformation mapping t, where t[I[1]] = I[n], and for 2 \u2264 k \u2264 n, t[I[k]] = I[k \u2013 1], where I[k] is the index of the k-th highest confidence.\n(3) Apply the transformation mapping t to the original confidence vector c to obtain the transformed vector c'.\nWe express the operation CL(c) as follows:\nc'i = { Ct[i], if r(ci) > 1 Ct[i], if r(ci) = 1"}, {"title": "PCFDLA", "content": "Despite the success of FDLA, it did not consider the importance of maintaining the attacker's own ability to accurately judge samples. PCFDLA is an enhanced and controllable malicious attack method specifically designed for FD, and it is smarter than FDLA. Although malicious clients also disrupt the joint learning process of the model by transforming the confidence levels of logits on local data, attackers only modify the uploaded data and do not modify the local training results. More importantly, when modifying confidence values, the attackers are aware of which incorrect answers are most misleading and make the most impactful changes to affect the final result. When this modified answer is used as a reference answer by other models, it may cause the decline of their performance. Specifically, clients do not share their correct answers with others but upload their erroneous knowledge Zk to the server. This manipulation involves transforming the logits values C for each sample, where C = [C1, C2, ..., Cn] contains confidence levels corresponding to each label, with c\u2081 representing the confidence of the i-th element, and n representing the total number of corresponding labels.\nThe transformation process of PCFDLA is illustrated in Figure 2: First, the confidence values c are sorted to obtain the sorted indices I. Then, a transformation mapping tis defined, mapping the element with the highest confidence to a set value S, while the remaining elements are mapped to the opposite of the set value. When the manipulated knowledge Zk from the malicious client is uploaded to the aggregation server, the server performs the aggregation operation A. The aggregated global knowledge is then distributed to each client, which updates their local models based on this information. Since the attacker's model does not first accept and learn this incorrect answer but instead tells everyone the incorrect answer first to gain their acceptance, the attacker will know which incorrect answer is the most misleading each time they encounter a new test and upload the answer.\nOnce all the tests are passed, meaning the logits stored on the server after aggregation are already polluted, and the attacker and other models have accepted the incorrect answer, the attacker then convinces themselves to accept the incorrect answer like the other models, completing the brainwashing of all models and ending the attack, ultimately causing erroneous guidance throughout the training process.\nWhen uploading local data Dk at the t-th time, the malicious client k modifies the confidence of the knowledge generated for the i-th sample X in the current communication round, denoted as Z\u0141. We define a confidence ranking function r() and a confidence tampering function x(). The ranking function r(ci) maps ci to its rank in the confidence vector (the highest confidence is ranked 1, the second highest is 2, and so on). Then, the transformation function x(c2) tampers c2 to a new confidence S, where S is a preset hyperparameter value. Besides mapping the second highest confidence to the preset confidence, other confidences ci is a preset hyperparameter value. Besides mapping the second highest confidence to the preset confidence, other confidences -S. This transformation can predict the most similar result as the correct result and completely negate the correctness of other labels to further reduce recognition accuracy.\nOur proposed execution process is formulated into three steps:\n(1) Sort the confidences c to obtain the sorted indices I.\n(2) Define a transformation function x, where x(c[I[2]], S), and for 1 \u2264 k \u2264 n except k = 2, x(c[I[k]], \u2212S), where I[k] is the index of the k-th highest confidence.\n(3) Apply the transformation function x to the original confidence vector c to obtain the transformed vector c'.\nWe express the operation CL(c) as follows:\nc'i ={S, if i = 2\u2212S, if i \u2260 2\nAmong them, c\u00f3 is the i-th element in the transformed confidence vector c', i is the index of the confidence vector, and S is the preset confidence value.\nOriginally, the value c2, which was identified as merely the closest to the correct result, is considered the most correct result, while other possibilities ci, including the correct answer c\u2081, are firmly regarded as incorrect. When the malicious attacker's Zk is uploaded to the aggregation server, the server performs the aggregation operation A as shown in Algorithm 2.\nAfter obtaining the aggregated global knowledge, it is distributed to each client. Subsequently, the clients update their local models based on this newly acquired information, as shown in Algorithm 1. During this process, since the logits stored on the server after aggregation have already been polluted, the logits ultimately sent to each client are affected, leading to a misled training process."}, {"title": "Victims Oriented Metric in FD", "content": "To evaluate the effectiveness of PCFDLA on FD systems, a rigorous assessment of its performance using standard and well-known metrics is required for fair comparison. The evaluation should cover multiple aspects to determine the system's effectiveness and practicality, including the average recognition accuracy of the models. Given the way FDLA attacks FD systems, certain metrics may be more important than others, such as the change in accuracy of the victims as a whole before and after the attack. To ensure scientific rigor, it is crucial to provide detailed metrics for effectively evaluating the proposed method.\nFDLA has proposed several evaluation metrics to measure the accuracy and robustness of various misleading attacks. These metrics typically focused on the average accuracy of the entire model. We report the current metrics used to evaluate the performance of misleading attack methods on the FD system and introduce a new evaluation metric based on this: the change in accuracy of the victim group.\nSpecifically, the previous research work metrics mainly calculate the average model accuracy across various scenarios: different proportions of malicious attackers on various datasets, different data distributions, different numbers of clients, and different model structures."}, {"title": "EXPERIMENTS", "content": ""}, {"title": "Experimental Setup", "content": "We conducted experiments on three common datasets: CINIC-10, CIFAR-10, and SVHN. Each dataset was divided into 50 non-i.i.d. (non-independent and identically distributed) parts, using the hyperparameter a to control the degree of data heterogeneity, set to 1.0. We ensured that all models reached convergence. We evaluated our method under two FD frameworks, namely FedCache and FD. We adopted the Af client model architecture from . Other hyperparameters were consistent with those used in ."}, {"title": "Baselines", "content": "We compared our proposed PCFDLA algorithm with three baseline algorithms from FDLA that modify logits during training: Random poisoning randomly modifies logits, turning their values into random numbers between 0 and 1; Zero poisoning sets all logits to zero before uploading; Misleading attack (FDLA) uses a transformation mapping t that maps the element with the highest confidence to the position of the lowest confidence, while the remaining elements move up one position in the ranking."}, {"title": "Results", "content": "Based on the main experimental setup, the final results are shown in Tables 2, 3, and 4. The data indicate that under both FD and FedCache algorithms, whether on the SVHN, CIFAR-10, or CINIC-10 datasets, the PCFDLA attack has the most significant impact on model accuracy in all scenarios, causing an accuracy loss of up to 5% - 20%. In contrast, the attack strategies proposed in this paper (random poisoning, zero poisoning, and misleading attacks) do not significantly affect the model. In some cases, they can even slightly improve the model's recognition rate. As the proportion of malicious attackers increases, the effectiveness of PCFDLA further strengthens, reducing the model recognition accuracy to around 20%. In contrast, the other three baseline algorithms show no noticeable change in model accuracy interference. In the FD algorithm, with 30% malicious attackers, PCFDLA proves to be the most effective, achieving the lowest recognition accuracy of 31.22%. Similarly, in the FedCache algorithm, with 30% malicious attackers, PCFDLA also proves to be the most effective, achieving the lowest recognition accuracy of 26.57%."}, {"title": "Ablation Study", "content": "The ablation study is divided into three parts, aiming to explore the relationship between hyperparameters and performance: The experimental default configuration is the same as the main experiment, but only the SVHN dataset is used. The basic FD algorithm used is FedCache, and the poisoning ratio is set to 20%.\n\u2022\nThe first part examines the robustness of PCFDLA concerning data distribution:\nThis experiment compares the effects of three different poisoning methods by setting a to three different values: 0.5, 1.0, and 3. This allows us to explore how the effectiveness of PCFDLA is influenced by the distribution of data among clients.\n\u2022\nThe second part investigates the robustness of PCFDLA concerning the number of participating clients:\nThis experiment sets up scenarios with different numbers of participants, specifically 10, 50, and 200, to compare the effects of three different poisoning methods. This investigation aims to understand how the effectiveness of FDLA is influenced by the number of clients in the federated learning network.\n\u2022\nThe third part of the study explores the robustness of PCFDLA concerning model architecture:\nThis experiment evaluates the performance of three different poisoning methods when configured with homogeneous or heterogeneous models to further study the robustness of PCFDLA to model structure. The experiment considers the homogeneity and heterogeneity of client models. Specifically, in the homogeneous model experiment, all clients use the same model architecture, denoted as Af. In the heterogeneous model case, FedCache assigns different model architectures to clients based on the client index modulo 3. When the index modulo 3 equals 0, 1, and 2, the assigned model architectures are AC, A, and A\u00e7, respectively. That is:\nA = Amod 3)+1"}, {"title": "CONCLUSION", "content": "This paper introduces a novel logits poisoning attack method, PCFDLA, for federated distillation. PCFDLA modifies the logits sent from the client to the server, guiding the model to generate high-confidence incorrect predictions. It can control the strength of the attack, thereby interfering with model training at different intensities. During Federated Distillation, PCFDLA subtly influences model training by redesign the knowledge-sharing mechanism. Experimental results demonstrate the strong attack effect of PCFDLA across various settings."}]}