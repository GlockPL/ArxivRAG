{"title": "ROBUSTFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response", "authors": ["Junyu Luo", "Xiao Luo", "Kaize Ding", "Jingyang Yuan", "Zhiping Xiao", "Ming Zhang"], "abstract": "Supervised fine-tuning (SFT) plays a crucial role in adapting large language models (LLMs) to specific domains or tasks. However, as demonstrated by empirical experiments, the collected data inevitably contains noise in practical applications, which poses significant challenges to model performance on downstream tasks. Therefore, there is an urgent need for a noise-robust SFT framework to enhance model capabilities in downstream tasks. To address this challenge, we introduce a robust SFT framework (ROBUSTFT) that performs noise detection and relabeling on downstream task data. For noise identification, our approach employs a multi-expert collaborative system with inference-enhanced models to achieve superior noise detection. In the denoising phase, we utilize a context-enhanced strategy, which incorporates the most relevant and confident knowledge followed by careful assessment to generate reliable annotations. Additionally, we introduce an effective data selection mechanism based on response entropy, ensuring only high-quality samples are retained for fine-tuning. Extensive experiments conducted on multiple LLMs across five datasets demonstrate ROBUSTFT's exceptional performance in noisy scenarios. Our code and data are publicly available.", "sections": [{"title": "1 Introduction", "content": "Supervised fine-tuning (SFT) has emerged as a critical technique for optimizing Large Language Models' (LLMs) capabilities, particularly in domain-specific tasks and adapting them to specific scenarios (Minaee et al., 2024; Raffel et al., 2020). High-quality scenario-specific data plays a vital role in enhancing model performance on downstream tasks (Xia et al., 2024; Jeong et al., 2024). While such data can be acquired through various means including human annotation (Li et al., 2024), scenario-specific collection (Clark et al., 2019), and model-based self-labeling (Wang et al., 2024), these data sources inherently contain noise stemming from both human annotation errors and model hallucinations (Farquhar et al., 2024).\nData noise in scenario can have catastrophic effects on model performance. As shown in Figure 1, the MMLU (Hendrycks et al., 2020) evaluation results clearly demonstrate this degradation: as the proportion of noisy data increases, model accuracy shows a sharp decline. Specifically, with just 30% noise in the training data, the model's performance deteriorates by 8.9% compared to the vanilla LLM baseline. This performance degradation becomes increasingly severe as noise levels rise further. These findings underscore the critical importance and practical value of developing noise-robust fine-tuning frameworks for LLMs to maintain reliable downstream performance. This motivates our central research question:\nCan LLMs detect inevitable noise and enhance data quality, to improve its performance on target tasks?\nThe development of a noise-robust LLM fine-tuning framework encounters two major challenges. First, direct noise detection through LLM predictions proves unreliable due to model hallucinations and overconfidence, as validated by our empirical studies in Section 4. Second, while existing noise-robust methods work well for classification tasks with discrete label spaces (Yuan et al., 2024; Wang et al., 2023a), they are inadequate for LLM fine-tuning scenarios that require contextual and open-ended text generation. Traditional relabeling strategies not only fail to utilize valuable information in noisy generated responses. These challenges highlight the complexity of developing a framework that effectively leverages both model capabilities and data characteristics for robust noise detection and denoising in LLM fine-tuning.\nIn this paper, we propose ROBUSTFT (Noise-robust LLM Supervised Fine-Tuning), a framework for effective adaptation in downstream scenarios with noisy data. At its core, ROBUSTFT introduces multi-view noise detection and denoising strategies. For noise detection, ROBUSTFT employs a collaborative multi-expert system, incorporating reasoning-enhanced models to identify potentially noisy data effectively. For the identified noisy data, ROBUSTFT designs a denoising and data selection process. First, ROBUSTFT utilizes high-confidence data as contextual references for reliable relabeling of noisy samples. Subsequently, for both context-enhanced and reasoning-enhanced inference, ROBUSTFT employs Review Agent to examine and synthesize responses. Finally, by computing confidence scores based on model response entropy and excluding low-confidence samples, we obtain a denoised fine-tuning dataset that facilitates model adaptation to downstream tasks. Overall, by combining noise detection and denoising processes, ROBUSTFT effectively enhances the quality of the fine-tuning dataset while maximizing data utility. We validate ROBUSTFT's effectiveness through extensive experiments across five datasets, spanning both general and domain-specific tasks with varying noise levels. Through comprehensive comparative analyses and ablation studies, we demonstrate the superiority of our approach.\nOur contributions can be summarized as follows:\n\u2022 New Perspective. We investigate the critical yet understudied challenge of noise-robust supervised fine-tuning for LLMs, which aligns more closely with real-world scenarios where noise is inevitable.\n\u2022 Principled Methodology. We design a self-contained framework to leverage the intrinsic interactions between models and data for effective noise detection and denoising, eliminating dependencies on external models or resources.\n\u2022 Superior Performance. ROBUSTFT exhibits robust performance across diverse noise conditions, demonstrating significant improvements on three open-source LLMs across both general and domain-specific tasks, which validates its broad applicability and practical value."}, {"title": "2 Preliminary", "content": ""}, {"title": "2.1 Real-world Challenge", "content": "In practical applications of Large Language Models (LLMs), our objective extends beyond enhancing their general capabilities to improving their performance on downstream tasks. To achieve this, we utilize Supervised Fine-Tuning (SFT) to optimize an LLM $M$ for a target downstream task $D_{task} = \\{q_i, Y_i\\}_{i=1}^{N}$, where $q_i$ denotes the query and $y_i$ is the expected response. The model's performance is enhanced by minimizing the loss between its predictions and the expected outputs.\nHowever, the effectiveness of SFT is heavily dependent on the quality of the downstream task data (Bhatt et al., 2024; Xia et al., 2024). Various factors, including annotation errors, data processing inconsistencies, and model hallucinations, can introduce both random and systematic noise into downstream datasets $D_{task}$. Our empirical studies in Section 4 demonstrate that 30% noise in the training data can lead to an 8.9% degradation on downstream tasks. Therefore, developing robust mechanisms for noise detection and mitigation during the SFT process, particularly ones that can effectively handle open-ended text generation, is crucial and holds significant practical value for optimizing LLM performance."}, {"title": "2.2 Problem Definition", "content": "As discussed above, during the fine-tuning of LLMs on downstream tasks, the training data contains both correctly and incorrectly labeled data pairs. Our primary objective is to develop an effective mechanism for identifying these mislabeled instances. Furthermore, we aim to leverage both the model's capabilities and contextual information within the dataset to denoise incorrectly labeled data pairs where possible. Through this process, we seek to construct a refined dataset with reduced noise levels. Ultimately, this curated dataset enables more effective enhancement of LLM performance on downstream tasks."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Overview", "content": "Adapting and fine-tuning Large Language Models (LLMs) in real-world scenarios presents significant challenges, particularly due to the presence of noise in downstream task datasets that can compromise model performance. Our approach addresses this challenge through a systematic framework comprising noise detection-and-denoising mechanisms to prevent performance degradation.\nFor noise detection, we leverage the consensus among multiple expert LLMs and employ a Checker to identify noisy samples. For denoising, we employ a two-pronged approach: first, we utilize context-enhanced reasoning with clean samples to relabel noisy instances through a Review Agent; second, we implement a perplexity-based data selection mechanism to exclude samples with low confidence scores. As demonstrated in Figure 2, this dual-process framework effectively mitigates noise-induced performance deterioration."}, {"title": "3.2 Noise Detection", "content": "Effective noise identification is crucial for handling noisy data in downstream tasks. In our approach, we leverage collaborative learning among multiple LLMs to uncover potentially noisy samples, enabling a more robust detection mechanism.\nInitially, we utilize the base LLM to generate predictions for all data samples:\n$\\hat{y}_i = M(q_i),$\nwhere $q_i$ represents the query, $M$ denotes the LLM and $\\hat{y}_i$ is the base prediction.\nFor internal noise detection, we introduce a reasoning-enhanced LLM that iteratively combines reasoning and reflection processes. This LLM first performs step-by-step reasoning, followed by self-reflection on its reasoning path, and iterates between these two stages to achieve superior reasoning capabilities. For each data sample, this iterative process can be formalized as:\n$\\hat{y}_{reas} = M_{reas} (q_i, M_{Refl} (M_{Reas} (q_i,\\ldots))),$\nwhere $\\hat{y}_{reas}$ represents the final prediction, $M_{reas}$ and $M_{Reft}$ denote the reasoning and reflection LLMs, respectively, with each reflection stage evaluating and refining the previous reasoning output.\nTo ensure prediction reliability, we implement a consistency-based Checker mechanism that analyzes multiple prediction sources: the original label ($y_i$), the base LLM prediction ($\\hat{y}_i$), and the reasoning-enhanced prediction ($\\hat{y}_{reas}$). This mechanism evaluates the agreement among these predictions through a consistency metric:\n$r_i = Checker(y_i, \\hat{y}_i, \\hat{y}_{reas}) \\in \\{0,1\\},$\nwhere $r_i = 1$ indicates high prediction consistency (reliable sample) and $r_i = 0$ indicates prediction inconsistency (potentially noisy sample). Based on this consistency evaluation, we partition the dataset into clean samples $D_{clean} = \\{(q_i, Y_i)|r_i = 1\\}$ and potentially noisy samples $D_{noise} = \\{(q_i, Y_i)|r_i = 0\\}$ for subsequent denoising treatment."}, {"title": "3.3 Data Denoising", "content": "For the potentially noisy dataset $D_{noise}$, we employ a context learning approach for data relabeling, leveraging external knowledge to reduce noise in the data. Specifically, we project queries from both the reliable dataset $D_{clean}$ and the potentially noisy dataset $D_{noise}$ into a shared latent space:\n$h_i = Encoder(q_i) \\in \\mathbb{R}^d,$\nwhere $h_i$ represents the d-dimensional latent representation of query $q_i$ obtained through the encoder network.\nDuring inference, for each noisy sample, we retrieve the k most similar samples from the reliable dataset as context for reasoning:\n$\\hat{y}_{cont} = M (q_i | \\{(q_j, Y_j)\\}j\\in N_k(q_i, D_{clean})),$\nwhere $N_k (q_i, D_{clean})$ denotes the indices of the k most similar samples to $q_i$ in $D_{clean}$ based on their latent representations.\nBy incorporating external context, we enable the model to generate more reliable responses $\\hat{y}_{cont}$. Combined with the previously obtained reasoning-enhanced predictions $\\hat{y}_{reas}$, we introduce a Review Agent to evaluate and relabel the data:\n$Y_i = Review(q_i, \\hat{y}_{cont}, \\hat{y}_{reas}).$\nThrough the Review Agent's assessment and synthesis, we obtain the relabeled predictions $Y_i$, forming the denoised dataset $D_{denoise}$. However, considering the potential for model errors and uncertainties, we must implement a data selection mechanism for the self-annotated denoised dataset to ensure quality and reliability."}, {"title": "3.4 Data Selection", "content": "While our denoising process generates a refined dataset $D_{denoise}$ through self-annotation, ensuring the quality of these auto-labeled samples remains crucial. To maintain high data quality and prevent error propagation during subsequent training, we introduce a confidence-based filtering mechanism leveraging entropy metrics. This approach enables us to quantitatively assess the uncertainty in context-enhanced predictions and retain only the most confident samples.\nThe entropy score for each context-enhanced response is computed as:\n$H(\\hat{y}_{cont}) = - \\frac{1}{N} \\sum_{j=1}^{N} log p(Y_{ij}|q_i, Y_{i<j}),$\nwhere $p(Y_{ij}|q_i, Y_{i<j})$ represents the model's prediction probability for the j-th token conditioned on the input query and previous tokens, and N denotes the sequence length. Lower entropy scores indicate higher model confidence and more deterministic predictions. Based on these scores, we rank and filter the samples to form our final selected dataset:\n$D_{select} = \\{(q_i, Y_i) | rank (H (\\hat{y}_{cont})) \\leq \\beta |D_{denoise}|\\},$"}, {"title": "3.5 Summary", "content": "Through the integration of the processes described above, we combine the reliable dataset $D_{clean}$ and the selected denoised dataset $D_{select}$ to form our final fine-tuning dataset $D_{ft} = D_{clean} \\cup D_{select}$. Then, we fine-tune the LLM on $D_{ft}$:\n$M' = \\arg \\min_M E_{(q,y)\\sim D_{ft}} [-logp_M(y|q)],$\nwhere $M'$ represents the evolved model trained on the noise-reduced downstream task dataset. The complete algorithm is summarized in Algorithm 1."}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Experiment Setup", "content": ""}, {"title": "4.1.1 Datasets", "content": "We conducted comprehensive evaluations on five diverse benchmark datasets: MMLU (Hendrycks et al., 2020), ARC (Clark et al., 2018), PubMedQA (Jin et al., 2019), Drop, and FPB (Malo et al., 2014). These datasets span multiple domains and task types: MMLU and ARC evaluate general knowledge across various academic disciplines;"}, {"title": "4.1.2 Backbones and Baselines", "content": "Base Models. We employed diverse model architectures, including Gemma2-9B (Team et al., 2024) and Llama3.1-8B (Dubey et al., 2024), along with models of varying parameter sizes such as Llama3.2-3B (Dubey et al., 2024).\nBaselines. To comprehensively validate our method's effectiveness, we implemented several baseline approaches: (1) Vanilla: direct model inference; (2) SFT-enhanced solutions utilizing supplementary data to improve LLM performance, including Hermes-3 (Teknium et al., 2024) and Tulu-3 (Lambert et al., 2024) 2; (3) Standard SFT (Hu et al., 2021) using potentially noisy training data; (4) Denoising approaches, including the state-of-the-art NoiseAL (Yuan et al., 2024) and LLM-based denoising methods such as SelfLabel and SelfSelect; (5) Self-enhancement methods like SelfRAG (Lewis et al., 2020), which augments inference context using training data. Detailed baseline implementations are provided in the Appendix."}, {"title": "4.1.3 Implementation Details", "content": "We partitioned each dataset into training and test sets, introducing varying degrees of noise perturbation in the training data. For model fine-tuning, we employed Low-Rank Adaptation (LoRA) (Hu et al., 2021) implemented through Llama-factory (Zheng et al., 2024) across all open-source models. The fine-tuning process was conducted for 2 epochs.\nWe set the n = 4 and 0 = 50%, with further parameter analysis planned for subsequent experiments. The implementation code is available in our anonymous repository. Comprehensive data and training configurations are detailed in the Appendix."}, {"title": "4.2 Main Result", "content": ""}, {"title": "4.2.1 Comparison with Baselines", "content": "Our comparative experiments with Llama3.1-8B revealed several significant findings. ROBUSTFT consistently demonstrated superior performance across all datasets. The experimental results yielded the following key insights.\nNoise management is critical in LLM fine-tuning. The SFT results clearly demonstrate that direct fine-tuning with noisy data substantially degrades model performance, emphasizing the necessity for robust noise detection and removal.\nLLMs exhibit limited inherent noise detection capabilities. SelfSelect's inferior performance compared to SFT indicates that LLMs cannot effectively identify noise, necessitating specialized noise detection and removal mechanisms.\nEnhanced SFT approaches lack consistent improvement. Methods like Tulu-3 and Hermes-3 failed to show uniform performance improvements across downstream tasks, suggesting the need for task-specific LLM adaptation strategies.\nInference enhancement methods show modest gains. Notably, these approaches achieved some performance improvements despite potential noise in context data, though the improvements were not comparable to our method's results.\nDenoising approaches demonstrate mixed results. While methods such as NoiseAL and SelfLabel show noise resistance and improvements on some datasets, they exhibit degradation on others."}, {"title": "4.2.2 Comparison across Architectures and Parameter Sizes", "content": "We conducted extensive experiments across multiple model architectures (Llama3.2-3B, Llama3.1-8B, and Gemma2-9B), as shown in Table 2. Our investigation revealed several noteworthy insights:\nLarger models are not inherently more robust. Contrary to common intuition, increased parameter count does not correlate with better noise resistance. In fact, general-purpose large models may be more susceptible to noise during domain-specific fine-tuning due to their lack of domain priors.\nTransformation mechanism from general models to domain experts. While Gemma2-9B showed strong general capabilities, it initially performed worse on domain-specific tasks. However, after ROBUSTFT, it effectively adapted to these domains and outperformed Llama3.1-8B, demonstrating the importance of denoising in LLM adaptation.\nCritical importance of denoising for smaller models. Smaller models benefit more significantly from denoising strategies during domain-specific training. Our experiments show that effective denoising mechanisms can substantially mitigate the performance gaps of smaller models in downstream tasks."}, {"title": "4.3 Analysis and Discussions", "content": ""}, {"title": "4.3.1 Ablation Study", "content": "We conducted ablation experiments on RobustFT across different noise levels (30%, 50%, 70%) using MMLU and ARC datasets. The results reveal several key findings: (1) The complete RobustFT framework consistently achieves optimal performance across all settings, validating its effectiveness. (2) The Selection component proves crucial, as its removal leads to substantial performance drops (e.g., accuracy decreases from 68.2 to 65.7 on MMLU with 30% noise). (3) The Checker component significantly contributes to model performance, particularly on the ARC dataset, demonstrating the effectiveness of our multi-model collaborative noise detection. (4) While the Reviewer component shows modest impact, it still contributes to overall data quality. (5) Both Context-Enhanced Relabeing (CER) and Reasoning-Enhanced LLM (REL) components prove essential, with their removal leading to notable performance degradation, highlighting the importance of our multi-experts collaborative mechanisms in handling noisy data."}, {"title": "4.3.2 Sensitivity Analysis", "content": "We conducted sensitivity analysis of ROBUSTFT on MMLU under different noise levels. As shown in Figure 3, we examine the impact of selection ratio $\\beta$ and context length k. The results show that model performance peaking at $\\beta = 40-50\\%$, with performance degrading significantly beyond this range due to the inclusion of excessive noisy samples. For context length, performance improves with increasing k but plateaus, particularly in the range of k = 3-5, suggesting that moderate k provide sufficient reasoning support. These findings validate our default parameter choices ($\\beta = 50\\%$, k = 3) without requiring extensive hyperparameter search, as our primary focus was on demonstrating the framework's overall effectiveness."}, {"title": "4.3.3 Perplexity Analysis", "content": "We conducted perplexity analysis of the models, as shown in Figure 4, revealing several key findings: (1) Noise significantly increases perplexity, as evidenced in both SFT and vanilla models. In contrast, ROBUSTFT maintains relatively low perplexity levels even with increased noise, demonstrating its robustness. (2) The vanilla model exhibits flatter and more dispersed perplexity distributions, indicating frequent uncertainty in predictions. ROBUSTFT effectively concentrates perplexity in lower ranges, suggesting more confident and reliable predictions. (3) The method shows consistency across datasets, with similar perplexity reduction patterns observed on both MMLU and ARC, validating its generalizability across different domains."}, {"title": "4.3.4 Category-wise Performance Analysis", "content": "We analyzed performance across MMLU categories, as shown in Figure 5. (1) The impact of noise varies significantly across different knowledge domains, with knowledge-intensive categories such as History, Healthcare, and Law experiencing more severe performance degradation under noisy conditions. (2) ROBUSTFT demonstrates balanced and expanded performance across all categories, achieving comprehensive noise resistance rather than isolated improvements, as evidenced by its smooth and expanded radar plot."}, {"title": "4.3.5 Stability Analysis", "content": "We evaluated the inference stability of models under different noise conditions, as shown in Figure 6. Specifically, we employed GPT-4O to rephrase the instructions and conducted five independent tests, reporting both mean performance and standard deviation. Results show that ROBUSTFT maintains consistent performance, with only minimal variance increase at higher noise rates."}, {"title": "5 Related Work", "content": ""}, {"title": "5.1 Noisy Label Learning", "content": "Noisy label learning has been a fundamental challenge in NLP (Yuan et al., 2024; Kim et al., 2024; Sun et al., 2023; Qi et al., 2023; Merdjanovska et al., 2024; Xu et al., 2024; Liang et al., 2024), primarily focusing on learning from text classification data containing label noise. Existing approaches can be categorized into three main strategies: (1) Sample selection methods (Qiao et al., 2022) that identify clean samples using fixed thresholds, (2) Label correction techniques (Sohn et al., 2020; Zhang et al., 2021) that rectify original labels based on model predictions, and (3) Consistency regularization approaches (Zhuang et al., 2023; Northcutt et al., 2021) that leverage prediction consistency under different perturbations for label refinement.\nChallenges in LLM Era. These conventional methods are primarily designed for well-defined scenarios, with finite discrete label spaces, making them less effective for open-ended generation problems. Moreover, LLMs' tendency towards hallucination poses significant challenges in noise detection and correction. To address these limitations, ROBUSTFT introduces a novel framework specifically designed for noise-robust downstream fine-tuning of LLMs, moving beyond these constraints."}, {"title": "5.2 Toxicity Attacks and Defense", "content": "The vulnerability of LLMs to adversarial attacks through toxic and harmful data during post-training stages has garnered significant attention (Huang et al., 2024). Current defense mechanisms primarily focus on several key strategies: distance-based regularization (Mukhoti et al., 2023; Wei et al., 2024), alignment data mixing (Bianchi et al., 2023), prompt engineering (Lyu et al., 2024), and data filtering (Choi et al., 2024). Different from these methods, ROBUSTFT takes a different approach by emphasizing detection and relabeling mechanisms to prevent performance degradation caused by noisy data introduction, rather than specifically defending against toxic content."}, {"title": "5.3 Self-Evolution and LLM Data Selection", "content": "Recent advances in Large Language Models (LLMs) (Zhao et al., 2023) have emphasized the critical role of data quality in Supervised Fine-Tuning (SFT) (Taori et al., 2023; Longpre et al., 2023). Current research primarily explores two approaches: downstream data selection (Bhatt et al., 2024; Xia et al., 2024; Bukharin and Zhao, 2023) and data synthesis (Mukherjee et al., 2023; Chung et al., 2024) for improved instruction following. To reduce dependence on annotated data, researchers have developed self-evolution methodsthrough self-instruction (Wang et al., 2023b) and self-play (Tu et al., 2024), enabling models to learn with minimal supervision. Additionally, SemiEvol (Luo et al., 2024) has demonstrated promising progress by combining a small amount of labeled data with large-scale unlabeled data to enhance LLM performance on downstream tasks. While existing work focuses on instruction selection (Parkar et al., 2024) and self-training mechanisms (Wang et al., 2024), ROBUSTFT takes a distinct approach by leveraging noisy real-world data for model self-training to enhance downstream performance."}, {"title": "6 Conclusion", "content": "In this work, we address the practical challenge of handling noisy data in downstream LLM applications, a critical issue that has been unexplored in previous research. We propose a novel noise detection and denoising framework ROBUSTFT, which is specifically designed for LLMs. Our approach leverages a multi-expert collaborative mechanism for noise detection, enhanced by a reasoning-enhanced process. Furthermore, we implement context-enhanced reasoning for data relabeling and utilize response entropy for data selection. The effectiveness of ROBUSTFT is consistently demonstrated across various datasets and noise scenarios."}]}