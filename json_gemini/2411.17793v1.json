{"title": "Engineering AI Judge Systems", "authors": ["Jiahuei (Justina) Lin", "Dayi Lin", "Sky Zhang", "Ahmed E. Hassan"], "abstract": "AI judge systems are designed to automatically evaluate Foundation Model-powered software (i.e., FMware). Due to the intrinsic dynamic and stochastic nature of FMware, the development of AI judge systems requires a unique engineering life cycle and presents new challenges. In this paper, we discuss the challenges based on our industrial experiences in developing AI judge systems for FMware. These challenges lead to substantial time consumption, cost and inaccurate judgments. We propose a framework that tackles the challenges with the goal of improving the productivity of developing high-quality AI judge systems. Finally, we evaluate our framework with a case study on judging a commit message generation FMware. The accuracy of the judgments made by the AI judge system developed with our framework outperforms those made by the AI judge system that is developed without our framework by up to 6.2%, with a significant reduction in development effort.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid rise of Foundation Model (FM) powered software (i.e., FMware) has led to a growing need for robust evaluation mechanisms. However, given the open-ended nature of these responses of FMs that are part of an FMware, it is difficult for developers to either manually evaluate all the responses, or craft a test dataset that includes all the possible responses of an FMware as oracles, and use the test dataset to measure the quality of the FMware. Therefore, practitioners have been developing FM-based evaluators (i.e., AI judges) to reduce the manual evaluation effort for these FMware.\nDeveloping Al judge systems for the automatic evaluation of FMware offers several advantages. First, AI judges have shown to be capable of evaluating open-ended responses and can generate detailed explanations for their judgments [26, 28, 69]. As the FMs in Al judges were trained on vast corpora, these models possess expertise in natural language understanding and instruction following, allowing them to measure various aspects of text quality effectively. Second, compared to human evaluators, AI judges require significantly less time for evaluations, enabling the evaluation of a large amount of data quickly [16, 69]. Additionally, AI judges are not affected by factors that are associated with human annotators (e.g., fatigue, emotions, and distractions).\nHowever, translating high-level requirements to measurable units is inherently challenging. Unlike conventional software applications, where judging requirements are often directly tied to specific functionalities or outputs, Al judge systems for FMware must grapple with abstract concepts such as fairness, accuracy, and contextual understanding. These high-level requirements can be difficult to operationalize, leading to ambiguity in how an FMware should be evaluated. As a result, developers must engage in a complex iterative process to refine these requirements into quantifiable metrics [51]. This involves not only extensive testing and calibration of the judging systems [47, 69] but also a deep understanding of the underlying FMware that are intended to be assessed. The labor-intensive nature of this task can slow down the development process and impact the overall effectiveness of the AI judge systems.\nIn addition, developers must navigate the intricacies of de-veloping AI judge systems through a combination of cognitive architectures for evaluation and various jury FMs. We refer to judging cognitive architecture that includes components such as jury FMs, judging heuristics, prompts and their relations, as well as metrics and their interactions, to make accurate judgments. There is no universally applicable judging architecture for all FMware. Each target FMware may require unique judging cognitive architectures tailored to its specific characteristics and objectives. This requires a flexible design that can incorporate diverse cognitive architectures, which may vary significantly in their methodologies and underlying assumptions. Furthermore, as the FMware field continues to evolve rapidly, AI judge systems must be adaptive, continu-ously integrating new judging techniques into their judging architectures to keep pace with these changes. Such a dy-namic environment often results in inconsistent judgments, particularly when a target FMware undergoes updates. As such, developers must maintain a balance between establishing robust judging architectures and ensuring that their AI judge systems remain accurate to ongoing developments in the FMware landscape.\nIn this paper, we discuss the challenges that developers face when defining judging requirements, developing an AI judge systems, and evolving it. We highlight the difficulty of applying mitigations for these challenges based on insights gained from the in-depth discussions with academic and industrial leaders. These insights were gathered from events including (1) SEMLA 2023 and 2024 [41], (2) FM+SE Vision 2030 [32], (3) SE 2030 workshop-FSE 2024 [6, 7], (4) FM+SE Summit 2024 [8] and (5) through our active contributions to the Open Platform for Enterprise AI (OPEA) initiative [9]. OPEA is a multi-organization collaborative effort, including"}, {"title": "II. CHALLENGES OF DEVELOPING AI JUDGE SYSTEMS USING A MOTIVATIONAL EXAMPLE", "content": "Although existing related work touches on the issues of developing AI judge systems and suggests ways to improve them, there is a lack of systematic thinking about the engineer-ing of AI judges throughout their life cycle. Since AI judge systems are a special type of FMware and face the typical challenges in the life cycle of FMware [33], we highlight the challenges that hinder the development of Al judge systems and affect the quality of AI judge systems significantly in this section. We introduce a motivational example in Section II-A and describe the challenges along with the example. The challenges are categorized into three groups: 1) Defining judging requirements in Section II-B, 2) Developing judges in Section II-C, and 3) Evolving judges in Section II-D.\nWe select the commit message generation (CMG) task [27, 58, 59, 67] as a motivational case to elaborate the challenges around the development of AI judge systems. The CMG task involves writing concise and descriptive commit messages for a given code diff in natural language. The commit messages are important for program comprehension and maintenance and collaboration among developers.\nBelow we elaborate on the challenges of developing a judge system, in the context of judging a CMG system."}, {"title": "B. Challenges associated with defining judging requirements", "content": ""}, {"title": "B.1 Complexity of articulating requirements in a concise and clear manner.", "content": "A primary complexity arises from the probabilistic nature of the outputs that are generated by FMware and its embedded FMs, rather than the deterministic outputs in conventional software applications. This difference complicates the task of defining precise requirements. De-velopers usually define ambiguous requirements specific to their use cases, such as faithfulness to contextual information, or the naturalness of conversation. To address the ambiguity, developers then refine these requirements to various detailed dimensions and criteria. This iterative process allows develop-ers to better align their goals with evaluating the capabilities of the FMware and typically involves testing the performance of the AI judge system to ensure that the outputs meet their expectations.\nIn the context of CMG, developers will need to iteratively modify requirements until they arrive at a clear and concise version. Developers may initially come up with an ambiguous requirement such as \"the commit message should be clear and concise to the changes made in a commit\", and expand it with more details such as \"Use clear and descriptive language to convey the purpose of the commit. Avoid jargon and am-biguous terms to ensure that anyone reading the message can understand the changes made.\", before arriving at a concise version below:\n..."}, {"title": "B.2 Low efficiency in the requirements process of an AI judge as there is limited reuse across AI judge systems and using general benchmarks does not work for specific contexts.", "content": "Developers need to align the judging requirements with diverse business use cases and intended goals, as this requires a profound understanding of both the technical details of the FMware and the specific needs of the specific context at hand. Hence, developers have to extensively collaborate with the stakeholders to capture both technical and business aspects. Developers face many challenges in the communica-tion to specify the requirements, such as various standardized practices and inadequate artifacts or information [50].\nIn the context of CMG, developers need to specify re-quirements for different programming languages since they have different standards for commit messages. For example, the C++ standard changes should be recorded in the commit messages. The use or modification of streams and lambda expressions should be highlighted as these features are pivotal in Java 8+. This is a time and labor consuming process for developers and worse the ability to reuse parts of requirements across different FMware contexts is very low."}, {"title": "C. Challenges associated with developing Al judge systems", "content": ""}, {"title": "C.1 The difficulty of judging fosters continuous and rapid innovation with many new ideas coming up for architec-tures, metrics, and judge-optimized FMs.", "content": "Developers need an approach to select appropriate judging architectures for the predefined criteria and methods before the implementation, since judging involves complex steps, including selecting jury FMs, metrics and architectures, deriving judging cognitive architectures, and integrating them seamlessly. Deciding the most appropriate jury FMs in AI judge systems poses chal-lenges due to the high cost and diverse judging requirements. Intuitively, developers usually select a strong FM (e.g., GPT-4) as the jury FM, since a strong jury FM obtains more accurate and reliable judgments. However, using strong mod-els is usually expensive [2, 3] and they are biased to the outputs generated by their model families [29]. Alternatively, researchers and practitioners obtain open LLM leaderboards (e.g., HuggingFace open LLM leaderboard [4]) and arenas (e.g., Chatbot arena [23], Alpacafarm [28]) where developers could see the performance and rankings of jury FMs and use it for the selection. However, the rankings of models obtained on the open leaderboards are derived from common benchmarks that are less likely to meet the specific business requirements of target FMware for a particular context. Developers usually realize such limitations after at least a round of evaluation.\nDevelopers either leverage common judging cognitive ar-chitectures or derive their own evaluation methods in their Al judge systems. Common cognitive architectures for judging [5, 36, 38, 39, 61] include reference-based judging, reference-free judging, pairwise judging, ensemble judging, and deliberation judging. Reference-based judging requires the ground truth that is derived by deterministic functions or real-world data. This judging architecture assesses how well the generated output aligns with the ideal response. By contrast, reference-free judging evaluates the responses of an FMware without any ground truth, and relies on the internal knowledge of the jury FM and learned patterns within the prompt to determine the quality. Pairwise judging allows a jury FM a more granular and direct evaluation of two instances against each other, leading to more reliable and consistent judgments, compared to single instance evaluation [21, 61]. This judging architecture is widely used in scenarios where subjective judgment is required, such as the relevance of retrieved documents.\nThe ensemble judging involves several AI judges that issue their own decision and reach a conclusion based on certain rules (e.g., hierarchical structures, majority votes [63]). Such a judging architecture can decompose evaluation criteria into binary or categorical decisions, which is useful for NLP tasks like content moderation. Deliberation judging involves multiple jury FMs engaging in structured debates to evaluate complex outputs, such as text quality [17] or translation [44]. This architecture captures diverse perspectives and mitigates biases, leading to more balanced and thorough evaluations. Kenton et al. [38] reported that debate generally outperforms consultancy and direct question-answering, especially in tasks with information asymmetry, and ensures weaker judge ac-curacy. While the ensemble judging architecture is efficient and transparent, the deliberation architecture offers depth and bias reduction, making both architectures complementary in ensuring the robustness and fairness of AI judge systems.\nDevelopers need to implement various mitigation strategies to improve the fairness and accuracy of Al judge systems, since jury FMs are known to be biased and unfair due to the training data [15, 29], even for the strong models. Prior studies have reported many biases [54, 69], including verbosity bias, positional bias, and familiarity bias, that lead to invalid judgments. For example, the verbosity bias refers to that AI judges preferring longer and verbose text than short ones, even if the longer ones are not as clear, high-quality and accurate as the alternatives. To mitigate the effect of the bias, developers need to implement various techniques (e.g., Chain-of-Thought (CoT), in-context learning (ICL)) [54] or judging architectures. Another critical bias is positional bias, which refers to that Al judges prefer a certain position over the others in pairwise judging. Similar to the verbosity bias, developers need to employ mitigation (e.g., swap the comparisons) for such a bias. In other words, developers require considerable effort to aggregate a wide range of mitigations into their judging architectures to mitigate all the biases. Additionally, there might exist new biases that have not yet been discovered, so Al judges exhibit unreliable and inaccurate results and developers need to fix them on a case-by-case basis."}, {"title": "C.2 No silver bullet method instead mostly case-by-case optimization.", "content": "There is no universal solution or best practices that fit all judging tasks. Optimization strategies need to be tailored to the specific requirements of each use case. This means developers must carefully analyze the unique aspects of each judging task and apply appropriate components in their judging architecture. Given the diverse judging architectures with each of them having limitations, the complexity of setting up and maintaining the judging architectures can be high, and there might exist nuanced or ambiguous cases that do not fit neatly into predefined categories. For example, the pairwise judging might be less feasible for online evaluation since it is less likely to find a proper comparison instance for every user query. Developers might struggle with balancing the trade-offs between accuracy and resource limitations [65]. Designing judging architectures typically leads to an ad-hoc version of the Al judge system that is designed for one particular FMware, i.e., labor burden.\nIn the context of CMG, developers do not know how to optimize the AI judge system since FMs have their weak-nesses and different judging architectures fulfill their specific contexts. For example, using the reference-free judging archi-tecture vs. the decision-tree judging architecture. While the reference-free architecture is simple to develop, the decision-tree judging architecture is more complex, but a more nuanced and robust decision-making process. Developers need to de-velop both architectures and verify which one is better."}, {"title": "C.3 Creation of judging data is labor intensive.", "content": "Developers typically select datasets by three common strategies: (1) leveraging common benchmarks (e.g., MMLU [34] and GSM8K [24]), (2) manually curating a dataset, and (3) synthe-sizing a particular dataset. The first strategy is often employed when evaluating FMs, while the others could be used in the evaluation of both FMs and FMware. However, none of the aforementioned strategies is perfect. Common benchmarks are designed to evaluate the general capabilities (e.g., math) of FMs and may not be specific to a given set of requirements. In addition, the benchmarks may have leaked to FMs during pretraining [12], ending up with invalid evaluation results. Manually curating datasets is both time-consuming and labor-intensive [46]. Despite the availability of various synthetic data generation techniques (e.g., ZeroGen [30], SuperGen [48], Alpaca [60]), synthetic datasets present challenges in areas such as factuality and fidelity of synthetic data [66] and often fail to generalize to real-world scenarios [62]. Additionally, after a dataset has been curated, developers need to maintain the dataset to be applicable when FMs and the FMware co-evolve.\nIn the context of CMG, developers need to specify test datasets for different levels and types of changes across different programming languages to ensure the test coverage of the AI judge system. Developers could make a code diff at the line, class, method, package, or system level and the diff could be associated with any of the following types: bug fixes, refactoring, feature additions, performance optimization, and security updates. The creation of test data for all the aforementioned changes for all programming languages is labor-intensive."}, {"title": "C.4 Balancing between cost, accuracy, and latency is crucial.", "content": "Developers often prioritize the accuracy of AI judge systems over the other factors due to its critical importance. However, to achieve high accuracy, developers often leverage techniques such as calibrating prompts with examples to a jury FM to reduce its weakness [69], i.e., increase costs due to the increased number of input tokens, or increasing the number of requests to jury FMs [63], i.e., increase latency. Balancing these factors requires a nuanced understanding of judging cognitive architecture and a flexible approach to adapting new techniques.\nIn the context of CMG, developers may aim to improve the accuracy of the AI judge system using few-shot learning. To achieve this, developers incorporate examples of code diffs and their corresponding commit messages into the prompt, providing substantial context in the prompt to guide the jury FM. While this approach improves the accuracy of judgments, it significantly increases the cost due to the high token count."}, {"title": "D. Challenges associated with evolving Al judge systems", "content": ""}, {"title": "D.1 In the rapidly active field of FMware, developers always have to chase new techniques and update them to the evolving AI judge systems.", "content": "The relentless pace of innovation in the field means developers must constantly adapt to incorporate the latest advancements. The continuous need for integration can be overwhelming, requiring frequent updates and adjustments to ensure the AI judge system re-mains at the cutting edge. In addition, novel advancements could be raised from any unit inside an AI judge system, including code, sophisticated judging architectures, FMs, and mitigation, since the change of one unit affects the overall reliability and robustness of the AI judge system. For example, when a new mitigation for the positional bias is introduced, developers observe that the new mitigation outperforms the prior mitigation when testing the AI judge system. However, after deploying it online, developers may discover that the new mitigation performs worse than the old one on certain real-world data subsets. Consequently, developers maintain both mitigations for different data groups. Tracking the versions of each mitigation and their usefulness against a certain bias is critical in maintenance and evolution. In addition, mitigation might not be applicable for all FMs, indicating the complexity and difficulty in the evolution of AI judge systems.\nIn the context of CMG, imagine a new judging cognitive architecture is released that significantly improves the accuracy of weaker jury FMs. Developers must rapidly understand the new architecture, implement, and test the new architecture within the existing Al judge system. Developers need to carefully assess the performance of the new architecture by applying all historical online data before deploying a new version to the production environment."}, {"title": "D.2 Field problems.", "content": "Deploying the AI judge systems in pro-duction environments presents numerous practical challenges. Developers must tackle a range of issues, from handling diverse and unpredictable data inputs to ensuring the robust-ness and reliability of the Al judge system, since common pitfalls (e.g., biases [29], hallucinations [35], human prefer-ence alignment [64]) are amplified when using the real-world user data and potentially end up with negative consequences (e.g., reputational damage or incidents). One of the important challenges is data drift due to real-world data and jury FMs themselves [22, 42]. Real-world data often exhibits out-of-distribution (OOD) instances that were not apparent during testing. Additionally, jury FMs may drift significantly over a short period, especially when using proprietary models (e.g., OpenAI). Chen et al. [22] reported significant behavior changes between the March 2023 and June 2023 versions of GPT-4, including a decreased ability to follow user in-structions. Similarly, developers experienced significant per-formance degradation while using Claude 3 Sonnet in March, 2024, a few weeks after it was released [1]. These subtle changes compromise the reliability and accuracy of AI judge systems significantly.\nIn the context of CMG, the deployed Al judge systems may require additional handling for certain real-world data. For example, a commit message includes sensitive information (e.g., API token) that the Al judge system is less likely to identify such information and determines the low quality of the commit message."}, {"title": "D.3 New/better understanding of requirements.", "content": "As the AI judge system is deployed in production, developers gain new insights into its requirements. This evolving understanding can lead to significant changes in tuning jury FMs, judging architectures, feature development, and data collection strate-gies. While this deepening knowledge improves the system's effectiveness, it also requires developers to constantly reassess and update their judge systems [51].\nIn the context of CMG, developers may get new require-ments that the deployed AI judge system needs to support Kotlin, a preferred programming language on Andriod devel-opment. Developers need to explore issues such as how to emulate an Android environment within the AI judge system and integrate the environment seamlessly."}, {"title": "III. SEARCH-DRIVEN CONSTITUTION-BASED FRAMEWORK FOR AI JUDGE SYSTEMS", "content": "In light of the discussed challenges in the prior section, we aim to increase the productivity of developing AI judge system while ensuring the quality of judgments through a search-driven constitution-based framework . In an AI judge system, the principles outlined in a constitution are designed to specify and quantify judging requirements, along with their associated criteria and methods. The concept of constitutions was introduced in a prior work [11] where the authors designed a set of guiding principles in a constitution that models must adhere to ensure they are harmless, helpful, and honest. The framework transforms requirements into principles as knowl-edge in Al judge systems, i.e., from a data-driven approach to a knowledge-driven approach. Since the data-driven ap-proach often exhibits inefficiencies in data utilization and lacks precise control mechanisms [10], while the knowledge-driven"}, {"title": "Stage I. Creation of general constitution", "content": "The primary goal in this stage is to transform the re-quirements into general and reusable guidelines that remain applicable, even when the FMware evolves and when data and models drift over time, aiming to address challenges B.1, and B.2. The framework transforms judging requirements into criteria in a certain context. Such transformation in the framework provides a consistent and scalable method for the ongoing evaluation and potential changes in the FMware that developers of the AI judge system could leverage when maintaining and updating the AI judge system.\nFirst, each predefined judging requirement is transformed into a set of principles (i.e., requirement-principle) through prompts given to an FM. These principles are refined through at least four rounds of critiques and revisions, as suggested by the prior work [11], to ensure quality and applicability. Further optimization of these principles can be achieved using tech-niques such as CoT, few-shot learning, or fusion techniques.\nNext, for each requirement-principle, a corresponding set of principles is generated and these principles are served as evaluation criteria (criteria-principles). This process also involves multiple rounds of critiques and revisions to refine the criteria-principles, resulting in a final set of criteria-principles.\nThese sets of principles collectively form a general con-stitution, that is embedded into an agent's knowledge. Such an agent could serve as two roles: validator and advisor. The validator role monitors and assesses changes in the evolution of an FMware and its FMs, ensuring that updates adhere to the established principles. The advisor role provides guidance to developers of AI judge systems, helping them align their evaluations with the core principles and maintain consistency across different versions of the FMware.\nThe general constitution is designed to be reused across various developer teams in the next stage. These teams work on building an Al judge system for similar types of FMware, such as text summarization, but across different contexts or dimensions, such as technical documents vs. news. This adapt-ability allows the general constitution to serve as a flexible and reusable resource, supporting diverse evaluation needs in the same foundation and fostering consistency across different FMware."}, {"title": "Stage II. Specialization from general to the contextualized constitution", "content": "The primary objective of this stage is to incorporate context-specific knowledge into the constitution, develop a new set of specific principles with assistance from an FM that em-beds the general constitution as knowledge, and form a new constitution. This stage is designed to address the challenges B.1, B.2, and C.3. This process involves collaboration between developers and the constitution to establish context-specific sets of principles.\nIn this stage, developers prompt the FM using any prompt engineering techniques (i.e., RAG, ICL) to get a new set of context-specific principles tailored to their subjective re-quirements. The developers then review and critique these new principles (either manually or by an FM), and request refinements from the FM that embeds the general constitution as knowledge. Similar to Stage I, this interactive process involves several rounds of feedback and adjustment until a final and well-satisfied set of principles is obtained. The final set of context-specific principles forms a contextualized constitution that incorporates the context-specific knowledge and expertise relevant to the target FMware."}, {"title": "Stage III. Searching for cognitive architectures using the contextualized constitution", "content": "The primary objective of this stage is to facilitate the development of AI judge systems while ensuring the delivery of high-quality judgments through a search-based exploration. This stage aims to address the challenges C.1-2, C.4, and D.1-2. Given a context-specific principle, Fig 2 depicts that the agent searches for the optimal combination of the components (e.g., jury FMs) to construct a judging architecture. The con-textualized constitution generates judging data points, which are then injected into the architecture to verify the accuracy of the judgments. This methodology draws inspiration from Test-Driven Development (TDD) in conventional software appli-cations, which ensures that developers thoroughly understand the requirements before they start to write code [14]. The test-driven approach has been used in certain techniques, such as DSPy [40]. When the agent encounters inaccurate judgments, the agent adapts by re-constructing a new architecture. This iterative feedback loop ensures that the agent continuously learns from weaker areas and refines the constructed architec-ture. By consistently refining the architecture, the framework enhances the overall effectiveness and precision of the AI judge system. This dynamic and adaptive approach not only ensures that the AI judge system remains responsive to evolv-ing requirements but also improves the reliability and accuracy of judgments over time. Furthermore, the approach mitigates the challenge of the ever-changing field, as one can simply add new techniques to the search space of the framework, which can then automatically reconstruct a judging architecture with the updated best combination."}, {"title": "Stage IV. Evolving the judge", "content": "The purpose of this stage is to address flaws in the principles outlined in the general constitution (Stage I). This stage is designed to address the challenge B.2 and D.3. Since these general principles were derived from the judging requirements, any potential flaw should be traced back through the trans-formation obtained in Stage I to ultimately reveal whether there is a bug in the original judging requirement, due to the knowledge-driven approach. This is important for debugging and bug fixing in the ongoing evolution of FMs, FMware, and AI judge systems. Our framework seeks to identify potential requirement bugs in the general constitution through a semi-auto process among the general and contextualized constitutions and the developers. Since the context-specific principles that are outlined in the contextualized constitutions have been manually reviewed and verified, the contextualized constitutions identify potential bugs or discrepancies in the general principles automatically, instead of by the developers themselves manually. The process involves four key steps:\n\u2022 Identification of potential flaws: The general and contex-tualized constitutions perform a comparative analysis of their principles and detect potential requirement bugs by the inaccurate judgments obtained in the prior stage. This step leverages the manually verified principles from contex-tualized constitutions to assess and validate the principles of the general constitution.\n\u2022 Review and consensus: Any potential requirement bugs must be reviewed, discussed and agreed by a majority of the constitutions. This consensus-driven approach ensures that the identified requirement bugs are valid and relevant across different contexts within the FMware.\n\u2022 Private knowledge review: Since the contextualized con-stitutions often contain private knowledge or context that should remain confidential, the developers of these con-textualized constitutions must review the identified poten-tial requirement bugs. This step ensures that sensitive or proprietary information is protected while the identified requirement bugs are validated.\n\u2022 Incorporation of fixes: After the requirement bugs have been validated and reviewed, the corrections are integrated into the general constitution. This step ensures that the general constitution evolves to address identified requirement bugs, maintaining its relevance and effectiveness.\nOur framework ensures that the general constitution remains adaptive and robust when facing the dynamic changes in FMware and evolving requirements."}, {"title": "IV. CASE STUDY AND RESULTS", "content": "As discussed in Section II, we select the commit message generation (CMG) task [27, 58, 59, 67] as a case study to evaluate the productivity and quality of judgments made by our proposed framework. We focus on the quality of the AI judge system when using reusable constitutions and their principles (Stages I and II). The aspects of searching for the best cognitive architectures and improving the general principles (Stages III and IV) will be addressed in future work."}, {"title": "A. Dataset", "content": "We employ the multi-language commit message dataset (MCMD) [59] with 5 popular programming languages, i.e., C++, C#, Java, Javascript, and Python, which has been widely used in the CMG-related studies [27, 57, 58, 67]. The MCMD dataset was collected from the top 100 most-starred reposi-tories in GitHub and each data point in the MCMD dataset included a code diff and the corresponding commit messages. We leverage the MCMD dataset obtained by Shi et al. [58] that the authors added more fine-grained code change actions (e.g., <REPLACE_OLD>) in a given code diff. Since the MCMD dataset contains a total of approximately 100K data points and making judgments for such a large number of data is resource consuming, we select a representative random sample set (confidence level = 95%, confidence interval = 5%) of data points in each of the 5 programming languages and report the corresponding sample sizes in Table I.\nTo emulate the outputs of a target FMware, we replicate a prior work [67] using an FMware for the CMG task. To the best of our knowledge, this work was the latest work and had been published in one of the top SE venues. Wu et al. [67] used the few-shot learning technique to generate commit messages and reported the performance of their approach increased when the number of shots increased, then dropped, due to the limited length of the context window of the used model (i.e., GPT-3.5-turbo-1106). Therefore, to avoid the limitation issue, we replicate the 16-shot result for making judgments and evaluating the performance of our framework."}, {"title": "B. Approach", "content": "We evaluate our search-driven constitution-based framework for AI judge systems from two aspects: 1) the effectiveness of principles obtained in the constitutions, and 2) the quality of judgments. As discussed in Section III, we prompt FMs to generate a set of general principles for writing commit mes-sages in software development, before performing four rounds of critiques and revisions. We obtain 17 general principles in the constitution.\nIn stage 2, we tailor the general principles to each of the 5 programming languages and generate context-specific prin-ciples. For each programming language, we prompt context-specific requirements (e.g., the name of the programming language) to the FM while iterating critiques and revisions. We review the revised principles and remove the inadequate ones to ensure the quality of the context-specific principles. We perform this process iteratively until the final set of context-specific principles is satisfied and verified. As a result, we obtain 5 high-quality contextualized constitutions (see Table I). These constitutions guide the judges during the quality assess-ment of the FM-generated commit messages. Specifically, each context-specific principle represents a metric with detailed criteria.\nTo evaluate the quality of the judgments, we compare the judgments made by two Al judge systems: with (w/) vs. without (w/o) our proposed framework. To simplify the evaluation process, the search agent sets the score range between 0 and 1 for every metric and selects a specific FM (i.e., GPT-40-2024-05-13) as our jury FM for constructing the judging architecture. The search agent computes the final score by summing the scores of all principles in the AI judge system with our proposed framework. By contrast, the AI judge system without our proposed framework is instructed with assigning a score on a scale from 0 to n, where n is the number of context-specific principles used by the corresponding AI judge system developed with our framework. This approach ensures both AI judge systems operate on a comparable scale and indicates the normalization of the scores. For a given dataset of a certain programming language, we obtain two scores (w/ and w/o our framework) for every data point.\nWe derive heuristics to determine the ground truth of judgments for several reasons. First, there is not any ground truth in the MCMD dataset for how many scores of a commit message should be assigned. Second, manually reviewing the score of every diff requires expertise in the 5 programming languages. Lastly, the manual review is time consuming and not scalable. We leverage relative comparisons of two scores for a given pair of data points (a, b) as a proxy to evaluate the accuracy of judgments. When the score of a is larger than that of b, we determine a is better than b. The ground-truth heuristic of relative comparisons is derived from automatic metrics and the majority-voting approach. Since the prior work [67] utilized four metrics (i.e., BLEU [52], ROUGE-L [45], CiDEr [49], METEOR [13]), we add one additional metric, i.e., BLEURT [55], to reduce the probability of ties in the majority-voting approach. BLEURT is a parametric reference-based metric and captures deeper semantic relation-ships between the candidate and the ground truth, assessing both the fluency of the candidate and its effectiveness in conveying the meaning of the reference.\n$P$ denotes the obtained set of paired data points and is composed of a total of $C(n,r) = \\frac{n!}{(n-r)!r!}$ pairs, where n indicates the number of data points and r is 2. A pair of data points is denoted as $p_i = (a,b)$ at index i of P. Based on the majority-voting approach, for a given pair $p_i$, a is better than b when at least the values of three used metrics of a are larger than those of b. In the case that the values of certain metrics of a are equal to those of b (i.e., ties), a is better than b if the number of non-tied metrics where a has larger values is greater than that of b. $LarCnt(p_i, a)$ denotes the number of non-equal metrics of a with larger values than that of the other point b in the $p_i$. The ground-truth heuristic, denoted as"}, {"title": "C. Results", "content": ""}, {"title": "Productivity of developing AI judge systems", "content": "Our framework leads to an increased productivity by facilitating the reuse of principles and reducing the development effort.\nIn the AI judge system with our proposed framework, the majority (i.e., an average of 58%) of the general principles that are generated in Stage I are reused in the contextualized constitutions (Stage II) across the 5 programming languages.  Given that Stage I operates automatically, the large numbers of percentages of the reusable principles suggest that the framework can effectively address the challenges B.1 and B.2 (e.g., need reusable requirements and generic criteria) mentioned in Section II and reduce manual effort. For example, \"Explain the Why, Not Just the What\" is one of the reused general principles for writing commit messages across the 5 programming languages. The details of the why and what principle indicate detailed criteria, such as \u201cIn the body of the commit message, provide 'context' by explaining 'why' a change was made, not just 'what' was done.\u201d This reused general principle for CMG is aligned with the finding in prior work [56, 68"}]}