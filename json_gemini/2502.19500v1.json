{"title": "Conversational Planning for Personal Plans", "authors": ["Konstantina Christakopoulou", "Iris Qu", "John Canny", "Andrew Goodridge", "Cj Adams", "Minmin Chen", "Maja Matari\u0107"], "abstract": "The language generation and reasoning capabilities of large language models (LLMs) have enabled conversational systems with impressive performance in a variety of tasks, from code generation, to composing essays, to passing STEM and legal exams, to a new paradigm for knowledge search. Besides those short-term use applications, LLMs are increasingly used to help with real-life goals or tasks that take a long time to complete, involving multiple sessions across days, weeks, months, or even years. Thus to enable conversational systems for long term interactions and tasks, we need language-based agents that can plan for long horizons. Traditionally, such capabilities were addressed by reinforcement learning agents with hierarchical planning capabilities. In this work, we explore a novel architecture where the LLM acts as the meta-controller deciding the agent's next macro-action, and tool use augmented LLM-based option policies execute the selected macro-action. We instantiate this framework for a specific set of macro-actions enabling adaptive planning for users' personal plans through conversation and follow-up questions collecting user feedback. We show how this paradigm can be applicable in scenarios ranging from tutoring for academic and non-academic tasks to conversational coaching for personal health plans.", "sections": [{"title": "1 Introduction", "content": "Consider would happens when we ask another person about a complex real-life task, such as planning a vacation, learning how to draw, or improve one's fitness level. If the person does not know you, they would first ask you a couple of questions to clarify your preferences, your skill level, and your goals. Having gained a better understanding about you and the task, they might provide some suggestions for you to consider, and then keep adapting those as you provide more feedback through conversation. For example, for the vacation planning task, they might start with steps such as \"determine the budget\" and \"figure out the preferred kind of vacation\", and \"narrow down potential destinations\". For each step, they might ask clarifying questions, or answer your questions and provide some relevant resources like a web site for a relevant city or airline. As you start getting more concrete about the specifics of your goal, and take actions such as choosing the destination, you need help with the next step, such as booking accommodations and planning specific activities. Therefore, the plan evolves\u2014either the person recommends new steps, revises some existing steps to account for the updated information, or asks further questions. A similar conversational planning process is followed whenever a fitness coach is asked to provide a plan adapted for their client, and whenever a tutor needs to provide a personalized learning plan. All of these plans need to evolve over time based on the updated needs of the user.\nHowever, when we ask the popular conversational interfaces of today to help with long term real-life tasks or goals, they behave very differently than people. They tend to generate long responses with bulleted lists of steps to follow. They do not ask questions proactively, but only respond in a reactive"}, {"title": "2 Related Work", "content": "Inspired by the strong in-context learning [4, 14] and complex reasoning [28, 31] capabilities of LLMs [5], there has been a lot of recent interest into LLM-driven agents [27, 30, 1, 19, 16, 25, 26, 18, 3]. This body of work can be largely divided into (i) text-based agents [19, 29, 17], and (ii) open-world embodied agents [27, 1, 25, 15, 21, 24, 22]. Our work, anchored on the realization that real-life user goals/tasks take a long time to be achieved, brings language-based hierarchical long-term planning techniques typically considered only in open-world environments (e.g., Minecraft) to text-based agents. Traditionally, in non-language based reinforcement learning, whenever tasks of long term nature need to be solved, a hierarchical goal execution architecture is considered [2], where a planner generates action plans that would then be executed by low-level goal-conditioned controllers. Recently, thanks to the LLM advances, interactive LLM-based hierarchical planning approaches have been proposed and proven successful for open-world environments [22, 21, 25, 11, 30, 27]. However, little to no attention has been given to language-based hierarchical planning for text-based agents [19]. This is where our work comes in and aims to advance the frontier of the traditionally short-term focused text-based agents. We offer a novel framework allowing for conversational planning for users' personal plans, that can be adapted over time based on human feedback. This also advances the state of the art for conversational recommendation [8, 12]."}, {"title": "3 Proposed Approach", "content": "As described in Section 1, the goals and tasks users pursue can last for multiple days, months, or even years [7]. Developing AI agents assisting towards these can be framed as a long-horizon planning problem [23]. Traditionally, this would be framed in a hierarchical reinforcement learning (RL) framework, where the underlying hierarchy of options/ skills would need to be given or learned [2]."}, {"title": "3.1 Overview: Language-based Interactive Hierarchical Planning", "content": "RL Framing. We frame the conversational AI system as a language-based agent that can interact with the user u who is part of the world/environment E. The user starts by using language to specify the goal they want to pursue g \u2208 L \u2014 this can be viewed as the initial observation o1. The agent can interact with the user only via natural language feedback o \u2208 L. The feedback can contain new observations about the user, allowing the user to provide more information about themselves and their needs/goals/preferences/how their journey towards this goal is evolving (e.g., \"I've already chosen my vacation destination\"/skill level (e.g., \"I have prior drawing experience\" for a learning journey), and rewards about the provided plan (e.g., \"I don't like this plan\", or \"Can you add more steps on relaxation techniques before going to bed?\" for a health journey). Given the observations/feedback o from the user u, the agent decides the next action a to take, which changes the state from st to St+1. The agent can only see partial observations about the user's state based on the natural language feedback. This can be framed as a POMDP [23], where the agent takes actions according to a 0-parameterized policy, i.e., a ~ \u03c0(a|s; 0). We consider language-based agents with policies instantiated via CoT-prompted LLMs.\nHierarchical RL Framing. The agent can take various actions, such as choosing content resources, asking a question, and providing a plan for the user to follow. Analogously to human reasoning, the agent can make these decisions in a hierarchical fashion, starting from a high level and proceeding to more granular actions. This allows for dealing with long horizons. At every user-agent interaction, rather than providing a single (primitive) action, the agent needs to choose a macro-action / goal / option / skill at every c steps. Then given that macro-action, more granular primitive actions can be selected. This corresponds to a hierarchical policy, consisting of a high-level policy or meta-controller deciding the macro-action (Section 3.2), and the low-level policy, a sub-policy over primitive actions (Sections 3.2, 3.2), both updated over time based on user feedback and with potentially different temporal granularity. This paradigm has been long considered in the RL literature [23, 2], but has received little attention in the context of LLMs assisting long-term user goals/tasks. Next, we detail the language-based meta-controller and sub-policies."}, {"title": "3.2 Proposed Framework Components", "content": "Language-based Meta-Controller: Deciding to ask questions, add or alter plan steps The meta-policy is instantiated via a CoT-prompted LLM [28] capable of generating structured language objects such as JSON [31]. The space of actions Z is discrete, i.e., add-steps, alter-steps, and ask-question, but the underlying LLM can also generate natural language arguments for each macro-action xz, e.g., the specific step names to add or alter, or more context about the question asked. The LLM can generate an action z, generate arguments for that action, and reason about its actions through thoughts T\u2208 L. The thoughts, actions, and arguments for the actions become the structured language meta-actions the controller can take, denoted as \u00ee, with 2 = ZU TU X C L. Thus, if \u03a6 denotes all the parameters of the LLM implementing the policy, then the meta-controller selects the macro-action 2 given the context ct, which includes the user's initial goal g, user feedback ot, the history of user-system interactions H, and previous macro-actions the meta-controller has taken 20,..., 2t-1: Meta-Controller: \u03c0(2ct; \u03a6), where context ct = Concat(ot, H, 20, ..., 2t\u22121, 9).\nLanguage-based sub-policies: Generating the added/altered steps or question Once the meta-controller selects the macro-action 2, the corresponding language-based sub-policy comes into play. The sub-policies are implemented also via (potentially the same) LLM, but CoT-prompted with different examples. We have three different language sub-policies, one per discrete action z of the meta-controller: Tadd-steps (\u00b7|; 21), Talter-steps (\u00b7|; 2) and Mask-question(\u00b7|; 23), each with parame- ters 21, 22, and \u03a93, respectively. The add-steps generates sets of actions in the structured language space. After instantiating the specific schema each plan step should follow (name, description, follow-up per step, search keywords used by tool-use policies), the LLM-based policy for adding steps generates a tentative plan of new steps along with the reasoning/thought behind it, as well as a"}, {"title": "Language-based Low-Level Policies: Connecting the user with content per plan step", "content": "Framing recommendation as an RL problem, the action the system takes at every step is the content item c to show to the user [6]. This becomes the primitive action space of our language-based hierarchical agent. The language-based low-level policies decide what content to show to the user per plan step via a combination of tool use and CoT-prompted LLMs. Specifically, for each step in the updated plan, after the execution of the add-steps or alter-step policy, the LLM can decide which tool to call (e.g., SEARCH, RECOMMEND-ENGINE) and fetch n content items per step. Then, given these fetched content items, the same, but different CoT-prompted LLM can decide which of those items should be in the top-k that are shown to the user. In other words, we approximate the retrieval-ranking two-stage recommendation approach [9] by tool use and CoT prompting, respectively.\nInteractive Planning based on User's Natural Language Feedback Using LLMs as the meta- controller, the sub-policies, and the low-level policies, the AI agent can create or update a plan, and ask a question to assist with a user's goal g \u2208 G C L expressed in natural language. As the user's journey towards this goal evolves over time, the user can provide natural language feedback o which is then used to update both the meta-controller in terms of the next macro-action to take, the sub-policies in terms of the specific steps to add / alter or specific questions to ask, and the specific content items to include per plan step. The policies are updated by including user feedback in the context window, as is done with natural language-based RL agents [20, 30]. This mechanism allows for the plan to be interactively adapted and personalized to different users."}, {"title": "4 Qualitative Evaluation", "content": "We performed a qualitative evaluation and found that our proposed framework is powerful in providing interactive planning for user goals across a range of domains, including learning skills, pursuing DIY projects, hobbies, fitness, health, aspirational goals, and many more nuanced personal journeys. Here, we show examples from two important real-world domains: learning [13] and health [10].\nConversational Tutoring for a Learning Goal Figure 2 demonstrates an example of a user-agent interaction in which the user asks for a learning plan: \"How do I explain to my kids about inventors?\" We can see that given the initial query, the meta-controller decided to add three new steps with names Start with stories, Use everyday examples, and Visit museums or science centers. Each of these steps is structured, as executed by sub-policy of add-steps, generating the aforementioned names, a description of why this step is useful, and search keywords. The search keywords are then used by the low-level tool-use policies to fetch and rank content from SEARCH, and RECOMMEND-ENGINE. The content includes animation-based videos and articles regarding children's books on inventions. Additionally, the sub-policy generated follow-up questions per each plan step, as shown at the bottom. The user can interact via either answering one of the provided questions (e.g., Q: \"What's your kids favorite invention?\", A: \"They're really interested in computers and tablets\"), or by writing their own natural language feedback, i.e., \"I would love to introduce them to female inventors too.\". This feedback becomes part of the context of the agent across all levels of the hierarchical policy, and leads to a plan adaptation where new steps on Investigate modern inventions, Explore the history of the tablet, Introduce Charles Babbage are added to the plan. The plan is further adapted based on the user's free-form feedback on female inventors to include steps on Ada Lovelace, Grace Hopper, and Hady Lamarr. The figure illustrates that the LLM-based policies fetch relevant content and generate appropriate follow-up questions per step to allow the user to propel their journey forward.\nConversational Coaching for Personal Health Similarly, Figure 3 demonstrates an example of a user-agent interaction regarding a personal health journey in the domain of fitness\u2014\"I want to do crossfit\". The hierarchical agent again initially decided to add-steps, which then the CoT-prompted LLM-based sub-policy mapped to three steps in the plan: Learn the basics of crossfit, Assess your current fitness level, and Set realistic goals. The tool-enhanced low-level policy then fetched and ranked content for each of the steps. The user can interact with the sub-policy generated follow-up questions or write their own free-form feedback to further adapt the plan. Here, the user answered the question \"What are your fitness goals?\", by saying \"I would like to improve my cardiovascular health.\" Then, the meta-controller decided to alter-step with argument Set Realistic goals. Given the meta-controller's decision, the corresponding sub-policy Walter-step executed the macro-action and altered the step, and the low-level policy used the updated search keywords to update the step."}, {"title": "5 Conclusions", "content": "We proposed a language-based hierarchical agent that is able to assist users across their real-life journeys towards goals and tasks that can last across sessions and evolve over long periods of time. The framework is general and aims to behave in a similar fashion as a human when asked to assist"}]}