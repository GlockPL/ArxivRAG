{"title": "Empirical Capacity Model for Self-Attention Neural Networks", "authors": ["Aki H\u00e4rm\u00e4", "Marcin Pietrasik", "Anna Wilbik"], "abstract": "Large pretrained self-attention neural networks, or transform-\ners, have been very successful in various tasks recently. The performance of\na model on a given task depends on its ability to memorize and generalize\nthe training data. Large transformer models, which may have billions of\nparameters, in theory have a huge capacity to memorize content. However,\nthe current algorithms for the optimization fall short of the theoretical\ncapacity, and the capacity is also highly dependent on the content. In\nthis paper, we focus on the memory capacity of these models obtained\nusing common training algorithms and synthetic training data. Based on\nthe results, we derive an empirical capacity model (ECM) for a generic\ntransformer. The ECM can be used to design task-specific transformer\nmodels with an optimal number of parameters in cases where the target\nmemorization capability of the task can be defined.", "sections": [{"title": "1 Introduction", "content": "The central processing element of the transformer model [25] is the self-attention\ncircuit [1], which computes weighted sums of input vectors based on content.\nLarge transformer models, which may have billions of parameters, typically have\nmultiple layers of parallel sets, or multihead, self-attention circuits, and several\nother processing units. The parameters are optimized using stochastic gradient\nbackpropagation methods [7]. Neural networks based on transformer architecture\nhave been very successful in recent years in various tasks such as natural language\nprocessing [4,10], speech recognition [16], and image processing [11].\nThe memory capacity discussed in this paper does not refer to the ability of\na model to learn a causal pattern in the sequences, which has been studied, for\nexample, in [27,15]. With a sufficient number of parameters, a neural network can\nmemorize, or shatter [23], the machine learning problem. The self-attention circuit\ncan be considered as an associative memory that has a certain capacity determined\nby the number of parameters. Transformer models have a known relation to\nHopfield networks [18] and associative memories [17,3,19]. In theory, transformer\nmodels may have a large storage capacity that depends on the architecture\nchoices [6,12]. However, translating those results into actual attainable capacity\nis difficult."}, {"title": "2 Capacity in networks", "content": "The basic quality measure for the Hopfield networks is the storage capacity,\nunderstood as the maximum number of patterns that are remembered as fixed\npoints by the network. The storage capacity of a Hopfield network with N nodes\nis $C N/log(N)$ where $C < 1/2$ [14], and the stored memories are binary vectors\nof length N. It is interesting to note that a Hopfield network of size GPT-3 [4],\nwith 175B parameters, could theoretically have the capacity to memorize 3.4B\nsequences of 2048 tokens. Furthermore, the reported number of GPT-3 training\ndata tokens is 500B, that is, about seven percent of the theoretical capacity of\nthe network of this thought experiment.\nThe storage can be measured as mean squared errors (MSEs) between the\noriginal and retrieved patterns across different numbers of patterns the model\nwas trained to memorize [21]. Some authors use recall rate [28], measured as\na rate of how many patterns can be reconstructed correctly given a pattern\nwith a certain number of missing bits. Steinberg et al. [20] found that there is a\nminimum initial cue (relative to the total length) that gives a very small recall\nerror. This minimum initial cue is defined as\n$l_c = \\frac{L_o}{L}$,\nwhere $L_o$ is the cue length and $L$ is the pattern length.\nBesides storage capacity, many authors were also interested in measuring the\nso-called basin of attraction [20,22]. The basin of attraction is understood as a"}, {"title": "3 Transformer models", "content": "The input X into the model is a sequence of N discrete symbols $t_i$, or tokens\nin the NLP terminology. In the model, each sequence of tokens is represented\nby the vectors of B elements $x_i, i = 0,..., N-1$. The core component is the\nself-attention circuit which in the conventional form is given by\n$Attn(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}}) V$\nwhere $Q = XW_Q, K = XW_K$, and $V = XW_V$, and matrices $W_Q, W_K$, and $W_V$\nare B \u00d7 dh matrices of a selected xi token vector size B, and a head dimension\ndh. The coefficients of the three matrices are trained in the model optimization\nprocess. For example, the model of [12] suggests that the capacity to memorize C\nindependent data sets is related to O(HN), where H is the number of attention\nheads.\nThe self-attention circuit is the key component of the transformer architecture\nand is mainly responsible for memorization of training data. The $QK^T$ matrix"}, {"title": "4 Data sequences", "content": "Let us consider the system of T tokens in an autoregressive task of predicting\nthe next token\n$t_p = argmax_k F(t_k | t_i, i = 0,\u00b7\u00b7\u00b7, k \u2212 1)$\nwhere the model $F()$ is a transformer followed by a softmax layer\n$F(x_i, i \u2260 k) = softmax(Attn(Q, K, V)H)$"}, {"title": "5 Experiments", "content": "There are several ways to address capacity measurement in learning experiments.\nIn this paper, we consider two approaches. In the Maximum Library Size (MLS)\nmethod, the goal is that the network memorizes every item of the input vector\nlibrary, and the measurement is performed by measuring how large the library\ncan be fully memorized. In the Maximum Attainable Capacity (MAC) method,"}, {"title": "5.1 comparison of the MAC and MLS methods", "content": "As shown in Fig. 4 the MLS and MAC capacity measurements have a similar\noverall shape but there is an offset in the MAC values. The offset is due to\nthe correct guesses in the MAC method. In this case, the size of the library\nK = 32000 and with the token count of T = 128 we get, by (8), K/T = 250,\nwhich agrees with the offset.\nFor larger vector sizes B, the MAC capacity, after compensating the offset,\nseems to be lagging behind the MLS capacity estimates. Therefore, we may\nconsider MLS to represent a lower bound for storage capacity. Since the MAC\ncase represents a more realistic use case, in this paper, we focus on modeling the\nresults of MAC experiments."}, {"title": "6 Empirical capacity model", "content": "We utilize the results obtained from the experiments to develop an empirical ca-\npacity model for self-attention transformers. The model describes the relationship\nbetween the number of memorized sequences as a function of the transformer's\nhyperparameters. We build our model sequentially, that is, by decomposing it\ninto intuitions obtained from examining the marginal effects of each of its inputs.\nThese effects are then modeled using low-order algebraic functions. This allows for\na capacity model that is interpretable and low in the number of parameters. We\nshow that even such a simple formulation outperforms more complex, high-order\npolynomial models. What follows is a description of the marginal impacts of the\nmodel's inputs and how they are leveraged to derive a capacity function."}, {"title": "6.1 Marginal impacts of hyperparamaters", "content": "We begin by capturing the marginal impact of B, that is, its behavior independent\nof the values of H and N. This is shown by the solid lines of Fig. 9, which plots\nthe number of memorized sequences as a function of B at differing H and N.\nWe notice that this number increases monotonically with larger B values until\na saturation point is reached. In our experiments, this saturation occurs at or\nslightly below the total number of sequences. We reflect this intuition in our\nmodeling. Specifically, we first model the initial rise using a linear function. A\nlogistic function was also considered to capture the curves' slight sigmoidal nature;\nhowever, we found that the increase in the parameters in such a modeling was\nnot empirically justified by a closer fit. In modeling a linear rise, it is necessary\nto determine the value of two parameters, the slope and the intercept. The\nintercept is trivial since a value of B = 0 implies a transformer with dimensionless\nfeatures and thus no memorized sequences, resulting in an intercept of 0. The\nslope is determined by the values of H and N, which we model by the function\nf(H, N). We can thus model the presaturation of memorized sequences Cpre as\nCpre = f(H, N) * B. To model the slope values at the corresponding H and N,\nf(H, N), we first calculated them as the slope from the origin to the value at the\nhighest B before saturation. This then allows us to model the impact of H and\nN on Cas captured in Fig. 7. In this regard, we notice an exponential decay in\nthe slope with increasing N values. Indeed, during our modeling, the exponential\ndecay function was our first consideration. We found, however, that it does not\ncapture the tapering that occurs at higher N values. As such, we opted for a\ngeneralized rational function with a power function denominator. This allows us\nto better capture the changes in slope, whilst still maintaining an explainable\nformulation with few parameters. Finally, we notice that H influences the rate of\ndecay in the slope with the rate linearly rising at an increasing number of heads.\nThis phenomenon corresponds to the exponent of the power term and can be\nmodeled accordingly. Specifically, we raise N to a linear function of H, resulting\nin the following slope function:\n$f(N, H) = \\frac{a}{N^{b*H+c} + d} + e$,"}, {"title": "6.2 Empirical capacity model formulation", "content": "Having assessed the marginal impacts of the hyperparameters, the Empirical\nCapacity Model can be formulated. Presaturation C, which is modeled by a\nlinear function, increases without bound with respect to B and thus does not\nmodel the point of saturation. This saturation point is a function of H, with the\nnumber of memorized sequences saturating at higher values with more attention\nheads. This rise is modeled by the linear function \u03b1 * H + \u03b2 which we found\nto offer a good fit with minimal additional parameters. The learned parameters\nof this saturation function on our data are shown also in Table 1. This allows\nus to formulate our Empirical Capacity Model as the synthesis of the pre- and\npost-saturation states as follows:\n$C = max(f(H, N) * \u0392, \u03b1 * \u0397 + \u03b2)$\nThe fit of this model to our data is depicted graphically in Fig. 9. We can\ncompare the fit of our model against a fifth-order polynomial of H, N, and B\nas shown in the bottom row of this figure. We notice that despite the lower\nnumber of parameters (our model's seven compared to the polynomial's 56), our\nmodel provides a better fit. This is reflected in the mean absolute percentage\nerrors (MAPE) of the two models. Specifically, our model has a MAPE of 0.497\ncompared to 1.0785 of the fifth-order polynomial. We analyze our proposed model\nin the following section."}, {"title": "7 Analysis of the capacity model", "content": "The parametric model of Equation 11 was optimized based on experiments with\na library size of K = 16000. The ECM was fitted with a selection of parameter"}, {"title": "8 Discussion and conclusions", "content": "In this paper, we evaluated the capacity of self-attention networks through an\nempirical analysis of their memorization potential in different settings. Such\nan analysis departs from the existing literature focusing on theoretical big-O\nbounds through the use of empirically derived capacities from experiments with\nsynthetic data. In performing this work, our aim was to close the gap between the\ntheoretical storage capacity and the factual, observed capacity of the transformer\nmodels that have garnered widespread interest and use in recent years. To this\nend, we developed an empirical capacity model which can be used to infer the\ncapacity of a network given its hyperparameters. This allows practitioners to\nhave a principled tool for a priori hyperparameter selection that ensures the\nmaximal utility of each parameter is achieved. This model was developed through\nan analysis of the marginal impacts of hyperparameters on network capacity.\nSpecifically, we tested models on a synthetic autoregressive task in which the\ntransformer must predict a subsequent token given a previously observed token\nsequence. The number of correctly predicted sequences was used as a proxy\nmeasurement of the capacity of the model. We ran models to capture the network\ncapacities at different points in the hyperparameter space. The results of these\nmodels served as the basis for a model constructed to reflect insights about the\nbehavior of each hyperparameter. Specifically, insights were synthesized to obtain\nthe model as described in Equation 11. This model, due to its simplicity and low\nnumber of parameters, is intuitive and outperforms more complex, higher-order\npolynomial models.\nIn the future, we plan to expand our work to include more comprehensive and\nrealistic data and testing. Specifically, we will obtain denser results that span a\nlarger hyperparameter range and introduce the effects of the number of network\nlayers. Furthermore, we will expand the data used to include natural language\ncontent. This will ensure that our model's conclusions are relevant outside of\nsynthetic experiments and can be applied in real-world contexts. Finally, we will\npresent a more thorough analysis of our capacity model and include guidelines\nfor a priori hyperparameter selection."}]}