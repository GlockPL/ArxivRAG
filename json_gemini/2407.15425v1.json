{"title": "Empirical Capacity Model for Self-Attention Neural Networks", "authors": ["Aki H\u00e4rm\u00e4", "Marcin Pietrasik", "Anna Wilbik"], "abstract": "Large pretrained self-attention neural networks, or transformers, have been very successful in various tasks recently. The performance of a model on a given task depends on its ability to memorize and generalize the training data. Large transformer models, which may have billions of parameters, in theory have a huge capacity to memorize content. However, the current algorithms for the optimization fall short of the theoretical capacity, and the capacity is also highly dependent on the content. In this paper, we focus on the memory capacity of these models obtained using common training algorithms and synthetic training data. Based on the results, we derive an empirical capacity model (ECM) for a generic transformer. The ECM can be used to design task-specific transformer models with an optimal number of parameters in cases where the target memorization capability of the task can be defined.", "sections": [{"title": "Introduction", "content": "The central processing element of the transformer model [25] is the self-attention circuit [1], which computes weighted sums of input vectors based on content. Large transformer models, which may have billions of parameters, typically have multiple layers of parallel sets, or multihead, self-attention circuits, and several other processing units. The parameters are optimized using stochastic gradient backpropagation methods [7]. Neural networks based on transformer architecture have been very successful in recent years in various tasks such as natural language processing [4,10], speech recognition [16], and image processing [11].\nThe memory capacity discussed in this paper does not refer to the ability of a model to learn a causal pattern in the sequences, which has been studied, for example, in [27,15]. With a sufficient number of parameters, a neural network can memorize, or shatter [23], the machine learning problem. The self-attention circuit can be considered as an associative memory that has a certain capacity determined by the number of parameters. Transformer models have a known relation to Hopfield networks [18] and associative memories [17,3,19]. In theory, transformer models may have a large storage capacity that depends on the architecture choices [6,12]. However, translating those results into actual attainable capacity is difficult."}, {"title": "Capacity in networks", "content": "The basic quality measure for the Hopfield networks is the storage capacity, understood as the maximum number of patterns that are remembered as fixed points by the network. The storage capacity of a Hopfield network with N nodes is $CN/log(N)$ where $C < 1/2$ [14], and the stored memories are binary vectors of length N. It is interesting to note that a Hopfield network of size GPT-3 [4], with 175B parameters, could theoretically have the capacity to memorize 3.4B sequences of 2048 tokens. Furthermore, the reported number of GPT-3 training data tokens is 500B, that is, about seven percent of the theoretical capacity of the network of this thought experiment.\nThe storage can be measured as mean squared errors (MSEs) between the original and retrieved patterns across different numbers of patterns the model was trained to memorize [21]. Some authors use recall rate [28], measured as a rate of how many patterns can be reconstructed correctly given a pattern with a certain number of missing bits. Steinberg et al. [20] found that there is a minimum initial cue (relative to the total length) that gives a very small recall error. This minimum initial cue is defined as\n$l_c = \\frac{L_o}{L}$,\nwhere $L_o$ is the cue length and $L$ is the pattern length.\nBesides storage capacity, many authors were also interested in measuring the so-called basin of attraction [20,22]. The basin of attraction is understood as a"}, {"title": "Transformer models", "content": "The input X into the model is a sequence of N discrete symbols $t_i$, or tokens in the NLP terminology. In the model, each sequence of tokens is represented by the vectors of B elements $x_i, i = 0,..., N - 1$. The core component is the self-attention circuit which in the conventional form is given by\n$Attn(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}})V$,\nwhere $Q = XW_Q, K = XW_K$, and $V = XW_V$, and matrices $W_Q, W_K$, and $W_V$ are B \u00d7 $d_h$ matrices of a selected $x_i$ token vector size B, and a head dimension $d_h$. The coefficients of the three matrices are trained in the model optimization process. For example, the model of [12] suggests that the capacity to memorize C independent data sets is related to $O(HN)$, where H is the number of attention heads.\nThe self-attention circuit is the key component of the transformer architecture and is mainly responsible for memorization of training data. The $QK^T$ matrix"}, {"title": "Data sequences", "content": "Let us consider the system of T tokens in an autoregressive task of predicting the next token\n$t_p = argmax_k F(t_k|t_i, i = 0,\\cdot\\cdot\\cdot, k - 1)$\nwhere the model $F()$ is a transformer followed by a softmax layer\n$F(x_i, i \\neq k) = softmax(Attn(Q, K, V)H)$"}, {"title": "Experiments", "content": "There are several ways to address capacity measurement in learning experiments. In this paper, we consider two approaches. In the Maximum Library Size (MLS) method, the goal is that the network memorizes every item of the input vector library, and the measurement is performed by measuring how large the library can be fully memorized. In the Maximum Attainable Capacity (MAC) method,"}, {"title": "comparison of the MAC and MLS methods", "content": "As shown in Fig. 4 the MLS and MAC capacity measurements have a similar overall shape but there is an offset in the MAC values. The offset is due to the correct guesses in the MAC method. In this case, the size of the library K = 32000 and with the token count of T = 128 we get, by (8), K/T = 250, which agrees with the offset.\nFor larger vector sizes B, the MAC capacity, after compensating the offset, seems to be lagging behind the MLS capacity estimates. Therefore, we may consider MLS to represent a lower bound for storage capacity. Since the MAC case represents a more realistic use case, in this paper, we focus on modeling the results of MAC experiments."}, {"title": "Impact of batch size", "content": "The number of MAC memorized sequences of 32 or 128 tokens, as a function of batch size over a long training time, is shown in Fig. 6. The memorization is low with small batch sizes, which is also reflected in the large variation in the gradient noise. The capacity grows when the batch size increases and saturates, for the model sizes studied in this paper, after the batch size of 512.\nIn this paper, the transformer model is based on the implementation in the popular x-transformers Python library [26] and is trained using the Adam [7] optimizer available in the PyTorch library, for which we use default settings. The selection of the optimal batch size can be estimated based on noise gradients; see, e.g., [13]. In the current paper we use a batch size of 512. In total, we trained approximately 500 models, the total training time was 260 hours on an Nvidia A100-SXM4-40GB GPU. Each training was restarted five times, and we selected"}, {"title": "Detailed figures of measured model capacities", "content": "The measured and predicted capacity figures for different transformer model architectures are shown in Figs. 8."}, {"title": "Empirical capacity model", "content": "We utilize the results obtained from the experiments to develop an empirical ca-pacity model for self-attention transformers. The model describes the relationship between the number of memorized sequences as a function of the transformer\u2019s hyperparameters. We build our model sequentially, that is, by decomposing it into intuitions obtained from examining the marginal effects of each of its inputs. These effects are then modeled using low-order algebraic functions. This allows for a capacity model that is interpretable and low in the number of parameters. We show that even such a simple formulation outperforms more complex, high-order polynomial models. What follows is a description of the marginal impacts of the model\u2019s inputs and how they are leveraged to derive a capacity function."}, {"title": "Marginal impacts of hyperparamaters", "content": "We begin by capturing the marginal impact of B, that is, its behavior independent of the values of H and N. This is shown by the solid lines of Fig. 9, which plots the number of memorized sequences as a function of B at differing H and N. We notice that this number increases monotonically with larger B values until a saturation point is reached. In our experiments, this saturation occurs at or slightly below the total number of sequences. We reflect this intuition in our modeling. Specifically, we first model the initial rise using a linear function. A logistic function was also considered to capture the curves' slight sigmoidal nature; however, we found that the increase in the parameters in such a modeling was not empirically justified by a closer fit. In modeling a linear rise, it is necessary to determine the value of two parameters, the slope and the intercept. The intercept is trivial since a value of B = 0 implies a transformer with dimensionless features and thus no memorized sequences, resulting in an intercept of 0. The slope is determined by the values of H and N, which we model by the function f(H, N). We can thus model the presaturation of memorized sequences $C_{pre}$ as $C_{pre} = f(H, N) * B$. To model the slope values at the corresponding H and N, f(H, N), we first calculated them as the slope from the origin to the value at the highest B before saturation. This then allows us to model the impact of H and N on C as captured in Fig. 7. In this regard, we notice an exponential decay in the slope with increasing N values. Indeed, during our modeling, the exponential decay function was our first consideration. We found, however, that it does not capture the tapering that occurs at higher N values. As such, we opted for a generalized rational function with a power function denominator. This allows us to better capture the changes in slope, whilst still maintaining an explainable formulation with few parameters. Finally, we notice that H influences the rate of decay in the slope with the rate linearly rising at an increasing number of heads. This phenomenon corresponds to the exponent of the power term and can be modeled accordingly. Specifically, we raise N to a linear function of H, resulting in the following slope function:\n$f(N, H) = \\frac{a}{N^{b * H + c}} + d + e$,\nwhere a, b, c, d, and e are parameters learned from our experiments. Their values are summarized in Table 1. Our slope formulation lends itself to a simple explanation for both input variables. N represents the exponential decay in the rate at which sequences are memorized as a function of B, and H influences the rate of this decay with higher values of H, resulting in a less pronounced drop."}, {"title": "Empirical capacity model formulation", "content": "Having assessed the marginal impacts of the hyperparameters, the Empirical Capacity Model can be formulated. Presaturation C, which is modeled by a linear function, increases without bound with respect to B and thus does not model the point of saturation. This saturation point is a function of H, with the number of memorized sequences saturating at higher values with more attention heads. This rise is modeled by the linear function $\\alpha * H + \\beta$ which we found to offer a good fit with minimal additional parameters. The learned parameters of this saturation function on our data are shown also in Table 1. This allows us to formulate our Empirical Capacity Model as the synthesis of the pre- and post-saturation states as follows:\n$C = max(f(H, N) * B, \\alpha * H + \\beta)$\nThe fit of this model to our data is depicted graphically in Fig. 9. We can compare the fit of our model against a fifth-order polynomial of H, N, and B as shown in the bottom row of this figure. We notice that despite the lower number of parameters (our model's seven compared to the polynomial's 56), our model provides a better fit. This is reflected in the mean absolute percentage errors (MAPE) of the two models. Specifically, our model has a MAPE of 0.497 compared to 1.0785 of the fifth-order polynomial. We analyze our proposed model in the following section."}, {"title": "Analysis of the capacity model", "content": "The parametric model of Equation 11 was optimized based on experiments with a library size of K = 16000. The ECM was fitted with a selection of parameter"}, {"title": "Discussion and conclusions", "content": "In this paper, we evaluated the capacity of self-attention networks through an empirical analysis of their memorization potential in different settings. Such an analysis departs from the existing literature focusing on theoretical big-O bounds through the use of empirically derived capacities from experiments with synthetic data. In performing this work, our aim was to close the gap between the theoretical storage capacity and the factual, observed capacity of the transformer models that have garnered widespread interest and use in recent years. To this end, we developed an empirical capacity model which can be used to infer the capacity of a network given its hyperparameters. This allows practitioners to have a principled tool for a priori hyperparameter selection that ensures the maximal utility of each parameter is achieved. This model was developed through an analysis of the marginal impacts of hyperparameters on network capacity. Specifically, we tested models on a synthetic autoregressive task in which the transformer must predict a subsequent token given a previously observed token sequence. The number of correctly predicted sequences was used as a proxy measurement of the capacity of the model. We ran models to capture the network capacities at different points in the hyperparameter space. The results of these models served as the basis for a model constructed to reflect insights about the behavior of each hyperparameter. Specifically, insights were synthesized to obtain the model as described in Equation 11. This model, due to its simplicity and low number of parameters, is intuitive and outperforms more complex, higher-order polynomial models.\nIn the future, we plan to expand our work to include more comprehensive and realistic data and testing. Specifically, we will obtain denser results that span a larger hyperparameter range and introduce the effects of the number of network layers. Furthermore, we will expand the data used to include natural language content. This will ensure that our model's conclusions are relevant outside of synthetic experiments and can be applied in real-world contexts. Finally, we will present a more thorough analysis of our capacity model and include guidelines for a priori hyperparameter selection."}]}