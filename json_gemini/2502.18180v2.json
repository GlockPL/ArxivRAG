{"title": "ChatMotion: A Multimodal Multi-Agent for Human Motion Analysis", "authors": ["Lei Li", "Sen Jia", "Jianhao Wang", "Zhaochong An", "Jiaang Li", "Jenq-Neng Hwang", "Serge Belongie"], "abstract": "Advancements in Multimodal Large Language Models (MLLMs) have improved human motion understanding. However, these models remain constrained by their \"instruct-only\" nature, lacking interactivity and adaptability for diverse analytical perspectives. To address these challenges, we introduce ChatMotion, a multimodal multi-agent framework for human motion analysis. ChatMotion dynamically interprets user intent, decomposes complex tasks into meta-tasks, and activates specialized function modules for motion comprehension. It integrates multiple specialized modules, such as the MotionCore, to analyze human motion from various perspectives. Extensive experiments demonstrate ChatMotion's precision, adaptability, and user engagement for human motion understanding.", "sections": [{"title": "1 Introduction", "content": "Human motion understanding (Li et al., 2024b; Zhou et al., 2024; Loper et al., 2023; Jiang et al., 2024, 2023b) has gained attention due to its wide-ranging applications in fields such as healthcare, human-computer interaction, rehabilitation, sports science, and virtual human modeling (Plappert et al., 2016; Zhang et al., 2021; Hong et al., 2022; Qu et al., 2024). A deep understanding of human motion can drive advancements in areas like physical therapy (Smeddinck, 2020), immersive virtual experiences (Xiao et al., 2024), and assistive technology interfaces (Khiabani, 2021). As human motion data becomes more accessible, the demand for systems capable of effectively processing and analyzing this data has increased (Zhang, 2024). However, existing motion understanding models often struggle to handle the accurate analysis of human motions and the dynamic nature of user requirements (Meng et al., 2020; Smeddinck, 2020). These MLLMs tend to exhibit limited adaptability to complex, multi-faceted user queries and are often constrained by biases inherent in single-model analyses (Frangoudes et al., 2022), failing to integrate diverse insights into a comprehensive, generalizable, and accurate analysis (Xu et al., 2021).\nWith the LLM-driven application development(Shi et al., 2025, 2024; Cai et al., 2024a; Liu et al., 2024a; Zheng et al., 2025; Yang et al., 2024; Cai et al., 2024b), recent advancements in human motion understanding have progressed, particularly with LLM-based methods targeting specialized tasks and domain-specific applications. Models such as MotionGPT (Jiang et al., 2023a) and MotionLLM (Chen et al., 2024a) propose methods to encode motion into structured formats, translating motion data (e.g., videos) into textual descriptions for general motion understanding tasks. Building on this foundation, LLaMo (Li et al., 2024b) integrates a motion encoder and cross-talker without relying on motion quantification, demonstrating capabilities in general motion comprehension and specialized analysis across professional domains. These LLM-based motion models aim to bridge raw motion data and interpretable insights, enabling applications in diverse fields.\nDespite these advancements, existing approaches still face limitations when applied to broader motion analysis tasks. A key challenge is their reliance on single-model architectures, which often struggle to address complex user requirements (Wei et al., 2024). These models show limited adaptability to dynamic user goals and lack mechanisms to integrate insights from multiple MLLMS, constraining their ability to provide comprehensive results. Additionally, they lack effective frameworks for verifying outcomes or refining analyses based on user feedback, which may affect reliability (Lan et al., 2022). As a result, current Motion LLMs encounter challenges in delivering accurate and complete human motion analyses.\nTo address these challenges and based on LLaMo (Li et al., 2024b), we introduce ChatMotion, the first agent-based framework for motion understanding, combining multi-agent systems with the MotionCore toolbox. Given motion or video data with a user prompt, ChatMotion uses a planner to decompose the task into sub-tasks, which are then handled by the Executor using tools within MotionCore. The MotionCore consists of four modules: MotionAnalyzer, Aggregator, Generator, and Auxiliary Module. The Executor calls upon the MotionAnalyzer, utilizing multiple motion LLMs to analyze data from various perspectives. The Aggregator, with two mechanisms, synthesizes the most probable result from the MotionAnalyzer outputs. The Generator reviews the user's request and synthesizes the answer, leveraging contextual information from other modules. A verifier ensures consistency and relevance of intermediate results, enhancing the reliability of the final output. Through coordinated agent efforts, ChatMotion provides a flexible, precise, and reliable approach to motion analysis, overcoming the limitations of traditional motion LLMs.\nWe validate ChatMotion across a wide range of general human motion understanding datasets (e.g., Movid (Chen et al., 2024a), BABEL-QA (Endo et al., 2023), MVbench (Li et al., 2024a), and Mo-Repcount (Li et al., 2024b)), demonstrating its effectiveness across both standard and complex tasks. Experimental results highlight the improvements in accuracy, adaptability, and user engagement, establishing new benchmarks in the field of human motion analysis. In summary, the contributions of this work are as follows:\n\u2022 ChatMotion, a multi-agent system with a planner-Executor-verifier architecture for comprehensive human motion analysis.\n\u2022 A robust MotionCore for invoking functional tools to achieve advanced comprehension by synthesizing multiple perspectives from various MLLMs and can be readily extended, ensuring adaptability and scalability.\n\u2022 Empirical validation across multiple datasets demonstrates that ChatMotion achieves improved performance in human motion analysis compared to existing MLLMs."}, {"title": "2 Related works", "content": ""}, {"title": "2.1 Human Multimodal Representations", "content": "Multimodal representation learning is pivotal for human-centric analyses, especially in tasks requiring spatial-temporal reasoning to interpret complex behaviors (Lin et al., 2023b; Ning et al., 2023; Li et al., 2023). Recent advancements, such as Video-LLaVA, integrate visual information from images and videos into a unified linguistic feature space, enabling improved visual reasoning for behavioral analysis (Lin et al., 2023b). However, many models remain limited to isolated video frames and privacy concerns, constraining their effectiveness in the dynamic real world. (Ning et al., 2023; Heilbron et al., 2015; Maaz et al., 2023). To address these limitations, motion data has emerged as a privacy-preserving alternative, allowing action analysis without revealing identifiable visual details (Song et al., 2023b; Yang et al., 2023b). By combining visual and motion data, emerging multimodal frameworks offer comprehensive, privacy-aware solutions, leveraging the complementary strengths of both modalities for enhanced adaptability across diverse applications."}, {"title": "2.2 Human Motion Understanding", "content": "Human motion analysis traditionally relies on skeletal data, represented as joint keypoint sequences, to capture movement dynamics while preserving user privacy (Shi et al., 2023; Plappert et al., 2018; Yang et al., 2023a). Early methods, such as 2s-AGCN (Shi et al., 2019), and recent transformer-based models like MotionCLIP (Chen et al., 2024b), have demonstrated success in tasks such as activity recognition, caption generation, and behavior analysis by translating motion data into language tokens. While effective in modeling structural movement patterns, these approaches often neglect environmental context, which is crucial for interpreting motions that may convey different meanings based on situational factors (Song et al., 2023a; Maaz et al., 2023). To address this, recent models integrate motion and visual data, enabling improved generalization in dynamic and diverse environments (Liu et al., 2024b; He et al., 2023). Frameworks like LLaMo(Li et al., 2024b) have further advanced the field by incorporating motion encoders, estimators, and efficient fusion mechanisms, achieving state-of-the-art results in both general and specialized motion analysis."}, {"title": "3 ChatMotion", "content": "As shown in Fig. 2, ChatMotion is a multi-agent system that processes user queries involving motion and video data through the Planner, Executor, and Verifier, with LLaMA-70B (Touvron et al., 2023) employed for all agents. The Planner decomposes the task into meta-tasks, the Executor executes them via MotionCore function calls, and the Verifier ensures accuracy, delivering context-aware, precise results for complex motion analysis."}, {"title": "3.1 Planner", "content": "The planner serves as the decision-maker, interpreting user intent and subdividing complex tasks into structured meta-tasks. It first analyzes the input query to identify the core objectives and dependencies within the task, and then breaks the task down into smaller, manageable meta-tasks. It operates as the initial step in the multi-agent framework, ensuring that user requirements are translated into a structured workflow that aligns with evolving goals.\nSpecifically, let us denote a user query by \\(R\\). As the simplified version is illustrated in Fig. 2, the Planner will receive an instruction containing user query and available tools functionality in MotionCore which is a function toolbox tailored for human motion analysis (see Sec. 3.4). Then, the Planner will follow the instructions and identify a set of core objectives \\(O = \\{o_1, o_2,..., o_m\\}\\) simply based on \\(R\\). These objectives are then decomposed into finer-grained meta-tasks guided by the specific functionalities available in the MotionCore tools.\n\\[M = \\{M_1, M_2,..., M_k\\},\\]\nwhere each \\(M_i\\) represents a meta-task in the overall workflow. This decomposition allows the system to handle a wide range of user inputs, from simple queries to multi-step, dynamic tasks."}, {"title": "3.2 Executor", "content": "Executor serves as the core execution component, responsible for translating the Planner's meta-tasks into actionable operations using a suite of function tools. After provided the meta-tasks \\(M\\), the Executor will process each task in turn guided by the instruction as illustrated in Fig. 2, determining and using the most appropriate function tools in MotionCore (see Sec. 3.4) based on the alignment between their functional description and the objectives of the meta-task.\nFormally, for a given meta-task \\(M_i \\in M\\), The Executor will traverse functions capabilities within MotionCore and choose an appropriate tool \\(\\phi_i\\) from a function tool set \\(\\Phi = \\{\\phi_1, \\phi_2,..., \\phi_s\\}\\) in MotionCore, according to a mapping\n\\[\\Phi(M_i) \\rightarrow \\phi_i,\\]\nwhere \\(\\phi_i\\) is the specific function tool that best addresses the requirements of meta-task \\(M_i\\).\nIf any meta-task proves infeasible, e.g., due to missing functionality, the Executor returns complete error information to the Planner, which will then update its tasks accordingly. The Executor reattempts these updated tasks, iterating through multiple rounds until the overall complex objective is met."}, {"title": "3.3 Verifier", "content": "The Verifier acts as a supervisory agent, ensuring the accuracy and reliability of the multi-agent workflow. It has two main roles: first, it checks that the Planner's meta-tasks are logically structured and aligned with the user's prompt; second, it verifies that the meta-tasks can be executed using available tools and that the results meet expectations. If any meta-task cannot be executed or produces incorrect results, or if the Executor calls an inappropriate function, the Verifier prompts the Planner to revise the task list or the Executor to select a different tool. This feedback loop ensures that tasks are executed correctly using the right tools."}, {"title": "3.4 MotionCore", "content": "MotionCore is a comprehensive toolkit that enables efficient human motion understanding by integrating various modules and auxiliary functions. It also includes auxiliary tools for tasks like motion visualization and video retrieval, meeting users' diverse requirements. MotionCore is orchestrated by the Executor Agent, which autonomously selects the appropriate tools from the toolkit to complete tasks based on a given meta-task list."}, {"title": "3.4.1 MotionAnalyzer", "content": "The MotionAnalyzer in MotionCore enhances motion understanding and mitigates biases through a dynamic, multi-model approach. It integrates human motion models, such as MotionLLM (Chen et al., 2024a), MotionGPT (Jiang et al., 2023a), and LLaMo(Li et al., 2024b), alongside video captioning models such as VideoChat2 (Li et al., 2024a), GPT-4v (OpenAI, 2023b), and video-LLaVA (Lin et al., 2023a) to handle human motion input.\nLet the set of motion understanding models be denoted as \\(\\{F_1, F_2, . . ., F_N \\}\\), where each model \\(F_i\\) processes the multimodal input data \\(D\\) (e.g., video frames, motion capture data) to produce text analysis \\(r_i\\), i.e., \\(r_i = F_i(D)\\), \\(i = 1, 2, ..., N\\). Each model is assigned a predefined confidence score \\(c_i\\), based on the previous evaluation performance, independent of the model's predictions. These confidence scores are allocated based on the input modalities, which can be motion capture, video, or motion-video. The outputs and their corresponding confidence scores are represented as \\(\\{(r_1, c_1), (r_2, c_2),..., (r_n, c_n)\\}\\), where \\(c_i\\) denotes the predefined confidence score for the output \\(r_i\\) of model \\(F_i\\) in its respective task. This integration of predefined confidence scores ensures a robust and flexible understanding of motion, leveraging the strengths of each model across diverse modalities and tasks."}, {"title": "3.4.2 Aggregator", "content": "The Aggregator in MotionCore identifies the most reliable result from a set of \\(\\{(r_i, c_i)\\}\\) pairs, employing two strategies: the Confidence Mechanism and the Motion-aware Mechanism, which enhance the robustness of motion understanding by selecting the most accurate outcome from diverse perspectives.\nConfidence Mechanism Rooted in game theory, this method considers the set\n\\[\\{(r_i, c_i) | i = 1, 2, ..., N\\},\\]\nwhere \\(r_i\\) is a model's output and \\(c_i\\) is its associated confidence score. The mechanism assigns higher weight to more confident outputs, with a \"majority wins\" principle when models converge on similar results. Rather than using a fixed function, the analysis-confidence pairs \\(\\{(r_i, c_i)\\}\\) are passed to LLaMA (Touvron et al., 2023), which adaptively integrates the outputs by balancing consensus with individual model expertise. This ensures a flexible and robust aggregation process, emphasizing shared conclusions while considering outlier predictions.\nThough foundational, this approach is basic, relying primarily on confidence scores and model consensus. The next step incorporates a motion-aware mechanism to refine the process.\nMotion-aware Mechanism With LLaMo's (Li et al., 2024b) specialized motion-understanding capabilities, this mechanism evaluates \\(\\{(r_i, c_i)\\}\\) pairs alongside the original motion or video data \\(M\\), generating an initial estimate:\n\\[r' = \\text{LLaMo}(r_1,...,r_N; c_1,...,c_N; M).\\]\nLLaMA (Touvron et al., 2023) then re-examines the preliminary result \\(r'\\) and the original pairs \\(\\{(r_i, c_i)\\}\\) to mitigate model bias and refine the outcome. This dual-layer evaluation leverages LLaMo's domain-specific motion expertise and LLaMA's context-aware reasoning, improving both reliability and precision.\nThe Aggregator is a powerful tool within MotionCore, enabling ChatMotion to identify the most accurate analyses from diverse model outputs, fostering a more comprehensive understanding of human motion."}, {"title": "3.4.3 Generator", "content": "In MotionCore, the Generator is responsible for synthesizing contextual information from previous function calls and the user's original request to produce a final answer. As illustrated in Fig. 3.4, the Generator reviews the user query and organizes the context into a coherent and accurate answer. The answer could be in the form of textual analysis, motion feedback, or other formats, depending on the user's request. Contextual information from earlier interactions is denoted as \\(t^*\\). The module then integrates this context with the user's specific requirements, represented as \\(R\\), to generate a comprehensive response:\n\\[\\text{Answer} = \\Gamma(t^*, R),\\]\nwhere \\(\\Gamma(\\cdot)\\) denotes LLaMA (Touvron et al., 2023) by default. The purpose of the Generator is to transform the context into an answer that directly addresses the user's needs, ensuring the answer is concise and contextually accurate."}, {"title": "3.4.4 Auxiliary Tools", "content": "The Auxiliary Tools in MotionCore, which can be accessed by the Executor, extend ChatMotion's capabilities by orchestrating external, domain-specific functionalities that go beyond the scope of the multimodal model alone. For instance, the system can retrieve professional analysis by querying specialized knowledge bases, which provide context-specific insights based on user inputs. Additionally, it enables motion retrieval by identifying relevant motion data based on the user's request, leveraging a stored database of labeled motion data and utilizing vector-based search to match the query to the most relevant motion. As a result, it equips ChatMotion with diverse motion analysis capabilities that simple MLLMs do not possess. By offering a unified, modular interface for diverse auxiliary function calls, ChatMotion readily integrates and extends new capabilities without overburdening the core model."}, {"title": "4 Experimental Setup", "content": "Datasets We evaluate ChatMotion on general human motion understanding benchmarks including Movid-bench (Chen et al., 2024a), BABEL-QA (Endo et al., 2023) and MVbench (Li et al., 2024a), as well as Mo-Repcount (Li et al., 2024b) for fine-grained motion capture capabilities. MoVid-Bench specifically assesses the model's ability to understand human behavior in both motion and video contexts. It consists of 1,350 data pairs, with 700 motion and 650 video samples, covering diverse daily scenarios in real-world. In addition, ChatMotion is tested on BABEL-QA and MVbench to evaluate motion-based and video-based question answering respectively.\nTasks and Metrics ChatMotion is evaluated on tasks including action recognition, motion reasoning, and question answering. For MoVid-Bench, we follow established LLM evaluation metrics, assessing body-part recognition, sequential analysis, directionality, reasoning, and hallucination control in both motion and video contexts. BABEL-QA uses similar metrics with a focus on motion-related question answering, while Mo-Repcount employs specialized metrics like OBO, MAE, OBZ, and RMSE for fine-grained motion tracking accuracy. In the MVBench video understanding evaluation, we respond to multiple-choice questions by selecting the most suitable option as outlined in.\nBaselines For our baselines, we select SOTA Motion LLMs for human-centric motion understanding, e.g., LLaMo (Li et al., 2024b), MotionLLM (Chen et al., 2024a) and MotionGPT (Jiang et al., 2023a). These models are widely recognized for their ability to process and understand human motion in both video and action contexts. For ChatMotion, ChatMotion(CB) and ChatMotion denote the versions using confidence-based and motion-aware aggregation, respectively. Through extensive comparison, our results highlight ChatMotion's exceptional ability to handle complex human motion understanding tasks, outperforming the selected baselines across a range of evaluation metrics."}, {"title": "5 Results", "content": ""}, {"title": "5.1 Quantitative Analysis", "content": "Evaluation on Motion Understanding in MoVid-Bench. Table 1 compares the performance of motion-based LLMs on MoVid-Bench-Motion. Both ChatMotion(CB) and ChatMotion outperform existing baselines across all metrics. ChatMotion achieves an accuracy of 58.79% and a score of 3.80, surpassing LLaMo by 3.47% in accuracy and 0.13 in score. It also demonstrates strong hallucination control, achieving 70.39% accuracy compared to LLaMo's 61.17%, underscoring the effectiveness of ChatMotion's multi-model integration via its robust selection strategy.\nPrevious models, such as MotionLLM and MotionGPT, lose fine-grained motion details due to motion discretization, leading to lower performance. Although LLaMo improves motion encoding, its single LLM-based structure introduces biases that limit its motion understanding capabilities. In contrast, ChatMotion leverages multi-agent collaboration and multi-model aggregation to enhance motion understanding. This approach reduces biases inherent in single LLM-based motion models and improves performance in motion sequence analysis. By integrating multiple agents, ChatMotion achieves greater robustness, demonstrating its superior capabilities to capture diverse motion dynamics and delivers more accurate, reliable results in complex motion understanding tasks.\nEvaluation on Video Understanding in MoVid-Bench. ChatMotion(CB) demonstrates improvements across multiple metrics on MoVid-Bench-Video as shown in Table 1, achieving an overall accuracy of 53.51% and a score of 3.19, surpassing baseline models in all evaluated tasks. This performance gain is due to its effective aggregation of diverse video analysis perspectives, combined with confidence scores to ensure more reliable and stable reasoning. Furthermore, ChatMotion, with its motion-aware mechanism, further refines the analysis by better handling motion-related tasks, surpassing ChatMotion(CB) with an accuracy improvement of 1.45% and a score increase of 0.06. This enhancement allows it to more effectively aggregate and analyze motion data, pushing performance beyond that of standard models. These innovations in model design, coupled with the synergistic effects of specialized modules, allow ChatMotion(CB) and ChatMotion to set new benchmarks in multimodal human motion analysis, outperforming existing LLM-based motion models across multiple tasks and metrics.\nEvaluation on BABEL-QA. We evaluated ChatMotion on the BABEL-QA dataset to assess its performance in responding to complex motion-based queries. As shown in Table 2, both ChatMotion(CB) and ChatMotion outperform other LLM-based motion models across several metrics. ChatMotion(CB) achieves an overall score of 0.467, while ChatMotion further improves this to 0.473, demonstrating its enhanced capability. This improvement is due to ChatMotion's motion-aware mechanism, which takes both motion inputs and candidate results into account. By leveraging LLaMo's advanced multimodal capabilities, ChatMotion esures more robust and stable results. Despite some limitations on specific metrics, ChatMotion compensates for these and delivers superior overall results. These advancements position ChatMotion as a new benchmark in motion-based question answering, highlighting the effectiveness of multimodal aggregation and motion-aware mechanisms in achieving more accurate and reliable results.\nEvaluation on MVBench. We evaluated ChatMotion on the MVBench dataset to assess its performance in video question answering across seven motion understanding sub-tasks. As shown in Table 3, ChatMotion(CB) outperforms MotionLLM (Chen et al., 2024a), the LLM-based motion understanding model, achieves an average score of 52.4, while ChatMotion increases this to 53.2. These results highlight the efficacy of ChatMotion's multi-agent framework, which reduces biases inherent to LLM-based motion models by incorporating dynamic function calls. Performance gains are particularly evident in most metrics, demonstrating the advantages of multi-agent integration for robust motion understanding. While slight performance gaps persist in specific tasks compared with expert models (e.g., EN of VideChat2), the overall improvement over the LLM-based motion model, MotionLLM, remains statistically better.\nEvaluation on Mo-Repcount To evaluate ChatMotion's performance on fine-grained motion tasks, we benchmarked it on Mo-Repcount against SOTA Motion LLMs. The results in Table 4 show that ChatMotion outperforms LLaMo by 4%-8% across all metrics, demonstrating ChatMotion's advanced capability to aggregate the strengths of specialized models and achieve superior performance in fine-grained motion tasks."}, {"title": "5.2 Qualitative Analysis", "content": "Qualitative results, as shown in Fig. 4, demonstrate ChatMotion's superior capabilities in understanding human motion across diverse scenarios. In a task where a human expresses sadness, using both video and motion inputs, MotionLLM fails to provide a correct interpretation, while LLaMo identifies the emotion, though with some ambiguity. Notably, ChatMotion excels in tasks that current LLM-based motion models struggle with, including fine-grained counting and comprehensive analyses utilizing RAG, alongside detailed comparisons of motion-capture and video data. These results showcase the model's ability to handle complex, multimodal motion tasks that require context-sensitive reasoning beyond the capabilities of existing models."}, {"title": "6 Conclusion", "content": "In this paper, we introduced ChatMotion, a sophisticated multi-agent framework that integrates large language models with specialized motion-analysis modules to address the limitations inherent in single-model systems. By dynamically breaking down complex tasks, aggregating diverse model outputs, and carefully selecting the most reliable results, ChatMotion effectively mitigates biases in motion understanding and delivers robust, context-aware analyses. Through experiments conducted on human motion benchmarks such as MoVid-Bench and BABEL-QA, we demonstrated significant improvements in both accuracy and adaptability across various motion tasks."}]}