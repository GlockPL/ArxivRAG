{"title": "Augmenting Legal Decision Support Systems with LLM-based NLI for Analyzing Social Media Evidence", "authors": ["Ram Mohan Rao Kadiyala", "Siddartha Pullakhandam", "Kanwal Mehreen", "Subhasya Tippareddy", "Ashay Srivastava"], "abstract": "This paper presents our system description and error analysis of our entry for NLLP 2024 shared task on Legal Natural Language Inference (L-NLI) (Hagag et al., 2024). The task required classifying these relationships as entailed, contradicted, or neutral, indicating any association between the review and the complaint. Our system emerged as the winning submission, significantly outperforming other entries with a substantial margin and demonstrating the effectiveness of our approach in legal text analysis. We provide a detailed analysis of the strengths and limitations of each model and approach tested, along with a thorough error analysis and suggestions for future improvements. This paper aims to contribute to the growing field of legal NLP by offering insights into advanced techniques for natural language inference in legal contexts, making it accessible to both experts and newcomers in the field.", "sections": [{"title": "Introduction", "content": "1 In today's digital age, vast amounts of information circulate online, creating an overwhelming stream of text that spans news articles, social media, and user-generated content. Within this unstructured data, legal violations often remain hidden, blending into the surrounding noise. Legal violations frequently leave behind data traces. To identify these traces and detect violations, prior research in Legal NLI (Koreeda and Manning, 2021) has typically utilized specialized models designed for particular domain applications (Silva et al., 2020) (Yu et al., 2020). Uncovering these violations is not only crucial for upholding individual rights and ethical standards, but also for maintaining societal justice in an increasingly digital world. Addressing this challenge requires more than traditional methods. While existing models have proven effective within their specialized domains, they lack the flexibility needed to tackle the complex and varied nature of legal violations found in diverse online contexts. Our work seeks to bridge this gap by leveraging advanced language models for the nuanced task of Legal Natural Language Inference (L-NLI), as part of the NLLP 2024 shared task. The aim was to classify relationships between legal complaints and reviews as either entailed, contradicted, or neutral. In this study, we implemented a range of techniques, including multi-layered fine-tuning and alignment strategies, to enhance text classification. We experimented with several LLMs, such as Gemma (Team, 2024), Phi3 (Abdin, 2024), Zephyr (Tunstall et al., 2023), LLaMA (Dubey et al., 2024), Mistral (Jiang et al., 2023), OpenHermes (Teknium, 2023) and Qwen (Yang et al., 2024) refining each model for optimal performance. These approaches proved highly effective, with our system outperforming other entries by a large margin. Beyond technical achievements, we present a thorough error analysis, highlighting where the models excelled / struggled. Through our findings, we aim to advance the field of legal NLP, making complex legal analysis accessible to a wider audience, while pushing the boundaries of NLI in legal domain. The code and models used in the official submission and the later found best model can be found here."}, {"title": "Dataset", "content": "The dataset for the NLI task consists of a legal premise (a summary of resolved class-action cases) and a corresponding hypothesis (an online media text). The training and test splits of the dataset consist of 312 and 84 samples. For the initial fine-tuning, the test and validation subsets of the SNLI dataset (Bowman et al., 2015) were used consisting of 20000 samples. The distributions of each of the training sets and the test set can be seen in Table 1. The original dataset (Bernsohn et al., 2024) used had just 312 rows, the aggregation of datasets is explained in detail in Appendix. The length of the texts are both mostly 4-7 sentences long in both the premise and hypothesis."}, {"title": "System Description", "content": "Various LLMs were tested with and without additional training data or additional training stages. They were also tested with various alignment approaches in various configurations. The metrics obtained on the test set with each of these approaches/models can be seen in Table 2. The official metric used was Macro F1 score [F1]. Additionally accuracy [A], precision [P] and recall [R] were also reported."}, {"title": "Multi-stage Learning", "content": "Given the small size of the existing training dataset (312 samples), we have additionally tested multi-stage learning by first fine-tuning over a subset of 20000 rows from the SNLI dataset to first let the models adapt to generic NLI tasks with a lower learning rate and then further fine-tuned the resultant models on the NLLP training samples with a higher learning rate. Additionally we have tested using additional training data from previous works (more in Appendix). Both of these approaches did result in better performance. An overview of the process can be seen in Figure 1."}, {"title": "Alignment approaches used", "content": "We have tested using ORPO (Hong et al., 2024) during fine-tuning using various LLMs in 3 different configurations i.e the rejected sample being a) random, b) preferred and c) multiple rejected samples. The usage of ORPO did improve the performance over all of the domains in any of the configurations."}, {"title": "Random Rejection", "content": "In this approach, the actual label being the accepted response would lead to the rejected response being a random class form the remaining two. The results did improve compared to not using ORPO but by a very slight margin."}, {"title": "Preferred Rejection", "content": "In cases where the actual label is Neutral, a random label is chosen as the rejected sample among the other two. We chose 'Neutral' as the rejected response when the actual label is either Entailed or Contradict. The reason being all of the errors being one of the other two classes being labelled as 'Neutral or vice versa. This did improve the performance significantly by reducing the mis-classified samples between Neutral and the other classes."}, {"title": "Multiple Rejections", "content": "In this approach, while the label class would be the accepted class, both the other two classes were added as the rejected samples. Although this was computationally expensive, the results were close to those from preferred rejection approach."}, {"title": "Error Analysis", "content": "We were able to completely avoid Type-1 errors i.e classification of 'Entailed' as 'Contradict' and vice versa, limiting the error cases to Type-2 errors i.e classification of 'Neutral' as another and vice versa. Confusion matrix of our models' predictions on the test set can be seen in Figure 2 and Figure 3. It can be observed from both Figure 2 and Figure 3 that most common case of errors was those being mis-classified among Neutral and Entailed. We found these to be cases where the hypothesis consisted of multiple sentences which entail the premise followed by a vague / unrelated statement, while some are to be labelled as 'Entailed' and rest as 'Neutral' based on the perceived tone/feeling of the user, it would be likely that there might not be consensus among human annotators as well in many such cases. It is worth looking into the performance of models trained on not just the labels, but also the reasoning of the annotators on why a certain label was chosen, as it might help the model learn better."}, {"title": "Performance on each Domain", "content": "The performance of our system on each domain in the test set can be seen in Table 3. The metrics obtained on most of the domains were significantly higher than that of the baseline. The system worked well on all domains, however comparatively weaker on BIPA which was imbalanced in the training set."}, {"title": "Scope For Improvement", "content": "As seen in Table 3 the performance across each domain varied by a significant margin. However, the domains over which some models under-performed, some other performed well. It is likely that using ensembles can improve the performance by a considerable margin."}, {"title": "Low training data", "content": "Some cases did get misclassified too often especially those whose domain data was less represented in the training dataset. From what was observed from comparison of performance over original and aggregated datasets and the models with and without SNLI fine-tuning step involved, It can be determined that more training data would improve the performance considerably especially the domains with less data."}, {"title": "Individual Annotations availability", "content": "In models built using Preferred Rejection, cases with Neutral as the label had used a random label from the other two as the rejected sample. However availability of individual annotations might provide more info on what choice of rejected label might lead to better results compared to choosing a rejected label at random."}, {"title": "Conclusion", "content": "Compared to the well known SNLI dataset which consist of premise and hypothesis pair which are usually one or two sentences long, the current dataset has texts (both premise and hypothesis) which are roughly four times longer leading to more complexity. Since, the SNLI dataset has a 98% consensus and 58% unanimous annotation among 5 annotators, it can be expected that a human annotation on the current dataset can lead to even less proportion of texts where a consensus or unanimous vote can be reached. Yet, our models were able to provide a reliable performance completely avoiding Type-1 errors, performing better than human annotations expected from those with domain knowledge, hinting at a potential of practical applicability."}, {"title": "Limitations", "content": "Due to computational resource limitations, the base models of LLMs were initially loaded in 4-bit precision, It is likely that a larger model used in full-precision might perform better. Since the test dataset used in the task is relatively small, the LLMs/approaches that might perform better in practical scenarios may vary from those found to be better on the current dataset."}, {"title": "Ethics Statement", "content": "Automating the identification of legal violations may inadvertently generate false positives or negatives, potentially impacting individual rights and the integrity of the legal system. Therefore, we emphasize that our models are intended to complement, not replace, legal professionals. It is critical that any use of our models is approached with caution, recognizing the inherent limitations and biases that automated systems may present."}, {"title": "Training Data Aggregation", "content": "Due to training dataset provided being not large enough, we have used additional training data which include the dataset from the LegalLens paper. The aggregated training dataset used is what was obtained by merging both the datasets, upon removal of duplicates.\n\u2022 Current Dataset huggingface.co/datasets/darrow-ai/LegalLensNLI-SharedTask : 312 training samples\n\u2022 Additional Dataset huggingface.co/datasets/darrow-ai/LegalLensNLI : 312 training samples\n\u2022 Aggregated Dataset huggingface.co/datasets/1-800-SHARED-TASKS/EMNLP-2024-NLLP : 462 training samples"}, {"title": "System Replication", "content": "We have used each of the LLMs tested by loading them in 4bit precision before fine-tuning on each dataset in both the training stages using LoRA. The hyper parameters used in each of the training stages can be seen in Table 4. The hyper parameters not specified below were used with their default values in both stages. The code used can be found here: github.com/1-800-SHARED-TASKS/EMNLP-2024-NLLP."}, {"title": "Models used / SNLI version of LLMs", "content": "The models used in the paper including the best performing model and the one used in the official submission can be found here :\n\u2022 Best performing model : huggingface.co/1-800-SHARED-TASKS/EMNLP-NLLP-NLI-GEMMA2-27B-withSNLI-withORPO\n\u2022 Model used for submission : huggingface.co/1-800-SHARED-TASKS/EMNLP-NLLP-NLI-PHI3-medium-withoutSNLI-withORPO\nAdditionally the models obtained after fine-tuning LLMs used on the SNLI dataset can be found here :\n\u2022 GEMMA NLI : huggingface.co/1-800-SHARED-TASKS/GEMMA2-27B-NLI-16bit\n\u2022 PHI3 NLI : huggingface.co/1-800-SHARED-TASKS/PHI3-Medium-NLI-16bit"}, {"title": "Performance of both models : domain wise", "content": "The performance of our best performing model (GEMMA-2-27B-SNLI) can be seen below followed by those from our submission model (PHI-3-SNLI)."}]}