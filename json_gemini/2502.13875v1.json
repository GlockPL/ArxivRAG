{"title": "MEX: Memory-efficient Approach to Referring Multi-Object Tracking", "authors": ["Huu-Thien Tran", "Thai-Son Tran", "Phuoc-Sang Pham", "Khoa Luu"], "abstract": "Referring Multi-Object Tracking (RMOT) is a relatively new concept that has rapidly gained traction as a promising research direction at the intersection of computer vision and natural language processing. Unlike traditional multi-object tracking, RMOT identifies and tracks objects and incorporates textual descriptions for object class names, making the approach more intuitive. Various techniques have been proposed to address this challenging problem; however, most require the training of the entire network due to their end-to-end nature. Among these methods, iKUN has emerged as a particularly promising solution. Therefore, we further explore its pipeline and enhance its performance. In this paper, we introduce a practical module dubbed Memory-Efficient Cross-modality \u2013 MEX. This memory-efficient technique can be directly applied to off-the-shelf trackers like iKUN, resulting in significant architectural improvements. Our method proves effective during inference on a single GPU with 4 GB of memory. Among the various benchmarks, the Refer-KITTI dataset, which offers diverse autonomous driving scenes with relevant language expressions, is particularly useful for studying this problem. Empirically, our method demonstrates effectiveness and efficiency regarding HOTA tracking scores, substantially improving memory allocation and processing speed.", "sections": [{"title": "I. INTRODUCTION", "content": "The traditional multi-object tracking (MOT) task focuses on tracking specific classes of objects in each video frame, playing a crucial role in video understanding. Despite substantial developments and breakthroughs in this field, the inherent specificity of class names limits their flexibility and generalization. To address this issue, an emerging task named referring multi-object tracking (RMOT) integrates the multi-object tracker with additional language expressions as semantic cues, frame-by-frame. This procedure is more general and flexible as predictions per frame can be arbitrary, targeting only the referred objects. For example, with the language description \"moving cars in black,\" the tracker will predict trajectories corresponding only to the specified prompt, ignoring parked cars or cars of other colors. While this method enhances flexibility, it adds complexity to the tracking pipeline, which already involves detection, association, and the additional referring task.\nThere are numerous approaches to solving this problem. TransRMOT [1], an extension of MOTR [2], incorporates a linguistic module. MENDER [3] presents a unified network that addresses three distinct components within the cross-modality module, including the prompt as a semantic cue and visual data sources from the frame and corresponding trajectories over time. These networks are all trained end-to-end and have made significant contributions to the field. However, they face limitations when applied to datasets with different distributions, necessitating a complete retraining of the network. Recently, iKUN [4] has approached this task more effectively. It decouples the problem into two sub-tasks, termed tracking-to-referring. It allows the tracker to remain frozen during training, while the referring module can be used plug-and-play with any tracker.\nWith that motivation, our method focuses on improving effectiveness during training and inference processes. Although the iKUN pipeline aligns well with the problem's purpose, there is still room for improvements in memory efficiency. To address this, we incorporate an elegant referring module using our designated MEX mechanism further to enhance the overall performance of the tracking pipeline. We conduct extensive experiments on the recently released Refer-KITTI dataset, and our method achieves compelling results compared"}, {"title": "II. RELATED WORK", "content": "Multi-Object Tracking. Prevailing approaches to multi-object tracking problems can be classified into two main paradigms: tracking-by-detection and tracking-by-attention. Tracking-by-detection involves two consecutive stages: using a detector to predict the bounding boxes of objects and their corresponding features, followed by an association step to match these instances across frames. SORT [8] employs the Kalman filter for motion modeling and associates tracking instances based on the intersection-over-union (IoU) of bounding boxes. DeepSORT [9] enhances this method by integrating a deep learning-based module to extract the appearance features of objects. More recently, ByteTrack [10] treats detection boxes as bytes and associates all of them, significantly boosting the tracker's performance. Advanced techniques in BoT-SORT [11], and OC-SORT [12] further refine the association and post-processing steps, achieving even better performance. On the other hand, the advent of the Transformer architecture [13] has driven considerable advancements in the tracking-by-attention paradigm in recent years. TrackFormer [14] introduces tracking instances as queries during the detection step. MOTR [2] expands on this by incorporating a query interaction module, and MOTRv2 [15] further improves its performance by initializing queries using detection results from YOLOX [16]. Furthermore, by integrating historical features across frames as memory, MeMOT [17], and MeMOTR [18] noticeably improve the performance of attention-based trackers.\nReferring Multi-Object Tracking. Referring multi-object tracking is an emerging challenge, whereas referring single-object tracking, or segmentation, has been studied for years and has achieved good performance results [19]-[23]. However, due to some intrinsic limitations, referring multi-object tracking cannot be directly expanded from these approaches. Precedent methods employ various techniques to tackle this additional textual referring expression. MENDER [3] utilizes the cross-modality fusion module to handle three distinct inputs, including features from the video frame, tracking instances, and textual captions. To further enhance performance, they further implement third-order tensor decomposition. TransRMOT [1] places the cross-attention module at the beginning of the pipeline, aggregating the textual features into the detection step. Despite the flexibility gained, training these end-to-end tracking networks entirely for diverse video distributions incurs considerable computational costs due to the demanding training process. Recently, iKUN [4] designed a plug-and-play pipeline for referring multi-object tracking problems by decoupling the task into two sub-tasks: tracking and referring. It allows the tracker network to remain frozen during training, offering flexibility in utilizing different off-the-shelf trackers. Inspired by this approach, we explore this paradigm further and facilitate the method with our memory-efficient technique during the referring task, enhancing the performance of the pipeline."}, {"title": "III. METHODOLOGY", "content": "In this section, we first discussed the preliminary related to our problem, including the detailed formulation of the problem and the materials coming from [4] as the baseline method we want to improve in III-A. Then, we introduce our fusion block using scaled dot-product cross-modality attention in III-B."}, {"title": "A. Preliminary", "content": "As described in [4], the input of RMOT consists of a sequence $I = \\{I_t\\}_{t=1}^K$ with $K$ frames and $P = \\{P_e\\}_{e=1}^L$ as a language expression with $L$ tokens. Given a tracker $F_{tr}(\\cdot)$, all predicted trajectories are $T = F_{tr}(I)$, where $T = \\{T_1, T_2,\\ldots, T_N\\}$ includes $N$ tracking instances. This tracking module is followed by a referring module $F_{ref}(\\cdot)$, applied to mark candidate instances $T$ by the textual prompt with scoring criterion as $S = F_{ref}(I,T,P)$. In the iKUN's pipeline, the similarity calibration module further refined this referring score, which can enhance the overall performance. In that case, we also integrate this module into our pipeline and run experiments to observe the results. Finally, candidates $T$ are filtered by the refined scores $S'$ and the tracking outputs $T'$, with $M$ created trajectories ($M \\leq N$).\nThe empirical effectiveness of utilizing three components, $I$, $T$, and $P$, stems from their enhanced spatial-temporal relationship between frames, trajectories, and linguistic expressions. Designing a suitable architecture for these inputs is particularly challenging due to the inherent complexity of the RMOT task. Specifically, each prompt can address multiple objects in a frame, while multiple prompts can target a single object. Existing methods [3], [4] also consider these three factors, with advanced techniques to deal with them. However, they are intrinsically intricate and challenging to replicate on resource-constrained devices such as personal computers. Further details are discussed in III-B.\nTo elaborate on the techniques outlined in the original iKUN paper, we present a detailed exploration of the similarity calibration module. This module alleviates non-uniform and open test sets encountered in RMOT scenarios. Initially, (1) defines the normalized similarity between language descriptions during test-time, denoted as $W_{ij}$."}, {"title": "B. Fusion Block", "content": "We propose a novel fusion block that can tackle the inputs from the three sources above of data. To effectively resolve the interdependencies among these three components, we introduce a method called Memory-Efficient Cross-Modality Attention \u2013 MEXAttn. Unlike the traditional scale dot-product attention described in [13], which typically handles pairwise relationships between two sets of criteria, our technique can simultaneously accommodate three types of components. This architectural enhancement enables the transformer architecture to learn correlations more effectively. Our methodology is illustrated in Fig. 3, alongside a depiction of the cascade attention method introduced in [4], which outperforms other designs in their knowledge unification module.\nTo formulate the MEX attention mechanism, we first need to look at the original scale dot-product attention algorithm described in (2).\n$Attn(Q, K, V) = \\sigma (\\frac{Q K^T}{\\sqrt{d_k}})V$ where $Q$, $K$, and $V$ represent the query, key, and value vector, respectively ($K^T$ denotes the transpose of $K$); $\\sqrt{d_k}$ serves as a scaling factor, where $d_k$ is the dimension of the features; and $\\sigma(\\cdot)$ denotes the softmax function. Notably, $Q$ interacts exclusively with one data source, whereas $K$ and $V$ interact with the other. We extend this concept by inserting an"}, {"title": "IV. EXPERIMENTS", "content": "Refer-KITTI [1] is a publicly available dataset designed for referring multi-object tracking, an extension of the KITTI [24]. Refer-KITTI consists of 18 high-resolution, lengthy videos containing 818 expressions, each corresponding to an average of 10.7 objects. This dataset covers various scenes, such as pedestrians, public roads, and highways, and assigns a unique identification number to each instance. We adhere to the official split protocols, dividing these videos into 15 for training and 3 for testing. The training set includes 80 distinct language descriptions, while the testing set includes 63."}, {"title": "B. Evaluation Metrics", "content": "We utilize Higher Order Tracking Accuracy (HOTA) [7] as the primary metric to evaluate the performance of our technique. Also, for further analysis, we include MOTA [25] and IDF1 [26] as supplementary metrics."}, {"title": "C. Implementation Details", "content": "The input data is categorized into three types: bounding boxes of tracked objects, video frames, and textual captions. Frames are cropped into images of objects (local images) through dataset bounding boxes. These cropped patches are then square-padded and resized into the shape of 224x224. The video frames are also square padded and resized to the shape of 672x672 (global images), and textual features are standardized using iKUN's [4] vocabularies before being tokenized with the CLIP tokenizer [6]."}, {"title": "D. Evaluation Results", "content": "Our method was evaluated against the state-of-the-art on Refer-KITTI using NeuralSORT [4] as the base tracker and YOLOv8 [28] as the detector. It is important to note that the tracking results are provided beforehand. Our approach, MEX, achieved 45.07%, 32.81%, and 62.52% for HOTA, DetA, and AssA, respectively, surpassing the baseline method's results of 44.56%, 32.05%, and 62.49%. This improvement demonstrates the efficacy of our approach while leveraging the advantages of iKUN [4] with single-training efficiency.\nWe also compare the number of trainable parameters counted in Table II. Our referring module, MEX, has 81 million trainable parameters, less than iKUN [4] with 92 million. With 10 million fewer trainable parameters, we can minimize memory usage during training. As a result, during training, our process memory utilization is significantly lower than iKUN's, approximately 47.20% with system memory"}, {"title": "E. Limitations", "content": "Leveraging the MeMOTR-BDD100K [18] tracker, our model demonstrates solid object identification capabilities but struggles with generalizing to complex objects. For instance, when querying \u201cpeople riding on bicycle\", as depicted in Fig. 6(a), the tracker identifies two separate objects, the person and the bicycle, rather than recognizing them as a single entity. Consequently, our model evaluates these as distinct objects and fails to match the query accurately.\nMotion query. Due to a lack of diverse motion description datasets for tracking, our model faces limitations in filtering objects based on motion queries. For example, when queried with \"people walking the street\", as witnessed in Fig. 6(b), the model exhibits confusion between detecting walking or standing postures."}, {"title": "V. CONCLUSION", "content": "In this paper, we have introduced a novel approach to the referring module in the tracking-then-referring pipeline, solving the referring multi-object tracking task. Our method, the Memory-Efficient Cross-Modality (MEX) module, accommodates three types of necessary data sources for the proposed task, effectively addressing the cumbersome fusion steps between different modalities. This module helps the network work fine on the Refer-KITTI benchmark compared to the previous method, iKUN. Also, with the utilization of a more robust encoder, our designated pipeline works more efficiently, lessening memory usage and inference speed."}]}