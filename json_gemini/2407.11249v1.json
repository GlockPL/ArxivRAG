{"title": "Disentangling Representations in RNNs through Multi-task Learning", "authors": ["Pantelis Vafidis", "Aman Bhargava", "Antonio Rangel"], "abstract": "Abstract, or disentangled, representations are a promising mathematical framework for efficient and effective generalization in both biological and artificial systems. We investigate abstract representations in the context of multi-task classification over noisy evidence streams a canonical decision-making neuroscience paradigm. We derive theoretical bounds that guarantee the emergence of disentangled representations in the latent state of any optimal multi-task classifier, when the number of tasks exceeds the dimensionality of the state space. We experimentally confirm that RNNs trained on multi-task classification learn disentangled representations in the form of continuous attractors, leading to zero-shot out-of-distribution (OOD) generalization. We demonstrate the flexibility of the abstract RNN representations across various decision boundary geometries and in tasks requiring classification confidence estimation. Our framework suggests a general principle for the formation of cognitive maps that organize knowledge to enable flexible generalization in biological and artificial systems alike, and closely relates to representations found in humans and animals during decision-making and spatial reasoning tasks.", "sections": [{"title": "1 Introduction", "content": "Humans and animals can generalize to new settings effortlessly, leveraging a combination of past experiences and world models [43, 44]. Modern foundation models also display emergent out-of-distribution (OOD) generalization abilities, in the form of zero- or few-shot learning [10, 59, 36, 57]. For generalization, it is crucial to have appropriate representations which extrapolate or interpolate previous experiences [67, 35, 38], or the use of task compositionality as an inductive bias [76, 62].\nOne mechanism for generalization is through abstract, or disentangled, representations [29, 39, 38]. A property of abstract representations is that they preserve the latent structure present in the world in their geometry, i.e. they are low-dimensional, rectangular, and compositional, each axis corresponding to a factor of variation in the data. Then a linear decoder (i.e. downstream neuron) trained to discriminate between two categories can readily generalize to stimuli not observed in training, due to the structure of the representation. From a neuroscience perspective, this corresponds to decomposing a novel stimulus into its familiar attributes, and reasoning based on those. For instance, imagine you are at a grocery store, deciding whether a fruit is ripe or not. If the brain's internal representation of food attributes (ripeness, caloric content, etc.) is disentangled, then learning to perform this task for bananas would lead to zero-shot generalization to other fruit (e.g. mangos, Figure 1a).\nThis raises the question of under which conditions do such representations emerge in biological and artificial agents alike. Several brain areas including the amygdala, prefrontal cortex and hippocampus have been found to encode variables of interest in an abstract format [65, 5, 8, 55, 16]. Previous work showed that feedforward neural networks develop abstract representations when trained to multitask [38]. However, real-world decisions typically rely on imperfect, noisy information, evolving dynamically over time [9, 41]. To account for this important feature of the world, we train RNNs to multitask canonical neuroscience tasks involving the accumulation of evidence over noisy streams.\nContributions. The main contributions of this paper are the following:\n\u2022 We prove that any optimal multi-task classifier is guaranteed to learn a disentangled rep-resentation of the ground truth contained in the noisy measurements in its latent state, if the classification boundary normal vectors span the input space (Appendix B). Intriguingly, noise in the observations is necessary to guarantee the latent state would compute an optimal, disentangled representation of the ground truth.\n\u2022 Through experiments, we confirm that RNNs trained to multitask develop abstract rep-resentations that zero-shot generalize OOD, when the number of tasks exceeds the input dimensionality. The computational substrate of these representations is a 2D continuous attractor [2] storing a ground truth estimate in a product space of the latent factors.\n\u2022 We demonstrate that our setting is robust to a number of manipulations, including interleaved learning of linear and non-linear tasks and free reaction time decisions.\n\u2022 Finally, we discuss the relation between our results and important neuroscientific find-ings, their implications for generalizable representation learning in artificial systems, and demonstrate the strong advantage of multitasking over previously proposed mechanisms of representation learning in the brain [48, 76].\nDespite being framed in the context of neuroscience, our results are general; they apply to any system aggregating noisy evidence over time."}, {"title": "1.1 Related work", "content": "Disentanglement has long been recognized as a promising strategy for generalization [4] (although note [46, 54] for a contrarian view), yet most modern work focuses on feedforward architectures [29, 39, 74]. In terms of autoregressive models, Hsu et al. [33], Li and Mandt [45] showed that variational LSTMs disentagle representations of underlying factors in sequential data allowing style transfer; however to the best of our knowledge the underlying representational geometry has not been characterised. Other work focuses on fitting RNNs to behavioral data while enforcing disentanglement for interpretability [18, 51]. Work on context-dependent decision making has shown that RNNs re-purpose learned representations in a compositional manner when trained in related tasks [76, 20]; however, the abstractness of the resulting representations was not established. Finally John et al. [37] show that multitasking results in disentanglement, however they directly enforce latent factor separation through their adversarial optimization objectives. Our approach is most closely related to weakly supervised disentanglement, without comparing across samples [66].\nPrevious work showed that multitasking feedforward networks learn abstract representations, as quantified by regression generalization [38]. We expand upon these findings in several ways. First, we extend the framework of Johnston and Fusi [38] to RNNs that can update their representations as further information arrives. Second, we prove theorems on optimal multi-task classifiers that guarantee the emergence of disentangled representations in any optimal multitask classifier if the number of tasks exceeds the input dimensionality D. Third, we rigorously analyze the role of noise in forming disentangled representations, extending the noise-free regime studied in [38]. Finally, we explore a range of values for D, providing experimental validation of our theory. Whereas related work proved that disentangled representations emerge in feedforward architectures from multitask learning in sparse tasks when a sparsity regularization constraint is placed on the predictors [42], we place no such constraints and still uncover disentangled representations."}, {"title": "2 Model and setup", "content": "We consider canonical cognitive neuroscience tasks that involve evidence aggregation over time, mirroring decision-making under uncertainty. The input consists of D noisy evidence streams X(t) \u2208 \\mathbb{R}^D where X(t) = x^* + \\sigma \\mathcal{N}(0, I_D) and x^* \u2208 \\mathbb{R}^D is the ground truth evidence in a certain trial (x^* \\sim \\text{Uniform}(-0.5, 0.5)) and \\sigma is the input noise standard deviation (Figure 1c, left). Each element x_i^* of x^* corresponds to different options under consideration or different attributes of the same item. The target output y(x^*) \u2208 \\{-1,+1\\}^{\\text{Ntask}} is a vector of \\text{Ntask} + 1s and -1s, depending on whether x^* is above or below each of the classification boundaries (Figure 1b). To ensure the reproducibility of our results, we incorporate our tasks within the NeuroGym framework [53].\nThe decision maker should integrate noisy samples X(t) over time, viewed through an injective observation map (encoder) f, to estimate Y_i(t) = \\text{Pr}\\{y_i(x^*) = 1|f(X(1)), ..., f(X(t))\\} (Figure S2). The classification lines reflect criteria based on which decisions will be made. Imagine for example that x_1 corresponds to food and x_2 to water reward. Depending on the agent's internal state, one could take precedence over the other, and the degree of preference is reflected in the slope of the line. In Appendix B we prove that any optimal multi-task classifier will learn disentangled representations of x^* in its latent state Z(t), when the \\text{Ntask} classification boundaries span the latent space."}, {"title": "2.1 Problem formulation", "content": "We train leaky RNNs which represent a brain area responsible for decision-making to multitask the aforementioned tasks. The networks contain Nneu = 64 neurons, and their activations z(t) obey:\n\\tau \\frac{dz(t)}{dt} = -z(t) + [W_{\\text{rec}} z(t) + W_{\\text{in}} x_{\\text{in}}(t) + b]_+\t\t(1)\nwhere Wrec is the recurrent weight matrix, Win is the matrix carrying the input vector xin, bis a unit-specific bias vector, \\tau is the neuronal time constant and [\\cdot]_+ is the ReLU applied element-wise to a vector. We simulate Equation (1) using the forward Euler method for T = 20 timesteps of duration \\Delta t = \\tau = 100 ms, which we find to be stable. The RNN's output \\hat{y}(t) of size \\text{Ntask} is given by:\n\\hat{y}(t) = g(W_{\\text{out}} Z(t))\t\t(2)\nwhere Wout is a readout matrix and g the output activation function applied elementwise. We choose g(z) = \\text{tanh}(z) for classification, g(z) = 5 \\text{tanh}(z) for integration-to-bound, and g(z) = z for other tasks. The inputs described above are first passed through a randomly initialized encoder f, which represents a static mapping from latents to observations (e.g. a perceptual system). The encoder is a 3-layer MLP with hidden dimensions 100, 100, 40 and ReLU non-linearities, is unique to each network and kept fixed during training. An additional fixation input which is 1 during the trial and turns 0 when the network should report its decisions is directly passed to the hidden layer (Figure 1c). The RNN is trained to produce the target outputs. By minimizing loss across trials, the network is incentivized to estimate \\text{Pr}\\{y_i(x^*) = +1\\}. We use mean-squared-error loss and train all parameters (i.e. Wrec, Win, Wout and b) with Adam default settings except lr = 10^{-3} [40]. Table S1 summarizes all hyperparameters and their values. Network training takes ~ 10 minutes on a commercial laptop CPU."}, {"title": "2.2 Architecture", "content": "We seek to understand the properties of optimal multi-task classifiers with random variable latent state Z(t) in the paradigm illustrated in Figure S2 and described in Section 2.1. Specifically, we are interested in classifiers that, at each time point t, estimate multiple linear classifications y(x^*) = [y_1(x^*), ..., y_{\\text{Ntask}}(x^*)] \u2208 \\{0,1\\}^{\\text{Ntask}} based on noisy, potentially non-linearly mapped observations f(X(1)), ..., f(X(t)) with maximal likelihood. The classifier output at time t, denoted \\hat{Y}(t) \u2208 [0, 1]^{\\text{Ntask}}, represents a set of Bernoulli random variable parameters, where each element \\hat{Y}_i(t) corresponds to the probability \\text{Pr}\\{y_i(x^*) = 1|f(X(1)), ..., f(X(t))\\}."}, {"title": "3 Theoretical Results", "content": "Each of the \\text{Ntask} decision boundaries can be expressed as a pair of boundary normal vector c_i \u2208 \\mathbb{R}^D and offset b_i \u2208 \\mathbb{R} (Equation 8). Let C\u2208 \\mathbb{R}^{\\text{Ntask}\u00d7D} represent the matrix of decision boundaries and b\u2208 \\mathbb{R}^{\\text{Ntask}} be the vector of decision boundary offsets.\nTheorem 3.1 (Disentangled Representation Theorem). If C \u2208 \\mathbb{R}^{\\text{Ntask} \u00d7D} is a full-rank matrix and \\text{Ntask} > D, then\n1. any optimal estimator of y(x^*) must encode an estimate \\mu(t) of the ground truth evidence variable x^* in its latent state Z(t), and\n2. if the activation function g = \\text{tanh}, \\mu(t) will be linearly decodable from Z(t), thus implying that Z(t) contains a linear disentangled representation of \\mu(t) [30, 38].\nSpecifically, \\mu(t) is the maximum likelihood estimate of x^* given observations f (X(1)), ..., f (X(t)). Equations 3, 4 are closed-form expressions for extracting \\mu(t) from Z(t) with a generic activation function g and g = tanh respectively.\n\\mu(t) = (C^T C)^{-1}C^T (\\Phi^{-1}(g(Z(t))) + b)\t\t(3)\n\\mu(t) \\approx \\frac{2\\sqrt{3}\\sigma}{\\pi \\sqrt{\\tau t}}(C^T C)^{-1}C^TZ(t) + (C^T C)^{-1}C^Tb\t\t(4)\nWhere \\Phi is the CDF of the normal distribution, \\sigma is the noise magnitude and t the trial duration.\nProof. Point 1 and Equation 3 are proven in Appendix B in Theorem B.6. Point 2 and Equation 4 are proven in Corollary B.7.\nTheorem 3.1 holds for any system that performs optimal multi-task classification based on noisy, injectively transformed observations f(X(t)) (e.g., RNNs, biological neural circuits, Bayesian filters, etc.). Our results are readily extensible to sub-optimal multi-task classification models under the assumption that classification error w.r.t. optimal estimation is randomly distributed with zero mean (see Appendix B). In this case, the closed-form solutions in Equations 3, 4 correspond to a least-squared error estimate of \\mu(t) given sub-optimal Z(t). Surprisingly, our theoretical results indicate Z(t) will contain a disentangled representation of x^* when the readout map g(Z(t)) := \\hat{Y}(t) is a tanh function (Equation 4, Corollary B.7). Our theoretical framework readily generates testable predictions for our computational experiments, including theoretical r\u00b2 values for estimating x^* from Z(t) (Appendix A.3, Figure S9a), and decodability of x^* from Z(t) as a function of latent dimension D (Figure 3a), number of tasks \\text{Ntask} (Figure 1f,g, 2c), noise level \\sigma (Figure 3b), evidence aggregation time t (Lemma B.3), and latent factor correlation (Figure S6)."}, {"title": "4 Experiments", "content": "We train RNNs to do simultaneous classifications for 24 linear partitions of the latent space for D = 2 (Figure 1b, 6 partitions shown). Figure 1d shows the top 3 PCs (capturing ~ 85% of the variance) of network activity after training (final accuracies ~ 95%) for multiple trials, along with the fixed points of network dynamics. To find the fixed points, we follow a standard procedure outlined in Sussillo and Barak [69] (see Appendix A.1 for details). Looking at Figure 1d the fixed points span the entire two-dimensional manifold that the trials evolve in, which corresponds to a continuous attractor with stable states across a 2D \"sheet\". This means that the network can store a short-term memory [73] of the current amount of accumulated evidence in a product space of the latent variables.\nFurthermore, the representation is low-dimensional, rectangular and compositional; all characteristics of disentangled representations. In comparison, the representations after the encoder are non-linearly mixed, high-dimensional and overlapping (Figure S5a). Individual trials with noise show how the representation maintains a sense of metric distances in the RNN representation space (Figure S5b). Figure S5c demonstrates how this representation comes about during learning, and Figure S5d that the short-term memory persists when a delay period is included before the decision. Therefore, it seems that multitasking has led to disentagled, persistent representations of the latent variables. Importantly, and in line with our theory, this only happens when noise is present in the input, which forces the network to learn a notion of distance from classification boundaries. Indeed, when the network is trained without input noise, it does not learn a 2D continuous attractor (Figure S5e).\nTo quantify the disentanglement of the representations, we evaluate regression generalization by training a linear decoder to predict the ground truth x^* while network weights are frozen (Figure le). We perform out-of-distribution 4-fold crossvalidation, i.e. train the decoder on 3 out of 4 quadrants and test in the remaining quadrant (Appendix A.2 for details). We also evaluate in-distribution (ID) performance by training the decoder in all quadrants. An example of train and test losses is shown in Figure S5f. We find that the network's OOD and ID generalization performance are excellent (median r\u00b2 = 0.96, 0.97 respectively across 5 example networks, also see Figure 1f); therefore the network has indeed learned a disentangled representation that zero-shot generalizes OOD. In addition, ID performance increases with the number of tasks \\text{Ntask}, and the OOD generalization gap decreases (Figure 1f). Therefore we conclude that simultaneously training an RNN in classification tasks that span the latent space results in disentangled representations in the recurrent hidden layer.\nSince x^* can be decoded by this representation in unseen (by the decoder) parts of the state space, it follows that the representation can be used to solve any task involving the same underlying variables, without requiring further pretraining. In other words, to solve any other task we do not need to deal with the denoising and unmixing of the latent factors x1, x2; we would just need to learn the (non-linear) mapping from x1, x2 to task output. Crucially, these findings are architecture-agnostic: they hold for non-leaky (\"vanilla\") RNNs, which outperform leaky ones, and LSTMs which drastically reduce performance variability (Figure 1g). We still choose leaky RNNs because of their closer correspondence to biological neurons, which have a membrane voltage that decays over time."}, {"title": "4.1 Multitasking leads to disentangled representations", "content": "We were also curious to see the impact of correlated inputs. A problem with high correlations is that they render parts of the state space virtually invisible to the network (Figure S6a). Surprisingly, OOD generalization performance is very weakly affected by input correlations, even though the state space is sampled uniformly in test (Figure S6b). The behavior is highly non-linear: performance is great until p = 0.97, but when the inputs are perfectly correlated (p = 1), performance drop is sharp.\nFinally, we examined RNN unit activations for these representations, and their relation to the latent variables. In Figure S7a we plot the steady-state firing rates for all 64 neurons, while regularly sampling x1 and x2. Interestingly, we find that in these networks only ~ 10% of neurons are active at any time, which is in line with sparse coding in the brain. In addition, the average firing rate is consistently ~ 0.25 spikes/s, which is surprisingly close to cortical values. Furthermore, we find that all of the active neurons display mixed selectivity, i.e. they are tuned to both variables, which is a known property of cortical neurons [61] (Figure S7b). This suggests that metrics of disentanglement that assume that individual neurons encode distinct factors of variation [29, 39, 13, 32, 21] might be insufficient in detecting disentanglement in networks that generalize well. While recent work incorporates such axis-alignment in the definition of disentaglement, our work along with others [38] showcases the advantages of approaching disentanglement from a mixed representations perspective. Importantly, these properties were not imposed during training, nor was there any parameter fine tuning involved; rather they emerged from task structure and optimization objective."}, {"title": "4.2 Nonlinear tasks and interleaved learning", "content": "So far we have been training RNNs to perform linear classifications. However there are cases where the streams might need to be combined nonlinearly. For instance, if the two streams represent the amount and probability of reward respectively, an agent needs to multiply the two and decide whether the expected value exceeds a certain (metabolic) cost \\gamma of performing an action to obtain said reward. Figure S8a shows the classification lines for the multiplicative task, where the network should decide whether the ground truth x^* lies above or below the curve x_1 x_2 = \\gamma, for multiple values of \\gamma. RNNs trained on this task learn a representation that separates for trials in different quadrants (Figure S8b). This heuristic might have come about because individual classification lines span only a single quadrant for this task. Furthermore, trials belonging to the same quadrant seem to be more convoluted, with a corresponding drop in OOD performance (median r\u00b2 = 0.71), which might reflect the non-linearity of the task. Therefore, we wondered how the representation would look like if the network was trained on both the linear and multiplicative tasks, as animals need to do.\nFor that we perform interleaved training of both tasks (i.e. train in batches sampled from one of the tasks at a time), a setting where neural networks excel at, compared to humans who excel at blocked training, where tasks are learned sequentially (but see Flesch et al. [24]). Figure S8c shows that the network learns a two-dimensional continuous attractor, and the learned representation is no longer separated by quadrants. OOD generalization for this network is excellent, and almost identical to ID performance (median r\u00b2 = 0.94, 0.97 respectively). Overall, we conclude that our framework extends to interleaved learning of a mixture of linear and nonlinear tasks, which better reflects the challenges encountered by agents in the real world. Note that during interleaved training, linear and non-linear tasks are not performed simultaneously; yet they are in immediate succession which also places pressure to the network to gradually learn representations that satisfy all tasks. The relationship between interleaved learning and multitasking could be an interesting direction for future work."}, {"title": "4.3 Abstract representations are learned for a free reaction time, integrate to bound task", "content": "So far we have been training networks to produce a response at the end of the trial. However, in many situations agents are free to make a decision whenever they are certain enough. Therefore, we here seek to extend our framework to free reaction time (RT) decisions. A canonical model accounting for choices and reaction times in humans and animals is the drift-diffusion model [41, 11]. It is composed of an accumulator that integrates noisy evidence over time, until a certain amount of certainty, represented by a bound, is reached, triggering a decision. In the linear classification task setting, the accumulated amount of evidence at time t for a line with slope a, A_a(t) is given by:\nA_a(t) = A_a(t - 1) + X_1(t) - a X_2(t)\t\t(5)"}, {"title": "4.4 RNNs confirm and extend theoretical predictions", "content": "Here we expore the relation between the theory in Section 3 and Appendix B and experiments in Section 4 in more depth. First, we wondered why performance saturates in our networks to a high yet non-1 r2. The central limit theorem predicts that the estimate of the ground truth x^* in any optimal multi-task classifier becomes more accurate with \\sqrt{t}, providing a theoretical maximum r\u00b2 given trial duration T (Appendix A.3). Since the networks trained on the free RT task are required to output their confidence at any time in the trial, we can compute OOD r\u00b2 of network predictions at any timepoint t, and compare that to the theoretical prediction. Figure S9a shows that indeed the highest RNN r\u00b2 falls in the vicinity of or just short of the theoretical maximum. This indicates that RNNs trained with BPTT on these tasks behave like near-optimal multi-task classifiers that create increasingly accurate predictions with time, tightening the relation between our theoretical and experimental results.\nAn important prediction of our theory is that to learn abstract representations \\text{Ntask} should exceed D. To test this, we increase D (adding more inputs to Figure 1c), and vary \\text{Ntask}. Sampling classification hyperplanes methodically in high-dimensional spaces is non-trivial; therefore we resort to randomly sampling them. Figure 3a shows OOD generalization performance for various combinations of D and \\text{Ntask}. We observe that performance is bad when the \\text{Ntask} < D, but it increases when \\text{Ntask} \\geq D. This increase is abrupt for smaller D and more gradual for higher, which is in line with remarks by Johnston and Fusi [38] that it is easier to learn abstract representations when D is high. Overall, our findings confirm our theory that abstract representations emerge when \\text{Ntask} \\geq D. This result is remarkable, especially for high D, because it goes against our intuition that \\text{Ntask} should scale exponentially with D to fill up the space adequately; instead it need only scale linearly."}, {"title": "5 Discussion", "content": "Our work provides insight for understanding how and why \u201cunderstanding\u201d emerges from solving certain prediction tasks, which is particularly relevant to the study of foundation models [10, 31, 6]. Foundation models exhibit \u201cunderstanding\u201d of the world through semantically rich abstract representations that enable efficient downstream prediction. Our work suggests a profound connection between multi-task prediction and implicit knowledge of the underlying data generation process.\nConsider an analogy with masked autoencoder vision foundation models, where x^* is the \u201cground truth\" of a scene (objects, positions, states, and relationships), the measurement variable X is an image with missing patches [19, 28], and the model predicts the missing patch data y(x^*). The model's latent variable Z exhibits some \u201cunderstanding\u201d of x^* in the form of abstract representations useful for downstream tasks. This analogy extends to masked language models [17] and autoregressive language models [60], where x^* is \u201cmeaning\u201d in a semantic space, X(t) are words, and y is the next word. Localizing x^* from Z(t) relates to constructing a world model, showing that Z represents x^* abstractly and with high fidelity.\nOur work provides a geometric starting point for understanding the correspondence between x^* and Z(t) in terms of mutual information I(x^*; Z(t)) under various prediction tasks y(x^*). Relating the properties of prediction task y with x^* and Z(t) has implications for optimal synthetic data generation"}, {"title": "5.1 Implications for deep learning", "content": "An ongoing debate in the brain sciences is whether to solve tasks the brain learns abstracts represen-tations, or simple input-to-output mappings. Here we show that training RNNs to multitask results in shared, disentangled representations of the latent variables, in the form of continuous attractors. In this multitask setting, one task acts as a regularizer for the others, by not letting the representation collapse, or overfit, to specific tasks [77]. In the brain, such pressure from multiple tasks could be imposed by thalamocortical loops for internal state-dependent action selection operating in parallel, while the integration occurs in corticostriatal circuits [63]. This framework is also consistent with proposed theories of parallel processing in cortical columns [27].\nOur findings directly link to two important neuroscientific findings: spatial cognition and value-based decision-making. First, the tasks here bear close resemblance to path-integration, i.e. the ability of animals to navigate space only relying on their proprioceptive sense of linear and angular velocity [52, 12, 72]. In path-integration animals integrate velocity signals to get location, while here we integrate noisy evidence to get rid of the noise. We learn abstract representations in the form of a 2D \"sheet\" continuous attractor, while the computational substrate for path integration is a 2D toroidal attractor [25, 68] \u2013 not an abstract representation. The conditions under which a 2D sheet vs. toroidal continuous attractor is learned is a potential area of future research. Second, decision making experiments in monkeys result in a 2D abstract representation in the medial frontal cortex, which supports novel inferential decisions [7]. Likewise, context-dependent decision-making experiments in humans also resulted in orthogonal, abstract representations [24].\nThe workhorse model for computational neuroscience has been context-dependent computation, where tasks are carried out one at a time and task identity is cued to the RNN by a one-hot vector [48]. However, this approach can be algorithmically inefficient, scaling linearly with the number of tasks, and exponentially with input dimensionality D. That is because context-dependent computation utilizes different parts of the state space for different tasks, and the resulting representations collapse to what is minimally required for each task (see [48, 75] and Appendix A.4). This can be detrimental for brains, which need to pack a lot of computation within a large yet limited neural substrate. In contrast, abstract representations are general, compact [47], can be used for any downstream task involving the same variables, scale linearly with D, and readily emerge from relatively simple tasks.\nOverall, we believe that multitasking may present a paradigm swift for generalizable representation learning in biological and artificial systems alike. That is not to say that context-dependent repre-sentations are not useful; they are great at leveraging the compositional structure of tasks [76, 20], but tend to overfit to the specifics of the task, while multitask representations serve as world models applicable to various scenarios. Both types of representations are likely to be found in the brain."}, {"title": "5.2 Implications for neuroscience", "content": "A limitation of the present work is that factorization is assumed. Yet not all problems are factorizable, or should be factorized. For instance, a more coarse-grained understanding of the world, that doesn't disentangle all factors, might be more suitable in many cases, and that might be reflected in the nature of the tasks. Furthermore, we focus on canonical cognitive neuroscience tasks which are somewhat removed from standard ML benchmarks. Normally, disentanglement methods would be tested against a benchmark such as dSprites [50]; however to the best of our knowledge no such benchmark exists for dynamic tasks where evidence has to be aggregated over time. Future work could endeavor to apply our setting to ML tasks, like self-supervised image completion, and showcase the potential benefits of scaling up the number of masked out pixels in relation to the dimensionality of the latent space D. Furthermore, we assume that noise is Gaussian and IID, which might not be always the case; yet our theory accounts for other noise distributions as long as the posterior P(x^*|X(1), ..., X(t)) is non-zero everywhere and has a self-consistent/invariant maximum likelihood estimate over x^* (see Appendix B)."}, {"title": "5.3 Limitations", "content": "We focus on the problem of optimal estimation of a latent ground truth x^* from noisy, non-linear, high-dimensional versions of it. We provide theoretical guarantees that estimation succeeds when the amount of partial information about the truth available \\text{Ntask} scales linearly with the dimensionality of the latent space D. Our results hold for any injective mapping of latent variables to observables, which incorporates a vast range of transformations. They offer exciting implications for the manifold hypothesis [22], proving that the latent representations of optimal evidence aggregators observing high-dimensional world data inevitably estimate the structure of the low-dimensional manifold. Of particular interest is extending the theory to non-injective and non-deterministic observation maps f.\nThese findings shed light in the conditions under which biological and artificial systems alike develop representations that generalize well: they do so when there is enough pressure from many tasks that involve the same latent variables. Quoting Bengio et al. [4]: '.. the most robust approach to feature learning is to disentangle as many factors as possible, discarding as little information about the data as is practical'. Apart from understanding learning in brains, we hope this work will inspire the development of deep learning systems with OOD generalization in mind."}, {"title": "5.4 Conclusion", "content": "PV would like to thank the Onassis Foundation and AR the NOMIS Foundation for funding. AB thanks the NIH PTQN program for funding. No competing interests to declare. We would like to thank Yisong Yue and Stefano Fusi for early discussions, and Aiden Rosebush for early discussions of proof methods."}]}