{"title": "Disentangling Representations in RNNs through\nMulti-task Learning", "authors": ["Pantelis Vafidis", "Aman Bhargava", "Antonio Rangel"], "abstract": "Abstract, or disentangled, representations are a promising mathematical framework\nfor efficient and effective generalization in both biological and artificial systems.\nWe investigate abstract representations in the context of multi-task classification\nover noisy evidence streams a canonical decision-making neuroscience paradigm.\nWe derive theoretical bounds that guarantee the emergence of disentangled repre-\nsentations in the latent state of any optimal multi-task classifier, when the number\nof tasks exceeds the dimensionality of the state space. We experimentally confirm\nthat RNNs trained on multi-task classification learn disentangled representations in\nthe form of continuous attractors, leading to zero-shot out-of-distribution (OOD)\ngeneralization. We demonstrate the flexibility of the abstract RNN representations\nacross various decision boundary geometries and in tasks requiring classification\nconfidence estimation. Our framework suggests a general principle for the forma-\ntion of cognitive maps that organize knowledge to enable flexible generalization in\nbiological and artificial systems alike, and closely relates to representations found\nin humans and animals during decision-making and spatial reasoning tasks.", "sections": [{"title": "Introduction", "content": "Humans and animals can generalize to new settings effortlessly, leveraging a combination of past\nexperiences and world models [43, 44]. Modern foundation models also display emergent out-of-\ndistribution (OOD) generalization abilities, in the form of zero- or few-shot learning [10, 59, 36, 57].\nFor generalization, it is crucial to have appropriate representations which extrapolate or interpolate\nprevious experiences [67, 35, 38], or the use of task compositionality as an inductive bias [76, 62].\nOne mechanism for generalization is through abstract, or disentangled, representations [29, 39, 38]. A\nproperty of abstract representations is that they preserve the latent structure present in the world in their\ngeometry, i.e. they are low-dimensional, rectangular, and compositional, each axis corresponding to a\nfactor of variation in the data. Then a linear decoder (i.e. downstream neuron) trained to discriminate\nbetween two categories can readily generalize to stimuli not observed in training, due to the structure\nof the representation. From a neuroscience perspective, this corresponds to decomposing a novel\nstimulus into its familiar attributes, and reasoning based on those. For instance, imagine you are\nat a grocery store, deciding whether a fruit is ripe or not. If the brain's internal representation of\nfood attributes (ripeness, caloric content, etc.) is disentangled, then learning to perform this task for\nbananas would lead to zero-shot generalization to other fruit (e.g. mangos, Figure 1a).\nThis raises the question of under which conditions do such representations emerge in biological and\nartificial agents alike. Several brain areas including the amygdala, prefrontal cortex and hippocampus\nhave been found to encode variables of interest in an abstract format [65, 5, 8, 55, 16]. Previous\nwork showed that feedforward neural networks develop abstract representations when trained to\nmultitask [38]. However, real-world decisions typically rely on imperfect, noisy information, evolving\ndynamically over time [9, 41]. To account for this important feature of the world, we train RNNs to\nmultitask canonical neuroscience tasks involving the accumulation of evidence over noisy streams.\nContributions. The main contributions of this paper are the following:\n\u2022 We prove that any optimal multi-task classifier is guaranteed to learn a disentangled rep-\nresentation of the ground truth contained in the noisy measurements in its latent state, if\nthe classification boundary normal vectors span the input space (Appendix B). Intriguingly,\nnoise in the observations is necessary to guarantee the latent state would compute an optimal,\ndisentangled representation of the ground truth.\n\u2022 Through experiments, we confirm that RNNs trained to multitask develop abstract rep-\nresentations that zero-shot generalize OOD, when the number of tasks exceeds the input\ndimensionality. The computational substrate of these representations is a 2D continuous\nattractor [2] storing a ground truth estimate in a product space of the latent factors.\n\u2022 We demonstrate that our setting is robust to a number of manipulations, including interleaved\nlearning of linear and non-linear tasks and free reaction time decisions.\n\u2022 Finally, we discuss the relation between our results and important neuroscientific find-\nings, their implications for generalizable representation learning in artificial systems, and\ndemonstrate the strong advantage of multitasking over previously proposed mechanisms of\nrepresentation learning in the brain [48, 76].\nDespite being framed in the context of neuroscience, our results are general; they apply to any system\naggregating noisy evidence over time."}, {"title": "1.1 Related work", "content": "Disentanglement has long been recognized as a promising strategy for generalization [4] (although\nnote [46, 54] for a contrarian view), yet most modern work focuses on feedforward architectures\n[29, 39, 74]. In terms of autoregressive models, Hsu et al. [33], Li and Mandt [45] showed that\nvariational LSTMs disentagle representations of underlying factors in sequential data allowing style\ntransfer; however to the best of our knowledge the underlying representational geometry has not been\ncharacterised. Other work focuses on fitting RNNs to behavioral data while enforcing disentanglement\nfor interpretability [18, 51]. Work on context-dependent decision making has shown that RNNs\nre-purpose learned representations in a compositional manner when trained in related tasks [76, 20];\nhowever, the abstractness of the resulting representations was not established. Finally John et al.\n[37] show that multitasking results in disentanglement, however they directly enforce latent factor\nseparation through their adversarial optimization objectives. Our approach is most closely related to\nweakly supervised disentanglement, without comparing across samples [66].\nPrevious work showed that multitasking feedforward networks learn abstract representations, as\nquantified by regression generalization [38]. We expand upon these findings in several ways. First,\nwe extend the framework of Johnston and Fusi [38] to RNNs that can update their representations\nas further information arrives. Second, we prove theorems on optimal multi-task classifiers that\nguarantee the emergence of disentangled representations in any optimal multitask classifier if the\nnumber of tasks exceeds the input dimensionality D. Third, we rigorously analyze the role of noise\nin forming disentangled representations, extending the noise-free regime studied in [38]. Finally, we\nexplore a range of values for D, providing experimental validation of our theory. Whereas related\nwork proved that disentangled representations emerge in feedforward architectures from multitask\nlearning in sparse tasks when a sparsity regularization constraint is placed on the predictors [42], we\nplace no such constraints and still uncover disentangled representations."}, {"title": "2 Model and setup", "content": "We consider canonical cognitive neuroscience tasks that involve evidence aggregation over time,\nmirroring decision-making under uncertainty. The input consists of D noisy evidence streams\nX(t) \u2208 RD where X(t) = x* + \u03c3\u039d(0, ID) and x* \u2208 RD is the ground truth evidence in a certain\ntrial (x~ Uniform(-0.5, 0.5)) and o is the input noise standard deviation (Figure 1c, left). Each\nelement x of x* corresponds to different options under consideration or different attributes of the\nsame item. The target output y(x*) \u2208 {\u22121,+1}\\task is a vector of Ntask +1s and -1s, depending\non whether x* is above or below each of the classification boundaries (Figure 1b). To ensure the\nreproducibility of our results, we incorporate our tasks within the NeuroGym framework [53].\nThe decision maker should integrate noisy samples X(t) over time, viewed through an injective obser-\nvation map (encoder) f, to estimate Yi(t) = Pr{yi(x*) = 1|f(X(1)), . . ., f(X(t))} (Figure S2).\nThe classification lines reflect criteria based on which decisions will be made. Imagine for example\nthat x1 corresponds to food and x2 to water reward. Depending on the agent's internal state, one\ncould take precedence over the other, and the degree of preference is reflected in the slope of the line.\nIn Appendix B we prove that any optimal multi-task classifier will learn disentangled representations\nof x* in its latent state Z(t), when the Ntask classification boundaries span the latent space."}, {"title": "2.2 Architecture", "content": "We train leaky RNNs which represent a brain area responsible for decision-making to multitask the\naforementioned tasks. The networks contain Nneu = 64 neurons, and their activations z(t) obey:\n$\\frac{dz(t)}{dt} = -z(t) + [W_{rec} z(t) + W_{in} x_{in}(t) + b]_+$\nwhere Wrec is the recurrent weight matrix, Win is the matrix carrying the input vector xin, bis a\nunit-specific bias vector, 7 is the neuronal time constant and [.]+ is the ReLU applied element-wise to\na vector. We simulate Equation (1) using the forward Euler method for T = 20 timesteps of duration\n\u2206t = \u03c4 = 100 ms, which we find to be stable. The RNN's output \u0177(t) of size Ntask is given by:\n\u0177(t) = g(Wout Z(t))\nwhere Wout is a readout matrix and g the output activation function applied elementwise. We choose\ng(z) = tanh(z) for classification, 5 tanh(z) for integration-to-bound, and g(z) = z for other tasks.\nThe inputs described above are first passed through a randomly initialized encoder f, which represents\na static mapping from latents to observations (e.g. a perceptual system). The encoder is a 3-layer\nMLP with hidden dimensions 100, 100, 40 and ReLU non-linearities, is unique to each network and\nkept fixed during training. An additional fixation input which is 1 during the trial and turns 0 when\nthe network should report its decisions is directly passed to the hidden layer (Figure 1c). The RNN is\ntrained to produce the target outputs. By minimizing loss across trials, the network is incentivized\nto estimate Pr{yi(x*) = +1}. We use mean-squared-error loss and train all parameters (i.e. Wrec,\nWin, Wout and b) with Adam default settings except lr = 10-3 [40]. Table S1 summarizes all\nhyperparameters and their values. Network training takes ~ 10 minutes on a commercial laptop CPU."}, {"title": "3 Theoretical Results", "content": "We seek to understand the properties of optimal multi-task classifiers with random variable latent\nstate Z(t) in the paradigm illustrated in Figure S2 and described in Section 2.1. Specifically,\nwe are interested in classifiers that, at each time point t, estimate multiple linear classifications\ny(x*) = [y1(x*), ..., YNtask (x*)] \u2208 {0,1} Ntask based on noisy, potentially non-linearly mapped\nobservations f(X(1)), . . ., f(X(t)) with maximal likelihood. The classifier output at time t, denoted\n\u0176(t) \u2208 [0, 1] Ntask, represents a set of Bernoulli random variable parameters, where each element\nY(t) corresponds to the probability Pr{yz(x*) = 1|f(X(1)), . . ., f(X(t))}).\nEach of the Ntask decision boundaries can be expressed as a pair of boundary normal vector c\u00bf \u2208 RD\nand offset bi \u2208 R (Equation 8). Let C\u2208 RNtask\u00d7D represent the matrix of decision boundaries and\nb\u2208 RNtask be the vector of decision boundary offsets.\nTheorem 3.1 (Disentangled Representation Theorem). If C \u2208 RNtask \u00d7D is a full-rank matrix and\nNtask > D, then\n1. any optimal estimator of y(x*) must encode an estimate \u00b5(t) of the ground truth evidence\nvariable x* in its latent state Z(t), and\n2. if the activation function g = tanh, \u00b5(t) will be linearly decodable from Z(t), thus implying\nthat Z(t) contains a linear disentangled representation of \u00b5(t) [30, 38].\nSpecifically, \u00b5(t) is the maximum likelihood estimate of x* given observations f (X(1)), ..., f (X(t)).\nEquations 3, 4 are closed-form expressions for extracting \u00b5(t) from Z(t) with a generic activation\nfunction g and g = tanh respectively.\n\u00b5(t) = (CTC)-1CT (\u03a6-1(g(Z(t))) + b)\n\u03bc(t) \u2248\n2\u221a3\u03c3\n\u03c0\u03bd\u03c4\n(CTC)\u00af\u00b9CTZ(t) + (CTC)\u00af\u00b9CTb\nWhere is the CDF of the normal distribution, o is the noise magnitude and t the trial duration.\nProof. Point 1 and Equation 3 are proven in Appendix B in Theorem B.6. Point 2 and Equation 4 are\nproven in Corollary B.7.\nTheorem 3.1 holds for any system that performs optimal multi-task classification based on noisy,\ninjectively transformed observations f(X(t)) (e.g., RNNs, biological neural circuits, Bayesian filters,\netc.). Our results are readily extensible to sub-optimal multi-task classification models under the\nassumption that classification error w.r.t. optimal estimation is randomly distributed with zero mean\n(see Appendix B). In this case, the closed-form solutions in Equations 3, 4 correspond to a least-\nsquared error estimate of \u00b5(t) given sub-optimal Z(t). Surprisingly, our theoretical results indicate\nZ(t) will contain a disentangled representation of x* when the readout map g(Z(t)) := \u0176(t) is a\ntanh function (Equation 4, Corollary B.7). Our theoretical framework readily generates testable\npredictions for our computational experiments, including theoretical r\u00b2 values for estimating x*\nfrom Z(t) (Appendix A.3, Figure S9a), and decodability of x* from Z(t) as a function of latent\ndimension D (Figure 3a), number of tasks Ntask (Figure 1f,g, 2c), noise level \u03c3 (Figure 3b), evidence\naggregation time t (Lemma B.3), and latent factor correlation (Figure S6)."}, {"title": "4 Experiments", "content": "We train RNNs to do simultaneous classifications for 24 linear partitions of the latent space for D = 2\n(Figure 1b, 6 partitions shown). Figure 1d shows the top 3 PCs (capturing ~ 85% of the variance) of\nnetwork activity after training (final accuracies ~ 95%) for multiple trials, along with the fixed points\nof network dynamics. To find the fixed points, we follow a standard procedure outlined in Sussillo\nand Barak [69] (see Appendix A.1 for details). Looking at Figure 1d the fixed points span the entire\ntwo-dimensional manifold that the trials evolve in, which corresponds to a continuous attractor with\nstable states across a 2D \"sheet\". This means that the network can store a short-term memory [73] of\nthe current amount of accumulated evidence in a product space of the latent variables.\nFurthermore, the representation is low-dimensional, rectangular and compositional; all characteristics\nof disentangled representations. In comparison, the representations after the encoder are non-linearly\nmixed, high-dimensional and overlapping (Figure S5a). Individual trials with noise show how the\nrepresentation maintains a sense of metric distances in the RNN representation space (Figure S5b).\nFigure S5c demonstrates how this representation comes about during learning, and Figure S5d that"}, {"title": "4.2 Nonlinear tasks and interleaved learning", "content": "So far we have been training RNNs to perform linear classifications. However there are cases where\nthe streams might need to be combined nonlinearly. For instance, if the two streams represent\nthe amount and probability of reward respectively, an agent needs to multiply the two and decide\nwhether the expected value exceeds a certain (metabolic) cost y of performing an action to obtain\nsaid reward. Figure S8a shows the classification lines for the multiplicative task, where the network\nshould decide whether the ground truth x* lies above or below the curve X1 X2 = \u03b3, for multiple\nvalues of y. RNNs trained on this task learn a representation that separates for trials in different\nquadrants (Figure S8b). This heuristic might have come about because individual classification lines\nspan only a single quadrant for this task. Furthermore, trials belonging to the same quadrant seem to\nbe more convoluted, with a corresponding drop in OOD performance (median r\u00b2 = 0.71), which\nmight reflect the non-linearity of the task. Therefore, we wondered how the representation would\nlook like if the network was trained on both the linear and multiplicative tasks, as animals need to do.\nFor that we perform interleaved training of both tasks (i.e. train in batches sampled from one of the\ntasks at a time), a setting where neural networks excel at, compared to humans who excel at blocked\ntraining, where tasks are learned sequentially (but see Flesch et al. [24]). Figure S8c shows that the\nnetwork learns a two-dimensional continuous attractor, and the learned representation is no longer\nseparated by quadrants. OOD generalization for this network is excellent, and almost identical to\nID performance (median r\u00b2 = 0.94, 0.97 respectively). Overall, we conclude that our framework\nextends to interleaved learning of a mixture of linear and nonlinear tasks, which better reflects the\nchallenges encountered by agents in the real world. Note that during interleaved training, linear and\nnon-linear tasks are not performed simultaneously; yet they are in immediate succession which also\nplaces pressure to the network to gradually learn representations that satisfy all tasks. The relationship\nbetween interleaved learning and multitasking could be an interesting direction for future work."}, {"title": "4.3 Abstract representations are learned for a free reaction time, integrate to bound task", "content": "So far we have been training networks to produce a response at the end of the trial. However, in\nmany situations agents are free to make a decision whenever they are certain enough. Therefore,\nwe here seek to extend our framework to free reaction time (RT) decisions. A canonical model\naccounting for choices and reaction times in humans and animals is the drift-diffusion model [41, 11].\nIt is composed of an accumulator that integrates noisy evidence over time, until a certain amount of\ncertainty, represented by a bound, is reached, triggering a decision. In the linear classification task\nsetting, the accumulated amount of evidence at time t for a line with slope a, Aa(t) is given by:\nAa(t) = Aa(t - 1) + X\u2081(t) \u2212 a X2(t)"}, {"title": "4.4 RNNs confirm and extend theoretical predictions", "content": "Here we expore the relation between the theory in Section 3 and Appendix B and experiments in\nSection 4 in more depth. First, we wondered why performance saturates in our networks to a high yet\nnon-1 r2. The central limit theorem predicts that the estimate of the ground truth x* in any optimal\nmulti-task classifier becomes more accurate with \u221at, providing a theoretical maximum r\u00b2 given trial\nduration T (Appendix A.3). Since the networks trained on the free RT task are required to output their\nconfidence at any time in the trial, we can compute OOD r\u00b2 of network predictions at any timepoint\nt, and compare that to the theoretical prediction. Figure S9a shows that indeed the highest RNN r\u00b2\nfalls in the vicinity of or just short of the theoretical maximum. This indicates that RNNs trained with\nBPTT on these tasks behave like near-optimal multi-task classifiers that create increasingly accurate\npredictions with time, tightening the relation between our theoretical and experimental results.\nAn important prediction of our theory is that to learn abstract representations Ntask should exceed D.\nTo test this, we increase D (adding more inputs to Figure 1c), and vary Ntask. Sampling classification\nhyperplanes methodically in high-dimensional spaces is non-trivial; therefore we resort to randomly\nsampling them. Figure 3a shows OOD generalization performance for various combinations of D\nand Ntask. We observe that performance is bad when the Ntask < D, but it increases when Ntask \u2265 D.\nThis increase is abrupt for smaller D and more gradual for higher, which is in line with remarks by\nJohnston and Fusi [38] that it is easier to learn abstract representations when D is high. Overall,\nour findings confirm our theory that abstract representations emerge when Ntask \u2265 D. This result\nis remarkable, especially for high D, because it goes against our intuition that Ntask should scale\nexponentially with D to fill up the space adequately; instead it need only scale linearly."}, {"title": "5 Discussion", "content": "Our work provides insight for understanding how and why \u201cunderstanding\u201d emerges from solving\ncertain prediction tasks, which is particularly relevant to the study of foundation models [10, 31,\n6", "understanding": "f the world through semantically rich abstract\nrepresentations that enable efficient downstream prediction. Our work suggests a profound connection\nbetween multi-task prediction and implicit knowledge of the underlying data generation process.\nConsider an analogy with masked autoencoder vision foundation models, where x* is the \u201cground\ntruth\" of a scene (objects, positions, states, and relationships), the measurement variable X is an image\nwith missing patches [19, 28"}]}