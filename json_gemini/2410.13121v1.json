{"title": "TRUST BUT VERIFY: PROGRAMMATIC VLM EVALUATION IN THE WILD", "authors": ["Viraj Prabhu", "Senthil Purushwalkam", "An Yan", "Caiming Xiong", "Ran Xu"], "abstract": "Vision-Language Models (VLMs) often generate plausible but incorrect responses\nto visual queries. However, reliably quantifying the effect of such hallucinations in\nfree-form responses to open-ended queries is challenging as it requires visually ver-\nfying each claim within the response. We propose Programmatic VLM Evaluation\n(PROVE), a new benchmarking paradigm for evaluating VLM responses to open-\nended queries. To construct PROVE, we provide a large language model (LLM)\nwith a high-fidelity scene-graph representation constructed from a hyper-detailed\nimage caption, and prompt it to generate diverse question-answer (QA) pairs, as\nwell as programs that can be executed over the scene graph object to verify each QA\npair. We thus construct a benchmark of 10.5k challenging but visually grounded\nQA pairs. Next, to evaluate free-form model responses to queries in PROVE, we\npropose a programmatic evaluation strategy that measures both the helpfulness\nand truthfulness of a response within a unified scene graph-based framework. We\nbenchmark the helpfulness-truthfulness trade-offs of a range of VLMs on PROVE,\nfinding that very few are in-fact able to achieve a good balance between the two.", "sections": [{"title": "1 INTRODUCTION", "content": "Vision-language models (VLMs) have emerged as an effective solution for generating responses\nto queries about visual content. However, despite impressive progress (and much like their LLM-\ncounterparts), VLMs are still known to hallucinate \u2013 to generate plausible but incorrect answers that\nare either inconsistent or unverifiable against the provided visual context. This crucial shortcoming\nhas the potential to erode trust in such systems and has already begun to attract significant research (Yu\net al., 2024; Liu et al., 2023a; Gunjal et al., 2024; Huang et al., 2024) and regulatory (Biden, 2023)\ninterest, particularly as using such models as the \"foundation\" of various high-stakes applications\nbecomes imminent (Bommasani et al., 2021).\nThis has led to a flurry of research on reliably benchmarking VLM performance (Liu et al., 2024a), by\nmeasuring not just the helpfulness but also the truthfulness of their responses. Existing benchmarks\nfall into two categories \u2013 discriminative (Hu et al., 2023; Lovenia et al., 2023; Li et al., 2023),\nwhich evaluate the model's responses to close-ended, existence-based queries (\u201cIs there a man in this\nimage?", "Describe this image.\"). While\ndiscriminative benchmarks ease evaluation, they do not realistically simulate in-the-wild usage. On\nthe other hand, generative benchmarks, while realistic, are extremely challenging to reliably evaluate,\nas they require verifying both that the model response fully answers the question (i.e. is helpful) and\ndoes not make any false claims (i.e. is truthful).\nEvaluating such free-form responses typically relies on external models (usually, a proprietary LLM)\nto score responses given some image context (typically ground-truth annotations). However, we\nfind that in several such benchmarks, the context provided is completely insufficient to judge if the\nresponse contains hallucinations. Consider Fig. 1: a VLM may respond to the query \\\"How many\npuppies are in the image?\\\" (correct answer = \\\"four\\\"), with \\\"There are four labradoodle puppies\\\".\"\n    },\n    {\n      \"title\": \"2 RELATED WORK\",\n      \"content\": \"Benchmarking VLM hallucination. Existing benchmarks fall into one of two groups (see Fig. 2):\n\u25ba Discriminative benchmarks generate a series of binary questions to verify the presence (or\nabsence) of various entities (or distractors) in the image. Early benchmarks like POPE (Li et al., 2023)\nlimited their scope to object entities annotated by humans or external off-the-shelf models (Zou et al.,\n2024), whereas follow-up works additionally evaluate responses to negative presence queries (Lovenia\net al., 2023), which stress-test the model's abstention capabilities on questions about entities absent\nfrom the image, or use an LLM to generate a broader range of existence-based questions covering\nobjects and their attributes (Hu et al., 2023). However, while the binary questions that typify such\nbenchmarks simplify evaluation, they do not realistically simulate in-the-wild use.\n\u25b7 Generative benchmarks instead evaluate model hallucinations in response to free-form questions.\nCHAIR (Rohrbach et al., 2018) measures the precision and recall of entities mentioned in a generated\nimage description against the ground truth. HaELM (Wang et al., 2023b) additionally uses a large\nlanguage model (LLM) to judge generations, whereas M-HalDetect (Gunjal et al., 2024) has humans\nannotate hallucinations in model generated descriptions are used to train a predictive model. Recently,\nAMBER (Wang et al., 2023a) combines a POPE style evaluation with a generative evaluation over\nan open-ended split. While these benchmarks are indeed more realistic, they still restrict the query\ninstruction to image captioning-style templates (\u201cDescribe this image in detail.\").\nMost recently, a few benchmarks with truly open-ended queries have been proposed (Sun et al.,\n2023; Liu et al., 2023a; Jing et al., 2023; Liu et al., 2023b), which either hand-design or use an\nLLM to generate free-form questions, and use external models to judge the corresponding responses.\nHowever, these too have limitations: MMHal (Sun et al., 2023) and HallusionBench (Liu et al.,\n2023a) rely on a series of off-the-shelf models at various stages which introduce noise (see Fig. 2,\ncol 3). GAVIE's (Liu et al., 2023b) reliance on dense captions and bounding boxes leads to a\nmajority of questions querying localized image regions and spatial relationships, many of which\nhave unnatural-sounding responses (eg. mentioning image coordinates, see Fig. 2, col 4). Finally,\nGPT-4-based evaluation is both expensive and inherits the model's own limitations.\nUnderstanding and mitigating VLM hallucination. Several works have sought to better understand\nwhy VLMs hallucinate. One prevalent theory is the model learning spurious correlations between the\ninput and the output: either due to overly strong text priors learned by the LLM backbone (Huang\net al., 2024; Leng et al., 2024), or due to distilling synthetic outputs generated by stronger models\n(such as GPT-4V) that may themselves contain confabulation (Liu et al., 2024b). This is often\nexacerbated by the predominant training recipe (Liu et al., 2024b; Abdin et al., 2024) that learns a\nshallow projection from the visual input to the text embedding space which limits the expressivity of\nthe model to learn visually grounded representations.\nRecent work has proposed training-based and training-free strategies for mitigating hallucinations.\nThe former involves finetuning (Liu et al., 2023b) or preference optimization (Yu et al., 2024; Sun\net al., 2023) of \\\"preferred\\\" ground truth responses against dis-preferred synthetically generated\"\n    },\n    {\n      \"title\": \"3 APPROACH\",\n      \"content\": \"Vision-language models are trained to respond to a question Q about an image I with a ground-truth\nanswer A. Let me(.) denote a VLM model trained on a large dataset of such (I, Q, A) triplets.\nAt test time, we wish to evaluate the model response \u00c2=m\u0259(Q, I). Specifically, while prior work\ntypically evaluates either the response's correctness (is \u00c2=A) or truthfulness (is p(\u00c2|Z) > threshold),\nwe propose a unified framework that jointly evaluates both.\",\n      \"latex\": [\n        \"\\hat{A} = m_{\\theta}(Q, I)\",\n        \"is \\hat{A} = A\",\n        \"p(\\hat{A}|Z) > threshold\"\n      ]\n    },\n    {\n      \"title\": \"3.1 GENERATING VERIFIABLE VISUAL QUESTION-ANSWERS\",\n      \"content\": \"To build PROVE, we first download image-caption pairs (I, C) from the test set of the recently\nproposed DOCCI (Onoe et al., 2024) dataset, containing 5k manually curated images with com-\nprehensive human-annotated descriptions. DOCCI is particularly well-suited for VLM evaluation\nbecause: i) its captions are extremely detailed, with a higher median caption length than competing\ndatasets, which correlates with high image recall ii) its comprehensive and rigorous 3-stage human\nannotation protocol leads to high-fidelity captions that are suitable to test a range of image under-\"\n    },\n    {\n      \"title\": \"Building a robust scene-graph representation.\",\n      \"content\": \"Scene-graphs are comprised of en-\ntity (<entity>), attribute (<entity, attribute>), and relationship (<entity_1,\nattribute, entity_2>) tuples that describe a scene. We use the tuples included with the\nDOCCI test set that were automatically extracted from its captions using an LLM (Cho et al., 2024),\nand use it to construct a scene graph representation g(C) as a directed graph with attributed entities as\nnodes and relationships as edges. The scene graph is implemented as a Python class with methods\nto query the graph for its entities, attributes, and relationships, as well as to extract and describe\nsubgraphs in natural language (full API in Lst. 1).\"\n    },\n    {\n      \"title\": \"Generating open-ended questions with verifiable answers.\",\n      \"content\": \"Next, for each image, we prompt a\npre-trained LLM to generate 10-15 challenging, diverse, and unambiguous question-answer (QA)\npairs given a caption and scene graph, along with an accompanying Python program that accepts the\nscene graph as input and can be executed to verify the generated QA pair (Gupta & Kembhavi, 2023;\nSur\u00eds et al., 2023). We include a few in-context examples of such scene-graph and QA+program\ninput/output pairs in the prompt (see Fig. 8). We repeat this procedure to generate a large dataset of\nopen-ended image+QA pairs {(Zi, Qi, Ai)}=1 and their verification programs.\",\n      \"latex\": [\n        \"{(Z_i, Q_i, A_i)}_{i=1}^N\"\n      ]\n    },\n    {\n      \"title\": \"Filtering QA pairs.\",\n      \"content\": \"Next, we perform two rounds of filtering:\n1. Programmatic: First, we execute the generated program with the scene graph as input to verify the\nQA pair. We discard pairs for which the program either fails or returns an answer that is semantically\ndifferent (Reimers, 2019) from the ground truth answer.\n2. Text-based: Next, we perform a few additional post-processing steps to exclude low-quality QA\npairs which are i) trivial, ungrammatical, ambiguous, or incomplete (using an LLM, see Fig. 9), ii)\nnot entailed by the image (using a visual entailment model (Cai et al., 2019)), iii) include one or more\nwords from a manually curated list of taboo words that we find to result in low-quality questions, or\niv) semantic duplicates for the same image (using SemDeDup (Abbas et al., 2023)). Our final dataset\nafter filtering contains 10.5k high-quality visual question answers see Fig. 3.\"\n    },\n    {\n      \"title\": \"Dataset statistics.\",\n      \"content\": \"We now present some statistics about PROVE, which comprises of 10.5k QA\npairs generated from 5k image-caption pairs from the DOCCI test set. These are obtained after\napplying both programmatic filtering i.e. either the unit test fails (18.3%) or returns the wrong answer\n(9.8%), and text-based filtering (~ 50% of the total from the previous stage). Note that we opt to\nfilter out such a large percentage of QA pairs in the interest of ensuring high-quality evaluation data.\nFurther, our benchmark curation process is fully automatic and so can be readily scaled to a larger\nimage-caption source. Questions in PROVE average 10.3 words in length whereas answers average\n13.4 words (see Fig. 6, right). In Fig. 6, left we present a sunburst visualization of the first 4 words in\nthe questions, highlighting the diversity of questions in our benchmark.\"\n    },\n    {\n      \"title\": \"3.2 PROGRAMMATIC VLM EVALUATION (PROVE)\",\n      \"content\": \"After ensuring the validity of the generated QA pairs, we proceed to evaluating free-form VLM\nresponses to the same \u00c2=m\u0259(Q, I). We first extract tuples from \u00c2 (using an LLM (Dubey et al.,\n2024) with in-context prompting), that we use to build a scene graph representation g(A). We also\nbuild a similar scene graph from the ground truth answer tuples after excluding \u201cpremise": "uples\nincluded in the question g(A) \u2013 g(Q). We then measure response helpfulness hscore(.) based\non recall of this scene graph, i.e. the fraction of tuples (nodes, attributes, and relationships) in\ng(A) - g(Q) that are recovered by g(A). Concretely, we compute average cosine similarity between\neach ground truth tuple and its closest response tuple in embedding (Reimers, 2019) space.", "latex": ["\\hat{A} = m_{\\theta}(Q, I)", "g(\\hat{A})", "g(A)", "g(\\hat{A}) - g(Q)", "g(A) - g(Q)", "g(\\hat{A})"]}, {"title": "tscore(\u00c2) =", "content": ";", "latex": ["hscore(A) = \\frac{\\sum_{t \\in g(A) - g(Q)} \\max_{t' \\in g(\\hat{A})} sim(t, t')}{|g(A) - g(Q)|} -;", "tscore(A) =  \\frac{\\sum_{t'\\in g(\\hat{A})} \\max(\\max_{t \\in g(C)} sim(t', t), p(I |= t'))}{|g(\\hat{A})|}"]}, {"title": "where", "content": "denotes visual entailment, and p(I |= t') is approximated using a visual entailment\nmodel (Cai et al., 2019). Note that hscore and tscore are not necessarily correlated \u2013 a response can\nbe helpful (by answering the query) but not entirely truthful (might contain hallucinations), and vice\nversa. Naturally, different models may achieve different trade-offs between the two an aspect that\nPROVE is uniquely suited to analyze.", "latex": ["p(I |= t')", "hscore", "tscore"]}, {"title": "4 EXPERIMENTS", "content": "We now present our benchmarking experiments on PROVE. We include a broad set of models spanning\na range of sizes and learning strategies and extensively analyze their performance, including their\nperformance trade-offs. We also conduct a human study to validate both the quality of our benchmark\nand how well our proposed metrics correlate with human judgement."}, {"title": "4.1 SETUP", "content": "Baselines. We include VLMs of three sizes \u2013 small (<5B parameters), medium (5-10B parameters),\nand large (>10B parameters) \u2013 and include both open-source and proprietary models. We also\nbenchmark an additional LLM-based \u201coracle\u201d model as an upper bound (a blind model that is provided\nwith the ground truth caption image). For the model, we use a LLaMA-3.1-8B backbone (Dubey\net al., 2024) with in-context prompting.\nData. PROVE is constructed from images, tuples, and captions released under a CC by 4.0 license as\nthe test split of the DOCCI (Onoe et al., 2024) dataset. DOCCI images were reviewed both by human\nand automatic methods to remove or obfuscate PII (faces, phone numbers, and URLs) and unsafe\ncontent. Images underwent a rigorous 3-stage human annotation phase resulting in hyper-detailed\nand high-recall captions averaging 136 words."}, {"title": "4.2 RESULTS", "content": "Table 1 and Figure 4 present evaluation results. We find that:\n\u25b7 Few models strike a good balance between helpfulness and truthfulness. As Fig. 4 (left) shows,\nmodels tend to exhibit a range of trade-offs between helpfulness and truthfulness, with very few\nfrom the subset that we study (GPT-40, Phi-3.5-Vision, Pixtral) managing to strike a good balance\nbetween the two. In fact, we find that many recent models that rank highly on perception and\nreasoning-focused aggregate benchmarks (Lu et al., 2024), such as Claude-3.5-Sonnet (Anthropic,\n2023) and Intern-VL2 (26B) (Chen et al., 2024) do not necessarily translate to high truthfulness on\nPROVE, lagging behind simpler and smaller models like LLaVA-1.5 (Liu et al., 2024c) in tscore. In\nfact, we find the LLaVA-1.5 model series to obtain the best tscore overall. Overall, we observe a\nweak linear correlation of 0.03 between hscore and tscore averaged across models, suggesting that the\nimpressive recent gains in model helpfulness have not necessarily translated to higher truthfulness.\nTable 1 shows a more detailed breakdown of performance and includes additional baselines, cate-\ngorized by parameter count. As seen, the oracle model does considerably better than other models,\nindicating that there is still significant room for improvement in the current generation of VLMs.\n\u25b7 Increasing model size improves hscore but not necessarily tscore. Across both model families\nthat we benchmark at multiple sizes - InternVL2 (Chen et al., 2024) (2B, 8B, and 26B), and\nLLaVA (Liu et al., 2024b) (1.5-7B, Next-7B, and 1.5-13B), we find that larger or more recent variants\ntend to outperform smaller ones in terms of helpfulness but not necessarily truthfulness.\n\u25b7 Models fail in different ways. In Fig. 5 we provide example responses from two models with high\nhscore (GPT-40) and tscore (LLaVA-1.5-7B) respectively. We find that while both models struggle\nwith subtasks such as OCR, counting, and reading an analog clock, GPT-40's errors tend to be less\negregious (e.g. reading 3/6 letters of the graffiti correctly, while LLaVA only gets 1/6). Further,\nGPT-40 tends to generate more descriptive answers (e.g. correctly identifying that while the wall\nin the first image is white, the bricks at the bottom are gray), which boost its hscore."}, {"title": "Human evaluation of PROVE and proposed metrics.", "content": "Finally, we conduct two human studies of\nour benchmark. We first ask human annotators (3 per example) to evaluate the question relevance\nand answer correctness of QA pairs generated from the qual-test split of the DOCCI dataset (100\nimages, 170 generated QA pairs) that is specifically set aside for human evaluation. After majority\nvoting, annotators judge 163/170 questions to be relevant (95.9%) and 167/170 answers to be correct\n(98.2%). We manually inspect the small number of examples judged as irrelevant or incorrect in and\nfind most to be either particularly challenging or subjective, rather than irrelevant or incorrect.\nIn the second study, we ask subjects (3 per example) to rate responses from four models \u2013 GPT-40,\nLLaVA-1.5-7B, LLaVA-Next-7B, and GPT-40-mini on the same set of 170 QA pairs based on their\nhelpfulness (0=unhelpful, 1=helpful) and truthfulness (0=fully false, 0.5=partially false, 1.0=fully\ntrue), and average. We then automatically compute hscore and tscore for the same set of responses\nand measure the Pearson correlation between the two, observing a strong correlation of 0.81 for\nhscore and a modest correlation of 0.45 for tscore, supporting the validity of these metrics."}, {"title": "5 DISCUSSION", "content": "Our work takes a step towards reliably evaluating the helpfulness-truthfulness trade-offs of vision-\nlanguage models. Our design leverages an LLM prompted with a robust scene graph representation\nand API to construct \u201cin-the-wild\u201d visual question answer pairs that are grounded by design. Further,\nthese QA pairs lend themselves to programmatic evaluation via comparing scene-graph representa-\ntions. The reliability of our benchmark comes from three factors: i) high-recall human-annotated\nimage captions that seed the scene graphs, which make it possible to (almost) exhaustively validate\nthe veracity of any claim ii) programmatic verification of the generated QA pairs that ensure that both\nthe question and answer are indeed grounded in the visual input, and iii) evaluation metrics that are\nboth holistic (i.e. consider all the provided context) and interpretable (i.e. provide a concrete scoring\nrubric based on scene graph-based matching).\nLimitations. While we hope that PROVE will serve as a useful test-bed for reliable VLM evaluation\nand spur future research on the topic, it is not without limitations. While we try to ensure high\nprecision in QA pairs retained in our benchmark (via programmatic verification), this naturally\ncomes at some cost to recall (i.e. some hard-to-verify question types may be excluded from the"}, {"title": ""}]}