{"title": "Large Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Level", "authors": ["Antoine Grosnit", "Alexandre Maraval", "James Doran", "Giuseppe Paolo", "Albert Thomas", "Refinath Shahul Hameed Nabeezath Beevi", "Jonas Gonzalez", "Khyati Khandelwal", "Ignacio Iacobacci", "Abdelhakim Benechehab", "Hamza Cherkaoui", "Youssef Attia El-Hili", "Kun Shao", "Jianye Hao", "Jun Yao", "Balazs Kegl", "Haitham Bou-Ammar", "Jun Wang"], "abstract": "We introduce Agent K v1.0, our first version of an end-to-end autonomous data science agent designed to automate, optimise, and generalise across diverse data science tasks. Fully automated, Agent K v1.0 manages the entire data science life cycle by learning from experience. Unlike traditional chain-of-thought and reflection methods, which are rigid and limited in incorporating feedback, Agent K v1.0 leverages a highly flexible structured reasoning framework. This enables it to dynamically process memory in a nested structure, effectively learning from accumulated experience stored to handle complex reasoning tasks. Agent K v1.0 optimises long- and short-term memory by selectively storing and retrieving key information, guiding future decisions based on environmental rewards. This iterative approach allows it to refine decisions without traditional fine-tuning or backpropagation, achieving continuous improvement through experiential learning. We evaluate our agent's capabilities using Kaggle competitions as a case study. Following a fully automated protocol, Agent K v1.0 systematically addresses complex data science tasks, employing Bayesian optimisation for hyperparameter tuning and advanced feature engineering. Integrating libraries like Torchvision and HuggingFace, it handles diverse data modalities and, once models are trained, determines optimal submission strategies to the Kaggle leaderboard. This adaptive refinement, based on performance feedback, occurs without human intervention. Our new evaluation framework rigorously assesses Agent K v1.0's end-to-end capabilities, from retrieving a Kaggle competition URL to submitting results and ranking on the leaderboard. Results demonstrate that Agent K v1.0 achieves a 92.5% success rate across tasks, spanning tabular, computer vision, NLP, and multimodal domains. When benchmarking against 5,856 human Kaggle competitors by calculating Elo-MMR scores for each, Agent K v1.0 ranks in the top 38%, demonstrating an overall skill level comparable to Expert-level users. Notably, its Elo-MMR score falls between the first and third quartiles of scores achieved by human Grandmasters. Furthermore, our results indicate that Agent K v1.0 has reached a performance level equivalent to Kaggle Grandmaster, with a record of 6 gold medals, 3 silver medals, and 7 bronze medals, as defined by Kaggle's progression system.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) excel at human interactions through natural language [1], but their ability to engage with the physical world remains in its early stages [6]. Some progress has been made in areas such as robotics [51, 53] through careful skill engineering, as well as in navigating mobile apps [16], and across reinforcement learning (RL) domains [8, 12, 68, 85]. To improve LLM performance in complex real-world domains, various prompting strategies have been proposed to enhance reasoning and planning abilities, such as chains of thought [79, 81], trees of thought [88], and graphs of thought [5], among other methods [80]. These advancements, along with tool integration [25, 66], have motivated the development of generalist AI agents that successfully solve (though still relatively simple and simulated) sequential decision-making problems by relying on LLM outputs as decision policies [14, 24]. Real-world challenges often demand more than isolated solutions, requiring systematic approaches to complex problems. This has driven research into how LLMs can handle agent-based tasks with sequential or parallel modules, enabling dynamic, multi-step problem-solving. In this paper, we adopt a first-principle approach, framing the analysis, processing, and prediction of data (i.e., data science) as a core and transferable skill for LLMs interacting with real-world environments and external systems. Our motivation is threefold. First, data science is designed to extract insights and knowledge from structured and unstructured data, addressing various complex problems. This necessitates a system-level approach, demanding automation and optimisation to adapt to specific task objectives. For example, in Kaggle data science contests (illustrated in Figure 1), data scientists follow a structured workflow: collecting, cleaning, preprocessing, and standardising data, creating data loaders for efficient management, generating key evaluation metrics, and developing custom models. These insights and predictions then inform strategic decisions and drive operational optimisation. Thus, resolving these workflows requires data science agents to decompose tasks into subproblems and interact with various subsystems to achieve specified objectives. Second, data enables LLM agents to perceive and comprehend external environments (both physical and virtual). LLM agents can derive meaningful insights and make informed decisions by collecting, cleaning, and analysing this data. This essential connection between data and action bridges computational reasoning with tangible, real-world outcomes. Third, data science is essential for enterprises, driving innovation by transforming raw data into actionable insights that enhance efficiency and competitiveness. As a result, global investment in data science is projected to reach around $740 billion by 2031 [61]. A data science agent would amplify this impact by generating codes for automating tasks like data cleaning, modelling, and prediction, allowing organisations to scale their data-driven decision-making to maximise returns and profits. An LLM agent for data science faces two challenges, automation and optimisation, due to the complexity and multifaceted nature of data science pipelines, which require technical formulation and domain-specific expertise. First, data science workflows are inherently dynamic, requiring constant monitoring and adaptation to real-time data changes, making"}, {"title": "Learning to Reason by Experience", "content": "Standard protocols for training LLMs rely on fine-tuning [54] and backpropagation [62], which are computationally heavy, memory-intensive and require extensive datasets to achieve meaningful improvements. This process becomes even more cumbersome in data science problem settings due to the slow pace of data collection. Typically, data must be gathered by setting up specific tasks and allowing an agent to model potential solutions. This time-intensive process can take days or weeks to yield a single data point, particularly for large-scale data science tasks. Consequently, while theoretically promising, this approach proves challenging to scale for our objectives in this work. Instead, we propose an alternative framework that leverages the agent's internal memory alongside an external database, enabling the LLM to adapt dynamically without relying on backpropagation and fine-tuning. To formalise this learning objective, we envision an agent operating within an environment where its goal is to maximise returns through optimal decision-making, best defined as a Markov decision process (MDP) [70] $\\mathcal{M} = (\\mathcal{S}_{env}, \\mathcal{A}, P, R, \\gamma)$. Here, $\\mathcal{S}_{env}$ represents the environment's state space, $\\mathcal{A}$ our agent's action space, $P : \\mathcal{S}_{env} \\times \\mathcal{A} \\times \\mathcal{S}_{env} \\rightarrow [0, 1]$ the environment's state transition, $R : \\mathcal{S}_{env} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ a reward function measuring the learner's behaviour and $\\gamma \\in [0, 1)$ a discount factor used to trade-off immediate versus future returns. The agent's actions directly influence the environment, causing state transitions based on its choices. Before deciding on action at step h, the agent has access to three key sources of information: the current state of the environment, $s_h^{(env)} \\in \\mathcal{S}_{env}$, a working memory (its internal memory that retains information for immediate and short-term decisions), $M_h$, and the current status of an external database $D_h$. Contrary to standard sequential decision-making, we allow the agent to structure its reasoning process by altering $M_h$ before deciding on an extrinsic action to execute in the environment. Adding this flexibility generalises standard notions from reinforcement learning and enables scalability to high-dimensional"}, {"title": "Autonomous Data Science Agents", "content": "Based on the formulation above, we aim to build a highly scalable and autonomous data science agent that can handle and solve multimodal tasks belonging to $\\mathcal{T} = \\{T_1, ..., T_n\\}$ where n is the total number of available data science problems. In its first version, Agent K handles tabular, time series, computer vision, natural language processing, and multimodal tasks. We represent each task as the following tuple $T_i = (Description_i, Raw-Data_i)$ with $Description_i$ denoting the natural language details of $T_i$ and $Raw-Data_i$ its available datasets. Agent K v1.0 commences from a set of $\\mathcal{T}$ tasks by automatically fetching and scraping Kaggle using their URLs; see Section 4 for a more detailed exposure of the benchmark. Next, we focus on automatically setting up one task"}, {"title": "Phase I (Automation) - Setting Up Data Science Tasks", "content": "Creating valid code for the setup phase is unsurprisingly tricky. Some consider it the most tedious step data scientists must complete due to the vast variability in data science problems across domains. Data science tasks rarely follow a clean and standardised format, each necessitating custom and potentially intricate preprocessing. For instance, while many tabular tasks on Kaggle are neatly structured and adhere to standard evaluation metrics, tasks involving image or textual inputs often have raw data organised in nested subfolders with diverse structures and domain-specific evaluation protocols."}, {"title": "Setup Pipeline", "content": "The pipeline begins by extracting raw data and task descriptions, summarising, and planning. The plan depends on what the agent identifies as the nature of the task $t \\in \\mathcal{T}$ and as the input and target modalities, which must be inferred from the raw data. In Figure 4, we outline the different stages of the setup pipeline with blue boxes, omitting duplication across different data modalities. For instance, \u201cTraining Input Map\" encompasses building maps for tabular, image, and textual inputs, depending on the modalities. The input maps are standardised tables designed to handle the diverse data types and folder structures in Kaggle competitions. The maps act as structured guides, with rows that gather the correct inputs and input types from various files and align them with their corresponding targets. The agent automates this process and aims to pass a series of unit tests that we detail in Appendix A.1.3. After creating the plan, Agent K v1.0 tackles each relevant stage, whose completion is assessed through stage-specific unit tests, determining the progression to the next stage. Additionally, meta-unit-tests evaluate the consistency of outputs across multiple stages, such as the \"Data loader Unit Tests\u201d. In the figure, we display green boxes for the stage-specific unit tests and the meta-unit tests. If the final unit test passes, we consider the task adequately formatted. This final unit test runs a light version of the solution generation process, ensuring that the data loaders prepared by our setup agent are compatible with the following solution generation pipeline."}, {"title": "The Setup MDP", "content": "We instantiate the MDP described in Section 2 to model the automatic setup process of data science tasks. We define the MDP as follows: $\\mathcal{M}_{setup} = (\\mathcal{S}_{setup}, \\mathcal{A}_{setup}, P_{setup}, R_{setup}, \\gamma, H)$ where $\\mathcal{S}_{setup}$ is the state space encompassing information about the current stage and the workspace content, $\\mathcal{A}_{setup}$ is the action space, $P_{setup} : \\mathcal{S}_{setup} \\times \\mathcal{A}_{setup} \\times \\mathcal{S}_{setup} \\rightarrow [0, 1]$ is the transition probability function, $R_{setup}: \\mathcal{S}_{setup} \\times \\mathcal{A}_{setup} \\rightarrow \\mathbb{R}_{setup}$ is a reward function taking as input a state and an action, $\\gamma \\in [0, 1)$ is a discount factor, and H is the time horizon or maximum number of steps. The state space is the Cartesian product $\\mathcal{U}_{setup} \\times \\mathcal{W}$ of the discrete set of pipeline stages $\\mathcal{U}_{setup} = \\{u^{(1)}, ..., u^{(K)}\\}$, and the set of possible workspace contents $\\mathcal{W}$. At each step h, given a state $s_h = (u_h, w_h) \\in \\mathcal{U}_{setup} \\times \\mathcal{W}$, the environment expects an action $a_h$ whose nature is stage-specific and can correspond to the generation of a summary in natural language, the creation of a new file, etc. Therefore, the action space $\\mathcal{A}_{setup}$ is a union of stage-dependent action spaces $\\mathcal{A}_{setup}(u)$. Our policy, $\\pi_{setup}$, only assigns weights to actions within the current stage's action space $\\mathcal{A}_{setup}(u)$. If unit tests follow a stage, the reward function assigns penalties or bonuses based on the test's outcomes. Failing to set up a task leads to an overall negative sum of decayed rewards. Moreover, error logs from unit tests are stored in the workspace, enabling the agent to use this feedback to refine its actions."}, {"title": "Setup Transition Dynamics", "content": "To continue, we formalise the transition dynamics our agent encounters between stages. Image that we have the following sequence of ordered stages $\\mathcal{U}_{setup} = (u^{(1)}, ..., u^{(K)})$ to handle. As the first k stages tackle task analysis and raw data summarisation, they arrive with no unit test. As such, for those stages, we have: $P(u_h, a_h^{(ext)}, u) = \\delta_{u=u_{h+1}}$ for $0 < h < k$. At the end of the $k^{th}$ step, the input and output modalities are determined and induce skipping some setup stage, leaving us with the remaining subset to handle: $(v^{(1)}, . . ., v^{(L)}) \\subseteq \\mathcal{U}_{setup}$. Each of those stages is associated with unit tests $test^{(1)}, ..., test^{(L)}$. Moreover, we assume the existence of meta-unit-tests $\\{meta-test^{(1)}, ..., meta-test^{(M)}\\}$. Each meta-test$^{(m)}$ is responsible for checking the consistency of several stages. We denote by $\\mathcal{V}(meta-test^{(m)})$ the ordered list of stages covered by the meta-test $meta-test^{(m)}$. Finally, we denote by $\\mathcal{V}(v^{(l)})$ = $\\{meta-test^{(m)} \\text{ s.t. } \\mathcal{V}(meta-test^{(m)})[-1] = v^{(l)}\\}$ the subset of meta-unit tests that cover $v^{(l)}$ as their last stage. Here, we used the notation \u201cz[i]\u201d to refer to the $i^{th}$ element of a tuple z, and \"z[-i]\" to its $i^{th}$ element starting from the end. This notation allows us to describe the stage transition dynamics as follows. At stage $v^{(l_h)}$ (with $l_h \\geq k$), if the action $a_h^{(ext)}$ leads to a failure of $test^{(l_h)}$, the stage remains the same at the next step: $\\mathcal{T}(v^{(l_h)}, a_h^{(ext)}, v) = \\delta_{v = v^{(l_h)}}$. On the other hand, if the unit test $test^{(l_h)}$ passes, then the meta-unit-tests in $\\mathcal{V}(v^{(l_h)})$ are executed. If all of them pass (or if the set is empty), we move on to the next stage, whereby $\\mathcal{T}(v^{(l_h)}, a_h^{(ext)}, v) = \\delta_{v = v^{(l_{h+1})}}$. Finally, if one of the meta-unit tests, $meta-test^{(m)} \\in \\mathcal{V}(v^{(l_h)})$ fails, then we transition to the earliest stage covered such that $\\mathcal{T}(v^{(l_h)}, a_h^{(ext)}, v) = \\delta_{v = \\mathcal{V}(meta-test^{(m)})[1]}$."}, {"title": "Action Generation through Structural Reasoning", "content": "While standard RL techniques could theoretically address the MDP outlined above, the high dimensionality of state and action spaces presents significant challenges to directly applying state-of-the-art RL methods. Specifically, the actions involve generating extensive sequences of tokens to produce code, rewards are exceedingly sparse since it is challenging to attribute unit test failures to specific code segments, and policy adaptation necessitates backpropagation and differentiation through large-scale policy models. To navigate this complex environment, we extend our Pangu-Agent framework [14] to encompass data science scenarios, as in Section 2. This framework empowers agents to structure their internal reasoning processes by incorporating intrinsic functions alongside the standard extrinsic policies that interact with external environments. We generically refer to $\\pi_{setup}^{(u)}$ as the intrinsic function that Agent K v1.0 uses to take action when being at stage u. Therefore, we express the automated data science setup policy $\\pi_{setup}$ as: $\\mathcal{A}_{setup}(a_h^{(ext)}, a_h^{(M)}, a_h^{(D)} | w_h, u_h, M_h, D_h) = \\pi_{ALLM}(a_h^{(ext)} | M_{h+1}) \\pi_{setup}^{(u)}(a_h^{(M)}, a_h^{(D)} | w_h, u_h, M_h, D_h)$ where $M_{h+1} = a_h^{(M)}(M_h)$. Moreover, we can adapt Equation (2) to turn task setup automation"}, {"title": "Tackling Credit Assignment with Nested Reasoning", "content": "Credit assignment (CA) is another crucial component in solving the above problem \u2013 the process by which $\\pi_{setup}$ determines which parts of the generated code should be rewritten to maximise the likelihood of passing meta-unit tests. Standard RL performs this analysis by training state critics, which is a data- and compute-intensive process. While building those critics is a valuable direction for future work, this paper enables credit assignment directly from LLM agents through structured reasoning after encountering a meta-unit test failure. Our strategy can be intuitively described as follows: when Agent K v1.0 encounters a meta-unit-test failure, we prompt an LLM to generate a thought explaining the potential cause of the error. Armed with this insight, the agent revisits earlier stages of the process and, for each stage, it re-executes the step incorporating the generated thought and the previously written code into its prompt. If the agent suspects the code from that stage contributed to the failure, it will attempt to create a new code. If it believes the code is correct, it simply regenerates the same code and moves on to the next potentially faulty stage. This cycle continues, stage by stage until the faulty stage is identified and the meta-unit-test is finally passed. Credit Assignment Intrinsic Functions: Credit assignment occurs after a meta-unit test $meta-test^{(m)}$ breaks at a step h \u2013 1, leaving an error message stored in the workspace and available in $w_h$. The cause of the error is shared among the actions taken at each stage v of $\\mathcal{V}(meta-test^{(m)})$. However, not all actions during these stages are necessarily wrong. Repeating all steps without considering previously successful actions would be inefficient, especially if only a single step was the root cause of the failure. Instead, we allow the agent to reflect on which parts to retain and which to correct by analysing the previous steps and the meta-error. As explained when defining the transition dynamics, the stage following the meta-error, $u_h=v^{(l_h)}$, corresponds to $\\mathcal{V}(meta-test^{(m)})[1]$. To generate the action $a_h^{(ext)}$, we introduce the following structure for Agent K v1.0, omitting $D_h$ for the sake of simplicity: $M_{h,1} = \\alpha_{h,1}^{(M)}(w_h, M_h) = M_h \\cup \\{META-ERROR-THOUGHT : \\mathcal{F}_1(\\mathcal{L}\\mathcal{L}\\mathcal{M}(\\Phi_1(w_h, M_h)))\\}$ with $a_{h,1}^{(M)} \\sim \\mu_1^{(M,u_h)}(\\cdot | w_h, M_h)$,"}, {"title": "Phase II (Optimisation) - Solving Data Science Tasks", "content": "Once $\\pi_{setup}$ successfully sets up a task t, notably constructing valid train and test data loaders for each input and target modality, the role of the policy $\\pi_{solve}$ is to generate code that can involve feature engineering and creating and training machine learning models to maximise task-specific performance metrics, which have been automatically coded by Agent K v1.0 during the task's setup phase. Additionally, $\\pi_{solve}(\\cdot)$ populates submission files with target predictions. To get a fair comparison, when running experiments on Kaggle competitions, we let our agent upload its submission files to observe the public score and measure the final performance on a leaderboard, as we detail in Section 4."}, {"title": "Solution generation process", "content": "Modality-based approach: The solution generation process is tailored to the modalities identified during task setup. Not all tools and actions are applicable depending on the nature of the task, as a customised approach is required. AutoML tools can offer competitive solutions for fully tabular tasks due to the extensive attention they have received in the field over the past decades. However, they often lack flexibility and require careful data formatting, where Agent K v1.0 can excel. To address these challenges, we developed a specialised"}, {"title": "Phase III (Generalisation) - Multi-Task and Active Task Selection", "content": "The methods detailed above only consider one data science task at a time. However, effective agents must be able to tackle multiple tasks with varying domains. This section starts by equipping Agent K v1.0 with cross-domain set-up and solving capabilities by generalising the previous formulations to a multi-task setting. Noticing the multi-task solution methodology's infeasibility, we then present an online continual learning extension. Data Science Agents as Multi-Task Learners: Considering the whole set of multimodal tasks from $\\mathcal{T} = \\{T_1, ..., T_n \\}$, we rewrite the setup optimisation problems as: $\\underset{\\pi_{ALLM}\\{\\pi_{setup}^{(u)}\\}_{\\forall u \\in \\mathcal{U}_{setup}}}{\\operatorname{argmax}} \\frac{1}{n} \\sum_{j=1}^{n} \\mathbb{E}_{\\tau^{(j)} \\sim p_{setup}(\\tau)} \\left[ \\sum_{h=0}^{H} \\gamma^{h} R_{setup}(s_{h}^{(j)}, a_h^{(ext, j)}) \\right]$ (Multi-Task Setup), with n being the total number of tasks, and the superscript (j) is used to denote the $j^{th}$ task's states and actions. Moreover, $p_{setup}$ matches Equation (3) where the initial state distribution $\\nu^{(j)}_0$ is a Dirac with weight on the initial state of the jth task, and $\\nu_D^{(j)}$ deterministically yielding the last state of the long-term memory database at the end of the previous episode, $\\mathcal{D}_{H-1}^{(j)}$. Importantly, sharing the long-term memory across tasks enables knowledge transfer between data science domains. Like the setup phase, we define a multi-task version of the solution phase such that: $\\underset{\\pi_{ALLM}\\{\\pi_{solve}^{(u)}\\}_{\\forall u \\in \\mathcal{U}_{solve}}}{\\operatorname{argmax}} \\frac{1}{n} \\sum_{j=1}^{n} \\mathbb{E}_{\\tau^{(j)} \\sim p_{solve}(\\tau)} \\left[ \\sum_{h=0}^{H} \\gamma^{h} R_{solve}(s_{h}^{(j)}, a_h^{(ext, j)}) \\right]$ (Multi-Task Solution). Again, we leave the intrinsic functions, and the long-term memory shared to allow for knowledge transfer when solving cross-domain tasks. Continual Learning and Active Selection From Long-Term Memory: Data science problems are inherently dynamic and non-stationary, especially as the number and diversity of tasks grow. As such, Agent K v1.0 must actively and effectively learn from past experiences stored in the long-term memory to make informed decisions about future tasks. By utilising this memory, our agent can build a curriculum that prioritises specific tasks to improve the likelihood of success while acquiring new information to expand its knowledge base. Interestingly, trading off exploration and exploitation in task selection improves the agent's learning efficiency and offers computational advantages. By focusing on tasks with higher expected returns, the agent reduces the number of failed attempts and conserves resources."}, {"title": "Related Work", "content": "Achieving performance and automation in data science tasks has been a long-standing challenge. Numerous techniques have been developed to automate the human-intensive steps of the DS pipeline using machine learning. Given that each step in the DS pipeline poses challenges, many approaches have focused on specific aspects of the problem, often resulting in narrow application ranges or use-case limitations. For example, probabilistic inference rules have been used to automate data cleaning [59], while various deep learning methods have been developed to filter, select, or transform features to enhance the accuracy of downstream models [38, 39, 41]. In the modelling domain, a significant research area known as neural architecture search (NAS) optimises the structure of neural networks for predictions [46, 82], shifting the manual effort from model design to the design of a model search space. Beyond neural architecture, many hyperparameters can be automatically tuned to achieve better performance using black-box optimisation methods. Techniques like Bayesian optimisation address black-box optimisation in a sample efficient way and are supported by various libraries [2, 4, 15, 18], which users can utilise by correctly identifying and providing the hyperparameters of their pipelines. Additionally, meta-learning has been introduced to further improve data efficiency by leveraging past experiments to predict performance on new tasks based on related tasks solved with similar configurations [29, 49, 52]. Contrasting with these targeted methods, several libraries have been developed to cover broader parts of the data science pipeline, such as H2O-AutoML [43], AutoKeras [36], Autosklearn [21], and AutoGluon [73]. These tools aim to automate and optimise by making rule-based decisions based on input data modality and combining a fixed collection of models that can be refined using preset optimiser tools. However, these approaches have limitations, including restricted scope, flexibility, and usability barriers. Their performance is limited by the set of predefined models and hyperparameters they support, and they require expert knowledge to properly format the problem [11] (e.g., manual specification of column types for AutoKeras). Tools like H2O-AutoML are also limited to tabular data tasks, requiring data to be structured in specific ways. The emergence of data-science agents powered by large language models and multi-agent systems represents a shift towards more autonomous and adaptable solutions. Unlike traditional Auto-DS approaches, these agents are designed to perform end-to-end tasks by leveraging advanced reasoning and decision-making capabilities. Recent advancements in LLMs [1, 33, 50, 60], combined with techniques like chain of thought prompting [79, 92] and ReAct [87] have significantly expanded the scope of AI applications beyond natural language processing. These models are now capable of generating code and interacting with APIs [17, 64, 57], laying the groundwork for their integration into multi-agent systems [24, 28, 74, 76, 83] that can execute complex tasks by thinking,"}, {"title": "Intelligent DS assistants", "content": "Several agents function as intelligent assistants, enhancing human decision-making by providing automated solutions while keeping users in the loop. For instance, CleanAgent [56] handles data cleaning based on user specifications, and LAMBDA [69] generates data analysis reports based on technical queries. MatPlotAgent [86] pairs a coding agent with a visual agent to meet the expert's needs in terms of data visualisation, while WaitGPT [84] offers a smoother monitoring of the code generation by creating graph representations of the generated code and supporting on the fly manual edition or questions. Although these agents enhance user efficiency, they require substantial user input. They are not designed to autonomously achieve high performance in complex, real-world scenarios, such as winning a Kaggle competition."}, {"title": "End-to-end DS agents", "content": "End-to-end data-science agents aim to manage the entire data science pipeline autonomously, from data preprocessing to model evaluation and hyperparameter optimisation, without requiring significant human oversight. While some existing approaches address specific aspects of this workflow, such as CAAFE [26] which allows agents to iteratively generate codes to modify tabular features and reflect based on accuracy feedback, or EVAPORATE-CODE+ [3], which focuses on data mining by generating code to extract structured data from semi-structured documents, other methods have tackled the entire data science pipeline more holistically. Recent multi-agent systems like SEED [11], AutoM3L [47], Data Interpreter [27], AutoML-Agent [78], HuggingGPT [65], MLCopilot [90], DS-Agent [23], SELA [13], and AutoKaggle [45] achieve capacities to deal with data-science by empowering their system with numerous tools, using retrieval from expert demonstrations, and supporting complex multiple steps reasoning. These systems employ various strategies for generating automated pipelines for different stages of the data science process such as preprocessing, modelling, and feature selection. While DS-Agent, MLCopilot and AIDE generate and refine the entire solution script as a single block, SEED performs a skyline-based optimisation to select the modules to apply to solve the task. Alternatively, HuggingGPT delegates the planning to a high-level agent, and Data-Interpreter lets this agent decide on a hierarchical and dynamic plan. Meanwhile, AutoM3L follows a pre-defined modality-specific pipeline to produce final predictions. Our work integrates elements from these various approaches by imposing a modality-dependent structure during the task setup phase while allowing agents greater flexibility in selecting the optimal sequence of steps to achieve the best possible outcome. Moreover, several existing systems also incorporate mechanisms for continual learning, enabling them to leverage past experiences to improve performance on new tasks."}, {"title": "Data-science benchmarking", "content": "Assessing agents' performance on data science tasks has been approached from various perspectives. While many benchmarks, such as those for coding proficiency [35, 93], focus on general code generation capabilities, they do not necessarily capture the full range of skills required for end-to-end data science. Assessing the performance of agents on data-science tasks has been considered from different scopes. Some benchmarks are tailored to specific aspects of data science. For example, [42] focuses on code infilling based on data from Stack Overflow and common libraries, targeting localised code generation rather than comprehensive pipeline development. Unlike traditional AutoML benchmarks [22], which typically provide a standardised input format to facilitate solver design, data science agent benchmarks evaluate broader capabilities. These include understanding the task requirements and adapting solutions accordingly, offering flexibility in problem formulation and solution strategies. Authors in [32] introduce a benchmark using six Kaggle tasks, where success is defined as achieving a 10% improvement over a baseline provided by starter code. However, this baseline can sometimes represent minimal predictive performance, such as random predictions, rather than a robust comparison against established leaderboards. This benchmark also offers predefined scripts and environments for each task, guiding the problem-solving approach. Other benchmarks, such as [7], emphasise the orchestration of workflows and interactions with professional software tools, while [30] provides a carefully curated set of questions requiring language models to generate code that answers specific dataset-related queries. This differs from end-to-end problem-solving in competitions like"}, {"title": "Limitations & Future Work", "content": "This paper introduced Agent K v1.0, the first LLM-based agent to achieve Kaggle Grandmaster level performance across a wide range of data science tasks. These tasks spanned multiple domains, including tabular data, time series analysis, computer vision, natural language processing, and multimodal challenges. Our agent operates fully autonomously, seamlessly handling everything from navigating a URL to building models, making submissions, and winning competitions. Agent K v1.0 advances structured reasoning by introducing novel intrinsic functions for action generation and credit assignment. These functions enable the agent to learn and adapt using memory, eliminating the need for sample-intensive backpropagation methods. We further presented a method for scaling our agent based on continual active learning and curriculum building. We demonstrated a reduced number of generated tokens to autonomously and correctly set up cross-domain data science tasks from their natural language descriptions. We introduced new techniques that enable Agent K v1.0 to interact with external tools such as hyperparameter optimisation frameworks, custom LLM-based feature engineers, RAMP, Torchvision, and Torchtext. This incorporation allows the agent to sequence, update, and generate code in these tools to address data science problems. As a result, our agent has achieved remarkable success, earning 6 gold, 3 silver, and 7 bronze medals. We also conducted Elo-MMR ratings to avoid overfitting specific tasks, demonstrating that our agent ranks in the top 38% among 5,856 participants. While our results are successful and present new state-of-the-art performance of data science agents, Agent K v1.0 can benefit from further improvements as we detail next. In Section 3.1, we introduced two new intrinsic functions that allow Agent K v1.0 to automatically set up data science tasks. Currently, those functions only consider feedback from unit and meta-unit tests for credit assignment and adaptation from external memory. While this option is viable, as we demonstrated in Section 5, incorporating feedback from the second phase (Section 3.2) could further enhance the setup process by identifying which parts of the code and data preprocessing techniques contribute to higher performance. Furthermore, Agent K v1.0 leveraged various external tools to achieve high performance in Kaggle competitions. Some of these tools were developed in our previous work (e.g., HEBO [15", "40": "while others were created by third parties (e.g., Torchvision). New tools, such as feature engineering modules, were also developed specifically for Agent K. Our current mechanism to improve performance based on those tools uses standard intrinsic functions from [14"}]}