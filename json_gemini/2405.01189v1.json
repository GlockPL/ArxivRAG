{"title": "GRADIENT-CONGRUITY GUIDED FEDERATED SPARSE TRAINING", "authors": ["Chris Xing Tian", "Yibing Liu", "Haoliang Li", "Ray C.C. Cheung", "Shiqi Wang"], "abstract": "Edge computing allows artificial intelligence and machine learning models to be deployed on edge devices, where they can learn from local data and collaborate to form a global model. Federated learning (FL) is a distributed machine learning technique that facilitates this process while preserving data privacy. However, FL also faces challenges such as high computational and communication costs regard- ing resource-constrained devices, and poor generalization performance due to the heterogeneity of data across edge clients and the presence of out-of-distribution data. In this paper, we propose the Gradient-Congruity Guided Federated Sparse Training (FedSGC), a novel method that integrates dynamic sparse training and gradient congruity inspection into federated learning framework to address these issues. Our method leverages the idea that the neurons, in which the associated gradients with conflicting directions with respect to the global model contain ir- relevant or less generalized information for other clients, and could be pruned during the sparse training process. Conversely, the neurons where the associated gradients with consistent directions could be grown in a higher priority. In this way, FedSGC can greatly reduce the local computation and communication over- heads while, at the same time, enhancing the generalization abilities of FL. We evaluate our method on challenging non-i.i.d settings and show that it achieves competitive accuracy with state-of-the-art FL methods across various scenarios while minimizing computation and communication costs.", "sections": [{"title": "1 INTRODUCTION", "content": "Machine learning has seen significant success in various fields, but traditional centralized training methods pose challenges due to the need for large data sets, high costs, and potential privacy risks. Federated Learning (FL) Yang et al. (2019); McMahan et al. (2017) addresses these issues by allow- ing multiple parties to collaboratively learn a model without sharing private data. In an FL system, each party trains a local model on their own data, and the weights or gradients are sent to a central server for aggregation. The server updates the global model and sends it back for further training.\nFL is ideal for edge computing applications involving distributed devices with limited resources. However, training Deep Neural Networks (DNNs) on these devices presents challenges related to re- source efficiency and data heterogeneity. Edge devices may have limited resources for local training and communication with the central server, and they may exhibit heterogeneous data distributions, affecting the generalization performance of FL.\nModel compression techniques have been proposed to address resource efficiency, involving the transmission of a compressed parameter/gradient vector between clients and servers Wu et al. (2022); Chen et al. (2021). However, this only reduces communication workload and does not create a smaller, more efficient model. The lottery ticket hypothesis Frankle et al. (2020) suggests that dense neural networks contain sparse subnetworks that can achieve the same accuracy as the original model when trained alone. Some researchers aim to extract a lightweight model from the original model for more efficient client-side training Mugunthan et al. (2022); Li et al. (2021a);"}, {"title": "2 RELATED WORK", "content": "Federated Learning. Federated learning algorithms aim to obtain a global model that minimizes the training loss across all clients. Each client j has a small set of local data $D_j$ for local training, but to preserve user privacy, clients do not share their local data, where the process can be formulated as $\\min_w f(w) = \\Sigma_{k=1}^{K}p_kF_k(w)$, where $F_k(w)$ denotes the objective of the deep learning model on the k-th client, K is the set of clients, $p_k > 0$, and $\\Sigma_{k} p_k = 1$. In practice, one can set $p_k = n_k/n$, where $n_k$ and n denote the number of data points in the k-th client and the total number of data points, respectively. It is worth noting that federated learning is different from the traditional distributed learning scenario where data partitions are assumed to be i.i.d., meaning that they are generated from the same memoryless stochastic process. However, this assumption does not hold in federated learning. Instead, data can often be heterogeneous among clients.\nIn the FedAvg McMahan et al. (2017) family of algorithms, training proceeds in communication rounds. At the beginning of each round r, the server selects a subset of clients $C_r$, $C_r \\subseteq K$, and sends the current server model parameters $\\theta^r$ to the clients. Each client $c \\in C_r$ performs E epochs of training using the received model parameters on its local data to produce updated parameters $\\theta_c$, which are then uploaded to the server. The server then updates the global model with a weighted average of the sampled clients' parameters to produce $\\theta^{r+1}$ for the next round.\nSparse Training. Sparse training performs training and pruning simultaneously by adding a sparsity regularization term to the loss function, producing structured sparsity. Sparse training with dynamic sparsity, also known as Dynamic Sparse Training (DST), is a recent research direction that aims at accelerating the training of neural networks without sacrificing performance. A neural network is initialized with a random sparse topology from scratch. The sparse topology (connectivity) and the weights are jointly optimized during training. During training, the sparse topology is changed periodically through a prune-and-grow cycle, where a fraction of the parameters are pruned, and the same portion is regrown among different neurons. An update schedule determines the frequency of topology updates. Many DST works have been proposed, focusing on improving the performance of sparse training for supervised image classification tasks by introducing different criteria for neuron growth Mostafa & Wang (2019); Evci et al. (2020). DST has demonstrated its success in many fields, such as continual learning Sokar et al. (2021a), feature selection Atashgahi et al. (2022), ensembling Liu et al. (2021), adversarial training \u00d6zdenizci & Legenstein (2021), and deep reinforcement learn- ing Sokar et al. (2021b). FedDST Bibikar et al. (2022), which is closely related to our research, is"}, {"title": "3 \u041c\u0415\u0422\u041dODOLOGY", "content": "We adopt a federated learning setting based on the common FedAvg-like formulation $\\min_w f(w)$. Instead of sending the full dense parameters $\\theta^r$ to the central server at each round r, each client j only uploads the sparse parameters along with a bit mask $(\\theta^r_c, m^j_c)$. The bit mask has the same shape as the parameter to indicate whether the parameter at the corresponding index is zero or not.\nThe central server aggregates the parameters and masks from all clients to produce the global pa- rameters and direction mask $(\\theta^{r+1}, d^{r+1})$ for the next round, along with the parameter mask $m^{r+1}$. These are then passed to the selected clients for the next round. The selected clients use the received parameters and direction mask to perform local sparse training. Following Evci et al. (2020), each client maintains a target overall sparsity $S = \\Sigma_l S^l$, $S \\in [0,1)$, where l denotes the lth layer of the network and $W^l$ is the number of parameters in that layer. Each layer may have a different layer sparsity $s^l$, which is defined as the ratio of zero parameters to the total number of parameters in that layer, $s^l \\in [0,1)$. We adopt Erd\u00f6s-R\u00e9nyi Kernel as the layer sparsity distribution $(s^l)$ across the network, which is a modified version of Erd\u00f6s-R\u00e9nyi formulation Mocanu et al. (2018), to generally allocate higher sparsities to the layers with more parameters while allocating lower sparsities to the smaller ones."}, {"title": "3.1 THE PRUNE-AND-GROW MECHANISM FOR SPARSITY TRAINING", "content": "The dynamic sparse training consists of two phases: learning and re-adjusting. In the learning phase, each client updates the masked parameters using its local training data through standard backpropagation. Parameters with zero mask values remain unchanged and do not join the training. In the re-adjusting stage, each selected client will periodically re-adjust/update its parameter mask to enable dynamic sparsity. The re-adjusting stage is triggered by the global server: starting from the initial training round, for every AR rounds till $R_{end}$, the global server requests for mask re- adjustment. Each client records its cumulative training epochs since the start of the first round and re-adjusts its mask every \u2206T epochs until it reaches $T_{end}$ i.e. the total expected training epochs of the client c. The re-adjusting process has two steps: pruning and growing. In the pruning step, for each layer, given the target layer sparsity $s^l$, each client c prunes its layer parameters to a slightly higher sparsity $\\hat{s}^l = s^l + \\sigma_c(1 - s^l)$, where $\\sigma_c$ is a factor that controls the level of over-pruning, i.e., $k = \\hat{s}^lN^l$ parameters will be pruned (mask set to 0) in total. Inspired by Evci et al. (2020), we also set $\\sigma_c$ to be a periodic variable along the federated learning process in our work as $\\sigma_c = (1+\\cos(\\frac{t_c\\pi}{T_{end}}))$, $\\sigma_c \\in [0, \\alpha]$, where \u03b1 is a hyper-parameter and $t_c$ is the current cumulative training epochs of client c. This means that each client prunes more parameters at the beginning than in the middle. In the growing step, we need to grow some neurons back (corresponding mask values set from 0 to 1) to achieve the target sparsity level. That means for each layer, we need to grow $k'$ parameters, where $k' = (\\hat{s}^l - s^l) * N^l$. We will discuss how we select which neurons to grow in the next section. Through the prune-and-grow cycle, the mask space for sparsity neural network training adaptively changes over time."}, {"title": "3.2 GRADIENT GUIDED PRUNING AND GROWING", "content": "Our main contribution is to propose a novel criterion for pruning and growing neurons in sparse neural networks. Unlike previous methods (e.g., Atashgahi et al. (2022)) that grow connections randomly without considering the issue of data heterogeneity, we design our criterion specifically for federated learning settings, where we leverage a global pseudo-gradientYao et al. (2019) direction map to optimize the network topology. The global direction map indicates the direction of the"}, {"title": "3.3 GLOBAL AGGREGATION", "content": "When the central server receives the sparse parameters and masks from the clients, we perform a sparse weighted average to aggregate them as follows:\n$\\theta^{r+\\frac{1}{2}}_i = \\frac{\\Sigma_{c \\in C_r} n_c\\theta^c_i + n_{rest}\\theta^m_i}{\\Sigma_{c \\in C_r} n_c + n_{rest}m_i},$\n$N_{rest} = n - \\Sigma_{c \\in C_r}n_c$,\n$\\theta^{r+1} = prune(ArgTopK(-|\\theta^{r+\\frac{1}{2}}|, k))$,\nDifferent from FedDST, in the central aggregation phase, we still consider the parameters and masks that are held by those clients who do not participate, as we find that updating the global model only by partially selected clients' learning results may lead to unstable performance of the global model.\nIn this process, neurons that are zero-masked by all clients get pruned globally. In most cases, the global model sparsity after aggregation may be lower than the target sparsity S since we use OR logic to aggregate the client masks. In such case, we prune some additional neurons with the smallest absolute value of the aggregated weights, i.e. $ArgTopK_i(-|\\theta^{r+\\frac{1}{2}}|, k)$, where k is the number of neurons to be pruned. This again helps us achieve the target sparsity S and obtain the final global parameters $\\theta^{r+1}$."}, {"title": "4 EXPERIMENTS", "content": "In this section, we evaluate our proposed FedSGC on two benchmark datasets: MNIST, CIFAR-10. Our evaluation primarily focuses on comparing FedSGC with two SoTA FL methods that utilize pruning techniques: FedDST Bibikar et al. (2022) and PruneFL Jiang et al. (2022). We also adapt GraSP Wang et al. (2020) into FL setting as another baseline. We skip LotteryFL Li et al. (2020) as it is designed for Personalized FL setting. We assess the performance of these methods while con- sidering communication costs. To reflect a challenging data heterogeneity environment, we create a highly non-IID data distribution among clients, following the same non-iid partition strategy as in FedAvg, where most client only has data from two classes. We also adopt the Dirichlet distribution with parameter \u03b2 to distribute the CIFAR-10 to simulate more relaxing but realistic heterogeneity."}, {"title": "4.1 RESULTS ON MNIST", "content": "Implementation. We select 100 clients from the MNIST dataset and randomly choose 10 clients to participate in each round of federated training. All methods are training for 400 federated rounds, 5 local epochs each round. We first sort the MNIST samples by label and split them into 200 shards of size 300. Each client is assigned two shards, resulting in pathological non-IID MNIST dataset partitions, where most clients only have data from two classes. resulting in a pathological non-IID partition where most clients only have data from two classes. For the federated training settings, we adopt the same CNN architecture and local training algorithm as in FedDST. The CNN consists of two 5x5 convolution layers (the first with 10 channels, the second with 20), a fully connected layer with 50 units and ReLU activation, and a final softmax output layer. The local training is performed via vanilla SGD with a learning rate of lr = 0.001 and batch size of 50.\nResult analysis. We compared the accuracy of each method on challenging pathological partitions with a fixed upload bandwidth. The results are shown in Table 1 and Figure 2. PruneFL and GraSP were excluded due to their high bandwidth consumption. Our FedSGC outperformed other meth- ods in accuracy and convergence speed, despite not excelling at the initial stage, likely due to the model's random initialization. FedSGC quickly reached over 80% accuracy and achieved the high-"}, {"title": "4.2 RESULTS ON CIFAR10", "content": "Implementation. We use the same data partition setttings as for MNIST. We follow FedAvg to use the network architecture from the TensorFlow official tutorial google (1999), which is a CNN with three 5x5 convolution layers (the first with 32 channels, the second and third with 64), a fully connected layer with 1024 units and ReLU activation, and a final softmax output layer. The local training is performed 20 epochs, via Adam with a learning rate of lr = 0.0001 and batch size 50. We also incorporate the FedProx term with \u03bc = 0.1 to show that our method is compatible and effective with other FL frameworks.\nResult analysis. Table 2 and Figure 3 present the results. Both FedDST and FedSGC, benefiting from dynamic sparse training, show marked performance enhancements over full parameter train- ing methods like FedAvg and FedProx. This is attributed to the sub-network ensembling effect of DST, where each client's unique mask (network topology) represents their local data features and contributes to the global ensemble. FedSGC surpasses FedDST early on (around 40 MiB upload capacity), maintaining this lead and achieving the highest accuracy at the training's end. This trend is consistent across all methods with FedProx terms, with our FedSGC with FedProx term ranking a close second. We roughly tuned the FedProx term weight from [0.01, 0.1, 1.0], selecting the best one, demonstrating our method's compatibility and effectiveness with other FL algorithms.\nComparison at different Dirichlet parameters. To ascertain the resilience of our proposed FedSGC in diverse heterogeneous data environments, we conducted an evaluation using a Dirichlet- distributed CIFAR-10 dataset with varying parameters of B (0.1, 0.5, and 1.0). A higher \u03b2 value corresponds to a larger number of locally observed classes, indicating more homogeneous client distributions. For instance, with \u1e9e = 0.1, the unique labels per client range from 1 to 10. As de- picted in Figure 4, FedSGC consistently outperforms FedDST, with the performance gap widening as \u03b2 decreases, indicating increased heterogeneity."}, {"title": "5 CONCLUSIONS", "content": "In this paper, we introduce FedSGC, a novel federated sparse training scheme that seamlessly com- bines sparse neural networks and FL paradigms with the inspection of gradient congruity, which can effectively reduce the communication and computation costs of federated learning, while achieving competitive performance under heterogeneous data distributions. We have evaluated FedSGC on"}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 \u0421\u043e\u043cMUNICATION AND COMPUTATION SAVINGS", "content": "Communication Analysis. The sparse training nature of FedSGC allows it to conserve substantial communication bandwidth, both in terms of upload and download, when compared to the full train- ing methods like FedAvg. Let's denote the total number of network parameters as n, each taking up 4 bytes (32 bits). Given a target sparsity S, the average upload and download cost for FedSGC in most communication rounds is 32(1 \u2013 S)n. With a mask re-adjust round frequency of AR, FedSGC incurs an additional cost of  2(1 \u2013 S)n during the re-adjusting rounds for distributing the global di- rection map and the newly adjusted mask. Therefore, the average download communication cost for FedSGC is $(32(1-S) + \\frac{2(1-S)}{A_R})n$.\nComputation Analysis. FedSGC also significantly reduces local computational workloads by maintaining sparse networks throughout the FL process. In most rounds, FedSGC does not require full dense training. In terms of FLOP savings, this allows us to bypass most of the FLOPs in both training and inference, proportional to the model's sparsity. Only few epochs of full training are needed in the re-adjust round for neuron growth. This cost can be further optimized by limiting the parameters exploration space to a sparsity lower than the target sparsity only (e.g.,), instead of exploring in the full parameters space (i.e., zero sparsity)."}, {"title": "A.2 RESULTS ON PACS", "content": "Furthermore, to evaluate the robustness of our method against data heterogeneity, we also use the PACS dataset, which is a mainstream benchmark for domain generalization tasks. This dataset contains four different domains (i.e., Photo, Art Painting, Cartoon, Sketch) with the same seven label classes (i.e., Dog, Elephant, Giraffe, Guitar, Horse, House, Person). The data distribution among different domains is significantly different, so we can test the global model's generalization ability against the data heterogeneity by using the leave-one-domain-out strategy, where we use three domains as three clients for federated training and the remaining one domain for testing.\nImplementation. We follow the settings of previous works which also adopt PACS in federated learning experiments Nguyen et al. (2022); Zhang et al. (2023). We use ResNet-18 as the backbone, SGD with learning rate 0.001 as the optimizer for clients' training. The target sparsity S to 0.6. We iteratively pick one domain out as the target domain for global model evaluation, and make the remaining 3 domains as clients to join the federated learning process for 40 rounds, 2 epochs each round. Again, we compare all methods with the FedProx term using weight \u03bc = 0.1 for all experiments, after rough tuning \u03bc from [0.01, 0.1, 1.0].\nResult Analysis. We conduct four separate experiments, each with a different domain as the target domain for testing. The full results of the four experiments are reported in Table 3, and the four line graphs showing the best accuracy achieved along the training process are presented in Figure 5. As we can see, in all experiments, FedSGC and FedSGC with FedProx term can consistently outperform the other baselines, ranking first or second. Especially in the experiment with the target domain Sketch, which is known to have the largest domain gap with the other three domains, FedSGC outperforms the other dynamic sparse training baseline, FedDST, significantly by 7.7%. We also find that the sparse training with target domains of ArtPainting and Cartoon benefits most from the FedProx term, as both FedDST and FedSGC with the prox term outperform the ones"}, {"title": "A.3 DETAILS OF ALGORITHM", "content": ""}]}