{"title": "NTU-NPU System for Voice Privacy 2024 Challenge", "authors": ["Nikita Kuzmin", "Hieu-Thi Luong", "Jixun Yao", "Lei Xie", "Kong Aik Lee", "Eng Siong Chng"], "abstract": "In this work, we describe our submissions for the Voice Privacy Challenge 2024. Rather than proposing a novel speech anonymization system, we enhance the provided baselines to meet all required conditions and improve evaluated metrics. Specifically, we implement emotion embedding and experiment with WavLM and ECAPA2 speaker embedders for the B3 baseline. Additionally, we compare different speaker and prosody anonymization techniques. Furthermore, we introduce Mean Reversion FO for B5, which helps to enhance privacy without a loss in utility. Finally, we explore disentanglement models, namely B-VAE and NaturalSpeech3 FACodec.\nIndex Terms: Voice Privacy Challenge 2024, Speaker Anonymization, Emotion Embedding, Voice Conversion", "sections": [{"title": "1. Introduction", "content": "Voice Privacy Challenge 2024 [1] introduces a new metric to evaluate the ability to maintain the conveyed emotion of the original source speaker.\nGiven the baselines provided by organizers, we test various techniques and methods aimed at improving the evaluated metrics. Specifically, we create submissions for all four of EER conditions by using a modified version of B3 [2] and B5 [3] systems, with the former responsible for conditions with EER lower than 30% and the latter for those with EER from 30% and above. Additionally, we experiment with disentanglement-based models such as B-VAE [4] and NaturalSpeech3 FACodec [5].\nThe rest of the paper is constructed as follows: Section 2 summarizes the two baselines that we used for our submissions, Section 3 introduces all the modifications and proposed techniques used in our experiments, Section 4 provides the detailed results and the systems that we submitted for evaluation, Section 5 concludes our findings. Furthermore, we provide the summary tables with a description of submitted systems in Appendix A."}, {"title": "2. Baseline Systems", "content": "In this section, we summarize two baseline systems, B3 and B5, that we use in our experiments. A modified version of B3 is used to create submission for the first condition (EER1) with EER between 10% and 20% and the second condition (EER2) with EER between 20% and 30%, while a modified version of B5 is used for the third condition (EER3) with EER between 30% and 40% and the last condition (EER4) with EER larger than 40%. The details about all proposed changes are laid out in Section 3."}, {"title": "2.1. B3", "content": "The baseline system B3 uses a Wasserstein generative adversarial network with Quadratic Transport Cost (WGAN-QC) [6] to generate artificial pseudo-speaker embeddings, anonymizing the speaker's identity through four main steps:\n1. Phonetic Transcriptions Extraction: Phonetic transcriptions are extracted using an end-to-end automatic speech recognition (ASR) model with a hybrid CTC-attention architecture.\n2. Speaker Embeddings Speaker embeddings are obtained using an adapted Global Style Tokens (GST) model [7].\n3. Anonymization: The original speaker embedding is swapped with an artificial one generated by a WGAN. If the cosine distance between the artificial and original embeddings is less than 0.3, the replacement is considered successful. Otherwise, the process is repeated up to 30 times. Additionally, the pitch and energy values for each phoneme are adjusted using random values between 0.6 and 1.4.\n4. Speech Synthesis: The anonymized speaker embedding, adjusted prosody, and original phonetic transcription are used to create anonymized speech using the FastSpeech2 model and HiFi-GAN [8] vocoder, as implemented in IMS-Toucan [9]."}, {"title": "2.2. B5", "content": "The B5 system used a HiFi-GAN model conditioned on fundamental frequency and a linguistic representation of the source utterance along with speaker embedding of a designated speaker to generate anonymized speech.\n1. Fundamental Frequency (F0): B5 uses a pytorch implementation of YAAPT Pitch Tracking [10] to extract FO from speech. In authors' of B5 thesis [3], the original authors of B5 suggest several complementary normalization or transformations to be applied to F0, none of which are included in B5.\n2. Linguistic Representation: B5 uses the output of a vector quantization bottleneck layer (VQ-BN) put the top of the acoustic model (AM) of an automatic speech recognition (ASR) trained to classify left-biphone as the linguistic representation.\n3. Speaker Embedding: a designed speaker embedding of a speaker included in the training stage is used to change the voice of anonymized speech. We randomly pick a speaker embedding to anonymize each utterance similar to the B5 baseline provided by the organizer.\nWe use the same pre-trained B5 model provided by organizers without doing any further training or tuning. Instead, we introduce a new method of transformation that can be applied to FO"}, {"title": "3. Our Methods", "content": "In this subsection, we elaborate on the details of proposed modifications to the baseline systems."}, {"title": "3.1. Modifications of B3", "content": "We experimented with the following main modifications for this system is as follows:\n\u2022 Emotion embeddings are implemented as an additional input to the FastSpeech2 model.\n\u2022 The Global Style Tokens (GST) model is replaced by different speaker embedders such as WavLM [11] and ECAPA2 [12].\n\u2022 Some experiments related to speaker selection strategy for anonymization and prosody manipulation.\nWe start off with emotion embeddings. For extracting emotion-based embeddings, we employ a fine-tuned Wav2Vec2 Large Robust model [13] on MSP-Podcast [14]. Notably, the model is pruned from 24 to 12 transformers, and the CNN component is frozen prior to fine-tuning. Embeddings are extracted from the hidden layer, which has 1024-dimensional vectors as output. We employ it to FastSpeech2 in the same way as speaker embeddings in [15] by adding one more linear projection and concatenating it with an output from Conformer [16].\nIn addition to the GST model, we implement different speaker embedding models such as ECAPA2 and WavLM with 128 and 512 embedding sizes correspondingly. As both these speaker embedders work on audios instead of spectrograms, we add a pre-trained HiFi-GAN to the setup for the second phase of FastSpeech2 training in order to generate audio and extract embeddings for cycle consistency loss. In contrast to the GST model, ECAPA2 and WavLM speaker embedders are frozen during FastSpeech2 training. Similarly, the HiFi-GAN is also frozen.\nFurthermore, we explore various anonymization strategies, such as random speaker selection, which involves replacing the source speaker embedding for each utterance with a randomly selected embedding from a pool of embeddings. Additionally, we evaluated the importance of the usage of cross-gender for anonymization for the modified model. In cross-gender anonymization technique, we select a target speaker from the pool that has the opposite gender with respect to the source speaker. Finally, we examine how different powers of prosody anonymization affect privacy-based and utility-based metrics. To be more specific, we experiment with different offsets for pitch and energy multipliers.\nThe training process is the same as for the B3 baseline system. We retrain each part of the system except HiFi-GAN and ASR model."}, {"title": "3.2. Disentanglement-based models", "content": "Next, we explore disentanglement-based models, which might be useful for removing speaker-related information from other components (such as prosody, content, and acoustic information). We compare two models: B-VAE [4] and NaturalSpeech3 FACodec [5]. Anonymization for these models is implemented by passing anonymized speaker embedding instead of source embedding and then performing voice conversion."}, {"title": "3.3. Mean Reversion F0 for the B5 system", "content": "For conditions EER3 and EER4, we base our submissions on the B5 [3] system provided by the organizer. Champion proposes a voice anonymization model using a HiFi-GAN vocoder which takes in the FO and the linguistic representation of a source utterance and then turns it into the speech with the voice of a target speaker. In his thesis, the author explores different transformation techniques that can be applied to FO including linear transformation, Additive White Gaussian Noise, and quantization. In this work, we propose a new type of transformation that uses the original and the n-frame moving average FO (Fo), with n=32 in our calculation:\n$F_o = (1 - a) F_o + a\\overline{F_o}$", "latex": ["F_o = (1 - a) F_o + a\\overline{F_o}"]}, {"title": "4. Experiments", "content": "In this section, we provide experimental results for the system modifications explained in Section 3. To begin, we present the results for different modifications of system B3. The modified system B3 is illustrated in Figure 1."}, {"title": "4.1. Modified B3", "content": "From the results in 1, it can be observed that emotion embeddings help to improve Emotion Recognition performance while maintaining ASR performance at the same level. However, there is some degradation in privacy, which might be due to speaker identity leakage in the emotion embeddings. In addition, we provide experimental results for this system without prosody anonymization to check how modifications of prosody affect SER performance. As shown in the results, removing prosody modifications improves SER and ASR but also reduces privacy, making this system suitable for condition with a minimum EER1 = 10%.\nTable 2 shows a comparison between WGAN and Random-Speaker anonymization techniques. There is almost no difference in the privacy and utility metrics for these methods, so we chose to stick with Random-Speaker as it requires no training."}, {"title": "4.2. Disentanglement-based models", "content": "The comparison results between B-VAE and NaturalSpeech3 FACodec are shown in Table 4. It can be seen that B-VAE performs poorly in utility-based tasks, likely because of the fact that content representations are not rich enough.\nAs one might notice from the results in Table 4, NaturalSpeech3 has decent utility results. Therefore, we decided to employ anonymization techniques to improve privacy protection for NS3, aiming to meet a condition with minimum EER1 = 10%. We experimented with the following tricks: Additive White Gaussian Noise (AWGN) to Speaker Embedding and conversing a source speaker to a target speaker of the opposite gender (cross-gender). The results are shown in Table 5. As we can see, cross-gender conversion helps to improve privacy and ASR performance on the corresponding test sets. Interestingly, it also improves SER performance on both development and test sets. As expected, AWGN enhances privacy at the cost of utility."}, {"title": "4.3. Modified B5", "content": "Table 7 lists the results of the Mean Reversion FO method discussed in Section 3.3 given different a values. We can see a general trend that EER increases when a is increased while UAR and WER fluctuate but not very significantly. We submitted the sample generated with a = 0.75 for the condition EER3.\nFor the last condition EER4, we add a 10-db AWGN to the mean reversion F0 with a = 0.75 and manage to obtain an EER above 40%. The result can be found in Table 8.\nWe note that the EER results of these systems were highly volatile during our experiments, often producing different results even if we ran with the same configuration. It seems that convergence of an attacker ASV model depends on factors such as the machine, GPU, randomly picked speaker embedding, and other random parameters. The systems that we selected for submissions were based on the results available at that time and represented our methods of Mean Reversion FO and AWGN."}, {"title": "4.4. Submitted systems", "content": "In this subsection, we provide a summary of all submitted systems. Table 8 shows privacy and utility results for each of the conditions."}, {"title": "5. Conclusion", "content": "In this system description, we modified the baseline systems (B3 and B5) for the Voice Privacy Challenge 2024 to enhance speaker anonymization while preserving emotional and content features. Specifically, we integrated emotion embeddings and different speaker embedders such as WavLM and ECAPA2 into system B3. In addition, we explored random-speaker and cross-gender anonymizations and different setups of prosody manipulation. For B5, we introduced Mean Reversion method and AWGN for prosody which allowed us to enhance privacy while maintaining utility. Finally, we experimented with disentanglement-based approaches such as \u1e9e-VAE and Natural-Speech3. An additional analysis on NaturalSpeech3 showed promising results for the Voice Privacy problem."}]}