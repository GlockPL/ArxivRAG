{"title": "HARP: A Large-Scale Higher-Order Ambisonic Room Impulse Response Dataset", "authors": ["Shivam Saini", "J\u00fcrgen Peissig"], "abstract": "This contribution introduces a dataset of 7th-order Ambisonic Room Impulse Responses (HOA-RIRs), created using the Image Source Method. By employing higher-order Ambison-ics, our dataset enables precise spatial audio reproduction, a critical requirement for realistic immersive audio applications. Leveraging the virtual simulation, we present a unique microphone configuration, based on the superposition principle, designed to optimize sound field coverage while addressing the limitations of traditional microphone arrays. The presented 64-microphone configuration allows us to capture RIRs directly in the Spherical Harmonics domain. The dataset features a wide range of room configurations, encompassing variations in room geometry, acoustic absorption materials, and source-receiver distances. A detailed description of the simulation setup is provided alongside for an accurate reproduction. The dataset serves as a vital resource for researchers working on spatial audio, particularly in applications involving machine learning to improve room acoustics modeling and sound field synthesis. It further provides a very high level of spatial resolution and realism crucial for tasks such as source localization, reverberation prediction, and immersive sound reproduction.", "sections": [{"title": "I. INTRODUCTION", "content": "Spatial audio reproduction plays a vital role in enhancing user immersion in modern applications such as virtual reality (VR), augmented reality (AR), and interactive environments. The development of precise methods for capturing and reproducing sound fields is essential for accurately conveying the spatial characteristics of acoustic environments. Higher-order ambisonics (HOA) has emerged as a leading technique for this purpose, allowing for increased spatial resolution and accurate 3D audio rendering through spherical harmonic encoding [1].\nHowever, the availability of high-quality, large-scale datasets specifically designed for HOA-based room impulse responses (HOA-RIRs) is limited. Most current datasets either focus on first-order Ambisonics failing to capture the complex sound interactions in real-world environments or does not contain enough variation of rooms and configurations for statistical analysis and machine learning based tasks. To address these limitations, we present a dataset of 7th-order HOA RIRs, generated using the Image Source Method (ISM), which simulates relatively more realistic room acoustics for a variety of room configurations.\nThis paper describes the methodology for generating the proposed dataset HARP, including the microphone arrangement, simulation parameters, and spherical harmonic decomposition techniques used. The dataset provides an essential resource for researchers working in spatial audio, room acoustics, and machine learning, enabling the development of new algorithms for sound field analysis and immersive audio reproduction. We provide a rich, diverse resource for training and evaluating machine learning models that look over real-world reverberation applications such as Room parameter estimation [2]-[5], Dereverberation [6], Immersive Music Generation [7], Spatial Audio Compression [8], Reverberation Synthesis [9], Spatial Upsampling [10], Sound Event Localization and Detection Tasks [11] etc. One major contribution of this research is the implementation of a 7th-Order Ambisonics microphone configuration in Pyroomacoustics library [12]."}, {"title": "II. RELATED WORK", "content": "Room Impulse Responses (RIRs) are fundamental to room acoustics, providing a detailed characterization of how sound propagates within an enclosed space. Traditional methods of RIR generation often rely on either measured responses in real rooms or computational methods such as the imagesource method or ray tracing. These approaches have been used extensively in both acoustic modelling and spatial audio applications.\nOn the other hand, Higher-order Ambisonics (HOA) has gained significant traction in recent years as a means of capturing and reproducing 3D audio scenes [13]. HOA with the use of Spherical Harmonics (SH) provides a more detailed description of the sound field, making it more useful when rendering for immersive applications [14]. However, there's a shortage of existing HOA Impulse Responses datasets due to the computational cost and efforts involved.\nMost existing multi-channel datasets, however, are limited in scale (few room configurations) and often employ microphone arrays that do not provide optimal spatial coverage. FOr instance, traditional ambisonics datasets are capturedd using first-order microphones, which can lead to inaccuracies. This limitation arises from the finite number of transducers used to reproduce a physical sound field, resulting in issues such as localization blur [15], reduced lateralization [16], and comb-filtering spectral artefacts [17]. Our approach addresses this gap by generating a dataset of synthetic 7th-order Ambisonic RIRs using superpositioned microphone placement which ensures uniform spatial sampling and enhances the accuracy of spherical harmonic-based sound field encoding.\nTo our knowledge, this is the first large-scale HOA-RIR dataset, contributing both a novel dataset and a new method for spatial sampling in the context of room acoustics. Our dataset aims to support a variety of signal processing applications, including spatial audio rendering, room acoustics modelling, and machine learning-based sound field reconstruction. A comparison to the existing RIR dataset is given later in Discussion and Table I."}, {"title": "III. METHODOLOGY", "content": "We apply N3D normalization for the spherical harmonic coefficients, which ensures that the spherical harmonics are orthonormal. The normalization factor is:\n$\\displaystyle N_n^m = \\sqrt{\\frac{(2n + 1) \\frac{(n - m)!}{4\\pi}}{(n + m)!}}$\nUsing these spherical harmonics, we can decompose the sound field into its HOA components up to the 7th order. For each microphone position $r_m$, the signal can be expressed as a weighted sum of the spherical harmonic functions:\n$\\displaystyle p(r_m,t) = \\sum_{n=0}^{N}\\sum_{m=-n}^{n} c_n^m(t) Y_n^m(\\theta_m, \\varphi_m)$\nwhere $c_n^m(t)$ are the HOA coefficients.\nTo allow pyroomacoustics to be able to simulate the RIRs, the Real Spherical Harmonics $Y_{n,m}$ are derived from the Complex Spherical Harmonics $Y_n^m$ and are defined as follows:\n$\\displaystyle Y_{n,m}(\\theta, \\varphi) = \\begin{cases} \\sqrt{2}(-1)^mIm\\left(Y_n^m(\\theta,\\varphi)\\right), & m<0 \\\\ Y_0^n (\\theta, \\varphi), & m=0 \\\\ \\sqrt{2}(-1)^mRe\\left(Y_n^m(\\theta,\\varphi)\\right), & m > 0 \\end{cases}$\nwhere $\\theta$ is the colatitude, $\\varphi$ is the azimuth, and $Y_n^m(\\theta,\\varphi)$ are the complex spherical harmonics $Y_n^m(\\theta,\\varphi)$. The derived microphone directivities are given in Figure 1 (up to 4th order). The free field measurement of the proposed microphone configuration is given in Figure 2."}, {"title": "A. Microphone Configuration", "content": "To capture the sound field with a high spatial resolution, we propose a 64-channel 7th-order HOA framework. Traditional grid-based microphone arrays often suffer from spatial aliasing as mentioned earlier. We mitigate this by placing the microphones using a superposition arrangement, which ensures even sampling of the sound field. The simulation strategy is implemented in the Pyroomacoustics library for the ease of room acoustic simulation and automation.\nHowever, since Pyroomacoustics does not include Spherical Harmonics directivity, it was implemented from scratch. A new SphericalHarmonicDirectivity class was created in the Pyroomacoustics' directivities framework which takes the order and degree of the microphone as the input and generates the directivity response compatible with the Pyroomacoustics' room simulation framework.\nEach microphone's directivity is decomposed into spherical harmonics using the following spherical harmonic function $Y_n^m(\\theta,\\varphi)$, where n is the order and m is the degree:\n$\\displaystyle Y_n^m(\\theta,\\varphi) = \\sqrt{\\frac{(2n+1) \\frac{(n - m)!}{4\\pi}}{(n+m)!}} P_n^m(cos\\theta)e^{im\\phi}$\nHere, $P_n^m$ represents the associated Legendre polynomial, $\\theta$ is the zenith angle (or colatitude), and $\\phi$ is the azimuth."}, {"title": "B. Room Simulation", "content": "Since the Pyroomacoustics library does not support Ray Tracing with microphone directivity, for room acoustic simulations we utilize the Image Source Method (ISM) [18]. This method models sound reflections in an enclosed space by treating the sound source and its reflections as a set of image sources. Each reflection is computed based on the geometry of the room and the absorption coefficients of its surfaces. We simulate each room with a very image source order (40) and a large variety of room environments, including room geometries, wall absorption coefficients, and source-receiver configurations. A point source and the HOA microphone were placed randomly within the room boundaries, and for each room configuration, the corresponding RIRs were computed for all 64 microphones using the method described. An example simulation can be seen in Figure 3 where 4 different sources and a HOA microphone are placed in a room."}, {"title": "C. Absorption Material Lookup Table", "content": "In simulating realistic room acoustics, selecting appropriate absorption coefficients for materials is essential. A practical approach to assigning acoustic properties is using a predefined lookup table of common materials based on their absorption characteristics across frequency bands. This lookup table spans a wide range of materials typically found on walls, floors, and ceilings, allowing for a more generalized and adaptable configuration compared to direct measurements in specific scenarios, such as curtains on the floor or highly uncommon materials in unrealistic placements.\nThe lookup table includes materials such as brickwork, ceramic tiles, and plasterboard for walls, while floor materials consist of options like carpet, concrete, and marble. Ceiling materials are selected from absorbers like fibre panels, as well as the harder reflective surfaces that are common in various environments. Each material is referenced by its type and associated location, ensuring accurate and realistic RIR generation across various room configurations. This method improves the flexibility of the room models and enhances the reproducibility of results in different acoustic conditions.\nThis lookup table allows for the realistic simulation of different room configurations, ensuring that the generated RIRs accurately reflect the acoustic properties of common listening environments."}, {"title": "D. Dataset Creation and Ambisonic Encoding", "content": "We simulate various room configurations, including small, medium, and large spaces with different absorption character-istics. The simulation involves:\n\u2022 Room Dimensions: Varied to include cuboid, L-shaped, and hexagonal geometries.\n\u2022 Absorption Materials: Different absorption coefficients are assigned to room surfaces, simulating materials such as carpets, curtains, and reflective surfaces chosen from a lookup table mentioned in the previous section.\n\u2022 Source-Receiver Positions: These are randomized to provide a diverse range of acoustic conditions. Each room consists 20 combinations of source and receiver positions.\nThe final dataset consists of 100,000 simulated 7th-order Ambisonics RIRs, each corresponding to a unique room configuration and source-microphone setup. Each RIR is stored in the AmbiX format (ACN/SN3D),which is widely compatible with spatial audio rendering engines. The metadata associated with each RIR including room dimensions, absorption properties, and source-receiver configurations are provided in the form of a .csv file. This comprehensive dataset is designed to support a wide range of research in spatial audio, from room acoustics modelling to machine learning-based sound field reconstruction."}, {"title": "IV. DISCUSSION", "content": "A comprehensive analysis of the provided dataset was performed to ensure its usability and practical application in various spatial audio and machine learning tasks. Figure 4 shows the RT60 distribution calculated using the Oth-order (Omnidirectional) microphone. The majority of Room Impulse Responses (RIRs) exhibit RT60 values within the range of 0.4 to 0.8 seconds. This range captures typical room reverberation characteristics in real-world environments, such as living rooms, offices, and lecture halls. These values ensure that the dataset is relevant for a broad range of acoustic conditions encountered in real-world applications. Furthermore, Figure 5 summarizes the distribution of RIRs based on RT60 values, room volumes, and average absorption coefficients. As shown, the dataset focuses on typical real-world scenarios, while also including edge cases to support more extreme acoustic conditions."}, {"title": "V. CONCLUSION", "content": "This paper presents HARP, a large-scale dataset of up to 7th-order Higher-Order Ambisonic Room Impulse Responses, generated using the Image Source Method and carefully curated to cover a wide variety of acoustic environments. HARP offers an vast amount of spatial RIR data, featuring 100,000 SRIR samples with diverse room configurations, absorption materials, and spatial characteristics. One of the key contributions of this dataset is the creation of a 64 7th-order spherical harmonic microphone configuration based on superposition principle which was used to synthesize the RIRs.\nDespite the high quality and large scale of the HARP dataset, several limitations remain. First, the dataset is generated using the Image Source Method, which, while efficient, may not capture certain real-world acoustic phenomena such as diffraction and scattering with the same precision as other methods like Boundary Element Methods (BEM). Moreover, while the dataset covers a wide range of room types, it could benefit from additional real-world objects such as chairs, tables etc. to simulate a more realistic sound field.\nFuture work will focus on expanding the HARP dataset in several directions. First, incorporating simulation of real-world objects to increase the dataset's robustness and provide valuable data for comparing synthetic and real-world acoustics. Additionally, future updates may involve adding dynamic environments where the room configuration or source-receiver positions change over time, catering to emerging research areas in dynamic sound field modelling.\nFinally, there is potential for using HARP in the machine learning domain. By using this dataset to train (pre-train) spatial audio models, future work could explore improving source separation, dereverberation, and spatial audio enhancement in real-world interactive systems. In summary, HARP provides an essential foundation for current and future research in spatial audio, room acoustics, and machine learning, with opportunities for further development and refinement."}]}