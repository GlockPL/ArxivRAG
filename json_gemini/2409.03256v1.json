{"title": "E2CL: Exploration-based Error Correction Learning\nfor Embodied Agents", "authors": ["Hanlin Wang", "Chak Tou Leong", "Jian Wang", "Wenjie Li"], "abstract": "Language models are exhibiting increasing ca-\npability in knowledge utilization and reasoning.\nHowever, when applied as agents in embodied\nenvironments, they often suffer from misalign-\nment between their intrinsic knowledge and\nenvironmental knowledge, leading to infeasi-\nble actions. Traditional environment alignment\nmethods, such as supervised learning on expert\ntrajectories and reinforcement learning, face\nlimitations in covering environmental knowl-\nedge and achieving efficient convergence, re-\nspectively. Inspired by human learning, we pro-\npose Exploration-based Error Correction Learn-\ning (E2CL), a novel framework that leverages\nexploration-induced errors and environmental\nfeedback to enhance environment alignment for\nLM-based agents. E2CL incorporates teacher-\nguided and teacher-free exploration to gather\nenvironmental feedback and correct erroneous\nactions. The agent learns to provide feedback\nand self-correct, thereby enhancing its adapt-\nability to target environments. Evaluations in\nthe Virtualhome environment demonstrate that\nE2CL-trained agents outperform those trained\nby baseline methods and exhibit superior self-\ncorrection capabilities.", "sections": [{"title": "1 Introduction", "content": "Language Models (LMs) are becoming increas-\ningly capable of knowledge utilization and reason-\ning across various knowledge-intensive tasks. This success motivates researchers to ap-\nply LMs to build LM-based agents in embodied\nenvironments, which similarly requires the use of\nreasoning and planning upon environmental knowl-\nedge. In this\ncase, LM-based agents are asked to plan appropri-\nate actions based on the given environmental in-\nformation and the history of actions already taken.\nHowever, the knowledge acquired by these LM-\nbased agents comes from general-purpose corpora"}, {"title": "2 Method", "content": "In this section, we introduce our framework, E2CL.\nThis framework equips LM-based agents with self-\nfeedback and self-correction capabilities, which\nenhances their ability to tackle tasks in new envi-\nronments."}, {"title": "2.1 Task Formulation", "content": "The LM-based embodied agent is asked to com-\nplete a set of tasks via interacting with a virtual en-\nviconment. The interaction between the agent and\nthe environment can be formalized as a partially\nobservable Markov decision process (POMDP)\n(Q, S, A, O,T,R) with instruction space Q, state\nspace S, action space A, observation space O, tran-\nsition function T : S \u00d7 A \u2192 S, and reward func-\ntion R : S \u00d7 A \u2192 [0, 1]. In our LM-based agent\nscenario, Q, A, O are subsets of language space.\nThe interaction process between the agent and\nthe environment is described as follows. Given a\nplanning instruction qp \u2208 Q that prompts the agent\nto plan for a task, the agent with parameter 0 gener-\nates the first action a1 ~ \\pi_{\\theta}(\\cdot|qp) \u2208 A according to\nits policy \\pi_{\\theta}. Each action at step t induces a trans-\nformation in the latent state space st \u2208 S. And the\nagent would face a new observation ot \u2208 O. Then\nthe agent would incorporate task instruction qp and"}, {"title": "2.2 Exploration-based Error Correction\nLearning", "content": "Our E2CL framework consists of three phases of\nlearning and exploration within the environment:\nthe pre-tuning phase, the exploration phase, and the\ntraining phase. In the pre-tuning phase, we equip\nthe agent with basic planning ability before explo-\nration. Then, in the exploration phase, the agent\ncollects exploration experience in the environment\nvia two complementary schemes, as shown in Fig-edge, we can simply let the agent freely execute its\npredicted plans and collect the trajectories. How-\never, when utilizing these trajectories, we need to\ncorrect the errors made by the agent. Since these\ntrajectories are newly generated, we do not have\nthe correct action data corresponding to the errors.\nAlthough one can use a more powerful LLM to\ncorrect these errors automatically, the quality of\nthe generated data is inevitably lower compared to\nexpert data. To balance data diversity and quality,\nwe propose a limited exploration scheme guided\nby expert trajectories, referred to as teacher-guided\nexploration (TGE). Correspondingly, we call the\naforementioned free exploration scheme teacher-\nfree exploration (TFE). These two schemes com-\nplement each other and enhance the diversity and\nquality of the collected experiences.\nSpecifically, for each expert's sub-trajectory\n(qp, jt) \u2208 Dp, the agent conducts TGE by exe-"}, {"title": "Pre-tuning Phase", "content": "To serve as the founda-\ntion for environmental exploration, we aim\nto empower LM-based embodied agents with\na basic planning capability. Given a dataset\nI = {(qp, in)}|I| with |I| task instructions and\nexpert trajectories where each trajectory has ni\nsteps, we first construct a planning dataset Dp\nby slicing each trajectory into sub-trajectories\nof varying lengths from 1 to ni. Formally,\nthe planning dataset Dp is defined as: Dp =\n{(qp, jt) | j\u012f \u2286 in ^ (qp, in) \u2208 I}.\nNotably, we sample a subset of Dp, denoted as"}, {"title": "Exploration Phase", "content": "Intuitively, to gather diverse\nexperiences that fully cover environmental knowl-"}, {"title": "Training Phase", "content": "After the above two phases,\nwe obtain the planning dataset Dp, the feedback\ndataset Df = Df TGE UDTFE, and the correction\ndataset D = D TGE UDTFE.\nNext, we train the agent to align with the environ-\nmental knowledge gathered from these datasets and\nto develop the ability to provide feedback and cor-\nrect its own errors. This is achieved by fine-tuning\nthe agent to minimize the following losses:\nLp(\u03b8) = E\u223cDp [\u2212 log \u03c0\u03c1(at | qp, jt\u22121)],\nLf(\u03b8) = E\u223cDf [\u2212 log \u03c0\u03b8(ft | qf, jt\u22121, \u00e2t)],\nLc(\u03b8) = E\u223cDc [\u2212 log \u03c0\u03c1(at | qc, jt\u22121, \u00e2t, ft)],\nLtotal(\u03b8) = Lp(\u03b8) + Lf(\u03b8) + Lc(\u03b8). (2)\nWe refer the reader to the pseudo-code of the\noverall E2CL process in Appendix F."}, {"title": "Algorithm 1: Speculative Inference", "content": "Input: \u03c0\u03bf: The policy of robot agent, VS:\nVirtualhome Simulator\nOutput: R: Execution result for each task\nwhile Step length less than threshold do\nGenerate initial action \u00e2t\nif The task is finished then\nIteration stops\nGenerate feedback ft for \u00e2t\nif \u00e2t is non-executable then\nGenerate correction action ac\nGenerate feedback ft for ac\nif ac is executable then\nL\u00e2c executed in VS\nelse\nL\u00e2t executed in VS\nExecution information recorded into R\nRenew to next time step\nreturn R: Execution result for each task"}, {"title": "2.3 Speculative Inference", "content": "To utilize the learned abilities in the training phase,\nwe propose speculative inference, a process of pre-\ndicting the error occurring ahead and correcting\nitself. This process reduces execution errors and\ngenerates correct action based on self-generated\nfeedback.\nTo be more precise, when given each test task\ninstruction qp, the agent initially predicts a ac-\ntion \u00e2t ~ \u03c0\u03bf(\u00b7qp, jt\u22121). However, this action \u00e2t\nwill not be executed immediately. The agent will\n'reflect' itself and generate an environment feed-\nback ft ~ \u03c0\u03bf(\u00b7|qf, jt\u22121, \u00e2t). If the agent believes\nthe initial action \u00e2t is executable, then this action\nwill be executed. Otherwise, the embodied agent\nwill correct this action \u00e2 and predict a new action\n\u00e2c ~ \u03c0\u03bf(\u00b7 qc, jt\u22121, \u00e2t, ft). Once the corrected ac-\ntion ac passes its own check, this action will be\nexecuted at this step and concatenated into trajec-\ntory jt\u22121. The above process is iterated until the\nagent assumes the task is finished or the total steps\nexceed the maximum threshold. The process of\nspeculative inference for each test task is shown in\nAlgorithm 1."}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Experimental Settings", "content": "We eval-\nuate our method in the VirtualHome Environ-"}, {"title": "3.2 Results", "content": "As shown in Table 1, our method outperforms\nboth prompting-based and tuning-based baseline\nmethods across multiple metrics, showing the su-\nperiority of our method.\nNotably, the prompting-based method signifi-\ncantly lags behind all tuning-based methods in dif-\nferent metrics. Despite this contradicts with the\nexperience that LLMs exhibit exceptional general\nreasoning capabilities, we observe that the actions\ngenerated by prompt-based methods, while seem-\ningly reasonable, fail to comply with the physical\nconstraints of the environment often. Regarding\ntuning-based baseline methods, our method demon-\nstrates significant improvements over BC in both\nseen and unseen tasks.\nMoreover, LWM and Plasma, which are also\nfed by expert planning data and can be seen as\naugmented versions of BC, only show a marginal\nincrease in performance. Compared to these BC-\nbased methods, the method utilizing failure data,\ni.e., Lema and NAT, demonstrates better perfor-\nmance. Taking a step further, we evolve this idea\nby training the agent to develop self-feedback and\nself-correction capabilities through its failure expe-\nriences. The results show that our method increases\nexecutability-related metrics by up to 15% and LCS\nby up to 10% compared with Lema and NAT. This\ndemonstrates that these two capabilities effectively\nenable the agent to align with the environment for\ntask-solving."}, {"title": "3.3 Ablation Study on Training Data", "content": "In this section, we explore the impact of the col-\nlected training data, i.e., feedback data Df and\ncorrection data De, on overall performance by ab-\nlating them in the training. During the inference\nphase, we employ speculative inference for all set-\ntings to ensure consistency. As shown in Table 2,\nwe observe that both Df and De are each benefi-\ncial for the agent, but lag behind the combination\nof them. We hypothesize that the improvement\nobserved when training with De is primarily due\nto the enhanced self-correction capability of the\nagent. However, the limited ability to generate\nhigh-quality action feedback hampers the effec-\ntiveness of self-correction during speculative infer-\nence, as demonstrated in Section 3.6. Compared to\nthe agent training without both Df and Dc, train-\ning with Dp and Df improves the performance by\ntraining to predict environmental feedback, which\nexplicitly aligns with environmental knowledge.\nHowever, the weak self-correction capability of\nthe agent constrains the agent from generating ex-\necutable and correct action in speculative infer-\nence, which is demonstrated in Section 3.5. In\nour method, we integrate both types of data, en-\nabling our agent to generate higher-quality action\nfeedback and exhibit stronger self-correction abili-\nties. This results in a substantial performance boost"}, {"title": "3.4 Analysis on Different Size of the Model", "content": "To investigate the impact of model size on perfor-\nmance, we train models of different sizes on seen\ntasks using both BC and our method, and eval-\nuated them on unseen tasks. Our results are in\nline with common experience, where larger models\nperform relatively better across all aspects, indicat-\ning that model scale significantly impacts perfor-\nmance. Moreover, it can also be observed that our\nmethod outperforms BC in both Affordance rate\nand LCS across models with different parameter\nsizes, which demonstrates that our method con-\nsistently provides superior performance regardless\nof model size. Notably, when using our method,\nsmaller models achieve performance surpassing\nlarger models using BC across all metrics. This\nfinding suggests that our method is able to release\nthe potential of small language models and lays the\nfoundation for building agents that work on edge\ndevices in the future."}, {"title": "3.5 Evaluation on Self-Correction Ability", "content": "We further evaluate the self-correction capability\nof our constructed agent. We conduct two different\nexperiment settings to validate the performance of\nthe agent. For seen tasks, we randomly select 100\nsamples from correction data. For unseen tasks,\nwe collect 100 correction data samples in a similar\nprocess to TGE. For comparison, we also evaluate\nthe prompting-based agent and the agent trained\nby BC. Since these two agents have both under-\ngone general instruction tuning, we instruct them\nto conduct self-correction off the shelf.\nAs shown in Figure 4, our method generates cor-\nrect corrected actions far more frequently than BC\nand prompting-based methods in both seen tasks\nand unseen tasks, which demonstrates our agent's\nstrong self-correction capability. The powerful self-\ncorrection capability reflects our agent can truly\nalign with the environment and generate correct\ncorrective actions that do not violate physical con-\nstraints. Furthermore, we can observe from Fig-\nure 4 that our agent generates correct actions at\na high proportion in both seen and unseen tasks.\nThis ensures a reliable self-correction process in\nspeculative inference."}, {"title": "3.6 Analysis on Speculative Inference", "content": "To analyze the contribution of speculative infer-\nence to overall performance, as well as to explore\nthe quality and effectiveness of self-generated feed-\nback, we conduct an analysis on speculative infer-\nence.\nFirstly, as shown in Table 3, we conduct three\nkinds of experiment settings and test their perfor-\nmance on unseen tasks. Employing speculative\ninference significantly improves the agent's exe-\ncutability and affordance rate. This shows that spec-"}, {"title": "3.7 Error Analysis", "content": "We also perform an error analysis to identify the\naspects where the agent constructed using our\nmethod outperforms BC. There are a total of\neight types of errors, which can be further clas-\nsified into grounding errors (object availability)\nand execution-related errors (others). The detailed\ndemonstration can be found in the Appendix B. As\nshown in Figure 6, we observe that all error types\ndecreased by more than 24%, with Over occupied\nerror showing the highest reduction rate of 94.4%.\nThis highlights the effectiveness of our method\nin reducing various types of errors, highlighting\nits comprehensiveness. For the two most frequent\ntypes of execution-related errors, unflipped boolean\nstate and agent proximity, our method achieves a\nreduction in error count by over 37% compared to\nBC, thereby demonstrating its effectiveness. Al-\nthough our method primarily aims to avoid execu-\ntion errors related to physical constraint and does\nnot specifically target grounding errors such as ob-\nject availability, the fact that it still reduces this\ntype of error demonstrates the generalizability of\nour method."}, {"title": "4 Related Work", "content": "LM-based Agent Nowadays, due to the increas-\ningly powerful generalization capabilities of lan-\nguage models, they are often regarded as the pol-\nicy function of agents to plan their behavior (Tan however, one is-\nsue is that there may be a misalignment between\nthe knowledge in the environment and the internal\nknowledge of the model. Consequently, a signifi-\ncant amount of work aims to ground the language"}, {"title": "Learning from Failure", "content": "After exploration, the\nagent would encounter failure in the past experi-\nence, which is assumed as negative samples. The\ntopic of learning from negative samples has increas-\ningly gained attention as an alternative approach to\nlearning solely from positive samples. Tradition-\nally, some studies aim to decrease the probability of\nnegative samples while increasing the probability\nof positive samples in order to achieve better perfor-Additionally, some works construct\ncorrecting dataset and tuning language models on\nthese data. Besides, there are other efforts\naimed at leveraging the comprehension abilities of\nlanguage models to widen the gap between pos-\nitive and negative samples. In our work, we\nsimilarly leverage the inherent understanding capa-\nbilities of language models and enhance the embod-\nied agent's learning from environmental feedback\non exploration errors, as well as its ability to self-\ncorrect."}, {"title": "5 Conclusion", "content": "In this work, we aim to align the embodied agent\nwith the environment to enhance its task-solving\nperformance. Firstly, we present E2CL, a novel\nframework that leverages exploration-induced er-"}, {"title": "Limitations", "content": "The baseline model for the robot agent constructed\nusing our method is a text-based model, meaning\nthe agent's observations are input in textual form.\nHowever, there is a gap between textual descrip-\ntions of real-world visual images and the actual\nvisual information, which cannot fully encapsulate\nall real-world details. This discrepancy affects the\nrobot agent's ability to ground itself in the environ-\nment. In future work, we aim to incorporate visual\ninformation directly into the input to better align\nwith real-world scenarios. Additionally, although\nVirtualHome is a relatively com-\nplex environment, we have not conducted experi-\nmental validation in other embodied environments\nor the real world. In the future, we will perform\nmore experiments for validation."}, {"title": "Ethical Considerations", "content": "This work aims to construct a robot agent within\nVirtual Environment. The virtual environment\nsetup and related data strictly follow the specifi-\ncations of VirtualHome. We\nrefer to VirtualHome v2.3.0 to conduct out our ex-\nperiments (MIT license). The models, i.e. flan-t5-"}]}