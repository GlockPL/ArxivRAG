{"title": "E2CL: Exploration-based Error Correction Learning\nfor Embodied Agents", "authors": ["Hanlin Wang", "Chak Tou Leong", "Jian Wang", "Wenjie Li"], "abstract": "Language models are exhibiting increasing capability in knowledge utilization and reasoning. However, when applied as agents in embodied environments, they often suffer from misalignment between their intrinsic knowledge and environmental knowledge, leading to infeasible actions. Traditional environment alignment methods, such as supervised learning on expert trajectories and reinforcement learning, face limitations in covering environmental knowledge and achieving efficient convergence, respectively. Inspired by human learning, we propose Exploration-based Error Correction Learning (E2CL), a novel framework that leverages exploration-induced errors and environmental feedback to enhance environment alignment for LM-based agents. E2CL incorporates teacher-guided and teacher-free exploration to gather environmental feedback and correct erroneous actions. The agent learns to provide feedback and self-correct, thereby enhancing its adaptability to target environments. Evaluations in the Virtualhome environment demonstrate that E2CL-trained agents outperform those trained by baseline methods and exhibit superior self-correction capabilities.", "sections": [{"title": "Introduction", "content": "Language Models (LMs) are becoming increasingly capable of knowledge utilization and reasoning across various knowledge-intensive tasks (Yao et al., 2022; Lewkowycz et al., 2022; Hao et al., 2023). This success motivates researchers to apply LMs to build LM-based agents in embodied environments, which similarly requires the use of reasoning and planning upon environmental knowledge (Li et al., 2022; Xiang et al., 2024). In this case, LM-based agents are asked to plan appropriate actions based on the given environmental information and the history of actions already taken. However, the knowledge acquired by these LM-based agents comes from general-purpose corpora\nduring pre-training, and as a result the intrinsic knowledge of these models often misalign with environmental knowledge. Such environmental knowledge involves physical constraints that LMs have not yet explored. For example, if the embodied agent holds two objects, it is prohibited to grab one more other object. This misalignment causes LM-based agents to frequently infer actions that cannot be executed in the environment, hindering their application in real-world environments.\nTo address this issue, two primary types of environment alignment methods have been explored. The first type involves having LM-based agents undergo supervised learning on expert trajectories, which are human-labeled sequences of observations and actions (Li et al., 2022; Chen et al., 2023). Nevertheless, these trajectories often fail to fully"}, {"title": "", "content": "cover the knowledge within the environment, such\nas scenarios where certain actions cannot be exe-\ncuted. The second type is based on reinforcement\nlearning, which allows agents to freely explore the\nenvironment, collect trajectories that comprehen-\nsively cover the environment\u2019s knowledge, and ob-\ntain rewards based on these trajectories\u2019 success or\nfailure (Tan et al., 2024; Carta et al., 2023). How-\never, the rewards are sparsely obtained because\nthe performance evaluation of the agent is based\non a complete trajectory. This makes the learning\nprocess difficult to converge.\nHuman learning is not comprehensive or effi-\ncient if it relies solely on imitating experts\u2019 behav-\nior or merely knowing whether an action is correct.\nInstead, by collecting and understanding feedback\nfrom the environment via exploration and learning\nto correct errors based on the feedback, humans\ncan learn comprehensively and efficiently. Inspired\nby this, we propose a novel exploration framework\nfor LM-based agents to align with environments,\nwhich is called Exploration-based Error Correc-\ntion Learning (E2CL). As depicted in Figure 1,\nour framework incorporates exploration-induced er-\nrors and environmental feedback, leading to a com-\nprehensive alignment with target environments.\nDuring the process of E2CL, we ask a pretrained\nagent to perform predefined tasks and explore the\nenvironment to collect experiences in both efficient\nand comprehensive manners. This is achieved by\ntwo different proposed schemes, namely teacher-\nguided exploration and teacher-free exploration.\nThe former prompts the agent to perform one-\nstep exploration given sliced expert trajectories,\nwhereas the latter allows the agent to continue ex-\nploring until it infers a stop. In these two explo-\nration phases, we collect the feedback given by the\nenvironment when the agent makes errors, as well\nas the correct actions corresponding to these error\nactions. Having these exploration trajectories with\nadditional correction, we train the agent to provide\nfeedback on their trajectories and correct their error\nactions based on the feedback. To apply learned\nself-correction ability, we further propose Specu-\nlative Inference, which performs corrections if the\ninitial planned actions are inferred to be errors by\nthe agent\u2019s feedback.\nWe evaluate the agent trained by E2CL in Vir-\ntualhome (Puig et al., 2018), a household embod-\nied environment. E2CL-trained agent surpasses\nthe agents trained by other baseline methods in\nall agentic metrics, demonstrating its effective-"}, {"title": "Method", "content": "ness. Furthermore, our analysis reveals that the\nsmall models constructed using our method out-\nperform larger models of the same series that have\nonly undergone behavior cloning. In addition, in\nevaluations based on feedback-driven re-planning,\nour models demonstrate self-correction capabilities\nthat are comparable to LLMs.\nIn summary, our main contributions are as fol-\nlows. (1) We introduce the Exploration-based Er-\nror Correction Learning (E2CL) framework, en-\nabling LM-based agents to align with environments\nthrough effective feedback-driven exploration and\ncorrection. (2) We propose two novel exploration\nschemes, teacher-guided and teacher-free explo-\nration, that facilitate the collection of correction\nand feedback via agent-environment interaction.\n(3) We introduce a novel action inference algorithm,\nnamely speculative inference, which can prevent\nexecutable errors from occurring. (4) We demon-\nstrate the superior performance of E2CL-trained\nagents in the Virtualhome environment, surpassing\nbaseline methods and showcasing the potential of\nour approach for real-world deployment.\nIn this section, we introduce our framework, E2CL.\nThis framework equips LM-based agents with self-\nfeedback and self-correction capabilities, which\nenhances their ability to tackle tasks in new envi-\nronments."}, {"title": "Task Formulation", "content": "The LM-based embodied agent is asked to com-\nplete a set of tasks via interacting with a virtual en-\nvironment. The interaction between the agent and\nthe environment can be formalized as a partially\nobservable Markov decision process (POMDP)\n(Q, S, A, O,T,R) with instruction space Q, state\nspace S, action space A, observation space O, tran-\nsition function T : S \u00d7 A \u2192 S, and reward func-\ntion R : S \u00d7 A \u2192 [0, 1]. In our LM-based agent\nscenario, Q, A, O are subsets of language space.\nThe interaction process between the agent and\nthe environment is described as follows. Given a\nplanning instruction qp \u2208 Q that prompts the agent\nto plan for a task, the agent with parameter \u03b8 gener-\nates the first action a1 ~ \u03c0\u03b8(\u00b7|qp) \u2208 A according to\nits policy \u03c0\u03b8. Each action at step t induces a trans-\nformation in the latent state space st \u2208 S. And the\nagent would face a new observation ot \u2208 O. Then\nthe agent would incorporate task instruction qp and"}, {"title": "Exploration-based Error Correction\nLearning", "content": "interaction trajectories jt = (a1, 01, ..., at, Ot) to\ngenerate the next action at+1 ~ \u03c0\u03bf(\u00b7|qp, jt). The\ninteraction loop repeats until the agent assumes the\ntask is finished or the number of steps exceeds the\nmaximum steps.\nOur E2CL framework consists of three phases of\nlearning and exploration within the environment:\nthe pre-tuning phase, the exploration phase, and the\ntraining phase. In the pre-tuning phase, we equip\nthe agent with basic planning ability before explo-\nration. Then, in the exploration phase, the agent\ncollects exploration experience in the environment\nvia two complementary schemes, as shown in Fig-\nure 2. Following this, in the training phase, the\nagent is trained to align with the environmental\nknowledge from the collected experience and to\ndevelop the ability to provide feedback and correct\nits own errors."}, {"title": "Pre-tuning Phase", "content": "To serve as the founda-\ntion for environmental exploration, we aim\nto empower LM-based embodied agents with\na basic planning capability. Given a dataset\nI = {(qp, in)}|I| with |I| task instructions and\nexpert trajectories where each trajectory has ni\nsteps, we first construct a planning dataset Dp\nby slicing each trajectory into sub-trajectories\nof varying lengths from 1 to ni. Formally,\nthe planning dataset Dp is defined as: Dp =\n{() | j\u012f \u2286 in ^ (qp, in\u2081) \u2208 I}.\nNotably, we sample a subset of Dp, denoted as"}, {"title": "", "content": "Dp', for pre-tuning to avoid overfitting to expert\ntrajectories and maintain exploration diversity.\nThen, we fine-tune the LM-based agent by\nminimizing neg-likelihood loss:\n$L(\\theta) = \\mathbb{E}_{\\sim D_{p'}}, [-\\log \\pi_{\\theta} (a_t | (q_p, j_{t-1}))]$. (1)\nNotably, at consists of multiple tokens. There-\nfore, when calculating the loss, it effectively be-\ncomes an auto-regressive loss over a sequence of to-\nkens, following previous practices. This approach\nis consistently applied in the latter stages of train-\ning as well."}, {"title": "Exploration Phase", "content": "Intuitively, to gather diverse\nexperiences that fully cover environmental knowl-\nedge, we can simply let the agent freely execute its\npredicted plans and collect the trajectories. How-\never, when utilizing these trajectories, we need to\ncorrect the errors made by the agent. Since these\ntrajectories are newly generated, we do not have\nthe correct action data corresponding to the errors.\nAlthough one can use a more powerful LLM to\ncorrect these errors automatically, the quality of\nthe generated data is inevitably lower compared to\nexpert data. To balance data diversity and quality,\nwe propose a limited exploration scheme guided\nby expert trajectories, referred to as teacher-guided\nexploration (TGE). Correspondingly, we call the\naforementioned free exploration scheme teacher-\nfree exploration (TFE). These two schemes com-\nplement each other and enhance the diversity and\nquality of the collected experiences.\nSpecifically, for each expert\u2019s sub-trajectory\n(qp, jt) \u2208 Dp, the agent conducts TGE by exe-"}, {"title": "", "content": "cuting the action at ~ \u03c0\u03bf(\u00b7|qp, jt\u22121). The en-\nvironment then provides feedback ft indicating\nthe executability of this action. Since the agent\nonly performs one step of exploration under the\nguidance of the expert, we can naturally use at\nas the ground truth action for that step. After\ntraversing all the expert trajectories, we obtain the\nfeedback dataset DTGE consisting of samples in\nthe form of (qf, \u00cdt\u22121, \u00e2t, ft), and the correction\ndataset DTGE consisting of samples in the form of\n(qc, jt-1, \u00e2t, ft, at) where \u00e2t \u2260 at, qf is the instruc-\ntion that prompts the model to generate feedback,\nand qc is the instruction that prompts the model\nto correct errors. Please refer to Appendix A for\ntemplates of these samples.\nDuring TFE, the agent iterates through each task\ninstruction qp ~ Q and accrodingly obtain trajec-\ntories jt = (a1, \u00f41,..., \u00e2t, \u00f4t). Similar to TGE,\nwhenever the agent predicts a non-executable \u00e2t,\nthe environment provides feedback ft indicating\nwhy this action is non-executable. To obtain the\nexecutable action at this step without manual inter-\nvention, we leverage an LLM with powerful reason-\ning ability (e.g., GPT-40) to automatically correct\nthe action, yielding at. Considering the LLM may\nnot always provide perfect corrections due to a lack\nof environment alignment, we further filter its pre-\ndictions to ensure the corrections are executable.\nAs a result, we obtain the feedback dataset DFFE\nand DTFE, each in the same form as the samples in\nDIGE and DTGE, respectively."}, {"title": "", "content": "Training Phase After the above two phases,\nwe obtain the planning dataset Dp, the feedback\ndataset Df = DIGE UDTFE, and the correction\ndataset D = DTGE UDTFE.\nNext, we train the agent to align with the environ-\nmental knowledge gathered from these datasets and\nto develop the ability to provide feedback and cor-\nrect its own errors. This is achieved by fine-tuning\nthe agent to minimize the following losses:\n$L_p(\\theta) = \\mathbb{E}_{\\sim D_p} [-\\log \\pi_{\\theta}(a_t | q_p, j_{t-1})]$,\n$L_f(\\theta) = \\mathbb{E}_{\\sim D_f} [-\\log \\pi_{\\theta}(f_t | q_f, j_{t-1}, \\hat{a}_t)]$,\n$L_c(\\theta) = \\mathbb{E}_{\\sim D_c} [-\\log \\pi_{\\theta}(a_t | q_c, j_{t-1}, \\hat{a}_t, f_t)]$,\n$L_{total}(\\theta) = L_p(\\theta) + L_f(\\theta) + L_c(\\theta)$. (2)\nWe refer the reader to the pseudo-code of the\noverall E2CL process in Appendix F."}, {"title": "Speculative Inference", "content": "To utilize the learned abilities in the training phase,\nwe propose speculative inference, a process of pre-\ndicting the error occurring ahead and correcting\nitself. This process reduces execution errors and\ngenerates correct action based on self-generated\nfeedback.\nTo be more precise, when given each test task\ninstruction qp, the agent initially predicts a ac-\ntion at ~ \u03c0\u03bf(\u00b7qp, \u0130t\u22121). However, this action \u00e2t\nwill not be executed immediately. The agent will\n'reflect' itself and generate an environment feed-\nback ft ~ \u03c0\u03bf(\u00b7|qf, jt-1, \u00e2t). If the agent believes\nthe initial action \u00e2t is executable, then this action\nwill be executed. Otherwise, the embodied agent\nwill correct this action \u00e2 and predict a new action\n\u00e2c ~ \u03c0\u03bf(\u00b7 qc, \u00cdt-1, \u00e2t, ft). Once the corrected ac-\ntion ac passes its own check, this action will be\nexecuted at this step and concatenated into trajec-\ntory jt-1. The above process is iterated until the\nagent assumes the task is finished or the total steps\nexceed the maximum threshold. The process of\nspeculative inference for each test task is shown in\nAlgorithm 1."}, {"title": "Experiments", "content": "In this section, we introduce our framework, E2CL.\nThis framework equips LM-based agents with self-\nfeedback and self-correction capabilities, which\nenhances their ability to tackle tasks in new envi-\nronments."}, {"title": "Experimental Settings", "content": "VirtualHome Environment & Tasks We eval-\nuate our method in the VirtualHome Environ-\nment (Puig et al., 2018), a simulation platform for"}, {"title": "Evaluation Metrics", "content": "performing typical household activities.\nDuring the experiment, we are able to access\ndata, such as states of objects, observation of\nagents, and environmental feedback, from the sim-\nulator. We refer to Appendix D for more details of\nthe environment.\nWe use the predefined tasks from ActivityPro-\ngrams Knowledge Base (Puig et al., 2018) for\nthe experiment. It contains 292 unique high-level\nhousehold tasks, with 1374 unique action plans\nand 6201 unique environmental settings in total\nextracted from VirtualHome.\nAfter filtering low-quality tasks, we conduct ex-\nperiments on a total of 285 tasks. They are ran-\ndomly divided into a training set of 235 tasks and\na test set of 50 tasks. We select 50 tasks from the\ntraining set as seen tasks, while the 50 tasks in the\ntest set as unseen tasks. We evaluate the method on\nboth seen tasks and unseen tasks."}, {"title": "Baselines", "content": "We compare our method with both\nprompting-based methods and other tuning-based\nbaseline methods. Similar to our approach, tuning-\nbased methods achieve alignment between the em-\nbodied agent and the environment via model fine-\ntuning. (1) Language-planner (Huang et al., 2022)\naims to inject environment knowledge into prompt\nand prompted Large Language Models to output ac-\ntion. We utilize GPT-40 as the foundational model\nfor our baseline method. (2) We perform Behav-\nior Cloning (BC) on expert planning data (Chen\net al., 2023; Zeng et al., 2023), which is the same\nmethod used in the pre-tuning phase of our meth-\nods and other baselines. (3) LWM (Xiang et al.,\n2024) employs a robot agent to interact with the\nenvironment and collect a large amount of envi-\nronmental knowledge data to fine-tune the model.\n(4) Plasma (Brahman et al., 2023) leverages Chat-\nGPT to generate multi-task planning-related data\nfor model training. (5) Lema (An et al., 2023)\nenhances the agent's reasoning capabilities by pro-\nviding error-correction data pairs during model fine-\ntuning. (6) NAT (Wang et al., 2024b) implements\na negative-aware training approach, enabling LM-\nbased agents to effectively learn from both positive\nand negative examples.\nFollowing previous stud-\nies (Puig et al., 2018; Raman et al., 2022), we\nevaluate our action plans across three metrics: ex-\necutability (Exec.), affordance rate (AR), and\nlongest common sequence (LCS). Executability\nmeasures whether an action plan can be correctly"}, {"title": "", "content": "parsed and satisfies the common-sense constraints\nof the environment. Specifically, the parsed action\nmust contain only allowable action, and the objects\nmust be in the environment. Moreover, the action\nmust satisfy the pre-conditions (e.g., the embodied\nagent cannot send email before walking to the com-\nputer) and post-conditions (e.g., the state of TV\nchanges from closed to open after the agent opens\nit). Similarly to executablility, affordance rate mea-\nsures the average percentage of all plan steps that\nare executable, in cases where the entire plan is not\nexecutable. However, executability and affordance\nrate only can reflect whether the agent could com-\npliant with environment physical constraints, but\nthey cannot reflect whether the plan is correct. LCS\ncalculates the length of the longest common sub-\nsequence between generated plans and the ground\ntruth plans, normalized by the maximum length of\nthe two."}, {"title": "Results", "content": "As shown in Table 1, our method outperforms\nboth prompting-based and tuning-based baseline\nmethods across multiple metrics, showing the su-\nperiority of our method.\nNotably, the prompting-based method signifi-\ncantly lags behind all tuning-based methods in dif-\nferent metrics. Despite this contradicts with the\nexperience that LLMs exhibit exceptional general\nreasoning capabilities, we observe that the actions\ngenerated by prompt-based methods, while seem-\ningly reasonable, fail to comply with the physical\nconstraints of the environment often. Regarding\ntuning-based baseline methods, our method demon-\nstrates significant improvements over BC in both\nseen and unseen tasks.\nMoreover, LWM and Plasma, which are also\nfed by expert planning data and can be seen as\naugmented versions of BC, only show a marginal\nincrease in performance. Compared to these BC-\nbased methods, the method utilizing failure data,\ni.e., Lema and NAT, demonstrates better perfor-\nmance. Taking a step further, we evolve this idea\nby training the agent to develop self-feedback and\nself-correction capabilities through its failure expe-\nriences. The results show that our method increases\nexecutability-related metrics by up to 15% and LCS\nby up to 10% compared with Lema and NAT. This\ndemonstrates that these two capabilities effectively\nenable the agent to align with the environment for\ntask-solving."}, {"title": "Ablation Study on Training Data", "content": "In this section, we explore the impact of the col-\nlected training data, i.e., feedback data Df and\ncorrection data De, on overall performance by ab-\nlating them in the training. During the inference\nphase, we employ speculative inference for all set-\ntings to ensure consistency. As shown in Table 2,\nwe observe that both Df and De are each benefi-\ncial for the agent, but lag behind the combination\nof them. We hypothesize that the improvement\nobserved when training with De is primarily due\nto the enhanced self-correction capability of the\nagent. However, the limited ability to generate\nhigh-quality action feedback hampers the effec-\ntiveness of self-correction during speculative infer-\nence, as demonstrated in Section 3.6. Compared to\nthe agent training without both Df and Dc, train-\ning with Dp and Df improves the performance by\ntraining to predict environmental feedback, which\nexplicitly aligns with environmental knowledge.\nHowever, the weak self-correction capability of\nthe agent constrains the agent from generating ex-\necutable and correct action in speculative infer-\nence, which is demonstrated in Section 3.5. In\nour method, we integrate both types of data, en-\nabling our agent to generate higher-quality action\nfeedback and exhibit stronger self-correction abili-\nties. This results in a substantial performance boost"}, {"title": "Analysis on Different Size of the Model", "content": "compared to other ablation settings.\nTo investigate the impact of model size on perfor-\nmance, we train models of different sizes on seen\ntasks using both BC and our method, and eval-\nuated them on unseen tasks. Our results are in\nline with common experience, where larger models\nperform relatively better across all aspects, indicat-\ning that model scale significantly impacts perfor-\nmance. Moreover, it can also be observed that our\nmethod outperforms BC in both Affordance rate\nand LCS across models with different parameter\nsizes, which demonstrates that our method con-\nsistently provides superior performance regardless\nof model size. Notably, when using our method,\nsmaller models achieve performance surpassing\nlarger models using BC across all metrics. This\nfinding suggests that our method is able to release\nthe potential of small language models and lays the\nfoundation for building agents that work on edge\ndevices in the future."}, {"title": "Evaluation on Self-Correction Ability", "content": "We further evaluate the self-correction capability\nof our constructed agent. We conduct two different\nexperiment settings to validate the performance of\nthe agent. For seen tasks, we randomly select 100\nsamples from correction data. For unseen tasks,\nwe collect 100 correction data samples in a similar\nprocess to TGE. For comparison, we also evaluate\nthe prompting-based agent and the agent trained\nby BC. Since these two agents have both under-\ngone general instruction tuning, we instruct them\nto conduct self-correction off the shelf.\nAs shown in Figure 4, our method generates cor-\nrect corrected actions far more frequently than BC\nand prompting-based methods in both seen tasks\nand unseen tasks, which demonstrates our agent's\nstrong self-correction capability. The powerful self-\ncorrection capability reflects our agent can truly\nalign with the environment and generate correct\ncorrective actions that do not violate physical con-\nstraints. Furthermore, we can observe from Fig-\nure 4 that our agent generates correct actions at\na high proportion in both seen and unseen tasks.\nThis ensures a reliable self-correction process in\nspeculative inference."}, {"title": "Analysis on Speculative Inference", "content": "To analyze the contribution of speculative infer-\nence to overall performance, as well as to explore\nthe quality and effectiveness of self-generated feed-\nback, we conduct an analysis on speculative infer-\nence.\nFirstly, as shown in Table 3, we conduct three\nkinds of experiment settings and test their perfor-\nmance on unseen tasks. Employing speculative\ninference significantly improves the agent's exe-\ncutability and affordance rate. This shows that spec-"}, {"title": "Error Analysis", "content": "ulative inference effectively reduces errors during\nexecution, which demonstrates the effectiveness of\nthe design. Moreover, LCS has not changed regard-\nless of using speculative inference. This indicates\nthat speculative inference contributes to the perfor-\nmance gain mainly by generating more executable\nactions, instead of recovering the expert trajectories\nin the training data.\nNext, we provide our agent with self-generated\nfeedback as well as three other types of feedback,\nand test its performance on unseen tasks. As\nshown in Figure 5, given random feedback to\nthe agent, the agent performs worst in both affor-\ndance rate and LCS, which underscores the impor-\ntance of high-quality feedback. When fed with\nself-generated feedback, the agent performs better\nthan that of using random feedback and boolean ex-\necutability signals, while slightly worse than that of\nusing ground truth. This suggests that our method\nenables the agents to generate feedback with good\nquality. Overall, we can observe that feedback with\nbetter qualities yields a better performance, which"}, {"title": "Related Work", "content": "We also perform an error analysis to identify the\naspects where the agent constructed using our\nmethod outperforms BC. There are a total of\neight types of errors, which can be further clas-\nsified into grounding errors (object availability)\nand execution-related errors (others). The detailed\ndemonstration can be found in the Appendix B. As\nshown in Figure 6, we observe that all error types\ndecreased by more than 24%, with Over occupied\nerror showing the highest reduction rate of 94.4%.\nThis highlights the effectiveness of our method\nin reducing various types of errors, highlighting\nits comprehensiveness. For the two most frequent\ntypes of execution-related errors, unflipped boolean\nstate and agent proximity, our method achieves a\nreduction in error count by over 37% compared to\nBC, thereby demonstrating its effectiveness. Al-\nthough our method primarily aims to avoid execu-\ntion errors related to physical constraint and does\nnot specifically target grounding errors such as ob-\nject availability, the fact that it still reduces this\ntype of error demonstrates the generalizability of\nour method."}, {"title": "LM-based Agent", "content": "Nowadays, due to the increas-\ningly powerful generalization capabilities of lan-\nguage models, they are often regarded as the pol-\nicy function of agents to plan their behavior (Tan\net al., 2024; Carta et al., 2023). However, one is-\nsue is that there may be a misalignment between\nthe knowledge in the environment and the internal\nknowledge of the model. Consequently, a signifi-\ncant amount of work aims to ground the language"}, {"title": "Learning from Failure", "content": "model to the environment (Brohan et al., 2023; Fu\net al., 2024; Song et al., 2023). Some studies har-\nness the immense capabilities of large language\nmodels and employ intricate prompts or integrate\nspecifically designed modules (Huang et al., 2022,\n2024; Raman et al., 2022; Singh et al., 2023; Wang\net al., 2023; Guan et al., 2023). However, LLM-\nbased agents would cost heavily and are not suit-\nable for offline scenarios. Some line of work de-\nploys language model as decision-making agents\nto align with embodied environments via reinforce-\nment learning (Tan et al., 2024; Carta et al., 2023).\nThis type of approach tends to have low learning\nefficiency in embodied environments with large ac-\ntion spaces. In addition, similar to our approach,\nother research efforts have proposed frameworks\nwhere the agent first explores the environment and\nsubsequently utilizes the exploration experience for\nlearning (Li et al., 2022; Xiang et al., 2024). These\napproaches often overly focus on the agent and lack\ncomprehensive environmental feedback modeling,\nmaking it difficult to avoid execution errors.\nAfter exploration, the\nagent would encounter failure in the past experi-\nence, which is assumed as negative samples. The\ntopic of learning from negative samples has increas-\ningly gained attention as an alternative approach to\nlearning solely from positive samples. Tradition-\nally, some studies aim to decrease the probability of\nnegative samples while increasing the probability\nof positive samples in order to achieve better perfor-\nmance (Wang et al., 2024a; Zheng et al., 2023; Liu\net al., 2022). Additionally, some works construct\ncorrecting dataset and tuning language models on\nthese data (An et al., 2023; Wang et al., 2024b;\nBai et al., 2022). Besides, there are other efforts\naimed at leveraging the comprehension abilities of\nlanguage models to widen the gap between pos-\nitive and negative samples (An et al., 2023; Liu\net al., 2023; Zhang et al., 2023). In our work, we\nsimilarly leverage the inherent understanding capa-\nbilities of language models and enhance the embod-\nied agent's learning from environmental feedback\non exploration errors, as well as its ability to self-\ncorrect."}, {"title": "Conclusion", "content": "In this work, we aim to align the embodied agent\nwith the environment to enhance its task-solving\nperformance. Firstly, we present E2CL, a novel\nframework that leverages exploration-induced er-"}, {"title": "Limitations", "content": "rors and environmental feedback to enhance en-\nvironment alignment for LM-based agents during\nteacher-guided and teacher-free exploration. Fur-\nthermore, we introduce speculative inference, a\nprocess in which the agent utilizes learned abilities\nfor self-feedback and self-correction to reduce exe-\ncution errors. Extensive experiments show that our\nmethod outperforms behavior cloning and other\nbaseline methods.\nThe baseline model for the robot agent constructed\nusing our method is a text-based model, meaning\nthe agent's observations are input in textual form.\nHowever, there is a gap between textual descrip-\ntions of real-world visual images and the actual\nvisual information, which cannot fully encapsulate\nall real-world details. This discrepancy affects the\nrobot agent's ability to ground itself in the environ-\nment. In future work, we aim to incorporate visual\ninformation directly into the input to better align\nwith real-world scenarios. Additionally, although\nVirtualHome (Puig et al., 2018) is a relatively com-\nplex environment, we have not conducted experi-\nmental validation in other embodied environments\nor the real world. In the future, we will perform\nmore experiments for validation."}, {"title": "Ethical Considerations", "content": "This work aims to construct a robot agent within\nVirtual Environment. The virtual environment\nsetup and related data strictly follow the specifi-\ncations of VirtualHome (Puig et al., 2018). We\nrefer to VirtualHome v2.3.0\u00b9 to conduct out our ex-\nperiments (MIT license\u00b2). The models, i.e. flan-t5-\nsmall, flan-t5-base and flan-t5-large (Chung et al,\n2024), we use for fine-tuning are all open-source,\nand we will strictly follow the protocols for the\nacademic use of these language models (Apache\nLicense 2.03). In addition, we partially use AI as-\nsistants, such as Copilot and ChatGPT, to help with\nour coding and writing."}, {"title": "Pseudocode", "content": "This section presents the pseudocode of E\u00b2CL in\nAlgorithm 2. A detailed discussion of the method\nis given in Section 2.2."}, {"title": "Data Template", "content": "We show the data template of planning data (Dp),\nfeedback data (Df) and correction data (De). Each\npiece of data adheres to the specified format."}, {"title": "Illustration of Error Type", "content": "During the interaction between the agent and the\nenvironment, we collect error feedback from the\nenvironment and classify it into eight categories as\nfollowings.\nUnflipped Boolean State error occurs when an\naction meant to change the state of an object with\na Boolean attribute (such as open/closed or on/off)"}, {"title": "Length Analysis", "content": "does not achieve the intended effect, like attempt-\ning to open an already open door. Missing Object\nerror arises when the agent is not holding the nec-\nessary object to complete an action, preventing the\ntask's execution. Enclosed Object error involves\nthe target object being contained within a closed\nstructure, with the action failing to free the object\nfor use. Invalid Action error occurs when the agent\nattempts to perform an action on a target object\nthat is not afforded to it, such as trying to pull a\nceiling. Over-occupied Agent error happens when\nthe agent's hands are occupied or already interact-\ning with objects, leaving it unable to interact"}]}