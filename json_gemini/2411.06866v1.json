{"title": "Subgraph Retrieval Enhanced by Graph-Text Alignment for Commonsense Question Answering", "authors": ["Boci Peng", "Yongchao Liu", "Xiaohe Bo", "Sheng Tian", "Baokun Wang", "Chuntao Hong", "Yan Zhang"], "abstract": "Commonsense question answering is a crucial task that requires machines to employ reasoning according to commonsense. Previous studies predominantly employ an extracting-and-modeling paradigm to harness the information in KG, which first extracts relevant subgraphs based on pre-defined rules and then proceeds to design various strategies aiming to improve the representations and fusion of the extracted structural knowledge. Despite their effectiveness, there are still two challenges. On one hand, subgraphs extracted by rule-based methods may have the potential to overlook critical nodes and result in uncontrollable subgraph size. On the other hand, the misalignment between graph and text modalities undermines the effectiveness of knowledge fusion, ultimately impacting the task performance. To deal with the problems above, we propose a novel framework: Subgraph Retrieval Enhanced by GraPh-Text Alignment, named SEPTA. Firstly, we transform the knowledge graph into a database of subgraph vectors and propose a BFS-style subgraph sampling strategy to avoid information loss, leveraging the analogy between BFS and the message-passing mechanism. In addition, we propose a bidirectional contrastive learning approach for graph-text alignment, which effectively enhances both subgraph retrieval and knowledge fusion. Finally, all the retrieved information is combined for reasoning in the prediction module. Extensive experiments on five datasets demonstrate the effectiveness and robustness of our framework.", "sections": [{"title": "1 Introduction", "content": "Commonsense question answering (CSQA) is a critical task in natural language understanding, which requires systems to acquire different types of commonsense knowledge and possess multi-hop reasoning ability [27,19,22]. Though massive pre-trained models have achieved impressive performance on this task, it is difficult to learn commonsense knowledge solely from the pre-training text corpus, as the commonsense knowledge is evident to humans and rarely expressed explicitly in natural language. Compared with unstructured text, structured data like knowledge graphs is much more efficient in representing commonsense [26]. The incorporation of external knowledge aids PLMs in comprehending question-answer (Q-A) pairs, while the entity relations enhance the model's reasoning capabilities. Therefore, various commonsense knowledge graphs (CSKGs) (e.g., ConceptNet [25]) have been adopted in previous studies.\nExisting KG-augmented models for CSQA primarily adhere to a extracting-and-modeling paradigm [36,26,29,32,28,35]. First, the knowledge subgraphs or paths related to a given question are extracted by string matching or semantic similarity, which indicate the relations between concepts or imply the process of multi-hop reasoning. Subsequently, diverse strategies emerge for the efficient representation and fusion of the extracted structural knowledge. One research path [12,8] involves elaborately crafting graph neural networks for better modeling the extracted subgraphs, whereas another [34,26] explores the efficient incorporation of knowledge from KG into language models by enhancing the interactions between PLMs and GNNs.\nDespite their success, these approaches still have several limitations. First, the subgraph's quality suffers when retrieved through a simple string or semantic matching, posing limitations for subsequent operations. To obtain sufficient relevant knowledge, the number of nodes will expand dramatically with the increase of hop count, inevitably raising the burden of the model. Despite its ample size, certain crucial nodes might remain elusive, since some entities are not learned during the pre-training. Besides, the edges linked to the peripheral nodes within the subgraph are pruned, causing the message-passing mechanism of GNN to be blocked and impairing the attainment of effective representations, consequently undermining valuable information. Second, the misalignment between graph and text encoders presents a challenge for PLMs to internalize the knowledge contained in the acquired subgraph, especially in scenarios with limited data, leading to a reduced task performance [35]. Though Dragon [33] proposes a pre-training method to align GNNs and PLMs, it requires additional corpus, and the text-to-graph style to construct semantically equivalent graph-text pairs is challenging. The necessity for substantial computational resources poses another hurdle, prompting the search for a more efficient alignment method.\nIn this paper, we propose a novel framework: Subgraph Retrieval Enhanced by GraPh-Text Alignment (SEPTA), for CSQA. To mitigate the shortcomings of the subgraph extraction process, we establish a database of subgraph vectors derived from the knowledge graph. Consequently, the challenge shifts from retrieving a pertinent subgraph to obtaining relevant subgraph vectors. A BFS-style sampling method is employed to obtain the connected graph for each node and the embedding of the subgraph is subsequently stored in the database. Drawing on the parallels between BFS and the message-passing mechanism of GNNs, the central node's representation learned from the subgraph could be closely aligned with that derived from the entire graph, with almost no information loss. Besides, to further improve the retrieval accuracy and facilitate"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Commonsense Question Answering", "content": "Commonsense question answering aims to evaluate the reasoning ability of models based on commonsense knowledge [7], e.g., physical commonsense [2]. To incorporate external knowledge and enhance reasoning ability, some works introduce commonsense knowledge graphs (CSKGs, e.g. ConceptNet [25]). Generally, these methods [34,37,36,26,29,12,33,8,32,28,35] extract relevant knowledge subgraphs through entity linking and adopt graph neural networks to learn knowledge representations. Among them, a category of research focuses on designing more efficient knowledge encoders. For example, SAFE [12] proposes a 2-layer MLP to improve the efficiency of graph encoding. HamQA [8] considers learning hierarchical structures in KGs with hyperbolic geometry. Another research line tries to enhance the interactions between PLMs and GNNs. For instance, QAGNN [34] adds a QA context node to the retrieved subgraphs and incorporates relevant information from other entities. Unlike previous works, we convert the knowledge graph into a subgraph database and transform the task to a subgraph vector retrieval problem, thus bypassing the challenges inherent in the extracting-and-modeling paradigm."}, {"title": "2.2 Graph-Text Alignment", "content": "Aligning the embedding spaces of text encoders and graph encoders is an effective way to take the strengths of two modalities [18]. Previous alignment methods"}, {"title": "3 Task Formulation", "content": "We study the multiple-choice CSQA [27,19], which can be formulated as: given a natural language question q and a set of answer candidates $C = {C_1, C_2, ..., C_n}$, the aim is to identify the optimal choice $c^* \\in C$. Consistent with previous works [15], the CSQA problem is addressed in a knowledge-aware setting, that is, we can utilize external commonsense knowledge graphs (CSKGs) to facilitate model prediction. A CSKG can be formally described as a multi-relational graph $G = (V,R,E,X)$, where V is the set of concept nodes (e.g., Sun and Holiday), R is the set of relation types (e.g., HasProperty and AtLocation), $E \\in V \\times EXV$ is the link set of the knowledge graph (or fact triplets, e.g. (House, MadeOf, Wood)), and $X \\in R^{|V|\\times d}$ denotes pre-trained embedddings of all concept nodes. Generally, the task can be treated as a score prediction task for each Q-A pair."}, {"title": "4 Methods", "content": "In this section, we will introduce the design of our SEPTA. Departing from previous extracting-and-modeling approaches, we reframe the task as a subgraph vector retrieval problem and propose a graph-text alignment method to improve the retrieval accuracy and facilitate knowledge fusion for prediction. For ease of exposition, we first introduce the graph-text alignment process in Section 4.1. Then with the aligned encoder, the subgraph vector database is constructed and retrieved, which will be presented in Section 4.2. Finally, how to combine all the structural information retrieved for answer prediction is discussed in Section 4.3. Figure 1 shows the overview of our SEP\u0422\u0410."}, {"title": "4.1 Graph-Text Alignment", "content": "To coordinate the embedding spaces of graph and text encoders and fully harness the respective strengths of text and KG, we propose an alignment process before downstream tasks. In our method, we initially address the challenge of generating training graph-text pairs with equivalent semantics and subsequently employ a bidirectional contrastive learning method to train the encoders of both modalities. The alignment process plays a pivotal role, as for one thing, it decides the efficacy of retrieving question-related subgraphs from the vector base, and for another, it determines the successful integration of graph information with the question context during the prediction phase.\nConstruction of Graph-Text Pairs The construction of high-quality semantically equivalent graph-text pairs is crucial for the alignment process, yet not that straightforward. Previous methods mostly adopt a text-to-graph approach to construct training pairs, where the goal is to discover a graph structure that corresponds to the semantics of a provided text segment. However, utilizing existing transformation tools e.g. dependency graph could not well accommodate the downstream subgraph retrieval, while rule-based methods to extract text-related subgraphs from CSKGs are challenging. It is also notably time-consuming and laborious to construct through manual annotation. Therefore, in this paper, we propose a graph-to-text approach and consider constructing synonymous text descriptions of the subgraphs.\nSpecifically, we propose a BFS-style sampling strategy for subgraph construction, which initiates from the central node and proceeds to sample neighbors layer by layer. During the process, to address the challenge of an excessive number of neighbors, we set p as the probability for immediate neighbor selection. In addition, since it is sufficient to describe local neighborhoods for determining structural equivalence [10], we set a parameter d to constrain the depth of the sampled subgraphs. By restricting the search to nearby nodes, our sam-\npling method achieves this characterization and obtains a microscopic view of the neighborhood of every node. Furthermore, a parameter n is established to regulate the size of the subgraphs. Once the number of sampled nodes reaches n, the layer-wise sampling is halted. Since nodes in the sampled neighbors tend to repeat many times, our method could reduce the variance in characterizing the distribution of 1-hop nodes with respect to the source node. The process can be formulated as:\n$G_i = (V_i, E_i, A_i, X_i) = BFS(v_i, p, d, n),$ (1)\nwhere $G_i$ is the connected graph obtained, $V_i, E_i$ are sets of concept nodes and relation links in $G_i$, $A_i$ represents the adjacent matrix, $X_i$ denotes embeddings of concept nodes, and $v_i$ is the central concept node.\nAfter that, it is necessary to textualize the subgraphs to construct synonymous text descriptions. The first step is to convert all relation links into triplet descriptions, which are later combined to compose the final description. Specifically, to transform relation links into sentences, we first map each relation type to a relation template and then concatenate the head concept, relation template, and tail concept as the description of each fact triplet. The textualization process can be denoted as:\n$S_i = \\underset{e_j \\in E_i}{\\bigoplus} TEXT(e_j),$ (2)\nwhere $s_i$ is the description of the graph $G_i$ and $\\bigoplus$ denotes the concatenation of sentences. Therefore, the training set can be denoted as {$(G_i, S_i)$}$_{i=1}^{N}$. The overview of the construction process is shown in Figure 2.\nGraph-Text Contrastive Learning The graph-text alignment procedure is presented in the left part of Figure 1. First, GNN and PLM are utilized to encode the knowledge subgraphs and natural language descriptions to obtain the corresponding representation, respectively, which can be formulated as:\n$\\bar{e}_i = Pool_g(GNN(G_i)),$\n$h_i = Pool_t(PLM(s_i)),$ (3)\nwhere $\\bar{e}_i$ is the average of all nodes' embeddings and $h_i$ is the representation of the [CLS] token. To project $\\bar{e}_i$ and $h_i$ into the same semantic space, two linear"}, {"title": "SEPTA", "content": "projection layers are designed as follows:\n$e_i = W_G\\bar{e}_i + b_g,$\n$h_i = W_Th_i + b_T,$ (4)\nwhere $W_G, W_T$ and $b_g, b_T$ are the transform matrices and biases of the linear projection layers.\nWe then employ InfoNCE with in-batch negative sampling to align the representations of two modalities bidirectionally. The graph-to-text contrastive loss can be formulated as:\n$L_{G2T} = - \\frac{1}{N} \\sum_{i=1}^{N} log \\frac{exp(sim(e_i, h_i) / \\tau)}{\\sum_{j=1}^{N} exp(sim(e_i, h_j) / \\tau)},$ (5)\nwhere $\\tau$ is a temperature coefficient and N is the number of instances in a batch. Besides, function $sim(\\cdot,\\cdot)$ measures the similarity between two representations, which can be calculated by:\n$sim(e_i, h_i) = \\frac{e_i^Th_i}{||e_i|| ||h_i||}$ (6)\nSimilarly, we also design a text-to-graph contrastive loss for uniformly aligning into the same semantic space, which is shown as:\n$L_{T2G} = - \\frac{1}{N} \\sum_{i=1}^{N} log \\frac{exp(sim(h_i, e_i) / \\tau)}{\\sum_{j} exp(sim(h_i, e_j) / \\tau)}$ (7)\nThe final contrastive loss $L_{GT}$ is defined as the average of $L_{G2T}$ and $L_{T2G}$:\n$L_{GT} = \\frac{1}{2} (L_{G2T} + L_{T2G}).$ (8)\nRemarks (1) To avoid the loss of inherent knowledge caused by over-fitting of the PLM, only the GNN and the linear projection layers are trainable during actual implementation. From another perspective, we essentially distill the semantic information from the PLM into the GNN, enabling the graph representations encoded by the GNN to encompass both structural and textual information. (2) As our ultimate goal is to obtain subgraph representations, we employ graph-level contrastive learning to align subgraph embeddings with text embeddings, yielding promising results. We also attempt other granular alignment signals, such as aligning entity node representations with text representations or applying the Masked Language Model (MLM) to text based on subgraph representations. However, with our current computational resources, these methods are unable to converge effectively. Through our contrastive learning, the model is able to rapidly converge and achieve promising performance."}, {"title": "4.2 Subgraph Retrieval Module", "content": "In this section, we initially present the establishment of the subgraph vector database. After that, to better accommodate the alignment process, we propose the query enhancement. Finally, we will outline the subgraph retrieval procedure.\nDatabase Construction Previous methodologies primarily adopt an extracting-and-modeling paradigm for knowledge subgraph retrieval. However, as the cornerstone of subsequent work, the retrieval of high-quality subgraphs proves to be challenging. Consequently, we suggest transforming the knowledge graph into a subgraph vector database, thereby transitioning the focus towards retrieving pertinent subgraphs. Leveraging the analogy between BFS and the message-passing mechanism, we adopt a BFS-style subgraph sampling strategy to construct subgraphs, instead of DFS or Random Walk. On the one hand, each subgraph contains complete neighbor information for at least one node, and on the other hand, each node appears in at least one subgraph. Therefore, the information supplied for our retrieval is complete, and each subgraph vector holds fine-grained knowledge regarding the central node. Specifically, we first apply the same method as in Section 4.1 to produce the graph embedding $e_i$ and the text embedding $h_i$. Then we add them up as the subgraph vector $g_i \\in R^d$:\n$g_i = \\frac{1}{2} (\\frac{h_i}{||e_i||} e_i + h_i),$ (9)\nwhere the regularization coefficient $\\frac{h_i}{||e_i||}$ maintains the consistency between the norm of the subgraph vectors and the norm of the text representations, preventing the prediction from relying predominantly on the features with larger norms. Finally, a subgraph vector database $G = {g_i}$ is constructed with all subgraph vectors. Note that we only need to generate subgraph vectors once before performing downstream tasks, which saves computational resources.\nQuery Enhancement Given a problem, we need to find the relevant subgraph vectors. An intuitive method is to apply the embedding of the question-answer pair as a query. However, there is a certain difference between such textual query and the pre-trained corpus of the aligned encoder, as the latter is constructed through triplet concatenation, which makes it difficult to ensure the quality of text encoding and reduces the accuracy of the retrieval process. Therefore, we propose to enhance the query by retrieving question-related triplets in the knowledge graph and concatenating them after the Q-A pairs. Specifically, given a pair of question-answers $(q, c_1)$, we first apply entity linking to find all entities $E_q = {(e_q^{(1)}, e_q^{(2)},..., e_q^{(n_q)})}$, $E_{c_1} = {(e_{c_1}^{(1)}, e_{c_1}^{(2)},..., e_{c_1}^{(n_{c_1})})}$ appearing in question q and choice c, respectively. Then, we find all triplets in the CSKG containing the entities in $E_q$ and $E_{c_1}$, which can be formulated as $T = {(e^*, r, e), (e, r, e^*)|e \\in E_q \\cup E_{c_1}}$. All fact triplets in T are serialized to natural language sentences and a pre-trained dense retriever is adopted to find the most relevant ones. We"}, {"title": "SEPTA", "content": "concatenate the fact triplets retrieved together, along with the questions and options: $s_i = q \\oplus c_i \\oplus text_1 \\oplus text_2 ... \\oplus text_k$. The aligned PLM is then utilized to encode $s_i$ to $t_i = PLM(s_i) \\in R^d$.\nSubgraph Retrieval After that, we employ the embedding of Q-A pairs concatenated with factual triples to retrieve the relevant subgraph vectors from the subgraph vector database. As the embedding space of two modalities has been aligned, the cosine similarity of $t_i$ with each subgraph vector $g_i$ in G is competent for the retrieval. We recall the top k subgraph vectors with the highest similarities, which is denoted as $G_{q,c_i} \\in R^{k \\times d}$."}, {"title": "4.3 Prediction", "content": "We combine all the knowledge retrieved to make the final predictions. We first integrate the retrieved subgraph vectors through multi-head attention with $t_i$ as the query, which can be formulated as:\n$\\alpha_i^{(h)} = \\frac{(t_iW_Q^{(h)}) (G_{q,c_i}W_K^{(h)})^T}{\\sqrt{d}},$ (10)\n$r_i^{(h)} = Softmax(\\alpha_i^{(h)}) (G_{q,c_i}W_V^{(h)}),$\n$r_i = Concat(r_i^{(1)}, r_i^{(2)},..., r_i^{(H)})W_o,$\nwhere $W_Q^{(h)}, W_K^{(h)}, W_V^{(h)} \\in R^{d \\times d}$ are projection matrices under head h. Subsequently, $r_i$ and $t_i$ are added and fed into a linear layer to predict the score of option $c_i$:\n$p_i = W_1 (t_i + r_i) + b_1.$ (11)\nSince some questions are expected to be answered based solely on the question context, we also encode the Q-A pair $(q, c_i)$ to infer directly:\n$v_i = PLM(q, c_i),$\n$p_i = W_2 v_i + b_2.$ (12)\nThe two scores are weighted and summed to yield the final score:\n$p_i = \\lambda \\hat{p}_i + (1 - \\lambda)p_i,$ (13)\nwhere $\\lambda$ is the hyper-parameter for the balance.\nDuring the training phase, we employ the softmax function to normalize the score for each choice and optimize the model by cross-entropy loss. For inference, we determine the prediction by selecting the choice with the highest score."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Datasets", "content": "We conduct experiments to evaluate our method on five CSQA datasets, which are shown in Table 1:\n\u2022 CommonsenseQA [27] is a 5-way multiple-choice QA dataset, which is created based on ConceptNet [25]. Due to the dual split of CommonsenseQA: the official split [27] and the in-house (IH) split [15], we report the results for both settings. For the official split, the ground truth of the test set is not publicly available, so we submit our model's predictions to the official leaderboard4 to evaluate the test accuracy.\n\u2022 OpenBookQA [19] is a 4-choice dataset about elementary science questions to evaluate the science commonsense knowledge. We also submit the predictions of the test set to the official leaderboard5.\n\u2022 SocialIQA [22] is a 3-choice dataset to evaluate the understanding of commonsense social knowledge. Due to the unavailability of the test set, consistent with prior works [24], we report the accuracy of the development set.\n\u2022 PIQA [2] is a 2-choice QA dataset regarding physical commonsense. Since the test set is hidden, evaluations are conducted on the development set.\n\u2022 RiddleSenseQA [16] is a 5-choice QA dataset about commonsense riddles. Because the test set is not released, we only report the validation accuracy."}, {"title": "5.2 Baselines", "content": "We compare with the mainstream RoBERTa-Large + GNN methods, including RN [21], RGCN [23], GconAttn [31], MHGRN [9], QA-GNN [34], DGRN [37], GreaseLM [36], JointLK [26], GSC [29], SAFE [12], DRAGON [33], HamQA [8], and DHLK [32]. Among them, DRAGON introduces BookCorpus to joint-train GNN and PLM, and DHLK additionally retrieves paraphrases of key entities in WordNet and Wiktionary."}, {"title": "5.3 Implementation Details", "content": "According to the previous works, we use RoBERTa-Large [17] as the text encoder and use GraphGPS [20] for the graph encoder. We also test AristoRoBERTa [6] for OpenBookQA. We use ConceptNet [25], including 799,273 nodes and 2,487,003 edges, as the commonsense knowledge graph. For each node, we treat it as the center and employ BFS to obtain the subgraph, which is then translated into the corresponding natural language description.\nIn the graph-text alignment phase, we randomly sample 64,000 graph-text pairs to train, and sample 16,000 pairs to evaluate two encoders. We fix the learning rate to 1e-3, the number of GNN layers to 2, and the dimensions of all embeddings to 1024. During the fine-tuning stage, we set the number of fact triplets to 10, tune the number of retrieved subgraph vectors k in {10, 30, 50, 70, 100}, the batch size in {4, 8, 16}, the balance coefficient A from 0.1 to 1.0, and the learning rate in {2e-5, 1e-5, 5e-6, 2e-6, 1e-6}. The parameters of the model are optimized by RAdam. We train 30 epochs until the performance does not improve on the development sets for 3 consecutive epochs. We use the default parameter settings as their original implementations for the baseline methods. We conduct all experiments on NVIDIA A100-40GB GPUs."}, {"title": "5.4 Main Results", "content": "Following previous works [34,29,12,11], we compare our method with different baselines on CommonsenseQA and OpenBookQA as main results, which are shown in Table 2. The best and runner-up results in each column are highlighted in bold and underlined, respectively.\nFrom the results, we can observe: (1) Our method can contribute performance gains to LMs, which improves 6.54% and 6.09% on IHdev and IHtest of CommonsenseQA compared to fine-tuned RoBERTa. (2) SEPTA outperforms all baselines without additional corpus on both datasets. For example, compared to the GSC method, our method improves by 2.00% and 0.70% on OpenBookQA using ROBERTa and AristoRoBERTa, respectively. (3) Compared to baselines incorporating additional corpus, our method also achieves comparable performance. Specifically, we surpass DHLK on both datasets and DRAGON on OpenBookQA and slightly lag behind DRAGON on CommonsenseQA. It should be noted that the DRAGON undergoes MLM training on the BookCorpus dataset and requires training on 8\u00d7A100 GPUs for a week [33,32]. By eliminating the MLM, our SEPTA model demonstrates a definitive enhancement.\nIn Table 3, we evaluate SEPTA on the official CommonsenseQA and OpenBookQA leaderboards (as of March 22, 2024). Our method achieves results surpassing all baselines based on the same PLM and exhibits comparative performance compared with methods with larger-scale parameters (e.g., UnifiedQA).\nTo comprehensively evaluate the efficiency of SEPTA, we extend our comparative analysis to other commonsense reasoning datasets originating from diverse domains or tasks. As shown in Table 4, our SEPTA consistently achieves superior performance. This observation underscores the overall effectiveness of SEPTA in addressing various commonsense reasoning datasets or tasks, demonstrating a unified methodology."}, {"title": "5.5 Ablation Study", "content": "We conduct an ablation study on CommonsenseQA and OpenBookQA to explore the effectiveness of each component of SEPTA. We remove the alignment process (w/o alignment), retrieved subgraph vectors (w/o subgraph), fact triplets (w/o triplets), and scores predicted based on Q-A pairs (i.e. set $\\lambda$ = 1.0), respectively.\nAs shown in Table 5, four components are all crucial for SEPTA, and removing any part will result in a decrease in performance. Specifically, the performance drops the most significantly when we remove the graph-text alignment. This is because if the representations of graphs and texts are not semantically aligned, then during the knowledge retrieval stage, the retrieved subgraph vectors may be irrelevant and situated in different latent spaces from the textual information. Moreover, removing either fact triplets or subgraph vectors will affect the performance. On one hand, they represent different aspects of information, with the former providing more specific knowledge and the latter describing more comprehensive relationships between entities. On the other hand, fact triplets also play an auxiliary role in retrieving relevant subgraph vectors. Furthermore, only using knowledge-enhanced representations for predictions (i.e. $\\lambda$ = 1.0) cannot achieve optimal results. This is because some questions do not require additional knowledge, or relevant information cannot be found in CSKGs, which may instead become interference."}, {"title": "5.6 Low-Resource Setting", "content": "To evaluate the robustness of SEPTA, we conduct extensive experiments in low-resource settings, with different proportions of training data, including 5%, 10%, 20%, 50%, and 80%, in CommonsenseQA (IHtest) and OpenBookQA."}, {"title": "5.7 Evaluation with Other GNNS", "content": "To demonstrate the generality of SEPTA, We employ GraphGPS [20], FILM-GNN [4], and RGCN [23] as the graph encoders, respectively. Table 7 illustrates the results on CommonsenseQA and OpenBookQA. From the results, we can observe that different graph encoders achieve competitive results on both datasets, with their performances being relatively close (within a difference of around 0.5%), which demonstrates the effectiveness and robustness of SEPTA."}, {"title": "5.8 Hyper-parameter Analysis", "content": "We further conduct in-depth analyses to investigate the impact of hyper-parameters. With other parameters fixed, we compare the effect of the number of retrieved subgraph vectors k, the maximum number of nodes n in each subgraph, and"}, {"title": "6 Ethical Considerations and Limitations", "content": ""}, {"title": "6.1 Ethical Considerations", "content": "Our work proposes a novel and effective framework to combine PLMs and external knowledge graphs for commonsense question answering. However, potential"}, {"title": "6.2 Limitations", "content": "We propose a subgraph retrieval enhanced by a graph-text alignment framework named SEPTA for commonsense question answering. However, there are still limitations that demand resolution. Firstly, the corresponding text generated by rules from knowledge subgraphs still exhibits disparities from natural language. One possible solution is to reorganize the text using LLMs, but the cost is prohibitively high. Therefore, acquiring large-scale, high-quality graph-text pairs remains an ongoing challenge. Secondly, the number of retrieved subgraph vectors is required to tune according to the accuracy of development sets, which is time-consuming. Designing a module to automatically select the number may be a solution worth exploring. Thirdly, due to considerations of fairness in comparison and limited computational resources, we do not employ other PLMs, especially LLMs, as text encoders, which will be considered in our future work."}, {"title": "7 Conclusion", "content": "We propose an effective framework: subgraph retrieval enhanced by graph-text alignment, named SEPTA, for commonsense question answering. In our method, we reframe the task as a subgraph vector retrieval problem and introduce a graph-text alignment method to enhance retrieval accuracy and facilitate knowledge fusion for prediction. Subsequently, all the structural information retrieved is then combined by a simple attention mechanism to bolster the reasoning capabilities of PLMs. Extensive experiments on five benchmarks demonstrate the effectiveness of SEPTA.\nIn the future, our work will focus on the following aspects. First, we will explore more efficacious pre-training tasks for semantic alignment. Second, if there are sufficient computational resources, we intend to apply our approach to larger language models. Third, we will try SEPTA on relevant tasks, e.g., node classification and link predictions on text-attributed graphs."}]}