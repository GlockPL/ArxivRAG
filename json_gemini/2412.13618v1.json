{"title": "NPC: Neural Predictive Control for Fuel-Efficient Autonomous Trucks", "authors": ["Jiaping Ren", "Jiahao Xiang", "Hongfei Gao", "Jinchuan Zhang", "Yiming Ren", "Yuexin Ma", "Yi Wu", "Ruigang Yang", "Wei Li"], "abstract": "Fuel efficiency is a crucial aspect of long-distance cargo transportation by oil-powered trucks that economize on costs and decrease carbon emissions. Current predictive control methods depend on an accurate model of vehicle dynamics and engine, including weight, drag coefficient, and the Brake-specific Fuel Consumption (BSFC) map of the engine. We propose a pure data-driven method, Neural Predictive Control (NPC), which does not use any physical model for the vehicle. After training with over 20,000 km of historical data, the novel proposed NVFormer implicitly models the relationship between vehicle dynamics, road slope, fuel consumption, and control commands using the attention mechanism. Based on the online sampled primitives from the past of the current freight trip and anchor-based future data synthesis, the NVFormer can infer optimal control command for reasonable fuel consumption. The physical model-free NPC outperforms the base PCC method with 2.41% and 3.45% more significant fuel saving in simulation and open-road highway testing, respectively.", "sections": [{"title": "I. INTRODUCTION", "content": "Fuel costs are integral to the commercial vehicles' life cycle, particularly in the context of truck operations, where they comprise a substantial fraction of logistics companies' operational expenses. According to ATRI [1], fuel costs account for 28% of truck operational costs. Furthermore, road transportation is the major consumer of fossil fuel energy, contributing significantly to worldwide CO2 and pollutant emissions [2]. Therefore, improving fuel consumption efficiency can not only reduce freight costs but also benefit the environment. However, the driving skill of human drivers in terms of fuel savings follows a distribution with large diversity and variance [3]-[5]. Thus, autonomous vehicles, or even advanced driving assistance systems (ADAS) are very promising to improve fuel consumption efficiency with a consistent high-performance driving strategy.\nRecent studies [6]-[10] have demonstrated a significant fuel-saving advantage of autonomous vehicles. Regarding fuel efficiency for autonomous highway trucking, there is a substantial amount of researches [11]\u2013[13] currently focusing on truck driving strategies. Hu et al. [14] develop an Adaptive Cruise Control (ACC) based on the Pulse-and-Gliding strategy to reduce fuel consumption, with a focus on car-following situations. Dynamic Programming has also been used for minimizing the energy consumption of trucks [15] using the local traffic information as the major optimization cost. Ren et al. [16] employ the linear quadratic regulator algorithm for predictive energy saving control based on future road slope, as well as the vehicle dynamic model. Among those fuel-saving solutions, predictive cruise control (PCC) [17]-[20] is the key line of research possessing the most significant potential for large-scale commercial applications.\nThe core of PCC-related works is an optimal control problem (OCP). Solutions for such an OCP using Pontryagin's minimum principle show high performance in fuel saving capability, as well as computation costs [21]\u2013[24]. However, the performance in the real world is heavily dependent on the dynamic model and the engine/fuel model of the target vehicle. The accuracy of the parameters in the dynamic vehicle model, e.g. mass or weight, as well as the coefficient of resistance to drag and rolling, is always overlooked in laboratory experiments and research papers. However, it cannot be ignored in practical applications due to the huge impact on the result of PCC optimization. Technically, these vehicle parameters are estimated using classical system identification solutions [25], [26]. The engine control unit (ECU) of trucks used in our experiments can yield the estimated weight directly. We experimentally find that the ECU's weight estimation error is 8.5% in our autonomous fleet (747 trips with ground-truth weight from 17.7 tons to 51.85 tons are evaluated). Note that, diverse working conditions in freight tasks, e.g. different speed profiles and loads, terrains, weather, and climate would significantly increase the difficulty and error of vehicle parameter estimation.\nTo reduce the dependency on the accurate model of vehicle dynamics and engine, we propose a Neural Predictive Control method (see Fig. 1) for fuel-efficient autonomous trucks on hilly roads. NPC is a purely data-driven method, which is free of any physical model for the vehicle. In other words, weight, drag and rolling coefficient, and even the BSFC map of the engine are not needed anymore. Technically, a novel attention-based module NVFormer is designed to implicitly model the relationship between vehicle dynamics, road slope, fuel consumption, and control commands (speed, torque, etc.). Based on the online sampled primitives (data from the past of the current freight trip) and anchor-based future data synthesis, the NVFormer can accurately infer control commands for optimal fuel consumption. The physical model free NPC outperforms the base PCC method [18] in terms of not only the robustness over varying conditions but also the fuel-saving capability.\nThe main contributions of this paper are as follows:\n\u2022 A pure data-driven NPC is proposed to solve the fueloptimal control problem. NPC is free of any physical"}, {"title": "II. NPC FRAMEWORK", "content": "In this section, we present the framework of Neural Predictive Control (NPC) for enhancing the fuel efficiency of autonomous trucks. Fig. 1 shows the overview of NPC. The input for NPC includes offline and online data with features vehicle speed (v), acceleration (a), slope (\u03b8), engine torque (T), engine speed (S), and fuel consumption (f) correspondingly. For one freight trip, complete past data samples and partial future data with features v, a, \u03b8 are generated by the Online Past Data Sampler and the Future Data Sampler, respectively. Next, NPC yields the inference primitive and the sample primitives. Inference primitive comprises the latest data sequence along with partial future data. Sample primitives are distilled from the online past data, excluding the latest data sequence. We utilize the NVFormer to predict the missing part (T, S, f) of the future data by the inference primitive and sample primitives. With NVFormer, the Fuel-saving Optimizer can produce the optimal truck control commands that leverage fuel consumption while meeting transportation constraints.\nNPC works on Frenet coordinates due to the pre-determined lengths of commercial transportation routes. This coordinate system is extensively employed in autonomous planning tasks [27]. We use s to present distance, and {\u03b7\u00b9,...,\u03b7\u1d50} to describe the data with m features: v,a,\u03b8,T,S,f. The data sequence of feature i at a distance s is \u03b7\u2071. The distance at time t is s\u209c. We sample the data every \u2206s m. We use data chunks to represent the sequences of m data. The data chunk at s\u209c with length l is \u03b7(\u1d50,s\u209c,l) = {\u03b7\u02e2: i = 1, ...,m, s = s\u209c + \u2206s, ..., s\u209c + l\u2206s}. We denote the s \u2208 {s\u209c + \u2206s, s\u209c + \u2206s, ..., s\u209c + l\u2206s} as endpoint."}, {"title": "B. Online Past Data Sampler", "content": "The Online Past Data Sampler distills important samples from past data with features v,a,\u03b8,T, S, f in current freight trip. We name such sequential sampled data as primitive, which encompasses representative vehicle states and environment information. The inference primitive comprises the latest data sequence along with the known partial future data (v, a, \u03b8), collectively constituting local features (weather, road friction etc.) in NVFormer. Sample primitive is a data chunk sampled from the online past data, excluding the latest data sequence, which works as global context features (engine properties, vehicle shapes, tire friction characteristics etc.) in NVFormer. The length of sample primitives is lh and the latest past data is \u03b7(\u1d50, s\u209c - lh\u2206s, lh) at time t. When selecting p sample primitives at time t \u2208 {t\u2081,t\u2082,...,t\u209a}, the sample primitive k is \u03b7(\u1d50, s\u209c\u2096, lh).\n1) Composition of Primitive: The primitive encompasses the following features: v, a, \u03b8, T, S, and f. Specifically, the v, a, and \u03b8 represent the value at the endpoint over the distance interval \u2206s. The parameters T and S represent the average torque and engine speed, while f denotes the total fuel consumption during this interval. The NPC model implicitly learns the vehicle's status and dynamic characteristics with the above parameters. v and a could reflect the vehicle's kinetic information, while \u03b8 encapsulates gravitational potential energy. Additionally, we could generate control commands directly based on T and S.\n2) Sample Primitives Selection: Sample primitives play a vital role in providing global context features. The vehicle information embedded within sample primitives enhances the prediction capabilities of the model. To provide better global information, sample primitives should encompass various working conditions and vehicle states. When the distance"}, {"title": "C. Future Data Sampler", "content": "The known future data includes the velocity status(v,a) generated by the future data sampler and slope (\u03b8) obtained from the high-fidelity map. We predict T, S, and f by pre-trained NVFormer and obtain the complete future data. We use the data chunk \u03b7(\u1d50, 0, lf) to represent the future data, where lf is the length of the future data chunk. We sample the speed for the future lf\u2206s m according to the altitude z (see Fig.2). Initially, we identify anchor points of the future altitude curve. Subsequently, we calculate speed limits at each anchor point using the reference speed line, which is created based on the current speed and the altitude curve, and the user-defined target speed line. Following this, we generate key points for each sampled speed line and get the future data chunk samples based on the key points.\n1) Anchor Point Detection: An anchor point is a point with local maximum or minimum altitude. We detect anchor points by the condition:\n$\\frac{\\delta z}{\\delta s} < \\epsilon,$\nwhere z is the altitude, \u03f5 is the near zero value. For each anchor point, we set the speed bound according to the target speed line defined by the user and the reference speed line created based on the current speed and the altitude curve. The closer, the greater the impact on decision-making at the current location. So, we first sample velocity around anchor points 1 and 2. As shown in Fig. 2, the speed bound of the anchor points 1 and 2 is defined by the reference line. Suppose the corresponding reference speed is vf and the range is \u00b1va m/s, the speed bound is [v-va, v + va], i = 1,2. The speed of the start point is v\u209c, and the speed bounds"}, {"title": "2) Speed Sample Generation:", "content": "We evenly divide each speed bound of the first two anchor points (anchor points 1, 2) into x parts, and get x\u00b2 sampled speed series. The arbitrary speed series is {v\u1d62\u02b2| v\u2081\u02b2,..., v\u1d62\u02b2,..., v\u2093\u02b2}, for i\u2208 {1,2,...,x} and j\u2208 {1,2,...,x}. The speed samples can be generated by interpolating the key points' sampled speed series. Suppose the corresponding s values of the key-points is {0, sp1,..., spk, ..., llf\u2206s}, spk is the s value of the anchor point k. For su \u2208 [spk, spk+1], the uth interpolated speed of the ijth speed sample is\n$v_{ij}^{su} = \\frac{s_{pk+1}-s_u}{s_{pk+1}-s_{pk}}v_{ij}^{pk} + \\frac{s_u-s_{pk}}{s_{pk+1}-s_{pk}}v_{ij}^{pk+1},$\nwhere v\u1d62\u2c7c\u1d56\u1d4f is the speed of the kth key-point of the ijth sampled speed series, su is the s value of the uth endpoint. Endpoint is the point with s value in \u2206s,...,u\u2206s,...,llf\u2206s},u \u2208 {1,2,..., lf}.\n3) Future Data Chunks: The future data chunk \u03b7(\u1d50,s\u209c,lf) represents the composition for the future lf\u2206s m, where {\u03b7\u02e2: i = 1,...,m, s = s\u209c + \u2206s, ...s\u209c+lf\u2206s}. We divide the total m = 6 composition into the known composition v, \u03b1, \u03b8 with number mf = 3 and the unknown composition T, S, f with number mu = 3. We get velocities v from the velocity sampler, generate acceleration a from v, and get slope \u03b8 from the high-fidelity map. With the data chunks, we leverage the pre-trained NVFormer model to predict the unknown composition T, S, and f. Hence, \u03b7\u02e2 = 0 when i > 3 before prediction."}, {"title": "D. Neural Predictive Model", "content": "As illustrated in Fig. 1, the NVFormer model is built upon an Encoder-Decoder architecture, where the Encoder captures relevant information from sample primitives and inference primitive, and the decoder regresses the predictions. NVFormer is a transformer-based model since the multi-head attention mechanism serves a dual purpose. On the one hand, it can attend to the internal relationships within a single primitive. On the other hand, it can establish associations between the global context features of sample primitives and the local features of inference primitive, thereby providing additional information for predictions.\n1) Encoder Module: The Encoder comprises a dual ensemble of Transformer Encoders [28], namely Sample Former and Inference Former. Sample Former is responsible for processing sample primitives and consists of ns layer of vanilla Transformer Encoder Layers. Inference Former integrated a stack of ni Transformer Encoder Layers devoted to processing historical feature series derived from inference primitive. The multi-head attention mechanism in the Transformer Encoder enhances the model's capacity to emphasize pivotal aspects within the input data."}, {"title": "2) Decoder Module:", "content": "The decoder is structured with ni layers of Transformer Decoder Layers. It assimilates insights emanating from the Encoder. Concurrently, a sequence mask is employed within the decoder to engage with forthcoming requests, fostering a heightened interdependence between the output and the sequential context of the data.\n3) Model Workflow: Through the collaboration of the Future Data Sampler and the Online Past Data Sampler, we obtain a set of p sample primitives and a single inference primitive as inputs, where the future data sequences only encompass the known information (v, a, \u03b8), while the unknown features (T, S, f) constitute the output of NVFormer. In a single mini-batch, the shape of sample primitives is p\u00d7 lh \u00d7 m. These dimensions undergo reduction via a fully connected (FC) layer, resulting in lh \u00d7 m. Based on the description above, in an inference primitive, the shape of the latest data is lh \u00d7 m. The input future sequence has mf composition, and the sequence's shape is lf \u00d7 mf. For the output sequence, it is lf \u00d7 mu, where mu + mf = m. All data undergo an embedding process before being fed into the attention-based module. This embedding procedure encompasses both positional encoding [28] and an FC layer. During the cross-attention module, the concatenated outputs of the Sample Former and Inference Former are employed as the Key (K) and Value (V) to the decoder module. The future input data sequences, with embedding and a sequence mask, serve as the future request Query (Q) for the decoder. Subsequently, the output of the decoder undergoes the FC layer, leading to the final output.\n4) Loss Function: Since NVFormer aims to predict the engine status and fuel consumption, we utilize Mean Squared Error (MSE) as a loss function for training as depicted in Eq. 3, where N represents the sample quantity, yi signifies the ground truth values, \u0177i denotes the predicted values.\n$L_{MSE} = \\frac{1}{N}\\sum_{i=1}^N (y_i - \\hat{y}_i)^2.$"}, {"title": "E. Fuel-saving Optimizer", "content": "Given the sample primitives, the latest data, and the future data samples, the fuel-saving optimizer selects the best output sequences through the utilization of the pre-trained NVFormer (refer to Algorithm 1). Specifically, the optimizer iterates through each future data sample. For each sample, we employ the pre-trained NVFormer to estimate the curves for torque, engine power, and fuel consumption. Subsequently, we compute the cost by the following equation:\n$C = w_1 \\sum{\\eta_i} + w_2(|V_{mean} - V_{target}|),$\nwhere F = \u03a3\u03b7\u1d62|s=\u2206s,...,llf\u2206s denotes the mth feature total fuel consumption. Vmean is the average velocity of the future data samples. Vtarget is the target speed. Vmean - Vt is the speed difference between the vmean and Vtarget. The closer between Vmean and Vtarget, the better to meet the transportation timing requirement. w\u2081 is the weight of fuel and w2 is the weight of velocity. Since we focus more on fuel-saving efficiency, we set w\u2081 = 1.0 and w2 = 0.1 in our implementation. The best future data sample is the one with the lowest cost."}, {"title": "III. EXPERIMENTAL RESULTS", "content": "To validate NPC's fuel-saving efficiency, we train and test the model on an offline dataset derived from real vehicle data. Subsequently, close-loop validation of NPC is performed on both simulation and open-road highway testing."}, {"title": "A. Experimental Results of NVFormer", "content": "We conducted NVFormer training and testing experiments on real-world Inceptio's autonomous truck data. Training the model on offline past data enables it to capture the characteristics of vehicle motion and engine behavior. Additionally, the comparative models included in this part are LSTM and Transformer.\n1) Dataset: The existing datasets are not applicable for trucks [29]. Our data is derived from real commercial daily routes of Inceptio's autonomous trucks. All the data was obtained from 4 different vehicles with the same truck series from 26 trips of 4 routes, and the total mileage of all the data is more than 20,000 km. Each vehicle uses GPS, cameras, an inertial measurement unit, and a wire control chassis to record data. The length of each trip is approximately 600~850km, and more than 95% of the mileage in these trips is on highways. To enhance the model's applicability across various external environments, these routes exhibit varying altitudes, encompassing flat terrains and slopes with different degrees of gradient. For instance, Fig. 3 illustrates two routes from our dataset. As shown in the figure, the northern part of Route (a) consists of mountainous terrain, while the southern part is characterized by plains. In Route (b), the northern section exhibits significant altitude changes, whereas the southern portion has relatively minor elevation variations. Furthermore, our data includes diverse weather conditions and different loads (18.5 ~ 36.0 ton, average 28.2 ton), aiming to increase the diversity of the dataset and enhance the generalization ability of the model. The dataset is used to train the vehicle dynamics, and the drivers' level of driving skill does not matter. The model can acquire"}, {"title": "2) Baselines and Experimental Settings:", "content": "We choose an LSTM-based Encoder-Decoder architecture model (hereinafter referred to as LSTM) and the original Transformer model as baselines for comparative analysis, which could only receive the inference primitive as input. We use 2 encoder layers and 2 decoder layers for all models. Here Transformer and NVFormer are constructed with 8 multi-heads. For NVFormer, we set the number of sample primitives to 10 with 2 encoder layers in Sample Former. In both LSTM and Transformer architectures, additional FC layers are employed to facilitate the outputs. All models are constructed in the same embedding dimension and compared in data point interval \u2206s = 50 m, which leads to input length lh = 40 and output length lf = 60.\nFor all models, we conduct training for hundreds of epochs and select the model with the lowest validation loss after achieving stability for evaluation. A warm-up period of 10 epochs is employed for both Transformer and NVFormer. The initial learning rate for LSTM is 10\u207b\u2074 and for Transformer and NVFormer is 10\u207b\u2075. The batch size of training with Adam optimizer [30] is set to 256. All experiments are implemented in PyTorch [31] and conducted on two NVIDIA RTX4090 24G GPUs. MAE and MSE are employed as evaluation metrics.\n3) Results and Model Analysis: As demonstrated in Tab. I, in comparison to both LSTM-based and Transformer models, NVFormer exhibits a notable enhancement in predictive accuracy for engine torque. The prediction precision of engine speed and fuel consumption is also significantly elevated through the utilization of NVFormer. Incorporating sample primitives as historical information strengthens the model's"}, {"title": "B. Close-loop Verification of NPC", "content": "We compare our NPC method with PCC [18] by close-loop verification in simulation and open-road highway. Building upon offline learning, the model within NPC can forecast future vehicle states based on data recorded by the Online Past Data Sampler. All models are executed using the C++ ONNX Runtime API [32].\n1) Simulation: We use simulation to compare PCC, NPC with LSTM model, NPC with Transformer model, and NPC with NVFormer model on real-world road network. All simulation experiments are conducted on the personal computer with 128G memory and Intel Core i9-13900KF processor."}, {"title": "IV. CONCLUSION AND FUTURE WORK", "content": "In this paper, we present a novel framework to estimate future torque and engine speed series with optimal fuel consumption under transportation timing constraints. To reduce the negative influence of inaccurate vehicle dynamics and engine models, the data-driven NPC method is proposed to take full advantage of a large amount of offline and online vehicle data. NPC performs better than the baseline PCC by the open-loop and close-loop verification. In the future, the efficiency and capability of the network can be enhanced to handle complex traffic on the fly while achieving a safe and fuel-saving autonomous freight trip. We will also learn from large-scale truck operational data to optimize the planner with a deep neural network directly instead of sampling future data to improve the computation efficiency of NPC."}]}