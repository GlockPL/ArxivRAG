{"title": "More is not always better? Enhancing Many-Shot In-Context Learning with Differentiated and Reweighting Objectives", "authors": ["Xiaoqing Zhang", "Ang Lv", "Yuhan Liu", "Flood Sung", "Wei Liu", "Shuo Shang", "Xiuying Chen", "Rui Yan"], "abstract": "Large language models (LLMs) excel at few-shot in-context learning (ICL) without requiring parameter updates. However, as the number of ICL demonstrations increases from a few to many, performance tends to plateau and eventually decline. We identify two primary causes for this trend: the suboptimal negative log-likelihood (NLL) optimization objective and the incremental data noise. To address these issues, we introduce DR-ICL, a novel optimization method that enhances model performance through Differentiated Learning and advantage-based Reweighting objectives. Globally, DR-ICL utilizes differentiated learning to optimize the NLL objective, ensuring that many-shot performance surpasses zero-shot levels. Locally, it dynamically adjusts the weighting of many-shot demonstrations by leveraging cumulative advantages inspired by reinforcement learning, thereby improving generalization. This approach allows the model to handle varying numbers of shots effectively, mitigating the impact of noisy data. Recognizing the lack of multi-task datasets with diverse many-shot distributions, we develop the Many-Shot ICL Benchmark (MICLB)\u2014a large-scale benchmark covering shot numbers from 1 to 350 within sequences of up to 8,000 tokens\u2014for fine-tuning purposes. MICLB facilitates the evaluation of many-shot ICL strategies across seven prominent NLP tasks and 50 distinct datasets. Experimental results demonstrate that LLMs enhanced with DR-ICL achieve significant improvements in many-shot setups across various tasks, including both in-domain and out-of-domain scenarios. We release the code and benchmark dataset hoping to facilitate further research in many-shot ICL.", "sections": [{"title": "1 Introduction", "content": "In-context learning (Brown et al., 2020) enables models to quickly adapt and address specific issues by utilizing contextual cues, improving adaptability and generalization. With the expansion of the context length in advanced LLMs, the ability to process text lengths up to 1 million tokens allows LLMs to accept increasingly more demonstrations. The ICL scenarios with hundreds or thousands of shots are called many-shot learning (Agarwal et al., 2024). However, many-shot does not always result in better performance than few-shot. Some models exhibit a linear decrease in ICL capabilities with the increase in ultra-long text lengths (Liu et al., 2024). As shown in Figure 1, we present the accuracy variations of Mistral-7B-Instruct-v0.2 on the CLSClusteringS2S dataset (Li et al., 2022). As the number of ICL examples increases, models' performance exhibits a trend of rising and then falling.\nThe performance trends of many-shot ICL can be attributed to several factors. The first factor is the training objective. As Agarwal et al. highlights, while the straightforward NLL decreases during testing with ICL, performance on many downstream tasks also deteriorates. The second factor is the increasing noise with the large number"}, {"title": "2 Related Work", "content": "In-context Learning. In-context learning allows models to execute downstream tasks without the need for parameter updates, enabling language models to serve as a universal tool for a variety of tasks. As the number of examples supplied to LLMs grows, supplementary strategies become essential to bolster the model's ICL capabilities. For instance, Anil et al. (2024) employ multi-example prompts, which can accommodate up to 256 demonstrations, to overcome the inherent limitations of language models. Hao et al. (2022) propose the structured prompting method to overcome length restrictions and extend in-context learning to thousands of examples. Li et al. (2023) use a customized model architecture to support the expansion of contextual examples to 2,000, and (Agarwal et al., 2024) utilize reinforced ICL and unsupervised ICL to extend the scope of contextual examples to 8,192. Unlike their work, we enhance the ICL capability of LLMs by improving the model's parameters rather than the form of contextual examples.\nInstruction Tuning of LLMs. Instruction tuning has become an effective technique for enhancing the capabilities and controllability of LLMs (Zhang et al., 2023). In the domain of ICL, studies like MetaICL (Min et al., 2022) and PEFT (Bertsch et al., 2024) have demonstrated that fine-tuning LLMs with both small and large demonstration sizes, denoted as k, lead to improved ICL performance. Despite their studies being confined to a modest quantity of tuning data\u2014capped at 10,000 entries\u2014there is an evident necessity for deeper research into how ICL performs when scaled up with more extensive datasets. Consequently, we introduce MICLB, a significantly larger dataset, designed to delve into the strategies for amplifying ICL's potential.\nLLM Data Reweighting. As LLMs rapidly advance, the application of data reweighting in training has become increasingly prevalent. In the pre-training stage, SoftDedup significantly improves training efficiency by selectively reducing the sampling weight of data with high commonness through a soft deduplication method, rather than removing them to increase the integrity of the dataset (He et al., 2024). ScaleBiO reweights the data of LLMs by filtering irrelevant data samples and selecting informative samples, demonstrating its effectiveness and scalability across models of different sizes on tasks such as data denoising, multilingual training, and instruction tuning (Pan et al., 2024). In the ICL scenario, Yang et al. (2023) propose WICL to enhance the performance of ICL by assigning optimal weights to demonstration samples in the inference. Unlike other works, we set the weights during the training process based on the positions of multiple examples in a sequence."}, {"title": "3 DR-ICL", "content": "In this work, we propose the DR-ICL learning framework, which adjusts the weights of demonstrations and integrates reweighting within differentiated objectives, as illustrated in Figure 2. In DR-ICL, we organize training data through many-shot and zero-shot demonstrations. By simultaneously training the sequence of many-shot and zero-shot with a differentiated objective, we strengthen the model's overall ICL capability. At the same time, to further reduce the noise of demonstrations in many-shot scenarios, we introduce a weighted training objective towards different samples in the many-shot demonstrations. By sampling the model's performance under different demonstrations, we calculate the cumulative advantage gained as the number of demonstrations increases and use this cumulative advantage to adjust the learning process. Below, we show the components of the DR-ICL framework from both a global and local perspective, as well as the learning strategy."}, {"title": "3.1 Global Perspective: Differentiated Learning", "content": "We apply differentiated learning for the trade-off of many-shot and zero-shot sequences due to differing sample lengths, where longer sequences might introduce more noise. We expect that after refining the learning objectives, the model can still perform well in scenarios with numerous demonstrations, longer samples, and potentially noisy backgrounds. In each iteration, we sample K pairs of examples (xk, Yk) from the training dataset, where k ranges from 1 to K. Then, we concatenate the examples xk and their corresponding labels Yk, and the instruction I generated by GPT-3.5-turbo for the current task as the input sequence $S_K = \\{I; X_1Y_1X_2Y_2 ... x_kY_k\\}$. We train the model to predict the label yk of the k-th example based on the instruction and the features and labels of the previous k - 1 examples. The training objective of the model is to minimize the NLL loss $L_{NLL}$, with the previous k 1 examples as the training examples and the k-th example as the test example. This training method helps the model learn in context during the inference stage. We organize the number of demonstration examples according to k. When k > 0, we perform many-shot instruction-tuning, and when k = 0, we perform zero-shot instruction-tuning. During our training process, we expect that different examples of the same training sequence Sk can serve as helpful contexts Ch for each other. In the absence of context, we define Cnone, so we update the original NLL loss combined with the additional objective for many-shot and zero-shot as follows:\n$L_{many-shot} = L_{NLL}(LLM(C_h, Q; \\theta), A_{gt}),$\n$L_{zero-shot} = L_{NLL}(LLM(C_{none}, Q; \\theta), A_{gt}),$\nwhere Q is the input question to the model, and Agt is the corresponding ground truth answer. We utilized many-shot data as Q and transformed it into a degraded zero-shot format using the Parallel Context Windows (PCW) method (Ratner et al., 2022). PCW works by masking many-shot sequences to generate a zero-shot sequence, effectively enabling us to leverage both formats. In our implementation, PCW was used solely to simplify the input for coding purposes. We aim to simultaneously optimize these two losses, such that $L_{many-shot} < L_{zero-shot}$. A lower many-shot loss signifies that the model has"}, {"title": "3.2 Local Perspective: Advantage-based Reweighting", "content": "After global Differential Learning, we noticed that loss fluctuates at certain k-shot points instead of decreasing consistently, suggesting some samples affect the model's context significantly, possibly introducing noise. To address this, we introduced a reweight mechanism that adjusts weights based on performance differences between adjacent windows, giving higher weights to samples with larger differences, and helping the model adapt to dynamic contexts. The model optimizes the weights of demonstration data, continuously balancing exploration and data utilization to achieve a rapid and stable ICL performance. Below, we describe the overall process from three aspects: importance sampling, advantage functions, and reweighting."}, {"title": "3.2.1 Importance Sampling", "content": "Importance sampling adjusts the sampling weights to reduce bias and imbalance brought by noisy data. In this section, we leverage the loss of samples on the training dataset evaluated by the training model to calculate the importance weights of these samples. For each training sequence $S_K = \\{X_1Y_1X_2Y_2...X_kY_k\\}$, we calculate the loss $L_{many-shot}$ generated by the sequence $\\{X_1Y_1X_2Y_2...X_k\\}$ at the current k-th position to represent the features of Sk. Our objective is to adjust the weighting of examples based on their significance, thereby ensuring that more emphasis is placed on the critical instances while reducing focus on the less important ones.\nTo prevent an undue focus on specific parts of the data, we introduce a reweighting window, designed to segment the sequence into multiple parts. Each window is intended to handle a portion of the sequence with a total length of K. The sequence is segmented into $[\\frac{K}{W}]$ equal windows, each with a size of W. For the k-th demonstration we have the reweighting window index w as follows:\n$w = \\lceil\\frac{k}{W}\\rceil,$\n$\\frac{k}{W} - 1 < w \\leq \\lceil\\frac{k}{W}\\rceil,$\n$|w| = \\frac{k}{W}$.\nWe designate the preceding window w 1 as the sampling window, to select |S| demonstrations for those in the reweighting window w, compiling these into a set S. The demonstrations within set S are then utilized for further training, leveraging accumulated benefits to enhance learning.\n$w-1= \\lceil\\frac{k}{W}\\rceil - 1$\n$\\{\\lfloor\\frac{k}{W}\\rfloor\\times W + 1, \\lfloor\\frac{k}{W}\\rfloor\\times W \\}$.\nWe define a target distribution p(x) and an importance distribution q(x) with their probability density functions to achieve set S. Specifically, for each training sample Sk and feature vector Lk of the k-th demonstrations, we use the ratio of the values of the target distribution p(x) and the importance distribution q(x) to calculate the weight weightk for the k-th demonstration in Sk and select the top |S| samples with the highest weights:\n$weight_k = \\frac{p(L_{many-shot_k})}{q(L_{many-shot_k})}$\nThrough these steps, we calculate the weights of important samples and select the top |S| representative samples from the given sample distribution."}, {"title": "3.2.2 Advantage Functions", "content": "To assess the model's cumulative advantages as the k-shots grow, we select the sample set S for the k-th instance within the weighting window w. Subsequently, we determine the average loss of the sampling window w 1 using the formula:\n$L_{sampling_{w-1}} = \\frac{1}{|S|} \\sum_{instance \\in S} L_{instance}$\nThe reward is defined as the difference between the loss of the instance at the current position k and the average loss of the instances in window w 1:\n$R_k = L_{many-shot_k} - L_{sampling_{w-1}}.$\nHere, $L_{sampling_{w-1}}$ represents the performance of the model on all sampled instances before window w. It denotes the model's performance with fewer than k shots, whereas $L_{many-shot_k}$ signifies the model's performance with k shots. Next, we define the accumulated advantages to measure the strategy's performance in different positions k:\n$A_k = exp(R_k/\\gamma),$\nwhere \u03b3 is a temperature parameter used to adjust the sensitivity of the rewards. The exponential increase in the advantages metric strengthens positive rewards while suppressing negative rewards, guiding the model to select strategies that bring significant performance improvement."}, {"title": "3.2.3 Reweighting", "content": "In the DR-ICL framework, we select important samples in the previous window and calculate the reward that measures the model's performance in different positions to update the NLL loss for many-shot scenarios. We adjust the overall training objective of the many-shot sequence as follows:\n$L_{many-shot} = \\frac{1}{k}\\sum_{1-k} L_{many-shot_k} * A_k$"}, {"title": "3.2.4 Learning Strategy", "content": "The detailed process of the DR-ICL is presented in Algorithm 1. It enables the model to build upon prior knowledge at each iteration, avoiding uniform learning, thereby achieving progressive performance enhancement over long-term training."}, {"title": "4 Experiments", "content": "To delve into the exploration of many-shot ICL in LLMs, we need plenty of data across a wide range of k-shots. The datasets employed in MetaICL, like CROSSFIT (Ye et al., 2021) and UNIFIEDQA (Khashabi et al., 2020), have a notable constraint: their task lengths are generally centered around 100 tokens. This focus restricts the wide range of k-shot distributions, especially when the training sequence length is constrained. On the other hand, the Long-ICLBench dataset introduced by Li et al. (2024) significantly extends the length ranging from 1,000 to 50,000 tokens. Nonetheless, the dataset's limitation to a few hundred task instances renders it more suitable for inference rather than extensive training. In light of these limitations, we have developed the MICLB dataset. It encompasses 7 tasks of diverse difficulty levels and includes 50 datasets with average sample lengths per task that vary from 10 to 14,000 tokens. With the number of samples extending from the hundreds into the hundreds of thousands, the MICLB dataset ensures a substantial volume of data suitable for training and inference. More details can be found in the Appendix."}, {"title": "4.1.2 Base Models", "content": "We perform our experiments using two foundational models, namely Llama-2-7b-chat-hf and Mistral-7B-Instruct-v0.2. The base models are trained by different paradigms: \u2022 NFT (Touvron et al., 2023; Jiang et al., 2023): The foundational models with No Fine-tuning. \u2022 IT (Wei et al., 2021): Instruction Tuning foundational models with zero-shot examples. \u2022 MetaICL (Min et al., 2022): Fine-tuning foundational models with many-shot examples."}, {"title": "4.1.3 Evaluation Metrics", "content": "In our evaluation, we employ accuracy for assessing the performance of question answering, clustering, logical reasoning, classification, and retrieval tasks. For the summarization task, we utilize Distinct of trigram tokens (D3), ROUGE for unigrams (R1), and BLEU for unigrams (B1) as our metrics. In the case of reranking tasks, we apply standard ranking metrics, including Precision at k P@k, Recall at k R@k, and Normalized Discounted Cumulative Gain G@k."}, {"title": "4.1.4 Implementation Details", "content": "For the Llama-2-7b-chat-hf model, we configured the hyperparameter \u03b1 to 0.2, while for Mistral-7B-Instruct-v0.2, we set \u03b1 to 0.4. We set the parameter \u03b3 to counteract the effects of weight explosion, and our experiments identified 11 as the optimal value for this parameter. We determined that the optimal sampling size for S is 1, with the reweighted window size W set at 10. For details on the experimental hyperparameter settings, please refer to Appendix B.1. For all training and evaluation tasks, we utilized 8 A100 GPUs."}, {"title": "4.2 Results of Tasks", "content": "We validate our method on 12 datasets with both in-domain and out-of-domain tasks. Figure 3 compares baseline models on the CLSClusteringS2S dataset across different k-shots of Llama-2-7b-chat-hf and Mistral-7B-Instruct-v0.2. Table 1 shows summarization performance, while Table 2 details retrieval metrics. Results for question answering, clustering, and classification are summarized in Table 3, and Table 5 presents reasoning task performance. Our reranking experiments are shown in Table 6. Given Mistral-7B-Instruct-v0.2's superior performance on sequences over 4,000 tokens, we compare baseline variations across k-shots for the tasks with the highest k, as detailed in Table 4.\nAs shown in Tables 1, 2, 3, 5, and 6, DR-ICL significantly improves performance across various tasks. While MetaICL shows substantial fluctuations in k-shot performance on datasets like OpenbookQA and ARC, DR-ICL maintains more stable results. The slight advantage of our method over Meta-ICL is due to its focus on optimizing many-shot loss. IT's performance declines with increasing context length, as it relies solely on zero-shot, which is less effective in many-shot scenarios. Additionally, Llama-2-7b-chat-hf's 4,000-token limit causes performance on the ARC dataset to drop from 0.82 to 0.78 when k exceeds 50. Under the DR-ICL framework, among the 12 datasets tested, k = 0 led to a performance decrease on 5 datasets, no change on 2, and improvement on 5. Overall, performance improved by 0.5% with k = 0, remaining stable. For k > 0, datasets like CLSClusteringS2S showed continuous improvement, while DR-ICL effectively maintained performance stability as k increased across most datasets."}, {"title": "4.3 In-Context Learning Analysis", "content": "We observe that the foundation models underperform on both datasets. After fine-tuning, the IT strategy achieves its best in the few-shot. MetaICL, benefiting from many-shot training data, performs well at larger k-shot levels but still shows significant fluctuations. In contrast, DR-ICL delivers more stable results, with accuracy steadily improving as k increases. DR-ICL not only outperforms MetaICL in many-shot scenarios but also demonstrates faster loss convergence, as shown in Figure 6(a) in the Appendix B.1, indicating its tradeoff of many-shot and zero-shot demonstrations."}, {"title": "4.3.2 Performance Variance", "content": "We track the performance variance across all datasets with NFT(6.49E-03), IT(2.71E-02), MetaICL(2.38E-03), and DR-ICL(1.56E-03) as k varied. On average, DR-ICL has the lowest variance, suggesting a more stable performance with k-shot of demonstration changes. For details of the performance variance of each dataset please refer to Table 10 in Appendix B.2."}, {"title": "4.3.3 Data Noise Sensitivity", "content": "We compare DR-ICL with and without local reweighting by examining how loss variance trends for each k-shot demonstration during training. The reweighting window in DR-ICL reduces loss variance and effectively balances the impact of noisy data by appropriately weighting demonstrations. This reduction in sensitivity to data noise helps maintain stable performance as the number of demonstrations increases. For details of the noise variation please refer to Table 11 in Appendix B.3."}, {"title": "4.4 Ablation Studies", "content": "Figure 7, 8, and 6(b) illustrate the impact of varying the hyperparameters \u03b1, \u03b3, and S on the training of Llama-2-7b-chat-hf and Mistral-7B-Instruct-v0.2. For details of the study of hyperparameters please refer to Appendix B.1."}, {"title": "4.4.2 Global and Local Contribution", "content": "Table 7 displays the outcomes of DR-ICL when applying only global strategies or local strategies exclusively to the WinoWhy dataset. The results show that refining learning objectives via a global hyperparameter to trade off the performance and the local reweighting of demonstration examples can boost the LLMs' ICL capabilities."}, {"title": "4.4.3 Analysis of Window Size", "content": "Table 7 shows that increasing the window size improves performance. As the sequence length grows, the number of k-shots also increases. Relying only on data from position k 1 based on previous k 1-shot demonstrations can cause significant variability, amplifying the impact of data fluctuations. Expanding the sampling range helps mitigate this effect. We also tested sampling from positions 0 to k - 1, but found the model preferentially selected certain data points, which didn't fully reflect the model's overall performance. As a result, we selected a window size of 10."}, {"title": "5 Conclusions", "content": "To enhance the ICL capacity as context lengths grow and demonstration k-shots rise, we introduce the DR-ICL algorithm to tackle the inaccurate objective and noise. This innovative method strategically calibrates global training goals to prioritize many-shot examples over zero-shot ones and employs local reweighting of many-shot instances using cumulative advantages as dynamic rewards, steering the model toward effective learning trajectories. To substantiate the effectiveness of our approach, we have curated and released the MICLB dataset, characterized by its diverse tasks and a broad spectrum of text lengths and quantities. Our method demonstrates notable enhancements in both in-domain and out-of-domain tasks. We anticipate that our research will stimulate further exploration into ICL's potential and contribute to the advancement of LLM performance."}, {"title": "A Dataset", "content": "This provides a solid data foundation for the performance study of ICL in many-shot scenarios.\nICLB dataset includes the following tasks:\n\u2022QA: MMLU (Hendrycks et al., 2020), HellaSwag (Zellers et al., 2019), BoolQ (Clark et al., 2019), NarrativeQA (Ko\u010disk\u1ef3 et al., 2018), TruthfulQA (Lin et al., 2021), OpenbookQA (Mihaylov et al., 2018), ARC (Clark et al., 2018), and QUAC (Choi et al., 2018).\n\u2022Reasoning: GSM8K (Cobbe et al., 2021), APPS (Hendrycks et al., 2021), MATH (Hendrycks et al., 2021), BABI (Weston et al., 2015), and AR-LSAT (Zhong et al., 2021).\n\u2022Summarization: XSUM (Narayan et al., 2018) and CNN/DailyMail (Hermann et al., 2015).\nWe refer to the MTEB (Muennighoff et al., 2022) and C-MTEB (Xiao et al., 2023) that contains the description of the following dataset.\n\u2022Clustering: ArxivClusteringS2S, ArxivClusteringP2P, BiorxivClusteringS2S, BiorxivClusteringP2P, MedrxivClusteringP2P, RedditClustering, RedditClusteringP2P, StackExchangeClustering, StackExchangeClusteringP2P, CLSClusteringS2S, CLSClusteringP2P, ThuNewsClusteringS2S, BlurbsClusteringS2S, BlurbsClusteringP2P, TenkgnadClusteringS2S, TenkgnadClusteringP2P.\n\u2022Classification: AmazonPolarity, AmazonReviews, Emotion, ToxicConversations, TweetSentimentExtraction, JDReview, MultilingualSentiment, OnlineShopping, Waimai and WinoWhy (Zhang et al., 2020).\n\u2022Retrieval: MedQA, TREC-COVID, DuReaderRetrieval, EcomRetrieval, MMarco, MedicalRetrieval, T2R, and VideoRetrieval.\n\u2022Reranking: cMedQA and AskUbuntuDupQuestions."}, {"title": "A.2 Data Analysis", "content": "The statistics of data volume for each task can be referred to in Table 8, where the data volume for 50 tasks ranges from several hundred to hundreds of thousands of entries, providing ample data for the model's training and inference. The distribution of the number of tokens for tasks is between 10 and 14,000, as shown in Figure 4. When the many-shot fine-tuning sequence length is fixed, the k-shot number varies significantly. Figure 5 illustrates the k-shot distribution with a fixed training sequence length of 8,000, ranging between 0 and 350. When the training sequence length is increased to 32,000, the range of k-shot variation will exceed 1,000."}, {"title": "B.1 Evaluation", "content": "We evaluated our method on 12 datasets, each with 1,600 samples, totaling 19,200 test samples, and sampling rates ranging from 2% to 100%. For each dataset, we collected 16 types of demonstrations with various k-shot values, including 0, 1, 3, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, and 300. For reference, Table 9 presents the results from Table 3 for the total test set. The findings show that performance trends remain consistent across different sampling instances from each dataset."}, {"title": "B.2 Hyperparameters Study", "content": "\u03b1 plays a crucial role in determining the model's performance, as illustrated in Figure 7, the variance in performance between zero-shot and many-shot scenarios is model-dependent.\n\u03b3 adjusts the sensitivity of the rewards and makes the training process stable. Figure 8 illustrates that the best setting of \u03b3 is 11.\nS represents the loss computed from three randomly sampled positions within the sampling window to calculate the reward. A high value of S leads to non-representative sampling. From our experiments in Figure 6(b), we found that the best"}]}