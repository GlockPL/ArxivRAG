{"title": "Linear Attention for Efficient Bidirectional Sequence Modeling", "authors": ["Arshia Afzal", "Elias Abad Rocamora", "Leyla Naz Candogan", "Pol Puigdemont", "Francesco Tonin", "Yongtao Wu", "Mahsa Shoaran", "Volkan Cevher"], "abstract": "Transformers with linear attention enable fast and parallel training. Moreover, they can be formulated as Recurrent Neural Networks (RNNs), for efficient linear-time inference. While extensively evaluated in causal sequence modeling, they have yet to be extended to the bidirectional setting. This work introduces the LION framework, establishing new theoretical foundations for linear transformers in bidirectional sequence modeling. LION constructs a bidirectional RNN equivalent to full Linear Attention. This extends the benefits of linear transformers: parallel training, and efficient inference, into the bidirectional setting. Using LION, we cast three linear transformers to their bidirectional form: LION-LIT, the bidirectional variant corresponding to (Katharopoulos et al., 2020); LION-D, extending RetNet (Sun et al., 2023); and LION-S, a linear transformer with a stable selective mask inspired by selectivity of SSMs (Dao & Gu, 2024). Replacing the attention block with LION (-LIT, -D, -S) achieves performance on bidirectional tasks that approaches that of Transformers and State-Space Models (SSMs), while delivering significant improvements in training speed. Our implementation is available in github.com/LIONS-EPFL/LION.", "sections": [{"title": "1. Introduction", "content": "Transformers (Vaswani et al., 2017) are widely used in sequence modeling tasks such as causal language modeling (Brown et al., 2020; Gemini et al., 2023) due to their high performance and support for parallelized training. However, their quadratic cost is often limiting (Tay et al., 2020b), increasing interest in RNN-like models for inference.\nCausal linear attention was introduced as a replacement for softmax attention, using linear attention which is equiv-"}, {"title": "2. Preliminaries and Background", "content": "Notation. Matrices (vectors) are denoted by uppercase (lowercase) boldface letters, such as X for matrices and x for vectors. Scalars are represented by lowercase letters, e.g., x, and the Hadamard product is denoted by ."}, {"title": "2.1. Causal Linear Transformers: Transformers with Linear Attention", "content": "Given a data sequence \\(X = [X_1, X_2, ..., X_L]^T \\in \\mathbb{R}^{L \\times d}\\), a single-head softmax-attention uses a softmax function to normalize the attention scores:\n\\((q_i, k_i, v_i) = (W_q x_i, W_k x_i, W_v x_i),\\) (1)\n\\[ y_i = \\sum_{j=1}^L \\frac{exp(q_i^T k_j)}{\\sum_{p=1}^L exp(q_i^T k_p)} v_j, \\] (2)\nwhere \\(x_i, q_i, k_i, v_i, y_i \\in \\mathbb{R}^d\\) and the weights \\(W_q, W_k, W_v \\in \\mathbb{R}^{d \\times d}\\) with \\(d\\) being the projection dimension. With \\(Q := [q_1, ..., q_L]^T, K := [k_1, ..., k_L]^T, V := [v_1, \\dots, v_L]^T \\in \\mathbb{R}^{L \\times d}\\), we can then express the attention layer output as the following matrix form:\n\\[ Y = \\text{softmax}(Q K^T M_C) V, \\] (3)\nwhere \\(M_C \\in \\{- \\infty, 1\\}^{L \\times L}\\) is a causal mask for preventing future tokens to attend to past. Such matrix form is crucial for parallelized training over the sequence length.\nIn contrast, (2) is used during inference for generating or processing tokens. However, for causal Transformers (Kojima et al., 2022), employing (2) requires storing the previous L tokens to attend to the latest token during inference (i.e., the KV cache). This approach is less efficient than RNNs, where only the state is stored regardless of the previous sequence (cf., (Orvieto et al., 2023)).\nKatharopoulos et al. (2020) introduces Linear Attention which replaces the exponential kernel \\(exp(q_i^T k_j)\\) with feature map function \\(\\phi(q_i) \\phi(k_j)\\) where \\(\\phi(\\cdot) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{\\tilde{d}}\\) maps the input to a higher-dimensional space. For simplicity of notation, we use \\(q_i := \\phi(W_q x_i)\\) and similarly for \\(k_i := \\phi(W_k x_i)\\) in the sequel. This formulation allows the linear transformer to be expressed as an RNN with linear recurrence. This structure eliminates the need to store previous tokens during inference, while still enabling parallelized computation during training:\n\\(Y = SCALE(Q K^T \\odot M_C) V,\\) (4)\n\\(S_i = S_{i-1} + k_i v_i^T, Z_i = Z_{i-1} + k_i, y_i = \\frac{q_i^T S_i}{q_i^T Z_i}\\) (5)\nHere, the causal mask \\(M_C \\in \\{0, 1\\}^{L \\times L}\\) is a lower triangular matrix that enforces causal constraints. \\(SCALE(\\cdot)\\) denotes the scaling of the attention matrix across its rows (\\(SCALE(A)_{ij} = \\frac{A_{ij}}{\\sum_{p=1}^L A_{ip}}\\)) and \\(S_i \\in \\mathbb{R}^{d \\times d}\\) and \\(z_i \\in \\mathbb{R}^d\\) are the hidden state matrix and the vector used for scaling. Eq. (4) can also written as \\(Y = P V\\) with \\(P \\in \\mathbb{R}^{L \\times L}\\) known as sequence mixer (Hwang et al., 2024).\nThis models with 2D hidden state also known as \"fast weights\" (Hinton & Plaut, 1987; Schmidhuber, 1992) and their connection to transformers were explored in (Schlag et al., 2021a)."}, {"title": "2.2. Chunkwise Parallel Form of Linear Transformers", "content": "Causal Linear Transformers balance the memory-speed tradeoff during training by employing chunkwise parallelization (Yang et al., 2023; 2024; Dao & Gu, 2024). In this approach, the input sequence \\(X \\in \\mathbb{R}^{L \\times d}\\) and the corresponding query, key, and value vectors are split into non-overlapping chunks, each of size \\(C\\). Let \\(Q^{[i]}, K^{[i]}, V^{[i]} \\in \\mathbb{R}^{C \\times d}\\) represent the chunked query, key, and value matrices, and define the chunk-level hidden state after processing \\(i\\) chunks as \\(S^{[i]} \\in \\mathbb{R}^{d \\times d}\\). The causal sequence mixer which maps the input to output \\(Y = P V\\) is expressed as:\n\\[ S^{[i]} = S^{[i-1]} + \\sum_{j=iC+1}^{i(C+1)} k_j v_j^T\\] (9)\n\\[Y^{[i]} = Q^{[i]} S^{[i]} + (Q^{[i]} K^{[i]T} \\odot M_C) V^{[i]} \\] (10)\ninter\nintra\nThe above form is the chunkwise form of Linear Transformer without decay factor and the mask \\(M_C \\in \\{0, 1\\}^{L \\times L}\\) is the causal mask. Chunking is essential for training Linear Transformers and SSMs with decay factors, such as Mamba-2 (Dao & Gu, 2024) and GLA (Yang et al., 2023) even more, since treating the entire sequence mixer \\(P\\) as a single intra-chunk block, is numerically unstable. This instability arises from computing the cumulative product"}, {"title": "3. LION: Casting Full Linear Attention as a Bidirectional RNN", "content": "In this section, we construct the full linear attention formulation Eq. (11), derive its equivalent bidirectional RNN Eq. (27), and introduce a chunking strategy for full linear attention Eq. (30). These formulations are presented in a box to highlight their equivalence, demonstrating various inference strategies within our framework. The sections are organized as follows: Section 3.2 derives the bidirectional RNN for full linear attention, Section 3.4 presents an efficient chunkwise parallel method to balance memory and speed during inference, and Section 3.3 addresses stable training using selective and fixed masks."}, {"title": "3.1. Full Linear Attention", "content": "In bidirectional sequence modeling tasks, the entire sequence is available during both training and inference. For softmax-based attention, this results in the form:\n\\(Y = \\text{softmax} (Q K^T) V\\)\nwithout the need for causal masking. For Linear Transformers, as a natural extension of the causal Linear Transformer we write the full Linear Attention as:\n\\(Y = SCALE (Q K^T \\odot M) V\\) (11)\nwith \\(M\\) being a full matrix including both upper and lower triangular parts created based on \\(\\lambda_i\\). The full mask \\(M\\) in"}, {"title": "3.2. Equivalent Bi-directional RNN for Full Linear Attention", "content": "Since we aim to derive an equivalent RNN for full scaled Linear Attention we first show that summing two causal Linear Transformers in RNN form (a1 equation below) is not equal to full linear attention (a2 equation below):\nObservation 3.1. Considering the following bidirectional recurrence equations:\n\\[ \\begin{aligned} a1) \\; S^{F/B}_i &= \\Lambda_i S^{F/B}_{i-1} + k_i v_i^T, z^{F/B}_i &= \\Lambda_i z^{F/B}_{i-1} + k_i, y^{F/B}_i &= \\frac{q_i^T S^{F/B}_i}{q_i^T z^{F/B}_i}\\\\ &\\neq a2) \\; Y = SCALE(Q K^T \\odot M) V \\end{aligned} \\] (13)\n\\[ F/B \\] indicates that the same recurrence is applied in both forward and backward directions, in the following content, when doing backward recurrence, the subscript of S, z, y, q, k, v should be flipped by the rule of \\(i := L - i + 1\\). The final output is the addition of the forward and the backward recurrences, i.e., \\(y_i = y_i^F + y_i^B, \\forall i \\in \\{1, ..., L\\}\\).\nFigure 2 illustrates that the summation of two Linear Transformers in opposite directions does not equal full Linear Attention in its parallel form (proofs are provided in Appendix C.1). This discrepancy arises from two factors: (i) dual counting of diagonal elements, as they are included in both forward and backward directions, and (ii) separate scaling applied to forward and backward directions, whereas full Linear Attention (14) scales the attention matrix over the entire sequence similar to softmax-based attention (\\(Y = \\text{softmax} (Q K^T) V\\)).\nWe precede our proof considering the unified formulation of causal Linear Transformer with scalar selective decay shown at Equation (6):\nProposition 3.2. Considering the following forward recurrence:\n\\[ S_i = \\Lambda_i S_{i-1} + k_i v_i^T, \\; z_i = \\Lambda_i z_{i-1} + k_i, \\; y_i = \\frac{q_i^T S_i}{q_i} \\] (15)\nThe parallel form of output is:\n\\[ Y = \\begin{cases} (SCALE(Q K^T \\odot M))V, \\\\ M = \\begin{cases} \\prod_{k=j+1}^i \\lambda_k, i \\geq j, \\\\\\ 0, i < j. \\end{cases} \\end{cases} \\] (16)\nOur goal is to derive a bidirectional RNN for Eq. (11), as this framework is more generalized and can be adapted to various Linear Transformers models (proof and more detail on different variation like scaling prior to masking\\(SCALE(Q K^T) \\odot M\\) are provided at Appendix C.1). Motivated by Eq. (16) and the observation of how the attention matrix is divided into causal and non-causal components, we begin our method by splitting the attention matrix and the mask into upper and lower triangular parts:"}, {"title": "3.3. Stable and Fast Implementation of Attention Masks", "content": "As \\(Y = SCALE(Q K^T \\odot M) V\\) represents the parallel form for training with the mask M, it is crucial that: (i) The mask is created fast during training for models trained using LION, particularly LION-S and LION-D. (ii) The mask ensures stable training with full attention. Below we describe details for selective and fixed decay masks:\nStability and implementation of selective mask ( LION-S ): Observing from Eq. (18), the upper (MB) and lower (MF) triangular parts of the mask M are rank-1 semi-separable matrices (proof in Appendix C.6), enabling efficient computation via matrix multiplications. During training, the decay factors \\(\\lambda_i\\) are stacked into \\(A_F \\in \\mathbb{R}^L\\), and the cumulative product \\(L_F = cumprod(A_F) = \\prod_{i=0}^{k}\\lambda_i\\) is used to generate the lower triangular mask MF. For the upper triangular mask MB, the input sequence is flipped, and the decay factors are computed as \\(A_B = FLIP(A_F)\\), with \\(L_B = cumprod(A_B)\\). The masks are then calculated as:\n\\[ M^F = Tril(L_F L_F^T), \\; M^B = Triu(L_B L_B^T), \\]\nwhere Tril(X) and Triu(X) only output the lower and upper triangular part of the matrix X, respectively. The full mask is then obtained using \\(M = M^F + M^B - I\\) as shown in Equation Eq. (18). To improve numerical stability, the selective scalar \\(\\lambda_i\\) is designed in exponential form \\(\\lambda_i = e^{a_i}\\) (coming from Zero-Order Hold (ZOH) discretization, see Appendix B.8). This results in the cumulative sum:\n\\[ \\begin{aligned} &D_{ij}^F = \\begin{cases} \\sum_{k=j+1}^i a_k & \\text{if } i > j,\\\\\\ \\sum_{k=i+1}^j a_k & \\text{if } i < j,\\\\\\ 0 & \\text{if } i = j, \\end{cases} \\\\\\ &M^F = exp(D^F), \\end{aligned} \\]\nwhere \\(exp(\\cdot)\\) is applied element-wise, the same process holds for MB by flipping the input sequence order. Here, \\(D^{F/B} = cumsum(a^{F/B})\\), with \\(a \\in \\mathbb{R}^L\\) containing the selective exponents \\(a_i\\).\nHowever, ensuring stability remains critical, as \\(L^{F/B}\\) can overflow or underflow when forming the full mask without chunking (Dao & Gu, 2024). To address this, we define \\(a_i = \\text{log}(\\sigma(W x_i + b))\\), where \\(\\sigma\\) is the sigmoid function. This approach ensures stability by bounding \\(a_i\\) within the interval \\([0, 1]\\) (Orvieto et al., 2023). An ablation study on alternative activations, such as Mamba's activation, is provided in Appendix D.3.\nStability and implementation of fixed mask ( LION-D ): By fixing \\(\\lambda_i = \\lambda\\), the mask M has the form:\n\\[ M_{ij} = \\lambda^{|i - j|}, D_{ij} = |i - j| \\text{log}(\\lambda). \\] (29)\nM above is a Toeplitz mask (Qin et al., 2023) and therefore, creating the decay mask can be made even faster using"}, {"title": "3.4. LION Chunk: Chunkwise Parallel form of Full Linear Attention", "content": "Chunking Full Linear Attention is simpler than for causal Linear Attention since there is no intra-chunk. Considering the chunks for queries, keys and values as \\(Q^{[i]}, K^{[i]}, V^{[i]} \\in \\mathbb{R}^{C \\times d}\\) with chunk size being \\(C\\) and total number of \\(N = \\frac{L}{C}\\) chunks, we can chunk the full Linear Attention as:\n\\(A^{[ij]} = Q^{[i]} K^{[j]T} \\odot M_{[ij]}, \\) (30)\n\\(C^{[ij]} = C^{[ij-1]} + \\text{Sum}(A^{[ij]}),\\) (31)\n\\(S^{[iN]}\\)\nwhere Sum operations applies summation over the row of the input matrix. The chunk hidden states \\(C^{[ij]}\\) and \\(S^{[ij]}\\) iterate over \\(j\\), with the final output for chunk \\(i\\) computed using their last values at \\(j = N\\). The chunk mask \\(M_{[ij]}\\) corresponds to a submatrix of the full attention mask from Eq. (18), defined as:\n\\[ M_{[ij]} = M_{i C + 1 : i (C + 1), j C + 1 : j (C + 1)} \\in \\mathbb{R}^{C \\times C}. \\]\nFor further visualizations and details, refer to Appendix C.9. Below, we construct both fixed and selective chunked masks \\(M_{[ij]}\\). For the fixed mask, we have:\n\\[ M_{[ij]} = \\begin{cases} (L_E^F)^T \\Gamma^{ij}, & \\text{if } i > j,\\\\\\ (L_E^B)^T \\Gamma^{ij}, & \\text{if } i < j,\\\\\\ (\\frac{(L_E^F)^T + (L_E^B)^T}{\\Gamma} - I) & \\text{if } i = j, \\end{cases} \\] (33)\nwith \\(L_E \\in \\mathbb{R}^C\\) and \\(\\Gamma \\in \\mathbb{R}^{C \\times C}\\) being the vector and matrix used for creating the mask \\(M_{[ij]}\\) and they are only depending on the decay parameter \\(\\lambda\\) and the chunk size \\(C\\). For the selective mask we have:\n\\[ M_{[ij]} = \\begin{cases} \\text{Tril}(L^F) \\frac{L_I^F 1^T}{L_I^F[0]}, & \\text{if } i > j,\\\\\\ \\text{Tril}(L^B) \\frac{L_I^B 1^T}{L_I^B[0]}, & \\text{if } i < j,\\\\\\ (\\frac{\\text{Tril}(L^F) + \\text{Tril}(L^B)}{\\Gamma} - I) & \\text{if } i = j, \\end{cases} \\] (34)\n\\[ \\begin{aligned} &L^F = cumprod(A_F)_{i C + 1 : (i + 1) C}, \\\\\\ &L^B = cumprod(A_B)_{i C + 1 : (i + 1) C}. \\end{aligned} \\] (35)\nwhe \\(X^F, X^B\\) are the vectors containing all decay parameters for forward and backward directions as defined in Section"}, {"title": "4. Experiments", "content": "Our main results focus on well-known bidirectional sequence modeling tasks: Image Classification on ImageNet-1K (Russakovsky et al., 2015) and Masked Language Modeling (MLM) on the C4 dataset (Dodge et al., 2021). We evaluate the LION family on these tasks Additionally, we conduct experiments on the LRA dataset to ensure the stability of our framework. For Causal Language Modeling and additional experiments, we refer to Appendix A and D."}, {"title": "4.1. Baselines", "content": "We evaluate the LION framework using its adapted bidirectional Linear Transformers LION-S, LION-D, and LION-LIT and compared them with softmax-based Transformers, including ViT (Dosovitskiy et al., 2021) and DeiT (Touvron et al., 2021), as well as bidirectional SSMs such as Vision Mamba (Vim) (Zhu et al., 2024) and Hydra (Hwang et al., 2024) for image classification. For Masked Language Modeling, we compare LION against BERT (Fu et al., 2023)."}, {"title": "4.2. Image Classification", "content": "To enhance the spatial representation of patches (tokens) for image classification, we designed different masks for LION-S and LION-D by altering token order. This strategy resulted in the LION-D and LION-S\u00e2\u00aa models, which naturally capture spatial information of patches while"}, {"title": "4.5. Long Range Arena Stability Ablation", "content": "We evaluated LION-S, LION-D, and LION-LIT on the Long Range Arena (LRA) benchmark to verify the stability of training with full masked attention. We expand the dimensions of \\(A_i\\) (as described in Appendix C.7) and initialized \\(a_i\\) based on HIPPO theory (Gu et al., 2020) for LION-S and LION-D to solve LRA tasks. However, LION-LIT, a bidirectional Linear Transformer without a decay factor, was unable to solve LRA tasks. Additionally, randomly initializing LION-S and LION-D were not able to solve LRA consistent to prior work (Amos et al., 2023). For LRA tasks, the selectivity of LION-S takes the form \\(a_i = \\sigma(A_i) + B_i\\), where \\(B_i\\) is initialized using HIPPO-based diagonal initialization (Gupta et al., 2022). We also observed that omitting attention scaling led to poor performance, indicating that scaling attention plays a crucial role in improving training stability (more ablations at Table 6 & Appendix D.3)."}, {"title": "5. Conclusions", "content": "We introduce LION, a framework that formulates full linear attention as a bidirectional RNN, enabling linear transformers to achieve the training speed of softmax-based Transformers while maintaining the inference efficiency of RNNs and SSMs for bidirectional tasks. LION enjoys stable training with 9\u00d7 faster speed than Vision Mamba and ~ 2\u00d7 faster than Hydra, while maintaining competitive performance in tasks like image classification and masked language modeling. Furthermore, LION Chunk, our chunkwise parallelization strategy, balances memory and speed during inference, offering flexibility based on resource availability. Our results demonstrate LION'S effectiveness in balancing performance, scalability, and efficiency for bidirectional sequence modeling."}, {"title": "6. Limitations & Future Work", "content": "Our experiments focused on three main mask choices, but LION has the potential to accelerate other Linear Transformer variants mentioned in Appendix C.5 for bidirectional tasks. The chunkwise parallel implementation during inference was done in PyTorch, leaving room for optimization through GPU kernel programming to reduce I/O overhead and improve speed. Additionally, Hydra and Mamba activations led to unstable training under full attention, suggesting LION could be used to stabilize these variants in the future."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}]}