{"title": "Exploring Memorization and Copyright Violation in Frontier LLMs: A Study of the New York Times v. OpenAI 2023 Lawsuit", "authors": ["Joshua Freeman", "Chloe Rippe", "Edoardo Debenedetti", "Maksym Andriushchenko"], "abstract": "Copyright infringement in frontier LLMs has received much attention recently due to the New York Times v. OpenAI lawsuit, filed in December 2023. The New York Times claims that GPT-4 has infringed its copyrights by reproducing articles for use in LLM training and by memorizing the inputs, thereby publicly displaying them in LLM outputs. Our work aims to measure the propensity of OpenAI's LLMs to exhibit verbatim memorization in its outputs relative to other LLMs, specifically focusing on news articles. We discover that both GPT and Claude models use refusal training and output filters to prevent verbatim output of the memorized articles. We apply a basic prompt template to bypass the refusal training and show that OpenAI models are currently less prone to memorization elicitation than models from Meta, Mistral, and Anthropic. We find that as models increase in size, especially beyond 100 billion parameters, they demonstrate significantly greater capacity for memorization. Our findings have practical implications for training: more attention must be placed on preventing verbatim memorization in very large models. Our findings also have legal significance: in assessing the relative memorization capacity of OpenAI's LLMs, we probe the strength of The New York Times's copyright infringement claims and OpenAI's legal defenses, while underscoring issues at the intersection of generative AI, law, and policy.", "sections": [{"title": "1 Introduction", "content": "The generative AI market has grown rapidly since OpenAI released ChatGPT in 2022 [Cerullo, 2023]. As competitors have emerged in the large language model (LLM) space, they all have followed a similar training approach: feeding multi-billion-parameter models massive amounts of data scraped"}, {"title": "2 Background", "content": "GPT-2 [Radford et al., 2019] was a breakthrough language model that demonstrated strong zero-shot learning capabilities across multiple NLP tasks. While initial research focused on these models'\nAs suggested above, the emergence of LLMs, and their alleged propensity to memorize, has spurred numerous lawsuits. The ongoing The New York Times Company v. Microsoft Corp. lawsuit was the first in a series of cases testing the boundaries of copyright law in the face of generative AI. Other relevant cases include: Chabon v. OpenAI, Inc., a class action lawsuit waged by authors alleging that OpenAI's use of their works to train its models constituted copyright infringement; and Doe v. GitHub, Inc., a class action alleging that Github Codex and Github Copilot, automated software development tools, produce verbatim copies of plaintiffs' source code without abiding by terms of the code's applicable licenses, in violation of the Digital Millenium Copyright Act (\u201cDMCA\u201d).\nMemorization causes and metrics. Studies such as Carlini et al. [2021] have demonstrated that straightforward attacks can extract verbatim training data, including personally identifiable information. Research by Lee et al. [2022] identified duplicate data in training sets as the primary known cause of memorization, a finding later supported by Chen et al. [2024]. Additionally, Carlini et al. [2023] identified a third factor in eliciting memorization: attacks using increased context tokens. Different metrics and experimental setups have been developed to measure memorization in text generation. While exact reproduction of an entire article clearly demonstrates memorization, there are numerous ways to assess partial or near-verbatim reproduction, both qualitatively and quantitatively. Since the 1960s, researchers have needed automated methods to measure textual similarity for tasks like information retrieval, plagiarism detection, and computational linguistics. Various metrics have emerged for different purposes, several of which are relevant to our work (see section 3). The Levenshtein distance [Levenshtein, 1966] provides the most fundamental approach to quantifying string similarity by measuring the minimum number of edits required to transform one string into another. BLEU [Papineni et al., 2002], originally designed for automated evaluation of machine translation, primarily analyzes the overlap of n-grams between generated and reference texts. ROUGE [Lin, 2004] was developed to evaluate machine-generated summaries by comparing their word sequences and word pairs with human-written reference summaries. Although neither BLEU nor ROUGE was initially intended for detecting verbatim copying, researchers have adapted them for this purpose [Wei et al., 2024]. BLEU is particularly effective for identifying exact matches with copyrighted text because it focuses on n-gram precision, measuring how many sequences in the generated text appear in the reference text. In contrast, ROUGE is better suited for evaluating overall content coverage rather than exact matching. More recent work has introduced new approaches to studying LLM memorization. For instance, [Sonkar and Baraniuk, 2024] employed the longest verbatim match to compare sequence similarity, using statistical methods like Kolmogorov-Smirnov testing to analyze distributional differences in the longest common substrings under specific attack conditions. Our methodology draws inspiration from all these approaches.\nMemorization mitigations. After initial training and instruction tuning, LLMs undergo further alignment with their intended purpose (whether general or task-specific) through Reinforcement Learning from Human Feedback (RLHF) [Christiano et al., 2023]. This process involves humans evaluating and ranking model responses according to specific criteria, which can help reduce undesired memorization. Various approaches have been developed to address the memorization challenge. One strategy involves identifying and separating copyrighted or sensitive data for specialized processing. While completely removing such data from the training set is possible, this approach risks eliminating valuable, non-sensitive information that often coexists within copyrighted materials. An alternative method focuses on teaching models to selectively forget sensitive data [Golatkar et al., 2020]. This technique, known as model unlearning, has demonstrated effectiveness with LLMs [Yao et al., 2024]. However, unlearning remains computationally intensive and, like complete removal, risks excluding non-copyrighted content embedded within copyrighted works\u00b9. Moreover, robust unlearning still remains an open research problem [Lynch et al., 2024, \u0141ucki et al., 2024]. Differential Privacy (DP) offers a mathematical framework enabling model training while preserving the privacy of specified portions of the training dataset. Research has shown that these methods can effectively reduce verbatim memorization during LLM fine-tuning [Behnia et al., 2022]. However, like other approaches, DP involves a fundamental trade-off between model utility and privacy protection. While maximum privacy would mean revealing no information from sensitive data, this approach would also conceal non-sensitive facts within that data. Consequently, developing an optimal in-training memorization mitigation technique remains an active area of research. To address these limitations, researchers have developed post-generation solutions to prevent the output of memorized sensitive"}, {"title": "Legal aspects of memorization", "content": "A legal claim of copyright infringement generally requires showing that a defendant made an unauthorized copy (a \u201csubstantially similar\" reproduction) or other unlawful use of a work subject to a valid copyright. See 17 U.S.C. \u00a7 106. The New York Times alleges that infringement of its news articles has occurred at multiple stages in the training and use of ChatGPT, including when the copyrighted articles were allegedly reproduced as training data and subsequently, when certain articles were \u201cmemorized\u201d and regurgitated in ChatGPT outputs. Memorization in LLMs can be thought of as an application of the idea/expression doctrine in copyright law. This doctrine underscores the dichotomy between abstract ideas (which are generally not copyrightable) and the original expression of such ideas (which may be copyrightable). LLMs are trained to identify abstract features and relationships in training data (where the data itself might be copyrightable but not the LLM's mathematical inferences about such data). When memorization occurs, the LLM has not adequately \u201cabstracted\u201d its inferences about the data, thereby increasing the risk that its outputs will infringe.\nOpenAI's argument from OpenAI [2024] that memorization is a bug rather than a feature of ChatGPT, as supported by the research herein, could influence the court's analysis with respect to the defendants' defenses to these infringement claims. In particular, if heeded, OpenAI's stance could influence the court's analysis of fair use as a defense to direct infringement or of the New York Time's claim for contributory infringement in the context of a publicly accessible LLM. For context, fair use is a copyright law doctrine that negates a finding of copyright infringement if the court, upon balancing four statutory factors, determines the use is \u201cfair\". 17 U.S.C. \u00a7 107. Case law, the use at issue and even policy concerns may inform this balance, making the analysis highly context specific and at times unpredictable. Furthermore, contributory infringement is a doctrine through which a product provider can be found liable for copyright infringement performed by a product user [Sony Corp. of Am. v. Universal City Studios, Inc., 464 U.S. 417, 104 S. Ct. 774, 78 L. Ed. 2d 574, 1984]. The implications of this research on these doctrines as applied to this case will be discussed in greater depth later in Section 5.\nThis paper will not focus on certain legal issues raised in the Complaint, including the copyright implications of \"synthetic search\" associated with certain Microsoft Bing products. This feature enables the relevant Bing products to scrape the internet for data in real time. The disparity in ability to reconstitute pieces of the New Articles (out of the training set of the LLMs) versus articles from before the model publication ostensibly shows that none of the LLMs tested have the ability to access NYT articles in real time. We will further not focus on potential DMCA violations and the allegation that the LLM itself infringes The New York Times' copyrights. Instead, this paper will focus on copyright infringement with respect to LLM training inputs and LLM outputs that do not rely on contemporaneous internet searches."}, {"title": "3 Methodology", "content": "Data collection. First, we curate three sets of articles from the New York Times by hand. One set of articles corresponds to 99 articles from Exhibit J. The other one is a set of 100 arbitrary articles published no later than December 27th, 2022 that do not overlap with the articles from Exhibit J. The intention is to make it likely that they would have been in the model's training set. Lastly, 90 articles published after the end of training of any models experimented on were included, published no earlier than July 5th, 2024. To refine the data, mentions of images or other pieces of text that were not part of the article text were removed. In addition, the title was removed, leaving only the article's text and a line mentioning the author's name(s) (i.e., By Jane Doe, John Smith, and Ada Lovelace).\nAttacks. In The New York Times Company [2023], the New York Times alleges that it submitted short subtexts of articles to the GPT-4 model, thus extracting 2000 character-long near-exact matches to the articles. We were not able to reproduce those strong claims. There are two possible reasons for this. Either the models analyzed differ from when the New York Times sued or because an element is omitted, such as a system prompt or other parts of the extraction attack. If the temperature, a setting for the stochasticity of the generation, with 0 being fully deterministic, were not 0, that would also explain the difficulty of reproducing the attacks. We took a multi-staged approach to get as close as"}, {"title": "A Experimental Settings", "content": "The prompts we used as attacks can be summarized in Figure 2, as well as Figure 4. Note that Anthropic has a stricter API rules that force the input to start with a User prompt. We thus had to add a User prompt, which we chose to be of the form \"User: Get it?\", for attacks 2 and 3."}, {"title": "B Additional Tables and Figures", "content": null}, {"title": "5 Further Legal Discussion", "content": "Fair Use. As this case could be the first to apply fair use analysis to the reproduction and public display of training data, it could have a significant effect on the present and future use of LLMs, in addition to other copyright-based industries. On the one hand, if the court holds that OpenAI engaged in fair use, LLM developers may be able to avoid the potentially crippling costs of injunctive relief (for example, a court order to remove the training inputs from an existing LLM would necessitate retraining a model from scratch). On the other hand, this outcome could harm the financial interests of news companies while compelling them to implement stronger IP (Intellectual Property) protection strategies.\nIf accepted to establish that the typical use of ChatGPT does not result in memorized outputs, the research in this paper may bolster OpenAI's fair use defenses with respect to both use of the inputs and regurgitation in the outputs. First, with respect to infringement at the input stage, this research could affect consideration of the fourth statutory fair use factor: \u201cthe effect of the [allegedly infringing] use upon the potential market for or value of the copyrighted work\u201d. 17 U.S.C. \u00a7 107(4). A primary consideration for this factor is \"whether defendant's utilization functions as a market substitute for plaintiff's work\" (see Nimmer and Nimmer [2024] \u00a7 13F.08). Legal scholars have speculated that"}, {"title": "6 Conclusions", "content": "While some degree of memorization in LLMs may be inevitable, the New York Times' complaint presents an incomplete picture of verbatim memorization in ChatGPT. Our research indicates that ChatGPT and similar LLMs typically exhibit less verbatim memorization of arbitrary news articles than the New York Times may suggest, though frequently republished articles (including those from the New York Times) are much more likely to be memorized. Among the four LLM families we evaluated, OpenAI's models demonstrated the least amount of memorization in absolute terms, at least half a year after the lawsuit (i.e., when our experiments were done). We also confirmed previous research showing that memorization risk increases with both model size and content duplication frequency. Our analysis assumes that average-case memorization scenarios are most relevant for copyright considerations; from a privacy perspective, even a single instance of a model verbatim reproducing sensitive training data would be considered problematic.\nThis research opens up several avenues for future investigation, particularly regarding the reduction of verbatim text reproduction from copyrighted materials. One promising direction would be to"}]}