{"title": "HESPI: A PIPELINE FOR AUTOMATICALLY DETECTING\nINFORMATION FROM HEBARIUM SPECIMEN SHEETS", "authors": ["Robert Turnbull", "Karen Thompson", "Emily Fitzgerald", "Joanne L. Birch"], "abstract": "Specimen associated biodiversity data are sought after for biological, environmental, climate, and\nconservation sciences. A rate shift is required for the extraction of data from specimen images to\neliminate the bottleneck that the reliance on human-mediated transcription of these data represents.\nWe applied advanced computer vision techniques to develop the 'Hespi' (HErbarium Specimen\nsheet PIpeline), which extracts a pre-catalogue subset of collection data on the institutional labels on\nherbarium specimens from their digital images. The pipeline integrates two object detection models;\nthe first detects bounding boxes around text-based labels and the second detects bounding boxes\naround text-based data fields on the primary institutional label. The pipeline classifies text-based\ninstitutional labels as printed, typed, handwritten, or a combination and applies Optical Character\nRecognition (OCR) and Handwritten Text Recognition (HTR) for data extraction. The recognized text\nis then corrected against authoritative databases of taxon names. The extracted text is also corrected\nwith the aide of a multimodal Large Language Model (LLM). Hespi accurately detects and extracts\ntext for test datasets including specimen sheet images from international herbaria. The components\nof the pipeline are modular and users can train their own models with their own data and use them in\nplace of the models provided.", "sections": [{"title": "Introduction", "content": "There are an estimated 3,500 active herbaria globally, con-\ntaining approximately 398 million physical specimens\n[Thiers, updated continuously]. Specimen-associated bio-\ndiversity data are sought after for biological, environmen-\ntal, climate, and conservation sciences [Lacey et al., 2017,\nDavis, 2023]. Increased investment in natural history\ncollections-based digitization efforts has significantly in-\ncreased the availability of high-resolution specimen im-\nages in the last decade [Davis, 2023, Walker et al., 2022].\nHowever, transcription rates associated with the digitiza-\ntion of biodiversity data have remained flat [Vollmar et al.,\n2010, Guralnick et al., 2024]. A rate shift for data ex-\ntraction from specimen images is required to eliminate\nthis impediment to the mobilization of biodiversity data.\nAdvanced computer vision techniques hold the potential\nto achieve that rate shift by reducing the time required\nfor digitization of text-based label data and, in doing so,\nto mobilize vast quantities of biodiversity data from digi-\ntal specimen images [Thompson and Birch, 2023, Walker\net al., 2022]. Natural Language Processing methods also\nhave the potential to increase accuracy of text digitization\nfrom specimen labels by enabling language detection and\nterminology extraction Owen et al. [2020]. This paper\nintroduces 'Hespi' a HErbarium Specimen sheet PIpeline.\nHespi detects components of specimen sheets, detects the\nfields in the institutional labels and recognizes the text\nusing Optical Character Recognition (OCR), Handwritten\nText Recognition (HTR) and multimodal Large Language\nModels (LLMs)."}, {"title": "Background", "content": "Herbarium specimens contain a preserved biological sam-\nple and both primary (e.g. taxonomic identity, collector,\ncollection date or location) and secondary (e.g. redeter-\nmination or confirmation of taxonomic identity, date of\ncuration event) collection data. These specimens provide\na verifiable record of the presence of a taxon at a point\nin time [Funk et al., 2018, Kirchhoff et al., 2018]. His-\ntorically, specimen-associated data were documented on\npaper [Groom et al., 2019, Walton et al., 2020]; recorded in\nfield notebooks, transcribed into printed catalogs, and pri-\nmary and secondary data were written on labels that were\nattached to the specimens. Mobilization of these speci-\nmen data is typically achieved by processing specimens\nthrough a digitization workflow, involving the production\nof a digital specimen image followed by the extraction\nof text data from that digital image either manually (i.e.,\nvia a human intermediary) or semi-automatically [Thomp-\nson and Birch, 2023, de la Hidalga et al., 2020, Kirchhoff\net al., 2018, Nelson et al., 2015]. Their digitization, con-\nforming to biological data standards [e.g., ABCD (Access\nto Biological Collections Data) [Holetschek et al., 2012]\nand DarwinCore [Wieczorek et al., 2012]], is essential\nfor maintaining their accuracy and ensuring their avail-\nability for reuse [Groom et al., 2019]. The conversion of\nimaged labels into digital text and the parsing of that text\ninto standard data fields are some of the slowest steps in\nthe digitization pipeline and are significant bottlenecks for\nbiodiversity mobilization [Groom et al., 2019, Gural-\nnick et al., 2024, Walton et al., 2020, Tulig et al., 2012,\nKirchhoff et al., 2018].\nDeep learning models using artificial neural networks have\nshown to be effective for data extraction from digital im-\nages, including phenological [Pearson et al., 2020, Mora-\nCross et al., 2022] or morphological [Mora-Cross et al.,\n2022, Wilson et al., 2023] data, taxonomic identifications\nShirai et al., 2022], and other textual data [Walker et al.,\n2022, Guralnick et al., 2024, Milleville et al., 2023]. Such\ntechniques hold potential to reduce the reliance on human-\nmediated transcription and processing of specimen data\n[Owen et al., 2020, Milleville et al., 2023]. Neural network\nmodels require large datasets of carefully curated and la-\nbeled images for training, to create models that achieve\ngood performance [Walker et al., 2022]. The optimal tech-\nniques and parameters for tasks required for data extraction\nfrom herbarium specimens continue to be elucidated. We\nhave previously described training a deep learning model to\ndetect the various components of a specimen sheet [Thomp-\nson et al., 2023a]. Here we extend this approach as part\nof a larger pipeline to extract textual information from\ninstitutional labels to enable their digitization.\nOptical Character Recognition (OCR) protocols have long\nbeen recognized as holding potential for mobilization of\ntextual data from specimens. However, this potential has\nnot yet been realized due to limitations in the accuracy\nof data extraction using OCR software. Workflows have\nprogressed from applying OCR to whole specimen sheets\nDrinkwater et al., 2014, Haston et al., 2012, Tulig et al.,\n2012], to (manually) identifying the label area and apply-\ning OCR [Alzuru et al., 2016, Anglin et al., 2013, Barber\net al., 2013, Dillen et al., 2019, Haston, 2015]. It has been\ndemonstrated that applying OCR to the label-only image\nis more effective than applying OCR to the whole image\nAlzuru et al., 2016, Haston, 2015, David et al., 2019],\nand that running OCR over individual text lines cropped\nfrom a label image is faster than processing the whole label\nDavid et al., 2019].\nThe capture of handwritten text is one of the most chal-\nenging aspects of optical character recognition. Hand-\nwritten text does not always conform to standard character\nshapes or sizes, which poses a significant challenge for\nOCR [Owen et al., 2020]. The text on natural history spec-\nimen labels provides additional challenges, as labels may\ncontain a mixture of handwritten and typed text or the\nhandwriting of multiple individuals [Owen et al., 2020].\nTests of accuracy for software targeted at Handwritten Text\nRecognition (HTR) (e.g., ABBYY Fine Reader Engine,\nGoogle Cloud Vision) indicate that HTR tools can cap-\nture a proportion of specimen data that is accurate and of\nhigh quality suggesting that this technology is already a vi-\nable technique for data capture [Owen et al., 2020, Haston,\n2015].\nLarge Language Models (LLMs) are trained to predict the\nnext token in a textual sequence. This self-supervised task\nallows for massive datasets to be used in training, and\nproduces models which are able to perform sophisticated\ntasks in natural language processing. As Radford et al.\n[2019] state: 'high-capacity models trained to maximize\nthe likelihood of a sufficiently varied text corpus begin to\nlearn how to perform a surprising amount of tasks without\nthe need for explicit supervision.' Multimodal LLMs allow\nnon-textual data inputs such as images and combine these\nsources to handle complex data processing tasks. Recently,\nmultimodal LLMs have been applied to the field of doc-\nument understanding [\u0141ukasz Borchmann, 2024]. It was\nfound that multimodal LLMs show significant promise\nfor document analysis and the results are vastly improved\nwhen including results from OCR in the input to the LLM."}, {"title": "Material and Methods", "content": "A diagram of the Hespi pipeline is shown in Fig. 1. The\nstages of the pipeline are explained in more detail below.\nBriefly, Hespi first takes a specimen sheet and detects the\nvarious components within it using the Sheet-Component\nModel. Any full institutional label that is detected is\ncropped and serves as input for the Label-Field Model,\nwhich detects text in a subset of data fields written on the\nlabel. A neural network Label Classifier is used to deter-\nmine the type of text (typeset or handwritten) on the label.\nThe text within each field is recognized using OCR and\nHTR engines. The recognized text is post-processed and\ncross-checked against authoritative plant and fungal name\nlists for specific fields. The institutional label and the rec-\nognized text are given to a multimodal LLM for correction.\nThe final extracted labels and text data are written to an\nHTML report and a CSV file for viewing and subsequent\ndata processing.\nThis object detection model is based on the work of Thomp-\nson et al. [2023b]. It takes specimen sheet images and\noutputs bounding boxes for 11 components:\n1. Institutional label\n2. Data on the specimen sheet outside of a label\n('original data'; often handwritten)\n3. Taxon and other annotation labels\n4. Stamps\n5. Swing tags attached to specimens\n6. Accession number (when outside the institutional\nlabel)\n7. Small database labels\n8. Medium database labels\n9. Full database labels\n10. Swatch\n11. Scale"}, {"title": "The Sheet-Component Model", "content": "This object detection model is based on the work of Thomp-\nson et al. [2023b]. It takes specimen sheet images and\noutputs bounding boxes for 11 components:\n1. Institutional label\n2. Data on the specimen sheet outside of a label\n('original data'; often handwritten)\n3. Taxon and other annotation labels\n4. Stamps\n5. Swing tags attached to specimens\n6. Accession number (when outside the institutional\nlabel)\n7. Small database labels\n8. Medium database labels\n9. Full database labels\n10. Swatch\n11. Scale\n\u2022 Meise Botanic Garden (BR)\n\u2022 Royal Botanic Gardens, Kew (K)\n\u2022 The Natural History Museum, London (BM)\n\u2022 ZE Botanischer Garten und Botanisches Museum,\nFreie Universit\u00e4t Berlin (B)\n\u2022 Royal Botanic Garden Edinburgh (E)\n\u2022 Mus\u00e9um National d'Histoire Naturelle, Paris (P)\n\u2022 University of Tartu (TU)\n\u2022 Naturalis Biodiversity Center (L)\n\u2022 University of Helsinki (H)\nThompson et al. [2023a] discuss the results for training\na version of this model using YOLOv5 (You Only Look\nOnce) [Jocher, 2020]. YOLO is a neural network object\ndetection model which simultaneously predicts bounding\nboxes and class probabilities. Thompson et al. trained a\nYOLOv5 model on the MELU annotations and then fine-\ntuned on the annotations for the other nine herbaria. Here\nwe use YOLOv8 [Jocher et al., 2023] and train using all\nthe annotations together. Six versions of the model were\ntrained, with resolutions at 640 and 1240 pixels and sizes\n'm', 'l' and 'x'. The mean average precision at an intersec-\ntion over union value of 50% (mAP50) and the f1 score\non the validation set for each of these models is shown in\nFig. 3. The most critical component for Hespi is the 'in-\nstitutional label', which refers to the label generated when\nthe specimen is first accessioned into an herbarium. The\n'institutional label' typically contains information about\nthe specimen that needs to be digitized and this is the com-\nponent used for downstream tasks in the pipeline. The\nhighest f1 score for accurate detection of the institutional\nlabel was 98.5%, which was achieved with a model size of\n'x' and a resolution of 1280. This is the configuration of\nthe model that is used in the Hespi pipeline."}, {"title": "The Label-Field Model", "content": "The Label-Field Model takes any institutional label de-\ntected from the Sheet-Component Model and detects\nbounding boxes for the following fields:\n1. Family\n2. Genus\n3. Species\n4. Infraspecific taxon\n5. Authority of the taxon at the lowest rank provided\n6. Collector's field number\n7. Collector\n8. Locality\n9. Geolocation (latitude, longitude, elevation, and\nelevation units)\n10. Year\n11. Month\n12. Day\n\u2022 typewritten\n\u2022 printed\n\u2022 handwritten\n\u2022 combination\n\u2022 empty"}, {"title": "Label Classifier", "content": "We have trained a classifier to detect the following types\nof writing on the institutional label:\n\u2022 typewritten\n\u2022 printed\n\u2022 handwritten\n\u2022 combination\n\u2022 empty\nThese writing types were annotated to the 3,152 images\nfrom the MELU dataset. This dataset was partitioned into\n2,521 training images and 631 validation images. Images\nand annotations are available on FigShare [Turnbull et al.,\n2024b]. Pretrained ResNet [He et al., 2016] and Swin\nTransformer V2 [Liu et al., 2022] models were used and\nfine-tuned on this dataset using torchapp [Turnbull, 2023]\nfor 20 epochs with a batch size of 16 and at a resolution\nof 1024 pixels. The best performing models were the\nResNet-34 and the Swin Transformer V2 of size 's' with an\naccuracy of 97.9% (Fig. 5). The ResNet-34 model is used\nas part of the Hespi pipeline due to its lower computational\ncomplexity."}, {"title": "Text Recognition", "content": "Each field detected by the Label-Field Model is input into\nthe Text Recognition module. This uses the Tesseract OCR\nengine [Kay, 2007, Smith, 2007] and the TrOCR 'large'\nHTR model [Li et al., 2023].\nText formatting is applied to Tesseract/TrOCR results for\nthe family, genus, and species fields, where a standardized\nformat is expected. Family and genus fields are changed to\ntitle case, species to lower case. For all three, punctuation\nmarks are stripped from the beginning and end of the text,\nas well as any whitespace/empty characters.\nFor the family, genus, species and authority fields, any rec-\nognized text is cross-checked against the World Flora On-\nline database [WFO, 2023], an international compendium\nof vascular plants and mosses, and against databases within\nthe Australian National Species List [AuNSL], a nationally\nrecognized taxonomy of Australian biodata [Cooper et al.,\n2023]. The AuNSL databases used by Hespi are the:\n\u2022 Australian Plant Name Index [APNI]\n\u2022 Australian Bryophyte Name Index [AusMoss]\n\u2022 Australian Fungi Name Index [AFNI]\n\u2022 Australian Lichen Name Index [ALNI]\n\u2022 Australian Algae Name Index [AANI]\nIf the extracted text matches to a taxonomic name in the ref-\nerence datasets with a similarity of 80% or more using the\nGestalt (Ratcliff/Obershelp) approach [Ratcliff and Met-\nzener, 1988], Hespi will assign the taxonomic name from\nthe reference dataset to that field. In this way, minor dif-\nferences of the taxon name/s on the specimen label or the\nextracted data to those in taxonomic reference datasets are"}, {"title": "Large Language Model (LLM) Correction", "content": "After the text recognition, the results are passed through\na multimodal large language model (LLM) to correct any\nerrors. By default, Hespi uses OpenAI's \u2018gpt-40' model\nOpenAI, 2023]. This can be changed to any other model\nfrom OpenAI or Anthropic by specifying the model name\non the command line. The LLM is prompted with the im-\nage of the institutional label, the list of the desired fields,\nthe currently accepted text for each field and the outputs\nfrom the OCR and HTR engines and how the text has been\nadjusted after cross-checking with the relevant datasets.\nThe LLM is requested to output the text for any fields\nwhere the accepted text is incorrect. Currently, no exam-\nples of this process are provided through the prompt and so\nHespi is using the LLM as a 'zero-shot' learner [Radford\net al., 2019]. Hespi could be modified to provide the LLM\nwith examples of images from a particular herbarium and\nso use the LLM as a 'few-shot' learner which will likely\nimprove the results for similar institutional labels [Brown\net al., 2020]. This is left for future experimentation."}, {"title": "Outputs", "content": "Hespi produces a directory of outputs with the cropped\nimage files and the predictions of both the Tesseract and\nTrOCR results in both CSV and text files. The pipeline\noutputs are summarized as an HTML report which displays\nthe cropped images from each model and the derived rec-\nognized text (Fig. 6). In this way it is possible to manually\ncross-check the accuracy of the derived text by comparing\nit with the original data, visualized from the entire speci-\nmen label or the corresponding extracted data field. The\nCSV file includes the match score between 0 and 1 for the\nfamily, genus, species, and authority, alongside all OCR\nand HTR results. These scores are a value between 0 and\n1, with 1 indicating a perfect match and no corrections\nmade; 0.8 to 1.0 indicating how similar a match was, and\n0 indicating no match found with a similarity of 80% or\nhigher."}, {"title": "Accuracy of data extraction on unseen datasets", "content": "We created three test datasets for Hespi to evaluate its\nperformance end-to-end. The first, \u2018MELU-T', consists\nof 100 images of specimen sheets from The University of"}, {"title": "Sorting of Specimen", "content": "Digital sorting of specimen labels holds the potential to\nimprove efficiency and accuracy of curation workflows by\nincreasing the format consistency within datasets [de la\nCerda and Beach, 2010, Drinkwater et al., 2014, Tulig\net al., 2012, de la Cerda and Beach, 2010]. For example,\nDrinkwater et al. [Drinkwater et al., 2014] note that more\nrapid downstream manual data capture and quality control\nwas achieved when specimen sheets were sorted by the\ncountry from which specimens were collected or by the\ncollector's name. Collation of similar specimen sheets\nis enabled in the Hespi pipeline in multiple ways. The\nLabel Classifier enables segregation of labels according\nto their print type, enabling downstream curation accord-\ning to requirements of those data types. Additionally, the\nHespi pipeline enables rapid and accurate detection and\nextraction of other text-bearing label types (e.g. annotation\nlabels and data written directly on the specimen sheet) and\nother sheet components (e.g., rulers and color bars) en-\nabling sorting of these specimens, for efficient downstream\ncuration of a set of specimens with consistent data or data\nformats.\nThe diversity of formats of specimen labels within a sin-\ngle institution over time, and among national and inter-\nnational herbaria, has provided a major challenge for au-"}, {"title": "Detection of textual fields", "content": "Accurate parsing of textual data extracted from entire spec-\nimen labels into Darwin Core fields remains a significant\nchallenge Haston [2015] and continues to represent an\nimpediment to full automation of the post image capture\nprocess. By detecting individual data fields within the in-\nstitutional label, Hespi reduces the need for downstream\nparsing of textual data.\nTaxon name fields (family, genus, and species) were consis-\ntently accurately detected (mAP50=96.4\u201396.7%, f1=94.1-\n95.3%). These results are comparable to those extracted\nby Quaesitor [Little, 2020], which achieved 0.80\u20130.97 re-\ncall and a 0.69\u20130.84 precision in the detection of Latin\nscientific names in the 16 most common languages for\nbiodiversity articles. The detection of infraspecific taxon\nnames was low (mAP50=68.5, f1=68.1), a challenge that\nwas also noted by Little [Little, 2020], who noted that\ninfraspecific rank was difficult to discern from hybrid com-\nbinations. Milleville et al. [2023] applied Google Cloud\nVision API to test accuracy of taxon name recognition\nfrom 50 specimen labels, noting that only 24 percent of the\nwords in taxon names were recognized correctly and 36\npercent were partially recognized. Extraction of the taxon\nauthority field (mAP50=92.8%, f1=90.3%) enabled com-\nparison of extracted taxonomic names with those present\nin taxon lists during post-processing. Our post-processing\nresults indicate that comparisons with global lists of names\n(e.g. International plant names index), which include both\ncurrent names and synonyms, increases accuracy of taxon\nnames where global taxonomic diversity is represented.\nNumerical date fields were readily detected (mAP50=97.5-\n97.8%, f1=95.4\u201395.9%), although the automated extrac-"}, {"title": "Text Recognition", "content": "The Hespi pipeline includes some post-processing of the\ntext recognition, to minimize the need for downstream\nmanual processing. One aspect of this is simply formatting\ntext to nomenclatural protocols, so that fields identified\nas family or genus are capitalized, and those identified\nas species or an infraspecific taxon are set as lower case;\nanother was stripping punctuation from the beginning and\nend of the family, genus, and species fields.\nAnother aspect is the cross-referencing of text identified\nas family, genus, species, or authority against a list of\neach, developed from the Australian National Species\nList and the World Flora Online Taxonomic Backbone.\nInitially these reference lists were derived from five in-\ndices of the Australian National Species List, incorporat-\ning vascular plants (angiosperms, pteridophytes and gym-\nnosperms), bryophytes (mosses, hornworts and liverworts),\nfungi, lichen, and algae. To extend the cross-referencing\nability beyond Australian species, taxonomic information\nfrom the World Flora Online, which includes vascular\nplants (angiosperms, pteridophytes, and gymnosperms)"}, {"title": "Conclusion", "content": "We present Hespi, an open-source pipeline for automati-\ncally extracting labels containing textual data from herbar-\nium specimen sheets and recognizing a subset of textual\ndata from the original institutional label. It takes specimen\nsheet images and outputs a suite of formatted informa-\ntion, including the text written on the institutional label,\nwhich is typically the target data of digitization of speci-\nmen sheets. The text is corrected using a multimodal LLM\nwhich substantially improves the results of standard OCR\nand HTR engines. Hespi achieves accurate results on the\ntest datasets. The various components of the model can\nbe fine-tuned for other herbaria to improve results in other\ncontexts. It can be incorporated into a wider strategy of\ndigitizing specimen sheets and thereby making available\nthe wealth of data that are associated with those specimens."}, {"title": "Software and Availability", "content": "Hespi is available as open-source software on Github\n(https://github.com/rbturnbull/hespi) under the\nApache 2.0 Open Source License. It installed directly\nfrom the Python Package Index (https://pypi.org/\nproject/hespi/). The automated testing as part of the\nContinuous Integration/Continuous Deployment (CI/CD)\npipeline has 100% code coverage. The whole pipeline\nruns with a single command and instructions for us-\nage are provided in the online documentation (https:\n//rbturnbull.github.io/hespi/)."}, {"title": "Competing interests", "content": "No competing interest is declared."}, {"title": "Author contributions statement", "content": "R.T. conceived the pipeline. R.T., K.T., E.F. and J.B. cre-\nated the image annotations. R.T. trained the models. R.T.\nand E.F. developed the software. R.T., K.T., E.F. and J.B.\nwrote and reviewed the manuscript. J.B. conceived the re-\nsearch and obtained resources to support the collaboration."}]}