{"title": "Spiking Point Transformer for Point Cloud Classification", "authors": ["Peixi Wu", "Bosong Chai", "Hebei Li", "Menghua Zheng", "Yansong Peng", "Zeyu Wang", "Xuan Nie", "Yueyi Zhang", "Xiaoyan Sun"], "abstract": "Spiking Neural Networks (SNNs) offer an attractive and energy-efficient alternative to conventional Artificial Neural Networks (ANNs) due to their sparse binary activation. When SNN meets Transformer, it shows great potential in 2D image processing. However, their application for 3D point cloud remains underexplored. To this end, we present Spiking Point Transformer (SPT), the first transformer-based SNN framework for point cloud classification. Specifically, we first design Queue-Driven Sampling Direct Encoding for point cloud to reduce computational costs while retaining the most effective support points at each time step. We introduce the Hybrid Dynamics Integrate-and-Fire Neuron (HD-IF), designed to simulate selective neuron activation and reduce over-reliance on specific artificial neurons. SPT attains state-of-the-art results on three benchmark datasets that span both real-world and synthetic datasets in the SNN domain. Meanwhile, the theoretical energy consumption of SPT is at least 6.4\u00d7 less than its ANN counterpart.", "sections": [{"title": "Introduction", "content": "Bio-inspired Spiking Neural Networks (SNNs) are regarded as the third generation of neural networks. In SNNs, spiking neurons transmit information through sparse binary spikes, where a binary value of 0 denotes neural quiescence and a binary value of 1 denotes a spiking event. Neurons communicate via sparse spike signals, with only a subset of spiking neurons being activated to perform sparse synaptic accumulation (AC), while the rest remain idle. Their high biological plausibility, sparse spike-driven communication, and low power consumption on neuromorphic hardware make them a promising alternative to traditional AI for achieving low-power, efficient computational intelligence.\nDrawing on the success of Vision Transformers, researchers have combined SNNs with Transformers, achieving significant performance improvements on the ImageNet benchmark and in various application scenarios. A question is naturally raised: can transformer-based SNNs be adapted to the 3D domain while maintaining their energy efficiency and fully leveraging the ability of transformers? To this end, we present Spiking Point Transformer (SPT), the first spiking neural network based on transformer architecture for deep learning on point cloud.\nThe successful application of transformer-based traditional artificial neural networks (ANNs) in the 3D point cloud domain has been widely demonstrated. Since point clouds are collections embedded in 3D space, the core self-attention operator in Transformer networks is in essence a set operator which is invariant to the permutation and number of input elements, making it highly suitable for processing point cloud data. Considering the computational costs, point cloud transformers cannot perform global attention. The Point Transformer series calculates local self-attention within the k-nearest neighbors (KNN) neighborhood. In order to integrate this self-attention operation with SNNs, we follow the design of spiking self-attention and employ a spiking local self-attention mechanism to model sparse point cloud using spike Query, Key, and Value. By using AC operations instead of numerous multiply accumulate (MAC) operations, we significantly reduce the energy consumption of self-attention computations for 3D point cloud.\nTraining point cloud networks requires more expensive memory and computational costs than images because point cloud data requires more dimensions to describe itself. Researchers have proposed various optimization strategies, including sparse convolutions, optimization during the data processing phase, and local feature extraction. If the existing direct encoding methods used by transformer-based SNNs for 2D static images or used by SNNs for 3D point clouds are directly applied to the Transformer structure for point cloud, the training of SNNs with multiple time steps will result in a sharp increase in computational costs. Point cloud data is high-dimensional but has low information density. The current direct encoding methods for point clouds means we need to repeat T"}, {"title": "Related Work", "content": "There are typically three ways to address the challenge of the non-differentiable spike function: (1) Spike-timing-dependent plasticity (STDP) schemes . (2) converting trained ANNs into equivalent SNNs using neuron equivalence, i.e., ANN-to-SNN conversion schemes. (3) Training SNNs directly using surrogate gradients. STDP is a biology-inspired method but is limited to small-scale datasets. Spiking neurons are the core components of SNNs, with common types including Integrate-and-Fire (IF) and Leaky Integrate-and-Fire (LIF) . IF neurons can be seen as ideal integrators, maintaining a constant voltage in the absence of spike input. LIF neurons build on IF neurons by adding a voltage decay mechanism, which more closely approximates the dynamic behavior of biological neurons. In addition to IF and LIF neurons, Exponential Integrate-and-Fire (EIF) and Parametric Leaky Integrate-and-Fire (PLIF) neurons are also commonly used models. These neurons better simulate the dynamic characteristics of biological neurons.\nVarious studies have explored Transformer-based SNNs that fully leverage the unique advantages of SNNs. Spikformer firstly converts all components of ViT into spike-form. Spike-driven Transformer advances further by introducing the spike-driven paradigm into Transformers. Spikingformer proposes a hardware-friendly spike-driven residual learning architecture. In this work, we extend the Transformer-based SNNS from 2D images to 3D point clouds while employing efficient direct training methods."}, {"title": "Deep Learning on Point Cloud", "content": "Deep neural network architectures for understanding point cloud data can be broadly classified into projection-based, voxel-based, and point-based methods. Projection-based methods project 3D point clouds onto 2D image planes, using a 2D CNN-based backbone for feature extraction. Voxel-based methods convert point clouds into voxel grids and apply 3D convolutions. Pioneering point-based methods like PointNet use max pooling for permutation invariance and global information extraction, while Point-Net++ introduces hierarchical feature learning. Recently, point-based methods have shifted towards Transformer-based architectures. The self-attention mechanism of the point transformer, insensitive to input order and size, is applied to each point's local neighborhood, crucial for processing point clouds.\nconstruct a point-to-spike residual classification network by stacking 3D spiking residual blocks and combining spiking neurons with conventional point convolutions . Spiking PointNet, the first SNN framework for point clouds, proposes a trained-less but learning-more paradigm based on PointNet . It adopts direct encoding of point clouds, repeating over time steps, making it hard to train point clouds with large time steps. Due to these limitations, further accuracy improvement is challenging. To address this, we propose a transformer-based SNN framework and design Q-SDE, significantly saving computational costs, enabling training in multiple time steps, and achieving higher accuracy."}, {"title": "Method", "content": "In this paper, we propose a Spiking Point Transformer (SPT) for 3D point cloud classification, integrating the spiking paradigm into Point Transformer. First, we perform Queue-Driven Sampling Direct Encoding (Q-SDE) on the point cloud. Then, we preliminarily encode the membrane potential with an MLP Module and a Spiking Point Transformer Block (SPTB). Next, further encoding is done through L Spiking Point Encoder Modules, mainly including Spiking Transition Down Block (STDB) for downsampling and SPTB for feature interaction. Finally, membrane potential is sent to Classification Head to output the prediction."}, {"title": "Queue-Driven Sampling Direct Encoding", "content": "Most of the high-performance SNN studies are based on direct encoding. Direct encoding is to repeat the input T times along the time dimension, which incurs expensive computational costs. We design an encoding method suitable for point clouds, which is an improved direct encoding called Queue-Driven Sampling Direct Encoding (Q-SDE). Q-SDE uses a first-in, first-out queue-driven sampling method to retain the most effective support points of the original points at different time steps, while reducing computational costs.\nThe original point queue P has a shape of (N, Co). We initialize the encoded multi-time-step point matrix Pe with a shape of (T, Ns, Co). T represents the number of time steps, Ns represents the number of sampled points per time step, and Co represents the number of feature dimensions per point.\nAs shown in Figure 1, through furthest point sampling (FPS), Ns points are extracted from P and stored in the first time step of Pe. The sampled points at first time step contain the object's key contours but lacks the N - Ns points which are unsampled, which are crucial for recognizing difficult objects. Subsequent time step sampling should efficiently cover the unsampled points.\nThe specific approach is to dequeue the first $N_p$ points referred to as discarded points from P, then use FPS to select Np points called sampling points from the unsampled points, and concatenate these points with the first N \u2013 Np points of\n$N_p = \\lfloor{(N - N_s) / (T - 1)}\\rfloor$\nWhen T = 1, the first time step of Pe is also the only time step that stores all points in P. Together, the main steps of Q-SDE are summaried in Algorithm 1."}, {"title": "Spiking Point Encoder Module", "content": "As shown in Figure 1, Spiking Point Encoder Module is the main component of the whole architecture, which contains the Spiking Transition Down Block (STDB) and Spiking Point Transformer Block (SPTB).\nSTDB is employed for spatial downsampling of point clouds to expand the spatial receptive field. Specifically, it involves obtaining a new spatial point cloud P\u2081 and its corresponding membrane potential features Ur through FPS. We then utilize K-nearest neighbors (KNN) sampling to extract the features of the nearest points for each point in the new point cloud and project these features into a higher-dimensional space after spiking neuron firing. Finally, by using LocalMaxPooling (LAP), we aggregate the local features F from the neighborhood of spatial point cloud Pi onto the membrane potential features U. STDB can be expressed as:\n$F^{i-1} = \\{P^{i-1}, U^{l-1}\\}$\n$P^i = FPS(F^{i-1}, N_l)$\n$F^i = KNN(F^l, F^{l-1}, N_k)$\n$U^i = LAP(MLP(SN(F^i)))$"}, {"title": "Hybrid Dynamics Integrate-and-Fire Neuron", "content": "The spiking neuron model is simplified from the biological neuron model. In this paper, we uniformly adopt the LIF for SN function. Meanwhile, we design HD-IF which integrate different neuronal dynamic models, including LIF, IF, EIF, and PLIF and place it before each SPTB.\nWe begin by briefly revisiting their dynamic characteristics. Figure 2(b) shows that the IF neuron acts as an ideal integrator, with membrane potential changing through input accumulation. The LIF neuron is IF neuron with leakage, where the membrane potential gradually approaches the input with input and returns to the resting state without input. The EIF neuron is a nonlinear LIF model. It adds an exponential term to the LIF model to simulate the sudden jump in potential near the firing threshold. The PLIF neuron adds a learnable membrane time constant \u03c4, dynamically adjusted by the parameter w via Sigmoid(w) function. The detailed equations for each neuron can be found in the Appendix.A. Then, we introduce a novel HD-IF neuron, which aims to promote competition among different neurons by selectively"}, {"title": "Experiments", "content": "In this experiment, we evaluate our model's performance using two metrics: overall accuracy (OA) and mean class accuracy (mAcc). These metrics provide a comprehensive assessment of our model on the test set."}, {"title": "Energy Efficiency", "content": "In this section, we investigate energy efficiency of our SPT model on the ModelNet40 dataset. In the ANN domain, the dot product operation, or MAC operation, involves both addition and multiplication operations. However, the SNN leverages the multiplication-addition transformation advantage, eliminating the need for multiplication operations in all layers except the first Conv+BN layer. According to the research , a 32-bit floating-point consumes 4.6pJ for a MAC operation and 0.9pJ for an AC operation."}, {"title": "Conclusion", "content": "In this paper, we present the Spiking Point Transformer (SPT) which combines the low energy consumption of SNN and the excellent accuracy of Transformer for 3D point cloud classification. The results show that SPT achieves overall accuracies of 94.76%, 91.43%, and 78.03% on the ModelNet10, ModelNet40, and ScanObjectNN datasets, respectively, making it the state-of-the-art in the SNN domain. We hope that our work can inspire the application of SNNs in other tasks, such as 3D semantic segmentation and object detection, and also promote the design of next-generation neuromorphic chips for point cloud processing."}]}