{"title": "E-CGL: An Efficient Continual Graph Learner", "authors": ["Jianhao Guo", "Zixuan Ni", "Yun Zhu", "Siliang Tang"], "abstract": "Continual learning has emerged as a crucial paradigm for learning from sequential data while preserving previous knowledge. In the realm of continual graph learning, where graphs continuously evolve based on streaming graph data, continual graph learning presents unique challenges that require adaptive and efficient graph learning methods in addition to the problem of catastrophic forgetting. The first challenge arises from the interdependencies between different graph data, where previous graphs can influence new data distributions. The second challenge lies in the efficiency concern when dealing with large graphs. To addresses these two problems, we produce an Efficient Continual Graph Learner (E-CGL) in this paper. We tackle the interdependencies issue by demonstrating the effectiveness of replay strategies and introducing a combined sampling strategy that considers both node importance and diversity. To overcome the limitation of efficiency, E-CGL leverages a simple yet effective MLP model that shares weights with a GCN during training, achieving acceleration by circumventing the computationally expensive message passing process. Our method comprehensively surpasses nine baselines on four graph continual learning datasets under two settings, meanwhile E-CGL largely reduces the catastrophic forgetting problem down to an average of -1.1%. Additionally, E-CGL achieves an average of 15.83\u00d7 training time acceleration and 4.89\u00d7 inference time acceleration across the four datasets. These results indicate that E-CGL not only effectively manages the correlation between different graph data during continual training but also enhances the efficiency of continual learning on large graphs. The code is publicly available at https://github.com/aubreygjh/E-CGL.", "sections": [{"title": "1 Introduction", "content": "Graphs have garnered significant research attention in recent years due to their ubiquity as a data form [11, 24, 27]. In real-world applications, graph data tends to expand and new patterns emerge over time. For instance, recommendation networks may introduce new product categories, citation networks may witness the emergence of new types of papers and research hotspots, and chemical design may uncover novel molecules and drugs. To keep up with these evolving scenarios and provide up-to-date predictions, graph models need to continuously adapt. However, traditional training strategies [18, 11] will suffer from catastrophic forgetting [29] while adapting the model on new data, resulting in poor performance in previous tasks.\nHence, there is a need for methods that can rapidly adapt to new classes while maintaining performance on previous tasks. Continual learning (also known as incremental learning or lifelong learning) aims to achieve this by learning from new data while preserving previous knowledge [23]."}, {"title": "2 Related Works", "content": "Continual learning (CL) aims at progressively acquiring and integrating knowledge from new task without forgetting previously learned information. A naive solution to continual learning is retraining the model with both old and new data, however, this is impractical since it is time-consuming, labor-intensive, and costly for real-world applications. Another solution involves fine-tuning the model with only new data, but this often leads to catastrophic forgetting, where the model rapidly declines in its ability to distinguish old classes. Hence, both training efficiency and robustness to catastrophic forgetting are crucial for successful continual learning.\nExisting approaches for continual learning can be roughly divided into three categories: regularization-based, replay-based, and architecture-based. Regularization-based methods penalize changes in the model's parameters by incorporating regularization terms to preserve past task performance. For instance, LwF leverages knowledge distillation to regularize the model's parameters, it uses the previous model's output as soft labels for current tasks. EWC [12], on the other hand, directly adds quadratic penalties to the model weights to prevent drastic parameter shifting. MAS [1] is similar to EWC, that uses the sensitivity of predictions on parameters as the regularization term. Replay-based methods prevent forgetting by selecting a set of exemplary data from prior tasks to retrain the model along with new data. For example, GEM [16] stores representative data in the episodic memory and modifies the gradients of the current task to prevent forgetting. Architecture-based approaches introduce different parameter subsets for different tasks to avoid drastic changes. While most of these methods can be integrated into graph learning with proper modifications, they often overlook the distinct properties of graph data and result in performance degradation.\nRecently, with a growing interest in applying continual learning to graph data, several specific methods have been proposed. TWP [14] adds a penalty to preserve the topological information of the previous graphs. ER-GNN [32] integrates three intuitive replay strategies to GNNs. However, these node sampling techniques are tailored for i.i.d. data, and do not take graph topology into consideration. ContinualGNN [25] is another replay-based approach that samples nodes based on novel pattern discovery and old pattern preservation. DyGRAIN [10] proposes time-varying receptive fields to capture pattern changes. SSM [31] sparsifies the replayed graph to reduce memory consumption. CaT [15] employs graph condensation techniques to generate synthesized replayed graphs. Furthermore, CGLB [30] summarizes and proposes unified benchmark tasks and evaluation protocols for continual graph learning, highlighting the importance of inter-task and intra-task edges among different tasks.\nDespite the promising prospects, the field of Continual Graph Learning (CGL) still lacks sufficient research contributions, and several challenges remain to be addressed. In addition to the interdependency and efficiency problems that we aim to improve in our work, there are other open issues that require attention. These include investigating the impact of neighborhood evolution on continual learning, addressing the complexities of continual learning on heterogeneous graphs, devising strategies for actively forgetting stale patterns, and more. Further research in the field of CGL is essential to tackle these challenges and advance our understanding of continual learning on graphs."}, {"title": "3 Methodology", "content": "In this section, we will delve into the technical details of E-CGL. Firstly, we will provide the fundamental concepts of continual graph learning. Following that, we will present two essential"}, {"title": "3.1 Preliminaries", "content": "Notations A growing graph can be decomposed into a series of dynamic graphs G = {G1, G2,...,GT} according to timestamps. Each subgraph Gt = (Vt, Et) constitutes a distinct task t and has different types of nodes Vt and edges Et. In the context of node classification tasks in a continual learning setting, we denote At \u2208 \\mathbb{R}^{Nt \\times Nt} as the adjacency matrix, Xt \u2208 \\mathbb{R}^{Nt \\times K} as the raw node features, and Yt \u2208 \\mathbb{R}^{N_t \\times C} as the one-hot node labels for Gt. Here, Nt = |Vt| represents the number of nodes, K denotes the dimension of raw features, and C represents the total number of classes. M is a memory bank that stores historical data. Since the methodologies in this section are all based on the current task t, unless otherwise specified, we will omit the subscript t for brevity.\nProblem Definition In the continual graph learning setting, each subgraph Gt in G has no overlap in category labels, and only the data at current time t is visible to the model due to the storage limitation. The goal is to learn the newly emerging information while prevent catastrophic forgetting of previous knowledge. For each subgraph Gt, we split it into training set G_{t}^{tr} = (V_{t}^{tr}, E_{t}^{tr}) and testing set G_{t}^{te} = (V_{t}^{te}, E_{t}^{te}) to train and evaluate the current model f parameterized by \\theta_{t}."}, {"title": "3.2 Graph Dependent Replay", "content": "As mentioned in Section 1, continual graph learning faces novel challenges for alleviating catastrophic forgetting due to the topological dependencies of nodes and graphs. To overcome these challenges, a direct solution is to replay historical data and rebuild such dependencies. And to justify replay-based strategy from a probabilistic perspective, we compute the posterior probability p(\\theta|D) by applying Bayes's rule to the prior probability of the parameters p(\\theta) and likelihood function p(D|\\theta):\n$\\log p(\\theta|D) = \\log p(D|\\theta) + \\log p(\\theta) \u2013 \\log p(D)$,\nwhere D represents the dataset. By rearranging Equation 1 and considering the combination of new data $D_{new}$ and previous data $D_{old}$, we have:\n$\\log p(\\theta | D_{new} \\cup D_{old}) = \\log p(D_{new} | \\theta, D_{old}) + \\log p(\\theta | D_{old}) \u2013 \\log p(D_{new}).$\nThe first term $\\log p(D_{new} | \\theta, D_{old})$ on the right-hand side shows the interdependencies of graph data, where the distribution of new data can be influenced by the old data. Such interdependencies make the problem challenging to address if the original data $D_{old}$ is unavailable. Based on above analysis, we opt to use replay-based methods to prevent catastrophic forgetting under the continual graph learning scenario. In general, the replay-based strategy maintains a memory buffer M, which is used to store historical data from previous tasks. When a new task t arrives, the model learns from the new data, and replays from the old data:\n$L_{new} = \\sum_{i \\in V_{t}^{tr}} L_{CE}(f(x_i), y_i), L_{replay} = \\sum_{j \\in M} L_{CE}(f(x_j), y_j),$\nwhere $L_{CE}(f(x_i), y_i) = -y_i \\log f(x_i)$ refers to the cross-entropy loss. Due to storage limit, it is impractical to replay all historical data, and effective sampling strategies are needed. In this work, we present a novel sampling strategy that takes into account both the importance and diversity of graph data, utilizing their topological and attributive characteristics.\nImportance Sampling PageRank was proposed by Google [19] as a web page ranking algorithm. It evaluates the importance of each node by considering the importance of other nodes linked to it. Formally, it iteratively updates the ranking scores using the 1st-order Markov chain until convergence:\n$\\pi = d T\\pi + \\frac{1-d}{N}1, T_{ij} = \\begin{cases}\\frac{1}{d_j} & \\text{if directed edge } (j, i) \\in E \\\\0 & \\text{if } d_j = 0 \\\\0 & \\text{otherwise}\\end{cases}$"}, {"title": "3.3 Efficient Graph Learner", "content": "To address the efficiency challenge in practical applications involving large growing graphs, we propose an efficient graph learner. This approach involves incorporating a Multi-Layer Perceptron (MLP) for training purpose, and utilizing its weights to initialize the GNN for inference tasks. Specifically, a typical graph convolution network [11] layer can be formulated as:\n$H^{(l)} = \\sigma (\\tilde{A}H^{(l-1)}W_{GCN}^{(l)}),$ \nwhere \u03c3 is the non-linear activation function, $\\tilde{A}$ is the normalized adjacency matrix [11], $H^{(0)} = X$ is the input node features, and $W_{GCN}$ are the learnable weight matrix of the l-th layer. We can further decouple the formulation into two operations, i.e., message passing and feature transformation [26]:\nmessage passing: $\\tilde{H}^{(l-1)} = \\tilde{A}H^{(l-1)},$\nfeature transformation: $H^{(l)} = \\sigma (\\tilde{H}^{(l-1)} W_{GCN}^{(l)}).$\nMessage passing mechanism, which works by aggregating information from each node's neighbor-hood, has long been considered as the core part for GNN models (e.g., GCN [11], GraphSAGE [6], GAT [24]). However, it also consumes most of computation time due to the sparse matrix multipli-cation. Recent studies [28, 7] have found that the effect of message passing mainly come from its generalization ability in inference, rather than its representation ability of learning better features in training. This finding opens up the possibility of removing message passing during training to improve efficiency. Besides, message passing is non-parametric in most cases, which means it can be plug-and-play integrated into existing models without training. Consequently, Equation 12 degrades to a simple MLP layer by removing message passing:\n$H^{(l)} = \\sigma (H^{(l-1)}W_{MLP}^{(l)}).$\nNote that when the dimensions of hidden layers are set same for $W_{MLP}$ and $W_{GCN}$, they will have identical weight space, meaning the weights of an MLP and its counterpart GCN can be transferred to each other. Based on such premise, we propose an efficient graph learner for fast model adaptation under continual setting. Concretely, we first remove the message passing scheme of a GCN network and initialize its counterpart MLP network. Then we optimize the parameters $W_{MLP}$ with only the node features as input, which speeds up the training process by avoiding sparse matrix multiplication. Formally, the training objective of Equation 3 with MLP is derived as:\n$L_{new} = \\sum_{i \\in V_{t}^{tr}} L_{CE}(MLP(x_i; W_{MLP}), y_i),$\n$L_{replay} = \\sum_{j \\in M} L_{CE}(MLP(x_j; W_{MLP}), y_j),$\nDuring inference, we adopt the trained $W_{MLP}$ as the parameters of the corresponding GCN model, and the topological information is leveraged again with message passing added:\n$H^{(l)} = \\sigma (\\tilde{A}H^{(l-1)}W_{MLP}),$\n$\\hat{Y} = softmax(H^{(L)}).$\nNote that the GCN encoder can be generalized to any message-passing-based GNN networks with proper modifications, we use GCN here for notation simplicity. Empirical results show that our efficient graph learner achieves comparable or even better results compared with GNNs, but largely reduce the training time up to 15.83x."}, {"title": "3.4 Discussions", "content": "Training Objective Integrating both Graph Dependent Replay module and Efficient Graph Learner module, the framework of our E-CGL is outlined in Algorithm 1. The overall training objective for E-CGL at each task is formulated as:\n$L = L_{new} + \\lambda L_{replay},$\nwhere \u03bb is a weight that balances the strength of two losses, we simply set it as 1."}, {"title": "4 Experiments", "content": "In this section, we provide detailed information regarding the experiment settings, quantitative results (including both task-IL and class-IL settings), running time analysis, ablation studies, hyperparameter sensitivity analysis, and visualizations. Further information on the hyperparameter initialization, and computation resources can be found in the Appendix B."}, {"title": "4.1 Experimental Settings", "content": "Task-IL and Class-IL Settings In a benchmark study conducted by CGLB [30], two settings for continual graph learning were introduced: task-incremental (task-IL) and class-incremental (class-IL). In the task-IL setting, a task indicator guides the model in distinguishing between classes within each task. On the other hand, in the class-IL setting, task indicators are not provided, thus requiring the model to differentiate among all classes from both current and previous tasks.\nTo elucidate these settings, we utilize examples from their work. Consider a model trained on a citation network with a sequence of two tasks: (physics, chemistry), (biology, math). In the task-IL setting, a document comes with a task indicator specifying whether it belongs to (physics, chemistry) or (biology, math). Consequently, the model only needs to classify the document into one of these two task categories without distinguishing between all four classes. Conversely, in the class-IL setting, a document can belong to any of the four classes, and the model must classify it into one of these four classes.\nThe class-IL setting is generally more challenging as it requires the model to handle an increasing number of classes over time. Without loss of generality, we conducted experiments on both task-IL and class-IL settings.\nDatasets We conducted experiments on four node classification datasets: CoraFull [2], Reddit [6], OGBN-Arxiv [9], and OGBN-Products [9]. Following the methodology outlined by CGLB [30], we partitioned the label space of the original datasets into several non-intersecting segments to simulate the task/class incremental setting. For example, in the case of CoraFull, we divided the original dataset into fourteen tasks, each comprising a five-way node classification task. The detailed statistics of the datasets are presented in Table 1."}, {"title": "4.2 Task-IL Results and Analysis", "content": "The results for task-IL node classification are presented in Table 2. Our proposed E-CGL method demonstrates notable performance among the competing methods. It achieves the highest average accuracy (AA) on the CoraFull, OGBN-Arxiv, and OGBN-Products datasets, and the second-highest average accuracy on Reddit. On the OGBN-Arxiv dataset, E-CGL surpasses the second-best result by a margin of 1.1%, closely approaching the performance upper bound of joint training. Notably, E-CGL only employs simple MLP for training, while still outperforms its GNNs competitors, which highlights the effectiveness of our method. Additionally, E-CGL exhibits an average forgetting of -1.1% across four datasets, indicating its ability to effectively mitigate catastrophic forgetting.\nMoreover, we compared the performance of different methods with the lower bound (fine-tune) and upper bound (joint train) as benchmarks. Nearly all methods fall between these bounds, indicating varying degrees of improvement over fine-tuning. However, most methods still suffer from catas-trophic forgetting, as indicated by negative average forgetting (AF) values. Additionally, it is worth mentioning that graph-specific continual learning methods, tend to rank higher compared to other CV-based methods. This observation emphasizes the existence of a representation gap between graph data and other Euclidean data, underscoring the necessity of tailored approaches for graph-related tasks."}, {"title": "4.3 Class-IL Results and Analysis", "content": "The experimental results under the class-IL setting are depicted in Table 3. Firstly, it's evident that all statistics in Table 3 are notably worse compared to the task-IL results in Table 2. This disparity arises due to the increased difficulty of the class-IL prediction setting, which involves a much larger label space. For example, on the CoraFull dataset, the classifier trained under the task-IL setting only predicts within 5 possible labels, while under the class-IL setting, it must discern among all 70 classes. Consequently, the outcomes of four CV-based traditional methods cannot fit in this setting and have poor performances similar to the fine-tuning."}, {"title": "4.4 Running Time Analysis", "content": "To evaluate the efficiency of E-CGL, we conducted a running time analysis comparing our method with other graph continual learning methods under the task-IL setting. The results, presented in Table 4, show the average time required for each training and inference epoch.\nWe observed that E-CGL significantly reduces both training and inference time compared to its GCN version. On average, E-CGL achieves a speedup of 15.83 times during training and 4.89 times during inference. The magnitude of improvement becomes more significant as the dataset size increases. For example, the improvement on the OGBN-Products dataset reaches 28.44 times during training and 8.89 times during inference, respectively. Additionally, E-CGL comprehensively outperforms other continual graph learning methods in terms of both training and inference time.\nThese efficiency improvements can be attributed to the design of the Efficient Graph Learner, where E-CGL eliminates the need for time-consuming message passing during training and allows for direct training on large graphs without batching. The reduced training time of E-CGL makes it a practical and efficient solution for continual graph learning tasks, enabling faster experimentation, model iteration, and deployment."}, {"title": "4.5 Ablation Study", "content": "To assess the effectiveness of different components in E-CGL, we conducted ablation studies under the task-IL setting. Two key components were evaluated: the replay-based sampling strategy and the choice of training encoder (MLP vs. GCN). The results are presented in Table 5.\nFirstly, we examined the impact of removing the importance and diversity samplers, respectively, while keeping the same sampling budget for the other. The results indicate that the performance of E-CGL decreases in terms of AA when either sampler is removed, underscoring the importance of both components of the proposed replay-based sampling strategy. Secondly, it is notable that in most cases, removing the importance sampler resulted in a larger performance drop compared to removing the diversity sampler. Additionally, the magnitude of the difference also reflects the dataset's difficulty, as will be discussed in the subsequent visualization section. On simpler datasets such as Products, there is minimal difference between the full version and the version with removed samplers, whereas on relatively more challenging datasets like Arxiv and Reddit, the choice of sampling strategy has a more significant impact on the results.\nFurthermore, we replaced the Efficient Graph Learner (MLP-based encoder) with GCN as the training encoder, as shown in the last row of Table 5. While it is not surprising that the GCN version outperforms the MLP version in most cases, given the importance of graph topological information for graph training, it is worth noting that the performance gap is not substantial. This suggests that despite a minor performance degradation, our E-CGL still achieves acceptable results while offering a significant training speedup compared to GCN-based methods."}, {"title": "4.6 Parameter Sensitivity", "content": "We conducted parameter sensitivity analysis on three factors: 1) diversity sampling ratio, 2) sampling budget for M, and 3) loss weight \u03bb. The average accuracy (AA) and the average forgetting (AF) results are depicted in Figure 2.\nFirstly, we observed that the diversity sampling ratio has a minimal impact on the results of continual graph learning. Both AA and AF exhibit slight fluctuations within a narrow range as the diversity ratio varies. The differing curve trends also indicate that the effect of different sampling strategies depends on specific datasets, thus highlighting the necessity of a combined strategy.\nSecondly, the performance of E-CGL consistently improves with an increase in the sampling budget. This is aligned with expectations, as a larger budget enables the replay of more nodes, effectively enhancing model performance. However, it's important to note that the budget cannot be infinitely expanded due to storage limitations and concerns regarding training efficiency. Eventually, when the sampling budget becomes extremely large, the replay-based method converges to joint training.\nLastly, the model's performance initially increases and then decreases (more noticeably on OGBN-Arxiv) as the loss weight \u03bb is raised. This pattern aligns with intuition. In our main experiment, we simply set \u03bb to 1."}, {"title": "4.7 Visualization", "content": ""}, {"title": "4.7.1 Learning Curve", "content": "We also visualized the learning curves of certain methods in Figure 3 to provide further insight into the training process over the task sequence."}, {"title": "4.7.2 Performance Matrix", "content": "We have also generated performance matrices to visualize the performance of several baseline methods and E-CGL under task-IL setting, as depicted in Figure 4. Similar to the analysis conducted in the previous section 4.2, most methods fall between the lower bound of fine-tuning and the upper bound of joint training. Generally, E-CGL demonstrates higher values, and graph-specific continual learning techniques show better overall performance.\nOne interesting finding in the performance matrix is related to the diagonal entries, which represent the model's ability to adapt to new tasks. It can be observed that in certain cases, some regularization methods (e.g., TWP on CoraFull, GEM on Products) exhibit weaker adaptation ability. We speculate that these methods impose constraints on model parameter updates, which can preserve the model's performance on previous tasks but limit its ability to adapt to new tasks.\nThe overall value range of the performance matrix also directly reflects the difficulty of each dataset. It is noticeable that most methods have darker (i.e., lower) values on OGBN-Arxiv compared to OGBN-Products. This observation aligns with the findings discussed in section 4.7.1."}, {"title": "5 Conclusion", "content": "In this paper, we have tackled two challenging obstacles in continual graph learning: the interde-pendencies in graph data and the efficiency concerns associated with growing graphs. To address these challenges, we have introduced an efficient continual graph learner (E-CGL) that utilizes a graph-specific sampling strategy for replay and an MLP encoder for efficient training. Our extensive empirical results demonstrate the effectiveness of our method in terms of both performance and efficiency.\nDespite the growing interest in continual graph learning, there are still some limitations that need to be addressed. For instance, there is a need to explore methods for protecting historical data privacy during the replay phase and actively forgetting unneeded stale knowledge for model adaptation. Furthermore, further investigation into handling heterogeneous graphs and addressing graph classification tasks is warranted. We hope that E-CGL serves as a step forward in continual graph learning and inspires future research to explore broader applications and develop more advanced techniques in continual graph learning domain."}, {"title": "A Theoretical Proofs", "content": ""}, {"title": "A.1 Proof for Equation 8", "content": "Based on the definition of Q and r in Equation 5 and 7 respectively, we can derive that [8]:\n$(Qr)_{i} = \\sum_{j \\in V} Q_{ij}r_{j}$\n$=\\sum_{j \\in V} \\frac{s(i, j)}{\\sum_{k \\in V} s(k, j)} r_{j}$\n$=\\sum_{j \\in V} \\frac{s(i, j)}{\\sum_{k \\in V} s(k, j)} \\frac{1}{\\sum_{i} \\sum_{j} s(i, j)} \\sum_{k} s(j, k)$\n$=\\frac{1}{z} \\sum_{j} s(i, j)$\n$= 1 \\cdot r_{i}$"}, {"title": "A.2 Simplify r Using Taylor Expansion", "content": "Consider the $r_{i}$ in Equation 7 being unnormalized: $r_{i} = \\sum_{j \\in V} s(i, j)$ and with the Radial Basis\nFunction $s(i, j) = e^{-\\gamma ||x_{i}-x_{j}||_{2}^{2}}$ as similarity, it can be simplified using Taylor expansion [8]:\n$r_{i} = \\sum_{j \\in V} e^{-\\gamma ||x_{i}-x_{j}||_{2}^{2}}$\n$=\\sum_{j \\in V} e^{-\\gamma (||x_{i}||^{2}+||x_{j}||^{2}-2x_{i}^{T}x_{j})}$\n$=e^{-\\gamma ||x_{i}||^{2}}\\sum_{j \\in V} e^{-\\gamma (||x_{j}||^{2}-2x_{i}^{T}x_{j})}$\n$=e^{-\\gamma ||x_{i}||^{2}}\\sum_{j \\in V} e^{-\\gamma ||x_{j}||^{2}} (1+2\\gamma x_{i}^{T}x_{j} + \\frac{1}{2}(2\\gamma x_{i}^{T}x_{j})^{2})$\n$=e^{-\\gamma ||x_{i}||^{2}} [\\sum_{j \\in V} e^{-\\gamma ||x_{j}||^{2}} + x_{i}^{T} (2\\gamma \\sum_{j \\in V} e^{-\\gamma ||x_{j}||^{2}} x_{j}) + x_{i}^{T}C x_{i}],$\nwhere $\\omega_{i} = e^{-\\gamma ||x_{i}||^{2}}$, $a = \\sum_{j \\in V} e^{-\\gamma ||x_{j}||^{2}}$, $b = 2 \\gamma \\sum_{j \\in V} e^{-\\gamma ||x_{j}||^{2}} x_{j}$, and $c = 2 \\gamma^{2} \\sum_{j \\in V} e^{-\\gamma ||x_{j}||^{2}} x_{j}x_{j}^{T}$ can be pre-calculated and all of them requires an $O(|V|D^{2})$ time complexity. Considering $|V| \\gg D$ in most cases, such cost is acceptable for large graphs."}, {"title": "B Implementation Details", "content": ""}, {"title": "B.1 Running Environment", "content": "The experiments were conducted on a machine with NVIDIA 3090 GPU (24GB memory). The E-CGL model and other baselines were implemented using Python 3.9.16\u00b9, PyTorch 1.12.12, CUDA 11.33, and DGL 0.9.14. The code was developed based on the benchmark CGLB[30]."}, {"title": "B.2 Model Configurations", "content": "For a fair comparison, a two-layer GCN [11] with a hidden dimension of 256 is used as the backbone for all compared methods. Unless otherwise specified, the same training configurations, including optimizer, learning rate, weight decay, and training epochs, are used for all baseline methods. Specifically, a batch size of 8000 is used when batching is necessary. Adam is employed as the optimizer with a learning rate of 0.005, and the weight decay is set to $5 \\times 10^{-4}$. Each task is trained for 200 epochs. The reported mean and standard deviations of AP and AF are based on five independent runs with random seeds ranging from 0 to 4."}, {"title": "B.3 Hyperparameters", "content": "Comprehensive hyperparameters specific to each method are provided in Table 6, and the reported results are based on the best outcomes obtained through grid search on these hyperparameters. To reproduce the results of E-CGL, set the sampling budget for Graph Dependent Replay as 1000 for CoraFull, 3000 for OGBN-Arxiv, 5000 for Reddit and OGBN-Products. Among all sampled nodes, 25% are selected using diversity sampling and 75% are selected using importance sampling. The loss weight \u03bb is set to 1."}]}