{"title": "Heterogeneous Decision Making in Mixed Traffic: Uncertainty-aware Planning and Bounded Rationality", "authors": ["Hang Wang", "Qiaoyi Fang", "Junshan Zhang"], "abstract": "The past few years have witnessed a rapid growth of the deployment of automated vehicles (AVs). Clearly, AVs and human-driven vehicles (HVs) will co-exist for many years, and AVs will have to operate around HVs, pedestrians, cyclists, and more, calling for fundamental breakthroughs in AI designed for mixed traffic to achieve mixed autonomy. Thus motivated, we study heterogeneous decision making by AVs and HVs in a mixed traffic environment, aiming to capture the interactions between human and machine decision-making and develop an AI foundation that enables vehicles to operate safely and efficiently. There are a number of challenges to achieve mixed autonomy, including 1) humans drivers make driving decisions with bounded rationality, and it remains open to develop accurate models for HVs' decision making; and 2) uncertainty-aware planning plays a critical role for AVs to take safety maneuvers in response to the human behavior. In this paper, we introduce a formulation of AV-HV interaction, where the HV makes decisions with bounded rationality and the AV employs uncertainty-aware planning based on the prediction on HV's future actions. We conduct a comprehensive analysis on AV and HV's learning regret to answer the questions: 1) How does the learning performance depend on HV's bounded rationality and AV's planning; 2) How do different decision making strategies impact the overall learning performance? Our findings reveal some intriguing phenomena, such as Goodhart's Law in AV's learning performance and compounding effects in HV's decision making process. By examining the dynamics of the regrets, we gain insights into the interplay between human and machine decision making.", "sections": [{"title": "1. Introduction", "content": "Automated vehicle (AV) is emerging as the fifth screen in our everyday life, after movies, televisions, personal computers, and mobile phones [1, 2]. Their potential impact on safety and economic efficiency is substantial [3\u20136]. For instance, the National Highway Traffic Safety Administration (NHTSA) reported that preventable crashes in the United States caused $871 billion in economic and societal losses in 2010\u2014approximately 1.9% of the GDP. While over 30 U.S. states have enacted AV legislation and AI-equipped vehicles continue to advance, experts acknowledge significant technical challenges remain [7\u20139]. Perhaps the most fundamental challenge is achieving both safety and efficiency in mixed-traffic environments, as AVs must coexist with human-driven vehicles (HVs), pedestrians, cyclists, and other road users for the foreseeable future.\nThe complicated interactions between HVs and AVs could have significant implications on the traffic efficiency given their different decision making characters. As such, a fundamental understanding on the heterogeneous decision making in the interplay, especially the impact of HVs' decision making with bounded rationality on AVs' performance, is crucial for achieving efficient mixed autonomy.\nExisting works on modeling the interaction between AV and HV largely fall within the realm of conventional game formulation, in which both agents try to solve the dynamic game and adopt Nash equilibrium strategies [10-13]. This line of formulation faces the challenge of prohibitive computational complexity [14]. Needless to say, the decision making of HV and AV are different by nature. As supported by evidence from psychology laboratory experiments [15\u201317], human decision-making is often short-sighted and deviates from Nash equilibrium due to their bounded rationality in the daily life [18\u201320]. In particular, HV's bounded rationality is unknown a prior and it remains challenging to develop an accurate model for HV's decision making. As a result, it is sensible for AVs' decision making to leverage uncertainty-aware planning for safety maneuvers in response to human behavior [21, 22]. Clearly, the heterogeneous decision making by HVs and AVs exposes intrinsic complexities in the mixed autonomy.\nAlong the line of [13, 23], we consider a two-agent system with one AV and one HV, where the HV takes the action by planning for a short time horizon, and the decision-making is sub-optimal and noisy due to bounded rationality. The AV utilizes uncertainty-aware lookahead planning based on predictions of the HV's future actions. The primary objective of this study is to understand the performance of heterogeneous decision making in the mixed autonomy by answering the following questions: 1) How does the learning performance depend on HV's bounded rationality and AV's planning? 2) How do different decision making strategies between AV and HV impact the overall learning performance?\nThe main contributions of this paper can be summarized as follows:\n(1) We first focus on the characterization of the regrets for both the HV and the AV, based on which we identify the impact of bounded rationality and planning horizon on the learning performance. In particular, we present the upper bound on the regret, first for the linear system dynamics model case and then for the non-linear case. We start with the linear case, and show the accumulation effect due to the AV's prediction error and its impact on AV's learning performance. Building on the insight from the linear case, we model the prediction error as a diffusion process in the non-linear case to capture the accumulation effect. By studying the upper bound, we identify the compounding effects in HV's decision making due to bounded rationality and the Goodhart's law in AV's decision making associated with the planning horizon.\n(2) We study the impact of HV's bounded rationality on the overall learning performance and the regret dynamics of AV and HV. We first establish the upper bound on the regret of the overall system due to HV's bounded rationality and AV's uncertainty-aware planning. Our regret bound naturally decompose into two parts, corresponding to the decision making of AV and HV, respectively. We examine the regret dynamics of the overall system theoretically and show how do different learning strategies between AV and HV affect the learning performance during each individual interaction through empirical study. The experiments details are available in Appendix F."}, {"title": "2. Related Work", "content": "Mixed Autonomy. Prior work on mixed autonomy traffic has primarily focused on specific dynamics models and empirical studies. For instance, [24] uses Bando's model for vehicle behavior analysis, while [25] studies AV's impact on HV driving volatility using predetermined AV acceleration models. The human factor has been examined through high-fidelity driving simulators [26], and stochastic models have been proposed to capture human behavior uncertainty [27]. On the learning side, [4] demonstrates congestion reduction using deep RL under the intelligent driver model (IDM). Without imposing specific models on HV and AV's decision making dynamics, our work focuses on the performance of different learning strategies in the mixed autonomy.\nHV-AV Interaction Model. For modeling HV-AV interactions specifically, several game-theoretic approaches have been proposed. [10] and [13] use Stackelberg and two-player game formulations respectively, while [12] develops a hierarchical planning scheme. Although [23] attempts to address game formulation limitations using underactuated dynamical systems, it assumes identical decision-making horizons for both vehicle types. While related fields like ad-hoc team problems [28] and zero-shot coordination [29] provide empirical insights, they focus on either cooperative scenarios or self-play robustness. Our work differs by analyzing the interaction between agents with different decision-making strategies without assuming cooperation, particularly examining the impact of opponent modeling errors on learning performance [30]. Despite the rich empirical results in the related field, e.g., Ad-hoc team problem and zero-shot coordination, we remark that the theoretical"}, {"title": "3. Preliminary", "content": "Stochastic Game. We consider the Stochastic Game (SG) defined by the tuple $\\mathcal{M} := (\\mathcal{X}, \\mathcal{U}_A,\\mathcal{U}_H, P,r_A,r_H, \\gamma)$ [35], where $\\mathcal{U}_A$ and $\\mathcal{U}_H$ are the action space for AV and HV, respectively. Meanwhile, we assume the action space for HV and AV are with the same cardinality $M$ and let $\\mathcal{U} = \\mathcal{U}_A \\times \\mathcal{U}_H$. We denote $\\mathcal{X}$ as the state space that contains both AV and HV's states. $P(x'|x,u_A,u_H):\\mathcal{X}\\times \\mathcal{U} \\times \\mathcal{X} \\rightarrow [0,1]$ is the probability of the transition from state $x$ to state $x'$ when AV applies action $u_A$ and HV applies action $u_H$. $r_H(x,u_A,u_H) : \\mathcal{X} \\times \\mathcal{U} \\rightarrow [0, R_{\\max}]$, $r_A(x,u_A,u_H) : \\mathcal{X} \\times \\mathcal{U} \\rightarrow [0, R_{\\max}]$ is the corresponding reward for HV and AV. $\\gamma \\in (0,1)$ is the discount factor. We denote the AV's policy by $\\pi : \\mathcal{X} \\times \\mathcal{U}$ and use $\\hat{u}_H(t)$ to represent AV's prediction on HV's real action $u_H(t)$ at time step $t$. We use $p_0$ to represent the initial state distribution.\nValue Function. Given AV's policy $\\pi$, we denote the value function $V^{\\pi}(x) : \\mathcal{X} \\rightarrow \\mathbb{R}$ as\n$V^{\\pi}(x) =  E \\Big[ \\sum_{t=0}^{\\infty} \\gamma^t r_A(x(t), u_A(t), u_H(t)) \\Big| x(0)=x, u_A(t)\\sim \\pi \\Big] $,\nto measure the average accumulative reward staring from state $x$ by following policy $\\pi$. The expectation is taken over $u_A(t) \\sim \\pi$ and $x(t + 1) \\sim P(x, u_H(t), u_A(t))$.\nWe assume the maximum value of the value function to be $V_{\\max}$. We define Q-function $Q^{\\pi}(x,u_A,u_H): \\mathcal{X} \\times \\mathcal{U} \\rightarrow \\mathbb{R}$ as $Q^{\\pi}(x,u_A,u_H) = E_{\\pi} \\Big[\\sum_{t=0}^{\\infty} \\gamma^t r_A(t) | x(0)=x, u_A(0)=u_A, u_H(0)=u_H\\Big]$ to represent the expected return when the action $u_A, u_H$ are chosen at the state $x$. The objective of AV is to find an optimal policy $\\pi^*$ given HV's action $u_H$ such that the value function is maximized, i.e.,\n$\\pi^* = \\arg \\max_{\\pi} E_{x\\sim p_0, u_A \\sim \\pi(\\cdot|x,u_H)}[Q^{\\pi}(x,u_A,u_H)].$ (1)\nSimilarly, the objective function can be written as $E_{x \\sim p_0} [V^{\\pi}(x)]$.\nNotations. We use $||\\cdot||$ or $||\\cdot||_2$ to represent the Euclidean norm. $||\\cdot||_F$ is used to denote Frobenius norm. $\\mathcal{N}(\\mu, \\sigma^2)$ is the normal distribution with mean $\\mu$ and variance $\\sigma^2$. $I$ is an identity matrix."}, {"title": "3.1. Modeling AV-HV Interaction: Heterogeneous Decision Making", "content": "In this section, we examine in detail the interaction between one AV and one HV in a mixed traffic environment. More specifically, we have the following models to capture the interplay between human and machine decision making in the mixed autonomy.\nAV's Decision Making via L-step lookahead planning. At time step $t$, after observing the current state $x(t)$, AV will first need to predict HV's future action $\\hat{u}_H(t + i), i = 0, 1, 2, \\dots, L - 1$ due to the unknown bounded rationality of HV. Based on this prediction, AV strives to find an action sequence that maximizes the cumulative reward with the predicted HV actions using trajectory optimization. In order to facilitate effective long-horizon reasoning, we augment the planning trajectory with a terminal value function approximation $Q_{t-1}$, which is obtained by evaluating the policy obtained from previous time step. For convenience, we denote policy $\\hat{\\pi}_t$ as the solution to maximizing the"}, {"title": "4. Characterization of HV and AV's Learning Performance", "content": "4.1. Regret of AV with L-step Lookahead Planning\nIn this subsection, we study the impact of bounded rationality and uncertainty-aware planning on the performance of AV. To this end, we first quantify the performance gap between choosing optimal actions and sub-optimal actions, for given HV's behavior fixed. Therefore, conditioned on HV's action $\\mathbf{u}_H = \\{u_H(t)\\}_{t=1}^T$, the regret for $T$ interaction of AV is defined as\n$R_A(\\mathbf{u}_H) = \\frac{1}{T}\\sum_{t=1}^T Reg_A(t) := \\frac{1}{T} \\sum_{t=1}^T  E \\Big[ V^*(x|\\mathbf{u}_H(t)) - V^{\\hat{\\pi}_t}(x) \\Big]$,\nwhere we use $V^*(x|\\mathbf{u}_H(t))$ to denote the optimal value function attained by the optimal policy $\\pi^*$ given HV's action $\\mathbf{u}_H$. $\\hat{\\pi}_t$ is the policy obtained in the $t$-th time step while AV solving $L$-step lookahead planning objective Equation (2) based on its prediction on HV's future actions. In particular, at each time step $t$, conditioned on HV's action $u_H(t)$, the optimal value function $V^*(x|\\mathbf{u}_H(t))$ is determined by choosing a policy $\\pi_1(t)$ from policy space $\\Pi_1$. Hence, the regret defined for AV is closely related to adaptive regret [37]. Without loss of generality, we have a general model on HV's prediction error."}, {"title": "5. Regret Dynamics in Mixed Autonomy", "content": "Aiming to understand \"How do different decision making strategies impact the overall learning performance?\", especially on the impact of HV's bounded rationality on AV's performance, we study the regret dynamics in this section. More concretely, we denote the regret for the whole system as,\n$R_{A-H}(T):=\\frac{1}{T}\\sum_{t=1}^T  E \\Big[ V^*(x|\\mathbf{u}(t)) - V^{\\hat{\\pi}_t}(x) \\Big] + \\frac{1}{T}\\sum_{t=1}^T [\\Phi(x(t), u(t), u_H(t)) - \\Phi(x(t), u_A(t), u_H(t)) ]$,\n(i)\n(ii)\nwhere $V^*(x|\\mathbf{u}(t))$ is the optimal value function when HV also takes the optimal action $u_H^*(t)$, e.g., $u_H^*(t) = \\arg \\max_{u_H} \\Phi(x(t), u(t), u_H)$. Meanwhile $\\Phi(x(t), u(t), u_H^*)$ is the optimal value when"}, {"title": "6. Conclusion", "content": "In this work, we take the regret analysis approach to address the questions 1) \u201cHow does learning performance depend on HV's bounded rationality and AV's planning horizon?\u201d and 2) \u201cHow do different decision making strategies between AV and HV impact the overall learning performance?\u201d. To this end, we propose a formulation that captures heterogeneous HV-AV interactions and derive regret upper bounds for both vehicle types. Our analysis reveals two key phenomena: a Goodhart's law effect in AV's planning-based RL with predicted human actions, and error accumulation in HV's decision-making due to bounded rationality. We characterize the overall system performance through theoretical bounds and empirical studies, demonstrating the impact of different learning strategies on system efficiency."}, {"title": "Appendix", "content": "Proxy in the System Dynamics. In the linear case, we first derive the resulting state transition model when AV is planning for the future steps while using the prediction of HV's action. The corresponding state dynamics can be written as, i.e., after observing x(t),\n$\\hat{x}(t+1) =Ax(t) + B_Au_A(t) + B_H\\hat{u}_H(t)$\n$=Ax(t) + B_Au_A(t) + B_Hu_H(t) + B_H\\epsilon_A(t)$\n$:=\\hat{x}(t + 1) + B_H\\epsilon_A(t)$\nwhere $\\hat{x}(t + 1)$ is the true state when AV and HV takes action $u_A(t)$ and $u_H(t)$.\nThen at the next step, we have,\n$\\hat{x}(t+2) =A\\hat{x}(t + 1) + B_Au_A(t + 1) + B_H\\hat{u}_H(t + 1)$\n$=A\\hat{x}(t+1) + B_Au_A(t + 1) + B_H\\hat{u}_H(t+1)$\n$=A(x(t+1) + B_Au_A(t + 1) + B_Hu_H(t + 1) + AB_H\\epsilon_A(t) + B_H\\epsilon_A(t+1)$\nIt can be seen that the estimated state and the real state has the following relationship,\n$\\hat{x}(t + 1) = x(t + 1) +  A^{i-1}B_H\\epsilon_A(t + 1 - i)$. (6)\n$i=1$\nQuantify the Regret. Recall the definition of the regret (performance gap), i.e.,\n$\\frac{1}{T}$ [V(x(t)) - V(x(t))]\nTA FA Error\n:=V(x(t)) - V(x(t)) + V(x(t)) - V(x(t)) (7)\n(1)  (2)\nFor simplicity, we define the following notations,\n$\\hat{\\tau}$ trajectory obtained by running $\\hat{\\pi}_A$ with function approximation error (FA)\n$\\tau^{\\dagger}$ trajectory obtained by running $\\pi$ with FA error\n$\\tau^*$ trajectory obtained by running in $\\pi_M$ without FA error\n$\\mathbf{u}_t = (u_A(t), u_H(t))$\nMeanwhile, we use $\\hat{\\pi}$ to denote the policy obtained by running lookahead on a inaccurate model and $\\pi$ is the policy using the accurate model. Note that in both cases, the terminal cost are estimated by V (with function approximation error).\nPart 1. Impact of the Function Approximation Error. We first quantify the first term (1) in Equation (7) as follows,\nV^(x) - V^(x) =E[* y*r(x, u\u2081) + V^(x)] - E [7r(xu, u\u2081) + V^(x)]\n=E* y*r(xu, u\u2081) + y*V^(x)] - E [r(xu, u\u2081) + y^*V^(x)]\n+ E[y'r(xu, u\u2081) + yV*(x\u2081)] - E[y'r(xu, u\u2081) + yV*(x\u2081)]\n=E* y*r(xu, u\u2081) + yV^(x)] - E [r(xu, u\u2081) + yV^(x)]\n+ y*E [V(x)-V(x\u2081)] (8)"}, {"title": "A. Proof of AV's Regret.", "content": "Assume the value function is bounded by $V_{\\max}$ and the learned value function is $\\hat{V}$. Assume that the function approximation error is $e_V$ with mean $\\mu_0$ and variance $\\sum_{V}$, i.e.,\nV*(x) - V(x) = e(x)\nBy combing all three parts, we have the upper bound and lower bound as follows,\nV(x) - V(x\u2081) \u2264 1 Rax + y V\u2081x + y evix\n1-7"}, {"title": "4.2. Regret of HV with Bounded Rationality", "content": "Given AV's action $u_A$, we define the regret for HV conditioned on AV's action $u_A$ as follows:\n$R_H(T|u_A) = E_{x(0) \\sim p_0}  \\frac{1}{T}\\sum_{t=1}^T [\\Phi^*(t) - \\Phi(t)]$,\nwhere $\\Phi^*(t) := \\Phi_H(x(t), u_A(t), u_A(t))$ is the optimal value and it is determined by choosing a policy $\\pi(t)$ from policy space $\\Pi_H$ such that $\\Phi(x, \\pi_A, \\pi_H)$ is maximized. $\\Phi(t) := \\Phi_H(x(t), u_H(t), u_A(t))$ represents the value achieved when HV chooses sub-optimal action due to bounded rationality. For ease of exposition, we assume HV's decision making is myopic and HV's planning horizon is $N = 1$, such that $\\Phi_H(x(t), u_A(t), u_H(t)) := r_H(x(t), u_A(t), u_H(t))$. Meanwhile, we assume HV makes sub-optimal decision as follows,\n$u_H(x(t), u_A(t)) = u_H^*(x(t), u_A(t)) + \\epsilon_H(t)$\nwhere $\\epsilon_H(t) \\sim \\mathcal{N}(\\mu_H, \\Sigma_H)$ is due to bounded rationality of humans and it is not known by AV.\nLet $C_H = \\max_{u_A} u_A \\mu_H(\\mu_H^T \\mu_H)^{-1}$ and $\\Lambda_H = \\sqrt{\\lambda_{eigmax} (C_H^T S_H C_H)}s_{\\max}$, then we have the following results on the upper bound of HV's regret which shows the impact of bounded rationality on HV's performance. The proof of Theorem 4.7 is available in Appendix B.\nSuppose Assumption 4.5 holds. Then we have the regret of HV's decision\n$R_H(T) \\le n s_{\\max} \\cdot \\sigma_H + (s_{\\max} + \\Lambda_H)||\\mu_H||^2$"}, {"title": "B. Proof of HV's Regret.", "content": "Due to the bounded rationality, HV does not choose the optimal action and thus introduces the regret as follows\n$Reg_H(\\mathbf{u}):= \\frac{1}{T}  Reg_H(t)$\nwhere we assume that HV can observe the action of AV in a timely manner. Next, we impose the assumptions on the reward structure to be quadratic, i.e.,\nrx(x,u\u2081, u\u03bc) = f(x,ux) + \u0438\u043d\u0405\u043d\nwhere SH are positive definite matrices.\nThen we have the regret for HV to be"}, {"title": "C. Proof of Corollary 5.1", "content": "We denote the regret for the whole system as RAH(T), i.e., RAH(T) :="}, {"title": "D. General Setting with Time-varying Prediction Error Distribution.", "content": "Multimodal Predictions in Autonomous Driving. In the context of trajectory prediction in au- tonomous driving, multimodality arises from the fact that, given the observed information, there can be multiple plausible future trajectories for the HV. Consequently, the AV necessitates the ability to learn from the historical interactions with HV and adjust its own prediction model. Toward this end, we consider the general setting for AV's prediction error distribution, i.e., we assume the prediction error follows a time-variant distribution as follows,\ne(t) ~ N(ut),o(1)I), (14)\nwhere pa(t) is the time-varying mean and a(t) is the time-varying variance. In what follows, we demonstrate the major modification (in blue) of the proof of regret derived in the main paper."}, {"title": "E. Generalization of AV and HV's Learning Strategies.", "content": "We clarify that Equation (2) can be degenerated into many commonly\nAV's Learning Strategies. used RL algorithms, for instance,\n-1, Equation (2) is the model-free Q-function update and our\nModel-free Case) Set L regret analysis still holds.\n(Actor-Critic Case) Let Q-function and policy T be parameterized by 0 and P, respectively, Then Equation (2) can be learned by using Actor-Critic, i.e., in the actor step, 6 is updated\nby maximizing the L-step look-ahead objective and & is updated using policy gradient. Note that in this case, the approximation error in both Actor and Critic update can be encapsulate into ex as in Assumption 1. Our proof of the regret remains the same.\nHV's Learning Strategies. In Equation (3), we consider AV's decision making to be N-step planning while we do not impose any constrains on the length of N. In particular, when N \u2192 oo, the decision making of HV is related to dynamic programming (assume the model is available) and otherwise, the decision making of AV is in the same spirit of Model Predictive Control (MPC)."}, {"title": "F. Experimental Settings.", "content": "In this section, we include the detailed parameter setup when conducting the experiments. The default setting is as follows:"}, {"title": "G. Extension beyond Two Agent Case.", "content": "ur analysis approach is feasible to extend beyond one AV and one HV setting. Assume there are NH number of HVs and NA number of AVs in the mixed traffic system. With abuse of notations, we define the action vector for AVs and HVs as follows, at time step t, y\u03bc(t) = [PH(t), P(t)...., uuu(t)], u(t) = [UAU(t), UAU(t),..., unua(t). By defining the prediction error as in Equation (4) and HV's bounded rationality as in Section 4.2, our analysis framework still can be applied. The dimension of the approximation error term and the bounded rationality term is thus NA and NH times higher than the two-agent case. Hence, the resulting regret in Theorem 3 and Theorem 4 are Na and Ng times higher than the two-agent case."}]}