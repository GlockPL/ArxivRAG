{"title": "BUMBLE: Unifying Reasoning and Acting with Vision-Language Models for Building-wide Mobile Manipulation", "authors": ["Rutav Shah", "Albert Yu", "Yifeng Zhu", "Yuke Zhu", "Roberto Mart\u00edn-Mart\u00edn"], "abstract": "To operate at a building scale, service robots must perform very long-horizon mobile manipulation tasks by navigating to different rooms, accessing different floors, and interacting with a wide and unseen range of everyday objects. We refer to these tasks as Building-wide Mobile Manipulation. To tackle these inherently long-horizon tasks, we introduce BUMBLE, a unified Vision-Language Model (VLM)-based framework integrating open-world RGBD perception, a wide spectrum of gross-to-fine motor skills, and dual-layered memory. Our extensive evaluation (90+ hours) indicates that BUMBLE outperforms multiple baselines in long-horizon building-wide tasks that require sequencing up to 12 ground truth skills spanning 15 minutes per trial. BUMBLE achieves 47.1% success rate averaged over 70 trials in different buildings, tasks, and scene layouts from different starting rooms and floors. Our user study demonstrates 22% higher satisfaction with our method than state-of-the-art mobile manipulation methods. Finally, we demonstrate the potential of using increasingly-capable foundation models to push performance further.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous service robots aiming to assist humans daily must perform mobile manipulation (MoMa) in homes, hospitals, universities, offices, and other building-wide environments. Given a user instruction, like \"I am on a diet, but I want soda,\" the robot must interpret the free-form instruction, create a high-level task plan to accomplish the task, and instantiate low-level robot commands to fulfill it. To accomplish such tasks in buildings, the robot might have to traverse to a kitchen, potentially using an elevator if it is on a different floor. To use the elevator, the robot must draw on prior experiences with elevators in other buildings. The robot must also handle unexpected obstacles, such as by pushing items blocking a narrow corridor or opening doors to move in and out of the room. Finally, even after reaching the kitchen, the robot must identify an appropriate soda can from the diverse clutter often present in human-inhabited buildings, recognizing the \u201cdiet\" option even if it has never been encountered before. We refer to such long-horizon and spatially expansive tasks as Building-wide Mobile Manipulation (see Fig. 1) and present a unified framework to address it.\nBuilding-wide MoMa entails a series of technical challenges, including integrating complex locomotion and manipulation behaviors in buildings, perceiving objects and their context in the open-world, and using past experiences to learn from mistakes reason about long-horizon strategies (example execution in Fig. 2). Decades of research in Task and Motion Planning (TAMP) [1-4] and recent neuro-symbolic planning [5\u20138] have been applied to long-horizon robotic tasks. However, these approaches require pre-defined symbolic abstractions and domain knowledge, confining their reasoning capabilities to a closed set of objects. Large Language Models (LLMs) [9-14] are capable of reasoning about open-world scenarios to create task plans. However, they lack grounded and direct reasoning through visual perception, inherently falling short in the precise geometric reasoning crucial for producing executable robot actions. Recently, Vision-Language Models (VLMs) have emerged as a promising tool for Building-wide MoMa, unlocking grounded reasoning in open-world scenes [15-23]. Some works [24-28] have demonstrated the potential of VLMs in simple MoMa tasks but lack the necessary memory and motor skills to operate at a building scale. In contrast, building-wide MoMa requires 1) an open-world perception system for reasoning about diverse objects, 2) complex motor skills to act effectively in buildings, and 3) memory for temporally extended reasoning in long-horizon task execution.\nWe propose to tackle Building-wide MoMa by unifying reasoning and acting through a VLM-based framework-one that can handle diverse objects with open-world perception, act effectively in building-wide situations with a broad set of gross-to-fine motor skills, and learn and adapt from past experience through memory. By seamlessly unifying these capabilities into rapidly advancing VLMs, our general framework improves as VLM backbones become better at sophisticated perception and reasoning.\nTo this end, we introduce BUMBLE (BUilding-wide MoBiLE Manipulation), a new framework that addresses all of these needs with an adaptable approach capable of reasoning and acting over new objects, tasks, and buildings. Given a task instruction in free-form language, BUMBLE reasons over the scene, past experiences, and motor skill capabilities to predict the next parameterized skill to execute. BUMBLE comprises four key ingredients: a) a VLM serving as the central reasoning module connecting perception, memory, and skills; b) dual-layered memory: short-term memory [29] to maintain robot execution history, and long-term memory [30] to store valuable experience and concepts from past trials; c) a diverse skill library of parameterized skills that enable the robot to navigate to different floors and rooms (e.g., GoToLandmark[GoalImage], UseElevator[Button]), adjust the robot base for manipulation (e.g., MoveBase[Dir.]), and interact with objects through diverse contact-rich behaviors (e.g., PushObjOnGround[ObjSeg., Dir.]) present in different buildings; d) perception system for the VLM to not only visually reason about open-world, cluttered scenes but also process depth information necessary for embodied decisions. Our experiments demonstrate that BUMBLE can successfully execute long-horizon, building-wide tasks requiring up to 12 parameterized skills and over 15 minutes per trial (excluding VLM query time). These tasks involve navigating different floors and rooms, clearing pathway obstacles, retrieving objects, and making granular decisions, such as adjusting the base for manipulation. Notably, BUMBLE achieves a 47.1% average success rate in tasks like retrieving soda cans and rearranging chairs across three buildings. We demonstrate that BUMBLE solves previously unseen tasks involving multiple plausible solutions and operating in completely new rooms. Through a user study, we find that BUMBLE rollouts align with human preferences 22% better than prior state-of-the-art VLM-based MoMa approaches. Furthermore, we demonstrate how BUMBLE improves performance with advancing VLM capabilities. Finally, we thoroughly analyze failures, guiding future research."}, {"title": "II. RELATED WORK", "content": "Pioneering research on mobile manipulators [31-35] performed tasks in human environments, which naturally occur in buildings [36-38]. Early work in whole-body control [39-43] and motion planning [44\u201348] laid the groundwork for low-level control in building-level mobile manipulation, but did not address reasoning required in long-horizon building-wide tasks. Task and motion planning (TAMP) [1\u20134, 49] integrates reasoning for task and motion generation to achieve long-horizon tasks. However, it is confined to a fixed preprogrammed set of object categories and scenes, hindering its application to building-scale environments with open-world objects.\nRecently, foundation models [50] have been leveraged for mobile manipulation. LLMs excel at semantic reasoning for planning [51, 52], but their predictions are not grounded in the scene, requiring additional machinery to match their plans to executable robot actions [9, 11, 53, 54]. VLMs provide a more promising engine to unify and ground reasoning for tabletop manipulation [15-17, 26], navigation [19-22, 55], and mobile manipulation [24, 25, 27, 28, 56], but prior methods lack a diverse skill repertoire (navigation and manipulation, coarse and fine motion) and long-horizon memory-based reasoning capability, both of which are necessary to perform building-wide mobile manipulation tasks. BUMBLE is a natural extension of VLM-based robotic systems: it uses VLMs to unify semantic and geometric reasoning for manipulation and navigation at the building-wide scale, enabling it to leverage current and future advances in VLM models. Our framework integrates perceptual and motor skills and long and short-term memory to obtain efficiency and generalization to diverse objects and scenes in different buildings.\nAll aforementioned prior work in mobile manipulation assumes the navigation path is either obstacle-free or that obstacles can be navigated around. In interactive navigation [57, 58] however, obstacles must be moved or manipulated-such as pushing away path-blocking boxes, opening doors, or pressing elevator buttons\u2014to reach a destination. Prior methods tackled this with motion planning [59\u201361] and learning [62\u201364] but focused solely on the interactive navigation problem using only geometric information. In contrast, BUMBLE integrates both semantic reasoning (e.g., avoiding pushing delicate objects) and geometric reasoning (e.g., avoiding object collisions while pushing an obstacle) into a unified method for mobile manipulation."}, {"title": "III. BUMBLE: VLM-BASED BUILDING-WIDE MOMA", "content": "BUMBLE is a new framework integrating perception, motor skills, and memory in a VLM-based solution for high-level reasoning and low-level control of a mobile manipulator for building-wide tasks. In this section, we describe the key technical blocks of BUMBLE (see Fig. 3): (a) capabilities for open-world perception, a library of diverse skills to act in the physical world, and short and long-term memory for on-the-fly adaptation and learning from past mistakes; and (b) a VLM-based decision-making module to reason and ground the text instruction in the current scene and predict the next parameterized skill to execute (see App. VII-A for details).\n**A. Perception System, Skill Library, and Memory**\na) Open-world Perception System: Robots must perceive and localize diverse open-world objects to operate effectively in buildings. We address this using a perception system with a robust segmentation model, Grounded-SAM (GSAM) [65-67], for segmenting foreground (i.e., interactable) objects (Fig. 3, purple). We use GSAM instead of directly querying the VLM because segmentation models offer pixel-level accuracy, allowing for more precise localization and manipulation. After obtaining the object masks for the scene, we calculate the object point cloud by back-projecting depth images and determine the precise distance between the robot and the detected objects. The object proximity information enables BUMBLE to estimate the correct skill for manipulation in building environments, while the object point cloud enables precise manipulation using motor skills.\nb) Skill Library: To overcome the challenges of building-wide MoMa, BUMBLE requires a diverse skill library allowing for high-level abstract behaviors like navigating to a room, down to more fine-grained behaviors like adjusting the base for manipulation (Fig. 3, yellow; examples in Fig. 2, 4). The skill library must also support using building-specific fixtures like elevators and perform manipulation skills like pushing objects and picking up items, addressing interactive navigation challenges [47, 58]. To this end, we construct a skill library to which new skills can easily be added and chosen by the VLM. Our library includes skills such as GoToLandmark, NavigateNearObj, MoveBase, Pickup, PushObjOnGround, OpenDoor, CallElevator, and UseElevator. The parameters of each skill are based on object or robot configurations, except for GoToLandmark. It uses a topological visual map [68] of the building, with landmark images as nodes and 2D occupancy maps to generate trajectories between them (See Fig. 4). With the library of parameterized skills and VLM's reasoning capabilities, BUMBLE can integrate semantic and geometric reasoning at the task level by predicting the subtask and skill and the robot's motion by estimating skill parameters to accomplish building-wide tasks.\nc) Memory: Building-wide MoMa is inherently long-horizon, requiring the robot to track its state-action history. Moreover, the robot must recover from failed skill executions whenever possible, such as by moving away obstacles or adjusting the robot base after a failed grasping attempt. To enable these recovery behaviors, the robot must store and analyze prior failed executions when making future predictions. To address this issue, BUMBLE maintains short-term memory (Fig. 3, blue). The short-term memory stores the scene image, subtask, skill name, parameter, and the system-detected execution result (success/failure) at each prediction step for the current execution trial. This allows the VLM to reason over the entire execution history when predicting its next skill, crucial for long-horizon building-wide tasks.\nThe VLM may make reasoning errors, such as failing to consider object collisions with the environment, which could, for example, result in pushing an obstacle into a wall. To allow BUMBLE to learn from these mistakes and reduce them [69], BUMBLE maintains a long-term memory of previously collected prediction failures. Erroneous VLM predictions, flagged by a human operator, are stored together with the decision context (user instruction, scene image, predicted subtask, skill name, and predicted parameters) and a VLM text analysis of the failure reason, serving as lessons for improving future predictions and reducing the dependency on human annotations."}, {"title": "B. VLM-based Decision Making", "content": "BUMBLE factorizes each decision-making step into two sequential steps (Fig. 3, yellow box). First, BUMBLE exploits a VLM to perform perceptual and memory-based reasoning for predicting the next subtask to solve (e.g., \"go to a place where you can find a soda\") and for selecting the skill to handle it (e.g., GoToLandmark). Then, BUMBLE queries the VLM to estimate the best skill parameters (e.g., goal image of kitchen) and instantiates the corresponding motor skill execution.\na) Subtask prediction and skill selection: First, BUMBLE needs to infer the next step of the task and what skill to use to solve it. To that end, it integrates information about i) the language task instruction, ii) the description of robot skills, iii) the current scene (RGB) and robot state (location in the building), and iv) execution history (short-term memory) and failures from prior trials (long-term memory) into a unified multimodal VLM query.\nThe VLM must also perceive the object distance information to infer the object's reachability, which is necessary for manipulation. Naively prompting the VLM with depth images or point clouds does not yield desirable results because VLMs are not trained on those modalities. To allow the VLM to make informed decisions, BUMBLE implements a multimodal Set-of-Mark (SoM) [70] prompting: foreground objects are marked with IDs in the input RGB image, and their distances to the robot, calculated using the perception system (Sec. III-A), are passed in the prompt.\nBUMBLE employs the resulting multimodal prompt to query the VLM-backbone about the next step in the task and the best-suited skill to solve it, requesting intermediate reasoning with Chain-of-Thought (CoT) [71] for better accuracy. With this VLM-backed procedure, BUMBLE can reason about long-horizon tasks, eliminating the need for a classical planner's computationally heavy search.\nb) Skill parameter estimation: After predicting the sub-task and skill to execute, BUMBLE predicts the parameters needed to correctly instantiate the skill and generate a sequence of low-level robot commands. These parameters must be contextualized and grounded in the current scene for informed decision-making. To that end, BUMBLE creates a multimodal VLM query that includes i) the predicted subtask, ii) memory of previously made mistakes in predicting the skill parameters, and iii) an SoM-based visual grounding of the possible skill parameters with explanations of each marker (example in Fig. 4). For example, for the MoveBase skill, the possible parameters (forward, left, and right) are marked in the RGB image as arrow endpoints. For object-centric skills, the candidate parameters are generated by querying the GSAM with a skill-specific prompt (e.g., find 'Buttons' for UseElevator). BUMBLE queries the VLM to choose the most promising skill parameter using CoT prompting for better reasoning about the subtask, scene, and past experiences. The execution outcome is stored in the short-term memory for iterative prediction."}, {"title": "IV. EXPERIMENTAL EVALUATION", "content": "To systematically evaluate the performance of BUMBLE, we benchmark three very long-horizon tasks up to 12 skills long: Retrieving marker, Retrieving diet soda can and Rearranging chairs. To test the system's language understanding capabilities, the tasks are specified using free-form text instructions, and the evaluations are averaged over three different task specifications for each task. For instance, in the retrieving marker task, we use \u201cI want to color the sky in my drawing. Can you get me a marker?\u201d, \u201cI want to color grass in my drawing. Can you get me a marker?\u201d and \u201cI need to color some hearts. Can you get a marker for that?\u201d (Refer to App. VII-B for details).\nWe test BUMBLE's building-wide MoMa capabilities by initializing the agent on different floors (possibly requiring elevator use), with randomized obstacles along the robot's path, such as closed doors, chairs, wet floor signs, and cardboard boxes. BUMBLE must reason geometrically (e.g., selecting the correct arm to open a door or direction to push an obstacle) and semantically (e.g., avoiding wet floor signs). We evaluate open-world generalization with multiple target objects per task-different brands of diet soda and markers, in random positions under varying degrees of clutter (5-25 distractor objects). We test building-level generalization across three university buildings of different layouts, visual appearance, and room structures. Results are measured using 10 trials per building per task (totaling 90+ hours of evaluations). Since BUMBLE demonstrated consistent performance on the first 2 tasks across the 3 buildings, we only evaluated our third task, rearranging chairs, in one building.\nWe compare BUMBLE with two baselines: Inner-Monologue (IM) [72] that reasons using only a language scene description without RGB images or long-term memory, and COME [28], which reasons using images, like BUMBLE, but without long-term memory. Note that both methods were originally unsuitable for building-wide tasks due to the limited skill library and lack of memory, so we extended both with BUMBLE's diverse MoMa library and short-term memory, for fair comparison. We instantiate BUMBLE with GPT-4o for our experiments since we found it empirically to perform best. We gather long-term memory experiences before evaluations and do not update them during evaluations. Finally, in our evaluation, we complement the full-fledged long-horizon building-wide MoMa experiments described above by only assessing the parameter estimation for given skills in predefined scenes. To that end, we manually collected an offline dataset (OfflineSkillDataset) of around 120 images for three different skills and annotated them with ground truth parameters.\nIn our evaluation, we investigate the following questions: 1) How does BUMBLE perform compared to other methods when addressing building-wide tasks? Table I summarizes the results of our experiments. Inner-Monologue (IM) performs poorly compared to COME and BUMBLE, indicating that multimodal prompting with visual information is critical to making the right decisions. Due to its poor performance, we restricted the evaluation of IM to that first building. BUMBLE scores best in all tasks and buildings and outperforms COME by 12.1% on average, thanks to learning from past mistakes. To directly compare the reasoning capability of these approaches over a diverse but fixed set of scenarios, we test each method against the ground truth skill parameters in OfflineSkillDataset. BUMBLE achieves 80.2% success, outperforming COME (72.6%) and Inner-Monologue (61.7%), suggesting that BUMBLE's improvements in single-step skill parameter prediction help drive more successful executions in long-horizon tasks.\n2) How well does BUMBLE scale with increasing VLM capabilities? In BUMBLE, we sought to unify reasoning with a VLM-backbone to leverage advances in this fast-paced field. For that, BUMBLE would have to work well with different VLMs and demonstrate improving capabilities with more powerful versions. Fig. 5 includes the results of our evaluation of BUMBLE using Claude (Red), Gemini (Yellow), and GPT-4o (Green) model series as VLM-backbone on the OfflineSkillDataset. BUMBLE works well with any VLM, and its performance with each model scales with VLM capabilities, highlighting the potential of BUMBLE to leverage next-generation foundation models.\n3) What are the common failure modes in BUMBLE? We analyze and break down the failure cases observed during the evaluation of BUMBLE in the three long-horizon tasks and present the results in Fig. 6. Several of our errors (10/38) are caused by sensor failures (bad depth values, lidar failures) or GSAM segmentation mistakes, but most of them can be attributed to VLM reasoning mistakes, e.g., picking the wrong object, wrongly adjusting the base or failing to solve an interactive navigation problem [47]. We notice that these errors occur more often when BUMBLE queries the VLM to localize markers demanding precise spatial understanding, such as elevator buttons, which often leads to pushing an incorrect one. The VLM is also likelier to pick an incorrect object when there are many distractors (20-25, 38.9%) than a few (5-10, 10.0%), indicating limitations when reasoning with clutter, possibly due to presence of numerous markers.\nDuring evaluations, we noticed a surprising emergent behavior: BUMBLE overcomes sensor failures using its diverse skill library. We observed that BUMBLE learns to use MoveBase to a) reposition towards the elevator buttons, and b) approach an obstacle (chair), both after failed manipulation attempts due to NaN depth sensor values.\n4) How well does BUMBLE align to human judgement? To analyze how well BUMBLE decisions align with human judgment, we test on two completely unseen tasks without any prompt changes. These two tasks involve navigating to a completely new (shower) room: a)", "I dropped my phone in water and dried it off with a napkin, but it's still not working. Can you pick up something to get rid of all the moisture?": "We then collect images of the execution using BUMBLE and COME and annotate them with the task description and the parameterized skill selected by each one. We ask 10 participants to annotate each trajectory by selecting the description that best fits the result of the robot's last decision. Participants choose from five categories: 1: Irrecoverable failure causing permanent damage to the robot or environment, 2: Recoverable failure that can be corrected without lasting damage, 3: Ill-specified decision, 4: Sub-optimal task completion, 5: Task completion that fully satisfies the user request. Participants respond on an online form with questions shuffled with a mix of decisions made by COME and BUMBLE.\nFig. 7 summarizes the results of our survey. BUMBLE achieves a Likert rating of 3.7 out of 5.0. Compared to COME (Avg. rating: 2.6), BUMBLE has 22% points less irrecoverable and 12% points less recoverable failures. Participants rate BUMBLE as suboptimally completing the task 33% of the time, such as successfully picking up an object, but not the one most suitable on the scene. We hypothesize this is due to greedy plans generated by VLMs, which often pick up the nearest, most visible object. Incorporating multi-step reasoning using VLMs to overcome greedy behavior is an exciting future direction for BUMBLE.\n5) What is the contribution of CoT and SoM to BUMBLE? We ablate two components of the BUMBLE VLM interface: Chain-of-Thought reasoning and Set-of-Mark prompting to measure their effect on performance. Not using CoT decreases performance by 19.5% points, down to 60.7%, underscoring the importance of having intermediate reasoning steps for decisions in building-wide tasks. Removing SoM and instead using the VLM to predict a desired object description in natural language, then asking GSAM to segment this object [26], decreases performance by 31% points, highlighting GSAM's difficulty in segmenting specific object instances such as \u201cDiet Dr. Pepper\u201d when a non-diet Dr. Pepper can is present (See Table II)."}, {"title": "V. CONCLUSION", "content": "We demonstrate that a VLM-backed policy, acting as a central reasoning module equipped with a diverse skill library, memory, and open-world perception system, can effectively solve building-wide mobile manipulation tasks. Comprehensive ablations and experiments demonstrate the potential of BUMBLE as a stepping stone for achieving autonomous service robots in buildings.\nLimitations and Future Work: BUMBLE's long-term memory storing previous failures can become computationally intractable with time; using a dynamic retrieval process from a large storage space or compressing these experiences in learnable weights can be explored in future work. Furthermore, BUMBLE assumes access to landmarks with goal images in the GoToLandmark skill, which requires human effort in collecting the landmarks. Autonomous exploration with landmark frame selection can be explored to offload human effort. Lastly, while BUMBLE can be easily interfaced with semantically meaningful skills, adding skills or parameters that cannot be easily expressed via language [74, 75] requires additional machinery for video conditioning or specialized weights to allow such interfacing."}, {"title": "VI. ACKNOWLEDGMENTS", "content": "We thank Shivin Dass for developing the robot system infrastructure. We also thank Ben Abbatematteo, Shuijing Liu, and Mingyo Seo for their feedback on the manuscript. We thank all UT Austin Robot Perception and Learning Lab and Robot Interactive Intelligence Lab members for their invaluable feedback on BUMBLE. This work was partially supported by the National Science Foundation (FRR2145283, EFRI-2318065), the Office of Naval Research (N00014-22-1-2204, N00014-24-1-2550), and the DARPA TIAMAT program (HR0011-24-9-0428)."}, {"title": "VII. APPENDIX", "content": "A. Method details\na) Perception System: We use a strong open-world segmentation model, GSAM, to extract interactable objects from the scene image with pixel-level accuracy. GroundingDINO [66] (groundingdino_swinb_cogcoor) provides the bounding box and SAM-HQ [76] (sam_hq_vit_b) serves as the segmentation model. We borrow the code from ORION [77].\nb) Skill Library: BUMBLE has a diverse skill library to operate effectively in buildings. For each skill, we provide the description generating the parameters, type of parameters, name & description of the skill provided to the VLM for skill selection, and description of parameter mapping to low-level robotic actions (See Fig. 4 for prompt images).\nGoToLandmark skill enables the robot to navigate between locations on the same floor using a topological visual map with landmark images as nodes and a 2D occupancy map to generate trajectories. The VLM receives all landmark images with markers and the corresponding floor number. Based on the image semantics, it selects the most suitable landmark for the subtask (including the elevator landmark, if necessary). Skill parameter(s): Landmark image\nSkill description prompt for skill selection:\nskill_name: goto_landmark\narguments: Selected landmark image from the\nenvironment from various options.\ndescription: Navigates to the landmark in the\nenvironment. For instance, bedroom, kitchen,\ntool shop, etc.\nParameter to low-level robot commands: The landmark images have a pre-defined mapping to the corresponding 2D pose on the occupancy map, which acts as the target pose. We use a global and local planner to generate a trajectory from the current robot pose to the target pose using the 2D occupancy map and the robot's front 2D lidar.\nNavigateNearObj skill enables the robot to traverse near an object that is visible to the robot. We use a GSAM query all objects to segment objects in the scene. The segmentations are overlayed with markers on the image. Skill parameter(s): Object segmentation\nSkill description prompt for skill selection:\nskill_name: navigate_to_point_on_ground\narguments: object\ndescription: Moves the robot to a point near the\nselected object. This skill can be used to move\nto a point in the room to perform a task,\nexample, navigating near the toaster to make a\ntoast.\nParameter to low-level robot commands: Object segmentation is used to calculate the object's pose relative to the robot. The nearest point on the ground to the object is converted to a 2D pose on the map, which in turn is used to generate low-level robot base commands.\nMoveBase skill equips the robot to adjust the base by 30 cm in one of the four directions: forward, left, right, and backward. The pose of the robot base after moving 30 cm in each direction is calculated and overlayed on the scene image.\nSkill parameter(s): Direction\nSkill description prompt for skill selection:\nskill_name: move_base\narguments: direction\ndescription: Moves the robot base either forward,\nbackward, left, or right by 0.3 meters, w.r.t.\nthe camera view. This skill should only be used\nto adjust the base of the robot in the final\nfew meters of navigation and not to traverse\nbetween rooms.\nParameter to low-level robot commands: The robot's pose relative to the map frame after moving 30 cm in the selected direction is calculated and used as the target base pose.\nPickup skill allows the robot to pick up an object using the left arm. We use a GSAM query all objects to segment objects in the scene. The segmentations are overlayed with markers on the image.\nSkill parameter(s): Object segmentation\nSkill description prompt for skill selection:\nskill_name: pick_up_object\narguments: object_of_interest\ndescription: pick_up_object skill moves its arms to\npick up object_of_interest. The pick_up_object\nskill can only pick up objects within arm\nreach and does not control the robot base. The\nrobot cannot pick up heavy objects like chairs,\ntables, etc.\nParameter to low-level robot commands: The object pose is calculated relative to the robot using the segmentation mask, which then helps determine an approach and a goal pose for the left-arm gripper. Inverse Kinematics is used to find the corresponding joint configuration, which is then achieved via the joint controller.\nPushObjOnGround skill enables the robot to push a visible object forward, left, or right. We query the VLM twice: a) to select the object to push where the possible options are generated using a GSAM query (all objects), and b) to determine the push direction, object position after pushing is approximated and overlayed on the image.\nSkill parameter(s): Object segmentation, and direction\nSkill description prompt for skill selection:\nskill_name: push_object_on_ground\narguments: object, direction\ndescription: Pushes an object on the ground one of\nthe directions. The robot can push objects that\nare 2-3m away from the robot. The skill\ndecides which object and direction to push.\nExample use cases: pushing obstacle to clear\nthe pathway for the robot to go forward,\npushing objects for rearrangement, etc.\nParameter to low-level robot commands: The selected object's pose is used to determine the target 2D pose for the robot base via heuristics. The robot then moves to the target pose and executes a predefined arm trajectory based on the chosen direction.\nOpenDoor enables the robot to open push-doors using the left or right arm. Two markers indicating left or right are overlayed on the image (central left, central right image positions).\nSkill parameter(s): Left or Right\nSkill description prompt for skill selection:\nskill_name: open_door\narguments: door_side\ndescription: Opens the door by pushing on a certain\nside of the door (left or right). The robot\nmoves forward with its door_side arm out to\npush open the door.\nParameter to low-level robot commands: The robot aligns itself with the robot door. It moves the selected arm to a predefined joint configuration and then moves toward the door to push it open using the forearm of the selected arm as the contact point.\nCallElevator allows the robot to call the elevator to the current floor and move inside the elevator. The buttons are segmented using GSAM query buttons.\nSkill parameter(s): Button Segmentation\nSkill description prompt for skill selection:\nskill_name: call_elevator\narguments: button position depending whether you\nwant to go to a floor above or below.\ndescription: Equips the robot with calling the\nelevator capability. The robot will push the\nbutton selected in the argument to call the\nelevator in the current floor. The subtask must\nindicate the current floor number and the\ndestination floor number to go to. Example: 'Go\nto the second floor from first floor.'\nParameter to low-level robot commands: The button pose is calculated using the selected segmentation mask. The button is pressed until reaching a force threshold, measured by the force-torque sensor on the robot's right wrist. After pressing the button, the robot moves inside the elevator to a predefined map pose.\nUseElevator allows the robot to use the elevator to go to a target floor and then move outside the elevator. The buttons are segmented using GSAM query buttons.\nSkill parameter(s): Button Segmentation\nSkill description prompt for skill selection:\nskill_name: use_elevator\narguments: button position of the floor to go to.\ndescription: Equips the robot with using elevator\ncapabilities. The robot will push the button\nselected in the argument. This skill is used to"}, {"title": "B. Experimental Evaluations", "content": "a) Task Specifications: For each task, we evaluate the three task specifications highlighted below to demonstrate the robustness of our system to diverse user language instructions.\nRetrieve diet soda can: 1) \"Could you grab me a drink that is low in calories?\u201d 2) \u201cAny chance you can find me a sugar-free soda?\u201d 3) \u201cI want something fizzy to drink, but I am on diet. Can you help me with that?\u201d\nRetrieve colored marker: 1) \"I want to color the sky in my drawing. Can you get me a marker?\u201d 2) \u201cI want to color grass in my drawing. Can you get me a marker?\u201d 3) \u201cI need to color some hearts. Can you get a marker for that?\u201d\nRearrange chairs: 1) \u201cCould you make the seating chairs in the reception area more orderly?\u201d 2) \u201cMake the reception area more welcoming by arranging the chairs\u201d 3) \u201cCan you help me arrange the chairs in the reception area?\u201d\nb) VLM checkpoints: We demonstrate in Fig. 5 that BUMBLE is a general framework that leverages the rapidly advancing capabilities of VLM. We provide the VLM checkpoint details for reproducibility: GPT4o (gpt-4o-2024-05-13), GPT4o-mini (gpt-4o-mini-2024-07-18), Pro-1.5-Gemini (Gemini-1.5-Pro-Exp-0827), Flash-1.5-Gemini (Gemini-1.5-Flash-Exp-0827), Haiku-3-Claude (claude-3-haiku-20240307), Sonnet-3-Claude (claude-3-sonnet-20240229), Opus-3-Claude (claude-3-opus-20240229), Sonnet-3.5-Claude (claude-3-5-sonnet-20240620)."}]}