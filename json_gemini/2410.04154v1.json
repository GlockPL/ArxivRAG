{"title": "Applying Quantum Autoencoders for Time Series\nAnomaly Detection", "authors": ["Robin Frehner", "Kurt Stockinger"], "abstract": "Anomaly detection is an important problem with applications in various domains such as fraud detec-\ntion, pattern recognition or medical diagnosis. Several algorithms have been introduced using classical\ncomputing approaches. However, using quantum computing for solving anomaly detection problems\nin time series data is a widely unexplored research field.\nThis paper explores the application of quantum autoencoders to time series anomaly detection.\nWe investigate two primary techniques for classifying anomalies: (1) Analyzing the reconstruction\nerror generated by the quantum autoencoder and (2) latent representation analysis. Our simulated\nexperimental results, conducted across various ansaetze, demonstrate that quantum autoencoders con-\nsistently outperform classical deep learning-based autoencoders across multiple datasets. Specifically,\nquantum autoencoders achieve superior anomaly detection performance while utilizing 60-230 times\nfewer parameters and requiring five times fewer training iterations. In addition, we implement our\nquantum encoder on real quantum hardware. Our experimental results demonstrate that quantum\nautoencoders achieve anomaly detection performance on par with their simulated counterparts.", "sections": [{"title": "1 Introduction", "content": "Anomaly detection in time series is a critical\ntask across various domains, including network\nmonitoring, medical diagnosis, and financial fraud\ndetection [1-5]. The challenges in this field are\nmultifaceted, with a significant obstacle being the\nscarcity of anomalies [1, 6, 7]. Traditional tech-\nniques for event classification often fall short when\nfaced with the imbalance between anomalous and\nnormal events [2]. Furthermore, the increasing\nvolume of data necessitates novel approaches.\nRecent advancements in quantum computing\n[8-12] and in particular in quantum machine\nlearning [10, 13\u201315], with its demonstrated expres-\nsive power, present new opportunities to address\nthe complexities of time series anomaly detection.\nIn this paper, we investigate the applicability of\nquantum autoencoders [16, 17] and demonstrate\ntheir effectiveness compared to classical autoen-\ncoders [3] using a set of challenging benchmark\ndatasets.\nThe quantum autoencoder introduced by [16,\n17] shares the same idea with classical deep\nlearning-based autoencoders where the dimension\nof the input time series is reduced by an encoder\nand then the compressed representation can be\ndecoded by a decoder to produce a model out-\nput. Similar to deep learning-based autoencoders,"}, {"title": "2 Preliminaries", "content": "This section presents the foundational concepts\nnecessary for this study, organized into two key\nsubsections. The first subsection, Time Series\nAnomalies, introduces the concept and classifica-\ntion of anomalies in time series data, emphasizing\nthe challenges associated with their detection. The\nsecond subsection, details the architecture, oper-\national principles, and application of quantum\nautoencoders for anomaly detection in time series\ndata."}, {"title": "2.1 Time Series & Anomalies\nFoundation", "content": "A time series, represented as $X = {x_i}$, is\na sequence of values recorded at different time\npoints, where $x_i$ denotes the value at time i.\nFor simplicity, $x_i$ is described as a scalar value,\nalthough it can be a vector. The terms \"data\npoint\" or \"point\" refer to $x_i$, while a \"window\"\nrepresents a contiguous subsequence of X, denoted\nas ${x_i, x_{i+1},..., x_{i+w}}$, where w represents the\nwindow width.\nThe sliding window approach is commonly\nemployed in time series analysis, dividing the time\nseries into fixed-length segments or windows. Each\nwindow is analyzed separately, with the width w\ndetermining the number of data points in each seg-\nment. By sliding the window along the time series\nwith a step size, overlapping segments are created,\nenabling a more comprehensive understanding of\nthe data. The window width and step size can\nbe adjusted to suit the specific characteristics of\nthe data being studied. Anomaly detection in time\nseries data involves identifying deviations from\nexpected behavior. Three types of anomalies have\nbeen identified in the literature [6, 7]:"}, {"title": "Point Anomalies", "content": "These anomalies occur at individual time points\nand significantly deviate from the general pattern\nor trend of the time series data. They can be spikes\nor glitches compared to neighboring points [7]. A\ndepiction can be found in Figure la."}, {"title": "Contextual Anomalies", "content": "Contextual anomalies, shown in Figure 1b, refer\nto data points or sequences observed within a\nshort time window that do not deviate from the"}, {"title": "Collective Anomalies", "content": "Collective anomalies, illustrated in Figure 1b,\noccur over an extended period and represent a\nset of data points that collectively deviate from\nnormal patterns. Detecting such anomalies is chal-\nlenging as they may not be immediately apparent\nand require examination of long-term context\n[6, 7]."}, {"title": "2.2 Quantum Autoencoder", "content": "[16] and [17] describe in their work a specific quan-\ntum machine learning model designed to perform\nunsupervised learning tasks, particularly data\ncompression and representation learning, on quan-\ntum systems. The described quantum autoencoder\nbuilds upon the concept of classical autoencoders\nand adapts it to quantum computing. In this\nwork, we utilize the implementation proposed\nby [17], which introduces a two-step architec-\nture employing two dedicated circuits. While the\nfinal architecture, shown in Figure 2b, is simi-\nlar to deep learning-based approaches in that it\nefficiently projects high-dimensional data into a\nlower-dimensional space, the training process in\nquantum computing differs. It requires an addi-\ntional circuit (Figure 2a) that is discarded after\nthe training phase is completed.\nIn this particular case, the quantum autoen-\ncoder uses 7 qubits where qubit 0-5 represent the\nlatent space and qubit 6 is called trash state as\nit will be reset\nor discarded - after encoding the\nquantum state $|x\\rangle = f(x)$. The different layers\nare described in more depth below."}, {"title": "State Preparation", "content": "The procedure (7) transforms input data into the\ninitial quantum state. In this specific instance, the\nfeature space resides in the $2^7 = 128$-dimensional\nHilbert space $H^{128}$. It is crucial to note that\nstate preparation focuses solely on producing a\nvalid quantum state and does not concern itself\nwith reconstructability. When preparing a quan-\ntum state for a quantum autoencoder, one must\nbe aware of the fact that, although the pre-\npared quantum state may contain negative values,\nthe output of the circuit comprises measure-\nments that represent a probability distribution,\nwith all its inherent constraints. Consequently, the\nreconstruction post-measurement will not include\nany negative values. This aspect is significant in\nanomaly detection settings, where the identifica-\ntion of anomalies is based on the dissimilarity\nbetween the input and the reconstructed, mea-\nsured output."}, {"title": "Encoder", "content": "The encoder block is built upon multiple varia-\ntional quantum gates and is applied to the ini-\ntial quantum state to map the data to a lower\ndimension, resulting in compression. In the illus-\ntration, the encoder was trained to map from a\n$2^7$-dimensional Hilbert-Space to $2^6$. Ideally, qubit\n6 does not hold any information (i.e it is equal\nto the zero state) after application of the encoder\nresulting in all of the information condensed into\nqubits 0 to 5."}, {"title": "Qubit Reset", "content": "In this step, the qubits corresponding to the trash\nstates are reset to the zero state to condense\nall information into the qubits representing the\nlatent space. This process produces the encoded\ndata, which can be represented with a reduced\nnumber of qubits. As demonstrated in Figure 2,\nqubits 0-5 represent the latent space, while qubit\n6 corresponds to the trash state and will be reset."}, {"title": "Decoding", "content": "The unitary property of the encoding transforma-\ntion removes the necessity of training a decoder, in\ncontrast to classical deep learning-based autoen-\ncoders. The decoder, in this context, becomes a\nstraightforward inversion of the transformation\naccomplished by applying the compressed data\nto the conjugate transpose of the encoder. The\nobtained quantum state in the ideal case is equal\nto the quantum state after applying the state\npreparation procedure."}, {"title": "Training", "content": "Resetting of the trash qubit(s) during the encod-\ning process leads to the loss of all information\nstored in the corresponding qubits and influences\nthe decoding process. Therefore, to mitigate such\ninformation loss, an ideal scenario entails the\nencoder to efficiently map all relevant informa-\ntion to the qubits associated with the latent space\n(i.e. qubits 0-5 as depicted in Figure 2) before the\nresetting operation occurs. This approach aims to\nminimize the impact of information loss during the\nencoding procedure.\nThe mapping is accomplished using the cir-\ncuit depicted in Figure 2a, which incorporates the\nstate preparation function (7) and the encoder\ncircuit of the desired autoencoder. This circuit is\nextended by introducing a reference qubit (qubit\n7) initialized as $|0\\rangle$ and an ancillary qubit (qubit\n8) for executing the Swap-Test [19], which deter-\nmines whether a pair of states are inequivalent."}, {"title": "3 Methodology of Quantum\nAutoencoders for Anomaly\nDetection", "content": "This section outlines the methodology for apply-\ning quantum autoencoders to time series anomaly\ndetection depicted in Figure 3. The approach\nincludes preprocessing, state preparation, autoen-\ncoding, postprocessing, and anomaly classifica-\ntion. State preparation and autoencoding are done\naccording to [17]."}, {"title": "3.1 Preprocessing", "content": "Upon considering a time series, we generate slid-\ning windows of a fixed size, specifically $2K$, where\nK corresponds to the number of qubits incorpo-\nrated in the quantum autoencoder, as outlined\nin Section 2.2. Notably, at this stage, we refrain\nfrom normalizing the time series data since this\noperation is subsequently performed during the\nquantum state preparation process, as elaborated\nupon in Section 3.2. Nevertheless, it is important\nto note that optional normalization of the time\nseries can be carried out if deemed appropriate as\nin the case of using a classical deep learning-based\nautoencoder."}, {"title": "3.2 State Preparation", "content": "For state preparation, we adopt amplitude encod-\ning, wherein each window $w_i$ is transformed into\na quantum state $|\\Psi_i\\rangle$ by leveraging probability\namplitudes. We selected amplitude encoding due\nto its efficiency in encoding a large number of\ninput features with a minimal qubit requirement.\nSpecifically, this method requires only log(N)\nqubits to encode N datapoints, which is sig-\nnificantly more efficient compared to alternative\ntechniques such as angle encoding. This encod-\ning method establishes a direct correspondence\nbetween the input and the probability amplitudes\nof the 2K qubit state.\nSpecifically, given a window $w_i =\n[w_{i0}, w_{i1}, ..., w_{i2^K -1}]$ of size $2^K$ spanning from\nindex i to i + $2^K$-1 of the time series, the pre-\npared quantum state using amplitude encoding is\nexpressed as $|\\Psi_i\\rangle = \\sum_{i=1}^{2^K} (\\frac{w_{ii}}{\\sqrt{w}}) |w_i\\rangle$. It is important to\nhighlight that the state preparation step does not\ninvolve trainable parameters for autoencoding\nand is entirely deterministic in nature."}, {"title": "3.3 Autoencoding", "content": "After state preparation, the time windows are\nautoencoded using either the trainable or trained\narchitecture shown in Figure 2a and Figure 2b,\nrespectively. Utilizing the trained architecture,\nwhich involves the inverse transform after reset-\nting the trash qubits, resembles classical deep"}, {"title": "3.4 Postprocessing", "content": "Upon autoencoding the initial quantum state $|\\Psi_i\\rangle$\nand deriving the output state $|\\phi_i\\rangle$, we either com-\npute the mean squared error $e_i = \\frac{1}{2^k} \\sum_{i=1}^{2^k} ((\\phi_i - \\Psi_i)^2)$ between input and reconstructed\noutput in case we use the autoencoder architec-\nture depicted in Figure 2b. If we use the Swap-Test\nmeasurements (i.e the trainable setup shown in\nFigure 2a) we generate a vector $\\epsilon= [e_1...e_n]$ of the\nsame length as the number of time windows we\nprocessed, where $e_i$ is the Swap-Test measurement\nfor the particular time window starting at $t_i$.\nMoving averaging is employed to eliminate\npotential outliers and assumes that for time\nwindows containing anomalous data points the\nrespective metric employed consistently differs\nfrom time windows comprising benign data. In\nthis paper we adopt a moving average approach\nwith fixed window size on $\\epsilon$, yielding $|\\epsilon - |M_w||$\nestimations for each data point, where $|M_w|$ is the\nsize of the moving average window employed. The\nresultant post-processed time series is denoted as\nP, where $p_i$ corresponds to the post-processed\nreconstruction error (or Swap-Test measurement)\nat time step i."}, {"title": "3.5 Classification", "content": "The final step involves classifying the time series,\nidentifying anomalies based on the post-processed\ndata as outlined in Section 3.4. Commonly, a\nthreshold-based classification approach is used,\nwherein a time step i is designated as anomalous\nif its corresponding post-processed reconstruction\nerror $p_i$ surpasses or equals a pre-defined threshold\nt. The classification procedure used for this study\nis explained in Section 4.1."}, {"title": "4 Experimental Setup", "content": "The primary objective of this research is to\nassess the effectiveness of quantum autoencoders\nin detecting anomalies in time series data, as well\nas to demonstrate their successful implementation\non real quantum hardware.\nThe first experiment evaluates the perfor-\nmance of quantum autoencoders using quantum\nsimulators compared to a deep learning-based\nbaseline as detailed in Section 4.3. The second\nexperiment evaluates quantum autoencoders on\nreal quantum hardware as described in Section\n4.4."}, {"title": "4.1 Data Set", "content": "For our experiments we use the University of Cal-\nifornia at Riverside (UCR) Time Series Anomaly\nArchive [18], a benchmark dataset specifically\ndesigned for time series anomaly detection. This\narchive provides a diverse array of datasets from\nvarious domains, including medicine, sports, ento-\nmology, industry, space science, robotics, and\nmore. Each dataset contains only a single anomaly\nin the test set and no anomalies in the training\ndata.\nUsing a window-based methodology, an\nanomaly is considered a valid detection of the\nanomaly if it starts within the valid detection\nrange, which includes both the 'Anomaly Pre-\ncursor' and 'Anomalous' ranges. The 'Anomaly\nPrecursor' denotes the time windows at time t that\nprecede the anomalous range but overlaps with it.\nFor our experiments, we focus on a subset of\nthe datasets provided by the authors. We\npreprocess the data into sliding windows of size\n128 with a stride of 1 as done by [5], with each\nwindow serving as input to the autoencoder."}, {"title": "4.2 Baseline: Classical Autoencoder", "content": "To enable a comparative analysis between quan-\ntum autoencoders and classical deep learning-\nbased autoencoders, we establish a classical base-\nline model. The architecture shown in Figure\n5 replicates the autoencoder utilized by [5],\nwhich has demonstrated effectiveness on the UCR\nBenchmark. Given our window size of 128, the\ninput dimension is correspondingly set to 128."}, {"title": "4.3 Quantum Simulator Experiment", "content": "For our experiments we use various quantum\nautoencoder architectures as shown in Table 1.\nPerformance will be reported based on the average\nof 5 executions for each employed ansatz.\nWe employ the autoencoder architecture\ndetailed in Section 3, in combination with ampli-\ntude encoding for data representation. The input\ndata comprises time windows generated accord-\ning to the procedure described in Section 4.1. In\nour simulated experiments, 7 qubits are allocated\nfor data encoding purposes. During the training\nphase, elaborated upon in Section 2.2, additional\nqubits are required, totaling 9 qubits. The latent\nspace consistently utilizes 6 qubits, thereby aim-\ning to compress the initial 7 qubit quantum state\ninto a more concise representation."}, {"title": "RealAmplitudes Ansatz", "content": "The RealAmplitudes ansatz by IBM Qiskit fea-\ntures alternating layers of Y rotations and CX\nentanglements. The entanglement pattern can be\nuser-defined or chosen from a predefined set. We\nwill use RealAmplitudes with varying depth (i.e.\nthe number of repeating Y rotations and entangle-\nment) as well as different entanglement patterns."}, {"title": "Pauli TwoDesign Ansatz", "content": "This ansatz implements a specific type of circuit,\nfrequently explored in quantum machine learning\nliterature, particularly in studies involving barren\nplateaus in variational algorithms [20]. The circuit\ncomprises alternating rotation and entanglement\nlayers, starting with an initial layer of $R_y$ (4)\ngates. In the rotation layers, single-qubit Pauli\nrotations are applied, with the rotation axis ran-\ndomly chosen from X, Y, or Z. The entanglement\nlayers consist of pairwise CZ gates. For consis-\ntency, the same random seed (42) to generate the\ndifferent ansaetze is used throughout this work."}, {"title": "Quantum Simulator System", "content": "For this experiment, the IBM Qiskit 1.0.0 [21] and\nQiskit Aer 0.13.1 [21] Python library were used\nwith the statevector as the chosen simulator. No\nchanges were made to the simulator settings and\nthe numbers of shots per execution was set to\n5,000 as we only measure 1 qubit without noise\nsimulation. All experiments were conducted on\n16 Intel Broadwell CPU cores. For optimization\nwe use COBYLA with default settings and 45\niterations."}, {"title": "Baseline and Anomaly Classification", "content": "The baseline deep learning architecture is trained\non the same dataset as the quantum autoencoders,\nutilizing a batch size of 150, the ADAM optimizer\nwith default settings, and 250 epochs. Anomalies\nare classified based on the mean squared error\nbetween the input and its reconstructed output.\nThe quantum autoencoders detect anomalies\nusing two approaches: a single Swap Test measure-\nment (architecture shown in Figure 2a) and input\nreconstruction with mean squared error calcula-\ntion (architecture shown in Figure 2b), similar to\nthe baseline method.\nAnomaly classification is performed on the\npost-processed output, following the procedure\noutlined in Section 3.4. This involves a moving\naverage window of size 128, which corresponds\nto the time window size. This window size is\narbitrarily chosen and is not optimized for each\ndataset. The time step exhibiting the largest\npost-processed value is flagged as anomalous."}, {"title": "4.4 Real Quantum Hardware\nExperiment", "content": "For evaluating the quantum autoencoder on real\nhardware, a couple of changes are required because\nof the noisiness of NISQ quantum computers. Sub-\nsequently we describe the experimental setup for\nthe quantum autoencoder on real hardware."}, {"title": "Data and Encoding", "content": "For the experiments conducted on a real quan-\ntum computer, we will concentrate on a single\ndataset, specifically dataset no. 54, and further\nreduce the training data to 150 sliding time win-\ndows. The training data spans from ts = 400 to\nte = 550 + 127 = 677, thereby comprising 150\ntime windows and covering 277 time steps. This\nreduction in data is essential, as each time window\ncorresponds to an individual circuit execution,\nwhich incurs substantial quantum computational\ntime.\nA critical aspect of executing a quantum cir-\ncuit on actual hardware is the transpilation pro-\ncess. This involves converting the logical design of\nthe desired circuit into a format suitable for a spe-\ncific target quantum system, such as IBM Torino\n[22]. During transpilation, the designed circuit is\ntransformed into a logically equivalent one com-\nprising only the quantum operations supported by\nthe target system. Additionally, at this stage, the\nlogical qubits are mapped onto physical qubits,\na process that is both complex and non-trivial."}, {"title": "Quasi Amplitude Encoding", "content": "To address this issue, we employ quasi-amplitude\nencoding, following the approach outlined in\n[23]. Quasi-amplitude encoding is a technique\nin quantum computing that enables the effi-\ncient embedding of classical data into quantum\nstates. Unlike exact amplitude encoding, which\nrequires complex and resource-intensive quan-\ntum circuits to precisely represent data values,\nquasi-amplitude encoding introduces a controlled\napproximation that significantly reduces circuit\ncomplexity while maintaining acceptable accu-\nracy. Figure 7 illustrates a time series comparison"}, {"title": "Further Hardware Optimization", "content": "To further optimize for hardware efficiency, we\nnarrow our focus to anomaly identification using\nthe quantum autoencoder architecture depicted\nin Figure 2a and classify anomalies based on\nSwap-Test measurements. This method reduces\nthe number of operations required by omitting the\ndecoding step and solely evaluating the encoder's\nefficacy in representing the data in the latent\nspace. This evaluation is consistent with the mea-\nsurement minimized during the training phase.\nThe training process involves the 150 training\ntime windows over 45 epochs, utilizing COBYLA\nwith default parameters for optimization and\n8192 shots. We chose 8192 as it is the maxi-\nmum available on the IBM Torino machine at\nthe time of writing and to ensure best possible\nerror mitigation. All experiments on real quan-\ntum hardware are conducted on the 133-qubit\nIBM Torino system in session mode, leveraging\nthe Sampler primitive [24]. The Sampler primitive\ngenerates a quasi-probability distribution with\nbuilt-in error mitigation, in contrast to raw mea-\nsurement counts. In this work, we leverage this\nintegrated error mitigation."}, {"title": "Baseline Setup and Anomaly Classification", "content": "To enable meaningful comparison, the baseline\ndeep learning-based autoencoder uses the quasi-\namplitude encoded time windows to ensure that\nany additional complexity introduced by quasi\namplitude-encoding the time windows does not\ndisadvantage the quantum autoencoder. Conse-\nquently, both the training and test data consist\nof the extracted quantum states from the quasi\namplitude-encoded circuits corresponding to the\nassociated time windows."}, {"title": "5 Results", "content": "In this section we discuss the results of our\nanomaly detection experiments on a quantum\nsimulator as well as on a real quantum device."}, {"title": "5.1 Results on Quantum Simulator", "content": "The results in Table 2 demonstrate that\nfor each dataset, quantum autoencoders\noutperform the classical autoencoder. How-\never, performance across datasets and ansaetze\ndiffer greatly. Performance disparities range from\nnot detecting the anomaly a single time for the\nMSE-based approach on dataset number 28 to\ncorrectly detecting the anomaly at least once for\nevery setup on dataset 176 with the Swap-based\nclassification approach. Additionally, it appears\nthat anomalies are especially hard to detect in\ndatasets 28 and 99.\nThis variability in performance underscores\nthe necessity of \"hyperparameter\" optimization,\nsimilar to traditional machine learning, as even\nminor parameter adjustments can significantly\nimpact performance. For instance, on data set 28,\na RealAmplitudes ansatz with SCA entanglement\nand 28 parameters could not identify the anomaly\nonce out of the 5 executions but adding one more\nlayer and having 35 trainable parameters resulted\nin correctly identifying the anomaly two times.\nIn addition, we want to emphasize that the\nquantum autoencoders are less complex\nin terms of trainable parameters (ranging\nfrom 21 to 77) than the classical autoen-\ncoder baseline with 4,686 parameter. More-\nover, the quantum autoencoders required"}, {"title": "5.1.1 Swap-based vs. MSE-based\nDetection", "content": "Significant performance disparities between MSE-\nbased and Swap-based anomaly detection are not\nlimited to dataset 176, as shown in Figure 11. For\ndatasets like 118 and 176, where these discrep-\nancies are pronounced, MSE-based classification\noften requires low training loss, as illustrated in\nFigure 12. Specifically, for dataset 176 (Figure\n12b), MSE-based detection outperforms Swap-\nTest detection at low training loss, correctly iden-\ntifying anomalies in 5 out of 6 cases within the\n[0.012, 0.02] interval, compared to only 2 out of 6\nfor the Swap-Test.\nIn summary, Swap-Test-based anomaly detec-\ntion appears more robust at higher loss levels,\nwhile MSE-based detection is more effective at\nlower loss. This pattern, along with the overall\ndistribution of losses for the evaluated ansaetze\n(Figure 12), may explain the superior performance\nof the Swap-Test approach on datasets 118 and\n176, where most ansaetze converge to loss levels\nfavoring Swap-based classification."}, {"title": "5.1.2 Complexity Analysis", "content": "We end by discussing the overall impact of the\ncomplexity of the chosen ansaetze on their train-\ning performance. Figure 13 illustrates the behavior\nof training performance as the number of train-\nable parameters increases. Figure 13a focuses on\nthe ansaetze, where the training loss at each\nnumber of trainable parameters represents the\naverage loss for that particular ansatz across all\ndatasets. Conversely, Figure 13b displays the aver-\nage training loss at each number of parameters per\ndataset, with the averaged value derived from all\nansaetze for a given dataset at a specific number\nof parameters.\nIt is evident that the training loss tends to\nincrease with the number of parameters. This\ntrend appears to be independent of the dataset.\nNotably, the RealAmplitudes ansatz combined\nwith the SCA entanglement scheme demonstrates\na significant drop in training loss at 35 parameters\nacross all datasets.\nThe phenomenon of increasing training loss\nfor increasing complexity is counterintuitive, as\none would typically expect better performance\non training data with increased model complex-\nity. However, this could be attributed to vari-\nous factors, including the optimizer used in this\nstudy (COBYLA). Further research is necessary\nto determine whether this is a consequence of the\ntraining paradigm.\nThe only exception to the increase in training\nloss poses the PauliTwoDesign where the train loss\nremains roughly constant with increasing com-\nplexity and might result from the fact that it was\nespecially designed by [20] to overcome barren\nplateaus.\nTo summarize, ansaetze relying on the\nRealAmplitudes design in combination with\nCOBYLA optimization tend to perform poorly\nwith increasing number of parameters with the\nnotable exception of RealAmplitudes in combina-\ntion with the shifted circular alternation entan-\nglement scheme and 35 parameters which signifi-\ncantly outperforms all ansaetze in terms of train-\ning performance. The PauliTwoDesign demon-\nstrates near constant training performance across\nall complexities."}, {"title": "5.2 Results on Real Quantum\nHardware", "content": "The results in Table 3 show that all quantum\nautoencoders, whether trained on a simu-\nlator or on IBM Torino, successfully iden-\ntified the anomaly by selecting a time window\nwithin the valid detection range.\nFigure 14 presents the Swap-Measurement for\nthe time window starting at ts. Each value at ts\ncorresponds to the Swap-Measurement obtained\nfrom the autoencoder, which encodes the time\nwindow starting at ts and spanning |W| time\nsteps, where |W| is 128 in this study. These mea-\nsurements are derived from the best-performing\nquantum autoencoder trained on real hardware,\nspecifically at the epoch exhibiting the lowest\nrecorded loss, which in this case is epoch 25. The\nmoving average used a window size of 30.\nThe similarity between measurement values\nat the start of the valid detection range (high-\nlighted in orange) and those of non-anomalous\nvalues can be explained by the dataset character-\nistics discussed in Section 4.1. Early time windows\nin this range typically have minimal overlap with\nactual anomalies and consist mostly of benign\nvalues. This is supported by the increase in Swap-\nMeasurements toward the middle of the range,\nwhere time windows fully capture the anomaly.\nThe moving average smoothing process reveals\nthat anomalous time windows, on average, pro-\nduce larger Swap-Measurements, indicating the\nautoencoder's difficulty in accurately representing\nthese anomalies within the 6-qubit latent space."}, {"title": "Separability of Anomalies", "content": "To evaluate the efficacy of separating anomalous\ntime windows from benign ones, we examine the\ndata presented in Table 3, which reports the frac-\ntion of overlapping post-processed measurements.\nThis fraction is calculated by dividing the num-\nber of post-processed measurements of anomalous\ndata that falls within the bounds of the distribu-\ntion of benign measurements by the total number\nof anomalous measurements. This analysis is per-\nformed for all valid detection windows as well as\nfor those fully overlapping with the anomalous\nregion. This method corresponds to calculating\nthe fraction of overlap depicted in Figure 15,\nwhere, in this particular example, the distribution\nof Swap-Test measurements of benign time win-\ndows with those fully encompassing the anomalous\nregion would result in zero overlap.\nDespite quantum autoencoders trained on real\nquantum hardware exhibiting significantly larger\nloss values compared to their simulator-trained\ncounterparts, as indicated by the first and second\nrow in Table 3, their ability to separate anoma-\nlous windows from benign ones shows relatively\nlittle difference. When comparing mean values and\nexcluding the outlier for the simulated quantum"}, {"title": "Training Performance", "content": "We conclude by examining the performance of\nquantum autoencoders trained on the IBM Torino\nquantum computer and the quantum simulator.\nFigure 16 depicts the epoch loss for both setups,\nrepresenting the average results of four executions,\nwith the minimal and maximal values observed\nper epoch indicated.\nThe loss curves reveal that the quantum\nautoencoders trained on a simulator achieves a\nmore rapid reduction in loss on average, whereas\nthe quantum autoencoder exhibits a more grad-\nual decline. By the conclusion of 45 epochs, both\nmethods converge to similar loss values, although\nthe simulator's final epoch shows a notable spike\nand generally maintains a lower loss through-\nout the training period. Additional epochs are\nrequired to effectivley determine whether both\nmethods converge to identical loss values over an\nextended training period."}, {"title": "6 Related Work", "content": "The field of anomaly detection using quan-\ntum computing has gained considerable atten-\ntion, but most research has focused on non-time\nseries anomaly detection, employing a variety of\napproaches. Notably, [25", "26": "explore hybrid\nmethodologies, combining traditional deep learn-\ning with quantum computing. Specifically, [25"}, {"26": "replace individual lay-\ners in deep learning-based image classification,\nsuch as CNNs, with quantum variational circuits.\nNotably [27", "28": "further demonstrate the\nanomaly detection capabilities of quantum\nautoencoders. However, the dataset utilized for\nthis task does not adhere to commonly estab-\nlished benchmarks. Instead, it is generated by\ncreating quantum states based on randomly cho-\nsen parameters for a given circuit. The benign\nand anomalous datasets are subsequently cre-\nated by sampling parameters for the generating\ncircuit from normal distributions N(0,0.1) and\nN(5, 0.1), respectively.\nFurther applications of quantum autoencoders\nwere demonstrated by [29", "30": "nintroduce Quantum Variational Rewinding\n(QVR). QVR trains parameterized unitary time-\ndevolution operators to cluster normal time series\ndata represented as quantum states"}]}