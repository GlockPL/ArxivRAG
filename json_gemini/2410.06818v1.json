{"title": "An Improved Approach for Cardiac MRI Segmentation based on 3D UNet Combined with Papillary Muscle Exclusion", "authors": ["Narjes Benameur", "Ramzi Mahmoudi", "Mohamed Deriche", "Amira fayouka", "Imene Masmoudi", "Nessrine Zoghlami"], "abstract": "Background: Left ventricular ejection fraction (LVEF) is the most important clinical parameter of cardiovascular function. The accuracy in estimating this parameter is highly dependent upon the precise segmentation of the left ventricle (LV) structure at the end diastole and systole phases. Therefore, it is crucial to develop robust algorithms for the precise segmentation of the heart structure during different phases.\nMethodology: In this work, an improved 3D UNet model is introduced to segment the myocardium and LV, while excluding papillary muscles, as per the recommendation of the Society for Cardiovascular Magnetic Resonance. For the practical testing of the proposed framework, a total of 8,400 cardiac MRI images were collected and analysed from the military hospital in Tunis (HMPIT), as well as the popular ACDC public dataset. As performance metrics, we used the Dice coefficient and the F1 score for validation/testing of the LV and the myocardium segmentation.\nResults: The data was split into 70%, 10%, and 20% for training, validation, and testing, respectively. It is worth noting that the proposed segmentation model was tested across three axis views: basal, medio basal and apical at two different cardiac phases: end diastole and end systole instances. The experimental results showed a Dice index of 0.965 and 0.945, and an F1 score of 0.801 and 0.799, at the end diastolic and systolic phases, respectively. Additionally, clinical evaluation outcomes revealed a significant difference in the LVEF and other clinical parameters when the papillary muscles were included or excluded.\nConclusion: The proposed framework is shown to outperform state-of-the-art methods by around 0.1 in terms of dice index, demonstrating its accuracy in the precise assessment of the left ventricular function.", "sections": [{"title": "1. INTRODUCTION", "content": "Ventricular function evaluation in routine clinical practice has seen significant development in recent years, thanks to the technological revolution we are experiencing across various fields, including medical imaging. In addition to the development of a helium-free MRI machine and the latest trends of using virtual sequences derived from artificial intelligence [1,2], radiologists have also benefited from the recently development to streamline protocols for interpreting MRI images [3].\nThe protocol of LV function often begins with a single shot fast spin echo, gradient echo or bright blood balanced steady state free precession (bSSFP) images acquired in 3 planes of views: 2-chamber, 4-chamber, and short axis. Then, endocardial and epicardial contours were delineated by radiologist using a series of images that represents the entire cardiac cycle. The Left ventricular ejection fraction (LVEF) and other clinical parameters such as end diastolic volume, end systolic volume and myocardial mass are computed using the software available on the console acquisition [4,5]. Therefore, the specificity of these clinical parameters is highly influenced by the"}, {"title": "2. Related works", "content": "In the last two decades, a wide range of approaches have been proposed to segment the left ventricle using MRI. The initial works began with the use of classical techniques such as the thresholding and the clustering approaches [28]. Furthermore, many researches have utilized cardiac atlas and deformable models for heart segmentation [29]. However, most of these techniques require user interaction and a tedious learning phase, and their segmentation results are influenced by the gray level variations around the myocardium. As a result, their outcomes are prone to"}, {"title": "3. MRI Dataset description", "content": "Two datasets were used in this study: the first is a local dataset collected from the Military Hospital of of Tunis Tunisia (HMPIT) which we called \u201cMHT-MRI\". It comprises DICOM cine MRI images representing image slices related to 156 subjects, including 7,800 MRI images (50 images per subjects * 156 subjects) of patients with cardiac diseases and healthy controls. The acquired data were fully anonymized and processed in compliance with regulations established by the local ethics committee of the HMPIT hospital of Tunisia. The data is available from the corresponding author upon request. All clinical characteristics of the studied population are summarized in"}, {"title": "4. Proposed Framework", "content": "4.1. Preprocessing\n\u2022\nResizing MRI images and their masks\nTo input data into our model, we standardized the shape of all images. A bounding box was utilized to localize the heart and determine its center's coordinates in space. Subsequently, based on the center's position relative to the desired shape (156, 156, 6), we extracted the portion of the image containing the heart. If the image's dimensions were less than 156 for either width and/or height, a new image was created from the original one using a matrix with the desired shape, prefilled with zeros.\n\u2022 Cleaning masks\nThe masks generated from segmentation results available in public datasets play an important role in evaluating the performance of deep learning models. Since the aim of this study is to segment left ventricle and myocardial while excluding the papillary muscles, a mask-cleaning step is required to remove unwanted myocardial structures from different images. Accordingly, two radiologists (7 years, and 12 years of CMRI experience, respectively) participated in identifying the left ventricle without papillary muscles using the CVI42 (Circle Cardiovascular Imaging) software available in the acquisition console. Based on these results, new masks were generated for both datasets to train the proposed 3D UNet.\n\u2022 Patching the images into smaller cubes"}, {"title": "4.2. Architecture of the modified model 3D-Unet", "content": "The proposed model architecture represents a 3D U-Net designed for medical image segmentation tasks. Here are some key features of our approach:\n\u2022\nEncoder-Decoder Structure: Similar to the original U-Net, the model consists of contracting (encoder) and expanding (decoder) paths, creating a U-shaped architecture.\n\u2022\nContextual Feature Extraction: The encoder utilizes a series of 3D convolutional layers, each followed by Batch Normalization and ReLU activation, to progressively extract and downsample features, capturing contextual information at different scales. Max Pooling layers further contribute to downsampling and reducing spatial dimensions.\n\u2022 Precise Localization: The decoder path employs 3D transposed convolutions for upsampling the feature maps, effectively increasing their resolution. Additional 3D convolutions are used to refine the feature maps for accurate localization of structures within the image.\n\u2022 Skip Connections: The model incorporates skip connections that bridge corresponding levels of the encoder and decoder paths. These connections facilitate the transfer of fine-grained details from the encoder to the decoder, leading to more precise segmentation results.\n\u2022 Output Layer: The final layer of the model is a 3D convolutional layer with a number of filters equal to the number of target classes. This layer produces the segmentation map, which has the same spatial dimensions as the input image.\nThe model is trained on 3D patches of size 64 x 64 x 4 voxels extracted from volumetric MRI images. This information is explicitly shown in the input layer of the provided architecture diagram. The training process involves feeding the model the 3D patches and optimizing its parameters to minimize a chosen loss function: Dice loss. The architecture's strengths lie in its ability to capture multi-scale contextual information, effectively localize structures, and produce accurate segmentation maps, making it a valuable tool in medical image analysis."}, {"title": "4.3. Hardware Setup for Deep Learning Network Training and Trials", "content": "The proposed architecture was trained for 100 epochs in a Google Colab Pro Environment having 61 System RAM using a batch size of 32 and the ADAM optimizer with a learning rate that evolved from 0.005 for the first 40 epochs, to 0.001 between the 41st and 60th epochs and gradually decreased to reach a value of 0.0004457 using a callback. The model was implemented in Python 3.10.12 using Keras 3.3.0 deep learning library, with TensorFlow 2.13.0 as the backend and Nvidia-SMI L4 Ti having 22.5 GB graphics memory."}, {"title": "4.4. 3D Reconstruction of segmentation results", "content": "3D LV and myocardium geometries were generated from our segmentation output using CATIA V5 (Dassault Systems) software. The algorithm takes the output results as the input. During the 3D reconstruction, the results of LV contours segmentation with 3D UNet at end diastolic and end systolic phases in short axis view were used to reconstruct the myocardium and the LV surface."}, {"title": "4.5. Validation and Statistical Analysis", "content": "Dice coefficient, Dice loss, F1 score and Intersection over Union (IoU) were used to evaluate segmentation accuracy and to assess precision between the obtained results from DL approach and the ground truth segmentation performed by radiologists. These metrics were reported for both LV and myocardium geometries."}, {"title": "5. Results", "content": "5.1. Evaluation Metrics\nIn our study, we used the Dice Similarity Coefficient (DSC) as a metric to assess the overall performance of our MRI images segmentation approach. The DSC is widely recognized and used in the discipline to quantify the similarity among units of contours or masks. The computation of the DSC is primarily based on the following equation:\n$DSC = \\frac{2 |P\\cap G|}{|P| + |G|}$                                                                                                        (1)\nwhere P is the reference contour or masks, and G is the contour or masks expected with the aid of using our approach. The numerator is the dimensions of the intersection among the 2 units, at the same time as the denominator is the sum of the sizes of the 2 units.\nA DSC near 1 shows a correct segmentation with an excessive correspondence among the expected contours and the reference contours. On the other hand, a DSC near zero shows an erroneous segmentation or a negative correspondence among the contours, indicating much less correct results.\nThe F1 score is a metric utilized in our paper to assess the overall performance of our segmentation method. It combines each precision and recall offering a standard degree of the overall performance of the model. The calculation of the F1-score is presented as:\n$F1 score = 2 * (precision * recall) / (precision + recall)$                                                (2)\nin which precision represents the capacity of our technique to successfully discover superb pixels, and recall represents the capacity of our technique to locate all superb pixels. An excessive F1 score suggests a correct segmentation with a good aggregate of precision and recall while a low F1 score indicates erroneous segmentation or a poor balance of precision and recall.\nwe also assess the degree of similarity between the actual segmentation and the anticipated segmentation using other statistical measures, such as the Jaccard Coefficient, also referred to as Intersection-Over-Union (IoU) as shown in Eq. 4:\n$IoU (%) = TP/(TP + FP + FN)\\times 100$                                                                                 (3)\nwhere FP and FN are the false-positive and false-negative values, respectively, and TP and TN are the true-positive and true-negative numbers.\n5.2. Segmentation results\n5.3. Clinical evaluation metrics\nTo study the segmentation performances of our algorithm on cardiac clinical measurements, EDV, ESV, myocardial mass and LVEF were measured using two segmentation results: with inclusion of papillary muscles and without inclusion of these structures. All the mentioned parameters were computed using the same selected images. To reduce interobserver variability, two radiologists participated in the evaluation of these measurements.\nFirst, the clinical cardiac parameters were measured using the workstation (syngo imaging software, Siemens Healthcare, NC). LV function values with the exclusion of papillary muscles were obtained using our proposed 3D UNet model. These measurements were computed for 30 patients. The same parameters were also computed with the inclusion of papillary muscles using CVI42 software. The outcomes of both LV measurements were compared to study the impact of including or excluding papillary muscles during the segmentation process."}, {"title": "6. Discussion", "content": "In this study, an improved 3D UNet was proposed and validated for automated segmentation of LV and myocardium without including papillary muscles. The main outcome of our study is that the proposed 3D UNet is better suited for MRI Images since it requires only 4 slices per patient. This is a crucial point because the majority of public dataset include variation of number of frames and slices, making is difficult to apply 3D model that require a high frame numbers per patient.\nIn addition, the improved 3D model showed high performances in both myocardium and LV structures with a mean dice coefficient of 0.955 and 0.961 for LV and myocardium, respectively, and an F1 score of 0.801 and 0.799, an IoU of 0.712, and 0.701 for end diastolic and end systolic phases, respectively. These metric results include different cardiac geometries related to healthy controls and subjects with various cardiac pathologies (e.g., myocarditis, infarction, ...). The 3D model was also trained, tested and validated on two important instances of the cardiac cycle: end diastole and end systole phases. In both instances, good results were obtained."}, {"title": "7. Conclusion", "content": "In this study, we proposed a novel approach for cardiac MRI segmentation using a modified 3D UNet architecture. In particular, a patching technique is introduced to extract volumetric cubes from the MRI images, hence allowing the 3D UNet to learn more robust features while maintaining lower computational complexity. The proposed framework demonstrated the ability to accurately segment the left ventricle and myocardium while excluding papillary muscles, which is crucial for the precise assessment of the left ventricular function. The model achieved high Dice coefficients of 0.965 and 0.945 for the LV and the myocardium segmentation at end-diastole and end-systole phases, respectively. Additionally, the F1 scores were 0.801 and 0.799, indicating excellent overall performance.\nA key strength of our approach is the ability to capture the complex 3D structure of the heart across different slices, even for the challenging apical regions with poor contrast. Moreover, the large and diverse dataset used for training, which included 5,880 images across two datasets, ensured the model's generalizability.\nThe exclusion of papillary muscles from the LV segmentation, as recommended by the Society for Cardiovascular Magnetic Resonance, is a key advantage of our approach. By accurately delineating the LV cavity without papillary muscles, our model enables more precise quantification of clinical parameters such as end-diastolic volume, end-systolic volume, myocardial mass, and ejection fraction. This is particularly important for accurate diagnosis and monitoring of cardiac diseases. In conclusion, the large dataset and robust 3D architecture make this approach a promising tool for automating cardiac MRI analysis and enhancing clinical decision-making."}, {"title": "Acknowledgment", "content": "The authors gratefully knowledge the staff of the Military Hospital of Instruction of Tunis, Tunisia, for their assistance in the image analysis of this study, and Dr. Arous younes for facilitating the use of imaging data."}, {"title": "Declaration of competing interest", "content": "The authors declare that they have no conflict of interest."}, {"title": "CRediT authorship contribution statement", "content": "Narjes Benameur: Conceptualization, Methodology, Visualization, Validation, Writing \u2013 Original Draft.: Ramzi Mahmoudi: Methodology, Visualization, Validation, Supervision, Review and Editing. Mohamed Deriche: Validation, Supervision, Writing \u2013 Review and Editing. Amira fayouka: Methodology and Data Curation. Imene Masmoudi: Methodology and Visualization. Nessrine Zoghlami: Validation, Supervision, Review and Editing."}, {"title": "Funding sources", "content": "This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors."}]}