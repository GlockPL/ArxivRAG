{"title": "Reinforcement learning for Quantum Tiq-Taq-Toe", "authors": ["Catalin Dinu", "Thomas Moerland"], "abstract": "Quantum Tiq-Taq-Toe [5] is a well-known benchmark and playground for both quantum computing and machine learning. Despite its popularity, no reinforcement learning (RL) methods have been applied to Quantum Tiq-Taq-Toe. Although there has been some research on Quantum Chess [15,3], this game is significantly more complex in terms of computation and analysis. Therefore, we study the combination of quantum computing and reinforcement learning in Quantum Tiq-Taq-Toe (code for our work can be found [1]), which may serve as an accessible testbed for the integration of both fields.", "sections": [{"title": "1 Introduction", "content": "Quantum Tiq-Taq-Toe [5] is a well-known benchmark and playground for both quantum computing and machine learning. Despite its popularity, no reinforcement learning (RL) methods have been applied to Quantum Tiq-Taq-Toe. Although there has been some research on Quantum Chess [15,3], this game is significantly more complex in terms of computation and analysis. Therefore, we study the combination of quantum computing and reinforcement learning in Quantum Tiq-Taq-Toe (code for our work can be found [1]), which may serve as an accessible testbed for the integration of both fields."}, {"title": "2 Methodology", "content": "Quantum games are challenging to represent classically due to their inherent partial observability and the potential for exponential state complexity. In Quantum Tiq-Taq-Toe, states are observed through Measurement (a 3x3 matrix of state probabilities) and Move History (a 9x9 matrix of entanglement relations), making strategy complex as each move can collapse the quantum state.\nOur study examines two versions of Quantum Tiq-Taq-Toe from the quantumlib repository [6]. The first version restricts entanglement moves to include at least one empty cell, blending traditional rules with quantum mechanics. The second version lifts these restrictions, allowing more diverse quantum states and interactions, thereby increasing strategic depth."}, {"title": "3 Results", "content": "We conducted a comparative analysis of self-play PPO [12,13,10] agents in Quantum Tiq-Taq-Toe, exploring their performance with access to both measurement matrices and historical entanglement records (M&H agent), as well as with access to only the measurement matrix (M) or historical entanglement record (H).\nFor the first set of rules, which imposes constraints on entanglement moves, we observe a tendency for the first player to gain an advantage (Fig. 1). This advantage is noticeable despite inherent randomness in the game, which prevents guaranteed wins, and therefore suggests the presence of discernible winning strategies."}, {"title": "4 Discussion", "content": "Most quantum problems require both precise control and mitigation of partial observability, making machine learning particularly suitable. Indeed, RL has already shown promise in fields like quantum error correction [2,11]. We identify Quantum Tiq-Taq-Toe, with its various subtypes, as an accessible testbed for the development of RL methods in the quantum setting. Future work could investigate other methods to mitigate partial observability, such as the use of state windowing [9], recurrent neural networks [8], recurrent state space models [7], or transformers [14]."}, {"title": "A. Environment", "content": "Quantum Tiq-Taq-Toe [4] is an altered variant of the classic Tic-Tac-Toe game. In the traditional game, each cell on a 3x3 board can be empty, X, or O. In the quantum version, each cell is a qutrit, existing in a superposition of three quantum states: empty ($|\\_\\rangle$), X ($|X\\rangle$), or O ($|O\\rangle$). Given the full board state ($\\eta$), the probability of collapsing to a specific state $|c_1c_2...c_8c_9\\rangle$ ($c_i \\in \\{\\_, X, O\\}$) is:\n$\\begin{aligned}\n|\\eta) &= \\sum_{\\phi \\in \\{\\_,X,O\\}^9} a_\\phi |\\phi\\rangle, \\quad \\sum_{\\phi \\in \\{\\_,X,O\\}^9} a_\\phi^2 = 1 \\\\\nP(\\eta = c_1c_2...c_8c_9) &= || \\langle c_1c_2...c_8c_9 | \\eta \\rangle ||^2 = a_{c_1c_2...c_8c_9}^2\n\\end{aligned}$\nwhere $\\eta$ is the quantum state of the system, that can be express as a linear combination of all possible classical states ($|\\phi\\rangle$) with $a_\\phi$ being the probability to observe the state $\\phi$.\nAction Space In classical Tic-Tac-Toe, a move changes an empty cell to X or O (left of Figure 3). In the quantum version, these moves are XNOT $|\\_\\rangle \\rightarrow |X\\rangle$ and ONOT $|\\_\\rangle \\rightarrow |O\\rangle$.\nAdditionally, quantum moves involve entangled pairs of cells. These moves create two possible states: one with X/O in the first cell and another with X/O in the second cell, leading to complex quantum states. The simplest case entangles two empty cells with X/O, resulting in a 50% probability of X/O appearing in either cell (right of Fig. 3)."}, {"title": "State Collapsing", "content": "A pivotal phenomenon in the quantum game is State Collapsing, as illustrated in Fig.4. This occurrence occurs when the game board becomes saturated with moves, utilizing both quantum and classical moves that impact all cells on the board. Upon the state collapsing, a specific state is selected from the multitude of possible states. This selection is determined by the probability distribution outlined by the existing quantum state ($|\\eta) = \\sum_{\\phi \\in \\{\\_,X,O\\}^9} a_\\phi |\\phi\\rangle$)."}, {"title": "B. Observation Space", "content": "A paramount challenge in this game revolves around effectively representing quantum information in a classical format to facilitate an Agent's learning process. The most straightforward method involves classically storing the quantum state and simulating the game. However, this approach is deemed unreliable due to the impracticality of saving a 9-qutrit quantum state, which requires complex numbers. This not only contradicts the essence of a quantum game but also poses a significant computational burden.\nTo address this challenge, two classical pieces of information are explored in this report: Measurements and Moves History. These representations offer a more manageable way to capture and convey quantum aspects within the framework of the game, enabling effective learning for an Agent.\nMeasurements A piece of essential information that can help an Agent learn to play the game would be the probability of each cell being in either _/X/O state. However, to compute the real values of those would imply the access to"}, {"title": "", "content": "the quantum state. To overcome this, we can estimate those probabilities. Given the quantum state of the game board $\\eta$, we can simulate the state collapsing a number of times (N) and estimate the probabilities $P(c_i = \\_)$, $P(c_i = X)$and $P(c_i = O)$ as the number of appearances of _/X/O on each cell divided by N. The relation between the estimates and real values is: $\\overline{P(c_i = \\_)} = P(c_i = \\_ ) + \\mathcal{N}(0, \\epsilon)$, $\\overline{P(c_i = X)} = P(c_i = X) + \\mathcal{N}(0, \\epsilon)$ and $\\overline{P(c_i = O)} = P(c_i = O) + \\mathcal{N}(0, \\epsilon)$\nSo, these estimations provide a practical means for an Agent to learn and make decisions based on approximated probabilities, offering a computationally feasible approach in the absence of direct access to the precise quantum state.\nMoves History An additional informative resource for an agent's learning process involves maintaining a history of past moves. This data is structured using two matrices, one for X and one for O, each with dimensions of 9 \u00d7 9 (MHX and MH\u00b0). The matrices are defined as follows:\nMHX/O: Represents the number of moves entangling $c_i$ and $c_j$ using X/O, where $i, j \\in \\{1, ..., 9\\}$ and $i \\neq j$.\nMHXO: Indicates the number of classical moves using X/O on $c_i$, where $i \\in \\{1, ..., 9\\}$.\nThis fixed-dimension representation efficiently captures the historical moves in a structured manner, providing valuable information for the agent's learning process."}]}