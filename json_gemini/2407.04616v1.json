{"title": "Isomorphic Pruning for Vision Models", "authors": ["Gongfan Fang", "Xinyin Ma", "Michael Bi Mi", "Xinchao Wang"], "abstract": "Structured pruning reduces the computational overhead of deep neural networks by removing redundant sub-structures. However, assessing the relative importance of different sub-structures remains a significant challenge, particularly in advanced vision models featuring novel mechanisms and architectures like self-attention, depth-wise con- volutions, or residual connections. These heterogeneous substructures usually exhibit diverged parameter scales, weight distributions, and com- putational topology, introducing considerable difficulty to importance comparison. To overcome this, we present Isomorphic Pruning, a sim- ple approach that demonstrates effectiveness across a range of network architectures such as Vision Transformers and CNNs, and delivers com- petitive performance across different model sizes. Isomorphic Pruning originates from an observation that, when evaluated under a pre-defined importance criterion, heterogeneous sub-structures demonstrate signif- icant divergence in their importance distribution, as opposed to iso- morphic structures that present similar importance patterns. This in- spires us to perform isolated ranking and comparison on different types of sub-structures for more reliable pruning. Our empirical results on ImageNet-1K demonstrate that Isomorphic Pruning surpasses several pruning baselines dedicatedly designed for Transformers or CNNs. For instance, we improve the accuracy of DeiT-Tiny from 74.52% to 77.50% by pruning an off-the-shelf DeiT-Base model. And for ConvNext-Tiny, we enhanced performance from 82.06% to 82.18%, while reducing the number of parameters and memory usage. Code is available at https: //github.com/VainF/Isomorphic-Pruning.", "sections": [{"title": "1 Introduction", "content": "Deep Neural Networks are typically over-parameterized, containing numerous re- dundant sub-structures. Such redundancy can be effectively eliminated via struc- tured pruning, without significantly sacrificing the network's performance [16, 22, 25]. Pruning usually follows a \"ranking-to-prune\" framework: To facilitate compression while retaining the learned capabilities of networks, a carefully- designed criterion is utilized to assess the importance score of different struc- tures [11, 22, 40], followed by a ranking process to identify and remove those"}, {"title": "2 Related Works", "content": "Network Pruning. Deep neural networks are typically over-parameterized and can be pruned without significant performance drop [16, 25]. The exploration of network pruning can be dated back to early works [1,17,25] about neural networks. Over the past few years, a large number of works have been con- ducted on convolutional networks to verify the effectiveness of pruning, which can be roughly categorized into unstructured [16, 29, 35] and structured prun- ing [5, 11, 18, 22, 57]. Unstructured pruning compresses a pre-trained model by forcing partial weights to be zero, leading sparse networks [29]. To accelerate"}, {"title": "3 Method", "content": "The core idea of Isomorphic Pruning is straightforward: we group sub-structures in a network by their network topology and perform importance ranking within groups. This imposes two questions 1) how to identify removable sub-structures in networks and 2) how to compare the topology graph for grouping and pruning."}, {"title": "3.1 Sub-structure in Network", "content": "Structured Pruning aims to remove sub-structures in networks, such as the di- mensions of hidden states or features. To make the formulation clear, we pri- marily concentrate on stacked fully-connected networks, although the principles established herein can be readily extended to CNNs and transformers. Initially, we consider a single linear layer which is parameterized by a matrix $W\\in \\mathbb{R}^{m\\times n}$. To articulate the pruning process, we introduce a pruning function $g(W, d, k)$ defined as follows:\n\n$w = g(W, d, k) =\\begin{cases}\ng(W, 0, k) = W[k,:] \\in \\mathbb{R}^{1\\times n}, \\text{ if pruning output dimension,}\\\\\ng(W, 1, k) = W[:,k] \\in \\mathbb{R}^{m\\times 1}, \\text{ if pruning input dimension,}\n\\end{cases}$"}, {"title": "3.2 Graph Modeling of Sub-structures", "content": "In this section, we delve deeper into the methodology for decomposing a network into several independent and minimal sub-structures in pruning, and model them as graphs. We can begin with the MLP network as illustrated in Figure 3, where the activation functions are omitted. The network comprises a sequence of 2- D parameter matrices {$W_1, W_2, ..., W_L$}, with $L$ denoting the network depth. Initially, all parameters are not assigned to any sub-structure. We randomly select one parameter $W$ as the seed to find its associated substructures and build the corresponding graph. For instance, selecting parameter $W_1$ and setting the axis $d = 0$ allows us to establish an initial graph $G = (V = {{(W, 0, k)}}, E = {})$, focusing on the elimination of $k$-th the output dimension in $W_1$. Then we iterate through all potential $(W',d',k') \\notin V$ to ascertain any dependency to $(W, d, k) \\in V$. Dependencies between layers can be ascertained through either manually-crafted patterns or automatic algorithms. In this work, we adhere to the approach of [11] to assess dependency based on two rudimentary rules:\n\nDependency Modeling. For a pair of triplets $(W', d', k')$, $(W, d, k)$, a depen- dency is established if either of the following criteria is met: 1) $W'$ and $W$ are adjacent layers with $d = d'$ and $k = k'$; 2) $W' = W$ and $g(W', d', k') = g(W, d, k)$.\n\nThe first criterion addresses adjacent layers as exemplified in Figure 3, while the second pertains to specialized layers, such as normalization layers, which share the same pruning pattern for both outputs and inputs. Thereafter, we update the vertex set $V$ and edge set $E$ with the discovered dependency between $(W', d', k')$ and $(W, d, k)$ accordingly:\n\n$V = V \\cup {{(W',d',k')}},$\n\n$E = E \\cup {{{{((W, d, k), (W', d', k'))}}}}.$\n\nWe can further repeat the above process until no additional vertex is incorpo- rated into the current graph. The resultant graph $G$ then serves as a topological"}, {"title": "3.3 Ranking with Graph Isomorphism", "content": "With the graph-based representation of network sub-structures, we advance to the ranking phase. This stage involves estimating and ranking the significance of each sub-structure to identify redundancies. A pivotal component of this process is an importance criterion, which quantifies the relative significance of various sub-structures. Previous works in the literature presents numerous well-crafted criteria, including magnitude pruning and Taylor expansion. In this work, we focus on the general importance function $I(\\cdot)$ for a single layer, represented as:\n\n$I(W, d, k) = \\begin{cases}\n||g(W, d, k))||, \\text{ if Magnitude Pruning}\\\\\n|| \\frac{aL}{ag(W,d,k)} (fr) g(W,d,k))||_2, \\text{ if Taylor Pruning}\\\\\n\\text{Other importance Criteria.}\n\\end{cases}$\n\nIn structural pruning, our objective is to remove sub-structures encompassing parameters across multiple layers. Existing approaches [5,11,30] generally extend the single-layer importance to a holistic measure by aggregating scores across all constituents $(W, d, k)$ for the vertexes set, denoted as $I^*(\\cdot)$ as follows:\n\n$I^* (G(V, E)) = \\sum_{(w,d,k)\\in V} I(W, d, k),$\n\nthereby cumulating the importance of all elements within graph $G$. However, the inherent diversity in parameters and computational topology among different"}, {"title": "4 Experiments", "content": "This section reports our empirical results on the ImageNet-1K dataset [8], which comprises 1,281,167 images for training and 50,000 for validation. We deploy the proposed Isomorphic Pruning to compress several vision models, including ConvNext [33], ResNet [36], MobileNetv2 [45], and Vision Transformers [10, 51]. Our approach crafted a series of streamlined models varying in size and Multiply-Accumulate Operations (MACs). We report the parameter amount"}, {"title": "4.1 Settings for ImageNet-1K", "content": "Pruning. For simplicity, we adopt the Taylor-based pruning framework [39], which applies a data-driven Taylor expansion to the loss function. For this pur- pose, we randomly sampled 100 mini-batches from the ImageNet dataset, each containing 64 images. These images were resized to 256x256 pixels and then center-cropped to 224x224 pixels. We normalize all images using the mean and standard deviation specific to ImageNet. Notably, while some methods, such as ConvNext such as ConvNext [33] and DeiT [51] employ additional training techniques like label smoothing and knowledge distillation, our approach only leverages a straightforward cross-entropy loss for the Taylor expansion. We accu- mulate gradients from all the batches for a more reliable importance estimation and apply Isomorphic pruning in a one-shot manner without iterative pruning and fine-tuning [40].\n\nFinetuning. During the fine-tuning process of pruned models, we adhere to the original training protocol for a fair comparison. While maintaining most of the original settings unchanged, we adjusted the learning rates and batch sizes to make the training suitable for A5000 GPUs, following the linear LR scaling rule proposed in [13]. For example, to compressing a pre-trained DeiT models [51], we adopt the same augmentation strategies for training, including RandAugmentation [7], Mixup [69], CutMix [68], Random Erasing [71], Repeated Augumentation [23], label smoothing [50]. The pruned models are distilled with a pre-trained RegNetY [44,51] for 300 epochs using the AdamW [34] optimizer, with a learning rate of 0.0005 and a total batch size of 2048. A weight decay of 0.05 and a cosine annealing scheduler is deployed for training. For other models like ConvNext [33], ResNet [19] and MobileNet-v2 [45], the same principle will be applied. More details of hyper-parameters, pruning, and finetuning can be found in the appendix.\n\nEvaluation. We report the optimal classification accuracy achieved on the ImageNet-1K validation set, employing the standard resize-and-crop protocol [19, 33] for evaluation. To further validate the efficiency of models, we profiled the la- tency and the peak memory consumption of the pruned models on both GPU (a single RTX A5000) and CPU. To guarantee a fair and standardized comparison, we also tested the latency of several baseline models sourced from the Pytorch- Image Models [58] and Torchvision [38], following the same testing procedure. The latency was estimated with a batch size of 256 for GPU and 8 for CPU, averaged on 100 repeats. Appropriate GPU warmup was deployed in our test."}, {"title": "4.2 Pruning Convolutional Neural Networks", "content": "Convolutional Neural Network is a critical focus in the area of network prun- ing. In this section, we concentrate on pruning three popular ConvNets: Con- vNext [33], ResNet [19] and MobileNet-v2 [45].\n\nConvNext. Our experiments in Table 1 begin with the evaluation on an ad- vanced CNN, ConvNext [33], which achieves impressive performance on several benchmarks. We use the official ConvNext-Base [38] as the pre-trained model for pruning, which achieves an accuracy of 83.83% on the ImageNet-1K valida- tion set. The pruning of ConvNext models is a relatively underexplored topic. Hence, we establish our baselines using the official variants with fewer parame- ters, ConvNext-S and ConvNext-T. For ConvNext, we also generalize our method to depth pruning, which removes unimportant layers with the minimal average scores on substructures, so that the pruned models have the same depth as baselines. The pruned models are fine-tuned for 300 epochs following the same training protocol as [33]. During fine-tuning, we observed that the pruned Con- vNext with half of the parameters removed, achieves an accuracy of 80.09 in only"}, {"title": "4.3 Pruning Vision Transformers.", "content": "Vision Transformers (ViT) has profoundly reshaped the paradigm of visual mod- eling over the past few years. Nevertheless, their inherent over-parameterization often imposes huge overheads to both training and inference. Compared to CNNS that are composed of similar convolution layers or blocks, Vision Transformers are usually built with MLP, Multi-head Attention, and skip connections, which are more challenging for pruning due to the heterogeneous internal structures. Table 2 presents our pruning results on DeiT, a plain vision transformer enhanced with knowledge distillation [51]. We benchmark our approach with a range of innovative pruning baselines specifically tailored for vision trans- formers [46, 48, 51, 56, 60, 62, 63, 66, 67, 70, 72]. A DeiT model consists of various components, such as MLP and attention layers. Within the attention layers, we can even find two feasible pruning schemes: pruning 1) the number of heads or 2) within-head dimensions. We implement Isomorphic Pruning to compress a pre-trained DeiT-Base model by pruning both heads, head dimensions, and the"}, {"title": "4.4 Analysis on the Pruned Models", "content": "On-device Latency Table 3 measures the actual latency and peak memory consumptions on a single RTX A5000 GPU and CPU. We evaluate all models with a batch size of 256 for GPU and 8 for CPU, respectively. The average latency over 100-round experiments are reported, with a 20-step warmup process before evaluation. Firstly, results show that the pruned DeiT models can achieve comparable performance (82.41% vs. 82.40%) to some advanced transformers like Efficient Former [26], while preserving a competitive latency. Additionally, we observe that the peak memory consumption is primarily determined by the largest layer width. Therefore, under the same budget, a uniform DeiT may require less peak memory (1363 MB for DeiT-S) for inference. In contrast, a non-uniform architecture may require slightly more memory (1547 MB for the DeiT-S Pruned) under the given budget. But non-unfirom pruning can achieve better"}, {"title": "5 Conclusion", "content": "In this paper, we introduce Isomorphic Pruning, a practical approach designed to compress vision networks with novel mechanisms and architectures. This method focuses on mitigating the challenges posed by heterogeneous sub-structures in networks, thereby enhancing the reliability of ranking and pruning processes. Our empirical results on the ImageNet-1k dataset, showcase the effectiveness of Iso- morphic Pruning across several vision models like ConvNext, ResNet, MobileNet- v2, and Vision Transformers."}, {"title": "A Details of Vision Transformer Pruning", "content": "Vision transformers, in contrast to Convolutional Neural Networks, encompass a more diverse composition of substructures within their network architecture. This section presents a detailed case study on vision transformers, elucidating the isomorphic pruning process. As depicted in Figure 5, a fundamental block of vision transformers, as described in [10], comprises a multi-head attention layer and a Multi-Layer Perceptron (MLP) layer. We annotate the dimensions of intermediate features and demarcate their isomorphic groups using different colors. Owing to their heterogeneous composition, vision transformers naturally form several groups:\n\nThe Embedding Group: This group encompasses parameters responsible for gen- erating intermediate features between modules, of witch the dimension is marked as E. In the ViT-Base model, as specified in [10], the embedding size is typically 768. The presence of residual connections mandates uniformity in embedding sizes across different blocks, necessitating simultaneous pruning. Consequently, the embedding group in a ViT-Base model comprises $E = 768$ substructures.\n\nThe MLP Group: A vision transformer includes several MLP layers, each with an identical structure. This group maps $N \\times E$ embeddings to $N \\times M$ intermediate results before transforming them back into E-dimensional features. Dimension M is pruned within this group to effectively reduce the model size.\n\nHead Dimension Group: Central to the vision transformer is the self-attention module, which aggregates information across tokens. A typical self-attention module maps embeddings to Query, Key, and Value, with the dimension such as $N \\times H \\times Q$ for the Query. The dimensions of Q and K must be identical, while the dimension of V can be set variably. However, many implementations, such as Pytorch-Image-Models [58], require identical QKV dimensions. Therefore, this work prunes Q, K, and V concurrently. Additionally, the input dimension of the subsequent projection layer is adjusted accordingly.\n\nHead Group: In addition to the aforementioned groups for width pruning, the pruning of the attention head is also considered for further acceleration. This involves compressing the H dimension as shown in Figure 5.\n\nThe above analysis of substructures within vision transformers presents 4 unique isomorphic groups, associated with the dimensions E, QKV, H, and M, in which all elements have the same architecture and computational topology. In isomorphic pruning, ranking is applied within each isomorphic group for a reliable comparison. For vision transformers, the above analysis is feasible since all sub-structures are aligned with the modular design. However, for CNNs like ConvNext, ResNet, the substructures can be more complicated. Thus, our imple- mentation automate the identification of substructures with dependency analy- sis [5,11,30] as discussed in the main paper."}, {"title": "B Details of CNN Pruning", "content": "This section elaborates on the pruning strategies applied to ConvNext, ResNet, and MobileNet-v2 networks. Compared to Transformers, Convolutional Neural Networks used in our experiments are more irregular, with intricated internal"}, {"title": "C Experimental Details", "content": "Training. This section further details the training process and hyper-parameters in our experiments. We report the training configurations including optimizer, learning rate, and augmentation in Table 7. All models are fine-tuned with 8 RTX A5000 GPUs, with Automatic Mixed Precision implemented by PyTorch [41]. For DeiT and ConvNext, we use strong augmentations as mentioned in the origi- nal paper [51,59], but did not deploy warmup, layer-wise lr decay and layer scale for training. ResNet and MobileNet-v2 were trained with weak augmentation described in [19,45].\n\nLatency Test For the Latency test on GPU, we forward the model a batch size of 256 for 20-step warmup and 100-step experiments. We report the average execution time of the 100 rounds. For CPU test, we deploy a batch size of 8 and follow the same principle as GPU testing."}, {"title": "D Limitations", "content": "In this study, we empirically examine the impact of isomorphic sub-structures on pruning. Structural similarity might not be the sole determinant of importance distribution. Factors such as the training methodology, regularization techniques, and network depth can also play significant roles in shaping different distribu- tions. While our experiments demonstrate the effectiveness of isomorphic prun- ing, further investigation in the future into these additional factors is necessary for a more comprehensive framework."}]}