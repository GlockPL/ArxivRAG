{"title": "Isomorphic Pruning for Vision Models", "authors": ["Gongfan Fang", "Xinyin Ma", "Michael Bi Mi", "Xinchao Wang"], "abstract": "Structured pruning reduces the computational overhead of deep neural networks by removing redundant sub-structures. However, assessing the relative importance of different sub-structures remains a significant challenge, particularly in advanced vision models featuring novel mechanisms and architectures like self-attention, depth-wise convolutions, or residual connections. These heterogeneous substructures usually exhibit diverged parameter scales, weight distributions, and computational topology, introducing considerable difficulty to importance comparison. To overcome this, we present Isomorphic Pruning, a simple approach that demonstrates effectiveness across a range of network architectures such as Vision Transformers and CNNs, and delivers competitive performance across different model sizes. Isomorphic Pruning originates from an observation that, when evaluated under a pre-defined importance criterion, heterogeneous sub-structures demonstrate significant divergence in their importance distribution, as opposed to isomorphic structures that present similar importance patterns. This inspires us to perform isolated ranking and comparison on different types of sub-structures for more reliable pruning. Our empirical results on ImageNet-1K demonstrate that Isomorphic Pruning surpasses several pruning baselines dedicatedly designed for Transformers or CNNs. For instance, we improve the accuracy of DeiT-Tiny from 74.52% to 77.50% by pruning an off-the-shelf DeiT-Base model. And for ConvNext-Tiny, we enhanced performance from 82.06% to 82.18%, while reducing the number of parameters and memory usage.", "sections": [{"title": "1 Introduction", "content": "Deep Neural Networks are typically over-parameterized, containing numerous re- dundant sub-structures. Such redundancy can be effectively eliminated via struc- tured pruning, without significantly sacrificing the network's performance [16, 22, 25]. Pruning usually follows a \"ranking-to-prune\" framework: To facilitate compression while retaining the learned capabilities of networks, a carefully- designed criterion is utilized to assess the importance score of different struc- tures [11, 22, 40], followed by a ranking process to identify and remove those"}, {"title": "3 Method", "content": "The core idea of Isomorphic Pruning is straightforward: we group sub-structures in a network by their network topology and perform importance ranking within groups. This imposes two questions 1) how to identify removable sub-structures in networks and 2) how to compare the topology graph for grouping and pruning."}, {"title": "3.1 Sub-structure in Network", "content": "Structured Pruning aims to remove sub-structures in networks, such as the di- mensions of hidden states or features. To make the formulation clear, we pri- marily concentrate on stacked fully-connected networks, although the principles established herein can be readily extended to CNNs and transformers. Initially, we consider a single linear layer which is parameterized by a matrix \\(W \\in \\mathbb{R}^{m\\times n}\\). To articulate the pruning process, we introduce a pruning function \\(g(W, d, k)\\) defined as follows:\n\\[ w = g(W, d, k) = \\begin{cases} g(W, 0, k) = W[k,:] \\in \\mathbb{R}^{1\\times n}, & \\text{if pruning output dimension,} \\\\ g(W, 1, k) = W[:,k] \\in \\mathbb{R}^{m\\times 1}, & \\text{if pruning input dimension,} \\end{cases} \\tag{1} \\]"}, {"title": "3.2 Graph Modeling of Sub-structures", "content": "In this section, we delve deeper into the methodology for decomposing a network into several independent and minimal sub-structures in pruning, and model them as graphs. We can begin with the MLP network as illustrated in Figure 3, where the activation functions are omitted. The network comprises a sequence of 2- D parameter matrices \\(\\{W_1, W_2, ..., W_L\\}\\), with L denoting the network depth. Initially, all parameters are not assigned to any sub-structure. We randomly select one parameter W as the seed to find its associated substructures and build the corresponding graph. For instance, selecting parameter \\(W_1\\) and setting the axis d = 0 allows us to establish an initial graph G = (V = \\(\\{(W, 0, k)\\}\\), E = \\(\\{\\}\\)), focusing on the elimination of k-th the output dimension in \\(W_1\\). Then we iterate through all potential (W',d',k') \u2209 V to ascertain any dependency to (W, d, k) \u2208 V. Dependencies between layers can be ascertained through either manually-crafted patterns or automatic algorithms. In this work, we adhere to the approach of [11] to assess dependency based on two rudimentary rules:\nDependency Modeling. For a pair of triplets (W', d', k'), (W, d, k), a depen- dency is established if either of the following criteria is met: 1) W' and W are adjacent layers with d = d' and k = k'; 2) W' = W and g(W', d', k') = g(W, d, k). The first criterion addresses adjacent layers as exemplified in Figure 3, while the second pertains to specialized layers, such as normalization layers, which share the same pruning pattern for both outputs and inputs. Thereafter, we update the vertex set V and edge set E with the discovered dependency between (W', d', k') and (W, d, k) accordingly:\n\\[ V = V \\cup \\{(W',d',k')\\}, \\\\ E = E \\cup \\{((W, d, k), (W', d', k'))\\}. \\tag{2} \\]\nWe can further repeat the above process until no additional vertex is incorpo- rated into the current graph. The resultant graph G then serves as a topological"}, {"title": "3.3 Ranking with Graph Isomorphism", "content": "With the graph-based representation of network sub-structures, we advance to the ranking phase. This stage involves estimating and ranking the significance of each sub-structure to identify redundancies. A pivotal component of this process is an importance criterion, which quantifies the relative significance of various sub-structures. Previous works in the literature presents numerous well-crafted criteria, including magnitude pruning and Taylor expansion. In this work, we focus on the general importance function I(\u00b7) for a single layer, represented as:\n\\[ I(W, d, k) = \\begin{cases} \\frac{||g(W, d, k))||_1}{||W||_1}, & \\text{if Magnitude Pruning} \\\\ || \\frac{\\partial L}{\\partial g(W,d,k)} (fr) g(W,d,k))||_2, & \\text{if Taylor Pruning} \\\\ \\text{Other importance Criteria}. \\end{cases} \\tag{3} \\]\nIn structural pruning, our objective is to remove sub-structures encompassing parameters across multiple layers. Existing approaches [5, 11, 30] generally extend the single-layer importance to a holistic measure by aggregating scores across all constituents (W, d, k) for the vertexes set, denoted as \\(I^*(\\cdot)\\) as follows:\n\\[ I^* (G(V, E)) = \\sum_{(w,d,k)\\in V} I(W, d, k), \\tag{4} \\]\nthereby cumulating the importance of all elements within graph G. However, the inherent diversity in parameters and computational topology among different"}, {"title": "Isomorphic Pruning for Vision Models", "content": "substructures G introduces potential ambiguities in this metric. Experimental evidence, particularly in the context of a vision transformer as shown in Fig- ure 4, elucidates this issue: the largest substructure encompasses all embedding dimensions, containing a substantial number of parameters, whereas the most minimal sub-structure comprises merely two linear layers. Consequently, such aggregation may fail to accurately reflect the true relative importance of these entities. To address the challenges posed by the heterogeneity of substructures, we delineate a simple approach designed to preserve the nuanced information encapsulated by the aggregated importance, as outlined in Equation 4, whilst alleviating the detrimental repercussions raised by the heterogeneity of substruc- tures. Our principal strategy entails the clustering of sub-structures into various isomorphic groups, predicated on their topological logic and parameter configu- rations. In this case, applying aggregated importance within groups can be mean- ingful, since isomorphic sub-structures show one-to-one correspondence among their parameters, and how different parameters participate in computations is entirely identical. To facilitate this, the key step is to examine the isomorphism of two graphs \\(G_1\\) and \\(G_2\\). Formally, we adopt the following definition to ascertain graph isomorphism.\nDefinition 1 (Graph Isomorphism). Two labeled graphs G and G'are iso- morphic if there exists a bijection between their vertices so that, all mapped vertices preserve the same labels, and are connected in the same way.\nTo develop a general indicator Isomorphic\\((g_1, g_2)\\), we need to examine both the label and connectivity in two graphs.\nVertex Label and Connectivity. In our settings, each vertex is labeled by the triplet (W, d, k), encompassing the parameter matrix W, axis d, and index k. We follow a simple principle for labling: Two vertices (W,d,k) and (W', d', k') are labeled identically if and only if the parameter W' originates from the same layer type such as Linear or Conv, with the same pruning dimension d = d'. We permit the index k to differ, since within the same parameter matrix, the weight vectors sliced along the d-th dimension are consistently homogeneous and thus can be considered isomorphic. After determining the label for each vertex, we further explore the connectivity in two graphs \\(G_1\\) and \\(G_2\\). As illustrated in Figure 3, we show three substructures alongside their respective graphs \\(G_1\\), \\(G_2\\), and \\(G_3\\), where \\(G_1\\) and \\(G_2\\) incorporate a singular edge, whereas \\(G_3\\) encompasses three edges. If we check the edges following the execution order of the computational graphs, it's natural to observe the isomorphism of \\(G_1\\) and \\(G_3\\), since they share the same topology. To make this process more formal, we assume the edges in graph G are already sorted according to their execution order in forwarding, and check the isomorphism by\n\\[ \\text{Isomorphic} (G, G')) = 1\\{\\|G| = |G'| \\land \\text{label}(V_i) = \\text{label}(V_i'); \\forall v_i \\in \\{1,2,..., |G'|\\}\\}, \\tag{5} \\]\nwhere 1 is an indicator function. |G'| denotes the number of edges in the graph and \u2227 signifies the logical AND operation. The first condition ensures the equiva-"}, {"title": "Algorithm 1 Isomorphic Pruning", "content": "1: Input: Network with parameters {W1, W2,..., Wn}, Pruning ratio p%.\n2: Output: Pruned Network {W1,W2,..., W}\n3: procedure IDENTIFY SUBSTRUCTURES ({W1, W2,..., Wn})\n4: Initialize empty list of sub-structures S = {}\n5: for each layer Wi in {W1, W2,..., Wn} do\n6: if Wi is not assigned to any substructure then\n7: Initialize a graph G(V = {Wi}, E = {}).\n8: Find and add coupled Wj to G until there is no new dependency.\n9: S = SU {G}\n10: return S\n11: procedure PRUNEBYGROUPEDRANKING(S, p)\n12: for each substructure Gi in {G1, G2, ..., Gn} do\n13: if Gi is not assigned to any group then\n14: Initialize a Group R = {Gi}.\n15: Find and add Gj that satisfies Isomorphism(Gi, Gj) = 1 to R.\n16: Calculate the importance scores \\(\\{I^*(G)|G \\in R\\}\\).\n17: Remove p% substructures in R by ranking the importance scores.\n18: return S\n19: S IDENTIFY SUBSTRUCTURES({W1, W2,..., Wn})\n20: S PRUNEBYGROUPEDRANKING(S, p)"}, {"title": "Isomorphic Pruning for Vision Models", "content": "lence of the number of vertices in the graphs, while the second condition confirms the matching labels of vertices on either side of the edge pairs Vi and V'.\nUsing Equation 5, it is easy to cluster substructures into different groups and perform rankings within each group exclusively. Specifically, we examine a collec- tion of substructures \\(\\{G_1, G_2, ..., G_N \\} that share identical graph topologies. We estimate the aggregated significance of each graph independently using Equa- tion 4, resulting in an N-dimensional importance vector \\([I^*(G_1),...,I^*(G_N)]\\). This facilitates a straightforward ranking of the importance vector, followed by the elimination of the p% unimportant substructures based on their scores. This ranking method is deemed reliable since all substructures within a group possess the same architectural design, parameter scale, and computational graph. A sim- ilar principle can be uniformly applied to other isomorphic groups for pruning, as detailed in Algorithm 1."}, {"title": "4 Experiments", "content": "This section reports our empirical results on the ImageNet-1K dataset [8], which comprises 1,281,167 images for training and 50,000 for validation. We deploy the proposed Isomorphic Pruning to compress several vision models, including ConvNext [33], ResNet [36], MobileNetv2 [45], and Vision Transformers [10, 51]. Our approach crafted a series of streamlined models varying in size and Multiply-Accumulate Operations (MACs). We report the parameter amount"}, {"title": "4.1 Settings for ImageNet-1K", "content": "Pruning. For simplicity, we adopt the Taylor-based pruning framework [39], which applies a data-driven Taylor expansion to the loss function. For this pur- pose, we randomly sampled 100 mini-batches from the ImageNet dataset, each containing 64 images. These images were resized to 256x256 pixels and then center-cropped to 224x224 pixels. We normalize all images using the mean and standard deviation specific to ImageNet. Notably, while some methods, such as ConvNext such as ConvNext [33] and DeiT [51] employ additional training techniques like label smoothing and knowledge distillation, our approach only leverages a straightforward cross-entropy loss for the Taylor expansion. We accu- mulate gradients from all the batches for a more reliable importance estimation and apply Isomorphic pruning in a one-shot manner without iterative pruning and fine-tuning [40].\nFinetuning. During the fine-tuning process of pruned models, we adhere to the original training protocol for a fair comparison. While maintaining most of the original settings unchanged, we adjusted the learning rates and batch sizes to make the training suitable for A5000 GPUs, following the linear LR scaling rule proposed in [13]. For example, to compressing a pre-trained DeiT models [51], we adopt the same augmentation strategies for training, including RandAugmentation [7], Mixup [69], CutMix [68], Random Erasing [71], Repeated Augumentation [23], label smoothing [50]. The pruned models are distilled with a pre-trained RegNetY [44,51] for 300 epochs using the AdamW [34] optimizer, with a learning rate of 0.0005 and a total batch size of 2048. A weight decay of 0.05 and a cosine annealing scheduler is deployed for training. For other models like ConvNext [33], ResNet [19] and MobileNet-v2 [45], the same principle will be applied. More details of hyper-parameters, pruning, and finetuning can be found in the appendix.\nEvaluation. We report the optimal classification accuracy achieved on the ImageNet-1K validation set, employing the standard resize-and-crop protocol [19, 33] for evaluation. To further validate the efficiency of models, we profiled the la- tency and the peak memory consumption of the pruned models on both GPU (a single RTX A5000) and CPU. To guarantee a fair and standardized comparison, we also tested the latency of several baseline models sourced from the Pytorch- Image Models [58] and Torchvision [38], following the same testing procedure. The latency was estimated with a batch size of 256 for GPU and 8 for CPU, averaged on 100 repeats. Appropriate GPU warmup was deployed in our test."}, {"title": "4.2 Pruning Convolutional Neural Networks", "content": "Convolutional Neural Network is a critical focus in the area of network prun- ing. In this section, we concentrate on pruning three popular ConvNets: Con- vNext [33], ResNet [19] and MobileNet-v2 [45].\nConvNext. Our experiments in Table 1 begin with the evaluation on an ad- vanced CNN, ConvNext [33], which achieves impressive performance on several benchmarks. We use the official ConvNext-Base [38] as the pre-trained model for pruning, which achieves an accuracy of 83.83% on the ImageNet-1K valida- tion set. The pruning of ConvNext models is a relatively underexplored topic. Hence, we establish our baselines using the official variants with fewer parame- ters, ConvNext-S and ConvNext-T. For ConvNext, we also generalize our method to depth pruning, which removes unimportant layers with the minimal average scores on substructures, so that the pruned models have the same depth as baselines. The pruned models are fine-tuned for 300 epochs following the same training protocol as [33]. During fine-tuning, we observed that the pruned Con- vNext with half of the parameters removed, achieves an accuracy of 80.09 in only"}, {"title": "Isomorphic Pruning for Vision Models", "content": "20 epochs, which effectively validates the efficiency of pruning. Moreover, as illus- trated in Table 1, the final model after 300 full fine-tuning, achieves competitive performance (82.19% vs. 82.06%) to pre-trained models with fewer parameters (25.32M vs. 28.59M) and MACs (4.19G vs. 4.47G).\nResNet. In line with prior studies [5, 11, 22, 30], we demonstrate the effi- cacy of our method by structurally pruning pre-trained ResNet MACs. ResNet- 50/101/152 consists of four residual blocks, each formed by several Bottle-neck structures and residual connections [19]. The proposed Isomorphic Pruning iden- tifies five kinds of distinct isomorphic groups within ResNet-50, four targeting the block structures and one associated with bottleneck structures [19]. It is worth- while to note that different sub-structures vary in their parameter and MAC counts. For example, residual blocks naturally contain more parameters than the bottleneck structures which are only composed of a few convolutional layers. The conventional global pruning, employing a naive global ranking threshold, faces the potential risks of over-pruning certain layers due to the biased impor- tance distribution. Isomorphic Pruning, however, employs a more stable scheme for pruning. As illustrated in Table 1, since different baseline methods deploy pre-trained models with diverged accuracy, we report both the accuracy and the accuracy drop compared to their base models. Our method utilizes a simple Taylor criterion [39] and obtains a series of lightweight ResNets. Notably, Iso- morphic pruning achieves lossless compression on ResNet-101, under both 4.5G and 3.8G settings.\nMobileNet-v2. We also conduct experiments on lightweight models, MobileNet- v2, by compressing its MACs from 0.32 G to 0.15 G. In MobileNet-v2, we can also discover 5 isomorphic groups in MobileNet. The pruned model achieves better accuracy compared to DepGraph [11] which requires sparse training."}, {"title": "4.3 Pruning Vision Transformers", "content": "Vision Transformers (ViT) has profoundly reshaped the paradigm of visual mod- eling over the past few years. Nevertheless, their inherent over-parameterization often imposes huge overheads to both training and inference. Compared to CNNS that are composed of similar convolution layers or blocks, Vision Transformers are usually built with MLP, Multi-head Attention, and skip connections, which are more challenging for pruning due to the heterogeneous internal structures. Table 2 presents our pruning results on DeiT, a plain vision transformer enhanced with knowledge distillation [51]. We benchmark our approach with a range of innovative pruning baselines specifically tailored for vision trans- formers [46, 48, 51, 56, 60, 62, 63, 66, 67, 70, 72]. A DeiT model consists of various components, such as MLP and attention layers. Within the attention layers, we can even find two feasible pruning schemes: pruning 1) the number of heads or 2) within-head dimensions. We implement Isomorphic Pruning to compress a pre-trained DeiT-Base model by pruning both heads, head dimensions, and the"}, {"title": "Isomorphic Pruning for Vision Models", "content": "embedding sizes following [60]. This is achieved through a straightforward first- order Taylor expansion for importance estimation, and clustering isomorphic sub-structures for ranking.\nWe developed four lightweight vision transformers: DeiT-S, DeiT-2.6G, DeiT- T and DeiT-0.6G. The pruned DeiT-S and DeiT-T models achieve comparable latencies comparable to the standard DeiT-S and DeiT-T with manually designed architecture. The empirical results are reported in Table 2, which shows that our method yields greater performance gains in complex models with extensive heterogeneous internal structures, especially in comparison to CNNs. Specifically, the optimized DeiT-T achieves an accuracy of 77.50%, with only 1.2G MACS, which is better than the results (74.52%) of the pre-trained DeiT-T with uniform width across layers. Detailed configurations, such as base models and pruning ratios, are available in the appendix."}, {"title": "4.4 Analysis on the Pruned Models", "content": "On-device Latency Table 3 measures the actual latency and peak memory consumptions on a single RTX A5000 GPU and CPU. We evaluate all models with a batch size of 256 for GPU and 8 for CPU, respectively. The average latency over 100-round experiments are reported, with a 20-step warmup process before evaluation. Firstly, results show that the pruned DeiT models can achieve comparable performance (82.41% vs. 82.40%) to some advanced transformers like Efficient Former [26], while preserving a competitive latency. Additionally, we observe that the peak memory consumption is primarily determined by the largest layer width. Therefore, under the same budget, a uniform DeiT may require less peak memory (1363 MB for DeiT-S) for inference. In contrast, a non- uniform architecture may require slightly more memory (1547 MB for the DeiT-S Pruned) under the given budget. But non-unfirom pruning can achieve better"}, {"title": "Isomorphic Pruning for Vision Models", "content": "accuracy, which reveals an important trade-off between accuracy and memory consumption for transformers. In addition, for CNNs like ResNets, the pruned model requires much less memory due to the compressed feature dimensions. Furthermore, we note that the actual speed-up may not be linearly correlated with the MAC reductions. This is particularly evident in ResNet-50, where only a 1.33\u00d7 actual speed-up is achieved despite the removal of half of the MACs. In contrast, compression on Mobv2 is more effective, achieving a 1.64\u00d7 acceleration with a similar compression ratio on MACs.\nDistribution of Importance Scores. To further explore how Isomorphic Pruning affects the pruning behavior, we study the distribution of importance score in a DeiT-Base model [51]. As shown in Figure 4 (c), the importance histogram of different sub-structures is highlighted with different colors, each corresponding to one kind of isomorphic structure. It is natural to find that, the MLP layer and the attention layer have diverged importance distributions due to their different computation process. Besides, when we apply a simple 50% global pruning on the DeiT model, most parameters in Attentions #5, #9, and #13 will be removed, since the importance scores in attentions are centered around 0. In contrast, the embedding number as #1 will not be even been pruned in this case, due to the relatively large importance scores. This inevitably leads to an imbalanced pruning of different structures. Notably, the embedding structure,"}, {"title": "5 Conclusion", "content": "In this paper, we introduce Isomorphic Pruning, a practical approach designed to compress vision networks with novel mechanisms and architectures. This method focuses on mitigating the challenges posed by heterogeneous sub-structures in networks, thereby enhancing the reliability of ranking and pruning processes. Our empirical results on the ImageNet-1k dataset, showcase the effectiveness of Iso- morphic Pruning across several vision models like ConvNext, ResNet, MobileNet- v2, and Vision Transformers."}, {"title": "C Experimental Details", "content": "Training. This section further details the training process and hyper-parameters in our experiments. We report the training configurations including optimizer, learning rate, and augmentation in Table 7. All models are fine-tuned with 8 RTX A5000 GPUs, with Automatic Mixed Precision implemented by PyTorch [41]. For DeiT and ConvNext, we use strong augmentations as mentioned in the origi- nal paper [51,59], but did not deploy warmup, layer-wise lr decay and layer scale for training. ResNet and MobileNet-v2 were trained with weak augmentation described in [19,45].\nLatency Test For the Latency test on GPU, we forward the model a batch size of 256 for 20-step warmup and 100-step experiments. We report the average execution time of the 100 rounds. For CPU test, we deploy a batch size of 8 and follow the same principle as GPU testing."}, {"title": "D Limitations", "content": "In this study, we empirically examine the impact of isomorphic sub-structures on pruning. Structural similarity might not be the sole determinant of importance distribution. Factors such as the training methodology, regularization techniques, and network depth can also play significant roles in shaping different distribu- tions. While our experiments demonstrate the effectiveness of isomorphic prun- ing, further investigation in the future into these additional factors is necessary for a more comprehensive framework."}]}