{"title": "Enhancing Reasoning Capabilities of LLMS via Principled Synthetic Logic Corpus", "authors": ["Terufumi Morishita", "Gaku Morio", "Atsuki Yamaguchi", "Yasuhiro Sogawa"], "abstract": "Large language models (LLMs) are capable of solving a wide range of tasks, yet\nthey have struggled with reasoning. To address this, we propose Additional Logic\nTraining (ALT), which aims to enhance LLMs' reasoning capabilities by program-\ngenerated logical reasoning samples. We first establish principles for designing\nhigh-quality samples by integrating symbolic logic theory and previous empirical\ninsights. Then, based on these principles, we construct a synthetic corpus named\nFormal Logic Deduction Diverse (FLD\u00d72), comprising numerous samples of\nmulti-step deduction with unknown facts, diverse reasoning rules, diverse linguistic\nexpressions, and challenging distractors. Finally, we empirically show that ALT on\nFLD\u00d72 substantially enhances the reasoning capabilities of state-of-the-art LLMs,\nincluding LLaMA-3.1-70B. Improvements include gains of up to 30 points on\nlogical reasoning benchmarks, up to 10 points on math and coding benchmarks,\nand 5 points on the benchmark suite BBH.", "sections": [{"title": "1 Introduction", "content": "Knowledge and reasoning have long been considered essential elements for achieving artificial\nintelligence (McCarthy, 1959; Weizenbaum, 1966; Winograd, 1971; Colmerauer and Roussel, 1973;\nShortliffe, 1976; Elkan and Greiner, 1993). Knowledge refers to facts about the world, e.g., \"objects\nwith mass generate a gravitational field\u201d and \u201cthe Earth has mass.\u201d Reasoning involves combining\nmultiple facts according to specific rules to obtain new knowledge. For example, the new knowledge\nthat \"the Earth generates a gravitational field\" can be derived from the aforementioned two facts.\nRecent observations suggest that LLMs can solve problems using memorized knowledge of similar\nsamples seen during pre-training, but they cannot solve novel, unknown problems that require\nreasoning (Hodel and West, 2023; Dasgupta et al., 2023; Zhang et al., 2024). For instance, LLMs can\nsolve famous arithmetic problems as is but not when the numbers or names are changed (Razeghi\net al., 2022; Mirzadeh et al., 2024), and they can solve coding tests from past years before the\n\"knowledge cutoff\u201d but not from the present year (Mitchell, 2023). This bias towards knowledge has\nbeen observed even in state-of-the-art LLMs such as GPT-4 (Liu et al., 2023b; Wu et al., 2023; Dziri\net al., 2023).\nLLMs' poor reasoning capabilities can stem from the lack of high-quality reasoning samples in the\npre-training corpus, which primarily consists of human-written texts (Betz et al., 2021; Morishita\net al., 2023). Indeed, reasoning samples in human-written texts often exhibit low quality, as evidenced\nby fallacies and biases commonly found in online debates (Hansson, 2004; Guia\u0219u and Tindale,\n2018; Cheng et al., 2017). This is unsurprising given that humans usually think reflexively rather\nthan through rigid reasoning (Kahneman, 2011; Sunstein and Hastie, 2015; Paglieri, 2017). Thus, a"}, {"title": "2 How Should Synthetic Logic Samples Be Designed?", "content": "In synthetic generation, computer programs generate samples according to pre-designed patterns,\nso this design largely determines the quality of the samples. While Previous studies have examined\nseveral designs (Clark et al., 2021; Betz et al., 2021; Tafjord et al., 2021; Morishita et al., 2023), these\ndesigns were not systematically discussed, so they may not be the most effective ones.\nThus, we start by discussing how to optimally design synthetic logic samples. To this end, we consider\nsymbolic logic theory as suggested by Morishita et al. (2023) and integrate empirical findings from\nprevious studies. First, we observe that the essence of logical reasoning, based solely on the logical\nrelationships between facts, lies in its ability to handle unknown facts, unlike knowledge, which by\ndefinition deals solely with established facts (Section 2.1). Therefore, we argue that samples should\ncover reasoning with unknown facts to represent this essential aspect of logical reasoning. We also\nobserve that logical reasoning involves various other aspects, such as illogical reasoning, reasoning\nrules, and linguistic expressions that represent logical statements (sections 2.2 to 2.4). The samples\nshould cover various patterns regarding these aspects to enable LLMs to solve various reasoning\nproblems. We summarize these discussions into the following design principles, which guide the\ndesign of synthetic logic samples."}, {"title": "2.1 Teaching Reasoning with Unknown Facts", "content": "We first explore the essence of logical reasoning that differentiates itself from knowledge. Consider\nthe following logical step:\nThe Earth orbits the Sun.\nIf the Earth orbits the sun, the Earth has four seasons.\n(1)\nThe Earth has four seasons.\nThis step is valid because the conclusion is logically derived from the two premises. Next, consider\nanother logical step:\nThe Earth orbits the Sun.\nIf the Earth orbits the sun, the Earth does not have four seasons.\n(2)\nThe Earth does not have four seasons.\nThe second premise and consequently, the conclusion, is factually wrong. Nevertheless, if the premise\nwas hypothetically correct, the conclusion could be logically derived. Therefore, step (2) is also\nlogically valid. Finally:\n1. A Foo star exists.\n2. If a Foo star exists, a Bar star also exists.\n(3)\nA Bar star exists.\n\"Foo star\" and \"Bar star\" are unknowns; nonetheless, we can still determine that step (3) is logically\nvalid. Steps (1) to (3) above can be abstracted into a deduction rule, i.e., modus ponens, using\nsymbols:\n$F\\rightarrow G\\over G$ modus ponens\n(4)\nAs we have seen, the logical validity of a deduction rule depends solely on whether the conclusion\nis logically derived from the premises, not on the factual correctness of the contents of F and G.\nTherefore, the contents of F and G can be arbitrary.\nNow, we consider what kind of samples would be needed to teach the deduction rule (4) to LLMs.\nWe assume a task to generate the conclusion given the premises as prompt inputs. If the learner were\nhuman, they would be able to infer the underlying deduction rule (4) by observing samples such as\n(1) to (2). As a result, they would become able to solve the unknown problem (3).\nHowever, from a purely inductive perspective, samples (1) to (2) cannot simply be generalized to the\ndeduction rule (4). This is because the samples (1) to (2) themselves do not contain the information\nthat the contents of F and G are arbitrary. In fact, one could generalize samples (1) to (2) to other\nrules; for example, the conclusion G can be derived if F and $F \\rightarrow G$ are given as premises and F\nand G include 'Earth' as their contents. Innumerable such deduction rules can be inductively inferred\nfrom the given samples. In other words, induction has arbitrariness (Hume, 1748; Goodman, 1954;\nQuine, 1969)."}, {"title": "2.2 Teaching Illogical Reasoning", "content": "Suppose we have LLMs trained on a large number of samples as follows:\n$F\\land G\\over H$ \n$(F\\land G)\\rightarrow H$\n(5)\nwhere $\\land$ denotes logical conjunction, and arbitrary contents are assigned to F, G, H. Suppose that\nwe give this LLM a problem such as:\n$(F\\land G)\\rightarrow H$\n$F\\over ??$\n(6)\nSince the premises are insufficient for logically deducting the conclusion, outputting nothing is the\ncorrect answer."}, {"title": "2.3 Teaching Diverse Reasoning Rules", "content": "Deduction rules other than (4) exist:\n$(F\\land G)\\over F$ \n$(F\\land G)\\over G$ \n$F\\rightarrow G$\n$\\lnot G\\rightarrow \\lnot F$\n$\\frac{(F\\rightarrow G)\\land (G \\rightarrow H)}{F\\rightarrow H}$ syllogism\n$\\frac{\\lnot (F \\lor G)}{\\lnot F \\land \\lnot G}$ De Morgan's laws\nAelimination\n(7)\n$\\frac{\\lnot (F \\land G)}{\\lnot F \\lor \\lnot G}$\nwhere $\\lor$ denotes logical disjunction and $\\lnot$ negation. Since there are infinitely many possible logical\nformulas that can appear as premises and conclusions, there are infinitely many deduction rules.\nProviding LLMs with these infinite deduction rules is obviously intractable.\nInstead of directly providing these infinite deduction rules, we can take another approach. Consider\nmulti-step deductive reasoning (Figure 2 left), where multiple deduction rules derive a conclusion.\nNotice that the syllogism in (7) can be expressed by multi-step deductive reasoning using more\n\"atomic\" deduction rules. Indeed, there exists a set of atomic deduction rules called the axioms that\nsatisfies the following:"}, {"title": "2.4 Teaching Diverse Linguistic Expressions that Represent Logical Statements", "content": "There are various linguistic structures for expressing the logical relationship F \u2192 G, such as \u201cIf F\nthen G\u201d, \u201cF leads to G\u201d, and \u201cF results in G\u201d. If we only include specific expressions in the corpora,\nLLMs may only learn to react to these specific expressions, which has been observed in previous\nexperiments (Zhang et al., 2022; Yuan et al., 2023). To prevent this,\nDesign Principle 4 (Diverse Linguistic Expressions). Samples should include diverse linguistic\nexpressions that represent logical statements.\nIn this chapter, we have established the principles to guide the design of synthetic logic samples.\nNext, we construct a synthetic logic corpus based on these principles."}, {"title": "3 Creating a Synthetic Corpus based on Design Principles", "content": "To prepare diverse samples reflecting the design principles 1 to 4 (DP1-4), we built a novel sample\ngenerator by extending the previous one by Morishita et al. (2023) and then generated the synthetic\nlogic corpus named FLD\u00d72 (Formal Logic Deduction Diverse). Figure 2 shows a schematic of our\ngenerator and a deduction sample. Table 1 compares FLD\u00d72 with existing corpora. Figure D.3\nprovides an actual deduction sample included in FLD\u00d72.\nMore specifically, our generator generates deduction samples through the following steps. First, the\ngenerator randomly generates a sample of multi-step deductive reasoning written in logical formulas,\nas shown on the left side of Figure 2, where a conclusion is derived from premises using multiple\ndeduction rules (See Appendix D.3 for more details of this generation procedure). At this time, the\ngenerator also generates 'distractor' logical formulas, which express negative premises of DP2. Next,\nthe generator converts each logical formula into English expressions. To achieve this, the generator\nfirst randomly selects a template from pre-defined options, such as \"If F, then G,\u201d \u201cF leads to G,\"\nor \"F results in G,\" for the logical formula \u201cF \u2192 G.\u201d It then assigns English content randomly\nconstructed from a vocabulary, such as \u201c(that) a Foo star exists\" and \"(that) a Bar star exists", "to\neach symbol, such as F and G. Finally, it converts the multi-step deduction into a deduction sample\n(right side of Figure 2) by using the premises as 'facts', the conclusion as 'hypothesis', and the\nintermediate logical steps as 'logical steps'. The deduction sample requires LLMs to generate logical\nsteps that derive a given hypothesis based on the given facts.\nTable 1 outlines the comparison of FLD\u00d72 with other existing corpora (Clark et al., 2021; Bao et al.,\n2022; Morishita et al., 2023) in terms of DP1-4, which is detailed as follows": "n\u2022 DP1: We assign F and G content randomly constructed from a vocabulary. While the existing\ncorpora used small-sized vocabulary of up to 15k, we use a large vocabulary of around 100k words\nbuilt from WordNet (Miller, 1995). This will teach LLMs that F and G are truly arbitrary, ultimately\nenabling them to reason with unknown facts.\n\u2022 DP2: The existing corpora used randomly generated logical formulas as distractors. In contrast,\nwe implement adversarial distractors. For example, for a premise FAG, we use F with missing\ninformation (see Equations (5), (6)), and for a premise F \u2192 H, we use F / G \u2192 H with missing\ninformation as distractors. These distractors teach LLMs precisely when a conclusion can and\ncannot be derived. As with previous corpora, we include a variable number of distractors in each\nsample, randomly chosen from a range of 0 to 20.\n\u2022 DP3-3", "DP4": "We manually craft several more English templates per logical formulas than those used\nin FLD. Since the templates have a nested structure, they yield combinatorially more diverse\nEnglish expressions. While counting the exact number of the resulting expressions is intractable,\nwe observed at least dozens of expressions per logical formula, including minor variations. See\nAppendix D.4 for details."}, {"title": "4 Experimental Setup", "content": "We briefly explain the experimental settings. Refer to Appendix E for the details.\nSynthetic Logic Corpora: We examine the proposed FLD\u00d72 and previous corpora (Table 1).\nLLMs: We used the state-of-the-art LLM, LLaMA-3.1 (8B and 70B) (AI@Meta, 2024).\nTraining Settings: We trained the LLMs by a method similar to supervised fine-tuning; as illustrated\nin Figure 2, we used the facts and hypothesis as inputs and logical steps and additional answer label\n(see Appendix D.1) as outputs. We excluded loss computation for the inputs to prevent LLMs from\nlearning to generate unknown facts. We trained the LLMs for 1 epoch on 100k samples (~ 0.1B\ntokens) from the training split of each corpus, with a batch size of 256, resulting in 390 steps, with a\nlinear warmup for 200 steps. We used the learning rate of 2e-05 for the 8B model and 3e-06 for the\n70B model. We used Huggingface (Wolf et al., 2020) for implementation.\nPrevention of Knowledge Forgetting by Recall Adam Optimizer: Synthetic logic corpora include\nmany samples with unknown facts, so training on them should cause LLMs to forget their knowledge\nof existing facts. To prevent this, we employed the Recall Adam optimizer (Chen et al., 2020),\nwhich regularizes parameter updates to avoid deviating too far from the pre-training parameters.\nRecall Adam stands out for LLM training for several reasons (see Appendix E.0.1 for details).\nWe used our re-implemented version 2. The hyperparameters were: \u03b2\u2081 = 0.9, \u03b22 = 0.999, \u20ac =\n10-6, fisher coefficient = 4000 for the 8B model and 2000 for the 70B model.\nBenchmarks: We evaluated the trained LLMs on 31 benchmarks shown in Table E.7 using 5-shot\nin-context learning, except for BBH and AbuductionRules, which used 3-shot in-context learning.\nThese benchmarks cover a wide range of tasks and are prominent in LLM evaluation. Note that we\nexcluded the synthetic logic corpora used for training, as training on them often leads to overfitting\nto their superficial and statistical cues (Zhang et al., 2022; Yuan et al., 2023), failing to measure\ntruly generalizable reasoning capabilities. We used Im-evaluation-harness (Gao et al., 2023) and\nbigcode-evaluation-harness (Ben Allal et al., 2022) for the implementation."}, {"title": "5 Can Additional Logic Training Enhance LLMs' Capabilities?", "content": "Table 2 show the performance of LLMs before and after ALT. Most LLMs trained with ALT\noutperformed their counterparts without ALT. Notably, ALT yielded substantial gains of up to 10\npoints even for LLaMA-3.1-70B, the largest LLM pre-trained on over 15 trillion tokens. These results\nverify that ALT can enhance the capabilities of state-of-the-art LLMs.\nAmong the LLMs trained with ALT, the one trained on FLD\u00d72 (i.e., ALT-FLD\u00d72) achieved the\nhighest generalization performance across the benchmarks. Table 3 shows the performance of the\nLLMs trained on ablated FLD\u00d72 corpora, each of which lacks one of the design principles. As\nseen, ablating any design principle almost always led to performance degradation. These results\ndemonstrate that the proposed design principles are critical to obtaining the maximum possible gain\nfrom ALT, and each principle is indispensable.\nTable F.8 shows that the LLMs trained with ALT without preventing knowledge forgetting by Recall\nAdam optimizer underperformed compared to their counterparts trained with knowledge forgetting\nprevention and even the LLM without ALT. This behavior presumably occurred because the unknown\nfacts included in synthetic logic corpora displaced the LLM's knowledge of existing facts. Therefore,\nknowledge-forgetting prevention is critically important for the success of ALT."}, {"title": "6 What Capabilities Can Additional Logic Training Enhance and Why?", "content": "We analyze the results on each benchmark or each case and discuss whether and why the LLM's\ncapabilities to solve the tasks can or cannot be enhanced by ALT."}, {"title": "6.1 Logical Reasoning Tasks", "content": "Table 4a shows that ALT substantially boosted LLaMA-3.1-70B's performance by up to 30 points\non various benchmarks dealing with logical reasoning tasks. Surprisingly, we also observed im-\nprovements on abductive reasoning tasks, which go beyond the original deductive reasoning tasks"}, {"title": "6.2 Math and Coding Tasks", "content": "Tables 4b, 4c shows that ALT substantially boosted the LLaMA-3.1-70B's performance by up to 7\nand 10 points on math and coding tasks, respectively. The math improvements are reasonable, as\nunderstanding predicate logic is a prerequisite for solving mathematical problems. For coding, some\nrecent studies have verified the opposite direction, namely, that training on coding data improves\nlogical reasoning abilities (Jiang et al., 2024b; MA et al., 2024; Uchiyama et al., 2024)."}, {"title": "6.3 NLI Tasks", "content": "Table 4d shows that ALT substantially boosted the LLaMA-3.1-70B's performance by up to 6 points\non various natural language inference (NLI) benchmarks. NLI is similar to deductive reasoning in\nassessing whether a premise supports or contradicts a hypothesis. However, the main difference is\nthat this judgment requires a rich set of commonsense knowledge beyond the given premise.\nConsider the fifth problem in Table 5: by supplementing the given fact \u201cAn Indian woman is dancing\nwith her partner\u201d with the commonsense knowledge \"If someone is dancing, then he/she is moving.", "A woman is moving.": "he sixth problem is more challenging as we\nhave to trace multiple logical steps while supplementing with sufficient commonsense knowledge as\nfollows:", "baseball is often played at a baseball field,\" \"a person\ncannot be in two or more places at the same time,": "therefore, a church choir cannot sing for baseball."}, {"title": "6.4 Other Tasks", "content": "Improvements across various other tasks (Table 4e) demonstrate the broad benefits of the obtained\nreasoning capabilities beyond standard reasoning tasks; though the improvements were modest at up\nto 2 percentage points, which may be due to the following reasons. First, these benchmarks include\nproblems that purely test knowledge, such as the first one in Table 6. Since ALT does not aim to\nprovide new knowledge, the ability to solve such problems does not improve by nature. Next, some\nproblems may require knowledge that is too advanced for LLMs, so potential improvements by the\nenhanced reasoning capabilities may be bottlenecked. For example, the second problem does involve\nreasoning but requires sufficient quantum mechanics knowledge as a prerequisite. However, these\nknowledge-related issues should be solved by improving the quantity and quality of pre-training.\nFinally, LLMs may not be able to fully utilize the potential of enhanced reasoning capabilities for\nproblems that require complex procedures. To solve the third problem, LLMs first must attempt\nreasoning related to each choice as follows: \u201cTo build homes in an aquatic environment, one needs to\nmaintain body heat and insulation despite being frequently submerged in cold water. Therefore, the\nwaterproof fur of (A) is essential\u201d, and \u201cTo build , one must gather and process natural materials\nlike wood. Large, sharp teeth of (C) are critical as they allow beavers to cut down trees and shape\nbranches.\" Next, while reasoning traces on (A) to (D) all seem reasonable, LLMs must choose the\nsingle best answer, considering the subtle nuance of the question context, as follows: \u201cSince the\nquestion emphasizes the aquatic environment, the least related reasoning trace should be (C).\u201d This\ncomplex procedure contrasts with logical reasoning and NLI problems, where LLMs can directly\nobtain an answer from a single reasoning trace. Previous studies also observed that such procedure on\nmultiple-choice QA problems are challenging for LLMs (Robinson and Wingate, 2023; Zheng et al.,\n2024; Wang et al., 2024a). Since ALT alone does not teach LLMs such task-specific procedures,\nadditional training on these procedures should be necessary to solve these problems.\""}, {"title": "7 Conclusion", "content": "Towards versatile artificial intelligence with reasoning capabilities, we proposed Additional Logic\nTraining on synthetic logic samples. We established systematic design principles well-grounded on\nsymbolic logic theory and previous empirical findings. We constructed a corpus named Formal Logic\nDeduction Diverse (FLD\u00d72) based on the design principles. We empirically showed that ALT on\nFLD\u00d72 substantially enhances the capabilities of state-of-the-art LLMs."}, {"title": "A Related Work", "content": "A.1 Investigation of Reasoning Capabilities of LLMs\nMany studies examine LLMs' reasoning capabilities (Askell, 2020; Rae et al., 2021; Razeghi et al.,\n2022; Liu et al., 2023b; Turpin et al., 2023; Lanham et al., 2023; Wu et al., 2023; Hodel and West,\n2023; Dziri et al., 2023; Dasgupta et al., 2023). Patel et al. (2024) observed LLMs' performance\nsignificantly declines as reasoning steps increase in multi-step logical reasoning tasks. Dougrez-\nLewis et al. (2024) revealed ChatGPT struggles with abductive reasoning when verifying claims by\ndecomposing their evidence into atomic reasoning steps. Wang et al. (2024b) found that GPT-series\nmodels showed significant gaps compared to humans in dealing with inference rules. Parmar et al.\n(2024) introduced LogicBench and showed that existing LLMs struggle with instances involving\ncomplex reasoning and negations. Wan et al. (2024) introduced LogicAsker, which assesses whether\nLLMs can employ a set of atomic reasoning skills grounded in propositional and predicate logic and\nfound significant gaps in LLMs' learning of logical rules. Bhuiya et al. (2024) proposed a challenging\nmulti-hop reasoning benchmark with seemingly plausible but incorrect multi-hop reasoning chains\nand found that state-of-the-art LLMs' capabilities to perform multi-hop reasoning is affected by such\nchains. Mondorf and Plank (2024) introduced TruthQuest, which assesses LLMs' capabilities to\nconduct suppositional reasoning, i.e., reasoning where each statement can be false, and found that\nLLMs exhibit significant difficulties solving these tasks. Sprague et al. (2024) introduced a complex\nmulti-step reasoning benchmark, MuSR, and characterized the gaps that remain for techniques like\nchain-of-thought to perform robust reasoning.\nBiases and Errors Ando et al. (2023); Ozeki et al. (2024); Bertolazzi et al. (2024); Eisape et al.\n(2024) found that LLMs exhibit human-like reasoning biases in syllogistic arguments. Jiang et al.\n(2024a) found that LLMs exibit \u201ctoken-biases\" in solving logical reasoning problems. Aoki et al.\n(2024) revealed that LMs rely heavily on heuristics, such as lexical overlap, in the earlier stages of\nreasoning. Zhao et al. (2024a) constructed a MATHTRAP with carefully designed logical traps into\nthe problem descriptions of MATH and GSM8k and found that while LLMs possess the knowledge\nrequired to solve these traps, they do not spontaneously use such knowledge them to handle the\nproblems. Han et al. (2024) found that LLMs exhibit A-Not-B errors similar to human infants, failing\nto suppress the previously established response pattern during ICL. Liu et al. (2024) found that LLMs\noften contradict themselves in reasoning tasks involving contextual information understanding or\ncommonsense. Zhou et al. (2024b) found that subtle alterations in the surface form can significantly\nimpact the answer distribution, suggesting that LLMs solve reasoning problems using surface cues.\nChen et al. (2024) found that the reasoning performance of LLMs is affected by the order of the\npremises. Hong et al. (2024); Huang et al. (2024) found that LLMs struggle to identify fallacious\nreasoning steps accurately, suggesting challenges in self-verification methods.\nReasoning in Unknown Situation Zhao et al. (2024b) found that LLMs struggle with reasoning in\nuncommon situations. Zhu et al. (2024) introduced a framework to dynamically generate reasoning\nsamples, and LLMs perform worse in those samples. Hu et al. (2024) found that while LLMs can\nconduct reasoning when relevant knowledge is given in context, they are not proficient at reasoning\nwith knowledge embedded in the training data.\""}, {"title": "A.2 Synthetic Logic Corpus for Training LLMs", "content": "Later studies (Saha et al., 2020; Dalvi et al., 2021; Tafjord et al., 2021; Sanyal et al., 2022b) showed\nthat T5 can generate even the intermediate logical steps as well as the final answer.\nPARARULE-Plus (Bao et al., 2022) is the enhanced version of PARARULE (Clark et al., 2021), a\nvariation of RuleTaker, that includes more samples and more logical steps. RoBERTa (Liu et al.,\n2019) trained on PARARULE-Plus outperformed the models trained on RuleTaker.\nArtificial Argument Corpus (Betz et al., 2021) includes single-step deductive reasoning samples\nconstructed from hand-selected deduction rules useful for critical thinking. They showed that the\nGPT-2 (Radford et al., 2019) trained on this corpus can generalize to solve NLI tasks. However, at\nthe same time, they found that the LM does not generalize well to solve more challenging reasoning\ntasks such as ARC (Habernal et al., 2018) and LogiQA (Liu et al., 2020)."}, {"title": "A.3 Distilling Reasoning Traces from Very Large LLMS", "content": "Recent approaches (Ho et al., 2023; Magister et al., 2023; Li et al., 2022, 2023; Shridhar et al.,\n2023; Wang et al., 2023; Mitra et al., 2023; Liu et al., 2023c; Ben Allal et al., 2024; Lu et al., 2024)\nutilize very large LLMs, such as GPT-4, to prepare synthetic reasoning datasets to train smaller\nLLMs. A typical procedure is as follows: (i) prepare existing reasoning problems, (ii) prompt large\nLLMs to generate reasoning traces to solve these problems using techniques such as chain-of-thought\nprompting (Wei et al., 2022), and (iii) train smaller LLMs on these reasoning traces.\nThe distillation approach and the synthetic logic corpora approach examined in this paper have\nspecific advantages and disadvantages, as follows.\nThe advantage of the distillation approach is its immediate practical effect, as it directly teaches\nLLMs solutions to various existing problems. The disadvantages could be that (i) it is non-trivial for\nspecific solutions to specific problems to generalize to other problems, (ii) the number of training\nsamples is limited to existing problems in nature, (iii) the correctness and faithfulness of the reasoning\ntraces are not guaranteed; indeed, some studies (Turpin et al., 2023; Lanham et al., 2023) suggest that\nlarge LLMs do not always faithfully follow the \u201creasoning traces\u201d they themselves generate, and (iv)\nit cannot enhance the very large LLMs themselves by nature.\nThe advantages of synthetic logic corpus approaches are that (i) since they teach the fundamentals of\nreasoning, such as deductive reasoning, they have the potential to generalize to various problems,\n(ii) they can generate an unlimited number of new samples, and (iii) the correctness of the reasoning\ntraces is guaranteed by nature. The disadvantage of this approach is that, as it only teaches the basics\nof reasoning, additional training may be needed to solve more complex real-world problems, as\nsuggested in Section 6.4."}, {"title": "B Limitations", "content": "\u2022 We only used deductive reasoning samples for ALT. Future work should examine other\nreasoning samples, e.g., abductive and inductive reasoning.\n\u2022 We only examined the first-order predicate logic system. Future work should examine other\nlogic systems, such as modal and linear logic."}, {"title": "C Ethics and Social Impacts", "content": "The ultimate goal of the direction of this study is to develop an AI capable of reasoning logically\nstep by step. If AI can make a decision one logical step at a time, it would be highly explainable and\ntransparent to users. Furthermore, the user would be able to trace the AI's errors. We believe that our\nstudy is a step towards such AI that will positively impact society."}, {"title": "D Details of Formal Logic Deduction Diverse", "content": "Figure D.3 shows a real sample from FLD\u00d72. Below, We briefly explain our sample generator. Please\nrefer to Morishita et al. (2023) for the details."}, {"title": "D.1 Answer Labels", "content": "In addition to the logical steps, the samples of FLD\u00d72 and previous corpora include answer labels\n(Figure D.3): \"proved\u201d indicating that the hypothesis can be proved by the logical steps, \"disproved\"\nindicating that the hypothesis can be disproved, and \"unknown\" indicating that the given facts are\ninsufficient for either proving or disproving the hypothesis. For samples with \"unknown\" labels, the\nlogical steps are \u201cNone.\u201d. FLD\u00d72 have a uniform distribution over the labels."}, {"title": "D.2 Splits", "content": "FLD\u00d72 includes 100k/5k/5k samples for train/valid/test splits."}, {"title": "D.3 Generation of Multistep Deduction", "content": "Our sample generator first randomly generates examples of multistep deduction by forward- and\nbackward random deduction, using the deduction rules specified by a user.\nThe forward random deduction is done as follows. The generator first chooses a deduction rule\nrandomly and forms the initial tree where the root node is the conclusion of the chosen deduction\nrules and the child nodes are the premises of the chosen deduction rule. The generator next randomly\nchooses another deduction rule that can be \"jointed\" to the root note of the tree. A deduction rule can\nbe jointed to the root node of a tree if one of the premises of that deduction rule can be identified\nwith the root node. Then, the generator updates the tree by jointing this chosen deduction rule. The\ngenerator continues this step multiple times until the tree achieves the required depth.\nThe backward random deduction is done as follows. For each step, the generator randomly chooses a\nleaf node of the tree. Then, the generator randomly chooses a deduction rule that can be jointed to\nthe leaf node. Here, a deduction rule can be jointed to the leaf node if the deduction rule's conclusion\ncan be identified with the leaf node. Then, the generator updates the tree by jointing this chosen\ndeduction rule. The generator continues this step multiple times until the complexity of branches\nachieves the required level."}, {"title": "D.4 Linguistic Expressions", "content": "We prepared linguistic templates for each logical formula", "follows": "n((AB) \u2192 C): If ((AB).predicate_phrase), then (C"}]}