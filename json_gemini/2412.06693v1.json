{"title": "OmniEvalKit: A Modular, Lightweight Toolbox for Evaluating Large Language Model and its Omni-Extensions", "authors": ["Yi-Kai Zhang", "Xu-Xiang Zhong", "Shiyin Lu", "Qing-Guo Chen", "De-Chuan Zhan", "Han-Jia Ye"], "abstract": "The rapid advancements in Large Language Models (LLMs) have significantly expanded their applications, ranging from multilingual support to domain-specific tasks and multimodal integration. In this paper, we present OMNIEVALKIT, a novel benchmarking toolbox designed to evaluate LLMs and their omni-extensions across multilingual, multidomain, and multimodal capabilities. Unlike existing benchmarks that often focus on a single aspect, OMNIEVALKIT provides a modular, lightweight, and automated evaluation system. It is structured with a modular architecture comprising a Static Builder and Dynamic Data Flow, promoting the seamless integration of new models and datasets. OMNIEVALKIT supports over 100 LLMs and 50 evaluation datasets, covering comprehensive evaluations across thousands of model-dataset combinations. OMNIEVALKIT is dedicated to creating an ultra-lightweight and fast-deployable evaluation framework, making downstream applications more convenient and versatile for the AI community.", "sections": [{"title": "1. Introduction", "content": "The rapid development of Large Language Models (LLMs) (Du et al., 2022; Jiang et al., 2023; OpenAI, 2022; Touvron et al., 2023a; Bai et al., 2023a) has made their question-answering capabilities crucial in many applications. Recently, the inputs for LLMs have been continually expanded to include multiple languages and various specialized domains, including applications worldwide, code generation (Chen et al., 2021; Rozi\u00e8re et al., 2023), mathematical problem-solving (Romera-Paredes et al., 2024; Yang et al., 2024b), legal inference (Fei et al., 2024), economic decision-making (Xie et al., 2023), and medical diagnosis (Wang et al., 2023). Furthermore, Multimodal LLMs (Chen et al., 2023; Zhu et al., 2024; Liu et al., 2023a, 2024; Yao et al., 2024; OpenAI, 2023, 2024a,b) (MLLMs) can integrate diverse forms of information, such as image (Bai et al., 2023b; Wang et al., 2024), video (Zhang et al., 2023a; Chi et al., 2021), or tabular (Hegselmann et al., 2023) inputs. These advances across multilingual, multidomain, and multimodal as M\u00b3 omni-applications are steering us toward Artificial General Intelligence (AGI) systems (Goertzel, 2014).\nUnlike previous single-task models, LLMs and their extensions are expected to excel in comprehensive zero-shot capabilities, remaining effective in traditional text-only tasks (Zhang"}, {"title": "2. OMNIEVALKIT for Comprehensive Evaluation", "content": "In this section, we first introduce the key components of the OMNIEVALKIT framework. In Figure 1, we illustrate how these components are connected through the entire evaluation pipeline using a data flow via the static builder."}, {"title": "2.1 Key Features", "content": "\u2022 Model Types & Series: The LLMs and their M\u00b3 omni-extensions. In Figure 1, we present a list of models that OMNIEVALKIT considers, supporting over 100 LLM, as well as LLM's extensions, including the Llama (Touvron et al., 2023a,b), Qwen (Bai et al., 2023a; Yang et al., 2024a,b; Hui et al., 2024; Bai et al., 2023b; Wang et al., 2024), Mistral (Jiang et al., 2024), and Phi (Abdin et al., 2024) series, alongside a variety of proprietary series. The extended LLMs are proficient at following specific instructions across various language, domain, and modality-based dimensions, while also comprehending general text-only instructions in the original LLM. OMNIEVALKIT also accommodates diverse structural inputs, such as image, video, tabular, and other non-text modes.\n\u2022 Question Types & Evaluation Benchmarks: Multiple tasks for general question-answering capabilities, covering multidomain knowledge, multilingual migration, and multimodal information fusion tasks. We demonstrate that OMNIEVALKIT is compatible with mainstream evaluation benchmarks, including various examination assessment datasets such as MMLU/CMMLU (Hendrycks et al., 2021), BBH (Suz-gun et al., 2023), ARC-Easy/Challenge (Clark et al., 2018), and OpenbookQA (Mihaylov et al., 2018); typical language datasets like GLUE (Wang et al., 2019), HellaSwag (Zellers et al., 2019), and ANLI (Nie et al., 2020); and specialized knowledge datasets such as ACLUE (Zhang and Li, 2023), which focuses on ancient Chinese content, EQ-Bench (Paech, 2023) for emotional intelligence assessment, and PIQA (Bisk et al., 2020), which targets commonsense reasoning, among others. Additionally, OMNIEVALKIT includes multilingual extensions as MMMLU (Hendrycks et al., 2024), XStoryCloze (Lin et al., 2022), OALL (Elfilali et al., 2024) and multilingual translations of ARC or HellaSwag (Zellers et al., 2019). Further, OMNIEVALKIT covers the five major domains of coding, mathematics, law, finance, and healthcare. It also covers a comprehensive range of general multimodal assessments, including MMMU (Yue et al., 2023), MME (Fu et al., 2023), MMBench (Liu et al., 2023b), MMStar (Chen et al., 2024b), and MMVet (Yu et al., 2023), as well as multimodal-specific scientific question datasets like AI2D (Kembhavi et al., 2016), ScienceQA (Lu et al., 2022) and Hallusion Bench (Guan et al., 2023) for capabilities related to specific tasks.\n\u2022 Answer Extraction Facility: The omni-extensions of LLMs generate fluent, natural, and human-like responses during deployment, often including additional analysis and connecting words. For example, some models tend to combine longer thoughts in their responses. In OMNIEVALKIT, it supports not only pre-defined rich regular expressions to extract key answers but also the additional LLM to summarize answers from the response. It features a variety of regularization templates, and the answer extraction model interface aligns with the general one, enabling flexible customization options.\n\u2022 Model Generation Options: OMNIEVALKIT employs a traditional generation approach, incorporating perplexity (PPL) measurements, as MLLM also supports this feature. It provides a flexible assessment interface and offers multiple decoding modes to improve the generation process.\n\u2022 Accuracy Calculation Center: In addition to the default predefined evaluation metrics such as accuracy, BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and others available in the source benchmarks, OMNIEVALKIT also offers customizable metrics. It supports various question formats, including single-choice, multiple-choice, yes-or-no, fill-in-the-blank, and free-open ones."}, {"title": "2.2 Evaluation Pipeline", "content": "\u2022 Static Builder: Constructs the LLM or its omni-extension and evaluation facilities.\nModel Constructor: Configuration component for the evaluated LLM or its extension. The construction and initialization of the model are organized through independent modular files. Customizable interface parameters can be effortlessly passed, enhancing user adaptability. This module is currently responsible for updating the model structure, initializing pretrained parameters, performing GPU mapping, and transforming tokens. Implementing new models is streamlined to focus solely on the relevant model construction class while ensuring alignment with interfaces for token preprocessing, prompt concatenation, and response generation. Also, the constructor component supports customization for all generation choices, including different data settings and inference methods. When necessary, default settings are employed to simplify integration and maintain robustness throughout the evaluation.\nEvaluation Facilities: Create components dedicated to extracting answers from responses and assessing metrics. This module involves distinct member classes for filters and estimators. The filters remove excessive and redundant information while extracting the critical answers embedded in the model's response. Subsequently, each filtered answer is evaluated against the ground truth by the estimators, and the results are summarized across the entire dataset.\nOther Components: Additional parts that assist in the evaluation process. The flexibility of OMNIEVALKIT allows for the seamless integration of extra processing functions into the overall system. For example, a prompt handling module can be equipped to dynamically concatenate instructions for corresponding additional keys in JSON data files and a token preprocessing unit for effectively reading visual input, such as images, video, or tabular information. These components serve as crucial supportive elements within the overall data flow.\n\u2022 Data Flow: All datasets are stored in a unified JSON format as a list of dicts. Each dimension, such as domain, language, modality, instruction, and ground truth answer, is recorded in key-value pairs. The file includes different settings for Chain-of-Thought (CoT) (Wei et al., 2022) and Few-Shot In-Context Learning (FSL, ICL) (Brown et al., 2020; Dong et al., 2024). This standardized and compact format facilitates the expansion of new tasks within the OMNIEVALKIT framework, enhancing the overall versatility and scalability.\nData flow-driven interaction with models and evaluation facilities. When instructions from the data flow are input into the model, the corresponding responses are processed through the answer extraction module and, subsequently, the evaluation module. Specifically, relevant keys in the data JSON record the prompts required for the instruction, such as in-context few-shot examples or relevant thoughts. As the data flows through the evaluated model, it is concatenated with highly available custom prompts. After inference, the outputs are filtered through a key answer extraction module, where the core content of the responses is extracted using regular expressions or additional models. Subsequently, the estimator module evaluates metrics for each or the entire dataset, aggregating the results obtained. This systematic approach guarantees coherent and reliable results, preserving both integrity and utility."}, {"title": "3. Conclusion & Derivative Fields", "content": "OMNIEVALKIT is a highly flexible and modular evaluation framework designed for assessing M\u00b3 types of LLMs and their omni-extensions. It enables convenient combinations that allow the addition of new models and datasets with just a single-file modification. To date, OMNIEVALKIT has integrated over 50 different evaluation datasets, generating more than 5,000 sets of results for various LLMs and their omni-extensions. This adaptability opens up numerous possibilities for related application areas:\n\u2022 New Pattern and Law Exploration: Investigating the fundamental patterns and laws behind extensive evaluation results. For example, scaling laws are crucial for understanding the trends of LLMs and their omni-extensions concerning relevant variables (Kaplan et al., 2020; Zhang et al., 2024a), such as model performance relative to the dataset scale and training FLOPs. Scaling laws can guide the selection of hyperparameters and architectures, as well as the prediction of model capabilities for downstream tasks. Leveraging the extensive evaluation results from OMNIEVALKIT, custom metrics, and training-related performance can be utilized for scaling law research (Kaplan et al., 2020). In particular, the set of evaluation models quickly deployed within OMNIEVALKIT and the flexible, adjustable metric assessment module support researchers in exploring the relationships of model capabilities across different dimensions and hierarchical layers for extremely large-scale and diverse model types.\n\u2022 Evaluation and Selection of Special Models, Metrics, or Vertical Domain: When LLMs and their omni-extensions face deployment demands in vertical domains that involve new modalities, domains, and tasks, how to rapidly evaluate existing solutions becomes crucial for guiding pre-selection of optimal models. Some methods (You et al., 2021) consider relying on proxy metrics for generalization, measuring them against the evaluation results. Additionally, some learnable strategies (Zhang et al., 2023b) investigate how to learn generalized criteria for mapping model performance from existing data. Moreover, OMNIEVALKIT can also be extended to specific model evaluations and data filtering. For example, guiding re-rankers to act as reward models in reinforcement learning from human feedback (Ouyang et al., 2022), batch-generating data quality paradigms (Biderman et al., 2023), or initiating embedding models (Chen et al., 2024a) for a comprehensive assessment of data components.\n\u2022 Exploration of New embeddings and Key Outputs: OMNIEVALKIT'S modular components continuously evolve, embedding themselves in both the input and output data streams. This framework effectively captures the dynamics of each stage in the model inference process. Not only do these components log the relevant representations produced at any layer, but they also track information regarding future predictions, enabling researchers to gain deeper insights into the model's behavior. Additionally, OMNIEVALKIT offers customizable decoding methods for generated sequences, creating a flexible environment for the unified evaluation of various search strategies. This comprehensive approach allows researchers to analyze and compare different generation strategies more effectively.\nOMNIEVALKIT has shown stable performance across diverse devices and will continue to evolve to support additional GPU architectures and deep learning deployment frameworks."}]}