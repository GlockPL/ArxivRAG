{"title": "ROBOT NAVIGATION WITH ENTITY-BASED COLLISION AVOIDANCE USING DEEP REINFORCEMENT LEARNING", "authors": ["Yury Kolomeytsev", "Dmitry Golembiovsky"], "abstract": "Efficient navigation in dynamic environments is crucial for autonomous robots interacting with various environmental entities, including both moving agents and static obstacles. In this study, we present a novel methodology that enhances the robot's interaction with different types of agents and obstacles based on specific safety requirements. This approach uses information about the entity types, improving collision avoidance and ensuring safer navigation. We introduce a new reward function that penalizes the robot for collisions with different entities such as adults, bicyclists, children, and static obstacles, and additionally encourages the robot's proximity to the goal. It also penalizes the robot for being close to entities, and the safe distance also depends on the entity type. Additionally, we propose an optimized algorithm for training and testing, which significantly accelerates train, validation, and test steps and enables training in complex environments. Comprehensive experiments conducted using simulation demonstrate that our approach consistently outperforms conventional navigation and collision avoidance methods, including state-of-the-art techniques. To sum up, this work contributes to enhancing the safety and efficiency of navigation systems for autonomous robots in dynamic, crowded environments.", "sections": [{"title": "1 Introduction", "content": "The development of robots, designed to assist humans and perform different tasks, has revolutionized numerous sectors, including industrial automation, manufacturing, agriculture, space, medical science, household utilities, delivery, and social services. Autonomous navigation, a key attribute of these robots, enables them to move independently, perceive the environment, and take appropriate actions to complete tasks efficiently, effectively, and safely.\nRecent research interest in mobile robots, particularly in the context of practical applications such as delivery, search and rescue, service, and warehouse robots has emphasized the importance of robot navigation. This field solves the problem of planning the robot's path from the start point to the destination while avoiding other agents and obstacles in different dynamic environments. This task becomes even more challenging when the robot needs to navigate through crowded places like airports, hospitals, streets, squares, and shopping centers. Traditional navigation approaches, which often treat moving agents as static obstacles or react through short-sighted, one-step look-ahead, result in unsafe and unnatural behaviors."}, {"title": "2 Background", "content": "Recently extensive research has led to significant advancements in robot navigation including navigation in crowded environments.\nSocial Force Model (SFM) Helbing and Moln\u00e1r [1995] uses attractive and repulsive forces to model crowd interactions. Other approaches like Reciprocal Velocity Obstacle (RVO) and Optimal Reciprocal Collision Avoidance (ORCA) van den Berg et al. [2011] consider surrounding agents as velocity obstacles to calculate optimal collision-free velocities under reciprocal assumptions. These methods, while effective in specific scenarios, rely heavily on hand-crafted rules and often fail in real social scenarios where their assumptions are not fully met, compromising the safety of human-robot interaction and leading to the \u201cfreezing robot\" problem in dense environments.\nRecently, learning-based methods have gained significant attention. Several studies use imitation learning to derive navigation strategies from demonstrations of desired behaviors. In studies Tai et al. [2018], Long et al. [2016] navigation policies using depth images, lidar measurements, and local maps as inputs are developed by imitating expert demonstrations. In addition to behavioral cloning, inverse reinforcement learning has been applied in Kretzschmar et al. [2016] to extract the underlying cooperation features from human data using the maximum entropy method. However, the effectiveness of these methods heavily relies on the scale and quality of the demonstrations, which can be resource-intensive and limit the policy's quality due to human constraints."}, {"title": "2.2 Problem Formulation", "content": "We consider the task of a delivery robot navigating towards a goal in a dynamic environment populated with adults, children, bicycles, and static obstacles. This task can be formulated as a partially observable Markov decision process (POMDP). The robot must navigate in a socially compliant manner, avoiding collisions while reaching its goal. We define the state of each agent (robot or environmental entity) in terms of observable and unobservable components:\n\u2022 Observable states: position p = [$p_x$, $p_y$], velocity v = [$v_x$, $v_y$], and radius r.\n\u2022 Unobservable states: goal position g = [$g_x$, $g_y$], preferred speed $v_{pref}$, and heading angle $\\theta$.\nWhile acting in the environment robot knows his observable and unobservable states and also robot knows the observable states of all entities. We use the robot-centric parameterization described in Chen et al. [2017b] and Everett et al. [2018], where the robot is positioned at the origin and the x-axis is aligned with the robot's goal direction. Denote $e^i$ as a type of entity i. The robot's state and the i-th entity's observable state at time t could be represented as:\n$s_r^t = [d_g^t, v_x^t, v_y^t, r^t, v_{pref}^t, \\theta^t]$,\n$s_{i0}^t = [p_x^t, p_y^t, v_x^t, v_y^t, r_i^t, d_i^t, r_i^t + r^t, e_i]$,\nwhere $d_g = ||p_t \u2013 g||_2$ is the distance from the robot to the goal, and $d^i = ||p_t - p_i||_2$ is the distance from the robot to the i-th entity.\nThe robot can acquire its own state and the observable states of other agents at each time step. The joint state at time t is defined by:\n$s^t = [s_r^t, s_{10}^t, s_{20}^t, ..., s_{n0}^t]$.\nThe robot's velocity $v_t$ is determined by the action command $a_t$ from the navigation policy, i.e., $v_t = \\pi(s_r^t) = a_t$.\nThe reward function R($s^t$, $a_t$) is defined as a mapping from the current state $s^t$ and action $a_t$ to a scalar value, which represents the immediate benefit of taking action $a_t$ in state $s^t$. Formally:\nR($s^t$, $a_t$) = $r_t$,\nwhere $r_t$ is the reward received at time step t.\nOur objective is to find the optimal policy $\\pi^*(s_r^t)$ that maximizes the expected reward.\nThe optimal value function is given by Sutton and Barto [2018]:\n$V^*(s_r^t) = \\sum_{t'=t}^T \\gamma^{t'-t} R(s_r^{t'}, \\pi^*(s_r^{t'}))$,\nwhere $\\gamma \\in (0, 1)$ is a discount factor and T is the time step at which the episode ends.\nUsing value iteration, the optimal policy is derived as:"}, {"title": "3 New Approach", "content": "In this section, we formulate our new approach which we call Entity-Based Collision Avoidance using Deep Rein-forcement Learning (EB-CADRL). We propose a novel reward function. Then, we propose a model based on a neural network that utilizes the information about the types of entities. Finally, we formulate an improved version of the deep V-learning algorithm that utilizes multiprocessing and significantly improves the performance during training, validation, and testing."}, {"title": "3.1 Reward Function", "content": "For concise usage in the subsequent formulas we denote the entity types:\n\u2022 Adult: A\n\u2022 Bicycle: B\n\u2022 Child: C\n\u2022 Obstacle: O\nLet d(e) be the minimum distances between the robot and each of entity types where e $\\in$ {A,B,C,O}. Let $d_{min}$ = min{d(A), d(B), d(C), d(O)} be the minimum distance between the robot and all entities around the robot over the period [t \u2013 \u2206t, t]. If a collision with the robot occurs for entity e, then d(e) and $d_{min}$ are less than or equal to zero.\nThe function $R_t(t)$ represents the time reward:\n$R_t(t) = \\begin{cases}\n1 & \\text{if } t < t_{good} \\\\\n\\frac{t_{max} - t}{t_{max} - t_{good}} & \\text{if } t_{good} \\leq t \\leq t_{max} \\\\\n0 & \\text{otherwise}\n\\end{cases}$\nwhere: $t_{good}$ is the time duration considered as \"good\" for reaching the goal, $t_{max}$ is the maximum allowable time to reach the goal.\nLet $d_{max}$ be the maximum distance to the goal, which is the Euclidean distance between the starting position of the robot at time t = 0 and the position of the goal. Let $d_g(t)$ be the distance to goal at time t. Then proximity reward can be represented as:\n$R_p(t) = 1 - \\frac{d_g(t)}{d_{max}}$\nLet $R_e(e)$ represent the penalties for collisions. The penalties are defined as:\n$R_c(e) = \\begin{cases}\n-0.5, & \\text{if } e = O \\\\\n-1.0, & \\text{if } e = A \\\\\n-1.5, & \\text{if } e = B \\\\\n-2.0, & \\text{if } e = C\n\\end{cases}$\nThe rationale behind the collision penalties is based on the predictability of the entity's behavior and the severity of the consequences of a collision. Static obstacles are the most predictable and have the least severe consequences, resulting in the smallest penalty (-0.5). Adults exhibit predictable behavior with moderate collision consequences, resulting in a penalty of -1.0. Cyclists, due to their higher speed and potential for severe consequences such as falling from the"}, {"title": "3.2 Model", "content": "Our model builds upon the architecture described in Chen et al. [2019], enhancing it for the specific task of controlling a delivery robot navigating in dynamic environments with various obstacles and pedestrians. To account for different types of entities (e.g., humans, children, bicycles), we incorporated an additional module. Initially, this module uses one-hot encoding to represent each agent type. For more advanced scenarios, one could use trainable embeddings for each agent type. These encoded features or embeddings are then concatenated with the primary features of the agents and passed into the model.\nWe embed the state of each entity i and the map tensor $M_i$, along with the state of the robot, into a fixed-length vector $g_i$ using a multi-layer perceptron (MLP):\n$g_i = \\phi_g(s_r^t, s_{i0}^t, M_i; W_g)$\nwhere $\\phi_g(\u00b7)$ is an embedding function with ReLU activations, and $W_g$ are the embedding weights.\nThe embedding vector $g_i$ is then fed into another MLP to obtain the pairwise interaction feature between the robot and entity i:\n$h_i = \\psi_h(g_i; W_h)$\nwhere $\\psi_h(\u00b7)$ is a fully-connected layer with ReLU nonlinearity, and $W_h$ are the network weights."}, {"title": "3.3 Algorithm Parallel Deep V-learning", "content": "The value network is trained by the temporal-difference method with standard experience replay and fixed target network techniques. However, we improve it by using parallelism. We run several agents in several environments in parallel which helps us to collect the experience much faster than in the previous work. We call this approach \"Parallel Deep V-learning\", the pseudocode is shown in Algorithm 1 and Algorithm 2.\nThe approach that we present in Algorithms 1 and 2 is a bit similar to asynchronous methods for deep reinforcement learning Mnih et al. [2016]. However, the key difference here is that we do not do asynchronous updates of neural network. Instead, we accumulate state and value pairs into Experience replay in parallel and then we make a centralized update using the whole replay buffer with random sampling. In our setup, the episode duration is much longer than the gradient learning step, so the bottleneck of the original method was in running episodes. So using parallel deep V-learning solves the problem even without asynchronous model updates. In the future, we can also test our algorithm with asynchronous model updates."}, {"title": "3.4 Implementation Details", "content": "We assume holonomic kinematics for the robot, i.e., it can move in any direction. The action space consists of 80 discrete actions: 5 speeds exponentially spaced between (0, $v_{pref}$] and 16 headings evenly spaced between [0, 2\u03c0).\nThe hidden units of functions $\\phi_g(\u00b7)$, $\\psi_h(\u00b7)$, $\\psi_a(\u00b7)$, $f_v(\u00b7)$ are (300, 200), (200, 100), (200, 200), (300, 200, 200) respectively.\nWe implemented the policy in PyTorch and trained it with a batch size of 100 (for each episode) using stochastic gradient descent. For imitation learning, we collected 3000 episodes of demonstration using ORCA and trained the policy 50 epochs with a learning rate of 0.01. For reinforcement learning, the learning rate is 0.001 and the discount"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Simulator", "content": ""}, {"title": "4.2 Simulation Setup", "content": "The simulated agents: adults, children, and bicyclists are controlled by Optimal Reciprocal Collision Avoidance (ORCA) van den Berg et al. [2011]. The sizes and speed of agents are generated from Uniform distribution to introduce diversity. The parameters of the distribution depend on the entity type. The distribution parameters are chosen so that, on average, adults are slightly larger and faster than children, and cyclists are slightly larger and faster than adults. Additionally,"}, {"title": "4.3 Results", "content": "The results of the train and validation steps are shown in Figure 2. Firstly, based on the Success Rate metric we can see that the SARL method completely fails to learn policy that can reach the goal. The SARL model can only learn a policy that successfully avoids collisions. SARL has the lowest Collision Rate metric. As our main task is to provide an algorithm that achieves the goal (while avoiding obstacles), we do not consider the SARL model in further analysis, since it fails to do such a task completely.\nWe can see that both methods SARL-GP and EB-CADRL (Ours) successfully converge during 50000 train episodes and get to a plateau approximately from 30000 episodes. We can see that EB-CADRL outperforms other methods both on train and validation using the Collision Rate metric and Weighted Score metric. We compute the Weighted Score metric using the following formula:\n$Weighted Score = SR \u2013 CR(A) \u2013 4.0 \u00b7 CR(C)\n\u2013 2.0 \u00b7 CR(B) - 0.5 \u00b7 CR(O)$\nThe reward metric in this case cannot be used to compare different models as each model has its own reward function. However, it indicates how the method converges and could be used to get the episode for early stopping based on results on the validation dataset.\nIn Table 1 we provide the result of testing each model on test data which has a size of 1000 episodes. For comparison, we use the following metrics: SR - success rate, CR - collision rate, CR(A) - collision rate with adults, CR(B) - collision rate with bicycles, CR(C) - collision rate with children, CR(O) - collision rate with static obstacles, Time - average time to reach the goal, DD(A) - mean danger distance with adults, DD(B) - mean danger distance with bicycles, DD(C) - mean danger distance with children, DD(O) - mean danger distance with static obstacles. We assume that danger starts when the distance between the robot and the entity is less than 30cm.\nSARL has a success rate of 0, meaning it fails to reach the goal in all attempts. Both SARL-GP and EB-CADRL have a success rate of 0.681, indicating they are equally successful in reaching the goal. SARL has the lowest collision rate, but it's important to note that its success rate is zero, indicating it did not navigate successfully enough to encounter many obstacles. EB-CADRL has a significantly lower collision rate compared to SARL-GP, indicating better overall safety. EB-CADRL has lower collision rates with bicycles and static obstacles compared to SARL-GP and is on par with SARL in terms of collisions with children. It also has a lower collision rate with adults compared to SARL-GP, although slightly higher than SARL. SARL-GP and EB-CADRL have similar average times to reach the goal, indicating that"}, {"title": "4.4 Ablation Experiments", "content": "To evaluate the impact of including the agent type as a feature, we conducted an ablation study by training and testing two models: one incorporating the agent type and the other excluding it. While the model can infer some aspects of the agent type through features such as radius and velocity, these features alone do not provide a clear distinction between different agent types. For instance, a small, slow-moving cyclist or a large, fast-moving human could mislead the model. Therefore, we hypothesize that explicitly adding the agent type to the feature set could enhance the model's performance. Both models in this experiment utilize a new reward function and are based on the EB-CADRL method.\nFor ablation experiments, we use circle and square crossing scenarios. The radius of the circle is 6m and the size of the square is 13m \u00d7 13m.\nTo test models during ablation experiments we use the following metrics. Reward is the reward function that we formulated in 3.1. Success measures the percentage of times the robot successfully reached its goal without collisions. Collision metrics (Adult, Bicycle, Child, Obstacle) measure the frequency of collisions with different entities. Danger time is the mean time during which the robot was in a potentially dangerous state (close to collision). A lower value means the robot spent less time in dangerous zones in which there is a risk of getting into a collision. Apart from that, the long presence of the robot in such areas may cause discomfort to surrounding pedestrians and other entities which is not desirable. Time is the average time it took for the robot to reach its goal. A lower time indicates that the robot was able to complete its task more quickly.\nThe results of the ablation experiment on the Test dataset are shown in Table 2. The reinforcement learning model with entity type outperforms the one without entity type in several key aspects. The model with entity type achieved a higher reward and a better success rate, indicating superior overall performance. It also registered fewer collisions with bicycles, a shorter time to reach the goal, and less time in danger. However, it is worth noting that the model without entity type had fewer collisions with adults and obstacles. Overall, the inclusion of entity type in the model appears to enhance its performance in most aspects."}, {"title": "5 Discussion", "content": "While our EB-CADRL method shows promising results, several limitations remain. The simulation environment may not fully capture real-world complexities such as varying terrains and unpredictable human behaviors. The predefined entity types and simplified reward function might limit the model's adaptability to more nuanced scenarios. Moreover, the current implementation assumes holonomic kinematics, which may not be applicable to all types of robots. Future work should focus on real-world testing, incorporating advanced perception systems, developing adaptive reward functions, optimizing computational efficiency, extending the model to non-holonomic robots, and exploring multi-robot coordination. Addressing these areas will enhance the robustness, safety, and efficiency of autonomous robot navigation in dynamic environments."}, {"title": "6 Conclusion", "content": "In this study, we introduced a novel reinforcement learning approach, termed Entity-Based Collision Avoidance using Deep Reinforcement Learning (EB-CADRL), for the navigation of autonomous delivery robots in dynamic environments. Our primary focus was on enhancing safety and efficiency by leveraging entity-specific information to guide the robot's interactions with different types of agents and obstacles. Our proposed reward function penalized the robot differently based on the type of entity involved in a collision and its proximity to entities, thereby ensuring more context-aware and safer navigation. Our experimental results demonstrate that EB-CADRL significantly outperforms traditional navigation methods and state-of-the-art techniques. Notably, our approach achieved a higher success rate in reaching the goal while maintaining a lower collision rate across various entity types. The model effectively balanced the trade-off between safety and efficiency, maintaining safe distances from entities without significantly increasing travel time. The ablation experiments further validated the importance of incorporating entity-specific information, as models with this feature consistently performed better in terms of reward, success rate, and overall safety metrics compared to those without it. This underscores the relevance and added value of our work in developing more sophisticated and context-aware navigation systems for autonomous robots. In conclusion, our proposed EB-CADRL approach represents a significant advancement in the field of autonomous robot navigation, offering a robust, safe, and efficient solution for navigating dynamic, crowded environments."}, {"title": "A Appendix", "content": "In this section we show figures of collision rate for each entity type on train and validation data."}]}