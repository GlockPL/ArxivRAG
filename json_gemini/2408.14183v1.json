{"title": "ROBOT NAVIGATION WITH ENTITY-BASED COLLISION\nAVOIDANCE USING DEEP REINFORCEMENT LEARNING", "authors": ["Yury Kolomeytsev", "Dmitry Golembiovsky"], "abstract": "Efficient navigation in dynamic environments is crucial for autonomous robots interacting with\nvarious environmental entities, including both moving agents and static obstacles. In this study, we\npresent a novel methodology that enhances the robot's interaction with different types of agents and\nobstacles based on specific safety requirements. This approach uses information about the entity types,\nimproving collision avoidance and ensuring safer navigation. We introduce a new reward function\nthat penalizes the robot for collisions with different entities such as adults, bicyclists, children, and\nstatic obstacles, and additionally encourages the robot's proximity to the goal. It also penalizes the\nrobot for being close to entities, and the safe distance also depends on the entity type. Additionally,\nwe propose an optimized algorithm for training and testing, which significantly accelerates train,\nvalidation, and test steps and enables training in complex environments. Comprehensive experiments\nconducted using simulation demonstrate that our approach consistently outperforms conventional\nnavigation and collision avoidance methods, including state-of-the-art techniques. To sum up, this\nwork contributes to enhancing the safety and efficiency of navigation systems for autonomous robots\nin dynamic, crowded environments.", "sections": [{"title": "Introduction", "content": "The development of robots, designed to assist humans and perform different tasks, has revolutionized numerous sectors,\nincluding industrial automation, manufacturing, agriculture, space, medical science, household utilities, delivery, and\nsocial services. Autonomous navigation, a key attribute of these robots, enables them to move independently, perceive\nthe environment, and take appropriate actions to complete tasks efficiently, effectively, and safely.\nRecent research interest in mobile robots, particularly in the context of practical applications such as delivery, search\nand rescue, service, and warehouse robots has emphasized the importance of robot navigation. This field solves the\nproblem of planning the robot's path from the start point to the destination while avoiding other agents and obstacles in\ndifferent dynamic environments. This task becomes even more challenging when the robot needs to navigate through\ncrowded places like airports, hospitals, streets, squares, and shopping centers. Traditional navigation approaches, which\noften treat moving agents as static obstacles or react through short-sighted, one-step look-ahead, result in unsafe and\nunnatural behaviors."}, {"title": "Background", "content": "Recently extensive research has led to significant advancements in robot navigation including navigation in crowded\nenvironments.\nSocial Force Model (SFM) Helbing and Moln\u00e1r [1995] uses attractive and repulsive forces to model crowd interactions.\nOther approaches like Reciprocal Velocity Obstacle (RVO) and Optimal Reciprocal Collision Avoidance (ORCA)\nvan den Berg et al. [2011] consider surrounding agents as velocity obstacles to calculate optimal collision-free velocities\nunder reciprocal assumptions. These methods, while effective in specific scenarios, rely heavily on hand-crafted rules\nand often fail in real social scenarios where their assumptions are not fully met, compromising the safety of human-robot\ninteraction and leading to the \u201cfreezing robot\" problem in dense environments.\nRecently, learning-based methods have gained significant attention. Several studies use imitation learning to derive\nnavigation strategies from demonstrations of desired behaviors. In studies Tai et al. [2018], Long et al. [2016]\nnavigation policies using depth images, lidar measurements, and local maps as inputs are developed by imitating expert\ndemonstrations. In addition to behavioral cloning, inverse reinforcement learning has been applied in Kretzschmar\net al. [2016] to extract the underlying cooperation features from human data using the maximum entropy method.\nHowever, the effectiveness of these methods heavily relies on the scale and quality of the demonstrations, which can be\nresource-intensive and limit the policy's quality due to human constraints."}, {"title": "Problem Formulation", "content": "We consider the task of a delivery robot navigating towards a goal in a dynamic environment populated with adults,\nchildren, bicycles, and static obstacles. This task can be formulated as a partially observable Markov decision process\n(POMDP). The robot must navigate in a socially compliant manner, avoiding collisions while reaching its goal. We\ndefine the state of each agent (robot or environmental entity) in terms of observable and unobservable components:\n\u2022 Observable states: position p = [Px, Py], velocity v = [vx, vy], and radius r.\n\u2022 Unobservable states: goal position g = [gx, gy], preferred speed vpref, and heading angle 0.\nWhile acting in the environment robot knows his observable and unobservable states and also robot knows the observable\nstates of all entities. We use the robot-centric parameterization described in Chen et al. [2017b] and Everett et al. [2018],\nwhere the robot is positioned at the origin and the x-axis is aligned with the robot's goal direction. Denote ei as a type\nof entity i. The robot's state and the i-th entity's observable state at time t could be represented as:\ns = [dg, vx, Vy, r, Upref, \u03b8],\n\\(s_{i}^{o} = [P_{x}^{i}, P_{y}^{i}, v_{x}^{i}, v_{y}^{i}, r^{i}]\\),\nsio = [P, Py, v, vy, r\u00b2, d\u00b2, r\u00b2 + r, ei],\nwhere dg = ||pt \u2013 g||2 is the distance from the robot to the goal, and d\u00b2 = ||pt - pi||2 is the distance from the robot to\nthe i-th entity.\nThe robot can acquire its own state and the observable states of other agents at each time step. The joint state at time t\nis defined by:\ns = [s, s\u2070, s,, s\nThe robot's velocity vt is determined by the action command at from the navigation policy, i.e., vt = \u03c0(s\u012f). at\nThe reward function R(s, at) is defined as a mapping from the current state s and action a\u0165 to a scalar value, which\nrepresents the immediate benefit of taking action at in state s. Formally:\nR(s, at) = rt,\nwhere rt is the reward received at time step t.\nOur objective is to find the optimal policy \u03c0*(s) that maximizes the expected reward.\nThe optimal value function is given by Sutton and Barto [2018]:\nV*(s) = \\sum_{t=t'}^{T} \\gamma^{t-t'} R(s_t, \\pi^*(s_t)),\nwhere \u03b3\u2208 (0, 1) is a discount factor and T is the time step at which the episode ends.\nUsing value iteration, the optimal policy is derived as:"}, {"title": "New Approach", "content": "In this section, we formulate our new approach which we call Entity-Based Collision Avoidance using Deep Rein-\nforcement Learning (EB-CADRL). We propose a novel reward function. Then, we propose a model based on a neural\nnetwork that utilizes the information about the types of entities. Finally, we formulate an improved version of the\ndeep V-learning algorithm that utilizes multiprocessing and significantly improves the performance during training,\nvalidation, and testing."}, {"title": "Reward Function", "content": "For concise usage in the subsequent formulas we denote the entity types:\n\u2022 Adult: A\n\u2022 Bicycle: B\n\u2022 Child: C\n\u2022 Obstacle: O\nLet d(e) be the minimum distances between the robot and each of entity types where e \u2208 {A,B,C,O}. Let\ndmin = min{d(A), d(B), d(C'), d(O)} be the minimum distance between the robot and all entities around the robot\nover the period [t \u2013 \u2206t, t]. If a collision with the robot occurs for entity e, then d(e) and dmin are less than or equal to\nzero.\nThe function Rt(t) represents the time reward:\nR_{t}(t) = \\begin{cases}\n    1 & \\text{if } t < t_{\\text{good}} \\\\\n    \\frac{t_{\\text{max}} - t}{t_{\\text{max}} - t_{\\text{good}}} & \\text{if } t_{\\text{good}} \\leq t \\leq t_{\\text{max}} \\\\\n    0 & \\text{otherwise}\n\\end{cases}\nwhere: tgood is the time duration considered as \"good\" for reaching the goal, tmax is the maximum allowable time to\nreach the goal.\nLet dmax be the maximum distance to the goal, which is the Euclidean distance between the starting position of the\nrobot at time t = 0 and the position of the goal. Let dg (t) be the distance to goal at time t. Then proximity reward can\nbe represented as:\nRp(t) = 1 - \\frac{d_{g}(t)}{d_{\\text{max}}}\nLet Re(e) represent the penalties for collisions. The penalties are defined as:\nRc(e) = \\begin{cases}\n    -0.5, & \\text{if } e = O \\\\\n    -1.0, & \\text{if } e = A \\\\\n    -1.5, & \\text{if } e = B \\\\\n    -2.0, & \\text{if } e = C\n\\end{cases}\nThe rationale behind the collision penalties is based on the predictability of the entity's behavior and the severity of the\nconsequences of a collision. Static obstacles are the most predictable and have the least severe consequences, resulting\nin the smallest penalty (-0.5). Adults exhibit predictable behavior with moderate collision consequences, resulting in\na penalty of -1.0. Cyclists, due to their higher speed and potential for severe consequences such as falling from the"}, {"title": "Model", "content": "Our model builds upon the architecture described in Chen et al. [2019], enhancing it for the specific task of controlling\na delivery robot navigating in dynamic environments with various obstacles and pedestrians. To account for different\ntypes of entities (e.g., humans, children, bicycles), we incorporated an additional module. Initially, this module uses\none-hot encoding to represent each agent type. For more advanced scenarios, one could use trainable embeddings for\neach agent type. These encoded features or embeddings are then concatenated with the primary features of the agents\nand passed into the model.\nWe embed the state of each entity i and the map tensor Mi, along with the state of the robot, into a fixed-length vector\ngi using a multi-layer perceptron (MLP):\ngi = $g(s^r, s^{io}, M_i; W_g)\nwhere g() is an embedding function with ReLU activations, and Wg are the embedding weights.\nThe embedding vector gi is then fed into another MLP to obtain the pairwise interaction feature between the robot and\nentity i:\nhi = h(gi; Wh)\nwhere h (\u00b7) is a fully-connected layer with ReLU nonlinearity, and Wh are the network weights."}, {"title": "Algorithm Parallel Deep V-learning", "content": "The value network is trained by the temporal-difference method with standard experience replay and fixed target\nnetwork techniques. However, we improve it by using parallelism. We run several agents in several environments in\nparallel which helps us to collect the experience much faster than in the previous work. We call this approach \"Parallel\nDeep V-learning\", the pseudocode is shown in Algorithm 1 and Algorithm 2.\nThe approach that we present in Algorithms 1 and 2 is a bit similar to asynchronous methods for deep reinforcement\nlearning Mnih et al. [2016]. However, the key difference here is that we do not do asynchronous updates of neural\nnetwork. Instead, we accumulate state and value pairs into Experience replay in parallel and then we make a centralized\nupdate using the whole replay buffer with random sampling. In our setup, the episode duration is much longer than\nthe gradient learning step, so the bottleneck of the original method was in running episodes. So using parallel deep\nV-learning solves the problem even without asynchronous model updates. In the future, we can also test our algorithm\nwith asynchronous model updates."}, {"title": "Implementation Details", "content": "We assume holonomic kinematics for the robot, i.e., it can move in any direction. The action space consists of 80\ndiscrete actions: 5 speeds exponentially spaced between (0, Upref] and 16 headings evenly spaced between [0, 2\u03c0).\nThe hidden units of functions (\u00b7), \u03c8\u03b7(\u00b7), \u03c8\u03b1(\u00b7), fr(\u00b7) are (300, 200), (200, 100), (200, 200), (300, 200, 200)\nrespectively.\nWe implemented the policy in PyTorch and trained it with a batch size of 100 (for each episode) using stochastic\ngradient descent. For imitation learning, we collected 3000 episodes of demonstration using ORCA and trained the\npolicy 50 epochs with a learning rate of 0.01. For reinforcement learning, the learning rate is 0.001 and the discount"}, {"title": "Experiments", "content": "We train and test our models using our simulator. The example of one step of an episode is shown in Figure 1. The\nrobot, adults, bicyclists, children, and static obstacles are shown. The goals of each entity (excluding static obstacles)\nand the robot's goal are marked as stars. Each dynamic entity and its goal is enumerated. We use a square crossing\nscenario, where all the agents are randomly positioned on a square and their goal positions are chosen randomly on the\nopposite side of the square. Also, we use a circle crossing scenario, where all the agents are randomly positioned on a\ncircle and their goal positions are chosen on the opposite side of the circle."}, {"title": "Simulation Setup", "content": "The simulated agents: adults, children, and bicyclists are controlled by Optimal Reciprocal Collision Avoidance (ORCA)\nvan den Berg et al. [2011]. The sizes and speed of agents are generated from Uniform distribution to introduce diversity.\nThe parameters of the distribution depend on the entity type. The distribution parameters are chosen so that, on average,\nadults are slightly larger and faster than children, and cyclists are slightly larger and faster than adults. Additionally,"}, {"title": "Results", "content": "The results of the train and validation steps are shown in Figure 2. Firstly, based on the Success Rate metric we can\nsee that the SARL method completely fails to learn policy that can reach the goal. The SARL model can only learn a\npolicy that successfully avoids collisions. SARL has the lowest Collision Rate metric. As our main task is to provide an\nalgorithm that achieves the goal (while avoiding obstacles), we do not consider the SARL model in further analysis,\nsince it fails to do such a task completely.\nWe can see that both methods SARL-GP and EB-CADRL (Ours) successfully converge during 50000 train episodes\nand get to a plateau approximately from 30000 episodes. We can see that EB-CADRL outperforms other methods both\non train and validation using the Collision Rate metric and Weighted Score metric. We compute the Weighted Score\nmetric using the following formula:\nWeighted Score = SR \u2013 CR(A) \u2013 4.0 \u00b7 CR(C)\n\u2013 2.0 \u00b7 CR(B) - 0.5 \u00b7 CR(O)\nThe reward metric in this case cannot be used to compare different models as each model has its own reward function.\nHowever, it indicates how the method converges and could be used to get the episode for early stopping based on results\non the validation dataset.\nIn Table 1 we provide the result of testing each model on test data which has a size of 1000 episodes. For comparison,\nwe use the following metrics: SR - success rate, CR - collision rate, CR(A) - collision rate with adults, CR(B) - collision\nrate with bicycles, CR(C) - collision rate with children, CR(O) - collision rate with static obstacles, Time - average time\nto reach the goal, DD(A) - mean danger distance with adults, DD(B) - mean danger distance with bicycles, DD(C) -\nmean danger distance with children, DD(O) - mean danger distance with static obstacles. We assume that danger starts\nwhen the distance between the robot and the entity is less than 30cm.\nSARL has a success rate of 0, meaning it fails to reach the goal in all attempts. Both SARL-GP and EB-CADRL have a\nsuccess rate of 0.681, indicating they are equally successful in reaching the goal. SARL has the lowest collision rate, but\nit's important to note that its success rate is zero, indicating it did not navigate successfully enough to encounter many\nobstacles. EB-CADRL has a significantly lower collision rate compared to SARL-GP, indicating better overall safety.\nEB-CADRL has lower collision rates with bicycles and static obstacles compared to SARL-GP and is on par with\nSARL in terms of collisions with children. It also has a lower collision rate with adults compared to SARL-GP, although\nslightly higher than SARL. SARL-GP and EB-CADRL have similar average times to reach the goal, indicating that"}, {"title": "Ablation Experiments", "content": "To evaluate the impact of including the agent type as a feature, we conducted an ablation study by training and testing\ntwo models: one incorporating the agent type and the other excluding it. While the model can infer some aspects of the\nagent type through features such as radius and velocity, these features alone do not provide a clear distinction between\ndifferent agent types. For instance, a small, slow-moving cyclist or a large, fast-moving human could mislead the\nmodel. Therefore, we hypothesize that explicitly adding the agent type to the feature set could enhance the model's\nperformance. Both models in this experiment utilize a new reward function and are based on the EB-CADRL method.\nFor ablation experiments, we use circle and square crossing scenarios. The radius of the circle is 6m and the size of the\nsquare is 13m \u00d7 13m.\nTo test models during ablation experiments we use the following metrics. Reward is the reward function that we\nformulated in 3.1. Success measures the percentage of times the robot successfully reached its goal without collisions.\nCollision metrics (Adult, Bicycle, Child, Obstacle) measure the frequency of collisions with different entities. Danger\ntime is the mean time during which the robot was in a potentially dangerous state (close to collision). A lower value\nmeans the robot spent less time in dangerous zones in which there is a risk of getting into a collision. Apart from that,\nthe long presence of the robot in such areas may cause discomfort to surrounding pedestrians and other entities which is\nnot desirable. Time is the average time it took for the robot to reach its goal. A lower time indicates that the robot was\nable to complete its task more quickly.\nThe results of the ablation experiment on the Test dataset are shown in Table 2. The reinforcement learning model\nwith entity type outperforms the one without entity type in several key aspects. The model with entity type achieved a\nhigher reward and a better success rate, indicating superior overall performance. It also registered fewer collisions with\nbicycles, a shorter time to reach the goal, and less time in danger. However, it is worth noting that the model without\nentity type had fewer collisions with adults and obstacles. Overall, the inclusion of entity type in the model appears to\nenhance its performance in most aspects."}, {"title": "Discussion", "content": "While our EB-CADRL method shows promising results, several limitations remain. The simulation environment\nmay not fully capture real-world complexities such as varying terrains and unpredictable human behaviors. The\npredefined entity types and simplified reward function might limit the model's adaptability to more nuanced scenarios.\nMoreover, the current implementation assumes holonomic kinematics, which may not be applicable to all types of\nrobots. Future work should focus on real-world testing, incorporating advanced perception systems, developing adaptive\nreward functions, optimizing computational efficiency, extending the model to non-holonomic robots, and exploring\nmulti-robot coordination. Addressing these areas will enhance the robustness, safety, and efficiency of autonomous\nrobot navigation in dynamic environments."}, {"title": "Conclusion", "content": "In this study, we introduced a novel reinforcement learning approach, termed Entity-Based Collision Avoidance\nusing Deep Reinforcement Learning (EB-CADRL), for the navigation of autonomous delivery robots in dynamic\nenvironments. Our primary focus was on enhancing safety and efficiency by leveraging entity-specific information to\nguide the robot's interactions with different types of agents and obstacles. Our proposed reward function penalized the\nrobot differently based on the type of entity involved in a collision and its proximity to entities, thereby ensuring more\ncontext-aware and safer navigation. Our experimental results demonstrate that EB-CADRL significantly outperforms\ntraditional navigation methods and state-of-the-art techniques. Notably, our approach achieved a higher success rate in\nreaching the goal while maintaining a lower collision rate across various entity types. The model effectively balanced the\ntrade-off between safety and efficiency, maintaining safe distances from entities without significantly increasing travel\ntime. The ablation experiments further validated the importance of incorporating entity-specific information, as models\nwith this feature consistently performed better in terms of reward, success rate, and overall safety metrics compared to\nthose without it. This underscores the relevance and added value of our work in developing more sophisticated and\ncontext-aware navigation systems for autonomous robots. In conclusion, our proposed EB-CADRL approach represents\na significant advancement in the field of autonomous robot navigation, offering a robust, safe, and efficient solution for\nnavigating dynamic, crowded environments."}, {"title": "Appendix", "content": "In this section we show figures of collision rate for each entity type on train and validation data."}]}