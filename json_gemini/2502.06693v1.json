{"title": "Recent Advances, Applications and Open Challenges in Machine Learning for Health: Reflections from Research Roundtables at ML4H 2024 Symposium", "authors": ["Amin Adibi", "Xu Cao", "Zongliang Ji", "Jivat Neet Kaur", "Winston Chen", "Elizabeth Healey", "Brighton Nuwagira", "Wenqian Ye", "Geoffrey Woollard", "Maxwell A Xu", "Hejie Cui", "Johnny Xi", "Trenton Chang", "Vasiliki Bikia", "Nicole Zhang", "Ayush Noori", "Yuan Xia", "Md. Belal Hossain", "Hanna A. Frank", "Alina Peluso", "Yuan Pu", "Shannon Zejiang Shen", "John Wu", "Adibvafa Fallahpour", "Sazan Mahbub", "Ross Duncan", "Yuwei Zhang", "Yurui Cao", "Zuheng Xu", "Michael Craig", "Rahul G. Krishnan", "Rahmatollah Beheshti", "James M. Rehg", "Mohammad Ehsanul Karim", "Megan Coffee", "Leo Anthony Celi", "Jason Alan Fries", "Mohsen Sadatsafavi", "Dennis Shung", "Shannon McWeeney", "Jessica Dafflon", "Sarah Jabbour"], "abstract": "N/A", "sections": [{"title": "Introduction", "content": "The 4th Machine Learning for Health (ML4H) symposium was held in person on December 15-16, 2024, in the traditional, ancestral, and unceded territories of the Musqueam, Squamish, and Tsleil-Waututh Nations in Vancouver, British Columbia, Canada. The symposium included research roundtable sessions to foster discussions between participants and senior researchers on timely and relevant topics for the ML4H community (Jeong et al., 2024; Hegselmann et al., 2023).\nThe organization of the research roundtables at the conference involved 13 senior and 27 junior chairs across 13 tables. Each roundtable session included an invited senior chair (with substantial experience in the field), junior chairs (responsible for facilitating the discussion), and attendees from diverse backgrounds with interest in the session's topic."}, {"title": "Organization Process", "content": "We began by identifying an initial set of topics from articles published in the ML for health literature over the past 3-5 years, complemented by suggestions from ML4H roundtable subchairs. After eliminating duplicates, we curated a list of 24 candidate topics. Then, the roundtable subchairs vote for each topic and finalize the 13 topics. For each topic, we invited senior chairs with domain expertise, targeting one chair per roundtable. We then selected junior chairs, prioritizing two individuals with prior experience relevant to the topic. Researchers in the program committee are also welcomed to serve as junior chairs in the roundtable.\nBefore the event, junior and senior chairs collaborated to draft an introductory paragraph, which was shared on the ML4H website. They also proposed up to four discussion questions to guide the sessions. Given the popularity of roundtables from previous years, where two 25 min sessions were held, we instead had two 50-minute discussions, which allowed the participants to discuss topics in more depth. Following the event, the chairs submitted written summaries that highlight the key insights and takeaways from their discussions."}, {"title": "Research Roundtables", "content": null}, {"title": "Foundation Models and Multimodal AI", "content": "The convergence of multimodal foundation models and healthcare presents a frontier where diverse data streams from medical imaging and clinical text to biosignals and genomic data-intersect to create comprehensive patient representations. During this roundtable, participants explored critical questions shaping the development of multimodal AI in healthcare: How does multimodality affect model robustness and feature learning? What are the privacy and security implications of multimodal healthcare models? How should demographic and clinical features be incorporated into foundation models? How do we handle incomplete and misaligned data across modalities? How do we address causality and confounding in multimodal healthcare models? What are the trade-offs between general-purpose and healthcare-specific foundation models? How do we effectively handle temporal data and evaluation in clinical settings?\nChairs: Jason Fries, Max Xu, and Hejie Cui\nSpecial acknowledgment goes to Junyi Gao for helping with taking the notes.\nBackground: The emergence of multimodal foundation models marks a pivotal advancement in artificial intelligence, particularly for healthcare applications where patient data naturally spans multiple modalities. These models transcend traditional single-modality approaches by simultaneously processing and integrating diverse data types medical imaging, clinical narratives, structured EHR data, genomic information, and temporal biosignals. This multimodal capability offers unprecedented opportunities to capture the intricate interplay between different aspects of patient health, potentially leading to more comprehensive and nuanced clinical understanding."}, {"title": "DATA INTEGRATION AND ROBUSTNESS", "content": "The first topic highlighted how the integration of multiple data modalities presents both opportunities and challenges for healthcare AI. Participants noted that combining different data types (imaging, text, biosignals) could inherently increase model robustness by reducing reliance on spurious correlations and encouraging more grounded learning. Vision-language models were frequently cited as examples where multimodal integration leads to better generalization. However, this benefit comes with increased complexity in handling data alignment, as healthcare data is often incomplete across modalities. The group emphasized that while more modalities can enrich understanding, they can also limit the available sample size of fully aligned data sets."}, {"title": "CLINICAL CONTEXT AND DATA QUALITY", "content": "A recurring topic was the importance of understanding the clinical context in data collection and model development. Healthcare data collection is inherently biased by clinical decision-making processes MRI scans are typically only performed for suspected cases, and lab tests are ordered based on clinical necessity. This selection bias poses unique challenges for developing robust multimodal models. The group stressed that understanding these clinical sampling biases is crucial for building trustworthy models. Additionally, the quality of data varies significantly across modalities, with some data types being more standardized (lab values) than others (clinical notes)."}, {"title": "MODEL ARCHITECTURE AND SPECIALIZATION", "content": "The discussion revealed insights about model architecture choices and the degree of specialization needed. While general-purpose foundation models can provide quick baselines, they often lack the fine-grained understanding necessary for complex medical tasks. The group noted that even state-of-the-art general models might struggle to outperform simpler, specialized models on specific clinical prediction tasks. This observation led to a consensus that healthcare-specific architectures, particularly those designed to handle multiple modalities effectively, may be necessary for optimal performance."}, {"title": "TEMPORAL CONSIDERATIONS AND INFRASTRUCTURE", "content": "The handling of temporal data emerged as a critical challenge in multimodal healthcare AI. Healthcare systems generate complex time series data across different modalities, often with obfuscated timestamps and events not recorded in chronological order. The group emphasized the need for robust methods to handle temporal alignment across modalities and the importance of maintaining temporal coherence in model predictions. This challenge is compounded by current healthcare IT infrastructure limitations, which often make it difficult to align and synchronize data from different sources properly."}, {"title": "PRIVACY AND SECURITY", "content": "The integration of multiple modalities raises significant privacy and security concerns. Each additional modality introduces new potential vulnerabilities and increases the complexity of protecting sensitive patient information. Resource constraints were identified as a major barrier to building specialized healthcare foundation models from scratch. Many participants advocated for a pragmatic approach of fine-tuning pre-trained general models on healthcare-specific tasks, balancing computational costs with performance requirements."}, {"title": "FUTURE DIRECTIONS", "content": "The discussion highlighted three critical areas requiring attention for advancing multimodal foundation models in healthcare. First, there is a pressing need for robust methods to handle incomplete multimodal data and temporal alignment, particularly in real-world clinical settings where data streams are often misaligned or partially missing. Second, the development of healthcare-specific benchmarks that reflect real-world clinical conditions was identified as crucial for meaningful evaluation of these models. Third, participants emphasized the importance of improving data infrastructure to better support multimodal data integration while maintaining privacy and security standards. These directions reflect the practical challenges encountered in current implementations and represent concrete areas where focused research efforts could yield significant improvements."}, {"title": "Causality", "content": "Subtopic: Many sub-areas in machine learning (e.g., Computer Vision and Natural Language\nProcessing) have greatly benefited from having standardized benchmarks; if causality were to create\nsuch a benchmark, what should it look like? And what might be some challenges? Predictive\nmodels are widely used in many healthcare applications; how could causality be used to better\ndevelop predictive models?\nChairs: Rahul Krishnan, Johnny Xi, Trenton Chang, and Winston Chen\nBackground: Causal inference is widely applied in healthcare to derive reliable medical insights\nand develop robust AI tools to enhance the quality and efficiency of care. We started our discussion\nby having participants share their expertise in causal inference and what healthcare problems they\nhave tried to solve using causal inference.\nStandardized benchmarks have been proven effective in moving subareas of machine learning\nforward. We further discussed the potential of creating standardized benchmarks for causal inference\nin healthcare. Participants shared their thoughts on what those benchmarks would look like and\nwhat challenges the community must overcome.\nFinally, machine learning has been widely used to develop predictive models in healthcare. We\nfurther discussed how causality methods can improve predictive models. What are some considera-\ntions when building causally motivated models?"}, {"title": "\u0392\u0395NCHMARKS AND EVALUATION", "content": "Creating standard benchmarks has immensely benefited many sub-fields of machine learning. How-ever, such a standard benchmark does not yet exist in causality and healthcare. One of the biggestchallenges in creating a standard causality benchmark is that evaluating the estimated causal rela-tionship is not straightforward. This is because ground-truth causal effects are rarely observed.Some participants shared their thoughts on what a principled evaluation protocol for causal infer-ence might look like. One possibility is to leverage both observational and interventional (randomizedcontrol trial) data to assess the performance of different methods in inferring causal relationships. Inthis approach, causal effects may be estimated from observational data and subsequently validatedon interventional data.\nAnother approach for evaluating the causality method is considering the downstream applicationfor which the causal model is built. For example, when causal inference is developed for decision-making, a meaningful approach to evaluate different methods is quantifying the decision-makingperformance following the effects estimated by each method.\nInterventional data can be used to accurately estimate decision-making performance by leveragingoff-policy evaluation (OPE) methods. Other participants also proposed creating benchmarks byselecting applications with which true causal effects can be accurately measured. For example, inmany biological and molecular applications, outcomes conditioned on different treatments can beobtained with careful experimental designs. In such cases, researchers can obtain the true causalrelationship and use it to validate the performance of different causality methods. With the recentrise of large language models (LLMs), simulation-based evaluation has gained interest in causality.Instead of estimating the performance of causal methods via OPE methods or selecting applicationsfor which true causal relationships can be experimentally validated, creating high-fidelity simulationcan be a good and cheap starting point for evaluating estimated causal insights.\nThis idea of simulation-based evaluation is common in social sciences such as psychology. In thisfield, LLMs simulate different human agents and use the simulation results to rank hypotheses andprioritize experimental validation. These ideas can inspire the evaluation of causality.\nSome participants also pointed out the importance of having a standardized reporting guideline.Future guidelines should consider reporting the sensitivity of estimation instead of simply reportingthe estimated effects. For example, some important information to report include whether certainsubpopulations disproportionately contributing to the estimated effect, and how much the estimatedeffect vary when modifying specific individuals in the cohort."}, {"title": "CAUSALITY AND PREDICTION", "content": "Lastly, our round table discussed how causality could be used to improve predictive models inhealthcare. It is important to note that although causality has many desirable properties, it is notalways applicable. This is because, in practice, we often observe a trade-off between robustness andperformance when building causally-motivated models. Fully causal models are often robust yet"}, {"title": "TEMPORAL CONSIDERATIONS AND INFRASTRUCTURE", "content": "ADIBI ET AL.\nsuboptimal in terms of performance. Good use of causality should consider the need for specificapplications. For example, if certain applications are concerned with having the most performativemodel instead of leveraging causal relationships, using causal methods might not be necessary. Inpractice, high-performance predictive models often need to leverage non-causal correlations.\nDisentangling causal and correlational representations in the learned predictive model can be apowerful future research direction. Learning such disentangled representations allows practitioners toreuse the causal components across different environments (e.g., hospitals). Then, by augmentingthe causal components with environment-specifically learned correlational representations, predictivemodels' performance can be further enhanced to obtain the best of both the causal and correlationalworlds.\nLastly, our participants shared their interests in applying causality of building foundation modelsin healthcare. Currently, the majority of the foundation models are built purely by leveragingcorrelations. It is unclear how causality can be foundation models and what some benefits of havingcausally-motivated foundation models are. This largely unexplored area makes a very intriguing andan impactful future research direction."}, {"title": "Challenges of Interdisciplinary Research", "content": "Subtopic: Navigating the complexities of interdisciplinary collaboration in health AI poses significant challenges, such as integrating diverse expertise and overcoming generational technology gaps. How can we effectively overcome these barriers to enhance data accessibility and collaborative efforts?\nChairs: Dennis Shung, Vasiliki (Vicky) Bikia, and Nicole Zhang\nBackground: Combining expertise from computer science and clinical fields to effectively leverage health data is fraught with challenges (Kelly et al., 2019). Accessing comprehensive datasets, such as digital mammography data, often highlights barriers due to the need for clinician partnerships and extensive data curation. The process often involves significant time and effort to link or anonymize data, making it a lengthy endeavor to obtain usable datasets. Additionally, collaboration across disciplines often encounters communication barriers and technological reluctance. Engineers unfamiliar with medical intricacies and clinicians unaccustomed to machine learning paradigms often struggle to find a common language, while generational gaps in technology adoption can further complicate collaboration. Integrating and generalizing AI models within clinical workflows remains a significant challenge due to the diversity of data modalities and the complexity of healthcare settings. The need for model-agnostic approaches is critical to ensure that AI tools are adaptable and effective across different healthcare environments.\nDiscussion: Addressing these interdisciplinary challenges requires strategic partnerships and dedicated educational initiatives. The formation of robust collaborations with clinicians and the investment in continuous interdisciplinary education are essential to bridge knowledge gaps.\nEngaging in direct conversations with clinicians not only helps computer scientists understand the intricacies of medical practice but also allows clinicians to grasp the potential and limitations of AI technologies. Additionally, proactive measures such as attending interdisciplinary conferences provide valuable opportunities for networking and learning from leading experts in both fields (Patel et al., 2024). Conferences often feature workshops and symposiums that specifically focus on integrating AI into healthcare, offering practical insights and fostering collaborations. Similarly, reading interdisciplinary journals and books can enhance understanding. Exposure to each other's work environments can further enhance collaboration. Visits to clinical settings can help computer scientists see firsthand the challenges clinicians face, while visits to tech labs can show clinicians the complexities of developing and training AI models. This mutual exposure not only deepens understanding but also builds trust and respect between the disciplines, paving the way for more effective and empathetic collaborations."}, {"title": "EMBRACING EPISTEMIC HUMILITY AND PLURALITY", "content": "Participants noted the speed at which the field is progressing, making it almost impossible for any single person to be an expert in all aspects of AI research. Incorporating social scientists-particularly those with expertise in feminism, race theory, and ethics\u2014 and patient views into our teams enriches AI research and ensures that marginalized voices are heard. This diversity of expertise helps shape AI systems that are not only more inclusive but also more reflective of the complexity and nuances of human experience. However, despite the critical importance of interdisciplinary perspectives in AI development, even leading universities may be reluctant to recruit multidisciplinary scientists for faculty positions if they lack traditional technical expertise.\nThe discussion also emphasized the role of value judgments in research and the importance of incorporating diverse viewpoints from various cultural backgrounds. One participant shared how working with Indigenous researchers revealed alternative frameworks for ensuring research benefits affected communities through relevance, equity of partnership, and data sovereignty. Epistemic humility and plurality require engaging with others in a way that values their ways of knowing,"}, {"title": "EPISTEMIC JUSTICE AND ALGORITHMIC HUMILITY", "content": "Epistemic injustice and oppression constrain scientific progress by excluding vital perspectives andinsights (Dotson, 2014; Fricker, 2007; Kay et al., 2024). When entire groups face marginalizationfrom knowledge production and scientific discourse, we lose insights that could challenge prevailingassumptions and enrich our understanding. To paraphrase an old adage: \"If everyone is thinkingalike, then no one is really thinking.\" This is where algorithmic humility AI designed to offsethuman arrogance and ignorance comes in. Algorithmic humility is an approach to AI systemdesign that explicitly acknowledges the boundaries of both human and machine intelligence. Unlikeconventional AI approaches, systems built on algorithmic humility principles incorporate mechanismsto actively counteract human overconfidence and uncritical acceptance of automated outputs (Kompaet al., 2021). Through this framework, algorithmic humility seeks to develop AI systems that notonly address human cognitive biases but also resist becoming unquestioned sources of authoritythemselves."}, {"title": "STRUCTURAL BARRIERS TO INCLUSIVE RESEARCH", "content": "Participants identified several systemic barriers to achieving truly inclusive ML research in healthcare. The publish or perish culture continues to incentivize quantity over quality and methodologyover impact. Conference accessibility remains severely limited by visa restrictions with wait timesfor US visa interviews exceeding two years in some countries and prohibitive travel costs (Hut-son, 2018; Owusu-Gyamfi, 2024; Thompson, 2024). Even when researchers from underrepresentedgroups participate, power dynamics can make it challenging for junior scholars to speak up aboutbias and discrimination without risking career advancement. The discussion also highlighted howmajor conferences like NeurIPS, ICML, and ICLR often prioritize the novelty of methodology andlarger datasets over real-world impact, perpetuating existing power structures. Extractive AI prac-tices exacerbate barriers to inclusivity by exploiting the resources and labor of the global majoritywithout contributing to the societies that bear the costs of AI development (Crawford, 2021; Liet al., 2023; Luccioni et al., 2024)."}, {"title": "NOVEL APPROACHES TO MEASURING DISPARITIES", "content": "While much work on bias and fairness has focused on technical approaches, apparent technicalsolutions can mask deeper systemic issues. Participants shared examples of how such solutions canfall short. For example, attempting to debias models for multiple protected attributes simultaneouslycan degrade the model's performance to the point of clinical irrelevance, achieving equality throughuniversal poor performance.\nEnsuring the safe and effective deployment of AI requires continuous evaluation, transparency,and human oversight, with rigorous testing and active involvement of clinicians in decision-making(Adibi et al., 2020; Blumenthal and Patel, 2024). Strong regulation and ongoing model evaluationare key to ensuring AI tools are ethically integrated. Fairness in AI is not about achieving perfectequality but preventing harm to vulnerable populations.\nParticipants highlighted innovative methodologies for identifying and measuring healthcare dis-parities without relying on traditional demographic categories. One innovative approach introducedthe concept of care phenotypes, patterns in care delivery that reveal systematically underserved pa-tient populations. For instance, research demonstrated that the heaviest intubated patients werenot only turned in their beds less frequently than others, but also, inexcusably, received mouth careless often (Weishaupt et al., 2025). This approach revealed systematic disparities in care deliverythat map to social determinants and potentially accounted for intersectionality, without requiring"}, {"title": "EDUCATION AND TRAINING REFORM", "content": "Healthcare systems often lack the expertise and infrastructure for responsible AI deployment. The roundtable highlighted the need to fundamentally reimagine medical and technical education, with concerning findings that patients sometimes perceive chatbots as more empathetic than human doctors (Ayers et al., 2023). Participants emphasized the need to reimagine medical education to nurture empathy and incorporate systems thinking and critical analysis of technology. This includes teaching students to question assumptions, understand local context, and consider long-term impacts rather than just immediate clinical outcomes. A systems-level approach is needed to integrate epistemic humility into AI and medical curricula. Empathy and interdisciplinary collaboration should be prioritized to equip students with the tools to address the ethical complexities of AI and its intersection with healthcare."}, {"title": "OPTIMISM AND CALL FOR ACTION", "content": "The discussion yielded several concrete recommendations, including expanding conference accessibility through virtual participation options and events in diverse locations and languages, advocating for diverse representation in research teams and paper authorship, developing new frameworks for clinical validation that account for local context and temporal shifts in care patterns, creating dedicated courses on epistemic humility and plurality in computer science, reforming publication incentives to value impact and ethical considerations over methodological novelty, and building research teams that include social scientists and bioethicists and incorporate patient perspectives from project inception.\nWhile acknowledging the magnitude of needed changes, participants expressed optimism about growing awareness and willingness to confront bias in the field. They emphasized that progress requires moving these conversations from the margins to the center, with everyone taking responsibility for change within their sphere of influence. The discussion highlighted how even those early in their careers can make meaningful contributions, citing examples of PhD students choosing to focus on bias and fairness despite institutional pressure to pursue more traditional technical topics. Success will require sustained effort to reimagine education, research, and clinical validation while maintaining focus on benefits to vulnerable populations."}, {"title": "Clinician-AI Interaction", "content": "Subtopic: What are the primary use cases of AI tools in clinicians' work? What are challenges, limitations, and concerns of adopting AI tools in clinical workflows? What are the lessons learned from evaluations of Clinician-AI interactions?\nChairs: Shannon McWeeney, Yuan Pu, and Shannon Zejiang Shen\nBackground: The development of many clinical AI tools is predominantly driven by the model creators: the AI researchers identify the underlying machine learning problem and build competent models with good accuracy in such tasks. However, it is often an afterthought how clinicians can interact and collaborate with these powerful but imperfect AI models in practice, and the efficacy of such AI systems is often evaluated in isolation from clinical workflows (Yang et al., 2019). In this roundtable discussion, we examine clinician-AI interactions in real-world deployments. With a cross-disciplinary audience of around 10 clinicians and 20 computer science/AI researchers, we identify both gaps and opportunities for building effective human-AI collaboration systems in clinical settings."}, {"title": "USE CASES OF AI IN CLINICAL WORKFLOW", "content": "A wide range of AI support is developed across different stages of clinical practice, from streamlining operational workflows to supporting complex clinical decisions (Zaj\u0105c et al., 2023). The participants discussed several imminent AI applications for automating routine tasks like patient check-ins, prescription management, and payment processing. Beyond these operational improvements, AI systems have demonstrated promising capabilities in information synthesis, for example generating patient history summaries (Keszthelyi et al., 2023) and explainable X-ray reports (Deperlioglu et al., 2022). Moreover, recent research shared during the discussion has explored both AI-powered diagnostic predictions (Jabbour et al., 2023) and LLM-based chatbots that help interpret risk scores and facilitate access to relevant medical guidelines (Chan et al., 2023; Rajashekar et al., 2024).\nDifferent modes of Clinician-AI interactions were discussed: ambient, passive, and proactive. Ambient systems, such as automated scribing tools, work continuously in the background to capture and organize patient-clinician interactions, freeing healthcare providers to focus fully on patient care (Tierney et al., 2024). Passive support comes in the form of on-demand tools like chatbots, which clinicians can consult to help navigate complex medical records and surface critical information for decision-making. The third category, proactive support, represents systems that actively monitor clinical workflows and autonomously surface relevant information and recommendations to clinicians (Jiang et al., 2023), enhancing their work without adding cognitive burden."}, {"title": "CHALLENGES", "content": "The roundtable discussion explored both the challenges and opportunities in fostering effective clinician-AI interactions. Participants highlighted clinicians' mistrust of AI and their misunder-standing of its applications as significant barriers. They emphasized that addressing these issues will require collaboration and thoughtful design to build trust and enhance the integration of AI into clinical practice.\nSocio-technological challenges in practical AI deployment Roundtable participants frequently mentioned that clinicians often show reluctance to adopt AI predictions, even in scenarios where AI demonstrates high accuracy. For instance, in a study shared during the session, clinicians disregarded 20% of AI-generated predictions, even when those predictions were 100% accurate (Jabbour et al., 2023). The group suggested that this hesitancy may stem from clinicians' strong preference for relying on their professional judgment developed through extensive training and hands-on experience. Resistance to revising decisions based on AI recommendations was further linked to cognitive dissonance. As several participants observed, healthcare professionals often experience notable discomfort when faced with contradictory opinions, whether from human peers or AI systems. Accepting such recommendations may be perceived as admitting an error, creating tension with a clinician's professional self-image and confidence. This psychological barrier can lead to an unconscious dismissal of the Al's inputs, even when the evidence strongly supports them.\nTrustworthiness of the model outputs Concerns about accuracy, accountability, and ethics also undermine trust (Hengstler et al., 2016). Clinicians frequently perceive AI systems as \"black boxes,\" making it difficult to understand and trust their recommendations. Ethical issues, such as potential biases in AI models and the privacy of patient data, further complicate this trust dynamic.\nFor example, systems that use heatmaps (or activation maps) to explain model predictions for medical imaging often fail to present reasoning in an intuitive and easy-to-understand way for clinicians without AI background. Clinicians often develop their own interpretations of the outputs, increasing their cognitive burden and reducing confidence in the tools. Participants also notes that it is very hard to identify the occasional errors in the AI ambient scribing system, thus making it"}, {"title": "OPPORTUNITIES", "content": "Towards holistic evaluation at the model development stage Going beyond only evaluating competence of the models, participants emphasized the need to capture multiple dimensions that reflect real-world user needs in clinical settings. For example, when evaluating AI generated X-ray reports, rather than just assessing the \"accuracy\" through lexical overlap between the generation and reference doctor's writing, one should consider metrics that directly address physicians' information needs. These include assessments of readability, completeness of critical findings, and the cognitive effort required to locate decision-relevant information. Through collecting first-hand subjective feedback from clinicians users, the developers can better understand potential challenges and limitations of the AI systems in real-world settings, and iteratively refine the models to better align with clinical workflows.\nRethinking the mechanisms of using AI Participants also suggested a reframing of how AI systems are positioned within clinical use cases. For example, rather than providing definitive answers, AI systems could be designed to prompt clinicians to engage in deeper analytical thinking (system 2 reasoning) - they can prompt the clinicians to think step by step and highlight potentially overlooked patterns or suggesting alternative diagnostic considerations. This approach aligns with how clinicians naturally collaborate with colleagues, where discussion and collective reasoning often lead to better outcomes than isolated decision-making. The group also mentioned that incorporating AI education into medical training can help future clinicians develop a nuanced understanding of both the capabilities and limitations of AI tools."}, {"title": "Drug Discovery and Development", "content": "Subtopic: Integrating Foundation Models: How can foundation models streamline specific stages like target identification, lead optimization, and clinical candidate selection, and what measurable impacts have been observed? Mitigating Risks in AI Integration: What strategies address risks like data bias, overfitting, or lack of interpretability when integrating AI into drug discovery pipelines? Discovering Novel Biological Knowledge: How have ML models uncovered new biological mechanisms or pathways, and what are the key challenges in simulating complex processes like protein-ligand interactions? Enhancing Chemical Design and Synthesis: How can AI improve understanding of computational compound generation/modification and synthesis planning to accelerate the design of novel, drug-like molecules?\nChairs: Michael Craig, Zuheng Xu, and Geoffrey Woollard\nBackground: AI and ML continue to rapidly advancing in drug discovery and development, accompanied by growing biological and chemical data and improved computational power. AI/ML is increasingly present in science broadly considered in terms of perspectives, language, tools, and community (both research and business). Computational biology, broadly considered, has become increasingly integrated with AI/ML communities in statistical inference, simulation, high perfor-"}, {"title": "STREAMLINING DRUG DISCOVERY", "content": "The roundtable discussion touched on leveraging self-supervised learning, large-scale imaging, and high-dimensional phenotype modeling to decode biological systems and automate workflows. Participants opined that while measurable impacts include faster hit identification and integration of"}, {"title": "MITIGATING AI INTEGRATION RISKS", "content": "Participants noted risks like data bias, lack of interpretability, and slow validation cycles in drug discovery. Strategies to mitigate these include cross-disciplinary training to improve communication between machine learning practitioners and domain scientists, company culture surrounding decision making and risk aversion, using robust evaluation metrics, and adopting active learning approaches for iterative validation. The slow feedback loops in biological experiments amplify the difficulty, highlighting the need for well-grounded models and transparency in decision-making, and active learning perspectives like Bayesian optimization, where the cost of obtaining new data is itself modeled to optimize exploit/explore tradeoffs (Garnett, 2022)."}, {"title": "DISCOVERING NOVEL BIOLOGICAL KNOWLEDGE", "content": "Participants commented on to what extent AI/ML has revealed new mechanisms, such as gene-gene or gene-drug interactions, from image and transcriptomics data. Digital twin models were highlighted as promising for simulating patient-level drug responses, albeit limited by challenges in verification and cross-scale integration. Simulating complex biophysical mechanisms at multiple scales, like how protein-ligand dynamics gives rise to cellular states, seems computationally intractable due to chaotic multi-scale interactions familiar to computational fluid dynamics. Despite this, this area is receiving attention, for example from the Chan Zuckerberg Institute for Advanced Biological Imaging's cryogenic electron tomography community data challenge (Harrington et al., 2024) on a highly visible online platform. Participants discussed the pros and cons for hybrid approaches combining traditional physics-based methods with data-driven simulations and observations to advance understanding and do inference, and noted the NeurIPS 2024 workshop on Data-driven and Differentiable Simulations, Surrogates, and Solvers (https://d3s3workshop.github.io/)."}, {"title": "\u0395\u039dNHANCING CHEMICAL DESIGN AND SYNTHESIS", "content": "Participants offered anecdotes on to what extent AI/ML aids in computational compound generation and synthesis planning by predicting properties like solubility and toxicity, and to what extent this informs decision making criteria of domain expert experimentalists like medicinal chemists. Integration of simulation frameworks, akin to those in aerodynamics or chip design, was suggested to enable more real-time decision around drug prototypes. Furthermore, some natural language information like synthesis or even economic/fiscal aspects of regulatory reimbursement could perhaps benefit from large language model approaches."}, {"title": "BRIDGING SCIENCE AND STRATEGY", "content": "The discussion emphasized fostering collaboration across disciplines and industries. Much of the discussion was taken up by anecdotal knowledge of how teams work together in different settings (startups, contract research organizations, clinical research centers, academic biomedical research institutes, internal consulting teams in large pharmaceutical companies). Participants appreciated learning from each other in a way that is not formally taught but typically gained through job experience and networking at meetings such as this one."}, {"title": "FUTURE OUTLOOK", "content": "Participants suggested truly AI-native information-first teams either as an independent startup, or within larger pharmaceutical companies could accelerate progress with their agility and organization. Addressing cultural inertia and building credibility in AI/ML predictions were identified as crucial for broader adoption. Who will be the first to demonstrate never before seen success, which will drive cultural change within scientific teams?"}, {"title": "Social AI and Healthcare", "content": "As artificial intelligence continues to evolve, its applications in healthcare have emerged as one of the most promising avenues for improving patient outcomes. Social AI, which focuses on understanding and interacting with human behavior, emotions, and social contexts, offers transformative potential for both mental and physical health interventions. During this roundtable participants explored critical questions on the development of Social AI for healthcare: How can ML-powered models of human social behavior improve the diagnosis and treatment of mental health conditions like depression, anxiety, and post-traumatic stress disorder (PTSD)? How do we combine and balance longitudinal history data and short-term behaviors for the modeling? How can Social AI enable real-time monitoring of patients' emotional and social health, both clinically and through wearable devices? How can Social AI tools adapt to individual patients' social and cultural contexts for equitable and effective care across diverse populations? How can models strike a balance between personalization and generalization for similar patients, benefiting from insights drawn from comparable cases?\nChairs: James M. Rehg, Yuwei Zhang, and Yurui Cao\nBackground: Advances in Machine Learning have led to the development of models capable of describing various aspects of human social communication and behavior. These technological advancements hold significant potential for revolutionizing healthcare by introducing innovative methods for diagnosing and treating mental and developmental conditions, ultimately improving patient outcomes.\nIn the realm of mental health, AI's ability to model human behavior has been particularly impactful. Emerging tools now enable pre-diagnosis screening and risk assessment, offering insights into an individual's predisposition to mental illnesses. For example, AI systems can analyze social media interactions to detect early signs of mental health disorders. Additionally, real-time monitoring through wearable devices and smart technologies enables continuous tracking of physiological and behavioral data, facilitating timely interventions while respecting patient autonomy.\nDespite the promising applications, the integration of Social AI into healthcare presents challenges. Designing systems that adapt to individual patient needs and preferences is essential for effective care. Yet, personalization must be carefully balanced with concerns about bias and privacy to ensure equitable and secure healthcare delivery. Furthermore, incorporating social determinants of health into AI systems adds complexity, emphasizing the need for multidisciplinary approaches to ensure that these tools are both effective and inclusive."}, {"title": "HUMAN BEHAVIOR MODELING FOR MENTAL HEALTH", "content": "The discussion began with an example application focused on understanding textual communications between partners to track the frequency of conflicts. Approaches brought up for modeling the mental health in this context include calculating manually defined and statistically derived metadata features, extracting text embeddings using pretrained embedding models, as well as using modern large language models. Traditional ML approaches and LLMs could be used in conjunction to address the overall goal.\nOne persistent challenge in this area is inferring informative states that cannot be directly measured. While conditions like diabetes offer measurable indicators, mental health presents more difficulty in identifying clear, informative measures. This uncertainty underscores the challenge of determining what can be measured to accurately assess mental health, making it a key issue in the development of Social AI tools for healthcare applications."}, {"title": "AI AND CHATBOTS FOR MENTAL HEALTH SUPPORT", "content": "AI models, such as ChatGPT, are increasingly being used informally as therapeutic tools, where individuals engage in conversations to express frustrations and emotional experiences. These interactions often elicit supportive and empathetic responses from the AI, which may seem overly idealized compared to human responses. While might be perceived as unusual initially, this practice highlights the non-judgmental nature of AI, which may appeal to users seeking a space free from human judgment. This raises questions about the potential role of AI in mental health support, especially in terms of its emotional neutrality and its capacity to provide a comforting, non-critical outlet for users.\nWhile AI models like ChatGPT can provide supportive responses, they face limitations in recognizing and addressing urgent or complex situations. For example, if a patient experiences experiences financial misconduct, a human therapist might offer practical guidance, such as advising legal action. In contrast, AI may respond with general empathy and suggest continuing the conversation, failing to recognize the immediate need for legal intervention or other urgent support. This issue is compounded by safety concerns, as research has"}]}