{"title": "Controlling for Unobserved Confounding with Large Language Model Classification of Patient Smoking Status", "authors": ["Samuel Lee", "Zach Wood-Doughty"], "abstract": "Causal understanding is a fundamental goal of evidence-based medicine. When randomization is impossible, causal inference methods allow the estimation of treatment effects from retrospective analysis of observational data. However, such analyses rely on a number of assumptions, often including that of no unobserved confounding. In many practical settings, this assumption is violated when important variables are not explicitly measured in the clinical record. Prior work has proposed to address unobserved confounding with machine learning by imputing un- observed variables and then correcting for the classifier's mismeasurement. When such a classifier can be trained and the necessary assumptions are met, this method can recover an unbiased estimate of a causal effect. However, such work has been limited to synthetic data, simple classifiers, and binary variables. This paper extends this methodology by using a large language model trained on clinical notes to predict patients' smoking status, which would otherwise be unobserved. We then apply a measurement error correction on the categorical predicted smoking status to estimate the causal effect of transthoracic echocardiography on mortality in the MIMIC dataset.", "sections": [{"title": "Introduction", "content": "Evidence-based medicine seeks to use data from clinical research to inform best practices [Sackett et al., 1996, Masic et al., 2008]. While randomized control trials (RCTs) are the gold standard of clinical research, randomization is often impossible or unethical [Sanson-Fisher et al., 2007]. In the absence of randomization, retrospective causal analyses must control for con- founding variables that could influence both a patient's treatment assignment and their outcome [VanderWeele and Shpitser, 2013]. For example, if a certain medication is only given to the patients with the highest risk, a naive correlational analysis may suggest that the medication itself increases the risk of mortality. Causal inference methods establish causal claims by controlling for confound- ing and other sources of bias [Pearl, 2009].\nAll causal inference methods rely on assumptions about the underlying data-generating process; a common but untestable assumption is the absence of unobserved confounding [Groenwold et al., 2009]. If a confounding variable is unobserved, it is in general impossible to estimate the desired causal effect without bias [Shpitser and Pearl, 2006]. Even with the wide availability of electronic health record (EHR) data, it may be difficult to rule out the possibility of unobserved confounding [Groenwold et al., 2008]. Many causal methods attempt to detect and mitigate unobserved confound- ing [Flanders et al., 2011, Baiocchi et al., 2014, Uddin et al., 2016, Tchetgen et al., 2020]."}, {"title": "", "content": "One such methodological approach considers the case where a necessary confounder is unobserved, but a proxy for that variable is instead observed. A canonical example is Medicaid enrollment as a proxy for socioeconomic status (SES), because Medicaid eligibility is tied to household in- come [Simpson, 2020]. While a naive approach of treating SES and Medicaid enrollment as equiva- lent variables could introduce bias [Greenland and Robins, 1985, Ogburn and VanderWeele, 2012], methods developed by Pearl [2010] and Kuroki and Pearl [2014] can recover unbiased causal es- timates through a measurement error correction. When such a proxy (e.g., Medicaid enrollment) is unavailable, one option may be to train a machine learning (ML) classifier to predict the values of the unobserved confounder and then apply this measurement error correction to account for the classifier's errors [Wood-Doughty et al., 2020].\nThis paper builds upon this line of work, extending prior methods from simple classifiers and syn- thetic data to large language models (LLMs) and two real-world datasets. Following Feng et al. [2018], we analyze the causal effect of transthoracic echocardiography (TTE) on 28-day mortality among ICU patients diagnosed with sepsis. A limitation of prior work is its inability to account for patient smoking status; this variable is not explicitly recorded in the EHR data, but tobacco use is considered a risk factor for mortality among patients with sepsis [Alroumi et al., 2018, Zhang et al., 2022]. Smoking status could also plausibly affect a clinician's decision to administer TTE, which would make it an unobserved confounder. We train and apply an ML classifier to predict patients' smoking status and estimate the model's error rate [Mulyar et al., 2019]. We extend the measure- ment error correction to categorical variables and use it to debias our causal estimates. Our method produces estimates with modest variance and our results provide evidence that TTE prevents mortal- ity, with or without accounting for patient smoking status.\nThis primary contribution of this paper is a methodology for imputing unmeasured variables with large language models and then estimating unbiased causal effects with a measurement error correc- tion. We release our code to enable future research."}, {"title": "Related Work", "content": ""}, {"title": "ML and LLMs for Clinical Prediction", "content": "Machine learning (ML) methods have advanced over the past decade to provide impressive real- world performance in a wide variety of domains. For text data, large language models (LLMs) have become the dominant paradigm for text generation and classification tasks [Zhou et al., 2023]. The most common approach for training LLMs uses generative pretraining followed by supervised task- specific fine-tuning [Radford et al., 2018]. In the clinical domain, LLMs have been applied to a wide variety of tasks, such as 30-day all-cause readmission prediction, in-hospital mortality prediction, co- morbidity index prediction, length of stay prediction, and insurance denial prediction [Jiang et al., 2023]. While such methods can demonstrate state-of-the-art performance on a variety of datasets, their applications to real-world clinical practice have been somewhat limited [Chen and Asch, 2017]. Such models are complicated and often uninterpretable, which may prevent clinicians from trusting the reasoning underlying their predictions [Tonekaboni et al., 2019]. Additionally, test set evalua- tions may not be fully indicative of real-world performance in the presence of domain shift and other sources of bias [Subbaswamy and Saria, 2020, Finlayson et al., 2021].\nWe focus on the specific task of predicting patient smoking status from clinical notes, using a la- beled dataset released by Uzuner et al. [2008]. Variations on this task have been a sustained focus of ML applications because of the relevance of tobacco use to a wide variety of health outcomes [Sohn and Savova, 2009, Palmer et al., 2019]."}, {"title": "Causal Inference", "content": "Whereas practical applications of ML methods to the clinical domain are a relatively recent phe- nomenon [Jiang et al., 2023], many of the foundational methods in causal inference were developed to enable epidemiological studies [Pearl, 2009]. In this work, we use the potential outcomes frame- work that explores the causal effect of a treatment X on an outcome Y: specifically, following Feng et al. [2018], the causal effect of TTE on 28-day mortality among patients with sepsis. We use causal do-notation p(Y | do(x)) to denote the distribution of a patient's hypothetical mortality had they be randomly assigned TTE value x [Pearl, 2012]. In an RCT, E[Y | X = x] is equivalent to E[Y | do(x)] because correlation between the randomly-assigned treatment and outcome implies (in probability) a causal effect [Pearl, 2009]. Because in our dataset TTE was not randomly assigned, we need additional assumptions to identify the counterfactual distribution E[Y | do(x)] from the observed data.\nWe use directed acyclic graphs (DAGs) to represent our assumptions about the data [Pearl, 1995]. Figure 1 shows the causal DAG for our problem. In addition to our treatment X and outcome Y, C represents our vector of observed confounders, including patient age, gender, and several clinical markers. U is smoking status, which is hypothesized as a possible confounder but is unobserved in our data. U*, introduced in \u00a72.3, is our noisy proxy.\nBoth our treatment and outcomes are binary variables, and our causal estimands of interest are the risk ratio and odds ratio, equations (1) and (2) below, respectively [Hernan and Robins, 2020]:\nRR:  $\\frac{p(Y = 1 | do(x = 1))}{p(Y= 1 | do(x=0))}$  (1)\nOR: $\\frac{p(Y= 1 | do(x = 1))p(Y=0|do(x=0))}{p(Y=0 | do(x = 1))p(Y=1|do(x=0))}$ (2)"}, {"title": "Measurement Error", "content": "If all confounders C and U were observed, then p(Y | do(x)) is identified as we show in Equation 10 below \u2013 and can be estimated in a number of ways [Glynn and Quinn, 2010]. If U is unobserved, however, then the causal effect of X on Y is unidentified and the problem becomes impossible without additional assumptions [Shpitser and Pearl, 2006]. We assume access to U*, a noisy proxy with the same cardinality as U that is governed by some noisy relationship p(U* | U). If a matrix representation of this probability p(U* | U) can be inverted, then the work of Pearl [2010] and Kuroki and Pearl [2014] show that we can recover p(Y | do(x)). This method is known as and alternatively referred to as 'matrix adjustment' or 'effect restoration' [Kuroki and Pearl, 2014]; we demonstrate this derivation in \u00a73.2. Throughout this paper, following \u00a73.1, we assume that U* is predicted by an LLM classifier and that p(U* | U) is the error rate of the classification. Our method assumes no additional unobserved confounding other than U.\nAn alternative to matrix adjustment methods is simulation-extrapolation (SIMEX), a widely-used method for measurement error [Cook and Stefanski, 1994]. SIMEX generally requires strong para- metric assumptions about the data [Sevilimedu and Yu, 2022], prior knowledge of the known mea- surement error variance [Cook and Stefanski, 1994], and the unknown confounder to be continuous [Lederer and K\u00fcchenhoff, 2006]. To overcome these limitations, K\u00fcchenhoff et al. [2005] proposed MC-SIMEX which avoids parametric assumptions but has weaker theoretical foundations. While we focus on developing matrix correction methods and leave a comprehensive survey of SIMEX methods for other work [Sevilimedu and Yu, 2022], we compare our proposed methodology against MC SIMEX in our experiments."}, {"title": "Methodology", "content": "We divide our methodological approach into two sections. First, we discuss the classifier we use to predict smoking status for patients in the MIMIC dataset. We discuss the model's training and validation procedure and its classification of our data. Then, we incorporate our model's predictions and our estimate of its error rate into the measurement error formulation to estimate causal effects. We discuss our extensions of past work to categorical mismeasured variables and high-dimensional observed confounders. Figure 2 shows the high-level overview of our entire methodology, including the pretrained LLMs and most important data sources."}, {"title": "Smoking Status Classifier", "content": "Our modeling approach most closely follows that of Mulyar et al. [2019]. We start with a Clin- icalBERT LLM architecture using the weights that were released by Alsentzer et al. [2019] after pretraining on PubMed abstracts, PMC articles, and all MIMIC-III notes [Alsentzer et al., 2019, Lee et al., 2019]. We then adapt this LLM by training an LSTM on top of its representation to pre- dict categorical smoking status in the 2006 n2c2 smoking dataset [Uzuner et al., 2008]. The training dataset contains 398 clinical notes and corresponding labels of the patient as either past smoker, cur- rent smoker, non-smoker, and unknown. These labels were manually annotated by pulmonologists. We follow the hyperparameter choices of Mulyar et al. [2019] and train for 1,000 epochs.\nAfter training, we evaluate our model on the test set of 101 additional patient records. Our model achieves 86.1% accuracy with the confusion matrix shown in Table 1. This confusion matrix, after dividing by the row sums to produce a stochastic matrix, provides the error-rate matrix p(U* | U) that will be used in our measurement error correction in the following section.\nTo apply this model to patients in the MIMIC dataset, we first preprocess that data using the MIMIC- EXTRACT approach [Wang et al., 2020]. We then follow the preprocessing steps of Feng et al. [2018] to enable a direct comparison against their analysis. Our trained classifier is then used to predict smoking status for the patients in the evaluation set of MIMIC patients who met the criteria for sepsis [Angus et al., 2016]. Of those, 2,058 were classified as past smoker, 93 as current smoker, 1,413 as non-smoker, and 1,171 as unknown. We discard 64 patients for whom the classifier did not make a prediction due to a confidence threshold set by Mulyar et al. [2019]. Specifically, the model makes no prediction if its final tanh activation outputs a negative logit value for all four classes."}, {"title": "Matrix Adjustment", "content": "We can now incorporate our smoking status predictions into our estimate of the causal effect of TTE on 28-day mortality. Figure 1 portrays our assumptions about the underlying data distribution. Our treatment X is TTE, our outcome Y is 28-day mortality, and C is a vector of 39 observed covariates. Patient smoking status U is our unobserved confounder, and U* is our classifier's prediction. Given this notation, we have data on the joint probability p(X, Y, C, U*) from patients in MIMIC. We additionally have the error rate P(U* | U) from our classifiers validation on the n2c2 dataset. We assume that the classifier's error rate remains constant between the two datasets; future work could relax this assumption by collecting additional ground-truth annotations on the MIMIC dataset. We further make a non-differential error assumption, that:\np(U* | U, X, Y, C') = P(U* | U) (3)\nBecause our smoking status is a categorical variable with four values, our error rate distribution p(U* | U) can be represented as a 4x4 matrix M(U*, U) computed from the confusion matrix in Table 1 by dividing each cell by its corresponding row sum. In general, using ui to denote U = i and $u_j^*$ to denote $U^* = j$, this can be written as:\n\nWe can use M(U*, U) to write our observed data distribution as a function of the underlying distri- bution and our error rate p(U* | U).\np(Y, X, C, U*)=\\sum_{U}p(U^* | Y, X, C,U)p(Y, X, C,U) (4)\n=\\sum_{U}p(U^* | U)p(Y, X,C,U) (5)\n= M(U^*,U) \\times p(Y, X, C, U) (6)\nEquation 4 holds by rules of probability, (5) holds by (3) above, and (6) is rewritten to formulate this summation over probability distributions as a matrix multiplication. This follows the approach of Pearl [2010]; for fixed values of Y, X, C, p(Y, X, C, U) is a vector of four values. We then multiply that vector by our 4x4 error matrix M(U*, U) to get a different vector of four values.\nAssuming M(U*, U) has an inverse I(U*, U), we then have:\np(Y, X, C, U) = I(U^*,U) \\times p(Y, X, C, U^*) (7)\nThis inverse matrix I(U*,U) allows us, given p(Y,X,C,U*) and M(U*,U), to recover p(Y, X, C,U). This derivation follows those presented in prior work by Pearl [2010] and Kuroki and Pearl [2014]. To move from p(Y, X, C,U) to our counterfactual p(Y | do(x)), we can follow a standard do-calculus derivation:"}, {"title": "", "content": "p(Y | do(x)) = \\sum_{C,U}p(Y | do(x), C, U)p(C, U | do(x)) (8)\n=\\sum_{C,U}p(Y | X = x, C,U)p(C, U | do(x)) (9)\n=\\sum_{C,U}p(Y | X = x,C,U)p(C,U) (10)\nEquation 8 hold by marginalization and chain rule, (9) holds by Rule 2 of do-calculus, and (10) holds by Rule 3 of do-calculus [Pearl, 2012]. However, the relative simplicity of the proof that the counterfactual is identified obscures some of the practical challenges of estimating causal effects in our specific domain application. To implement estimators for the risk and odds ratios requires fitting models to these probability distributions. Unlike for example in Wood-Doughty et al. [2020] where there is a single binary observed confounder, we have 39 covariates.\nAs Equation 10 is a function of p(Y | X,C,U) and p(C,U), we can first use marginalization and conditioning to derive these from p(Y, X, C, U) as provided to us by Equation 7. To actually compute Equation 7, we fit three models: logistic regressions for p(Y | X, C, U*) and p(X | C), and a multinomial logit model p(U* | X, C'). All models are fit using the predicted smoking statuses (i.e., using U*, not on U); we perform the measurement error correction after fitting models.\np(Y,X,U | C) = I(U^*,U)p(Y,X,U^* | C) (11)\n= I(U^*,U)p(Y|X,U^*,C)p(U^* | X,C)p(X|C)\nBecause C is high-dimensional, we avoid modeling it and use the empirical approximation 1/N.\np(U | C) = \\sum_{X}p(U, X | C) (12)\n=\\sum_{X}I(U^*,U)p(U^*, X | C)\n=\\sum_{X}I(U^*, U)p(U^* | X, C)p(X | C)\n\\frac{1}{N}\\sum_{i=1}^{N}p(U | C_i) (13)\nWhile Equation 11 defines p(Y, X, U | C), we can easily transform it to calculate the conditional probability we need for our counterfactual.\np(Y | X, C,U) =\\frac{p(Y, X,U | C)}{\\sum_{y}p(Y = y, X,U | C)} (14)\nThen, plugging Equations 14 and 13 into (10), we have written our counterfactual p(Y | do(x)) as a function of our observed data distribution. We can plug that counterfactual into the definitions of Equations 1 and 2 to estimate our causal effects."}, {"title": "MC-SIMEX", "content": "Following our discussion in \u00a72.3, we compare our proposed matrix adjustment method against MC-SIMEX, another common method for handling measurement error. While the original SIMEX method was designed for continuous unobserved variables, MC-SIMEX allows for categorical vari- ables [Lederer and K\u00fcchenhoff, 2006]. We use the R package released by the authors. As indicated in Figure 2, SIMEX also requires access to both the noisy imputed U* values and our estimate of the error rate p(U* | U). Despite the similarities of these methods, our matrix adjustment approach has the benefit of more established theoretical foundations, and makes no assumptions about the parametric form of the estimator. It can also be easily applied to more complicated counterfactual estimands where the identifying functional of the observed data is not simply a linear model of the treatment and covariates, e.g., a front-door or proximal estimator [Bellemare et al., 2020, Cui et al., 2023]."}, {"title": "Results", "content": "Our experimental analysis follows Figure 2 in applying both matrix adjustment and MC-SIMEX to the MIMIC-III dataset as preprocessed and analyzed by Feng et al. [2018]. We use our trained LLM classifier to impute patients' smoking status, fit our models for p(Y | X, U*, C), p(U* | X, C), and p(X | C), and then incorporate those models and our estimated error rates p(U* | U) to produce our causal estimates. Our full experimental code is provided as an appendix.\nTable 2 shows a direct comparison of our odds ratio estimates against the five separate estimation methods presented by Feng et al. [2018]. These numbers being less than 1 indicates our treatment re- duces the probability of the outcome \u2013 that is, TTE prevents mortality among the patient population. Our results match those of past work, but produce more conservative estimates of the causal effect. This comparison cannot prove or refute the presence of unobserved confounding via smoking sta- tus; it merely shows that when taking smoking status into account, we produce a more conservative estimate of the protective effect of TTE.\nIf our assumptions from \u00a72 and 3 are valid, our methods should produce unbiased estimates of the causal effects. However, with any finite sample data sample, we must worry about the variance of our methodology. Feng et al. [2018] also provide (Wald-type) confidence intervals for their analyses. To produce the confidence intervals for our methods shown in Table 3, we use the non-parametric bootstrap [DiCiccio and Efron, 1996]. Bootstrapping our entire analysis from LLM pretraining to effect estimation is unfortunately prohibitively expensive; we instead use two bootstrap approaches that can give us a full picture of our method's uncertainty.\nFirst, we consider resampling our evaluation subset of MIMIC-III to produce 100 new datasets of 4,735 patients, with each patient in each new dataset chosen independently with replacement from the original dataset. For each of these datasets, we fit new models for p(Y | X,U*,C), p(U* | X, C), and p(X | C). For each of these analysis, we hold fixed the estimate for M(U*, U) computed with the n2c2 test dataset. The 95% confidence interval from this bootstrap alone is shown in the 'MIMIC' row of Table 3.\nOur second, orthogonal bootstrapping method follows Wood-Doughty et al. [2020]. Rather than resampling our analysis dataset of MIMIC patients, we resample the test set of 101 patients in the n2c2 data. For each of 100 such resamplings, we recalculate our error matrix M(U*, U), invert it to produce I (U*, U), and then use that in Equation 11 and (13) to compute our causal effects. Each recalculated M(U*, U) is used alongside the original MIMIC dataset of 4,735 patients. The 95% confidence intervals from these 100 resamplings is shown in the 'n2c2' row of Table 3.\nThe 'Both' row of the table shows a combination of both bootstrap methods. We resample 10 new M(U*, U) matrices and 10 new MIMIC datasets on which we fit our models. Then, for all 10 \u00d7 10 = 100 comparisons, we compute our final estimates of the risk and odds ratios. This gives us 100 resamplings, matching the MIMIC-only and n2c2-only bootstrap intervals. The confidence intervals we showed previously in Table 2 use this combined bootstrap method, as it has the widest interval of the three.\nTable 3 allows us to highlight and compare two sources of uncertainty in our method. For the matrix adjustment method, it is unsurprising that the n2c2 bootstrap introduces more uncertainty into our final estimates, as the test set contains only 101 patients. The matrix inverse we compute in Equation 7 and use in (11) and (12) implicitly requires us to divide by values in our estimated M(U*,U) matrix. As with inverse propensity weighting, this can introduce high variance when dividing by small numbers [Ma and Wang, 2020]. We could reduce some of this uncertainty by using a larger dataset to validate the smoking status classifier. Interestingly, our MC-SIMEX estimates have much less variance when bootstrapping the n2c2 test set. This may be related to the lack of theoretical foundation to explain valid estimates for standard errors in simulations [Lederer and K\u00fcchenhoff, 2006]. None of our current methods are able to quantify the uncertainty introduced from the BERT pretraining or classifier fine-tuning. While there are some methods that can capture uncertainty in neural networks (e.g., Gal and Ghahramani [2016]), we leave further sensitivity analyses and uncertainty quantification for future work."}, {"title": "", "content": "Table 4 shows our risk ratio broken down across our four subgroups of smoking statuses. While the risk and odds ratios are closely related, the former is collapsible, so the risk ratio of the entire population is the weighted average of the risk ratios within each subgroup [Hernan and Robins, 2020]. This allows us to easily ask whether TTE is more or less effective among patients with different smoking statuses. For the matrix adjustment method, for all categories except current smokers, the confidence intervals in this subgroup analysis widen and several intervals contain 1.0, suggesting we lack the requisite data for such focused analyses. For non-smokers the point estimate of our risk ratio is above 1.0, which indicates that TTE is in fact harmful; while this could warrant further study, the wide intervals suggest this may be due to random variability within the small sample size. Across both methods, an analysis of the coefficients in our p(Y | X, U*, C) models for the smoking status variable indicates that non-smokers have the best overall odds of survival, which aligns with past investigations of the connection between tobacco use and mortality among sepsis patients [Alroumi et al., 2018, Zhang et al., 2022]. An unlikely but alternative explanation for the risk ratio of 1.13 could additional unobserved confounding if, hypothetically, non-smoker patients are only administered TTE if they are at higher risk of mortality due to a factor not captured by our observed covariates X.\nIn our matrix adjustment subgroup analysis for current smokers, our methods produce a risk ra- tio confidence interval that has almost zero width. This is somewhat unintuitive, especially as our MIMIC classifier predicts only 93 current smokers. We believe this is a consequence of our test set confusion matrix from Table 1; there are no examples in which the model falsely predicts a current smoker. Because we do not smooth this confusion matrix, p(U* = Current | U \u2260 Current) = 0 which means that our matrix adjustment represented by Equation 7 has relatively little effect. This helps explain the zero-width interval for the n2c2 bootstrap. Homogeneity amongst the patients clas- sified as current smokers could possibly explain the lack of variability from the MIMIC bootstrap.\nThe subgroup analyses for MC-SIMEX show an overall narrowing of confidence intervals, despite the reduced sample sizes. As in Table 3, we see very narrow intervals especially for the n2c2 boot- strap. The average width of MC-SIMEX n2c2 bootstrap intervals is 0.05, which is rounded down to 0 with only two significant digits. We leave for future work simulation studies and theoretical analyses that could better explore the unexpectedly low variance of this method."}, {"title": "Conclusions and Future Work", "content": "Our method can control for unobserved confounding using an LLM classifier and a measurement error correction. We have applied this method to estimate a causal effect with implications for clinical care, accounting for a possible violation of the assumptions of past work. Our overall results that TTE protects against mortality \u2013 comport with prior work and do not suggest any particular changes to the standards of clinical care. However, our method more broadly offers a straightforward approach to predicting unobserved confounders from clinical notes, correcting for measurement error, and controlling for confounding. As unobserved confounding is an untestable assumption on which all non-randomized causal analyses rely, our method demonstrates a widely-applicable method for exploring the impact of possible confounding. For any analysis of clinical data where physicians' notes and corresponding classifiers are available, our method can be used to control unobserved confounding or as a sensitivity analysis to test the robustness of the results.\nOur method relies on a number of important assumptions, and future work could explore relaxing them. First, our choice of datasets and smoking status requires us to validate the test accuracy of our LLM classifier on the n2c2 dataset and to assume that the classifier's performance is the same on the MIMIC dataset we analyze. In general, we would not expect the classifier to perform identically on two separate datasets, though our bootstrapping analysis suggests our results are robust to variations in our estimates of the model's performance. This assumption could be relaxed in future work by collecting a small test set of labeled smoking statuses amongst the MIMIC patient data. With unobserved confounders that could hypothetically be labeled by manual chart review, our method could be rendered unnecessary when sufficient time and money is available. In practice, however, the large quantity of data involved and possible concerns about manual review of clinical notes provides strong motivation for our method.\nOur approach also relies on Equation (3), the assumption of non-differential error. While it seems plausible that the classifier we use should have an error rate independent of patients' covariates, this may not be true. In the most severe case where the error rate depends on X, Y, and C, this would require fitting an additional model p(U* | U, X, Y, C'). We leave such an exploration to future work.\nOur method demonstrates a general-purpose approach to adjusting for an unobserved confounder when that variable can be accurately predicted from an existing classifier with known error rate. Ex-"}]}