{"title": "Improving 3D Finger Traits Recognition via Generalizable Neural Rendering", "authors": ["Hongbin Xu", "Junduan Huang", "Yuer Ma", "Zifeng Li", "Wenxiong Kang"], "abstract": "3D biometric techniques on finger traits have become a new trend and have demonstrated a powerful ability for recognition and anti-counterfeiting. Existing methods follow an explicit 3D pipeline that reconstructs the models first and then extracts features from 3D models. However, these explicit 3D methods suffer from the following problems: 1) Inevitable information dropping during 3D reconstruction; 2) Tight coupling between specific hardware and algorithm for 3D reconstruction. It leads us to a question: Is it indispensable to reconstruct 3D information explicitly in recognition tasks? Hence, we consider this problem in an implicit manner, leaving the nerve-wracking 3D reconstruction problem for learnable neural networks with the help of neural radiance fields (NeRFs). We propose FingerNeRF, a novel generalizable NeRF for 3D finger biometrics. To handle the shape-radiance ambiguity problem that may result in incorrect 3D geometry, we aim to involve extra geometric priors based on the correspondence of binary finger traits like fingerprints or finger veins. First, we propose a novel Trait Guided Transformer (TGT) module to enhance the feature correspondence with the guidance of finger traits. Second, we involve extra geometric constraints on the volume rendering loss with the proposed Depth Distillation Loss and Trait Guided Rendering Loss. To evaluate the performance of the proposed method on different modalities, we collect two new datasets: SCUT-Finger-3D with finger images and SCUT-Finger Vein-3D with finger vein images. Moreover, we also utilize the UNSW-3D dataset with fingerprint images for evaluation. In experiments, our FingerNeRF can achieve 4.37% EER on SCUT-Finger-3D dataset, 8.12% EER on SCUT-Finger Vein-3D dataset, and 2.90% EER on UNSW-3D dataset, showing the superiority of the proposed implicit method in 3D finger biometrics. For access to our project page and code, please visit our project page.", "sections": [{"title": "1 Introduction", "content": "With the development of biometrics, many advanced biometrics methods are proposed. Among them, three-dimention (3D) biometrics is one of the most potential mainstreams due to the following advantages: 1) 3D biometric traits have more identity-discriminating information, which is directly related to authentication accuracy; 2) 3D biometric traits are more robust than two-dimention (2D) biometric traits; because its more comprehensive information can avoid the impacts caused by the capture-perspective variation; 3) 3D biometric traits are more difficult to forge, therefore, have better anti-spoofing capabilities."}, {"title": "2 Related Work", "sections": [{"title": "2.1 3D Verification on Finger Biometrics", "content": "For evaluating the authentication performance of 3D fingerprint and the compatibility of 2D and 3D fingerprint, Zhou et al. [25] establish a dataset that includes both 3D fingerprint and its corresponding 2D fingerprint. This is one of the earliest 3D fingerprint biometrics exploration and the results show that the performance of 3D fingerprint authentication is comparable to that of the traditional 2D fingerprint. Cui et al. [1] propose an approach for 3D finger reconstruction and unwarping method. First, they sent the preprocessed fingerprint into the network to estimate its surface gradients; then these estimated gradients are used for 3D shape reconstruction, and finally the fingerprint is unwrapped. Experimental results show that the proposed unwarping method can reduce perspective distortion, which is significant for fingerprint matching. Recently, Dong et al. [12] propose a method for accurately synthesizing the multi-view 3D fingerprint to develop a large-scale multi-view fingerprint dataset, which can ensure a high degree of freedom and realness of synthetic 3D fingerprint model which balance the computation time. This is the first attempt to synthesize 3D fingerprint and will also the explore for unlocking a range of new possibilities and new research directions about 3D fingerprint.\nIn addition to 3D fingerprint, 3D finger knuckle is also one of the main 3D finger biometric traits, Cheng et al. [4] propose the first 3D finger knuckle dataset for public scientific research, basing on photometric stereo approach. In their work, a new feature descriptor for extracting discriminative 3D finger knuckle is also proposed. For addressing the challenges of 3D finger knuckle feature extraction by deep network, Cheng et al. [3] further propose a FKNet, which is demonstrated by the experimental results that superior than the SOTA handcraft finger knuckle feature. Recently, there is a latest follow work [26], which can achieve outperforming results in classification and identification tasks under the practical feature comparison scenario."}, {"title": "2.2 Neural Rendering", "content": "Recent progresses in neural scene representations have been proposed to realize novel view synthesis and geometric reconstruction [15, 27, 28]. The pioneering work of NeRF [15] firstly utilizes MLPS to represent the radiance field and optimize these MLPs via differentiable neural rendering on multi-view images. The implicit representation of NeRF can model the 3D scenarios and render photo-realistic images from arbitrary viewpoints. However, the requirement of dense multi-view inputs and per-scene optimization make the NeRF representation not suitable for realistic applications, which usually provides only sparse viewpoints and need to generalize towards unseen scenes without finetuning, like the 3D finger biometrics of this paper. As a result, some researches [14, 18, 19] aiming at endowing the NeRFs with generalization ability have been proposed. Pixel-NeRF [18] conditions a NeRF representation on image features extracted from a fully-connected convolutional neural network. IBRNet [19] combines the techniques of image-based rendering and NeRF, and aggregate information from source views along a given ray to compute the radiance filed. MVSNeRF [14] is a combination of Multi-view Stereo (MVS) networks [20-23] and NeRF. MVS is a classical computer vision problem, aiming to achieve dense 3D reconstruction of scenarios using multi-view images. Unlike MVS that computes a plane-swept cost volume at the reference view for depth estimation, MVSNERF leverages the feature extracted from the cost volume to condition the NeRF and achieve superior generalization ability in neural rendering. Despite the impressing progress in neural rendering techniques, the shape-density ambiguity problem still restricts the application of neural rendering techniques, especially the 3D finger biometrics in this paper, as discussed in Section 1 and Fig. 1. Our approach aims at handling this shape-density ambiguity problem via inserting extra prior of finger traits in this paper and meantime achieve the generalization ability given sparse views on unseen scenarios."}]}, {"title": "3 Method", "content": "In this section, we introduce the proposed FingerNeRF. A brief overview of FingerNeRF is presented in Fig. 2. We first revisit the preliminary knowledge about Neural Radiance Fields (NeRFs) in Section 3.1. Then, we discuss the limitation of existing generalizable NeRFs and introduce the details of the proposed FingerNeRF in Section 3.2."}, {"title": "3.1 Preliminary", "sections": [{"title": "3.1.1 NeRF and Volume Rendering", "content": "We first briefly review the NeRF representation [15]. A scene is encoded as a continuous volumetric radiance field \u03a6 of color and density in NeRF. Given the input of a 3D point on the ray \\(x \\in \\mathbb{R}^3\\) and view direction vector \\(d \\in \\mathbb{R}^3\\), the NeRF \u03a6 returns the volume density \\(\\sigma \\in \\mathbb{R}\\) and color \\(c \\in \\mathbb{R}^3\\): \\(\\Phi(x, d) = (\\sigma, c)\\).\nBy sampling 3D points of the rays on 2D pixels and calculating the view direction from pixel coordinate and camera center, we can build a volumetric radiance field, that can then be rendered into a 2D image via:"}, {"title": "3.1.2 Training Loss", "content": "With the help of the volume rendering function, the rendered pixel value on camera ray r can be compared against corresponding ground truth pixel C(r) in original images. The photometric loss to supervise the NeRF rendering results is computed as follows:\n\\(\\mathcal{L} = \\sum_{r \\in \\mathcal{R}(P)} ||C(r) - \\hat{C}(r)||_2 \\)  (2)\nwhere \\(\\mathcal{R}(P)\\) is the set of all camera rays in the 2D image on target pose P."}, {"title": "3.1.3 MVSNERF", "content": "To endow the generalization ability of MVSNet [20] into the NeRF representation, the cost volume is used to condition the volume rendering process in MVSNeRF. The multi-view feature maps are extracted via a deep 2D CNN. Given the input image \\(I_i \\in \\mathbb{R}^{N_H \\times N_W \\times 3}\\) on view i, the output feature map of the 2D CNN is \\(F_i \\in \\mathbb{R}^{N_H/4 \\times N_W/4 \\times N_C}\\). \\(N_H\\) and \\(N_W\\) are the height and width. \\(N_C\\) is the output feature channels. Given the camera intrinsic K and extrinsic parameters [R, t], the homography function can be computed:\n\\(H_i(z) = K_i (R_i \\cdot R^\\top + (\\frac{t_1 - t_i}{z}) \\cdot n_1^\\top) K_1^{-1}\\)  (3)\nwhere \\(H_i(z)\\) is the matrix warping from view i to the reference view (i = 1) at depth z and normal \\(n_1\\). Then the feature maps can be warped to the reference view by: \\(G_{i,z}(u, v) = F_i(H_i(z)[u, v, 1]^T)\\). \\(G_{i,z}\\) is the warped feature map at depth z, and (u, v) means the pixel coordinates. By stacking a series of depth planes z, the cost volume can be constructed as:\n\\(P(u, v, z) = \\text{var}(G_{i,z} (u, v))\\)  (4)\nwhere var computes the variance across different views. Afterwards, the cost volume will be fed to a 3D U-Net B which can effectively aggregate geometry features encoded in the cost volume, leading to a meaningful neural encoding volume S: S = B(P)."}]}, {"title": "3.2 FingerNeRF", "content": "As Section 1 and Fig. 1 show, the shape-radiance ambiguity problem in MVSNeRF may return incorrect 3D geometry and overfit on the radiance rendering process. Before introducing the solutions, we can first rethink the architecture of generalizable NeRFs (e.g. MVSNeRF) as an \"encoder-decoder\" structure:\n1. The encoder is a 3D reconstruction network, such as MVSNet used in MVSNERF. The intermediate implicit representation of cost volume is preserved as output rather than the estimated depth maps.\n2. The decoder is a NeRF conditioned on the feature of cost volume trilinearly interpolated on the sampled 3D points.\nSpecifically, the shape-radiance ambiguity problem can then be traced back to the encoder and decoder respectively:\n1. In the encoder, the 3D reconstruction network can not recover the cross-view correspondence effectively due to the particularity of finger trait images. Consequently, we aim to involve the correspondence prior of finger traits which inherently preserves personal identity into the construction of cost volume. (Section 3.2.1)\n2. In the decoder, the rendering process is only supervised by the RGB images. Without direct constraints on 3D geometry, the weighted coupling between volume density and color suffers from imbalance caused by the overfitting effect on color images. Consequently, we can design a series of geometric constraints as a regularization to the neural rendering. (Section 3.2.2)"}, {"title": "3.2.1 Trait Guided Transformer", "content": "In this section, we introduce the Trait Guided Transformer (TGT) which aims to involve extra correspondence prior of finger traits into the cost volume. The details of the network modules and the related formulas are shown in Figure 3. Denote that the input multi-view images are \\(I_i \\in \\mathbb{R}^{N_H \\times N_W \\times 3}\\), where i is the index of views. From the multi-view images of finger, we can extract the finger trait images \\(\\hat{I}_i \\in \\mathbb{R}^{N_H \\times N_W \\times 1}\\) from the original images. For simplicity, the finger trait \\(\\hat{I}_i\\) is a binary image representing whether the trait exists or not. The finger trait is extracted via traditional methods in previous works, i.e. fingerprint [29], finger vein [30].\nFeature Extraction: We respectively use two different CNNs to extract 2D feature maps from original finger images and trait images:\n\\(F_i = \\Phi(I_i)\\)  (5)\n\\(\\hat{F}_i = \\Psi(\\hat{I}_i)\\)  (6)\nwhere the network \u03a6 extracts 2D feature map \\(F_i \\in \\mathbb{R}^{N_H \\times N_W X N_{C1}}\\) from original image \\(I_i\\), and the network \u03a8 extracts 2D feature map \\(\\hat{F}_i \\in \\mathbb{R}^{N_H \\times N_W X N_{C1}}\\) from trait image \\(\\hat{I}_i\\). \\(N_{C1}\\) is the number of feature channels.\nFeature Volume Construction: Utilizing the homography warping function in Eq. 3, we can then build the warped feature volume on the reference view:\n\\(G_{i,z}(u, v) = F_i(H_i(z)[u, v, 1]^T)\\)  (7)\n\\(\\hat{G}_{i,z}(u, v) = \\hat{F}_i(H_i(z)[u, v, 1]^T)\\)  (8)\nwhere \\(H_i(z)\\) is the homography mapping matrix on the depth plane z. \\(G_{i,z}\\) and \\(\\hat{G}_{i,z}\\) are the warped feature matrix on the depth plane z. Then, the feature volume \\(G_i \\in \\mathbb{R}^{N_H \\times N_W \\times N_D \\times N_{C1}}\\) and \\(\\hat{G}_i \\in \\mathbb{R}^{N_H \\times N_W \\times N_D \\times N_{C1}}\\) of view i can be built by traversing depth planes ranging from the nearest depth value to the farthest one. \\(N_D\\) is the number of depth hypotheses. Instead of directly computing the variance of \\(G_i\\) like MVSNeRF in Eq. 4, we utilize the finger trait feature volume \\(\\hat{G}_i\\) to guide the cost volume construction.\nEpipolar Self-attention: Given the scaled product attention as follows:\n\\(\\text{Att}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\\)  (9)"}, {"title": "3.2.2 Training with Extra Geometric Priors", "content": "In this section, we first introduce the details of training a generalizable NeRF, and then elaborate the proposed losses which aims to embed the neural rendering of NeRF with extra geometric priors.\nEnd-to-end Training: Given the radiance field (\u03c3, Cout), the differentiable volume rendering enables the regression of image colors in a discrete form:\n\\(C = \\sum_{k} (\\prod_{j=1}^{k-1} \\text{exp}(-\\sigma(t_j)\\delta_k)) (1 - \\text{exp}(-\\sigma(t_k)\\delta_k)) C_{\\text{out}}(t_k)\\) (17)\nwhere \\(\\delta_k = t_{k+1} - t_k\\). Each point along the ray is sampled by a series of depth values tk. C is the final rendered pixel color, and \\(\\text{exp}(-\\sum_{j=1}^{k-1} \\sigma(t_j)\\delta_k)\\) represents the volume transmittance. k is the index of the sampled 3D points along the ray.\nThe photometric loss for reconstructing realistic pixel colors can be computed as an L2 loss:\n\\(\\mathcal{L}_{pho} = ||C - C_{gt}||^2\\)  (18)\nwhere \\(C_{gt}\\) is the ground truth pixel color in target image. This photometric loss is the only loss used to supervised the generalizable NeRF in previous works [14].\nFrom Eq. 17 and 18, we can find that the photometric loss only directly constrains the predicted color Cout. However, there is no direct entanglement with the volume density \u03c3, but only the indirect coarse regularization on \u03c3 based on the color constancy assumption [31]. The assumption is that the point matches if the color is the same. Obviously, such a coarse assumption may fail in finger biometrics, because the images can barely hold abundant variation of color and texture like natural images. As a result, this coarse assumption can not provide correct supervision on matching points of different views, and thus misleading the geometry representation in NeRF (shape-radiance ambiguity problem). Consequently, it is natural to consider involving extra supervision on the density term \u03c3 in Eq. 17 to handle these problems.\nDepth Rendering: To interact with the density \u03c3, we can render the depth values by simply modifying Eq. 17:\n\\(D = \\sum_{k} (\\prod_{j=1}^{k-1} \\text{exp}(-\\sigma(t_j)\\delta_k)) (1 - \\text{exp}(-\\sigma(t_k)\\delta_k)) t_k\\)  (19)\nwhere D is the rendered depth value. With the help of Eq. 19, we can involve constraints on the rendered depth map to regularize the density \u03c3.\nDepth Distillation Loss: A naive way is to involve the supervision of ground truth depth maps to Eq. 15 like [32]. However, no available ground truth depth is supported in our task of 3D finger biometrics. As an alternative, we distill the pseudo depth ground truth estimated by large model for monocular depth estimation, Midas [24], to the depth representation of NeRF. Despite of the great performance of Midas on zero-shot depth estimation, the scale of its predicted depth map is totally different from the the scale of 3D fingers in our task. Direct supervision with these pseudo depth labels is not feasible because the difference between scales may disturb the training and mislead the training. Although the absolute depth is not available for depth supervision, the relative change of depth in local regions can still provide an effective regularization on the 3D shape.\nTo model the relative change of depth in local regions, we adopt window-based sampling in the neural rendering process. As shown in Fig. 4, we randomly sample local windows in images during ray sampling. It is used as an alternative to the naive random sampling strategy used in previous NeRFs [14]. The rendered color and depth can be concatenated in these windows to local patches.\nAssume that there exists scale parameters \\((\\theta_s, \\theta_t)\\) that can map the predicted depth to the same scale of pseudo depth ground truth \\(D_{pse}\\). To align the prediction to the pseudo ground truth, we can optimize these parameters based on a least-squares criterion:\n\\(\\theta_s^*, \\theta_t^* = \\arg \\min_{\\theta_s, \\theta_t} \\sum_p (\\theta_s D(p) + \\theta_t - D_{pse}(p))^2\\)  (20)\nwhere s represents the scale and Ot represents the shift. p is the index of pixel in the sampled window.\nRewriting the formulas into matrix form:\n\\(\\theta^* = \\arg \\min_{\\theta} \\sum_p ([D(p), 1] \\theta - D_{pse}(p))^2\\)  (21)\nwhere \\(\\theta = [\\theta_s, \\theta_t]^\\text{T}\\).\nThe closed-form solution is:\n\\(\\theta^* = (\\sum_p [D(p), 1]^\\text{T} [D(p), 1])^{-1} (\\sum_p [D(p), 1]^\\text{T} D_{pse}(p))\\)  (22)\nThe scale parameter \\(\\theta_s\\) and shift parameter \\(\\theta_t\\) can be obtained from \\(\\theta^*\\).\nHowever, the solution in Eq. 22 is not robust to unexpected presence of outliers in pseudo depth labels. Specifically, the black background regions in finger images is meaningless and textureless, whose depth map can not be correctly estimated by any method and leads to an incorrect solution in Eq. 22. Hence, we further modify Eq. 22 as follows:\n\\(\\theta^* = (\\sum_{p \\in \\mathcal{P}} [D(p), 1]^\\text{T} [D(p), 1])^{-1} (\\sum_{p \\in \\mathcal{P}} [D(p), 1]^\\text{T} D_{pse}(p))\\)  (23)\nwhere p is the index of meaningful pixel in the sampled windows. All pixels located in the black background regions are abandoned.\nFurthermore, we treat \\(\\theta_s\\) and \\(\\theta_t\\) as learnable parameters in the training phase. The solution \\(\\theta^*\\) of Eq. 23 is used as of \\(\\theta_s\\) an initialization during training. The mean-squared loss can be formulated to supervised the predicted depth maps with pseudo depth labels:\n\\(\\mathcal{L}_{dep} = ||\\theta_s D + \\theta_t - D_{pse}||^2\\)  (24)\nTrait Guided Rendering Loss: As the finger traits naturally preserves identity-preserving visual clues, the correspondence between the trait image on different views can be used to involve extra matching priors in the neural rendering. The original photometric neural rendering loss in Eq. 14 is easy to overfit and converge to the mean values of pixels in local regions, thus suffering from oversmoothing effect. Following the same ray sampling strategy of window based sampling shown in Fig. 4, we utilize the trait intensity in the sampled window as a soft weight to determine which regions to foucs on It can enforce the neural rendering loss to concentrate on the areas with distinguishing trait features, benefiting the optimization of multi-view correspondence.\nConcretely, the binary trait intensities \\(C_{tra} \\in \\mathbb{R}^{s \\times s}\\) in the sampled windows is firstly smoothed by the softmax function:\n\\(W_{tra} = \\text{softmax}_p (C_{tra})\\)  (25)\nwhere p is the index of pixels in the s\u00d7s window. The original binary trait intensity is converted to a soft score by softmax function.\nThen the frist-order rendering loss can be computed via:\n\\(\\mathcal{L}_{1st} = || W_{tra} (C - C_{gt})||^2\\)  (26)\nThe second-order rendering loss can be computed by:\n\\(\\mathcal{L}_{2nd} = || W_{tra} (\\nabla_x C - \\nabla_x C_{gt})||^2 + || W_{tra} (\\nabla_y C - \\nabla_y C_{gt})||^2\\) (27)\nwhere \\(\\nabla_x\\) means the gradient along x axis of the image, and \\(\\nabla_y\\) means the gradient along y axis of the image.\nThe trait guided rendering loss can then be formulated as:\n\\(\\mathcal{L}_{tra} = \\mathcal{L}_{1st} + \\mathcal{L}_{2nd}\\)  (28)\nOverall Loss: Finally, the overall loss can be computed:\n\\(\\mathcal{L} = \\lambda_1 \\mathcal{L}_{tra} + \\lambda_2 \\mathcal{L}_{dep}\\)  (29)\nwhere \\(\\lambda_1 = 1.0\\), \\(\\lambda_2 = 0.1\\) in default."}, {"title": "4 Experiment", "content": "In this section, we introduce the implementation details and benchmark evaluations respectively in Section 4.2 and 4.3. Then we conduct detailed experiments aimed at addressing the following questions:\n\u2022 Q1: Aiming at boosting the finger trait recognition with 3D information, how does our implicit method compared with the existing explicit 3D reconstruction pipelines? (Section 4.4)\n\u2022 Q2: Considering the one-shot setting in open-set finger trait recognition, the neural rendering techniques should be generalizable without per-scene finetuning. How does our proposed method compared with other NeRF-based techniques under the generalizable neural rendering setting? (Section 4.5)\n\u2022 Q3: As a trade-off between efficiency and accuracy in 3D finger trait recognition, the multi-view methods with sparse views lead the state-of-the-art. Can the proposed method bring some improvement compared with these methods? (Section 4.6)\n\u2022 Q4:Existing 3D finger recognition methods are highly customized towards specific device capturing single modality like fingerprint image or finger vein image. Can the proposed methods be generalized to different modalities of finger traits like fingerprint or finger vein images? (Experiments under different modalities in Section 4.4, 4.5, and 4.6.)"}, {"title": "4.1 Dataset", "content": "As summarized in Table 1, three databases are used for evaluating the proposed method: SCUT-Finger-3D, SCUT-Finger Vein-3D, UNSW-3D. Due to the particularity of the neural rendering task in this paper, we propose two novel datasets for neural rendering of finger biometrics: SCUT-Finger-3D and SCUT-FingerVein-3D. These two datasets are used to evaluate the generalization ability of the proposed method towards different modalities of finger traits. UNSW-3D is a publicly available database which is widely used in 3D fingerprint. We utilize UNSW-3D to evaluate the generalization performance of the proposed method.\nSCUT-Finger-3D contains contactless finger images. The camera is set in the bottom of the imaging device. Volunteers are asked to rotate their finger from -30 degrees to 30 degrees causually, meantime the video is captured to represent a finger. Assume that the finger is rigid, different frames of the video can be treated as multi-view images. We use this dataset to evaluate the generalization ability of the proposed FingerNeRF.\nSCUT-Finger Vein-3D contains contactless finger vein images captured under infrared light. In analogy with SCUT-Finger-3D, the camera is set in the bottom of the imaging device. The only difference compared with SCUT-Finger-3D is that this dataset is based on a different modality of finger vein images. We use this dataset to evaluate the generalization ability of the proposed FingerNeRF across different modalities.\nUNSW-3D [33] also contains contactless finger images with clear fingerprints. We use the raw finger images of this dataset to evaluate the generalization performance of the proposed method towards other multi-view datasets."}, {"title": "4.2 Implementation Details", "content": "The experiments are conducted with one RTX 3090 GPU. The output feature dimension of TGT is \\(N_{C1} = 8\\). We construct the cost volume of multi-view images with NC2 = 8 channels, which is also the output dimension of the feature extraction network with shared weights. During the ray sampling process, we adopt \\(N_d = 128\\) depth hypotheses uniformly sampled from the nearest to the farthest depth planes. The sampled window size is set to sxs = 64 x 64 in default. We train the FingerNeRF on each dataset for 20 epochs, which may take half a day in total. The resolution of image is resized to \\(N_H \\times N_W = 320 \\times 200\\) during training in order to remedy the cost of GPU memory."}, {"title": "4.3 Benchmark Evaluation", "sections": [{"title": "4.3.1 Protocol and Task", "content": "For data-driven biometric recognition methods, the subject-dependent protocol assumes that all identities in the test set are the predefined in the training set. This ideal setting rarely fits the real-world scenario, because it is more likely to happen that the evaluation process and the training process may have samples with different identities, which follows the subject-independent protocol [44]. Since the testing identities are disjoint from the training identities, the evaluation metrics can demonstrate the open-set generalization performance of the methods in a better way.\nBiometric benchmarks usually have two kinds of tasks: verification and identification. Verification task computes the 1-to-1 similarity score between a probe and a gallery to find out whether the two samples are from the same subject. Identification task [45] has two different settings: close-set identification and open-set identification. The close-set setting requires the probe to appear in the gallery identities, while in the open-set setting, the probe may not appear in the gallery identities. Following [11], we conduct experiments on the the verification task and close-set identification task following the subject-independent protocol and subject-dependent protocol respectively."}, {"title": "4.3.2 Dataset Division", "content": "In our experiments, each finger is considered as an individual subject. We randomly separate all subjects of the whole dataset into training set, validation set and testing set with the ratio of 5:2:3. Following [11], we fix the partition of the separated sets and record the subject names into csv files of the corresponding identities to ensure the reproducibility and fair comparison in this work."}, {"title": "4.3.3 Evaluation Metrics", "content": "For the verification task, the adopted metrics are Detection Error Tradeoff (DET) curve, Equal Error Rate (EER), TAR@FAR=0.01, and TAR@FAR=0.001. DET curve is ploted to reflect the variation of Fasle Accept Rate (FAR) versus False Reject Rate (FRR) when varying the threshold of the matching scores between different samples. It also represents the tradeoff between the metric of FAR and FRR. EER is the value when FAR and FRR are equal, which is an overall measure of performance in biometrics. TAR@FAR=0.01 and TAR@FAR=0.001 are measures of practical scenarios [11]. TAR@FAR=0.01 is the True Accept Rate (TAR) when the False Accept Rate (FAR) equals 0.01. In analogy, TAR@FAR=0.001 reflects the TAR when FAR is 0.001.\nFor the identification task, the adopted metrics are Mean Average Precision (mAP), Rank-1 Accuracy and Rank-5 Accuracy in the Cumulative Match Characteristic (CMC) curve. CMC curve plots the probability that the positive result can be found in the top K samples sorted with the matching scores. The horizontal axis is K, and the vertical axis is the prbability of positive samples. Rank-1 and Rank-5 Accuracy are the vertical value of CMC when K is 1 and 5 respectively. These metrics are important metrics in practical identification tasks. Besides, to evaluate the overall representative of performance, the metric of mAP is also used here to measure the overall ranking effect."}]}, {"title": "4.4 Implicit vs Explicit 3D Reconstruction for Finger Trait Recogntion", "sections": [{"title": "4.4.1 Experiment Settings", "content": "In this section, we aim to evaluate the effectiveness of the reconstructed 3D representation in finger trait recognition tasks. These reconstruction methods can be categorized in to two categories: explicit methods, and our proposed implicit methods.\n1. For explicit methods, the pipeline is comprised of two steps: firstly reconstruct the 3D model from given images, and then extract 3D features from the reconstructed models. We adopt the benchmarking software for 3D reconstruction, COLMAP [46, 47], to reconstruct the 3D point clouds from the video sequences explicitly. The explicit reconstruction process requires dense view inputs containing all available images in the video sequences. The reconstructed point clouds are further fed to various networks specially designed for point cloud perception for feature extraction, including: PointNet [34], PointNet++ [35], DGCNN [36], DPAM [37], GSNet [38], PointMLP-E [40], PointMLP-E/PointMLP [40], Point-Transformers-H [41], Point-Transformers-M [42], and Point-Transformers-N [43].\n2. For our implicit method, our FingerNeRF requires several multi-view images as input and can render color images and depth maps on arbitrary views given the camera poses. Instead of utilizing dense view inputs like explicit methods, we adopt a more challenging setting with sparse view inputs (3 views in default). We randomly select 3 views from all videos in the dataset to represent the 3D subject, and train the FingerNeRF to reconstruct the remaining views in the video given only 3 views as input. After training, the FingerNeRF is used to render V unseen views randomly selected from the camera trajectory of the video to construct multi-view RGB and depth maps. Note that V is set to 20 in default. Then these multi-view RGB and depth maps are further fed to a multi-view convolution network [48] to extract the feature embedding for finger trait recognition.\nThe comparison between implicit and explicit pipelines can reflect the effectiveness of the reconstructed 3D finger traits for recognition. Furthermore, we conduct the experiments under different modalities respectively in SCUT-Finger-3D (Section 4.4.2) and SCUT-FingeVein-3D (Section 4.4.3). The ablation experiments are provided in Section 4.4.4 to evaluate the effectiveness of different components."}, {"title": "4.4.2 Comparison on SCUT-Finger-3D", "content": "In this section, we provide the qualitative results of finger trait recognition in Table 2 and 3. Both of the verification and identification tasks are evaluated and the results of explicit and our implicit 3D methods are compared in the tables. Table 2 only utilizes the 3D geometric feature to evaluate the effectiveness of 3D shape information reconstructed from given inputs in the aforementioned verification and identification tasks. Table 3 reports the experimental results when both 3D geometric feature and texture features are used in verification and identification tasks. Moreover, we provide the visualization of DET curves of finger trait recognition with different modalities in Fig. 5.\nIn Table 2, we only utilize the 3D geometric feature for finger trait recognition and compare the performance with explicit methods. The explicit methods only use the X/Y/Z coordinates of the reconstructed 3D point cloud in the experiments, and our implicit methods only utilizes the rendered multi-view depth maps for comparison. As shown in the table, the EER of FingerNeRF is 22.60% which is much better than the EERs of explicit methods that is about 35% to 40%. Other verification metrics like"}]}]}