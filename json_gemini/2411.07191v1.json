{"title": "THE SUPER WEIGHT IN LARGE LANGUAGE MODELS", "authors": ["Mengxia Yu", "De Wang", "Qi Shan", "Colorado Reed", "Alvin Wan"], "abstract": "Recent works have shown a surprising result: a small fraction of Large Language Model (LLM) parameter outliers are disproportionately important to the quality of the model. LLMs contain billions of parameters, so these small fractions, such as 0.01%, translate to hundreds of thousands of parameters. In this work, we present an even more surprising finding: Pruning as few as a single parameter can destroy an LLM's ability to generate text increasing perplexity by 3 orders of magnitude and reducing zero-shot accuracy to guessing. We propose a data-free method for identifying such parameters, termed super weights, using a single forward pass through the model. We additionally find that these super weights induce correspondingly rare and large activation outliers, termed super activations. When preserved with high precision, super activations can improve simple round-to-nearest quantization to become competitive with state-of-the-art methods. For weight quantization, we similarly find that by preserving the super weight and clipping other weight outliers, round-to-nearest quantization can scale to much larger block sizes than previously considered. To facilitate further research into super weights, we provide an index of super weight coordinates for common, openly available LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have been growing in size and capability at an unprecedented rate, enabling them to capture increasingly complex linguistic patterns across a wide range of tasks. However, with this increase in model scale, new and unexpected behaviors have emerged. Dettmers et al. (2022) discovered that once LLMs reach a certain scale, a small set of hidden state features contains outliers of exceptionally large magnitude. These outliers account for a small percentage of all activations but are crucial for preserving the compressed model's quality (Dettmers et al., 2022; Xiao et al., 2023; Wei et al., 2023; Shao et al., 2024).\nHowever, not all outliers are equally important. In this paper, we study a tiny yet important set of outliers in LLMs, termed super weights. In Llama-7B, pruning the super weight, a single scalar, completely destroys the model's ability to generate text; the average accuracy of zero-shot downstream tasks effectively plummets to zero. Conversely, pruning the other top 7,000 outliers, including outliers that are larger than the super weight, affects no more than a few percentage points.\nIntriguingly, super weights behave similarly across model families and sizes. For one, the super weight is always found in the mlp.down_proj weight, always in an early layer. We also find that the super weight amplifies input activation inliers to ultimately produce the exceptionally large magnitude activation observed by Sun et al. (2024) we term this the super activation. This super activation persists throughout the model at exactly the same magnitude and position regardless of the prompt, and we find this is uniquely enabled by skip connections. Finally, super weights suppress stopword likelihood. Taken together, pruning the super weight destroys quality by dampening the super activation and shifting almost all logit probability mass to stopwords.\nBoth super weights and super activations, which we collectively refer to as super outliers, are critical to model quality. Fortunately, there are no more than a handful of scalar super outliers per tensor; in light of this, we revisit round-to-nearest quantization, equipped only with the ability to hold"}, {"title": "2 RELATED WORK", "content": "2.1 OUTLIERS IN LLMs\nLLM outliers are widely observed in existing literature. Kovaleva et al. (2021) notes weight out-liers, which emerge gradually, beginning early in pre-training, and cause abnormal spikes at select dimensions in the output embedding vectors. Disabling those outliers significantly degrades both the training loss and the downstream task performance. Bondarenko et al. (2021) notes activation outliers, which encourage specific attention patterns, such as attending to the special separator to-ken. However, Sun et al. (2024) first observes an exceptionally extraordinary outlier; in particular, they discover massive activations in LLMs that persist across layers in a fixed position, which Yang et al. (2024) hypothesizes is caused by gated linear units (GLU) and its variants, such as GEGLU and SwiGLU. To mitigate these massive activations, Sun et al. (2024) proposes a learnable atten-tion bias, and (Son et al., 2024; Yang et al., 2024) inserts certain prefixes. To complement these mitigation studies, our focus is instead to leverage, rather than mitigate, these super activations.\n2.2 OUTLIER-AWARE QUANTIZATION METHODS\nQuantization is one of the most popular techniques for reducing LLM resource consumption. How-ever, quantizing LLMs is non-trivial, due to outliers that increase the range of values. Existing works typically study two settings for LLM quantization: (1) Weight-only quantization, where only weights are quantized into low-bit integers; (2) Weight-activation quantization, where both activa-tion and weights are quantized.\nFor weight-only quantization, several common solutions including using smaller block sizes, to limit the number of values any single outlier can impact (Dettmers et al., 2024; Shao et al., 2024; Dettmers & Zettlemoyer, 2023; Frantar et al., 2022; Dettmers et al., 2023); scaling sensitive weights, via a grid-searched channel-wise scaling, Lin et al. (2024); or clipping outliers via learned optimal thresholds (Shao et al., 2024; Lin et al., 2024). The most common approach is to extract and store"}, {"title": "3 SUPER WEIGHTS", "content": "Many studies corroborate the importance of weight outliers, showing that a small percentage of the largest magnitude outliers are essential to model quality. This percentage can be as small as 0.01%, but for these billion-parameter models, 0.01% can still include hundreds of thousands of weights. Our investigation reveals a surprising fact about even this group of weight outliers: There exists a single, scalar weight that, despite not being the largest, holds more importance than thousands of other outlier weights combined. We call this single scalar weight a super weight.\nIn our analysis, we find that the super weight is necessary for quality, having an outsized influence on quality if removed. Without the super weight, LLMs fail to generate text, resulting in qualitatively (Figure 1) and quantitatively (Table 1) gibberish responses. In particular, zero-shot dataset accuracy is reduced to guessing, and perplexity increases by several orders of magnitude (Prune SW). To quantify this influence, we prune all other outlier weights (Prune Non-SW), comparing the impact of a single super weight against 7,000 other outliers. Remarkably, the accuracy drop associated with pruning this single weight is much greater than the effect of all other outliers combined.\n3.1 IDENTIFICATION OF SUPER WEIGHTS\nSuper weights create super activations. Sun et al. (2024) first discover a handful of exceptionally massive activations, which are crucial to model quality. Massive activations persist across many layers, feature constant magnitude, and always exist at the same position, regardless of input. We"}, {"title": "3.2 MECHANISMS OF SUPER WEIGHTS", "content": "We find that super weights (1) induce super activations, which have lasting effects throughout the entire model, and (2) suppress stopword likelihood (Figure 2).\nSuper weights (partially) operate via super activations. To assess whether the super weight's impact on model quality is solely mediated by the super activations or also by activations of other"}, {"title": "4 SUPER-OUTLIER AWARE QUANTIZATION", "content": "Quantization is a powerful technique for compressing models and reducing memory requirements. However, the presence of outliers can significantly degrade quantization quality, for both weight quantization and activation quantization. As we mentioned before, we refer to these problematic outliers, both super weights and super activations, as super outliers. As we have shown above, these super outliers carry disproportionate importance for model quality, making their preservation during quantization critical. Quantization generally maps continuous values to a finite set of values; we consider one of the simplest forms \u2013 namely, asymmetric round-to-nearest quantization:\n$Q(X) = Round(\\frac{X-MIN(X)}{MAX(X)-MIN(X)}) ,Q^{-1}(X) = \\Delta \\cdot \\hat{X} + MIN(X)$\nwhere $\\Delta = \\frac{MAX(X)-MIN(X)}{2^{N}-1-1}$ is the quantization step and N is the number of bits. Note that the maximum value is used to calculate \u2206, so super outliers in X drastically increase the step size. With larger step sizes, inliers are rounded to more distant values on average, increasing the quanti-zation error. With increasingly super outliers, inliers are rounded to fewer discrete values, and more quantization bins remain unused. In this way, super outliers cause poor quantization fidelity.\nWe specifically consider the case where hardware performs arithmetic in half precision, meaning the tensor X is quantized and dequantized before usage; in this setting, we can leverage prior knowledge of super outliers in two ways. First, hold out the super outlier to prevent adverse effects on inlier quantization. Second, restore the super outlier's value after dequantization, to ensure the super outlier's effects are preserved. We adopt this insight in two forms below, for weights and activations."}, {"title": "4.1 ACTIVATION QUANTIZATION", "content": "We conduct experiments using round-to-nearest quantization, with a small modification \u2013 replace the super activation with the median value (REPLACE), quantize (Q) and dequantize (Q\u00af\u00b9) activations, then restore the super activation in FP16 (RESTORE). This can be expressed as the following,\n$\\hat{A} = RESTORE(Q^{-1}(Q(REPLACE(A)))$                                   (1)\nSince the super activation is a single scalar, the bitrate and kernel complexity are not significantly impacted."}, {"title": "4.2 WEIGHT QUANTIZATION", "content": "Prior art uses (Dettmers et al., 2023; Lin et al., 2024) small group sizes of 64 or 128, as Dettmers & Zettlemoyer (2023) finds that small group sizes are required for precise low-bit quantization. How-ever, the small group sizes come with computational and bitrate overhead, requiring other techniques to handle a high number of half precision scales and biases.\nTo address this challenge, we propose a simple method to improve INT4 quantization with large blocksizes. First, we identify super weights using Section 3.1. Second, to improve inlier fit, we clip (CLIP) the outlier weights; in this step, the super weight is clipped as well. Quantize (Q) and dequantize (Q-1) the clipped weights. Then, to ensure the effect of the super weight is preserved, we restore the half-precision super weight after dequantization (RESTORE).\n$W = RESTORE(Q^{-1}(Q(CLIP_{z}(W)))$                                                 (2)\nAs described in the equation above, we parameterize clipping using a z-score. Assuming all weights fit a Gaussian, we consider all values with a z-score beyond a certain threshold z to be an outlier. To tune this hyperparameter z, we find the minimum reconstruction error z-score using 500 examples from the Wikitext-2 train set."}, {"title": "5 EXPERIMENTS", "content": "To comprehensively demonstrate the effects of super weights, we conduct experiments across LLAMA 7B to 30B, (Touvron et al., 2023), Mistral 7B (Jiang et al., 2023), and OLMO (Groeneveld et al., 2024) 2 To assess the practical application capabilities of LLMs, we evaluate their accuracy on zero-shot benchmarks, including PIQA (Bisk et al., 2020), ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), Lambada (Paperno et al., 2016), and Winogrande (Sakaguchi et al., 2021). We use the Im-evaluation-harness (Gao et al., 2024) library to evaluate the above tasks. We also calculate the perplexity for Wikitext-2 (Merity et al., 2017) and C4 (Raffel et al., 2020), following the widely accepted setting from (Frantar et al., 2022)."}, {"title": "5.1 ACTIVATION QUANTIZATION", "content": "Following SmoothQuant's setting, we simulate W8A8 quantization with FP16 arithmetic. Specifically, we perform 8-bit per-tensor quantization for weights, and 8-bit per-token quantization for activations. We quantize the inputs and weights for linear layers (including q, k, v, gate, up, down projections), and BMM (i.e., batched matmul) in attention layers. For SmoothQuant, we use the default \u03b1 as 0.85.\nWe compare our method with SmoothQuant in Table 3. For three Llama models on both datasets, we achieve over 70% of SmoothQuant's improvement over naive quantization. On C4 with Llama-7B and on Wikitext with Llama-30B, we attain above 80% of SmoothQuant's improvement. Our method demonstrates that a significantly simplified approach to quantization can achieve competitive results compared to more complex methods. Unlike SmoothQuant, which applies scales to every activation channel, our method focuses solely on addressing one critical activation outlier.\nWe extended our evaluation to include additional LLMs: OLMo (1B and 7B), Mistral-7B, and Llama-2-7B. Results are shown in Table 4 and Appendix Table 7. These models represent a di-verse set of architectures and training paradigms, allowing us to assess the generalizability of our quantization method. Since SmoothQuant does not report on this set of models, we compare our results with naive W8A8 quantization. Across all models and datasets, our method consistently out-performs naive W8A8 quantization. Our method demonstrates remarkable performance on OLMO models.\nNotably, OLMo models use non-parametric LayerNorm, making them incompatible with the SmoothQuant method, which relies on LayerNorm weights to apply the per-channel scales. On Mistral-7B, the improvements are smaller. We hypothesize that this is because the LayerNorm of these models may have learned weights that aggressively suppress the super activation, resulting in a more uniform distribution of activation magnitudes.\nThese results underscore the critical importance of the super activation in maintaining model per-formance during quantization. By addressing this single activation with minimal computational overhead, our method captures a significant portion of the benefits achieved by more complex quan-tization schemes. This finding suggests that the super activation plays a disproportionately large role in preserving model quality during the quantization process."}, {"title": "5.2 WEIGHT QUANTIZATION", "content": "Recent advancements in LLM quantization techniques have inadvertently highlighted the importance of super weights. Two notable methods, AWQ (Lin et al., 2024) and SqueezeLLM (Kim et al., 2024), demonstrate the significance of preserving these super weights, albeit through different approaches.\n5.2.1 EXISTING WORKAROUNDS FOR THE SUPER WEIGHT\nAWQ, recognizing the need to minimize quantization errors for important weights, introduced a per-channel scaling method. This technique automatically searches for optimal scaling factors, ef-fectively amplifying crucial weight channels. Our analysis of Llama-7B revealed that AWQ scales"}, {"title": "5.2.2 SCALING UP BLOCK SIZES", "content": "To evaluate the effectiveness of the proposed super weight-aware quantization method, we compare it with the traditional round-to-near quantization approach. We assess the models on a suite of zero-shot downstream tasks, with results illustrated in Figure 7.\nIn the traditional round-to-near quantization, we observe a clear trend: as the block size increases, model quality significantly degrades. This decline likely results from the increased quantization er-ror introduced when larger blocks of weights are quantized together, which allows outliers to impact more weights. In contrast, our super weight-aware quantization method demonstrates much greater robustness to larger block sizes. As the block size increases, the degradation in model quality is noticeably smaller compared to the round-to-near method. This robustness stems from our method's ability to preserve the most critical weight (the super weight) while minimizing the influence of out-lier weights on the overall quantization process. By clipping outliers and focusing on inlier weights, our method maintains higher fidelity in representing the model's parameters.\nA key advantage of our method is its ability to support larger block sizes with less loss in model quality. This capability leads to a lower average bitrate and smaller file sizes, which are essential for deploying models in resource-constrained environments, such as mobile devices or edge computing scenarios."}, {"title": "6 CONCLUSION", "content": "Our study of Large Language Models has revealed the critical role of super outliers \u2013 specifically, the super weight and its induced super activation. Although these super outliers are small in number, identifying and preserving them is essential for model quality; pruning the super weight completely destroys the model's ability to generate text, and retaining the super weight can significantly improve the quantized model's quality.\nOur findings shed light on how these outliers influence model behavior and provide practical strate-gies for their detection and management. By sharing a directory of super weights, we furthermore hope to inspire further research into their properties and implications."}, {"title": "A APPENDIX", "content": "A.1 SUPER WEIGHT SENSITIVITY\nIn this section, we show the full set of results on zero-shot downstream datasets. Results are shown in Table 5 for FP16 models. From the table, we can see that some datasets are more sensitive to super weight (SW) amplification. For example, Winogrande and Lambada show consistent improvements across models when amplifying SW, while PIQA shows same or slightly lower accuracy. We also evaluate the 4-bit quantized Llama-7B with amplified SW in Table 6. We witness similar minor improvements when SW is amplified."}, {"title": "\u0391.2 MAXIMUM-MAGNITUDE ACTIVATION OF DOWN PROJECTION MODULE", "content": "Below, we show more examples of identifying super weights. We visualize the maximum-magnitude activations in the inputs and outputs of down proj of all the transformer layers. The outlier \"sparks\" indicate the row and column index of super weights."}, {"title": "A.3 LOGIT DISTRIBUTION WITH SUPER WEIGHT REMOVAL", "content": "Below, we visualize more of the logit distribution, when super weights are removed from Llama-7B. Despite the more thorough visualization, the conclusions remain the same: stopwords are amplified and non-stopwords see a drastic decrease in likelihood."}, {"title": "A.4 ADDITIONAL ACTIVATION QUANTIZATION RESULTS WITH SUPER ACTIVATIONS", "content": "Below, we include results for Llama-2 7B using our activation quantization. See Table 7."}, {"title": "A.5 SUPER WEIGHTS AND ATTENTION SINKS", "content": "Given that super activations are typically observed on the first token of an input sequence, we hy-pothesized a potential relationship between super weights and the well-documented phenomenon of attention sinks (Xiao et al., 2024; Son et al., 2024). To test this hypothesis, we conducted experi-ments comparing attention weight patterns in the presence and absence of super weights. Contrary to our initial expectations, we find that attention sinks persist even when super weights are removed from the model, while not preserving model quality."}]}