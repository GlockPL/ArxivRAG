{"title": "IMPLICIT SEARCH VIA DISCRETE DIFFUSION: A STUDY ON CHESS", "authors": ["Jiacheng Ye", "Zhenyu Wu", "Jiahui Gao", "Zhiyong Wu", "Xin Jiang", "Zhenguo Li", "Lingpeng Kong"], "abstract": "In the post-AlphaGo era, there has been a renewed interest in search techniques such as Monte Carlo Tree Search (MCTS), particularly in their application to Large Language Models (LLMs). This renewed attention is driven by the recognition that current next-token prediction models often lack the ability for long-term planning. Is it possible to instill search-like abilities within the models to enhance their planning abilities without relying on explicit search? We propose DIFFUSEARCH, a model that does implicit search by looking into the future world via discrete diffusion modeling. We instantiate DIFFUSEARCH on a classical board game, Chess, where explicit search is known to be essential. Through extensive controlled experiments, we show DIFFUSEARCH outperforms both the searchless and explicit search-enhanced policies. Specifically, DIFFUSEARCH outperforms the one-step policy by 19.2% and the MCTS-enhanced policy by 14% on action accuracy. Furthermore, DIFFUSEARCH demonstrates a notable 30% enhancement in puzzle-solving abilities compared to explicit search-based policies, along with a significant 540 Elo increase in game-playing strength assessment. These results indicate that implicit search via discrete diffusion is a viable alternative to explicit search over a one-step policy.", "sections": [{"title": "1 INTRODUCTION", "content": "Search is central to problem-solving in AI (Russell & Norvig, 2010). One of the most notable examples is IBM's Deep Blue (Campbell et al., 2002), which performs extensive search over a large space through a strong search algorithm (alpha-beta pruning; Knuth & Moore 1975), defeated the world chess champion Garry Kasparov in 1997. Search has also been utilized in neural networks. A noteworthy advancement in this progression is exemplified by AlphaGo (Silver et al., 2016) and its successors (Silver et al., 2017b;a), where the policy is guided by an extra value network through Monte Carlo Tree Search (MCTS; Coulom 2006; Browne et al. 2012). By explicitly searching into the future, the decision to be taken can be iteratively refined (Silver et al., 2016; 2017b;a; Schrittwieser et al., 2020).\nRecent research on Large Language Models (LLMs) demonstrates the utilization of a similar frame-work. Although scale-up, the autoregressive one-step policy addresses only a portion of the prob-lems and relies on explicit search on complex tasks (Hao et al., 2023; Yao et al., 2024; Zhao et al., 2024; Trinh et al., 2024, inter alia), highlighting their inherent limitations in long-term plan-ning (Valmeekam et al., 2022; Bubeck et al., 2023; Bachmann & Nagarajan, 2024). This explicit search-demanding approach, however, is not quite satisfactory, as the repeated invocation of the value model can result in an accumulation of errors if the value model is inaccurate and increased inference costs for long-horizon rollouts (Yao et al., 2024). Given the essence of explicit search (e.g., MCTS) over one-step policies lies in iteratively looking into the future and leveraging the future to enhance the next token (or action) prediction, our research question is:\nCan the policy model predict and utilize the future by itself to improve the next token (or action) prediction without relying on explicit search during inference?"}, {"title": "2 PRELIMINARIES", "content": "This section introduces key concepts and notations in the chess-playing problem and diffusion mod-eling."}, {"title": "Problem Setting", "content": "Chess, along with other games of perfect information like checkers, othello, backgammon, and Go, fits the framework of alternating Markov games (Littman, 1994). In chess, there exists a state space S, an action space A, a state transition function f(s, a) that determines the subsequent state after taking action a in state s, and two reward functions $r^{0}(s)$ and $r^{1}(s)$ representing the two players' reward in state s (rewards being zero except at the final time-step). The outcome of the game $o_{i} = \\pm 1$ is the terminal reward at the end of the game from the perspective of the current player at time-step i. Chess is also a zero-sum game which indicates $r^{0}(s) = -r^{1}(s)$. A policy $p(a|s)$ is a probability distribution over actions space A. A value function $v^{p}(s)$ represents the expected outcome when all actions for both players adhere to policy p, denoted as $v^{p}(s) = E[o_{i}|s_{i} = s, a_{i} ... 1 \\sim p]$. The goal is to build a policy that, when actions are taken based on it, results in the highest possible final outcome."}, {"title": "Discrete Diffusion Modeling", "content": "Discrete diffusion models (Sohl-Dickstein et al., 2015; Hoogeboom et al., 2021; Austin et al., 2021) are a class of latent variable models characterized by a forward and a backward Markov process. Suppose $x_{0} \\sim q(x_{0})$ is a discrete random variable with K possible categories and represented as a one-hot vector. The forward process $q(x_{1:T}|x_{0}) = \\prod_{t=1}^{T} q(x_{t}|x_{t-1})$ corrupts the original data $x_{0}$ into a sequence of increasingly noisy latent variables $x_{1:T}: x_{1},..., x_{T}$. The learned backward process $p_{\\theta}(x_{0:T}) = p(x_{T})\\prod_{t=1}^{T} p_{\\theta}(x_{t-1}|x_{t})$ gradually denoises the latent variables to the data distribution. In order to optimize the generative model $p_{\\theta}(x_{0})$ to fit the data distribution $q(x_{0})$, we typically optimize a variational upper bound on the negative log-likelihood due to the intractable marginalization:\n$\\mathcal{L}_{vb} = E_{q(x_{0})}[D_{K L}[q(x_{T}|x_{0})||p(x_{T})]] + \\sum_{t=2}^{T} E_{q(x_{t}|x_{0})}[D_{K L}[q(x_{t-1}|x_{t}, x_{0})||p_{\\theta}(x_{t-1}|x_{t})]] - E_{q(x_{1}|x_{0})}[log p_{\\theta}(x_{0}|x_{1})]$     (1)\nwhere $\\mathcal{L}_{T}$ is a constant when a fixed prior $p(x_{T})$ is employed. In discrete diffusion, both the forward and backward distribution are defined as categorical distribution, e.g., $q(x_{t}|x_{t-1}) = Cat(x_{t}; p = Qx_{t-1})$ and $p_{\\theta}(x_{t-1}|x_{t}) = q(x_{t-1}|x_{t}, f(x_{t}; \\theta))$ (Hoogeboom et al., 2021), where $Q_{t}$ is a pre-defined K \u00d7 K transition matrix and K is the size of categories. Therefore, the forward process posterior $q(x_{t-1}|x_{t}, x_{0})$ and each KL term can be calculated analytically."}, {"title": "3 METHODOLOGY", "content": "In this section, we introduce DIFFUSEARCH, an approach that looks into the future world via discrete diffusion modeling without any explicit search at inference time, with a focus on the chess-playing task."}, {"title": "3.1 MODELING", "content": "In order to endow the model with the capability to predict and utilize the future, we consider train-ing the model in a supervised way following (Ruoss et al., 2024), leaving self-play training from scratch (Silver et al., 2017b) for future work. We provide the current state $s_{i}$ as the history repre-sentation following prior studies (Silver et al., 2016; 2017b; Ruoss et al., 2024). For future world representation, we consider a variety of alternative variants, such as purely future actions (denoted as s-aa), action-states (denoted as s-asa), and action-state-values (denoted as s-avsav, etc. We analyze the performance of different future paradigms in Section \u00a74.3. The s-asa approach is ultimately chosen as our modeling paradigm considering the effectiveness and simplicity. The policy distribution at state $s_{i}$ considering the future is given by:\n$P_{\\theta}(a_{i}, s_{i+1}, a_{i+1},..., s_{i+h-1}, a_{i+h-1}|s_{i})$,   (2)\nwhere h > 1 is the future horizon."}, {"title": "3.2 TRAINING", "content": "In order to train a policy that models Eq.(2), we consider a supervised training approach lever-aging Stockfish (Romstad et al., 2008). We utilize Stockfish 16, currently the world's strongest search-based engine, as an oracle to label board states extracted from randomly selected games on lichess.org. We approximate the optimal policy $\\pi^{*}$ with $\\pi^{SF}$ and obtain each action by taking $a_{j}^{S F} = arg max_{a_{j}} Q^{S F}(s_{j}, a_{j})$. For a given world horizon h, we construct a dataset $\\mathcal{D} = \\{(s_{i},(a_{i}^{S F},\\mathcal{Z}_{i}^{S F}))\\}$, where the oracle future path means playing some move that has the maximum evaluation for the best opponent's reply for both players. Nonetheless, we observe that this approach not only fails to predict the future but also impedes the learning of the next action $a_{i}^{S F}$ (see Section \u00a74.3). Therefore, we resort to diffusion modeling (Sohl-Dickstein et al., 2015) as a powerful sequence modeling approach with strong expressive capabilities. Specifically, we consider discrete diffusion modeling and streamline $\\mathcal{L}_{vb}$ in Eq.(1) into a weighted cross-entropy loss motivated by Austin et al. (2021); Zheng et al. (2023); Shi et al. (2024); Sahoo et al. (2024). The KL term $D_{K L}[q(x_{t-1}|x_{t}, x_{0})||p_{\\theta}(x_{t-1}|x_{t})$ for each individual random variable is simplified as $-\\mathbb{E}_{q(x_{t}|x_{0})}[1_{x_{t} \\neq x_{0}} log f(x_{t}; \\theta)]$, where and $\\mathcal{L}_{tb}$ becomes:\n$\\mathcal{L}_{vb} = -E_{q(x_{0})}[\\sum_{t=1}^{T} \\lambda_{t} E_{q(x_{t}|x_{0})}[1_{x_{t} \\neq x_{0}} log f(x_{t}; \\theta)]],$    (3)\nwhere $\\lambda_{t} = \\frac{\\gamma_{t}}{T} \\in (0,1]$ is a time-dependent reweighting term that assigns lower weight for noisier $x_{t}$, and $\\alpha_{t} \\in [0,1]$ belongs to a predefined noise scheduler that controls the level of noise in $x_{t}$ at timestep t. We explore multiple variants of $\\lambda_{t}$ in Section \u00a74.3. To enable conditional training with a given state, we freeze the state tokens and perform denoising on the next action $a_{i}^{S F}$ and all futures tokens $\\mathcal{Z}_{i}^{S F}$. We employ Monte Carlo sampling with regard to $x_{0}, x_{t}$ and t when optimizing $\\mathcal{L}_{vb}$."}, {"title": "3.3 INFERENCE", "content": "During inference, taking $arg max_{a_{i}} P_{\\theta}(a_{i}|s_{i})$ as in one-step policy in DIFFUSEARCH requires marginalizing over all future with horizon h, i.e., $p_{\\theta}(a_{i}|s_{i}) = \\sum_{z_{i}} P_{\\theta}(a_{i}, z_{i}|s_{i})$, which is intractable due to the exponential-growing search space when h goes larger, e.g., the game tree contains $b^{h}$ nodes and the branching factor b is around 31 on average in chess (Barnes, 2019). One simplified approach to comparing actions is to measure the best future if one action is taken, which can be reflected by the joint probability $p_{\\theta}(a_{i}, z_{i}|s_{i})$. Therefore, we resort to $arg max_{a_{i} z_{i}} P_{\\theta}(a_{i}, z_{i}|s_{i})$, which does not involve marginalization and can be achieved by sampling from the trained model."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 SETUP", "content": "Baselines We compare our model with three Transformer models proposed in Ruoss et al. (2024): State-action model (S-A) which learns to predict next move via behavioral cloning; State-value model (S-V) which predicts next move via comparing the value of next states; and Action-value model (SA-V) which predicts next move via comparing the value of each legal actions at the current state. We also integrate the trained S-A and S-V models into MCTS following AlphaZero (Silver et al., 2017a)."}, {"title": "4.2 MAIN RESULTS", "content": "We report the prediction and playing strength comparison for our model against baselines in Table 2. Additionally, we report the performance of Stockfish 16 with a time limit of 0.05s per legal move, which stands as the oracle used to generate our dataset. We find DIFFUSEARCH significantly outperforms the S-A model by 653 Elo and 19% action accuracy, indicating the effectiveness of DIFFUSEARCH in improving next action prediction through future prediction. Remarkably, despite utilizing 20 times fewer data records than the SA-V model, our model demonstrates superior performance with approximately 10% higher action accuracy. Our model demonstrates superior performance over the MCTS-based agent by achieving a higher Elo difference of 542 and an increased action accuracy of 14%. This highlights the effectiveness of DIFFUSEARCH in modeling multi-step simulations when compared with the step-by-step MCTS-enhanced policy, which relies on a robust value model and necessitates a careful balance between the policy and value models."}, {"title": "4.3 ABLATIONS", "content": "Future paradigm matters We compare baselines and different future paradigms during the training of DIFFUSEARCH with horizon h = 4 in Table 3. For each future paradigm, we compare training with au-toregressive Transformer and DIFFUSEARCH. We find that directly performing future action prediction (S-AA) (Chi et al., 2023) with DIFFUSEARCH hurts performance compared to S-A due to the difficulty of future move prediction in chess. However, after we integrate future states, we observe significant performance improvements when comparing S-AA (15.07) to S-ASA (41.31), and also when comparing S-AVAV (17.63) to S-AVSAV (40.69). No further improvement is observed when integrating the values in DIFFUSEARCH, which may be attributed to training on the optimal future trajectory rather than all possible trajectories. For training using autoregressive Transformer, we observe that the overall performance hovers around 26%. This per-formance level is superior to that achieved by S-A (22.1%), attributed to the utilization of more (S, A) pairs. However, the performance falls short when compared to DIFFUSEARCH, which underscores the importance of modeling bidirectional context to leverage future information for subsequent action prediction."}, {"title": "Ensuring the validity of future world dynamics is crucial", "content": "After we discuss the future paradigm, we now investigate the effect of future quality on performance, as shown in Table 4. For better illustration, denote a sequence of future horizon 2 as $[s_{1} = f(s_{0}, a_{0}), a_{1} = g(s_{1}), s_{2} = f(s_{1}, a_{1}), a_{2} = g(s_{2})]$, where f is a world dynamic function and g is a pol-icy function. So is the current state and ao is the move sug-gested by Stockfish. We first utilize random state-action se-quences for future steps, where both actions and states were randomly selected (i.e., random world f and random policy g). This methodology did not yield performance enhance-ments. Subsequently, we explore selecting random actions and incorporating the resulting state"}, {"title": "Proper discrete diffusion modeling helps", "content": "Given the dataset D annotated with future states and actions, we in-vestigate alternative ways to train the model, as presented in Table 5. We first observe it is hard to teach the model to directly output the entire future sequence, leading to lower performance compared to auto-regressive train-ing. Secondly, we employ continuous Gaussian diffu-sion VDM (Kingma et al., 2021) and observe its superior performance compared to the Direct and auto-regressive methods, but inferior compared to discrete approaches. The absorbing diffusion with reciprocal $\\lambda_{t} = 1/t$ ob-tained by setting $\\alpha_{t}$ = 1 - $\\sqrt[T]{\\frac{t-1}{t}}$ in Eq.(3) is a simplified expression from D3PM (Austin et al., 2021), which we find significantly outperforms continuous diffusion. Fi-nally, we discover a linear $\\lambda_{t}$ (Bond-Taylor et al., 2022; Zheng et al., 2023) further exceeds the constant and reciprocal ones, as well as the multinomial counterpart."}, {"title": "4.4 ANALYSIS", "content": "Does DIFFUSEARCH predict accurate future information? We analyze the percentage of valid actions and the optimal action recommended by Stockfish for each predicted action. The best $a_{0}$ metric is exactly the action accuracy by definition.. Additionally, we assess whether each predicted state is a valid representation and if $s_{i}$ corresponds to the resulting state when action $a_{i-1}$ is taken at $S_{i-1}$. The initial state $s_{0}$ provided as input is excluded, and the results are presented in the left figure of Figure 2. We observe that the first action, denoted as $a_{0}$, are almost 100% valid. As we progress through future steps, both the valid rate and the optimal rate decline. However, even at i = 3, where the valid rate stands at 50%, it surpasses the random move baseline of approximately 1.6% (calculated as the average number of legal actions per move, 31, divided by the total number of moves, 1968). This indicates that the model retains a certain level of predictive capability for future moves, albeit with reduced performance. A similar pattern appears in the evaluation of states, where the accuracy is perfect for the first predicted state $s_{1}$ but diminishes in subsequent predictions. In Appendix Table 8, we demonstrate that further increasing the training data enhances the effective-ness of the world model within DIFFUSEARCH, achieving over 90% accuracy in predicting valid and matched future states corresponding to the preceding action.\nHow does DIFFUSEARCH leverage future information? We attributes the future-aware ability of DIFFUSEARCH mainly to self-attention and iterative decoding process, as shown in the middle and right figures of Figure 2, respectively. When employing a single self-attention layer, our model"}, {"title": "Explicit search vs. Implicit search", "content": "Based on our previous analysis, we can consider DIFFUSEARCH as performing implicit search through the inner self-attention layers and the multi-step diffusion process. Now, we aim to evaluate the efficiency and effectiveness of this implicit search in comparison to explicit search using MCTS when conducting deeper searches. In DIFFUSEARCH, deeper search is realized by increasing the context length (80 tokens per search depth), whereas in MCTS, it is achieved through running more simulations. In the left figure of Figure 3, it is evident that DIFFUSEARCH exhibits significant enhancement when increasing search depth, while MCTS becomes stagnant after 50 simulations at a search depth of around 4. This could be attributed to the accumulated errors caused by the value network due to a limited horizon. In the middle figure of Figure 3, we measure the latency per move for Transformer with MCTS and DIFFUSEARCH on a single V100 GPU with batch size 1. The performance of Transformer combined with MCTS is notably affected by the necessity of invoking the value network for every simulation. In contrast, DIFFUSEARCH experiences only a slight rise in latency as it requires just one call for greater depth."}, {"title": "Scaling", "content": "In Figure 2, the effectiveness of model scaling in DIFFUSEARCH has been observed. Here we explore the impact of increasing the dataset size on the performance. Specifically, we conduct experiments training the DIFFUSEARCH S-ASA model with a horizon of 4 and the Transformer S-A using game sizes ranging from 5k to 100k, as shown in the right figure of Figure 3. Both the Transformer and DIFFUSEARCH models exhibit a log-2 scaling behavior, showing that doubling the training data results in a linear increase in accuracy. Scaling also enhances future prediction significantly, leading to a more valid and accurate representation of future actions and states, as well as a near-perfect level of capturing the state-action transition dynamics."}, {"title": "Case study", "content": "We sample several challenging puzzles from Lichess (with Elo ratings above 1800) to compare the predictions of DIFFUSEARCH and Transformer (S-A). Two instances are shown in Figure 4, with additional cases provided in Appendix C.4. DIFFUSEARCH demonstrates superior foresight, accurately predicting critical exchanges and piece sacrifices that lead to long-term strategic advantages. In the left puzzle, DIFFUSEARCH strategically sacrifices the rook to set up a long-term checkmate situation against the opponent. This maneuver compels the opponent to defend and creates an opportunity to capture the queen, facilitating valuable piece exchanges. The S-A model, unfortunately, makes a critical error by focusing on achieving direct checkmate without considering the possibility of the opponent's queen launching a counterattack. Similarly, in the right puzzle, DIFFUSEARCH anticipates an exchange sacrifice, correctly valuing the long-term positional benefits"}, {"title": "5 RELATED WORK", "content": ""}, {"title": "5.1 NEURAL NETWORKS FOR CHESS", "content": "The development of chess AI has undergone a significant transformation, shifting from the explicit design of search strategies and heuristics to the more data-driven and learning-based approaches. The early research, exemplified by Turing's investigations (Burt, 1955) and NeuroChess (Thrun, 1994), heavily depended on handcrafted search algorithms and heuristics, eventually leading to the development of powerful search engines like Deep Blue (Campbell et al., 2002) and Stockfish (Rom-stad et al., 2008). However, the emergence of neural network-based approaches, typically Alp-haZero (Silver et al., 2017a), marked a paradigm shift, where deep reinforcement learning equipped with Monte Carlo Tree Search (MCTS) enabled the system to learn its own heuristics, i.e., the policy and value networks, without the need for manual design (Klein, 2022; McGrath et al., 2021). The rise of large language models (LLMs) has also inspired innovations in chess AI, such as the eval-uation (Toshniwal et al., 2022; Carlini, 2023) and interpretation (Li et al., 2023a; Karvonen, 2024) of LLMs' ability to play chess, the integration of chess-related text data into training (Feng et al., 2024), and the exploring of searchless models by scaling the policy networks (Ruoss et al., 2024). Despite this, lookahead search methods like beam search (Feng et al., 2024) and even depth-one search with the value network (Ruoss et al., 2024) remain superior to the policy models as action predictors, which is the same as in the AlphaZero era (Silver et al., 2017b; Team., 2018). This under-scores the continued significance of lookahead information for move prediction in chess. In contrast to prior research, we explore directly teaching the policy model to look ahead, thereby eliminating the requirement of handcrafted search algorithms or separate value networks."}, {"title": "5.2 DIFFUSION MODELS", "content": "Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Austin et al., 2021), a powerful class of generative models, have been applied to various fields such as image generation (Dhariwal & Nichol, 2021; Rombach et al., 2022; Croitoru et al., 2023, inter alia), text generation (Li et al., 2022; Gong et al., 2022; Zheng et al., 2023; Lou et al., 2023; Li et al., 2023b, inter alia) and reinforcement learning (Janner et al., 2022; Ajay et al., 2022; Chi et al., 2023; Zhu et al., 2023, inter alia). Theoretically, diffusion models perform a multi-step denoising process to progressively convert a random noise into a data sample, and the denoising procedure can be seen as parameterizing the gradients of the data distribution (Song & Ermon, 2019), connecting them to score matching (Hyv\u00e4rinen & Dayan, 2005) and energy-based models (LeCun et al., 2006). Particularly, diffusion models have demonstrated their effectiveness in tasks that require global control and future planning, such as math reasoning (Ye et al., 2024b; Gong et al., 2024), paragraph generation (Zhang et al., 2023b),"}, {"title": "5.3 WORLD MODELS", "content": "The primary goal of a world model is to capture the underlying dynamics of the environment and predict the future outcome of certain actions in the context of model-based reinforcement learning (MBRL) (Wang et al., 2019; Moerland et al., 2023). The learned world model can be used for policy optimization of a RL agent (Sutton, 1991; Feinberg et al., 2018; Hafner et al., 2020a) and allow the agent to explicitly reason about the future consequences of its actions (Hafner et al., 2019; Schrittwieser et al., 2020; Ye et al., 2021). Most of the conventional world models (Hafner et al., 2020a;b; 2023) rely on single-step prediction, which suffer from compounding errors (Asadi et al., 2019; Xiao et al., 2019; Lambert et al., 2022). Recently, there has been growing interest in building multi-step world models utilizing diffusion models (Zhang et al., 2023a; Rigter et al., 2023; Jackson et al., 2024; Ding et al., 2024), which, however, separate the world model and policy. Similar to ours, Diffuser (Janner et al., 2022) and Decision Diffuser (DD; Ajay et al. 2022) also unify the world model with the policy. However, the modeling details, training paradigm, and action prediction differ. Specifically, both of them employ continuous diffusion while we use discrete diffusion. In addition, Diffuser trains an unconditioned model and requires a guidance function to obtain desired actions, while we model the best action and future trajectory condition on a given state. DD models state-only future trajectories and predicts the action through an inverse dynamics model while we model both future states and actions. Finally, the comparison of diffusion world model and explicit search has not been rigorously explored in domains that require precise and sophisticated lookahead such as chess, to the best of our knowledge."}, {"title": "6 CONCLUSION AND DISCUSSION", "content": "In this study, we present evidence showcasing the potential transition from employing explicit search on a one-step policy to implicit search within a future-aware policy on the classic board game Chess. The proposed model, DIFFUSEARCH, demonstrates not only superior performance compared to the searchless policy but also the policy empowered by explicit search. We provide extensive experi-ments to demonstrate and analyze DIFFUSEARCH. More broadly, the ideas and techniques discussed in this controlled task may eventually be valuable in natural language settings to improve the current next-token prediction LLMs as well.\nWe now discuss some limitations and workarounds in our study. Firstly, one usage of explicit search such as MCTS is to enhance policy performance through self-play training, such that is able to achieve amazing performance without any human supervision (Silver et al., 2017b). However, our model currently relies on an oracle (Stockfish) to provide future supervision. The integration of DIFFUSEARCH with self-play is an interesting direction to explore. Secondly, our model achieves a deeper search by increasing the context length, with the current training limited to a depth of 7, corresponding to a context length of 648. For scenarios requiring more tokens to represent a state or deeper searches, integrating techniques for long-context models may be useful for efficient training or inference (Dao et al., 2022; Gu & Dao, 2023; Xiong et al., 2024; An et al., 2024). Finally, our model's performance is currently constrained by the relatively small training dataset of up to 100k games due to resource restrictions, considerably less than the 10 million games used in the study by Ruoss et al. (2024). Continuing to scale the model and data remains a valuable direction."}, {"title": "A DETAILS ABOUT MCTS-ENHANCED POLICY", "content": "This baseline is fully aligned with the approach used in AlphaZero (Silver et al., 2017a). The one-step policy directly predicts the next action, while the MCTS-enhanced Policy constructs a search tree that simulates the future to enhance the evaluation of potential next actions. Each node s in the search tree contains edges (s, a) for all legal actions $a \\in A(s)$. Each edge stores a set of statistics,\n$\\left\\{N(s, a), W(s, a), Q(s, a), P(s, a)\\right\\},$      (4)\nwhere N(s, a) is the visit count, W(s, a) is the total action-value, Q(s, a) is the mean action-value, and P(s, a) is the prior probability of selecting that edge. The algorithm proceeds by iterating over the former three phases below and then selects a move to play:\nSelection. The algorithm begins at the root node and traverses the tree, selecting child nodes based on strategies to maximize the exploration of promising paths. Specifically, at each in-termediate node, an action is selected according to the statistics in the search tree, $a_{t} = argmax_{a}(Q(s_{t}, a) + U(s_{t}, a))$, using a variant of the PUCT algorithm,\n$U(s, a)=c_{p u c t} P(s, a) \\frac{\\sqrt{\\sum_{b} N(s, b)}}{1+N(s, a)},$     (5)\nwhere $c_{p u c t}$ is a constant determining the level of exploration; this search control strategy initially prefers actions with high prior probability and low visit count, but asymptotically prefers actions with high action-value.\nExpansion and evaluation. Upon reaching a leaf node, if it does not represent a terminal state (i.e., the end of the game), one or more new child nodes are expanded and evaluated by the policy and value model. The leaf node $S_{L}$ is added to a queue for neural network evaluation, $v = v_{\\theta}(S_{L})$ and $p = p_{\\theta}(S_{L})$. The leaf node is expanded and each edge $(S_{L}, a)$ is initialized to $\\left\\{N(S_{L}, a)=0, W\\left(S_{L}, a\\right)=0, Q(s L, a)=0, P\\left(s_{L}, a\\right)=p_{a}\\right\\}$; the value v is then backed up.\nBackup. The edge statistics are updated in a backward pass through each step $t \\leq L$. The visit counts are incremented, $N\\left(s_{t}, a_{t}\\right)=N\\left(s_{t}, a_{t}\\right)+1$, and the action-value is updated to the mean value, $W\\left(s_{t}, a_{t}\\right)=W\\left(s_{t}, a_{t}\\right)+v, Q\\left(s_{t}, a_{t}\\right)=\\frac{W\\left(s_{t}, a_{t}\\right)}{N\\left(s_{t}, a_{t}\\right)}$\nPlay. After iteratively cycling through the above phases, a move is selected to play in the root position $s_{0}$ at the end of the search based on the statistical information, e.g., proportional to its exponentiated visit count, $\\pi\\left(a \\mid s_{0}\\right)=\\frac{N\\left(s_{0}, a\\right)^{1 / \\tau}}{\\sum_{b} N\\left(s_{0}, b\\right)^{1 / \\tau}}$, where $\\tau$ is a temperature parameter that controls the level of exploration. The search tree is reused at subsequent time-steps: the child node corresponding to the played action becomes the new root node; the subtree below this child is retained along with all its statistics, while the remainder of the tree is discarded."}, {"title": "B DERIVATIONS", "content": ""}, {"title": "B.1 DISCRETE DIFFUSION", "content": "In this section", "illustration": "n$\\mathcal{L}_{v b}=E_{q(x_{0})}[D_{K L}[q(x_{T}|x_{0})||p(x_{T})"}]}