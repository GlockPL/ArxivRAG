{"title": "IMPLICIT SEARCH VIA DISCRETE DIFFUSION: A STUDY ON CHESS", "authors": ["Jiacheng Ye", "Zhenyu Wu", "Jiahui Gao", "Zhiyong Wu", "Xin Jiang", "Zhenguo Li", "Lingpeng Kong"], "abstract": "In the post-AlphaGo era, there has been a renewed interest in search techniques such as Monte Carlo Tree Search (MCTS), particularly in their application to Large Language Models (LLMs). This renewed attention is driven by the recognition that current next-token prediction models often lack the ability for long-term planning. Is it possible to instill search-like abilities within the models to enhance their planning abilities without relying on explicit search? We propose DIFFUSE-ARCH, a model that does implicit search by looking into the future world via discrete diffusion modeling. We instantiate DIFFUSEARCH on a classical board game, Chess, where explicit search is known to be essential. Through extensive controlled experiments, we show DIFFUSEARCH outperforms both the searchless and explicit search-enhanced policies. Specifically, DIFFUSEARCH outperforms the one-step policy by 19.2% and the MCTS-enhanced policy by 14% on action accuracy. Furthermore, DIFFUSEARCH demonstrates a notable 30% enhancement in puzzle-solving abilities compared to explicit search-based policies, along with a significant 540 Elo increase in game-playing strength assessment. These results indicate that implicit search via discrete diffusion is a viable alternative to explicit search over a one-step policy.", "sections": [{"title": "INTRODUCTION", "content": "Search is central to problem-solving in AI. One of the most notable examples is IBM's Deep Blue , which performs extensive search over a large space through a strong search algorithm (alpha-beta pruning; Knuth & Moore 1975), defeated the world chess champion Garry Kasparov in 1997. Search has also been utilized in neural networks. A noteworthy advancement in this progression is exemplified by AlphaGo and its successors , where the policy is guided by an extra value network through Monte Carlo Tree Search (MCTS; Coulom 2006; Browne et al. 2012). By explicitly searching into the future, the decision to be taken can be iteratively refined .\nRecent research on Large Language Models (LLMs) demonstrates the utilization of a similar framework. Although scale-up, the autoregressive one-step policy addresses only a portion of the problems and relies on explicit search on complex tasks , highlighting their inherent limitations in long-term planning. This explicit search-demanding approach, however, is not quite satisfactory, as the repeated invocation of the value model can result in an accumulation of errors if the value model is inaccurate and increased inference costs for long-horizon rollouts . Given the essence of explicit search (e.g., MCTS) over one-step policies lies in iteratively looking into the future and leveraging the future to enhance the next token (or action) prediction, our research question is:\nCan the policy model predict and utilize the future by itself to improve the next token (or action) prediction without relying on explicit search during inference?"}, {"title": "PRELIMINARIES", "content": "This section introduces key concepts and notations in the chess-playing problem and diffusion modeling."}, {"title": "METHODOLOGY", "content": "In this section, we introduce DIFFUSEARCH, an approach that looks into the future world via discrete diffusion modeling without any explicit search at inference time, with a focus on the chess-playing task."}, {"title": "MODELING", "content": "In order to endow the model with the capability to predict and utilize the future, we consider training the model in a supervised way following , leaving self-play training from scratch for future work. We provide the current state $s_i$ as the history representation following prior studies. For future world representation, we consider a variety of alternative variants, such as purely future actions (denoted as s-aa), action-states (denoted as s-asa), and action-state-values (denoted as s-avsav, etc.). We analyze the performance of different future paradigms in Section \u00a74.3. The s-asa approach is ultimately chosen as our modeling paradigm considering the effectiveness and simplicity. The policy distribution at state $s_i$ considering the future is given by:\n$P_{\\theta}(a_i, s_{i+1}, a_{i+1},..., s_{i+h-1}, a_{i+h-1}|s_i)$,\nwhere h > 1 is the future horizon."}, {"title": "TRAINING", "content": "In order to train a policy that models Eq.(2), we consider a supervised training approach leveraging Stockfish , currently the world's strongest search-based engine, as an oracle to label board states extracted from randomly selected games on lichess.org. We approximate the optimal policy $\\pi^*$ with $\\pi^{SF}$ and obtain each action by taking $a_j^{SF} = arg max_{a_j} Q^{SF}(s_j, a_j)$. For a given world horizon h, we construct a dataset $D = \\{(s_i, (a_i^{SF}, a_{i+1}^{SF}, s_{i+1},..., s_{i+h-1}, a_{i+h-1}^{SF}))\\}$, where the oracle future path means playing some move that has the maximum evaluation for the best opponent's reply for both players.\nAn intuitive way to use D is to train a network to directly predict the entire concatenated next action and future sequence $a_i^{SF} || z_i^{SF} (z_i^{SF} := s_{i+1} || a_{i+1}^{SF} || ... || s_{i+h-1} || a_{i+h-1}^{SF})$. Nonetheless, we observe that this approach not only fails to predict the future but also impedes the learning of the next action $a_i^{SF}$ (see Section \u00a74.3). Therefore, we resort to diffusion modeling as a powerful sequence modeling approach with strong expressive capabilities. The bidirectional multi-layer self-attention and iterative denoising mechanism are expected to enhance the prediction of the next action by considering future information. Specifically, we consider discrete diffusion modeling and streamline $L_{vb}$ in Eq.(1) into a weighted cross-entropy loss motivated by . The KL term $DKL[q(x_{t-1}|x_t, x_o)||p_\\theta(x_{t-1}|x_+)]$ for each individual random variable is simplified as $-\\lambda_{t} 1_{x_t\\neq x_0}x_o x_t \\log f(x_t;\\theta)$, where and $L_{vb}$ becomes:\n$L_{vb} = -E_{q(x_0)}[\\sum_{t=1}^{T} \\lambda_t E_{q(x_t|x_0)} 1_{x_t\\neq x_0}x_0 x_t \\log f(x_t;\\theta)]$,\nwhere $\\lambda_t = \\frac{a_{t-1}^{1-a_t}}{a_{t}^{1-a_{t-1}}} \\in (0, 1]$ is a time-dependent reweighting term that assigns lower weight for noisier $x_t$, and $a_t \\in [0, 1]$ belongs to a predefined noise scheduler that controls the level of noise in $x_t$ at timestep t. We explore multiple variants of $\\lambda_t$ in Section \u00a74.3. To enable conditional training with a given state, we freeze the state tokens and perform denoising on the next action $a_i^{SF}$ and all futures tokens $z_i^{SF}$. We employ Monte Carlo sampling with regard to x, $x_t$ and t when optimizing $L_{vb}$."}, {"title": "INFERENCE", "content": "During inference, taking $arg max_{a_i} P_{\\theta}(a_i|s_i)$ as in one-step policy in DIFFUSEARCH requires marginalizing over all future with horizon h, i.e., $p_{\\theta}(a_i|s_i) = \\sum_{z_i} P_{\\theta}(a_i, z_i|s_i)$, which is intractable due to the exponential-growing search space when h goes larger, e.g., the game tree contains $b^h$ nodes and the branching factor b is around 31 on average in chess . One simplified approach to comparing actions is to measure the best future if one action is taken, which can be reflected by the joint probability $P_{\\theta}(a_i, z_i|s_i)$. Therefore, we resort to $arg max_{a_i z_i} P_{\\theta}(a_i, z_i|s_i)$, which does not involve marginalization and can be achieved by sampling from the trained model.\nDuring diffusion sampling, we adopt an easy-first decoding strategy , which achieves better performance compared to the random decoding approach employed by Austin et al. (2021). Specifically, at diffusion timestep t, the tokens within the least $100 * \\frac{1}{T}%$ predictive log-likelihood are selected to be reset to the noise state. To change search depth, we mainly train separate models on D with different h, and study a single model on D with mixed h in Appendix C.1."}, {"title": "EXPERIMENTS", "content": "Baselines We compare our model with three Transformer models proposed in : State-action model (S-A) which learns to predict next move via behavioral cloning; State-value model (S-V) which predicts next move via comparing the value of next states; and Action-value model (SA-V) which predicts next move via comparing the value of each legal actions at the current state. We also integrate the trained S-A and S-V models into MCTS following AlphaZero"}, {"title": "MAIN RESULTS", "content": "We report the prediction and playing strength comparison for our model against baselines in Table 2. Additionally, we report the performance of Stockfish 16 with a time limit of 0.05s per legal move, which stands as the oracle used to generate our dataset. We find DIFFUSEARCH significantly outperforms the S-A model by 653 Elo and 19% action accuracy, indicating the effectiveness of DIFFUSEARCH in improving next action prediction through future prediction. Remarkably, despite utilizing 20 times fewer data records than the SA-V model, our model demonstrates superior performance with approximately 10% higher action accuracy. Our model demonstrates superior performance over the MCTS-based agent by achieving a higher Elo difference of 542 and an increased action accuracy of 14%. This highlights the effectiveness of DIFFUSEARCH in modeling multi-step"}, {"title": "ABLATIONS", "content": "Future paradigm matters We compare baselines and different future paradigms during the training of DIFFUSEARCH with horizon h = 4 in Table 3. For each future paradigm, we compare training with au-toregressive Transformer and DIFFUSEARCH. We find that directly performing future action prediction (S-AA) with DIFFUSEARCH hurts performance compared to S-A due to the difficulty of future move prediction in chess. However, af-ter we integrate future states, we observe significant performance improvements when comparing S-AA (15.07) to S-ASA (41.31), and also when comparing S-AVAV (17.63) to S-AVSAV (40.69). No further\nEnsuring the validity of future world dynamics is crucial\nAfter we discuss the future paradigm, we now investigate the effect of future quality on performance, as shown in Table 4. For better illustration, denote a sequence of future horizon\n$z$ as $[s_1 = f(s_0, a_0), a_1 = g(s_1), s_2 = f(s_1, a_1), a_2 = g(s_2)]$, where f is a world dynamic function and g is a pol-icy function. So is the current state and ao is the move sug-gested by Stockfish. We first utilize random state-action se-quences for future steps, where both actions and states were randomly selected (i.e., random world f and random policy g). This methodology did not yield performance enhance-ments. Subsequently, we explore selecting random actions and incorporating the resulting state"}, {"title": "ANALYSIS", "content": "Does DIFFUSEARCH predict accurate future information? We analyze the percentage of valid actions and the optimal action recommended by Stockfish for each predicted action. The best $a_0$ metric is exactly the action accuracy by definition.. Additionally, we assess whether each predicted state is a valid representation and if $s_i$ corresponds to the resulting state when action $a_{i\u22121}$ is taken at $S_{i\u22121}$. The initial state $s_0$ provided as input is excluded, and the results are presented in the left figure of Figure 2. We observe that the first action, denoted as $a_0$, are almost 100% valid. As we progress through future steps, both the valid rate and the optimal rate decline. However, even at i = 3, where the valid rate stands at 50%, it surpasses the random move baseline of approximately 1.6% (calculated as the average number of legal actions per move, 31, divided by the total number of moves, 1968). This indicates that the model retains a certain level of predictive capability for future moves, albeit with reduced performance. A similar pattern appears in the evaluation of states, where the accuracy is perfect for the first predicted state $s_1$ but diminishes in subsequent predictions. In Appendix Table 8, we demonstrate that further increasing the training data enhances the effective-ness of the world model within DIFFUSEARCH, achieving over 90% accuracy in predicting valid and matched future states corresponding to the preceding action.\nHow does DIFFUSEARCH leverage future information? We attributes the future-aware ability of DIFFUSEARCH mainly to self-attention and iterative decoding process, as shown in the middle and right figures of Figure 2, respectively. When employing a single self-attention layer, our model"}, {"title": "RELATED WORK", "content": "The development of chess AI has undergone a significant transformation, shifting from the explicit design of search strategies and heuristics to the more data-driven and learning-based approaches. The early research, exemplified by Turing's investigations and NeuroChess , heavily depended on handcrafted search algorithms and heuristics, eventually leading to the development of powerful search engines like Deep Blue and Stockfish . However, the emergence of neural network-based approaches, typically AlphaZero , marked a paradigm shift, where deep reinforcement learning equipped with Monte Carlo Tree Search (MCTS) enabled the system to learn its own heuristics, i.e., the policy and value networks, without the need for manual design. The rise of large language models (LLMs) has also inspired innovations in chess AI, such as the eval-uation and interpretation of LLMs' ability to play chess, the integration of chess-related text data into training , and the exploring of searchless models by scaling the policy networks . Despite this, lookahead search methods like beam search and even depth-one search with the value network remain superior to the policy models as action predictors, which is the same as in the AlphaZero era . This under-scores the continued significance of lookahead information for move prediction in chess. In contrast to prior research, we explore directly teaching the policy model to look ahead, thereby eliminating the requirement of handcrafted search algorithms or separate value networks."}, {"title": "DIFFUSION MODELS", "content": "Diffusion models , a powerful class of generative models, have been applied to various fields such as image generation , text generation and reinforcement learning . Theoretically, diffusion models perform a multi-step denoising process to progressively convert a random noise into a data sample, and the denoising procedure can be seen as parameterizing the gradients of the data distribution , connecting them to score matching and energy-based models. Particularly, diffusion models have demonstrated their effectiveness in tasks that require global control and future planning, such as math reasoning , paragraph generation ,"}, {"title": "WORLD MODELS", "content": "The primary goal of a world model is to capture the underlying dynamics of the environment and predict the future outcome of certain actions in the context of model-based reinforcement learning (MBRL) . The learned world model can be used for policy optimization of a RL agent and allow the agent to explicitly reason about the future consequences of its actions . Most of the conventional world models rely on single-step prediction, which suffer from compounding errors . Recently, there has been growing interest in building multi-step world models utilizing diffusion models , which, however, separate the world model and policy. Similar to ours, Diffuser and Decision Diffuser (DD; Ajay et al. 2022) also unify the world model with the policy. However, the modeling details, training paradigm, and action prediction differ. Specifically, both of them employ continuous diffusion while we use discrete diffusion. In addition, Diffuser trains an unconditioned model and requires a guidance function to obtain desired actions, while we model the best action and future trajectory condition on a given state. DD models state-only future trajectories and predicts the action through an inverse dynamics model while we model both future states and actions. Finally, the comparison of diffusion world model and explicit search has not been rigorously explored in domains that require precise and sophisticated lookahead such as chess, to the best of our knowledge."}, {"title": "CONCLUSION AND DISCUSSION", "content": "In this study, we present evidence showcasing the potential transition from employing explicit search on a one-step policy to implicit search within a future-aware policy on the classic board game Chess. The proposed model, DIFFUSEARCH, demonstrates not only superior performance compared to the searchless policy but also the policy empowered by explicit search. We provide extensive experi-ments to demonstrate and analyze DIFFUSEARCH. More broadly, the ideas and techniques discussed in this controlled task may eventually be valuable in natural language settings to improve the current next-token prediction LLMs as well.\nWe now discuss some limitations and workarounds in our study. Firstly, one usage of explicit search such as MCTS is to enhance policy performance through self-play training, such that is able to achieve amazing performance without any human supervision . However, our model currently relies on an oracle (Stockfish) to provide future supervision. The integration of DIFFUSEARCH with self-play is an interesting direction to explore. Secondly, our model achieves a deeper search by increasing the context length, with the current training limited to a depth of 7, corresponding to a context length of 648. For scenarios requiring more tokens to represent a state or deeper searches, integrating techniques for long-context models may be useful for efficient training or inference . Finally, our model's performance is currently constrained by the relatively small training dataset of up to 100k games due to resource restrictions, considerably less than the 10 million games used in the study by . Continuing to scale the model and data remains a valuable direction."}, {"title": "DETAILS ABOUT MCTS-ENHANCED POLICY", "content": "This baseline is fully aligned with the approach used in AlphaZero (Silver et al., 2017a). The one-step policy directly predicts the next action, while the MCTS-enhanced Policy constructs a search tree that simulates the future to enhance the evaluation of potential next actions. Each node s in the search tree contains edges (s, a) for all legal actions a \u2208 A(s). Each edge stores a set of statistics,\n$\\begin{aligned} \\{N(s,a), W(s,a), Q(s, a), P(s,a)\\},\\end{aligned}$,\nwhere N(s, a) is the visit count, W(s, a) is the total action-value, Q(s, a) is the mean action-value, and P(s, a) is the prior probability of selecting that edge. The algorithm proceeds by iterating over the former three phases below and then selects a move to play:\nSelection. The algorithm begins at the root node and traverses the tree, selecting child nodes based on strategies to maximize the exploration of promising paths. Specifically, at each in-termediate node, an action is selected according to the statistics in the search tree, $a_t= argmax_a (Q(s_t, a) + U(s_t, a))$, using a variant of the PUCT algorithm,\n$U(s, a) = C_{puct}P(s, a)\\frac{\\sqrt{\\sum_b N(s,b)}}{1 + N(s,a)}$,\nwhere $C_{puct}$ is a constant determining the level of exploration; this search control strategy initially prefers actions with high prior probability and low visit count, but asymptotically prefers actions with high action-value.\nExpansion and evaluation. Upon reaching a leaf node, if it does not represent a terminal state (i.e., the end of the game), one or more new child nodes are expanded and evaluated by the policy and value model. The leaf node SL is added to a queue for neural network evaluation, $v = v_{\\theta}(s_L)$ and $p = p_{\\theta}(s_L)$. The leaf node is expanded and each edge $(s_L, a)$ is initialized to $\\{N(s_L, a) = 0, W(s_L, a) = 0, Q(s_L, a) = 0, P(s_L, a) = p_a\\}$; the value v is then backed up.\nBackup. The edge statistics are updated in a backward pass through each step t \u2264 L. The visit counts are incremented, $N(s_t, a_t) = N(s_t, a_t) + 1$, and the action-value is updated to the mean value, $W(s_t, a_t) = W(s_t, a_t) + v$, $Q(s_t, a_t) = \\frac{W(s_t, a_t)}{N(s_t, a_t)}$\nPlay. After iteratively cycling through the above phases, a move is selected to play in the root position $s_0$ at the end of the search based on the statistical information, e.g., proportional to its exponentiated visit count, $\\pi(a|s_0) = \\frac{N(s_0,a)^{1/\\tau}}{\\sum_b N(s_0, b)^{1/\\tau}}$, where \u03c4is a temperature pa-rameter that controls the level of exploration. The search tree is reused at subsequent time-steps: the child node corresponding to the played action becomes the new root node; the subtree below this child is retained along with all its statistics, while the remainder of the tree is discarded."}, {"title": "DISCRETE DIFFUSION", "content": "In this section, we provide a detailed derivation of the representation for distributions used in the objective Eq.(1), which we bring here for a better illustration:\n$L_{vb} = E_{q(x_0)}[DKL[q(x_T|x_0)||p(x_T)] + \\sum_{t=2}^{T} E_{q(x_t|x_0)} [DKL[q(x_{t-1}|x_t, x_0)||p_\\theta(x_{t-1}|x_t)]]-E_{q(x_1|x_0)} [log p_\\theta(x_0|x_1)]]"}, {"title": "A SIMPLIFIED OBJECTIVE", "content": "The categorical distribution parameterized by p for each variable that follows q(xt-1|Xt, xo) based on Eq.(6) is given as:\n$p = \\frac{q(x_t|x_0)}{q(x_t|x_0)} = \\frac{Q_tx_tQ_{t-1}x_0}{xQ_t^{T} x_0}$\n$\\begin{aligned}=\\frac{[(1 - \\beta_t)x_t + \\beta_t\\sigma_{\\alpha_t}1][\\alpha_{t-1}X_0 + (1 - \\alpha_{t-1})q_{noise}]}{\\alpha_{\\tau}x_{\\tau} + (1 - \\alpha_t)x_{\\tau}q_{noise}}\\\\= \\frac{(1 - \\beta_t)\\alpha_{t-1}x_tx_0 + (1 - \\beta_t)(1-\\alpha_{t-1})x_t\\odot q_{noise} +\\beta_t\\alpha_{t-1}\\sigma_{x_t} 1x_0 +\\beta_t(1-\\alpha_{t-1})\\sigma_{x_t}1q_{noise}}{\\alpha_t x_t + (1 - \\alpha_t)x_tq_{noise}}\\\\=\\frac{(1-\\beta_t)\\alpha_{t-1}1_{x_t\\ne x_0} +(1 - \\beta_t)(1-\\alpha_{t-1})+( \\beta_t\\alpha_{t-1}\\sigma_{x_t} 1_{x_t=x_0}+ \\beta_t(1-\\alpha_{t-1})\\sigma_{x_t}))}{\\alpha_{\\tau} x_0 + (1 - \\alpha_t)x_t},\\end{aligned}$\nwhere $\\sigma_{\\alpha_t}:=q_{noise}(u = x_t)$ represents the probability of noise drawn from $q_{noise}$ being equal to $x_t$. Note $x_t\\ne x_0 = 0$ if $x_t \\ne x_0$ otherwise 1. Thus the computation of p that parameterize $q(x_{t-1}|x_t, x_0)$ breaks down into two cases:\n$p = \\begin{cases} \\alpha_{\\tau} x_0 + (1 - \\alpha_t)\\sigma_{\\alpha t},& \\text{ if } x_t = x_0\\\\\\frac{[\\lambda_t x_t + (1 - \\lambda_t)q_{noise}]}{\\lambda_{I}x_t +(1-\\lambda_t)q_{noise(x_t)}},& \\text{if } x_t \\neq x_0, \\end{cases}$\nwhere $\\eta_t := \\frac{1-\\beta_t(1-\\alpha_{t-1})q_{noise}(u = x_t)}{1-\\alpha t}$, and qnoise (xt) = (1-t)xt+\u1e9etqnoise denotes\na noise distribution that interpolates between \u00e6t and qnoise.\nSince we set $p_{\\theta}(x_{t-1}|x_t) = q(x_{t-1}|x_t, f (x_t;\\theta))$, the KL divergence between $q(x_{t-1}|x_t, x_0)$ and\n$p_{\\theta}(x_{t-1}|x_t)$ becomes 0 when xt = x0. In the case of absorbing diffusion, \u00e6t = qnoise = em if\nxt \u2260 xo and qnoise(xt) = qnoise. p has probability At on index x0 and 1 At on the absorbing state. The model f(xt; 0) has zero-probability on the absorbing state as it never predicts the mask token. Therefore, p\u03b8(xt-1|xt) also has 1 At probability on the absorbing state. Putting them together, we derive the KL divergence as:\n$DKL [q(x_{t-1}|x_t, x_0)||p_{\\theta}(x_{t-1}|x_t)] = 1_{x_t \\neq x_0} [\\lambda_t log \\frac{\\lambda_t}{f(x_t;\\theta)x_0}+ (1 - \\lambda_t) log \\frac{1 - \\lambda_t}{1 - \\lambda_t}]$\n$\\lambda_t 1_{x_t \\neq x_0} \\log \\frac{\\lambda_t}{f(x_t;\\theta)}+C,$\nwhere $1_{x_t \\neq x_0}$ is 1 if $x_t \\neq x_0$ otherwise 0, and C is a constant. Moreover, given $a_0 = 1$ by definition and therefore \u03bb\u03bf = 1, Lo in Eq.(1) can also be written into the final formulation:\n$L_{vb} = -E_{q(x_0)}[\\sum_{t=1}^{T} \\lambda_t E_{q(x_t|x_0)} 1_{x_t \\neq x_0}x_0 \\log f (x_t; \\theta)]$\nFor xo that represents a sequence of random variables x0 = (X0,1,...,X0,N), we can add all computed losses for each token, arriving at the final expression for the whole sequence:\n$L_{vb} = -E_{q(x_0)}[\\sum_{n=1}^{N} \\sum_{t=1}^{T} \\lambda_t E_{q(x_{t,n}|x_{0,n})} 1_{x_{t,n}\\neq x_{0,n}}x_{0,n} \\log f (x_{t,n}; \\theta)]$.\nFor multinomial diffusion, we follow Zheng et al. (2023) to adopt a reparameterized form, which results in the above formulation as well."}, {"title": "DYNAMIC SEARCH DEPTH IN DIFFUSEARCH", "content": "In explicit search algorithms, the search depth is predefined either through an exact parameter as in depth-first search, or a related parameter such as the number of simulations as in MCTS. In DIF-FUSEARCH, the deeper search is achieved by extending the context length of the input. In this"}]}