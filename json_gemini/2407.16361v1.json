{"title": "Virtue Ethics For Ethically Tunable Robotic Assistants", "authors": ["Rajitha Ramanayake", "Vivek Nallur"], "abstract": "The common consensus is that robots designed to work alongside or serve humans must adhere to the ethical standards of their operational environment. To achieve this, several methods based on established ethical theories have been suggested. Nonetheless, numerous empirical studies show that the ethical requirements of the real world are very diverse and can change rapidly from region to region. This eliminates the idea of a universal robot that can fit into any ethical context. However, creating customised robots for each deployment, using existing techniques is challenging. This paper presents a way to overcome this challenge by introducing a virtue ethics inspired computational method that enables character-based tuning of robots to accommodate the specific ethical needs of an environment. Using a simulated elder-care environment, we illustrate how tuning can be used to change the behaviour of a robot that interacts with an elderly resident in an ambient-assisted environment. Further, we assess the robot's responses by consulting ethicists to identify potential shortcomings.", "sections": [{"title": "1 Introduction", "content": "Ethical behaviour represents a crucial aspect of Human-Robot Interaction (HRI) in real-world applications. It is commonly agreed that the robots that share their working environments with humans, such as those that provide care, assistance, transportation, or companionship to humans, must adhere to and reason about their behaviour within the ethical framework of the community around them [50]. This is especially true in the care domain as care robots interact with vulnerable populations that require special care and protection [43]. Consequently, many ways have been proposed to implement ethical behaviour in robots. Many proposed techniques adopt a top-down approach, based on ethical theories such as deontology [2] or consequentialism [44] (e.g., [11,47,49]). A relatively small number of proposed methods use a bottom-up approach where they utilise learning algorithms such as reinforcement learning, that enable the robots to learn the ethical behaviours from the environment (e.g., [45,1]). Another set of implementations uses a hybrid approach where they integrate both top-down and bottom-up methods in different parts of their ethical framework (e.g., [3,10,52])."}, {"title": "2 Current Approaches and Ethical Tuning", "content": "The field of Machine Ethics acknowledges that human value alignment is an important part of any robotic application that interfaces with humans in social contexts [41]. Some researchers argue that there is no value in developing explicitly ethical agents [51], and many others disagree [18,48]. Large-scale human preference studies such as 'The moral machine experiment' [9] bring forth an interesting finding, which is that the expected behaviour of a robot, even for a minimal ethical dilemma, changes throughout the world according to the culture and the environment the robot is situated in. This is also discussed in the elder-care robots domain. Specifically, the behavioural expectations of the same robot in ethically charged scenarios can vary depending on the context in which it is used [38]. This suggests that no single behaviour is universally appropriate, as imposing a majority's values on a group with a distinct cultural ethos can be considered morally incorrect.\nThree approaches have been used to align human ethical values with autonomous systems, namely; Top-down, Bottom-up, and Hybrid [50]. Many generalist ethical theories of the world follow a top-down approach to ethics. This means that almost all the systems that are based on traditional ethical theories, such as deontology, legal codes, and consequentialist ethics use the top-down approach to machine-implemented ethics. In this method, the system designers encode the ethical or unethical behaviour into the robot at the design time (e.g., [49,13,46,21,11,47]). A bottom-up approach to system design involves creating social and cognitive processes that interact with each other and the en-"}, {"title": "3 Role of Character in a Robot", "content": "Many ethical traditions, such as Buddhism [30], Confucianism [14] and Aristotelian [6], share a common focus on \"virtue ethics\". Among these, the Aristotelian approach is regarded as the most influential in the Western context. All virtue ethics traditions share two core characteristics [19];\n1. Character as a primary aspect of moral evaluation\n2. Learn to act morally by observing virtuous individuals.\nThe concept of virtue ethics works with humans because it is in line with the natural way we acquire knowledge. Our core values, which are shaped by our upbringing, guide us to the flourishing state that Aristotle's Nicomachean Ethics describes [6]. As a society, we uphold virtuous characters as moral exemplars for others to observe and imitate. These actors are not expected to fulfil specific ethical codes, but to have a good character in a way that allows society and themselves to flourish. However, according to virtue ethics, moral behaviour is not simply a matter of learning from habituation. It also requires that the moral agent makes deliberate decisions and acts with the right reason, once the moral character is well-established by habituation. This makes a virtuous person consistent, predictable and appropriate in many situations [32]. Virtue ethics posits that one should avoid extremes in behaviour, as these are considered vices. Virtuous behaviour is always finding the right balance between two vices, one of excess of trait and one of lack of it, which is known as the 'golden mean' [29]. The knowledge of identifying this 'mean' comes from the wisdom a person has gathered in their lifetime.\nUnfortunately, this account of virtue ethics is difficult to formulate in the form of a program. By nature, virtue ethics is, and ought to be, imprecise and uncodifiable [26]. However, with bottom-up techniques such as machine learn- ing, one could potentially make the robot grasp the ethical patterns of the envi- ronment. Many ethical agent implementations that champion flavours of virtue ethics such as Howard and Muntean's model of a moral agent using neuroevolu- tion [25], Guarini's model of a moral agent using recurrent neural networks [23], ethical decision making systems using tuned large language models [27,24], and"}, {"title": "4A PSRB-Capable Medication Reminding Robot", "content": "We simulate an ethical dilemma faced by a care robot operating in an ambient assisted living (AAL) setting, referred to as the \"Medication Dilemma\". This ethical dilemma, presented by Anderson et al. [3], is derived from the works of Buchanan and Brock [12]. Many variations of this dilemma are used in computational machine ethics literature [4,36,10,5]. The dilemma centres on the conflict between resident autonomy and their wellbeing, a frequent issue in care environments with elderly [43,38]. The manner in which a system handles this issue can significantly impact the resident's trust, dignity, and quality of life.\nWe created a virtual simulation of an AAL environment, using a modified version of MESA agent-based modelling framework [28]. The environment contains a resident agent and a robot agent (more details in Appendix A.1) who communicate with each other.\nMedication Dilemma One of the tasks of a robot is to remind the residents of their medication times. The robot can detect whether the resident took the medicine or not, with good accuracy. The robot uses timers to keep track of the medication cycles. The resident can either acknowledge the reminder, or snooze it. Each interaction between the robot and the resident is documented and is reviewed several times a week by an offsite care worker. Once the robot observes the resident taking the medicine, it ends that cycle and resets the timer. If the robot does not detect the resident taking the medication, it has three options:\n1. The system can immediately report the incident to an offsite care-worker, allowing them to take swift action to enhance the resident's well-being. However, this may impede the resident's autonomy. 2. The system can simply log the incident for transparency and then reset the timer. This approach prioritises resident autonomy, although it will not result in the optimal wellbeing of the resident. 3. The system can offer a follow-up reminder to the resident, focusing on their wellbeing and supporting their autonomy by suggesting they take their medication while still providing them the freedom to choose. Picking this option continuously degenerates into option 2.\nThe dilemma is as follows: The robot reminds the resident to take their medication. After snoozing once, the resident acknowledges the reminder. Yet, the robot detects that the medication has not been taken. Choosing among the three options mentioned above involves a trade-off of values. What should the robot do next?"}, {"title": "4.2 Ethical Governor", "content": "Figure 1 shows an overview of the implementation. The ethical layer acts as an ethical governor [7] to the system. It evaluates behavioural alternatives in a given situation using the robot's perception data, reasons about the ethical acceptability of each behaviour using the predefined rules, expected utilities, expert opinions, and the robot's character. Then it recommends the most ethically acceptable behaviours to the robot. If the ethical layer suggests more than one action, the robot prioritises the resident's commands over other actions.\nRule Checking Module The rule checking module (d) checks the permissibility of each action against a pre-programmed rule set and stores the results with the IDs of the violated rules in the blackboard. This implementation's rule checking module adheres to the following rules:\n1. It is not permissible to disobey user instructions.\n2. If the resident acknowledged the reminder and did not take the medication, report it to the care-worker.\nStakeholder Utility Calculation Module This module computes the utilities, resident's wellbeing (Wi) and autonomy (Aui), for each action i, at every step (Appendix A.2), as these values are central to the dilemma. The highest autonomy value is given when the robot follows the resident's instructions. The biggest violation of autonomy is when the resident is physically restrained by the robot. Immediately reporting an incident to a care-worker, is considered equal to disobeying the resident. However, just recording the incident carries a positive autonomy score assuming that missing the dose is the resident's intention. Under the same assumption, the module also gives an increasing negative autonomy score for each follow up (f) after the first reminder. The module uses a Gamma distribution to model the wellbeing distribution. The parameters for the gamma"}, {"title": "5 Simulation results", "content": "We used six cases of the 'Medication Dilemma' by changing the medicine impact (suggesting the resident's need for the medication), and the number of previously missed doses to demonstrate how different ethical governor implementations affect the robot's behaviour. Agents differ only in their character configurations. The medicine impact is low (Em = 1) when the medication does not have a substantial health benefit or harm (e.g., Painkiller), medium (em = 2) if missing the medication can cause some health problems (e.g., Blood pressure medication), and high (em = 3) when missing a dose of medication can be fatal (e.g., Insulin). In all cases, the resident neglects to take their medication, opting instead to snooze and acknowledge the reminder, repeatedly, until the robot resets the timer. The logs from the experiment runs and the source code for the implementations are available in an online public repository 1.\nFor this simulation we use four different character profiles as shown in Figure 3. These profiles have varying priorities and risk tolerances:\n\u2022 Character_a - High Autonomy, Very Low Risk Propensity: This character values autonomy, but is less likely to take risks and does not have precedence on wellbeing."}, {"title": "6 Discussion", "content": "The simulations show that the PSRB-capable ethical governor makes the robot more flexible while maintaining predictability. The PSRB model offers a simpler way to localise than the existing methods (reviewed in section 2) because they are either inflexible or unpredictable.\nAlthough the output of the PSRB capable ethical governor is not as predictable as a top-down engineered agent (e.g., a deontological agent), the rigidity of the top-down approaches does not align with the virtue ethics tradition, which rejects such extreme actions. Agents based on the PSRB evaluator can behave in a moderate manner, even when the character strongly favours a particular trait (e.g., Mar, Mwr, and Ma), by utilising bottom-up knowledge in the KB. On the other hand, the display of character can be held back by a KB, as evidenced by the second ethicist's feedback. However, this conservatism was introduced by the PSRB process as a safety measure to control the excessive utility optimisation in the utilitarian calculations involved in the pro-social reasoning process.\nThe virtue ethics-inspired architecture allows the robots to be tuned in two ways. The first approach involves adjusting the character parameters' values to align with the ethical requirements of the environment. For example, one facility may require its robots to be more proactive towards enhancing residents' autonomy, whereas another may also want to focus on the residents' autonomy, but only under minimal risk conditions. The former institution can opt for a character profile akin to Character_ar, while the latter institution can tune the robot with a character profile resembling Character_a. If the first tuning approach does not suffice in certain specific instances, one can employ the secondary method to further refine the robot's performance. This involves adjusting the knowledge base to reflect the desired behaviour. Unlike other machine learning approaches, using CBR, one can precisely identify the cases that influence behaviour. Consequently, one can integrate or adjust expert insights to better represent their perspective. As long as this desired behaviour is within the robot's configured character, it will adapt its actions accordingly."}, {"title": "6.1 Limitations", "content": "The PSRB implementation presented in this paper also has limitations. This is mainly because the character traits in the PSRB evaluator are implemented as constraints on linear axes. By doing so, it removes the ability to tune the behaviour precisely, if the requirements involve a range of behaviours that cannot be described using a single set of character traits. For example, let's assume that the required behaviour of a deployment is to have behaviour 2 in case 2 and behaviour 6 in case 1. According to this implementation, there will not be any set of values for character traits that fulfil this requirement, because to have behaviour 6 in case 1, the character should have a higher wellbeing bias, which will trigger similar wellbeing-focused behaviour in case 2 (because case 2 has a higher wellbeing impact), which behaviour 2 is not. However, a different character trait implementation, in the same PSRB evaluation framework will be able to solve this issue.\nFinally, it should be noted that the implementation showcased in this simulation, encompassing both the dilemma and the environment, has been deliberately simplified to illustrate the concept and is not intended for real-world use. More capable robots, more precise rule sets, accurate utility models, and well-researched character models are a must when using this approach for real-world implementation. However, the system works with sub-optimal rules and utility functions by compensating for each other's shortcomings by working together [39]. The PSRB architecture allows ethical tests other than stakeholder utility calculation. Hence we recommend that robot developers explore and integrate different models that can better capture the context alongside utilitarian calculations. Furthermore, it is important to update the KB regularly to compensate for the evolving nature of ethics."}, {"title": "7 Conclusion", "content": "This paper introduces a method inspired by virtue ethics, which enables more efficient ethical tuning compared to current techniques. The method combines decision-making based on character traits with bottom-up learning from moral exemplars, to maintain the predictability of the robot's behaviour while keeping the flexibility the latter offers. To the best of our knowledge, there is no other implementation that combines both these desirable features of virtue ethics. The paper demonstrates the system's capabilities by analysing its behaviour and presenting the perspectives of two ethicists on these behaviours. It concludes with recommendations for implementing the proposed architecture in real-world elder-care robots."}, {"title": "A Appendix", "content": "The module uses the formula in equation 1 to calculate Aui, based on the number of follow ups (f) that the robot performs.\n1:\nif the robot obeys a resident instruction\n0.5:\nif only recording a incident\n0:\nif no instructions given\nAui =\n(1)\n\u22120.1 \u22c5 f:\nif follow up\n\u22120.7:\nif the robot disobeys a resident instruction or report\n\u22121:\nif the resident is physically restrained by the robot\nThe module uses a Gamma distribution (eq. 2) to model the wellbeing distribution, with the shape parameter \u03b1 = 1.325em \u2212 9.475em + 18.15, the scale parameter \u03b2 = e-2.65 \u2212 (d/2) + 0.01 (each follow up is considered as a fraction of missed dose depending on the state of the reminder), and location parameter \u03c5=-1. The highest probable utility value is obtained by PMax_util (g(x, \u03b1, \u03b2, \u03c5)) using a resolution of 0.05. The wellbeing utility Wi for behaviour i is computed using Equation 3.\ng(x, \u03b1, \u03b2, \u03c5) = ( x \u2212\u03c5 )( \u03b1 \u2212 1 ) \u22c5 exp ( \u2212( x \u2212\u03c5 )/ \u03b2 )\n\u03b2 \u22c5 \u0393( \u03b1 )\n(2)"}, {"title": "B Algorithm", "content": "The action breaks the rules <rule_ids>. However, this action in this context is considered desirable by experts Since it increases values greatly, while not reducing the other values <other_values> by a considerable amount, and the outcome is within accepted risk levels, deemed accepted by the PSRB system.\nThe action does not break any rules. However, this action in this context is considered undesirable by experts. Since the action outcomes introduce a high risk, deemed not accepted by the PSRB system.\nThe action does not break any rules. However, this action in this context is considered undesirable by experts. Since it decreases values by a considerable amount, the action is deemed unacceptable by the system\nThe action breaks the rules <rule_ids>. However, this action in this context is considered desirable by experts Although the value tradeoff is satisfactory, the risk taken by the action is not acceptable to bend the rule."}]}