{"title": "Improving Fast Adversarial Training via Self-Knowledge Guidance", "authors": ["Chengze Jiang", "Junkai Wang", "Minjing Dong", "Jie Gui", "Xinli Shi", "Yuan Cao", "Yuan Yan Tang", "James Tin-Yau Kwok"], "abstract": "Adversarial training has achieved remarkable ad-vancements in defending against adversarial attacks. Among them, fast adversarial training (FAT) is gaining attention for its ability to achieve competitive robustness with fewer computing resources. Existing FAT methods typically employ a uniform strategy that optimizes all training data equally without con-sidering the influence of different examples, which leads to an imbalanced optimization. However, this imbalance remains unexplored in the field of FAT. In this paper, we conduct a comprehensive study of the imbalance issue in FAT and observe an obvious class disparity regarding their performances. This disparity could be embodied from a perspective of alignment between clean and robust accuracy. Based on the analysis, we mainly attribute the observed misalignment and disparity to the imbalanced optimization in FAT, which motivates us to optimize different training data adaptively to enhance robustness. Specifically, we take disparity and misalignment into consideration. First, we introduce self-knowledge guided regularization, which assigns differentiated regularization weights to each class based on its training state, alleviating class disparity. Additionally, we propose self-knowledge guided label relaxation, which adjusts label relaxation according to the training accuracy, alleviating the misalignment and improving robustness. By combining these methods, we formulate the Self-Knowledge Guided FAT (SKG-FAT), leveraging naturally generated knowledge during training to enhance the adversarial robustness without compromising training efficiency. Extensive experiments on four standard datasets demonstrate that the SKG-FAT improves the robustness and preserves competitive clean accuracy, outperforming the state-of-the-art methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning models have demonstrated impressive per-formance across various applications [1], [2]. However, they remain vulnerable to adversarial attacks, which raise signifi-cant security threats [3]\u2013[5]. Adversarial attacks craft elaborate perturbations to clean examples which can deceive the model"}, {"title": "B. Multi-Step Adversarial Training", "content": "Adversarial training improves the robustness of the model fo(\u00b7) by adding perturbations to clean examples, thereby generating AEs x', which are then incorporated into the training data [27]\u2013[29]. This process can be mathematically represented as a minimax optimization problem:\n$\\min _{\\theta}\\left\\{E_{\\{x, y\\} \\sim D}\\left[\\max _{x^{\\prime}:\\left\\|x^{\\prime}-x\\right\\|_{p} \\leq \\epsilon} L\\left(f_{\\theta}\\left(x^{\\prime}\\right), y\\right)\\right]\\right\\}$, (1)\nwhere \u03f5 denotes the perturbation budget, ||\u00b7||p is the p-norm operator, and AE x' = x + \u03b4 with \u03b4 representing the adversarial perturbation [15], [30]. The internal optimization problem towards maximizing the classification loss is achieved by generating the worst-case AEs. Meanwhile, the model requires correctly classifying AEs to minimize the classification loss [31], [32]. Multi-step adversarial training exploits PGD for generating AEs to achieve internal maximization within the paradigm (1). The iterative role of PGD is defined as follows:\n$\\mathbf{x}_{t+1}=\\Pi\\left(\\mathbf{x}_{t}+\\alpha \\operatorname{sign}\\left(\\nabla_{\\mathbf{x}} L\\left(f_{\\theta}\\left(\\mathbf{x}_{t}\\right), y\\right)\\right)\\right)$, (2)\nwhere \u03a0(\u00b7) denotes the projection operator, sign(\u00b7) represents the sign function, \u03b1 is step size, and \u2207x denotes the gradi-ent of loss L(fo(x), y) respect to x. Multi-step adversarial training executes multiple backward propagations during each training step, depending on the iteration number of PGD [33]. Although this approach improves model robustness, it significantly increases the training time, requiring more than several or even ten times the normal training duration. [17]."}, {"title": "C. Fast Adversarial Training", "content": "To enhance model robustness while minimizing the training time expenses, FAT employs FGSM instead of PGD to gener-ate AEs for training. FGSM requires only a single backward propagation step to produce AEs [34], [35], significantly reducing training time compared to multi-step adversarial training methods [36], [37]. The specific definition of FGSM within FAT is introduced as follows:\nx' = x + \u03a6 (\u03b4o + \u20ac \u2022 sign (VL(f(x + 80), y))), (3)\nwhere \u03b4\u03bf represents the initialization perturbation [18]. For the case of \u03a6(\u00b7) being identity map and \u03b4\u03bf equals zero, the equation (3) is the formula of FGSM [38]. To improve the diversity of AEs and robustness, FGSM-RS samples from a uniform distribution U(\u2212\u20ac, \u20ac) to generate initialization perturbation \u03b4\u03bf [17]. This method can effectively alleviate catastrophic overfitting and improve robustness. On this basis, the Noise-FGSM (NFGSM) removes the projection operator \u03a6(\u00b7) and increases noise intensity to enhance the training stability [18]. After that, the sample-dependent adversarial initialization FGSM (FGSM-SDI) is introduced to enhance the quality of AEs [39]. This approach leverages a generator to improve AEs quality, thereby boosting overall perfor-mance. Besides, GradAlign introduces a regularization that aligns the gradients between clean examples and AEs as Ex~D[cos(\u2207xL(fo(x), y), \u2207\u00e6'L(fe(x'), y))], where cos(\u00b7) denotes cosine similarity [36]. GradAlign mitigates catas-trophic overfitting and even makes the performance of FAT more competitive than multi-step adversarial training [33]. Subsequently, Jia et al. present the FGSM-PGI [40], which leverages historical adversarial perturbations obtained from the last batch (PGI-BP), last epoch (PGI-EP), previous all epoch (PGI-MEP) as the initialization for generating adversarial perturbations. FGSM-PGI also contains a regularization to minimize the prediction gap between clean examples and AEs. This method significantly enhances the performance of"}, {"title": "D. Limitation of Existing Solutions and Overall Motivations", "content": "While existing methods have attempted to enhance FAT by addressing various aspects such as initialization, regulariza-tion, and mitigating catastrophic overfitting, they adopt a uni-fied optimization and processing for all examples, overlooking the impact of robust accuracy disparity among examples [24], [26]. The performance variations caused by accuracy disparity in standard adversarial training [41] prompt us to investigate whether FAT exhibits similar characteristics. Meanwhile, re-cent advances demonstrate that addressing or exploiting these performance disparities can significantly improve adversarial robustness [25]. This drives our exploration of performance disparities within the FAT paradigm, leveraging empirical analysis to guide novel strategies that enhance FAT perfor-mance without extensive hyperparameter tuning or sacrificing computational efficiency. To this end, we design and conduct comprehensive experiments, analyzing FAT from a class-wise perspective and uncovering some counter-intuitive phenomena."}, {"title": "III. PROBLEM ANALYSIS", "content": ""}, {"title": "A. Analysis from Class-Wise Perspective", "content": "To explore the training accuracy disparities and their man-ifestation under the FAT paradigm, we reexamine the class-wise clean and robust accuracy. We conduct experiments using ResNet-18 on the CIFAR-10 dataset with two representative FAT methods: FGSM-RS and PGI-BP. The perturbation budget epsilon is set to 8/255 and step size alpha is set to 8/255. Given that the vanilla FGSM-RS is prone to catastrophic overfitting, we introduced a regularization technique to align the outputs of clean examples and AEs [42]. This regularization helps prevent catastrophic overfitting, enabling a more reliable analysis. Figure 1 presents the class-wise clean and robust accuracy of ResNet18 on the CIFAR-10 training set using FGSM-RS [17] and PGI-BP [40]. As illustrated, there are notable differences in the robust and clean accuracy across different classes. Additionally, Figure 1 reveals that the MSE loss also varies among classes, and identifies a pattern consistent across different FAT methods. Note that while such investigations have been reported in prior studies on standard adversarial training, we extend these findings to FAT, confirming their presence in this context as well. Moreover, our analysis uncovers deeper and more intriguing phenomena that have not been previously reported. Specifically, the top three classes with the highest clean accuracy consist of features that are easily learned by neural networks, leading to superior robust accuracy. Conversely, the bottom two classes with the lowest clean accuracy demonstrate the opposite trend. In other words, improving robust accuracy is generally easier in classes where clean accuracy is more readily improved. According to the trends depicted in Fig.1(a) and (b) for different class curves, it appears that there exists a positive correlation between clean and robust accuracy in each class. This prompts us to raise an intuitive question: Can we develop differentiated training strategies for different classes, thereby improving adversarial robustness? Inspired by the above observations and this question, we propose two methods in subsections IV-A1 and IV-B to enhance FAT. Meanwhile, the positive correlation between clean and ro-bust accuracy weakens for classes with middle-range accuracy, such as horse' and frog' or dog' and deer'. The competitive clean accuracy of these classes does not necessarily translate to robust accuracy, resulting in a misalignment between their rankings in clean and robust accuracy (as highlighted in red). We identify this issue as being caused by the misalignment between clean and robust performance. This counter-intuitive disparity cannot be fully analyzed from a purely class-wise perspective. Therefore, we present a perspective of accuracy alignment and conduct the corresponding analysis."}, {"title": "B. Analysis from a Perspective of Accuracy Alignment", "content": "Previously, we have identified that the clean and robust accuracy of partial classes is not aligned, which means that"}, {"title": "1) Definition of the Perspective of Accuracy Alignment:", "content": "The following definitions are provided to introduce the per-spective of accuracy alignment. First, the perspective index (a, a) is formulated as (a, a) = ((ci - c), (ri - r)), where a and a denote the clean and robust classification coefficients of the i-th class, respectively. Meanwhile, ci, ri, c, and r represent the i-th class clean accuracy, i-th class robust accuracy, overall clean accuracy, and overall robust accuracy, respectively. Accordingly, the examples are divided into four groups in Fig. 2. Subsequently, the good clean and good robust (GCGR) class is defined as (a > 0, a > 0); the good clean and bad robust (GCBR) class for (a > 0, a < 0); the bad clean and good robust (BCGR) class for (a\u00b2 < 0, a > 0); the bad clean and bad robust (BCBR) class for (a\u00b2 < 0, a < 0). The training information of FGSM-RS on CIFAR-100 and ImageNet-100 training sets from the perspective of accuracy alignment is presented in Fig. 3. As shown in Fig. 3, the clean and robust accuracy of some examples is not strictly positively correlated, as is observed in the cases of GCBR and BCGR. Therefore, we further investigate and leverage this result to enhance the performance of FAT."}, {"title": "2) Observations and Analysis:", "content": "First, the model exhibits favorable clean and robust accuracy for classes in GCGR, indicating that the model effectively learns the features of examples for classification. This leads to output predictions for clean examples and AEs that are more closely aligned. In contrast, the model shows low clean and robust accuracy for classes in BCBR, suggesting that the model does not learn the features of BCBR examples well, resulting in a significant difference between the predictions for clean examples and AEs. Meanwhile, both clean and robust accuracy for the GCBR and BCGR classes are close to the average accuracy, indicating that the model extracts effective classification fea-tures. Furthermore, the robust accuracy of GCBR and BCGR classes is much better than that of BCBR classes (+14% for GCBR and +18% for BCGR) on CIFAR-100, indicating that the model successfully learns some of the robust features of these classes during training. Therefore, the learning difficulty of the model for these classes is lower than that for hard examples in BCBR classes. Further learning the robust features of these classes and improving overall robust accuracy is promising, making it necessary to emphasize these examples during training. Additionally, both GCBR and BCGR classes exhibit better clean accuracy than robust accuracy, indicating that their clean accuracy provides more positive guidance compared to that of BCBR classes. Leveraging their clean accuracy as additional guidance to enhance their robust accuracy is meaningful. Based on these observations, our goal is to improve FAT performance through a regularization method that utilizes naturally generated metrics during training. We achieve this by aligning the outputs for clean examples and AEs through regularization and adjusting the regularization strength according to the differences among the four groups. The strategy for regularization strength across the four groups varies based on their classification performance. For GCGR, which demonstrates good classification accuracy during train-ing, the regularization intensity is reduced to focus solely on maintaining the current performance of the model. In contrast, for BCBR, where both clean and robust accuracy are relatively low, aligning the predictions for clean examples and AEs leads to instability without contributing to improved training. Thus, the regularization intensity for BCBR is also should decreased. For GCBR and BCGR, their robust features have the potential to be further developed to enhance overall"}, {"title": "3) Class Transition Analysis:", "content": "To analyze the transition trends of classes among groups from the perspective of accuracy alignment, we select several representative classes from the CIFAR-100 for in-depth analysis. Since classes belonging to GCGR and BCBR remain stable and rarely undergo transitions during training, our analysis focuses on the transition trends of classes in GCBR or BCGR. We begin this analysis from the 30th epoch to ensure that the training is stabilized. The corresponding results are illustrated in Fig. 4. For classes in GCBR, clean accuracy improves faster than robust accuracy, leading to oscillations between GCBR and BCBR. As training progresses, the robust accuracy of some classes increases, eventually reaching GCGR. Classes that frequently appear in BCGR exhibit similar oscillation patterns to those in GCBR, and are easier to improve to reach GCGR. By comparing the frequency difference between GCBR and BCGR classes being converted into GCGR classes, classes belonging to BCGR are easier and faster to reach GCGR than the classes belonging to GCBR, which indicates that clean accuracy is easier to improve. Moreover, note that when a class undergoes transition due to changes in accuracy, typically, each transition of one class only involves clean or robust accuracy. For example, classes in GCGR generally convert to GCBR or BCGR each time. In other words, the transition of classes among four groups involves a step-by-step process. These observations lead to two conclusions. First, the oscillations suggest that the model's classification accuracy for GCBR and BCGR classes hovers around the borderline, causing the accuracy of these classes to fluctuate around the average accuracy during optimization. This insight suggests potential strategies to enhance overall training performance by emphasizing the importance of these classes during training,"}, {"title": "4) Analysis on Different Models and Large Dataset:", "content": "To validate the universality of our perspective of accuracy align-ment analysis, experiments are extended and performed on ImageNet-1K. Furthermore, we also evaluate various models on the CIFAR-100 dataset. The results for CIFAR-100 with different models are presented in Figs. 5 (a) to (c), while the results for ImageNet-1K using ResNet50 are shown in Fig. 5 (d). Specifically, we consider DenseNet-121 [43], Inception V3 [44], and Swin Transformer [45] for evaluation. For DenseNet121 and Inception V3, we employ training methods consistent with those outlined in the manuscript. For the Swin Transformer Tiny, we use Adam optimizer with a learning rate of \u03c3 = 0.0001. The results indicate that, across different models and the large-scale dataset, the analysis reveals patterns consistent with those observed in results using ResNet-18."}, {"title": "C. Discussion and Motivation of Methodology", "content": "Empirical results from both class-wise and accuracy align-ment perspectives reveal two key conclusions. First, FAT en-counters a disparity between clean and robust accuracy across different class examples, with clean accuracy surpassing robust accuracy. Second, there are certain examples where clean and robust accuracy is not strictly positively correlated. Therefore, we aim to leverage these differences and observations to enhance robustness by aligning the model's clean and robust outputs, using clean outputs to guide robust outputs. Addi-tionally, we employ differentiated training configurations from class-wise and accuracy alignment perspectives to improve"}, {"title": "IV. METHODOLOGY", "content": ""}, {"title": "A. Self-Knowledge Guided Regularization", "content": "Using regularization to improve FAT has proven to be an effective approach [46]. First, the mathematical representation of objective minimax optimization for adversarial training with regularization \u03a9 is introduced and formulated as follows:\n$\\min _{\\theta} \\mathbb{E}_{x \\sim \\mathcal{D}} \\Big[\\max _{\\delta \\in \\Lambda} \\mathcal{L}\\left(f_{\\theta}(x+\\delta), y\\right)\\Big]$. (4)\nIn this paper, we develop our method based on MSE regu-larization for two primary reasons. On the one hand, MSE requires only the inference results for clean examples and AEs, which can ensure training efficiency [47]. In contrast, regularization in GradAlign requires computing the gradients with respect to clean examples and AEs, resulting in double training time [18], [36]. On the other hand, Pang et al. demonstrate that substituting KL divergence with distance metrics can achieve a better trade-off between robustness and clean accuracy [46]. Then, the formula for MSE regularization is recalled as follows:\n$\\Omega=\\lambda \\cdot \\sum_{u=1}^{n}\\left(f_{\\theta}\\left(x^{u}\\right)-f_{\\theta}\\left(x^{u}+\\delta\\right)\\right)^{2}$, (5)\nwhere the hyperparameter \u03bb controls the penalty strength of regularization. This method minimizes the distance between the probability vectors of clean examples and AEs, providing a more reliable and stable update direction [42]. Consequently, it results in stronger attacks that enhance training effectiveness."}, {"title": "As observed in subsection III-A, datasets exhibit significant accuracy differences during the training process. Rather than being overlooked, these differences should be effectively harnessed to improve model robustness. Motivated by these technology gaps, we develop new methods to achieve two key objectives for improving training performance. First, given that clean accuracy is higher than robust accuracy, we aim to enhance robust accuracy by minimizing the outputs for clean examples and AEs. Second, considering the performance differences across the class-wise and accuracy alignment per-spectives, we determine the regularization strength based on these differences without introducing extra hyperparameters. To accomplish these objectives, we utilize natural metrics derived from the training to develop new regularization.", "content": "1) Class-Wise Guided Regularization: Motivated by the difference in class-wise perspective as presented in subsection III-A, we introduce class-wise guided regularization (CWR) OCWR, which is formulated as follows:\n$\\Omega_{C W R}=\\frac{\\lambda}{n} \\cdot \\sum_{i=1}^{m} c_{i} \\sum_{u=1}^{n / m}\\left(f_{\\theta}\\left(x^{u}\\right)-f_{\\theta}\\left(x^{u}+\\delta\\right)\\right)^{2}$, (6)\nwhere xi represents the example from the i-th class, and Ci denotes the CWR guidance factor of the i-th class in the training set. We aim to implement the CWR guidance factor that can be applied to different datasets and models without introducing additional parameters. To accomplish this, a simple yet effective approach based on the clean accuracy of the corresponding class is employed, driven by three key considerations. First, clean accuracy generally exceeds robust accuracy and is easier to improve, making it a more reli-able measure for differentiation. Second, when training with datasets containing numerous classes, the robust accuracy of partial classes may be zero or close to zero. This makes ci ineffective for guiding regularization strength, whereas using clean accuracy avoids this issue. That is to say, clean accuracy can better reflect the differences in regularization strength between different classes. Furthermore, classification accuracy naturally possesses upper and lower bounds, it eliminates the need for hyperparameter adjustments. Therefore, the overall loss function Lon with CWR for performing training is pre-sented as LOA = LCE+\u03a9CWR, where LCE signifies the standard cross entropy loss. The integration of CWR enhances the performance of FAT by more effectively managing the impact of regularization across diverse classes. The pseudocode of our SKG-FAT with CWR is presented in Algorithm 1."}, {"title": "2) Why CWR Can Improve Robustness:", "content": "The proposed CWR aims to minimize the discrepancy between the clean pre-diction for each class and its corresponding prediction for AES according to class-wise training state. This approach leverages the mechanism that classes with higher clean accuracy indicate that the model has learned reliable classification features [48]. Meanwhile, since clean accuracy exceeds robust accuracy, CWR enhances the alignment between clean outputs and AE outputs for these high-accuracy classes, thereby utilizing the clean outputs to guide the model output of AEs and improve robustness. Conversely, for classes with lower accuracy, the model has not yet effectively learned the classification features."}, {"title": "3) Accuracy Alignment Guided Regularization:", "content": "Then, we leverage the analysis results from the perspective of accuracy alignment to introduce the accuracy alignment guided regular-ization (AGR), which effectively mitigates the adverse effects of differences in groups from the perspective of accuracy alignment. Considering the classes are divided into four groups and displaying distinct accuracy situations. Consequently, the mathematical description of the AGR NAGR is formulated as\n$\\begin{array}{l}\\Omega_{A G R}=\\lambda \\cdot \\frac{4}{n} \\sum_{j=1}^{4} \\frac{1}{n_{j}} \\sum_{u=1}^{n / 4}\\left(f_{\\theta}\\left(x^{u}\\right)-f_{\\theta}\\left(x^{u}+\\delta\\right)\\right)^{2} \\\\ \\text { where } j \\text { denotes the } j\\text {-th groups in the perspective of accuracy } \\\\ \\text { alignment as defined in subsection III-B and } d_{j} \\text { represents the }\\end{array}$, (7)\n$\\mathrm{d}_{j}=1 / n_{j}$, (8)\nwhere nj signifies the number of j-th groups. Similarly, the overall loss LOA with AGR is presented as LOA = LCE+\u03a9AGR. This regularization can enhance performance almost without introducing additional training time consumption, even if the number of classes in the dataset is large. The pseudocode for our SKG-FAT with CWR is presented in Algorithm 2."}, {"title": "4) Why AGR Can Improve Robustness:", "content": "AGR adjusts the impact of examples from different classes on training by considering the number of classes in each group, whereas the standard MSE loss neglects this variability. Specifically, the definition of MSE is introduced in equation (5), where 1/n can be seen as a uniform parameter for all examples. In contrast, AGR adapts group weights according to the number of classes within each group. As in equation (8), 1/nj as-signs variable weights across groups based solely on training-derived information, with \u03bb as the only hyperparameter, which is typical in regularization methods. Furthermore, subsection III-B suggests that regularization for GCGR and BCBR should be less emphasized while enhancing the weights for GCBR and BCGR. This adjustment is intuitive because GCGR and BCBR constitute most of the examples, whereas GCBR and BCGR are less frequent Thus, AGR dynamically adjusts the weights for examples from the four groups, effectively configuring their influence in training."}, {"title": "B. Self-Knowledge Guided Label Relaxation", "content": "It has been identified that catastrophic overfitting in FAT is linked to the imbalance of inner and outer optimization in the min-max problem (1) [47]. Existing methods demonstrate that stabilizing classification confidence [47] or preventing the over-memorization [49] can mitigate catastrophic overfitting and improve robust accuracy. Motivated by our findings of class-wise differences in example accuracy, we propose self-knowledge guided label relaxation (SKLR) as\n$y_{i}^{k}=\\kappa_{i}^{k} y_{i}+\\left(y_{i}-1\\right) \\cdot \\frac{\\kappa_{i}^{k}-1}{m-1}$, (9)\nwhere k denotes the class-wise label relaxation factor, which changes with the training performance. Consistent with the reasons involved in our CWR, clean accuracy is easier to improve and eliminates the requirements for hyperparameter upper and lower bound adjustments. Therefore, we exploit the knowledge of clean accuracy to implement the class-wise label relaxation factor to guide training as\n$\\kappa_{i}^{k}=\\min \\left(\\max \\left(\\left|\\log c_{i}^{k}\\right|, \\kappa_{\\min }\\right), 1\\right)$, (10)\nwhere che denotes the clean accuracy of the i-th class on training set in k-th training epoch, Kmin \u2208 ((1/m), 1] is minimum relaxation factor, y denotes the one-hot label, and y represents the relaxation label of i-th class in the k training epoch. SKLR dynamically adjusts labels based on the training state in a class-wise manner, effectively mitigating inner-outer optimization imbalance. This improves training performance and alleviates catastrophic overfitting. The imbalance optimization of the minimax in FAT causes catastrophic overfitting. SKLR addresses this by adjusting the strength of label relaxation based on the training status of each class, thereby stabilizing the training process. In the initial training, this method adopts higher confidence to support the model in learning the primary feature distribution of examples, thereby rapidly optimizing the accuracy. After that, this method facilitates label smoothness, encouraging the model to classify examples correctly without pursuing exces-sively high confidence, which contributes to preventing over-memory and thus preventing catastrophic overfitting. Overall, this approach stabilizes the minimax optimization in FAT, effectively preventing catastrophic overfitting."}, {"title": "C. Comparison with AEE-AT", "content": "Here, we outline the differences between our SKG-FAT and Advancing Example Exploitation (AEE) [48]. The AEE divides examples into accuracy-crucial (A-C) and robustness-crucial (R-C). On this basis, the performance of existing ad-versarial training methods is investigated. While our SKG-FAT also uses decision space information, it differs by exploring example characteristics at the feature level through specific indicators to guide adversarial training. The differences are summarized in three aspects. 1) For problem analysis, AEE utilizes robust accuracy and an extra hyperparameter, dividing all examples into A-C and R-C. AEE proposes that if the indicator ci approaches zero, the example can be consid-ered A-C. Meanwhile, we focus more on using naturally generated indicators during training to analyze and improve FAT. Furthermore, our research indicates that the assumption in AEE is not always valid (GCBR and BCBR). 2) For methodology, AEE enhances robustness for classes with easy-to-learn features (R-C) while emphasizing accuracy learning for classes with difficult-to-learn features (A-C). Our work minimizes interference among classes with different features as much as possible to improve performance. 3) For task objectives, AEE aims to address common issues in the entire adversarial training, such as the accuracy-robustness trade-off and overfitting. In contrast, our work utilizes relevant information to maximize robust accuracy while maintaining competitive clean accuracy."}, {"title": "V. EXPERIMENTS AND ANALYSIS", "content": ""}, {"title": "A. Experiment Settings", "content": "1) Datasets and Training Details: We evaluate and com-pare our method on CIFAR-10/100 [50], Tiny ImageNet, and ImageNet-100 [51]. Both our and existing methods are evaluated on ResNet18 [52] as default. We employ the SGD optimizer with a momentum of 0.9 and weight decay of 5e-4 for all datasets. For CIFAR-10, CIFAR-100, and Tiny ImageNet, the learning rate is initialed as 0.1 and divided by 10 at the 100-th and 105-th epoch with a total of 110 training epochs, respectively. For ImageNet-100, the learning rate is initialized as 0.1 and divided by 10 at the 40-th and 45-th epochs with a total of 50 epochs. We use perturbations from the previous batch to initialize the next batch examples [40]. All experiments are conducted on a single NVIDIA 3090 GPU."}, {"title": "2) Baselines for Compraison:", "content": "For a comprehensive evalu-ation, we compare our SKG-FAT with both classic and state-of-the-art FAT methods, including FGSM-RS [17], GAT [42], NuAT [33] FGSM-SDI [39], GradAlign [36], N-FGSM [18], FGSM-PGI (PGI-BP and PGI-MEP) [40], FGSM-SC with PGI-MEP [31], and AEE with GradAlign [48]. Additionally, to further demonstrate the competitiveness of our approach, we include comparisons with multi-step adversarial training, including LAS-AWP [53], and MART [54]."}, {"title": "3) Adversarial Attacks for Evaluation:", "content": "We compare our SKG-FAT with other methods across different adversarial attacks, covering MIFGSM (MI) [55], PGD-10/50 [15], C&W (CW) [56], and AutoAttack (AA) [57]. AutoAttack is eval-uated using its official implementation. For MIFGSM, PGD, and CW attacks, we utilize the TorchAttacks [58] with default settings for implementation. Moreover, the performance at both the best (selected by PGD-10) and the last checkpoint is reported to provide a comprehensive comparison [25], [59]."}, {"title": "B. Performance Comparison", "content": "1) Comparison Results on CIFAR-10: The comparison of the best and final checkpoints on CIFAR-10 across different methods is presented in Table I (Left). The results demon-strate that our SKG-FAT with either CWR or AGR maintains competitive clean accuracy and achieves better robust accuracy compared to state-of-the-art methods, without suffering catas-trophic overfitting. Specifically, SKG with CWR surpasses the state-of-the-art methods in defending against PGD-50 (+1.6%), CW (+0.6%), and AA (+0.4%) at the best check-point, while achieving better clean accuracy. Moreover, our method significantly reduces computational time, achieving performance comparable to multi-step adversarial training with only one-fifth of the time cost. In terms of computational efficiency when compared with FAT methods, the SKG adds only slight overhead compared to FGSM-RS or NFGSM, as it only involves the inference of the model on clean examples and AEs. Nonetheless, our method significantly improves the robustness of the model, particularly against PGD-50, CW, and AA, with improvements of 6.5%, 3.3%, and 4.0% at least, respectively. Furthermore, the comparison of GPU memory usage and robust accuracy against AA on the CIFAR-10 is presented in Fig. 7. The results demonstrate that our"}, {"title": "FAT with AGR achieved improvements of 1.1% and 0.5% against CW and AA at the best checkpoint, respectively. These results verify that the robustness enhancement provided by our method is a substantial improvement in training effectiveness, rather than merely a result of the training setting.", "content": "The results using WideResNet34-10 to evaluate differ-ent methods on the CIFAR-10 are provided in Table III. Due to the significantly larger scale of Wide ResNet-34 compared to ResNet-18, all methods exhibit improved accuracy over the results obtained with ResNet-18. However, our method still achieves better robust accuracy compared to other methods, with improvements of 1.5% and 1.3% against PGD-50 and AA, respectively. This confirms the scalability of our ap-proach, demonstrating its ability to enhance robust accuracy across different models, rather than being limited to improving the robustness of the single model."}, {"title": "2) Comparison Results on CIFAR-100:", "content": "The comparison results on CIFAR-100 are presented in Table I (Right). The corresponding results are consistent with those observed on CIFAR-10, where our method achieves the best adversarial robustness compared to existing FAT methods, while also preventing catastrophic overfitting. Specifically, our approach with CWR enhances the robustness when defending PGD-50 for +1.0%, CW for +0.6%, and AA for +0.4%. These results suggest that our FAT with CWR or AGR can effectively improve the adversarial robustness of the model. In terms of training efficiency, our method continues to hold an advantage, achieving the best robust accuracy with reduced computational time. Notably, while the time cost of SKG-FAT is slightly higher than that of FGSM-RS and NFGSM, it results in significant improvements, enhancing defense against PGD-50, CW, and AA attacks by 5.5%, 2.5% and 3.1% at least, respectively. Moreover, to ensure the reproducibility of the experimental results, we utilize three random seeds and report the average. The standard deviation of our CWR under AA is \u00b10.28%, while that of our AGR under AA is \u00b10.23%. These findings demonstrate that the proposed SKG-FAT achieves superior robustness on datasets with a larger number of classes."}, {"title": "3) Comparison Results on Tiny ImageNet:", "content": "The comparison results on the Tiny ImageNet dataset are provided in Table IV (Left). This dataset contains more classes and higher resolution than the CIFAR-10/100 datasets. For Tiny ImageNet, our method demonstrates stronger competitiveness. Specifically, while maintaining similar clean accuracy, our SKG-FAT with CWR or AGR achieves significant improvements in adversar-ial robustness. For SKG-FAT with AGR, significant improve-ments are observed against PGD-50, CW, and AA attacks,"}, {"title": "while SKG-FAT with CWR further surpasses the state-of-the-art methods, achieving a 1.4%, 1.6%, and 0.9% improvement in robustness against CW and AA, respectively. Overall, the proposed SKG-FAT with CWR or AGR demonstrates more pronounced advantages on datasets like Tiny ImageNet with a larger number of classes. This is because our method adjusts the training configuration for each class based on class-wise performance, leading to more significant improvements in adversarial robustness as the number of classes increases.", "content": "4) Comparison Results on ImageNet-100: The comparison results on ImageNet-100 are presented in Table IV (Right). The results indicate that our SKG-FAT remains competitive even on this higher-resolution dataset. Specifically, our SKG-FAT with CWR achieves the best robust accuracy, surpassing the state-of-the-art methods by 0.8% and 0.9% in defense against CW and AA at the best checkpoint, respectively. Notably, the GPU memory requirement of our SKG-FAT with CWR or AGR is only dependent on the batch size, allowing for single-GPU training even on large datasets like ImageNet-100. In contrast, the previous best method, PGI-MEP, requires loading the entire dataset simultaneously [40"}]}