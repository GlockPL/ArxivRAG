{"title": "Reliable Semantic Understanding for Real World Zero-shot Object Goal Navigation", "authors": ["Halil Utku Unlu", "Shuaihang Yuan", "Congcong Wen", "Hao Huang", "Anthony Tzes", "Yi Fang"], "abstract": "We introduce an innovative approach to advancing semantic understanding in zero-shot object goal navigation (ZS-OGN), enhancing the autonomy of robots in unfamiliar environments. Traditional reliance on labeled data has been a limitation for robotic adaptability, which we address by employing a dual-component framework that integrates a GLIP Vision Language Model for initial detection and an Instruction-BLIP model for validation. This combination not only refines object and environmental recognition but also fortifies the semantic interpretation, pivotal for navigational decision-making. Our method, rigorously tested in both simulated and real-world settings, exhibits marked improvements in navigation precision and reliability.", "sections": [{"title": "1 Introduction", "content": "Object navigation is crucial for the autonomous operation of robots, which has traditionally depended on extensive labeled visual data. In Object Goal Navigation (OGN), the objective is to navigate uncharted environments in search of a specified, yet initially unseen, target object. Traditional methodologies in this domain have predominantly hinged on visual cues through either imitation [11,20] or reinforcement learning [10,22] techniques, necessitating substantial data and annotations for effective training, thereby constraining their utility in diverse, real-world settings.\nThis necessity has catalyzed a paradigm shift towards ZS-OGN strategies, designed to imbue robots with the capacity for immediate adaptation to novel objects and contexts [6, 17, 27, 28]. Zero-shot object goal navigation (ZS-OGN)"}, {"title": "2 Problem Statement", "content": "In ZS-OGN tasks, a robot is tasked with locating the target object, denoted as $g_i$, which it has not previously encountered within an unexplored environment $s_i$, and this must be accomplished without prior navigational data training. At each timestep t, the robot captures a color image $I_t$, depth data $d_t$, and its own pose comprising its coordinates $(x_t, Y_t)$ and heading $\\theta_t$. The robot integrates its pose data over time to compute its current location. Utilizing the data from each timestep, the robot selects an action a from a set of possible actions A which includes actions such as advancing, rotating left, rotating right, and halting. The navigation process is deemed successful when the robot elects to halt within a specified proximity to the target object."}, {"title": "3 Approach", "content": "In this section, we delineate our framework for ZS-OGN. As depicted in Figure 2, our approach incorporates a frontier-based exploration method, which is widely recognized in the field of ZS-OGN. The process initiates with the transformation of the input image into semantic data, subsequently integrating this information into a semantic map. The framework then harnesses the commonsense reasoning capabilities of large language models to determine the subsequent frontier for exploration. Next, the Fast Marching Method is employed to compute the shortest path from the agent's current location to the designated target. Lastly, our innovative 'Doubly Right' semantic understanding framework is applied to verify the accurate detection of the target object."}, {"title": "3.1 Doubly Right Semantic Understanding", "content": "In developing a robust framework for ZS-OGN, we introduce a novel method called \"Doubly Right,\" which incorporates a dual verification system using Vision-Language Models (VLMs) to mitigate common detection errors. This approach, provided in Algorithm 1, seeks to increase the reliability of object detection and room recognition within navigational tasks.\nThe process begins with an Initiator VLM. Following the ESC framework, we employ the Grounded Language-Image Pre-training (GLIP) model for the preliminary identification of objects and rooms in environmental representations. The GLIP model processes visual inputs using zero-shot learning capabilities to generalize its detection beyond the training data through natural language prompts. When the GLIP model detects an object or room type, it assigns a provisional label and triggers the Validator VLM, InstructionBLIP, to assess the detection for accuracy. For each visual input $I_t$ at time t, the framework executes an initial detection with the GLIP model:\n$O_{init} = GLIP(I_t, P_o),$ (1)\nwhere $O_{init}$ denotes the set of detected target objects by GLIP, and $P_o$ represent the respective object prompting phrases applied to GLIP. Regardless of whether $O_{init}$ is an empty set or contains detections, the Validator VLM, InstructionBLIP, then reviews these initial findings. It cross-references the output from the GLIP with the task instructions to confirm their validity. If InstructionBLIP identifies a need for reassessment, it advises that the environment be re-examined, indicating potential discrepancies in the initial detection. The algorithm will then return to the Initiator VLM to conduct a reevaluation. In cases where InstructionBLIP does not advise further inspection, a flag Goal is set to True, indicating that the target object has been reliably detected and the navigational task is complete. This flag's status is critical as it confirms the end of the navigational sequence, ensuring that erroneous detections are addressed and the accuracy of the navigational decision-making is improved.\nThe validation process serves as a double-check mechanism, confirming the detected objects and their spatial contexts are indeed relevant and accurate for the navigational task at hand. By implementing this two-fold verification strategy, our framework aims to reduce detection errors that impede the success of autonomous navigation systems. The approach ensures that navigation decisions are based on a reliable semantic understanding of the environment, enhancing the system's performance in novel or previously unseen settings."}, {"title": "3.2 Semantic and Frontier Map", "content": "Our approach for ZS-OGN follows the ESC framework to construct a semantic navigation map critical for autonomous navigation. Utilizing depth input $d_t$ and the agent's 2D pose $[x_t \\space Y_t \\space \\theta_t] \\in R^2 \\times S$, we generate a foundational 2D navigation map. The GLIP model is then applied to enrich this map semantically by detecting objects and room types via zero-shot learning:\n$S_{map} = f(d_t, x_t, Y_t, \\theta_t, R_i),$ (2)\nwhere $S_{map}$ represents the semantic map, $d_t$ is the depth input at time t, and $R_i$ symbolizes the environmental representation obtained from the GLIP model including room types and commonly occurring objects."}, {"title": "3.3 Commonsense Policy for Exploration", "content": "Our navigation framework assimilates a commonsense reasoning module, adapted from the established ESC framework, to complement the autonomous navigation process. This module leverages the semantic understanding derived from our semantic navigation map to make contextual inferences, enhancing navigational decision-making.\nThe adopted ESC commonsense module ESCcs analyzes spatial relationships and object functionalities, drawing on a knowledge base of object-room correlations and navigational heuristics. Such inferences enable the anticipation of environmental elements indirectly indicated by the current sensory data:\n$C_i = ESC_{cs}(S_{map}),$ (4)\nwhere $C_i$ denotes the reasoned inferences based on the semantic map $S_{map}$. These inferences feed into the Global Commonsense Policy, informing frontier selection for targeted exploration.\nBy incorporating the ESC commonsense reasoning, the framework attains a sophisticated level of environmental interpretation, pivotal for navigating through dynamic spaces. This integration, although not our core innovation, significantly enhances the overall efficacy of the navigation system."}, {"title": "4 Simulation Studies", "content": "4.1 Dataset\nRoboTHOR The RoboTHOR dataset has been developed to validate navigation systems within authentic real-world scenarios. This benchmark features 89"}, {"title": "4.2 Metrics", "content": "Consistent with established benchmarks in the field, we utilize Success Rate (SR) and Success Weighted by Path Length (SPL) to evaluate agent performance. SR measures the agent's accuracy in reaching the target within a meter's distance, presented as a percentage, where a higher rate indicates better performance. Conversely, SPL gauges the efficiency of the navigation, comparing the agent's actual traveled path with the ideal shortest path, thus reflecting the agent's navigational efficacy and the optimization of its chosen route."}, {"title": "4.3 Baselines", "content": "In our experiment, we measure the performance of our method against baseline models such as CoW and ESC. CoW, designed for Zero-Shot Object Navigation (ZS-OGN), leverages CLIP for dynamic object detection, enabling localization without prior navigational training. We further evaluate CoW's efficacy by comparing it with its variants that utilize different CLIP-based localization strategies, namely CLIP-Ref, CLIP-Patch, CLIP-Grad, MDETR, and OWL. Additionally, we assess ESC, which incorporates commonsense knowledge into navigation actions through a pre-trained vision and language model, enhancing the agent's ability to navigate and reason about objects and rooms in unseen environments."}, {"title": "4.4 Results", "content": "The performance data in Table 1 reveals that our method leads with an average Success Rate (SR) of 23.0% and Success Weighted by Path Length (SPL) of 13.7, indicating effective and efficient navigation in diverse scenarios. It surpasses others, particularly in challenging categories involving uncommon objects and hidden distractions, underscoring its robustness and sophisticated semantic understanding. Notably, the OWL model also demonstrates commendable SRs, especially in environments with spatial distractions. In contrast, the CoW model and other CLIP-based methods display more modest performance, highlighting"}, {"title": "5 Experimental Studies", "content": "The proposed ZS-OGN pipeline was validated in real-world scenarios on real robot hardware. Following the procedures from the simulation environments and benchmarks, the robotic agent was tasked with navigation near various household objects with no prior information about the environment or the actual object.\nAdditional modifications were necessary to allow the system to operate in a real-world environment to improve overall system robustness and safety in the"}, {"title": "5.1 Environment", "content": "The layout of the apartment in which the tests are performed is provided in Figure 3, along with the locations of sample items to be detected and the first-person view from the robot when they are detected.\nThe placement of the objects was guided by common sense: a TV remote is expected to be found near the TV in a living room, and a garbage can can be found in a bathroom or a kitchen."}, {"title": "5.2 Robotic Platform", "content": "For the study, a Unitree B1 quadruped robot, equipped with a LiDAR (Pandar XT16) and an RGBD camera with IMU (Realsesne D455) was used. Simultaneous localization and mapping (SLAM), path planning, navigation, and low-level control were executed entirely on the robot's internal CPU, whereas the commonsense module for identifying goal location was run on the additional computer, attached to the robot. A photo of the vehicle is provided in Figure 4.\nThe platform additionally streams depth images from 5 extra depth cameras, inertial measurements from an internal IMU, and a proprioceptive odometry estimate. However, none of the aforementioned data is used for this study."}, {"title": "5.3 Odometry and Mapping", "content": "The related literature for ZS-OGN utilizes RGBD-based mapping schemes that assume the agent pose is always available. However, without external infrastructure (e.g. GNSS for outdoors, motion capture for indoors, or dead-reckoning in both) the agent pose is neither readily available nor always accurate.\nRTAB-Map [14] was selected as the main driver for pose estimation, mapping, and localization. The robotic platform uses the onboard LiDAR and IMU sensors to estimate its odometry using RTAB-Map's ICP odometry module. With the addition of color images from the RGBD sensor, RTAB-Map's SLAM module is used for localization and mapping with global loop closure identification. Pose estimation and mapping are restricted to 3D0F (xy-coordinates and yaw angle 0) since the environment is a single-story indoor location."}, {"title": "5.4 Safe Path Planning", "content": "Many of the ZS-OGN frameworks output an action from the action space, such as \"move forward\" or \"turn left\" to carry out the navigation task. While such schemes function well in simulated environments, in which the actions can be carried out instantaneously and precisely, the real-world interaction needs to perform a trade-off between speed and accuracy. discrete action spaces prevent the agents from executing complex maneuvers (e.g. go through a narrow opening for a robot with non-circular footprint). Finally, inherent noise in sensors and estimation for robotic platforms necessitate enhanced safety precautions to prevent inadvertent collisions with the environment. While a distance-optimal or effort-optimal path could take the robot to its desired location with high efficiency, the robot usually needs to navigate close to the obstacles in the environment. Latency in state estimation, control, and/or the actual movement can lead to collisions."}, {"title": "5.5 Implementation Details", "content": "The path planning algorithm as proposed was implemented as a planner plugin within Nav2 [16] framework, an open-source navigation stack with production deployments."}, {"title": "5.6 Results", "content": "The path taken by the robot to find a remote controller overlaid on the architectural plan of the apartment, is given in Figure 6, and the path for finding a trashcan is provided in Figure 7.\nIn finding the TV remote, the system appears to recognize the living room area, prioritizing exploration of the right-hand side of the apartment. Upon getting closer to the television, the remote controller was successfully recognized, and the mission was over.\nFor the setup to find a trash can, the robot initially attempts to navigate towards the bedroom on the left. At the time of exploration, the algorithm confused the featureless environment to be a bathroom and focused on its exploration. Eventually, the robot sees the bed and proceeds to focus on another frontier point. The robot observed the trashcan located in the kitchen initially, but the path planning algorithm proposed a more indirect approaching angle for the trashcan, in an attempt to minimize the cost of traversal. Eventually, the robot reached the trashcan, completing the task."}, {"title": "6 Conclusion", "content": "Our work presents the 'Doubly Right' framework, a step forward in Zero-Shot Object Goal Navigation (ZS-OGN), enabling reliable semantic understanding in robotics. Our approach stands out as the first to implement ZS-OGN in real-world settings, demonstrating strong potential through simulation and practical application. Real-world tests demonstrated that lack of discerning features in the environment can result in the system making a poor initial choice in hindsight, due to the lack of knowledge about the observed environment, but the system is nevertheless able to correct course and complete the task. This breakthrough lays the groundwork for future autonomous systems to navigate novel environments without prior training, offering a glimpse into the next frontier of robotic adaptability and intelligence."}]}