{"title": "LEVERAGING REVERBERATION AND VISUAL DEPTH CUES FOR SOUND EVENT\nLOCALIZATION AND DETECTION WITH DISTANCE ESTIMATION", "authors": ["Davide Berghi", "Philip J. B. Jackson"], "abstract": "This report describes our systems submitted for the DCASE2024\nTask 3 challenge: Audio and Audiovisual Sound Event Localiza-\ntion and Detection with Source Distance Estimation (Track B). Our\nmain model is based on the audio-visual (AV) Conformer, which\nprocesses video and audio embeddings extracted with ResNet50\nand with an audio encoder pre-trained on SELD, respectively. This\nmodel outperformed the audio-visual baseline of the development\nset of the STARSS23 dataset by a wide margin, halving its DOAE\nand improving the F1 by more than 3x. Our second system per-\nforms a temporal ensemble from the outputs of the AV-Conformer.\nWe then extended the model with features for distance estimation,\nsuch as direct and reverberant signal components extracted from\nthe omnidirectional audio channel, and depth maps extracted from\nthe video frames. While the new system improved the RDE of our\nprevious model by about 3 percentage points, it achieved a lower\nF1 score. This may be caused by sound classes that rarely appear in\nthe training set and that the more complex system does not detect, as\nanalysis can determine. To overcome this problem, our fourth and\nfinal system consists of an ensemble strategy combining the predic-\ntions of the other three. Many opportunities to refine the system and\ntraining strategy can be tested in future ablation experiments, and\nlikely achieve incremental performance gains for this audio-visual\ntask.", "sections": [{"title": "1. INTRODUCTION", "content": "Sound Event Localization and Detection (SELD) consists of simul-\ntaneously detecting and classifying the active sound sources over\ntime (sound event detection (SED)) while predicting their position\nor direction of arrival DOA [1]. This task is essential for a variety\nof applications, such as human-robot interaction, augmented real-\nity, navigation, smart home, and security, to name a few. Over time,\nresearchers have been addressing more and more challenges related\nto the task, including the detection of polyphonic sounds [2], si-\nmultaneous same-class events and moving sources [3], and external\ninterfering sounds that must not be detected [4]. This year, the chal-\nlenge also involves predicting the distance of the active sources [5].\nWhile SELD has been typically formulated as an audio-only task,\na parallel audio-visual track has been included in the past two edi-\ntions of the DCASE challenge, leveraging the Sony-TAu Realistic\nSpatial Soundscapes 2023 (STARSS23) dataset [6]. STARSS23 in-\ncludes 360\u00b0 video recordings spatially and temporally aligned to\nthe acoustic sound-field captured by the microphone array. This\nallows the exploration of SELD as a multimodal audio-visual prob-\nlem (AV-SELD). The two modalities are complementary and can be\nbeneficial to the task: vision provides high spatial accuracy whereas\naudio can detect occluded objects.\nThis report describes our systems submitted to the audio-visual\ntrack of the challenge. Specifically, we adopted two main mod-\nels and methods. The first extends our previous work on SELD\nwhere an audio-visual (AV) conformer takes as input the concate-\nnation of audio and video embeddings extracted with pre-trained\nencoders [7]. The extension consists of adapting the existing model\nto support distance estimation, as described in Section 2. In Sec-\ntion 3, the second method investigates additional input features and\nextends the model architecture to improve distance estimation and\ntake more advantage of the visual modality. We report our results\non the STARSS23 development set [6] in Section 4, including a\ntemporal ensemble and an ensemble of all three variants, improving\nsubstantially over the challenge baselines. Section 5 concludes."}, {"title": "2. AUDIO-VISUAL CONFORMER", "content": "As depicted in the left part of Fig. 1, this method extracts audio and\nvisual feature embeddings with an audio and a visual encoder, re-\nspectively. The embeddings are then concatenated and processed by\na Conformer module with 4 layers. Finally, the output features are\nfed to two fully connected layers to predict multi-ACCDDOA vec-\ntors [5]. Such representation is the extension of the multi-ACCDOA\nvectors proposed by Shimada et al. [8], adopted to include distance\npredictions. Specifically, at each time frame, the output presents\n$N = 3$ tracks, each predicting 4 positional values (x, y, z, and\ndistance), for each of the $C = 13$ classes, resulting in a 156-\ndimensional vector. We employed the hyperbolic tangent as the\nactivation function for the x, y, z predictions, while ReLU for the\ndistance. In this report, we will refer to this system as per \"AV-\nConformer\". The same audio-visual architecture and methods were\npresented in our recent work [7]. The main difference with the sys-\ntem presented for this challenge consists in the integration of the\ndistance estimation and the use of the new metrics. Below, we de-\nscribe the audio and visual encoders, and the data augmentation and\npre-training strategies."}, {"title": "2.1. Audio Encoder", "content": "The audio encoder takes as input acoustic features extracted from\nthe FOA spatial sound. We employed intensity vectors (IV) in\nthe log-mel space concatenated with the log-mel spectrograms ex-\ntracted from the FOA channels, yielding 7-dimensional input fea-\ntures with shape $7 \\times T_{in} \\times F_{in}$, where $T_{in}$ corresponds to the number\nof temporal bins and $F_{in}$ the frequency bins. The audio encoder in-\ncludes a CNN architecture followed by a Conformer [9]. The CNN\npresents 4 convolutional blocks with residual connections. Each\nblock consists of two 3\u00d73 convolutional layers followed by aver-\nage pooling, batch normalization, and ReLu activation. The average\npooling layer is applied with a stride of 2, halving the temporal and\nfrequency dimension at each block. The resulting tensor of shape\n$512 \\times T_{in}/16 \\times F_{in}/16$ is then reshaped and frequency average\npooling is applied to achieve a $T_{in}/16 \\times 512$ dimensional feature\nembedding. $T_{in}$ is chosen so that $T_{in}/16$ matches the label frame\nrate (10 labels per second). The feature embedding is further pro-\ncessed by a Conformer module with 4 layers and 8 heads each. The\nsize of the kernel for the depthwise convolutions is set to 51."}, {"title": "2.2. Visual Encoder", "content": "As visual encoder, we employed a ResNet-Conformer. Specifically,\nwe fed each video frame to ResNet50 [10] at a frame rate of 10\nfps. In such a way, we extract a number of frame embeddings that\nmatch the label frame rate as well as the audio embedding tem-\nporal resolution. We then process the frame embeddings with a\nConformer module identical to the one employed in the audio en-"}, {"title": "2.3. Data Augmentation and Pre-Training", "content": "The audio CNN-Conformer was pre-trained on SELD employing\nthe simulated data generator script provided for the DCASE 2022\nchallenge [11]. This allowed the generation of ~30h of spatial\nrecordings, including noiseless and noisy versions. The ResNet50\nmodel employed to process 2D frames is available with the Torchvi-\nsion library and it is pre-trained on ImageNet [12]. The synthetic\nSELD dataset used to pre-train the audio encoder was augmented by\na factor of 8 with the audio channels swap (ACS) technique [13].\nFor the AV-SELD dataset, we also augmented the visual modal-\nity consistently with the ACS transformation, i.e., the audio-visual\nchannel swap (AVCS) [7, 14]. This generates new video frames by\nflipping and rotating the original ones, creating an effective audio-visual spatial transformation."}, {"title": "3. DEPTH-CUED MODEL", "content": "The other main system submitted to the challenge differs signifi-\ncantly from the previous one in many aspects, including model ar-\nchitecture and size, audio and visual data pre-processing, and audio\npre-training. Consequently, their overall performances cannot be di-\nrectly compared. A step-by-step ablation study would be necessary\nto understand the impact of each change. We refer to this method\nas per \"Depth-cued model\", since particular attention was given to\nincorporating audio and visual depth features."}, {"title": "3.1. Direct and Reverberant Audio Features", "content": "While log-mel spectrograms and intensity vectors (IV) are effec-\ntive input features for the SED and DOAE subtasks, they are not\nideal for tackling the distance estimation task. Distance cues can\nbe extracted from the relationship between the direct and reverber-\nant components of the captured audio signals. Specifically, the later\ntail of the reverberant signal (late reverberation) carries informa-\ntion about the sound's apparent distance. We decided to include\nboth direct and reverberant components (\"DR\", for compactness)\nextracted from the Omnidirectional audio channel to the set of in-\nput features, in the form of two additional log-mel spectrograms. In\norder to estimate the direct sound, we applied the Weighted Predic-\ntion Error (WPE) dereverberation algorithm [15]. Specifically, we\nadopted the python implementation of the WPE algorithm released\nby Drude et al. [16] (taps=60; delay=5; iterations=5). The reverber-\nant component is then estimated by taking the difference between\nthe original and the direct signal in the temporal domain. RD fea-"}, {"title": "3.2. Cubemap Conversion and Depth Map Extraction", "content": "The 360\u00b0 videos of STARSS23 are in the form of 2:1 equirectan-\ngular views. Such representations produce heavy distortions that\nbecome more severe closer to the borders of the frame. We argue\nthat this might penalize the ability of the visual encoder to under-\nstand the scene since ResNet50 is pre-trained on frontal views. To\nthis end, we converted the video frames to cubemap representations.\nSuch image representation consists of mapping the 360\u00b0 image data\nonto the six faces of a cube. Since the sound events are primar-\nily located within the four horizontal faces (left-right-front-back)\nand little information can be obtained from the top (ceiling) and\nthe bottom (EigenMike) faces, we kept only the horizontal faces.\nThe resulting frames present an aspect ratio of 4:1, with resolution\n896\u00d7224p. Note, the conversion to cubemap is applied after the\nAVCS technique is performed to the equirectangular views.\nTo leverage the visual modality in support of the distance es-\ntimation task, we extracted depth features from video frames too.\nSpecifically, we applied the recent \"Depth Anything\" model [17] to\ngenerate depth views of the scene."}, {"title": "3.3. Depth-cued Model's Architecture", "content": "Part of the Depth-cued model architecture presents the same au-\ndio and visual encoders and Conformer unit employed in the AV-\nConformer model. However, the input frames used with the Depth-cued model are cubemap views, and the audio encoder is pre-trained\non the synthetic mixtures provided by the challenge's organizers\nand takes as input also DR features. Additionally, both audio and\nvisual encoders produce 1024-dimensional embeddings.\nAn additional frame encoder is included too. It consists of a vi-\nsion transformer (ViT) [18] pre-trained on ImageNet [12]. The in-\nput tokens fed to the ViT are obtained by dividing the central frame\ninto patches of 16\u00d716p. Therefore, a total of 14 \u00d7 56 = 784 tokens\nare extracted from the 896\u00d7224p frame, and each token presents\n16 \u00d7 16 \u00d7 3 = 768 dimensions, where 3 corresponds to the number\nof color channels. The same patch extraction technique is applied\nto the depth map frame feature too, which only presents a single\nchannel. The depth patches are then concatenated to the output of\nthe ViT to produce 782\u00d71024 dimensional features. These are em-\nployed to generate a set of Key-Value pairs for a multi-head cross-attention (MHCA) unit applied against the audio features (Queries).\nThe main motivation for employing the ViT and the MH\u0421A\nunit is to try to leverage the spatial accuracy provided by the visual\nmodality. Unlike ResNet50 which downsamples the spatial reso-\nlution of the input frames with the risk of losing spatial informa-\ntion, we argue that the ViT should preserve such information en-\ncoded within the processed patches. The outputs generated by the\nAV-Conformer and by the MHCA are then concatenated and fed to\na feed-forward network that predicts the output multi-ACCDDOA\nvectors [5]."}, {"title": "4. EXPERIMENTS", "content": ""}, {"title": "4.1. Implementation Details", "content": "To train our models, we divided the dataset into chunks of 3 sec-\nonds, extracted at steps of 1s for training and with no overlap for\ntesting. An STFT with 512-point Hann window and hop size of\n150 samples generates spectrograms discretizing the 3-second au-\ndio chunks (24kHz) into 480 temporal bins ($T_{in}$). We used 128 fre-\nquency bins to generate log-mel spectrograms and intensity vectors\n(IV) in log-mel space. We trained our models with batches of 32\ninputs and Adam optimizer for 40 epochs, then we selected the best\nepoch. The learning rate is set to 0.00005 for the first 30 epochs,\nthen and it is decreased by 5% every epoch. The metrics adopted\nfor the evaluation are the ones proposed in the DCASE 2024 Task3\nChallenge [6]."}, {"title": "4.2. Ensemble strategies", "content": "Inference is conducted on 3-second video segments without any\noverlap, meaning each segment starts exactly 3 seconds after the\nprevious one. To enhance spatial accuracy, we tested a temporal\nensemble (TE) strategy. For this, inference was performed with a\n1-second hop size, generating 3 predictions for each second of the\nsequence (except for the first 2 seconds). A sound event is consid-\nered active only if at least 2 out of the 3 predictions detect it. To\ndetermine if the sounds detected by different predictions are from\nthe same event, we applied a spatial threshold of 15\u00b0. Predictions\nare considered related if their positions are within this threshold."}, {"title": "4.3. Results", "content": "The results achieved on the development set of the STARSS23 [6]\ndataset are reported in Table 1. The AV-Conformer method achieves\nthe highest F1 score, while the Depth-cued model has the lowest\nRDE. However, the Depth-cued model's F1 score is approximately\n10 percentage points lower than that of the AV-Conformer, mainly\ndue to its failure to detect certain rare classes, like \"Knock\". The\nensemble system recovers these 10 points in the F1 score. How-\never, it does not provide the improvement in localization accuracy\nthat is achieved with the temporal ensemble. All submitted systems\nachieve superior performance than the audio-only (AO) and audio-visual (AV) baselines that employ the FOA audio format."}, {"title": "5. CONCLUSION", "content": "This technical report describes the 4 systems submitted to the Task 3\nof DCASE 2024 Challenge (Track B). Specifically, two main mod-\nels are explored: an AV-Conformer and a Depth-cued model, which\ncorrespond respectively to system one and system three of the four\nsubmitted. The second system applies a temporal ensemble strat-\negy to the output of the AV-Conformer, and the fourth system con-\nsists of an ensemble of the other three. All our systems outperform\nthe audio-only and audio-visual challenge baselines on the develop-\nment set of the STARSS23 dataset."}]}