{"title": "Active Learning for Derivative-Based Global Sensitivity Analysis with Gaussian Processes", "authors": ["Syrine Belakaria", "Benjamin Letham", "Stefano Ermon", "Barbara Engelhardt", "Janardhan Rao Doppa", "Eytan Bakshy"], "abstract": "We consider the problem of active learning for global sensitivity analysis of expensive black-box\nfunctions. Our aim is to efficiently learn the importance of different input variables, e.g., in vehicle\nsafety experimentation, we study the impact of the thickness of various components on safety\nobjectives. Since function evaluations are expensive, we use active learning to prioritize experimental\nresources where they yield the most value. We propose novel active learning acquisition functions\nthat directly target key quantities of derivative-based global sensitivity measures (DGSMs) under\nGaussian process surrogate models. We showcase the first application of active learning directly to\nDGSMs, and develop tractable uncertainty reduction and information gain acquisition functions for\nthese measures. Through comprehensive evaluation on synthetic and real-world problems, our study\ndemonstrates how these active learning acquisition strategies substantially enhance the sample\nefficiency of DGSM estimation, particularly with limited evaluation budgets. Our work paves\nthe way for more efficient and accurate sensitivity analysis in various scientific and engineering\napplications.", "sections": [{"title": "Introduction", "content": "Sensitivity analysis is the study of how variation and changes in the output of a function can be\nattributed to distinct sources of variability in the function inputs Iooss and Saltelli [2017]. More\nprecisely, we seek to determine how changes in each input variable impact the output. Sensitivity\nanalysis can be used for several purposes, including identifying the input variables that are most\ninfluential for the function output and those that are least influential [Iooss and Saltelli, 2017], and\nquantifying variable importance in order to explore and interpret a model's behavior [Van Stein\net al., 2022]. Sensitivity analysis is an important tool in many fields of science and engineering to\nunderstand complex, often black-box systems. It has proven particularly important for environmental\nmodeling [Razavi and Gupta, 2015, Wagener and Pianosi, 2019], geosciences [Wainwright et al., 2014,"}, {"title": "Background", "content": "In this section, we define the problem and provide a review of DGSMs, GPs, and their derivatives. For\na thorough review of GSA, see Iooss and Saltelli [2017].\nWe wish to analyze the sensitivity of a black-box function defined over a compact d-dimensional input\nspace X. We suppose that evaluating f at any particular input $x \\in X$, that is, observing $y = f(x) + \\epsilon$\nwith $\\epsilon$ the observation noise, is expensive. The ground-truth sensitivity measure, which we denote as\n$S(f, X)$, has a d-dimensional output that provides a sensitivity measure for each input dimension $x_i$,\n$i = 1, ..., d$. We estimate S by making t observations of f, for which we denote with $X = [x_1,..., x_t]$\nthe set of observed inputs, $Y = [y_1, \\ldots, y_t]$ the function evaluations at those inputs, and $D = {X,Y}$\nthe full observed data. We learn a surrogate model of f from D and then evaluate S on a surrogate for\nf. Here, we will denote the surrogate as $\\hat{f}$; in practice, we will use the GP posterior mean $\\hat{f}(x) = \\mu(x)$,\nwhich we introduce in Section 2.4. Given the surrogate, we estimate $\\hat{S}(\\hat{f}, X|D) = S(\\hat{f}, x)$.\nOur active learning problem is to select the input locations X so that $\\hat{S}$ provides the best estimate\nof the true global measure S. We do so sequentially with a budget of T total evaluations. Generally,\n$\\hat{S}$ will better approximate S, as the surrogate $\\hat{f}$ better approximates f. However, when T is small,\nit is important to consider the particular form of S to design the most effective strategy, as opposed\nto simply trying to learn a good global surrogate. Here, we develop strategies tailored for S being a\nDGSM. We introduce DGSMs in Section 2.3, and in Section 4 develop the acquisition strategies.\nThe methods we develop here are cast as Bayesian active learning algorithms. Bayesian active learning\nis a flexible framework that combines Bayesian inference principles with active learning strategies,"}, {"title": "Derivative-based Global Sensitivity Measures (DGSMs)", "content": "In this section, we provide the background and definitions for our target sensitivity measures, DGSMs.\nDGSMs are defined as the integral over the input space of a function of the derivative of the black-box\nfunction. There are three widely-used gradient functions in DGSMs: the raw gradient, the absolute\nvalue of the gradient, and the square of the gradient Kucherenko et al. [2009]:\n$\\mathrm{S}_{R}(f, x)=\\frac{1}{|X|} \\int_{X} \\frac{\\partial f(x)}{\\partial x_{i}} d x$,\n$\\mathrm{S}_{a b}(f, x)=\\frac{1}{|X|} \\int_{X} \\left|\\frac{\\partial f(x)}{\\partial x_{i}}\\right| d x$,\n$\\mathrm{S}_{s q}(f, x)=\\frac{1}{|X|} \\int_{X} \\left(\\frac{\\partial f(x)}{\\partial x_{i}}\\right)^{2} d x$.\nIn the remainder of the paper, we will refer to these quantities as the raw DGSM, absolute DGSM,\nand squared DGSM. These quantities may also be defined with non-uniform densities on X.\nFor the purpose of evaluating input sensitivity, the raw DGSM is considered unstable and uninformative\ndue to a phenomenon known as the cancellation effect. In nonmonotonic functions, positive parts of the"}, {"title": "GP Surrogates for Sensitivity Analysis", "content": "When function evaluations are expensive, the integrals over the input space required to compute the\nDGSMs may not be evaluated tractably from f. Moreover, if f is black-box, we do not always have\naccess to its gradients. Both of these issues can be avoided by using a surrogate function for f.\nGSA of expensive, black-box functions is usually done using a GP surrogate model [De Lozzo and\nMarrel, 2016]. GPs are characterized by a mean function $m : X \\rightarrow \\mathbb{R}$ and a kernel function covariance\n$K : X \\times X \\rightarrow \\mathbb{R}$. A GP prior for the function, $f \\sim GP(m, K)$, means that the function values at any\nfinite set of inputs are jointly normally distributed. For any input $x \\in X$, the function value at that\ninput has a normally-distributed posterior $f(x_*) | D \\sim \\mathcal{N}(\\mu_*, \\sigma^2_*)$, whose predictive mean and variance\nare defined as:\n$\\mu_* = K_{x_*,X} K_D^{-1} (Y - m_X) + m_{x_*}, \\quad \\sigma^2_* = K_{x_*,x_*} - K_{x_*,X} K_D^{-1} K_{X,x_*}$\nwhere $K_{x_*,**} = K(x_*,**), K_{x_*,x} = [K(x_*,x_j)]_{j=1}^t, m_X = m(x)$, and $K_D = K_{X,X} + \\eta^2 I$, with\n$K_{X,X} = K(X,X)$ and $\\eta^2$ the observation noise variance of y [Rasmussen and Williams, 2006]. We\nintroduce the short-hand notation $\\mu_* := \\mu(x_*)$ and $\\sigma^2 := \\sigma(x_*)^2$ for the posterior mean and variance\nfunctions.\nGPs are differentiable when using any twice-differentiable kernel function K and a differentiable mean\nfunction m. The gradient of the GP provides a tractable estimate of the gradient of the expensive\nblack-box function f under commonly-used kernels (e.g., RBF). DGSMs can then be computed in a\nfast and scalable way on the posterior of $\\hat{f}$ [De Lozzo and Marrel, 2016]."}, {"title": "Derivatives of Gaussian Processes", "content": "GPs are closed under linear operations, therefore the derivative of a GP is itself a GP [Rasmussen and\nWilliams, 2006]. This enables us to derive an analytical distribution for the surrogate model's gradient.\nSince f is defined over a d-dimensional input space, the model's gradient has a d-dimensional output.\nUnder a GP prior, the joint distribution between (potentially noisy) observations of f and the gradient"}, {"title": "Related Work", "content": "The GP surrogate allows for computing DGSMs with a limited set of function evaluations. However,\nin budget-restricted experiments, the GP will only provide a faithful representation of the sensitivity\nof f if the right set of inputs are evaluated. There has been limited work on efficiently selecting the\ninputs that lead to accurate DGSM estimation, particularly with a limited evaluation budget.\nThe most common approach for estimating DGSMs with a\nGP is to evaluate the function on either a random set of inputs or with a space-filling design. For\nthe latter, quasirandom sequences like scrambled Sobol sequences [Owen, 1998] and Latin hypercube\nsampling [McKay et al., 1979] are two common choices. Space-filling designs are effective for GSA\nwith a sufficiently large evaluation budget, however, as we will see below, they fail when the budget is\nlimited.\nSeveral Bayesian active learning approaches have been\ndeveloped for the purpose of reducing global uncertainty about f. Information-based strategies that\nselect the point that produces the largest information gain about a function's outputs are popular and\neffective for global identification of f [Shewry and Wynn, 1987, Krause et al., 2008, Houlsby et al.,\n2011]. Other global active learning approaches are based on variance reduction [Schein and Ungar,"}, {"title": "Bayesian Active Learning for Derivative-Based Sensitivity Analysis", "content": "In this section, we provide the expressions and details of the active learning acquisition functions that\nwe propose to target the DGSM measures. We derive acquisition functions following three general\nstrategies. The maximum variance acquisition functions select the point with the largest posterior\nvariance in the quantity of interest, indicating the point with the most uncertainty. The variance\nreduction acquisition functions measure how much an observation at a point will reduce the variance at\nthat point in expectation over the possible outcomes of the observation. Finally, the information gain\nacquisition functions quantify the expected reduction in entropy of the posterior of the quantity of\ninterest for each point. The latter two strategies require computing the look-ahead distribution for the\nderivative, which we introduce in Section 4.1. We additionally discuss global look-ahead acquisition\nfunctions that measure the impact of input on the posterior across the entire input space.\nEffective active learning often relies on computing look-ahead distributions that predict the impact\nthat making a particular observation will have on the model. For our purposes, we wish to predict"}, {"title": "The Derivative Look-Ahead Distribution", "content": "the impact that observing f at a candidate point x* will have on the model's derivatives at that\nlocation. This will allow us to select a point x* that most improves the posterior of the derivatives, in\nexpectation. Under a GP, this look-ahead distribution is tractable. Conditioned on the observations D,\n$f(x)$ and $\\frac{\\partial f(x_*)}{\\partial x_i}$ have a bivariate normal joint distribution for each input dimension i. The well-known\nformula for bivariate normal conditioning then provides the look-ahead distribution Lyu et al. [2021]:\n$\\frac{\\partial f(x)}{\\partial x_i} | f(x_*) = y_*, D \\sim \\mathcal{N}\\left(\\mu'_i, \\frac{{\\sigma'}^*_i}^2}{{\\sigma_*}^2} \\right)$, where the look-ahead mean and variance are\n$\\mu'_i = \\mu'_i - \\frac{{\\hat{\\sigma}}_{*, i}}{{\\sigma_*}^2} (y_* - \\mu_*)$,\n${\\hat{\\sigma'}}_i^2 = {\\sigma'_i}^2 - \\frac{{{\\hat{\\sigma}}_{*, i}}^2}{{\\sigma_*}^2}$,\nwith ${\\hat{\\sigma}}_{*,i} = \\mathrm{Cov}[f(x), \\frac{\\partial f(x_*)}{\\partial x_i} | D]$ the posterior covariance between f, and the derivative at x, and\n$(\\hat{\\sigma'_i})^2 = {\\sigma'_i}(x)^2 = [{\\Sigma'}(x_*)]_{ii}$ the posterior variance of the derivative. As before, we use the notational\nshort-hand $\\sigma_i = (x_*)$ and $\\mu'_i = \\mu'(x_*)$. This result holds when $y_*$ is a noisy observation by\nreplacing $\\sigma_i$ with $\\eta^2 + \\sigma^2$. Remarkably, the look-ahead variance is independent of the actual observed\n$y_*$, so acquisition functions that are based on the future variance of the derivative can be computed\nexactly in closed form. In the Appendix, we provide the look-ahead posterior distribution of the\nderivative of f at any point in the input space after observing f at x*."}, {"title": "Gradient Acquisition Functions", "content": "The posterior variance of each derivative is given in (Equation 2). The\nmaximum derivative variance acquisition function uses the sum of the variances across dimensions to\nfind points with high total uncertainty in the derivatives:\n$\\alpha_{DV}(x) = \\sum_{i=1}^d {\\sigma'_i(x)}^2$.\nThe derivative variance reduction acquisition computes the expected reduction\nin variance of the derivatives produced by making an observation of f at x:\n$\\alpha_{DVr}(x) = \\sum_{i=1}^d {\\sigma'_i(x)}^2 - E_y[{\\hat{\\sigma'}}_i(x)^2] = \\sum_{i=1}^d {\\sigma'_i(x)}^2 - {\\hat{\\sigma'}}_i(x)^2$,\nwhere ${\\hat{\\sigma'}}_i(x)^2$ is the look-ahead variance of the derivative (Equation 3). The expectation is dropped\nbecause the look-ahead variance is independent of the observed y at the candidate point.\nWe express our derivative information gain acquisition function as the sum of\ninformation gains for each derivative. Let $\\mathcal{H}(x) = h\\left(\\frac{\\partial f(x)}{\\partial x_i} | D\\right)$ be the differential entropy of each"}, {"title": "Absolute Gradient Acquisition Functions", "content": "The absolute value of a normal distribution is the folded normal distribution\n whose mean and variance, $\\mu_{i,Ab}(x)$ and ${\\sigma_{i,Ab}}(x)^2$, are analytical and can be\ncomputed from the moments of the corresponding normal distribution. Using those results, the\nposterior of $\\frac{\\partial f(x)}{\\partial x_i}$ has mean and variance:\n$\\mu'_{i, A b}(x)=\\frac{\\sigma_{i}^{\\prime}(x)}{\\sqrt{\\pi}} e^{-\\frac{\\nu_{i}^{2}(x)}{2}}+\\mu_{i}^{\\prime}(x)\\left(1-2\\Phi(-\\nu_{i}(x))\\right)$,\n$\\sigma_{i, A b}^{\\prime}(x)^{2}=\\mu_{i}^{\\prime}(x)^{2}+\\sigma_{i}^{\\prime}(x)^{2}-\\mu_{i, A b}^{\\prime}(x)^{2}$\nwhere $\\Phi$ is the standard normal CDF and, for convenience, we have denoted $\\nu_{i}(x)=\\frac{\\mu_{i}^{\\prime}(x)}{\\sigma_{i}^{\\prime}(x)}$.\nAnalogously to (4), we define the maximum variance acquisition for the absolute value of the derivative\nas $\\alpha_{A D A V}(x)=\\sum_{i=1}^{d} \\sigma_{i, A b}^{\\prime}(x)^{2}$.\nThe look-ahead variance for the absolute value of the derivative, denoted\n$\\hat{\\sigma_{i, A b}^{\\prime}(x)^{2}}$, can be computed by plugging the look-ahead moments from (3) into the formula for the\nfolded normal variance. However, unlike for the raw derivative, this variance depends on $\\mu'(x)$ and\nis thus a function of y, making the expectation in the variance reduction formula intractable. We\nfollow the strategy of Lyu et al. [2021] and approximate $E_y[\\frac{\\partial f}{\\partial x_i}(x)^2]$ with a plug-in estimator, fixing\ny = $\\mu$(x). Plugging this estimator into (3) gives an estimate for the look-ahead derivative mean that is\nindependent of y, denoted ${\\hat{\\sigma_{i}^{\\prime}}}^2$, and it follows that\n$\\alpha_{A D A b V r}(x)=\\sum_{i=1}^{d} \\sigma_{i, A b}^{\\prime}(x)^{2}-E_{y}[\\frac{\\partial f}{\\partial x_i}^{2}(x)] \\approx \\sum_{i=1}^{d} \\sigma_{i, A b}^{\\prime}(x)^{2}-{\\hat{\\sigma_{i}}}^{\\prime} (x)^{2}$"}, {"title": "Squared Gradient Acquisition Functions", "content": "Let $Q = \\left(\\frac{\\partial f(x)}{\\partial x_i}\\right)^{2}$. The posterior of $\\left(\\frac{\\partial f(x)}{\\partial x_i}\\right)^{2}$ has a noncentral chi-squared\ndistribution $\\frac{Q}{\\sigma_{i}^{\\prime}(x)^{2}} | D \\sim \\chi^{2}\\left(1, \\frac{\\mu_{i}^{\\prime}(x)}{\\sigma_{i}^{\\prime}(x)}\\right)$. The posterior variance of the squared derivative follows directly:\n$\\sigma_{i s q}^{\\prime}(x)^{2}=4 \\sigma_{i}^{\\prime}(x)^{2} \\mu_{i}^{\\prime}(x)^{2}+2 \\sigma_{i}^{\\prime}(x)^{4}$.\nAs before, we construct the maximum variance acquisition function as\n$\\alpha_{A D S q V}(x)=\\sum_{i=1}^{d} \\sigma_{i s q}^{\\prime}(x)^{2}$.\nAs with the absolute value of the derivative, the look-ahead variance for the\nsquare of the derivative depends on the observed value y via the term $\\mu'(x)$, making the expectation in\nvariance reduction intractable. We again approximate the variance reduction with a plug-in estimator\nin (3), substituting $\\mu'_{i}(x)$ for $\\mu_{i}^{\\prime}(x)$, to get an estimate for the look-ahead variance of the squared\nderivative:\n${\\hat{\\sigma}}_{i s q}^{\\prime}(x)^{2}=4 \\sigma_{i}^{\\prime}(x)^{2} {\\mu}_{i}^{\\prime}(x)^{2}+2 \\sigma_{i}^{\\prime}(x)^{4}$.\nThe variance reduction can then be computed as\n$\\alpha_{A D S q V r}(x)=\\sum_{i=1}^{d} \\sigma_{i s q}^{\\prime}(x)^{2}-E_{y}[{\\hat{\\sigma}}_{i s q}^{\\prime}(x)^{2}] \\approx \\sum_{i=1}^{d} \\sigma_{i s q}^{\\prime}(x)^{2}-{\\hat{\\sigma}}_{i s q}^{\\prime}(x)^{2}$.\nLet $H_i^{Q}(x)=h\\left(Q | D\\right)$ be the differential entropy of the square of the derivative\nposterior. Using properties of entropy [Cover, 1999], we have\n$H_i^{Q}(x)=h\\left(\\frac{Q}{\\sigma_{i}^{\\prime}(x)^{2}}|D\\right)+2 \\log (\\sigma_{i}^{\\prime}(x))$.\nThe entropy of the noncentral chi-squared distribution can be computed analytically in terms of the\nhypergeometric function $2 F_{2}$ [Moser, 2020]. Using that result yields:\n$H_i^{Q}(x)=2 \\log (\\sigma_{i}^{\\prime}(x))-2 F_{2}\\left(1,1, \\frac{3}{2}, \\frac{\\nu_{i}^{2}(x)}{2}\\right) \\nu_{i}(x)^{2}$.\nWe use as the acquisition function the sum of the expected entropy reductions across all derivatives,\n$\\alpha_{A D S q I G}(x)=\\sum_{i=1}^{d} H_i^{Q}(x)-E_{y}\\left[H_i^{Q}(x) | f(x)=y\\right]$.\nThis expectation is not tractable due to the dependence of the look-ahead entropy on y, via $\\nu_i(x)$. We\nuse the same plug-in estimator as before to get $\\nu_{i}^{\\prime}(x)=\\frac{\\mu_{i}^{\\prime}(x)}{\\sigma_{i}^{\\prime}(x)}$. The approximated look-ahead entropy\nis then\n$H_i^{Q, *}(x)=2 \\log (\\sigma_{i}^{\\prime}(x))-2 F_{2}\\left(1,1, \\frac{3}{2}, 2, \\frac{\\nu_{i}^{2}(x)}{2}\\right) {\\nu_{i}^{\\prime}(x)}^{2}$"}, {"title": "Global Variance Reduction and Information Gain", "content": "Our look-ahead acquisition functions were all local, in that they evaluated the impact of an observation\nat x only on the posterior at x*. In this section, we consider the global look-ahead acquisitions, where we\nevaluate the impact on the posterior across the entire input space. These acquisition functions\nare expressed as integral over the previously proposed local acquisition functions. Their complexity\nis $O(t^3 + t^2dM)$, with M defined as the granularity of the integration. We provide the results and\ndiscussion in Appendix D."}, {"title": "Experiments", "content": "We compared the proposed active learning acquisition functions for DGSMs to space-filling approaches\nand general uncertainty reduction approaches. We refer to quasirandom sequences as QR, variance\nmaximization of f as fVAR, and information gain about f [i.e., BALD, Houlsby et al., 2011] as fIG.\nFor the acquisition functions developed here, we use the following acronyms (Section 4): max variance\nof the raw, absolute, and squared derivatives are DV, DAbV, and DSqV; variance reduction of the raw,\nabsolute and squared derivatives are D$\\text{V}_{\text{r}}$ D$\\text{AbV}_{\text{r}}$ and D$\\text{SqV}_{\text{r}}$; and information gain of the raw and\nsquared derivatives are DIG and DSqIG. We used synthetic and real-world problems for evaluation.\nWe conducted experiments on a family of functions that were designed\nspecifically for evaluating sensitivity analysis measures [Kucherenko et al., 2009, Kucherenko and Iooss,\n2014]: Ishigami1 (d = 3), Ishigami2 (d = 3), Gsobol6 (d = 6), a-function (d = 6), Gsobol10 (d = 10),\nGsobol15 (d = 15) and Morris (d = 20). Ground-truth DGSMs are available for these problems.\nAdditionally, we tested the methods on other general-purpose synthetic functions where sensitivity\nmight be challenging to estimate [Dixon, 1978]: Branin (d = 2), Hartmann3 (d = 3) and Hartmann4\n(d = 4). For these functions, we numerically estimated ground-truth DGSMs.\nWe considered three real-world design problems. The Car Side Impact\nWeight problem simulates the impact of d = 7 design variables on the weight of a car, to study the\nimpact of weight on accident scenarios. Design variables are the thickness of pillars, the floor, cross\nmembers, etc. We also used the Vehicle Safety problem, which has two functions: the weight and the\nacceleration of the vehicle. Both are functions of d = 5 design variables describing the thickness of\nfrontal reinforcement materials. We study the two functions as independent problems. Ground truth\nDGSMs for these problems were estimated numerically.\nWe study settings with limited evaluation budgets. Quasirandom sequences\nare known to perform well given enough data [De Lozzo and Marrel, 2016, Chauhan et al., 2024]. Here,\nwe focus on the restrictive case where we initialize our experiments using five random inputs and run"}, {"title": "Results and Discussion", "content": "RMSE results for seven synthetic functions and three real-world problems (Figure 2, for both absolute\nand square DGSMs, show the benefits of our approach. Across this wide set of problems, the active\nlearning approaches targeting quantities of the derivatives developed here consistently outperformed\nquasirandom sequences (QR) and active learning methods that target learning about f globally (fVAR\nand fIG). The acquisition functions based on the information gain of the derivative (DIG) and squared\nderivative (DSqIG) performed best in the majority of experiments.\nIn the high-dimensional problems (GSobol15 and Morris), we see a substantial degradation of perfor-\nmance for the baseline methods (QR, fVAR, and fIG), with little reduction of RMSE across iterations.\nOur proposed active learning acquisition functions targeting the derivatives, on the other hand, contin-\nued to perform well in high dimensions. It is interesting to note that on these problems the derivative\nmax variance acquisition functions, DAbV, and DSqV, outperformed the derivative information gain\nacquisition functions. This is due to the myopic nature of the one-step look-ahead used for information\ngain. The most information gain about the derivative comes from adjacent points (Figure 1). In high\ndimensions, one-step look-ahead focuses on areas near existing observations, and is not exploratory\nenough to capture the whole function. Max variance is naturally more exploratory and thus per-\nforms better in high-dimensional settings. However, the derivative information gain acquisitions still\noutperform the baselines in high-dimensional settings."}, {"title": "Ablations and Additional Results", "content": "In settings where DGSMs are used to determine the relative importance of the\nvariables, we are interested in the ability of the method to correctly rank the variables. We replaced\nRMSE as the evaluation metric with normalized discounted cumulative gain (NDCG) , a commonly used metric of ranking quality. NDCG accounts for the position of\neach dimension, giving higher importance to dimensions at the top of the list, and is normalized to\na scale from 0 to 1, where 1 represents perfect recapitulation of the order. As with RMSE, DIG and\nDSqIG generally perform best on this task (Figure 5 in the Appendix)."}, {"title": "Conclusions", "content": "In this work, we developed a collection of active learning methods that directly target DGSMs.\nThese strategies substantially enhanced the sample efficiency of DGSM estimation when compared to\nquasirandom search and even compared to active learning strategies targeting f. Information gain\nabout the derivative and the squared derivative were generally the best approaches.\nOur work paves the way for additional work on active learning for DGSMs in several directions.\nAlthough both variance reduction and information gain approaches perform well in high dimensions,\ninformation gain approaches might benefit from increased exploration by using a two-step or batch\nlook-ahead to select pairs of points in acquisition function optimization. Our acquisition functions\nalso all take the form of a sum over dimensions. Computing entropy of multivariate posteriors for\nthe gradient is another possible avenue. Active learning for DGSMs could also be developed for\nnon-Gaussian models. Finally, DGSMs have been linked via several lower/upper bound inequalities to\nANOVA-based sensitivity indices . Understanding the impact of active\nlearning for DGSMs on ANOVA-based sensitivity indices is another useful direction for future work."}, {"title": "Additional Discussion of Related Work", "content": "Here we give further discussion of lines of work that incorporate derivatives into active learning, albeit\nnot for the purpose of GSA.\nSalem et al. [2019] proposed an optimization approach where at\neach step the dimensionality is reduced by identifying unimportant features. Unimportant variables are\nfixed while simultaneously optimizing the important ones with expected improvement. These variables\nare identified through their lengthscales. As part of theoretical analysis, the paper proves an asymptotic\nrelationship between lengthscale and the square DGSM: as lengthscale goes to infinity, DGSM for that\nparameter goes to zero. This work does not propose an active learning approach for DGSMs but rather\nuses DGSMs as a metric to asses the quality of the dimensionality reduction approach. Spagnol et al.\n used sensitivity analysis to eliminate variables during optimization. The sensitivity analysis is\ngoal-oriented rather than global, by applying the Hilbert-Schmidt independence criterion to portions\nof the function below a particular output quantile. Sensitivity measures are part of the algorithm and\nare assumed to be accurate, the paper does not study learning them.\nErickson et al. [2018] developed a method\nfor active learning of a non-stationary function that specifically targets areas of high gradient, although\nwithout trying to estimate the gradient globally. Variance reduction of the function serves as the\nacquisition, but weighted by the estimated derivative and its variance. The method thus focuses on\nminimizing the variance at points with large derivatives, but does not target learning the derivative\nitself. Marmin et al. [2018] proposed an active learning approach for non-stationary functions using\nwarping. The acquisition strategy uses both variance reduction and derivatives, following a similar\nidea as Erickson et al. [2018] that areas of high gradient will be helpful for learning the function.\nWycoff et al. [2021] proposed a set of three methods (Trace, Var1,\nand Var2) for active subspace identification. While the goal is different from our proposed methods,\nthe acquisition functions developed in the paper do aim to learn a quantity of the derivatives, as in\nour work. Specifically, the acquisitions attempt to learn the expectation of the outer product of the\ngradient. The elements of the diagonal of the outer product of the gradient are the squared DGSMs,\nso learning the outer product will learn a function of the squared DGSMs, albeit without targeting\nthem specifically.\nBecause these methods target learning a function of the gradient, we compared against them on three"}, {"title": "Computational Complexity", "content": "All of our acquisition functions have closed-form expressions. Computing the GP posterior is $O(t^3+t^2d)$,\nwith t the number of data and d the dimensionality of the function. Here the $t^3$ comes from inverting\n$k_{X,X}$, and the $t^2d$ comes from multiplying it with the gradient of $k_{X_*,X}$. Given that, the remaining\ncomputation will all scale linearly with d, since it is just applying various formulae to the posteriors\nfor each dimension."}, {"title": "Global Look-ahead Acquisition Functions", "content": "Our look-ahead acquisition functions were all local, in that they evaluated the impact of an observation\nat x only on the posterior at x*. In this section, we consider the global look-ahead acquisitions where\nwe evaluate the impact on the posterior across the entire input space. These acquisition functions\nare expressed as integral over the previously proposed local acquisition functions. Their complexity\nis $O(t^3 + t^2dM)$, with M defined as the granularity of the integration. We provide the results and\ndiscussion in Appendix D."}, {"title": "Derivative Look-Ahead Distribution at Different Input Location", "content": "We wish to predict the impact that observing f at a candidate location x* will have on the model's\nderivatives at a different location $x_+$. This will allow us to select a point x, that most improves the\nmodel of the derivatives in the overall search space, in expectation. Under a GP, this look-ahead\ndistribution is tractable. Conditioned on the observations D, f(x) and $\\frac{\\partial f(x_+)}{\\partial x_i}$ have a bivariate normal\njoint distribution for each input dimension i. The formula for bivariate normal conditioning once again\nprovides the look-ahead distribution:\n$\\frac{\\partial f(x_+)}{\\partial x_i} | f(x_*) = Y_*, D \\sim \\mathcal{N} \\left(\\mu'_{+,i}, {\\hat{\\sigma}}'_{+,i}^{2}\\right)$,\nwhere the look-ahead mean and variance are respectively\n${\\hat{\\mu}}'_{+,i}={\\mu'}_{+,i}+\\frac{{\\hat{\\sigma}}_{+*,i}}{{\\sigma_*}^2} (y_*- \\mu_*)$,\\\\\n${{\\hat{\\sigma'}}'}_{+,i}^2=({\\sigma'}_{+,i})^2-\\frac{{{\\hat{\\sigma}}_{+*,i}}^2}{{\\sigma_*}^2}$"}, {"title": "Global Look-Ahead Gradient Acquisition Functions", "content": "We first develop the global version of the look-ahead acquisition functions targeting the uncertainty\nand information gain about the derivative of the GP.\nWe compute the global expected reduction in variance produced by\nan observation at $x_*$ as follows:\n$\\alpha_{G D V r}\\left(x_{*}\\right)=\\frac{1}{|X|} \\int_{X} \\sum_{i=1}^{d} {\\sigma_{i}^{\\prime}(x)}^{2}-E_{y_{*}} [\\hat{{\\sigma_{i}^{\\prime}"}]}