{"title": "Trustworthy AI: Securing Sensitive Data\nin Large Language Models", "authors": ["Georgios Feretzakis", "Vassilios S. Verykios"], "abstract": "Large Language Models (LLMs) have transformed natural language processing\n(NLP) by enabling robust text generation and understanding. However, their\ndeployment in sensitive domains like healthcare, finance, and legal services raises\ncritical concerns about privacy and data security. This paper proposes a compre-\nhensive framework for embedding trust mechanisms into LLMs to dynamically\ncontrol the disclosure of sensitive information. The framework integrates three\ncore components: User Trust Profiling, Information Sensitivity Detection, and\nAdaptive Output Control. By leveraging techniques such as Role-Based Ac-\ncess Control (RBAC), Attribute-Based Access Control (ABAC), Named Entity\nRecognition (NER), contextual analysis, and privacy-preserving methods like\ndifferential privacy, the system ensures that sensitive information is disclosed\nappropriately based on the user's trust level. By focusing on balancing data\nutility and privacy, the proposed solution offers a novel approach to securely\ndeploying LLMs in high-risk environments. Future work will focus on testing\nthis framework across various domains to evaluate its effectiveness in managing\nsensitive data while maintaining system efficiency.\nKeywords: Large Language Models, Trust Mechanisms, Sensitive Informa-\ntion, Role-Based Access Control, Attribute-Based Access Control, Data Privacy,\nPrivacy-Preserving Techniques, Named Entity Recognition, Differential Privacy,\nAI Ethics.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 Background", "content": "Overview of Large Language Models (LLMs) and Their Significance"}, {"title": "AI\nThe Growing Concern over Sensitive Information Management in", "content": "However, simultaneously, LLMs have become a growing concern with regard\nto the handling of sensitive information [7]. Due to the large-scale nature of\ntheir training datasets, LLMs may inadvertently contain personal, confidential,\nor proprietary information. Thus, such LLMs can unintentionally produce an-\nswers that disclose such data [8]. Research has shown that LLMs can memorize\nand regurgitate pieces of their training data, even if it is sensitive personal in-\nformation such as names, addresses, or unique identifiers [9] [10]. This presents\nserious risks to privacy and security when LLMs are deployed in applications\naccessed by people or in situations where data confidentiality is of utmost im-\nportance.\nOne of the critical issues emerging today is that if such LLMs are employed\nwithin systems dealing with sensitive information-legal, medical, or financial"}, {"title": "1.2 Problem Statement", "content": "Challenges in Preventing Unauthorized Disclosure of Sensitive Infor-\nmation by LLMs\nWhile LLMs have unparalleled capabilities to generate text-like human lan-\nguage, this often involves considerable risks in terms of possible leakage or disclo-\nsure of private information. The issue is that LLMs can memorize and produce\nparts of their training datasets, which include PII data and confidential infor-\nmation at relevant questioning [9] [15]. This presents a significant challenge,\nconsidering that many LLMs are trained on extremely large sets of internet-\nscraped data, which might contain sensitive or private information [15] [16].\nAttackers can exploit LLMs through model inversion or other methods to\nextract sensitive data [17] [18]. For example, an attacker might incorporate a\nseries of crafted prompts to extract social security numbers, credit card informa-\ntion, or private communications that it may have inadvertently learned from the\nmodel's training dataset [15] [19]. This problem increases, given the black-box\nnature of most LLMs: their internal mechanisms have remained rather opaque.\nIt gives very little insight into whether one can predict or control what the\nmodel may reveal [20].\nAlso, since LLMs are probabilistic and generating text is a probability in\nitself, auditing and controlling its outputs are very hard to achieve [21]. Clas-\nsic content filtering methods such as keyword blacklists do not work since the\nsame sensitive information can be expressed using paraphrased or contextually\nchanged language [22]. Besides, it may generate sensitive information as a part\nof its normal operation without needing any malicious prompting [23].\nThe sensitivity lies in the balance between utility to LLMs and concerns\nabout sensitive information protection [11]. Over-constraining the output will\nlead to a model's lowered effectiveness and limited applications for legitimate\nuse. In contrast, this will enhance the risks of data breaches and non-compliance\nwith privacy regulations if adequate checks are not in place [24]. This will\nrequire a sophisticated balance, wherein the sensitivity level of the information\nbeing processed and generated by the large language models will be dynamically\nassessed and managed."}, {"title": "Limitations of Current Approaches in Managing Sensitive Data", "content": "General methods of preventing unauthorized disclosure of sensitive informa-\ntion by LLMs include data sanitization, differential privacy, and output filter-\ning [14], [24], [25]. Nevertheless, these methods have some important limitations\nthat impair their potential.\nData sanitization is the process of removing sensitive information from the\ntraining dataset before models are trained [26]. This approach in theory can de-\ncrease the probability of the model assimilating and reproducing sensitive data,\nbut in practice is often infeasible given the vast amount of training data and\nthe difficulty in recognizing all forms of sensitive content [27], [28]. Automated\nsanitization may miss context-specific or subtle forms of sensitive information,\nwhile manual sanitization of billions of words in datasets becomes practically\nimpossible [28].\nDifferential Privacy adds mathematical noise to the training process, en-\nsuring that the model cannot learn details about individual data points and is\nrestricted from memorizing such details [14], [29]. But then again, though differ-\nential privacy offers good theoretical guarantees against data extraction attacks,\nit degrades the performance of the model in complex language tasks [29] [30].\nBesides, differential privacy on a scale needed for LLMs has high computational\nrequirements and may not be practical in most real-world scenarios [30].\nOutput filtering inspects the model's outputs, being applied to discard or\nmodify sensitive content before it is delivered to the user [31]. This scheme\nhas difficulty in precisely identifying all occurrences of sensitive information\nwithout a high false positive rate [32]. These nuances in contextualization and\nthe model's ability to output semantic variations of the same expression make it\neven harder for filters to identify every instance of sensitive data disclosure [33].\nMoreover, experienced users will generally find ways to bypass output filters\nthrough skillful prompting techniques [33].\nMost of these approaches tend to also be myopic in terms of the trust level of\nthe user interacting with the LLM [34]. They invariably follow a one-size-fits-all\npolicy that does not make a distinction for users based on roles, permissions,\nor trustworthiness [35]. This inability to discriminate between users can result\nin either legitimate access for trusted users not being possible or unauthorized\ndisclosure to untrusted users.\nIn other words, the realization of sensitive information treatment cannot be\nachieved by any state-of-the-art methods at the current stage of LLMs. This\ncalls loudly for more active and fine-grained methods that embed mechanisms\nof trust into the very operation of LLMs, allowing responses to be shaped by\nthe user's trust level to fortify security without unduly eroding the utility of the\nmodel."}, {"title": "1.3\nObjectives", "content": "To Propose a Framework for Embedding Trust Mechanisms in LLMs\nThis chapter generally proposes a framework that embeds mechanisms of\ntrust inside LLMs. By incorporating trust management inside LLMs, this frame-"}, {"title": "2 Literature Review", "content": ""}, {"title": "2.1 Large Language Models and Their Limitations", "content": "Overview of LLM Architectures and Functionalities\nTo date, Large Language Models (LLMs) have significantly advanced the"}, {"title": "Discussion of Issues Related to Information Leakage", "content": "While doing so, LLMs run major risks of information leakage in sensitive\nor private data in the generated outputs from the training corpus [9, 14]. This\npresents serious privacy risks, as many models are trained on datasets with no\nexplicit consent for personal or proprietary information.\nA significant issue involved is the memorization of training data. Large\nLanguage Models can indeed memorize infrequent or unique sequences contained\nin the training dataset, which later could be verbatim reproduced when a specific\nprompt is given [9,15]. This poses problems when the memorized information\ninvolves PII or confidential data. Attackers may carry out extraction attacks\nby crafting inputs that get the model to reveal sensitive information [15, 17].\nSuch attacks would exploit the model's ability to memorize some inputs and\nthen reproduce them.\nTraditional mitigation techniques such as anonymization or filtering can only\ndo so much since models may infer or reconstruct sensitive information due to\npattern leakage in the data itself [26, 28]. Moreover, it is considered computa-\ntionally infeasible to apply successful methods of privacy preservation on the\nscale required by LLMs [30]. Methods like differential privacy aim to limit the\ninfluence of a single datapoint on model parameters [14,29]; however, finding a\nbalance between privacy and utility remains an open challenge [30].\nRecent works have pointed out these subjects. Carlini et al. [15] showed that\nGPT-2 and GPT-3 models, among others, could involuntarily leak sensitive per-\nsonal information kept inside their training data. Lehman et al. [10] observed\nthat BERT models pre-trained on clinical notes can leak sensitive patient infor-\nmation; this raises concern about their use in possible medical environments.\nMost promising solutions to achieve the above include mechanisms for strict\ndata curation and consents that forbid training on unauthorized sensitive in-\nformation [27,28], ways to embed user trust levels and access controls into the\nLLM systems, and ways to enable dynamic management of the disclosure of sen-\nsitive information [34,36,51]. However, numerous issues remain: performance\ntradeoffs and bypassing by the user with rephrased or alternative prompts are\nstill possible [22,33].\nInformation leakage in LLMs is of great concern because increasingly such\nmodels will be finding applications in sensitive data environments. There is\nhence the urgent need for sophisticated solutions capable of impeding unautho-\nrized disclosure, all without affecting model performance. Building mechanisms\nof trust and creating robust techniques for preserving privacy are what become\nnecessary for responsible deployment."}, {"title": "2.2 Trust Mechanisms in Computing", "content": "Existing Trust Models (e.g., RBAC, \u0410\u0412\u0410\u0421)\nTrust mechanisms are foundational in computing systems to ensure secure\nand appropriate access to resources. Two predominant models used to manage\nand enforce trust are Role-Based Access Control (RBAC) and Attribute-Based\nAccess Control (ABAC).\nRole-Based Access Control (RBAC) is an access control paradigm where per-\nmissions are associated with roles, and users are assigned to these roles, thereby\nacquiring the permissions of the roles [52]. This model simplifies management by\ngrouping permissions into roles that reflect organizational job functions. RBAC\nis widely adopted in enterprises due to its straightforward approach to access\nmanagement.\nKey components of RBAC include:\n\u2022 Users: Individuals who need access to system resources.\n\u2022 Roles: Defined job functions within an organization.\n\u2022 Permissions: Approval to perform certain operations."}, {"title": "Comparison and Integration", "content": "Comparison and Integration While RBAC offers simplicity and ease of\nadministration, ABAC provides the flexibility needed for complex and dynamic\naccess control scenarios [56]. There is a growing interest in integrating RBAC\nand ABAC to leverage the benefits of both models. Such integration aims to\nenhance scalability and manageability while providing granular access control\n[55,56]."}, {"title": "Trust in Human-Computer Interaction", "content": "Trust in Human-computer in-\nteraction (HCI) is critical for user acceptance and effective use of technol-\nogy [57,58]. It influences how users perceive, engage with, and rely on com-\nputing systems, particularly those that handle sensitive information or perform\ncritical functions.\nFactors Influencing Trust in HCI:\n1. Reliability: Consistent and dependable system performance builds user\ntrust [59]. Systems that frequently crash or produce errors can erode\nconfidence.\n2. Security and Privacy: Assurance that personal data is protected is\nessential for trust [58,60]. Users are more likely to trust systems that\ndemonstrate robust security measures and transparent privacy policies.\n3. Usability: Intuitive interfaces and user-friendly designs enhance trust by\nmaking systems accessible and easy to navigate [61].\n4. Transparency and Explainability: Systems that provide clear expla-\nnations for their operations and decisions help users understand and trust\nthe technology [62].\n5. Responsiveness: Timely and appropriate responses to user inputs con-\ntribute to a positive user experience and increased trust [63]."}, {"title": "Trust Challenges in AI Systems", "content": "With the rise of artificial intelligence\nand machine learning, new challenges have emerged in establishing trust [64,"}, {"title": "Building Trust in AI Systems:", "content": "\u2022 Explainable AI (XAI): Developing AI models that can provide explana-\ntions for their decisions enhances transparency and trust [66]. XAI helps\nusers understand the reasoning behind AI outputs.\n\u2022 Ethical Design: Incorporating ethical principles, such as fairness and\naccountability, ensure that AI systems operate responsibly [67,68]. Ad-\ndressing ethical considerations proactively can mitigate trust issues.\n\u2022 User-Centered Design: Involving users in the design process helps align\nthe system with user needs and expectations [61,68]. Feedback mecha-\nnisms and iterative design improve usability and trustworthiness.\n\u2022 Security Measures: Implementing robust security protocols protects\nagainst unauthorized access and data breaches, reinforcing user trust [60]."}, {"title": "Implications for LLMS", "content": "Implications for LLMS Clearly, trust is an element of great importance in\ninteracting and accepting Large Language Models (LLMs) by users in [69]. Ap-\nplications regarding sensitive information, such as virtual assistants, customer\nservice bots, and systems that support decision-making, are being led by large\nlanguage models. The user needs to trust that the system will result in trusted,\nrelevant, and appropriate information.\nBuilding trust mechanisms for LLMs through integration of mechanisms\nfrom access control models such as RBAC and ABAC, to manage information\ndisclosure according to the user's trust level [70]. By tailoring the output of\nLLMs according to users' trust attributes, systems can prevent unauthorized\naccess to sensitive information while maintaining usability.\nFor instance, an LLM featuring ABAC will assess the user attributes and the\nenvironmental context prior to generating the response, in a way that sensitive\ninformation is only received by the related persons [36, 70]. It will provide\nbetter security about compliance with privacy regulations and still maintain\nthe advantages of AI-based interaction."}, {"title": "2.3 Sensitive Information Management Techniques", "content": "Sensitive information management is crucial for organizations to protect data\nconfidentiality, integrity, and availability. Effective management techniques in-\nclude data classification frameworks and content filtering with Data Loss Pre-\nvention (DLP) methods. These strategies help organizations identify, categorize,\nand safeguard sensitive data, ensuring compliance with regulations and mitigat-\ning the risk of data breaches.\nData Classification Frameworks Data classification schemes provide a\nsystematic way or methodology of classifying organizational data according to"}, {"title": "Content Filtering and Data Loss Prevention (DLP) Methods", "content": "Content filtering and DLP are two important components in the prevention of sen-\nsitive information leakage [84]. While content filtering includes monitoring and\ncontrolling the transfer of data in accordance with the set parameters, DLP\nfocuses on the identification, monitoring, and protection of sensitive data at\nrest, in use, or even in transit through a network to prevent any potential data\nleak [85]."}, {"title": "2.4\nEthical and Legal Considerations", "content": "The fast development of artificial intelligence, especially large language models,\nraises apprehension in the area of ethics and law. All these technologies must fit\nthe requirements of privacy regulations and provide ethical norms for respecting"}, {"title": "3 Proposed Framework for Embedding Trust Mech-\nanisms in LLMs", "content": ""}, {"title": "3.1 Overview of the Framework", "content": "The proposed framework aims to integrate trust mechanisms within the oper-\nation of Large Language Models (LLMs) to manage and control sensitive in-\nformation disclosure dynamically. This approach draws from key concepts in\nRole-Based Access Control (RBAC), Attribute-Based Access Control (ABAC),\nand privacy-preserving techniques such as differential privacy. The primary ob-\njective is to balance data utility and protection by adjusting the LLM's output\nbased on the trust level assigned to the interacting user or entity. This frame-\nwork is applicable across various sensitive domains like healthcare, finance, and\nlegal services where privacy and data protection are paramount.\nThe framework consists of three main components:"}, {"title": "3.1.1 User Trust Profiling", "content": "User Trust Profiling is the foundational component that evaluates and assigns\ntrust levels to users based on a predefined set of attributes. It determines the\nlevel of access each user has to sensitive data, enabling dynamic control of the\nLLM's outputs. Profiling users based on their role, access purpose, and contex-\ntual factors ensures that sensitive information is disclosed only to authorized\nusers.\n\u2022 User Role: Based on RBAC principles, user roles (e.g., Administrator,\nHealthcare Provider, Public User) define access levels [52]. For instance,\nhealthcare providers may have higher access to detailed patient informa-\ntion than administrative staff or general users.\n\u2022 Purpose of Access: Drawing from ABAC, access control extends beyond\nroles to include the specific purpose for which information is requested\n[36]. A user seeking data for public information purposes may receive de-\nidentified summaries, while those requiring data for medical procedures\ncould access full patient records."}, {"title": "3.1.2\nInformation Sensitivity Detection", "content": "The second component of the framework, Information Sensitivity Detection,\ninvolves real-time identification of sensitive information within the model's out-\nputs. Since LLMs are trained on large datasets, they might unintentionally\ngenerate sensitive or confidential data. To mitigate this, multiple techniques\nare employed to detect sensitive content and prevent unintended disclosure.\n\u2022 Named Entity Recognition (NER): NER systems are deployed to\nflag personally identifiable information (PII), such as names, addresses,\nand social security numbers, as well as sensitive data in domains like\nhealthcare [10,37]. NER tools detect sensitive entities before they are\nincluded in model outputs.\n\u2022 Text Classification: Machine learning models trained on labeled datasets\nclassify text into sensitivity levels (e.g., public, restricted, or confiden-\ntial) [5]. This classification helps prevent sensitive information from being\nexposed inadvertently.\n\u2022 Contextual Analysis: Beyond individual entities, contextual analysis\ntools evaluate the broader context of the content to identify potentially\nsensitive information that may not include explicit PII [89]. For example,\na business transaction's details may not contain direct identifiers but still\nwarrant protection due to confidentiality.\nBy combining these techniques, the framework can detect and flag sensitive\ndata, safeguarding outputs from accidental disclosures."}, {"title": "3.1.3\nAdaptive Output Control", "content": "The Adaptive Output Control component dynamically adjusts the LLM's gen-\nerated outputs according to the user's trust profile and the sensitivity of the\ndetected information. This approach ensures that the right information is dis-\nclosed to the right users without compromising data privacy.\n\u2022 Redaction: For users with lower trust scores, sensitive information iden-\ntified by the NER or classification systems is redacted from the output [90].\nIn legal or medical contexts, specific details like names, account numbers,\nor patient information may be replaced with placeholders."}, {"title": "3.2 User Trust Profiling", "content": "The User Trust Profiling component builds on Attribute-Based Access Control\n(ABAC) principles, defining user access permissions based on a combination of\nattributes that allow for fine-grained access control. This mechanism ensures\nthat the right users can access appropriate levels of sensitive information de-\npending on their roles, the context of access, and the purpose of interaction.\nThis profiling approach enhances the security of LLM outputs by tailoring re-\nsponses to users' trustworthiness.\nThe key attributes in trust profiling include:\n\u2022 User Role: The role of the user plays a significant part in determining the\nlevel of access they should be granted. For instance, a healthcare provider\nwould require detailed patient information, while a general user might only\nhave access to anonymized or summarized data. This role-based approach\naligns with the Role-Based Access Control (RBAC) model [52,53].\n\u2022 Purpose of Access: The purpose of the user's interaction with the sys-\ntem helps define the scope of data disclosure. For example, if the user's\npurpose is to provide a medical diagnosis, the system would disclose more\ndetailed data than for a general research query [36,54]. The purpose-\ndriven control enhances data protection by aligning the disclosed informa-\ntion with the specific needs of the user.\n\u2022 Contextual Factors: Contextual elements such as location, device se-\ncurity, and network environment further refine the level of access. For"}, {"title": "3.3\nInformation Sensitivity Detection", "content": "The Information Sensitivity Detection component ensures that sensitive infor-\nmation is not inadvertently disclosed in the LLM's output. Multiple automated\ntechniques are employed to detect and manage sensitive content, providing ro-\nbust protection in domains such as healthcare, finance, and legal services.\nKey techniques and tools for sensitivity detection include:\n\u2022 Named Entity Recognition (NER): NER is a widely used method for\ndetecting sensitive entities such as names, social security numbers, medical\nidentifiers, and other personally identifiable information (PII) in the text\n[37]. Tools like Microsoft Presidio provide an open-source, customizable\nsolution for identifying and anonymizing sensitive information. Microsoft\nPresidio uses pre-configured recognizers for common entities and allows\nusers to define custom recognizers, making it highly adaptable to specific\nprivacy needs in domains such as healthcare and finance [130]. By tagging\nthese entities as sensitive, the system ensures that they are either redacted\nor appropriately anonymized before being disclosed to users.\n\u2022 Contextual Analysis: In addition to recognizing specific entities, con-\ntextual analysis is employed to detect sensitive information embedded\nwithin certain contexts. For instance, business transactions or legal discus-\nsions may involve sensitive content that is not directly identifiable through\nNER. Tools such as spaCy and Apache OpenNLP analyze surrounding\nlanguage structures and patterns to capture implicit sensitive information\nthat may not include direct identifiers but could still pose a confidentiality\nrisk. [131, 132]. This capability enhances the system's ability to capture\nimplicit sensitive information that may otherwise go unnoticed.\n\u2022 Privacy-Preserving Techniques: To protect against the risk of sen-\nsitive training data being memorized by the model, privacy-preserving\ntechniques like differential privacy are applied. Differential privacy works\nby adding controlled noise to the data during training, ensuring that no\nindividual data point can be reconstructed from the model's outputs [14].\nThis is crucial for preventing memorization attacks, in which adversaries\nattempt to extract sensitive information embedded in the model's out-\nputs [9]. Open-access libraries such as TensorFlow Privacy and PySyft\nprovide implementations of differential privacy, making it easier to inte-\ngrate these techniques into machine learning models [133].\n\u2022 Domain-Specific Models: Utilize transformer-based models fine-tuned\non datasets relevant to specific domains (e.g., medical records, financial\ndata) to enhance sensitivity detection."}, {"title": "3.4\nAdaptive Output Control", "content": "The Adaptive Output Control component dynamically adjusts the LLM's out-\nput based on the user's trust profile and the sensitivity of the information de-\ntected by the model. This ensures that only appropriate and necessary infor-\nmation is disclosed, while protecting sensitive data from unauthorized access.\nSeveral strategies can be employed to manage the level of information disclosure.\n\u2022 Redaction: For users with lower trust levels, sensitive information can\nbe redacted or anonymized before it is presented in the output. Tools\nlike Microsoft Presidio, mentioned previously for its role in detecting sen-\nsitive entities, can also be employed to automatically redact or replace\nsensitive information with placeholders. Redaction is particularly useful\nin healthcare and legal domains, where personal data like patient infor-\nmation or legal identifiers must be protected [130]. Open-source tools like\nApache Tika are also capable of integrating redaction functionalities into\nthe output pipeline [81].\n\u2022 Summarization: When full disclosure of detailed information is unneces-\nsary or potentially risky, the system can employ summarization techniques\nto provide high-level overviews while omitting sensitive details. Tools like\nOpenAI GPT models or BART (Bidirectional and Auto-Regressive Trans-\nformers) are commonly used for summarizing text, which helps maintain\nthe utility of the information without revealing sensitive content [134].\nSummarization is particularly valuable in cases where users require gen-\neral insights without access to detailed records, such as in public reports\nor research abstracts. While redaction ensures sensitive entities are hid-\nden or replaced with placeholders, summarization provides a high-level\noverview, removing the need for detailed sensitive data.\n\u2022 Differential Privacy: To ensure that sensitive data points are not leaked\neven in high-trust environments, differential privacy can be applied to the\noutput. By adding controlled noise to the generated responses, differential\nprivacy protects individual data points from being reconstructed from the\nmodel's output [14]. Libraries like TensorFlow Privacy and PySyft allow"}, {"title": "3.5 Framework Overview: Integrating Trust and Privacy\nMechanisms in LLMs", "content": "The flowchart provided in Figure 1 outlines a comprehensive framework devel-\noped to embed trust mechanisms directly into large language models (LLMs).\nThe aim is to dynamically manage the disclosure of sensitive information while\nbalancing data utility and privacy concerns. This framework is designed specif-\nically for high-stakes environments such as healthcare", "components": "n1. User Trust Profiling (blue section): This module determines the trust-\nworthiness of users by evaluating their role", "section)": "In this module"}, {"section)": "Based on the user's trust\nprofile and the sensitivity of the detected information, this module applies\ndynamic controls to the LLM's output. Such dynamic control may include\nredaction, summarization, differential privacy, and so on over the output\nof the LLM. For instance, a low-trust user may receive a summarized\nor redacted version of the output, while a high-trust user may access\nmore detailed information. This ensures that sensitive data is disclosed\nappropriately while maintaining the utility of the LLM in diverse scenarios.\nThe flowchart illustrates how these components interact seamlessly to create\na secure and privacy-aware LLM framework. The system dynamically adapts to\nvarying trust levels and information sensitivity, providing a robust solution for\ndep"}]}