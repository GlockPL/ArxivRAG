{"title": "Long-Term Prediction Accuracy Improvement of Data-Driven Medium-Range Global Weather Forecast", "authors": ["Yifan Hu", "Fukang Yin", "Weimin Zhang", "Kaijun Ren", "Junqiang Song", "Kefeng Deng", "Di Zhang"], "abstract": "Long-term stability stands as a crucial requirement in data-driven medium- range global weather forecasting. Spectral bias is recognized as the primary contributor to instabilities, as data-driven methods difficult to learn small-scale dynamics. In this paper, we reveal that the universal mechanism for these instabilities is not only related to spectral bias but also to distortions brought by processing spherical data using conventional convolution. These distortions lead to a rapid amplification of errors over successive long-term iterations, resulting in a significant decline in forecast accuracy. To address this issue, a universal neural operator called the Spherical Harmonic Neural Operator (SHNO) is introduced to improve long-term iterative forecasts. SHNO uses the spherical harmonic basis to mitigate distortions for spherical data and uses gated residual spectral attention (GRSA) to correct spectral bias caused by spurious correlations across different scales. The effectiveness and merit of the proposed method have been validated through its application for spherical Shallow Water Equations (SWEs) and medium-range global weather forecasting. Our findings highlight the benefits and potential of SHNO to improve the accuracy of long-term prediction.", "sections": [{"title": "1 Introduction", "content": "Accurate and timely weather forecasts play an important role in many aspects of human society. In the past few years, numerical weather prediction (NWP) has been the most commonly used tool for weather forecasting (L. Chen et al., 2023; Lam et al., 2023), which simulates the future state of the atmosphere by solving the partial differential equations (PDEs) numerically (Bauer et al., 2015). Although NWP models can get accurate forecasts, they are often slow and need the support of high- performance computing systems (Bauer et al., 2015; Bi et al., 2023; L. Chen et al., 2023; Lam et al., 2023). Moreover, errors in initial conditions, approximations of physical processes in parameterizations, and the chaos of the atmosphere introduce uncertainties to NWP (Bauer et al., 2015; L. Chen et al., 2023). Recently, deep learning has revolutionized the field of weather forecasts for obtaining more timely forecasts and more accurate results. For example, Rasp and Thuerey (2021) used a deep residual convolutional neural network (CNN) known as ResNet (He et al., 2016) to do continuous forecasts at a spatial resolution of 5.625\u00b0 \u00d7 5.625\u00b0 and obtain similar performance compared to a physical baseline at a similar resolution. FourCastNet (Pathak et al., 2022) firstly improved the data-driven global weather forecast model's resolution to 0.25\u00b0 \u00d7 0.25\u00b0, but the forecasting accuracy is slightly below the most advance NWP i.e. the operational integrated forecasting system (IFS) of the European Centre for Medium-Range Weather Forecasts (ECMWF). Before long, data-driven weather forecasting system achieved new breakthroughs. For example, Pangu-Weather (Bi et al., 2023) produces stronger deterministic forecast results than the operational IFS on all tested weather variables against reanalysis data. Soon after, GraphCast (Lam et al., 2023) achieved better results than IFS on more variables and support better severe event prediction. In 2023, a vision transformer variant called FengWu (K. Chen et al., 2023) solves the medium-range forecast problem from a multi-modal and multi-task perspective, and achieves state-of-the-art for longer forecast lead times. Moreover, FuXi (L. Chen et al., 2023) published with the comparable performance to ECMWF ensemble mean (EM) in 15-day forecasts. However, conventional convolution and Transformer models ignore the fact that the data is on the sphere, which would introduce distortions. These distortions seriously affect the performance of iterative forecasts. To reduce the accumulation errors for longer effective forecasts, Pangu-Weather (Bi et al., 2023) trained the model on 4 different lead times and used a greedy hierarchical temporal aggregation strategy to minimize the number of iteration steps. Similarly, to optimize performance for both short and long lead times, Fuxi (L. Chen et al., 2023) used a cascade (Ho et al., 2022; Li et al., 2015) model architecture and fine-tuned the pre-trained models in specific 5- day forecast time windows. FengWu (K. Chen et al., 2023) proposed the replay buffer to store the predicted results from previous optimization iterations and used them as the current model's input, miming the intermediate input error during the auto-regressive inference stage. Although these methods have achieved good results, they still suffer distortions for the spherical data. To settle these distortions, Weyn et al. (2020) introduced cubed-sphere remapping to minimize the distortion on the cube faces and provide natural boundary conditions for padding in the convolution operations. Shen et al. (2021) analyzed the equivariance error in the spherical domain for neural networks theoretically and designed a spherical equivariant CNN to settle the distortions from projection and the ineffective translation equivariance. McCabe et al. (2023) used the Double Fourier Sphere (DFS) method to correct the artificial discontinuity induced by the 2D Fast Fourier Transform (FFT), leads to significantly lower errors in long-range forecasts. But DFS still introduces spatial distortions, while spherical harmonic basis would not. Spherical harmonic basis has isotropy and rotation invariance, using Spherical Harmonic Transform (SHT) to process spherical data has natural advantages. To this end, Bonev et al. (2023) introduced Spherical Fourier Neural Operators (SFNOs) based on SHT for learning operators on spherical geometries, demonstrating stable autoregressive, while retaining physically plausible dynamics. However, they limit themselves to equivariance in the continuous limit, and the power of nonlinear fitting in the frequency domain still needs to be explored. Moreover, they ignored the spectral bias (Chattopadhyay & Hassanzadeh, 2023; John Xu et al., 2020; McCabe et al., 2023; Rahaman et al., 2019) introduced by data-driven models. In this work, we introduce the Gated Residual Spectral Attention (GRSA) to effectively leverage nonlinear information in the frequency domain, and develop a general neural operator to mitigate the accumulation of errors for long-term iterations caused by distortions and spectral bias on the sphere. Specifically, we use the SHT to extract spatial features of different scales and extend the Multi-Head Self-Attention (MHSA) to spectral domain to explore the correlation among them. Then, inspired by the Laplacian matrix in graph theory and the MEGA (Ma et al., 2023) model, we design GRSA module to utilize the spectral information and correct outliers caused by the spurious correlations across different scales. Moreover, a general neural operator named Spherical Harmonic Neural Operator (SHNO) was introduced to improve the accuracy and stability of long-term iterative forecasts. Experiments for Shallow Water Equations (SWEs) solving and medium-range global weather forecasting demonstrate the effectiveness of the proposed methods. The remainder of this paper is as follows. Section 2 introduces the proposed MHSA, the GRSA and the SHNO. Section 3 presents the employed datasets and the experimental designs. Section 4 describes some universal factors that cause instability for data-driven models iterative forecasts through spherical SWEs solving and medium- range global weather forecasting and evaluates the proposed methods. Finally, Section 5 concludes this work."}, {"title": "2 Methods", "content": ""}, {"title": "2.1 Spectral Multi-Head Self-Attention", "content": "Self-attention is the core component of the Transformers (Alexey et al., 2021; Vaswani et al., 2017), and multiple heads are usually used to enhance the performance. The general form of MHSA in Vision Transformers can be written as follows (Han et al., 2023; You et al., 2023):\n$Q^m = xW_Q^m, K^m = xW_K^m, V^m = xW_V^m,$\n$O_i^m = \\sum_{j=1}^{N} \\frac{Sim(Q_i^m, K_j^m)}{\\sum_{j'=1}^{N} Sim(Q_i^m, K_{j'}^m)} V_j^m,$\nwhere $i \\in \\{1, ..., N\\}$ refers to the i-th row, $m \\in \\{1, ..., M\\}$ refers to the m-th head,\n$Q^m, K^m, V^m \\in \\mathbb{R}^{N \\times d}$ are the query, key, and value matrices obtained by linearly projecting of the input N tokens $x \\in \\mathbb{R}^{N \\times C}$, $W_Q^m, W_K^m, W_V^m \\in \\mathbb{R}^{C \\times d}$ are learnable projection matrices and $Sim(\\cdot, \\cdot)$ denotes the similarity function. And modern Transformers usually adopt Softmax function to measure the similarities, where\n$Sim(Q, V) = e^{\\frac{QK^T}{\\sqrt{d}}},$ superscript $T$ means transpose.\nTo investigate the performance of the MHSA in the spectral domain, we implement a spectral multi-head self-attention (SMHSA). Since the formula of each attention head is the same, we take a single attention head as an example and its calculation formula is given as follows:\n$Q_{cv} = zW_{Qcv}, K_{cv} = zW_{Kcv}, V_{cv} = zW_{Vcv},$\n$O_{cv} = CSoftmax(\\frac{Q_{cv}K_{cv}^H}{\\sqrt{d}}) V_{cv},$\nwhere $Q_{cv}, K_{cv}, V_{cv} \\in \\mathbb{C}^{N \\times d}$ are the query, key, and value matrices respectively, obtained by linearly projecting of the input N tokens $z = x + iy \\in \\mathbb{C}^{N \\times C}$ with $x, y \\in \\mathbb{R}^{N \\times C}$, $W_{Qcv}, W_{Kcv}, W_{Vcv} \\in \\mathbb{C}^{C \\times d}$ are learnable projection matrices, $CSoftmax = Softmax(\\mathbb{R}) + iSoftmax(\\mathbb{I})$ with $\\mathbb{R}$ represent the real part, $\\mathbb{I}$ represent the imaginary part, and $K_{cv}^H \\in \\mathbb{C}^{d \\times N}$ is conjugate transpose of $K_{cv}$."}, {"title": "2.2 Gated Residual Spectral Attention with Parametric Laplacian Matrix", "content": "Previous studies have shown, that transformer-based models may easily learn spurious correlation in the data (Enstr\u00f6m et al., 2024), and display limited robustness when the pre-training dataset is relatively small (Ghosal & Li, 2024). To address the potential spectral bias arising from spurious correlations in the SMHSA, we present a parameterized Laplacian matrix to model the interplay among different scales rather than the correlation among them.\nIn graph theory, the Laplacian matrix can be considered as the discrete analog of the Laplacian operator in multivariable calculus. It also represents the degree of difference between a vertex and its nearby vertex values. Laplacian matrix is defined as $L := D - A$, where $A \\in \\mathbb{C}^{m \\times m}$ is a weighted adjacency matrix of $X \\in \\mathbb{C}^{m \\times n}$ and $D \\in \\mathbb{C}^{m \\times m}$ is the degree matrix with $D_{ii} = \\sum_{j=1}^m A_{ij}$ and $D_{ij} = 0$ if $i \\neq j$. The adjacency matrix $A$ and Laplacian matrix $L$ is unknown when $X$ is incompletely sampled, so parameterization is required for machine learning applications (Li et al., 2021; Zhemin Li et al., 2023). And there are there properties to note: (1) The Laplacian matrix $L$ is positive semi-definite and the sum of each row equals zero (Zhemin Li et al., 2023); (2) The natural spectral information is usually piecewise smooth, so $L$ should be nearly smooth; (3) The adjacency matrix $A$ of a directed graph is usually a Hermitian matrix. In this paper, we refer to (Zhemin Li et al., 2023), learning the adjacency matrix through an MLP and the SoftMax function. The difference is that through the multiplication of the learned lower triangular matrix and its conjugate transpose, we assurance the adjacency matrix to a Hermitian matrix. Then the parameterized Laplacian matrix was calculated by the adjacency matrix, and used as the attention coefficient. Additionally, we implemented a moving average to the parameterized Laplacian matrix to combine information from current and previous layers using an adaptive weight, inspired by the MEGA (Ma et al., 2023). Furthermore, the gating mechanism in the gated attention unit (GAU) (Hua et al., 2022) is equipped to control the flow of information (Lai et al., 2019). The structure of GRAS is shown in Figure 1(b), and the formula for each attention head is as follows:\n$Y_l = X_lW_{Ycv} + b_{Ycv},$\n$X'_l = Cat(X_l, reg),$\n$B_l = SoftMax(g(X'_l)),$\n$A_l = tril(B_l)(tril(B_l))^H,$\n$L_l = \\sigma(\\alpha_l)(A_l \\cdot 1_{N \\times N} \\odot I_N - A_l) + (1 - \\sigma(\\alpha_l))L_{l-1},$\n$Z_l = (L_l \\cdot \\varphi(X_lW_{Vcv} + b_{Vcv}),$\n$Z'_l = Z_l \\odot \\varphi(X_lW_{Qcv} + b_{Qcv}),$\n$O_l = Drop(Z'_lW_{PCV} + b_{PCV}) + Y_l,$\nwhere $X_l, O_l \\in \\mathbb{C}^{N \\times C}$ means the input and output, $l = 1... L$ is the number of network layers, $W_{YCV}, W_{VCV}, W_{PCV}, W_{QCV}, b_{Ycv}, b_{Vcv}, b_{Pcv}, b_{Qcv} \\in \\mathbb{C}^{C \\times d}$ are learnable projection matrices and bias, $reg$ means the registers (Darcet et al., 2024), $Cat(\u00b7, \u00b7)$ represents concatenate, $Drop(\\cdot)$ represents remove the registers, $g(): \\mathbb{C}^{N \\times C} \\rightarrow \\mathbb{C}^{N \\times d}$ is an MLP, which aims to capture self-similarity in $X_l$, $tril(\u00b7)$ mask the matrix with the lower triangular matrix, $H$ refers to conjugate transpose, $1_{NN}$ is an $N \\times N$ matrix whose entries are all 1s, $I_N$ is the $N \\times N$ identity matrix, $\\odot$ is Hadamard product, $\\varphi$ is the smooth maximum unit (SMU) (Biswas et al., 2022), $\\sigma$ is the sigmoid function, $\\alpha_l$ is a learnable parameter, and the parameterized Laplacian matrix $L_n$ can measures the similarity between rows of $X_l$, $L_{l-1}$ is the parameterized Laplacian matrix from the previous layer."}, {"title": "2.3 Spherical Harmonic Neural Operator", "content": "To make the neural operators more stable and efficient for iterative forecasts, we construct Spherical Harmonic Neural Operator (SHNO) based on the SFNO (Bonev et al., 2023), ViT (Alexey et al., 2021) and the proposed GRSA. ViTs need to split the input data into several patches through a technique named patch embedding, since it can reduce the computational overhead and improve the adaptability. However, this common technique introduces discontinuity. In this paper, we remove the patch embedding, and use the total and the zonal wave number to reduce the computational complexity. The architecture of SHNO is shown in the Figure 1(a). As we can see, the input is raised to the higher dimension channel space through the encoder, then the norm and the SHT is used to transform the data into spectral domain. This spectral coefficient matrix implies the degree information of the spherical harmonic functions, while the self-attention mechanism is insensitive to the degree information. To this problem, we add learnable degree encoding before the GRSA, inspired by the learnable position embeddings (Alexey et al., 2021). In the GRSA, we add registers (Darcet et al., 2024) to reduce the outliers of feature maps and improve the multi-step iteration performance. The operation in the spectral domain helps GRSA to capture global information easily. And we add the efficient local attention (ELA) (Xu & Wan, 2024) after GRAS to capture local information. Then the residual connection with a learnable parameter is used, according to (Ha & Lyu, 2022), this residual connection can help to correct the error caused by the numerical truncation of SHT and ISHT. After that, non-linear function, data norm and feed-forward network (FFN) is used. Conventional FFN integrates and maps global dependencies among different feature representations through a fully connected layer, which lacks local sensitivity (Shi et al., 2024). To this end, we choose the multi-path feed-forward network (MPFFN) (Shi et al., 2024) to integrate multi-scale dependencies. Finally, we use the decoder to transform the data back to the original dimension, and get the forecasts. Let the input signal is $x \\in \\mathbb{R}^{C \\times H \\times W}$, and the output is $y \\in \\mathbb{R}^{C \\times H \\times W}$, then the formula of SHNO is:\n$z_0 = Encoder(x),$\n$z'_l = F(Norm(z_{l-1})),$\n$z_l = ELA(F^{-1}(GRSA(z'_l + E_{degree}))) + F^{-1}(z'_l)W,$\n$z_l = FFN (Norm(GELU(z'_l))) + F^{-1}(z_l),$\n$y = Decoder(Cat(z_L, x))$\nwhere the $E_{degree} \\in \\mathbb{R}^{C \\times H \\times W}$ is a learnable degree encoding, $l = 1... L$ is the number of network layers, $F$ and $F^{-1}$ represents the SHT and ISHT, $Norm$ means the instance normalization (Ulyanov et al., 2016), $GELU$ is the GELU function, and $FFN$ is MPFFN."}, {"title": "3 Data and Experiments", "content": ""}, {"title": "3.1 Spherical Shallow Water Equations", "content": "The Shallow Water Equations (SWEs) on rotating sphere are a nonlinear hyperbolic PDEs system (Bonev et al., 2023), which are derived by integrating the Navier-Stokes equations over the depth of the fluid layer when the horizontal length scale is much larger than the vertical length scale. They are formulated as follows:\n$\\begin{cases}\n\\frac{\\partial \\varsigma}{\\partial t} = - \\frac{1}{a cos\\theta} \\frac{\\partial}{\\partial \\lambda} [(\\varsigma + f)u] - \\frac{1}{a cos\\theta} \\frac{\\partial}{\\partial \\theta} [(\\varsigma + f)vcos\\theta],\\\\\n\\frac{\\partial \\delta}{\\partial t} = - \\frac{1}{a cos\\theta} \\frac{\\partial}{\\partial \\lambda} [(\\varsigma + f)v] + \\frac{1}{a cos\\theta} \\frac{\\partial}{\\partial \\theta} [(\\varsigma + f)ucos\\theta] - \\nabla^2 \\varphi - \\frac{1}{2}[(\\overline{u}^2 + \\overline{v}^2)],\\\\\n\\frac{\\partial \\varphi}{\\partial t} = - \\frac{1}{a cos\\theta} \\frac{\\partial (\\varphi u)}{\\partial \\lambda} - \\frac{1}{a cos\\theta} \\frac{\\partial (\\varphi vcos\\theta)}{\\partial \\theta} - \\varphi \\delta .\n\\end{cases}$ where $f = 2\\Omega sin\\theta$ is the Coriolis parameter with $\\Omega$ being the angular velocity of the sphere, $\\varsigma, \\delta, \\varphi, \\overline{\\varphi}, u, v, a$ are vorticity, divergence, geopotential height, mean geopotential height, the $\\lambda$- and the $\\theta$-components of the velocity vector in the spherical coordinates, and the radius of the sphere, respectively. As a simplification of fluid motion equation, SWEs are widely used in atmospheric dynamics, tidal motion, tsunami propagation and the simulation of Rossby waves and Kelvin waves. The precision in addressing the SWEs serves as a crucial criterion for evaluating the efficacy and robustness of numerical solution methods. We choose the parameters of the Earth as the parameters of the SWEs on rotating sphere, and the initial conditions of the geopotential height and velocity fields are generated by the Gaussian random fields. The parameters for initializing the geopotential height and velocity fields are set consistent with (Bonev et al., 2023). Specifically, the average value, the standard deviation of the initial layer depth, and the average value, the standard deviation of the initial velocity are $\\varphi_{avg} = 10^3g$, $\\varphi_{std} = 120g$, and 0, 0.2$\\varphi_{avg}$, respectively. After setting the PDEs parameters and initial values, we use a classical spectral solver (Giraldo, 2001) to generate the numerical solutions with a spatial resolution of 256 \u00d7 512 and timesteps of 60 seconds. We use 128 initial conditions to simulate 240 hours and remove the first 100 hours of simulation because of the spin-up problem. Then the numerical solution is resampled to 64 \u00d7 128, and the training set is constructed with the remaining 140 hours. The solutions of the previous hour are used as the input of the model, and the solutions of the current moment are used as labels. The testing set uses 32 initial conditions, with 100 simulation hours burn-in. We choose 20% of the training data as the validation data, and train the U-Net (Ronneberger et al., 2015), FourCastNet (Pathak et al., 2022), SFNO Linear (Bonev et al., 2023), SFNO Non-Linear (Bonev et al., 2023), SHNO-SMHSA and SHNO on the remaining training data for 50 epochs, respectively. The SHNO-SMHSA model was obtained by replacing the attention mechanism of the SFNO in the spectral domain with SMHSA. And the best weight was saved according to the validation data, their performance was compared on the testing data. The batch size for training is 16, and the initial learning rate is 0.001, with a cosine decay reduced to 0.00002 at the end. The loss function is the mean geometric relative norm on the sphere of each channel, which formulate is (Bonev et al., 2023):\n$\\mathcal{L}[F_{\\theta}[u_n], u_{n+1}] = \\frac{1}{3 \\cdot \\text{CE channels}} \\sum_{c=1}^{\\text{CE channels}} \\frac{\\sum_{i \\in grid} w_i|F_{\\theta}[u_n](x_i) - u_{n+1}(x_i)|^2}{\\sum_{i \\in grid} w_i|u_{n+1}(x_i)|^2},$\nwhere $F_{\\theta}[u_n]$ is the predicted by deep learning models and $u_{n+1}$ is the ground truth, $w_i$ are the products of the Jacobian $sin d\\varphi$ and the quadrature weights."}, {"title": "3.2 Data and Experiment of Global Weather Forecast", "content": "The data we use for iterative medium-range global weather forecast is WeatherBench (Rasp et al., 2020), its publicly available at https://github.com/pangeo- data/WeatherBench. WeatherBench contains regirded ERA5 (Hersbach et al., 2020) data from 1979 to 2018 at hourly temporal resolution, and has three spatial resolutions to choose from: 5.625\u00b0 (32\u00d764 grid points), 2.8125\u00b0 (64\u00d7128 grid points) and 1.40525\u00b0 (128\u00d7256 grid points). Due to the limitation of our computing resources, 5.625\u00b0 was chosen as the spatial resolution, and following previous studies (L. Chen et al., 2023; Lam et al., 2023), 6h was chosen as the minimum time resolution for the iterative forecast. We use data from 1979 to 2015 as the training set, data from 2016 as the validation set, and data from out-of-sample, i.e., data from 2017 to 2018, as the testing set. And there are 22 variables for iterative forecasts, which are 10U, 10V, T2M, U1000, V1000, Z1000, U850, V850, Z850, T850, RH850, U500, V500, Z500, T500, RH500, U250, V250, Z250, T250, T100, Z50 respectively. The abbreviations and their descriptions are shown in supporting information (Table S3). Additionally, the input to the model contains two constant fields: the land-sea mask and the orography. We use supervised training to predict a single time step on the training dataset. The loss function we choose is the latitude-weighted L2 loss, which is defined as follows:\n$\\mathcal{L}2[F_{\\theta}[u_n], u_{n+1}] = \\frac{1}{C \\times H \\times W} \\sum_{c=1}^{C} \\sum_{i=1}^{H} \\sum_{j=1}^{W} W_i (F_{\\theta}[u_n](x_{c,i,j}) - u_{n+1}(x_{c,i,j}))^2$ where $C,H,W$ are the number of channels, grid points in latitude, grid points in longitude, respectively. $F_{\\theta}[u_n](x_{c,i,j})$ and $u_{n+1}(x_{c,i,j})$ are predicted and ground truth for same variable and latitude longitude coordinates at time step of n + 1. $W_i$ is the latitude weighting factor for the latitude at the $i$th latitude index, which is calculated as follows:\n$W_i = \\frac{cos (lat (i))}{\\frac{1}{H} \\sum_{i=1}^{H} cos (lat (i))}$\nwhere cos is the cosine function. The models are developed using the PyTorch framework (Paszke et al., 2017) and utilize the training workflow provided by ClimaX (Nguyen et al., 2023). The models are trained with 100 epochs using a batch size of 80 on a single Nvidia GeForce RTX 3090 GPU. The initial learning rate is 2.0\u00d710-4, with a linear warmup schedule for 6 epochs, followed by a cosine-annealing schedule (Loshchilov & Hutter, 2017b) for 94 epochs. In addition, the AdamW (Kingma & Ba, 2014; Loshchilov & Hutter, 2017a) optimizer with parameters \u03b2\u2081 = 0.9, \u03b22 = 0.99 and weight decay of 1.0\u00d710-5, and bfloat16 floating point precision are applied for training. The model with the lowest latitude-weighted RMSE on the validation set is saved, and evaluate on the test set. The evaluation metrics latitude-weighted RMSE and ACC are calculated as follows (Rasp et al., 2020; Rasp & Thuerey, 2021):\n$RMSE = \\frac{1}{N_{forecasts}} \\sqrt{\\sum_{n=1}^{N_{forecasts}} \\frac{1}{N_{lat} N_{lon}} \\sum_{i=1}^{N_{lat}} \\sum_{j=1}^{N_{lon}} w_i(f_{n,i,j} - t_{n,i,j})^2 }$\n$ACC = \\frac{\\sum_{n,i,j} w_i f'_{n,i,j} t'_{n,i,j}}{\\sqrt{\\sum_{n,i,j} w_i f'^2_{n,i,j} \\sum_{n,i,j} w_i t'^2_{n,i,j}}}$ where $f$ is the model forecast and $t$ is the ERA5 truth, $w_i$ is the latitude weighting factor for the latitude at the $i$th latitude index, the prime ' denotes the difference to the climatology and the climatology is defined as $climatology_{i,j} = \\frac{1}{N_{time}} \\sum_t t_{i,j}$."}, {"title": "4 Results", "content": "This section will first describe some universal factors that cause instability for data- driven models solving spherical SWEs. Then, we will present a method to overcome these challenges and enhance the accuracy of long-term predictions. Finally, the main findings and the proposed method will verified by ERA5 data and popular data-driven medium-range global weather forecasting."}, {"title": "4.1 Spherical Shallow Water Equations", "content": "We begin by demonstrating experimentally that conventional convolution models cause distortions when processing spherical data. The widely used and high-performing U-Net (Ronneberger et al., 2015) and FourCastNet (Pathak et al., 2022) were selected as the baseline of conventional convolution, SFNO Linear (Bonev et al., 2023), and SFNO Non-Linear (Bonev et al., 2023) as the baseline of spherical convolution. As shown in Figure 2(a)-(d), the relative errors of geopotential height forecasted by U-Net are obvious near the poles and on the east-west boundary, even at the initial iteration. With the number of iterations increases, the errors propagate from the poles to the mid and low latitudes and gradually encompass the entire domain. This phenomenon is caused by distortions at the poles and the zero-padding at the boundaries seriously affects the accuracy. FourCastNet uses Discrete Fourier Transform (DFT) to ensure the continuity of the east-west boundary and does not need paddings, obtaining minimum relative errors for one-step prediction. However, due to the implicit flat hypothesis of DFT and the period in the meridian direction, errors initially concentrated near the poles rapidly spread to the entire domain with an increase in iteration steps (Figure 2 and Figure S1 in supporting information). To overcome this limitation, SFNO (Bonev et al., 2023) introduced SHT to the data-driven models. As shown in Figure 2(i)-(p), using SHT can ensure the continuity of the east-west boundary and reduce the distortions at the poles. Although the transformation from the Gaussian grid to the latitude and longitude grid still causes bias in the poles, it is well-limited and has small effects on other domains. And in the first iteration, the errors of SFNO Linear are smaller than those of SFNO Non-Linear. But as the number of iterations increases, SFNO Non- Linear gradually outperforms SFNO Linear. The key difference between SFNO Non- Linear and SFNO Linear is whether the non-linearity functions were used in the frequency domain. SFNO Linear attempts to maintain trivially equivariant using point- wise operations under the assumption of continuous. However, in the discrete data, this method only keeps approximately equivariant (Bonev et al., 2023), and the effect of this error is intensified with the increase of iteration steps. SFNO Non-Linear, which applies non-linearities in the frequency domain, remedied this situation (Bonev et al., 2023; Poulenard & Guibas, 2021)."}, {"title": "4.2 Medium-Range Global Weather Forecast", "content": "To explain the potential effects of data distortions and spectral bias for data-driven weather forecast models, this study uses widely noticed FourCastNet (Pathak et al., 2022), Pangu-Weather (Bi et al., 2023), and FuXi (L. Chen et al., 2023) models for iterative forecasts. Figure 5 shows the spatial distribution of forecasts relative errors for geopotential height at 500hPa pressure level with a temporal resolution of 6 hours and a spatial resolution of 0.25\u00b0 \u00d7 0.25\u00b0, and the input time is 00:00 UTC on 1 September 2018. When using FourCastNet for iterative forecast, Nan (not a number) values appear soon near the South Pole, indicating that there have been severe distortions. Although Nan values do not appear in the other two models, the maximum relative errors are also concentrated near the poles. And similarly to the results of spherical SWEs, the distortions also intensify as the number of iterations increases and gradually affect the mid and low-latitude domains. Furthermore, because we did not use the hierarchical temporal aggregation strategy (Bi et al., 2023), the relative errors of Pangu-Weather accumulated rapidly with the number of iterations. This phenomenon is not only associated with the architecture of the model but also with the forecasting strategies employed. Pangu-Weather can get accurate forecasts of specific lead times but ignores the stability of iterations. Therefore, it needs the hierarchical temporal aggregation strategy to reduce the iteration steps and control the accumulation of errors in long-term forecasts. Although the hierarchical temporal aggregation strategy leads to considerable performance gains, it suffers from temporal inconsistency (L. Chen et al., 2023). To settle this problem, FuXi presents a cascade model that balances accuracy for specific lead times with the stability of iterations. However, a single model of FuXi only guarantees stability for specific 5-day forecast times. When utilizing FuXi-Short for 7- day forecasts, as Figure 5(1) shows, the error accumulates fast. In general, none of them addresses the effects of spherical data distortion."}, {"title": "5 Conclusion", "content": "Recently"}, {"title": "Long-Term Prediction Accuracy Improvement of Data-Driven Medium-Range Global Weather Forecast", "authors": ["Yifan Hu", "Fukang Yin", "Weimin Zhang", "Kaijun Ren", "Junqiang Song", "Kefeng Deng", "Di Zhang"], "abstract": "Long-term stability stands as a crucial requirement in data-driven medium- range global weather forecasting. Spectral bias is recognized as the primary contributor to instabilities, as data-driven methods difficult to learn small-scale dynamics. In this paper, we reveal that the universal mechanism for these instabilities is not only related to spectral bias but also to distortions brought by processing spherical data using conventional convolution. These distortions lead to a rapid amplification of errors over successive long-term iterations, resulting in a significant decline in forecast accuracy. To address this issue, a universal neural operator called the Spherical Harmonic Neural Operator (SHNO) is introduced to improve long-term iterative forecasts. SHNO uses the spherical harmonic basis to mitigate distortions for spherical data and uses gated residual spectral attention (GRSA) to correct spectral bias caused by spurious correlations across different scales. The effectiveness and merit of the proposed method have been validated through its application for spherical Shallow Water Equations (SWEs) and medium-range global weather forecasting. Our findings highlight the benefits and potential of SHNO to improve the accuracy of long-term prediction.", "sections": [{"title": "1 Introduction", "content": "Accurate and timely weather forecasts play an important role in many aspects of human society. In the past few years, numerical weather prediction (NWP) has been the most commonly used tool for weather forecasting (L. Chen et al., 2023; Lam et al., 2023), which simulates the future state of the atmosphere by solving the partial differential equations (PDEs) numerically (Bauer et al., 2015). Although NWP models can get accurate forecasts, they are often slow and need the support of high- performance computing systems (Bauer et al., 2015; Bi et al., 2023; L. Chen et al., 2023; Lam et al., 2023). Moreover, errors in initial conditions, approximations of physical processes in parameterizations, and the chaos of the atmosphere introduce uncertainties to NWP (Bauer et al., 2015; L. Chen et al., 2023). Recently, deep learning has revolutionized the field of weather forecasts for obtaining more timely forecasts and more accurate results. For example, Rasp and Thuerey (2021) used a deep residual convolutional neural network (CNN) known as ResNet (He et al., 2016) to do continuous forecasts at a spatial resolution of 5.625\u00b0 \u00d7 5.625\u00b0 and obtain similar performance compared to a physical baseline at a similar resolution. FourCastNet (Pathak et al., 2022) firstly improved the data-driven global weather forecast model's resolution to 0.25\u00b0 \u00d7 0.25\u00b0, but the forecasting accuracy is slightly below the most advance NWP i.e. the operational integrated forecasting system (IFS) of the European Centre for Medium-Range Weather Forecasts (ECMWF). Before long, data-driven weather forecasting system achieved new breakthroughs. For example, Pangu-Weather (Bi et al., 2023) produces stronger deterministic forecast results than the operational IFS on all tested weather variables against reanalysis data. Soon after, GraphCast (Lam et al., 2023) achieved better results than IFS on more variables and support better severe event prediction. In 2023, a vision transformer variant called FengWu (K. Chen et al., 2023) solves the medium-range forecast problem from a multi-modal and multi-task perspective, and achieves state-of-the-art for longer forecast lead times. Moreover, FuXi (L. Chen et al., 2023) published with the comparable performance to ECMWF ensemble mean (EM) in 15-day forecasts. However, conventional convolution and Transformer models ignore the fact that the data is on the sphere, which would introduce distortions. These distortions seriously affect the performance of iterative forecasts. To reduce the accumulation errors for longer effective forecasts, Pangu-Weather (Bi et al., 2023) trained the model on 4 different lead times and used a greedy hierarchical temporal aggregation strategy to minimize the number of iteration steps. Similarly, to optimize performance for both short and long lead times, Fuxi (L. Chen et al., 2023) used a cascade (Ho et al., 2022; Li et al., 2015) model architecture and fine-tuned the pre-trained models in specific 5- day forecast time windows. FengWu (K. Chen et al., 2023) proposed the replay buffer to store the predicted results from previous optimization iterations and used them as the current model's input, miming the intermediate input error during the auto-regressive inference stage. Although these methods have achieved good results, they still suffer distortions for the spherical data. To settle these distortions, Weyn et al. (2020) introduced cubed-sphere remapping to minimize the distortion on the cube faces and provide natural boundary conditions for padding in the convolution operations. Shen et al. (2021) analyzed the equivariance error in the spherical domain for neural networks theoretically and designed a spherical equivariant CNN to settle the distortions from projection and the ineffective translation equivariance. McCabe et al. (2023) used the Double Fourier Sphere (DFS) method to correct the artificial discontinuity induced by the 2D Fast Fourier Transform (FFT), leads to significantly lower errors in long-range forecasts. But DFS still introduces spatial distortions, while spherical harmonic basis would not. Spherical harmonic basis has isotropy and rotation invariance, using Spherical Harmonic Transform (SHT) to process spherical data has natural advantages. To this end, Bonev et al. (2023) introduced Spherical Fourier Neural Operators (SFNOs) based on SHT for learning operators on spherical geometries, demonstrating stable autoregressive, while retaining physically plausible dynamics. However, they limit themselves to equivariance in the continuous limit, and the power of nonlinear fitting in the frequency domain still needs to be explored. Moreover, they ignored the spectral bias (Chattopadhyay & Hassanzadeh, 2023; John Xu et al., 2020; McCabe et al., 2023; Rahaman et al., 2019) introduced by data-driven models. In this work, we introduce the Gated Residual Spectral Attention (GRSA) to effectively leverage nonlinear information in the frequency domain, and develop a general neural operator to mitigate the accumulation of errors for long-term iterations caused by distortions and spectral bias on the sphere. Specifically, we use the SHT to extract spatial features of different scales and extend the Multi-Head Self-Attention (MHSA) to spectral domain to explore the correlation among them. Then, inspired by the Laplacian matrix in graph theory and the MEGA (Ma et al., 2023) model, we design GRSA module to utilize the spectral information and correct outliers caused by the spurious correlations across different scales. Moreover, a general neural operator named Spherical Harmonic Neural Operator (SHNO) was introduced to improve the accuracy and stability of long-term iterative forecasts. Experiments for Shallow Water Equations (SWEs) solving and medium-range global weather forecasting demonstrate the effectiveness of the proposed methods. The remainder of this paper is as follows. Section 2 introduces the proposed MHSA, the GRSA and the SHNO. Section 3 presents the employed datasets and the experimental designs. Section 4 describes some universal factors that cause instability for data-driven models iterative forecasts through spherical SWEs solving and medium- range global weather forecasting and evaluates the proposed methods. Finally, Section 5 concludes this work."}, {"title": "2 Methods", "content": ""}, {"title": "2.1 Spectral Multi-Head Self-Attention", "content": "Self-attention is the core component of the Transformers (Alexey et al., 2021; Vaswani et al., 2017), and multiple heads are usually used to enhance the performance. The general form of MHSA in Vision Transformers can be written as follows (Han et al., 2023; You et al., 2023):\n$Q^m = xW_Q^m, K^m = xW_K^m, V^m = xW_V^m,$\n$O_i^m = \\sum_{j=1}^{N} \\frac{Sim(Q_i^m, K_j^m)}{\\sum_{j'=1}^{N} Sim(Q_i^m, K_{j'}^m)} V_j^m,$\nwhere $i \\in \\{1, ..., N\\}$ refers to the i-th row, $m \\in \\{1, ..., M\\}$ refers to the m-th head,\n$Q^m, K^m, V^m \\in \\mathbb{R}^{N \\times d}$ are the query, key, and value matrices obtained by linearly projecting of the input N tokens $x \\in \\mathbb{R}^{N \\times C}$, $W_Q^m, W_K^m, W_V^m \\in \\mathbb{R}^{C \\times d}$ are learnable projection matrices and $Sim(\\cdot, \\cdot)$ denotes the similarity function. And modern Transformers usually adopt Softmax function to measure the similarities, where\n$Sim(Q, V) = e^{\\frac{QK^T}{\\sqrt{d}}},$ superscript $T$ means transpose.\nTo investigate the performance of the MHSA in the spectral domain, we implement a spectral multi-head self-attention (SMHSA). Since the formula of each attention head is the same, we take a single attention head as an example and its calculation formula is given as follows:\n$Q_{cv} = zW_{Qcv}, K_{cv} = zW_{Kcv}, V_{cv} = zW_{Vcv},$\n$O_{cv} = CSoftmax(\\frac{Q_{cv}K_{cv}^H}{\\sqrt{d}}) V_{cv},$\nwhere $Q_{cv}, K_{cv}, V_{cv} \\in \\mathbb{C}^{N \\times d}$ are the query, key, and value matrices respectively, obtained by linearly projecting of the input N tokens $z = x + iy \\in \\mathbb{C}^{N \\times C}$ with $x, y \\in \\mathbb{R}^{N \\times C}$, $W_{Qcv}, W_{Kcv}, W_{Vcv} \\in \\mathbb{C}^{C \\times d}$ are learnable projection matrices, $CSoftmax = Softmax(\\mathbb{R}) + iSoftmax(\\mathbb{I})$ with $\\mathbb{R}$ represent the real part, $\\mathbb{I}$ represent the imaginary part, and $K_{cv}^H \\in \\mathbb{C}^{d \\times N}$ is conjugate transpose of $K_{cv}$."}, {"title": "2.2 Gated Residual Spectral Attention with Parametric Laplacian Matrix", "content": "Previous studies have shown, that transformer-based models may easily learn spurious correlation in the data (Enstr\u00f6m et al., 2024), and display limited robustness when the pre-training dataset is relatively small (Ghosal & Li, 2024). To address the potential spectral bias arising from spurious correlations in the SMHSA, we present a parameterized Laplacian matrix to model the interplay among different scales rather than the correlation among them.\nIn graph theory, the Laplacian matrix can be considered as the discrete analog of the Laplacian operator in multivariable calculus. It also represents the degree of difference between a vertex and its nearby vertex values. Laplacian matrix is defined as $L := D - A$, where $A \\in \\mathbb{C}^{m \\times m}$ is a weighted adjacency matrix of $X \\in \\mathbb{C}^{m \\times n}$ and $D \\in \\mathbb{C}^{m \\times m}$ is the degree matrix with $D_{ii} = \\sum_{j=1}^m A_{ij}$ and $D_{ij} = 0$ if $i \\neq j$. The adjacency matrix $A$ and Laplacian matrix $L$ is unknown when $X$ is incompletely sampled, so parameterization is required for machine learning applications (Li et al., 2021; Zhemin Li et al., 2023). And there are there properties to note: (1) The Laplacian matrix $L$ is positive semi-definite and the sum of each row equals zero (Zhemin Li et al., 2023); (2) The natural spectral information is usually piecewise smooth, so $L$ should be nearly smooth; (3) The adjacency matrix $A$ of a directed graph is usually a Hermitian matrix. In this paper, we refer to (Zhemin Li et al., 2023), learning the adjacency matrix through an MLP and the SoftMax function. The difference is that through the multiplication of the learned lower triangular matrix and its conjugate transpose, we assurance the adjacency matrix to a Hermitian matrix. Then the parameterized Laplacian matrix was calculated by the adjacency matrix, and used as the attention coefficient. Additionally, we implemented a moving average to the parameterized Laplacian matrix to combine information from current and previous layers using an adaptive weight, inspired by the MEGA (Ma et al., 2023). Furthermore, the gating mechanism in the gated attention unit (GAU) (Hua et al., 2022) is equipped to control the flow of information (Lai et al., 2019). The structure of GRAS is shown in Figure 1(b), and the formula for each attention head is as follows:\n$Y_l = X_lW_{Ycv} + b_{Ycv},$\n$X'_l = Cat(X_l, reg),$\n$B_l = SoftMax(g(X'_l)),$\n$A_l = tril(B_l)(tril(B_l))^H,$\n$L_l = \\sigma(\\alpha_l)(A_l \\cdot 1_{N \\times N} \\odot I_N - A_l) + (1 - \\sigma(\\alpha_l))L_{l-1},$\n$Z_l = (L_l \\cdot \\varphi(X_lW_{Vcv} + b_{Vcv}),$\n$Z'_l = Z_l \\odot \\varphi(X_lW_{Qcv} + b_{Qcv}),$\n$O_l = Drop(Z'_lW_{PCV} + b_{PCV}) + Y_l,$\nwhere $X_l, O_l \\in \\mathbb{C}^{N \\times C}$ means the input and output, $l = 1... L$ is the number of network layers, $W_{YCV}, W_{VCV}, W_{PCV}, W_{QCV}, b_{Ycv}, b_{Vcv}, b_{Pcv}, b_{Qcv} \\in \\mathbb{C}^{C \\times d}$ are learnable projection matrices and bias, $reg$ means the registers (Darcet et al., 2024), $Cat(\u00b7, \u00b7)$ represents concatenate, $Drop(\\cdot)$ represents remove the registers, $g(): \\mathbb{C}^{N \\times C} \\rightarrow \\mathbb{C}^{N \\times d}$ is an MLP, which aims to capture self-similarity in $X_l$, $tril(\u00b7)$ mask the matrix with the lower triangular matrix, $H$ refers to conjugate transpose, $1_{NN}$ is an $N \\times N$ matrix whose entries are all 1s, $I_N$ is the $N \\times N$ identity matrix, $\\odot$ is Hadamard product, $\\varphi$ is the smooth maximum unit (SMU) (Biswas et al., 2022), $\\sigma$ is the sigmoid function, $\\alpha_l$ is a learnable parameter, and the parameterized Laplacian matrix $L_n$ can measures the similarity between rows of $X_l$, $L_{l-1}$ is the parameterized Laplacian matrix from the previous layer."}, {"title": "2.3 Spherical Harmonic Neural Operator", "content": "To make the neural operators more stable and efficient for iterative forecasts, we construct Spherical Harmonic Neural Operator (SHNO) based on the SFNO (Bonev et al., 2023), ViT (Alexey et al., 2021) and the proposed GRSA. ViTs need to split the input data into several patches through a technique named patch embedding, since it can reduce the computational overhead and improve the adaptability. However, this common technique introduces discontinuity. In this paper, we remove the patch embedding, and use the total and the zonal wave number to reduce the computational complexity. The architecture of SHNO is shown in the Figure 1(a). As we can see, the input is raised to the higher dimension channel space through the encoder, then the norm and the SHT is used to transform the data into spectral domain. This spectral coefficient matrix implies the degree information of the spherical harmonic functions, while the self-attention mechanism is insensitive to the degree information. To this problem, we add learnable degree encoding before the GRSA, inspired by the learnable position embeddings (Alexey et al., 2021). In the GRSA, we add registers (Darcet et al., 2024) to reduce the outliers of feature maps and improve the multi-step iteration performance. The operation in the spectral domain helps GRSA to capture global information easily. And we add the efficient local attention (ELA) (Xu & Wan, 2024) after GRAS to capture local information. Then the residual connection with a learnable parameter is used, according to (Ha & Lyu, 2022), this residual connection can help to correct the error caused by the numerical truncation of SHT and ISHT. After that, non-linear function, data norm and feed-forward network (FFN) is used. Conventional FFN integrates and maps global dependencies among different feature representations through a fully connected layer, which lacks local sensitivity (Shi et al., 2024). To this end, we choose the multi-path feed-forward network (MPFFN) (Shi et al., 2024) to integrate multi-scale dependencies. Finally, we use the decoder to transform the data back to the original dimension, and get the forecasts. Let the input signal is $x \\in \\mathbb{R}^{C \\times H \\times W}$, and the output is $y \\in \\mathbb{R}^{C \\times H \\times W}$, then the formula of SHNO is:\n$z_0 = Encoder(x),$\n$z'_l = F(Norm(z_{l-1})),$\n$z_l = ELA(F^{-1}(GRSA(z'_l + E_{degree}))) + F^{-1}(z'_l)W,$\n$z_l = FFN (Norm(GELU(z'_l))) + F^{-1}(z_l),$\n$y = Decoder(Cat(z_L, x))$\nwhere the $E_{degree} \\in \\mathbb{R}^{C \\times H \\times W}$ is a learnable degree encoding, $l = 1... L$ is the number of network layers, $F$ and $F^{-1}$ represents the SHT and ISHT, $Norm$ means the instance normalization (Ulyanov et al., 2016), $GELU$ is the GELU function, and $FFN$ is MPFFN."}, {"title": "3 Data and Experiments", "content": ""}, {"title": "3.1 Spherical Shallow Water Equations", "content": "The Shallow Water Equations (SWEs) on rotating sphere are a nonlinear hyperbolic PDEs system (Bonev et al., 2023), which are derived by integrating the Navier-Stokes equations over the depth of the fluid layer when the horizontal length scale is much larger than the vertical length scale. They are formulated as follows:\n$\\begin{cases}\n\\frac{\\partial \\varsigma}{\\partial t} = - \\frac{1}{a cos\\theta} \\frac{\\partial}{\\partial \\lambda} [(\\varsigma + f)u] - \\frac{1}{a cos\\theta} \\frac{\\partial}{\\partial \\theta} [(\\varsigma + f)vcos\\theta],\\\\\n\\frac{\\partial \\delta}{\\partial t} = - \\frac{1}{a cos\\theta} \\frac{\\partial}{\\partial \\lambda} [(\\varsigma + f)v] + \\frac{1}{a cos\\theta} \\frac{\\partial}{\\partial \\theta} [(\\varsigma + f)ucos\\theta] - \\nabla^2 \\varphi - \\frac{1}{2}[(\\overline{u}^2 + \\overline{v}^2)],\\\\\n\\frac{\\partial \\varphi}{\\partial t} = - \\frac{1}{a cos\\theta} \\frac{\\partial (\\varphi u)}{\\partial \\lambda} - \\frac{1}{a cos\\theta} \\frac{\\partial (\\varphi vcos\\theta)}{\\partial \\theta} - \\varphi \\delta .\n\\end{cases}$ where $f = 2\\Omega sin\\theta$ is the Coriolis parameter with $\\Omega$ being the angular velocity of the sphere, $\\varsigma, \\delta, \\varphi, \\overline{\\varphi}, u, v, a$ are vorticity, divergence, geopotential height, mean geopotential height, the $\\lambda$- and the $\\theta$-components of the velocity vector in the spherical coordinates, and the radius of the sphere, respectively. As a simplification of fluid motion equation, SWEs are widely used in atmospheric dynamics, tidal motion, tsunami propagation and the simulation of Rossby waves and Kelvin waves. The precision in addressing the SWEs serves as a crucial criterion for evaluating the efficacy and robustness of numerical solution methods. We choose the parameters of the Earth as the parameters of the SWEs on rotating sphere, and the initial conditions of the geopotential height and velocity fields are generated by the Gaussian random fields. The parameters for initializing the geopotential height and velocity fields are set consistent with (Bonev et al., 2023). Specifically, the average value, the standard deviation of the initial layer depth, and the average value, the standard deviation of the initial velocity are $\\varphi_{avg} = 10^3g$, $\\varphi_{std} = 120g$, and 0, 0.2$\\varphi_{avg}$, respectively. After setting the PDEs parameters and initial values, we use a classical spectral solver (Giraldo, 2001) to generate the numerical solutions with a spatial resolution of 256 \u00d7 512 and timesteps of 60 seconds. We use 128 initial conditions to simulate 240 hours and remove the first 100 hours of simulation because of the spin-up problem. Then the numerical solution is resampled to 64 \u00d7 128, and the training set is constructed with the remaining 140 hours. The solutions of the previous hour are used as the input of the model, and the solutions of the current moment are used as labels. The testing set uses 32 initial conditions, with 100 simulation hours burn-in. We choose 20% of the training data as the validation data, and train the U-Net (Ronneberger et al., 2015), FourCastNet (Pathak et al., 2022), SFNO Linear (Bonev et al., 2023), SFNO Non-Linear (Bonev et al., 2023), SHNO-SMHSA and SHNO on the remaining training data for 50 epochs, respectively. The SHNO-SMHSA model was obtained by replacing the attention mechanism of the SFNO in the spectral domain with SMHSA. And the best weight was saved according to the validation data, their performance was compared on the testing data. The batch size for training is 16, and the initial learning rate is 0.001, with a cosine decay reduced to 0.00002 at the end. The loss function is the mean geometric relative norm on the sphere of each channel, which formulate is (Bonev et al., 2023):\n$\\mathcal{L}[F_{\\theta}[u_n], u_{n+1}] = \\frac{1}{3 \\cdot \\text{CE channels}} \\sum_{c=1}^{\\text{CE channels}} \\frac{\\sum_{i \\in grid} w_i|F_{\\theta}[u_n](x_i) - u_{n+1}(x_i)|^2}{\\sum_{i \\in grid} w_i|u_{n+1}(x_i)|^2},$\nwhere $F_{\\theta}[u_n]$ is the predicted by deep learning models and $u_{n+1}$ is the ground truth, $w_i$ are the products of the Jacobian $sin d\\varphi$ and the quadrature weights."}, {"title": "3.2 Data and Experiment of Global Weather Forecast", "content": "The data we use for iterative medium-range global weather forecast is WeatherBench (Rasp et al., 2020), its publicly available at https://github.com/pangeo- data/WeatherBench. WeatherBench contains regirded ERA5 (Hersbach et al., 2020) data from 1979 to 2018 at hourly temporal resolution, and has three spatial resolutions to choose from: 5.625\u00b0 (32\u00d764 grid points), 2.8125\u00b0 (64\u00d7128 grid points) and 1.40525\u00b0 (128\u00d7256 grid points). Due to the limitation of our computing resources, 5.625\u00b0 was chosen as the spatial resolution, and following previous studies (L. Chen et al., 2023; Lam et al., 2023), 6h was chosen as the minimum time resolution for the iterative forecast. We use data from 1979 to 2015 as the training set, data from 2016 as the validation set, and data from out-of-sample, i.e., data from 2017 to 2018, as the testing set. And there are 22 variables for iterative forecasts, which are 10U, 10V, T2M, U1000, V1000, Z1000, U850, V850, Z850, T850, RH850, U500, V500, Z500, T500, RH500, U250, V250, Z250, T250, T100, Z50 respectively. The abbreviations and their descriptions are shown in supporting information (Table S3). Additionally, the input to the model contains two constant fields: the land-sea mask and the orography. We use supervised training to predict a single time step on the training dataset. The loss function we choose is the latitude-weighted L2 loss, which is defined as follows:\n$\\mathcal{L}2[F_{\\theta}[u_n], u_{n+1}] = \\frac{1}{C \\times H \\times W} \\sum_{c=1}^{C} \\sum_{i=1}^{H} \\sum_{j=1}^{W} W_i (F_{\\theta}[u_n](x_{c,i,j}) - u_{n+1}(x_{c,i,j}))^2$ where $C,H,W$ are the number of channels, grid points in latitude, grid points in longitude, respectively. $F_{\\theta}[u_n](x_{c,i,j})$ and $u_{n+1}(x_{c,i,j})$ are predicted and ground truth for same variable and latitude longitude coordinates at time step of n + 1. $W_i$ is the latitude weighting factor for the latitude at the $i$th latitude index, which is calculated as follows:\n$W_i = \\frac{cos (lat (i))}{\\frac{1}{H} \\sum_{i=1}^{H} cos (lat (i))}$\nwhere cos is the cosine function. The models are developed using the PyTorch framework (Paszke et al., 2017) and utilize the training workflow provided by ClimaX (Nguyen et al., 2023). The models are trained with 100 epochs using a batch size of 80 on a single Nvidia GeForce RTX 3090 GPU. The initial learning rate is 2.0\u00d710-4, with a linear warmup schedule for 6 epochs, followed by a cosine-annealing schedule (Loshchilov & Hutter, 2017b) for 94 epochs. In addition, the AdamW (Kingma & Ba, 2014; Loshchilov & Hutter, 2017a) optimizer with parameters \u03b2\u2081 = 0.9, \u03b22 = 0.99 and weight decay of 1.0\u00d710-5, and bfloat16 floating point precision are applied for training. The model with the lowest latitude-weighted RMSE on the validation set is saved, and evaluate on the test set. The evaluation metrics latitude-weighted RMSE and ACC are calculated as follows (Rasp et al., 2020; Rasp & Thuerey, 2021):\n$RMSE = \\frac{1}{N_{forecasts}} \\sqrt{\\sum_{n=1}^{N_{forecasts}} \\frac{1}{N_{lat} N_{lon}} \\sum_{i=1}^{N_{lat}} \\sum_{j=1}^{N_{lon}} w_i(f_{n,i,j} - t_{n,i,j})^2 }$\n$ACC = \\frac{\\sum_{n,i,j} w_i f'_{n,i,j} t'_{n,i,j}}{\\sqrt{\\sum_{n,i,j} w_i f'^2_{n,i,j} \\sum_{n,i,j} w_i t'^2_{n,i,j}}}$ where $f$ is the model forecast and $t$ is the ERA5 truth, $w_i$ is the latitude weighting factor for the latitude at the $i$th latitude index, the prime ' denotes the difference to the climatology and the climatology is defined as $climatology_{i,j} = \\frac{1}{N_{time}} \\sum_t t_{i,j}$."}, {"title": "4 Results", "content": "This section will first describe some universal factors that cause instability for data- driven models solving spherical SWEs. Then, we will present a method to overcome these challenges and enhance the accuracy of long-term predictions. Finally, the main findings and the proposed method will verified by ERA5 data and popular data-driven medium-range global weather forecasting."}, {"title": "4.1 Spherical Shallow Water Equations", "content": "We begin by demonstrating experimentally that conventional convolution models cause distortions when processing spherical data. The widely used and high-performing U-Net (Ronneberger et al., 2015) and FourCastNet (Pathak et al., 2022) were selected as the baseline of conventional convolution, SFNO Linear (Bonev et al., 2023), and SFNO Non-Linear (Bonev et al., 2023) as the baseline of spherical convolution. As shown in Figure 2(a)-(d), the relative errors of geopotential height forecasted by U-Net are obvious near the poles and on the east-west boundary, even at the initial iteration. With the number of iterations increases, the errors propagate from the poles to the mid and low latitudes and gradually encompass the entire domain. This phenomenon is caused by distortions at the poles and the zero-padding at the boundaries seriously affects the accuracy. FourCastNet uses Discrete Fourier Transform (DFT) to ensure the continuity of the east-west boundary and does not need paddings, obtaining minimum relative errors for one-step prediction. However, due to the implicit flat hypothesis of DFT and the period in the meridian direction, errors initially concentrated near the poles rapidly spread to the entire domain with an increase in iteration steps (Figure 2 and Figure S1 in supporting information). To overcome this limitation, SFNO (Bonev et al., 2023) introduced SHT to the data-driven models. As shown in Figure 2(i)-(p), using SHT can ensure the continuity of the east-west boundary and reduce the distortions at the poles. Although the transformation from the Gaussian grid to the latitude and longitude grid still causes bias in the poles, it is well-limited and has small effects on other domains. And in the first iteration, the errors of SFNO Linear are smaller than those of SFNO Non-Linear. But as the number of iterations increases, SFNO Non- Linear gradually outperforms SFNO Linear. The key difference between SFNO Non- Linear and SFNO Linear is whether the non-linearity functions were used in the frequency domain. SFNO Linear attempts to maintain trivially equivariant using point- wise operations under the assumption of continuous. However, in the discrete data, this method only keeps approximately equivariant (Bonev et al., 2023), and the effect of this error is intensified with the increase of iteration steps. SFNO Non-Linear, which applies non-linearities in the frequency domain, remedied this situation (Bonev et al., 2023; Poulenard & Guibas, 2021)."}, {"title": "4.2 Medium-Range Global Weather Forecast", "content": "To explain the potential effects of data distortions and spectral bias for data-driven weather forecast models, this study uses widely noticed FourCastNet (Pathak et al., 2022), Pangu-Weather (Bi et al., 2023), and FuXi (L. Chen et al., 2023) models for iterative forecasts. Figure 5 shows the spatial distribution of forecasts relative errors for geopotential height at 500hPa pressure level with a temporal resolution of 6 hours and a spatial resolution of 0.25\u00b0 \u00d7 0.25\u00b0, and the input time is 00:00 UTC on 1 September 2018. When using FourCastNet for iterative forecast, Nan (not a number) values appear soon near the South Pole, indicating that there have been severe distortions. Although Nan values do not appear in the other two models, the maximum relative errors are also concentrated near the poles. And similarly to the results of spherical SWEs, the distortions also intensify as the number of iterations increases and gradually affect the mid and low-latitude domains. Furthermore, because we did not use the hierarchical temporal aggregation strategy (Bi et al., 2023), the relative errors of Pangu-Weather accumulated rapidly with the number of iterations. This phenomenon is not only associated with the architecture of the model but also with the forecasting strategies employed. Pangu-Weather can get accurate forecasts of specific lead times but ignores the stability of iterations. Therefore, it needs the hierarchical temporal aggregation strategy to reduce the iteration steps and control the accumulation of errors in long-term forecasts. Although the hierarchical temporal aggregation strategy leads to considerable performance gains, it suffers from temporal inconsistency (L. Chen et al., 2023). To settle this problem, FuXi presents a cascade model that balances accuracy for specific lead times with the stability of iterations. However, a single model of FuXi only guarantees stability for specific 5-day forecast times. When utilizing FuXi-Short for 7- day forecasts, as Figure 5(1) shows, the error accumulates fast. In general, none of them addresses the effects of spherical data distortion."}, {"title": "5 Conclusion", "content": "Recently, data-driven weather forecasting systems (Bi et al., 2023; K. Chen et al., 2023; L. Chen et al., 2023; Lam et al., 2023) have shown stronger deterministic forecast results than the ECMWF's IFS (Bougeault et al., 2010). Nevertheless, information loss in small-scale of conventional neural networks causes spectral bias, leading to long- term iteration instability. We found that the universal mechanism for these instabilities is not only related to spectral bias but also to distortions brought by processing"}]}]}