{"title": "CoVoSwitch: Machine Translation of Synthetic\nCode-Switched Text Based on Intonation Units", "authors": ["Yeeun Kang"], "abstract": "Multilingual code-switching research is often\nhindered by the lack and linguistically biased\nstatus of available datasets. To expand language\nrepresentation, we synthesize code-switching\ndata by replacing intonation units detected\nthrough PSST, a speech segmentation model\nfine-tuned from OpenAI's Whisper, using a\nspeech-to-text translation dataset, CoVoST 2.\nWith our dataset, CoVoSwitch, spanning 13 lan-\nguages, we evaluate the code-switching trans-\nlation performance of two multilingual transla-\ntion models, M2M-100 418M and NLLB-200\n600M. We reveal that the inclusion of code-\nswitching units results in higher translation per-\nformance than monolingual settings and that\nmodels are better at code-switching transla-\ntion into English than non-English. Further,\nlow-resource languages gain most from inte-\ngration of code-switched units when translat-\ning into English but much less when translat-\ning into non-English. Translations into low-\nresource languages also perform worse than\neven raw code-switched inputs. We find that\nsystems excel at copying English tokens but\nstruggle with non-English tokens, that the off-\ntarget problem in monolingual settings is also\nrelevant in code-switching settings, and that\nmodels hallucinate in code-switching transla-\ntion by introducing words absent in both of\nthe original source sentences. CoVoSwitch and\ncode are available at https://github.com/\nsophiayk20/covoswitch.", "sections": [{"title": "Introduction", "content": "Code-switching (CSW), otherwise known as code-\nmixing, refers to the use of linguistic units from\nmultiple languages in a conversation or utterance\n(Pratapa et al., 2018). In general, researching code-\nswitching comprehensively is a complicated task\ndue to the lack of code-switched data. One so-\nlution is to use existing code-switching datasets\n(Weller et al., 2022; Nguyen et al., 2023), but\nthere is a limited number of such datasets and us-\ning them constrains research to the few language\npairs that datasets are concentrated in, such as\nSpanish-English or Hindi-English (Winata et al.,\n2023). To alleviate the problem, previous work\n(Alastruey et al., 2023) brought together multiple\ndatasets, such as Fisher (Cieri et al., 2004) and Ban-\ngor Miami (Deuchar et al., 2014). Nevertheless,\nin the multilingual setting, collecting data from\nmultiple sources mixes different degrees of code-\nswitching and blocks parallel understanding across\nlanguages.\nAlternatively, most works have introduced syn-\nthetic datasets (Winata et al., 2023). These have\nbeen based on linguistic theories, such as the\nMatrix Language Frame (MLF) Model (Myers-\nScotton, 1997) and the Equivalence Constraint\n(Poplack, 1980). Applying the Equivalence Con-\nstraint requires the use of constituency parsers.\n(Rizvi et al., 2021) utilized the Stanford Parser\n(Klein and Manning, 2003) and the Berkeley Neu-\nral Parser (Kitaev and Klein, 2018; Kitaev et al.,\n2019). However, as of now, the Stanford Parser sup-\nports Arabic, Chinese, English, French, German,\nand Spanish, while the Berkeley Neural Parser sup-\nports Arabic, Basque, English, French, German,\nHebrew, Hungarian, Korean, Polish, and Swedish.\nThis presents a bottleneck in the number of lan-\nguages that can be used for research and impedes\nthe creation of code-switching data for unsupported\nor low-resource languages such as Tamil.\nSynthetic datasets have also introduced code-\nswitching mainly based on words. These include\nrandom replacements based on words (Rijhwani\net al., 2017; Xu and Yvon, 2021; Rizvi et al., 2021;\nTarunesh et al., 2021) and replacements based on\nconnected components of aligned words (Iyer et al.,\n2023). However, word-based switching may not\ncompletely reflect the code-switching phenomenon.\nRecent research (Pattichis et al., 2023) demon-"}, {"title": "Synthetic Data Generation", "content": "strated that code-switching is more common across\nintonation units than within as a result of looser syn-\ntactic relationships and that intonation units should\ntherefore serve as new replacement units instead of\nwords. This constraint is referred to as the Intona-\ntion Unit Boundary Constraint.\nTo expand language representation, experiment\nwith intonation units as basis units of code-\nswitching, and reflect both linguistic and prosodic\nconstraints, we synthesize data by following the\nMatrix Language Frame Model and the Intonation\nUnit Boundary Constraint. We keep English as the\nmatrix language and embed segments from non-\nEnglish languages by replacing English intonation\nunits of utterances from CoVOST 2 (Wang et al.,\n2021), a speech-to-text translation (S2TT) dataset,\ndetected with PSST (Roll et al., 2023), an English\nprosodic speech segmentation model fine-tuned\nfrom OpenAI's speech recognition model Whis-\nper (Radford et al., 2023). Utilizing S2TT datasets\nis advantageous for several reasons. First, they\ninclude transcripts for both languages and audio\nfiles for one language in each pair, which allows\nthe simultaneous incorporation of text and speech\nfeatures in code-switching data creation. Moreover,\nrecent datasets cover a multitude of high-resource\nand low-resource languages, which enables the in-\nclusion of diverse language pairs for synthetic code-\nswitching data.\nMeanwhile, we observe that while recent works\n(Zhang et al., 2023; Khatri et al., 2023) have demon-\nstrated the translation performance of multilingual\nlarge language models with billions of parameters\nsuch as XGLM-7.5B and BLOOMZ-7b1 on code-\nswitching data, performance of multilingual neural\nmachine translation (MNMT) models with millions\nof parameters remains relatively underexplored.\nWe therefore measure the zero-shot code-switching\ntranslation performance of M2M-100 418M (Fan\net al., 2021) and NLLB-200 600M (Costa-juss\u00e0\net al., 2022), capable of multilingual translation for\n100 and 200 languages respectively, on our syn-\nthetic dataset.\nOur contributions are summarized as follows:\nWe (1) apply a single synthetic data generation\nmethod to different language pairs, including low-\nresource languages such as Tamil, based on a sin-\ngle dataset and thereby eliminate differences that\nemerge from the discrepancies in data generation\nmethodology, (2) release a new code-switching\ndataset, CoVoSwitch, with similar code-switching\nlevels across 13 languages, and (3) compare trans-"}, {"title": "Intonation Unit Detection", "content": "We use the En\u2192X subset of the CoVOST 2 dataset,\nas this subset contains English recordings that we\nuse to detect English prosodic boundaries. For non-\nEnglish languages, we select Arabic (ar), Catalan\n(ca), Welsh (cy), German (de), Estonian (et), Per-\nsian (fa), Indonesian (id), Latvian (lv), Mongolian\n(mn), Slovenian (sl), Swedish (sv), Tamil (ta), and\nTurkish (tr). We follow the classification scheme of\n(Costa-juss\u00e0 et al., 2022) and denote Welsh, Mon-\ngolian, and Tamil as low-resource and others as\nhigh-resource. To match units of measurement for\nmetrics such as CMI and SPF detailed later in this\nstudy, we exclude Chinese and Japanese, which are\nnot whitespace separated. Further information on\nlanguages covered is contained in Appendix A.1.\nUsing the PSST model\u00b2 (Roll et al., 2023) fine-\ntuned from OpenAI's Whisper\u00b3 (Radford et al.,\n2023), we both generate transcriptions and detect\nintonation unit (IU) boundaries for English utter-"}, {"title": "Alignment Extraction and Intonation Unit\nReplacement", "content": "We obtain word alignments between English\nand non-English text from CoVoST 2 using an\naligner following previous research (Rizvi et al.,\n2021; Winata et al., 2019; Pratapa et al., 2018),\nbut replace fast_align (Dyer et al., 2013), a\nreparametrization of IBM Model 2, with a neural\naligner, awesome-align\u2074 (Dou and Neubig, 2021),\nbecause it outperforms fast_align in alignment\nerror rate. This aligner supports all target languages\ncovered in this work as it is a fine-tuned aligner\nfrom mBERT (Devlin et al., 2019).\nWe pick the number of intonation units to re-\nplace, r, from 1 to number of English intonation\nunits - 1 for each English sentence. For each r,\nwe randomly select a combination of r intonation\nunit indices, but nonconsecutive IU indices, if they\nexist, are prioritized over consecutive ones to rep-\nresent more active code-switching. For each of the\ntokens in each replacement intonation unit selected,\nwe find corresponding non-English tokens using\nword alignments. When replacing English tokens\nwith non-English tokens, we preserve the original\norder in non-English languages. If no tokens are\nmapped by the aligner, empty strings are appended\nto the code-switched text, following previous work\n(Pratapa et al., 2018). For tokens that are not in the\nintonation units selected for replacement, English"}, {"title": "Dataset Evaluation and Analysis", "content": "To evaluate our synthetic dataset, we report two\nautomatic metrics, Code Mixing Index (CMI) and\nSwitch Point Fraction (SPF). These metrics can be\ncomputed at either the utterance or corpus level, but\nwe report at the corpus level to facilitate parallel\nunderstanding across languages.\nCMI, first proposed by (Das and Gamb\u00e4ck,\n2014), measures the level of code-switching in a\ntext. We follow the definition of (Mondal et al.,\n2022) and report CMI as follows. For a code-\nswitching sentence comprised of \u03b7 tokens, with\n71 and 72 tokens in each language and \u03b7 = 71 + 72,\nCMI is defined as 1 - max(71,72)/\u03b7. We adhere to pre-\nvious convention and multiply this number by 100.\nSPF was proposed by (Pratapa et al., 2018) and\nmeasures the rate at which code-switching points\n\u03a3\u03b7\u22122i=0 S(i,i+1)\noccur in the code-switched text. SPF is defined\nas \u03b7\u22121 , where S(i, i + 1) is an indicator\nvariable that is equal to 1 if the tokens of indices i\nand i + 1 belong to different languages and else 0.\nTable 3 captures information relevant to the test\nsubset of our synthesized dataset, which is the only\nsubset that we utilize in the experiments that follow."}, {"title": "Machine Translation Experimental\nSetup", "content": "The total number of sentences generated is roughly\n1.5 times the number of correct transcripts used in\nTable 1, which is related to the average number of\nintonation units outlined in Table 2. CMI values\nrange from 32.76 to 33.71, which is comparable to\nCMI levels of 31.00 in (Pratapa et al., 2018). SPF\nvalues range from 0.15 to 0.18, which is compara-\nble to SPF values of 0.17 and 0.2 in (Winata et al.,\n2019). Because our dataset is created by replacing\nentire intonation units instead of words as in previ-\nous works, it contains longer same language spans\nand less switch points, resulting in relatively higher\nCMI values and lower SPF values. In our dataset,\nroughly half of the tokens come from each con-\nstituent language. Statistics on train and validation\nsubsets are included in Appendix A.2.\nModels. We use the HuggingFace pre-trained\nmodel checkpoints facebook/m2m100_418M and\nfacebook/nllb-200-distilled-600M for the\nM2M-100 418M and NLLB-200 600M models.\nThese two models were chosen for their excep-\ntional multilingual capabilities, with M2M-100\nintended for non-English centric translation and\nNLLB-200 designed to improve translation perfor-\nmance in low-resource languages. Both support all\nlanguages covered by our synthetic dataset.\nTranslation Settings. We experiment with four\ntranslation settings for each of the English and non-\nEnglish language pairs. First is csw\u2192En, in which\ncode-switched text is translated into English. This\nsetting was examined in previous research (Nguyen\net al., 2023; Xu and Yvon, 2021), but we also ex-\nperiment with csw\u2192X to analyze any performance\ngaps that may arise by setting target language for\ntranslation differently. We compare these two code-\nswitching translation settings to two monolingual\ntranslation settings, X\u2192En and En\u2192X, where X\nis a non-English language and En is English.\nBaselines. Our baselines are twofold. First, we\ncompare code-switching translations with monolin-\ngual translations and interpret deltas from monolin-\ngual baselines as the gains or losses from introduc-\ning code-switching units. We set our second base-\nline in consideration of our synthetic code-switched\ninputs. Because synthetic code-switched inputs al-\nready contain segments from reference texts, eval-\nuation scores for these may be higher than trans-\nlations of solely monolingual texts. In light of"}, {"title": "Results and Discussion", "content": "this, we consider deltas from raw code-switched\ninputs the performance of systems in translating\ncode-switched text.\nEvaluation Metrics. We measure the performance\nof translation models with the following automatic\nmetrics: chrF++ (Popovi\u0107, 2017) at the character\nlevel, spBLEU (Goyal et al., 2022) at the language-\nagnostic subword level tokenized through Sentence-\nPiece (Kudo and Richardson, 2018), and COMET\n(Rei et al., 2020) at the detokenized representa-\ntion level. spBLEU and chrF++ measure similarity\nbetween reference translation and system transla-\ntion, while COMET predicts human judgments of\nsystem translations based on a neural model. We\nuse the FLORES-200 (Costa-juss\u00e0 et al., 2022) tok-\nenizer available through SacreBLEU (Post, 2018)\nfor spBLEU and Unbabel/wmt22-comet-da (Rei\net al., 2022) for COMET calculation.\nWe supplement chrF++, spBLEU, and COMET\nwith copy and replacement rates to examine\nwhether translation systems can perform implicit\nlanguage identification to copy or replace tokens as\nappropriate. As in (Liu et al., 2021; Xu and Yvon,\n2021; Song et al., 2019), we define copy rate as\nthe rate at which the target tokens already present\nin code-switched input is successfully transferred\nover to the machine translation system output. We\ndefine replacement rate as the rate at which the sys-\ntem successfully converts non-target input tokens\nto target tokens. It follows that lower replacement\nrates indicate less translated outputs.\nAll experiments are conducted on a single\nNVIDIA L4 GPU."}, {"title": "Code-Switched Inputs Relative to\nMonolingual Translations", "content": "Results are shown in Table 4. Inspection of sp-\nBLEU in the to English setting reveals that 12 out\nof 13 synthetic code-switched inputs score higher\nthan M2M-100 translation outputs when evaluated\nagainst reference English texts. For NLLB-200,\nhowever, only 5 code-switched inputs score higher\nthan monolingual translations. In contrast, in the\nto non-English setting, raw inputs score higher\nthan monolingual translations for 11 and 10 lan-\nguages. We thus reaffirm the findings of (Nguyen\net al., 2023) that code-switched inputs score higher\nthan monolingual translations but with qualifica-\ntions that exceptional monolingual translations by\nstronger models can outperform code-switched in-"}, {"title": "Deltas Relative to Monolingual Baselines", "content": "Inclusion of code-switched units results in better\ntranslation than monolingual settings. This is\nseen in the predominantly positive deltas across\nspBLEU and chrF++ in Table 5. In particular,\nwhether the languages are low-resource or high-"}, {"title": "Deltas Relative to Code-Switched Input\nBaselines", "content": "NLLB-200 have a very high average gain of 23.2\nand a low standard deviation of 2.7. However,\nfor low-resource csw\u2192X translation, gains from\nmonolingual are much smaller than in csw\u2192En. In\nM2M-100, csw\u2192X deltas are halved or more than\nhalved from csw\u2192En deltas for Welsh, Mongolian,\nand Tamil, while csw\u2192X deltas become signifi-\ncantly larger for high-resource languages such as\nGerman, Estonian, and Latvian. In NLLB-200\ncsw\u2192X translation, all low-resource languages\nshow one digit spBLEU and chrF++ deltas. NLLB-\n200 benefits particularly little in Welsh given the\n2.7 increase in spBLEU and 1.1 increase in chrF++.\nThis extends findings of (Goyal et al., 2022) that\ntranslating into low-resource languages is harder\nthan translating out of them. Table 7 summarizes\ntwo languages with the most and least gains in"}, {"title": "Analysis of Translations", "content": "non-English languages. We confirm similar results\nin code-switching settings. This is most evident\nin Table 6 with gains in performance for chrF++\nand spBLEU for NLLB-200, where differences in\ndeltas between csw En and csw\u2192X are double\ndigits for the majority of the languages.\nHigh-resource languages gain further while low-\nresource languages lose performance gained\nthrough code-switched inputs in csw\u2192X. Per-\nformance already gained from code-switched in-\nput is lost in low-resource languages for csw\u2192X\ntranslation, whereas translations for high-resource\nlanguages effectively use code-switched inputs to\nresult in even greater gains than those seen in\ncsw\u2192En translation. For instance, deltas of chrF++\nscores in M2M-100 Catalan translation are 5.1 in\ncsw\u2192En and 19.0 in csw\u2192X, compared to values\nin Welsh of -2.1 in csw En and -17.2 in csw\u2192X.\nSimilar sized drops are seen for csw\u2192X in Tamil\nwith -13.7 and Mongolian with -9.2. Compara-\ntively, NLLB-200 performs better, but the increase\nin csw\u2192X in Mongolian is a mere 0.2 compared\nto 23.3 in Indonesian. NLLB-200 spBLEU scores\nyield similar conclusions, with a drop of 9.5 ob-\nserved in Mongolian compared to an increase of\n18.7 in Indonesian and 12.3 in Swedish. Overall,\nnegative deltas for csw\u2192X translation suggest that\nthere is room for improvement for code-switching\ntranslation into non-English languages."}, {"title": "Copy Rates", "content": "We report copy rates in Table 8. For\ncsw\u2192En translation, models show high copy rates\nranging from 90.4 to 94.5 percent for M2M-100\nand 95.5 to 98.4 percent for NLLB-200. This is"}, {"title": "Limitations", "content": "a considerable fraction of words are not being trans-\nlated, despite target language being specified. Ta-\nble 9 indicates that up to 33.8% of English to-\nkens are not translated into Indonesian with M2M-\n100 and up to 32.7% of English tokens are not\ntranslated into Indonesian with NLLB-200. Fig-\nure 2 shows examples of fully and partially trans-\nlated system outputs in Catalan-English and Welsh-\nEnglish. Words in orange are code-switched tokens\nthat remain in the system output of multilingual ma-\nchine translation models. We believe this points to\na case of the off-target problem seen in massively\nmultilingual translation models (Zhang et al., 2020;\nLiu et al., 2023; Chen et al., 2023; Guerreiro et al.,\n2023), studied primarily in monolingual translation\nsettings thus far. In our code-switching translation\nexperiments, models ignore the specified target lan-\nguage and instead copy the code-switched input as\nthe translation output.\nRecent work (Tan and Monz, 2023) demon-\nstrated that the off-target problem is a symptom\nrather than a cause of poor zero-shot translation\nin monolingual settings. To understand this in\nthe code-switching context, we apply their meth-\nods and measure the correlation between replace-\nment rates and spBLEU deltas relative to raw code-\nswitched inputs, shown in Figure 3. While there\nis a slight negative correlation, spBLEU deltas for\nreplacement rates of 100% vary significantly. We\ntherefore conclude that replacement rates are like-\nwise not direct causes of poor code-switching trans-\nlation, in accordance with prior findings.\nFigure 2 also illustrates a case of hallucination.\nIn the Welsh-English NLLB-200 translation, the\nwords in green, Whey and crempagai, are absent\nin the original Welsh and English sentences. We\nobserve, however, that the model attempted to trans-\nlate or scramble the Welsh words given the simi-\nlarity of Wyau and Whey and crempogau and crem-\npagai. In addition, this demonstrates the off-target\nproblem as models were tasked with translation\ninto English. Hallucinations observed in csw\u2192X\ntranslation are included in Appendix A.3."}, {"title": "Conclusion", "content": "We used English as the matrix language following\nthe Matrix Language Frame Model and detected\nEnglish intonation units. Future work could ex-\nplore code-switching based on intonation unit re-\nplacement on languages other than English and an-\nalyze any translation performance differences from\nthis work. Alternative methods for intonation unit\nreplacement could also be studied for scriptio con-\ntinua languages that we excluded for cross-lingual\ncomparative analysis."}, {"title": "Ethics Statement", "content": "In this work, we present CoVoSwitch, a code-\nswitching dataset created by replacing intonation\nunits detected by PSST, a speech segmentation\nmodel fine-tuned from Whisper, on CoVoST 2,\na speech-to-text translation dataset. Using CoV-\noSwitch, we examine the performance of two"}, {"title": "Acknowledgements", "content": "This work does not pose ethical issues. All datasets\nand models used in this study are publicly avail-\nable and were used under their respective Creative\nCommons licenses."}, {"title": "A Appendix", "content": "We thank the anonymous reviewers for their in-\nsightful comments and suggestions."}, {"title": "Languages in the Synthesized Dataset", "content": "In this work, we present CoVoSwitch, a code-\nswitching dataset created by replacing intonation\nunits detected by PSST, a speech segmentation\nmodel fine-tuned from Whisper, on CoVoST 2,\na speech-to-text translation dataset. Using CoV-\noSwitch, we examine the performance of two\nM2MNT models with millions of parameters, M2M-\n100 418M and NLLB-200 600M, and compare\ncode-switching translations against monolingual\ntranslations and high-resource languages against\nlow-resource languages. We discover that the\nintroduction of code-switching units results in\nhigher performing translations compared to mono-\nlingual settings and that models are better at code-\nswitching translation into English than into non-\nEnglish. Meanwhile, low-resource languages gain\nmost from monolingual baselines compared to\nother languages in csw En but much less in\ncsw\u2192X. Systems also exhibit poor translation abil-\nities in low-resource csw X translation to the ex-\ntent that performance already gained from code-\nswitched inputs is lost. Additionally, we find that\nmodels struggle to copy non-English tokens, iden-\ntify the off-target problem in code-switching set-\ntings, and confirm that models hallucinate in code-\nswitching translation by creating words nonexistent\nin the original source sentences. By releasing CoV-\noSwitch, we aim to support the inclusion of a wider\nvariety of languages in code-switching research."}, {"title": "Statistics on Train and Validation Subsets", "content": "We report the ISO 639-1 code, language name, fam-\nily, subgrouping, script, and resource level for the\n13 languages that we incorporated from CoVoST 2\nin Table 12. We draw the information on language\nfamily, subgrouping, script, and resource level from\n(Costa-juss\u00e0 et al., 2022). (Costa-juss\u00e0 et al., 2022)\nindicates resource level with either high or low.\nWe include statistics on train and validation subsets\nof CoVoSwitch, created from the train and valida-\ntion subsets of COVOST 2 in Tables 10 and 11."}, {"title": "Hallucinations in csw\u2192X Translation", "content": "We report the ISO 639-1 code, language name, fam-\nily, subgrouping, script, and resource level for the\n13 languages that we incorporated from CoVoST 2\nin Table 12. We draw the information on language\nfamily, subgrouping, script, and resource level from\n(Costa-juss\u00e0 et al., 2022). (Costa-juss\u00e0 et al., 2022)\nindicates resource level with either high or low.\nHallucinations, as shown in the csw\u2192En setting\nin Figure 2, are also seen in csw\u2192X. As such,\nwe provide a few observations of the problem in\nWelsh-English in Figures 4 and 5. Besides the hal-\nlucination of creating words noted in Figure 2, we\nfind repetitions of the same word. Additionally, we\nobserve that even if two different code-switching\nsentences share the same source sentences, transla-\ntion results can be significantly different, as seen\nin NLLB-200 outputs with one yielding repeated\nwords with no meaning and the other translated but\nalso including the repeated word Mae, highlighted\nin pink."}, {"title": "Parallel Examples of Code-Switching\nSentences Generated", "content": "Hallucinations, as shown in the csw\u2192En setting\nin Figure 2, are also seen in csw\u2192X. As such,\nwe provide a few observations of the problem in\nWelsh-English in Figures 4 and 5. Besides the hal-\nlucination of creating words noted in Figure 2, we\nfind repetitions of the same word. Additionally, we\nobserve that even if two different code-switching\nsentences share the same source sentences, transla-\ntion results can be significantly different, as seen\nin NLLB-200 outputs with one yielding repeated\nwords with no meaning and the other translated but\nalso including the repeated word Mae, highlighted\nin pink.\nAll code-switched texts in CoVoSwitch are made\nfrom parallel corpora in the En\u2192X subset of CoV-\nOST 2, and so are created using the same set of\nEnglish sentences. As a result, code-switched sen-\ntences across languages share English fragments.\nWe include an example from the test subset in Fig-\nure 6. For some languages, we demonstrate dif-\nferent intonation unit replacements than others to\nillustrate how resulting code-switched texts diverge\nbased on which intonation units are selected."}]}