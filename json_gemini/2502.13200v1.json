{"title": "Learning To Explore With Predictive World Model Via Self-Supervised Learning", "authors": ["Alana Santana", "Paula P. Costa", "Esther L. Colombini"], "abstract": "Autonomous artificial agents must be able to learn behaviors in complex environments without humans to design tasks and rewards. Designing these functions for each environment is not feasible, thus, motivating the development of intrinsic reward functions. In this paper, we propose using several cognitive elements that have been neglected for a long time to build an internal world model for an intrinsically motivated agent. Our agent performs satisfactory iterations with the environment, learning complex behaviors without needing previously designed reward functions. We used 18 Atari games to evaluate what cognitive skills emerge in games that require reactive and deliberative behaviors. Our results show superior performance compared to the state-of-the-art in many test cases with dense and sparse rewards.", "sections": [{"title": "1. Introduction", "content": "A prototypical Reinforcement Learning (RL) problem consists of an agent exploring the environment by choosing actions to maximize external rewards, also called extrinsic rewards. In many scenarios, these rewards are efficiently designed, but in the real world, they are scarce or non-existent. Nevertheless, autonomous artificial agents must be able to operate in complex environments without the need to pre-program rewards. This capability is still beyond the most advanced agents in the literature. In contrast, children exhibit a surprising ability to explore new environments, attend to objects, and physically engage with the outside world by creating new and exciting events (Haber et al., 2018). Such behavior is considered a self-supervised learning process guided by internal motivations. Inspired by this phenomenon, RL theorists realized that other aspects than extrinsic reward must be considered for constructing intelligent agents.\nEvidence suggests that intrinsic motivation is vital to guide intellectual development (Barto, 2013). While extrinsic motivation implies an agent behaving to achieve goals by receiving external rewards, such as money or a prize, intrinsic motivation implies that the agent can reach goals through internal motivators, such as curiosity, enjoyment, and learning. The idea of intrinsic rewards originates in the work of Robert White (White, 1959) and D. E. Berlyne (Berlyne, 1966). They criticize Freud's work and Hullian's view in which motivation is reduced only to drives related to biological needs (e.g., food). For example, the motivation for acquiring competence is not solely derived from energy sources conceptualized as drivers or instincts. Nevertheless, notions of novelty, complexity, and surprise are also drivers that motivate human behavior.\nIntrinsic-motivated RL has led to state-of-the-art results in Agent57 (Badia et al., 2020): Deep Mind's most success-"}, {"title": "2. Related Work", "content": "Intrinsic motivation is a very studied topic in the reinforcement learning field, and a good summary is presented by Barto et al. (Barto, 2013), Aubret et al. (Aubret et al., 2019), and Singh et al. (Singh et al., 2010). Initially, intrinsic motivation used concepts of emotion, surprise, empowerment, entropy, and information gain to formulate intrinsic rewards. Sequeira et al. (Sequeira et al., 2011) explored the hypothesis that affective states encode information that guides an agent's decision-making during learning. Achiam et al. (Achiam & Sastry, 2017) proposed a surprise-based approach in which the agent learns a probability transition model of an MDP concurrently with the policy and generates intrinsic rewards that approximate the KL divergence of the learned model's true transition probabilities. Mohamed et al. (Mohamed & Jimenez Rezende, 2015) developed an approach using the empowerment concept. Variational autoencoders and convolutional neural networks produce a stochastic optimization algorithm directly from image pixels. Similarly, Klyubin et al. (Klyubin et al., 2005) used empowerment as the gain of information based on the entropy of actions to formulate intrinsic rewards.\nCurrently, approaches based on prediction error in the feature space have been extensively explored in the literature. In 2015, Stadie et al. (Stadie et al., 2015) started their research using the feature space of an autoencoder to measure interesting states to explore. Pathak et al. (Pathak et al., 2017) proposed an approach based on an inverse dynamics model capable of scaling to high-dimensional continuous spaces and minimizing the difficulties of predicting directly in pixels, in addition to ignoring aspects of the environment that do not affect the agent. The approach showed that making predictions directly from the raw sensory space is unfeasible because it is challenging to predict pixels directly. Furthermore, some sensory spaces may be irrelevant to the agent's task. Agents trained with purely intrinsic rewards were able to learn task-relevant cognitive behaviors, demonstrating promising results in sparse environments. Similarly, Taylor et al. proposed an inverse dynamics model to assess the role of sensory space composition in the performance of an intrinsically motivated robotic arm that should manipulate objects on a table. Results showed that the approach"}, {"title": "3. Bidirectional Recurrent Models", "content": "Bidirectional Recurrent Models (BRIMs) mainly use self-attention to link identical LSTM modules, generating a very sparse and modular framework with only a small portion of modules actives at time t (Mittal et al., 2020). The approach separates the hidden state into several modules so that upward iterations between bottom-up and top-down signals can be appropriately focused. The layer structure has concurrent modules so that each hierarchical layer can send information both in the bottom-up and top-down directions. Bottom-up attentional subsystems communicate between modules of the same layer, as well as the composition of hidden states in initial layers using the entry xt as the target, and via top-down attention modules in different layers communicate with each other requesting information about hidden states of previous and posterior layers to compose the current hidden state. BRIMs is composed of the following structures.\nMulti-layer Stacked Recurrent Networks. Most multi-layer recurrent networks are configured to operate feedforward and bottom-up, meaning that higher layers are fed with information processed by inferior layers. In this sense, the traditional stacked RNN for L levels is defined as \\(h^l_t = F^l(h^{l-1}_t, h^{l-1}_{t-1})\\), where l = 0,1, ..., L. For a specific time step t, \\(y_t = D(h^L_t)\\) executes the prediction, based on input xt, where \\(h^0_t = E(x_t)\\) is the first hidden state at model, and \\(h^l_t\\) to the hidden state at layer l. D defines the decoder, E is the encoder, and \\(F^l\\) represents the recurrent dynamic at layer l (e.g., LSTM, GRU).\nRecurrent Independent Mechanisms (RIMs). Proposed by Goyal et al. (Goyal et al., 2019), RIM is a single-layered recurrent architecture that consists of hidden state ht decomposed into n modules. The main property introduced in this model is that on a specific time step, only a small subset of modules is activated. In this sense, the hidden states are updated following these steps: a) a subset of modules is activated depending on their relevance to the input; b) the activated modules independently process the information; c) the activated modules have contextual information from the other modules and update their hidden state to store such information.\nKey-Value Attention. The Key-Value Attention, also called the Scaled Dot Product, is responsible for the updates in RIM. This attentional mechanism is also employed in the self-attention modules, widely used in Transformer archi-"}, {"title": "4. Proposed Model", "content": "Our intrinsically-motivated agent has two models: 1) the predictive world model composed of a modular and competitive structure to generate the intrinsic rewards; 2) the policy model that learns a policy capable of generating a sequence of actions to maximize the reward signal. At the end of each action performed by the agent in the environment, it receives an intrinsic reward \\(r_{int}\\) generated by the predictive world model, as shown in Figure 2.\nThe predictive world model W is the key to our development. It is represented by a set of neural networks parameterized by \\(\\theta_E\\), \\(\\theta_P\\), and \\(\\theta_F\\). The model comprises a feature encoding E and independent recurrent modules that competitively generate representations of the agent's current and possible future state. At each time step t, the encoder E receives the current state st from the agent and generates a vector xt that encodes all the visual information. We used three 2D convolutional layers with linear layers to perform the encoding in our case. A key-value attention receives xt as a query and determines which independent recurrent modules should enter an active, inactive, or expected state. In summary:\n\\(x_t = E(s_t; \\theta_E),\\)\n(4)\n\\(h_t = \\phi (x_t, h_{t-1};\\theta_P),\\)\n(5)\n\\(h'_t = \\phi (x_t, h_{t-1};\\theta_F),\\)\n(6)\nwhere \\(h_t = \\{h^1_t, h^2_t, ....\\},\\) k\\in S_t is an embedding vector composed by activated modules with the current state \\(s_t\\). \\(S_t\\) are indices of activated modules, and \\(h'_t = \\{h^1_{t,w},h^2_{t,w},...., h^{|S_t|}_{t,w}\\}, w\\in S'_t\\), is an embedding vector composed by modules triggered in the expected state. \\(S'_t\\) are indices of modules in the expected state. \\(\\phi\\) and \\(\\phi^\\prime\\) are the LSTMs representing the modules.\nThe active modules use the encoder's information, passing it to the successive layers. At each time step t, when a module is inactive, it does not incorporate the new information, and there is no gradient flow. In parallel, some modules fire in the expected state (i.e., the modules activated to generate a future prediction to the next state st+1). As the structure is modular, the world representation is divided among the different modules; together, they make up a complete representation of the world. The attentional bottleneck directs the different modules' activation/deactivation/expectation flow so that the agent's internal representations of the world are useful for its actions without the need for inverse dynamics models. In addition, each module becomes an expert in certain aspects of the environment.\nAttention-driven bottom-up and top-down signals are especially beneficial in selecting agent actions. In phases where bottom-up information dominates, it can benefit the agent to act immediately to resolve unforeseen events. At the same time, top-down signals can be useful when the system needs a long-term plan. Furthermore, this representation is very similar to the activation and deactivation of pyramidal cells in the human neocortex (Hawkins et al., 2017a).\nOverall, attention-guided dynamic activation is essential in intrinsic agent learning, given that there is evidence in the literature that the modular structure can better handle distribution changes implicit in training as the policy evolves. At the end of the competition between modules, two state representations are created, ht and h't respectively. ht is the latent space vector of the agent's current state and will be input to the policy network. In contrast, h't is the representation expected to the next state st+1 after the agent executes the current action. Learning occurs when an expectation break is performed between ht and h't, so that the intrinsic reward is generated by \\(r_{int} = ||h_t - h'_{t-1}||^2_2\\), where n is the size of the vector. Finally, we represent the policy \\(\\pi (h;\\theta_p)\\) by a deep neural network with parameters \\(\\theta_p\\). Given the agent in state st, it executes action a ~ \\(\\pi (h; \\theta_p)\\) sampled from the policy. \\(\\theta_E\\), \\(\\theta_P\\), and \\(\\theta_F\\) are optimized to maximize the expected sum of intrinsic rewards, given by"}, {"title": "5. Experiments", "content": "In this section, we conduct a set of experiments to evaluate our proposed method."}, {"title": "5.1. Experiment Setup", "content": "Environments. We evaluate our method on 18 Atari Games, a standard setup for evaluating Reinforcement Learning methods. We choose Atari games with varying reward types (i.e., dense, sparse, and hybrid) and cognitive skills that the agent needs to learn. We aim to evaluate the agent in different games to analyze our approach's positive and negative points. Some games chosen are dynamic and require the agent to show fast and efficient reactive behaviors to survive in the environment. In contrast, other games are strategy games and require the agent to think of a plan to achieve a specific goal.\nArchitecture and pre-processing. We used the PPO (Proximal Policy Optimization) algorithm (Schulman et al., 2017), a robust learning algorithm that requires little hyperparameter tuning, for our experiments. While training with PPO, we normalize the advantages (Sutton & Barto, 2018) in a batch to have a mean of 0 and a standard deviation of 1. We use the same architecture to train all games. We use one layer of Recurrent Independent Mechanisms in the predictive model with eight modules, only 4 of which can be on active/expected states in each time step t. Two modules represent the current state st, two generate the expected representation to st+1, and four are inactive. Each module has a size of 32. All experiments were carried out in the pixels"}, {"title": "5.2. Results", "content": "In this section, we present the results obtained from our experiments. Our code is publicly available in our repository\u00b9. We compare our results with those obtained by the model proposed by Pathak et al. (Pathak et al., 2017), which was further explored by Burda et al. (Burda et al., 2018) using Atari games. We chose this model for comparison among many other works because it is a state-of-art approach to comparing results. In addition, it presents an extensive study of Atari games and is what most relates to our method. Our experiments showed that the observation normalization process adopted in (Burda et al., 2018) typically resulted in policy collapse after 50 million training steps. While this is an undesired behavior, we adopted the same normalization strategy (Section 5.1) for comparison purposes. We trained 18 Atari games with different gameplay features and reward patterns. All results are shown in Table 1. The results show that we scored quite significant games on 90% on cases of the test.\nReactive and deliberative environments. These environments require different cognitive skills from the agent over"}, {"title": "6. Conclusion", "content": "In this work, we present a new approach to generating intrinsic rewards. We have demonstrated that our purely intrinsic attentional and modular agent can play multiple Atari games without external rewards. Our agent learns to perform complex behaviors faster during training. Our approach is simpler, demonstrating a greater alignment of the agent's actions with the extrinsic rewards given by the environment.\nAs a result, we obtained results superior to the state-of-the-art in 90% of the tested games. Our approach has had surprising results in highly dynamic environments, such as Atlantis, Asterix, Assault, and Riverraid, that require highly reactive agent behaviors. The attention and modular structure contributed significantly to a concise and efficient world representation, providing the agent with only what is essential for the current action. However, our approach has limited performance in highly sparse environments such as Pitfall. This result is due to the low number of rollouts used in the experiments for this environment. Future work will address how cognitive behaviours emerge in complex humanoids to object manipulation tasks in highly sparse and realistic environments."}]}