{"title": "Human services organizations and the responsible integration of AI: Considering ethics and contextualizing risk(s)", "authors": ["Brian E. Perron", "Lauri Goldkind", "Zia Qi", "Bryan G. Victor"], "abstract": "This paper examines the responsible integration of artificial intelligence (AI) in human services organizations (HSOs), proposing a nuanced framework for evaluating AI applications across multiple dimensions of risk. The authors argue that ethical concerns about AI deployment\u2014including professional judgment displacement, environmental impact, model bias, and data laborer exploitation\u2014vary significantly based on implementation context and specific use cases. They challenge the binary view of AI adoption, demonstrating how different applications present varying levels of risk that can often be effectively managed through careful implementation strategies. The paper highlights promising solutions, such as local large language models, that can facilitate responsible AI integration while addressing common ethical concerns. The authors propose a dimensional risk assessment approach that considers factors like data sensitivity, professional oversight requirements, and potential impact on client wellbeing. They conclude by outlining a path forward that emphasizes empirical evaluation, starting with lower-risk applications and building evidence-based understanding through careful experimentation. This approach enables organizations to maintain high ethical standards while thoughtfully exploring how AI might enhance their capacity to serve clients and communities effectively.", "sections": [{"title": "Human services organizations and the responsible integration of AI: Considering ethics and contextualizing risk(s)", "content": "The adoption of artificial intelligence (AI) tools in human services organizations has been hindered by a lack of resources and a scarcity of evidence-informed practices (Goldkind, et al, 2024). Human services organization (HSO) leaders may also be skeptical of implementing AI tools that have been criticized as environmentally hazardous, inherently biased and developed using exploitative and extractive practices. The reticence to engage in the Al discourse and a lack of timely and accurate information obscures the complex reality that AI applications exist along a spectrum of risk ranging from benign to catastrophic, with consequential ethical implications. The lack of nuance impedes thoughtful evaluation of the potential benefits and drawbacks of implementing these new tools in social service delivery.\nThrough analysis of implementation contexts, evidence from real-world deployments, and an examination of ethical implications, this paper provides suggestions for HSOs to evaluate AI applications across multiple dimensions of risk. We contend that ethical concerns manifest differently depending on data sensitivity, potential impact on client wellbeing, professional oversight requirements, and the types of AI tools being used. Highlighted here are emerging solutions such as open-source local large language models that can help to facilitate responsible AI integration.\nTo start, we explore four commonly cited ethical concerns around the deployment of AI within HSOs: outsourcing professional judgment and decision-making, energy consumption and environmental depletion, model bias and the exploitation of data laborers in AI development; and how these ethical concerns vary in significance based on contextualizing factors (Li et al., 2022; Maphosa, 2024). This dimensional analysis reveals how the severity and manageability of each"}, {"title": "Ethical Issues in Context", "content": "The path to responsible AI integration in human services requires unpacking common ethical concerns that often lead to blanket rejection of AI technologies. The following section examines four key ethical issues frequently arising in AI adoption discussions: the potential displacement of professional judgment, energy consumption and environmental impact, model bias, and harms to data labelers. We argue that risk manifests differently across various applications for each concern, challenging the notion that all AI implementations carry equal ethical weight."}, {"title": "Outsourcing of Professional Judgment and Decision-Making", "content": "The ethical concern about Al replacing professional judgment represents the most fundamental challenge to AI adoption in social work. Critics rightly worry that automated systems might diminish professional autonomy or substitute algorithmic decisions for human judgment in client care (Creswell-Baez et al., under review). These concerns have been particularly acute in child welfare, with jurisdictions increasingly making predictive AI analytics available to workers in decision-making (Trail, 2024). We share these concerns and hold that humans -- not AI-powered tools -- should always remain the active, ethical agent in practice decisions. Our analysis suggests that the risk of outsourcing professional judgment to AI models exists across applications, with some technologies potentially enhancing rather than replacing professional judgment.\nFor example, retrieval augmented generation (RAG) systems demonstrate how carefully constrained AI can augment professional capabilities while preserving autonomy (Perron et al., in press; Victor et al., 2024). These systems combine a knowledge base containing verified information with a language model that processes user queries. When a professional queries the"}, {"title": "Energy Consumption and Environmental Impact", "content": "ChatGPT, Claude, and Gemini, among others, are frontier cloud-based Al models representing the commercial forefront of generative artificial intelligence. These models are characterized by their cutting-edge architecture, scalability, and ability to handle large-scale, complex tasks (Anderljung, et al, 2023). They are massive in scale, measured in \"parameters,\" and they are model-specific coefficients or instructions that give the model direction, whose exact numbers remain closely guarded trade secrets. The computational demands and resource requirements of these tools are substantial, particularly regarding energy consumption (Li, et al, 2024).\nThe environmental impact of these models (Ren, et al, 2024) has become a critical concern for our profession, especially given our commitment to environmental justice (Minnick et al., 2024). The training process alone requires enormous computational resources, translating into significant energy consumption. This impact is well-documented. For example, BLOOM, a 176-billion parameter language model, produced approximately 50.5 tonnes of CO2 emissions\u2014equivalent to about 60 transatlantic flights between London and New York (Luccioni et al., 2022).\nFortunately, the landscape of AI technology has expanded beyond resource-intensive frontier models to include more sustainable alternatives. For example, practical applications can use smaller Al models that run on local computers with far less environmental impact. These smaller models - called 'local' or 'open-source' models - are designed to run efficiently on standard computers without requiring cloud connections or specialized hardware. Unlike their larger counterparts, these models are typically free to download and use, and many are optimized for specific tasks like text classification or document analysis. They can operate entirely within an organization's IT infrastructure, which is more energy efficient and substantially increases data security. Small local models can run disconnected from the Internet, addressing privacy concerns and data leaks. While local models may not match the broad capabilities of frontier"}, {"title": "Model Bias", "content": "The challenges of model bias in high-stakes decision-making are well-documented (Gallegos, et al, 2024). Model bias manifests when Al systems perform differently or make systematically different recommendations across demographic groups, potentially perpetuating or amplifying societal inequities. For example, research has shown that large language models sometimes generate harmful, race-based medical advice when provided with patients' demographics (Omiye et al., 2023). Similarly, AI-powered facial recognition systems used in security and law enforcement consistently show higher error rates for Black people (Bacchini & Lorusso, 2019).\nThe risk of model bias can vary significantly, depending on the task the AI model is performing. Natural language processing (NLP) tasks like classification (categorizing text into predefined groups), extraction (pulling specific information from documents), and summarization (condensing text while maintaining key points) have a fundamentally different risk profile than creative content generation. These structured tasks operate on existing content rather than generating new ideas, making them less susceptible to bias and fabrication.\nThe key advantage of these NLP tasks is that their accuracy can be evaluated using the same methods we use to assess human performance. Unlike creative or predictive applications, these structured operations have clear, objective criteria for success: The classification aligns with expert judgment, extracted information either matches the source document or doesn't, and the summary either captures the key points or misses them. Even in these lower-risk scenarios, organizations can implement safeguards such as establishing clear quality thresholds before deployment, regularly auditing samples of AI-processed work against human review,"}, {"title": "Harms to Data Labelers", "content": "Most AI products depend on training data, which are used to \u201cteach\u201d the model how to generate outputs. Training data is often delivered to a software developer \u201craw\u201d or under- described and requires a cleaning and labeling process to be useful (Huang & Zhao, 2024). Data labeling in Al refers to annotating data with meaningful tags or labels that help machine learning (ML) models learn patterns from the data during training. During the labeling process, annotators are often exposed to disturbing content, including violence, abuse, and graphic imagery, causing psychological trauma and stress disorders (Crawford, 2023). Content moderators and data labelers frequently report anxiety, depression, and PTSD symptoms from reviewing harmful material (Strongylou, et al, 2024). This creates an ethical burden where AI development relies on human psychological harm.\nHowever, recent advances in synthetic data generation offer compelling alternatives that can eliminate the need for human exposure to traumatic content while maintaining model effectiveness and also increasing representation in training data (Kang, et al, 2024). For example, Microsoft's development of the Phi-3 model family demonstrates how synthetic data generation can avoid human labeling (Beatty, 2024). Their approach began with carefully curated educational content, using larger models to generate training data that maintained the clarity and structure of textbook explanations. This process, called \"CodeTextbook,\" eliminated the need for human annotators to review potentially harmful content. Instead, the synthetic data was generated through an iterative process where quality educational materials were used to create new training examples, filtered, and refined through automated processes."}, {"title": "A continuum of low- to high-stakes AI applications", "content": "The path to responsible AI integration in human services requires nuanced consideration of how different applications affect client well-being, professional judgment, and organizational effectiveness. While the field has seen growing interest in AI applications across practice areas, the risk profile of each application varies along multiple dimensions that must be carefully evaluated. These dimensions include the immediacy and magnitude of potential impact on client outcomes, the degree of professional oversight possible, and the sensitivity of data being processed.\nApplications involving direct client care decisions that require nuanced professional judgment and a deep understanding of complex human situations present the highest risk profiles. Treatment planning systems, risk prediction algorithms that guide intervention decisions, and AI-driven therapy without meaningful professional oversight all operate at the higher end of the risk spectrum. The limitations of current AI technology and the profound consequences of errors in these domains make such applications particularly challenging to implement responsibly. The inability of AI systems to fully grasp the context, account for"}, {"title": "Document Analysis and Classification", "content": "\u2022 Analyzing historical case notes, records, or reports to identify patterns and classify content for research, evaluation, or planning purposes where findings can be verified against source materials and validated through sampling before influencing decisions."}, {"title": "Information Access and Organization", "content": "\u2022 Retrieving relevant sections from organizational policies, procedures, and documentation in response to staff queries where every response can be verified against source materials.\n\u2022 Organizing and indexing institutional knowledge bases to improve information accessibility while maintaining clear links to authoritative sources."}, {"title": "Administrative Data Processing", "content": "\u2022 Cleaning and standardizing datasets for analysis where outputs can be validated against raw data using established quality control procedures.\n\u2022 Generating routine visualizations and reports from administrative data where results can be verified through standard statistical validation practices."}, {"title": "Documentation Support", "content": "\u2022 Identifying potential connections between cases and available organizational resources while leaving interpretation and decision-making to professionals.\n\u2022 Highlighting relevant passages in lengthy documents to support professional review while maintaining human control over interpretation."}, {"title": "Resource Management", "content": "\u2022 Analyzing historical resource utilization patterns to support planning where recommendations can be validated before implementation.\n\u2022 Identifying potential inefficiencies in administrative workflows where suggested changes can be evaluated before adoption."}, {"title": "Records Management", "content": "\u2022 Organizing and categorizing archived records for improved accessibility where classifications can be verified against original documents.\n\u2022 Identifying duplicate or inconsistent records where changes can be reviewed before being implemented."}, {"title": "Training Material Development", "content": "\u2022 Processing existing training materials to create indexed resources where all content comes from approved sources.\n\u2022 Generating practice scenarios from de-identified historical cases where materials can be reviewed before use.\n\u2022 Synthesize peer-reviewed academic literature on evidence-based practice for use in professional development contexts."}, {"title": "Quality Assurance Support", "content": "\u2022 Checking documentation against standardized requirements where discrepancies can be verified manually.\n\u2022 Identifying potential documentation gaps where findings can be validated before any corrective action."}, {"title": "Research Support", "content": "\u2022 Analyzing de-identified datasets for research purposes where findings can be validated through established scientific methods.\n\u2022 Extracting relevant information from published literature where results can be verified against source materials."}, {"title": "Operational Monitoring", "content": "\u2022 Tracking administrative metrics and generating alerts about operational issues where warnings can be verified before action is taken.\n\u2022 Analyzing workflow patterns to identify potential bottlenecks where findings can be validated before process changes."}, {"title": "Moving the Field Forward", "content": "Our analysis of ethical concerns and risk dimensions in AI adoption reveals several critical paths forward for human services research and practice. The field needs coordinated effort across three key areas to advance responsible AI integration: implementation research, professional education, and organizational capacity building.\nThe first priority is developing standardized frameworks for evaluating AI implementations in human services contexts. While our dimensional analysis provides a starting point, we need empirical research on how organizational contexts affect risk profiles and implementation outcomes. This research should focus on understanding how verification processes, oversight mechanisms, and professional integration practices influence implementation success across different positions on the risk continuum. Particular attention should be paid to studying how organizations successfully implement verification processes for administrative applications, as these findings could inform the development of more robust safeguards for higher-risk applications.\nProfessional education represents the second critical area for advancement. Public administration, nonprofit administration and social work programs need to integrate AI literacy into their curricula, focusing not on technical skills but on critical evaluation capabilities. Students and practitioners need frameworks to assess AI applications across our identified risk dimensions and understand how different implementation choices affect ethical concerns. This education should emphasize hands-on experience with local language models in controlled environments, allowing practitioners to develop a practical understanding of both capabilities and limitations while learning to identify appropriate use cases within their specific practice contexts."}]}