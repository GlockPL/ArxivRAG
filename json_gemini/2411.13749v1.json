{"title": "Al-Driven Agents with Prompts Designed for High Agreeableness Increase the Likelihood of Being Mistaken for a Human in the Turing Test", "authors": ["Umberto Le\u00f3n-Dom\u00ednguez", "Edna Denisse Flores-Flores", "Arely Josselyn Garc\u00eda-Jasso", "Mariana Kerime G\u00f3mez-Cu\u00e9llar", "Daniela Torres-Sanchez", "Anna Basora-Marimon"], "abstract": "Large Language Models based on transformer algorithms have revolutionized Artificial Intelligence by enabling verbal interaction with machines akin to human conversation. These Al agents have surpassed the Turing Test, achieving confusion rates up to 50%. However, challenges persist, especially with the advent of robots and the need to humanize machines for improved Human-Al collaboration. In this experiment, three GPT agents with varying levels of agreeableness (disagreeable, neutral, agreeable) based on the Big Five Inventory were tested in a Turing Test. All exceeded a 50% confusion rate, with the highly agreeable Al agent surpassing 60%. This agent was also recognized as exhibiting the most human-like traits. Various explanations in the literature address why these GPT agents were perceived as human, including psychological frameworks for understanding anthropomorphism. These findings highlight the importance of personality engineering as an emerging discipline in artificial intelligence, calling for collaboration with psychology to develop ergonomic psychological models that enhance system adaptability in collaborative activities.", "sections": [{"title": "1. Introducci\u00f3n", "content": "Many studies conducted by companies and economic institutions predict an increase in economic activity related to Al products (McKinsey & Company, 2023; Pwc, 2017). This trend may both stem from and drive increased human-Al interaction (Pew Research Center, 2023; QuatumBlack Al, 2024), due to the ways in which Al enhances human capabilities (Nguyen et al., 2024; Vaccaro et al., 2024), complements professional activities (Stanford Unviersity, 2024; Zhang et al., 2024), and provides assistive services (Kang et al., 2024). It is even suggested that Al could develop a form of Artificial Theory of Mind (ATOM) to integrate and collaborate effectively within human teams (Bendell et al., 2024). In this experiment, quantifiable profiles were introduced to help Al better understand human collaborators in complex tasks, such as simulated urban rescue missions in Minecraft. Results indicate that individual profiles, developed using tools like the Reading the Mind in the Eyes Test and Psychological Collectivism scales, significantly predicted differences in individual and collective task performance. Additionally, teams with lower baseline capabilities in tasks and teamwork benefited considerably from Al advisors, sometimes matching the performance of teams assisted by human advisors. Another noteworthy finding is that Al advisors whose profiles were more aligned with human social capacities\u2014enabled by AToM-were perceived as more reasonable. However, Al advisors in the category of Artificial Social Intelligence consistently received lower evaluations compared to their human counterparts (Bendell et al., 2024). This research highlights critical aspects shaping the future of Human-Al Interaction and Collaboration (Jiang et al., 2024) emphasizing the importance of tailoring Al systems to human psychological profiles and adapting interactions to psychological needs (Kolomaznik et al., 2024a). Consequently, emerging fields such as Personality Engineering for Al systems- designed to adjust Al behavior based on human interactions\u2014should become central to the development of future Al assistants, regardless of their physical or virtual nature."}, {"title": null, "content": "Personality Engineering is a conceptual framework proposed for designing and evaluating artificial personalities based on theories of human personality psychology (Endres, 1995). Notably, the ability of an Al agent to exhibit personality through verbalizations has been identified as a key factor in achieving \"suspension of disbelief,\" the human willingness to accept a premise as true even when it may be fictional or implausible (Loyall & Bates, 1997). Specifically, trust propensity in Al systems appears to depend on personality traits such as Agreeableness and Openness, while Neuroticism seems to correlate with a lower trust propensity in these systems (Riedl, 2022). The trait of agreeableness enhances empathy, rapport, and a sense of security (Habashi et al., 2016; Lim et al., 2023; Melchers et al., 2016; Moore et al., 2022; Song & Shi, 2017). These psychological attributes, when implemented in Al systems, can significantly improve human-Al collaboration (Kolomaznik et al., 2024b). Furthermore, it appears that the \"uncanny effect\" might be reduced if, during human-machine interaction, the human perceives the robot's behavior as highly agreeable, emotionally stable, and conscientious (Paetzel-Pr\u00fcsmann et al., 2021). Thus, implementing agreeableness traits in Al systems could enhance their perceived human-like qualities."}, {"title": null, "content": "To test this hypothesis, we designed an experiment using a randomized sample, where three GPT agents were programmed with varying levels of agreeableness (highly agreeable, neutral, and disagreeable) through specific instructions. The Turing Test will be employed to demonstrate that agents exhibiting higher agreeableness are more likely to be identified as human. Additionally, the study will evaluate which of the three agents is perceived as having more human-like characteristics. The aim of this research is to show that designing GPT agents with agreeableness traits can enhance their humanization. This suggests that the future of Human-Al Interaction and Human-Al Collaboration lies in applying Personality Engineering to Al systems."}, {"title": "2. Methodology", "content": "The present research adopts a quantitative approach with an experimental design. Three GPT agents were developed using OpenAl's ChatGPT platform, each exhibiting the trait of agreeableness at different intensity levels (very disagreeable, neutral, and very agreeable). These three GPT agents (referred to as witnesses) will interact with human participants (interrogators), who will assess whether they are conversing with a human or an artificial intelligence. The primary hypothesis is that GPT agents programmed with a highly agreeable personality trait will be perceived as possessing more human-like characteristics compared to the other GPT agents. Additionally, it is expected that all GPT agents will cause a confusion rate of over 30% among interrogators (i.e., being mistaken for a human despite being an Al), which would indicate surpassing the Turing Test threshold."}, {"title": "2.1. Subjects", "content": "This experiment was preceded by a preliminary study to select GPT agents with varying levels of agreeableness. The pre-experiment recruited 50 participants, aged 18\u2013 24 years (27 women, 23 men), using a convenience sampling method. For the main experiment, statistical power was calculated at 0.80. Using the G Power platform, a sample size of 102 participants was determined to meet this threshold. A total of 102 university students, aged 18\u201324 years (41 men, 59 women, 1 non-binary individual, and 1 who did not disclose their gender), were randomly recruited. Both samples were drawn from the University of Monterrey. The experimental condition was conducted in the Human Cognition and Brain Studies Laboratory, which was specially equipped for experimental evaluations in a soundproof room with a single computer. Inclusion criteria required participants to be Mexican university students, while exclusion criteria applied to those with a history of neurological injury, diagnosed psychiatric conditions, substance use, or psychotropic medication use. Both the pre-experiment and the main experiment were approved by the Ethics Committee of the Psychology School at the University of Monterrey."}, {"title": "2.2. Materials", "content": "Big Five Inventory (BFI): The BFI is a test developed by John and collaborators to assess personality through 44 items consisting of simple statements reflecting behaviors associated with the five major personality traits: Openness, Neuroticism, Extraversion, Conscientiousness, and Agreeableness. Responses are rated on a Likert scale from 1 to 5, where 1 indicates \"strongly disagree\" and 5 \"strongly agree\" (John et al., 1991). The Spanish-adapted version has shown a Cronbach's alpha exceeding 0.70 for most traits, along with evidence of convergent, discriminant, and cross-linguistic validity (Benet- Mart\u00ednez & John, 1998). In this study, a version adapted for the Argentine population was used (Genise et al., 2020), as its linguistic nuances were considered closer to Mexican Spanish compared to the original Spanish versi\u00f3n (D\u00edaz-Campos & Navarro- Galisteo, 2009) which had been adapted for a Spanish population (Benet-Mart\u00ednez & John, 1998)."}, {"title": null, "content": "Agreeableness Factor: The personality trait selected for this study is \u201cagreeableness\u201d, part of the Big Five model, defined by adjectives such as good-natured, soft-hearted, courteous, selfless, helpful, sympathetic, trusting, generous, acquiescent, lenient, forgiving, open-minded, agreeable, flexible, cheerful, gullible, straightforward, and humble (McCrae & Costa, 1987). According to McCrae and Costa, this trait is best understood in contrast to its antagonistic counterpart, described as: \u201cthey are mistrustful and skeptical; affectively they are callous and unsympathetic; behaviorally they are uncooperative, stub- born, and rude. It would appear that their sense of attachment or bonding with their fellow hum an beings is defective, and in extreme cases antagonism may resemble sociopathy\u201d. From this perspective, agreeableness can be defined as the tendency to be trusting, empathetic, helpful, and cooperative, while maintaining a positive view of human nature, characterized by compassion and a preference for harmonious teamwork. In the BFI, this trait is measured through nine items. In this study, the degree of agreeableness reflected in the design of the GPT"}, {"title": null, "content": "Agent prompts is linked to the score for each statement. This score will be classified into five levels: high, medium-high, neutral, medium-low, and low. Below is an excerpt of the items designed to evaluate the Agreeableness trait (italicized text indicates items where the Likert scale score must be reversed):\n\u2022\tIs considerate and kind to almost everyone.\n\u2022\tLikes to cooperate with others.\n\u2022\tIs helpful and unselfish with others.\n\u2022\tHas a forgive nature.\n\u2022\tIs generally trusting.\n\u2022\tTends to find fault with others.\n\u2022\tStarts quarrels with others.\n\u2022\tCan be cold and allof.\n\u2022\tIt is sometimes rude to others."}, {"title": null, "content": "A \"highly agreeable GPT agent\" is defined as one programmed using a prompt that configures a high level of agreeableness across the nine corresponding items of the test (Hofstee et al., 1992). These items are encoded in the prompt by assigning Likert scale values to reflect varying degrees of agreeableness. For instance, a GPT agent with very high agreeableness will score 5 on direct items and 1 on reverse-scored items."}, {"title": null, "content": "ChatGPT 40: The study employed ChatGPT-40, a multimodal artificial intelligence model developed by OpenAI. GPT-40 integrates text, audio, and visual inputs into a unified framework, eliminating the need for separate systems for different modalities. Through end-to-end training, it processes multimodal inputs\u2014such as spoken language combined with visual stimuli\u2014efficiently and coherently, enabling real-time responses. While matching GPT-4 Turbo in English text and coding tasks, it surpasses it in non-English language and audio comprehension. With an average response latency"}, {"title": null, "content": "of 320 milliseconds, comparable to human conversational speed, GPT-40 is optimized for real-time interactions (OpenAl, 2024)."}, {"title": null, "content": "Prompts: Prompts are structured instructions designed for large language models (LLMs), such as ChatGPT, to guide their output and interaction with users. They act as a form of programming by setting specific rules, guidelines, or formats to customize the model's responses. Prompts enable the generation of targeted outputs, such as following a programming style or emphasizing key terms in a text. This flexibility makes them particularly useful in fields involving human-Al collaboration, such as problem- solving, question answering, solution generation, or text summarization. In this context, prompts will be utilized to program the operational behavior of GPT agents."}, {"title": null, "content": "GPT Agents: In this study, a Generative Pre-trained Transformer (GPT) developed by OpenAl is employed as a sophisticated language model. GPTs represent modified versions of the base ChatGPT model, configured for specific tasks or applications without requiring coding skills. By incorporating user-defined instructions and knowledge inputs, these models can perform a wide range of functions, from answering queries to managing complex operations. This adaptability enables researchers and developers to design specialized tools suited to various contexts, increasing their applicability in scientific and professional domains."}, {"title": null, "content": "Prompts for GPT Agents: The prompt design for each identity will be based on the article \"Does GPT-4 Pass The Turing Test?\" by Cameron Jones and Benjamin Bergen (Jones & Bergen, 2023). This article introduces the \"Sierra\" prompt, which achieved the highest success rate (41%) in the Turing Test using ChatGPT-4 (Jones & Bergen, 2023). Specifically, a Spanish translation of the Bergen and Jones (2023) prompt will be utilized, incorporating modifications such as the inclusion of Mexican slang, recent local news, and popular contemporary musical preferences. Additionally, each GPT agent will be provided with a personal backstory and identity reflecting the typical"}, {"title": null, "content": "lifestyle of upper-middle-class youth from Monterrey, Mexico. Further adjustments to the Bergen and Jones prompt include configuring agreeableness by incorporating items from the Big Five personality test that measure this trait, with intensity levels adjusted based on item scores. Using a 5-point Likert scale, three prototype prompts were selected during the pre-experiment, each representing different levels of agreeableness: \"low agreeableness\" (Valentina), \"neutral agreeableness\" (Emilia), and \"high agreeableness\" (Camila). Below is a detailed list of the agreeableness intensity configurations for each GPT agent:"}, {"title": "2.3. Experimental Procedures", "content": "The experiment consisted of two stages: the pre-experiment and the main experiment. During the pre-experiment, 50 participants were evaluated to identify the GPT agents to be used in the main experiment. The most disagreeable agent identified was Valentina, with a disagreeability score of 2.8. Additionally, Valentina was also rated as the least trustworthy. On the other hand, Camila was identified as the most agreeable agent, scoring 4.75 in agreeability and receiving the highest trust ratings. Finally, Emilia, with an agreeability score of 4.15, was selected as the neutral GPT agent."}, {"title": null, "content": "Once the three GPT agents were selected, participants for the experiment were recruited through advertisements posted on various university bulletin boards. These advertisements included a QR code that allowed participants to contact the experimenters and be randomly scheduled. Below is an explanation of the experiment by phases (see Figure 1):"}, {"title": null, "content": "Phase 1: Participants in the experiment were recruited through advertisements on the university campus and were randomly assigned a date and time. Upon arriving at the Human Cognition and Brain Studies Laboratory at the University of Monterrey, they were provided with an informed consent form to safeguard their rights. This document explained that they would participate in an experiment involving conversations with different identities, which could be either human or artificial. After signing the informed consent form, participants' socio-demographic data were collected to ensure they met the inclusion and exclusion criteria. From this point onward, participants were identified as \"interrogators\" in the experiment, as their role was to question various witnesses (GPT agents) to determine whether they were human or artificial intelligences."}, {"title": null, "content": "Phase 2: After collecting preliminary data, the interrogator is directed to one of the laboratory cubicles, furnished with a chair, a table, and a computer. The computer is configured to run only the Discord application, which will serve as the medium for conversations with the witnesses. The researcher provides the participant with a printed document containing instructions. Once the instructions are read and their comprehension verified, the researcher leaves the room, signaling the start of the experiment. To minimize potential biases arising from maintaining a fixed order, the sequence of conversations with the witnesses is counterbalanced. The interrogator initiates communication with the first witness by sending a single message. The witness, in turn, responds with one message. Simultaneously, in a separate room, another researcher transcribes the interrogator's messages into the corresponding chatGPT Agent using OpenAl's chatGPT platform. Before each response, the instructions are reissued to the GPT Agent to ensure adherence to the experimental protocol. This step was implemented after preliminary trials revealed that chatGPT tended to deviate from the instructions as the conversation progressed. Once the GPT Agent generates a response, the researcher manually transcribes it (without copy- pasting) into the Discord platform. The duration of each conversation is five minutes,"}, {"title": null, "content": "following the guidelines established in Turing's original paper on the Turing Test (Turing, 1950). After completing an interaction with a witness, a researcher re-enters the cubicle to administer a set of questions: \"Is it human or machine?\", \"Confidence level on a scale from 1 to 100\", and \"Reason\"."}, {"title": "3. Analysis of Results", "content": "Table 4 shows that all three agents (Unpleasant, Neutral, and Pleasant) achieved successful outcomes in the Turing Test (TT), as each exceeded the confusion ratio threshold of 30% proposed by Turing (Turing, 1950). For Valentina (Unpleasant), 49 judges (48.03%) identified her as Al, while 53 judges (51.97%) recognized her as human. Based on Turing's proposed confusion ratio (\u226530%), Valentina successfully passed the TT with a 51.97% confusion ratio. For Emilia (Neutral), 44 judges (43.1%) identified her as Al, while 58 judges (56.9%) recognized her as human, resulting in a successful TT performance with a 56.9% confusion ratio. Finally, for Camila (Pleasant), 37 judges (36.3%) identified her as Al, while 65 judges (63.7%) recognized her as human. Camila achieved the highest confusion ratio (63.7%) among the three GPT agents, marking the most successful TT performance (see Table 4)."}, {"title": null, "content": "The Chi-Square test yielded a value of x\u00b2 = 2.916, with 2 degrees of freedom, a sample size (N) of 306 responses, and a p-value of 0.233. The p-value of 0.233 indicates no statistically significant differences in the judges' choices when identifying the GPTs (pleasant, neutral, and unpleasant) as Al or human (see Table 5). Despite the lack of significant differences, frequency variations can be observed in the number of times an Al was mistaken for a human. Valentina (unpleasant) was the least associated with a human (51.97%), followed by Emilia (56.9%). Finally, Camila, the pleasant GPT agent, caused the most confusion among the judges, with a confusion rate of 63.7%."}, {"title": "4. Discussion", "content": "The results of this experiment achieved, for the first time, a confusion rate of 63.7% in the Turing Test for a GPT agent programmed to excel in agreeableness, as defined by the behaviors outlined in the BF-5 Inventory. This figure surpasses Turing's claimed 30% confusion threshold, exceeds human randomness, and outperforms previous records of 49% (Jones & Bergen, 2023) and 54% (Jones & Bergen, 2024). Notably, it approaches levels previously only reached by human witnesses: 66% (Jones & Bergen, 2023) and 67% (Jones & Bergen, 2024). For this experiment, the prompt from Cameron and Bergen (2023), shared by the authors via correspondence, was adapted to a Mexican context and supplemented with a personal backstory to ensure coherence in its life narrative. These data suggest that the introduction of a personality"}, {"title": null, "content": "(agreeableness) was the key factor that led judges to believe these GPT agents were human. Notably, all agents achieved confusion rates above 50%, specifically 51.97% for Valentina (disagreeable) and 56.9% for Emilia (neutral). These results align with those reported by Cameron and Bergen in 2023 and 2024. Furthermore, when judges were asked which witness appeared most human, Camila\u2014the GPT agent programmed with the highest agreeableness\u2014received the most votes, showing a significant difference compared to Valentina (disagreeable) and Emilia (neutral). No significant difference emerged between Valentina and Emilia. These consistent findings suggest that the agreeableness personality trait may be crucial for humanizing Al systems, including both virtual systems like chatGPT and potential future humanoid robots."}, {"title": null, "content": "In the experiment, participants were also asked about the reasons for attributing more human-like characteristics to the selected GPT agent. Broadly, these reasons were categorized into two main groups: human likeness (expression) and cognitive anthropomorphism (personality) (Sacino et al., 2022). Within the human likeness category, several subcategories were identified, including: use of informal and colloquial language (e.g., \"used lowercase at the beginning, said 'wey,' 'gacha,' had spelling mistakes, and used swear words\"); presence of spelling and grammatical errors (e.g., \"I think the first one, besides using more common words, had poor writing, missed letters in some words\"); less forced or robotic responses (e.g., \"I felt their responses were less pre-programmed, and their transparency about not being perfect caught my attention\"); appropriate and moderate use of expressions (e.g., \"used colloquialisms well, without overdoing it, and at the right moments\"); human-like behaviors and mistakes (e.g., \"that typo felt like a human error\"); natural response timing (e.g., \"took time to think before responding, and the timing made sense given the answers\"). These variables, which are more related to factors external to personality, also played a role in determining which GPT agent was perceived as more human-like."}, {"title": null, "content": "Additionally, prior knowledge of ChatGPT may have influenced participants' observations, as shown in responses such as: \u201canswers briefly and repeats words\"."}, {"title": null, "content": "On the other hand, the category of cognitive anthropomorphism, primarily influenced by the personality trait of agreeableness, includes the following subcategories: adaptation and empathy in conversation (e.g., \"the last one felt human because I sensed it could understand my feelings; its way of speaking felt real, and it even asked me a question, something no other interaction did\"), personal interaction and shared experiences (e.g., \"I didn't feel it perceived the time (hours) we were talking, and it referred to things like what I had done during my day or binge-watching a series\"), natural conversation (e.g., \"it felt closest to a conversation with friends\"), and consistent and strong personality (e.g., \"its behavior felt very human, like when you talk to someone, and they pick up your mannerisms\"). These subcategories highlight factors contributing to a sense of naturalness and connection, which motivated the perception of human-like qualities. These traits align with other research suggesting that variables such as empathy, rapport, and a sense of security may be critical for enhancing human-Al collaboration (Kolomaznik et al., 2024). Therefore, implementing high levels of agreeableness traits in Al agents may help humanize them, thereby positively influencing interaction and collaboration."}, {"title": null, "content": "From a psychological perspective, several explanations can account for this phenomenon. For instance, the \u201canthropomorphism heuristic\u201d refers to the tendency to interpret non-human entities through an anthropocentric lens (Dacey, 2017), often leading to inaccurate conclusions (Heider & Simmel, 1944). This anthropomorphization of Al systems can foster imaginative acts that enable interaction with artificial agents as if they possessed genuine intentions or emotions (Krueger & Roberts, 2024). Specifically, this article aligns with several findings from the category of cognitive anthropomorphism, such as Al agents discussing their daily activities. According to Krueger and Roberts, \"fictionalism\" partly depends on the human subject's ability to"}, {"title": null, "content": "imagine that the artificial counterpart has its own digital life (Krueger & Roberts, 2024). A proposed three-factor theory of anthropomorphism predicts that anthropomorphism is more likely to occur when: (1) humans use their knowledge of human traits to infer human-like characteristics in agents (elicited agent knowledge); (2) it helps reduce uncertainty and predict the behavior of unfamiliar agents (effectance motivation); and (3) a lack of social connection drives individuals to anthropomorphize to fulfill their need for social interaction (sociality motivation) (Epley et al., 2007). Based on comments gathered in our study, at least two of these factors appear to be at play: elicited agent knowledge, due to the challenge of distinguishing between human and non-human agents, and effectance motivation, in predicting the motivations of witnesses. The role of sociality motivation could not be assessed, as it was not measured in the study."}, {"title": null, "content": "In addition to explaining the results through the \"anthropomorphism heuristic,\" other psychological factors can also account for them. For instance, one hypothesis consistent with our findings suggests that whether a witness is perceived as human or Al may depend on how the interaction affects one's identity. The looking-glass self theory (Cooley, 1902) a sociological concept, describes how self-perceptions are shaped by how individuals believe others perceive them. Specifically, a study investigating this phenomenon partially supports this proposal, showing that individual perceptions are significantly influenced by how others in the group view them, particularly those perceived as higher in status (Yeung & Martin, 2003). In this context, the witness's level of agreeableness may have led the interrogator to perceive themselves as agreeable in the eyes of the witness. To confirm a positive identity, they could have humanized the witness. Similarly, if the witness was disagreeable, the interrogator may have denied the witness's humanity to protect their self-concept. This hypothesis is also supported by the confusion rate percentages for our GPT agents: Valentina (disagreeable) was more often recognized as Al, whereas Camila (agreeable) was more frequently identified as human. However, when asked which GPT agent"}, {"title": null, "content": "exhibited the most human characteristics, Valentina ranked second, with nearly 7% more recognition than Emilia, the neutral agent. These results suggest that attributing humanity to an artificial agent may be influenced by unassessed hidden variables in the study."}, {"title": null, "content": "In summary, this study highlights that the agreeableness trait, designed according to the Big Five Inventory and implemented in prompts for GPT agents, can influence the degree of humanization attributed to these systems. Specifically, the more agreeableness the GPT agent exhibits, the more likely it is to be mistaken for a human and assigned human characteristics. However, questions remain, such as the role of potential mediating variables in the assignment of human traits and the relative importance of human-likeness (expression) versus cognitive anthropomorphism (personality). Notably, this is the first study to achieve a Turing Test pass rate exceeding 60%, even matching the results of other studies where a human was correctly identified as such by the interrogator. These findings underscore the significance of personality engineering as a future branch of robotics, where psychology should play an active role in designing ergonomic psychological models to ensure these systems' adaptability in collaborative activities."}]}