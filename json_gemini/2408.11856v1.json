{"title": "Dynamic Adaptive Optimization for Effective Sentiment Analysis Fine-Tuning on Large Language Models", "authors": ["Hongcheng Ding", "Xuanze Zhao", "Shamsul Nahar Abdullah", "Deshinta Arrova Dewi", "Zixiao Jiang"], "abstract": "Sentiment analysis plays a crucial role in various domains, such as business intelligence and financial forecasting. Large language models (LLMs) have become a popular paradigm for sentiment analysis, leveraging multi-task learning to address specific tasks concurrently. However, LLMs with fine-tuning for sentiment analysis often underperforms due to the inherent challenges in managing diverse task complexities. Moreover, constant-weight approaches in multi-task learning struggle to adapt to variations in data characteristics, further complicating model effectiveness. To address these issues, we propose a novel multi-task learning framework with a dynamic adaptive optimization (DAO) module. This module is designed as a plug-and-play component that can be seamlessly integrated into existing models, providing an effective and flexible solution for multi-task learning. The key component of the DAO module is dynamic adaptive loss, which dynamically adjusts the weights assigned to different tasks based on their relative importance and data characteristics during training. Sentiment analyses on a standard and customized financial text dataset demonstrate that the proposed framework achieves superior performance. Specifically, this work improves the Mean Squared Error (MSE) and Accuracy (ACC) by 15.58% and 1.24% respectively, compared with previous work.", "sections": [{"title": "Introduction", "content": "Sentiment analysis has become an essential tool for businesses to understand and analyze customer opinions and feedback, gauging public perception of their products, services, and brand reputation (Liu 2020; Alaei, Becken, and Stantic 2019). It is also used in demand forecasting by analyzing online product reviews (Kharfan, Chan, and Firdolas Efendigil 2021; Li, Lin, and Xiao 2022) and in supply chain management to monitor supplier performance and identify potential risks (Sharma, Adhikary, and Borah 2020). In the financial sector, sentiment analysis is crucial for forecasting stock market trends (Bai et al. 2023), predicting cryptocurrency prices (Chowdhury et al. 2020), and forecasting exchange rates (Ding et al. 2024a).\nTraditionally, sentiment analysis employs various approaches, primarily including machine learning algorithms (e.g., SVM and Naive Bayes (Dang, Moreno-Garc\u00eda, and De la Prieta 2020)) and deep learning methods (e.g., CNN"}, {"title": "Motivation", "content": "In the sentiment polarity analysis task for exchange rate texts, we observe that the performance of the RoBERTa-Large (Liu et al. 2019) is unsatisfactory after fine-tuning on the standard and customized financial text dataset (Ding et al. 2024a). This can be attributed to the model's lack of exposure to news domain-specific texts containing specialized jargon, implicit sentiments, and subtle variations. Subsequently, we employ an alternative RoBERTa-Large model, Twitter-ROBERTa-Large (Loureiro et al. 2023) fine-tuned on a tweet news dataset, and the model's performance shows a slight improvement."}, {"title": "Method", "content": "In this section, we will introduce traditional sentiment analysis, our framework incorporating the DAO module with LORA."}, {"title": "Traditional Sentiment Analysis", "content": "Backbone: RoBERTa-Large. In sentiment analysis, LLMs serve as text data embedding generators, concatenating various task-specific heads for various sentiment analysis tasks. This work employs RoBERTa-Large as the embedding generator for text data, as illustrated in Figure 3 (A). The training set is defined as \\(D = \\{(x_i, y_i)\\}_{i=1}^N\\), where N denotes the total number of texts, \\(x_i\\) represents each text in the dataset, and \\(y_i \\in [-1,1]\\) corresponds to the annotated sentiment polarity score.\nThe pre-trained RoBERTa tokenizer Tokenizer(.) processes each text \\(x_i\\) in the dataset, and then the tokenized text passes through 24 Transformer encoder blocks Encoder(.) to generate the final hidden state \\(H_i\\):\n\\[H_i = Encoder(Tokenizer(x_i)),\\]\nwhere \\(H_i \\in \\mathbb{R}^{1 \\times 1024}\\). This hidden state serves as input to task-specific heads. For sentiment polarity analysis, we implement a regression head (Figure 3 (B)) comprising two linear layers and a sigmoid activation function:\n\\[S_i = LL_2(\\sigma(LL_1(H_i))),\\]\nwhere \\(LL_1 : \\mathbb{R}^{1024} \\rightarrow \\mathbb{R}^{128}\\) is the first linear layer, \\(\\sigma(\\cdot)\\) denotes the sigmoid function, and \\(LL_2 : \\mathbb{R}^{128} \\rightarrow \\mathbb{R}\\) produces the final sentiment polarity score \\(S_i \\in [-1,1]\\).\nFor the regression task, we use Mean Squared Error (MSE) as the loss function, denoted as \\(L_r\\):\n\\[L_r = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\]\nwhere n denotes the number of texts texts in the training set, \\(\\hat{y}_r\\) represents the predicted polarity score, and \\(y_i\\) is the ground truth sentiment polarity score.\nStandard Learning. In sentiment analysis, particularly in data-limited scenarios, enhancing model performance is crucial. We introduce a multi-task learning approach. Specifically, we incorporate a classification task alongside the regression task to better capture the sentiment tendencies embedded in the text. This approach mitigates the impact of"}, {"title": "Dynamic Adaptive Optimization", "content": "In the context of multi-task learning with LLMs, two primary challenges arise: Task-level challenge: inter-task difficulty discrepancy. The loss functions associated with regression and classification tasks may exhibit significant disparities in their magnitudes. Consequently, this discrepancy can lead to the dominance of one task over others during the training process, potentially hindering the model's ability to effectively learn and generalize across all tasks. Data-level challenge: imbalanced data distribution. Within each training batch, the sample distribution can exhibit considerable variability, and the class distribution may be inherently imbalanced. As a result, the model may be susceptible to overfitting or underfitting for certain classes, compromising its overall performance and generalization capability.\nTo address these challenges, we propose a DAO module as shown in Figure 4 (A), which is a learnable neural network integrated into the multi-task learning framework.\nSolution #1: Inter-task Difficulty Discrepancy. Due to the different magnitudes of the loss functions for regression and classification tasks, directly summing them may cause one task to dominate the entire training process. To balance the contributions of different tasks, we introduce a gradient-based weighting method. Specifically, we define the total multi-task loss function \\(L_{mtl}\\) as follows:\n\\[L_{mtl} = \\lambda_r L_r + \\lambda_c L_c,\\]\nwhere \\(L_r\\) and \\(L_c\\) represent the losses of the regression and classification tasks at step t, respectively, \\(w_r^t\\) and \\(w_c^t\\) are the task-specific weights, and \\(\\lambda_r^t\\) and \\(\\lambda_c^t\\) are the gradient-based weighting coefficients.\nAt each training step, we compute the gradients of the regression and classification task losses:\n\\[g_r^t = \\nabla_{w_r^t} L_r^t, \\quad g_c^t = \\nabla_{w_c^t} L_c^t.\\]\nThen, we use the \\(L_2\\) norms of these gradients to compute the gradient-based weighting coefficients:\n\\[\\lambda_r^t = \\frac{|g_r^t|}{|g_r^t| + |g_c^t|}, \\quad \\lambda_c^t = \\frac{|g_c^t|}{|g_r^t| + |g_c^t|}.\\]\nThese gradient-based weighting coefficients are used to balance the learning progress of different tasks and scale them to similar magnitudes in the total multi-task loss function \\(L_{mtl}\\).\nSolution #2: Imbalanced Data Distribution. We introduce a regularization term and class-specific weights into the classification loss function. The regularization term \\(-\\alpha \\log p_k^t\\) encourages the model to pay more attention to minority classes with smaller sample proportions, while the class-specific weights \\(\\nu_k^t\\) are inversely proportional to the sample proportions \\(p_k^t\\):\n\\[\\nu_k^t = \\frac{1}{(p_k^t)^{\\beta}},\\]\nwhere \\(\\beta\\) is a parameter that controls the degree of class balancing.\nWe use D to denote the entire dataset and \\(D^t\\) to represent the batch of data sampled at time step t. For each batch \\(D^t\\), we calculate the proportion of samples \\(p_k^t\\) belonging to class k as:\n\\[p_k^t = \\frac{|D_k^t|}{|D^t|},\\]\nwhere \\(|D_k^t|\\) and \\(|D^t|\\) denote the number of samples in batch \\(D^t\\) belonging to class k and the total number of samples in batch \\(D^t\\), respectively.\nTo incorporate the regularization term \\(-\\alpha \\log p_k^t\\) and the class-specific weights \\(\\nu_k^t\\) into the classification loss function, we define the imbalanced data distribution loss \\(L_{imb}\\) conditioned on \\(D^t\\) as:\n\\[L_{imb}(D^t) = \\sum_{k=1}^B \\nu_k^t (p_{c,k}^t - \\alpha \\log p_k^t).\\]\nThis loss captures the sample distribution and class proportions specific to the current batch, allowing the model to dynamically adapt to the characteristics of each batch during training. We then incorporate the imbalanced data distribution loss \\(L_{imb}(D^t)\\) into the total multi-task loss function \\(L_{mtl}\\) from Challenge 1, along with the task-specific weights \\(w_r^t\\) and \\(w_c^t\\):\n\\[\\begin{aligned} L_{mtl}(D^t) &= w_r^t \\lambda_r^t L_r^t + w_c^t \\lambda_c^t (L_c^t + L_{imb}(D^t)) \\\\ &= w_r^t \\lambda_r^t L_r^t + w_c^t \\lambda_c^t (L_c^t + \\sum_{k=1}^B \\nu_k^t (p_{c,k}^t - \\alpha \\log p_k^t)), \\end{aligned}\\]\nwhere \\(\\lambda_r^t\\) and \\(\\lambda_c^t\\) are the gradient-based weighting coefficients, \\(\\nu_k^t\\) are the class-specific weights, and \\(L_k^t\\) is the classification loss for class k at time step t. The total multi-task loss \\(L_{mtl}\\) is now conditioned on the current batch \\(D^t\\), allowing the model to adapt to the specific characteristics of each batch during training.\nTo learn the hyperparameters \\(\\alpha\\) and \\(\\beta\\), we treat them as learnable parameters of the DAO Module and update them using gradient descent along with the other parameters of the module. During the backward pass, the gradients of the DAO Module loss \\(L_{mtl}(D^t)\\) with respect to \\(\\alpha\\) and \\(\\beta\\) are computed as follows:\n\\[\\frac{\\partial L_{mtl}(D^t)}{\\partial \\alpha} = - \\sum_{k=1}^B \\nu_k^t \\log p_k^t,\\]\nTo compute the gradient with respect to \\(\\beta\\), we apply the chain rule and substitute the expression for \\(\\nu_k^t\\):\n\\[\\begin{aligned} \\frac{\\partial L_{mtl}(D^t)}{\\partial \\beta} &= - \\sum_{k=1}^B \\frac{\\partial \\nu_k^t}{\\partial \\beta} (p_{c,k}^t - \\alpha \\log p_k^t) \\\\ &= \\sum_{k=1}^B \\alpha \\frac{\\partial p_k^t \\cdot \\beta}{\\partial \\beta} (p_{c,k}^t - \\alpha \\log p_k^t) \\end{aligned}.\\]\nThe hyperparameters \\(\\alpha\\) and \\(\\beta\\) are then updated using an optimizer such as Adam or AdamW.\nTo obtain the task-specific weights \\(w_r^t\\) and \\(w_c^t\\), we pass the gradient-weighted regression loss \\(\\lambda_r^t L_r^t\\), the gradient-weighted imbalanced data distribution loss \\(L_{imb}(D^t)\\), and the current batch \\(D^t\\) through the DAO Module:\n\\[w_r^t, w_c^t = DAO(\\lambda_r^t L_r^t, L_{imb}(D^t), D^t)\\]"}, {"title": "LORA", "content": "We implement LoRA (Hu et al. 2021) for parameter-efficient fine-tuning (PEFT) of the pre-trained ROBERTa-large model. LoRA injects trainable low-rank decomposition matrices into each layer, reducing parameters while maintaining performance.\nLORA introduces rank-r matrices \\(U \\in \\mathbb{R}^{d \\times r}\\) and \\(V \\in \\mathbb{R}^{r \\times d}\\) into each attention block and feed-forward network, as shown in Figure 4 (B). The figure illustrates how LORA is integrated into the Transformer encoder, with pretrained weights W remaining frozen while low-rank matrices are trained. The adjusted weight matrix \\(W'\\) during forward propagation is:\n\\[f(x) = (W + UV) \\cdot X + b,\\]\nwhere b is the bias, and X is the input as illustrated in Figure 4 (B). Only U and V are updated during training, significantly reducing trainable parameters and potentially accelerating training.\nIn traditional multi-task learning with constant weight, this approach can lead to suboptimal performance. Our proposed framework incorporates the DAO module that dynamically adjusts task weights based on their relative importance, gradients, and data characteristics during training. This batch-level dynamic adaptive loss addresses the limitations of constant-weight approaches by considering the"}, {"title": "Evaluation", "content": "Software and Hardware\nWe use Ubuntu 22.04, Python 3.9.19, PyTorch 2.3.1, PEFT 0.12.0, and CUDA 12.4, running on a system with 32GB RAM and an NVIDIA RTX 3090 Ti GPU with 24GB VRAM.\nSetup and Dataset\nBenchmark. We use diffrent benchmarks to verify our framework. We establish three benchmark: ROBERTa-Large (Liu et al. 2019) for regression, Twitter-RoBERTa-Large (Loureiro et al. 2023) for regression, and Twitter-RoBERTa-Large with constant-weight multi-task learning for regression and classification. Additionally, we propose two methods: Twitter-RoBERTa-Large with DAO for multi-task learning, and the multi-task learning framework using Twitter-ROBERTa-Large with DAO and LoRA.\nEvaluation Metrics. For the regression task, we utilize MSE, Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and the Coefficient of Determination (R2). For the classification task, we use ACC, weighted Precision, and weighted F1 score.\nDataset. We use a standard and customized financial text dataset that comprises 23,242 preprocessed news and analysis texts on EUR/USD exchange rates (Ding et al. 2024a). This dataset is randomly split into training and validation sets with a 9:1 ratio, and all experiments utilize these two identical datasets. Further information and supplementary details can be found in the Appendix."}, {"title": "Main Results", "content": "In this section, we will first introduce the hyperparameters and then analyze the experimental results.\nModel Hyperparameters. The training texts exceeding 512 tokens are truncated to the first 512 tokens. The model is trained with a batch size of 10 for 100 epochs, using the AdamW optimizer with a learning rate of 1e-5 and an epsilon value of 1e-8. The training process incorporates 1e2 warmup steps and implements cosine decay for learning rate scheduling. To ensure the reproducibility of the experiments, we set constant random seeds for Python (42), NumPy (42), and PyTorch (42) across all runs.\nDAO Module Hyperparameters. The optimizer is Adam with a learning rate of 0.001.\nLORA Parameters. Different LoRA configurations are applied in the experiments, with the rank values set to 8, 16, 32, 64, 128, 256, 384, and 512. Correspondingly, the scaling factor (alpha) for the LoRA matrices is set to one times the rank. The dropout rate for all LoRA configurations is constant at 0.05 to prevent overfitting.\nResults. Table 1 presents the performance metrics of RoBERTa-Large and Twitter-RoBERTa-Large on the task"}, {"title": "Ablation Studies", "content": "We ablate our method with DAO using different LoRA ranks, as shown in Table 3, which presents several intriguing observations. Increasing the rank consistently decreases MSE while improving ACC, Precision, and F1 score, suggesting that more trainable parameters enhance model performance. Table 4 illustrates that at a rank of 256, we achieve results comparable to a fully fine-tuned (FF) model with only 80.88% of the FF time. Using the same amount of time, MSE is slightly improved. At a rank of 384, the optimal performance is achieved in 53.00% of the FF time. When the rank increases to 512, we obtain results comparable to the FF model with only 36.85% of the FF time, and the optimal performance is obtained in 66.02% of the time, with MSE improving by 3.68% and ACC improving by 0.12%. Due to CUDA out of memory issues, the batch size was set to 7 in the experiment with a rank of 384, and to 10 in the experiment with a rank of 512."}, {"title": "Related Work", "content": "LLMS NLP advancements lead to the widespread application of LLMs in sentiment analysis tasks. ChatGPT shows significant potential in automating student feedback analysis, outperforming traditional deep learning models (Shaikh et al. 2023). Pre-trained models like BERT achieve state-of-the-arts results by learning contextual word representations (Liao et al. 2021). In Arabic sentiment analysis, transformer-based models like RoBERTa and XLNet push boundaries despite language complexities (Alduailej and Alothaim 2022). Krugmann and Hartmann (2024) reveals that GPT-3.5, GPT-4, and Llama 2 can compete with and sometimes surpass traditional transfer learning methods in sentiment analysis. Carneros-Prado et al. (2023) highlights the versatility of pre-trained LLMs like GPT-3.5 in diverse NLP applications, including emotion recognition. However, fully fine-tuning these LLMs for specific tasks remains computationally expensive and time-consuming, posing challenges for both academia and industry.\nPEFT To address computational challenges, researchers introduce PEFT methods for LLMs. Hu et al. (2023) presents LLM-Adapters, integrating adapters into LLMs and achieving comparable performance to powerful 175B parameter models using only 7B parameters in zero-shot tasks. Lei et al. (2023) introduces Conditional Adapters (CODA), which adds sparse activation and new parameters to pre-trained models for efficient knowledge transfer, significantly speeding up inference. Hu et al. (2021) proposes LoRA, which injects trainable low-rank decomposition matrices into each Transformer layer, reducing parameters while maintaining performance on par with full fine-tuning."}, {"title": "Conclusion", "content": "In this work, we propose a multi-task learning framework with a DAO module for LLM-based sentiment analysis. The DAO module dynamically adjusts task weights based on their relative importance and data characteristics, addressing inter-task difficulty and data imbalance issues. This plug-and-play module can be seamlessly integrated into existing models, enhancing their adaptability to diverse tasks and datasets. Combined with LoRA for efficient fine-tuning, our approach achieves state-of-the-art performance in financial sentiment analysis, significantly improving MSE and accuracy over previous methods.\nIn the future work, we will explore the use of data-aware classification tasks to enhance the performance of multi-task learning with DAO module under data-limited scenarios."}, {"title": "Appendix", "content": "In this work, the text dataset comes from another study (Ding et al. 2024a). Specifically, the dataset is collected from investing.com and forexempire.com, focusing on the EUR/USD exchange rate. The dataset spans from February 6, 2016, to January 19, 2024, encompassing all accessible data on these platforms, resulting in a total of 35,427 records. To address the presence of noise and information irrelevant to the target exchange rate, we use ChatGPT-4.0 and prompt engineering techniques to filter the raw dataset. Further analysis reveals that typically only individual paragraphs or multiple sentences within articles directly relate to the EUR/USD exchange rate, likely catering to readers' diverse interests. To extract the most relevant parts and refine the dataset, we process the text data using ChatGPT-4.0, yielding a final text dataset comprising 23,242 records.\nSentiment polarity annotation in exchange rate texts is particularly complex because such texts are often filled with professional terminology, implied emotions related to market conditions, and subtle variations across industries. Moreover, as exchange rate issues involve two countries, significant positive or negative news about one country can have"}, {"title": "Evaluation Metrics", "content": "For the sentiment score regression task, where the model predicts a continuous sentiment polarity score, we employ the following metrics:\n\u2022 Mean Squared Error (MSE) calculates the average squared differences between the predicted sentiment polarity scores (yi) and the annotated sentiment polarity scores (\\(y_i\\)) in the test set. MSE provides a measure of the model's accuracy in predicting the exact sentiment scores. It is defined as:\n\\[MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\]\nwhere n is the number of texts in the test set.\n\u2022 Mean Absolute Error (MAE) measures the average magnitude of the absolute errors between the predicted and annotated sentiment polarity scores, ignoring their direction. MAE helps in understanding the average error magnitude and is less sensitive to outliers compared to MSE. It is formulated as:\n\\[MAE = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|\\]\n\u2022 Root Mean Squared Error (RMSE) is the square root of MSE and provides the error magnitude in the same units as the sentiment polarity scores. RMSE is more interpretable than MSE and is calculated as:\n\\[RMSE = \\sqrt{MSE}\\]\n\u2022 R-squared (\\(R^2\\)) indicates the proportion of variance in the sentiment polarity scores that can be explained by the model's predictions. It provides a measure of how well"}, {"title": "Related Work", "content": "LORA\nMulti-task Learning with constant Weights\nSentiment Analysis Sentiment analysis is widely used across various domains. In the financial sector, Li, Shang, and Wang (2019) and Correia, Madureira, and Bernardino (2022) employ these methods to improve forecasting accuracy for crude oil prices and stock market movements, respectively. Extending this approach, Qian et al. (2022) examines NFT-related tweets to correlate public sentiment with market trends. Beyond finance, Wen et al. (2024) utilizes text mining on online reviews to assess product competitiveness, while Garner et al. (2022) applies similar techniques to analyze factors influencing consumer happiness in travel experiences. These studies collectively underscore the versatility and effectiveness of text analysis methods in extracting valuable insights from unstructured data across diverse fields.\nLexicon-based Methods Lexicon-based methods for sentiment analysis, while effective across various domains, face limitations due to their reliance on predefined sentiment dictionaries. Studies demonstrate their application in diverse fields. Barik and Misra (2024) and Liu et al. (2020) develop models for multi-domain sentiment analysis and online pharmacy reviews, respectively. During the COVID-19 pandemic, Khan et al. (2021), Marcec and Likic (2022), and Samaras, Garc\u00eda-Barriocanal, and Sicilia (2023) apply these methods to analyze public sentiment through social media data. These studies collectively highlight the versatility of lexicon-based approaches while acknowledging potential constraints in lexicon coverage and quality.\nTraditional Methods Recent studies explore various machine learning approaches for sentiment analysis across diverse domains. Traditional algorithms like SVM, Random Forest, and Na\u00efve Bayes are applied by Bengesi et al. (2023) for analyzing public sentiment on disease outbreaks, Ranibaran et al. (2021) for stock price prediction, and Asif et al. (2020) for multilingual extremism text classification. Naresh and Venkata Krishna (2021) proposes a hybrid algorithm for Twitter sentiment analysis, while Gopi et al. (2023) and"}]}