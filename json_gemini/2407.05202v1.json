{"title": "HARNESSING THE POWER OF LLMS: AUTOMATING UNIT TEST\nGENERATION FOR HIGH-PERFORMANCE COMPUTING", "authors": ["Rabimba Karanjai", "Aftab Hussain", "Md Rafiqul Islam Rabin", "Lei Xu", "Weidong Shi", "Mohammad Amin Alipour"], "abstract": "Unit testing is a standard practice in software engineering and is critical for ensuring software quality.\nHowever, for parallel and high-performance computing software, especially scientific computing\napplications, unit testing is not widely implemented. Compared with typical commercial software,\nhigh performance software usually have a smaller user base, and they are diverse and usually involve\ncomplex logic. These characteristics create several challenges for conducting unit testing for parallel\nand high performance software. On one hand, it is economically expensive to have a dedicated\ntesting team to do unit testing considering the number of users. On the other hand, it is hard for\na quality engineer without domain knowledge to design effective unit testings. Similarly, existing\nautomated unit testing tools are usually not effective for such software. Therefore, it is vital to devise\nan automated method for generating unit testing cases for parallel and high performance software,\nwhich considers the unique features of these software, including complex logic and sophisticated\nparallel processing techniques. Recently, large language models (LLMs) have attracted more attention\nand are believed to be a powerful tool for coding and testing, but its application in producing unit\ntests for parallel and high performance applications remains uncertain. To fill this gap, we explore\nthe capabilities of two well-known generative models, Davinci (text-davinci-002) and ChatGPT\n(gpt-3.5-turbo), in crafting unit testing cases for parallel and high performance software. Specifically,\nwe proposed novel ways to utilize LLMs to develop unit testing cases for high performance software\nwith C++ parallel programs and assessed their effectiveness on extensive OpenMP/MPI projects. Our\nfindings indicate that in the context of parallel programming, LLMs can create unit testing cases that\nare mostly syntactically correct and offer substantial coverage, while they exhibit some limitations\nlike repetitive assertions and blank test cases.", "sections": [{"title": "1 Introduction", "content": "Fueled by advancements in deep learning, Large Language Models (LLMs) have surged in development. Their versatility\nstretches across various applications, with a prominent one being software code generation. LLMs specializing in this\ndomain are trained on vast datasets of source code gleaned from software repositories like GitHub and programming\ncommunities like Stack Overflow. OpenAI Codex Chen et al. [2021a], Meta's Code Llama (a code-centric variant"}, {"title": "2 Motivation", "content": "Large Language Models (LLMs) can struggle with understanding and writing OpenMP and MPI unit tests due to the\ninherent complexity and domain-specific nature of parallel programming. These frameworks introduce concepts like\nthread synchronization, data distribution, and inter-process communication, which are not easily captured by the general\nlanguage patterns LLMs are trained on. Furthermore, OpenMP and MPI code often involves low-level system calls and\nplatform-specific optimizations, posing additional challenges for LLMs trained primarily on high-level natural language\ndata. This difficulty is reflected in research highlighting the limitations of LLMs in code generation tasks involving\nspecialized libraries and frameworks Chen et al. [2021b], Wang et al. [2023].\nUnit testing is crucial for OpenMP and MPI codes due to their inherent complexity and susceptibility to subtle errors\narising from race conditions, deadlocks, and non-deterministic behavior Wittwer [2006]. Unlike non-high-performance\ncodes, where errors often manifest as incorrect outputs, parallel programming errors can be silent and sporadic, making\nthem difficult to diagnose and reproduce Pacheco. Comprehensive unit tests provide a safety net by isolating individual\ncode units, enabling developers to verify their correctness and pinpoint issues early in the development process Gropp\net al. [1999]. Additionally, unit tests serve as regression tests, ensuring that code changes do not inadvertently introduce\nnew errors or break existing functionality Beck [2022]. This is especially important for OpenMP and MPI codes, which\noften undergo frequent modifications and optimizations to improve performance and scalability.\nGiven the challenges of manual test creation for OpenMP and MPI code, automated test generation emerges as a critical\nsolution. Automated tools can systematically explore the vast parameter space of parallel programs, uncovering corner\ncases and potential errors that may be overlooked by human developers Olsthoorn [2022], Gopalakrishnan et al.. By\ngenerating diverse and comprehensive test suites, these tools can significantly enhance the reliability and robustness\nof parallel applications, reducing the risk of costly failures in production environments Andrews and Olsson [1993].\nAdditionally, automated test generation can accelerate the development process by relieving developers from the tedious"}, {"title": "3 Methodology", "content": "The majority of the previous studies on code generation with the LLMs have primarily utilized the datasets from\nHumanEval Chen et al. [2021a] and Humaneval X Zheng et al. [2023]. However, these two datasets are not suitable for\nour purpose because they lack parallel and high performance related C/C++ code that utilizes OpenMP.\nTo overcome this challenge, we thoroughly review a large number of renowned high performance computing projects\nfrom GitHub spanning various sub-fields and select a subset from them to serve as the benchmark of the newly developed\nLLM-based unit test generation. Our selection criteria are as follows:\n\u2022 The projects should be popular (e.g., starred/forked count) so that we can hypothesize that they have acceptable\nquality human-written code.\n\u2022 The code repository should have well-documented unit tests written in English.\n\u2022 The unit tests should all be standalone so that code generation does not suffer from dependency problems.\n\u2022 Unit tests themselves should have a proper structure, so that while generating code using the LLMs, we can\nguide the generation process for better outcome.\nFinally, 7 projects are chosen for data collection, which are summarized in Table 1."}, {"title": "3.2 Test Case Generation", "content": "We investigated the following research questions to determine how well the large language models (LLMs) can generate\nunit tests for the parallel programming use cases. We followed the approach of Siddiq et al. Siddiq et al. [2023], who\nalso explored the potential of using LLMs for unit test cases with the HumanEval and EvoSuite datasets. However, our\nfocus is on the parallel and high performance domain. We explain our test case generation process in detail in Section 4.\nWe primarily aim to answer the following research questions through our work."}, {"title": "Research Question 1", "content": "How effectively can LLMs generate unit tests for parallel and high performance programs?\nWe choose two LLMs as the backend for unit test generation for the parallel and high performance projects listed in\nTable 1, Davinci (text-davinci-002) Godoy et al. [2023] and ChatGPT (gpt-3.5-turbo) Dong et al. [2023], OpenAI\n[2022]. Both are developed by OpenAI and corresponding APIs are used to interact with them. For each test, we select\n10 candidates with three different temperatures 0, 0.2, and 0.4. Here temperature is a hyper-parameter that regulates the\nrandomness, or creativity, of the LLM's responses. Value 0 will lead to more predictable results (i.e., the response is\ndeterministic and less creative), and the other two temperatures are selected to explore potential positive impacts on\nunit test generation. It is possible that a higher temperature may lead to worse performance. The test case generation\nmethod and details on the different considerations are provided in Section 4."}, {"title": "Research Question 2", "content": "Does the template guided code generation improve the unit tests?\nOur assumption is that the programmers will employ specific prompts similar to \u201cCan you generate a unit test for a code\"\nor something along that line. Often these prompts will encompass a code function to be tested. These often are sent as\n\"Write a Unit Test to test \u00absome task\u00bb functions as needed\". Here, the function being tested serves as the backdrop\nfor the directive. This backdrop, or context, can encompass various code components. In this study, we delve into the\ninfluence of these distinct components on the unit tests produced by controlling the dataset in different ways, including:\n\u2022 Providing the full code context.\n\u2022 Omitting the context entirely.\n\u2022 Solely presenting the associated libraries for guidance as a code template.\nThis approach may shed light on the efficacy of LLMs when steered by specific input cues. We gauged their efficiency\nby examining compilation success rates, the breadth of code coverage, and the aggregate of accurate unit tests produced."}, {"title": "Research Question 3", "content": "Do the generated Unit Tests also cover hierarchical parallelism checks for the original source code?\nHierarchical parallelism refers to the organization and execution of parallel tasks in a structured and layered manner. In\nparallel computing, this approach allows for the efficient decomposition of complex programs into more manageable\nsub-tasks, which can then be executed concurrently across multiple levels of parallelism. OpenMP, a popular parallel\nprogramming model, supports the nesting of parallel regions Nes [2021], enabling developers to define parallel tasks\nwithin already parallelized sections of code. This nested parallelism is crucial for harnessing the full potential of modern\nmulti-core and multi-threaded processors, especially when dealing with multifaceted computational problems. However,\nwith the increased complexity introduced by nested regions, unit tests become paramount. These tests ensure each\nindividual component or function operates correctly in isolation, helping to identify and rectify issues that might arise\ndue to the intricacies of hierarchical parallelism.\nWe further evaluate if the unit tests that we have generated can check whether these parallelism implementations are\npresent in the tested code or not, and whether they are implemented in the correct way."}, {"title": "4 Test Case Generation with LLMS", "content": "In this section, we investigate three distinct methods for leveraging large language models (LLMs) in generating unit\ntests for parallel and high-performance computing programs. We assess template-based generation, where existing\nunit test code serves as a structural blueprint for the LLM; contextual generation, where the LLM is informed by the\ncodebase context to create relevant tests; and guided generation, involving iterative interaction with the LLM through\nprompts and feedback."}, {"title": "4.1 Out of the Box Test Case Generation", "content": "The out-of-the-box (OOB) approach is the most straightforward one to generate test cases for parallel and high\nperformance programs. With the OOB approach, we do not provide any contextual information to the LLMs, and\ninstead just rely on the test case patterns for code generation. For most of cases, the unit test case generation can be\ntreated as a special type of code generation problem with specific expectations. These expectations were later used to\nevaluate how good or bad the code generation task was completed in the context of unit tests.\nFor unit tests, each of the prompts that we used was derived by giving a candidate code to the API. Even though both the\nevaluated LLMs are capable of generating code, the number of tokens that they can support and the memory capability\nis different. Due to these differences, we used a slightly different approach for unit test generation applying the LLMs.\nTo access both models, we used OpenAI's APIs 2. The APIs provide a convenient way to access and evaluate the\nmodels, as well as retrieve the outputs. For experiment orchestration, we used Langchain Chase [2022] to load the test\ncases into memories for the conversation with the OpenAI models. Each of the test case generations used a pre-built\nagent with the actual program loaded into memory. The agents of Langchain have been used to invoke the API to"}, {"title": "4.2 Code Template Guided Generation", "content": "Even though OOB generation already guided the LLMs towards using certain types of libraries and variables, it still left\nthe LLMs up for open interpretation about what a test case would be. We also tested the models' capability in a more\ncontrolled and guided way. The goal was to investigate if such an approach would result in better test coverage.\nFor a template-guided generation, one can formalize the unit test generation model as a function F. Given a prompt\nx, containing the original code for which the unit test needs to be written, the LLM can complete x to get y = F(x),\nwhere y is the response to the provided prompt x. This makes the entire code prompt (input prompt containing source\ncode along with code template from memory together with the response) as a grouping of A1 + A2, where A1 is the\ninput prompt x, and A2 is the output y given A1.\nWe can generate multiple outputs (completions) by using different sampling strategies: Random sampling and Beam\nsearch. Random sampling is a simple approach that selects the next token at random, and Beam search is a more\nsophisticated approach that takes into account the probability of each token Deshpande et al. [2019], Wang et al. [2017]."}, {"title": "4.2.1 Contextual Code Generation", "content": "We created the prompts as part of the Langchain agents and ran them 10 times with new memory to generate 10 separate\nresponses for a specific method. Each prompt pattern follows a similar format to classic software patterns, but with"}, {"title": "4.2.2 Analysis", "content": "The code generation can be illustrated as depicted in Figure 1. For analysis of the generated unit tests, we look for code\ncompilability, code correctness, and parallelism detection on the unit tests. For our evaluation, it is imperative that we\nuse the existing human-written unit tests as the gold standard and compare them with the generated ones."}, {"title": "4.2.3 Compilation Test", "content": "We used the existing makefiles of the projects to take the generated unit tests as input as a plug-and-play method so\nthat the existing infrastructure was leveraged to evaluate whether the unit test succeeds or not. The test code which\ndid not succeed was then inspected to prune out any kind of non-coding error that might have prevented the test from\nsuccessful execution. We observed that some of the compilation errors were caused by syntax problems that could be\nfixed by a heuristic-based post-processing step after unit test generation. More specifically, we have noticed certain\npatterns in the generated test cases:\n\u2022 Generate an extra test class that is incomplete. Thus producing unused code.\n\u2022 Include natural language explanations before and/or after the code. This was primarily noticed in code\ncompletion-based tasks, where we were guiding the output to become a code-infilling task.\n\u2022 Repeat the code under test and/or the prompt.\n\u2022 Change libraries and declarations or at times remove them.\n\u2022 Generate incomplete unit tests after it reaches its token size limit.\nWe mitigated some of these by post-processing the code with a rule-based fixer that we have created. For the token size\nlimitation, we attempted to utilize Langchain memory to pass on the truncated part of the code to send as a reference to\ngenerate the remaining part, essentially making it a code completion task. For code comments being introduced, we\nremoved them as much as possible using rule-based heuristics. This was more prevalent in the Davinci Godoy et al.\n2023] model."}, {"title": "5 Evaluation", "content": "For evaluation of the generated test code. We ran each test case through a series of tests, each designed to check\ndifferent aspects of the generated code. The first evaluation was to find out how many of them can be compiled through\nthe OOB method, and how many of those that have failed can be \"fixed\" using the contextual inputs. The series of tests\nthat have been applied to evaluate the generated code are described in the following subsections."}, {"title": "5.1 Test Coverage", "content": "We evaluated the line coverage and branch coverage of the generated unit tests and compared them with the original\nhuman-written unit tests from our selected project repositories in Table 1.\n\u2022 Line Coverage: In line coverage, we measure how many lines of the code from the original source code have\nbeen executed by the unit test out of the total number of lines present in the code.\nLine Coverage = $\\frac{\\text{No. of lines executed}}{\\text{No. of lines in source code}} \\times 100$   (1)\n\u2022 Branch Coverage: It is a software testing metric that measures how many branches in a program that are\nexecuted by a test suite. It is calculated by dividing the number of branches executed by the total number of\nbranches in a program. A branch is a point in the program where the control flow can go in one of two or more\ndirections. For example, an if statement has two branches: one for the true condition and one for the false\ncondition.\nBranch Coverage = $\\frac{\\text{No. of branches executed}}{\\text{Total no. of branches}} \\times 100$   (2)\n\u2022 Assuming the code has been implemented correctly, we evaluated whether the generated unit tests check the\ncorrect functions for correct outputs.\nFor each of these, we used certain pre-defined parameters as our indicators. For the line and branch coverages, we used\nthe open-source tool OpenCppCoverage Ope [2023], which provides the information needed for coverage evaluation."}, {"title": "5.2 RQ1: Parallel and High Performance Program Unit Test Generation using the LLMS", "content": "We analyzed the generated unit tests for parallel and high performance programs according to four dimensions following\nSiddiq et al. Siddiq et al. [2023]:"}, {"title": "5.3 Compilation Status", "content": "Since our code repositories were carefully chosen to have good quality unit tests, we could utilize the Makefiles for each\nproject. We used these Makefiles to just change the target unit tests from human written ones to the LLM generated\nones for our compilation check. This also helped us to log compilation status for successful and unsuccessful compiles\nfor post processing and data analysis. As shown in the second column of Table 2, less than 50% of the generated unit\ntests are compilable across all the studied models. Among the compilable tests, the ratio was noticeably lower when we\njust tried to generate using the partial test case and the name as input to the model."}, {"title": "5.4 Test Correctness", "content": "We used the following two metrics for measuring the correctness of a unit test, and evaluation results are reported in the\nTable 3.\n\u2022 Fully Correct: The unit test has a success rate of 100%, meaning that all of its test methods have passed.\n\u2022 Somewhat Correct: The unit test has at least one passing test method.\nThese results from Table 3 show that even though we could not generate correct unit tests through any of the models on\na large scale, they are still useful for generating some correct ones. We also noticed that increasing code context by"}, {"title": "5.5 Test Coverage", "content": "We assessed the line coverage and branch coverage of the generated unit tests and compared the results with the\ncoverage of the original tests. We used gcov gco [2023] to measure line and branch coverage. Line and branch coverage\nare two metrics used to measure the effectiveness of unit tests. Line coverage measures the percentage of lines of code\nthat are executed by the unit tests. Branch coverage measures the percentage of branches in the code that are executed\nby the unit tests. The results have been highlighted below in Table 4."}, {"title": "5.6 Quality", "content": "We used test smells as the indicator for code quality. Test smells can be applied for defining quality because they can\nindicate potential problems with the unit tests. These problems can make it difficult to maintain the tests, and they\ncan also lead to false positives and negatives. As a result, it is important to identify and address test smells as early as\npossible.\nTest smells are code smells that can indicate problems with unit tests. They can be caused by a variety of factors, such\nas poor design, bad practices, or simply a lack of attention to details. Following Siddiq et al. Siddiq et al. [2023], we\nstudied the following test smells in the generated parallel program unit tests by the LLMs:\n\u2022 Assertion Roulette (AR): This occurs when the unit test randomly selects values to assert against.\n\u2022 Conditional Logic Test (CLT): This occurs when the unit test checks the output of a conditional statement.\n\u2022 Constructor Initialization (CI): This occurs when the unit test checks the output of a constructor.\n\u2022 Empty Test (EM): This occurs when the unit test does not actually test anything.\n\u2022 Exception Handling (EH): This occurs when the unit test checks the output of exception handling code.\n\u2022 Redundant Print (RP): This occurs when the unit test prints the same output multiple times."}, {"title": "5.7 Parallelism", "content": "To evaluate whether the generated parallel and high performance program unit tests cover the original source code for\nparallelism we ensured that they have two categories of tests: 1) memory copy, reduction, and atomic, and 2) Multiple\ndatatypes: float, double<complex>.\nTests should be i. Self-contained, ii. Return 0 on success and not 0 on failure, and iii. Independent of the number of\nteams/threads. We used open source tool OvOovo [2023] to compare the results of our LLM-generated unit tests to see\nif they match."}, {"title": "6 Summary & Discussion", "content": "In this section, we summarize the results of our studies and discuss the implications of the findings.\nWriting unit tests for OpenMP and MPI codes is distinctively different from testing regular C++ code due to the inherent\nparallel and distributed nature of these frameworks. With standard C++ code, one typically deals with a sequential\nprogram flow, and the tests are designed to evaluate specific functionalities in that flow. However, with OpenMP (for\nshared-memory parallelism) and MPI (for distributed-memory parallelism), the challenge not only includes ensuring\nfunctional correctness but also assessing the proper synchronization, data sharing, and communication among threads\nor processes. For OpenMP, tests need to check for race conditions, deadlocks, and proper handling of shared and\nprivate variables. In the MPI context, the challenges lie in testing message passing correctness, ensuring that data is\ncommunicated correctly between processes, and managing deadlocks due to unmatched sends and receives. Validation\ninvolves checking that the code scales correctly with increasing number of threads or processes, that load balancing is\nachieved if applicable, and that there is no unintended data leakage or corruption across the parallel regions or during\ninter-process communications. All these make it challenging for LLMs trained on general purpose code to properly\nencode the knowledge needed to write test cases for HPC code.\nIn our work we have noticed that the LLMs can generate correct unit tests both without any guidance and with\nguidance for the parallel code. Often however the test code did not explicitly check parallelism. So even though the\ntests passed the correctness check, for functional purposes, they are inferior to the human written tests.\nLLMs tend to generate better unit tests and increase code coverage when provided with the source code as context\nand code template as guidance. Essentially reducing the task to be a code infilling task.\nLLMs may not take into account libraries often leading to missed library or variable name inclusion in the actual\ncode but not having correct reference, thus making the code failed to compile.\nLLMs have a tendency to introduce non code segments into the code as well as unit tests. These mostly were\nremoved using a post processing step during our evaluation.\nLLMs tend to hallucinate when provided with \"too much\" context. One interesting observation was that when we\nhave built our agents and memories using Langchain, often sending larger source code and code templates as guidance\nresulted in getting an output code with nonexistent types methods etc.\nThe Large language models (LLMs) often produce code that includes non-existent types, methods, and other constructs,\nwhich can result in compilation errors and related issues. For instance, in the case of Codex, it generates inputs with\ntypes like Tuple, Pair, Triple, Quad, and Quint that do not exist.\nThese code artifacts are not specific to high performance software or unit tests though. However the code correctness\nimprovements using template based guided approach shows that the approach can be a good way to improve code\nsynthesis for unit test case generations. Since unite tests are not self-contained code generation task, rather a specific\ntask based on pre-defined existing code, we can use code interpretation techniques to come up with better guidance and\nenhancement to improve the quality of the generated test cases."}, {"title": "6.1 Concerns", "content": "In our investigation, We have used a version of Davinci (text-davinci-002) that was trained on data up to September\n2021. This means that the tests in our benchmarks were also part of its training set. This raises the possibility that the\nmodel may be memorizing the existing tests, rather than generating new ones. This could limit the model's usefulness\nfor code projects that it was not trained on.\nOne approach to investigate the potential effects of memorization is to measure similarity through edit distance between\nthe generated code and the existing one. Prior work has shown that code plagiarism Lemieux et al. [2023] or clone\ndetection Schleimer et al. [2003] techniques might not be effective at identifying LLM code memorization."}, {"title": "7 Threats to Validity", "content": "Although our evaluation involved a comprehensive scale that surpasses the previous test generation approaches such as\nBarei\u00df et al. [2022], it is important to note that our results are derived from an analysis of 7 parellel and high performance\nprojects, which may have limitation in terms of generalizability to other high performance parallel program codebases.\nTo address this concern, we have taken several measures: (i) diversifying the parallel code repos to incorporate different\nkinds of hardware used in those and different domains; (ii) choosing newer test cases that should not have been present\nin the original training dataset of the LLMs; and (iii) assessing the similarity between the generated tests and the\nexisting tests. Despite these improvement efforts, it is crucial to recognize the potential limitation and consider further\ninvestigations across a wider range of codebases to achieve better generalizability."}, {"title": "8 Related Work", "content": "The related work can be broadly categorized into the following areas.\nResearch in parallel and high performance software unit testing Although TDD (Test-driven development) and\nunit testing framework are well established practice in software engineering, adopting them in the context of high-\nperformance computing environments often faces unique requirements and challenges. High performance computing\ndevelopers may encounter problems that differ from other application areas of programming Nanthaamornphong\nand Carver [2018]. For instance, a specific need in parallel and high performance computing is to develop tools for\nsupporting unit tests that can be integrated with certain legacy HPC code Hovy and Kunkel [2016]. Another effort is to\ncreate unit testing framework tailored for the HPC environments and applications, for instance extension of unit testing\nframework to support parallelization techniques in high performance applications such as OpenMP and MPI.\nEvaluation of the LLMs in code generation Another area of related work is evaluation of the LLMs for code\ngeneration Chen et al. [2021a], Xu et al. [2022]. A particular topic that has attracted large amount of attention from the\nresearch community is to evaluate the code quality and security aspect of the LLM generated code Pearce et al. [2021],\nPerry et al. [2022]. For instance, in Yetistiren et al. [2022], the authors have assessed the quality of the LLM generated\ncode from multiple aspects such as compilation, functional correctness and code efficiency. Researchers also conducted\nstudies of the bugs in the LLM generated code, which shows that there is a high chance that the LLMs may introduce\ndifficult to detect bugs in the generated code Jesse et al. [2023]. Besides bugs in the LLM generated code, researchers\nhave scrutinized and assessed the code generated by the LLMs for security vulnerabilities Pearce et al. [2021]. Quality\nof the LLM generated code is also investigated from the reliability and robustness perspective Zhong and Wang [2023].\nApplication of the LLMs for unit testing Last but not the least, researchers have explored and evaluated applicability\nof the LLMs for unit test generation Yuan et al. [2023]. For instance, Barei\u00df et al. Barei\u00df et al. [2022] examined the\nperformance of Codex in Java code test generation. They applied a few-shot learning approach, where the model was\nprovided with a function to be tested, an example of another function, and an associated test. This allowed the model to\nlearn the expected structure of a test. In another study Tufano et al. [2020], the authors evaluated test coverage of a\nBART transformer model fine-tuned on a training dataset consisting of functions and tests.\nHigh-Performance Computing (HPC) has become a pivotal tool for scientific and engineering advancements, enabling\nthe resolution of intricate problems that demand substantial computational prowess D'Ambra et al. [2003]. As these\nchallenges intensify, the imperative for robust and efficient software solutions becomes evident. This realization\nhas spurred a growing inclination towards the incorporation of established software engineering practices, such as\nTest-Driven Development (TDD) and unit testing, within the high performance realm Rilee and Clune [2014]. However,\nthe transition isn't straightforward. The unique requirements of parallel and high performance domain, like managing\nvast datasets, safeguarding data integrity across distributed systems, and optimizing for performance on dedicated\nhardware, pose distinct challenges Cartier-Michaud et al. [2013].\nThis work distinguishes from all the related work with its focus on high performance related applications and parallel\ncode. To the best of our knowledge, this is the first step investigating and evaluating the LLMs in the context of parallel\nand high performance programming and testing. Although the study only scratches the surface of this potentially rich\narea of endeavor, it shows initial results that could pave the way for further research and investigation."}, {"title": "9 Conclusion", "content": "Testing parallel and high performance applications is often considered to be difficult. Prior efforts in parallel program\ntesting have focused on generating test cases for legacy high performance code, testing support for parallel programs.\nManually crafted high performance computing test cases are not only laborious to develop but also time-consuming.\nThe recent advances of code-generating Large Language Models (LLMs) have the potential to revolutionize the field\nof software testing. The described study has filled a research gap to investigate the feasibility and effectiveness of\napplying the generative models (e.g., Davinci, and ChatGPT) for generating C++ parallel program test cases. We have\nreported the early discoveries that show potential benefits in coverage and test smell. The study paves a road for further\nexploration in this exciting and important new research direction."}]}