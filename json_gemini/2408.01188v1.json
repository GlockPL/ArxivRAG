{"title": "Multi-Objective Deep Reinforcement Learning Optimisation in Autonomous Systems", "authors": ["Juan C. Rosero", "Nicol\u00e1s Cardozo", "Ivana Dusparic"], "abstract": "Reinforcement Learning (RL) is used extensively in Autonomous Systems (AS) as it enables learning at runtime without the need for a model of the environment or predefined actions. However, most applications of RL in AS, such as those based on Q-learning, can only optimize one objective, making it necessary in multi-objective systems to combine multiple objectives in a single objective function with predefined weights. A number of Multi-Objective Reinforcement Learning (MORL) techniques exist but they have mostly been applied in RL benchmarks rather than real-world AS systems. In this work, we use a MORL technique called Deep W-Learning (DWN) and apply it to the Emergent Web Servers exemplar, a self-adaptive server, to find the optimal configuration for runtime performance optimization. We compare DWN to two single-objective optimization implementations: e-greedy algorithm and Deep Q-Networks. Our initial evaluation shows that DWN optimizes multiple objectives simultaneously with similar results than DQN and e-greedy approaches, having a better performance for some metrics, and avoids issues associated with combining multiple objectives into a single utility function.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous Systems (AS) are designed to operate by continuously adapting to their environment to maintain optimal performance. Self-adaptive Systems (SAS) have the capability to continuously monitor their environment, and self-adapt accordingly, finding more suitable behavior configurations for different environment conditions and situations to maintain an optimal performance [0].\nIn SAS, predefined actions and action sequences often do not perform optimally due to the dynamic nature of real-world environments, making runtime learning essential for optimizing behavior in dynamic or unpredictable scenarios. Reinforcement Learning (RL) is frequently used in these scenarios because it allows systems to adapt at run time. For example, in the domain of web servers and cloud computing, the Fuzzy Q-learning [0] RL technique has been used in an automated cloud scaling systems, to minimize the number of response time violations and the amount of resources acquired. Other examples include a combination of both fuzzy Q-learning and SARSA [0], used to manage auto-scaling, based on the workload, response time and the number of virtual machines, or multi-objective planning and model checking to maximize revenue, minimize cost, and keep server response time below a threshold on a cloud based load balancing system [0].\nHowever, most exiting approaches, use a single optimization function, in which multiple objectives are combined at design time into a single one. Truly multi-objective approaches are less frequent. For example, a model based on clustering and situation-driven optimization aims to enhance routing systems by optimizing trip overhead and route cost [0]. This approach focuses on improving both trip overhead and route cost si- multaneously throughout the use of multi-objective Bayesian optimization and multi-objective genetic algorithms. While combining multiple objectives into a single utility function is often used due to its simplicity, Truly Multi-objective Reinforcement Learning (MORL) is crucial for adapting to changing environments and priorities, as adaptability is a lot more challenging to implement on single objective aggrega- tions due to the need to make the combination of multiple objectives at the design phase, making them static.\nIn this paper we illustrate the use of RL to implement multi- objective optimization in SAS, using Deep W-learning [0] (DWN). DWN is an extension of a multi-objective tabular RL approach called W-learning [0], which integrates W-learning with neural networks. While tabular W-learning has been suc- cessfully applied in multiple examples of SAS (e.g., context- aware adaptation [0], and motorway traffic optimization [0]), DWN has only been applied to RL benchmarks so far, such as, mountain car and deep sea treasure, but no real world application has been implemented.\nThe contribution of this paper is the first implementation of the DWN Multi-Objective Deep Reinforcement Learning algorithm in an autonomous system. Specifically, we apply DWN to the Emergent Web Server (EWS) self-adaptive exem- plar [0], which dynamically transitions server configurations to find the optimal setup. We evaluate our multi-objective approach against the baseline e-greedy algorithm in EWS and a Deep Q-Networks (DQN) approach, optimizing for average response time and configuration cost. The full implementations of both DWN and DQN applied to EWS are available in our repository.\u00b9 Our initial results show DWN outperforms"}, {"title": "A. Deep Q-learning", "content": "The goal of RL is to find the optimal policy \u03c0* for an environment That is characterized by a Markov Decision Process (MDP). An MDP comprises a state space S, an action space A, a reward function R, and state transition probability P. The optimal policy \u03c0* maximizes long-term rewards over short-term rewards. The DQN algorithm, extends Q-learning using deep learning. In Q-learning, the agent interacts with the environment, receiving a reward r for an action a, aiming to estimate the action-value function Q(s, a). The RL algorithm iteratively updates Q-values using the Bellman equation:\n$Q_{i+1}(s, a) = E_{s'\\sim s}[r + \\gamma \\max_{a'} Q^* (s', a')|s, a]$,\nwhere \u03b3 is the discount factor. These values converge to the optimal value Q* as i \u2192 \u221e.\nHowever, exhaustive state-space exploration is computation- ally impractical. Instead, function approximators like artificial neural networks (ANN) are used. In DQN, an ANN with weights @ approximates the Q-network, updated by minimizing the loss:\n$L_i(\\theta_i) = E_{s,a\\sim \\rho(\\cdot);s'\\sim S} [(Y_i - Q(s, a; \\theta_i))^2]$,\nwhere $y_i = E_{s\\sim s}[r+\\gamma \\max_{a'} Q(s', a'; \\theta_{i-1})|s, a]$. Gradient descent optimizes this loss, but training can be unstable. To address this, DQN uses experience replay and separate target networks. Experience replay stores experiences (s,a, s',r), sampling them uniformly during training. Prioritized Expe- rience Replay (PER) improves this by prioritizing important experiences, with sampling probability $P(i) = \\frac{p_i}{\\sum_j p_j}$, where $p_i > 0 \\in [0,1]$. Target networks stabilize training by providing consistent Q-value estimates."}, {"title": "B. Deep W-Networks", "content": "In DWN, the goal is to adapt DQN for optimizing multiple objectives simultaneously, specifically average response time (T) and configuration cost (C). This is done by optimizing objectives separately with different policies that suggest po- tentially different actions (selecting a configuration in EWS). A mechanism then chooses the best action for the situation.\nEach policy has two DQN networks: one for Q-values and one for W-values, with two replay memories for Q-networks and W-networks. DWN is defined by the tuple (N, S, A, R, II). Here, N is the number of policies, S is the state space, A is the set of actions, R = {$R_1,...,R_N$} is the set of reward functions, and II = {$\u03c0_1,...,\u03c0_\u03bd$} is the set of policies. At each decision epoch t, policies observe state s(t) \u2208 S and suggest actions. The agent selects the best action, $a_{j(t)}$, from these suggestions.The objective is to balance multiple objectives using W-learning, with each objective represented by a Q- learning policy suggesting a greedy action $a_\u00bf(t)$. Conflicts are resolved by learning W-values for each state and selecting the action from the policy with the highest $W_i(t)$:\n$W_i(t) = max\\{W_1(t),..., W_N(t)\\}.$"}, {"title": "IV. IMPLEMENTATION", "content": "In this section we evaluate our implementation to optimize server configurations in EWS,2 an exemplar to implement online learning in self-adaptive systems, this server has the capability of changing configurations at run time."}, {"title": "A. Emergent web server", "content": "Emergent Web Server (EWS) [0] is a self-adaptive server capable of changing configurations at runtime. It is imple- mented in the Dana language. 3 A server configuration in EWS consists of small, reusable components, each implementing a specific interface with defined functions. These components can be swapped at runtime to adapt the server's behavior to changing conditions. The base components that compose different configurations include: Request handlers. HTTP processing modules \u2022 Additional required modules, such as compression algorithms and cache strategies.\nIn baseline EWS, these components enable a total of 42 different configurations, formed from 2 request handlers, 4 HTTP processing modules, 2 compression algorithms, and 4 cache strategies. Furthermore, EWS includes mechanisms for obtaining and utilizing response time as a performance metric. To facilitate interactions with EWS, a Python module"}, {"title": "B. Scenario Description", "content": "We compare our DWN implementation to a modified version of the baseline epsilon-greedy (e-greedy) algorithm implemented in EWS, and an implementation of DQN, both using scalar addition to combine multiple objectives. The goal is to optimize server performance across two metrics: (1) Average response time to requests T during a three-second data collection window (step i). (2) Cost C, associated with using a specific configuration.\nThe cost metric was not originally part of the algorithms. The cost metric was introduced and randomly generated, with manually created scenarios where cost conflicts with response time to illustrate the benefits of a multi-objective approach.\nThe goal of our optimization is to minimize both T and C. Originally, the e-greedy algorithm considers only one objective. Since both T and Care within a similar value range (from zero to one), we used an aggregated reward value for each i in the e-greedy algorithm. We applied the same aggregated reward to DQN, as it shares the same limitation. Additionally, to avoid configurations being optimal for one objective but not for the other, we introduced a measure to balance the optimization. Our individual rewards for each objective are calculated for every i:\nif $T_i > 0.5$ then $r_T$ = 2 else $r_T = T_i$   (1)\nif $C_i > 0.5$ then $r_C$ = 2 else $r_C = C_i$   (2)\nA notable concern regarding our comparison is the potential bias introduced by the rewards function defined in Equations 1 and 2. The choice to apply a penalty when either the average response time or cost exceeds 0.5 may indeed overemphasize these objectives. This weighting strategy was implemented to prevent the agent from excessively favoring one objective over the other during training.\nThe global rewards for DQN and e-greedy are calculated as follows:\n$r_i = -r_T - C_i$   (3)\nFor DWN, the global reward is a tuple composed of both values:\n$r_i = (-r_T,-r_C)$   (4)\nRegarding the optimization, once the individual rewards are computed using equations 1 and 2, both DQN and e- greedy optimize a combined reward, as expressed in Eq. 3. Optimization in DWN consists on 2 individual DQNs that are trained with the separate components of the tuple defined in"}, {"title": "V. EXPERIMENTAL RESULTS AND ANALYSIS", "content": "We conducted the experiment 8 times, storing data in separate data-frames. Each run involved randomly shuffling configurations to ensure algorithms explored and found opti- mal solutions. This tests were done on a windows machine with an Intel Core 7 processor 155H, 16GB Ram memory, and RTX 4060 GPU. Detailed results are available in the implementation repository's results folder:4\nFigures 2 and 3, respectively illustrate the evolution of average response time and cost. Notably, e-greedy, DWN, and DQN show similar optimization trends, converging around 64 steps and fully by 200 steps. The quicker convergence of e- greedy stems from its early emphasis on exploiting known configurations, while DWN and DQN first explore various options for 64 steps before optimizing based on gathered data. Additionally, Table III presents the average response time and cost, for the last hundred steps of the episode. The table provides insight about the performance of the DWN, DQN, and e-greedy algorithms during the latter stages of execution, comparing them to one another. We can observe that DWN provides a better T in comparison to DQN and DWN, while having a slightly higher C. However, due to the alternating of configurations from both DWN and DQN, the deviations resulting from that could possibly make the results less robust than they could appear at first sight due to results being within the uncertainty bounds.\nDWN slightly outperformed the other two algorithms in average response time and demonstrated similar cost per- formance, albeit with greater instability, as indicated by the shaded region. Unlike e-greedy, which selected a single best configuration, DWN evaluated various candidate configura- tions with different costs and trade-offs between cost and average time, leading to larger deviations but allowing for greater flexibility.\nDQN on the other hand, possibly due to the use of same reward system of e-greedy algorithm, a linear aggregation of"}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "This paper presents the implementation and evaluation of a Multi-Objective Reinforcement Learning algorithm using Deep W-Networks to optimize configurations in the Emergent Web Server. Our primary goal was to demonstrate the first application of RL in an AS, as DWN has previously only been tested in RL toy example benchmarks. We used DWN in EWS to minimize two performance metrics: average response time and cost of the available configurations. We compared DWN's performance with a modified version of the baseline e-greedy algorithm and a Deep Q-Network approach, both of which used an aggregated value to optimize T and C.\nOur experimental results demonstrate that both DQN and DWN perform comparably to the e-greedy algorithm in these metrics. DWN achieved the lowest average response time (T), outperforming DQN by 4.75% and e-greedy by 6.43%. However, this improvement came at a higher cost, with DWN's cost being 21.41% higher than DQN and 40.88% higher than e-greedy.Notably, these percentage differences are compelling but must be interpreted with caution due to the error bounds and the deviations resulting from alternating configurations. DWN achieved a better trade-off between response time and cost compared to DQN. While DQN's average response time was only 1.7% lower than the baseline e-greedy, its cost was 14.13% higher. For DQN to achieve a trade-off similar to DWN, its cost would need to be approximately 49.13% higher, whereas DWN's cost increase was only 40.88%. Another observation was the strategies the algorithms followed. The e-greedy algorithm tended to find an optimal configuration and stick with it, while DQN showed a middle ground, mostly sticking with one optimal configuration but occasion- ally changing. DWN, on the other hand, sought a compromise between the DQN agents optimizing separate objectives within the algorithm.\nFor future work, we propose more extensive analysis involv- ing multiple metrics, such as resource consumption, which are difficult to combine into a single objective. This is cur- rently unfeasible due to data collection limitations. Testing performance in scenarios where DQN or e-greedy struggle with multiple objectives would highlight another advantage of DWN, which doesn't require aggregating values for multi- objective optimization, as well as studying further tuning of hyperparameters to optimize performance. Additionally, we propose implementing combinations and mutations between EWS configurations to better optimize multiple objectives. Finally, we are exploring integrating more complex multi- objective optimization frameworks, such as ComInA[0], that even presents an application to a bus routing systems."}]}