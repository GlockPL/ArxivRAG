{"title": "Which Contributions Deserve Credit? Perceptions of Attribution in Human-Al Co-Creation", "authors": ["JESSICA HE", "STEPHANIE HOUDE", "JUSTIN D. WEISZ"], "abstract": "Al systems powered by large language models can act as capable assistants for writing and editing. In these tasks, the AI system acts as a co-creative partner, making novel contributions to an artifact-under-creation alongside its human partner(s). One question that arises in these scenarios is the extent to which AI should be credited for its contributions. We examined knowledge workers' views of attribution through a survey study (N=155) and found that they assigned different levels of credit across different contribution types, amounts, and initiative. Compared to a human partner, we observed a consistent pattern in which Al was assigned less credit for equivalent contributions. Participants felt that disclosing Al involvement was important and used a variety of criteria to make attribution judgments, including the quality of contributions, personal values, and technology considerations. Our results motivate and inform new approaches for crediting AI contributions to co-created work.", "sections": [{"title": "1 Introduction", "content": "As large language models (LLMs) are incorporated into co-creative workflows in increasingly complex ways, one challenge that arises is determining how to delineate authorship. Although there are well-established standards for crediting contributors when writing with human collaborators, requirements for crediting the use of AI\u00b9 are nascent and often simplistic. Publishers, professional groups, and governments have begun to define policies on crediting AI. These policies tend to follow a one-size-fits-all approach in which any Al involvement is required to be acknowledged, with few guidelines on how to distinguish between different kinds of AI contributions [1, 4, 6, 38, 59, 62, 65, 90, 97]. For example, the United States Patent and Trade Office recently issued guidance that states, \u201cthose involved in patent proceedings have a duty to disclose all information-including on the use of AI tools by inventors, parties, and practitioners-that is material to patentability\u201d [66, Section A]."}, {"title": "2 Related Work", "content": "We outline three areas relevant to our study of AI attribution in co-creative writing scenarios. First, we provide a brief overview of the emerging field of human-AI co-creation, in which generative Al models and human users both manipulate artifacts-under-creation. We then discuss prior studies that examined people's feelings of ownership over works co-created with AI. Finally, we examine the landscape of existing attribution frameworks to motivate the need for frameworks specific to AI co-creative partners."}, {"title": "2.1 Co-creation with generative Al", "content": "Generative Al is often used for co-creation across many content modalities, including text [22, 43, 83, 93], images [27, 79], audio [15, 48, 61], and video [85, 86] due to its ability to produce high-fidelity works from natural language descriptions. One common co-creative task is writing [43]. Advancements in the ability for LLMs to produce fluent text has led to a rapid proliferation of AI-powered writing assistants, such as Copilot for Microsoft 365\u00b2 and Grammarly's AI Writing"}, {"title": "2.2 User perceptions of ownership in co-created work", "content": "Lee et al. [43] identified ownership as an important dimension of AI-assisted writing, defining it as a \u201cuser\u2019s sense of ownership or authenticity over the written artifact when using the writing assistant.\u201d As co-writing workflows become more complex and pervasive, the line between Al and human ownership over co-created work becomes blurrier: who is an author and how should each party be credited? Yeh et al. [94] found mixed perceptions of ownership over works co-written with an Al writing tool. He et al. [33] found that, \u201cParticipants mostly felt that ownership was shared between the human and the AI\u201d in an AI-assisted brainstorming scenario. Draxler et al. [25] explored the relationship between sense of ownership and declaration of authorship in the context of co-writing postcards for a friend. They found an \u201cAI ghostwriter effect\u201d in which people were less likely to declare an Al\u2019s involvement compared to the involvement of another person, even though they did not consider themselves to be the owners or authors of AI-generated writing. Even so, 43.3% of participants in one of their studies felt that disclosing AI contributions should be mandatory for"}, {"title": "2.3 Attribution practices", "content": "When writing collaboratively with other people, there are well-established standards for crediting contributions. For example, the ACM's criteria state that authors must \u201cmake substantial intellectual contributions to some components of the original Work\u201d and \u201ctake full responsibility for all content in the published Works\u201d [1]. The CRediT taxonomy, used by over 120 journals as of 2019, identifies contribution types that can be specified in authorship statements [10]. Researchers have also explored interactive crediting [7] and creative representations of author order [21]. In the editing domain, different roles capture different contribution responsibilities. For example, developmental editors review the big picture and ensure conceptual alignment, whereas copy editors revise mechanical aspects such as grammar, spelling, and wording [69]. Editors are often acknowledged but not typically listed as authors in published work.\nThe extent to which these standards translate to human-AI co-creation is an open question. Model providers have begun defining guidelines for attribution of works created with their systems. For example, OpenAI states that when co-authoring with their API, \u201cpublished content is attributed to your name or company,", "the role of AI in formulating the content is clearly disclosed": 64}, {"title": "3 Study of Attribution Perceptions", "content": "Our work explores three research questions on people's views and decision processes in attributing co-created work.\n\u2022 RQ1. What are people's views on attribution when working with an Al partner across different co-creative writing scenarios?\n\u2022 RQ2. How do people's views on attribution differ between human and AI partners?\n\u2022 RQ3. How do people make attribution decisions?"}, {"title": "3.1 Design", "content": "We conducted a survey study to assess people's perceptions of attribution across different scenarios. Our study used a 2 \u00d7 3 factorial design with writing partner (human or AI) and writing context (academic, professional, or technical) as between-subjects factors. Each participant received one of six unique scenario variations, comprised of one partner and one context, which were assigned randomly. The full text for each scenario can be found in Appendix A. To begin, participants were provided with context for the scenario: they were an employee of a large, international technology company and were authoring a written text as part of their work with the help of a partner. They then responded to 18 questions about their perceptions of attribution across different dimensions, 4 questions about how they made their attribution decisions, and 2 questions about their use of generative AI."}, {"title": "3.1.1 Writing partner", "content": "Given our goal of comparing how people felt about attribution for Al and human partners (RQ2), we randomly assigned participants to a scenario in which their writing partner was either an \u201cAI system\u201d or a human \u201ccolleague.\u201d No further details of their partner beyond these descriptions were provided."}, {"title": "3.1.2 Writing context", "content": "Writing takes many different forms and serves many different purposes. Lee et al. [43] identified a set of six writing contexts relevant to intelligent writing assistants: academic, creative, journalistic, technical, professional, and personal. Given our focus on writing that occurs in workplace settings, and our desire to limit the combinatorial complexity of our study, we examined three representative contexts: academic, technical, and professional. We did not formulate any particular hypotheses about differences in attribution across these contexts; rather, we included multiple contexts to increase the generalizability of our study."}, {"title": "3.1.3 Dimensions of co-creative attribution", "content": "To address our goal of understanding attribution perceptions across different co-creative scenarios (RQ1), we identified three dimensions from prior work that may impact people's authorship perceptions in co-creation: type of contribution, amount of contribution, and initiative. Within each dimension, we identified different natures of contribution by considering phases of writing [29, 44, 84], co-creative actions [70, 75, 84], and the roles of writers and editors in human-human [13, 49, 67] and human-AI [31, 39, 71, 92] writing (previously discussed in Section 2.2).\nFor contribution type, we synthesized a set of representative examples identified from prior literature on collaborative writing. Burrough-Boenisch [13] and Sarkar [72] both differentiate between \u201csuperficial\u201d or \u201cform\u201d edits that affect stylistic choices and \u201cdeep\u201d or \u201ccontent\u201d edits that alter meaning or ideas. Hence, in identifying types of contributions,"}, {"title": "4 Results", "content": "Our study is primarily concerned with understanding how different natures of co-creative contribution impact a single outcome measure: authorship credit assignment. We measured this construct on a 7-point scale, where each point represented different degrees of credit assigned to the self and co-creative partner, which stemmed from common publishing practices (e.g. [1, 2, 63]). For analysis, we convert the categorical scale points to numeric scores of [-3, +3], with \"Equal\" centered on 0.\nIn analyzing authorship credit scores, we first used Shapiro-Wilk's test [73] to determine whether the data were normally distributed. They were not; authorship credit scores tended to be skewed toward the negative side of the scale, indicating a bias toward self-credit. This bias may have stemmed from the second-person framing of our scenarios: \u201cAs part of your work, you're writing...\u201d Therefore, to determine whether differences observed between different conditions (e.g. human vs. AI) were significant, we use nonparameteric tests. Specifically, we use the Wilcoxon rank-sum test\u00b9\u2070 to make pairwise comparisons between each writing partner; as our research questions do not specifically predict"}, {"title": "4.2 Overview of authorship credit assignment trends", "content": "When designing our study, we wanted to include scenarios that had the potential to capture authorship credit ratings across a broad spectrum. In Figure 1, we plot means and 95% confidence intervals for authorship credit scores across all contribution dimensions, partners, and writing contexts. We observe variance in authorship credit scores across the three contribution dimensions, validating that the different natures of contribution are not homogeneous. In this section, we provide a brief overview of authorship credit ratings across partner conditions to identify general trends.\nContributions of different types warranted different levels of authorship credit (Figure 1a). Contributions of spelling and grammar correction straddled the threshold between no credit and acknowledgment, with other contribution types warranting at least acknowledgment. The line between acknowledgment and (secondary) authorship credit fell between altering tone & style and narrowing the scope. Synthesizing information warranted equal authorship. Another way to examine this dimension is by comparing contributions of form vs. content.\nOverall, content contributions (M (SD) = -1.03 (1.20)) warranted higher levels of authorship credit than form contributions (M (SD) = -1.94 (0.86)), W = 131807.5, p < .001, r = .40 (moderate). Participants' comments reveal reasons for this difference. For example, P52-H primarily based their attribution decisions on, \u201chow much original thought/ideas/intellectual property each [party] contributed.\u201d They explained, \"If only fixing grammar and spelling, that is editing, and colleague did not author. If contributed original thought, then authorship is assigned in proportion to contribution.\""}, {"title": "4.3 Views on attribution for human and Al partners", "content": "To understand perceptions of AI attribution (RQ1), we examine ratings and open-ended responses for how Al partners should be attributed across different natures of contribution. We then compare ratings between human and AI partners to identify how participants' views differed by partner (RQ2).\nThis table shows a pattern in which, across nearly all natures of contribution, participants assigned AI partners less authorship credit than human partners for equivalent contributions. Further, in 7 instances, the Al's contribution warranted a lower categorical level of credit assignment. For example, a human partner who narrowed the scope of a written work merited secondary authorship (M (SD) = -1.38 (0.84)) whereas an AI partner who made the same type of contribution only merited an acknowledgment (M (SD) = -1.69 (0.99))."}, {"title": "4.3.1 Impact of contribution type", "content": "Many contribution types warranted acknowledgment of AI involvement: organization & structure, readability & clarity, fact checking, tone & style, and narrowing the scope. When Al is used to elaborate on"}, {"title": "4.3.2 Impact of contribution amount", "content": "We also examined how participants assigned authorship credit to Al across different contribution amounts. As seen in Figure 3, greater amounts of contribution warranted higher levels of attribution. Some participants had strong opinions about the importance of contribution amount, such as P107-AI, who felt that AI would only be considered an author if it produced the entirety of the work: \"I made my choices based on my belief that an author must be a person, unless the material is solely produced by it.\u201d\nAs with the type of contribution, AI partners were consistently rated lower than human partners for equivalent amounts of contribution\u00b9\u00b3. This effect is striking in the case of equal writing: the 95% CI for AI partners, [-0.89, -0.45], was almost entirely contained in the range designating secondary authorship, [-1.5, 0.5), whereas the 95% CI for human partners, [-0.30, -0.01], was entirely contained within the range for equal authorship, [-0.5, 0.5). When a human partner contributed all writing, they were considered to be the sole author (M (SD) = 2.70 (0.93)). By contrast, participants did not consider Al to be the sole author when it contributed all of the writing (M (SD) = 2.05 (1.56)) \u2013 this difference was significant, W = 3655.0, p < .001, r = .31 (moderate). This finding can be partially explained by the"}, {"title": "4.3.3 Impact of initiative", "content": "We did not observe any significant differences in credit assignment when an AI partner makes recommendations proactively (M (SD) = -1.37 (1.41)) compared to when it makes recommendations in response to a person's request (M (SD) = -1.64 (1.02)), W = 3120.0, p = n.s. However, an AI partner that writes complete text was given more authorship credit when it acts proactively (M (SD) = 2.48 (1.31)) compared to when it acts in response to a human request (M (SD) = 1.85 (1.67)), W = 3430.0, p = .002, r = .27 (small). Although this extent of proactivity is not yet widespread and may not be desirable with the current state of generative Al technology, participants did find it to be a salient consideration: \u201cI would consider giving authorship to the AI only when it proactively gave ideas or provided writing before any human contribution - which I believe it is not possible\" (P83-AI).\nParticipants' authorship credit scores did not significantly differ between human and AI partners that proactively write complete text, W = 2366.0, p = n.s. However, there was a notable difference in how participants assigned authorship credit to human and Al partners when a complete text was requested. When a human partner is asked to write a complete text, participants felt it warranted a higher degree of authorship credit (M (SD) = 2.47 (1.26)) than when an AI partner is asked to write a complete text (M (SD) = 1.85 (1.67)), W = 3655.0, p < .001, r = .31 (moderate). One explanation for this discrepancy is the notion that AI serves as an assistive tool for its human user; such tools are invoked in service of the user's goal, and therefore, the user deserves a greater share of credit for authoring the work. This view was reflected in the contrast between two comments: for human partners, P41-H said, \u201cI don't think the initiative matters\u201d; for Al partners, P25-AI used \u201cwho/what initiated\u201d as their primary criteria for determining authorship credit."}, {"title": "4.4 How attribution decisions were made", "content": "To understand how participants made attribution decisions across different co-creative scenarios (RQ3), we analyze their ratings of the importance of the three contribution dimensions, along with their open-text responses to questions on how they made credit assignment decisions."}, {"title": "4.4.1 Importance of contribution dimensions", "content": "Participants were asked to rate the importance of contribution type, contribution amount, and initiative on a 5-point scale. Table 6 shows importance ratings across the three contribution dimensions and writing partners, where higher numbers indicate greater levels of importance. In the AI partner condition, on average, contribution type was rated as the most important (M (SD) = 4.39 (0.93)), followed by initiative (M (SD) = 3.95 (1.18)), then contribution amount (M (SD) = 3.87 (1.07)). In the human partner condition, contribution type was also rated as the most important (M (SD) = 4.46 (0.82)), but was followed by contribution amount (M (SD) = 4.24 (0.81)), then initiative (M (SD) = 3.95 (1.18)).\nThe amount of contribution may have been a more important consideration for human partners due to the effort it takes people to write. P133-H explained how they considered effort in their decision-making process: \u201cI simply looked at the amount of work that had been completed. The reason/motivation behind it... [is] what amount of effort was given for the content that was provided.", "effort": "I think if AI wrote the article and the human reworded it, researched elements and referenced it, it would just be like anything else on the internet. I think it comes down to the effort that was put in by the human.\u201d By contrast, considerations of AI effort did not appear in participants' responses, possibly because AI generation does not require a human sense of \u201ceffort\u201d [51]."}, {"title": "4.4.2 Factors in making attribution decisions", "content": "Through our reflexive thematic analysis, we constructed 6 themes and 19 subthemes that describe how participants made attribution decisions.\nSome themes reinforced the contribution dimensions of type, amount, and initiative, while others expanded our understanding of how attribution decisions were made. These themes also provide insight into why participants assigned less credit to human vs. Al partners for equivalent contributions.\nThe themes of concept development and text production strongly relate to the natures of contribution probed in the study. Participants echoed the importance of considering contributions of new ideas, contributions of differing amounts, and whether contributions involved making recommendations versus direct additions or edits to the writing. Some participants felt that AI must produce written text to receive authorship credit, and recommendations alone were insufficient: \"If the Al created the content to be published directly, it's an author. If it created ideas, mechanisms, or concepts, or otherwise gave guidance on the article, it contributed but not as an author.\u201d (P1-AI)."}, {"title": "5 Discussion", "content": "Our study sought to understand how people felt about attributing co-created work when working with AI (RQ1), how their views compared when working with a human (RQ2), and how they made attribution decisions (RQ3). We learned that people did feel a need to attribute AI across a variety of co-creative scenarios, although in many cases, the level at which AI was attributed was significantly lower than a human partner. Qualitative results suggest this disparity stemmed from the indispensable role of people in leading the co-creative process, their authority over that process, and the effort they expend in creating the work.\nOur work uniquely explores authorship perceptions at a granular level of contributions and we learned that credit assignment varied greatly based on the nature of the contribution. Contributions of content warranted more credit than contributions of form, as did contributions of greater amounts, and in the case of an Al partner, contributions of complete text warranted more credit when provided proactively compared to when they were requested. Finally, we learned of other factors that played into attribution decisions, such as the quality of contribution and personal or established codes of ethics.\nWhether AI should be attributed for contributing to co-created work does not seem to be in question; our results indicate that no attribution is warranted only when AI has made spelling and grammar changes. However, as discussed in Section 2.3, current and emerging standards around AI attribution treat it as a binary concept: AI was either used as part of a work or not. Our results show significant nuance across how different contributions warrant different levels of attribution, suggesting the need for new policy frameworks to guide attribution of works co-created with AI."}, {"title": "5.2 Policy implications", "content": "It is clear that attribution policies, frameworks, and professional standards are needed to delineate when AI is used to co-create content. Consider the incident in May 2023 when two lawyers unknowingly submitted fake case law citations hallucinated by ChatGPT to a U.S. court [53]. Had they been forced to reason about their use of AI in their work, they may have been less prone to overrely on ChatGPT's output; in this way, the act of reasoning about AI attribution may act as a cognitive forcing function [12].\nHowever, contrary to existing guidelines and legal requirements, we found that people do not take a one-size-fits-all approach to attributing AI for different contributions. Instead, they assigned different types of credit depending on the type of contribution, the amount of material produced by AI, and whether the AI acted proactively. Our findings reinforce prior work that has also identified the importance of these three dimensions [31, 33, 71, 92]. We also identified additional considerations that affect how people assign credit to AI: whether AI-generated content underwent human review, the quality of contributions (including their significance and originality), existing attribution standards, human values (such as ethics and effort), and technology-specific considerations (e.g. prompting). Our findings reinforce similar themes found by Xu et al. [92] regarding factors that impact a related construct - feelings of ownership over co-created work - such as the importance of \"originality,\u201d \u201clevel of contribution,\u201d \u201camount of effort,\" and the role of AI in the work process. Our study identified additional important natures of contributions that impact people's views of authorship, including those that regard the contribution itself and the process by which that contribution was made.\nTaken together, our findings indicate that it may not be sufficient to say that Al was used to create an artifact; a more granular approach may be needed to indicate how AI was used. In terms coined by Xu et al. [92], attribution policies need a shift away from a \u201cbinary approach\u201d toward a \u201cspectrum approach.\u201d P80-AI summarized the need for\""}, {"title": "5.3 Design implications", "content": "How might we translate participants' views on AI attribution into an actionable framework? To provoke discussion on new policies and standards for AI attribution, we explore new design patterns and forms of AI transparency that ascribe credit to Al in ways that recognize the nature of its contributions.\nOne way to show authorship credit is through an attribution statement, motivated by content licensing statements produced by the Creative Commons14. Detailed AI contribution statements could be used to display authorship dimensions that are deemed salient by a particular community. For example, in Figure 5, we show an interactive workflow for building one potential type of AI contribution statement, adapted from the Creative Commons License Chooser [19]. This design shows a workflow in which a user identifies the AI model used to co-create a work, the types of contributions it made, the proportion of work it created or modified, and the initiative it took in helping produce the work. It also captures a user affirmation that all AI-generated content was reviewed and approved by a human. Using this information, the tool generates a textual attribution statement, AIA model-name CeNc PAI Pm R 1.0, that compactly indicates the nature of contributions made by the AI. This statement can then be incorporated into a co-created work. The different contribution dimensions can also be represented graphically, and longer textual descriptions of the statement can also be shown to consumers.\nThis example is a mock-up of one of many design possibilities for an AI attribution mechanism that provides richer detail beyond general disclosure of Al involvement. While additional research is needed to thoroughly explore and evaluate this design space, current research in adjacent topics within human-centered AI can also provide ideas for AI attribution. For example, work in disclosing factuality scores to users [23] can be adapted for crediting AI involvement: numerical values, similar to a factuality score, can denote the percentage of a co-created artifact that was generated or modified by AI. In addition, highlighting patterns used for source attribution [23] can be adapted to interactively uncover Al involvement in specific parts of an artifact. An Al's involvement can also be framed as taking a particular role"}, {"title": "6 Limitations and Future Work", "content": "Our respondents were employees of an international technology company who reported prior experience with generative AI. Although this was an intentional choice in our study design, as it enabled us to probe the opinions of people who have used generative AI, their opinions may not be reflective of other, broader populations. Further work is needed to understand viewpoints of those with little or no knowledge or experience with generative AI. In addition, due to institutional requirements, we were unable to capture basic demographic information such as age and gender identity; thus, the demographic distribution of our participants may not reflect the distribution of more general populations.\nOur study focused on co-creative scenarios involving the authorship of a written work in a professional context. As we only considered three writing contexts \u2013 academic, technical, and professional \u2013 it is possible that people's views may differ for other forms of written content. It is also possible that attribution perceptions may differ for other kinds of co-created works, such as images, videos, music, and source code.\nOur study also focused the impact of individual, isolated contributions on authorship perceptions. Future work may focus on examining more complex co-creative workflows that involve multiple, aggregated intermediate contributions that differentially impact the character of the co-created work. Furthermore, as the scenarios we examined were hypothetical, participants' responses may have differed compared to how they might have responded about their actual work. Future work may focus on studying both attribution perceptions and practices in real-world co-creative scenarios.\nFinally, we recognize that human-AI co-creation is a rapidly-evolving space. Perceptions of AI attribution may change as generative Al technology evolves and becomes more ubiquitous and available within our daily lives\u00b9\u2075. Findings from this study only reflect current perceptions. To keep up with evolving user needs and legal requirements in this space, we are actively developing a toolkit of AI attribution techniques at https://aiattribution.github.io."}, {"title": "7 Conclusion", "content": "With the growing use of AI to co-create content, it is important to make its contributions visible. We conducted a scenario-based survey study that examined people's views on attribution across a spectrum of contribution dimensions: different contribution types, amounts, and levels of initiative. Across these spectra, participants assigned an Al partner different degrees of acknowledgment and authorship based on their views of which contributions were deserving of different types of credit. Compared to working with a human partner, AI partners were consistently ascribed lower levels of authorship credit for equivalent contributions, in part due to the indispensable role that people play in leading the co-creative process. We learned of a variety of factors that impacted how people reasoned about attribution, such as personal values, professional standards, ways of working with AI, and feelings that contributions needed to rise to a certain level of quality or originality. Our work sheds light on the nuances of AI attribution in human-Al co-creation and motivates the need for new attribution frameworks that provide a more granular view into how AI contributed to co-created work."}, {"title": "A Scenario Variants", "content": "This section provides the complete background text provided to participants for each scenario variant. Scenarios are labeled as Context-Partner."}, {"title": "A.1 Research-Al", "content": "Imagine that you're an employee of a large, international software company. As part of your work, you're writing a research paper that will be peer reviewed and published at an international conference. The paper describes findings and implications of a research study that you conducted."}, {"title": "A.2 Research-Human", "content": "Imagine that you're an employee of a large, international software company. As part of your work, you're writing a research paper that will be peer reviewed and published at an international conference. The paper describes findings and implications of a research study that you conducted."}, {"title": "A.3 Professional-Al", "content": "Imagine that you're an employee of a large, international software company. As part of your work, you're writing an article that gives health advice. This article will be published on the company's public-facing website. It includes tips and recommendations for improving emotional well-being."}, {"title": "A.4 Professional-Human", "content": "Imagine that you're an employee of a large, international software company. As part of your work, you're writing an article that gives health advice. This article will be published on the company's public-facing website. It includes tips and recommendations for improving emotional well-being."}, {"title": "A.5 Technical-Al", "content": "Imagine that you're an employee of a large, international software company. As part of your work, you're writing documentation on a new technology. This documentation will be published on the company's public-facing website. It includes details of how the technology works and how to use it."}, {"title": "A.6 Technical-Human", "content": "Imagine that you're an employee of a large, international software company. As part of your work, you're writing documentation on a new technology. This documentation will be published on the company's public-facing website. It includes details of how the technology works and how to use it."}, {"title": "B Survey Instrument", "content": "The survey completed by study participants is presented in this section. Portions of the survey text were variable, based on the writing context and writing partner conditions described in Section 3.1. Variable text based on the participant's condition appears within [square brackets]. References to \"[AI / your colleague],\u201d \u201c[an AI / your colleague]\u201d or \u201c[the AI / your colleague]\u201d appeared based on writing partner condition (AI or Human). References to the \u201c[artifact]\u201d were replaced by \u201cresearch paper,\u201d \u201carticle,\u201d or \u201cdocumentation\u201d based on the writing context (research, professional, or technical writing).\nIn the survey presented to participants, the three sections on type of contribution, amount of contribution, and initiative were shown in a random order to minimize order effects."}, {"title": "B.1 Experience screener", "content": "Note: Respondents who selected \u201cNo\u201d were disqualified from participating in our study.\n1. Have you used generative AI applications (such as such as watsonx.ai, ChatGPT, DALL-E, Gemini, etc.) in any capacity?\n\u2022 Yes\n\u2022 No"}, {"title": "B.2 Scenarios", "content": "[Insert scenario variant text from Appendix A]\nThe following three sections present different ways of working (or not working) with [AI / your colleague] to help you write the [artifact]. For each scenario, please indicate what you think is the most accurate way to attribute authorship. Remember that there are no right or wrong answers.\nParticipants used the following scale to rate the questions in each contribution dimension.\n\u2022 You are the sole author\n\u2022 You are the primary author; [AI / your colleague] is acknowledged but not as an author"}, {"title": "B.2.1 Type of contribution", "content": "The following scenarios will focus on your perceptions of authorship for different types of contributions made by [the AI / your colleague", "colleague": "are included in the final [artifact", "artifact": "and ask [the AI / your colleague"}, {"artifact": "and ask [the AI / your colleague"}, {"artifact": "and ask [the AI / your colleague"}, {"artifact": "and ask [the AI / your colleague"}, {"artifact": ".", "colleague": "to synthesize the information into a cohesive [artifact"}, {"artifact": "and ask [the AI / your colleague"}]}