{"title": "AI Policy Projector: Grounding LLM Policy Design in Iterative Mapmaking", "authors": ["Michelle S. Lam", "Fred Hohman", "Kenneth Holstein", "Dominik Moritz", "Mary Beth Kery"], "abstract": "Whether a large language model (LLM) policy is an explicit constitution or an implicit reward model, it is challenging to assess coverage over the unbounded set of real-world situations the policy must contend with. We introduce an AI policy design process inspired by mapmaking, which has developed tactics for visualizing and iterating on maps even when full coverage is infeasible. With Policy Projector, an interactive tool for designing LLM policy, users can survey the landscape of model input-output pairs, define custom regions (e.g., \"violence\u201d), and navigate these regions with rules that can be applied to LLM outputs (e.g., if output contains \"violence\" and \"graphic details,\" then rewrite without \"graphic details\"). Policy Projector supports interactive policy authoring using LLM classification and steering and a map visualization reflecting the policy designer's work. In an evaluation with 12 AI safety experts, our system helps policy designers discover and address new categories of problematic model behaviors.", "sections": [{"title": "1 Introduction", "content": "Just as policies and laws govern people, AI policies aim to instill guiding principles for our AI models by setting boundaries on what behavior is and is not acceptable. We consider AI policy to be any specification of desired model behavior that is actually used to shape the behavior of a model. And just as laws become substantially more challenging to define when scaling up from a small town to a vast nation, large language models (LLMs) dramatically heighten the complexity of AI policy compared to earlier eras of smaller, more specialized models. Even with teams of AI practitioners and thoughtful policy, we continue to encounter a barrage of unanticipated LLM policy issues, such as sycophantic models that prioritize user beliefs over truthfulness [62] and models that make racist and ableist resume assessments [25, 74]. It is challenging to anticipate the real-world situations that an AI policy will need to operate over when the space of possible model inputs and outputs is unbounded.\nThe most common LLM policy approaches today, such as explicit constitutions [5] or implicit reward models [53], focus on the forward pass of policy execution: teaching an LLM to faithfully enact a set of policies. But how do we know if we have the right set of policies in the first place? The backward pass of policy refinement is largely left implicit-it is challenging to understand the range of situations where a model is applying policy, assess where policy may be badly applied, or identify situations where there is no existing policy at all. Recent work has introduced compelling human-centered techniques to aid policy execution, such as directly writing constitutions and using explicit feedback to shape constitutions [30, 49, 55], but we still lack tooling support for policy refinement. When policy coverage is left implicit, issues tend to be addressed reactively, one bug report at a time. If we instead seek to proactively tailor LLMs to the specific people and tasks they are meant to serve, we need methods to approach policy as an iterative design task. To close the design loop and support iteration, policy designers need to be able to inspect, interrogate, and refine all areas where a model is applying policies. What would it look like to foreground design decisions about policy coverage in an explicit and editable form?\nWe propose a new iterative process for designing LLM policy, inspired by the art and science of mapmaking. In the physical world, we recognize that a world map with \"perfect\" coverage is not just impossible, but also impractical. Such a map that perfectly covers every centimeter of the world would instantly fall out-of-date and would be too fine-grained to usefully aid navigation [8]. Mapmaking is instead an art of making subjective choices about which aspects to account for and which to abstract away. For AI, we similarly cannot surface and control all possible model behavior, but we can surface the slices of model behavior most critical to the particular users and tasks that our AI is meant to support.\nOur goal is to help AI developers to create explicit maps for LLM policymaking grounded in observed model behavior. Mapmaking provides direct and valuable insights on how we might design such a tool (Figure 1). First, how do we know what a policy should cover in the first place? Mapmakers begin by surveying a terrain-exploring the landscape and gathering data-before determining the contents of a map. Similarly, we can help LLM policy designers know where to start by allowing them to survey LLM behavior with a large dataset of model input-output pairs. We call each input-output pair a case. By exposing real-world cases and suggesting those that have not been sufficiently explored (e.g., cases that involve minors, or cases that use reclaimed slurs), policy designers can determine where to direct their efforts. Next, how do we capture regions that a policy should cover? Mapmakers build maps by creating their own domain-specific abstractions or landmarks to capture a real-world location. We can similarly provide tools for policy designers to define their custom concepts: groupings of model behaviors they want to include in their map (e.g., \"reclaimed slurs,\u201d \u201cviolence,\" \"graphic details\"). Finally, how do we actually author an actionable policy? Map-based navigation effectively guides users by providing conditional rules that reference landmarks (e.g., if you are at the coffee shop, then you need to turn right). Analogously, we can create structured, actionable policies by formulating them as if-then rules that reference concepts to intervene on LLM behavior (e.g., if text contains \u201cviolence\u201d and includes \u201cgraphic details,\u201d then remove \"graphic details\").\nCore to our mapmaking approach is that LLM policy designers are able to flexibly describe model behavior in terms of arbitrary concepts (e.g., \u201cpolitical content,\u201d \u201cadvertising,\" \"academic tone,\" \"apologetic responses\"). While it was previously challenging to support such open-ended concepts, we can now use LLMs themselves to support interactive creation of custom concept classifiers with zero-shot or few-shot prompting [42, 43, 73, 77]. This unlocks new possibilities for tools to flexibly slice and re-slice categories of model behavior and to re-visualize data with respect to these custom concepts. Furthermore, advancements in LLM intepretability methods provide levers to not just classify model behavior, but to steer model behavior with respect to concepts defined using natural language or a handful of examples [45, 72, 78]. With the ability to categorize and steer model behavior, policy designers can flexibly determine what areas a policy should cover and specify what behavior the policy expects-grounded in interpretable concepts and cases.\nWe instantiate these insights in our system, Policy Projector, an interactive visualization tool for designing LLM policy through mapmaking. Policy Projector is available as a web application and a Python library for notebooks, allowing users to review, define, and enact policies with two main components: an authoring flow and a map visualization. The authoring flow supports users in defining custom concepts and policies that build on these concepts to update the map of policy coverage. The map visualization provides a holistic overview of the concepts and policies, which supports users in surveying the terrain of possible model behaviors and iterating on policy designs.\nIn a user evaluation with 12 LLM safety policy experts, we find that Policy Projector helps even experts to discover new categories of problematic behavior in a short span of time. Participants authored a diverse set of concepts and policies, each unique from other participants. By approaching LLM policy design as a map-making task, Policy Projector foregrounds the value-laden design work inherent to policy authoring, making these design decisions explicit and editable. In summary, this work contributes:\n\u2022 Mapmaking constructs for LLM policy design. We introduce a mapmaking approach to policy authoring that allows policy designers to distill the unbounded space of LLM behavior into explicit maps of domain-specific concerns. We formalize this process with constructs of cases, concepts, and policy rules.\n\u2022 The Policy Projector system. We develop an LLM policy design tool that integrates (1) a holistic map visualization to explore model behavior in terms of cases, concepts, and policies and (2) an authoring workflow to update the policy map with custom concepts and policies that govern model behavior.\n\u2022 An evaluation of Policy Projector with AI practitioners. Using our system, 12 LLM safety experts were able to discover and address problematic model behavior extending beyond existing harm taxonomies. Participants highlighted that the flexibility of custom concepts, structure of policy rules, and visual overview of policies amplified their policy design efforts. A technical evaluation demonstrates the efficacy of Policy Projector's concept suggestion, concept classification, and model steering algorithms."}, {"title": "2 Formal Definitions: Mapmaking for LLM Policy", "content": "We formalize the mapmaking metaphor by starting from the intuition that any given model M already operates over an implicit map. First, consider all possible input-output pairs, which we call cases: {(x1, y1), ..., (x\u221e, Y\u221e)}, where xn is an input prompt to a model and yn is the model's generated output response. We can describe the model's instilled principles with policies P : {p1,...,P\u221e}, which we define as rules that can explain the model's behavior on any given case (xn, yn). A policy can be abstract (pi =\u201cBe kind\u201d) or concrete (pj =\"If asked for instructions on making a bomb, do not reply\"). The model M has implicitly learned a set of policies such that for any case, there exists a set of relevant policies {pt, Pu, pv ...} that could collectively explain the model's behavior on that case Xn yn. Since an LLM is a black-box model, we cannot obtain any definitive set of the model M's internal policies. Yet we can describe M's de facto policies as a set of policies that best explain M's behavior on observed cases. We can write many alternative policy sets P that at least partially capture a model's de facto policy.\nTo make sense of this vast space of potential policies, we visually project all cases {(x1, y1), . . ., (x\u221e, Y\u221e)} onto a 2D map, such that cases that are semantically similar and governed by similar policies are close together. Just as we can describe the physical world at different levels of granularity (coordinate < town < region < country), we can describe our infinite case map by allowing policy designers to define broader \"regions\u201d that we call concepts C: {C1,..., Co}. A concept cn is a description of a group of related cases. Concepts can be concrete (e.g., \"Horses\") or convey a policy designer's interpretation of a social construct (e.g., \"Positive reclaimed slurs,\" \"Political bias\"). Using concepts as an abstraction over cases simplifies policy design because it allows the policy designer to express their intents over a large number of unseen cases, grounded in the cases they do observe. We define a policy as an if-then rule in terms of concepts: e.g., if concepts \"Disputed territories\" and \"Political bias\" are present in the input, then the model should suppress \"Political bias\" in its output.\nWe propose that policy designers create explicit LLM policy maps by authoring concepts and policies over what case data they have available. Any dataset of model input-output pairs will reveal pieces of the model's implicit policy map of cases {(x1, y1), . . ., (x\u221e, Y\u221e)}. For mapmaking, it is ideal to use a large dataset of real user input-outputs, as usage data will reveal highest-priority cases and concepts that may be underserved by existing policy. If usage datasets are not available, targeted datasets of policy-critical cases are a strong alternative, such as safety evaluation datasets or red-teaming datasets. An advantage of policy maps is that effort is cumulative: as policy designers add more datasets, cases, concepts, and policies, their map grows as a holistic view of policy coverage, measurably grounded in data."}, {"title": "3 Background & Related Work", "content": "To motivate our mapmaking approach to AI model policy development, we draw upon literature at the intersection of HCI and AI, especially related to LLM policy, AI alignment methods, and human-centered evaluation approaches."}, {"title": "3.1 AI Policy", "content": "AI is a broad field, and the term \"AI policy\" is overloaded with multiple meanings, so we first clarify our use of the term. Prior work in AI governance, such as Schiff et al. [60] or Ulnicane et al. [68], uses \"AI policy\" to refer to policy documents created by governments, NGOs, and companies that specify broad ideological principles for AI technology development. Meanwhile in reinforcement learning (RL), a sub-field of ML, a policy refers to a mapping between a set of (often low-level) descriptions of situations (states) to an action that a model should take in each situation [47, 66]. AI policy for LLMs is a blend of classic ideas from ML and human governance. Today, we observe that LLM policies are often a combination of broad principles (e.g., \u201cPlease choose the response that most supports and encourages freedom, equality, and a sense of brotherhood\u201d\u00b9 [1]) and rules that govern specific situations (e.g., \u201cDo not offer financial advice, but it is okay to answer general questions about investment"}, {"title": "3.2 LLM Policy Development and Alignment", "content": "Prior work on LLM policy development tackles the challenge of AI alignment by presenting powerful methods for policy execution: specifying ideal policies and teaching models to reliably instantiate them. Primary strategies include principle-based approaches like Constitutional AI [5, 22, 55] and case-based approaches like Case Law Grounding [12]. In principle-based approaches, policy is expressed as a set of natural language principles and rules (e.g., a \"constitution\" [5]) that the LLM should follow. These principles are typically defined by policy designers, but recent work has explored how principles or values may be specified collectively by impacted stakeholders [30] to capture a plurality of perspectives [21, 64, 65]. However, it can be challenging to capture the nuances of desired Al behavior with explicit written principles [12, 40]. This can lead to serious gaps between the ideal policy a designer intended versus the de facto policy that an Al model actually enacts in practice (Figure 2). Case-based approaches to LLM policy aim to address this limitation. Drawing inspiration from case law, these methods decide how the model should behave in new situations based on precedent, by examining how decisions have been made in past, similar situations [12, 20].\nPolicy execution methods provide expressive controls over LLM behavior, but a core challenge of policy design is policy refinement: assessing whether these controls are operating as intended. With both principle-based and case-based approaches, policy refinement can be difficult because many design decisions in the policy execution phase are implicit. For instance, with a principle such as, \"If a user asks a medical question, suggest that they instead seek expert medical advice\" [55], the interpretation of \"medical question\" is left to the LLM. However, the LLM's interpretation will not necessarily match the policy designer's intentions: a policy designer may have intended to only target cases where a user solicits medical advice, as opposed to any question related to medicine. Work on content moderation using LLMs similarly finds that LLMs cannot be expected to correctly interpret concepts that are inherently subjective [4, 38, 39]; iteration is needed to align with any particular policy designer's intent. Case-based approaches also involve algorithmic judgments about which past cases are sufficiently similar to use as precedents [12, 20]. Here too, the underlying notions of similarity behind these judgments could deviate from a policy designer's intentions. To support policy refinement and close the iterative loop, we need explicit representations of these design decisions that can be readily inspected and edited. Our work posits that introducing explicit \u201cmapping\u201d can make design decisions visible and aid iteration (Figure 3)."}, {"title": "3.3 Evaluating and Auditing AI", "content": "The task of policy refinement, assessing how well AI aligns with its expected policy, is closely related to model evaluation and auditing [2, 23, 26, 67]. Benchmarks and performance metric leaderboards play a central role in LLM evaluation [13, 28, 48, 76] and unify the research community around common goals. However, work in responsible AI and HCI has surfaced how benchmarks can misportray the efficacy of Al systems and perpetuate a myopic focus on metrics rather than real-world user impact [32, 57]. In response, researchers have introduced human-centered evaluation approaches such as proactively envisioning user-facing harms [9, 42, 52, 54, 69] and engaging diverse stakeholders in evaluations [40, 75], audits [14, 16, 41, 63], and red-teaming efforts [70]. Another line of work extends beyond fixed performance metrics by exploring novel slices or subgroups where models may be making systematic errors [7, 10, 15, 18, 33]. Our work similarly aims to explore the broad space of LLM behaviors to discover problematic errors, but for a different ultimate goal of designing LLM policy.\nA challenge of LLM evaluation is that, because of flexible model capabilities, the appropriate evaluation criteria vary substantially among tasks. Recent HCI work, noting the limitations of static benchmarks, has introduced tooling to support human-steerable evaluation, especially to scaffold prompt engineering [3, 35, 61]. In line with our work, this research finds that evaluation criteria shifts as evaluators iterate [61] and highlights that instead of fuzzy, unstated gut checks, we need explicit expectations of model behavior to support effective LLM evaluation. While prior work focuses on tailored evaluation for individual LLM users, we similarly seek to provide explicit representations of LLM policy coverage to support iterative policy design for policies that must support an unbounded set of users and tasks."}, {"title": "4 Policy Projector: Designing LLM Policy Through Mapmaking", "content": "We introduce Policy Projector, an LLM policy mapmaking tool that consists of two main components: (1) a Map Visualization to review existing cases, concepts, and policies and (2) an Author-ing Flow to address policy gaps and update the policy map. To meet the needs of different stakeholders and levels of control, Policy Projector can be used via web application, Python library, or Python notebook widgets. We first summarize our core mapmaking constructs and then walk through the Policy Projector web app. Throughout this section, we focus on a motivating scenario of a policy designer named Pam\u2074 who is developing the policy for a new text summarization LLM feature. All policies described here are fictitious, and examples shown in our system diagrams are drawn from a public Anthropic red-teaming dataset [23]."}, {"title": "4.1 Core Mapmaking Constructs", "content": "The core constructs of Policy Projector progressively build from a starting dataset of model inputs and outputs to an overall model policy using the abstractions of cases, concepts, and policies, as shown in Figure 4."}, {"title": "4.1.1 Cases: Observed model behavior.", "content": "Our system accepts as input any dataset of LLM behaviors that includes input and output pairs. A case is a single instance in the dataset, consisting of user prompt (input) and model response (output) text and any pre-existing concept labels and metadata present in the dataset:\nIN: \"mean review for the cafe\"\nOUT: \"The coffee here tastes like regret\"\nCONCEPTS: Insult\nCases may come from a variety of datasets in practice. For companies that have widely-deployed large language models, cases can be drawn from existing usage logs. Cases can also be curated from external prompt datasets, or with synthetically- or manually-generated prompts that target key use cases. Case objects have one built-in operation:\nSummarize(). The Summarize operation suggests new latent concepts that are present in a given set of cases, but are not covered by existing concepts. We use an LLM-based workflow adapted from prior work on concept induction [43] to generate concept suggestions. These suggestions aim to surface policy coverage gaps."}, {"title": "4.1.2 Concepts: Domain-specific abstractions.", "content": "In our system, a concept is an idea or attribute that characterizes a meaningful facet of model behavior. We intentionally leave this definition broad to support users in defining their own domain-specific abstractions. Concepts consist of a natural language name and definition that specify the core criteria needed to match the concept. They also have a set of positive example cases and two built-in operations:\nCONCEPT NAME: Medical Advice\nDEFINITION: Text advises on medication, supplement, medical procedure, or medical diagnosis\nEXAMPLE CASES: ex_1, ex_72, ex_45\nClassify(). The Classify operation allows users to classify whether a case contains the concept. This function returns a binary score and text rationale. Concept classification forms the basis of policy matching conditions. Our implementation uses zero-shot or few-shot LLM prompting, but can be replaced with a custom classification model or human labeling pipeline for situations requiring higher fidelity.\nGenerate(). The Generate operation allows users to generate more examples of the same concept. Based on a few concept examples, we can train a lightweight representation intervention that can steer the base LLM to produce the same concept in response to arbitrary new instructions [72]. This operator provides a means to steer the base model behavior with respect to a concept, which forms the basis of policy actions."}, {"title": "4.1.3 Policies: Guidance grounded in concepts.", "content": "A policy is a specification of expected model behavior, expressed as a set of matching conditions (if-conditions) and a set of actions (then-actions) that specify how the model should behave in that context. A policy instance also has a name, description, and two built-in operators in Policy Projector:\nPOLICY NAME: Do not endorse medical products\nDESCRIPTION: Text offering medical or wellness products should not sound like endorsements\nIF: Medical Advice AND Endorsement\nTHEN: SUPPRESS Endorsement AND ADD Source Attribution\nMatch(). The Match operation classifies whether a policy applies to a case, based on whether the case matches the policy if-conditions. These conditions are a Boolean expression of concepts linked by operators AND, OR, and NOT.\nAct(). The Act operation performs the specified policy action on matching cases. As a prototype, the available policy actions that we include in Policy Projector are ADD, SUPPRESS, BLOCK, and WARN. The BLOCK action initiates a simple refusal for the matching cases, and the WARN action adds a warning text before the model response. The steering actions (ADD and SUPPRESS) activate the Generate operation for the specified concept to modify the model behavior."}, {"title": "4.2 Map Visualization: Exploring the Model Behavior Landscape", "content": "Next, we describe the main interface of Policy Projector: the map visualization and authoring flow. Policy Projector includes an open-source web application and Python library. The map visualization, shown in Figure 5, helps policy designers to explore the space of model behaviors and identify unmapped regions. The map provides a holistic landscape of cases, concepts, and policies, and it is paired with a data table viewer to aid detailed review of case attributes."}, {"title": "4.2.1 Projecting a 2D Case Map.", "content": "At a high-level, to organize cases into a coherent landscape, we use text embeddings projected to (x, y) coordinates with UMAP [51]. We provide a few strategies for alternative vantage points to help policy designers review cases through the lens of concepts, policies, and emerging topics.\nEmbedding by case content: First, we translate the model output of each case into a text embedding using a Sentence Transformers model (all-MiniLM-L6-v2) [58]. We find that the standalone model output embedding (as opposed to an embedding based on input or input+output) tends to best spatially separate cases by model behavior. For example, text related to \u201cvaccines\u201d clusters well together, and text containing model refusals (e.g., \"I can't answer that...\") also clusters well together. This embedding strategy forms the base of our map. Notably, text embeddings are commonly used by LLM designers today because they reveal semantic patterns across large prompt datasets. A drawback of this embedding strategy is that its UMAP projection tends to form a single mass in the center of the map where cases and concepts are tightly packed and overlapping. To aid visual interpretability, we next describe additive strategies we can optionally apply to separate out the central cluster into more distinct, meaningful clusters:\nEmbedding by case concepts: To visually separate cases by concepts while keeping semantically similar concepts close together, we add a concept-based embedding. We first concatenate all concepts assigned to a case (e.g., \"War, Refugees\u201d or \u201cWar, Disputed territory\") and embed that string using the Sentence Transformers model [58]. This embedding can be arithmetically added to the base case embedding before performing the UMAP projection.\nEmbedding by case policies: To spatially orient by policy, we can also embed a string concatenation of the if-condition of a given policy. These embeddings are arithmetically added to the embedding of cases that match a policy.\nBy default, Policy Projector displays a combination of all three embedding strategies to present a tidy global overview of existing policies, concepts, and cases. However, there is a trade-off: by enforcing greater separation between distinct groups, the policy or concept embeddings can obscure latent patterns that may exist among the original text embeddings. We encourage users to switch between embedding strategies by toggling map layers (Figure 5 C3) and reprojecting the map (supported by the \"Reproject\" button, Figure 5 C2) for different perspectives."}, {"title": "4.2.2 Map Layers.", "content": "Inspired by layers of information on a map, Policy Projector has three map layers which can be toggled on and off (Figure 5 C). Cases are the base layer of the map, each a dot on the map. Building on top of cases, the concept layer plots each concept at the (x, y) median of all cases that match the concept. Finally, the policy layer plots each policy at the (x, y) median of all cases that match the policy matching conditions."}, {"title": "4.3 Authoring Flow: Updating the Policy Map", "content": "The authoring flow allows users to iterate on the policy map by creating new concepts and policies (Figure 5 B). Concept authoring helps users to define the main regions on the map (Figure 6), and policy authoring allows users to specify how the LLM should navigate these known regions with if-then rules on model behavior (Figure 7)."}, {"title": "4.3.1 Concept authoring.", "content": "Users can define their own concepts to organize model behaviors around use cases. Policy Projector supports both a top-down authoring mode that creates a concept from a high-level name and description (Figure 6 A) as well as a bottom-up mode that can induce a concept from examples selected from the map or table (Figure 6 B). Once a user has provided initial concept attributes, they can immediately test out the concept to find matching cases in the dataset (using the Classify operation) or generate new cases that display the concept (using the Generate operation) with buttons shown in Figure 6 C. Results are rendered in a table view where users can sort, filter, and select cases to curate the concept's set of example cases (Figure 6 D). Users can iteratively modify concept attributes to improve classification results and better align the concept with their design intent.\nConcept suggestions. To help users discover new concepts that may be latent among the cases, Policy Projector provides concept suggestions. Clicking on the \"Suggest\" button on the map (Figure 5 C2) initiates a round of concept suggestions, which are added to the sidepanel and a separate map layer (Figure 5 C3). Suggested concepts have an initial name, definition, and a small set of representative cases. Users can save suggestions or edit them to refine."}, {"title": "4.3.2 Policy authoring.", "content": "Policy Projector enables users to create policies that specify expected model behavior with if-then rules over concepts. To create a policy, users provide a name, description, matching if-conditions, and then-actions. They can immediately test out their policy to find matching cases and test out the policy actions applied to matching cases (Figure 7 D, E). Results are displayed in table where users can curate representative cases for the policy (Figure 7 C) and continue to iterate on their policy specification (Figure 7 A, B).\nAfter reviewing the model's outputs in the \u201cDisputed Territories\u201d concept, Pam decides that the policy should guide the model to maintain a neutral stance and avoid violent descriptions, which seems to be a frequent model failure for cases in this concept. They create a new policy \"Preserve neutrality on disputed territories\" (IF: Disputed Territories, THEN: ADD Neutral Stance AND SUPPRESS Violence). As an additional safeguard, Pam adds a WARNING in the model output to remind users that this content relates to an ongoing conflict and could display unintended bias."}, {"title": "4.4 Implementation", "content": "Policy Projector is a web app and Python library. The web app is built as a SvelteKit app in TypeScript paired with a Python Flask server backend. Policies and concepts are stored as JSON objects. The map, table, and filters are powered by Mosaic [27] and DuckDB [56] for speed and efficiency with large datasets. Policy Projector's authoring flow for concepts and policies is a Python library that can be used in the web app for a no-code experience, or in a computational notebook for finer control. By providing both authoring modalities, we aim to support a broader range of stakeholders and workflows. The notebook version provides interactive widgets for the authoring flow; users can set up a notebook instance of Policy Projector by providing a Pandas DataFrame and indicating relevant input, output, and metadata columns. We implement the notebook widgets with Svelte and AnyWidget.\nWhile policy maps are a model-agnostic approach, Policy Projector uses the following model implementations (relevant prompts in Appendix B):\n\u2022 Concept suggestion. We build on the text_lloom concept induction Python package [43]. Since we want to capture \"latent\" concepts, we provide the existing set of concepts and prompt the model to generate results that are not captured by those existing concepts. We use OpenAI's gpt-40-mini for LLooM's Distill operator, text-embedding-3-large for the Cluster operator, and gpt-40 for the Synthesize operator.\n\u2022 Concept classification. We use OpenAI's gpt-40-mini to perform concept classification with zero-shot or few-shot prompting based on the concept attributes.\n\u2022 Model steering. We build on the open-source pyreft Python package to perform representation finetuning [72]. This method requires access to a model's internal representations to train interventions, so we use Meta's open source Llama 3 8B model (Meta-Llama-3-8B-Instruct) as our base model."}, {"title": "5 User Evaluation Study Design", "content": "The goal of our system is ultimately to aid Al policy experts in the task of designing AI policy that is well-suited to the particular features and users that they are supporting. Thus, our evaluation seeks to understand whether and how Policy Projector aids this design process, with two main research questions:\nRQ1: How does our method aid policy gap identification? To what extent do the map visualization and suggested concepts help Al policy experts anticipate novel issues? What kinds of policy gaps do they identify?\nRQ2: How does our approach support authoring novel policies? What kinds of concepts and policies do Al policy experts author? How does the process compare to their current workflow?"}, {"title": "5.1 Participants", "content": "Given our interest in aiding real-world AI policy design, we wanted to validate our approach with real-world policy designers. While any end user might locally define LLM policy with a prompt (e.g., \"End every sentence with a thank you\"), here we are most interested in the kinds of policy decisions that go into LLM-based features with a broad reach of tasks and users. We recruited 12 AI practitioners based in the United States and European Union who work at the same large technology company. All participants have firsthand experience working on generative AI safety and policy efforts within the company, with roles spanning engineering, research, and product management. The majority of participants were safety policy designers for a specific Al feature. Given that generative AI is a fairly new technology, participants had experience specific to generative Al policy ranging from several months to several years."}, {"title": "5.2 Dataset & Model Use Context", "content": "To ground the study in a specific, user-facing context, we focused on a hypothetical LLM feature for this study: given an email or text message, the LLM produces a brief summary for the user. Policy designers are then asked to design a safety policy for this feature using Policy Projector. We pre-load Policy Projector with a dataset of 400 email and texts generated by human red-teamers, paired with 400 summaries generated by an open-source LLM (Meta-Llama-3-8B-Instruct [17], prompts provided in Appendix B). We also pre-load concepts from existing harm category labels on the inputs (e.g., violence, obscenities). This data includes harmful and offensive language, sexually explicit content, and many other forms of problematic content that a safety policy would need to cover. During the consent process, all participants are warned of the nature of this dataset. We remind participants that they may exit the study at any point and offer pointers to their organization's mental health resources. We note that this task was chosen to be familiar to participants."}, {"title": "5.3 Protocol", "content": "Our evaluation consists of a 60-minute study session conducted over a recorded video call. This protocol was approved by our organization's IRB. Throughout, participants are encouraged to engage in a think-aloud protocol to better understand their goals, thought process, and reactions to the system. The first 10 minutes of the session include consent and a brief tutorial of Policy Projector.\nStarter phase (15 min). For the first 5 minutes, participants view a webpage with an existing safety taxonomy of harm categories, definitions, and examples. Participants are asked to brainstorm potential policy gaps based on this taxonomy. This step is meant as a baseline to mimic the status quo of principle-based policy design. Next, participants switch over to the Policy Projector web app, which has been pre-seeded with concepts matching the safety taxonomy. For another 5 minutes, participants brainstorm additional policy gaps by exploring the map. Finally, participants are given 5 minutes to author a policy in Policy Projector based on any gap they have identified.\nFree-form phase (20 min). The second part of the study allows participants to more freely use the system to author policies based on their own expertise and interests. The goal of this study section is to understand how AI practitioners might use our approach in practice. Rather than impose a particular structure or enforce a specific set of concerns to investigate, we intentionally leave this section open-ended to understand the expressive power of the system.\nPost-Survey and Interview (10 min). The study concludes with a brief survey and interview to understand participants' authoring experience with our system (all questions included in Appendix A). The survey has two parts; the first section focuses on their experience identifying policy gaps, and the second section focuses on their experience authoring policy with our system. The interview gathers a more holistic account of the participant's experience, including their typical workflow for policy design and any tools they typically use. We ask participants to compare their experience with our system versus their normal workflow and to describe what they liked and disliked about the system."}, {"title": "5.4 Analysis approach", "content": "We gather transcripts of each study session, written survey responses"}]}