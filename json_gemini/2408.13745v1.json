{"title": "DOCE: Finding the Sweet Spot for Execution-Based Code Generation", "authors": ["Haau-Sing Li", "Patrick Fernandes", "Iryna Gurevych", "Andr\u00e9 F. T. Martins"], "abstract": "Recently, a diverse set of decoding and reranking procedures have been shown effective for LLM-based code generation. However, a comprehensive framework that links and experimentally compares these methods is missing. We address this by proposing Decoding Objectives for Code Execution, a comprehensive framework that includes candidate generation, n-best reranking, minimum Bayes risk (MBR) decoding, and self-debugging as the core components. We then study the contributions of these components through execution-based evaluation metrics. Our findings highlight the importance of execution-based methods and the difference gap between execution-based and execution-free methods. Furthermore, we assess the impact of filtering based on trial unit tests, a simple and effective strategy that has been often overlooked in prior works. We also propose self-debugging on multiple candidates, obtaining state-of-the-art performance on reranking for code generation. We expect our framework to provide a solid guideline for future research on code generation.", "sections": [{"title": "1 Introduction", "content": "Despite the impressive capabilities of large language models (LLMs) in code generation (Chen et al., 2021; Li et al., 2022, 2023; Rozi\u00e8re et al., 2024; Luo et al., 2024; Guo et al., 2024a; Lozhkov et al., 2024), demonstrated by their strong performance on popular execution-based metrics (Hendrycks et al., 2021; Chen et al., 2021; Austin et al., 2021; Lai et al., 2022), the predominant methods for generating code answers still rely on deterministic decoding techniques, such as greedy decoding and beam search, which can lead to suboptimal output (Shi et al., 2022).\nAn alternative approach involves generating multiple candidates through sampling a generative model and selecting one using n-best reranking (Ng et al., 2019; Bhattacharyya et al., 2021; Cobbe et al., 2021; Shen et al., 2021) or Minimum Bayes Risk (MBR) decoding (Eikema and Aziz, 2020, 2022). Metrics for reranking have been extensively explored in domains like machine translation (Eikema and Aziz, 2020; Freitag et al., 2022; Farinhas et al., 2023), where it has been shown to outperform greedy decoding and beam search. Similar methods in code generation include reranking with likelihoods of generated programs and instructions (Zhang et al., 2022) and MBR decoding with execution outputs (Shi et al., 2022). However, a unified framework providing an apples-to-apples comparison between these methods is currently missing.\nBridging this gap is especially significant for current research. First of all, with lately released open-source LLMs like CodeLlama (Rozi\u00e8re et al., 2024), there is demand for understanding the effects of reranking methods. For example, filtering based on trial unit tests (Li et al., 2022) was initially proposed when an LLM can generate few sensible code candidates, while lately released LLMs like CodeLlama (Rozi\u00e8re et al., 2024) can generate sensible programs easily.Additionally, evaluation with substantially more high-quality unit tests (Liu et al., 2023) is more rigorous and can potentially boost"}, {"title": "2 Candidate Generation, Self-Debugging, and Reranking", "content": "An LLM-based code generation model defines a probability distribution $P_{\\Theta_{LLM}}(y|x)$ over the space of possible programs, conditioned on a description of the task x, where $\\Theta_{LLM}$ refers to learned parameters of the LLM. An x often contains a natural language description; some unit test cases, namely trial unit tests, are used to regularize and generate the corresponding code, and the function heads optionally.\nGenerated code is usually predicted using greedy decoding or p-nucleus sampling (Holtzman et al., 2020). While greedy decoding generates a single high-probability candidate, sampling allows obtaining a set of diverse generated candidates Y. Also note that in practice, nucleus sampling is often accompanied by temperature sampling (Dabre and Fujita, 2021). Previous studies (Shi et al., 2022; Chen et al., 2023) also found that sampling-based"}, {"title": "2.2 Reranking", "content": "We now assume access to Y containing N generated candidates for a given x, as mentioned in Section 2.1. To provide a final output \u0177, we explore how to rank and pick candidates for code generation effectively. Note that with outputs that have the same score, we choose the output with the smallest index."}, {"title": "2.2.1 n-Best Reranking.", "content": "In its simplest form, n-best reranking uses a single feature f and ranks candidates according to it, with the chosen candidate then being the one that maximizes this feature.\n$\\hat{y} = \\underset{y'\\in Y}{\\operatorname{argmax}} f(y').$ (1)\nExecution-based Feature. Previous works (Li et al., 2022; Chen et al., 2024) have experimented with filtering based on trial unit tests yet have not specifically introduced them in the context of reranking. It filters out outputs that do not pass any single unit test t(y) \u2208 {pass,fail} of the set of trial unit tests $T_{trial}$:\n$f(y') = \\underset{t \\in T_{trial}}{\\Pi} 1_{t(y')=pass}.$ (2)\nScores from External Models. Other than execution-based features, we can use a single model to predict the quality score of generated code with a trained model \u03b8. Specifically, previous efforts include training models using execution success/failure as signals (Inala et al., 2022; Ni et al., 2023), given the input x and generated candidate y'. During inference, a score $f_{\\theta}(x, y')$ is given for every generated candidate. Notice that for fair comparison of execution-based and execution-less methods, we do not consider models that require information from execution during inference time (Ni et al., 2023)."}, {"title": "2.2.2 MBR Decoding", "content": "While n-best list reranking can rank candidates based on reference-free features, it is also possible to rank candidates based on reference-based metrics using MBR decoding. Concretely, if we consider a utility function U(y*, y) that measures the similarity between a candidate y and some reference (correct) code y*, MBR selects the candidate in Y that maximizes the expected utility, when considering all candidates as possible references:\n$\\hat{y} = \\underset{y'\\in Y}{\\operatorname{argmax}} \\frac{1}{|Y|} \\sum_{y\\in Y} U(y, y').$ (3)\nWhile MBR is most frequently used to optimize translation quality metrics like BLEU (Papineni et al., 2002) or COMET (Rei et al., 2020), we are interested in metrics that specifically capture the quality of code. Shi et al. (2022) showed that translation-quality metrics do not outperform greedy decoding when evaluated with execution, for which we do not consider using translation-quality metrics like BLEU.\nExecution-based Metrics. If we can feed unit test inputs to candidates, a basic utility function (Shi et al., 2022; Li et al., 2022) is to consider if a candidate matches the outputs of some reference code with the set of unit tests used for evaluation. To utility function is the exact match of outputs between the reference and the candidates\n$U(y, y') = 1_{\\varepsilon_T(y)=\\varepsilon_T(y')}.$ (4)\nIn practice, this utility function reduces the MBR decision to a majority vote for the candidate that produces outputs that agree the most with other candidates' outputs (Wang et al., 2023).\nNote that in Shi et al. (2022) execution-based MBR both on the cases of using only inputs from trial unit tests only and from all unit tests used for evaluation. Since we decide to use these unit tests as filtering, we use all unit tests given but only their inputs. Our practice aligns with most previous common practices (Shi et al., 2022; Ni et al., 2023; Chen et al., 2024).\nExternal Models for MBR. We can also use external models for MBR. Existing external models include CodeBertScore (Zhou et al., 2023), CodeScore (Dong et al., 2023) to calculate $U(y, y')$. Specifically, with a trained model \u03b8, we calculate $U_{\\theta}(x, y, y')$ by inputting input x and two generated candidates y and y'."}, {"title": "2.3 Self-Debugging", "content": "The performance of reranking is oracleed by the maximum quality of generated candidates. An approach to further improve the quality of generated"}, {"title": "2.4 DOCE Framework", "content": "We define DOCE framework. Firstly, with only reranking and MBR, we have\n$\\hat{y} = \\underset{y'\\in Y}{\\operatorname{argmax}} \\begin{cases} f(y')\\\\ \\frac{1}{|Y|} \\sum_{y\\in Y} U(y, y').\\end{cases}$ (6)\nSelf-Debugging with Single Selected Candidate from Reranking. If we only debug selected candidates from reranking, namely SD-1 (Chen et al., 2024), we have the final code answer $debug_{\\Theta}(x, \\hat{y}, T_{trial})$.\nSelf-Debugging on All Candidates before Reranking. We can also perform self-debugging on all generated candidates before reranking, namely SD-Multi. We presume this outperforms self-debugging on one candidate since self-debugging aims to regenerate candidates that pass the trial unit tests, and more candidates passing trial unit tests leads to more candidates for effective MBR decoding with possibly better reranking results. Therefore, the final code answer is $\\underset{y_a\\in Y_a}{\\operatorname{argmax}}\\begin{cases} f(y_a)\\\\  \\sum_{y'_a \\in Y_a} U(y_a, y'_a).\\end{cases}$"}, {"title": "3 Experiments", "content": "We conduct experiments using two widely recognized execution-based datasets: HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). We employ the sanitized, higher-quality subset of the MBPP dataset, namely MBPP-S. Note that HumanEval is a test-only dataset and MBPP-S are normally treated as test-only. We utilize EvalPlus (Liu et al., 2023) which includes more than 35 times unit tests than the original benchmark for both HumanEval and MBPP-S, ensuring a more rigorous assessment of function correctness."}, {"title": "3.2 Candidate Generation", "content": "We generate candidates with the CodeLlama-{7B,13B}-Instruct (Rozi\u00e8re et al., 2024) models and DeepSeekCoder-6.7B-Instruct (Guo et al., 2024a) model because instruction fine-tuning allows them to generate output in the markdown format with code blocks bracketed by ```python and```, allowing self-debugged code to be processed easily. We generate {1,5,10,20,25,50}\u00b3 candidates for each task. We first explore sampling temperature by varying it between 0.2 and 2.0 with p-nucleus sampling p 0.95, before deciding what sampling temperature we use for subsequent experiments. With MBPP-S, we vary temperature between 0.2 and 1.8. When generating multiple candidates, we utilize the vLLM framework (Kwon et al., 2023) that enables fast inference. To answer research questions related to candidate generation and reranking, we present the results from CodeLlama-7B-Instruct."}, {"title": "3.3 Reranking", "content": "For n-best reranking, we analyze three metrics types: likelihood-based, execution-based, and neural. For likelihood-based metrics, we use the log-likelihood of the generated generations given the input x as well as CoderReviewer (Zhang et al., 2022) using log(p(x|y)) + log(p(y|x)).\u2074 For execution-based metrics, we consider passing the trial unit test case as mentioned in Equation 2. For neural metrics, we use CodeScore (Dong et al., 2023) as it is so far the only publicly available neural metric that does not rely on execution during inference.\u2075 We do not consider LEVER (Ni et al., 2023) as it requires execution during inference time and requires the reranker to be trained on the target task.\nFor MBR, we consider execution-based metrics and neural metrics. Specifically, among execution-free metrics, we include CodeBertScore (Zhou et al., 2023), which is not trained with execution-based signals, and CodeScore (Dong et al., 2023), which is trained with execution-based signals. For execution-based metrics, we consider MBR-Exec (Shi et al., 2022) mentioned in Equation 4. Note that we assume access to the inputs of unit test cases used for evaluation, which aligns with one"}, {"title": "4 Results", "content": "In this section, we provide results and discuss the effects of reranking and self-debugging. Firstly, we analyze the impact of candidate generation on the model's oracle performance (assuming we pick the best possible candidate) and the performance of execution-based reranking methods. We then compare different reranking methods and discuss the importance of execution for reranking. With the answers to these questions in mind, we study the effect of self-debugging and discuss the best way to combine all these methods to improve reranking performance. We provide a subset of all our experimental results to answer these research questions. For results on more experiments, please refer to Appendix A."}, {"title": "4.1 Candidate Generation", "content": "First, we study the impact of candidate generation on reranking. To do so, we vary the number of candidates sampled from CodeLlama-7B-Instruct, also exploring different temperature parameters. We also provide estimates of the oracle of reranking using Pass@k (Chen et al., 2021; Shi et al., 2022), commonly used as an evaluation metric for code generation models. Pass@k measures the proportion that at least one candidate passes all evaluation unit tests given k generated candidates. We evaluate the performance of reranking on EvalPlus (Liu et al., 2023) for rigorous evaluation.\nWhat is the impact of the number of candidates? We generate candidates using CodeLlama-7B-Instruct by setting temperature 1.6 and p 0.95. We"}, {"title": "4.2 Comparing Reranking Methods", "content": "With the best setting we observe from Section 4.1, we now try to provide an apples-to-apples comparison between different reranking methods that either directly utilize execution, predict executions, or utilize other information to understand the effect of filtering or execution more generally in reranking and provide suggestions for reranking methods.\nWe conduct experiments with n-best reranking and MBR to compare different raking approaches with and without filtering based on trial unit tests. We show our results in Table 1.\nFiltering is the most powerful method for n-best reranking. According to the group \"N-Best Reranking\" (see Table 1), filtering based on trial unit tests gives the largest gain from the random baseline compared to MAP decoding, CoderReviewer, and CodeScore. Note that while CodeScore is trained to predict the execution accuracy of generated code, it does not even outperform the random baseline, suggesting the gap in training execution-free rerankers for execution-based metrics.\nNot only is filtering based on trial unit tests powerful in itself, but it also boosts the performance of other n-best reranking methods substantially. Specifically, we find that all methods including MAP decoding, CoderReviewer, and CodeScore outperform the filtering baseline moderately when combined with filtering except for the MAP deciding on MBPP-S+. Given our findings, we again"}, {"title": "4.3 Efficient Reranking with Fewer Unit Tests", "content": "While MBR-Exec performs well with filtering, it requires access to execution unit test inputs, which is impractical considering the execution cost. Therefore, we ask: how many unit tests should we execute candidates on? With this question in mind, we experiment with 50 generated candidates using MBR-Exec using increasing unit tests. Note that as we use the extended unit tests (Liu et al., 2023), we should also evaluate cases with the extended unit tests. Additionally, when the number of unit tests is lower than those given in the original dataset, we only use unit tests from the original dataset instead of the extended partition. We present results in Figure 5.\nWith only a few more unit tests than from the original partition, MBR-Exec with filtering reaches peak performance. The original HumanEval benchmark contains an average of 9.6 unit tests per task, and the original MBPP benchmark contains around 3 unit tests per task. Therefore, for HumanEval+, we only need 20 unit test cases, while for MBPP-S+, MBR-Exec with 5 unit test cases already gives optimal performance. This suggests that MBR-Exec can already reach its optimal performance with a few unit tests provided. Moreover, we again see the power of bringing in filtering based on trial unit tests, as it substantially improves reranking results across different benchmarks. Specifically, filtering brings a larger impact"}, {"title": "4.4 Improving the Oracle with Self-Debugging", "content": "Since applying MBR-Exec with filtering based on trial unit tests approaches the theoretical oracle estimated by Pass@k, increasing this oracle seems more sensible than proposing better reranking methods. In this section, we aim to understand the effect of self-debugging on improving the oracle of performance and the performance of reranking generally. We then compare our proposed method of performing SD-Multi to just using one candidate. Note that for SD-Multi, we only use results with one round of debugging, while we use results with 3 rounds of debugging for SD-1.\nResults suggest that self-debugging improves oracle performance of reranking, and one sin-"}, {"title": "5 Related Work", "content": "Large Language Models for Programming Languages. Recent years have witnessed the success of pretrained large language models for code generation (Chen et al., 2021; Li et al., 2022, 2023) that have been successful on multiple execution-based benchmarks (Hendrycks et al., 2021; Austin et al., 2021; Chen et al., 2021; Liu et al., 2023; Lai et al., 2022; Wang et al., 2022). More recently, strong-performing open-source pretrained Code LLMs (Rozi\u00e8re et al., 2024; Luo et al., 2024; Guo et al., 2024a; Lozhkov et al., 2024) have been available, and they can perform tasks such as filling-in-the-middle (Rozi\u00e8re et al., 2024), code debugging (Guo et al., 2024b), as some are also trained with instruction tuning. Our works use open-source code LLMs and instruction tuning to study reranking and self-debugging.\nReranking and MBR Decoding. Reranking and MBR Decoding have been studied extensively in domains such as machine translation (Eikema and Aziz, 2020, 2022; Fernandes et al., 2022; Farinhas et al., 2023). Similarly, Shi et al. (2022) and Li et al. (2022) proposes MBR decoding based on agreement on execution results, with Li et al."}, {"title": "6 Conclusion and Future Work", "content": "We leverage recent advances in code generation, including N-best reranking, MBR, and self-debugging and we propose DOCE, a unified framework for generating code with higher execution accuracy. We perform an extensive study with our proposed framework on reranking by studying candidate generation, comparing across reranking methods, and studying self-debugging for increasing reranking upper bound. Importantly, we highlight the impact of filtering based on trial unit tests, a commonly used technique whose effect has been overlooked in previous works, showing that it is crucial for all procedures DOCE. Last but not least, we observe that self-debugging improves the upper"}, {"title": "A Full Results", "content": "We present the effect of the number of candidates with candidates generated by CodeLlama-{7,13}B-Instruct (see Figure 8 and Figure 10) DeepSeekCoder-6.7B-Instruct (see Figure 9) on both the original unit tests and extended test cases in EvalPlus (Liu et al., 2023). Most results align with those we present in Figure 3, where filtering based on trial unit tests improves MBR-Exec considerably and approaches the oracle performance when combined with MBR-Exec. The only exception happens on candidates generated by DeepSeekCoder-6.7B-Instruct on HumanEval (see Figure 9a) where the execution accuracy using MBR-Exec with filtering on no less than 10 candidates is already above 90.\nTo validate our choice of temperature, we also present the choice of sampling temperature of CodeLlama-7B-Instruct on both the original and extended test cases in Figure 11. For CodeLlama-13B-Instruct and DeepSeekCoder-6.7B-Instruct, we compare results with our choice of temperature for further experiments with results using temperature 0.8. Results are presented in Table 3. All models we experiment with allow a sampling temperature over 1, with lower mean execution accuracy but higher oracle performance. Combined with filtering on trial unit tests, MBR-Exec allows constant improvement in execution accuracy when sampling with higher temperatures, which is not guaranteed without filtering."}, {"title": "A.2 Comparing Reranking Methods", "content": "We compare reranking methods with candidates generated by DeepSeekCoder-6.7B-Instruct (see Table 4) and CodeLlama-13B-Instruct (see Table 5). Findings are similar to Section 4.2, where filtering based on trial unit tests is important itself and helps to boost other reranking methods, and MBR-Exec remains the best-performing method, giving close-to-oracle performance when combined with filtering. Log-likelihoods of the generated candidate and the input do not necessarily help better reranking."}, {"title": "A.3 MBR-Exec with Fewer Unit Tests", "content": "We present results using fewer unit tests using candidates generated from DeepSeekCoder-6.7B-Instruct Figure 12 and CodeLlama-13B-Instruct in Figure 13. Similar to Figure 5, we only need 20"}, {"title": "A.4 Improving Oracle with Self-Debugging", "content": "We first present results of improvement of Pass@k that estimates oracle improvement using candidates generated and self-debugged by CodeLlama-7B-Instruct (see Figure 14), DeepSeekCoder-6.7B-Instruct (see Table 15) and CodeLlama-13B-Instruct (see Table 16). Our findings align with Section 4.4 as one round of self-debugging is enough to improve the oracle.\nWe then present results over different numbers of candidates generated and debugged by CodeLlama-7B-Instruct (see Figure 17), DeepSeekCoder-6.7B-Instruct (see Table 18) and CodeLlama-13B-Instruct (see Table 19). We find that SD-Multi outperforms SD-1 constantly except for HumanEval(+) with candidates generated and debugged by DeepSeekCoder-6.7B-Instruct with 25 generated candidates."}, {"title": "B Prompts", "content": null}, {"title": "B.1 Prompt for Code Generation (0-Shot)", "content": "See Figure 20 and Figure 21."}, {"title": "B.2 Prompt for Reviewer (3-Shot)", "content": "See Figure 22 and Figure 23."}, {"title": "B.3 Prompt for Self-Debugging (6-Shot)", "content": "See Figure 24 and Figure 25."}]}