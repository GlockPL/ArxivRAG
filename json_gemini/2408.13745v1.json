{"title": "DOCE: Finding the Sweet Spot for Execution-Based Code Generation", "authors": ["Haau-Sing Li", "Patrick Fernandes", "Iryna Gurevych", "Andr\u00e9 F. T. Martins"], "abstract": "Recently, a diverse set of decoding and reranking procedures have been shown effective for LLM-based code generation. However, a comprehensive framework that links and experimentally compares these methods is missing. We address this by proposing Decoding Objectives for Code Execution, a comprehensive framework that includes candidate generation, n-best reranking, minimum Bayes risk (MBR) decoding, and self-debugging as the core components. We then study the contributions of these components through execution-based evaluation metrics. Our findings highlight the importance of execution-based methods and the difference gap between execution-based and execution-free methods. Furthermore, we assess the impact of filtering based on trial unit tests, a simple and effective strategy that has been often overlooked in prior works. We also propose self-debugging on multiple candidates, obtaining state-of-the-art performance on reranking for code generation. We expect our framework to provide a solid guideline for future research on code generation.", "sections": [{"title": "1 Introduction", "content": "Despite the impressive capabilities of large language models (LLMs) in code generation (Chen et al., 2021; Li et al., 2022, 2023; Rozi\u00e8re et al., 2024; Luo et al., 2024; Guo et al., 2024a; Lozhkov et al., 2024), demonstrated by their strong performance on popular execution-based metrics (Hendrycks et al., 2021; Chen et al., 2021; Austin et al., 2021; Lai et al., 2022), the predominant methods for generating code answers still rely on deterministic decoding techniques, such as greedy decoding and beam search, which can lead to suboptimal output (Shi et al., 2022).\nAn alternative approach involves generating multiple candidates through sampling a generative"}, {"title": "2 Candidate Generation, Self-Debugging, and Reranking", "content": "An LLM-based code generation model defines a probability distribution $P_{OLLM}(y|x)$ over the space of possible programs, conditioned on a description of the task $x$, where $OLLM$ refers to learned parameters of the LLM. An $x$ often contains a natural language description; some unit test cases, namely trial unit tests, are used to regularize and generate the corresponding code, and the function heads optionally.\nGenerated code is usually predicted using greedy decoding or $p$-nucleus sampling (Holtzman et al., 2020). While greedy decoding generates a single high-probability candidate, sampling allows obtaining a set of diverse generated candidates $Y$. Also note that in practice, nucleus sampling is often accompanied by temperature sampling (Dabre and Fujita, 2021). Previous studies (Shi et al., 2022; Chen et al., 2023) also found that sampling-based"}, {"title": "2.1 Candidate generation", "content": "An LLM-based code generation model defines a probability distribution $P_{OLLM}(y|x)$ over the space of possible programs, conditioned on a description of the task $x$, where $OLLM$ refers to learned parameters of the LLM. An $x$ often contains a natural language description; some unit test cases, namely trial unit tests, are used to regularize and generate the corresponding code, and the function heads optionally.\nGenerated code is usually predicted using greedy decoding or $p$-nucleus sampling (Holtzman et al., 2020). While greedy decoding generates a single high-probability candidate, sampling allows obtaining a set of diverse generated candidates $Y$. Also note that in practice, nucleus sampling is often accompanied by temperature sampling (Dabre and Fujita, 2021). Previous studies (Shi et al., 2022; Chen et al., 2023) also found that sampling-based"}, {"title": "2.2 Reranking", "content": "We now assume access to $Y$ containing $N$ generated candidates for a given $x$, as mentioned in Section 2.1. To provide a final output $\\hat{y}$, we explore how to rank and pick candidates for code generation effectively. Note that with outputs that have the same score, we choose the output with the smallest index."}, {"title": "2.2.1 n-Best Reranking.", "content": "In its simplest form, n-best reranking uses a single feature $f$ and ranks candidates according to it, with the chosen candidate then being the one that maximizes this feature.\n$\\hat{y} = \\underset{y' \\in Y}{\\operatorname{argmax}} f(y').$ (1)"}, {"title": "Execution-based Feature.", "content": "Previous works (Li et al., 2022; Chen et al., 2024) have experimented with filtering based on trial unit tests yet have not specifically introduced them in the context of reranking. It filters out outputs that do not pass any single unit test $t(y)$ \u2208 {pass,fail} of the set of trial unit tests $T_{trial}$:\n$f(y') = \\mathbb{1}_{t(y')=\\text{pass}}.$ (2)\n$\\text{teTtrial}$"}, {"title": "Scores from External Models.", "content": "Other than execution-based features, we can use a single model to predict the quality score of generated code with a trained model $\\theta$. Specifically, previous efforts include training models using execution success/failure as signals (Inala et al., 2022; Ni et al., 2023), given the input $x$ and generated candidate $y'$. During inference, a score $f_{\\theta}(x, y')$ is given for every generated candidate. Notice that for fair comparison of execution-based and execution-less methods, we do not consider models that require information from execution during inference time (Ni et al., 2023)."}, {"title": "2.2.2 MBR Decoding", "content": "While n-best list reranking can rank candidates based on reference-free features, it is also possible to rank candidates based on reference-based metrics using MBR decoding. Concretely, if we consider a utility function $U(y^*, y)$ that measures the similarity between a candidate $y$ and some reference (correct) code $y^*$, MBR selects the candidate in $Y$ that maximizes the expected utility, when considering all candidates as possible references:\n$\\hat{y} = \\underset{y' \\in Y}{\\operatorname{argmax}} \\frac{1}{|Y|} \\sum_{y \\in Y} U(y, y').$ (3)\nWhile MBR is most frequently used to optimize translation quality metrics like BLEU (Papineni et al., 2002) or COMET (Rei et al., 2020), we are interested in metrics that specifically capture the quality of code. Shi et al. (2022) showed that translation-quality metrics do not outperform greedy decoding when evaluated with execution, for which we do not consider using translation-quality metrics like BLEU."}, {"title": "Execution-based Metrics.", "content": "If we can feed unit test inputs to candidates, a basic utility function (Shi et al., 2022; Li et al., 2022) is to consider if a candidate matches the outputs of some reference code with the set of unit tests used for evaluation. Let $E_T$, the utility function is the exact match of outputs between the reference and the candidates:\n$U(y, y') = \\mathbb{1}_{E_T(y)=E_T(y')}.$ (4)\nIn practice, this utility function reduces the MBR decision to a majority vote for the candidate that produces outputs that agree the most with other candidates' outputs (Wang et al., 2023).\nNote that in Shi et al. (2022) execution-based MBR both on the cases of using only inputs from trial unit tests only and from all unit tests used for evaluation. Since we decide to use these unit tests as filtering, we use all unit tests given but only their inputs. Our practice aligns with most previous common practices (Shi et al., 2022; Ni et al., 2023; Chen et al., 2024)."}, {"title": "External Models for MBR.", "content": "We can also use external models for MBR. Existing external models include CodeBertScore (Zhou et al., 2023), CodeScore (Dong et al., 2023) to calculate $U(y, y')$. Specifically, with a trained model $\\theta$, we calculate $U_{\\theta}(x, y, y')$ by inputting input $x$ and two generated candidates $y$ and $y'$."}, {"title": "2.3 Self-Debugging", "content": "The performance of reranking is oracleed by the maximum quality of generated candidates. An approach to further improve the quality of generated"}, {"title": "2.4 DOCE Framework", "content": "We define DOCE framework. Firstly, with only reranking and MBR, we have\n$\\hat{y} = \\underset{y' \\in Y}{\\operatorname{argmax}} \\frac{f (y')}{\\sum_{y \\in Y} U(y, y')}.$ (6)"}, {"title": "Self-Debugging with Single Selected Candidate from Reranking.", "content": "If we only debug selected candidates from reranking, namely SD-1 (Chen et al., 2024), we have the final code answer $debug_{\\theta}(x, \\hat{y}, T_{trial})$."}, {"title": "Self-Debugging on All Candidates before Reranking.", "content": "We can also perform self-debugging on all generated candidates before reranking, namely SD-Multi. We presume this outperforms self-debugging on one candidate since self-debugging aims to regenerate candidates that pass the trial unit tests, and more candidates passing trial unit tests leads to more candidates for effective MBR decoding with possibly better reranking results. Therefore, the final code answer is $\\underset{y_a \\in Y_a}{\\operatorname{argmax}} \\frac{f(y_a)}{\\sum_{y_a \\in Y_a} U (y_a, Y_a)}.$"}, {"title": "3 Experiments", "content": "We conduct experiments using two widely recognized execution-based datasets: HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). We employ the sanitized, higher-quality subset of the MBPP dataset, namely MBPP-S. Note that HumanEval is a test-only dataset and MBPP-S are normally treated as test-only. We utilize EvalPlus (Liu et al., 2023) which includes more than 35 times unit tests than the original benchmark for both HumanEval and MBPP-S, ensuring a more rigorous assessment of function correctness."}, {"title": "3.1 Datasets", "content": "We conduct experiments using two widely recognized execution-based datasets: HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). We employ the sanitized, higher-quality subset of the MBPP dataset, namely MBPP-S. Note that HumanEval is a test-only dataset and MBPP-S are normally treated as test-only. We utilize EvalPlus (Liu et al., 2023) which includes more than 35 times unit tests than the original benchmark for both HumanEval and MBPP-S, ensuring a more rigorous assessment of function correctness."}, {"title": "3.2 Candidate Generation", "content": "We generate candidates with the CodeLlama-{7B,13B}-Instruct (Rozi\u00e8re et al., 2024) models and DeepSeekCoder-6.7B-Instruct (Guo et al., 2024a) model because instruction fine-tuning allows them to generate output in the markdown format with code blocks bracketed by `\u3001``python and```, allowing self-debugged code to be processed easily. We generate {1,5,10,20,25,50}\u00b3 candidates for each task. We first explore sampling temperature by varying it between 0.2 and 2.0 with p-nucleus sampling p 0.95, before deciding what sampling temperature we use for subsequent experiments. With MBPP-S, we vary temperature between 0.2 and 1.8. When generating multiple candidates, we utilize the vLLM framework (Kwon et al., 2023) that enables fast inference. To answer research questions related to candidate generation and reranking, we present the results from CodeLlama-7B-Instruct."}, {"title": "3.3 Reranking", "content": "For n-best reranking, we analyze three metrics types: likelihood-based, execution-based, and neural. For likelihood-based metrics, we use the log-likelihood of the generated generations given the input $x$ as well as CoderReviewer (Zhang et al., 2022) using $log(p(x|y)) + log(p(y|x)).^{4}$ For execution-based metrics, we consider passing the trial unit test case as mentioned in Equation 2. For neural metrics, we use CodeScore (Dong et al., 2023) as it is so far the only publicly available neural metric that does not rely on execution during inference.$^{5}$ We do not consider LEVER (Ni et al., 2023) as it requires execution during inference time and requires the reranker to be trained on the target task."}, {"title": "3.4 Self-Debugging", "content": "We consider the simple setting of self-debugging proposed by Chen et al. (2024). We consider the setting of using only unit test (UT) feedback, i.e., the feedback obtained from the compiler when a generated candidate is tested on trial unit tests. We only consider this simple setting because 1) our main focus is to test generally the impact of self-debugging on improving the oracle's performance, 2) it does not require steps of generation using the LLM other than self-debugging, thus requires less computation for inference and 3) it is the feedback that gives the largest gain to execution accuracy post-debugging according to Chen et al. (2024). We perform 3 rounds of self-debugging for candidates generated with sampling because Chen et al. (2024) demonstrated that most debugging can be finished in 3 rounds. In each round, the inputs to the LLM include few-shot examples, the refined candidate from the previous round, and the candidate's execution feedback on trial unit tests."}, {"title": "4 Results", "content": "In this section, we provide results and discuss the effects of reranking and self-debugging. Firstly, we analyze the impact of candidate generation on the model's oracle performance (assuming we pick the best possible candidate) and the performance of execution-based reranking methods. We then compare different reranking methods and discuss the importance of execution for reranking. With the answers to these questions in mind, we study the effect of self-debugging and discuss the best way to combine all these methods to improve reranking performance. We provide a subset of all our experimental results to answer these research questions. For results on more experiments, please refer to Appendix A."}, {"title": "4.1 Candidate Generation", "content": "First, we study the impact of candidate generation on reranking. To do so, we vary the number of candidates sampled from CodeLlama-7B-Instruct, also exploring different temperature parameters. We also provide estimates of the oracle of reranking using Pass@k (Chen et al., 2021; Shi et al., 2022), commonly used as an evaluation metric for code generation models. Pass@k measures the proportion that at least one candidate passes all evaluation unit tests given $k$ generated candidates. We evaluate the performance of reranking on EvalPlus (Liu et al., 2023) for rigorous evaluation."}, {"title": "4.2 Comparing Reranking Methods", "content": "With the best setting we observe from Section 4.1, we now try to provide an apples-to-apples comparison between different reranking methods that either directly utilize execution, predict executions, or utilize other information to understand the effect of filtering or execution more generally in reranking and provide suggestions for reranking methods.\nWe conduct experiments with n-best reranking and MBR to compare different raking approaches with and without filtering based on trial unit tests. We show our results in Table 1."}, {"title": "4.3 Efficient Reranking with Fewer Unit Tests", "content": "While MBR-Exec performs well with filtering, it requires access to execution unit test inputs, which is impractical considering the execution cost. Therefore, we ask: how many unit tests should we execute candidates on? With this question in mind, we experiment with 50 generated candidates using MBR-Exec using increasing unit tests."}, {"title": "4.4 Improving the Oracle with Self-Debugging", "content": "Since applying MBR-Exec with filtering based on trial unit tests approaches the theoretical oracle estimated by Pass@k, increasing this oracle seems more sensible than proposing better reranking methods. In this section, we aim to understand the effect of self-debugging on improving the oracle of performance and the performance of reranking generally. We then compare our proposed method of performing SD-Multi to just using one candidate. Note that for SD-Multi, we only use results with one round of debugging, while we use results with 3 rounds of debugging for SD-1."}, {"title": "5 Related Work", "content": "Large Language Models for Programming Languages. Recent years have witnessed the success of pretrained large language models for code generation (Chen et al., 2021; Li et al., 2022, 2023) that have been successful on multiple execution-based benchmarks (Hendrycks et al., 2021; Austin et al., 2021; Chen et al., 2021; Liu et al., 2023; Lai et al., 2022; Wang et al., 2022). More recently, strong-performing open-source pretrained Code LLMs (Rozi\u00e8re et al., 2024; Luo et al., 2024; Guo et al., 2024a; Lozhkov et al., 2024) have been available, and they can perform tasks such as filling-in-the-middle (Rozi\u00e8re et al., 2024), code debugging (Guo et al., 2024b), as some are also trained with instruction tuning. Our works use open-source code LLMs and instruction tuning to study reranking and self-debugging."}, {"title": "Reranking and MBR Decoding.", "content": "Reranking and MBR Decoding have been studied extensively in domains such as machine translation (Eikema and Aziz, 2020, 2022; Fernandes et al., 2022; Farinhas et al., 2023). Similarly, Shi et al. (2022) and Li et al. (2022) proposes MBR decoding based on agreement on execution results, with Li et al."}, {"title": "Self-Debugging with LLMs", "content": "Chen et al. (2024) study the effect of using the same Code LLM to debug a single generated code candidate by varying feedback information such as execution errors and code explanation. Olausson et al. (2024) also study self-debugging but focus on different sampling methods and their impacts on self-debugging. Zhong et al. (2024) additionally integrate information regarding intermediate states to provide additional information for debugging. Our work differs from theirs as we study the joint effect of both self-debugging and reranking. We also perform self-debugging on multiple candidates."}, {"title": "6 Conclusion and Future Work", "content": "We leverage recent advances in code generation, including N-best reranking, MBR, and self-debugging and we propose DOCE, a unified framework for generating code with higher execution accuracy. We perform an extensive study with our proposed framework on reranking by studying candidate generation, comparing across reranking methods, and studying self-debugging for increasing reranking upper bound. Importantly, we highlight the impact of filtering based on trial unit tests, a commonly used technique whose effect has been overlooked in previous works, showing that it is crucial for all procedures DOCE. Last but not least, we observe that self-debugging improves the upper"}, {"title": "Limitation", "content": "First of all, our paper is limited by the scale of experiments on more code generation models, due to the limit of compute and the limit. Our solution for this is to select representative classes of open-source models and experiments on models with parameters below 15 billions."}, {"title": "A Full Results", "content": "We present the effect of the number of candidates with candidates generated by CodeLlama-{7,13}B-Instruct (see Figure 8 and Figure 10) DeepSeekCoder-6.7B-Instruct (see Figure 9) on both the original unit tests and extended test cases in EvalPlus (Liu et al., 2023). Most results align with those we present in Figure 3, where filtering based on trial unit tests improves MBR-Exec considerably and approaches the oracle performance when combined with MBR-Exec. The only exception happens on candidates generated by DeepSeekCoder-6.7B-Instruct on HumanEval (see Figure 9a) where the execution accuracy using MBR-Exec with filtering on no less than 10 candidates is already above 90.\nTo validate our choice of temperature, we also present the choice of sampling temperature of CodeLlama-7B-Instruct on both the original and extended test cases in Figure 11. For CodeLlama-13B-Instruct and DeepSeekCoder-6.7B-Instruct, we compare results with our choice of temperature for further experiments with results using temperature 0.8. Results are presented in Table 3. All models we experiment with allow a sampling temperature over 1, with lower mean execution accuracy but higher oracle performance. Combined with filtering on trial unit tests, MBR-Exec allows constant improvement in execution accuracy when sampling with higher temperatures, which is not guaranteed without filtering."}, {"title": "A.1 Candidate Generation", "content": "We present the effect of the number of candidates with candidates generated by CodeLlama-{7,13}B-Instruct (see Figure 8 and Figure 10) DeepSeekCoder-6.7B-Instruct (see Figure 9) on both the original unit tests and extended test cases in EvalPlus (Liu et al., 2023). Most results align with those we present in Figure 3, where filtering based on trial unit tests improves MBR-Exec considerably and approaches the oracle performance when combined with MBR-Exec. The only exception happens on candidates generated by DeepSeekCoder-6.7B-Instruct on HumanEval (see Figure 9a) where the execution accuracy using MBR-Exec with filtering on no less than 10 candidates is already above 90.\nTo validate our choice of temperature, we also present the choice of sampling temperature of CodeLlama-7B-Instruct on both the original and extended test cases in Figure 11. For CodeLlama-13B-Instruct and DeepSeekCoder-6.7B-Instruct, we compare results with our choice of temperature for further experiments with results using temperature 0.8. Results are presented in Table 3. All models we experiment with allow a sampling temperature over 1, with lower mean execution accuracy but higher oracle performance. Combined with filtering on trial unit tests, MBR-Exec allows constant improvement in execution accuracy when sampling with higher temperatures, which is not guaranteed without filtering."}, {"title": "A.2 Comparing Reranking Methods", "content": "We compare reranking methods with candidates generated by DeepSeekCoder-6.7B-Instruct (see Table 4) and CodeLlama-13B-Instruct (see Table 5). Findings are similar to Section 4.2, where filtering based on trial unit tests is important itself and helps to boost other reranking methods, and MBR-Exec remains the best-performing method, giving close-to-oracle performance when combined with filtering. Log-likelihoods of the generated candidate and the input do not necessarily help better reranking."}, {"title": "A.3 MBR-Exec with Fewer Unit Tests", "content": "We present results using fewer unit tests using candidates generated from DeepSeekCoder-6.7B-Instruct Figure 12 and CodeLlama-13B-Instruct in Figure 13. Similar to Figure 5, we only need 20"}, {"title": "A.4 Improving Oracle with Self-Debugging", "content": "We first present results of improvement of Pass@k that estimates oracle improvement using candidates generated and self-debugged by CodeLlama-7B-Instruct (see Figure 14), DeepSeekCoder-6.7B-Instruct (see Table 15) and CodeLlama-13B-Instruct (see Table 16). Our findings align with Section 4.4 as one round of self-debugging is enough to improve the oracle.\nWe then present results over different numbers of candidates generated and debugged by CodeLlama-7B-Instruct (see Figure 17), DeepSeekCoder-6.7B-Instruct (see Table 18) and CodeLlama-13B-Instruct (see Table 19). We find that SD-Multi outperforms SD-1 constantly except for HumanEval(+) with candidates generated and debugged by DeepSeekCoder-6.7B-Instruct with 25 generated candidates."}, {"title": "B Prompts", "content": null}, {"title": "B.1 Prompt for Code Generation (0-Shot)", "content": "See Figure 20 and Figure 21."}, {"title": "B.2 Prompt for Reviewer (3-Shot)", "content": "See Figure 22 and Figure 23."}, {"title": "B.3 Prompt for Self-Debugging (6-Shot)", "content": "See Figure 24 and Figure 25."}]}