{"title": "HOW TO SYNTHESIZE TEXT DATA WITHOUT\nMODEL COLLAPSE?", "authors": ["Xuekai Zhu", "Daixuan Cheng", "Hengli Li", "Kaiyan Zhang", "Ermo Hua", "Xingtai Lv", "Ning Ding", "Zhouhan Lin", "Zilong Zheng", "Bowen Zhou"], "abstract": "Model collapse in synthetic data indicates that iterative training on self-generated\ndata leads to a gradual decline in performance. With the proliferation of AI mod-\nels, synthetic data will fundamentally reshape the web data ecosystem. Future\nGPT-{n} models will inevitably be trained on a blend of synthetic and human-\nproduced data. In this paper, we focus on two questions: what is the impact of\nsynthetic data on language model training, and how to synthesize data without\nmodel collapse? We first pre-train language models across different proportions\nof synthetic data, revealing a negative correlation between the proportion of syn-\nthetic data and model performance. We further conduct statistical analysis on\nsynthetic data to uncover distributional shift phenomenon and over-concentration\nof n-gram features. Inspired by the above findings, we propose token editing on\nhuman-produced data to obtain semi-synthetic data. As a proof of concept, we\ntheoretically demonstrate that token-level editing can prevent model collapse, as\nthe test error is constrained by a finite upper bound. We conduct extensive exper-\niments on pre-training from scratch, continual pre-training, and supervised fine-\ntuning. The results validate our theoretical proof that token-level editing improves\ndata quality and enhances model performance.", "sections": [{"title": "1 INTRODUCTION", "content": "As generative artificial intelligence (AI) (Rombach et al., 2021; Achiam et al., 2023) becomes in-\ncreasingly prevalent in research and industry, synthetic data will proliferate throughout the web\ndata ecosystem. Consequently, future training of GPT-{n} on a mixture of synthetic and human-\nproduced data will be inevitable. Thus, model collapse is a critical concern that must be considered\nwhen training models on synthetic data.\nModel collapse refers to a degenerative process in which the output data of learned generative models\ncontaminates the training sets of subsequent generations. As shown in Figure 1, iterative training\ncoupled with data synthesis induces a progressive accumulation of test errors (Shumailov et al.,\n2024; Dohmatob et al., 2024a). Consequently, generative models increasingly overfit to synthetic\ndata distributions, failing to capture the complexity in human-produced data. Through successive\niterations in Figure 1, these distortions accumulate, finally undermining the model's capacity for\ngeneralization.\nRecent studies focus on two aspects. First, theoretical foundations of model collapse. Shumailov\net al. (2024) and Dohmatob et al. (2024a) identify the model collapse phenomenon and formalize\na theoretical framework based on linear regression. Gerstgrasser et al. (2024) demonstrate that if\nsynthetic data is accumulated while retaining the initial real data, the test error will be bounded, thus\nbreaking model collapse. Dohmatob et al. (2024c;b) indicate that missing long tails of synthetic"}, {"title": "2 NON-ITERATIVE MODEL COLLAPSE", "content": "In this section, we conduct pre-training on synthetic data mixtures and explore the reasons behind\nnon-iterative model collapse. Prior studies (Shumailov et al., 2024; Dohmatob et al., 2024a) inves-\ntigate the curse of recursion, where iterative training on self-generated data leads to a degenerative\nprocess known as iterative model collapse. In contrast, we investigate non-iterative model collapse,\ntraining a model directly on data synthesized by other models. More discussion is provided in \u00a7 5.1."}, {"title": "2.1 PRE-TRAINING ON DATA MIXTURE", "content": "Extensive prior work have proved synthetic data can boost language models' capabilities, including\ninstruction following (Wang et al., 2022), reasoning (Zhu et al., 2023; Trinh et al., 2024), align-\nment (Cui et al., 2023), biomedicine (Zhang et al., 2024) and so on. In this section, we investigate\nthe impact of synthetic data on pre-training. Compared with prior studies focused on SFT and RLHF,\nwe examine synthetic data integration in a more fundamental stage of language model.\nSetup We define the mixing ratio between human-produced and synthetic data as \u03b1, where 0 <\n\u03b1 \u2264 1. The total amount of training data $D_{total}$ is expressed as a combination of human-produced\ndata $D_{human}$ and synthetic data $D_{synthetic}$, represented by the formula: $D_{total} = \u03b1 D_{human} + (1 \u2212\n\u03b1) D_{synthetic}$."}, {"title": "2.2 WHY DOES SYNTHETIC DATA FAIL IN LANGUAGE MODEL PRE-TRAINING?", "content": "We conduct three statistical analyses: (1) sample-level distribution, (2) feature-based overlap, and\n(3) distribution-reference data selection. The experimental results reveal that, compared to human-\nproduced data, synthetic data lacks long-tail coverage and suffers from coverage collapse. The\nlimited diversity and concentrated features in synthetic data make using human-produced data as a\nreference to select synthetic data particularly challenging.\nSetup We conduct statistical and feature-based analyses to explore why synthetic data fails in pre-\ntraining. (1) We leverage a prior distribution to estimate the human-produced and synthetic data. We\nuse Llama-3-8B (AI@Meta, 2024) and StableLM-Zephyr-3B (Bellagente et al., 2024). Different\npriors consistently yield the same results. (2) We analyze the n-gram features of human-produced\nand synthetic data from a feature-based perspective, such as n-gram response values. (3) Based\non the distribution of human-produced data, we sample data from the synthetic dataset that closely\nmatches the human-produced data distribution in an attempt to filter the synthetic data.\nFinding II: Synthetic data distribution not only misses long tails, but also causes coverage col-\nlapse. Figure 3 and 9 illustrate that the PPL of synthetic data is confined to the lower 25% of\nthe human-produced data, failing to capture the full range and complexity of human-produced data"}, {"title": "2.3 PROPOSED STRATEGY", "content": "Following these lessons so far, due to the cov-\nerage collapse and feature over-concentration\nproperties of synthetic data, the best approach\nis to rely entirely on human-produced data and\navoid including synthetic data. However, we\nare still wondering how synthetic data can be\nused to enhance human-produced data. This\nleads to a general guideline for synthetic data:\nrelying solely on synthetic data leads to model\ncollapse, so preserving the primary human-\nproduced data distribution is essential. In that\ncase, we propose token-level editing, which\nleverages a prior distribution to adjust the data.\nOur method can maintain the source distribu-\ntion while improving the source data, called\nsemi-synthetic data."}, {"title": "3 TOKEN-LEVEL EDITING", "content": "In this section, we introduce token-level editing as a method for generating semi-synthetic data.\nFurthermore, we present a theoretical analysis and proof demonstrating that the test squared error\nof our method has a finite upper bound, regardless of the number of iterations. Consequently, our\nmethod not only prevents model collapse but also achieves improved performance."}, {"title": "3.1 METHOD", "content": "We formulate data synthesis as follows: assuming P is a prior distribution, given a sequence of\ntokens x = (x1,...,xt), the full synthetic data is y = (y1,\u2026\u2026\u2026, yn). The synthesis process is\nderived as:\n$P(y_1,..., y_n | x_1,..., x_t) = \\prod_{i=1}^{n} P(y_i | y_1, ..., y_{i-1}, x_1, ..., x_t)$.\nThis conditional probability formulation outlines the generation of synthetic data conditioned on the\ngiven token sequence. Then the synthetic data is used to train models.\nInspired by previous studies of data selection (Mindermann et al., 2022; Ankner et al., 2024; Lin\net al., 2024), prior distributions can serve as pointers indicating useless or learnable samples. In\nthis case, we use a pre-trained language model to infer the pretraining corpus. As illustrated in\nFigure 6, even a model pre-trained on trillions of tokens can not fit the pretraining corpus perfectly.\nSpecifically, 75% is under 0.6 probability. The tokens with both high and low probabilities are the\nmost concentrated, suggesting the potential for data filtering. We leverage this U-shape distribution\nas a pointer to resample tokens. Specifically, we use a language model as prior distribution to\ncompute each token's conditional probability $P(\\cdot|x)$ if the probability exceeds a certain threshold\n$P(\\cdot|x) \u2265 p$, it indicates that the token is easy to learn, and we proceed with resampling at that point.\nToken-level Editing doesn't generate the entire sequence but leverages conditional probability $P(x_i\n| x_1,..., x_{i-1})$ to revise the input sequence. In this way, we can avoid using purely synthetic data\nwhile modifying the dataset to preserve long-tail features of human-produced data, aiming to obtain\nhigher-quality semi-synthetic data. Token-level Editing can be formulated as follows:\n$x'_i = \\begin{cases}\n\\hat{x_i}, & \\text{if } P(x_i | x_1,..., x_{i-1})  & \\text{if } P(x_i | x_1, ..., x_{i-1}) \\geq p\n\\end{cases}$"}, {"title": "3.2 THEORETICAL ANALYSIS", "content": "To gain deeper mathematical insights, we utilize an analytical framework of the linear model and\nadopt notations in prior research (Mobahi et al., 2020; Dohmatob et al., 2024a; Gerstgrasser et al.,\n2024). This theoretical framework primarily considers a linear model that iteratively trains on its\nown generated data, similar to pipelines like self-play and self-distillation, but without complex con-\nstraints. The process involves training continuously on the data generated by the previous generation\nof the model. Dohmatob et al. (2024a) point out that with iterative training, test errors accumulate\nprogressively, eventually leading to model collapse. Building on this theoretical framework, we\nintegrate our proposed token editing method and analyze whether our method can prevent model\ncollapse.\nNotation and Preliminaries For a given distribution $P_{\\Sigma,w,\\sigma^2}$, the data $(x, y) \\sim P_{\\Sigma,w,\\sigma^2}$ on $R^d x$\nIR, where x is drawn from a multivariate normal distribution $x \\sim N(0, \\Sigma)$, $\u03f5$ is an independent noise\nterm sampled from $N(0, \\sigma^2)$, and the label y is given by the linear model $y = x \\cdot w^* + \u03f5$.\nIterative Data Editing Process We utilize the model obtained from the previous round of training\nto make a limited number of modifications. Specifically, we resample and replace data points with\nrelatively high confidence. The editing operations are defined by the matrices {M1, M2, ..., Mn}.\nThe iterative data synthesis and model-fitting process is formalized as follows:\n$P_{\\Sigma, w^*, \\sigma^2} \\rightarrow P_{\\Sigma, \\hat{w}_1, \\sigma^2} \\rightarrow ... \\rightarrow P_{\\Sigma, \\hat{w}_n, \\sigma^2}$,\nwhere n is the number of iterations. The detailed process of data editing and iterations is described\nas follows:\nFor n = 1, we begin by initializing the covariates/features as $X_1 = X$. The target values are defined\nas $Y_1 = \\hat{Y_1} = X w^* + \u03f5_1$, where $\u03f5_1 \\sim N(0, \\sigma^2 I_T)$. The linear model is then fitted by solving for\n$\\hat{w}_1 = X^+Y_1$. To proceed to the next iteration, we resample the data, obtaining $\\hat{Y_2} = X \\hat{w}_1 + \u03f5_2$,\nwith $\u03f5_2 \\sim N(0, \\sigma^2 I_T)$.\nFor n \u2265 2, the input covariates/features remain as $X_n = X$, while the target values are updated\nusing the edited targets, following the equation $Y_n = M_{n\u22121} \\hat{Y_n} + (1 \u2212 M_{n\u22121})Y_{n\u22121}$. The linear\nmodel is then fitted by computing $\\hat{w}_n = X^+Y_n$. Finally, the data is resampled for the next iteration,\nyielding $Y_{n+1} = X \\hat{w}_n + \u03f5_{n+1}$, where $\u03f5_{n+1} \\sim N(0, \\sigma^2 I_T)$.\nThe matrix $M_n$ is a diagonal matrix, where some diagonal elements are 1, while others are 0. The\nmultiplication by M can be interpreted as an operation that selectively modifies certain data points\n(those corresponding to 1s) while retaining others (those corresponding to 0s). Then, the data editing\nprocess can be formulated as follows:\n$Y_n = M_{n-1} \\hat{Y_n} + (1 \u2212 M_{n\u22121})Y_{n\u22121}$\nwhere $Y_{n\u22121}$ is the data after editing in the $n\u22121$ generation, and $\\hat{Y_n}$ is the synthetic data from the n-th\ngeneration. This process can be described as: firstly, synthesizing labels for all inputs; secondly, the\nM matrix determining which data is edited and which is retained. For a matrix A with full column\nrank, its Moore-Penrose pseudo-inverse is $A^+ = (A^T A)^{\u22121}A^T$. The noise terms $\u03f5_1, \u03f5_2, ..., \u03f5_n$\nare independent of each other and the covariates/features. Since X has full column rank, Xn retains\nthis property for all n \u2265 1.\nTest Error Model collapse is ultimately reflected through test error. Following previous work, we\nadopt the standard test error definition as presented in (Gerstgrasser et al., 2024). For any linear\nestimator w derived from the training data, we evaluate the test error using the standard method as\nfollows:\n$E_{test}(w) \\overset{def}{=} E [(x_{Test}w - y_{test})^2] \u2013 \u03c3^2 = E[||w \u2013 w^*||_2^2]$"}, {"title": "3.3 TEST ERROR UNDER DATA EDITING", "content": "Our goal is to derive an analytical expression for the test error of the n-th model in the data editing\nsetting. As indicated by the test error in Eq. 4, this process involves two main steps: (1) establishing\nthe relationship between the fitted linear parameters $w_n$ and the true parameters $w^*$, and (2) sim-\nplifying the test error expression. We begin by formulating the relationship between $w_n$ and $w^*$.\nProofs are detailed in Appendix A.\nTheorem 1 In the data editing setting, \u2200n \u2265 1, the fitted linear parameters $w_{n+1}$ can be derived\nas:\n$\\hat{w}_{n+1} = w^* + (X^T X)^{-1} X^T (E_1 + \\sum_{i=1}^{n} M_i E_{i+1})$\nwhere, w* is the true parameter, X is the original design matrix, $E_i$ is the extra noise added at the\ni'th iteration, and Mi is an idempotent diagonal matrix, defining the editing operation.\nTheorem 2 Consider an n + 1 fold data editing process with T > d + 2 samples per iteration and\nisotropic features ($\u03a3 \\overset{def}{=} I_d$), the test error for the ridgeless linear model $w_n$ learned on the edited\ndata up to iteration n + 1, is bounded by:\n$E_{test}(\\hat{w}_{n+1}) \\leq \\frac{2 \u03c3^2 d}{T\u2212d\u22121}$\nFurthermore, assuming the editing operation satisfies $||Mi|| = ||M_{i\u22121}||\u03b7$ with \u03b7 \u2208 (0,1), the test\nerror can be further bounded by:\n$E_{test}(\\hat{w}_{n+1}) \\leq \\frac{\u03c3^2 d}{T\u2212d\u22121} + \u03c3^2 E [tr ((XX)^{-2})] \u00b7 \\frac{E [tr(M_1)]}{1\u2212\u03b7}$\nRecalling that the cause of model collapse (Dohmatob et al., 2024a): training iteratively on synthetic\ndata leads to an accumulation of error over iterations, as shown in the following equation:\n$E_{collapse} (w_n) = \\frac{\u03c3^2 d}{T\u2212d\u22121} \u00d7 n$\nComparing Eq. 6 with Eq. 8, the test error under data editing is bounded by a fixed value, preventing\ncontinuous error accumulation and thus avoiding model collapse. Based on the theoretical deriva-\ntions and statistical analysis of synthetic data (\u00a7 2.1), the underlying reason is that our approach\nretains the coverage of the initial distribution. We move away from pure data synthesis toward\ntoken-level editing, which allows us to obtain better data while avoiding model collapse. Moreover,\nremarkable previous studies (Dohmatob et al., 2024c; Gerstgrasser et al., 2024) pointed out similar\nconclusions. They indicated mixing real data with synthetic data will break model collapse and\nprovide an upper bound under data accumulation. Different from their work, our data editing aims\nto yield better data, enabling synthetic data to perform well both in theory and practice, not only\navoiding model collapse."}, {"title": "4 EXPERIMENTS", "content": "To validate our proposed method, we conduct experiments across three stages of language model\ntraining including: pre-training, continual pre-training (CPT) and supervised fine-tuning (SFT)."}, {"title": "4.1 IMPLEMENTATION", "content": "We use the Llama-3-8B (AI@Meta, 2024) as a prior distribution to estimate the token distribution\nin each text sample. The modification probability is set to p = 0.99. This means that we resample\ntokens in positions where the probability exceeds p, and the resampling is based on the conditional\nprobability given the preceding context. The entire process requires only a single forward pass, with-\nout auto-regressive generation. We integrate the fast inference engine vLLM (Kwon et al., 2023)"}]}