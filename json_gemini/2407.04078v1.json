{"title": "DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning", "authors": ["Chengpeng Li", "Guanting Dong", "Mingfeng Xue", "Ru Peng", "Xiang Wang", "Dayiheng Liu"], "abstract": "Large language models (LLMs) have made impressive progress in handling simple math problems, yet they still struggle with more challenging and complex mathematical tasks. In this paper, we introduce a series of LLMs that employs the Decomposition of thought with code assistance and self-correction for mathematical reasoning, dubbed as DotaMath. DotaMath models tackle complex mathematical tasks by decomposing them into simpler logical subtasks, leveraging code to solve these subtasks, obtaining fine-grained feedback from the code interpreter, and engaging in self-reflection and correction. By annotating diverse interactive tool-use trajectories and employing query evolution on GSM8K and MATH datasets, we generate an instruction fine-tuning dataset called DotaMathQA with 574K query-response pairs. We train a series of base LLMs using imitation learning on DotaMathQA, resulting in DotaMath models that achieve the remarkable performance compared to open-source LLMs across various in-domain and out-of-domain benchmarks. Notably, DotaMath-deepseek-7B showcases an outstanding performance of 64.8% on the competitive MATH dataset and 86.7% on GSM8K. Besides, DotaMath-deepseek-7B maintains strong competitiveness on a series of in-domain and out-of-domain benchmarks (Avg. 80.1%). Looking forward, we anticipate that the DotaMath paradigm will open new pathways for addressing intricate mathematical problems.", "sections": [{"title": "1 Introduction", "content": "The emergence of large language models (LLMs) (Ouyang et al., 2022; Anil et al., 2023b; OpenAI, 2024; Anil et al., 2023a; Anthropic, 2024) has profoundly revolutionized a diverse range of natural language processing benchmarks (Chen et al., 2021; Longpre et al., 2023; Wei et al., 2023; Luo et al., 2023b). However, in the challenging field of mathematical reasoning, enabling open-source LLMs to possess reasoning abilities for complex mathematical tasks remains a significant challenge (Gou et al., 2023; Yue et al., 2023, 2024a).\nExisting works have attempted to enhance the reasoning capabilities of LLMs through methods such as chain-of-thought (COT) (Wei et al., 2022), program-of-thought (POT) (Chen et al., 2022; Gao et al.), and tool-integrated reasoning approaches (Tool-based) (Gou et al., 2023; Wang et al., 2023a). The Tool-based approach effectively merges COT's semantic and abstract reasoning with POT's computational precision, demonstrating commendable performance. Meanwhile, several efforts utilize state-of-the-art proprietary models like GPT-4 to augment existing mathematical reasoning datasets (Yu et al., 2023; Luo et al., 2023a; Li et al., 2023a), thereby improving the reasoning capabilities of LLMs during the supervised fine-tuning (SFT) phase. Building on prior works, open-source LLMs have achieved commendable performance on simple math problems. For example, on GSM8K (Cobbe et al., 2021) which contains grade school math word problems, many math-specific LLMs exceeding 80% accuracy. However, they continue to struggle with complex mathematical reasoning tasks. For instance, on MATH (Hendrycks et al., 2021) dataset comprising challenging competition problems, almost all open-source LLMs cannot exceed 60% accuracy. Through our investigation, we find that these open-source LLMs lack meticulous design for complex mathematical tasks. They do not consider the necessity of task decomposition for complex tasks, nor do they account for the need for LLMs to obtain more feedback signals from tools to facilitate comprehensive analysis.\nTo improve the capabilities of open-source LLMs in complex mathematical reasoning tasks, this paper introduces DotaMath models, a series of LLMs which employ the Decomposition of thought with code assistance and self-correction for mathematical Reasoning. There are three special designs in DotaMath for complex mathematical tasks, as depicted in Figure 1. (1) Decomposition of thought: The principle of divide-and-conquer often allows complex tasks to be decomposed into more easily solvable subtasks. Inspired by some relevant works (Khot et al.; Li et al., 2023b), DotaMath break down mathematical problems into logical subtasks and use code to solve them. (2) Intermediate process display: While previous tool-based math-specifical LLMs (Wang et al., 2023a; Gou et al., 2023; Shao et al., 2024) obtain only single mathematical expressions from code interpreters, we aim for DotaMATH to receive more fine-grained feedback from the code interpreter for subsequent comprehensive analysis. To achieve this goal, we facilitate the model to print the results of all subtasks in the form of chain of thought within the code. This design also contributes to enhancing the human readability of the model's responses. (3) Self-correction: When solving complex tasks, the issue of not succeeding at once sometimes arises. Self-reflection and correction are appropriate for resolving this problem (Shinn et al., 2023; Zhang et al., 2024). We construct two types of instruction fine-tuning data to endow DotaMath with self-correction capabilities.\nFor data construction, we introduce an instruction-following dataset named DotaMathQA, based on the human-curated mathematical datasets GSM8K and MATH. As shown in Figure 2, DotaMathQA contains two types of data: one involves data that requires a single invocation of Python code, referred to as Single-turn QA; the other includes data with a self-correction process, necessitating multiple invocations of Python code, referred to as Multi-turn QA. Inspired by previous works (Luo et al., 2023a; Yu et al., 2023; Li et al., 2023a), we adopt the query evolution to bootstrap mathematical questions in GSM8K and MATH with the augmentation prompt in Appendix D.\nWith DotaMathQA, we fine-tune a series of backbone models, including Llama2-7B (Touvron et al., 2023), Llama3-8B (Meta, 2024), Llemma-7B (Azerbayev et al., 2024) and DeepSeekMath-Base-7B (Shao et al., 2024). As shown in Table 2, DotaMath outperforms open-source models across all scales on in-domain GSM8K and MATH datasets and four out-of-domain math-mathical benchmarks overall. Notably, DotaMath-deepseek-7B showcases an outstanding performance of 64.8% on the competitive MATH dataset. Besides, DotaMath-deepseek-7B maintains strong competitiveness on GSM8K (86.7%) and achieves an anverage of 80.1% on two in-domain benchmarks and four out-of-domain benchmarks. Looking ahead, we believe the DotaMath paradigm will pave a new avenue for solving complex mathematical tasks."}, {"title": "2 Related Work", "content": "Mathematical Reasoning Techniques in LLMs. Early attempts to solve mathematical problems using large language models rely on basic computational expressions and results presented as few-shot examples (Brown et al., 2020). Moreover, Wei et al. (2022); Kojima et al. (2022) employ intermediate steps to enhance the capability of large language models in tackling complex mathematical reasoning tasks. Building on this approach, Chen et al. (2022); Gao et al. introduce the use of code tools in the problem-solving process. Subsequent researches explore various collaborative paradigms involving Chain-of-Thoughts and coding, which lead to significant improvements in the accuracy of solutions provided by large language models (Yue et al., 2023; Gou et al., 2023; Liao et al., 2024; Ying et al., 2024). Different from them, we introduces DotaMath, a novel math problem-solving paradigm which decomposes mathematical problems into logical subtasks and utilizes code to address these tasks. Our approach demonstrates superior performance across two in-domain and four out-of-domain math datasets.\nData Augmentation for Improved Math Reasoning. Beyond the exploration of paradigms, recent researches have increasingly focused on utilizing high-quality data to enhance the mathematical capabilities of large language models. Some initiatives involve curating diverse collections of mathematical data to train specialized base models tailored specifically for mathematical tasks (Azer-"}, {"title": "3 Method", "content": "In this section, we first introduce how DotaMath performs mathematical reasoning through interaction with Python interpreter (\u00a73.1 & Fig. 1). Next, we introduce the pipline of using GPT-4 for data annotation to synthesize the instruction-tuning dataset, DotaMathQA.\u00b9 (\u00a73.2 & Fig. 2). Finally, we discuss the process of supervised fine-tuning a range of foundational LLMs on the DotaMathQA dataset (\u00a73.3).\n3.1 Inference Procedure\nMotivated by a series of efforts that integrate the Python interpreter's output as supervision (Le et al., 2022; Chen et al., 2023; Qiao et al., 2023; Dong et al., 2024), DotaMath solves mathematical problems through several operations, including task decomposition, writing Python programs, invoking the Python interpreter and self-correction (Figure 1). For a given problem q and system prompt p in Appendix D, DotaMath(M) initially decompose it into some sub-tasks, yielding d\u2081 = d\u2081 \u2295 d\u2082 \u2295 d\u2083, where \u2295 means concatenation."}]}