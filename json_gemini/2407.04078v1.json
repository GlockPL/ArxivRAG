{"title": "DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning", "authors": ["Chengpeng Li", "Guanting Dong", "Mingfeng Xue", "Ru Peng", "Xiang Wang", "Dayiheng Liu"], "abstract": "Large language models (LLMs) have made impressive progress in handling simple math problems, yet they still struggle with more challenging and complex mathematical tasks. In this paper, we introduce a series of LLMs that employs the Decomposition of thought with code assistance and self-correction for mathematical reasoning, dubbed as DotaMath. DotaMath models tackle complex mathematical tasks by decomposing them into simpler logical subtasks, leveraging code to solve these subtasks, obtaining fine-grained feedback from the code interpreter, and engaging in self-reflection and correction. By annotating diverse interactive tool-use trajectories and employing query evolution on GSM8K and MATH datasets, we generate an instruction fine-tuning dataset called DotaMathQA with 574K query-response pairs. We train a series of base LLMs using imitation learning on DotaMathQA, resulting in DotaMath models that achieve the remarkable performance compared to open-source LLMs across various in-domain and out-of-domain benchmarks. Notably, DotaMath-deepseek-7B showcases an outstanding performance of 64.8% on the competitive MATH dataset and 86.7% on GSM8K. Besides, DotaMath-deepseek-7B maintains strong competitiveness on a series of in-domain and out-of-domain benchmarks (Avg. 80.1%). Looking forward, we anticipate that the DotaMath paradigm will open new pathways for addressing intricate mathematical problems.", "sections": [{"title": "1 Introduction", "content": "The emergence of large language models (LLMs) has profoundly revolutionized a diverse range of natural language processing benchmarks. However, in the challenging field of mathematical reasoning, enabling open-source LLMs to possess reasoning abilities for complex mathematical tasks remains a significant challenge. Existing works have attempted to enhance the reasoning capabilities of LLMs through methods such as chain-of-thought (COT), program-of-thought (POT), and tool-integrated reasoning approaches (Tool-based). The Tool-based approach effectively merges COT's semantic and abstract reasoning with POT's computational precision, demonstrating commendable performance. Meanwhile, several efforts utilize state-of-the-art proprietary models like GPT-4 to augment existing mathematical reasoning datasets, thereby improving the reasoning capabilities of LLMs during the supervised fine-tuning (SFT) phase. Building on prior works, open-source LLMs have achieved commendable performance on simple math problems. For example, on GSM8K which contains grade school math word problems, many math-specific LLMs exceeding 80% accuracy. However, they continue to struggle with complex mathematical reasoning tasks. For instance, on MATH dataset comprising challenging competition problems, almost all open-source LLMs cannot exceed 60% accuracy. Through our investigation, we find that these open-source LLMs lack meticulous design for complex mathematical tasks. They do not consider the necessity of task decomposition for complex tasks, nor do they account for the need for LLMs to obtain more feedback signals from tools to facilitate comprehensive analysis.\nTo improve the capabilities of open-source LLMs in complex mathematical reasoning tasks, this paper introduces DotaMath models, a series of LLMs which employ the Decomposition of thought with code assistance and self-correction for mathematical Reasoning. There are three special designs in DotaMath for complex mathematical tasks, as depicted in Figure 1. (1) Decomposition of thought: The principle of divide-and-conquer often allows complex tasks to be decomposed into more easily solvable subtasks. Inspired by some relevant works, DotaMath break down mathematical problems into logical subtasks and use code to solve them. (2) Intermediate process display: While previous tool-based math-specifical LLMs obtain only single mathematical expressions from code interpreters, we aim for DotaMATH to receive more fine-grained feedback from the code interpreter for subsequent comprehensive analysis. To achieve this goal, we facilitate the model to print the results of all subtasks in the form of chain of thought within the code. This design also contributes to enhancing the human readability of the model's responses. (3) Self-correction: When solving complex tasks, the issue of not succeeding at once sometimes arises. Self-reflection and correction are appropriate for resolving this problem. We construct two types of instruction fine-tuning data to endow DotaMath with self-correction capabilities.\nFor data construction, we introduce an instruction-following dataset named DotaMathQA, based on the human-curated mathematical datasets GSM8K and MATH. As shown in Figure 2, DotaMathQA contains two types of data: one involves data that requires a single invocation of Python code, referred to as Single-turn QA; the other includes data with a self-correction process, necessitating multiple invocations of Python code, referred to as Multi-turn QA. Inspired by previous works, we adopt the query evolution to bootstrap mathematical questions in GSM8K and MATH with the augmentation prompt in Appendix D.\nWith DotaMathQA, we fine-tune a series of backbone models, including Llama2-7B, Llama3-8B, Llemma-7B and DeepSeekMath-Base-7B. As shown in Table 2, DotaMath outperforms open-source models across all scales on in-domain GSM8K and MATH datasets and four out-of-domain mathmathical benchmarks overall. Notably, DotaMath-deepseek-7B showcases an outstanding performance of 64.8% on the competitive MATH dataset. Besides, DotaMath-deepseek-7B maintains strong competitiveness on GSM8K (86.7%) and achieves an anverage of 80.1% on two in-domain benchmarks and four out-of-domain benchmarks. Looking ahead, we believe the DotaMath paradigm will pave a new avenue for solving complex mathematical tasks."}, {"title": "2 Related Work", "content": "Mathematical Reasoning Techniques in LLMs. Early attempts to solve mathematical problems using large language models rely on basic computational expressions and results presented as few-shot examples. Moreover, employ intermediate steps to enhance the capability of large language models in tackling complex mathematical reasoning tasks. Building on this approach, introduce the use of code tools in the problem-solving process. Subsequent researches explore various collaborative paradigms involving Chain-of-Thoughts and coding, which lead to significant improvements in the accuracy of solutions provided by large language models. Different from them, we introduces DotaMath, a novel math problem-solving paradigm which decomposes mathematical problems into logical subtasks and utilizes code to address these tasks. Our approach demonstrates superior performance across two in-domain and four out-of-domain math datasets.\nData Augmentation for Improved Math Reasoning. Beyond the exploration of paradigms, recent researches have increasingly focused on utilizing high-quality data to enhance the mathematical capabilities of large language models. Some initiatives involve curating diverse collections of mathematical data to train specialized base models tailored specifically for mathematical tasks"}, {"title": "3 Method", "content": "In this section, we first introduce how DotaMath performs mathematical reasoning through interaction with Python interpreter (\u00a73.1 & Fig. 1). Next, we introduce the pipline of using GPT-4 for data annotation to synthesize the instruction-tuning dataset, DotaMathQA.1 (\u00a73.2 & Fig. 2). Finally, we discuss the process of supervised fine-tuning a range of foundational LLMs on the DotaMathQA dataset (\u00a73.3).\n3.1 Inference Procedure\nMotivated by a series of efforts that integrate the Python interpreter's output as supervision, DotaMath solves mathematical problems through several operations, including task decomposition, writing Python programs, invoking the Python interpreter and self-correction (Figure 1). For a given problem q and system prompt p in Appendix D, DotaMath(M) initially decompose it into some sub-tasks, yielding \\(d_1 = d_1^1 \\oplus d_1^2 \\oplus d_1^3\\), where \\( \\oplus \\) means concatenation.\n\\(d_1 \\sim P_M(\\cdot \\vert p \\oplus q),\n(1)\nwhere \\(P_M\\) means the probability distribution of a LLM M. Subsequently, DotaMath generates a\n1 In this paper, all data generated by GPT-4 are derived from the gpt-4-turbo-2024-04-09 api.\nPython code segment \\(c_1\\) to address these sub-tasks, presenting the reasoning process of each sub-task internally via substituting the intermediate results of sub-tasks with variables in the code. In line with with , the Python code utilizes a specific start token \"```Python\" and a distinct end token output\" to determine when to cease generation for invoking the Python interpreter:\n\\(c_1 \\sim P_M(\\cdot \\vert p \\oplus q \\oplus d_1).\n(2)\nThe execution result \\(r_1 = r_1^1 \\oplus r_1^2 \\oplus r_1^3\\) (including results of all subtasks) obtained via the Python interpreter is fed back to DotaMath for further analysis. If the problem is resolved, DotaMath will generate the final result a and place the answer within \"\\boxed{}\" for evaluation:\n\\(a \\sim P_M(\\cdot \\vert p \\oplus q \\oplus d_1 \\oplus c_1 \\oplus r_1).\n(3)\nThis type of data is referred to as single-turn QA. Otherwise, DotaMath reflects on the previous decomposition and code based on the results of all sub-tasks, providing an explanation for any errors \\(e_1\\):\n\\(e_1 \\sim P_M(\\cdot \\vert p \\oplus q \\oplus d_1 \\oplus c_1 \\oplus r_1).\n(4)\nSubsequently, the process of task decomposition, code generation, and invoking the Python interpreter is repeated until the problem is resolved or a predetermined maximum number of tool invocations is reached. This iterative process is referred to as self-correction, and the corresponding data is termed multi-turn QA. Overall, the interaction trajectory between the model and the Python interpreter can be summarized as follows:\n\\(T = d_1 \\oplus c_1 \\oplus r_1 \\oplus e_1 \\oplus d_n \\oplus c_n \\oplus r_n \\oplus a.\\)\n(5)\n3.2 DotaMathQA Dataset Construction\nAs illustrated in Figure 2, our DotaMathQA dataset can be divided into four parts: {\\(D_{seed-single}, D_{aug-single}, D_{auto-multi}, D_{rule-multi}\\)} \u2208\\(D_{DotaMathQA}\\). As illustrated in Figure 2, queries for \\(D_{seed-single}, D_{auto-multi}\\), and \\(D_{rule-multi}\\) originate from two popular mathematical reasoning datasets, GSM8K and MATH. Additionally, queries for \\(D_{aug-single}\\) come from the augmented queries of these datasets. The single-round data \\(D_{single}\\) = {\\(D_{seed-single}, D_{aug-single}\\)} involves one-time tool usage (Python interpreter), excluding the self-correction process. In contrast, the multi-round data \\(D_{multi}\\) = {\\(D_{auto-multi}, D_{rule-multi}\\)} encompasses multiple tool invocations, including the self-correction process.\n\\(D_{seed-single}\\) Construction. Leveraging the powerful instruction-following capability of GPT-4, we"}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nDatasets. The models are evaluated on two in-domain datasets, GSM8K and MATH, as wel as four out-of-domain datasets: Mathematics, SVAMP, TabMWP and ASDiv. These six datasets encompass a wide range of difficulty levels, from grade school math word problems to challenging competition mathematics problems, covering multiple domains including Algebra, Number Theory, Counting and Probability, Geometry, and Precalculus. They include various types of questions, such as tabular-based, free-form, and multiple-choice, offering a comprehensive assessment of the model's mathematical reasoning capabilities.\nMetrics. We use the accuracy of predicted answers to evaluate LLMs. Following we round numbers and parsed expressions using sympy2.\n4.2 Baselines\nWe benchmark our models with following proprietary Models and Open-Source Models:\n\u2022 Proprietary Models: Claude-3 , GPT-3.5, GPT-4 , etc.\n\u2022 Open-Source Models: WizardMATH, MetaMATH, MuggleMATH, RFT MATHCoder ToRA MARIO, etc.\nFor space saving, only part results are listed in Table 2. More results can be found in Table 10.\n4.3 Main Results\nTable 2 compares DotaMath with a range of state of art mathematical methods across in-domain and out-of-domain benchmarks. We can draw several conclusions:\n(1) On the elementary mathematical task GSM8K, most state-of-the-art 70B open-source models achieve a performance of over 80, regardless of tool usage. This indicates that the tool-based paradigm does not offer a significant advantage over Chain-of-Thought (COT) methods in simple mathematical reasoning tasks. However, DotaMath, with a size of just 7B, surpasses most of the 70B open-source models, demonstrating strong competitiveness.\n(2) On the competition-level mathematical task MATH, models utilizing tools significantly outperform those that do not, emphasizing the necessity of the tool-based paradigm for complex mathematical tasks. DotaMath substantially outperforms all open-source models and even exceeds the strongest proprietary model, Claude-3 Opus.\n(3) The DotaMath series also demonstrates the best performance on untrained Out-of-Domain datasets, indicating our model's strong generalization capabilities and comprehensive mathematical reasoning abilities. On average, DotaMath-deepseek outperforms the previous best open-source SFT model, DeepSeek-MATH-Instruct, by 4.4 points..\n(4) On in-domain benchmarks, DotaMath-LLaMA2-7B, DotaMath-llemma-7B, DotaMath-LLaMA3-8B, and DotaMath-deepseek-7B exhibit incremental performance improvements. These differences are likely attributable to the quantity and quality of math-related data used in their pre-training or continual pre-training.\n4.4 Ablation Study\nTo verify whether our designs enhance the mathematical reasoning capabilities of models, we performed ablation studies on different parts of the data. All results are based on fine-tuning the DeepSeekMath-Base model. In summary, we conducted ablation studies on two components: data format and data augmentation. The data format ablation includes three aspects: decomposition of thought, intermediate process display, and self-correction."}, {"title": "5 Conclusion", "content": "In this paper, we introduce DotaMath, a series of LLMs which adopt decomposition of thought, code assistance, intermediate process display and self-correction to solve complex math problems. To train DotaMath, we construct an instruction fine-tuning datset named DotaMathQA with 574K query-response pairs. In detail, we use query evolution to GSM8K and MATH to augment to the existing queries. Then we use gpt-4 to annotate interactive tool-use trajectories on solve the original and augmented math problems. Ultimately, we fine-tune LLaMA2, LLaMA3, LLeMA, and DeepSeekMath-Base models using DotaMathQA, resulting in the DotaMath series models. Across two in-domain and four out-of-domain mathematical benchmarks, DotaMATH achieves the best or near-best performance among all open-source models and significantly improves performance on the competition-level MATH dataset. Upon analysis, we find that our designed module provides greater assistance with difficult problems in the MATH dataset, validating the rationale of our components for complex tasks. Interestingly, our design significantly enhances the model's ability to simulate program results, allowing DotaMATH to achieve strong performance even without invoking tools. Overall, DotaMATH has further enhanced the capabilities of open-source LLMs on complex mathematical tasks, offering insights for subsequent research in LLM for mathematics."}, {"title": "Appendix", "content": "A Implementation Details\nWe fine-tune LLaMA-2-7B, LLaMA-3-7B, llema-7B, and DeepSeekMATH-Base-7B with DDotaMathQA to get DotaMATH-LLMs. We train these base models with key settings including a 5e-5 learning rate, 256 global batch size, a cosine scheduler with a 3% warm-up, a maximum context length of 4,096 (except for LLaMA2, which uses 2,048) tokens and 3 training epochs. Responses are generated using greedy decoding with a maximum sequence length of 4,096 (except for LLaMA2, which applies 2,048) and a limit of 3 maximum tool uses. Checkpoints are not selected with early stops. The hardware setup involves 32 NVIDIA A100 GPUs.\nB Additional Analysis\nB.1 Analysis on Simulating Code Execution\nTo enhance the readability of the model's output, we designed a unique mechanism that enables the model's code to print sub-tasks of a problem in a chain-of-thought manner. Interestingly, this mechanism also endows the model with the capability to simulate program execution. Specifically, we enable the model to infer the results that would typically be produced by the Python interpreter, thereby deducing the answers to problems. From Table 7, we observe that even without relying on the Python interpreter, DotaMath was still able to achieve satisfactory results, demonstrating its accurate prediction of the Python interpreter's execution outcomes. While the TORA model and DeepseekMATH-Instruct model are also trained using the output of the Python interpreter, these models struggle to make correct inferences in its absence. On the GSM8K, DotaMath-deepseek experience only an 8.5% decrease in accuracy without the use of tools, while both TORA and ToRA-code see performance drops of over 80%, and DeepSeek-MATH-Instruct suffer a 52.3% loss. On the MATH dataset, DotaMATH experience a significant performance loss of 41.0%, while other models showe similar proportionate declines as observed on the GSM8K. The underlying reason may be attributed to models frequently invoking Python libraries such as sympy2 for complex calculations when solving intricate problems in MATH, resulting in a significantly increased difficulty in predicting code execution. This also highlights the necessity of"}, {"title": "DPrompt Templates", "content": "Our prompt templates are shown below:\nI want you to act as a math teacher. You should think of some ways to help students do variation training for challenging competition mathematics problems.\nHere are some ways you can refer: Introduce fractions or percentages, Combine multiple concepts, Include a conditional statement, Increase the complexity of the problem and so on. Response with specific format, like:\nIntroduce fractions or percentages: ##1 new question1 ##1\nCombine multiple concepts: ##2 new question2 ##2\nIncrease the complexity of the problem: ##10: new question10 ##10\nThe nth problem must be strictly limited to between ##n and ##n for our subsequent regular extraction.\nNow you are give a math problem, think for 10 different ways.\nGiven new problem:\n{Query}\nSystem Prompt\n{Query}\nGenerative Prompt\nYou are an exceptionally strong competitor in both math and programming contests, proficient in a wide range of mathematical knowledge and skilled in Python programming. Your command of Pre-algebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus is unparalleled. Your thinking is meticulous and profound, and the code you write always runs flawlessly and without error.\nIntegrate step-by-step reasoning and Python code to solve math problems using the following guidelines:\n1. Break the problem into subtasks.\n2. Write functions to solve the problem; the function should not take any arguments.\n3. Print the results of every subtask in the Python code, using the intermediate variables in Python programs to represent intermediate results, refer to the example below.\n4. When writing the python program, avoid using decimal. Utilize functions from sympy and other necessary Python library, and simplify all fractions and square roots without converting them to decimal values.\n5. Print the final answer on the last line.\nHere is an example you may refer to:\nCorrection Prompt\nYou are an exceptionally strong competitor in both math and programming contests, proficient in a wide range of mathematical knowledge and skilled in Python programming. Your command of Pre-algebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus is unparalleled. Your thinking is meticulous and profound, and the code you write always runs flawlessly and without error. You solve the problem with PLAN-CODE format.\nIntegrate step-by-step reasoning and Python code to solve math problems using the following guidelines:\n1. Break the problem into subtasks.\n2. Write functions to solve the problem; the function should not take any arguments.\n3. Print the results of every subtask in the Python code.\n4. When writing the python program, avoid using decimal. Utilize functions from sympy and other necessary python library, and simplify all fractions and square roots without converting them to decimal values.\n5. Print the numeric answer on the last line.\nExplanatory Prompt\nYou are an exceptionally strong competitor in both math and programming contests, proficient in a wide range of mathematical knowledge and skilled in Python programming. Your command of Pre-algebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus is unparalleled. Your thinking is meticulous and profound, and the code you write always runs flawlessly and without error.\nGiven a math problem, an incorrect solution and a correct solution, your task is to give the incorrect reason for the incorrect solution. Note that the reason is not allowed to be too long. You don't need to give the correct answer. You need to start your answer with: 'The solution is wrong since...'"}, {"title": "E Case Study", "content": "E.1 Singl-turn Successful cases\nSingle-trun successful case in GSM8K\nProblem: Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for 2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\nSolution: To solve the problem of calculating Janet's daily earnings from selling fresh duck eggs at the farmers' market, we can follow these steps:\n1. Calculate the total number of eggs laid by the ducks each day.\n2. Subtract the number of eggs Janet eats for breakfast and the number used in baking muffins to find the number of eggs available for sale.\n3. Calculate the earnings from selling the remaining eggs at 2 per egg.\nSingle-trun successful case in MATH\nProblem: Find all positive integer values of c such that the equation \\(x^2 - 7x + c = 0\\) only has roots that are real and rational. Express them in decreasing order, separated by commas.\nSolution: For the roots to be real and rational, the discriminant must be a perfect square. Therefore, \\((-7)^2 - 4 \\cdot 1 \\cdot c = 49 \u2013 4c\\) must be a perfect square. The only positive perfect squares less than 49 are 1, 4, 9, 16, 25, and 36. The perfect squares that give an integer value of c are 1, 9, and 25. Thus, we follow these steps:\n1. Solve the equation 49 - 4c = 1 to find a value of c.\n2. Solve the equation 49 - 4c = 9 to find another value of c.\n3. Solve the equation 49 \u2013 4c = 25 to find the last value of c.\nE.2 Multi-turn Successful cases\nMulti-turn successful case in GSM8K\nProblem:India has 4 more than half the number of states in the USA. What's the total number of states in both countries together? Solution: To solve the problem, we can break it down into the following subtasks:\n1. Define the number of states in the USA.\n2. Calculate the number of states in India based on the given relationship.\n3. Sum the number of states in both countries.\nMulti-turn successful case in MATH\nProblem: A lattice point in the xy-plane is a point both of whose coordinates are integers (not necessarily positive). How many lattice points lie on the hyperbola \\(x^2 \u2212 y^2 = 17\\)?\nSolution: To find the number of lattice points on the hyperbola \\(x^2 \u2013 y^2 = 17\\), we can follow these steps:\n1. Recognize that \\(x^2 \u2212 y^2 = (x + y)(x - y)\\).\n2. Since x and y are integers, (x + y) and (x - y) must be factors of 17.\n3. The number 17 is a prime number, so its only factors are 1 and 17 or \u20131 and -17.\n4. We will set up equations for x + y = 17 and x - y = 1, and also for x + y = -17 and x - y = -1.\n5. Solve these equations to find the integer solutions for x and y."}]}