{"title": "AI and the Problem of Knowledge Collapse", "authors": ["Andrew J. Peterson"], "abstract": "While artificial intelligence has the potential to process vast amounts of data, generate new insights, and unlock greater productivity, its widespread adoption may entail unforeseen consequences. We identify conditions under which AI, by reducing the cost of access to certain modes of knowledge, can paradoxically harm public understanding. While large language models are trained on vast amounts of diverse data, they naturally generate output towards the 'center' of the distribution. This is generally useful, but widespread reliance on recursive Al systems could lead to a process we define as \"knowledge collapse\", and argue this could harm innovation and the richness of human understanding and culture. However, unlike AI models that cannot choose what data they are trained on, humans may strategically seek out diverse forms of knowledge if they perceive them to be worthwhile. To investigate this, we provide a simple model in which a community of learners or innovators choose to use traditional methods or to rely on a discounted AI-assisted process and identify conditions under which knowledge collapse occurs. In our default model, a 20% discount on AI-generated content generates public beliefs 2.3 times further from the truth than when there is no discount. Finally, based on the results, we consider further research directions to counteract such outcomes.", "sections": [{"title": "Introduction", "content": "Before the advent of generative AI, all text and artwork was produced by humans, in some cases aided by tools or computer systems. The capability of large language models (LLMs) to generate text with near-zero human effort, however, along with models to generate images, audio, and video, suggest that the data to which humans are exposed may come to be dominated by AI-generated or Al-aided processes.\nResearchers have noted that the recursive training of AI models on synthetic text may lead to degeneration, known as \"model collapse\" (Shumailov et al., 2023). Our interest is in the inverse of this concern, focusing instead on the equilibrium effects on the distribution of knowledge within human society. We ask under what conditions the rise of AI-generated content and AI-mediated access to information might harm the future of human thought, information-seeking, and knowledge.\nThe initial effect of AI-generated information is presumably limited, and existing work on the harms of Al rightly focuses on the immediate effects of false information spread by \"deepfakes\" (Heidari et al., 2023), bias in AI algorithms (Nazer et al., 2023), and political misinformation (Chen and Shu, 2023). Our focus has a somewhat longer time horizon, and probes the impact of widespread, rather than marginal adoption.\nResearchers and engineers are currently building a variety of systems whereby AI would mediate our experience with other humans and with information sources."}, {"title": "Summary of Main Contributions", "content": "We identify a dynamic whereby AI, despite only reducing the cost of access to certain kinds of information, may lead to \"knowledge collapse,\" neglecting the long-tails of knowledge and creating an degenerately narrow perspective over generations. We provide a positive knowledge spillovers model with in which individuals decide whether to rely on cheaper AI technology or invest in samples from the full distribution of true knowledge. We examine through simulations the conditions under which individuals are sufficiently informed to prevent knowledge collapse within society. Finally, we conclude with an overview of possible solutions to prevent knowledge collapse in the AI-era."}, {"title": "Related Work", "content": "Technology has long affected how we access knowledge, raising concerns about its impact on the transmission and creation of knowledge. Yeh Meng-te, for example, argued in the twelfth century that the rise of books led to a decline in the practice of memorizing and collating texts that contributed to a decline of scholarship and the repetition of errors (Cherniack, 1994, p.48-49). Even earlier, a discussion in Plato's Phaedrus considers whether the transition from oral tradition to reading texts was harmful to memory, reflection and wisdom (Hackforth, 1972).\nWe focus on recent work on the role of digital platforms and social interactions, and mention only in passing the literature on historical innovations and knowledge (e.g. Ong, 2013; Mokyr, 2011; Havelock, 2019), and the vast literature on the printing press (e.g. Dittmar, 2011; Eisenstein, 1980). Like other media transitions before it (Wu, 2011), the rise of internet search algorithms and of social media raised concerns about the nature and distribution of information people are exposed to, and the downstream effects on attitudes and political polarization (Cinelli et al., 2021; Barber\u00e1, 2020).\nThe following section considers research on the impact of recommendation algorithms and self-selection on social media, and how this might generate distorted and polarizing opinions, as an analogy for understanding the transformation brought about by reliance on AI. We consider game theoretic models of information cascades as an alternative model for failure in social learning, in which the public to fails to update rationally on individuals' private signals. Next, we review the main findings of network analysis on the flow of information in social media, which also identify mechanisms which distort knowledge formation. We then examine the specific nature of generative AI algorithms, focusing on the problem of model collapse and known biases in AI outputs."}, {"title": "The media, filter bubbles and echo chambers", "content": "A common critique of social media is that they allow users to select in to \u201cecho chambers\u201d (specific communities or communication practices) in which they are exposed to only a narrow range of topics or perspectives. For example, instead of consulting the \u201cmainstream\" news where a centrist and relatively balanced perspective is provided, users are exposed to selective content that echoes pre-existing beliefs. In the ideological version of the echo-chamber hypothesis, individuals within a latent ideological space (for example a one-dimensional left-right spectrum), are exposed to peers and content with ideologically-similar views. If so, their beliefs are reinforced socially and by a generalization from their bounded observations, leading to political polarization (Cinus et al., 2022; Jamieson and Cappella, 2008; Pariser, 2011).\nA simple model for this assumes homophily within in a network growth model, in which similar individuals chose to interact. Implicitly the approach presumes that this is common on social media but not common within traditional media, which for technological reasons were constrained to provide the same content across a broad population with possibly heterogeneous preferences.\nThis general dynamic may hold even if traditional media and newspapers were themselves dynamic systems interacting with their consumers, markets and advertisers, and themselves adapting their message to specific communities and preferences (e.g. Angelucci, Cag\u00e9, and Sinkinson, forthcoming; Cag\u00e9, 2020; Boone, Carroll, and van Witteloostuijn, 2002).\nThe second main line of analysis focuses on \"filter bubbles,\" whereby the content to which users are exposed is selected based on a recommendation system. Jiang et al. (2019) model this as a dynamic process between a user's evolving interests and behavior (such as clicking a link, video, or text) and a recommender system which aims to maximize expected utility for the user. In their reinforcement learning-inspired framework, the aim is for the user to explore the space of items or topics without the algorithm assigning degenerate (extremely high or zero) probabilities to these items. As above, a key concern is the political or ideological content of rec-"}, {"title": "Network effects and Information Cascades", "content": "Information cascade models provide one approach to explaining a kind of herd behavior (where diverse and free individuals nonetheless make similar decisions). They explore the conditions under which private information is not efficiently aggregated by the public. This can occur where individuals sequentially make decisions from a discrete set after observing the behaviors but not the private signals of others. This can generate a \u201cherd externality\" (Banerjee, 1992) in which an individual ignores her private signal in deciding, and as a result the public is in turn unable to update on her private information. In the extreme, this can mean that all private information, aside from that of the first few individuals, is completely ignored (Bikhchandani, Hirshleifer, and Welch, 1998; Smith and S\u00f8rensen, 2000). In some variants of the model, individuals must pay to receive a signal, which encourages the tendency to want to free-ride on the information received by others, and thus the greater the cost, the more likely it is that a cascade develops.\nA related literature on the spread of information on social networks analyzes information cascades in terms of network structure, as a kind of contagion. Here, the focus is not on private information but how information flows within the network. For example, independent cascade models consider how an individual may change their beliefs based on some diffusion probability as a result of contact with a neighbor with that belief (Goldenberg, Libai, and Muller, 2001; Gruhl et al., 2004).\nMore generally, such models determine the probability of diffusion within a network as some function of the connected nodes, and may also incorporate additional characteristics such as each nodes' social influence, ideological or other preferences, or topics (Barbieri, Bonchi, and Manco, 2013). Alternatively, epidemic models allow that individuals may be in one of three states - susceptible, infected (capable of transmitting the information), and recovered (in which case they have the information but do not consider it worth sharing with others) (e.g. Kermack and McKendrick, 1927) and (Barrat, Barth\u00e9lemy, and Vespignani, 2008, ch.10)\nSocial (and even physical) proximity can lead individuals to share similar attitudes, such as when individuals randomly assigned housing together come to have attitudes similar to their apartment block and differing from nearby blocks (Festinger, Schachter, and Back, 1950), as modeled by Nowak, Szamrej, and Latan\u00e9 (1990). Empirically, Bakshy et al. (2012) show that weak-ties may be more important for information diffusion that strong-ties, while Centola (2010) demonstrates that the reinforcement of a message within a clustered network makes information spread more effective than in a random network. More sophisticated models allow for the evolution not only of opinion process but the edges between nodes of the network (Castellano, Fortunato, and Loreto, 2009, pp.47-48.).\nThese models suggest specific opinion-formation dynamics based on what other humans, texts, images, etc. an individual interacts with. By extension, we could consider the generalization of these networks to the case where LLMs play a key role as (possibly influential) nodes, or as determining how an individual navigates a knowledge graph. One of the key ideas of Web 2.0 was that users, not just authors or programmers, structure"}, {"title": "Model collapse", "content": "The idea of model collapse is rooted in the earlier phenomenon of \"mode collapse\" in generative adversarial networks (GANs). GANs are based on a generator neural network that proposes, e.g. an image, and a discriminator attempts to predict whether a given image is created by the generator or is a real image from the dataset. While ideally the generator attempts to produce images across the full range of input data, in practice they may settle into producing a narrow range of images for which it is good at fooling the discriminator, known as mode collapse (Goodfellow, 2016; Arora et al., 2017). The case of \"posterior collapse\" was also identified in modeling language data with variational autoencoders (Melis, Gy\u00f6rgy, and Blunsom, 2022).\nShumailov et al. (2023) introduced the term \"model collapse\" to describe a related process when models such as variational autoencoders, Gaussian mixture models, and LLMs are trained on data produced by an earlier version of the model. Incorporating AI-generated content in the training data causes loss of information which they categorize into two types. First, in \u201cearly model collapse,\" the tails of the distribution are lost due to statistical error (finite sampling bias) or functional approximation error, which leads to reversion to the mean. Second, \"late model collapse\u201d may occur when a model converges with narrow variance on a distribution unlike the original data. They provide evidence of such model collapse in LLMs and other models, see for example Figure 1.\nDohmatob et al. (2024) demonstrate conditions under which the injection of true (non AI-generated) data can preserve representation of the true distribution, though Bohacek and Farid (2023) show that even small amounts of synthetic data can poison an image model, and once distorted, it is difficult for such models to recover even after being trained on true data. Guo et al. (2023) demonstrate that training LLMs on synthetic data can lead to diminishing lexical, semantic and syntactic diversity."}, {"title": "Known biases in LLMS", "content": "Newer Al models such as LLMs are not immune to the problems of bias identified and measured in machine learning algorithms (Nazer et al., 2023) and which have plagued predictive algorithms in real-world uses cases"}, {"title": "A Model of Knowledge Collapse", "content": "A commonly held, optimistic view is that knowledge has improved monotonically over time, and will continue to do so. This indeed appears to be the case for certain scientific fields like physics, chemistry, or molecular biology, where we can measure the quality of predictions made over time. For example, accuracy in the computation of digits of \u3160 has increased from 1 digit in 200 BCE to 16 in 1424 (Jamashid al-Kashi) to $10^{14}$ digits recently.\nIn other domains, however, it is less clear, especially within regions. Historically, knowledge has not progressed monotonically, as evidenced by the fall of the Western Roman empire, the destruction of the House of Wisdom in Baghdad and subsequent decline of the Abbasid Empire after 1258, or the collapse of the Mayan civilization in the 8th or 9th century. Or, to cite specific examples, the ancient Romans had a recipe for concrete that was subsequently lost, and despite progress we have not yet re-discovered the secrets of its durability (Seymour et al., 2023), and similarly for Damascus steel (K\u00fcrnsteiner et al., 2020). Culturally, there are many languages, cultural and artistic practices, and religious beliefs that were once held by communities of humans which are now lost in that they do not exist among any known sources (Nettle and Romaine, 2000).\nThe distribution of knowledge across individuals"}, {"title": "Model Overview", "content": "The main focus of the model is whether individuals decide to invest in innovation or learning (we treat these as interchangeable) in the \u2018traditional' way, through a possibly cheaper Al-enabled process, or not at all. The idea is to capture, for example, the difference between someone who does extensive research in an archive rather than just relying on readily-available materials, or someone who takes the time to read a full book rather than reading a two-paragraph LLM-generated summary.\nHumans, unlike LLMs trained by researchers, have agency in deciding among possible inputs. Thus, a key dynamic of the model is to allow for the possibility that rational agents may be able to prevent or to correct for distortion from over-dependence on 'centrist' information. If past samples neglect the 'tail' regions, the returns from such knowledge should be relatively higher. To the extent that they observe this, individuals would be willing to pay more (put in more labor) to profit from these additional gains. We thus investigate under what conditions such updating among individuals is sufficient to preserve an accurate vision of the truth for the community as a whole.\nThe cost-benefit decision to invest in new information depends on the expected value of that information. Anyone who experiments with AI for, e.g. text sum-"}, {"title": "Results", "content": "Our main concern is with the view that AI, by reducing the costs of access to certain kinds of information, could only make us better off. In contrast to the literature on model collapse, we consider the conditions under which strategic humans may seek out the input data that will maintain the full distribution of knowledge. Thus, we begin with a consideration of different discount rates. First, we present the a kernel density estimate of public knowledge at the end of 100 rounds (Figure 3). As a baseline, when there is no discount from using AI (discount rate is 1), then as expected public knowledge converges to the true distribution, As AI reduces the cost of truncated knowledge, however, the distribution of public knowledge collapses towards the center, with tail knowledge being under-represented. Under these conditions, excessive reliance on AI-generated content over time leads to a curtailing of the eccentric and rare viewpoints that maintain a comprehensive vision of the world."}, {"title": "Discussion", "content": "We provide a theoretical framework for defining \"knowledge collapse\", whereby dependence on generative AI such as large language models may lead to a reduction in the long-tails of knowledge. Our simulation study suggests that such harm can be mitigated to the extent that (a) we are aware of the of the possible value of niche, specialized and eccentric perspectives that may be neglected by AI-generated data and continue to seek them out, (b) AI-systems are not recursively interdependent, as occurs if they use other AI-generated content as inputs or suffer from other generational effects, and (c) AI-generated content is as representative as possible of the full distribution of knowledge.\nEach of these suggest practical implications for how to manage AI adoption. First, while our work does not justify an outright ban, measures should be put in place to ensure safeguards against widespread or complete reliance on AI models. For every hundred people who read a one-paragraph summary of a book, there should be a human somewhere who takes the time to sit down and read it, in hopes that she can then provide feedback on distortions or simplifications introduced elsewhere. One extension to the model would be to allow for generational change but endogenize the choice of public subsidies to protect 'tail' knowledge. This is arguably what is done by governments that support academic and artistic endeavors that would otherwise have been under-provided by the private market. Protecting the diversity of information means also paying attention to the effect of AI adoption on the revenue streams of journalists that produce and not merely transmit information (e.g. Cag\u00e9, 2016).\nSecondly, there is an obvious need to avoid building recursively dependent AI systems (e.g. where one LLM or agent provides answers based on another AI-generated summary, etc.) and thereby playing an LLM-mediated game of 'telephone'. At a minimum, this requires a concerted effort to distinguish human- from AI-generated data. Preserving access to 'unmediated' texts, such as through a well-conceived retrieval augmented generation approach, can preserve the long-tails of knowledge (Delile et al., 2024), as may generating multiple results and re-ranking (Li et al., 2023).\nFinally, while much recent attention has been on the problem of LLMs misleadingly presenting fiction as fact (hallucination), this may be less of an issue than the problem of representativeness across a distribution of possible responses. Hallucination of verifiable, concrete facts is often easy to correct for. Yet many real world questions do not have well-defined, verifiably true and false answers. If a user asks, for example, \"What causes inflation?\" and a LLM answers \u201cmonetary policy\", the problem isn't one of hallucination, but of the failure to reflect the full-distribution of possible answers to the question, or at least provide an overview of the main schools of economic thought.\nThis could be considered in the setup of frameworks for reinforcement learning from human feedback and related approaches to shaping model outputs, since humans may by default prefer simple, monolithic answers over those that represent the diversity of perspectives. Particular care should also be given in the context of the use of AI in education, to ensure students consider not only the veracity of AI-generated answers but also their variance, representativeness, and biases, that is, to what extent they represent the full distribution of possible answers to a question.\nThe scaling laws (Kaplan et al., 2020; Hoffmann et al., 2022) demonstrate the advantage of training LLMs on the maximum amount of (quality) data. A valuable empirical question is therefore whether this leads to increasing or decreasing diversity within the training data (and the raises the related problem of the lack of transparency in the data used to train models). There are many diverse texts that could be included to expand the corpus, but practically, the approach of market-focused participants may be to focus on seeking texts with the lowest marginal cost (conditional on quality). This might exacerbate a reliance on texts that are not representative of the general public, such as if social media texts are easy to collect but not representative of the perspective of people who don't have access to social media or self-select out of them. Or, optimistically, companies with a global audience might be incentivized to seek out \"low and very-low resource languages\" (e.g. Gemini Team et al., 2023) and perhaps even the viewpoints and cultural perspectives of diverse users. Consideration should be given to ensuring and encouraging such diverse inputs as well as to monitoring of the diversity of outputs.\""}, {"title": "Defining Knowledge Collapse", "content": "To define knowledge collapse we need to distinguish between a few conceptual sets of 'knowledge', whether or not these are empirically observable. First, we con-sider the broad set of historical human knowledge that was at one point held in common within communities of humans, shared and reproduced in a regular way, which we might call 'broad historical knowledge'.\nSecond, we consider the set of knowledge that is held or accessible to us, (humans who are living in a given epoch), which we call 'available current knowledge.' In the example cited in the main section, the ancient Roman recipe for concrete is part of broad historical knowledge but not part of available current knowledge. Technological innovations from the printing press to the internet to AI mediate human interactions and human's exposure to historical and current sources of knowledge. The net effect might be to restrict or expand access to diverse knowledge and the long-tails of human knowledge. For example, the digitization of archives might make obscure sources available to a wider audience and thus increase the amount of 'broad historical knowledge' that is part of the \u2018available current knowledge.'\nWe also distinguish a third, narrower set of knowledge, which reflects not what is theoretically accessible to humans but which is readily part of human patterns of thinking or habits of thought. This we call 'human memory knowledge' or 'human working knowledge' by reference to human working memory.\nFor example, consider the problem of listing all the animals that have ever existed on earth. There might be some that humans previously knew about, but which subsequently went extinct and which do not exist anywhere among the scientific literature or individuals currently living on earth. More narrowly, the set of \"available current knowledge\" corresponds to the set of all animals that a team of all biologists could compile with access to the internet and other records. Finally, however, if we were able to conduct a survey of all humans on earth and ask them to name as many animals as possible in, say, one day, we would come up with a more limited list (that would include many repetitions).\nIn many practical applications, 'human working knowledge' is the most relevant because it is the knowledge that shapes human action and reflection. A doctor considering possible sources of a crossover pathogen might rely on their knowledge of common species in asking a patient if they had recently been in the presence of certain animals (even if a researcher who specializes in this area might consult know more and sources to find a longer possible list). A linguist trying to evaluate or create possible linguistic theories implicitly bases their judgement on the known language families and their structures, and so on. Edison and his team famously tried thousands of different filament materials, but if it bamboo had not been among the materials that came to mind as they searched alternatives, a practical electric bulb may have been invented only later.\nFinally, it is useful to define the 'epistemic horizon' as the set of knowledge that a community of humans considers practically possible to know and worth knowing. A common controversy in the public imagination is whether traditional medicines are worth consideration when searching for medical cures. Such traditional medicines might be outside of the epistemic horizon because they are not written down in the scientific literature, are only known by individuals speaking lesser known languages, or because the scientists in question consider them too costly to acquire or unlikely to be beneficial. One way to think about this relationship is as a generalization of 'availability bias', in which we take the set of readily recalled information to be more likely, important, or relevant (Tversky and Kahneman, 1973).\nIn these terms, we define \u2018knowledge collapse' as the progressive narrowing over time (or over technological representations) of the set of human working knowledge and the current human epistemic horizon relative to the set of broad historical knowledge.\nOn a theoretical level, the idea of epistemic horizon has an intellectual heritage in Immanuel Kant's argument about the forms and categories of understanding that underly the possibility of knowledge (Kant, 1933). Subsequent authors expanded on the implications if these categories are in some way fashioned by one's upbringing and community (e.g. Herder, 2024; Hegel, 2018; Mannheim, 1952). A related concern is the way that"}, {"title": "Appendix", "content": "As mentioned above, the reported results used a t-distribution with 10 degrees of freedom, which has slightly wider tails than a standard normal distribution. We can compare the results with a standard normal distribution (i.e.a t-distribution as the degrees of freedom becomes large) or with wider tails. In Figure 7, we plot a comparison of the results from the main section (with 10 degrees of freedom with wider or narrower tails (3 and 9999 degrees of freedom respectively). The main difference is for more extreme discounts provided by AI (< 0.7), for which the wider tails contribute to knowledge collapse (i.e.generate a public knowledge distribution further from the true distribution). Narrower tails, such as from a standard normal distribution, generate results broadly similar to the main model. Thus, as expected more information in the tails makes the effect of knowledge collapse more pronounced, but is plays less of a role than the other parameters discussed above in determining the dynamic of collapse."}, {"title": "Comparing width of the tails", "content": "That is, the innovation (individual payoff) I generated by an individuals additional (n + 1)th sample is calculated with respect to the true pdf $P_{true}(x)$ and the current public pdf $p_{public}(x)$, based on the Hellinger distance $H(p(x), q(x))$, as follows:\n\ninnovation = previous distance - new distance\n\n$I = H(p_{public}(x), P_{true}(x)) - H(p^{n+1}_{public}(x), P_{true}(x))$"}, {"title": "Known biases in LLMS", "content": "For the previous esti-mate $\u00fb_{t-1}$, the new estimate $\u00fb_t$ for each of the full- and truncated-samples is calculated from the observed value in the previous round ($I_{t-1}$) as:\n$\u00fb_t = \u00fb_{t-1} + \u03b7 \\cdot (\u00fb_{t-1} - I_{t-1})$"}]}