{"title": "Multi-Objective Planning with Contextual Lexicographic Reward Preferences", "authors": ["Pulkit Rustagi", "Yashwanthi Anand", "Sandhya Saisubramanian"], "abstract": "Autonomous agents are often required to plan under multiple objectives whose preference ordering varies based on context. The agent may encounter multiple contexts during its course of operation, each imposing a distinct lexicographic ordering over the objectives, with potentially different reward functions associated with each context. Existing approaches to multi-objective planning typically consider a single preference ordering over the objectives, across the state space, and do not support planning under multiple objective orderings within an environment. We present Contextual Lexicographic Markov Decision Process (CLMDP), a framework that enables planning under varying lexicographic objective orderings, depending on the context. In a CLMDP, both the objective ordering at a state and the associated reward functions are determined by the context. We employ a Bayesian approach to infer a state-context mapping from expert trajectories. Our algorithm to solve a CLMDP first computes a policy for each objective ordering and then combines them into a single context-aware policy that is valid and cycle-free. The effectiveness of the proposed approach is evaluated in simulation and using a mobile robot.", "sections": [{"title": "1 INTRODUCTION", "content": "In many real-world applications, such as navigation [12, 26], and warehouse management [13, 22], autonomous agents must optimize multiple, potentially competing objectives. The priority over these objectives can be conveniently expressed via a lexicographic ordering, and may vary based on the context. Consider a semi-autonomous car navigating in a city (Figure 1), where the possible set of objectives includes minimizing travel time, ensuring pedestrian safety, and minimizing going over uneven road surfaces. When the car navigates through a construction zone, it must prioritize minimizing going over uneven surfaces, followed by pedestrian safety, and finally the speed. When the car is on a highway, it is desirable to prioritize travel time over other objectives, since a highway is designed for high-speed navigation and is typically free of pedestrian traffic and other obstacles. When in urban areas with high foot traffic, ensuring pedestrian safety becomes the highest priority. Thus, during the course of its navigation, the car must optimize different objective orderings, corresponding to the context associated with the region. The reward functions associated with the objectives may also vary based on context.\nUsing contextual information for decision-making has often proven to be valuable, such as in contextual bandits [17], context-aware planning [16], contextual information retrieval and text generation [14, 32]. The existing literature often defines context as a parameter that influences the environment dynamics and rewards [6, 16, 18], the position of obstacles and other agents [15, 29], or the area of operation [4]. We define context as a set of exogenous features in the state representation that determine the objective ordering and corresponding reward functions for each state.\nWe consider a lexicographic ordering over objectives, which is a convenient approach for representing relative preferences over multiple objectives. Traditional approaches for multi-objective decision-making typically support only a single preference ordering over objectives throughout the entire state space. A prior work [34] that supports planning with multiple state partitions could potentially be adapted to handle different priority orderings. However, it lacks a systematic method for defining partitions and does not provide an explicit mapping from states to their respective objective orderings. Their framework also does not support varying reward functions for each partition or ensure that the resulting policy is valid and cycle-free. A recent work [36] uses a context map to track the scalarization weights for each objective in every state. However, it does not scale to larger state spaces.\nOur paper addresses these challenges by introducing Contextual Lexicographic Markov Decision Process (CLMDP), a framework that supports seamless switching between different objective orderings, determined by the state's context, during the course of agent operation. We present an algorithm to solve CLMDP by first computing a policy for each objective ordering across the state space, and then combining them into a single context-aware policy, given a state-context mapping. Our algorithm also ensures that the action recommendations across contexts do not conflict with each other, during the merge step. When state-context mapping is unknown, it is derived using Bayesian inference and limited expert trajectories.\nOur primary contributions are: (1) formalizing the problem of contextual multi-objective planning as a contextual lexicographic MDP; (2) presenting an algorithm to solve a CLMDP, producing a policy that is free of conflicts in a setting where multiple contexts coexist; (3) theoretical analysis of the algorithm's performance; (4) a Bayesian approach to infer the state-context mapping; and (5) empirical evaluation in simulation and using a physical robot."}, {"title": "2 RELATED WORK", "content": "Multi-objective planning. Multi-objective planning is gaining increasing attention. Common approaches to solve multi-objective planning problems include scalarization [2, 31, 33], lexicographic planning [19, 24, 25, 34], constraint optimization [9, 10], and Pareto optimization [3, 20]. Scalarization combines multiple objectives into a single objective value using weighted sum. While it is a popular approach, it often requires non-trivial parameter tuning [23]. Our work is based on the lexicographic formulation that considers a lexicographic ordering over objectives and solves them sequentially. While the lexicographic Markov decision process (LMDP) [34] can support multiple state partitions, each corresponding to a different objective ordering, it suffers from the following limitations: (1) lack of a principled approach to define state partitions, especially when a state could be associated with multiple contexts such as weather, road conditions, and time of day; (2) hard-coded partitions and lack of explicit state-to-context mapping which makes it difficult to adapt to newer settings where the set of contexts may change; (3) does not support scenarios where the context influences the reward function; and (4) lack of tools to resolve conflicts in action recommendation that arise from solving partitions sequentially, with no principled approach to determine the order. The CLMDP framework addresses these shortcomings and facilitates smooth transitions between contexts, avoiding the risk of conflicting actions. The explicit state-to-context mapping in CLMDP offers flexibility and scalability in handling new scenarios, without requiring manual interventions to redefine partitions or objective priorities.\nConstraint optimization. approach optimizes a single objective and converts other objectives into constraints [9, 10]. This approach cannot efficiently balance the trade-offs between different objectives [11]. Pareto optimization finds non-dominated solutions across objectives [3, 20], making it unsuitable for scenarios where specific preference orderings must be satisfied.\nContext-aware planning. Prior works use contextual information to determine environment dynamics and rewards [6, 16, 18], or represent specific configurations like obstacle layouts and operational areas [4, 15, 29]. While many definitions of context exist for different problem settings, we focus our discussion on those most pertinent to multi-objective scenarios. In multi-objective settings, context has been used to assign scalarization weights [36] but this approach struggles to scale in larger state spaces. We integrate contextual information in a lexicographic setting to enable planning with different objective ordering in different regions of the state space, with associated reward functions determined by the context.\nBayesian Inference. It is commonly used to infer values of unknown parameters by incorporating prior knowledge and updating beliefs as new information becomes available [1, 5, 37]. Bayesian inference is often used in inverse reinforcement learning (IRL) to estimate reward functions [7, 21], and goal inference in multi-agent settings [30, 37]. In multi-objective optimization problems, Bayesian methods are used to approximate the set of Pareto optimal solutions for competing objectives [5, 8], and identify solutions that satisfy user-defined preference ordering over competing objectives [1]. In this work, we apply Bayesian inference to determine the most likely context of a state, using limited number of expert trajectories."}, {"title": "3 CONTEXTUAL LEXICOGRAPHIC MDP", "content": "Consider an agent operating in an environment with multiple objectives modeled as a multi-objective Markov decision process (MOMDP). The agent must optimize a lexicographic ordering over the $n$ primitive objectives $o = \\{o_1,..., o_n\\}$. We focus on goal-oriented MDPs where one of the objectives $o_i$ is to maximize the reward associated with reaching the goal. The lexicographic ordering over objectives at a particular state is determined by the context associated with it. The context, inferred from a set of exogenous features in the state representation, determines the objective ordering at a state and the reward functions associated with the objectives. In the example illustrated in Figure 1, the set of contexts in the environment are: highway, urban area, and construction zone. Each context imposes a unique ordering over the objectives, which determines an acceptable behavior for that context. Since an agent may encounter multiple contexts during the course of its operation, its reasoning module must be able to seamlessly switch between optimizing different objective orderings.\nWe introduce Contextual Lexicographic Markov Decision Process (CLMDP), a framework that facilitates optimizing different objective orderings in different regions of the state space.\nDEFINITION 1. A contextual lexicographic Markov decision process (CLMDP) is denoted by $M = (C, \\Omega, o, w, f_w, S, A, T, R, f_R, Z)$, where:\n*   $C = \\{C_1,..., c_m\\}$ denotes a finite set of contexts;\n*   $\\Omega$ is a lexicographic ordering over contexts in $C$;\n*   $o = \\{o_1,..., o_n\\}$ denotes the set of primitive objectives;\n*   $w = \\{@_1, ..., @_m\\}$ is a set of unique lexicographic orderings over all primitive objectives, with $w_i$ denoting a lexicographic ordering over all objectives such as $o_1 > ... > o_n$, $o_i \\in o$;\n*   $f_w: C \\rightarrow w$ is a function that maps a context $c_j \\in C$ to a lexicographic ordering over the primitive objectives $w_i \\in w$;\n*   $S$ is a finite set of states, with initial state $s_0 \\in S$;"}, {"title": "4 SOLUTION APPROACH", "content": "We begin with an overview of our solution approach, illustrated in Figure 2. Given $Z$, our approach solves CLMDPs by computing a policy $\\pi_i$ for each context $c_i \\in C$ independently, and then combines them into a global policy $\\pi_G$, based on the context associated with each state. Combining different $\\pi_i$ into $\\pi_G$ may result in cycles as each policy is computed independent of other policies and contexts. The cycles are detected and resolved by updating the policies associated with lower priority contexts, based on $\\Omega$.\n4.1 Computing global policy $\\pi_G$\nA policy $\\pi_i$ for each context $c_i \\in C$ is first computed independent of other contexts. We do this by considering each $c_i \\in C$ to be the only context across the entire state space and use the corresponding lexicographic ordering over objectives ($f_w(c_i)$) and the associated reward functions ($f_R(c_i)$) over all states. This multi-objective problem, with a single ordering over the objectives, is solved using lexicographic value iteration algorithm [34]. Each $\\pi_i$ specifies the optimal actions for its respective context. These individual policies are then compiled into a global policy $\\pi_G$, where actions are mapped to each state, based on the actual context of each state, following state-to-context mapping $Z$.\n4.2 Conflict Detection\nCombining multiple policies, computed under different contexts and associated objective orderings, can lead to conflicting action recommendations that result in cycles and affect goal reachability.\nDEFINITION 2. A policy $\\pi_G$, in a goal-oriented MDP with goal state $s_g$, is said to have a conflict if there exists at least one state $s$ from which the probability of reaching the goal, $Pr(s_g|s, \\pi_G) = 0, \\exists s \\in S$.\nDEFINITION 3. A conflict-free policy $\\pi_G$ has a non-zero probability of reaching the goal from every state $s, Pr(s_g|s, \\pi_G) > 0, \\forall s \\in S$.\nASSUMPTION 1. There exists a conflict-free policy under $\\Omega$.\nWe assume that there exists an underlying conflict-free policy for CLMDP and any cycles detected are due to $\\pi_G$ computation based on policies calculated for individual contexts. To detect conflicts that introduce dead-ends, Algorithm 1 estimates the goal reachability for each state, under $\\pi_G$. The goal reachability is calculated using a Bellman backup-based method (lines 11-18) with a reward function $R_e$ defined as follows:\n$R_e(s) = \\begin{cases}\n+1, & \\text{for } s = S_{\\text{goal}} \\\\\n0 & \\text{otherwise}.\n\\end{cases}$\nStates with $V_r(s) = 0$ do not have a path to the goal, indicating a conflict (line 25). Thus, if $V_r(s) = 0$ for any $s \\in S$, it indicates a conflict. Conversely, if all states have $V_r > 0$, the policy is conflict-free. To handle large state spaces, a modified value iteration is used, marking states as solved once they individually converge"}, {"title": "4.4 Theoretical Analysis", "content": "In this section, we analyze the correctness and time complexities of Algorithms 1 and 2.\nPROPOSITION 4.1. Algorithm 1 correctly identifies conflicts.\nPROOF. We first describe $V_r$ convergence and then show that states from which the goal cannot be reached have $V_r(s) = 0$ under reward $R_e$ defined in Sec. 4.2. Since Algorithm 1 uses Bellman backup (lines 12-14), $V_r$ is guaranteed to converge [28].\nLet $S_u$ denote the set of all states from which $s_{\\text{goal}}$ is unreachable, and let $S_r$ represent the set of all states that can reach the goal, such that $S = S_u \\cup S_r$. In the worst case, the unsolved set may be the entire state space, $S_{\\text{unsolved}} = S$. $V_r$ is first initialized to zero for all states (line 3): $V_r^{(k=0)}(s) = 0, \\forall s \\in S$. We now show using induction that $V_r(s_u) = 0, \\forall s_u \\in S_u$ always. For $k = 0, V_r^{(0)}(s_u) = 0$ (by initialization). Using Bellman equation, the value of a state $s$ in the $k$th iteration is updated as:\n$V_r^{(k)}(s) = R_e(s) + \\gamma \\max_a \\sum_{s' \\in S} T(s, a, s')V_r^{(k-1)}(s')  \\qquad (1)$\nwhere $T(s, a, s')$ is the transition probability from $s$ to $s'$ using action $a$, and $\\gamma$ is the discount factor. Note that, while all actions in a state $s$ are considered here for generality, Algorithm 1 only requires $\\pi_G(s)$ for conflict detection. By Definition 2, $T(s_u, a, s_r) = 0$ and so the values of any $s_u \\in S_u$ never get updated. Hence for $k$th iteration: $V_r^{(k)}(s_u) = 0$. For iteration $k + 1$, using Equation 1,\n$V_r^{(k+1)}(s_u) = R_e(s_u) + \\gamma \\max_a \\sum_{s' \\in S} T(s_u, a, s')V_r^{(k)}(s'), \\forall s_u \\in S_u$\nFor $V_r^{(k+1)}(s_u)$ to be non-zero, at least one of its successors must have a path to goal, $s' \\in S_r$. Substituting $R_e(s_u) = 0$ and $T(s_u, a, s_r) = 0$ in the above equation, we get $V_r^{(k+1)}(s_u) = 0$. Thus, Algorithm 1 identifies a conflict, when there exists one. $\\qquad \\blacksquare$\nPROPOSITION 4.2. The time complexity of Algorithm 1 is $O(|S|^2)$.\nPROOF. The complexity of Algorithm 1 depends on the time taken for calculating $V_r$ and the time taken to check for conflicts based on the computed $V_r$. Computing $V_r$ (lines 8-24) is essentially policy evaluation with $\\pi_G$ under reward $R_e$. Hence the complexity of this step is $O(|S|^2)$. The worst case complexity of conflict detection step (lines 25-27) is $O(|S|)$ since it cycles through the entire state space. Thus, the worst case time complexity of Algorithm 1 is $O(|S|^2 + |S|) = O(|S|^2)$. $\\qquad \\blacksquare$\nPROPOSITION 4.3. Algorithm 2 guarantees a conflict-free policy.\nPROOF. We prove this using the property of lexicographic ordering, where the optimality of a policy in a higher-priority context does not depend on those of lower priority. Consider a meta-ordering of contexts $\\Omega = c_1 > ... > c_m$. Algorithm 2 identifies the lowest-priority context involved in the conflict, $c^*$ (lines 4-6). The algorithm then marks all contexts from the identified $c^*$ (in line 6) to least priority context $c_m$ for updates (lines 7-10). Higher-priority contexts are kept fixed (lines 11-12) while the policies for the identified contexts are updated using lexicographic value iteration (line 14). After the context's policy is updated, the actions are fixed (lines 15-17) before moving on to lower priority contexts following $\\Omega$.\nWe now show that the final iteration, where all context policies are updated in sequence according to $\\Omega$, yields a conflict-free policy. Specifically, considering policy update in line 14:\n$\\pi_{c_i}(s) = \\arg \\max_{a \\in A} \\Big[ R_{c_i}(s) + \\gamma \\sum_{s' \\in S} T(s, a, s') V_{c_i}(s') \\Big],  \\qquad \\forall 2 \\le i \\le m$\nHere, $A^{\\text{new}}$ represents the action space, with actions fixed for states in the higher-priority contexts $c_1$ through $c_{i-1}$. Since these updates do not affect the higher-priority contexts, no new conflicts can be introduced in $c_1,..., c_{i-1}$. Thus, the sequential updates across all contexts, respecting the lexicographic ordering $\\Omega$, ensure that no conflicts arise in higher-priority contexts. The resulting policy is also verified for conflicts using Algorithm 1 (line 18), ensuring that Algorithm 2 terminates with a conflict-free policy. $\\qquad \\blacksquare$\nPROPOSITION 4.4. The time complexity of Alg. 2 is $O(|C|^2 |S|^2 |A|)$."}, {"title": "5 LEARNING STATE-CONTEXT MAPPING", "content": "The mapping between the states and contexts, $Z$ is critical for effective planning but this information may sometimes be unavailable to the agent a priori. We address this challenge by using a Bayesian approach to infer the likely context of a state, using a limited number of expert demonstrations. Note that we assume access to all other parameters of CLMDP, except $Z$.\nConsider a set of expert trajectories $\\tau = \\{\\tau_1,..., \\tau_n\\}$, where each $\\tau_i$ is a trajectory that originates at a random start state and terminates at the goal state $s_g, \\tau_i (s) = \\{ (s, a_0), . . ., (s_g, a_n)\\}$. Given $\\tau$, the posterior probability of a context $c$ is computed as,\n$Pr(c|s, \\tau, t) \\propto \\sum_{a \\in A} Pr(r|s, a, c) \\cdot Pr(a|s, c, t) \\cdot Pr(c|t) \\qquad (2)$\nwhere $Pr(r|s, a, c)$ is the probability of observing the reward vector $r$ when executing action $a$ in state $s$ under context $c$, $Pr(a|s, c, t)$ is the probability of the expert executing action $a$ in state $s$ if $c$ is the underlying context associated with $s$, and $Pr(c|t)$ denotes the prior probability, given $t$. We marginalize over actions to ensure that the computed posterior reflects the overall probability of context $c$ across all potential actions that the expert could have taken, given the state and reward (which varies based on the context). We first describe how these terms are calculated for state-action pairs in the dataset and then discuss how to estimate the posterior for other states. Algorithm 3 outlines the steps involved in inferring $Z$.\nLikelihood estimation: The probability of observing a particular reward vector in the data is either one or zero, based on $f_R$ in the CLMDP (line 10). That is, $Pr(r|s, a, c) = 1$ when $r$ matches the reward vector under context $c$ ($f_R(c)$), and $Pr(r|s, a, c) = 0$ otherwise.\nAction probability: For a state $s$ in the expert data, the probability of taking an action $a$ under context $c$ is either one or zero depending on whether the expert followed that action during demonstration. To estimate this probability, we calculate a set of possible contexts, $P(s_k, a_k)$, that the expert might have followed for each state-action pair in the data, as follows. First, we compute a policy $\\pi_c$ for each"}, {"title": "6 EXPERIMENTS", "content": "We evaluate our approach against six baselines using three domains in simulation and validate our results through hardware experiments. Unless specified otherwise, all algorithms were implemented by us in Python and the simulations were run on an iOS machine with 18 GB RAM. 1\nBaselines. We evaluate the performance of two variants of our approach, contextual planning with conflict resolver: one with a provided state-to-context mapping $Z$ (01), and the other where $Z$ is learned by Bayesian approach (O2) using Algorithm 3. The performances are compared with six baselines:\n*   B1: Task only-a single objective planning that focuses solely on reaching the goal state and ignores the context;\n*   B2: LMDP using $\\Omega$-applies the lexicographic MDP formulation [34] assuming the entire state space falls under the highest priority context in $\\Omega$;\n*   B3: Scalarization using $\\Omega$-policy computation using scalarization, with all states mapped to the highest priority context in $\\Omega$;\n*   B4: LMDP for contexts-modifies LMDP [34] to plan for multiple contexts in descending order of priority following $\\Omega$;\n*   B5: Adaptive scalarization from Yang et al. (2019) [36]-uses a deep neural network (DNN) to learn a policy based on state and scalarization weights as input, producing actions as output;\n*   B6: Contextual planning w/o resolver-our approach with $Z$ given but without using the conflict resolver;\nB2 and B3 are multi-objective planning but with a single ordering over objectives. The following domains are used for evaluation.\nSample Collection using Salp. A salp-inspired underwater robot [27] optimizes the collection and deposition of chemical sample (01) while minimizing coral damage (02) and optimizing battery usage by avoiding eddy currents (03). The robot operates under three contexts: \\{c_1: task completion, c_2: coral, c_3: eddy\\}. Context $c_1$ prioritizes depositing the sample over minimizing coral damage and battery usage: $o_1 > o_2 > o_3$. Context $c_2$ prioritizes minimizing coral damage, followed by depositing the sample and battery usage: $o_2 > o_1 > o_3$. Context $c_3$ prioritizes battery usage, then depositing the sample, and lastly minimizing coral damage: $o_3 > o_1 > o_2$.\nStates where the agent carries the samples and is around corals are mapped to the coral context ($c_2$). Locations with eddy currents are assigned to context $c_3$. All remaining states are mapped to $c_1$. The meta-ordering of these contexts is $\\Omega \\equiv c_2 > c_1 > c_3$.\nSemi-Autonomous Taxi. We modify the semi-autonomous driving domain from [34] to consider three objectives and multiple contexts. The agent operates under three objectives: quickly dropping off passengers (01), maximizing travel on autonomy-enabled roads (02), and minimizing passenger discomfort by avoiding potholes (03)."}, {"title": "7 RESULTS", "content": "We evaluate the effectiveness of various approaches for contextual multi-objective decision making in terms of: (1) how well a technique can balance the trade-off between different objective values, measured by minimum value across objectives, and (2) validity of the resulting policy, measured in terms of number of conflicts detected and resolved. Additionally, we validate our approach using a mobile robot in an indoor setup of warehouse domain (Figure 5)."}]}