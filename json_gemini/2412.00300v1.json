{"title": "PlanCritic: Formal Planning with Human Feedback", "authors": ["Owen Burns", "Dana Hughes", "Katia Sycara"], "abstract": "Real world planning problems are often too complex to be effectively tackled by a single unaided human. To alleviate this, some recent work has focused on developing a collaborative planning system to assist humans in complex domains, with bridging the gap between the system's problem representation and the real world being a key consideration. Transferring the speed and correctness formal planners provide to real-world planning problems is greatly complicated by the dynamic and online nature of such tasks. Formal specifications of task and environment dynamics frequently lack constraints on some behaviors or goal conditions relevant to the way a human operator prefers a plan to be carried out. While adding constraints to the representation with the objective of increasing its realism risks slowing down the planner, we posit that the same benefits can be realized without sacrificing speed by modeling this problem as an online preference learning task. As part of a broader cooperative planning system, we present a feedback-driven plan critic. This method makes use of reinforcement learning with human feedback in conjunction with a genetic algorithm to directly optimize a plan with respect to natural-language user preferences despite the non-differentiability of traditional planners. Directly optimizing the plan bridges the gap between research into more efficient planners and research into planning with language models by utilizing the convenience of natural language to guide the output of formal planners. We demonstrate the effectiveness of our plan critic at adhering to user preferences on a disaster recovery task, and observe improved performance compared to an Ilm-only neurosymbolic approach.", "sections": [{"title": "I. INTRODUCTION", "content": "The ability of human planners to effectively manage situations involving complex, multifaceted objectives is limited. We consider the case of a non-expert human planner in particular. In an increasingly interconnected world the potential downstream impacts of failed planning only grow, a fact borne out by the worldwide supply chain shortages seen in the wake of COVID [1].\nFormally defining tasks, domains, and goals in a symbolic language, such as the Planning Domain Definition Language (PDDL), allows for automating plan generation from provided descriptions. While the classical planners carrying out that generation excel at solving complex tasks, the strict format in which problems must be specified typically makes these systems impractical for the non-experts in charge of planning that could benefit from them [2]. While large language models (LLMs) have demonstrated proficiency at translating natural language to symbolic language [3], the resulting specifications often suffer from syntactical mistakes or deviate from the user's intent [4].\nRecent research into utilizing LLMs for generating task description in PDDL have focused primarily on the ability of an LLM to generate accurate PDDL from natural language; in this paper, we focus on developing an LLM-based system for collaborative human-AI planning. In such a setting, plan generation may involve multiple iterations of human feedback to an Al in order to effectively capture the human's preferences for various plan constraints. Approaches focused purely on LLM-based PDDL generation also lack the ability to search the space of available plans beyond what may be arrived at downstream of the LLM's generation; we address that limitation by using a genetic algorithm to efficiently explore the planning space. Our contributions are as follows:\n1) We introduce PlanCritic, a neurosymbolic framework capable of assisting human planners in dynamic and complex environments by optimizing a plan's state trajectory constraints with respect to their preferences.\n2) We use reinforcement learning with human feedback and a genetic algorithm-based optimizer to search the space of possible plans beyond what can be derived from an LLM's outputs.\n3) We integrate PlanCritic into a broader cooperative planning system\n4) We demonstrate that PlanCritic outperforms LLM-only neurosymbolic approaches at replanning in response to changing user preferences in a time-constricted planning environment."}, {"title": "II. BACKGROUND AND RELATED WORKS", "content": "Interest in combining the flexibility of neural models with the speed and correctness of classical planners has been extensively studied. A number of early works focused on learning domains, with [5] using a deep-q network to translate existing instruction manuals into domains one-shot and [6] going a step further to learn STRIPS domains online with the"}, {"title": "A. Neurosymbolic Planning", "content": "RLHF is in natural language processing tasks, where the goal is to align language models with human preferences and ethical guidelines. For instance, OpenAI's GPT models have been fine-tuned using RLHF to ensure that their responses are not only relevant but also adhere to safety and ethical standards [20]. In the context of planning and decision-making, RLHF has been employed to refine the outputs of planning algorithms based on user feedback. For example, [21] demonstrated the use of RLHF in Atari games, where human feedback was used to train an agent to perform better than with traditional re-ward functions alone. Similarly, in robotic manipulation tasks, RLHF has been used to teach robots complex behaviors that are difficult to specify through conventional reward functions [22]."}, {"title": "III. METHOD", "content": "To ensure performance in highly dynamic environments, we focused on optimizing a plan with respect to user feedback as opposed to generating entire problem files. This enabled PlanCritic to rapidly adapt the way a plan is executed without the risk of derailing the ground-truth end goal. To achieve this, for any given problem $P_i$ we treat the classical planner $p$ as a model parameterized by a set of constraints $C_i = {C_1, C_2, ..., c_{on}}$ specified in PDDL such that:\n$\\rho : (C^i, P^i) \\rightarrow S$\nWhere $S$ is the resulting set of steps constituting the plan. In order to optimize this model with respect to user feedback specified in natural language, we followed RLHF and trained a reward model to score plans generated by the planning model."}, {"title": "A. Reinforcement Learning with Human Feedback", "content": "We assume that the symbolic planner used is deterministic. Thus, each $C^i$ corresponds to only one $S$, enabling us to use the reward model to uniquely determine the adherence of a constraint set to the feedback provided by the user.\n1) Reward Model: We implemented the reward $v$ as an LSTM-based classifier which could determine if a single plan adhered to a single planning constraint specified in natural language. It sequentially processes each step of the plan combined with the planning constraint in question. We do this for all the current planning constraints, and return the percentage adhered as the measure of fitness."}, {"title": "B. Genetic Algorithm", "content": "While RLHF approaches typically use a gradient-based optimizer to tune their initial model with respect to their score model, the use of an external symbolic planner to generate plans makes this gradient-based approaches infeasible. To address this concern we used a genetic algorithm, treating dif-ferent $C^i$ for problem $i$ as individuals in the GA's population. Since we used PDDL, we can rigidly define a constraint as:\n$c^i = [not] t p$\nwhere $t \\in {always, sometime, ...}$\n$p \\in \\mathcal{P}_{preds}$\nDefining constraints in this way gives three axes of change by which to mutate a constraint without entirely replacing it: negation, changing its state-trajectory constraint $t$, and modifying one of the arguments of its predicate $p$. In addition to constraint-level mutations, we define adding and removing constraints as rare individual-level mutations to ensure more rapid exploration and prevent getting stuck in local minima. Algorithm 1 describes the mutation operation used by the GA.\nWe describe the crossover operation in Algorithm 2. The crossover operation generates a child by selecting a random crossover point, and combining the constraints before the crossover point from one parent with the constraints after the crossover point from the other parent.\nTo assess the fitness of the individuals in the population, we use the LSTM-based reward model $v$ described in the previous section."}, {"title": "C. User Interaction", "content": "To both maximize the information available to the reward model and ensure that the genetic algorithm starts its op-timization in the neighborhood of the actual solution, we implemented the system architecture described in Fig. 1 as an interface between the user and the GA-based planner. After the user gives their preferences, GPT-4 is utilized to translate these preferences into symbolically grounded natural language constrains we term mid-level goals. Examples are given in Table I. These mid-level goals are then used by GPT-4 to generate an initial candidate $C$. This initial constraint set is duplicated to produce the initial population of candidate constraints for the GA, and these duplicates are mutated to ensure diversity. To prevent issues caused by GPT-4 returning invalid PDDL constraints, we developed a parser which takes in the model's output and corrects syntactical errors with the most likely valid replacement as determined by Gestalt pattern matching [24]. Once planning is complete, GPT-4 summarizes the result of planning and each of the plan steps and returns the outcome to the user.\n1) Cooperative Planning System: PlanCritic is developed as a component of a broader planning architecture which tack-les planning from plan creation through the end of execution. Our system fits into the beginning of that process, enabling users to adapt a plan created for a static goal to meet the shifting constraints of the planning environment. The interface developed to integrate PlanCritic with the user-facing system is shown in 2. Users enter preferences on the left, and a step-by-step summary of the plan is found on the right in addition to a visual representation of the plan execution and a summary of the planning outcome."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "To demonstrate the effectiveness of our GA-based opti-mization approach, we compared the performance of the plan revision system with and without the GA, in the latter case planning exclusively based on the constraints guessed by GPT-4. The domain we used was a waterway restoration domain with the goal being to retrieve a ship from a location blocked by debris and tow it to a ship dock. A figure illustrating the environment is given in Fig. 3. We used Optic [25] as the symbolic planner to generate plans from problem descriptions and constraints.\nTo ensure our results were as aligned with the downstream use of PlanCritic as possible, we conducted a user study in which participants stated each of the planning objectives in a predefined list as preferences in their own words and allowed the system to attempt to adhere to them."}, {"title": "V. RESULTS", "content": "Table II shows the results of our user study on the six predefined planning objectives chosen. Overall, the genetic algorithm had a higher success rate of 75% compared to 53% for GPT-4 by itself, which aligned exactly with its validation accuracy during training. Table III shows a cross-comparison of the results of the GA and the LLM. We find that the genetic algorithm is especially useful at correcting the LLM when it gives an incorrect answer, returning a correct plan 88% of the time when the initial guess generated by the LLM is either wrong or fails to generate a plan. However, in 36% of cases where the LLM did give an initially correct answer, the final answer returned by the GA is incorrect. In three out of four such cases, the correct answer required multiple constraints. In these cases, the GA had mutated one of the initially correct constraints into an incorrect one but still arrived at a partially correct answer (e.g., removing one out of the two desired pieces of debris) which the reward model misclassified. We attribute this to the way constraints were selected for the re-ward model training set; because the constraints corresponding to positive and negative samples were selected randomly, it was unlikely to have similar constraints be both positive and negative for the same plan, making the model less precise when detecting \"near misses\u201d."}, {"title": "VI. FUTURE WORK", "content": "We plan to conduct further studies testing out different architectures of reward model, as well as measuring how overall performance changes when using a smaller LLM (e.g. GPT-3.5) to generate the initial candidate. Potential avenues for expanding the scope of PlanCritic include support for weighted preferences and expanding on its ability to explicitly re-plan when downstream plan execution fails due to a change in the environment."}, {"title": "VII. CONCLUSION", "content": "We propose PlanCritic, a neurosymbolic architecture to op-timize PDDL plans with respect to user preferences in online and dynamic scenarios. We approach the problem through the lens of reinforcement learning with human feedback, treating a classical planner as a model parameterized by plan constraints and optimizing it with a genetic algorithm. We find that our approach is superior at generating plans whose state trajectory aligns with stated user preferences than a neurosymbolic architecture using an LLM alone, and that it is extremely effective at catching the LLMs mistakes. However, we also find that the reward model underpinning the genetic algorithm is prone to failure when a plan is a \u201cnear miss\u201d to the mid-level goal, though we expect that this can be resolved by accounting for this during model training in future works."}]}