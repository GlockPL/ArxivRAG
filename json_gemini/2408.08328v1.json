{"title": "Unleash The Power of Pre-Trained Language Models for Irregularly Sampled Time Series", "authors": ["Weijia Zhang", "Hao Liu", "Chenlong Yin", "Hui Xiong"], "abstract": "Pre-trained Language Models (PLMs), such as ChatGPT, have significantly advanced the field of natural language processing. This progress has inspired a series of innovative studies that explore the adaptation of PLMs to time series analysis, intending to create a unified foundation model that addresses various time series analytical tasks. However, these efforts predominantly focus on Regularly Sampled Time Series (RSTS), neglecting the unique challenges posed by Irregularly Sampled Time Series (ISTS), which are characterized by non-uniform sampling intervals and prevalent missing data. To bridge this gap, this work explores the potential of PLMs for ISTS analysis. We begin by investigating the effect of various methods for representing ISTS, aiming to maximize the efficacy of PLMs in this under-explored area. Furthermore, we present a unified PLM-based framework, ISTS-PLM, which integrates time-aware and variable-aware PLMs tailored for comprehensive intra- and inter-time series modeling and includes a learnable input embedding layer and a task-specific output layer to tackle diverse ISTS analytical tasks. Extensive experiments on a comprehensive benchmark demonstrate that the ISTS-PLM, utilizing a simple yet effective series-based representation for ISTS, consistently achieves state-of-the-art performance across various analytical tasks, such as classification, interpolation, and extrapolation, as well as few-shot and zero-shot learning scenarios, spanning scientific domains like healthcare and biomechanics.", "sections": [{"title": "1 INTRODUCTION", "content": "Irregularly Sampled Time Series (ISTS) are common in diverse domains such as healthcare, biology, climate science, astronomy, physics, and finance [10, 12, 29, 37]. Although pre-trained foundation models have driven significant progress in natural language processing and computer vision [45], their development in time series analysis has been limited by data sparsity and the need for task-specific approaches [17, 47]. Recent studies have demonstrated that Pre-trained Language Models (PLMs) possess exceptional abilities in semantic pattern recognition and reasoning across complex sequences [24], theoretically and empirically proving the universality of PLMs to handle broader data modalities [47]. Consequently, we have witnessed that a series of proactive studies explore adapting PLMs for time series analysis [18], highlighting the superiority in generalizability, data efficiency, reasoning ability, multimodal understanding, etc. [17]. However, these studies primarily concentrate on Regularly Sampled Time Series (RSTS) [13]. The significant challenges posed by the analysis of ISTS, which are characterized by their irregular sampling intervals and missing data, have not yet been explored.\nThe modeling and analysis of ISTS differ fundamentally from those of RSTS due to the inherent irregularity and asynchrony among them [29, 44], which results in diverse representation methods of ISTS tailored to suit different models [6, 15, 33]. These distinctive characteristics present the following significant challenges in fully harnessing the capabilities of PLMs for ISTS modeling and analysis: (1) Irregularity within ISTS. Unlike applying PLMs to process RSTS, the varying time intervals between adjacent observations within ISTS disrupt the consistent flow of the series data, making PLMs difficult to identify and capture the real temporal semantics and dependencies. For example, positional embeddings [26] in PLMs align well with the chronological order of RSTS, where observations occur at fixed intervals. However, it is unsuitable for ISTS as a fixed position might correspond to observations at varying times due to the irregularity of series, which results in inconsistent temporal semantics. (2) Asynchrony within ISTS. While considerable correlations often exist between time series of different variables, the observations within ISTS may be significantly misaligned due to irregular sampling or missing data [43, 44]. This asynchrony complicates making direct comparisons and discerning correlations between the series, potentially obscuring or distorting the true relationships across variables [42]. Consequently, this poses a significant challenge in modeling correlations across different time series. (3) Diverse representation methods of ISTS. Unlike RSTS, usually represented as an orderly matrix of a series of vectors containing values from multiple variables, the representation methods of ISTS vary across different models. Unfortunately,"}, {"title": "2 RELATED WORKS", "content": "2.1 Irregularly Sampled Time Series Analysis\nThe primary focus of existing research on ISTS analytical tasks includes classification, interpolation, and extrapolation. One straightforward method involves converting ISTS into a regularly sampled format with fixed time intervals [21], though this approach often results in significant information loss and missing data issues [32]. Recent studies have shifted towards directly learning from ISTS. Specifically, some studies have enhanced RNNs by integrating a time gate [25], a time decay term [6], or memory decomposition mechanism [2] to adapt the model's memory updates for ISTS. Additionally, inspired by the Transformer's success in processing linguistic sequences and visual data, numerous studies have sought to adapt the Transformer architecture and its attention mechanism for ISTS modeling [15, 20, 32, 41, 42, 44]. Another line of studies involve employing neural Ordinary Differential Equations (ODEs) [7] to capture the continuous dynamics and address the irregularities of ISTS [3, 10, 29, 30]. While these works offer a theoretically sound solution, their practical application is constrained by high computational costs associated with numerical integration [3].\nAlthough extensive efforts have been made on ISTS analysis, they primarily focus on addressing a limited range of analytical tasks, with a particular emphasis on classification. No previous works simultaneously address all mainstream tasks, i.e., classification, interpolation, and extrapolation, through a unified framework. Furthermore, despite PLMs having demonstrated transformative power in various research areas, such as NLP [24], graph learning [9], and even RSTS [18], their potential for ISTS remains under-explored.\n2.2 Pre-Trained Language Models for Time Series\nWe have witnessed that a series of proactive studies explore adapting PLMs for time series analysis not only to enhance task performance but also to facilitate interdisciplinary, interpretative, and interactive analysis [18]. These studies primarily fall into two categories: prompting-based and alignment-based methods. Prompting-based methods [14, 40] treat numerical time series as textual data, using existing PLMs to process time series directly. However, the performance is not guaranteed due to the significant differences between time series and text modalities. Therefore, most recent works focus on alignment-based methods, aiming to align the encoded time series to the semantic space of PLMs, hence harnessing their powerful abilities of semantic pattern recognition and reasoning on processing time series. Specifically, model fine-tuning is an effective and the most widely used approach, involving directly tuning partial parameters of PLMs [4, 5, 8, 22, 23, 47] or learning additional adapters [46]. Moreover, model reprogramming [17, 34] aims to directly encode the time series into text representation space that PLMs can understand, thus avoiding tuning PLMs' parameters.\nWhile significant efforts have been made to explore the potential of PLMs for RSTS, harnessing the power of PLMs for ISTS is much more challenging due to its characteristics of irregularity, asynchrony, and diverse representation methods, leaving it largely under-explored."}, {"title": "3 PRELIMINARY", "content": "3.1 Representation Methods for ISTS\nConsider that an ISTS $\\mathcal{O}$ has $N$ variables, each of which contains a series of observations that are irregularly sampled in varying time intervals. This ISTS can be represented by different methods as illustrated in Figure 1.\nSet-Based Representation. Set-based representation method [15] views ISTS as a set of observation tuples $\\mathcal{O}_{set} = \\{(t_s, n_s, x_s)\\}_{s=1}^{S}$, where $t_s$ is the recorded time, $n_s$ indicates the variable of this observation, $x_s$ denotes the corresponding recorded value, and $S$ represents the total number of observations within the ISTS.\nVector-Based Representation. Vector-based representation method [6] has been commonly employed as a standard in current works [2, 3, 6, 10, 29, 30, 32, 41, 44]. This method represents ISTS using three matrix $\\mathcal{O}_{vec} = (\\mathcal{T}, \\mathcal{X}, \\mathcal{M})$. $\\mathcal{T} = [[t_l]]_{l=1}^{L} \\in \\mathbb{R}^L$ represents the unique chronological timestamps of all observations across the ISTS. $\\mathcal{X} = [[x_{l}^{n}]]_{l=1,n=1}^{L,N} \\in \\mathbb{R}^{L \\times N}$ records the values of variables at these timestamps, with $x_{l}^{n}$ representing the observed value of $n$-th variable at time $t_l$, or 'NA' if unobserved. $\\mathcal{M} = [[m_{l}^{n}]]_{l=1,n=1}^{L,N} \\in \\mathbb{R}^{L \\times N}$ is a mask matrix indicating observation status, where $m_{l}^{n} = 1$ signifies that $n$ is observed at time $t_l$, and zero otherwise. As a result, the ISTS is represented as a series of vectors $\\mathcal{O}_{vec} = [t_l, x_{l}^{n}, m_{l}^{n}]_{n=1}^{N}]_{l=1}^{L} \\in \\mathbb{R}^{L \\times (2N+1)}$.\nSeries-Based Representation. Series-based representation method represents the time series of each variable separately, and thus leads to $N$ univariant ISTS involving only real observations $\\mathcal{O}_{ser} = \\{\\mathcal{O}_{1:L_n}^{n}\\}_{n=1}^{N} = \\{[(t_i, x_i)]_{i=1}^{L_n}\\}_{n=1}^{N}$, where $L_n$ represents the number of real observations for $n$-th variable.\n3.2 Problem Definitions\nFigure 2 showcases the mainstream ISTS analytical tasks studied by existing research works, including classification, interpolation, and extrapolation.\nProblem 1: ISTS Classification. Given ISTS observations $\\mathcal{O}$, the classification problem is to infer a discrete class $\\hat{y}$ (e.g., in-hospital mortality) for the ISTS: $F (\\mathcal{O}) \\rightarrow \\hat{y}$, where $F(\\cdot)$ denotes the classification model we aim to learn.\nDefinition 1: Prediction Query. A prediction query is denoted as $(t, n)$, indicating a query to predict the recorded value $x$ of variable $n$ at time $t$. The queried time may either fall within the observed time window for interpolation or extend beyond it for extrapolation.\nProblem 2: ISTS Interpolation and Extrapolation. Given ISTS observations $\\mathcal{O}$, and a set of ISTS prediction queries $\\mathcal{Q} = \\{(t_j, n_j)\\}_{j=1}^{Q}$, the problem is to predict recorded values $\\mathcal{X} = \\{\\hat{x}_j\\}_{j=1}^{Q}$ in correspondence to the prediction queries: $F (\\mathcal{O},\\mathcal{Q}) \\rightarrow \\mathcal{X}$."}, {"title": "4 METHODOLOGY", "content": "The overview of ISTS-PLM is illustrated in Figure 3. We introduce PLMs, such as GPT [26] and BERT [11], for ISTS analysis. We investigate the effects of set-based, vector-based, and series-based representation methods of ISTS as the inputs for PLMs. The unified PLM-based framework, ISTS-PLM, encompasses a trainable input embedding layer, a trainable task output layer, and the PLM blocks. Inspired by [47], we freeze all the parameters of the PLMs, except for fine-tuning a few parameters of the layer normalization. Nonetheless, we identify the following major differences from [47]: (1) We directly input the outcome series of different ISTS representation instead of patching them. (2) To better adapt PLM to model ISTS, we propose Time-Aware PLM by replacing its positional embeddings with learnable continuous-time embeddings, which empowers PLM to discern the irregular dynamics within ISTS. (3) For the modeling of series-based representation, we further present a variable-aware PLM that enables the model to understand and capture the correlations between variables within asynchronous multivariate ISTS.\n4.1 Input Embedding\nThe input embedding layer aims to align the embedded ISTS to the semantic space of PLMs. Different ISTS representation methods may involve specific subsets of embedders, including time embedder, variable embedder, value embedder, and mask embedder.\nTime Embedder. To incorporate meaningful temporal information into ISTS modeling, we introduce a time embedder [32] to encode the continuous-time within ISTS:\n$t[d] = \\begin{cases} w_0t + \\alpha_0, & \\text{if } d = 0 \\\\ sin (\\omega_d t + \\alpha_d), & \\text{if } 0 < d < D \\end{cases}$                                                                                                                          (1)\nwhere the $\\omega_d$ and $\\alpha_d$ denote learnable parameters, with $D$ representing the dimension of continuous-time embedding. The linear term captures non-periodic patterns evolving over time, while the periodic terms account for periodicity within the time series, with $\\omega_d$ and $\\alpha_d$ indicating the frequency and phase of the sine function, respectively.\nVariable Embedder. This embedder maps the variable $n$ into a $D$ dimensional embedding $\\mathbf{n}$. This can be achieved by utilizing a learnable variable embedding lookup table $V \\in \\mathbb{R}^{N \\times D}$ and retrieving the corresponding embedding from the lookup table based on the variable indicator, or by using PLM to encode the descriptive text of variable, etc."}, {"title": "4.2 PLMs for ISTS Modeling", "content": "This section describes how we adapt PLM to model ISTS based on three representation methods, i.e., set-based, vector-based, and series-based representations.\n4.2.1 PLM for Set-Based Representation. Given a set of observation tuples $\\mathcal{O}_{set} = \\{(t_s, n_s, x_s)\\}_{s=1}^{S}$, we first sort them in a chronological order. For each tuple, we integrate the embeddings of variable and value: $z_s = \\mathbf{n}_s + \\mathbf{x}_s$, obtaining a series of embedded observations $\\mathcal{Z}_{set} = \\{\\mathbf{z}_s\\}_{s=1}^{S} \\in \\mathbb{R}^{S \\times D}$, which is then inputted to PLM.\nDue to the irregularity of time series, the same position in PLM might correspond to observations at varying recorded time with completely different temporal semantics. To empower PLM to seamlessly handle the irregular dynamics within ISTS, we present time-aware PLM that replaces the positional embeddings of PLM with continuous-time embedding derived from the time embedder. Consequently, the embedded observations will be seamlessly incorporated with temporal information: $\\mathcal{Z}'_{set} = \\mathcal{Z}_{set} + TE_{set}$, where $TE_{set} = \\{\\mathbf{t}_s\\}_{s=1}^{S} \\in \\mathbb{R}^{S \\times D}$ are a series of embeddings of the continuous-times in correspondence to the input observations.\nAs the set size $S$ can vary across different ISTS, we summarize PLM's outputs, $H_{set} \\in \\mathbb{R}^{S \\times D}$, into a fixed dimensional vector, $\\mathbf{H}_{set}^\\circ = \\text{pool}(H_{set}) \\in \\mathbb{R}^{D}$, to facilitate the subsequent modeling and analysis, where $\\text{pool}(\\cdot)$ is a size-independent pooling function, such as average, maximum, and attention. We consistently use average pooling in our experiments for simplification.\n4.2.2 PLM for Vector-Based Representation. The vector-based representation observations $\\mathcal{O}_{vec}$ are first embedded into $\\mathcal{Z}_{vec} = \\{\\mathbf{z}_l\\}_{l=1}^{L} \\in \\mathbb{R}^{L \\times D}$, where $\\mathbf{z}_l = \\mathbf{x}_l + \\mathbf{m}_l$. We do not involve variable embedding here because $\\mathbf{z}_l$ represents an information integration of all variables at time $t_l$, and the value and mask embedders have been variable-aware during this integration. Likewise, $\\mathcal{Z}_{vec}$ are subsequently processed by a time-aware PLM that seamlessly incorporates the inputs with temporal information, and the output $H_{vec} \\in \\mathbb{R}^{L \\times D}$ will be summarized into a fixed dimensional vector, $\\mathbf{H}_{vec}^\\circ = \\text{pool}(H_{vec}) \\in \\mathbb{R}^{D}$, by a size-independent pooling function.\n4.2.3 PLM for Series-Based Representation. PLM for series-based representation includes the processes of intra-time series dependencies modeling and inter-time series correlations modeling.\nIntra-Time Series Modeling. This involves modeling each univariant ISTS independently by using a time-aware PLM. Specifically,"}, {"title": "4.3 Task Output Projection", "content": "The task output layer aims to project the output of PLMs to address different ISTS tasks, such as classification, interpolation, and extrapolation.\nClassification. A linear classification layer processes the resulting output of PLMs to infer a class for the ISTS: $\\hat{y} = \\text{Softmax}(\\mathbf{H}_\\circ W_c + b_c) \\in \\mathbb{R}^{C}$, where $C$ is the number of classes, $W_c$ and $b_c \\in \\mathbb{R}^{C}$ are learnable parameters, and $W_c \\in \\mathbb{R}^{D \\times C}$ for the outputs of set-based and vector-based representations, $W_c \\in \\mathbb{R}^{N \\times D \\times C}$ for the flattened output of series-based representation. The entire learnable parameters of ISTS classification model are learned by optimizing a cross-entropy loss between the inferred class and ground truth label.\nInterpolation and Extrapolation. The output projection of interpolation and extrapolation varies slightly across these representation methods. For set-based and vector-based methods, given the resulting output $\\mathbf{H}_\\circ$ of the ISTS and a prediction query $(t, n)$, a prediction layer instantiated by a Multi-Layer Perception (MLP) is used to generate the predicted values at time $t$: $\\hat{x} = \\text{MLP}([[\\mathbf{H}_\\circ||t]]) [n]$.\nFor series-based method, we directly utilize the output $\\mathbf{h}_n^\\circ \\in \\mathbb{R}^{D}$ of the corresponding variable $n$ to predict its value through a shared prediction layer: $\\hat{x} = \\text{MLP}([\\mathbf{h}_n^\\circ||t])$."}, {"title": "5 EXPERIMENTS", "content": "5.1 Experimental Setup\nTo demonstrate the effectiveness of ISTS-PLM, we conduct extensive experiments across mainstream ISTS analytical tasks, including classification, interpolation, and extrapolation. More experiments studying the effect of different PLM's layers, PLMs composition, training & inference cost are provided in A.1, A.2, A.3 in Appendix.\n5.1.1 Datasets. For ISTS classification task, we refer to previous works [20, 44] that introduce healthcare datasets P12 [16], P19 [28] and biomechanics dataset PAM [27] for a thorough evaluation. We follow the experimental settings of P12, P19, and PAM from ViTST [20], where each dataset is randomly partitioned into training, validation, and test sets with 8:1:1 proportion. Each experiment is performed with five different data partitions and reports the mean and standard deviation of results. The indices of these partitions are kept consistent across all methods compared.\nFor ISTS interpolation and extrapolation tasks, referring to t-PatchGNN [42], we utilize three natural ISTS datasets: PhysioNet, MIMIC, and Human Activity [36], from the domains of healthcare and biomechanics. Consistently, we randomly divide all the ISTS samples within each dataset into training, validation, and test sets, maintaining a proportional split of 6:2:2, and adopt the min-max normalization to normalize the original observation values. To mitigate randomness, we run each experiment with five different random seeds and report the mean and standard deviation of the results. More details of these datasets are provided in Appendix A.4.\n5.1.2 Metrics. For classification task, following prior research [20, 44], we utilize Area Under the Receiver Operating Characteristic Curve (AUROC) and Area Under the Precision-Recall Curve (AUPRC) for the performance evaluation of imbalanced datasets P12 and P19, and use Accuracy, Precision, Recall, and F1 score to evaluate balanced dataset PAM. Higher is better for all the above metrics.\nReferring to previous work [42], we introduce both Mean Square Error (MSE) and Mean Absolute Error (MAE) to evaluate the prediction performance for interpolation and extrapolation tasks. Lower is better for MSE and MAE.\n5.1.3 Baselines. To evaluate the performance in ISTS classification task, we incorporate the following baseline models for a fair comparison, including vanilla Transformer [35]; (sparse) multivariate time series analysis models: MTGNN [39], DGM\u00b2-O [38]; ISTS classification models: IP-Net [31], GRU-D [6], SeFT [15], mTAND [32], Raindrop [44], Warpformer [41], and pre-trained vision transformers-based model ViTST [20]; as well as PLM-based models designed for RSTS analysis: FPT [47], Time-LLM [17]. All these models are trained for 20 epochs, and the model's parameters achieving the highest AUROC on the validation set are selected for testing [20, 44].\nFor ISTS interpolation and extrapolation tasks, except adapting the representative baselines above to these two tasks, we further incorporate several models tailored for the ISTS prediction"}, {"title": "5.4 Few-shot and Zero-shot Learning", "content": "As existing ISTS works primarily focus on classification tasks, we conduct few-shot and zero-shot learning experiments on classification datasets. For each dataset, we randomly select 10% of the training set to assess the model's few-shot learning ability. Table 4 presents the model comparison in few-shot learning scenario, where ISTS-PLM consistently outperforms the other SOTA baselines. Notably, while ViTST, another cross-domain adaptation-based model for ISTS, suffers a significant performance drop, our model maintains a much more robust performance, likely due to its need to learn far fewer parameters than ViTST, as reported in Table 7.\nTo evaluate the model's zero-shot adaptation ability, we divide the samples (i.e., patients) in P12 dataset into multiple disjoint groups based on individual attributes, including ICUType (Coronary Care Unit, Cardiac Surgery Recovery Unit, Surgical ICU, Medical ICU) and Age (Old (>=65), Young (<65)). This division ensures marked diversity between groups. The model is trained on some groups and tested on others. For ICUType, we select patients belonging to Medical ICU as the test set and the others as training data. For Age, we select Young patients as the test set and Old patients as training data. The results in Figure 4 showcase that ISTS-PLM consistently outperforms other SOTA baselines, demonstrating its robust cross-group zero-shot adaptation ability.\n5.5 Analysis on Distinct Representation Methods\nThis section provides a further analysis of the key failure reasons for ISTS-PLM when using set-based and vector-based representations. We explore several variants of ISTS-PLM-S and ISTS-PLM-V. For set-based representation, we examine (1) Set-Hierarchy: observations are processed by PLMs in a hierarchical way, i.e., first independently modeling the observation series of each variable, then modeling the correlations between these variables. This makes it equivalent to the series-based representation. For vector-based representation, we examine (2) Vec-Independent: each variable's time series is first processed independently by the PLM, followed by PLM-based inter-variable modeling; (3) Vec-Imputation: missing values in the representation are imputed using a forward-filling strategy. Figure 5 displays the results of these variants across three ISTS analytical tasks. The findings suggest that the strategy of first modeling each variable's series independently, followed by modeling their correlations, significantly enhances the performance of PLMs in processing ISTS. Unlike ISTS-PLM-S, which models all observed tuples in a mixed time-variable manner, or ISTS-PLM-V, which mixes all variables' observations at each time point, this approach organizes ISTS in a more structured and cleaner manner, and mitigates interference and noise from other variables, thereby simplifying the learning task for PLM. It transforms ISTS modeling into a two-stage sequence modeling problem, i.e., intra- and inter-series modeling, more effectively harnessing the powerful capabilities of PLMs for sequence semantic recognition and understanding."}, {"title": "6 CONCLUSION", "content": "This paper explored the potential of Pre-trained Language Models (PLMs) for Irregularly Sampled Time Series (ISTS) analysis and presented a unified PLM-based framework, ISTS-PLM, to address various ISTS analytical tasks. We investigated three methods for representing ISTS and identified a simple yet effective series-based representation to maximize the efficacy of PLMs for ISTS modeling and analysis. We conducted comprehensive experiments on"}, {"title": "A ADDITIONAL EXPERIMENTS", "content": "A.1 Parameter Sensitivity\nTaking the interpolation task as a representative, we study the effect of using different numbers of PLMs' layers on various datasets. As shown in Figure 6, the optimal configuration for PLM's layer can vary across datasets. However, using too less (i.e., 1) or too many (i.e., 12) PLM's layers usually results in poorer performance. This is because too few layers may fail to capture the necessary complexity and dependencies within ISTS, while too many layers can complicate the process of adaptation and optimization, increasing the risk of overfitting.\nA.2 Effect of Distinct PLMs\nTable 6 presents the results of different PLMs backbone composition for our model's time-aware PLM and variable-aware PLM modelus on interpolation task. The results indicate that using GPT for intra-series modeling and Bert for inter-series modeling achieves the best overall performance. This might be attributed to GPT's causal masking pre-training strategy and unidirectional, autoregressive properties, making it effective in modeling sequences where the order of data points is important, such as intra-time series dependencies. In contrast, BERT's bidirectional and contextual understanding, derived from pre-training to consider both preceding and succeeding contexts, allows it to capture complex interactions between multiple variables more effectively.\nA.3 Training & Inference Cost\nThis test is performed on a Linux server with a 20-core Intel(R) Xeon(R) Platinum 8255C CPU @ 2.50GHz and an NVIDIA Tesla V100 GPU. Table 7 presents a comparison of training parameters, training time per update step, and inference time per sample for the classification task on P12. ISTS-PLM achieves a comparable training and inference efficiency to the recommended hidden dimensions of state-of-the-art (SOTA) baselines. Notably, when these models are standardized to the same hidden dimension of 768, ISTS-PLM outperforms most of the baseline models in both training and inference efficiency. Additionally, ISTS-PLM requires fewer training parameters compared to most of these baselines.\nA.4 Dataset Details\nTable 8 provides the statistics of the used datasets P12, P19, PAM for ISTS classification, and PhysioNet, MIMIC, Human Activity for interpolation and extrapolation.\nP12/PhysioNet: PhysioNet Mortality Prediction Challenge 20123. The P12/PhysioNet dataset [16] comprises clinical data from"}]}