{"title": "A Method for Evaluating Hyperparameter Sensitivity in Reinforcement Learning", "authors": ["Jacob Adkins", "Michael Bowling", "Adam White"], "abstract": "The performance of modern reinforcement learning algorithms critically relies on tuning ever increasing numbers of hyperparameters. Often, small changes in a hyperparameter can lead to drastic changes in performance, and different environments require very different hyperparameter settings to achieve state-of-the-art performance reported in the literature. We currently lack a scalable and widely accepted approach to characterizing these complex interactions. This work proposes a new empirical methodology for studying, comparing, and quantifying the sensitivity of an algorithm's performance to hyperparameter tuning for a given set of environments. We then demonstrate the utility of this methodology by assessing the hyperparameter sensitivity of several commonly used normalization variants of PPO. The results suggest that several algorithmic performance improvements may, in fact, be a result of an increased reliance on hyperparameter tuning.", "sections": [{"title": "Introduction", "content": "The performance of reinforcement learning algorithms critically relies on the tuning of numerous hyperparameters. With the introduction of each new algorithm, the number of these critical hyperparameters continues to grow. Consider the progression of value-based reinforcement learning algorithms, starting from DQN (Mnih et al., 2015), which has 16 hyperparameters that the practitioner must choose, to Rainbow (Hessel et al., 2018) with 25 hyperparameters. This increase can be observed in Figure 1. This proliferation is problematic because performance can vary drastically with respect to hyperparameters across environments. Often, small changes in a hyperparameter can lead to drastic changes in performance, and different environments require very different hyperparameter settings to achieve the reported good performances (Franke et al., 2021; Eimer et al., 2022, 2023; Patterson et al., 2024). Generally speaking, hyperparameter tuning requires a combinatorial search and thus many published results are based on a mix of default hyperparameter settings and informal hand-tuning of key hyperparameters like the learning rate. Our standard evaluation methodologies do not reflect the sensitivity of performance to hyperparameter choices, and this is compounded by a lack of suitable metrics to characterize said sensitivities.\nThere are many different ways one could characterize performance with respect to hyperparameter choices in reinforcement learning, but the community lacks an agreed standard. Hyperparameter"}, {"title": "Problem Setting and Notation", "content": "We formalize the agent-environment interaction as a Markov Decision Process (MDP) with finite state space S and action space A, bounded reward function R: S \u00d7 A \u00d7 S \u2192 R \u2286 R, transition function P: S \u00d7 A \u00d7 S \u2192 [0, 1], and discount factor \u03b3 \u2208 [0, 1]. At each timestep t, the agent observes the state St, selects an action At, the environment outputs a scalar reward Rt+1 and transitions to a new state St+1. The agent's goal is to find a policy, \u03c0 : A \u00d7 S \u2192 [0, 1], that maximizes the expected return, Gt = \u03a3\u03b3Rt+i+1, in all states: \u0395\u03c0[Gt|St = s] for all s \u2208 S.\nMost reinforcement learning agents learn and use approximate value functions in order to improve the policy through interaction with the world. The state-value function v\u03c0 : S \u2192 R is the state conditioned expected return following policy \u03c0 defined as v\u03c0(s) = \u0395\u03c0[Gt|St = s]. Similarly, the action-value function q\u03c0 : S \u00d7 A \u2192 R provides the state and action conditioned expected return following policy \u03c0 defined by q\u03c0(s, a) = E\u03c0[Gt|St = s, At = a]. The advantage function A\u03c0(s, a) = q\u03c0(s, a) \u2212 v\u03c0(s) describes how much better taking an action a in state s is rather than sampling an action according to \u03c0(s) and following policy \u03c0 afterward. Given an estimate of the value function \\^v, an agent can update estimates of the value of states based on estimates of the values of successor states. An n-step return is defined as Gt:t+n = Rt+1 + \u03b3Rt+2 + ... + \u03b3n\u22121Rt+n + \u03b3n \\^v(St+n). A mixture of n-step returns, called a truncated-\u03bb return can be created by weighting n-step returns by a factor \u03bb \u2208 [0, 1], Gt:t+n\u03bb = (1 \u2212 \u03bb) \u03a3\u03b3 Rt+j + \u03bb\u03b3n\u22121Gt:t+n. Truncated-\u03bb returns are often used in reinforcement learning algorithms such as PPO (Schulman et al., 2017) to estimate state values and state-action advantages, which are used to approximate the value function and improve the policy.\nOf particular interest to this work is the setting where an empiricist is evaluating a set of algorithms \u03a9 across a distribution of environments E. Each algorithm \u03c9 \u2208 \u03a9 is presumed to have some number of hyperparameters n(\u03c9). Each hyperparameter hi, 1 \u2264 i \u2264 n(\u03c9) is chosen from some set of choices Hi. The total hyperparameter space H\u03c9 is defined as a Cartesian product of those choices H\u03c9 = H1 \u00d7 H2 \u00d7 ...Hn(\u03c9). Once an algorithm \u03c9 \u2208 \u03a9 and a hyperparameter setting h \u2208 H\u03c9 are chosen, the tuple (\u03c9, h) specifies an agent."}, {"title": "Hyperparameter Sensitivity", "content": "First, we must define what we mean by sensitivity. A sensitive algorithm is an algorithm that requires a great deal of per-environment hyperparameter tuning to obtain high performance. Conversely, an insensitive algorithm is one where there exist hyperparameter settings such that the algorithm can obtain high-performance across a distribution of environments with fixed hyperparameters.\nThis section presents two contributions: a metric for assessing an algorithm's hyperparameter sensitivity, and a method of graphically analyzing the relationship between hyperparameter sensitivity and performance along a 2-dimensional plane. These tools may be used to develop a deeper understanding of existing algorithms and we hope will aid researchers in evaluating algorithms more holistically along dimensions other than just benchmark performance."}, {"title": "Sensitivity Metric", "content": "We want a performance metric that summarizes the learning of an online reinforcement learning agent. The natural choice of performance metric is to report the average return obtained during learning, which we call the area under the (learning) curve (AUC). The AUC on a run is denoted by p(\u03c9, e, h, \u03ba) where \u03c9 \u2208 \u03a9 is an algorithm, e \u2208 E is an environment, h \u2208 H\u03c9 is a hyperparameter setting, and \u03ba \u2208 K \u2286 N is the random number generator (RNG) seed. Performance observed during a run of a reinforcement learning agent depends on many factors: the reinforcement learning algorithm, the environment, the hyperparameter setting, and many forms of stochasticity. Even after fixing the algorithm, environment, and hyperparameter setting, performance distributions are often skewed and multi-modal (Patterson et al., 2023). Therefore, many runs are required to obtain accurate estimates of expected performance p(\u03c9, e, h) = \u03a3p(\u03c9, e, h, \u03ba) where K \u2286 N is the set of RNG seeds used during the experiment. In the experiments presented in this paper, we perform 200 runs averaging performance over a subset of runs, after filtering (described later), and report 95% bootstrap confidence intervals around computed statistics.\nIt is crucial to capture performance across sets of environments in order to compute sensitivity. Recall, that our notion of sensitivity captures the degree to which an algorithm relies on per-environment hyperparameter tuning for its reported performance gains. Our choice of AUC as a performance metric does not allow for cross-environment performance comparisons directly because the magnitudes of returns vary greatly between environments. Consider the distributions of performance presented in the left plot in Figure 2. The performance realized by good hyperparameter settings in Halfcheetah is orders of magnitude greater than the performance of good hyperparameter settings in Swimmer. Nevertheless, just because the absolute magnitude is lower or the range of observed performances is tighter, that does not mean the differences are any less significant. Thus, in order to consider how hyperparameters perform across sets of environments, we need to normalize performance to a standardized score.\nIn this work, we use [5, 95] percentile normalization. We choose percentile normalization as it has a lower variance than alternatives like min-max normalization. Other normalization methods, such as min-max or CDF normalization (Jordan et al., 2020), could also be used with our hyperparameter sensitivity formulation. After conducting a large number of runs across different algorithms, environments, and hyperparameter settings, for each environment e, we find the 5th percentile p5(e) and 95th percentile p95(e) of the distribution of observed performance in e. Then, for each algorithm, environment, and hyperparameter setting, the normalized environment score is obtained by squashing performance:\n\u0393(\u03c9, e, h) = p(\u03c9, e, h) \u2013 p5(e)/ p95(e) - p5(e) (1)\nNote the right hand side of Figure 2, the distributions of normalized scores for hyperparameter settings in Swimmer and Halfcheetah now lie in a common range.\nNormalized scores allow practitioners to determine which fixed hyperparameter settings do well across multiple environments. That is, a practitioner can find the hyperparameter setting that maximizes the mean normalized score across a distribution of environments. Consider the performance of the hyperparameter setting denoted by the blue stars in Figure 3. This setting performs in the top quartile of hyperparameter settings in both Swimmer and Halfcheetah. In contrast, consider the hyperparameter setting denoted by the red stars. While this hyperparameter setting sits near the top of the distribution for Halfcheetah, it performs poorly in Swimmer.\nGiven an algorithm \u03c9 \u2208 \u03a9, we define its hyperparameter sensitivity \u03a6 as follows:\n\u03a6(\u03c9) = 1 \u03a3 max \u0393(\u03c9, e, h) - 1max \u0393(\u03c9, e, h) (2)\nThe hyperparameter sensitivity of an algorithm is the difference between its per-environment tuned score and its cross-environment tuned score. The per-environment tuned score is the average nor-"}, {"title": "Sensitivity Analysis", "content": "Modern reinforcement learning algorithms are complex learning systems, and understanding them is a difficult task that requires multiple dimensions of analysis. Benchmark performance has been the primary metric (and often the only one) used for evaluating algorithms. However, this is only one dimension along which algorithms can be evaluated. Hyperparameter sensitivity is an important dimension to consider in the evaluation space, especially as practitioners begin to apply reinforcement learning algorithms to real-world applications. We propose the performance-sensitivity plane to aid in better understanding algorithms.\nConsider the performance-sensitivity plane shown in Figure 4. To construct the plane, the center point is set to the hyperparameter sensitivity and per-environment tuned score of some reference point algorithm. We can consider how other algorithms relate to this reference point by considering which region of the plane they occupy. There are 5 regions of interest shaded by different colors and labeled numerically, which we will consider in turn.\nAn ideal algorithm would be both more performative and less sensitive. Therefore, algorithms that fall in Region 1 (the top left quadrant) of the plane would be a strict improvement over the reference point algorithm. For some applications, perhaps additional sensitivity can be tolerated if the gains in performance are large enough. Algorithms that fall in Region 2 are an example of this. The region represents algorithms whose increase in performance is greater than the corresponding increase in sensitivity. Conversely, for some applications sensitivity may matter a great deal and some performance loss can be endured. Algorithms that fall in Region 3 are an example of those whose decrease in sensitivity outmatches their corresponding decrease in performance. Regions 1-3 represent algorithms that have notable redeeming qualities either in terms of performance, hyperparameter sensitivity, or both. However, perhaps a practitioner does not care about sensitivity. For example, they want to maximize the score of a specific benchmark, and hyperparameter tuning is no issue. Algorithms in Region 4 may be adequate as they are algorithms that exhibit performance improvements and an even higher reliance upon per-environment hyperparameter tuning. Finally, those unfortunate algorithms that live in Region 5 are in a space with both lower performance and higher sensitivity, making them undesirable.\nA natural application of this diagram is to set the reference (center) point to the hyperparameter sensitivity and performance of some base algorithm and study how proposed modifications (or ablations) affect both sensitivity and performance. Often, new algorithms are created by modifying existing algorithms, such as normalizing targets, adding a regularization term to the loss function, gradient clipping, etc. We illustrate an example of this using PPO as a reference point in the next section."}, {"title": "Sensitivity Experiments", "content": "To illustrate the utility of the sensitivity analysis presented above, we performed an experiment to study the hyperparameter sensitivity and performance of several variants of the PPO algorithm, a widely used policy-gradient method in reinforcement learning (Schulman et al., 2017). We considered several normalization variants commonly used in PPO implementations (Andrychowicz et al., 2020; Huang et al., 2022) and some normalization variants, introduced in DreamerV3, that were purported to reduce hyperparameter sensitivity (Hafner et al., 2023; Sullivan et al., 2023)."}, {"title": "Proximal Policy Optimization", "content": "PPO is an instance of an actor-critic method (Sutton & Barto, 2018) that maintains two neural networks: a policy network with parameters \u03b8 and a value network with parameters w. Given a policy \u03c0\u03b8old, the agent performs a roll-out of T steps, (S1, A1, R1, S2, A2, R2, ...ST, AT, RT), this rollout is then split into m batches of length k. The critic is then updated via ADAM (Kingma & Ba, 2015) to optimize MSE loss with the truncated-\u03bb return as a target. The actor is updated via ADAM (Kingma & Ba, 2015) to optimize a clipped surrogate objective toward maximizing the expected return. An entropy regularizer is added to the actor loss to encourage exploration."}, {"title": "Normalization variants", "content": "Several normalization variants have been used in PPO implementations. We focus on three categories of normalization: observation normalization, value function normalization, and advantage normalization. An intuition behind value function or advantage normalization for hyperparameter sensitivity is that the scale and sparsity of rewards vary greatly across environments and that value function or advantage normalization should make actor-critic updates invariant to these factors possibly requiring less tuning of the step-size hyperparameters (van Hasselt et al., 2016). Another claimed benefit of advantage normalization variants is that by normalizing the advantage term, it is easier to find an appropriate value for the entropy regularizer coefficient \u03b3 across a distribution of environments (Hafner et al., 2023). Observation normalization standardizes the network inputs. This can mitigate large gradients, which may stabilize the learning system for hyperparameter tuning (Hafner et al., 2023; Andrychowicz et al., 2020), especially critic and actor step-size hyperparameters \u03b1w, \u03b1\u03b8 > 0.\nAdvantage per-minibatch zero-mean normalization: A common implementation detail of PPO is per-minibatch advantage normalization (Huang et al., 2022). When performing an update, the advantage estimates used in the actor loss function are normalized by subtracting the mean of the advantage estimates in the sampled batch and dividing by the standard deviation of advantage estimates in the sampled batch.\nAdvantage percentile scaling : Another form of advantage normalization was introduced in the DreamerV3 (Hafner et al., 2023) ablations which divides the advantage estimate in the actor loss by a scaling factor. Exponential moving averages are maintained over the 95th and 5th percentiles of advantage estimates. The advantage term is divided by the difference of the two percentiles."}, {"title": "Sensitivity Experiment with PPO variants", "content": "We investigated the effect of each of the described normalization variants on PPO. To isolate these effects, we did not apply reward clipping, reward scaling, or observation normalization wrappers by default. We focused our attention on four critical hyperparameters of PPO: the step-size for the critic \u03b1w, the step-size for the actor \u03b1\u03b8, the coefficient of the entropy regularizer \u03b3, and the truncated-\u03bb return mixing parameter \u03bb. We performed a large grid search spanning five orders of magnitude across five Brax Mujoco domains. Near the extreme endpoints of the grid search, some hyperparameter configurations diverged. We ignored hyperparameter combinations that caused a particular algorithm to diverge over 10% of the time. We averaged the performance over the non-diverging runs.\nConsider the performance-sensitivity plane in Figure 5. The reference point at the center is the hyperparameter sensitivity and performance found for PPO without normalization. The error bars displayed indicate 95% confidence intervals formed from a 10,000 sample bootstrap. First, note that none of the normalization variants resulted in an improvement that both raised performance and lowered sensitivity. All forms of advantage normalization increased performance. However, this performance gain comes with a trade-off: increased hyperparameter sensitivity. The marginal gain in performance per unit of increased sensitivity varied between advantage normalization methods. Advantage per-minibatch zero-mean normalization had a greater increase in performance than sensitivity (Region 2). Both percentile scaling-based variants of advantage normalization resulted in more significant sensitivity increases than performance increases (Region 4), indicating an enhanced reliance on hyperparameter optimization methods. Applying the symlog function to the value target lowered performance and may have slightly increased sensitivity (Region 5). It may, however, be the case that the choice of environment distribution did not have enough variation in reward magnitude for the utility of value target symlog to be demonstrated. It appears that observation normalization may slightly reduce sensitivity, although it is unclear due to the width of the confidence intervals."}, {"title": "Effective Hyperparameter Dimensionality", "content": "There are many cases where a practitioner can tune some but not all of an algorithm's tunable hyperparameters. It may be the case that if a few key hyperparameters are tuned per environment, then a preponderance of an algorithm's potential performance can be gained. This motivates the definition of effective hyperparameter dimensionality, a metric that measures how many hyperparameters must be tuned in order to obtain near-peak performance."}, {"title": "for PPO", "content": "For a given algorithm \u03c9 with hyperparameter space H\u03c9, number of tunable hyperparameters n(\u03c9), and environment distribution &, let h\u2217 = arg maxh\u2208H\u03c9 \u03a3\u0393(\u03c9, e, h) be the hyperparameter setting which maximizes the cross-environment tuned normalized score. Define a similarity function p: H\u03c9 \u2192 [n(\u03c9)] that counts the number of hyperparameters in common with h\u2217, p(h) = \u03a31[hi = h\u2217i]. Effective hyperparameter dimensionality d(\u03c9) is defined as\nd(\u03c9) = max p(h) (4) \nh\u2208H\u03c9\ns.t. \u03a3\u0393(\u03c9, e, h) \u2265 0.95 \u03a3 max \u0393(\u03c9, e, h\u2032)\ne\u2208E \uff5cE\uff5c e\u2208E h\u2032\u2208H\u03c9\nThe effective hyperparameter dimensionality of an algorithm is the maximal number of hyperparameters that can be left to default (setting that maximizes cross-environment tuned performance) while retaining the majority of the performance that can be realized by tuning per environment. The threshold of 95% peak performance can be changed at a practitioner's discretion to whatever meets their performance requirements. To compute effective hyperparameter dimensionality, one needs to consider subsets of hyperparameters to find a minimum subset that achieves the required performance threshold.\nFor the same algorithmic variants of PPO as studied above, Figure 6 displays normalized scores as a function of the number of hyperparameters tuned per environment, choosing the most performant subset to tune. Table 2 in Appendix C provides a listing of the most performant subsets of varying sizes observed during this experiment. The curve interpolates between the normalized scores. The vertical dashed line indicates the point along the curve that reaches 95% of the per-environment tuned score. In the case of advantage percentile scaling, modifying PPO with the normalization variant moves the point to the right, indicating this variant improves performance at the cost of increasing pressure on the number of hyperparameters necessary to tune. Also, note how the performance"}, {"title": "Limitations and Future Work", "content": "The hyperparameter sensitivity and effective hyperparameter dimensionality metrics will depend heavily on several important empirical design choices. The premise of both metrics is that a practitioner is concerned with understanding sensitivity with respect to a distribution of environments that they care about. If the distribution of environments changes, the metrics will need to be evaluated with respect to the new distribution. This dependence on the environment distribution could be exploited, as someone could artificially make an algorithm appear less sensitive by including several easy environments that all hyperparameter settings will do well in, although score normalization will somewhat counteract this. In addition, the level of granularity with which hyperparameter sweeps are performed will have an effect on both metrics. Another factor that can impact the metrics is the choice of the score normalization method used. A practitioner could use other score normalization methods and the resulting sensitivity scores may be different.\nA next step is to apply the proposed sensitivity and dimensionality metrics to a larger set of algorithms and environments. Related to this work is the literature of AutoRL (Eimer et al., 2023). The goal of AutoRL is to tune hyperparameters via some hyperparameter optimization algorithm (which make use of their own hyperparameters\u2014hyper-hyperparameters). Future work could use the definitions provided here to try to understand if the algorithms proposed in the AutoRL literature reduce sensitivity over the base algorithms that are being modified-measuring the sensitivity of the hyper-hyperparameters. A study comparing the sensitivities and dimensionalities of AutoRL methods to the sensitivities and dimensionalities of the base learning algorithms they optimize would be prudent."}, {"title": "Conclusion", "content": "As learning systems become more complicated, careful empirical practice is critical. Modern reinforcement learning algorithms contain numerous hyperparameters whose interactions and sensitivities are not well understood. Common practice, which is focused on achieving state-of-the-art performance, risks overfitting to benchmark tasks and overly relying on hyperparameter optimization. Most empirical work in reinforcement learning has focused only on evaluating algorithms based on benchmark performance, leaving the effects of hyperparameters under-studied. In this work, we propose a new evaluation methodology based on two metrics that allow practitioners to better understand how an algorithm's performance relates to its hyperparameters. We show how this methodology is useful in evaluating methods purported to mitigate sensitivity. We identify that the studied advantage normalization methods, while improving performance, also increase hyperparameter sensitivity and can increase the number of sensitive hyperparameters."}, {"title": "Broader Impact Statement", "content": "Hyperparameter sweeps and per-environment tuning are the most computationally expensive and environmentally impactful parts of reinforcement learning research. Our study ran for approximately 4.5 GPU years on NVIDIA 32GB V100s. While this is substantial, we believe that using compute to better understand the sensitivity of current algorithms is an essential step towards developing more environmentally friendly algorithms. This work investigated an empirical methodology for evaluating the hyperparameter sensitivity of reinforcement learning agents. The immediate societal impact is minimal. However, our methodology may aid in developing performative algorithms with low hyperparameter sensitivity. If this occurs, those algorithms will result in less need for hyperparameter tuning and, as a result, have a positive impact on lowering the carbon footprint of reinforcement learning experiments."}, {"title": "Proliferation of Hyperparameters", "content": "There is a trend in which the current state-of-the-art algorithms often contain more hyperparameters than the previous state-of-the-art. Table 1 lists hyperparameter counts for representative algorithms from each of the three main categories of reinforcement learning methods: values-based, policy-gradient, and model-based."}, {"title": "Hyperparameter Sweep Details", "content": "The PPO implementation used was heavily inspired by the PureJaxRL PPO implementation (Lu et al., 2022). The variants advantage per-minibatch zero-mean normalization and observation zero-mean normalization are the standard implementations provided within PureJaxRL. The variants: symlog observation, symlog value target, percentile scaling, and lower bounded percentile scaling closely follow the implementation of the DreamerV3 tricks applied to PPO shown in Sullivan et al. (2023) as well as referencing the original DreamerV3 repository Hafner et al. (2023).\nThe policy and critic networks were parametrized by fully connected MLP networks, each with two hidden layers of 256 units. The network used the tanh activation function. Separate ADAM optimizers (Kingma & Ba, 2015) were used for training the actor and critic networks. The environments used in the experiments were the Brax implementations of Ant, Halfcheetah, Hopper, Swimmer, and Walker2d.(Freeman et al., 2021). The hyperparameter sweeps were grid searches over eligibility trace \u03bb\u2208 {0.1, 0.3, 0.5, 0.7, 0.9}, entropy regularizer coefficient \u03b3\u2208 {0.001, 0.01, 0.1, 1.0, 10.0}, actor step-size \u03b1\u03b8 \u2208 {0.00001, 0.0001, 0.001, 0.01, 0.1}, and critic step-size \u03b1w \u2208 {0.00001, 0.0001, 0.001, 0.01, 0.1}. Each run lasted for 3M environment steps. 200 runs were performed for each of the algorithms, environments, and hyperparameter settings. Like the PureJaxRL PPO implementation, the entire training loop was implemented to run on GPU. We will release code and experiment data at promoting the further investigation of hyperparameter sensitivity in the field of reinforcement learning."}]}