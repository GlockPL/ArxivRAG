{"title": "DMT-HI: MOE-based Hyperbolic Interpretable Deep Manifold Transformation for Unspervised Dimensionality Reduction", "authors": ["Zelin Zang", "Yuhao Wang", "Jinlin Wu", "Hong Liu", "Yue Shen", "Stan.Z Li", "Zhen Lei"], "abstract": "Dimensionality reduction (DR) plays a crucial role in various fields, including data engineering and visualization, by simplifying complex datasets while retaining essential information. However, the challenge of balancing DR accuracy and interpretability remains crucial, particularly for users dealing with high-dimensional data. Traditional DR methods often face a trade-off between precision and transparency, where optimizing for performance can lead to reduced interpretability, and vice versa. This limitation is especially prominent in real-world applications such as image, tabular, and text data analysis, where both accuracy and interpretability are critical. To address these challenges, this work introduces the MOE-based Hyperbolic Interpretable Deep Manifold Transformation (DMT-HI). The proposed approach combines hyperbolic embeddings, which effectively capture complex hierarchical structures, with Mixture of Experts (MOE) models, which dynamically allocate tasks based on input features. DMT-HI enhances DR accuracy by leveraging hyperbolic embeddings to represent the hierarchical nature of data, while also improving interpretability by explicitly linking input data, embedding outcomes, and key features through the MOE structure. Extensive experiments demonstrate that DMT-HI consistently achieves superior performance in both DR accuracy and model interpretability, making it a robust solution for complex data analysis. The code is available at https://github.com/zangzelin/code_dmthi.", "sections": [{"title": "I. INTRODUCTION", "content": "Dimensionality reduction [1], [2], [3], [4] simplifies complex datasets while preserving their intrinsic structure [5], [6]. This is crucial for managing high-dimensional data, which presents challenges in computational complexity, storage, and visualisation [7], [8]. Reducing data dimensionality allows for more efficient analysis, pattern recognition, and interpretation [9]. However, balancing high performance [10] and interpretability [11], [12] remains challenging, as efficient processing [13] often conflicts with human interpretability [14], [15]."}, {"title": "II. RELATED WORK", "content": "Dimension Reduction & Visualization. Dimensionality reduction (DR) methods can be classified into parametric-free and parametric approaches. Parametric-free methods, like MDS [31], ISOMAP [32], LLE [33], and t-SNE [34], optimize the output directly, focusing on preserving distances, particularly local manifold structures. However, these methods often lack generalizability and interpretability, as they do not capture underlying feature relationships and cannot handle new datasets well. Parametric methods, such as Topological Autoencoders (TAE) [35] and PUMAP [36], address these shortcomings by using neural networks for continuous mappings and input space constraints, improving both generalizability and interpretability. NeuroDAVIS [37] further enhance DR performance and visualization by focusing on both local and global structure retention. While progress has been made, many methods still struggle with effective loss functions and training approaches, especially in capturing complex relation-ships like those in biological data. Approaches integrating techniques like causal representation learning, as seen in Cells2Vec [38], represent a future direction for enhancing both interpretability and robustness in DR.\nExplainability & Interpretability of DR Methods. Explainability focuses on revealing a model's internal workings, while interpretability ensures the model's outputs are easily understood by users [11], [12]. In DR, explainability methods like DMT-EV [26] utilize saliency maps and interactive visual tools to provide insights into how different components influence the embedding process. Other approaches, such as DimReader [39] and DataContextMap [40], enhance the interpretability of embeddings by visually linking features to their impact on reduced data spaces. Interpretability is partic-ularly critical in domains like healthcare and finance, where understanding the relationship between input data and predic-tions is crucial for decision-making [41], [42]. Approaches such as DT-SNE [43], which combines t-SNE embeddings with decision tree rules, offer clear explanations of how data points relate in the embedding space, further advancing the interpretability of machine learning models in these high-stakes areas."}, {"title": "III. PROBLEM DEFENITION AND PRELIMINARIES", "content": "A. Data Description and Data Augmentation.\nIn this section, we provide a formal description of the data preprocessing, data augmentation [50], dimensionality reduction, and interpretability concepts used in the proposed method. These concepts are essential for understanding the subsequent sections of the paper. The input data X can be image data, tabular data, or sequential data. Assume we have N samples, with each sample represented as xi:\n$X = \\{X_1, X_2, ..., X_i, ..., X_N \\}.$ (1)\nData augmentation is applied to enhance the robustness and diversity of the training samples. Formally, we define the augmented data Xaug as:\n$X^{\\text{aug}} = \\{x_1^{\\text{aug}}, x_2^{\\text{aug}}, ..., x_i^{\\text{aug}},...,x_N^{\\text{aug}}\\},$ (2)\n$x^{\\text{aug}} = T(X_i),$\nwhere each xaug is obtained through specific augmentation techniques based on the data type.\nTabular Data. For tabular data, we normalize each feature to have zero mean and unit variance. This helps in stabilizing the training process and ensures that each feature contributes equally [51]. We further perform data augmentation through neighbor discovery and linear interpolation among neighbors. Specifically, for each data point xi,\n$x_i^{\\text{tab}} = \\lambda x_i + (1 - \\lambda) x_j,$ (3)\n$x_j \\in N^k(x_i), \\lambda \\sim U(0,1)$\nwhere \u03bb is a random interpolation factor sampled from a uniform distribution between 0 and 1, and xj is a randomly selected neighbor of xi, Nk(xi) is the function to discovery the k nearest neighbors of xi, k is a hyperparameter.\nImage Data. For image data, we employ common data augmentation techniques, including random cropping, flipping, and rotation, to enhance the diversity and robustness of the training dataset [52], [53], [50]. Formally, for a given image xi, we apply a set of transformations denoted as \u2533img,\n$x^{\\text{aug}} = T^{\\text{img}}(x_i),$ (4)\nwhere rimg includes color jittering [54], random cropping [55], applying Gaussian blur [56], Mixup [57] and other domain-specific augmentations [58]. The details of the augmentation techniques for image data are provided in the Appendix.\nB. Interpretability in Dimensionality Reduction.\nLet f to be a DR mapping function, $f : R^D \\rightarrow R^d$, hi = f(xi), d\u226a D. The parameterized methods like autoencoders learn mappings through optimization, allowing generalization, whereas non-parametric methods like t-SNE optimize for specific datasets.\nFeature Interpretability (Ifeat) assesses the influence of individual features on DR outputs,\n$I_{\\text{feat}}:(\\{x_i\\}_{i=1}^N, \\{h_i\\}_{i=1}^N, f) \\rightarrow I_{\\text{feat}},$ (5)\nwhere Ifeat measures the importance of features in determining low-dimensional representations.\nGroup Interpretability (Igroup) evaluates the collective importance of feature groups,\n$I_{\\text{group}}: (\\{x_i\\}_{i=1}^N, \\{h_i\\}_{i=1}^N, f) \\rightarrow I_{\\text{group}},$ (6)\nwhere Igroup quantifies the impact of interacting feature subsets on the embeddings.\nC. Gumbel-Softmax for Task Allocation.\nGumbel-Softmax [59] enables differentiable sampling from a categorical distribution, useful for task allocation in neural networks. Given logits L = (L1, L2, ..., LD) and temperature T, the D is the number of the dimensionality in raw data. The Gumbel-Softmax distribution introduces Gumbel noise gi and applies a softmax function,\n$y_i = \\frac{\\exp((L_i+g_i)/T)}{\\Sigma_{j=1}^D \\exp((L_j + g_j)/T)},$ (7)\nwhere gi is drawn from a Gumbel(0,1) distribution, and T controls the smoothness, As\u0442 \u2192 0, the distribution approaches a categorical one, making hard selections. Higher Tallows softer outputs for exploration. For task allocation in a Mixture of Experts (MOE) model, this technique assigns input x\u1d62 to an expert by computing the logits Li and sampling task probabilities. The expert with the highest probability is selected,\n$\\text{Expert Assignment} = \\underset{i}{\\arg \\max} y_i,$ (8)\nThis allows for dynamic task allocation while preserving gradient-based optimization."}, {"title": "IV. METHODS", "content": "To enhance both performance and interpretability in dimensionality reduction (DR), as shown in Fig. 2, we propose the Hyperbolic Interpretable Deep Manifold Transformation (DMT-HI), leveraging a Mixture of Experts (MOE) framework [29], [60]. DMT-HI addresses limitations in existing DR methods by capturing complex, non-Euclidean structures [61] while offering a more interpretable mapping process. It incorporates three key components, Multiple Gumbel Operator-Based Matchers, a MOE network, and a Hyperbolic Mapper. These components work together to improve DR quality while ensuring transparency in the data transformation process.\nA. Multiple Gumbel Matchers\nEffective task allocation is crucial in Mixture of Experts (MOE) models [29], as the way features are distributed to experts impacts both performance and interpretability. Random allocation can lead to suboptimal specialization. We address this by extending Gumbel-Softmax for selecting feature subsets, improving expert allocation efficiency.\nAs shown in Fig. 3, we propose the Multiple Gumbel Operator (\u2609), which extends traditional Gumbel-Softmax to select multiple features per expert. A neural network (NN) first processes the input data xi to generate logits, Li =\n[Li1, Li2, ..., LiD], where D is the number of features. We then sample independent noise values gi = [g1,2,\u2026, gD] from the Gumbel distribution, which are added to the logits and scaled by the temperature parameter 7. This yields adjusted probabilities,\n$Z_{ij}^e=\\frac{\\exp(\\frac{L_{ij} + g_{ij}}{\\tau})}{\\sum_{k=1}^{D}\\exp(\\frac{L_{ik} + g_{ik}}{\\tau})},$ (9)\nwhere Z; is probability of selecting feature j for expert e. To control feature allocation, we introduce the hyperparame-ter O, which specifies how many features each expert receives. The top-O features with the highest probabilities are selected. Formally, for each expert e, the mask element m, is defined as,\n$\\text{M}^e = \\{m_1^e, m_2^e, ..., m_j^e, ..., m_D^e\\}$ (10)\n$m_{ij}^e = \\begin{cases} 1, & \\text{if } j \\in \\text{Top-O}(Z_1, Z_2,..., Z_D), \\\\ 0, & \\text{otherwise}, \\end{cases}$\nwhere each m is a binary vector indicating the features selected for expert e.\nWe apply the mask matrix using the Hadamard product between m and the input data xi, generating the input for each expert,\n$z_i^e = x_i \\odot M_i^e.$ (11)\nThis approach ensures each expert processes a relevant subset of features, improving performance. The explicit selection mechanism also enhances interpretability by revealing which features are assigned to which experts, clarifying their contributions to the model's output.\nB. Mixture of Experts Network\nIn our proposed model, the integration of outputs from multiple experts is key to capturing the intricate, multi-faceted nature of the input data. Each expert model Ee(\u00b7) operates on the allocated input data x using the corresponding mask vector m generated by the Multiple Gumbel Operator. This allocation ensures that each expert focuses on specific subsets of features, allowing for specialized processing that enhances the overall model's expressiveness and interpretability.\nThe ensemble of expert models is formally defined as,\n$E = \\{E^1(\\cdot), E^2(\\cdot), ..., E^U(\\cdot)\\},$ (12)\nwhere each expert can be instantiated as either structurally identical or distinct backbone networks, such as Convolutional Neural Networks (CNNs) or Multi-Layer Perceptrons (MLPs)."}, {"title": "C. Hyperbolic Mapper", "content": "To transform the concatenated expert outputs \u0113\u00bf into low-dimensional embeddings, we propose a Hyperbolic Multi-Layer Perceptron (HMLP) [62]. The HMLP leverages hyperbolic geometry to map high-dimensional expert representations into a lower-dimensional hyperbolic space, preserving the hierarchical structures often missed in Euclidean spaces.\nThe transformation of \u1ebd\u00bf into a low-dimensional embedding h\u00bf \u2208 Rk (with k < d) in hyperbolic space is defined as,\nHMLP(\u0113\u00bf) = expen (Wlogen (\u0113i) + b), (14)\nwhere W is the weight matrix and b the bias vector. The logarithmic logen (\u00b7) and exponential expen (\u00b7) maps project data to and from the tangent space of the Poincar\u00e9 ball Bn, ensuring the model captures complex hierarchical relationships in a compact, hyperbolic form.\nThe HMLP comprises multiple layers, each performing a hyperbolic linear transformation followed by a hyperbolic acti-vation function, such as the hyperbolic tangent (tanh function). The utilization of hyperbolic activation functions enables the network to effectively model complex relationships and hierarchical structures present in the data, which are challenging to represent in Euclidean geometry. By embedding the data into a hyperbolic space, we aim to better capture these relationships and improve the performance of downstream tasks."}, {"title": "D. Sub-manifold Matching Loss Function", "content": "To ensure that similarities in the high-dimensional expert output space are preserved in the lower-dimensional hyperbolic embeddings, we introduce a manifold loss function, denoted as Lsmm. Let h is a batch of hyperbolic embeddings of the original raw data, and and h+ is the augmented data, respectively, from the Hyperbolic MLP. The manifold loss is formulated as,\n$L_{\\text{SMM}} (h, h^+) = \\frac{1}{2} (\\sum_{ij} \\log S_{ij}^{\\text{hh}} - \\sum_{ij} S_{ij}^{\\text{hh+}} ) - \\gamma (\\sum_{i} \\log S_{ii}^{\\text{hh}}),$\n(15)\n$ - \\gamma (\\sum_{i} \\log S_{ii}^{\\text{hh+}}),$ where \u03b3 > 0 is an exaggeration factor that emphasizes preserving local similarities in hyperbolic space. The similarity matrices Shh are computed using a t-distribution kernel, with pairwise distances calculated as,\n$S_{ij} = (1 + D_{ij}/\\nu)^{-\\frac{\\nu+1}{2}},$ (16)\nwhere v the degrees of freedom of the t-distribution, Dij is the distance metric of hi and hj. These t-Student similarity matrices ensure that hyperbolic embeddings preserve local pairwise similarities, maintaining hierarchical and relational structures.\nThe loss in Eq. (15) is an unsupervised function that guides experts in dimensionality reduction by preserving structural information. Unlike parameter-free methods, our model learns mapping parameters that can be used to interpret the model. By balancing alignment and uniformity, this loss ensures that hyperbolic embeddings capture the relational structure of the original data, crucial for tasks that rely on preserving topology."}, {"title": "E. Expert Exclusive Loss Function", "content": "To promote diversity among the representations of different experts and reduce redundancy, we introduce an orthogonal loss function, denoted as Lexc. This loss ensures that features learned by each expert are uncorrelated with those learned by others, while encouraging coherence within each expert's feature set.\nCosine similarity is computed both within the same expert and across different experts. For a batch of expert outputs e, the expert orthogonal loss is defined as,\n$L_{\\text{Exc}}(e) = \\frac{1}{N_B} \\sum_{i=1}^{N_B} \\sum_{e_a \\neq e_b} \\frac{1 + \\frac{e_a^T}{\\| e_a \\|} \\frac{e_b}{\\| e_b \\|}}{2},$ (17)\nwhere NB is the batch size, the ea and e from expert ea and eb are two different experts' outputs. The expert orthogonal loss penalizes high similarity between different experts, promoting inter-expert diversity. The orthogonal loss minimizing this loss balances intra-expert cohesion with inter-expert diversity, enhancing the model's ability to capture diverse features and improving generalization.\nThe overall loss function is the sum of the sub-manifold matching loss LsMM and the expert orthogonal loss LExc, ensuring both structural preservation and diverse expert,\n$L = L_{\\text{SMM}}(h, h^+) + L_{\\text{SMM}}(\\bar{e}, \\bar{e}^+) + \\lambda L_{\\text{Exc}}(e),$ (18)\nwhere, and \u03bb controls the trade-off between structural preservation and expert diversity."}, {"title": "F. Interpretability through MOE \u2013 Connecting Raw Data, Representation, and Raw Features", "content": "The Mixture of Experts (MOE) technique improves model performance by dividing input data into segments, each processed by specialized expert models. A gating network assigns samples to experts probabilistically, and the final prediction is a weighted sum of expert outputs. This structure ensures efficient resource allocation and enhanced feature representation through expert specialization. In our MOE-based model, task allocation masks Mi are assigned using the Multiple Gumbel Operator, targeting specific features for expert processing and strengthening the link between data and features, thereby improving interpretability.\nTo quantify the impact of each expert's output, we perturb the output of the e-th expert for the i-th sample, z + \u03b4, and calculate the change in model predictions,\n$\\Delta_i^e = \\E(z + \\delta) - \\E(z),$ (19)\nwhere E(\u00b7) is expert output and \u03b4 is a perturbation.\nWe define an importance metric I\u00bf for each sample xi, capturing the sensitivity of features processed by each expert,\n$I_{i,e,j} = |\\Delta_i^e / \\delta|.$ (20)\nThis importance tensor I\u00bf measures how experts contribute to features, aiding in understanding the model's decision process.To bridge the experts, input features, and data points, we define two connection matrices, one linking experts to features (Me,f) and another linking experts to data points (Me,i). These are computed by averaging the importance tensor across relevant dimensions. The expert-to-data connection matrix is,\n$M_{e,i} = \\frac{1}{D} \\sum_{j=1}^D I_{i,e,j},$ (21)\nwhere D is the number of features.\nBy visualizing these matrices, we can project expert repre-sentations onto the boundary of hyperbolic space, identifying key regions where experts are most active. This approach clarifies how experts specialize in processing different data subsets, enhancing both transparency and interpretability within the MOE system."}, {"title": "V. EXPERIMENTS", "content": "In this section, we conduct extensive experiments to demonstrate the effectiveness of our proposed DMT-HI for High dimensional data visualizatoin. In Sec V-A, we provide the details of the datasets and the baselines method. In Sec V-B, we provide the details of our implantation. In Sec V-C, we compare our method with other baselines in ten datasets. Qualitative analysis are conducted in Sec V-D, which directly demonstrate the effect of the method.\nA. Datasets and Baselines\nDatasets. Our comparative experiments include ten datasets (20News, MNIST, E-MNIST, K-MNIST, Cifar10, Cifar100, MCA, GAST, HCL, and SAM). The detailed descriptions of the datasets are provided in Table A1.\nBaseline Methods. The methods compared in this study include tSNE [34], [63], UMAP [23], PUMAP [36], [25], Ivis [64], PaCMAP [65], HNNE [66] and DMT-EV [26] (The result of HNNE is in the appendix). Each method maps the input data to a two-dimensional latent space to meet consistent visualization requirements.\nEvaluation Metrics. We evaluate the performance of the methods using three metrics: SVM classification accuracy, KNN classification accuracy and trustworthiness (TRUST) score [35]. The SVM classification accuracy is calculated using a linear SVM classifier trained on the latent space representations of the training set and tested on the test set. The Trust score is calculated using the Trust metric, which measures the preservation of the global structure of the data in the latent space. All models are trained on the training set (80%), validated on the validation set (10%), and evaluated on the test set (10%), with generalization performance evaluated using test metrics. The mean and variance of the results over 10 trials are provided in the Appendix.\nB. Implantation Details\nWe follow the basic implementation details as described in [26] to set up our experiments. In our proposed model, the Multiple Gumbel Matchers are implemented as a 2-layer Multilayer Perceptron (MLP) network, where the hidden layer contains 100 neurons. The O in Eq. (10) is set to [0.9 \u00d7 D], where D is the number of the features of the raw data. This structure allows the matchers to effectively learn task allocations for different feature subsets. In the Mixture of Experts (MOE) Network, each expert is designed as a 4-layer MLP, with each hidden layer comprising 512 neurons. The MOE network includes a total of 10 expert models, ensuring that each expert can specialize in processing distinct subsets of the input data, providing the necessary capacity and flexibility for complex data representations. The v latent space parameter Vlat, the embedding space parameter Vemb, the exaggeration parameter y the batch size (batch_size), and the number of neighbors in augmentation (K) are provided is different in different datasets. The detailed optimal parameters for each dataset are summarized in Table A2.\nC. Global Structure Preservation\nTo evaluate the accuracy and effectiveness of DMT-HI, we use two key metrics: classification accuracy and the Trust metric. These are standard for assessing the preservation of global and local structures in dimensionality reduction (DR).\nResults on linear SVM metric & global performance. classification accuracy is assessed using a linear Support Vector Machine (SVM) on the reduced data to evaluate the model's ability to preserve inter-cluster relationships from the original high-dimensional space. By using a simple linear classifier, we ensure the focus remains on structural preservation without the added complexity of a more sophisticated classifier. As shown in Table I, DMT-HI consistently achieves superior classification accuracy across various datasets, particularly excelling on complex image datasets like Cifar10 and Cifar100. For instance, on Cifar10, DMT-HI outperforms all other methods with an impressive training accuracy of 77.5% and a testing accuracy of 74.9%, compared to the much lower accuracy of traditional methods like t-SNE (22.3%) and UMAP (21.8%). Similarly, on the Cifar100 dataset, DMT-HI reaches a remarkable training accuracy of 39.1% and a testing accuracy of 38.9%, significantly higher than baseline methods like DMT-EV (5.2%) and UMAP (5.3%). The model also demonstrates strong performance on biological datasets, such as MCA and HCL. On MCA, DMT-HI achieves a training accuracy of 80.7%, outperforming other methods like UMAP (37.8%) and PaCMAP (76.2%). Similarly, on HCL, DMT-HI stands out with a training accuracy of 84.5% and a testing accuracy of 76.7%, while methods such as t-SNE and IVIS lag behind significantly. Moreover, DMT-HI exhibits robust performance across all datasets, maintaining the highest average classification accuracy (71.9% for training, 69.8% for testing), as compared to other methods like DMT-EV (61.2%) and PaCMAP (59.6%). This strong performance is reflected in DMT-HI's overall ranking, where it consistently ranks first across both training and testing sets.\nResults on trustworthiness metric & global performance. we evaluate the trustworthiness (TRUST) metric [35] to compare the local structure preservation capabilities of DMT-HI against other dimensionality reduction methods. Traditional methods like t-SNE and UMAP, which directly optimize the trustworthiness metric, have a natural advantage in preserving local neighborhood relationships, particularly when processing data in smaller batches. As shown in Table II, these methods perform slightly better in certain datasets, such as K-MNIST and MCA, where they achieve higher local structure preserva-tion. However, DMT-HI demonstrates consistently competitive performance across a wide range of datasets, maintaining a strong balance between global and local structure preservation. Despite not being specifically optimized for the trustworthiness metric, DMT-HI performs well on complex datasets such as SAM and Cifar100, surpassing traditional methods in average performance. This balance is reflected in DMT-HI's ability to achieve the best or nearly best trustworthiness scores across most datasets (see Table II). Overall, while traditional methods may show slight advantages in optimizing for the trustworthiness metric, DMT-HI's robust performance across all datasets, particularly in terms of average rankings, highlights its versatility and suitability for a wide range of high-dimensional data applications.\nMore advantages on larger datasets. DMT-HI consistently outperforms baseline methods, especially on large-scale datasets like MNIST and E-MNIST, where its higher classification accuracy and Trust scores demonstrate its robust performance. This is due to DMT-HI's neural network model, which is well-suited for high-complexity data, and its hyperbolic embedding, which captures complex nonlinear features. On complex datasets like SAM and HCL, DMT-HI excels, preserving intricate structures that traditional methods often struggle with.\nD. Local Structure Preservation\nTo evaluate local structure preservation in dimensionality reduction (DR), we used KNN clustering accuracy to assess how well neighborhood relationships were maintained in the reduced space. This metric is key for evaluating clustering and classification tasks as it ensures that close points in the original high-dimensional space remain close after reduction.\nResults on KNN metric & local performance. As shown in Table III, DMT-HI consistently outperformed methods like t-SNE, UMAP, PaCMAP, and Ivis, achieving an average KNN accuracy of 79.6% on the training set and 75.9% on the test set. Its superior performance on datasets like Cifar10 and Cifar100 demonstrates its ability to retain intricate local structures, where simpler methods often struggle. On biological datasets such as MCA and SAM, DMT-HI's hyperbolic embeddings excelled at capturing hierarchical and nonlinear structures, achieving 94.1% KNN accuracy on MCA, far surpassing DMT-EV. This adaptability across domains highlights DMT-HI's versatility.\nStrong generalization on complex image data, maintaining 74.3% accuracy on the Cifar10 test set. In contrast, traditional methods like t-SNE displayed more variability, underscoring their limitations on complex datasets. For instance, on Cifar100, DMT-HI achieved 42.1% accuracy, while t-SNE managed only 9.3%, illustrating DMT-HI's ability to capture complex hierarchical structures."}, {"title": "E. Visualization Results", "content": "In this section, we provide a detailed analysis of the visualization results across various dimensionality reduction methods on multiple datasets, including both training and test sets. These results demonstrate the superior performance of our proposed DMT-HI method in representing complex data structures across different domains, such as image and biological datasets. A more extensive set of results, including additional datasets and methods, can be found in the Appendix, where comprehensive visualizations for both training and test sets are presented across multiple approaches. The detailed visualization results are shown in Appendix (Fig. A1, Fig. A2, Fig. A3, and Fig. A4).\nClear boundaries and less manifolds overlapping. In terms of performance metrics, we have previously demonstrated that DMT-HI achieves significant improvements over traditional dimensionality reduction techniques such as t-SNE, UMAP, PaCMAP, and IVIS. Fig. 4 further support these findings by showing that DMT-HI not only preserves the global data structure in both training and test sets but also offers superior separation between classes, especially in more complex datasets such as K-MNIST and E-MNIST. The red-circled regions highlight specific areas where DMT-HI excels in clustering, providing more granularity and reduced class overlap, which is crucial for tasks requiring high-resolution data representation.\nClear advantage on complex image datasets (CIFAR-10 and CIFAR-100). As shown in Fig. 5, while all methods exhibit some level of consistency between training and test visualizations, DMT-HI excels in generating well-separated clusters with minimal overlap, even in the highly complex CIFAR-100 dataset, where subclasses exhibit significant similarity. The deep neural architecture of DMT-HI allows it to capture and represent rich semantic information, resulting in enhanced class distinction and a more interpretable low-dimensional space. In more complex image datasets, such as CIFAR-100, DMT-HI shows substantial performance gains compared to other methods, particularly in terms of semantic representation. The architecture of DMT-HI enables it to better capture the nonlinear relationships between subclasses, resulting in superior cluster separation in both training and test sets. This is evident in Fig. 5, where DMT-HI provides more distinct class boundaries and a clearer representation of the inherent hierarchical structure of the data, demonstrating its superiority.\nBetter hierarchical relationships on biological datasets. On biological datasets (SAMUSIK, HCL, and MCA), as shown in Fig. 6, DMT-HI demonstrates its ability to effectively capture and visualize complex hierarchical relationships. Compared to other methods, DMT-HI provides a more coherent representation of the biological data structure, preserving both local and global relationships. This is particularly important in biological data analysis, where understanding the hierarchical nature of the data is crucial. DMT-HI excels in this regard by providing a more interpretable and biologically relevant low-dimensional representation, as evidenced by its performance across the SAMUSIK, HCL, and MCA datasets.\nStability in training and testing datasets. To ensure the comprehensiveness of our analysis, we have extended our comparison to include a wider range of datasets and methods, considering both the training and test phases. These additional visualizations, which can be found in the Appendix, further reinforce the advantages of DMT-HI across various datasets and scenarios. By offering more detailed comparisons across both image and biological datasets, we demonstrate that DMT-HI consistently outperforms traditional methods in terms of cluster separation, hierarchical representation, and overall interpretability."}, {"title": "F. Time Comsumption", "content": "In this experiment, we evaluate the time consumption of different dimensionality reduction methods on various datasets, including image datasets (MNIST, K-MNIST, E-MNIST) and biological datasets (Gask, MCA, HCL). These datasets vary in size and complexity, allowing a thorough comparison of performance across tasks.\nWe selected popular non-parametric methods (tSNE, UMAP, PaCMAP) and two deep learning methods (ssuch as DMT-HI), adjusting parameters according to official guide-lines. The experiments were conducted on a GPU-accelerated platform, though non-parametric methods typically do not fully utilize GPU, affecting their time performance.\nTable VI shows the time consumption for each method. Non-parametric methods like PaCMAP perform well on smaller datasets such as MNIST and K-MNIST, completing tasks quickly. However, on larger datasets like E-MNIST and HCL, deep learning models (DMT-EV and DMT-HI) demonstrate a significant time advantage, benefiting from GPU parallelization. While non-parametric methods are efficient for smaller datasets, their time consumption rises sharply with data size and complexity."}, {"title": "VI. ABLATION STUDY", "content": "In this section, we present ablation studies to analyze the impact of key parameters on the performance and stability of the DMT-HI model. We focus on three key parameters, the number of neighbors K in Eq. (III-A), the number of experts in the Mixture of Experts (MOE) network, and the hyperbolic parameter v. The results are summarized in Tables V, VII, and IV. We explore the influence of the number of neighbors K, which governs local structure preservation in the hyperbolic embedding. As shown in Table V, K = 5 provides a good balance between capturing local and global structures. Smaller K values capture insufficient local information, while larger K values introduce noise. Thus, K = 5 is set as the default value, ensuring stable performance across datasets.\nWe assess the effect of the number of experts in the MOE network, which improves the model's ability to handle diverse feature subsets. Table VII shows that increasing the number of experts enhances performance by capturing complex patterns. However, beyond 10 or 16 experts, diminishing returns and overfitting risks emerge. Therefore, 10 or 16 experts strike an optimal balance between performance and computational cost. We analyze the hyperbolic parameter v, which affects the curvature of the hyperbolic space and the model's ability to capture hierarchical structures. Table IV indicates that v = 0.5 balances capturing hierarchical relationships and maintaining stability. Lower v values capture stronger hierarchies, while higher values flatten the space, reducing its effectiveness. Thus, v = 0.5 is chosen as the default value for stability across diverse datasets."}, {"title": "VII. CASE STUDY & INTERPRETABILITY", "content": "A. Performance & Interpretability on Image Datasets\nTo further validate the DMT-HI model, we conducted a case study on the K-MNIST dataset, which contains complex handwritten Kanji characters. Compared to MNIST, K-MNIST presents more intricate structures, making it a challenging benchmark for evaluating a model's ability to handle high-dimensional data. We employed standard configurations to ensure fairness and reproducibility, using hyperbolic embedding for data analysis.\nPerformance advantages on K-MNIST dataset & the ability to discover subclusters. Despite the complexity of the dataset, the hyperbolic representations are well-organized, forming distinct character clusters in hyperbolic space (Fig. 7(a)). The sharp boundaries between clusters confirm the model's ability to preserve data structure even after high-dimensional mapping. This highlights DMT-HI's effectiveness in managing complex data. The model also excels in distinguishing characters with the same label but different structures. As seen in Fig. 7(b) (regions i, ii, iii, and iv), DMT-HI accurately separates characters that vary in strokes and shapes, reflecting its sensitivity to fine-grained features. This precise feature differentiation supports tasks like character recognition, laying a foundation for classification and pattern recognition. DMT-HI shows robustness in handling mislabeled data. Fig. 7(b) (regions v and vi) illustrates how the model embeds mislabeled points closer to their true categories, reducing the impact of label noise. This suggests that DMT-HI can identify and adjust anomalous points, making it useful for data cleaning in practical applications. The model can further subdivide clusters into meaningful sub-clusters (Fig. 7(b) regions v and vi), showcasing its ability to capture internal hierarchy and diversity in the data. This is crucial for analyzing datasets with complex structures, such as single-cell sequencing, enhancing DMT-HI's applicability across domains.Interpretability of the key pixel patterns. The expert analysis in Fig. 7(c) demonstrates the model's interpretability. Each expert focuses on different character attributes, provid-"}, {"title": "B. Interpretability on Biological Datasets", "content": "To validate the effectiveness and interpretability of the DMT-HI model, we conducted a case study using the HCL (Human Cell Landscape) dataset [67", "68": [69]}]}