{"title": "Imperfect Vision Encoders: Efficient and Robust Tuning for Vision-Language Models", "authors": ["Aristeidis Panos", "Rahaf Aljundi", "Daniel Olmeda Reino", "Richard E Turner"], "abstract": "Vision-language models (VLMs) demonstrate impressive capabilities in visual question answering and image captioning, acting as a crucial link between visual and language models. However, existing open-source VLMs heavily rely on pre-trained and frozen vision encoders (such as CLIP). Despite CLIP's robustness across diverse domains, it still exhibits non-negligible image understanding errors. These errors propagate to the VLM responses, resulting in sub-optimal performance. In our work, we propose an efficient and robust method for updating vision encoders within VLMs. Our approach selectively and locally updates encoders, leading to substantial performance improvements on data where previous mistakes occurred, while maintaining overall robustness. Furthermore, we demonstrate the effectiveness of our method during continual few-shot updates. Theoretical grounding, generality, and computational efficiency characterize our approach.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have significantly transformed the landscape of natural language understanding and generation, revolutionizing a wide range of domains and applications. These advancements bring us one step closer to creating assistants with reliable intelligence levels. Given that vision and visual understanding play a crucial role in intelligent agents expected to operate in the real world, Vision and Language Models (VLMs) have emerged. These models either incorporate embeddings from vision-only models or are trained end-to-end with both vision and language input. Remarkably, VLMs consistently achieve impressive performance across question-answering and image-captioning benchmarks. We refer to [7] for a recent survey on VLMs. Approaches that rely on pretrained vision encoders, typically use variants of the CLIP model, which is kept frozen in the vision language binding process. CLIP [25], a widely deployed vision and text transformer, stands out for its robustness to domain shifts and outstanding capabilities of recognizing large scale of objects, scenes and actions. However, our evaluation reveals certain limitations in CLIP's performance. Specifically, when tested on an action recognition dataset featuring various simple actions with moderate image quality, CLIP exhibits substandard performance and seems easily confounded by the image content. Other works [19, 36, 2, 15] reveal similar shortcomings of CLIP for particular use cases. Surprisingly, CLIP and CLIP-enabled VLMs demonstrate a highly accurate understanding of images generated by AI, representing similar actions. These findings underscore weaknesses in visual understanding of CLIP, specially on challenging and previously unseen domains, and prompts the need for continuous model improvements to address these imperfections.\nIn order to enable VLMs to adapt to OOD data, we envision a realistic scenario where the model can be updated efficiently with minimal computational resources while maintaining its strong performance\non other data and domains. In other words, we aim to correct mistakes effectively while preserving existing knowledge.\nGiven the composite nature of VLMs, which combine vision encoders and language models, the crucial question arises: Which components are better suited for targeted updates? To address this, we conducted separate fine-tuning experiments on the vision encoder and the Language Model (LLM) using a dataset where the VLM exhibited numerous mistakes. The results were intriguing: separately updating the vision encoder significantly improved the performance on the specific data of interest achieving even better accuracy than updating the LLM. Updating the vision encoder is more efficient as it contains far fewer parameters than the language model and can improve different VLMs that build on it. Our findings suggest that separately updating the vision encoder provides a more robust alternative to LLM updates when visual shift is the primary source of errors.\nDespite the effectiveness of vision encoder updates, continuous and frequent updates can lead to performance deterioration. Therefore, we recognize the need for not only efficient updates but also localization to the data at hand, in order to limit degradation in unrelated areas of knowledge. Parameter-efficient fine-tuning methods often degrade unrelated knowledge similar to full fine-tuning, as shown previously by [34] and as demonstrated in our experiments. For instance, LoRa's low-rank updates still alter all model parameters, resulting in performance decline upon multiple updates.\nTo achieve localized updates, we propose that only parameters relevant to the task at hand should be modified while keeping the rest of the model's parameters intact. A similar direction is followed in Language Model Editing, although existing approaches are usually specific to factual knowledge updates and cannot be easily applied to vision-related updates.\nIn order to identify which parameters should be updated, we construct the selection process as masking of parameters on which said masks should preserve the gradient norm of the estimated update of the model. This can be achieved by selecting parameters with the greatest gradient norm. Now, for MLP layers of the transformer model, we first follow a similar approach to SPU [34] by selecting the top k parameters according to the gradient norm. Our approach is generalizable to attention heads, where we select specific attention heads based on the same rule. We combine the estimated masks with low rank updates [12], in a unique way, achieving both locality and efficiency.\nWe validate our method across various benchmarks, both by updating CLIP and by enhancing VLM models based on CLIP. Our approach demonstrates superior performance and preserves the model's generic knowledge. While our focus lies on updating the vision encoder, our method is generic and applicable to any transformer model whether for vision, language, or any other modality. Our contribution are as follows: 1) We evaluated CLIP on out-of-distribution benchmarks and observed shortcomings in certain scenarios. These limitations are then propagated to the VLMs that leverage CLIP's embeddings. 2) Our study demonstrates that updating the vision encoder separately, specifically on data where CLIP fails, can significantly correct VLM mistakes on previously unseen images from this data. 3) We propose a novel parameter-efficient tuning method that not only targets efficiency but also ensures the preservation of the model's generic knowledge. Our method exhibits robustness in both few-shot learning and continual updates, achieving new state-of-the-art results."}, {"title": "Is CLIP truly robust to all conditions?", "content": "VLMs are either trained in an end-to-end fashion or made as a composite of separate vision and language models. In the latter, it is typically the vision encoder that remains frozen throughout the training process of the VLM. CLIP [25], a vision transformer model, trained by contrasting vision and language, is the primary vision encoder deployed in this line of VLMs. For instance, it is used in such fashion in MiniGPT, MiniGPTv2, BLIP2 [15], CogVLM [32], Kosmos-2 [24], LLaVA [19] and LLavaNext [18]. CLIP has been trained on large numbers of vision and language pairs. When testing CLIP models in challenging domains, including adversarial scenarios [21], the model demonstrates unprecedented robustness to various domain shifts. It was also noted by [21] that the large pretraining dataset of CLIP might contain representative examples of those out of distribution benchmarks, which raises questions about the real robustness of CLIP.\nWe investigate the out-of-distribution generalization capabilities of pretrained vision encoders, in order to assess whether future model updates are indeed unnecessary or whether simply most publicly available datasets used for testing are indeed represented in the pretraining data. We first considered the XImageNet-12 benchmark [16], a recent benchmark designed to examine the robustness of vision models, by simulating diverse out of distribution effects. The basic CLIP model, CLIP-ViT-B-16, shows performance impressively close to perfect on most domains. Our results and a full description of the dataset can be found in Appendix E.\nHowever, XImageNet-12 is generated based on ImageNet images, and hence the generalization to never seen images under distribution shift is still not proven. We then conducted a simple yet realistic evaluation. We considered Toyota Smart Home (TSI) dataset [4] a dataset of daily living activities staged in a home-like environment. This dataset cannot be publicly crawled from the web, it is only accessible upon request. This makes it unlikely to have been used to train CLIP. Further, the data depict elderly people activities (age bias), blurred faces (blurring effect) and is captured from a mounted camera with somewhat low resolution, yet the actions are easily recognizable to the human eye. For more details, refer to the Appendix and Experiments Section 5.\nSurprisingly, when evaluating CLIP on images from the TSI dataset, we observed only moderate performance and encountered numerous mistakes. Table 2 reports CLIP accuracy on TSI dataset compared to ImageNet. Now, we consider whether a VLM using CLIP's vision encoder would be able to describe the activity in these images accurately or not. Further testing of a VLM (MiniGPTv2 [2]) using CLIP as the vision encoder revealed similarly poor performance. We refer to Figure 1 for an example response and to the Appendix for more qualitative results. The main failure modes present are hallucination of wrong activities or describing the background rather than the action. This suboptimal performance could be attributed to either CLIP's limited knowledge of the performed activities or its lack of robustness to the distribution shift present in the images. To address the first assumption, we leveraged diffusion models, specifically DALL\u00b7E 2, to generate images of people performing the same actions. After verifying the accuracy of these generated images, we tested CLIP's predictions on them. Remarkably, CLIP accurately recognized the actions on the synthetic images. Similarly, MiniGPTv2 [2] also provided very accurate descriptions of the generated images. Figure 1 show some generated images compared to images from TSI [4] of similar activities.\nFinally, we conducted an experiment where we finetune CLIP encoder on a training split of TSI dataset. We then plug CLIP back into MiniGPTv2 and evaluate VQA accuracy on TSI and DALL-E datasets. For a sanity check, we also finetune the LLM decoder (Llama2) and the linear layer using LoRa [12]. Results are shown in Table 1. Finetuning the vision encoder separately can drastically improve the performance of MiniGPTv2 on both TSI and DALL-E images, with 4% improvement over finetuning the LLM. Note that optimizing the LLM needs careful attention and hyper-parameter tuning, in contrast, tuning the CLIP model is comparatively straight-forward. Given the difference"}, {"title": "Low-Rank Adaptation with Structured Updates", "content": "To efficiently fine-tune the large visual encoder\u00b9 without incurring catastrophic forgetting, we develop a new parameter-efficient fine-tuning method, called Low-Rank Adaptation with Structured Updates (LorSU). LORSU aims to update specific parameters from each transformer block in a parameter efficient way while circumventing the risk of generic knowledge loss due to fine-tuning on the current task. To achieve this, we update a small part of the parameters of the first linear layer of the MLP block in the transformer module of each layer as in [34]. However, this approach might lack flexibility since all the other parameters from the transformer block remain frozen during optimization. To overcome this, we also update the most informative attention heads based on the gradient information of the loss function.\nMore specifically, let a dataset $\\mathcal{D}_t = \\{x_n, y_n\\}_{n=1}^N$ for the current task t where $x_n$ is an image with text description $y_n$ and $\\mathcal{L}(\\theta;\\mathcal{D}_t) := \\mathcal{L}_t(\\theta)$ is the contrastive loss used for CLIP pretraining and $\\theta \\in \\mathbb{R}^d$ is the full set of model's parameters. The standard Multi-head Self-Attention Mechanism [31], comprised of H heads, is defined as the concatenation of multiple self-attention (SA) blocks:\n$\\begin{aligned}\nq^{(i)} &= W_q^{(i)} Z^T, \\quad k^{(i)} = W_k^{(i)} Z^T, \\quad v^{(i)} = W_v^{(i)} Z^T \\in \\mathbb{R}^{D_h \\times N},\\tag{1}\\\\\nA^{(i)} &= \\text{softmax}(q^{(i)} k^{(i)} / \\sqrt{D_h}) \\in \\mathbb{R}^{N \\times N},\\tag{2}\\\\\nSA_i(Z) &= A^{(i)} v^{(i)} \\in \\mathbb{R}^{N \\times D_h}, \\quad i = 1, ..., H.\\tag{3}\n\\end{aligned}$\nwhere $Z \\in \\mathbb{R}^{N \\times D}$ is the input matrix of N tokens of dimension D and $W_q^{(i)}$, $W_k^{(i)}$, and $W_v^{(i)}$ are the query, key, and value matrices of learnable parameters for head i, respectively. The final MSA function is defined as\n$\\begin{aligned}\nMSA(Z) &= \\text{Concat} [SA_1(Z), ..., SA_H(Z)] W_o \\in \\mathbb{R}^{N \\times D}, \\quad W_o \\in \\mathbb{R}^{H D_h \\times D}.\\tag{4}\n\\end{aligned}$\nSince we care to update the parameters of the heads that cause the largest changes in $\\mathcal{L}_t(\\theta)$, we compute the gradient of the loss with respect to the parameters of each head and then we pick to update those heads that their cumulative contribution to the loss change is the largest. Since the matrices $W_q^{(i)}$, $W_k^{(i)}$, $W_v^{(i)}$ are all the parameters of head i, we can define an importance score for each head by adding the squared values of their corresponding gradients $G_q^{(i)} = \\nabla_{W_q^{(i)}} \\mathcal{L}$, $G_k^{(i)} = \\nabla_{W_k^{(i)}} \\mathcal{L}$, $G_v^{(i)} = \\nabla_{W_v^{(i)}} \\mathcal{L}$, and $G_o^{(i)} = \\nabla_{W_o^{(i)}} \\mathcal{L}$, i.e.\n$\\begin{aligned}\ns_i = \\sum_{m,l} \\Big( (G_q^{(i)}[m, l])^2 + (G_k^{(i)}[m, l])^2 + (G_v^{(i)}[m, l])^2 + (G_o^{(i)}[m, l])^2 \\Big) .\\tag{5}\n\\end{aligned}$\nWe provide a theoretical justification of (5) in the next section. We choose the parameters of the top-k heads $\\{s_1, ..., s_H\\}$, $I \\subset \\{1, ..., H\\}$, to be updated on the current task. However, the number of parameters remain high due to the large weight matrices. Therefore, we opt for parametrizing the original weights using LoRA parametrization [12] to reduce computational burden. The matrices $W_q^{(i)}$, $W_k^{(i)}$, $W_v^{(i)}$, $i \\in I$ are now defined as\n$\\begin{aligned}\nW_q^{(i)'} &= W_q^{(i)} + A_q^{(i)} B_q^{(i)}\\\\\nW_k^{(i)'} &= W_k^{(i)} + A_k^{(i)} B_k^{(i)}\\\\\nW_v^{(i)'} &= W_v^{(i)} + A_v^{(i)} B_v^{(i)}.\n\\end{aligned}$\\tag{6}\\\n\\tag{7}\\\n\\tag{8}\nFinally, to ensure that we only update $W_q^{(i)}$, $W_k^{(i)}$, $W_v^{(i)}$, $i \\in I$ we use a binary mask on the gradient vector with respect to all parameters of all attention heads. We keep the projection matrix $W_o$ frozen throughout optimization."}, {"title": "Theoretical justification", "content": "The scores in (5) can be derived from the following constrained (binary) optimization problem\n$\\begin{aligned}\np^* &= \\text{argmax}_{p \\in \\{0,1\\}^d} \\frac{\\|p \\odot \\nabla_w \\mathcal{L}(\\theta_0)\\|^2}{\\|\\nabla_w \\mathcal{L}(\\theta_0)\\|^2} \\\\\n\\text{s.t.} \\quad & \\mathcal{I}_l \\subset \\{1,2,..., d\\}, \\text{where } \\mathcal{I}_i \\cap \\mathcal{I}_j = \\emptyset, \\forall i \\neq j, \\\\\n&S = \\sum_{l=1}^G s_l, \\quad S_l \\leq |\\mathcal{I}_l| \\quad \\forall l, \\quad \\|p\\|_0 \\leq S,\\tag{10}\n\\end{aligned}$\nHere $\\theta_0$ is the pretrained vector of parameters before we use the $\\mathcal{D}_t$ for fine-tuning. The mask $p^*$ is chosen so that the gradient norm is preserved against masking as much as possible under these constraints.\nDefinition 3.1. The operator TOP-S : $\\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, for $1 \\leq S \\leq d$ is defined as\n$\\begin{aligned}\n(TOP-S(x))_{\\pi(i)} := \\begin{cases}\nx_{\\pi(i)}, & i \\leq S\\\\\n0, & \\text{otherwise},\\n\\end{cases}\n\\end{aligned}$\nwhere $\\pi$ is a permutation of $\\{1, 2, ..., d\\}$ such that $|x_{\\pi(i)}| \\geq |x_{\\pi(i+1)}|$, for $i = 1, ..., d-1$, i.e. the TOP-S operator keeps only the S largest elements of x in magnitude and truncates the rest to zero.\nLemma 3.2. For any $x \\in \\mathbb{R}^d - \\{0\\}, 1 < S \\leq d$, the optimal mask\n$\\begin{aligned}\np^* = \\text{argmax}_{p \\in \\{0,1\\}^d} \\frac{\\|p \\odot x\\|^2}{\\|x\\|^2} \\text{ s.t. } \\|p\\|_0 \\leq S,\n\\end{aligned}$\nhas zeros everywhere except the S largest elements of x in magnitude.\nProof. Rewriting the optimization problem as\n$\\begin{aligned}\n\\text{max}_{p \\in \\{0,1\\}^d} \\sum_{i=1}^d p_i x_i^2 \\text{ s.t. } \\sum_{i=1}^d p_i \\leq S,\n\\end{aligned}$\nwe notice that this a trivial binary knapsack problem with maximum weight capacity S and weights equal to one. Hence, the maximum is attained when we pick the S maximum $x_i^2$.\nRemark 3.3.\nIt holds that $TOP-S(x) = p^* x$.\nCorollary 3.4. The optimal mask $p^*$ in (10) has zeros everywhere except for the indices $i \\in \\{j : \\exists l \\in \\{1,...,G\\}, \\text{such that } j \\in \\{\\pi_l(1), ..., \\pi_l(s_l)\\}\\}$, where $\\pi_l$ is the same permutation as in Definition 3.1 for the set of indices $\\mathcal{I}_l$.\nProof. The result follows from the mutual exclusiveness of $\\mathcal{I}_l$ in the constraints of (10) and Lemma 3.2."}, {"title": "Related Work", "content": "Large language model and vision language models are strong foundation models but are still prone to mistakes and their knowledge can get outdated, consequently it is important to develop efficient updates that preserve unrelated knowledge. The main line of work in this area focuses on LLM editing, where previous factual knowledge has changed and the model must be updated effectively on those changes. Most notably [22] and [13] first analyze the models to identify specific layers for editing, i.e., where factual knowledge are \"stored\" in the model, and then apply algebra-based or meta-learning methods to adjust the weights of these localized layers. To insure the locality of the updates these methods usually leverage additional sets representing unrelated factual knowledge.\nAnother line of work focus on updating the model for a new task or dataset with parameter efficient finetuning. Low Rank Updates (LoRA) [12] approximates the parameter updates by a low rank matrix, achieving similar performance on the target task by optimizing only 1% of the parameters compared to the full model. The original version of LoRA updated only attention layers. Subsequently, several extensions have been proposed to enhance LoRA which modify all layers. Various options are available including adapting the learning rate [10] of the low rank matrix, using an adaptive rank [33] or decomposing the update matrix into magnitude and direction [20]. These approaches focus solely on efficiently updating the network without considering the impact on model performance for other unrelated tasks or enforcing any locality to specific layers or parameters. It is worth noting that LORA drop [35] attempts to localize the updates to specific layers. It initially allows a few iterations of LoRA updates and then assesses the impact of each low-rank update on individual layers and selectively updates only those layers where the change exceeds a specified threshold. However, this selectivity remains at the layer level and depends on the change introduced by a few full updates. In contrast, we treat each layer differently based on its structure and assess the relevance of individual parameters to the task at hand. We then holistically combine the importance and relevance of these parameters with low-rank updates.\nIn the context of updating vision models for specific tasks, SPT [11] estimates a mask of updates based on parameter sensitivity to the task. Depending on the number of relevant parameters, either low-rank or sparse updates are performed (using a threshold). With regards to continual updating CLIP while maintaining its generalization performance and reducing forgetting, SPU [34] treats layers of the transformers differently, and inspired by knowledge neuron theory, SPU localizes the updates to the first feedforward layer of each transformer block and then only relevant parameters to the task at hand are updated. We further refer to [5] for a survey on continual learning. In our approach, we select and identify relevant parameters to the current data. However, we generalize the updates to all layers while preserving the specificity of each layer. We choose masks that maintain the gradient norm of parameter updates and combine them with LoRA on selected attention heads, striking a balance between adaptivity and stability."}, {"title": "Experiments", "content": "The section serves to support our conclusions on CLIP's robustness and how finetuning it separately, with our method, can result in a robust performance both in offline and continual few-shot updates. The robust performance after updates is reflected in evaluations using two VLMs."}, {"title": "Setting", "content": "In the following we describe the construction of TSI based datasets, and other used datasets, training protocols, models considered, and methods compared.\nClassification datasets. TSI: We process the TSI [4] dataset as an image classification dataset where the target is to recognize the activity depicted in each image. We extract frames from videos and create a train set of approximately 10K images and a test set of approximately 5k images. We consider 22 represented classes of activities. DA-E: We consider the same 22 classes of activities represented in TSI and query DALL\u00b7E 2 to generate representative images of these activities. We extract 30 images per action totaling 660, all of them are designated for testing. ImgNet: We consider ImageNet [6] as a control set to measure how much CLIP models' performance deteriorates after being tuned on other datasets. GTS [28] the German Traffic Sign dataset. In [34] GTS was considered as out of distribution for CLIP pretraining, CLIP zero shot performance is significantly lower than the performance of a linear classifier trained on ResNet50 features [25].\nVisual Question Answering datasets. To evaluate how the examined VLM is performing before and after the vision encoder update, we consider 3 visual question answering datasets: VizWiz [9]: a visual question answering dataset designed to mimic the scenario where a visually impaired person takes a photo by phone and asks the visual assistant a question about it. It consists of about 31,000 visual questions. VSR [17]: a dataset for Visual Spatial Reasoning with more than 10k natural text-image pairs and 66 types of spatial relations. HM [14] hateful memes dataset designed to detect multimodal hateful memes. TSI: We convert the TSI classification dataset into a VQA dataset with multiple choice responses and measure the likelihood of the correct response following common practice in VQA evaluation. DA\u00b7E: Following the practice for TSI, we convert DA\u00b7E generated dataset to VQA format.\nTraining protocols. Offline: We first consider an offline fine-tuning setting as a sanity check where the vision encoder is updated offline on the full training set of TSI data. The goal is to asses the performance of CLIP before and after the update by different methods and the VLM responses when the updated vision backbone is plugged in.\nContinual & few-shot: We design this setting to imitate a realistic scenario where the model is updated on images where it makes mistakes with few-shot examples, and the process is to be repeated as long mistakes are shown. We follow the common practice in continual fewshot learning [23] to construct the sequences. We divide the dataset into 5 sets of disjoint classes and consider 20 shot setting where only 20 training examples of each action is provided. Accuracy is measured on the full test set. In the Appendix we consider 50 shots setting. We always report the accuracy (classification or VQA) on the test sets of the concerned datasets at the end of a training sequence, with that we measure the ability of the model to accumulate knowledge and resist forgetting [5]. Note that we do not consider any replay [1] of samples from classes of previous sessions.\nImplementation details. We refer to the Appendix B for implementation details.\nModels. We consider two versions of CLIP, namely OpenAI-CLIP-L-14 and EVA-CLIP-G-14. OpenAI-CLIP-L-14 [25] is a large ViT pretrained by OpenAI. EVA-CLIP-G-14, a giant ViT, is an improved version with some set of optimization and augmentation techniques [29] pretrained on LAION-400M dataset [26]. For vision language models, we consider two popular vision language models that leverage a frozen CLIP image encoder. LLaVA [19] Large Language and Vision Assistant, connects pretrained and frozen OpenAI-CLIP-L-14 with a LLM (Vicuna-7b [3]) though a linear layer. The LLM and the linear layer are optimized during the visual instruction tuning process while CLIP remains frozen. MiniGPTv2 [2] concatenates adjacent tokens from EVA-CLIP-G-14. OpenAI image embedding and process it with a linear layer as input to LLama-2 (7B-chat) [30]. Similar to LLaVA [19] the linear layer and the language model are optimized while the vision encoder remains frozen.\nMethods. When finetuning CLIP, we fine-tune both visual and text encoders following [8] with the same contrastive image language loss used in the pretraining of CLIP. We consider the following methods for fine-tuning. F-FT: Full fine-tuning of all model parameters. This can provide the best accuracy, but is prone to forgetting and overfitting. LN: Optimization of the layer norm parameters of the transformer, an adaptation of [27]. This approach modifies a very small fraction of parameters and has been shown to be a robust approach for few-shot updates [23]. LoRA: we consider Low rank updates [12] applied to all transformer layers. SPU: Selective Parameters Updates [34] is a recent method proposed to continually update CLIP with minimal forgetting and generic knowledge loss. LoRSU: We coin our approach LoRSU as short for Low Rank Structured Updates. We always report the zero-shot performance of the model (without training), we refer to this as Zr-shot."}, {"title": "Results", "content": "Offline updates. We first consider the common finetuning scenario where we fully finetune the CLIP encoder on the considered dataset and then evaluate the performance on classification and VQA tasks. Table 3 reports the accuracy after finetuning CLIP models on TSI and GTS datasets (in separate sessions). On the target dataset, F-FT achieves the best accuracy but lacks behind other methods on ImgNet and even on DA\u00b7E. For example, using an EVA-CLIP backbone in the TSI dataset, F-FT accuracy decreases on DA\u00b7E from 97% to 88.6% after fine-tuning. Our method LoRSU achieves a performance close to that of F-FT on the target dataset while keeping the accuracy on ImgNet and DA-E almost unchanged. LoRA achieves slightly lower performance on the target datasets (TSI and GTS) than LoRSU, but suffers a larger margin of forgetting on other datasets. SPU maintains the performance on ImgNet and DA\u00b7E similar to LoRSU but underperforms on the target dataset especially with TSI. LN accuracy on the target dataset is the lowest, however the accuracy on other dataset are also severely affected. LoRSU achieves the best performance on all datasets.\nContinual few-shot updates. Having evaluated the performance of LoRSU and other methods on the offline updates setup, we explore a more challenging setting: multiple consecutive sessions of updates with small numbers of examples in each. The goal is to examine the robustness of LORSU compared to other methods when continual few-shot updates are performed. Table 4 reports the accuracy on each test set at the end of each sequence for TSI and GTS with 20 shots in each session. Please refer to the Appendix for results with 50 shots. Due the challenging nature of this setting, we see larger gaps among the compared methods. F-FT fails to achieve the best accuracy on the target dataset. It suffers from catastrophic forgetting, resulting in performance deterioration on the ImgNet and DALL-E datasets when training sequentially on TSI. Interestingly, LoRA here shows a much weaker performance than the offline setting with accuracy on the target dataset close or worse than finetuning and an even larger rates of forgetting on ImgNet and DA-E, especially after training on TSI. LN here remains behind LoRSU but achieves better performance than LoRA. Low rank updates do not necessarily result in a reduced change in parameters and can significantly change the behaviour of the model on other datasets. While SPU remains robust and achieves good performance on both target and other datasets, our method LoRSU improves on the target dataset with a considerable margin and has the least negligible deterioration (<2%) on ImgNet and DA.E. Results are consistent for both models and for both sequences (TSI and GTS). Note that SPU performance drops by more than 10% on DA\u00b7E for the EVA-CLIP backbone under TSI sequence.\nVQA performance after continual fewshot updates of CLIP. We have so far only reported the accuracy of finetuned CLIP on the classifications task. Here we investigate how plugging this continually finetuned models in the respective VLMs affect the VQA tasks. VQA results are reported in Table 5 for TSI and GTS under the 20-shot continual learning setting. For TSI, all methods were able to improve the performance over Zr-Shot, with LoRSU performing best, and with a margin of 2.6% over LLaVa. Our method also shows the least degradation on other VQA datasets, followed by SPU. Our method, LoRSU, even improves the performance on other VQA datasets for MiniGPTV2 with a small margin. With respect to GTS, our method was the only method to improve the performance of VQA on GTS. Other methods perform worse than Zr-shot, except from SPU on LLaVA with similar performance to Zr-shot. The performance on other datasets is less pronounced than the case of TSI. We attribute this to the out-of-distribution nature and the finegrained classes of GTS. Further, the LLM seems to be able to cover for the changes in the CLIP backbones and suffers less disturbance on other datasets than when evaluating the classification performance of CLIP itself. Overall, it can be stated that LoRSU is able to improve the performance on the VQA tasks for both VLMs, even in a few-shot setting, and under different sequential sessions with no experience replay."}, {"title": "Limitations", "content": "We considered the question of whether vision encoders can be updated separately to improve VLMs performance. Our conclusions are made primarily based on the TSI and GTS datasets that were chosen to avoid using other publicly available datasets that have been used during CLIP pre-training. Further, we do not consider longer sequences due to compute resources, however interesting observations might follow. Our method is only applied to the ViT model, while in principle it can be integrated to update the LLM. We leave this for future work. Studying the effect of separate updates in the context of the composite nature of VLMs is challenging and we have only scratched the surface."}, {"title": "Conclusion", "content": "In this work, we investigated the limitations of CLIP on out-of-distribution benchmarks and found de-spite robustness on public and AI generated data it can fall short in certain scenarios with challenging visual conditions. These limitations are then inherited by the VLMs that utilize CLIP's embeddings. To address this, we propose a novel approach: updating the vision encoder separately, specifically on data where CLIP fails. Remarkably, this strategy significantly corrects VLM mistakes on previ-ously unseen images from the same data. We further introduce a parameter-efficient tuning method that not only targets efficiency but also ensures the preservation of the model's generic knowledge. In our experiments, our method, LoRSU, is the only method to systematically improve the VLM performance on VQA tasks even in the challenging, but realistic, continual fewshot setting. Our approach hence strikes a strong balance between efficiency, effectiveness and robustness, achieving new state-of-the-art results."}, {"title": "Appendix", "content": "Due to space limit, we report here the results of other experimental setting. First, we report VQA after offline updates of CLIP. In Table 13 and Table 16 we report the VQA for both considered VLMs after finetuning CLIP models on TSI and GTS datasets respectively. Our method achieves the best performance on the target dataset compared to other parameter efficient tuning methods while maintaining a strong performance on other VQA datasets. Next we considered a slightly more relaxed continual few-shot setting, by allowing 50 shots per session. Tables 11 and 14 report the classification accuracy of the two considered CLIP models after fewshot continual learning on TSI and GTS datasets. Similar conclusions can be made here. Our methods continue to perform the best on the target dataset while maintaining the performance on other datasets. Our method specifically set a large margin of 16% on DA-E compared to other methods when the target dataset is TSI. This indicates that our method updates are generalizable to other images of the same actions differently from other studied methods."}, {"title": "Prompts used to generate images from DALL\u00b7E 2", "content": "We generated images from DALL\u00b7E 2 using OpenAI python package and we used the prompt \"A person {a}\" where a \u2208 { using a white coffee machine, eating, cutting bread, stirring the pot, holding a glass, watching TV, holding a bottle, walking, making tea, cutting food, holding a cup, using a laptop, lying down, holding a can, person holding a black kettle, reading a book, cleaning up, sitting down, using a tablet, boiling water in a black kettle, using a cordless phone, washing dishes}."}, {"title": "Implementation Details", "content": "\u2022 We use a single A100 GPU for the experiments.\n\u2022 We use Adam as an optimizer with and learning rate scheduler of Cosine Annealing with Warmup for all methods.\n\u2022 We use batch size 8 for the few shot experiments and bsize 64 for the offline ones.\n\u2022 For few-shot experiments we use 50 epochs for the TSI dataset and 10 epochs for GTS. We use 20 epochs and 10 epochs for the TSI dataset and GTS, respectively for the offline setting.\n\u2022 For Lora we use rank=32 for all experiments.\n\u2022 For SPU we use sparsity=10% for all experiments.\n\u2022 For LoRSU we use sparsity=5%, rank=32, and we pick the top-2 attention heads for all experiments.\n\u2022 For LoRSU and SPU, to pick the top-k parameters from the first MLP layer we use either 800 for the offline setting or all the available data points in the dataset of the current task for CL-few shot setting.\n\u2022 For all VQA datasets we measure performance based on accuracy of the predicted answers of the VLM.\n\u2022 We converted TSI-VQA, GTS-VQA, and DA-E-VQA as a multiple choice VQA problem, each question has 5 options and the VLM is asked to choose the right one. For the other other datasets we follow [2] for the evaluation protocol."}, {"title": "Parameters efficiency", "content": "Table 9 reports the number of parameters updated by each method and the percentage with respect to model size for both considered CLIP models. LN uses the least amount of parameters, however it lacks behind in accuracy on all evaluated datasets. LoRSU operates on fewer parameters compared to LoRa and SPU and yet strikes a strong balance between target datasets and the maintenance of generic knowledge, achieving the best performance in both classification and VQA tasks."}, {"title": "TSI Datset construction", "content": "To extract images from the videos of the Toyota Smart Home dataset (TSI), we discretized each video clip into 2 frames per second and then selected the frame in the middle of the total time duration of the video clip. In Table 6 we describe the actions that were selected and the corresponding prompt used for CLIP classification. We also note dropping few actions to avoid ambiguous classes."}, {"title": "Evaluation of CLIP on XImageNet-12", "content": "In the Section 2 we evaluated CLIP robustness on XImageNet-12 benchmark [16]. Here we describe this experiment in more detail. XImageNet-12 benchmark [16] covers 12 common categories from ImageNet and simulating six diverse out of distribution effects, such as overexposure, blurring, and color changing. Table 17 reports the results of CLIP ViT-B-16 with different pretraining. Although only one domain with random backgrounds of other objects exhibits weak performance, this could be attributed to model confusion between the two objects in the foreground and background, rather than a weakness in understanding the image."}, {"title": "Examples of TSI and DALL\u00b7E (DA-E) datasets", "content": "We show additional examples of TSI images and DA-E generated images for some actions in Figures 2, 3, 4, 5, 6, 7, 8, 9, 10."}]}