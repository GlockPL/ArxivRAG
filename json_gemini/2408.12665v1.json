{"title": "Fairness-Aware Streaming Feature Selection with Causal Graphs", "authors": ["Leizhen Zhang", "Lusi Li", "Di Wu", "Sheng Chen", "Yi He"], "abstract": "This paper proposes a new online feature selection approach with an awareness of group fairness. Its crux lies in the optimization of a tradeoff between accuracy and fairness of resultant models on the selected feature subset. The technical challenge of our setting is twofold: 1) streaming feature inputs, such that an informative feature may become obsolete or redundant for prediction if its information has been covered by other similar features that arrived prior to it, and 2) non-associational feature correlation, such that bias may be leaked from those seemingly admissible, non-protected features. To overcome this, we propose Streaming Feature Selection with Causal Fairness (SFCF) that builds two causal graphs egocentric to prediction label and protected feature, respectively, striving to model the complex correlation structure among streaming features, labels, and protected information. As such, bias can be eradicated from predictive modeling by removing those features being causally correlated with the protected feature yet independent to the labels. We theorize that the originally redundant features for prediction can later become admissible, when the learning accuracy is compromised by the large number of removed features (non-protected but can be used to reconstruct bias information). We benchmark SFCF on five datasets widely used in streaming feature research, and the results substantiate its performance superiority over six rival models in terms of efficiency and sparsity of feature selection and equalized odds of the resultant predictive models.", "sections": [{"title": "I. INTRODUCTION", "content": "Streaming data deluges in online applications [1]\u2013[3], of which the huge volume tends to prohibit real-time decision-making that merely leverages manpower. Decisions aided by computing models are hence increasingly becoming a norm, which are about job or loan allocation [4]\u2013[6], criminal recidivism [7], fraud detection [8], online advertising and recommendation [9], to name a few. However, such algorithmic decisions may unfairly discriminate against certain social groups that are constructed upon protected user characteristics, such as gender, age, and ethnicity [10].\nThe pursuit of mitigating algorithmic biases originates from the research conducted by Friedman and Nissenbaum in 1996 [11] and has thrived within a decade [12]\u2013[21], mainly due to the recent popularity of data-driven prediction and classification models [6]. In those models, bias exists in the form of spurious (or superficial) statistical correlation between feature vectors and target labels, which represent users and decisions, respectively. Thus, the key idea shared by existing studies lies in the prevention of spurious correlations in training data from influencing the resultant model. Three main research thrusts stem from this idea [20]: 1) pre-processing methods that directly modify statistical distributions of training data [20], [22], 2) in-processing methods that impose fairness-based constraints on the objective function to regularize the training procedure [23], [24], and 3) post-processing methods that scrutinize predictions and finetune the predicted label distributions to enforce outcome fairness with respect to prediction accuracy [25], [26].\nDespite progresses, these thrusts mostly overlook one prominent stage in the lifecycle of data-driven applications, i.e., data generation. Too often in data-driven businesses, practitioners can source and inquiry hundreds even thousands of features for better user understanding and more precise user profiling [27]. To wit, to detect fraudulent activities in online transactions, various features describing user behaviors are procured, such as the max/min/mean/top-k transactions over the last 10 seconds, 3/24/48 hours, or 1/2/4 weeks, etc. Consider the substantial loss of financial frauds [28] and the fact that fraudsters can evolve their strategies to bypass the known indicative features [29], the banking industry urges to engineer more features and feature combinations to spotlight them. We refer to this data generation continuum as streaming features [30], where the feature set is non-exhaustive with new features constantly emerging.\nHow can algorithmic bias be eliminated from streaming features? This question remains open within the data mining community, and this study pioneers its first exploration. Its main technical challenge is twofold. On the one hand, the bias structure between features and label may not be associational [31], thereby complicating the strategy of predefining a subset of protected features and then removing all new features entailing them in the streaming features. On the other hand, new features may appear in high throughput, questioning the scalability of traditional feature selection methods being aware of fairness [20], [32], [33]. These methods operate and model the bias structure within a fixed feature space and thus must be reiterated each time a new feature is introduced. They falter, if the processing time for each feature exceeds the timespan of feature generation.\nTo overcome the challenge, we propose a new algorithm, named Streaming Feature Selection with Causal Fairness (SFCF). The key idea of SFCF is to leverage a causal structure that enables non-associational modeling among protected features, admissible features (not protected, but carry sensitive demographic information), and the target labels. Unlike previous causal fairness studies [12], [31] that mostly postulate a known causal structure (which is next to impossible without domain knowledge), our SFCF excels by learning causal structure from streaming features in an online manner. Specifically, SFCF dynamically constructs two causal graphs being egocentric regarding the protected feature and label; once a new feature arrives, its topological positions on the two graphs are determined by its contribution to the algorithmic bias and prediction accuracy, respectively. In particular, if this feature is irrelevant or redundant to the label, it has no contribution to prediction and thus can be discarded. On the contrary, if the feature is relevant as well as non-redundant to the protected feature, it is inadmissible by possibly leaking the bias of the protected user information through the causal structure, thus is also pruned. In this study, we leverage d-separation [34] to determine the feature positioning on causal graphs, allowing for the gauging of feature relevancy and redundancy in terms of conditional independence, regarding the target variable, i.e., label or protected feature, in their respective causal graphs.\nSpecific contributions of this paper are as follows:\n1) Our work is the first study of online feature selection in streaming features with respect to the algorithmic performance regarding both accuracy and fairness.\n2) We propose a new SFCF algorithm to solve the problem, with its key idea lying in a dynamic modeling of causal structure among protected features, inadmissible and admissible features, and labels.\n3) Extensive experiments on five benchmark datasets show the superiority of SFCF outperforming its six state-of-the-art competitors on average in the aspects of equalized odds, sparsity, and runtime by 52%, 98%, and 99%, respectively, while maintaining an average accuracy that remains virtually unchanged.\n4) \u03a4o champion reproducible research, our code and datasets are openly accessible at https://github.com/zlzhan011/FSFS.git."}, {"title": "II. RELATED WORK", "content": "A. Fairness-Aware Machine Learning\nWe can divide fairness-aware machine learning algorithms into three primary categories. 1) For pre-processing approaches [12], [17], [19], [20], [25], [31], [35], [36], they aim to directly modify statistical distributions of training data by assigning weights to features or labels to achieve a fair representation of protected and reference groups with respect to their label assignments. For example, [25], [35] propose massaging and reweighing techniques to adjust training data, ensuring predictions are independent of protected attributes. [19] proposes a CGF framework that integrates causal graph structure learning with fairness regularization to minimize unfair edge weights, ensuring fair predictions by eliminating unfair causal effects. [17] enforces statistical independence between model outputs and protected attributes, using Wasserstein-1 distances to align distributions across different groups. [31] resamples training data to reduce bias, ensuring predictions independent of sensitive attributes for causally fair prediction, which could be time-consuming due to the sampling process. [12] firstly introduces the inadmissible features, which are non-protected but could leak biased information from feature reconstruction. To mitigate their impact, [12] proposes Capuchin which repairs training data by implementing interventional fairness, where both protected and inadmissible features are d-separated in the causal structure. [20] introduces a HypeR framework, which uses a conditional probabilistic causal model to handle what-if and how-to queries, reducing discrimination for fair causal predictions in databases.\n2) For in-processing, the methods [37], [38] modify learning objectives or model structures to achieve fairness. For example, [37] introduces a method integrating discrimination and information gain as a new splitting criterion when constructing a Hoeffding Tree. [38] proposes techniques incorporating regularization terms motivated by Mixed-integer programming to encourage low discrimination score. 3) For post-processing, the methods [39], [40] adjust algorithm predictions to achieve fairness. They involve altering the confidence of classification rules or re-labeling predicted classes in decision trees to prevent biased decisions against protected groups. However, all these methods cannot be directly adapted to streaming feature selection, mainly for two reasons. First, they are offline and presume all features are accessible prior to the learning process, which cannot be satisfied in our context wherein features emerge one at a time. Second, they mostly postulate prior knowledge of domain data (e.g., a known causal structure) or model architecture (e.g., trees or tree ensembles), thereby suffering from a generalization crisis when such assumptions do not hold in practice. Our proposed approach excels in the sense that we allow a sequential feature input and we do not rely on any domain knowledge nor assuming the structure of classifiers, thus is more general.\nB. Online Streaming Feature Selection\nThe main task of the methods in this category [41]\u2013[45] is to generate a feature stream and select the optimal feature subset, aiming to maximize prediction performance while keeping selected features as few as possible. For example, [41], [42] introduce an Alpha-investing method, which incorporates a new feature into the framework hinges on the p-value, with the impact of this addition evaluated through linear regression. To guide the selection of potential features, their technique relies on a heuristic understanding of the feature space's structure. However, this strategy may struggle with processing unmodified streaming features directly due to the practical challenge of acquiring comprehensive prior insights into such a structure. To relax this requirement, OSFS [43] builds a Markov blanket for the target labels based on statistical independence, approximating the causal relationship between streaming features and labels. OCFSSF [44] extends OSFS to consider spouse features of the Markov blanket by analyzing both conditional and unconditional independence relationships. However, these models mainly focus on the accuracy of prediction models and do not consider algorithmic fairness. Thus, once the protected and inadmissible features are wrongly incorporated into the selected feature subset, the resulting model will likely cause discrimination against protected groups, leading to algorithmic bias. To fill this gap, our proposed approach joins two objectives for optimizing both accuracy and fairness performance in the feature selection process. In particular, our approach allows for new features originally being redundant to the label into the selected feature subset, so as to remedy the information loss incurred by singling out the protected and inadmissible features for classification, thereby balancing the tradeoff between accuracy and fairness."}, {"title": "III. PRELIMINARIES", "content": "A. Problem Statement\nLet a sequence of features be $X = {X_i | i = 1,..., D} \\in \\mathbb{R}^{D \\times N}$, where $X_i$ denotes the feature that emerges at the round i, and D signifies the length of the feature stream. The column vector $Y := {0,1}^N$ denotes the ground-truth labels. There are N instances in total. We have $S \\& X$ the protected feature (e.g., gender, age, or ethnicity). The feature subset selected at round i is denoted by $F^*, |F^*| \\leq i$.\nLet C denote the classifier trained on the selected features, our feature selection problem is constrained by empirical risk (ER) and group fairness measurement (GFM):\n$\\min_{i=1,...,T} E[Y, C(F^*)] + \\lambda ||F||_0, s.t. GFM(F) \\leq \\epsilon,$\nwhere the first term amounts for classification errors and the $l_0$-norm enforces the number of features in F to be minimized. The second GFM constraint can be implemented with demographic parity (DP) [46], equalized odds (\u0395\u039f) [14], [47], or other fairness metrics based on the domain requirements. In this paper, we employ EO, defined:\n$GFM(F) = \\max_{y={0,1}} {|P(C(F) = 1 | S = 0, Y = y) - P(C(F) = 1 | S = 1, Y = y)|} .$\nThe intuition behind (2) is to encourage the data points from the protected (S = 1) and reference (S = 0) groups to be predicted into the same class, with their maximum difference in opportunity controlled within a threshold \u03f5.\nB. Causal Graph\nCausal graphs are directed and acyclic (i.e., DAGs). Let G = (V,E) denote a causal DAG, where V is the set of nodes, and $V_i \u2208 V$ represents the i-th node with the feature $X_i$. A causal DAG is egocentric if it maps all connections from the perspective of a target node [48]. In this study, we deem protect feature S or label Y as the target node, generally denoted as T. Given two features $X_i$ and $X_j$, an edge $E_{ij}$ indicates the causal relationship between them, where $X_j \u2192 X_i$ means that $X_i$ is the immediate descendant of $X_j$. All features must be the descendants of T in our causal DAGs due to egocentricity. We propose to construct the edges from a Bayesian perspective, which entails a set of variable independence definitions, as follows.\nC. Bayesian Causal Relationship\nTo reason the causal relationship among features, we first define their independence and conditional independence.\nDefinition 1 (Null-Conditional Independence): For any i \u2260 j, $X_i, X_j \u2208 X$ are null-conditional independence, iif $P(X_i, X_j) = P(X_i)P(X_j)$, denoted as $X_i \u22a5 X_j$.\nDefinition 2 (Conditional Independence [49]): For any i \u2260 n \u2260 m, $X_n, X_m \u2208 X$ are conditional independence iif $P(X_n| X_m, X_i) = P(X_n|X_i)$, denoted as $X_n \u22a5 X_m|X_i$.\nThe topological position of a feature $X_i$ on G can be located by its relevancy and redundancy w.r.t. other features:\nDefinition 3 (Strong Relevance [50]): A feature $X_i$ is deemed strongly relevant to T, iif $\u2200X_j \u2286 Pow(G)$, it holds that $T \\ncong X_i \\& X_j$, where $Pow(\u00b7)$ denotes powerset.\nDefinition 4 (Redundance [51]): A feature $X_m$ is redundant to T, if there exists $X_i$ from the powerset of strongly relevant features, such that $P(X_m, X_j) \u2260 0$ and $T \u22a5 X_m|X_j$.\nDefinition 5 (Irrelevance [43]): A feature X is irrelevant, if it is not strongly relevant nor redundant to T.\nDefinition 6 (D-Separated): : If Z blocks all paths between X and Y, then X and Y are D-Separated and thus independent given Z.\nD. Technical Challenges and Our Thoughts\nThe main challenge arises from the existence of inadmissible features (a.k.a. proxy features [52]) that are not protected, but can leak or be used to reconstruct the bias information conveyed by the protected features. Examples of such inadmissible features abound, such as zipcode, which is commonly included in user profiling and model training but has been evidenced to reveal the user's ethnicity, resulting in predictions discriminating against certain races [53]. On the causal graphs generally, we can define:\nDefinition 7 (Inadmissible Feature): A feature X is inadmissible, denoted as IA, if $X_i$ is not irrelevant to S.\nThis general definition would result in a performance tradeoff between accuracy and fairness, because almost all features are null-conditional independent from others from the linear algebraic perspective when N is large. Thus, it is most likely that many features in F are relevant to label Y but are inadmissible. Pruning all such features would incur information loss, degrading classification performance of the trained model. In addition, computing the relevancy and redundancy of each incoming feature $X_i$ would require to compute the power set of the feature subset selected up to i, which entails combinatorial complexity depending on the size of F. The process is time and computationally intensive.\nTo overcome the challenge, we propose to establish causal graphs for a sensitive feature S and label Y independently. To ease notation, we denote them as the sensitive causal graph Gs and the label causal graph Gy. We cast the problem of searching for inadmissible features into finding the overlapped features between the two graphs and replace them with a set of admissible features, denoted by A. The features in A reside outside of Gs and Gy, which were originally deemed as redundant to Y. An inspiring observation is that they are also conditional independent or null-conditionally independent with the protected feature S, indicating their d-separation from S. Hence, it is possible that a feature subset in A may become redundant to S (thus do not leak bias) and relevant to Y (thus can provide classification information), after the removal of inadmissible features. As a result, locating this subset in A and replacing it with inadmissible features can be a viable solution to optimize the tradeoff between accuracy and algorithmic fairness."}, {"title": "IV. THE SFCF APPROACH", "content": "Our proposed SFCF proceeds in two steps. First, two causal graphs egocentric to S and Y are built independently using Markov blanket, to model the non-associational contribution of an arriving feature to prediction and algorithm bias. Second, we search for an admissible feature set to replace those inadmissible features causally related to classification and protected information, so as to remedy the predictive power loss incurred by removing them.\nTo realize our idea, we use Markov blanket (MB) to build two causal graphs for sensitive feature S and label Y independently. The motivation for using MB is twofold. 1) MB allows for an incremental modeling of feature correlation. In our context, where new features emerge in sequence, and therefore, most existing methods that require a complete feature space input falter. 2) MB causally d-separates an incoming feature from the target variable by retaining the strongly relevant features only, which lifts the computational overhead required by powerset search for redundant features.\nTo do that, we leverage the conditional independence tests including Fisher's z test and the G2 test [54], [55] (The details are explained in the supplementary materials). Specifically, given a target $T \u2208 {Y, S}$, for the i-th incoming feature $X_i$, we use conditional independence tests to identify three feature subsets and one corresponding relationship based on the MB for the i-th feature moment: $StrongRelevanti(T)$, $Redundanti(T)$, $Irrelevant_i(T)$, and $CORRE^{(i)}(T)$. These are derived from the (i \u2013 1)-th feature moment, incorporating any changes that occur at the i-th feature moment.\nThe first step is to see whether $X_i$ and target T satisfy the definition of null-conditional independence (Definition 1). If $X_i$ satisfies, we will add $X_i$ into $Irrelevanti(T)$; otherwise, we will add $X_i$ into the candidate features set, denoted as $CFS(T)$.\nThe second step involves the redundancy analysis phase, which is triggered by the addition of a new feature to CFS(T). This phase filters the redundant features from CFS(T). Once a new feature is added into CFS(T), we will do a loop for each feature $X_m$ in CFS(T), to see whether there exist $\u2203X_j\u2286 Pow(CFS(T) \\ X_m)$ that satisfies $P(X_m, X_j) \u2260 0$ and $T\u22a5X_m|X_j$ (Definition 4). If it exists, $X_m$ will be added to $Redundanti(T)$. Simultaneously, we record the corresponding relationship in $CORRE^{(i)}(T)$, a dictionary where the keys are $X_j$ and the values are redundant feature $X_m$. If it does not exist, we keep $X_m$ remaining in the CFS(Y). After the loop, the features in CFS(T) excluding feature in $Redundanti(T)$ are $StrongRelevanti(T)$. Here, we observe that $Strong Relevanti (T)$ and $MB(T)$ are equivalent based on Definition 3, as both are the minimal feature subset for which $T \\ncong X_i \\& X_j$ holds for any other previously arrived features or feature combinations, i.e., $X_j \\subseteq Pow(X_1,..., X_{i\u22121})$.\nB. Optimizing the Accuracy-Fairness Tradeoff\nBased on the constructed causal graphs, at round i, the inadmissible feature set $IA_i$, admissible feature set $A_i$ are:\n$IA = MB(S) \\cup Redundant_i(S)$,\n$A_i = MB_i(Y) \\cup Redundant_i(Y) \\setminus IA_i$,\nIn figure 1, the inadmissible features set is the red area, and the admissible features set is the yellow area excluding the overlap area between red and yellow.\nThe intersection $MI_i$ between $IA_i$ from $MB_i(Y)$ is:\n$MI_i = MB_i(Y) \u222a IA_i$,\nConsidering both accuracy and fairness, the initially selected feature set is $MB_i(Y)$ removed intersection $MI_i$:\n$RI_i = MB_i(Y) \\setminus MI_i$,\nThe motivation behind equation 6 is that we can remove all inadmissible features from $MB_i(Y)$, so that the selected features do not contain any sensitive information at all, but only contain accurate information.\nIf $MI_i$ is \u00d8, it is an ideal situation. However, according to our observation, for most situations, the $MI_i$ is not \u00d8. If $MI_i$ is not \u00d8, although removing it from $MB_i(Y)$ can remove the sensitive information from selected features, it causes the accuracy information loss. In the figure 1, the $MI_i$ contains feature X11.\nTo compensate for the accuracy information loss caused by removing the $MI_i$ from $MB_i(Y)$, we can select partial features from $Redundant_i(Y)$ to replace $MI_i$. Since some features are redundant given $MI_i$ with target Y, if we remove $MI_i$, they will be strong relevant with target Y. For each feature in $MI_i$, we can use the corresponding relationship $CORRE^{(i)} (Y)$ to get the corresponding redundant features $ICRF(Y)$ from $Redundant_i(Y)$ and put them into ISF.\nBecause $Redundant_i(Y)$ includes both admissible and inadmissible features, for the features $AD1_i$ of $ICRF(Y)$ belonging to the admissible set, incorporating $AD1_i$ into ISF will enhance accuracy without compromising fairness. For example, in figure 1, $AD1_i$ includes feature X10.\n$AD1 = ICRF_i(Y) \u2229 A_i$,\nFinally, for $AD1_i$, the final selected feature set is $F^*$: $F^* = RI_i \u222a AD2_i = MB_i(Y) \\setminus MI_i \u222a AD1_i$,\nHowever, for the other features $AD2_i$ of $ICRF(Y)$ in inadmissible features, considering $MI_i$ belongs to either $MB(S)$ or $Redundant_i(S)$, $AD2_i$, being a corresponding redundant feature linked to $MI_i$, must be situated within $Redundant_i(S)$. For example, in figure 1, $AD2_i$ includes feature X7. Our experiments indicate that $AD2_i$ can increase the accuracy of information while the induced adverse impact on fairness can be accepted.\n$AD2 = ICRF_i(Y) \u2229 Redundant_i(S)$,\nFinally, for $AD2_i$, the final selected feature set is $F^*$: $F^* = RI_i \u222a AD2_i = MB_i(Y) \\setminus MI_i \u222a AD2_i$,\nC. Time Complexity Analysis\nOur algorithms' main time consumption can be attributed to two key steps. Due to page limits, a detailed analysis of these steps is provided in the \u2018C. Time complexity Analysis\u2019 section under \u2018II. ALGORITHM\u2019 in the Supplementary."}, {"title": "V. EXPERIMENTS", "content": "A. Experiment Setup\nIn this work, we use five benchmark datasets, frequently employed in fairness literature, to robustly assess SFCF's effectiveness across diverse applications including income, credit card, and crime domains. For the competitors, we have selected six distinct methods as competitors, including Baseline(use all features, without features selection), Remove Sensitive, Kamiran-massaging/Kamiran-reweighting, Capuchin, OSFS [12], [25], [31]. from various perspectives. For the evaluation protocol, we leverage the Accuracy (ACC) and Equalized Odds (EO) to benchmark the experiments. The detailed introduction of datasets, competitors, and evaluation protocol was deferred into Section 'A. Datasets', 'B. Competitors', and 'C. Evaluation Protocol' under \u2018III. EXPERIMENTS' in the Supplementary due to the page limits.\nB. Experimental Results\nQ1: Do our algorithms outperform other methods?\nThree key observations can be drawn from Tables I, III, and II to answer this question.\nFirst, our SFCF-AD1 algorithm does very well in the online streaming features scenario, giving the highest average EO score of 0.103, while keeping a competitive average accuracy of 0.748. Our algorithms, SFCF-RI, SFCF-AD1, and SFCF-AD2, are better than the Baseline algorithm by ratios of 39.82%, 53.45%, and 47.16% in EO score, respectively, while only having a small decrease in accuracy ratios of 5.80%, 5.55%, and 4.92%.\nSecond, compared with the competitors like Kamiran-massaging, Kamiran-reweighting, Capuchin, and FairExp, which use different mechanisms to address fairness via features or label processing, Our SFCF-AD1 beats all of them. Our SFCF-AD1 is better than them in the EO score by average ratios of 59.0%, 8.8%, 58.32%, and 21.39%, respectively. This confirms that conventional offline methods do sub-optimally in the online streaming feature scenario. Compared to OSFS, which has the same scenario with our algorithms, our algorithms show a significant decrease in EO score, dropping by ratios of 29.83%, 46.21%, and 37.17%, while keeping a near-steady accuracy, which only dropped by ratios of 4.72%, 4.44%, and 3.75%, respectively.\nLast, as illustrated in Tables III and II, our algorithms select the minimal average proportion of features (14.17%). Compared to the Baseline, Kamiran-massaging, Kamiran-reweighting, Capuchin, and FairExp, our algorithms significantly reduce the proportion of the selected features by an average ratio of 85.83%.\nIt's worth noting that this selective use of features may result in some information loss, explaining the slight decrease in our algorithms' accuracy. However, this trade-off is acceptable given the significant reductions in EO score. Considering the aforementioned observations, we can state that our algorithm outperforms other methods.\nQ2: How efficient do our algorithms excel in the context of streaming features?\nWe can address this question through two observations. First, we compare OSFS, an online streaming feature selection method, to our proposed algorithmss: SFCF-AD1, SFCF-AD2, and SFCF-RI. In terms of EO score, our algorithms demonstrate an average decrease in the ratios of 29.38%, 46.20%, and 37.17% respectively, while the accuracy maintain stable, decreasing on average by the ratios of 4.72%, 4.44%, and 3.97%. These results emphasize our algorithm's versatility in the online streaming feature scenario, striking a balance between accuracy and EO score. Second, our algorithms show minimum time consumption, average 2.394 seconds. The average times reducing the time by ratios of 53.21%, 46.17%, and 43.90% compared to the Baseline's average time. For offline methods, in terms of time consumption, our algorithms beat all of them. The time for SFCF-RI is only 5.01%, 3.65%, 2.09%, and 0.01% of Kamiran-massaging, Kamiran-reweighting, Capuchin, and FairExp respectively. Our algorithms are online and use incremental computation, meaning that when a new feature arrives, calculations are only done with the previously selected features, avoiding unnecessary computations. In contrast, offline methods need to compute each new feature with all the previously arrived features, involving lots of redundant calculations and wasting resources.\nQ3: How to optimize the tradeoff between accuracy-centric and fair-centric online feature selection?\nIf we remove only the sensitive feature, it means that we want to retain more accurate information, and the selected features are accuracy-centric. If we remove all the features in Gs, it means that we aim to achieve greater fairness, and the selected features are fair-centric.\nFirst, for the accuracy-centric, we can answer the question by comparing 'Remove S' with Baseline and our algorithms. When compared to the Baseline, the accuracy of 'Remove S' remains almost unchanged, with an average difference of just 0.88%. However, the EO score shows an average decrease of 32.16% across all five datasets. Notably, the decrease in EO score on the Credit Card dataset is a mere 4.06%, whereas, on the other four datasets, the reductions are more considerable at 17.39%, 26.12%, 40.90%, and 69.68%, respectively.\nNow, comparing 'Remove S' with our algorithms, SFCF-RI, SFCF-AD1, and SFCF-AD2, our algorithms perform better, with EO score reductions by the ratios of 9.93%, 31.39%, and 19.86%, respectively. These findings align with our expectations that removing only sensitive features can not tackle the fairness problem thoroughly, since other features carry or contain sensitive information.\nSecond, for the fair-centric, we can answer it by comparing SFCF-RI with the 7 competitors. SFCF-RI is a method that eliminates all features in Gs. Here, SFCF-RI achieved a winning ratio of 57.14% and a losing ratio of 2.86% on the EO score when evaluated in a win/tie/loss format. In contrast, its performance on accuracy was less impressive, with a win ratio of only 14.28% and a higher loss ratio of 28.56%. This suggests that while removing all features in Gs can considerably improve the EO score, it doesn't fare as well in maintaining accuracy. This is due to the fact that SFCF-RI eliminates all features in Gs, some of which may contain label information beneficial for accuracy.\nQ4: How well can the admissible features remedy the information loss incurred by the removal of features in Gs?\nFirst, comparing SFCF-AD1 and SFCF-AD2 with online and offline methods provide insight to answer this question. When comparing with online methods SFCF-RI, we observe that across all five datasets, SFCF-AD1 and SFCF-AD2 display a substantial decrease in EO score, reducing it by ratios of 23.82% and 11.03% respectively, while maintaining a stable average accuracy with a relative fluctuation of within 1%. This suggests that AD1 and AD2 contain some information of intersection, can remedy the information loss caused by removing the intersection.\nSecond, compared to the offline method Baseline, our algorithms decrease the accuracy by an average of 5.50% and 4.82% but significantly reduced the EO score by an average of 53.45% and 45.64%. Against offline methods Capuchin and FairExp, our methods outperform in both EO and accuracy, improving them by 2.57%, 11.07%, and 58.32%, 21.39%, respectively. In comparison to offline methods of Kamiran-massaging and Kamiran-reweighting, SFCF-AD1 shows superior performance in terms of EO score, averaging a decrease by ratios of 59.05% and 8.80%, respectively. In terms of accuracy, Kamiran-massaging and Kamiran-reweighting outperform SFCF-AD1, with higher ratios of 3.35% and 2.29%, respectively. Among these competitors, Kamiran-reweighting is closest to our performance, but it utilizes 100% of the features, our algorithms, SFCF-RI, SFCF-AD1, and SFCF-AD2, use only 14.17%, 25.56%, and 45.97% of the features respectively. This indicates that our algorithms achieve comparable effectiveness while using fewer features. Hence, the information loss incurred by the removal of features in graph Gs can be remedied by the admissible features set."}, {"title": "VI. CONCLUSION", "content": "Our research presents SFCF, an innovative method addressing the challenges of online feature selection in streaming data while ensuring fairness and maintaining predictive accuracy. By dynamically exploring the causal structure from incoming features in real-time, SFCF effectively captures the complex relationships among protected features, admissible features, and labels using the Markov blanket model, thereby mitigating algorithmic bias. Leveraging d-separation, we accurately gauge feature relevance and redundancy, enhancing fairness and accuracy. Our experiments show that SFCF outperforms state-of-the-art baselines across various metrics on benchmark datasets."}]}