{"title": "Fairness-Aware Streaming Feature Selection with Causal Graphs", "authors": ["Leizhen Zhang", "Lusi Li", "Di Wu", "Sheng Chen", "Yi He"], "abstract": "This paper proposes a new online feature selection approach with an awareness of group fairness. Its crux lies in the optimization of a tradeoff between accuracy and fairness of resultant models on the selected feature subset. The technical challenge of our setting is twofold: 1) streaming feature inputs, such that an informative feature may become obsolete or redundant for prediction if its information has been covered by other similar features that arrived prior to it, and 2) non-associational feature correlation, such that bias may be leaked from those seemingly admissible, non-protected features. To overcome this, we propose Streaming Feature Selection with Causal Fairness (SFCF) that builds two causal graphs egocentric to prediction label and protected feature, respectively, striving to model the complex correlation structure among streaming features, labels, and protected information. As such, bias can be eradicated from predictive modeling by removing those features being causally correlated with the protected feature yet independent to the labels. We theorize that the originally redundant features for prediction can later become admissible, when the learning accuracy is compromised by the large number of removed features (non-protected but can be used to reconstruct bias information). We benchmark SFCF on five datasets widely used in streaming feature research, and the results substantiate its performance superiority over six rival models in terms of efficiency and sparsity of feature selection and equalized odds of the resultant predictive models.", "sections": [{"title": "I. INTRODUCTION", "content": "Streaming data deluges in online applications [1]\u2013[3], of which the huge volume tends to prohibit real-time decision-making that merely leverages manpower. Decisions aided by computing models are hence increasingly becoming a norm, which are about job or loan allocation [4]\u2013[6], criminal recidivism [7], fraud detection [8], online advertising and recommendation [9], to name a few. However, such algorithmic decisions may unfairly discriminate against certain social groups that are constructed upon protected user char-acteristics, such as gender, age, and ethnicity [10].\nThe pursuit of mitigating algorithmic biases originates from the research conducted by Friedman and Nissenbaum in 1996 [11] and has thrived within a decade [12]\u2013[21], mainly due to the recent popularity of data-driven predic-tion and classification models [6]. In those models, bias exists in the form of spurious (or superficial) statistical correlation between feature vectors and target labels, which represent users and decisions, respectively. Thus, the key idea shared by existing studies lies in the prevention of spurious correlations in training data from influencing the resultant model. Three main research thrusts stem from this idea [20]: 1) pre-processing methods that directly modify statistical distributions of training data [20], [22], 2) in-processing methods that impose fairness-based constraints on the objec-tive function to regularize the training procedure [23], [24], and 3) post-processing methods that scrutinize predictions and finetune the predicted label distributions to enforce outcome fairness with respect to prediction accuracy [25], [26].\nDespite progresses, these thrusts mostly overlook one prominent stage in the lifecycle of data-driven applications, i.e., data generation. Too often in data-driven businesses, practitioners can source and inquiry hundreds even thousands of features for better user understanding and more precise user profiling [27]. To wit, to detect fraudulent activities in online transactions, various features describing user behav-iors are procured, such as the max/min/mean/top-k transac-tions over the last 10 seconds, 3/24/48 hours, or 1/2/4 weeks, etc. Consider the substantial loss of financial frauds [28] and the fact that fraudsters can evolve their strategies to bypass the known indicative features [29], the banking industry urges to engineer more features and feature combinations to spotlight them. We refer to this data generation continuum as streaming features [30], where the feature set is non-exhaustive with new features constantly emerging.\nHow can algorithmic bias be eliminated from streaming features? This question remains open within the data mining community, and this study pioneers its first exploration. Its main technical challenge is twofold. On the one hand, the bias structure between features and label may not be associa-tional [31], thereby complicating the strategy of predefining a subset of protected features and then removing all new features entailing them in the streaming features. On the other hand, new features may appear in high throughput, questioning the scalability of traditional feature selection methods being aware of fairness [20], [32], [33]. These methods operate and model the bias structure within a fixed feature space and thus must be reiterated each time a new feature is introduced. They falter, if the processing time for each feature exceeds the timespan of feature generation.\nTo overcome the challenge, we propose a new algorithm, named Streaming Feature Selection with Causal Fairness (SFCF). The key idea of SFCF is to leverage a causal structure that enables non-associational modeling among pro-"}, {"title": "II. RELATED WORK", "content": "We can divide fairness-aware machine learning algorithms into three primary categories. 1) For pre-processing ap-proaches [12], [17], [19], [20], [25], [31], [35], [36], they aim to directly modify statistical distributions of training data by assigning weights to features or labels to achieve a fair representation of protected and reference groups with respect to their label assignments. For example, [25], [35] propose massaging and reweighing techniques to adjust train-ing data, ensuring predictions are independent of protected attributes. [19] proposes a CGF framework that integrates causal graph structure learning with fairness regularization to minimize unfair edge weights, ensuring fair predictions by eliminating unfair causal effects. [17] enforces statistical in-dependence between model outputs and protected attributes, using Wasserstein-1 distances to align distributions across different groups. [31] resamples training data to reduce bias, ensuring predictions independent of sensitive attributes for causally fair prediction, which could be time-consuming due to the sampling process. [12] firstly introduces the inadmissible features, which are non-protected but could leak biased information from feature reconstruction. To mitigate their impact, [12] proposes Capuchin which repairs training data by implementing interventional fairness, where both protected and inadmissible features are d-separated in the causal structure. [20] introduces a HypeR framework, which uses a conditional probabilistic causal model to handle what-if and how-to queries, reducing discrimination for fair causal predictions in databases.\n2) For in-processing, the methods [37], [38] modify learn-ing objectives or model structures to achieve fairness. For example, [37] introduces a method integrating discrimination and information gain as a new splitting criterion when constructing a Hoeffding Tree. [38] proposes techniques in-corporating regularization terms motivated by Mixed-integer programming to encourage low discrimination score. 3) For post-processing, the methods [39], [40] adjust algorithm predictions to achieve fairness. They involve altering the confidence of classification rules or re-labeling predicted classes in decision trees to prevent biased decisions against protected groups. However, all these methods cannot be directly adapted to streaming feature selection, mainly for two reasons. First, they are offline and presume all features are accessible prior to the learning process, which cannot be satisfied in our context wherein features emerge one at a time. Second, they mostly postulate prior knowledge of domain data (e.g., a known causal structure) or model architecture (e.g., trees or tree ensembles), thereby suffering from a generalization crisis when such assumptions do not hold in practice. Our proposed approach excels in the sense that we allow a sequential feature input and we do not rely on any domain knowledge nor assuming the structure of classifiers, thus is more general."}, {"title": "III. PRELIMINARIES", "content": "Let a sequence of features be $X = {X_i | i = 1,..., D} \\in R^{D\\times N}$, where $X_i$ denotes the feature that emerges at the round $i$, and $D$ signifies the length of the feature stream. The column vector $Y := {0,1}^N$ denotes the ground-truth labels. There are $N$ instances in total. We have $S \\in X$ the protected feature (e.g., gender, age, or ethnicity). The feature subset selected at round $i$ is denoted by $F^*$, $|F^*| \\leq i$.\nLet $C$ denote the classifier trained on the selected features, our feature selection problem is constrained by empirical risk (ER) and group fairness measurement (GFM):\n$\\min_{i=1,...,T}E[Y,C(F^*)]+\\lambda||F||_0, s.t. GFM(F) \\leq \\epsilon$,\nwhere the first term amounts for classification errors and the $l_0$-norm enforces the number of features in $F$ to be minimized. The second GFM constraint can be imple-mented with demographic parity (DP) [46], equalized odds (EO) [14], [47], or other fairness metrics based on the domain requirements. In this paper, we employ EO, defined:\n$GFM(F) = \\max_{y={0,1}} {|P(C(F) = 1 | S = 0, Y = y)\n-P(C(F) = 1 | S = 1, Y = y)| y)|}$.\nThe intuition behind (2) is to encourage the data points from the protected (S = 1) and reference (S = 0) groups to be predicted into the same class, with their maximum difference in opportunity controlled within a threshold $\\epsilon$.\nCausal graphs are directed and acyclic (i.e., DAGs). Let $G = (V,E)$ denote a causal DAG, where $V$ is the set of nodes, and $V_i \\in V$ represents the i-th node with the feature $X_i$. A causal DAG is egocentric if it maps all connections from the perspective of a target node [48]. In this study, we deem protect feature $S$ or label $Y$ as the target node, generally denoted as $T$. Given two features $X_i$ and $X_j$, an edge $E_{ij}$ indicates the causal relationship between them, where $X_j \\rightarrow X_i$ means that $X_i$ is the immediate descendant of $X_j$. All features must be the descendants of $T$ in our causal DAGs due to egocentricity. We propose to construct the edges from a Bayesian perspective, which entails a set of variable independence definitions, as follows."}, {"title": "C. Bayesian Causal Relationship", "content": "To reason the causal relationship among features, we first define their independence and conditional independence.\nDefinition 1 (Null-Conditional Independence): For any $i \\neq j$, $X_i, X_j \\in X$ are null-conditional independence, iif $P(X_iX_j) = P(X_i)P(X_j)$, denoted as $X_i\\bot X_j$.\nDefinition 2 (Conditional Independence [49]): For any $i \\neq n\\neq m$, $X_n, X_m \\in X$ are conditional independence iif $P(X_n X_m,X_i) = P(X_n|X_i)$, denoted as $X_n\\bot X_m|X_i$.\nThe topological position of a feature $X_i$ on $G$ can be located by its relevancy and redundancy w.r.t. other features:\nDefinition 3 (Strong Relevance [50]): A feature $X_i$ is deemed strongly relevant to $T$, iif $\\forall X_j \\subseteq Pow(G)$, it holds that $T \\not \\bot X_i|X_j$, where $Pow(\\cdot)$ denotes powerset.\nDefinition 4 (Redundance [51]): A feature $X_m$ is redun-dant to $T$, if there exists $X_i$ from the powerset of strongly relevant features, such that $P(X_m, X_j) \\neq 0$ and $T\\bot X_m|X_j$.\nDefinition 5 (Irrelevance [43]): A feature $X$ is irrele-vant, if it is not strongly relevant nor redundant to $T$.\nDefinition 6 (D-Separated): : If $Z$ blocks all paths be-tween $X$ and $Y$, then $X$ and $Y$ are D-Separated and thus independent given $Z$.\nThe main challenge arises from the existence of inad-missible features (a.k.a. proxy features [52]) that are not protected, but can leak or be used to reconstruct the bias information conveyed by the protected features. Examples of such inadmissible features abound, such as zipcode, which is commonly included in user profiling and model training but has been evidenced to reveal the user's ethnicity, resulting in predictions discriminating against certain races [53]. On the causal graphs generally, we can define:\nDefinition 7 (Inadmissible Feature): A feature $X_i$ is in-admissible, denoted as $IA_i$, if $X_i$ is not irrelevant to $S$.\nThis general definition would result in a performance tradeoff between accuracy and fairness, because almost all features are null-conditional independent from others from the linear algebraic perspective when $N$ is large. Thus, it is most likely that many features in $F_t$ are relevant to label $Y$ but are inadmissible. Pruning all such features would incur information loss, degrading classification performance of the trained model. In addition, computing the relevancy and redundancy of each incoming feature $X_i$ would require to compute the power set of the feature subset selected up to $i$, which entails combinatorial complexity depending on the size of $F$. The process is time and computationally intensive.\nTo overcome the challenge, we propose to establish causal graphs for a sensitive feature $S$ and label $Y$ independently. To ease notation, we denote them as the sensitive causal graph $G_s$ and the label causal graph $G_y$. We cast the prob-lem of searching for inadmissible features into finding the"}, {"title": "IV. THE SFCF APPROACH", "content": "Our proposed SFCF proceeds in two steps. First, two causal graphs egocentric to S and Y are built independently using Markov blanket, to model the non-associational contri-bution of an arriving feature to prediction and algorithm bias. Second, we search for an admissible feature set to replace those inadmissible features causally related to classification and protected information, so as to remedy the predictive power loss incurred by removing them.\nTo realize our idea, we use Markov blanket (MB) to build two causal graphs for sensitive feature S and label Y independently. The motivation for using MB is two-fold. 1) MB allows for an incremental modeling of feature correlation. In our context, where new features emerge in sequence, and therefore, most existing methods that require a complete feature space input falter. 2) MB causally d-separates an incoming feature from the target variable by retaining the strongly relevant features only, which lifts the computational overhead required by powerset search for redundant features.\nTo do that, we leverage the conditional independence tests including Fisher's z test and the $G^2$ test [54], [55] (The details are explained in the supplementary materi-als). Specifically, given a target $T \\in {Y, S}$, for the i-th incoming feature $X_i$, we use conditional independence tests to identify three feature subsets and one corresponding relationship based on the MB for the i-th feature moment: $StrongRelevanti(T)$, $Redundanti(T)$, $Irrelevant\u017c(T)$, and $CORRE(i)(T)$. These are derived from the (i \u2013 1)-th feature moment, incorporating any changes that occur at the i-th feature moment.\nThe first step is to see whether $X_i$ and target $T$ satisfy the definition of null-conditional independence (Definition 1). If $X_i$ satisfies, we will add $X_i$ into $Irrelevanti(T)$; otherwise, we will add $X_i$ into the candidate features set, denoted as $CFS(T)$.\nThe second step involves the redundancy analysis phase, which is triggered by the addition of a new feature to $CFS(T)$. This phase filters the redundant features from $CFS(T)$. Once a new feature is added into $CFS(T)$, we will do a loop for each feature $X_m$ in $CFS(T)$, to see whether there exist $\\exists X_j\\subseteq Pow(CFS(T) \\backslash X_m)$ that satisfies $P(X_m, X_j) \\neq 0$ and $T\\bot X_m|X_j$ (Definition 4). If it exists, $X_m$ will be added to $Redundanti(T)$. Simultaneously, we record the corresponding relationship in $CORRE (i)(T)$, a dictionary where the keys are $X_j$ and the values are redundant feature $X_m$. If it does not exist, we keep $X_m$ remaining in the $CFS(Y)$. After the loop, the features in CFS(T) excluding feature in $Redundanti(T)$ are $StrongRelevanti(T)$. Here, we observe that $Strong Relevanti (T)$ and $MB(T)$ are equivalent based on Definition 3, as both are the minimal feature subset for which $T \\not \\bot X_i|X_j$ holds for any other previously arrived features or feature combinations, i.e., $X_j\\subseteq Pow(X_1,..., X_{i-1})$."}, {"title": "B. Optimizing the Accuracy-Fairness Tradeoff", "content": "Based on the constructed causal graphs, at round $i$, the inadmissible feature set $IA_i$, admissible feature set $A_i$ are:\n$IA = MB(S) \\cup Redundant_i(S)$.\n$A_i = MB_i(Y) \\cup Redundant_i(Y) \\backslash IA_i$.\nIn figure 1, the inadmissible features set is the red area, and the admissible features set is the yellow area excluding the overlap area between red and yellow.\nThe intersection $MI_i$ between $IA_i$ from $MB_i(Y)$ is:\n$MI_i = MB_i(Y) \\cup IA_i$\nConsidering both accuracy and fairness, the initially se-lected feature set is $MB_i(Y)$ removed intersection $MI_i$:\n$RI_i = MB_i(Y) \\backslash MI_i$\nThe motivation behind equation 6 is that we can remove all inadmissible features from $MB_i(Y)$, so that the selected features do not contain any sensitive information at all, but only contain accurate information.\nIf $MI_i$ is $\\O$, it is an ideal situation. However, according to our observation, for most situations, the $MI_i$ is not $\\O$. If $MI_i$ is not $\\O$, although removing it from $MB_i(Y)$ can remove the sensitive information from selected features, it causes the accuracy information loss. In the figure 1, the $MI_i$ contains feature $X_{11}$.\nTo compensate for the accuracy information loss caused by removing the $MI_i$ from $MB_i(Y)$, we can select partial features from $Redundant_i(Y)$ to replace $MI_i$. Since some"}, {"title": "V. EXPERIMENTS", "content": "In this work, we use five benchmark datasets, frequently employed in fairness literature, to robustly assess SFCF's effectiveness across diverse applications including income, credit card, and crime domains. For the competitors, we have selected six distinct methods as competitors, including Baseline(use all features, without features selection), Re-move Sensitive, Kamiran-massaging/Kamiran-reweighting, Capuchin, OSFS [12], [25], [31]. from various perspectives. For the evaluation protocol, we leverage the Accuracy (ACC) and Equalized Odds (EO) to benchmark the experiments. The detailed introduction of datasets, competitors, and eval-uation protocol was deferred into Section 'A. Datasets', 'B. Competitors', and 'C. Evaluation Protocol' under 'III. EXPERIMENTS' in the Supplementary due to the page limits."}, {"title": "Q1: Do our algorithms outperform other methods?", "content": "Three key observations can be drawn from Tables I, III, and II to answer this question.\nFirst, our SFCF-AD1 algorithm does very well in the online streaming features scenario, giving the highest average EO score of 0.103, while keeping a competitive average ac-curacy of 0.748. Our algorithms, SFCF-RI, SFCF-AD1, and SFCF-AD2, are better than the Baseline algorithm by ratios of 39.82%, 53.45%, and 47.16% in EO score, respectively, while only having a small decrease in accuracy ratios of 5.80%, 5.55%, and 4.92%.\nSecond, compared with the competitors like Kamiran-massaging, Kamiran-reweighting, Capuchin, and FairExp, which use different mechanisms to address fairness via features or label processing, Our SFCF-AD1 beats all of them. Our SFCF-AD1 is better than them in the EO score by average ratios of 59.0%, 8.8%, 58.32%, and 21.39%, respectively. This confirms that conventional offline methods do sub-optimally in the online streaming feature scenario. Compared to OSFS, which has the same scenario with our algorithms, our algorithms show a significant decrease in EO score, dropping by ratios of 29.83%, 46.21%, and 37.17%, while keeping a near-steady accuracy, which only dropped by ratios of 4.72%, 4.44%, and 3.75%, respectively.\nLast, as illustrated in Tables III and II, our algorithms select the minimal average proportion of features (14.17%). Compared to the Baseline, Kamiran-massaging, Kamiran-reweighting, Capuchin, and FairExp, our algorithms signif-"}, {"title": "Q2: How efficient do our algorithms excel in the context of streaming features?", "content": "We can address this question through two observations.\nFirst, we compare OSFS, an online streaming feature se-lection method, to our proposed algorithmss: SFCF-AD1, SFCF-AD2, and SFCF-RI. In terms of EO score, our al-gorithms demonstrate an average decrease in the ratios of 29.38%, 46.20%, and 37.17% respectively, while the accuracy maintain stable, decreasing on average by the ratios of 4.72%, 4.44%, and 3.97%. These results emphasize our algorithm's versatility in the online streaming feature scenario, striking a balance between accuracy and EO score.\nSecond, our algorithms show minimum time consumption, average 2.394 seconds. The average times reducing the time by ratios of 53.21%, 46.17%, and 43.90% compared to the Baseline's average time. For offline methods, in terms of time consumption, our algorithms beat all of them. The time for SFCF-RI is only 5.01%, 3.65%, 2.09%, and 0.01% of Kamiran-massaging, Kamiran-reweighting, Capuchin, and FairExp respectively. Our algorithms are online and use incremental computation, meaning that when a new fea-ture arrives, calculations are only done with the previously selected features, avoiding unnecessary computations. In contrast, offline methods need to compute each new feature with all the previously arrived features, involving lots of redundant calculations and wasting resources."}, {"title": "Q3: How to optimize the tradeoff between accuracy-centric and fair-centric online feature selection?", "content": "If we remove only the sensitive feature, it means that we want to retain more accurate information, and the selected features are accuracy-centric. If we remove all the features in Gs, it means that we aim to achieve greater fairness, and the selected features are fair-centric.\nFirst, for the accuracy-centric, we can answer the question by comparing 'Remove S' with Baseline and our algorithms. When compared to the Baseline, the accuracy of 'Remove S' remains almost unchanged, with an average difference of just 0.88%. However, the EO score shows an average decrease of 32.16% across all five datasets. Notably, the decrease in EO score on the Credit Card dataset is a mere 4.06%, whereas, on the other four datasets, the reductions are more consider-able at 17.39%, 26.12%, 40.90%, and 69.68%, respectively.\nNow, comparing 'Remove S' with our algorithms, SFCF-RI, SFCF-AD1, and SFCF-AD2, our algorithms perform better, with EO score reductions by the ratios of 9.93%, 31.39%, and 19.86%, respectively. These findings align with our expectations that removing only sensitive features can not tackle the fairness problem thoroughly, since other features carry or contain sensitive information."}, {"title": "Q4: How well can the admissible features remedy the information loss incurred by the removal of features in Gs?", "content": "First, comparing SFCF-AD1 and SFCF-AD2 with online and offline methods provide insight to answer this question. When comparing with online methods SFCF-RI, we observe that across all five datasets, SFCF-AD1 and SFCF-AD2 display a substantial decrease in EO score, reducing it by ratios of 23.82% and 11.03% respectively, while maintaining a stable average accuracy with a relative fluctuation of within 1%. This suggests that AD1 and AD2 contain some information of intersection, can remedy the information loss caused by removing the intersection.\nSecond, compared to the offline method Baseline, our algorithms decrease the accuracy by an average of 5.50% and 4.82% but significantly reduced the EO score by an average of 53.45% and 45.64%. Against offline methods Capuchin and FairExp, our methods outperform in both EO and accuracy, improving them by 2.57%, 11.07%, and 58.32%, 21.39%, respectively. In comparison to offline methods of Kamiran-massaging and Kamiran-reweighting, SFCF-AD1 shows superior performance in terms of EO score, averaging a decrease by ratios of 59.05% and 8.80%, respectively. In terms of accuracy, Kamiran-massaging and Kamiran-reweighting outperform SFCF-AD1, with higher ratios of 3.35% and 2.29%, respectively. Among these competitors, Kamiran-reweighting is closest to our performance, but it utilizes 100% of the features, our algorithms, SFCF-RI, SFCF-AD1, and SFCF-AD2, use only 14.17%, 25.56%, and 45.97% of the features respectively. This indicates that our algorithms achieve comparable effectiveness while using fewer features. Hence, the information loss incurred by the removal of features in graph Gs can be remedied by the admissible features set."}, {"title": "VI. CONCLUSION", "content": "Our research presents SFCF, an innovative method ad-dressing the challenges of online feature selection in stream-ing data while ensuring fairness and maintaining predictive accuracy. By dynamically exploring the causal structure from incoming features in real-time, SFCF effectively captures the complex relationships among protected features, admissible features, and labels using the Markov blanket model, thereby mitigating algorithmic bias. Leveraging d-separation, we ac-curately gauge feature relevance and redundancy, enhancing fairness and accuracy. Our experiments show that SFCF outperforms state-of-the-art baselines across various metrics on benchmark datasets."}]}