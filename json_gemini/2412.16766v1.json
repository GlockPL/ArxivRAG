{"title": "Apples to Apples: Establishing Comparability in Knowledge Generation Tasks Involving Users", "authors": ["Ademar Crotti Junior", "Christophe Debruyne"], "abstract": "Knowledge graph construction (KGC) from (semi-)structured data is challenging, and facilitating user involvement is an issue frequently brought up within this community. We cannot deny the progress we have made with respect to (declarative) knowledge generation languages and tools to help build such mappings. However, it is surprising that no two studies report on similar protocols. This heterogeneity does not allow for a comparison of KGC languages, techniques, and tools. This paper first analyses the various studies that report on studies involving users to identify the points of comparison. These gaps include a lack of systematic consistency in task design, participant selection, and evaluation metrics. Moreover, there needs to be a systematic way of analyzing the data and reporting the findings, which is also lacking. We thus propose and introduce a user protocol for KGC designed to address this challenge. Where possible, we draw and take elements from the literature we deem fit for such a protocol. The protocol, as such, allows for the comparison of languages and techniques for the RDF Mapping Languages core functionality, which is covered by most of the other state-of-the-art techniques and tools. We also propose how the protocol can be amended to compare extensions (of RML). This protocol provides an important step towards a more comparable evaluation of KGC user studies.", "sections": [{"title": "1 Introduction", "content": "We preach to the choir that knowledge graphs are essential for meaningfully organizing and representing information in various domains. However, as knowledge graphs grow in complexity, efficient methods for their generation are crucial. When dealing with the challenges of (semi-)structured data sources, such as the"}, {"title": "2 Related Work", "content": "In [3], the authors presented an excellent survey on declarative KGC tools to help the community and practitioners choose which languages, tools, or techniques fit their needs. However, the article looks at those from a technical perspective. They look at the functionalities offered by the different options. In [2], the authors proposed a benchmark to compare KGC tools and applied it to some well-known implementations such as RMLMapper [11], Morph-KGC [1], and SDM-RDFizer [20]. It is surprising to see that the perceptions of users and practitioners have yet to be examined in a systematic manner.\nFrom a broader perspective, [23] describes three \"personas\" that engage with KGs: KG builders, KG analysts, and KG consumers, which were distilled from interviews with practitioners. As the name intuitively implies, the KG builder persona would be responsible for generating the KG from heterogeneous sources, but the persona is also in charge of ontology engineering. The authors state that builders could benefit from tools that help them ensure that the schema is respected (what the authors call an \"enforcer\") as well as adequate visualization tools. While the paper does not explicitly mention KG generation and mappings as tasks, they fall under the \"data integration\" umbrella. Key is here is that the interviews indicate that there are challenges impeding uptake.\nIt seems that the role of practitioners, or users, is at times neglected. This is certainly the case for KG Generation, as we will now demonstrate via our literature review. Our review looked at the following papers reporting on users, their experiences, and/or perceived usability: [25,17,26,4,9,16,8,7,13,28,6]. Most of these studies looked at the creation of mappings. Exceptions are [7] reporting on studies on mapping understanding and [6] reporting on a complex data flow that included mapping creation.\u00b9 We compare the various aspects of these user studies in Tables 1, 2, and 3. From these tables, we can observe a couple of important points:\nSome report on comparing mapping languages and/and tools (e.g., [16] and [8]), and others report on comparing mapping languages (e.g., [13] and [28]). Quite a few papers merely report on the perceived usability of their tool without any comparison. We argue that reporting on user studies only makes sense if there is a basis for comparison.\nLooking at the procedure, we see many recurring elements (some (training) resources are being shared, pre-assessment surveys, introduction of tasks, surveys, etc.). No two procedures are the same, which hampers our ability to compare the studies. Some studies reported asking about information such as gender and age but did not report on those in the data analysis.\nMost studies involved participants with expertise in it, databases and/or semantic web technologies. Many studies also report inviting MSc students in computer science or related fields. Self-reported prior knowledge and competencies are a recurring theme, but no two studies tackle this aspect comparably.\nThe same heterogeneity can be observed for the tasks and datasets, where we do notice that most studies adopt datasets that do not require specific domain expertise (people, movies, places, etc.)."}, {"title": "3 The KGC User Study Protocol", "content": "This section presents the protocol for comparing KGC tools and languages. The protocol's\u00b2 structure foresees placeholders for text to be easily adapted for research ethics applications.\nThe protocol as such can be used to analyze the perceived usability and cognitive load of a mapping language, tool, as well as the accuracy achieved by users and the task execution time. When users use this protocol on only one group, the hypotheses are limited to comparing participants with different demographics, or by comparing the results with other experiments using this protocol. Scholars adopting this protocol can easily extend the protocol to compare two tools, techniques, or languages. This will be explained in Section 3.5.\nAs we will explain in Section 3.5, there tasks can"}, {"title": "3.1 Participant Selection", "content": "Adopters of the protocol should indicate how participants are recruited and from where. Adopters should disclose potential biases by providing details about factors influencing the study or participant behavior. Examples include the hierarchical relationship between research group leaders and researchers and students recruited from classes. There is also a difference between voluntary participation and mandatory representation (e.g., in the context of a teaching activity).\nThere is an unwritten rule that states that 5 participants are sufficient to get an idea of a system's perceived usability or usefulness, but, as [12] observed in an experiment, \"increasing the number from 5 to 10 can result in a dramatic improvement in data confidence.\" They also found that increasing the number to"}, {"title": "3.2 Process", "content": "Participants begin by reviewing informed consent materials and completing a pre-questionnaire assessing their demographics, prior knowledge, and expectations. Next, they attend a presentation introducing the technology, review relevant documentation, and engage in a familiarization activity. The core of the study involves participants executing a defined task using the technology. Finally, participants complete a post-questionnaire evaluating their experience, including usability, efficiency, and perceived cognitive load. This structured process aims to gather comprehensive data on user interaction and perception of the technology.\nNext to presenting and demonstrating the tool or mapping language, we also request participants to handle the environment. We deem this familiarization activity novel compared to the related work. This activity ensures participants are comfortable executing mappings within the provided (tool's) environment. We guide participants in demonstrating the practical aspects of using the tool's interface, such as utilizing the command line in the terminal or identifying the correct buttons to click. This focused familiarization will prevent the environment from becoming an obstacle, allowing us to assess the tool or language's usability and gather unbiased feedback on its functionalities.\nFurthermore, we ask authors to report on how responses were submitted (e.g., email, paper, form) and the anticipated duration of the experiment. Note that while in-class experiments often have time constraints, other environments may be more flexible. Our protocol foresees 1 hour for the five tasks. If, for example, all steps are conducted in a classroom setting, the experiment would require 2:30. Finally, clarify whether participants will be allowed to ask questions during the experiment and help should be limited to aspects not core to the KG generation process and experiment. For example, helping participants navigate to the correct directory in a terminal or assessing whether there is a network issue is permitted, but providing help on how to execute a mapping is not. Ideally, studies would report on those (and their number of occurrences)."}, {"title": "3.3 Pre-questionnaire", "content": "Studies often inquire about participants and group them based on self-reported information on their background and proficiency with specific techniques. We aim to homogenize this by proposing an exhaustive list of current roles, formal training, and self-perceived competency levels in certain Semantic Web technologies. We also included three questions related to intrinsic motivation (enjoyment, curiosity, and value). It is important to analyze the impact of self-selection bias in voluntary participation."}, {"title": "3.4 Mapping tasks", "content": "Participants will be requested to complete five mapping tasks, some of which are interdependent. In our state-of-the-art surveys, many different types (e.g., mapping understanding vs. mapping creation) are applied in many domains. We propose choosing a general domain for participants to understand: projects, project tasks, and employees who manage projects and are assigned tasks. The rationale is that there are three distinct types, and users should, at this point, not struggle with ontological questions such as roles vs. types.\nFigure 1 depicts the UoD of the data to be transformed into RDF. We use an ERD, but JSON and XML files can easily represent the data. To ensure attribute names do not mislead participants, we ensured all attributes are unambiguous. In this simple UoD, all relations are many-to-one, though this can be easily extended to many-to-many when transforming documents.\nThe tasks can be summarized as follows:\n1. Generate instances of ex: Employee with their first and last-names. The IRIS of employees are based on the name.\n2. Generate instances of ex: Project with their name, start- and end-date. Both dates are of the type xsd:date, allowing us to assess the creation of typed literals. The IRIs of projects are based on the project's ID.\n3. Generate ex: managedBy properties from projects to employees.\n4. Generate instances of ex: Task with their descriptions (in two languages). The IRIs of tasks are based on the task's ID. The descriptions allow us to assess the creation of language tags.\n5. Generate ex:of and ex:assignedTo properties from tasks to, respectively, projects and employees.\nWe point out that the mappings mainly focus on RML-core [21] functionality. Part of RML-core are multi-valued expression maps, which are irrelevant for CSV files. People can easily adapt the JSON files to provide one or more task descriptions in different languages to test more complex language maps, for instance.\nWe draw attention to the fact that employees' IRIs are based on their names, whereas the project data sources refer to employees via their IDs. This requires"}, {"title": "3.5 Variants", "content": "We stated that the tasks focus on RML-core functionality, which is also covered by other KGC languages and techniques such as ShExML [13] and SPARQL Anything [10]. While one can argue that the set of desired functionalities is limited, [21] covered the requirements of all RML extensions. It is not feasible to formulate tasks that cover practically all use cases, not only in time but also in data source complexity.\nIn this section, we describe how this protocol can be used for comparing different aspects.\nWhen comparing languages, techniques, or tools. One can provide the same tasks to two different groups. One can compare different mapping languages (e.g., ShExML vs. RML), compare editors vs. \"bare bones\" languages (e.g., RMLEditor vs. RML), compare languages and abstractions of languages (e.g., RML vs. YARRRML), and even different editors and languages.\nWhen the aim is to compare support for an advanced KGC requirement such as named graphs, collections and containers, or RDF* (among others), then one can take this protocol as is for one group, and only change the last two tasks for the second in which those requirements are covered, with the first three tasks giving a comparable baseline, unless the extension covered another aspect such as storing the resulting trip.\nWhen the aim is to such compare KG construction across languages, techniques, or tools, then the protocol must be amended for both. Again, the first three tasks must remain unchanged as to ensure some comparative baseline with literature. This approach can be used to compare the languages and tools for more advanced KGC requirements such as named graphs, RDF collections and containers, and function calls, among others.\nParticipants are assumed to have access to prepared \"resources\" or \"environments\" to focus on the tasks. In the case of RML, for instance, the logical sources would be provided in the tool or for them to copy and paste. This allows researchers to assess the languages and tools with respect to these aspects by giving one group the prepared artifacts and requesting the other to formulate the logical sources themselves. We deem this a special case of comparing a baseline with an extension as described above, but where the five tasks remain unchanged."}, {"title": "3.6 Post-questionnaire", "content": "Both SUS and PSSUQ are used to measure usability. SUS is adequate for a rapid and general measure of a system's usability. Still, the latter offers more advantages because it assesses three aspects of a system: system usefulness, information quality, and interface quality. Furthermore, there is a question about the system as a whole, which allows one to damped the perception of individual aspects. The original PSSUQ survey uses 19 questions (as adopted by [9], for instance), but recent iterations have removed three redundant questions.\nAs for the perceived (mental) workload, we adopt both the Workload Profile (WP) [27] and the NASA Task Load Index (NASA-TLX) [14].\nWP adopts a theory in which participants have different capacities (dimensions) related to the stage, mode, input, and output of information processing. The eight dimensions are each quantified through subjective rates, and participants must rate the proportion of attentional resources used for performing a given task with a value from 0 to 100 after task completion. A rating of 0 means that the task placed no demand, while 100 indicates that it required maximum attention. The WP of a participant is calculated as $\\frac{\\Sigma_{i=1}^{18} d_i}{18}$.\nNASA-TLX has been validated in several domains [14] and combines six factors believed to influence the mental workload. Each factor is quantified with a subjective judgment and a weight computed via a paired comparison procedure. For each possible pair of the six factors, participants must decide which factor contributed the most to the mental workload during the task. The weights $w$ are the number of times each dimension was selected. The possible weights range from 0 (irrelevant) to 5 (most important). The final score is computed as a weighted average, considering the subjective rating of each attribute $d_i$ and the correspondent weights $w_i$: $\\frac{1}{15} \\sum_{i=1}^{6} d_i * w_i$. It is possible to calculate the scores by eliminating the weighted procedure, which yields the so-called Raw TLX.\nBoth instruments are used in industry and research. You may notice that both instruments use different rating systems, which may confuse participants in a paper survey. Erroneous inputs can be prevented by adopting electronic forms. We choose not to harmonize the scales, which has been done in [9], for instance, to obtain results that can be compared to other studies faithfully adopting those instruments.\nStudies should explicitly report the method used to calculate performance measures, such as accuracy. For instance:\nAccuracy should be determined by graph isomorphism (did they generate the expected graph, which is true or false), and, for more nuanced numbers, precision (the proportion of triples that are generated that are in the expected graph), recall (the proportion of expected triples that are in the generated graph), and F-measure combining precision and recall.\nThis approach accounts for situations where a participant generates additional triples, for instance. Figures should be reported both on task and global level."}, {"title": "4 Results and Analysis", "content": "As part of our protocol, we recommend a structured approach for reporting collected data. As mentioned, while many user studies focus primarily on presenting averages and standard deviations, we emphasize the importance of extending these reports to include statistical tests. This ensures robust comparisons between groups and tools, enhancing the general reliability and interpretability of the experiments. The following describes the recommended aspects and tests to be considered when reporting results and analysis.\nReliability and Internal Consistency Reliability refers to the degree to which the items within a test or survey consistently measure the same construct. High internal consistency strengthens the statistical reliability of metrics, thereby enhancing the validity of group comparisons. To evaluate internal consistency, we recommend using Cronbach's Alpha. Higher alpha coefficients indicate greater shared covariance among items, suggesting they assess the same underlying concept. A Cronbach's Alpha value of \u2265 0.7 is generally considered acceptable. [24]\nData Normality Data normality refers to how much data distribution aligns with a normal curve. While normality is not always required, t-tests and ANOVA that assume data follows such a distribution. ANOVA is fairly robust when data is not normality distributed when sample sizes are large, but that is difficult when dealing with user studies. We, therefore, require studies to test for normality and report on normality. Participants may, if they wish, avail of other statistical measures. As the sample sizes of a group will likely not exceed 50, we propose the Shapiro-Wilk test to assess normality. In this test, the sample is compared to a theoretical normal distribution.\nHomogeneity Some statistical tests, such as ANOVA, assume that the variances across groups are equal. This is known as the homogeneity of variances. Again, this assumption should be verified as part of the analysis. Levene's Test is a standard method for evaluating this assumption.\nGroup Comparisons To determine whether differences between groups are statistically significant, researchers should employ well-established statistical tests. The choice of test depends on the assumptions about the data. The recommended tests when the data is normal (also known as parametric tests) are Welch's t-test when comparing two groups and ANOVA when comparing more than two groups simultaneously. Both tests assume normality and homogeneity of variance. The recommended tests when the data is not normal (also known as non-parametric tests) are the Wilcoxon test for comparing two groups and the Kruskal-Wallis test for multiple.\nCorrelation Analysis Correlation methods assess the strength and direction of the relationship between variables, which can provide deeper insights into study outcomes. For instance, examining correlations between usability, accuracy, and mental workload can reveal important relationships in user behavior. When data is normally distributed, we recommend the Pearson's Correlation to measure the strength of a relationship, for example, between accuracy and usability. Otherwise, one should use the Spearman's Correlation. Researchers must report on correlations between all relevant variables (e.g., usability and mental workload, usability and accuracy, etc.) to provide a comprehensive analysis.\nTransparency and Accessibility To promote transparency and reproducibility, all collected data and the statistical tests performed should be made publicly available online. Providing access to raw data, analysis scripts, and detailed methodology facilitates validation and enhances the study's credibility. Moreover, sharing such data would allow one to more easily compare results across studies, provided that the conditions are similar."}, {"title": "5 Discussion", "content": "We presented a comparison of user studies in the KGC domain, and we noticed that all studies were different. This makes it impossible to compare KGC languages, tools, and software. To this end, we analyzed the related work, distilled"}, {"title": "6 Conclusions", "content": "Prior KGC user studies used different protocols, making comparison impossible. This paper thus highlights the lack of standardized protocols in user studies related to KGC, making it difficult to compare different studies. We present a new protocol to address these inconsistencies, focusing on participant selection, task design, and evaluation metrics. The protocol suggests detailed guidelines for recruiting participants and disclosing potential biases. The process guidelines for informed consent, pre-questionnaires, familiarization activities, task execution, and post-questionnaires. Five specific mapping sub-tasks are proposed, which can be solved with the equivalent of the RML-Core specification. The protocol recommends using the Post-Study System Usability Questionnaire (PSSUQ) for"}]}