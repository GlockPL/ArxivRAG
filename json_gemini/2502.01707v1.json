{"title": "CLIP-DQA: Blindly Evaluating Dehazed Images from Global and Local Perspectives Using CLIP", "authors": ["Yirui Zeng", "Jun Fu", "Hadi Amirpour", "Huasheng Wang", "Guanghui Yue", "Hantao Liu", "Ying Chen", "Wei Zhou"], "abstract": "Blind dehazed image quality assessment (BDQA), which aims to accurately predict the visual quality of dehazed images without any reference information, is essential for the evaluation, comparison, and optimization of image dehazing algorithms. Existing learning-based BDQA methods have achieved remarkable success, while the small scale of DQA datasets limits their performance. To address this issue, in this paper, we propose to adapt Contrastive Language-Image Pre-Training (CLIP), pre-trained on large-scale image-text pairs, to the BDQA task. Specifically, inspired by the fact that the human visual system understands images based on hierarchical features, we take global and local information of the dehazed image as the input of CLIP. To accurately map the input hierarchical information of dehazed images into the quality score, we tune both the vision branch and language branch of CLIP with prompt learning. Experimental results on two authentic DQA datasets demonstrate that our proposed approach, named CLIP-DQA, achieves more accurate quality predictions over existing BDQA methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Haze is a common natural phenomenon that significantly reduces visibility in scenes, causing many computer vision algorithms, such as object detection [1], [2] and image recognition [3], to experience severe performance degradation. To alleviate this issue, considerable image dehazing algorithms (DHAs) [4]-[10] have been proposed. However, before deploying these DHAs at scale, it is necessary to evaluate their effectiveness, i.e., assess the quality of dehazed images they generate.\nThe most accurate way to measure the quality of dehazed images is subjective quality evaluation [11], where the quality of dehazed images is directly evaluated by a certain number of human subjects. However, subjective quality evaluation has a narrow range of applications since it needs to conduct time-consuming and labor-intensive subjective experiments. As a result, objective quality evaluation is proposed, aiming to automatically assess the quality of dehazed images without human involvement.\nIn general, objective dehazed quality measures can be divided into three categories: full-reference dehazed image quality assessment [12]\u2013[14], reduced-reference dehazed image quality assessment [15]-[20], and no-reference dehazed image quality assessment [11], [21]\u2013[24]. When evaluating the quality of dehazed images, the former two categories require reference images, while the last category only takes dehazed images as input. In real-world scenarios, reference images are typically unavailable. Therefore, NR DQA, also known as blind dehazed image quality assessment (BDQA), has received considerable attention in recent years.\nExisting BDQA approaches are mainly composed of tra-ditional methods [11], [21]\u2013[23] and learning-based meth-ods [19], [24], [25]. Traditional methods usually manually design some haze-related features for quality evaluation. Since handcrafted features have limited capabilities in representing distortion and content of dehazed images, the performance of traditional BDQA methods is often unsatisfactory. To this end, learning-based methods employ deep neural networks to automatically extract representative features from dehazed images. Compared to traditional BDQA methods, learning-based BDQA methods achieve more accurate quality predic-tion. However, their performance is still limited by the small size of the DQA dataset.\nTo alleviate this issue, we resort to Contrastive Language-Image Pre-Training (CLIP) [26], which has shown good gen-eralization ability across various image recognition tasks [27], [28]. In the BDQA task, the perceptual characteristics of the human visual system need to be considered [11], that is, image understanding based on hierarchical features. Therefore, we feed patches that preserve the local details of the dehazed image and the resized image that preserves the global structure of the dehazed image into CLIP. Then, we tune CLIP by layerwisely inserting learnable prompts into its vision and language branches to accurately map the input hierarchical information of dehazed images into the quality score.\nThe contributions of this paper are mainly two-fold:\n\u2022 We present the first CLIP-based BDQA method, CLIP-DQA, which evaluates dehazed images from global and local perspectives.\n\u2022 We conduct extensive experiments, including ablation studies and visualizations, to verify the efficacy of the proposed method."}, {"title": "II. PROPOSED METHOD", "content": "Given a dehazed image I, blind dehazed image quality assessment aims to estimate the visual quality score of the dehazed image without any reference information. In existing works, a patch-based evaluation framework is widely used for BDQA. In such a framework, we first crop / patches {I}=-1 from the input dehazed image I, then estimate the quality score for each patch, and finally use the average score as the quality score of the whole picture:\n\\begin{equation}\nQ = \\frac{1}{N}\\sum_{i=0}^{N-1}f(I_i),\n\\end{equation}\nwhere f(\u00b7) denotes the BDQA method, and Q is the estimated result.\nInspired by the human visual system using hierarchical features for image understanding, we evaluate the dehazed image from global and local perspectives:\n\\begin{equation}\nQ = \\frac{1}{N}\\sum_{i=0}^{N-1}f(I_i^l, I_i^g),\n\\end{equation}\nwhere I keeps the local details and Is, a resized version of I, maintains the global structure. Moreover, we model f(.) using CLIP, which learns well-generalized knowledge from millions of image-text pairs. Next, we will detail how to evaluate dehazed images with CLIP.\nSince CLIP can judge the similarity between images and natural language descriptions, we can directly use CLIP to evaluate the dehazed image without training, i.e., zero-shot BDQA.\nLet Tp and Tn be a pair of antonym text prompts, e.g., \"Good photo.\" and \"Bad photo.\u201d. The estimated vi-sual quality, f(In, Is) in Equation. 2, can be calculated as follows:\n\\begin{equation}\nf(I_i^l, I_i^g) = \\frac{e^{sim(t_p,c_i)}}{e^{sim(t_p,c_i)} + e^{sim(t_n,c_i)}},\n\\end{equation}\nwhere tp and ts are the textual representation extracted by the language branch of CLIP from Tp and Tn, respectively. Ci is the visual representation extracted by the vision branch of CLIP from the pair of I and Is. Here, sim(\u00b7, \u00b7) calculates the cosine distance between textual and visual representations.\nHowever, the performance of the zero-shot BDQA is typi-cally far from satisfactory. There are two main reasons for this result. First, the handcrafted antonym text prompts are often sub-optimal for BDQA since designing effective antonym text prompts requires considerable expertise. Second, the visual representation extracted by CLIP may not be discriminative for BDQA as dehazed images differ from images used for training CLIP in terms of distortion type and appearance.\nIn order to better adapt CLIP to BDQA, it is necessary to fine-tune CLIP using the DQA dataset. Motivated by the suc-cess of prompt learning [30]\u2013[33], we tune CLIP with multi-modal prompts, including textual and visual prompt tuning, as shown in Fig. 1. The core idea of multi-modal prompt tuning is to use learnable textual prompts for automatically mining useful soft antonym text prompts from DQA datasets, and use learnable visual prompts for mitigating the domain gap between dehazed images and natural images.\nThe language branch of CLIP consists of K transformer layers, and the i-th transformer layer can be defined as:\n\\begin{equation}\n[W_i] = L_i(W_{i-1}), i = 1, \u2026\u2026\u2026, K,\n\\end{equation}\nwhere Wi-1 and Wi are the input and output of the i-th trans-former layer Li, respectively. The input of the first transform layer, Wo, corresponds to word embeddings of handcrafted antonym text prompts. To tune the language branch of CLIP, we layerwisely insert learnable prompts:\n\\begin{equation}\n[_, W_i] = L_i([F_{i-1}(P_{i-1}), W_{i-1}]),\n\\end{equation}\nwhere we map the set of learnable prompts Pi-1 into the same space as Wi-1 through a fully-connected layer Fi\u22121. Notably, the output of Pi-1 is discarded after the i-th transformer layer Li.\nIn this paper, the vision branch of CLIP is transformer-based and also contains K transformer layers. The formulation of each transformer layer is defined as follows:\n\\begin{equation}\n[c_i, E_i^l, E_i^g] = V_i([c_{i-1}, E_{i-1}^l, E_{i-1}^g), i = 1,\u2026, K,\n\\end{equation}\nwhere ci, E, and E are the class token, the token set of the input patch, and the token set of the input resized dehazed image, respectively. To tune the vision branch of CLIP, we also layerwisely insert learnable prompts:\n\\begin{equation}\n[c_i, E_i^l, E_i^g, _ ] = V_i([c_{i-1}, E_{i-1}^l, E_{i-1}^g, F_{i-1}(P_{i-1})]),\n\\end{equation}\nwhere we map the set of learnable prompts Pi\u22121 into the same space as Ei-1 through a fully-connected layer Fi-1. Similar to textual prompt tuning, we discard the output of the learnable prompts Pi-1 after the transformer layer Vi.\nFor the DQA task, we use Mean Square Error (MSE) as the training objective:\n\\begin{equation}\nL_{MSE} = \\frac{1}{B}\\sum_{j=1}^B \\|Q_j - \\hat{Q_j}\\|^2,\n\\end{equation}\nwhere B is the batch size, Q; and Qj are the predicted quality score and the mean opinion score of the j-th dehazed image."}, {"title": "III. EXPERIMENTS", "content": "To verify the effectiveness of our proposed method, we conduct experiments on two authentic dehazed image quality databases:\n\u2022 DHQ database [17]: It consists of 250 hazy images and 1,750 dehazed images generated by 7 image dehazing algorithms. Each dehazed image is labeled by a mean opinion score (MOS) ranging from 0 to 100.\n\u2022 exBeDDE database [13]: It contains 12 haze-free images, 167 hazy images, and 1,670 dehazed images produced by 10 image dehazing algorithms. Each dehazed image is annotated by a MOS ranging from 0 to 1.\nAs suggested by the video quality expert group (VQEG) [34], we employ Spearman rank order correlation coefficient (SRCC), Pearson linear correlation coefficient (PLCC), and Kendall rank order correlation coefficient (KRCC) for performance comparisons. All three evaluation criteria ranged from 0 to 1, and the higher the value, the better the performance.\nFor fair comparison, we evaluate each image quality assessment method 10 times, and report the average results. At each time, we randomly split the dataset into two parts based on the content of the image, 80% for training and 20% for testing.\nWe build the proposed method based on ViT-B/32 CLIP, and the length of learnable prompts at each transformer layer is set to 8. During training, we keep CLIP frozen and optimize the remaining parts using the Adam optimizer [35] with a learning rate of le-4. The total training epoch and the batch size are 50 and 64, respectively. All experiments are run on a single NVIDIA RTX 4090 GPU."}, {"title": "IV. CONCLUSION", "content": "In this paper, we present the first preliminary study on introducing CLIP to BDQA, dubbed CLIP-DQA. Motivated by characteristics of the human vision system, CLIP-DQA blindly evaluates dehazed images from global and local per-spectives. Furthermore, CLIP-DQA employs learnable multi-modal prompts to tune CLIP for accurate quality prediction. Comprehensive experiments demonstrate that the proposed method achieves state-of-the-art performance."}]}