{"title": "A Benchmark Suite for Evaluating Neural Mutual Information Estimators on Unstructured Datasets", "authors": ["Kyungeun Lee", "Wonjong Rhee"], "abstract": "Mutual Information (MI) is a fundamental metric for quantifying dependency between two random variables. When we can access only the samples, but not the underlying distribution functions, we can evaluate MI using sample-based estimators. Assessment of such MI estimators, however, has almost always relied on analytical datasets including Gaussian multivariates. Such datasets allow analytical calculations of the true MI values, but they are limited in that they do not reflect the complexities of real-world datasets. This study introduces a comprehensive benchmark suite for evaluating neural MI estimators on unstructured datasets, specifically focusing on images and texts. By leveraging same-class sampling for positive pairing and introducing a binary symmetric channel trick, we show that we can accurately manipulate true MI values of real-world datasets. Using the benchmark suite, we investigate seven challenging scenarios, shedding light on the reliability of neural MI estimators for unstructured datasets.", "sections": [{"title": "1 Introduction", "content": "Mutual Information (MI), denoted as $I(X; Y)$, serves as a fundamental measure in quantifying the dependency between two random variables [Cover, 1999]. It is mathematically defined as:\n\n$I(X; Y) = \\mathbb{E}_{p(x,y)} \\log \\frac{p(x, y)}{p(x)p(y)}$\n\nIn practice, we often rely on the estimations instead of the exact calculation of MI because we can only access the examples sampled from joint and marginals but not the underlying distribution functions ($p(x, y)$ and $p(x)p(y)$). To this end, various sample-based MI estimators have been proposed [Fraser and Swinney, 1986, Shwartz-Ziv and Tishby, 2017, Kraskov et al., 2004, Belghazi et al., 2018, Poole et al., 2019, Song and Ermon, 2019, 2020, Cheng et al., 2020], and they have played a key role in improving the deep learning performance across diverse applications, including generative models [Chen et al., 2016], language representation learning [Oord et al., 2018, Wang et al., 2020], domain generalization [Li et al., 2022], anomaly detection [Lei et al., 2023], and self-supervised learning [Hjelm et al., 2018, Bachman et al., 2019, Chen et al., 2020, Chen and He, 2020, Grill et al., 2020].\nDespite the huge success of MI estimators in developing useful real-world applications, the accuracy of MI estimators on real-world datasets largely remains underexplored because the true MI values cannot be calculated for such datasets. Gaussian datasets, the primary benchmark in the existing studies [Belghazi et al., 2018, Poole et al., 2019, Song and Ermon, 2019, 2020, Cheng et al., 2020], do not adequately represent the complexity of real-world datasets. This raises a fundamental question: do estimators that perform well on Gaussian datasets also excel with more complex datasets like images or texts? Recently, Czy\u017c et al. [2023] have explored non-Gaussian datasets to evaluate MI"}, {"title": "2 Backgrounds: Neural Mutual Information Estimators", "content": "Mutual information between two random variables X and Y is defined as follows.\n\n$I(X; Y) = KL(p(x, y)||p(x)p(y)) = \\mathbb{E}_{p(x,y)} \\log \\frac{p(x, y)}{p(x)p(y)}$\n\nWhen only a finite set of joint samples is available, the exact MI cannot be calculated, but an estimation can be made. Among the known MI estimation methods, including simple binning [Fraser and Swinney, 1986, Shwartz-Ziv and Tishby, 2017] and non-parametric kernel-density estimators [Kraskov et al., 2004], variational estimators based on variational bounds and deep neural networks (DNN) modeling have become dominant for complex datasets [Belghazi et al., 2018, Poole et al., 2019, Song"}, {"title": "Definition 2.1 (Variational MI estimators [Poole et al., 2019])", "content": "Let X, Y be two random variables taking values in X, Y, and $\\mathbb{D} = \\{(x_i, Y_i)\\}_{i=1}^N \\sim X, Y$ denote the set of samples drawn from a joint distribution over X and Y. The variational bounds of $I(X; Y)$ are formulated as:\n\n$I(X; Y) > \\hat{I}(X; Y) = 1 + \\mathbb{E}_{p(x,y)} \\log \\frac{e^{f(x,y)}}{a(y)} - \\mathbb{E}_{p(t)p(y)} [\\frac{e^{f(x,y)}}{a(y)}]$\n\nwhere a(y) > 0 is any value or function of y. A variety of MI estimators are defined by adopting different a(y). For example, a(y) = e (constant) corresponding to $\\hat{I}_{NWJ}(X; Y)$ [Nguyen et al., 2010] (also known as f-GAN KL [Nowozin et al., 2016] and MINE-f [Belghazi et al., 2018]) and a(y) = $\\mathbb{1} + e^{f(x,y)}$ corresponding to $\\hat{I}_{InfoNCE}(X; Y)$ [Oord et al., 2018].\nFor a neural MI estimator, a DNN is used to model the critic function f(x, y), and there are two associated steps. The first is the optimization (or training) step where the DNN parameters are learned. The second is the estimation step where the actual MI values are inferred with the optimized DNN. Variational MI estimators, such as DV [Donsker and Varadhan, 1983], NWJ [Nguyen et al., 2010], and InfoNCE [Oord et al., 2018], use a single loss function for both optimization and estimation, and the loss function corresponds to the theoretical MI bound in use. Other variational MI estimators, such as JS [Nowozin et al., 2016], MINE [Belghazi et al., 2018], and SMILE [Song and Ermon, 2019], adopt small modifications in either optimization or estimation to improve the robustness or accuracy of the estimator. The most popular variational MI estimators are summarized in Table 1.\nCommon choices for the critic function f(x, y) include (1) the inner product critic $f_{inner}(x_i, y_j) = x_i^T y_j$, (2) bilinear critic $f_{bi}(x_i, y_j) = x_i^T W y_j$ where W is trainable, (3) separable critic $f_{sep}(x_i, y_j) = f_1(x_i)^T f_2(y_j)$, and (4) joint critic $f_{joint}(x_i, y_j) = f_3([x_i, y_j])$. Here $f_1$, $f_2$, and $f_3$ are typically shallow MLPs and they model the relationship between all pairs of $(x_i, y_j) \\forall i, j \\in [1, K]$."}, {"title": "3 Related Works", "content": "Despite its theoretical validity, MI estimators present a few disadvantages because we typically have access to samples, but not to the underlying distribution functions [Poole et al., 2019, Song and Ermon, 2019, Paninski, 2003, McAllester and Stratos, 2020]. Most estimators exhibit sub-optimal"}, {"title": "4 Proposed Method and Benchmark Suite", "content": "We propose a comprehensive method for evaluating neural MI estimators across various data domains. While our method is applicable to any choice of data domain, we focus on three types of data domains in our benchmark suite: (1) a multivariate Gaussian dataset ($\\mathcal{D}_{Gaussian}$), corresponding to the most common case for evaluating MI estimators in the existing works [Poole et al., 2019, Song and Ermon, 2019, McAllester and Stratos, 2020]; (2) an image dataset consisting of digits ($\\mathcal{D}_{vision}$), as an example of vision tasks; and (3) a sentence embedding dataset consisting of BERT embeddings of movie review datasets ($\\mathcal{D}_{NLP}$), as an example of NLP tasks. We first consider the general formulation for Gaussian dataset, define three factors that can affect MI values, introduce same-class sampling, propose a method for generating unstructured datasets with adjustable true MI values, and finally explain how binary symmetric channel trick can be employed for manipulating true MI to any non-integer value."}, {"title": "4.1 General formulation for Gaussian dataset", "content": "Consider a dataset with K pairs of samples, where $(x_i, Y_i)$ is sampled from a joint distribution p(x, y). An MI estimator utilizes the dataset as its input and evaluates the estimated mutual information $\\hat{I}(X; Y)$. If the estimation is accurate, $\\hat{I}(X; Y)$ should be close to the true mutual information $I(X; Y)$. In previous studies, a Gaussian dataset associated with a multivariate Gaussian model was utilized to assess neural MI estimators [Belghazi et al., 2018, Poole et al., 2019, Song and Ermon, 2019, 2020, Cheng et al., 2020]. The Gaussian dataset has Gaussian samples with zero mean and a component-wise correlation of p between X and Y. The true MI is known and can be expressed analytically as $I(X; Y) = -\\frac{d_g}{2} \\log (1 - \\rho^2)$, where $x \\in \\mathbb{R}^{d_s}$ and $y \\in \\mathbb{R}^{d_g}$."}, {"title": "4.2 Definitions of ds, dr, and Z", "content": "In the benchmark suite, we design and focus on three essential factors that can affect mutual information I(X; Y), especially for unstructured datasets. For random variables X and Y with a joint distribution p(x, y), they can be defined as follows.\nDefinition 4.1 (Number of information sources ds). ds is the number of independent scalar random variables used to form the mutually shared information between X and Y.\nDefinition 4.2 (Representation dimension dr). dr is the size of the observational data. When X and Y are of the same size, it is the length of the vector formed by flattening either X or Y.\nDefinition 4.3 (Nuisance Z). Nuisance to a random variable X is defined as an equal-size random variable Z sharing no information with X. Mathematically, Z satisfies $I(X; Z) = 0$. Nuisance to (X, Y) can be defined similarly where Z is of the same size as (X, Y) and $I(X,Y; Z) = 0$.\nAs an example, consider the Gaussian dataset. Its number of information sources ds is equal to dg, its representation dimension dr is equal to dg, and the dataset contains no nuisance. The effects of three factors on neural MI estimators will be analyzed in Section 5."}, {"title": "4.3 Theoretical background: same-class sampling for positive pairing", "content": "Same-class sampling for positive pairing was proposed in Lee et al. [2023]. The key idea is to allow only the class information to be shared between two random variables X and Y, such that the true MI can be proven to be the same as the entropy of class variable C, i.e., $I(X; Y) = H(C)$. The proofs require mild assumptions of either a lower bound estimate of MI being equal to $H (C')$ or the existence of an error-free decoder (Theorem 3.1 and 3.2 in Lee et al. [2023]). In Lee et al. [2023], the first mild assumption was shown to be closely satisfied for commonly used image datasets, through extensive empirical evaluations. The second one essentially implies that the true MI is equal to H(C) when the class information is easily decodable. An example is MNIST dataset whose digit information as the class label is known to be easily decodable. Similarly, for NLP datasets, sentence embeddings of the IMDB dataset [Maas et al., 2011] can be made easily decodable by fine-tuning with the class label C. We carefully construct our unstructured datasets such that we can take advantage of the theoretical results. Once we can employ $I(X; Y) = H(C)$, the calculation of H (C) can be made trivial by choosing uniformly distributed class labels. Overall, same-class sampling makes it possible to access the true MI values even for unstructured datasets. For convenience, the theorems and proofs are provided in Supplementary B.1."}, {"title": "4.4 Generating datasets with adjustable true MI values", "content": "By utilizing Theorem 3.1 and 3.2 in Lee et al. [2023], it becomes possible to access the true MI of an unstructured dataset by drawing the positive pairs from a joint distribution p(x, y) where only the class information C is shared by X and Y. We first consider a binary random variable C with p(0) = p(1) = 0.5. We can design a simple stochastic function that maps C to X, where X is an image or sentence embedding. In our benchmark suite, to make use of the error-free classification function, we choose a dataset that easily achieves perfect classification accuracy with a simple classifier (e.g., 1-layer MLP). We adopt the MNIST dataset [Deng, 2012] for $\\mathcal{D}_{vision}$ and BERT [Devlin et al., 2018] fine-tuned sentence embeddings of the IMDB dataset [Maas et al., 2011] for $\\mathcal{D}_{NLP}$. In our implementation, x becomes a sample from $\\mathcal{D}$ of class 0 when c = 0, and a sample from $\\mathcal{D}$ of class 1 when c = 1. We design a mapping function from C to Y where a different image or text of same class is drawn. For this basic construction, it can be shown that $I(X; Y) = H(C) = 1$ bit. An image example is shown in Figure 2a.\nTo construct a dataset with larger MI, two straightforward approaches can be used. In Figure 2b, we combine four samples of Figure 2a to create an image that is four times larger, which means $I(X; Y) = 4$. In Figure 2c, we stack three pairs of samples from Figure 2a and map them to RGB; hence, $I(X; Y) = 3$. We can adopt flexibly use other stratagems to generate a dataset that has a specific value of true MI. Similarly, we generate the text dataset by concatenating the embedding vectors in 1D.\nFor images, we can insert random samples from other datasets as nuisance to X and Y to make the dataset more realistic without affecting the true MI value, as shown in Figure 2d. Because the source images remain on top without any occlusion, and there is no fixed relationship between"}, {"title": "4.5 Manipulating MI to non-integer values: binary symmetric channel", "content": "To manipulate the true MI and construct a dataset with a non-integer MI value, we adopt the concept of binary symmetric channel (BSC) [Cover, 1999]. BSC is a simple and well known form of noisy communication channel in information theory, and we utilize it for scaling down the true MI value in a fully controlled manner. With BSC, X is always consistent with the class variable C but Y is noisy where it is different from C with a crossover probability of B. Then, the true MI value can be controlled by adjusting \u03b2 between 0 and 0.5.\nTheorem 4.4 (Manipulating MI to be non-integer). When the information source C is transmitted perfectly to X, while it is transmitted to Y over a binary symmetric channel (BSC) with a crossover probability $\\beta\\in [0,0.5]$, the mutual information I(X;Y) between X and Y is determined as follows.\n\n$I(X; Y) = H(C) \\times (1 \u2013 H(\\beta))$\n(1)\n$H(3)$ refers to the entropy of a binary variable with the crossover probability \u03b2 [Cover, 1999], and is given by $H(\\beta) = -\\beta \\log \\beta \u2013 (1 \u2013 \\beta) \\log (1 \u2013 \\beta)$. When \u03b2 = 0, there is no information loss during transmission. Thus, $H(\\beta) = 0$ and $I(X; Y) = H(C)$. When \u03b2 = 0.5, the channel is completely noisy, and X and Y do not share any information. Thus, $H(\\beta) = 1$ and $I(X; Y) = 0$.\nAny MI value in between can be implemented by choosing an appropriate \u03b2. The proof is provided in Supplementary B.2."}, {"title": "5 Empirical investigations", "content": "In this section, we investigate seven key aspects that can affect the performance of MI estimators. All investigations are based on our benchmark suite and the empirical findings are reported together. As evaluation metrics, we calculate bias, variance, mean squared error (MSE), and the estimated MI (defined as the average of the estimations) during the training of the critic function. All experiments were conducted on a single NVIDIA GeForce RTX 3090. Detailed experimental setups and raw results are available in Supplementary C and D, respectively."}, {"title": "5.1 Choice of critic architecture: superiority of joint critic for unstructured datasets", "content": "Poole et al. [2019] observed that using a joint critic outperforms a separable critic for NWJ and JS estimators, while the InfoNCE estimator demonstrating robustness to the choice of critic architecture. For SMILE [Song and Ermon, 2019] estimator, a joint critic surpassed a separable critic in basic Gaussian setups, but this trend reversed in more complex setups of the same dataset. This section aims to extend these insights into the vision and NLP domains, providing guidance on selecting critic architectures across diverse data contexts.\nFigure 3 and Figure 9 in Supplementary D present the results of our experiments, which include scenarios where variables share the same domain (image and image, text and text) as well as cases involving cross-domain pairs (image and text). Our key observations are: (1) The joint critic consistently provides reliable estimations across all estimators and data domains; (2) The bilinear critic, while providing stable yet biased estimations for Gaussian datasets, is notably inaccurate in unstructured datasets; (3) The separable critic performs well on unstructured datasets while it often exhibits large variance for Gaussian cases; (4) Contrary to the theoretical proofs and empirical findings of Song and Ermon [2019] on the high variance of the DV estimator in Gaussian datasets, we observe stable performance in unstructured datasets even with large MI values. Notably, no significant advantage of SMILE over DV (or MINE) was observed for both images and sentence embeddings."}, {"title": "5.2 Choice of critic capacity: larger capacity does not ensure a higher estimation accuracy", "content": "While joint critics often yield the best estimation accuracy in various scenarios, this subsection delves into whether increasing critic capacity could further enhance estimation accuracy. A prior study [Tschannen et al., 2019] posited that larger critic capacities should correlate with more precise estimations. To assess critic capacity, we manipulated the depth of the critic network, with specific results for a true MI of 2 bits outlined in Table 2. (Full results are available in Supplementary D.3.)\nContrary to the assertion of Tschannen et al. [2019], our findings reveal an unexpected trend: no discernible positive correlation between critic capacity and estimation accuracy across any data domain. Specifically, the Pearson's correlation coefficient p between critic depth and estimation accuracy was -0.007 for $\\mathcal{D}_{Gaussian}$, 0.059 for $\\mathcal{D}_{vision}$, and -0.001 for $\\mathcal{D}_{NLP}$. These findings suggest that an increase in critic capacity does not inherently improve estimation accuracy and may even be counterproductive, contradicting previous assumptions and underscoring the need for a nuanced approach to enhancing critic architectures.\nBased on our findings, we fixed the critic architecture as the joint critic of 2-layer MLP for all subsequent sections of this study."}, {"title": "5.3 Choice of MI estimator: no universal winner exists across the three data domains", "content": "Recent advancements have introduced more accurate MI estimators, with notable efforts highlighted in Poole et al. [2019], Song and Ermon [2019], McAllester and Stratos [2020], Cheng et al. [2020]. Among these, the SMILE estimator has been acclaimed for efficiently reducing the estimation variance compared to other estimators, offering a more favorable bias-variance trade-off [Song and Ermon, 2019]. As evidenced in Table 3, the SMILE estimator exhibits a slight superiority over other estimators in Gaussian scenarios and a more pronounced advantage in NLP cases. However, in vision cases with large true MI values, the NWJ and MINE estimators demonstrate superior"}, {"title": "5.4 Number of information sources (ds): unstructured datasets outperform Gaussian in handling larger ds", "content": "In this subsection, we explore the influence of the number of information sources (ds), as previously defined in Section 4.2, on the accuracy of MI estimation. We incrementally increase de from 1 to 100, observing the effects on estimation accuracy. As shown in Figure 4, estimation accuracy deteriorates when ds becomes excessively large across all data domains.\nInterestingly, we observed domain-specific thresholds where MI estimators begin to falter: estimations become unreliable when ds exceeds 4 in the Gaussian case, 36 in the vision case, and 64 in the NLP case, approximately. This suggests that unstructured datasets, unlike the Gaussian datasets, allow for relatively accurate MI estimations with moderate de values within the range of [4, 36].\nGiven that a uniformly distributed classification problem typically involves classes much less than 10M (significantly lower than 236), these findings indicate that large de values might not be a limiting factor in practical applications."}, {"title": "5.5 Representation dimension (dr): it does not affect the estimation accuracy", "content": "Real-world datasets can have any representation dimension while having a fixed number of information sources. For example, the image datasets in Figure 2 can be represented in any reasonable dimension without compromising the semantic information. To analyze the MI estimation accuracy in these scenarios, we investigate a range of representation dimensions dr for a fixed number of information sources d, and the MI value I(X; Y). Specifically, we simply resize the images of size 642 in Figure 2 using a linear interpolation function to obtain images whose size ranges between 102 and 1002 while keeping ds and I(X; Y) fixed.\nAs shown in Table 4, we observe that the representation dimension does not affect estimation accuracy for images. Even as dr increases to 10000, we found that almost all the estimators provide accurate estimations. In other words, the sparsity does not impact the MI estimation accuracy for images. These results can be attributed to the diverse and complex pixel patterns in images, which provide robust information even as dr increases, and to high redundancy, which ensures essential information is maintained, unlike typical structured datasets."}, {"title": "5.6 Nuisance: MINE turns out to be relatively robust", "content": "Nuisance, integral to real-world datasets as defined in Section 4.2, presents unique challenges in MI estimation. To quantitatively assess their influence on images, we place the digits in x over the scaled background image z\u00b7 \u03b7 as depicted in Figure 5a. We varied the nuisance strength parameter \u03b7 from 0 to 1. Note that introducing nuisance does not alter the true MI values, as class labels remain perfectly predictable when a large number of samples are given. This is the first attempt to investigate how the nuisance affects the estimation of MI.\nOur first analysis focused on a fixed true MI of 2 bits, with results detailed in Figure 5b. It was observed that estimation accuracy declines significantly with increased nuisance strength, particularly beyond a threshold of 0.4, across all estimators. Further investigation into larger MI values (Figure 5c) reveals that while large \u03b7 adversely affects estimations for small MI values, MINE and SMILE-inf"}, {"title": "5.7 Network and layer dependency: estimation holds for invertible networks and upper layers", "content": "Our final investigation focuses on the accuracy of MI estimators in the context of deep representations (i.e., I(g(X); g(Y)), where g represents a deep network) because of the prevalent interest in understanding dependencies between representations rather than raw inputs. While Czy\u017c et al. [2023] highlighted concerns about the reliability of estimators in datasets with tractable distribution functions and under specific unrealistic transformations, our study expands the horizon by examining three invertible networks: MAF [Papamakarios et al., 2017], RealNVP [Dinh et al., 2016], and i-RevNet [Jacobsen et al., 2018] for images and texts. For $\\mathcal{D}_{vision}$, we additionally investigate a non-invertible ResNet-50 network, a widely used non-invertible network, which is pre-trained on the MNIST dataset, to provide more relevant insights for practitioners. This allows us to better align with practical interests in MI estimation beyond invertible networks. Remarkably, as demonstrated in Figure 11 in supplementary material, we found that estimation robustly persists for the representations of $\\mathcal{D}_{vision}$ and $\\mathcal{D}_{NLP}$ across various network architectures.\nIf deep representations are robust for estimating MI, should this hold across all layers? To address this question, we estimated MI for intermediate layers of ResNet-50 trained on $\\mathcal{D}_{vision}$ without nuisance. Results are summarized in Figure 6. According to the data processing inequality, lower-layer MI cannot be smaller than upper-layer MI. However, estimated MI values indicate the opposite. This discrepancy suggests that MI estimations at lower layers are less precise, whereas upper-layer representa-tions yield more accurate estimations. Interestingly, we observe the step-wise estimation results; the transition clearly occurs when the output size changes across all types of estimators. It appears that upper layers might capture abstract, high-level features, potentially offering more meaningful infor-mation for MI estimation. In contrast, lower layers might contain more noise and less discriminative features, which could lead to poorer accuracy."}, {"title": "6 Discussion and Conclusion", "content": "Quantifying complex dependency between variables is an essential topic in machine learning. In this realm, sample-based neural MI estimators have been the primary choice for many deep learning applications. The MI estimators have been directly used for improving downstream task performance or indirectly used for motivating learning method developments. However, there has been hardly any attempt to evaluate the accuracy of these MI estimators over real-world datasets such as images and texts. In this study, we proposed a novel benchmark suite for evaluating neural MI estimators on unstructured datasets, where the underlying distribution functions are not accessible. Our findings reveal discrepancies in estimation accuracy between traditional Gaussian benchmarks and unstructured data scenarios, highlighting the limitations of Gaussian benchmarks in capturing the nuances of MI estimation in practical settings. Notably, our findings on unstructured datasets demonstrate that MI estimators can yield remarkably accurate results, particularly in conjunction with deep representations, indicating their potential to continue driving advancements in deep learning research. While our study does not cover the entire spectrum of real-world datasets, it signifies a substantial step forward in evaluating and understanding MI estimators beyond purely statistical datasets. We hope that this benchmark suite not only offers a new standard for evaluating MI estimators but also catalyzes further research, enriching our comprehension of MI across a diverse data domains."}, {"title": "A Broader Societal Impact Statement", "content": "This paper introduces a new benchmark suite for evaluating mutual information (MI) estimators, particularly in unstructured datasets like images and texts. The broader impacts of our work are significant in two folds. First, by providing a more realistic and challenging benchmark for MI estimators, this research can lead to the development of more accurate and robust estimation methods, especially in complex data scenarios. Second, improved MI estimation methods can benefit fields such as computer vision and natural language processing, where understanding intricate data relationships is crucial, thereby enhancing the efficiency and effectiveness of AI systems in practical applications. In essence, our research has the potential to influence various aspects of society through the improved understanding and application of mutual information in complex data domains."}, {"title": "B Theorems", "content": "In Section 4.3, we adopt the same-class sampling method for positive pairing suggested in [Lee et al., 2023]. For convenience, we provide the theorems and proofs in [Lee et al., 2023] as follows.\nProposition B.1 (InfoNCE estimation as a lower bound of the true MI [Oord et al., 2018, Poole et al., 2019]). The InfoNCE estimation of mutual information is a lower bound of the true mutual information.\n\n$\\hat{I}(X; Y) \\leq I(X; Y)$\n(2)\nProposition B.2 (log (2K \u2013 1) Bound [Oord et al., 2018, Poole et al., 2019]). The InfoNCE estima-tion of mutual information is upper bounded by log(2K \u2013 1).\n\n$\\hat{I}(X; Y) \\leq \\log (2K \u2013 1)$"}, {"title": "Proof.", "content": "The proof is based on the variational bound derivation. Let $q(x|y) = \\frac{P(x) e^{\\frac{f(x,y)}{\\tau}}}{Z(y)}$, where $Z(y) = \\mathbb{E}_{p(x)} [e^{\\frac{f(x,y)}{\\tau}}]$. Then the true MI, I(X; Y), can be bounded as the following.\n\n$I(X; Y)$\n$= \\mathbb{E}_{p(x,y)} \\log \\frac{p(x, y)}{p(x)p(y)}$\n\n$= \\mathbb{E}_{p(x,y)} \\log \\frac{p(x|y)}{p(x)}$\n\n$= \\mathbb{E}_{p(x,y)} \\log \\frac{p(x|y) q(x|y)}{p(x)} + \\mathbb{E}_{p(y)}[KL(p(x|y)||q(x|y))]$\n\n$\\geq \\mathbb{E}_{p(x,y)} \\log \\frac{q(x|y)}{p(x)}$\n\n$= \\mathbb{E}_{p(x,y)} \\log \\frac{e^{\\frac{f(x,y)}{\\tau}}}{Z(y)}$\n\n$= \\mathbb{E} \\log [\\frac{e^{x_i Y_i/\\tau}}{\\frac{1}{2K}\\sum_{j=1}^{2K} [\\mathbb{1}_{j \\neq i}e^{x_i y_j/\\tau} + e^{x_i Y_i/\\tau}]} ]$\n\n$= log (2K \u2212 1)$\n\n$+ \\mathbb{E} \\log [\\frac{e^{x_i Y_i/\\tau}}{\\sum_{j=1}^{2K} (\\mathbb{1}_{j \\neq i}e^{x_i y_j/\\tau} + e^{x_i Y_i/\\tau})}]$\n\n$= log (2K - 1) \u2013 L_{InfoNCE}$\n\n$\\leq \\hat{I}(X; Y)$"}, {"title": "Proof", "content": "From the construction of same-class sampling, the dependency can be expressed as S \u2192 C\u2192 X and S \u2192 C \u2192 Y where C is the class label of the sampled source image S. Because of the error-free classifier fclass(\u00b7), the class label information can be perfectly extracted from X or Y. This means that X \u2192 C and Y \u2192 C also hold. Using the dependencies, we can conclude that the following is a valid Markov chain.\n\n$S\\rightarrow C\\rightarrow X \\rightarrow C \\rightarrow Y \\rightarrow C$\nThe desired equality proof can be obtained by deriving an upper bound $I(X; Y) \\leq H(C)$ and a lower bound $H(C) \\leq I(X; Y)$. The upper bound follows directly from the Proposition B.3. The lower bound can be derived by applying the data processing inequality to the Markov dependency C\u2192 X \u2192 Y \u2192 C that can be confirmed from Eq. (22).\n\n$I(C; C) < I(X; Y)$\n\n$\u21d2H(C) < I(X; Y)$\nNote that I(C; C) is the self-information that is equal to H(C)."}, {"title": "B.2 Proof of Theorem 4.4: Detailed explanation for binary symmetric channel (BSC)", "content": "Proof. In Section 4.5, we introduced binary symmetric channel (BSC) [Cover, 1999] to construct a dataset with a non-integer MI value. We first consider the basic construction case of Figure 2a where H(C) = 1. As shown in Figure 7, the transmission process of BSC for C \u2192 Y corresponds to a binary channel where the the label of Y is corrupted with probability B. Then, the mutual information can be evaluated as follows.\n\n$I(X; Y) = I(C;Y) = H(C) \u2013 H(CY)$\n$= 1 \u2212 \\sum p(y)H(C|Y = y)$\n\n$= 1 - \\sum p(y)H(\\beta)$\n$= 1 \u2212 H(\\beta)$\nThe first equality comes from the Markov equivalence between X and C. Note that $H (\\beta)$ is symmetric and it can be expressed as $H(\\beta) = -\\beta \\log \\beta \u2013 (1 \u2013 \\beta) \\log (1 \u2013 \\beta) = H(1 \u2013 \\beta)$. For the case where H(C) is an integer larger than 1, we can apply the above BSC trick to each binary information source. Then, we obtain the general result of $I(X; Y) = H(C) \\times (1 \u2013 H(\\beta))$. For instance, when we have three independent binary information sources, H(C) = 3 and $I(X; Y) = 3 \u00d7 (1 \u2013 H(\\beta))$ by applying BSC with the same parameter \u1e9e to each binary information source."}, {"title": "C Detailed experimental setups", "content": "We follow the setup of the case of multivariate Gaussian [Belghazi et al., 2018, Tschannen et al., 2019, Song and Ermon, 2019, Poole et al., 2019"}]}