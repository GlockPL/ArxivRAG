{"title": "Multi-Modal Adapter for Vision-Language Models", "authors": ["Dominykas Seputis", "Serghei Mihailov", "Soham Chatterjee", "Zehao Xiao"], "abstract": "Large pre-trained vision-language models, such as CLIP [26], have demonstrated state-of-the-art performance across a wide range of image classification tasks, without requiring retraining. Few-shot CLIP is competitive with existing specialized architectures that were trained on the downstream tasks. Recent research demonstrates that the per-formance of CLIP can be further improved using lightweight adapta-tion approaches. However, previous methods adapt different modalities of the CLIP model individually, ignoring the interactions and relation-ships between visual and textual representations. In this work, we pro-pose Multi-Modal Adapter, an approach for Multi-Modal adaptation of CLIP. Specifically, we add a trainable Multi-Head Attention layer that combines text and image features to produce an additive adaptation of both. Multi-Modal Adapter demonstrates improved generalizability, based on its performance on unseen classes compared to existing adap-tation methods. We perform additional ablations and investigations to validate and interpret the proposed approach.", "sections": [{"title": "1 Introduction", "content": "Large pre-trained vision-language models (VLMs) have demonstrated state-of-the-art capabilities by learning the joint embedding space of texts and images. Pre-training on vast data enables VLMs to learn diverse concepts, ensuring ro-bust performance across tasks. Generalizability is crucial for applying pre-trained models across diverse tasks with minimal adaptation. A common approach to assess generalizability, which we use in this work, is evaluating the models in an n-class-k-shot setting, where for a given task, the model is trained on k samples from n base classes, with unseen samples and classes during evaluation.\nCLIP [26], a prominent VLM, exceeds in zero-shot settings on new tasks. It utilizes contrastive learning to align text and image representations in a shared embedding space. For image classification tasks, CLIP predicts based on the similarity between text and image embeddings."}, {"title": "2 Related work", "content": ""}, {"title": "2.1 Vision-Language Models", "content": "Vision-language models have developed rapidly over the recent years. VLMs demonstrate advanced vision-language understanding (CLIP, ALIGN [14], VLMo [2], ImageBind [8]), text generation capabilities (GPT-4V [24], Flamingo [1], Frozen [30]), including multi-modal generation (Gemini [29]). The VLM ap-proaches focus on aligning text and image data in a shared embedding space, to then be compared or treated as a unified sequence. Trained on vast amounts of data across vision and text, these models have the emergent ability of few-shot learning: understanding diverse concepts and displaying high performance without task-specific training.\nOur primary focus is on CLIP, a dual-encoder model for vision-language understanding trained using contrastive loss. In the contrastive setting, models learn from N image-text pairs by maximizing similarity for correct pairs and minimizing it for others. Combined with the Transformer architecture [31] and large-scale data, this approach is scalable and efficient for learning robust text and image representations in a shared space. Our Multi-Modal adapter approach is aimed at jointly improving the representations on downstream tasks. In this work, we validate our approach on CLIP, however it could also be applied to other architectures aligning Multi-Modal embeddings."}, {"title": "2.2 Parameter-efficient fine-tuning (PEFT)", "content": "The growing scale of pre-trained models has motivated the development of effi-cient fine-tuning techniques that utilize existing knowledge with minimal adap-tations.\nLow-rank adaptation methods, such as LoRA [13] and DoRA [21], in-troduce trainable low-rank matrices for approximating weight updates during fine-tuning. In contrast, we introduce an additional adapter module that takes inputs from multiple modalities.\nPrompt-based methods, such as prompt-tuning [18] and prefix tuning [20], propose learning the input (or input prefix) as a series of trainable embedding, instead of fine-tuning parameters. In the context of CLIP prompt-tuning, Con-text Optimization (CoOp) and the subsequent Conditional Context Optimiza-tion (CoCoOp) are two notable approaches. CoOp learns task-specific context tokens appended to the text encoder input. Conditional Context Optimization (CoCoOp) builds upon CoOp by making the context tokens image-dependent, thus conditioning the text encoder on both the task and the specific image. These methods have proven to be effective for CLIP fine-tuning, however create additional overhead at inference, due to increasing input size and are in general sub-optimal compared to PEFT approaches.\nAdapter-based methods introduce trainable modules into pre-trained mod-els [12,27]. CLIP-specific adapters, such as CLIP-Adapter [6], Tip-Adapter [37] and XMAdapter [36], modify outputs from image and text encoders to enhance"}, {"title": "3 Method", "content": ""}, {"title": "3.1 Preliminary", "content": "CLIP. CLIP [26] is a vision-language model designed for zero-shot learning. It comprises two encoders: a visual encoder, which can be a CNN like ResNet-50 [9] or a ViT [5], to map images into a low-dimensional embedding space, and a text encoder, built on a Transformer [31], to generate text representations. CLIP predicts whether an image matches a textual description by comparing image features with the text ones using cosine similarity between the vectors. Unlike traditional classifiers that learn visual concepts from random vectors, CLIP's vision-language pre-training enables exploration of open-set visual con-cepts through a high-capacity text encoder, leading to a broader semantic space and more transferable representations for downstream tasks.\nCLIP-Adapter. [7] proposed an efficient method for adapting the CLIP model to new domains. The adapter appends a small number of additional learnable bottleneck linear layers with ReLu activation in between them to CLIP's lan-guage and image encoders, while keeping the original CLIP backbone frozen. To mitigate overfitting and enhance the robustness of CLIP-Adapter, residual connections are utilized to dynamically blend the fine-tuned knowledge with the original knowledge from CLIP's backbone."}, {"title": "3.2 Multi-Modal Adapter", "content": "We propose an iterative update to the CLIP-Adapter [7], where instead of using linear transformations to adapt image and text embeddings individually, we employ a Multi-Modal (MM) adapter base on Multi-Head Attention (\u041c\u041d\u0410) network [31]. The network aggregates Multi-Modal information of both textual and visual data representations and adapts the Multi-Modal embeddings jointly. By integrating task-specific information across modalities, our adapter achieves specific adaptation for each task, which avoids overfitting training tasks."}, {"title": "4 Experiments and Results", "content": "We construct our main experimentation pipeline employing eleven different vi-sual classification datasets. We primarily focus on few-shot training settings, where the number of samples per class is sixteen. To evaluate the model's ability to perform zero-shot classification after fine-tuning using our adapter, we addi-tionally test how the model performs on unseen classes by splitting the number of classes into two parts: first classes/2 classes, denoted as \"base\" (the first half) and \"new\" (the second half). For few-shot experiments, both the training and evaluation sets contain an identical number of samples (Nclasses\u00b7Nsamples). Where train and test splits were not provided by the datasets themselves, we used the splits employed in the work of [40].\nPrimarily, we train the model on \"base\" classes and evaluate it on \"base\" and \"new\" subsamples of the test set.\nWe train our model using the Adam optimizer [15], with a learning rate of 0.005, a batch size of 256, and momentum of 0.5. We employ early stopping with a patience of ten epochs. Our experiments are conducted using an NVIDIA T4 GPU.\nWe use a Multi-Head Attention network with 4 heads. With the employment of the downsampler mentioned before, we have 107, 712 trainable parameters. We use the following datasets in our experiments: Cifar10 [17], Caltech101 [19], Oxford Pets [25], Oxford Flowers [23], Food101 [3], Stanford Cars [16], FGVC-Aircarft [22], DTD [35], SUN397 [33], Imagenet-V1 [4], UCF101 [28] and EuroSat [10]."}, {"title": "4.1 Few-shot Experiment Results", "content": "We present the few-shot experiment results for all models across all datasets in Table 1, highlighting the difference between the accuracy of \"base\" and \"new\" class subsets. Our findings indicate that while Multi-Modal Adapter achieves the best performance on some datasets (Caltech101, Food101), it underperforms on others (Oxford Flowers, FGVC-Aircraft). Moreover, while improving over CLIP Base results, Multi-Modal Adapter is outperformed by existing approaches."}, {"title": "4.2 Inspecting Role of Text Adaptation", "content": "To examine the importance of text adaptation in our Multi-Modal Adapter, we conducted experiments on four different visual datasets. We compared the accuracy differences between adapters with and without text adaptation. The results are presented in Table 2. As indicated by these results, the adapter with text adaptation consistently outperforms the one without text adaptation. In tasks where even small differences in accuracy are crucial, a 1% improvement is significant. The positive effect of text adaptation is especially present within Oxford Flowers dataset, where without it, model fails to significantly increase accuracy for the \"base\" classes."}, {"title": "5 Conclusion", "content": "In this work, we propose a Multi-Modal Adapter for adapting vision-based datasets to specific tasks. We jointly process CLIP's visual and textual em-beddings through a Multi-Head attention network, utilizing its outputs to sepa-rately adapt visual and textual representations. Our studies indicate that while our approach does not excel in all tasks, it outperforms the alternatives in sev-eral instances. Notably, our adapter loses the least performance on new classes as compared to CLIP, while significantly improving over base class performance. We further explore the effect of textual representation adaptation within our Multi-Modal Adapter framework. Our findings demonstrate that the com-bination of both textual and visual representation adaptation achieves superior performance compared to adapting only visual representations.\nWe acknowledge that extensive configuration settings remain unexplored, as our experiments are based on a single Multi-Head attention setting. A more comprehensive hyperparameter search is necessary to fully investigate the capa-bilities of our approach. Future work should also consider the use of positional"}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Additional ablations", "content": "To validate our design choices for the Multi-Modal Adapter, we conduct abla-tions, comparing the use of multi-head attention (MHA) versus the transformer-encoder block, and up-/downsampling using a linear layer versus an MLP. Since we observed no significant differences in performance between these alternatives, we opted to use multi-head attention. For up-/downsampling, we implemented linear layers for downsampling and a 2-layer MLP for upsampling, paralleling the approach used in the CLIP Adapter architecture."}, {"title": "A.2 Noisy training data", "content": "We hypothesized that the Multi-Modal Adapter approach would show robust performance on the original test set when trained on noisy data, due to reduced over-fitting. To validate this hypothesis, we applied Gaussian noise to the train-ing data, as illustrated in Figure 4. We observed that the Multi-Modal Adapter yields robust performance, outperforming the CLIP Baseline and mostly improv-ing over the CLIP Adapter."}]}