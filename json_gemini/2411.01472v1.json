{"title": "Adaptive Domain Learning for Cross-domain Image Denoising", "authors": ["Zian Qian", "Chenyang Qi", "Ka Lung Law", "Hao Fu", "Chenyang Lei", "Qifeng Chen"], "abstract": "Different camera sensors have different noise patterns, and thus an image denoising model trained on one sensor often does not generalize well to a different sensor. One plausible solution is to collect a large dataset for each sensor for training or fine-tuning, which is inevitably time-consuming. To address this cross-domain challenge, we present a novel adaptive domain learning (ADL) scheme for cross-domain RAW image denoising by utilizing existing data from different sensors (source domain) plus a small amount of data from the new sensor (target domain). The ADL training scheme automatically removes the data in the source domain that are harmful to fine-tuning a model for the target domain (some data are harmful as adding them during training lowers the performance due to domain gaps). Also, we introduce a modulation module to adopt sensor-specific information (sensor type and ISO) to understand input data for image denoising. We conduct extensive experiments on public datasets with various smartphone and DSLR cameras, which show our proposed model outperforms prior work on cross-domain image denoising, given a small amount of image data from the target domain sensor.", "sections": [{"title": "Introduction", "content": "Noise generated by electronic sensors in a RAW image is inevitable. Over the past few years, learning- based methods have made significant progress in RAW image denoising [4, 19, 21, 35]. However, building a large-scale real-world dataset with noise-clean pairs for training a denoising model is time-consuming and labor-intensive. It is hard to collect ground truth that is noise-free and has no misalignment with the input noisy data. Moreover, due to the different noise distributions of different sensors (such as read noise and shot noise), the collected data from a particular sensor usually cannot be used to train the denoising model of other sensors, which causes a waste of resources. Therefore, it is important to develop a method to solve this problem.\nExisting solutions to data scarcity in RAW image denoising can be divided into two categories, noise calibration [35, 40, 23] and self-supervise denoising [19, 16, 21, 34, 17]. Noise synthesis and calibration methods first build a noise model, optimize for noise parameters according to a particular camera, and then synthesize training pairs from the noise model to train a network. Self-supervised denoising is designed based on the blindspots schemes. When the input noisy image masks out some pixels and forms a similar but different image from the input, the network learns to denoise instead of identity mapping. Therefore, the network can learn to denoise without pairwise noise-clean data.\nDifferent from prior work, we solve this problem by proposing a cross-domain RAW image denoising method, adaptive domain learning (ADL). Our method can utilize existing RAW image denoising datasets from various sensors (source domains) combined with very little data from a new sensor (target domain) together to train a denoising model for that new sensor. Some data in a source domain may be harmful to fine-tuning a model due to the large domain gap: for instance, synthetic data may be harmful to training a model for real-world applications if the synthetic data imposes unrealistic and unreasonable assumptions. In such cases, our method dynamically evaluates whether a data sample from a source domain is beneficial or harmful by evaluating the performance on a small validation set of the target domain, before and after fine-tuning the model on this data sample. If the performance improves after fine-tuning, we can use this data sample for training; otherwise, we should ignore it. As for the network architecture, we design a modulation network that takes sensor-dependent information as input (sensor types and ISO), which aligns the features from different sensors into the same space and ensembles useful common knowledge for denoising.\nTo evaluate our proposed model with ADL, we compare our model against prior methods on diverse real-world public datasets [2, 35, 4] captured by both smartphone and DLSR cameras. The results demonstrate that our method outperforms the prior work and shows consistent state-of-the-art performance with ADL on RAW data denoising, given a small amount of data in the target domain. We also demonstrate that our ADL can be applied to fine-tuning existing noise calibration models with cross-domain data to further improve its performance.\nThe contributions of this work can be summarized as follows.\n\u2022 We propose a novel adaptive domain learning (ADL) strategy that can train a model with little data from a new sensor (target domain), by automatically leveraging useful information and removing useless data from existing RAW denoising data from other sensors (source domains).\n\u2022 A customized modulation strategy is applied to provide sensor-specific information, which helps our network adapt to different sensors and noise distributions.\n\u2022 Our model outperforms prior methods in cross-domain image denoising in the target domain with little data."}, {"title": "Related Work", "content": "In recent years, methods based on RAW data denoising draw a lot of attention [24, 4, 35, 40, 3, 37, 21, 1, 41, 23]. SID [4] shows that RAW image denoising can perform well with a naive U-net architecture. Besides, they find difficulties in collecting large-scale datasets. Aware that collecting datasets is the research bottleneck, many approaches attempt to synthesize more realistic data. UIP [3] and CycleISP [37] attempt to inverse the image signal processing pipeline and synthesize noise in RAW space to train a RAW denoising framework. However, the generated pseudo-RAW data still has great differences compared to real RAW data. Jin et al. [14] utilize different noise distribution parameters to form a simulation camera to train a network. However, they still need to build a noise model to synthesize data. Another kind of approach is the noise calibration method [35, 40, 23, 38]."}, {"title": "Method", "content": "In this section, we introduce the three steps in our adaptive domain learning pipeline: target domain pretraining, source domain adaptive learning, and target domain fine-tuning. The overall pipeline is illustrated in Figure 1."}, {"title": "Adaptive Domain Learning Algorithm", "content": "Given the small training set of the target domain $T_{adp}$, we first pre-train a model for the target domain by minimizing pixel-wise $L_1$ Loss.\nTarget domain pre-training has benefits in two aspects. First, although the domain gap exists between the source domain and the target domain, denoising is a task that shares similar implicit feature representations. Therefore, pre-training can provide better initialization for the adaptive domain learning stage. Second, there is only very little data from the target domain, and the data from the source domain can be 100 times more than the data in the target domain. Pre-training on the target domain can improve the robustness and ensure the dominant position of the target domain data in the whole training process to prevent our model from overfitting to the source domain."}, {"title": "Source Domain Adaptive Learning", "content": "However, due to the domain gap between the source domain and the target domain, not all the data from the source domain contribute to the training of the target domain, some data might be harmful and will lead to performance reduction. Therefore, we proposed adaptive domain learning (ADL), to eliminate harmful data and make use of the one that has contributions to our model.\nIn each iteration t, we sample a batch of training data $S'$ from some source domain $S^{(i)}$, and adapted our pretrained parameter $\\theta_{t-1}$ to $\\theta'$ by\n$\\theta' \\leftarrow \\theta_{t-1} - \\alpha\\nabla_{\\theta_{t-1}}L(S'),$ (1)\nwhere $\\alpha$ is the learning rate and $L(S')$ is the $L_1$ loss defined on $S'$.\nDynamic validation set To tell whether the data batch $S'$ has contributions to our model, we evaluate the updated parameter $\\theta'$ on a target domain validation set $T_{val}$ that is sampled from the target domain dataset $T_{adp}$. The selection of the validation set in each iteration $t$ is crucial to the performance of our method. Fixed validation set selection for each iteration may make the training stuck in the local minima and easily overfit to the validation set. To avoid these problems from happening, in each iteration t, we randomly sampled a dynamic validation set $V'$ of size k from the target domain dataset $T_{adp}$ to let our model explore the feature space in a stochastic way. On the other hand, the rest of the dataset from $T_{adp}$, denoted as $T_{train}$, will combined with $S'$ to provide the correct direction for the training process. At the beginning of the training, k is set to 20% of the size of $T_{adp}$ and increases during the training process. At the end of the training, 50% of $T_{adp}$ will be used.\nMoreover, inspired by [14], when the size of $T_{adp}$ is extremely small, i.e., smaller than 10, we intentionally select the data that has very diverse system gain from $T_{adp}$ to form $V'$ in each iteration to avoid the over-fitting problem.\nDynamic average PSNR We evaluate whether the data batch $S'$ is useful or not by comparing the PSNR of the result of the updated network parameter $\\theta'$ to the PSNR of the result of previous iteration $\\theta_{t}$ on the sampled validation set $V'$. However, hard criteria based on PSNR usually make the training procedure unstable under the setting of the dynamic validation set. When the size of the dynamic validation set $V'$ is small, the variance of PSNR is large and thus is not that reliable. Some useful data might be removed accidentally. In such cases, we want our model to take a data batch $S'$ as useful data if $S'$ has a trend to improve the performance of our model. We design soft criteria based on PSNR by maintaining a priority queue $Q_{eval}$ of max size M that stores the value of highest PSNR value in the history during the training process. $Q_{eval}$ is ranked by the value of PSNR in ascending order. We denote the PSNR on our dynamic validation set $V'$ of model $\\theta'$ at iteration t in our training process as $Eval(V', \\theta')$. At the beginning of the adaptive domain learning stage, we push $Eval(V', \\theta_{t=0})$ into $Q_{eval}$. During the training, if the PSNR of the updated parameter $\\theta'$ on the dynamic validation set $V'$, $Eval(V', \\theta')$, is higher than the average PSNR in $Q_{eval}$, we keep the updated parameter $\\theta'$ and push $Eval(V', \\theta')$ into $Q_{eval}$ and pop out the first element in $Q_{eval}$ if it is full. Else, we retrieve the network parameter from $\\theta'$ to $\\theta_{t-1}$. This process can be characterized as\n$\\Theta_t =\\begin{cases} \\theta', & Eval(V', \\theta') > \\frac{1}{m}\\sum_{i=1}^{m} Q^{eval}_{i}\\\\ \\theta_{t-1}, & otherwise \\end{cases}$ (2)"}, {"title": "Channel-wise Modulation Network", "content": "To let our network better utilize the information from sensors that have different noise distributions, we need to adjust the feature space of different inputs. For a RAW data D captured by a CMOS sensor, we can model its noise by:\n$N = I + KN_{dep} + N_{indep},$ (3)\nwhere K is the system gain, $N_{dep}$ is the signal dependent noise and $N_{indep}$ is the signal independent noise. Based on this modeling, we propose a channel-wise modulation network to adjust the feature space by embedding two easy-to-access parameters, the sensor type and the ISO in our network. ISO is proportional to system gain K, while the sensor type can help the network know how to utilize the ISO to learn the signal-dependent noise $N_{dep}$ and recognize the signal-independent noise $N_{indep}$.\nGiven the one-hot encoding of the sensor type $p \\in \\mathbb{R}^{1\\times n}$ and the corresponding ISO $s \\in \\mathbb{R}^{1\\times n}$ (duplicate n times in the vector), our channel-wise modulation layer transfers the concatenated metadata (p, s) into a channel-wise scale $\\gamma$ and shifts $\\beta$ by\n$\\gamma = 1 + tanh(MLP(p, s)),$ (4)\n$\\beta = MLP(p, s),$ (5)\nwhere $\\gamma, \\beta \\in \\mathbb{R}^{1\\times C}$, and $MLP_{\\beta}$ and $MLP_{\\gamma}$ are two four layer Multi-Layer Perceptrons. Let the feature map of the i-th convolution layer be $F_i \\in \\mathbb{R}^{H\\times W \\times C'}$, we embed the sensor-specific data to $F_i$ by a channel-wise linear combination by\n$F'_i = \\gamma \\times F_i + \\beta.$ (6)\nNote that the type of the input metadata of our channel-wise modulation strategy is not fixed. The input concatenated vector can be extended as long as more meta information is provided along with the data."}, {"title": "Experiments", "content": "We evaluate the performance of our ADL and modulation on the dataset captured by smartphones in normal light conditions (SIDD dataset [2]), and the dataset captured by DLSR cameras in extremely low light conditions (ELD [35] and SID [4] dataset). Compared to RAW data captured by smartphones, RAW data captured by DSLR cameras in extremely low light environments is more difficult to denoise because the noise distribution is more complicated and the noise level is larger. Moreover, the domain gap between the RAW data captured by different DSLR cameras is larger than the domain gap between the RAW data captured by smartphones.\nSIDD [2] is a popular RAW denoising dataset that contains 160 pairs of noisy and ground-truth RAW data from 5 different smartphone cameras (G4, GP, IP, N6, S6) of different scenes. ELD Dataset [35] contains RAW data captured by 3 different brands of DSLR cameras (Nikon, Canon, Sony) with different ISO and light factors, while the SID dataset captured RAW data using 2 different brands of DSLR cameras (Sony and Fuji). Note that the ELD dataset and SID dataset [4] are using the same DSLR camera (Sony A7S2). We only use the data captured by this camera from SID dataset to keep the domain gap between each set of domains. Besides, the input RAW data of the ELD and SID datasets are captured in extremely low light environments, while the ground truth is captured in normal light conditions. We follow the training strategy in [4] by multiplying a light factor by the input RAW data to keep the input RAW data and ground-truth RAW data in the same space.\nBaselines and training settings. To evaluate the performance of our framework, we compare our method against several baselines: the fundamental baselines pre-train and then fine-tune. Self-supervise denoising methods: Blind2unblind [34], ZS-N2N [22], and DIP [31]. Meta transfer learning method MZSR [29], Prabhakar et al. [28] and Kim et al. [15] (denoted as transfer learning). Traditional approach BM3D [6]. Calibration Free Method Led [14]. Note that we try our best to find all possible work that has the same goal as our method for the comprehensive baseline comparisons. Although the approach might be different, we make the experiment as fair as possible with proper settings.\nWe compare the performance of our method against the above baselines by cross-validation on each sensor of all three datasets. In each experiment, we take one sensor as the target domain to represent the sensor with a very small number of data (around 20 pairs of data). The data from all other sensors will form the source domain, which represents the existing dataset. For the baselines, we only use the data from the target domain for the training of all self-supervised denoising methods, no data from the source domain is used since cross-domain data will reduce the overall performance of these methods. For fine-tuning, we first pre-train the model with the data from the source domain using U-net, then use the data from the target domain for fine-tuning. For DIP [31], the model is trained on"}, {"title": "Results on Real Data", "content": "We quantitatively evaluate our method by comparing the PSNR against other baselines on the smartphone dataset SIDD and DSLR camera dataset ELD and SID. Note that we only report PSNR [9] because there are no other systematic evaluation metrics designed for RAW data. Other popular evaluation metrics like LPIPS [39] is not suitable for RAW data. We also present the SSIM [9] metrics in the supplementary material. The result is illustrated in Table 1. Here, \"fine-tuning\u201d denotes the experiment training on the source domain and fine-tuning on the target domain. The camera name on the top row means that we take this sensor as the target domain while keeping the other sensors as the source domain. For example, \"G4\" in Table 1 means that this experiment takes G4 as the target domain and all data from the other four sensors, GP, IP, N6, and S6 will form the source domain dataset. From the table, we can see that our proposed method has the best performance on both the smartphone dataset and the DSLR dataset. To be specific, our method has 0.71dB and 0.39dB performance gains on average compared to MZSR [29] and transfer learning [15] baseline. The PSNR values in the table of the ELD and SID dataset are much lower than those in the table of the SIDD dataset because the ELD and SID datasets are captured in extremely low light environments: the noise level is higher, and the scenes are much more complicated than the SIDD dataset. Note that although the AIN module in Kim et al. [15] has a similar function and architecture to our modulation strategy, they can only estimate the noise level, which is very limited when the source domain contains data from many sensors. Our modulation strategy is more extensible and can somehow provide more hyper information(if provided in the dataset) and can let the network know the difference between the sensors.\nQualitative Result Since the PSNR of the RAW denoising is relatively high and difficult to tell the difference from the human visual perspective, We evaluate our method qualitatively by comparing the error map against all three baselines. As illustrated in Figure 2, since the RAW data is hard to"}, {"title": "Analysis of Calibration-related Methods", "content": "Noise calibration methods Although the noise calibration method is powerful, the calibrated model still does not include out-of-model noise such as fixed-pattern noise. Thus, fine-tuning using real data can further improve the performance of the model trained by those synthetic data. However, when the real data used for fine-tuning is scarce, the improvement is usually marginal. In such cases, we may want to utilize more data from different domains to further improve the result. In this section, we analyze the performance of applying our ADL to fine-tune the existing noise calibration model with data from multi-domain. We utilize the noise calibration method proposed by Zhang et al. [40]. The result is illustrated in Table 2 (a). Here \u201cSingle\u201d means that only the data from that corresponding sensor is used for fine-tuning, while \"Multiple\" means that the data from all sensors in that dataset is used for fine-tuning. The result shows that when utilizing limited data from a single domain, the improvement of both naive fine-tuning and ADL is marginal. When we use more data from multiple domains, naive fine-tuning cannot learn useful information from various domains and thus leads to a drop in the final performance. However, our ADL can remove the harmful data, and ensemble the useful information from the different domains to help the training.\nCalibration-free methods Different from the noise calibration method, the calibration-free method Led [14] embed noise distribution from the simulated camera into the pre-train model. Although LED is a calibration-free method, the network also has no prior knowledge of the noise distribution of the target domain data during the pre-train stage. The large intensity distribution gap between the simulation cameras and the test set will lead to low performance. However, our ADL has prior knowledge of the target domain noise intensity distribution throughout the training process, which can gain similar robustness to the calibration method. As illustrated in Table 2 (b), we replace the synthetic data from the well-calibrated simulated camera in Led [14] with the data from the source domain(data from existing sensors), which is the same as our method in the experiments. Our method can outperform Led [14] in this case. This is because the performance of Led [14] highly depends on the prior knowledge of noise distribution learning from the simulated camera. Our method will not use data with a huge gap in intensity distribution between the source domain and the target domain.\nPMN [7] PMN [7] aims to overcome the bottleneck of the learnability in real RAW denoising by reforming the data. It can be generalized and applied to all real RAW image denoising methods and improve their performance including our ADL. As illustrated in Table 3, the performance of our ADL can be improved by applying the training strategy proposed in PMN."}, {"title": "Analysis on Useful and Harmful Data", "content": "The domain gap in the deep learning model of other tasks is usually caused by the scene between the training data and test data. As for RAW image denoising, the difference in noise intensity among different sensors is more crucial. For example, if the noise intensity of two sensors is the same, then the noise model trained on one sensor can be generalized to the other. On the contrary, the model will fail if the noise intensity between these two sensors is very different. Based on this observation, we can say that the source domain data with similar noise intensity to the target domain data is useful, while the data with a large noise intensity difference is harmful. However, in cross-domain training, the harmful data has a more negative impact, because in cross-domain training, the model will tend to compromise different domains to reach the global minimum. As illustrated in Table 5, we utilize the target domain data from the ELD dataset as the base set, and we build two Harmful datasets. Here \"Harmfull\" is synthesized by using the ground truth of the SIDD dataset that is captured in bright light conditions and naive Gaussian noise with noise level \"harmful\" $\\sigma$ = 30, and \"Harmful2\" set is the data pairs that have mis-alignment. For example, the input and ground truth are from different scenes, or the ground truth is black. In these cases, even though we add more data, the performance still drops. However, our ADL ignores the harmful data and always optimizes the model towards the noise intensity of the target domain."}, {"title": "Ablation Study", "content": "ADL and Modulation Strategy We conduct an ablation study on the effectiveness of our ADL and modulation strategy. The ablation study is conducted on SID and ELD datasets with the same training settings and configuration as in the previous section. We ablate over the strategies of our method by training models without applying the target domain pretraining, source domain adaptive learning, and modulation(including sensor type and ISO modulation). As illustrated in Table 4, the result demonstrated that the target domain pretraining, source domain adaptive learning, ISO and sensor type modulation, and dynamic validation set all contribute to the final result. The source domain adaptive learning, which automatically evaluates whether data from the source domain is harmful or not, is the most crucial strategy in our framework."}, {"title": "Conclusion", "content": "We have proposed a novel adaptive domain learning (ADL) scheme for cross-domain image denoising. We leverage the data from other sensors to help the training of the data from new sensors in a smart fashion: ADL removes harmful data and utilizes useful data from the source domain to improve the performance in the target domain. Our proposed modulation strategy provides extra camera-specific information, which helps differentiate the noise patterns of input data. We evaluate our method on smartphone and DSLR camera datasets, and the results demonstrate that our method outperforms state-of-the-art approaches in cross-domain image denoising. Moreover, we show that ADL can also be easily extended to image deblurring. We believe ADL is general and can be generalized to other cross-domain tasks, which can be further explored in the future."}]}