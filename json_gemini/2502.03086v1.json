{"title": "Implementing Large Quantum Boltzmann Machines\nas Generative AI Models for Dataset Balancing", "authors": ["Salvatore Sinno", "Markus Bertl", "Arati Sahoo", "Bhavika Bhalgamiya", "Thomas Gro\u00df", "Nicholas Chancellor"], "abstract": "This study explores the implementation of large\nQuantum Restricted Boltzmann Machines (QRBMs), a key ad-\nvancement in Quantum Machine Learning (QML), as generative\nmodels on D-Wave's Pegasus quantum hardware to address\ndataset imbalance in Intrusion Detection Systems (IDS). By\nleveraging Pegasus's enhanced connectivity and computational\ncapabilities, a QRBM with 120 visible and 120 hidden units\nwas successfully embedded, surpassing the limitations of default\nembedding tools. The QRBM synthesized over 1.6 million attack\nsamples, achieving a balanced dataset of over 4.2 million records.\nComparative evaluations with traditional balancing methods,\nsuch as SMOTE and RandomOversampler, revealed that QRBMS\nproduced higher-quality synthetic samples, significantly improv-\ning detection rates, precision, recall, and F\u2081 score across diverse\nclassifiers. The study underscores the scalability and efficiency\nof QRBMs, completing balancing tasks in milliseconds. These\nfindings highlight the transformative potential of QML and\nQRBMs as next-generation tools in data preprocessing, offering\nrobust solutions for complex computational challenges in modern\ninformation systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Balanced datasets have long been recognized as essential for\neffective machine learning (ML), enabling models to general-\nize and produce robust predictions. In the context of intrusion\ndetection systems (IDS), the importance of balanced datasets\nbecomes even more pronounced. IDS are critical for securing\nnetworks by identifying malicious activities amidst normal\ntraffic. However, real-world datasets used for training IDS have\noften suffered from severe class imbalance, where the volume\nof normal traffic significantly outweighed that of intrusion\nattempts. This imbalance has limited the effectiveness of ML\nmodels, resulting in biased predictions favouring the majority\nclass, reduced detection rates for rare but critical intrusions,\nand increased false positive rates.\nThe challenges posed by class imbalance were multifaceted.\nML models trained on imbalanced datasets tended to opti-\nmize for the majority class, neglecting the minority class.\nTraditional balancing methods, such as oversampling and\nundersampling or Synthetic Minority Oversampling Technique\n(SMOTE) [7], have significant drawbacks. Oversampling in-\ntroduced redundancy and increased the risk of overfitting;\nundersampling often discarded valuable information from the\nmajority class. SMOTE can not handle class boundaries effec-\ntively and has high-dimensional or sequential data challenges.\nThese limitations underscored the need for novel approaches\nto address class imbalance in IDS datasets.\nThis study leveraged the potential of quantum computing\nto address these challenges, focusing specifically on quantum-\nrestricted Boltzmann Machines (QRBMs). QRBMs, as gen-\nerative models, can learn the statistical properties of datasets\nand synthesise realistic data samples. By utilising principles of\nquantum mechanics, QRBMs could efficiently explore high-\ndimensional data spaces, making them particularly suited\nfor generating synthetic data to balance imbalanced datasets.\nUnlike classical methods, QRBMs exploited quantum super-\nposition and entanglement to represent complex probability\ndistributions more effectively, offering significant advantages\nin addressing class imbalance.\nThe results and contributions of the study are summarized\nas follows:\n\u2022 Largest QRBM Implementation: Successfully imple-\nmented a large QRBM (120 visible and 120 hidden\nunits) on D-Wave's Pegasus architecture, overcoming the\nlimitations of D-Wave's default embedding tools. This\nimplementation highlights the potential of QRBMs to act\nas powerful generative models for balancing even the\nmost complex datasets.\n\u2022 Generative Modeling for IDS: Demonstrated the ef-\nficacy of QRBMs in generating high-quality synthetic\ndata, achieving superior precision, recall, and F\u2081 scores\ncompared to traditional methods.\n\u2022 Improved IDS Performance: Evaluated the impact of\nQRBM-generated datasets on IDS performance metrics.\nResults showed significant improvements in detection\nrates and reductions in false positives, demonstrating\nQRBM's ability to enhance IDS reliability and robust-\nness."}, {"title": "II. RELATED WORK", "content": "Class imbalance is a persistent challenge in ML, particularly\nin fields like IDS, where the normal traffic volume far exceeds\nintrusion events. Several classical methods have been proposed\nto address this imbalance, including SMOTE and Random\nOversampling. SMOTE generates synthetic samples by in-\nterpolating between existing minority class samples, while\nRandom Oversampling duplicates minority class samples to\nachieve balance [7], [12]. Although these methods improve\ndataset balance, they often lead to issues such as overfitting\nand redundancy, limiting their effectiveness in real-world\nscenarios [10] [19].\nPrevious studies, such as those by Gopalan et al. [11]\nand Abdulrahman et al. [1], emphasize balancing techniques\nfor IDS datasets, highlighting the challenges of achieving\nfair evaluation metrics. Abdulrahman et al. [1] investigate\nbalancing the CICIDS2017 dataset using classical techniques,\nwhich, while effective to some extent, often fail to capture\nnuanced feature interactions within the data. Liu et al. [18]\npropose using Generative Adversarial Networks (GANs) for\ndata balancing, showcasing their potential in generating robust\ndatasets for IDS tasks.\nQRBMs have emerged as a promising alternative to classical\nmethods for handling imbalanced datasets. QRBMs, an ex-\ntension of classical Restricted Boltzmann Machines (RBMs),\nleverage the principles of quantum computing to model com-\nplex probability distributions and generate synthetic data.\nUnlike classical RBMs, QRBMs utilize quantum annealing to\nefficiently sample from high-dimensional energy landscapes,\nmaking them particularly well-suited for generative tasks [3],\n[15]. Prior research has demonstrated the potential of QRBMS\nin various machine-learning applications, including anomaly\ndetection and classification tasks [3], [9].\nImplementing QRBMs on quantum hardware poses unique\nchallenges, particularly concerning embedding large models\non physical quantum processors. D-Wave's Chimera topology,\nan early quantum annealing architecture, provided limited\nqubit connectivity, which constrained the scalability of QRBM\nimplementations [22]. The introduction of D-Wave's Pegasus\ntopology addressed these limitations by offering enhanced\nqubit connectivity and increased computational capacity [16].\nRecent studies have explored methods for minor embedding\nof problems on Pegasus, showcasing significant improvements\nin performance and scalability [8], [20].\nThe proposed algorithm demonstrates remarkable flexibility\nand efficiency in embedding RBMs on D-Wave's Pegasus\narchitecture. It enables the minor embedding up to a 172x120\nRBM by optimizing chain lengths to remain short while\nmaximizing the utilization of qubits for visible and hidden\nnodes.\nThis algorithm outperforms D-Wave's default embedding\ntool. The ability to efficiently embed larger RBMs under-\nscores the algorithm's potential to enhance quantum annealing\napplications, particularly in generative modelling and data\nbalancing tasks."}, {"title": "III. RESTRICTED BOLTZMANN MACHINES", "content": "RBMs are stochastic neural networks that model the joint\nprobability distribution of visible and hidden variables. They\nare energy-based models that learn to represent data by\nminimizing an energy function. RBMs consist of two layers:\na visible layer v representing observed data and a hidden\nlayer h that captures latent features. There are no intra-\nlayer connections, simplifying the computation and making\nthe model tractable [15].\nThe energy of a configuration (v,h) of the visible and\nhidden units in an RBM is defined as:\n$E(v,h) = - \\sum_i b_i v_i - \\sum_j C_j h_j - \\sum_{i,j} V_i W_{ij}h_j$, (1)\n\\text{visible} \\text{hidden}\nwhere $b_i$ and $c_j$ are biases for the visible and hidden units,\nrespectively, and $W_{ij}$ represents the weights of the connection\nbetween visible unit i and hidden unit j. This energy function\ndetermines the likelihood of a given configuration; lower\nenergy corresponds to higher probability.\nThe joint probability of a visible vector v and a hidden\nvector h is expressed using the Boltzmann distribution:\n$P(v, h) = \\frac{\\exp(-E(v,h))}{Z}$  (2)\nwhere Z is the partition function defined as:\n$Z = \\sum_{v,h} \\exp(-E(v,h)).$ (3)\nThe partition function normalizes the probability distribution,\nensuring that P(v, h) is valid."}, {"title": "A. Conditional Independence and Sampling", "content": "One of the key features of RBMs is that the visible and\nhidden units are conditionally independent of each other. This\nallows efficient sampling from the distribution:\n$P(h_j = 1|v) = \\sigma \\left( c_j + \\sum_i W_{ij}v_i \\right),$ (4)\n$P(V_i = 1|h) = \\sigma \\left( b_i + \\sum_j W_{ij}h_j \\right)$ (5)\nwhere $\\sigma(x)$ is the sigmoid function $\\sigma(x) = 1/(1+e^{-x})$. This\nproperty enables Gibbs sampling for training and inference.\nRBMs are trained by maximizing the likelihood of the\nobserved data v. Direct computation of the log-likelihood\ngradient is intractable due to the partition function Z. To\novercome this, Contrastive Divergence (CD) is used as an\napproximation [13]. The update rule for the weights is given\nby:\n$\\Delta W_{ij} = (V_i h_j)_{data} - (V_i h_j)_{model},$  (6)\nwhere $\\langle \\cdot \\rangle_{data}$ represents expectations under the data distri-\nbution, and $\\langle \\cdot \\rangle_{model}$ represents expectations under the model\ndistribution."}, {"title": "B. RBMs as Generative Models", "content": "RBMs can be used as generative models as they learn and\nreplicate a dataset's statistical properties. Once trained, an\nRBM can generate new data samples by alternately sampling\nfrom the visible and hidden layers in a process known as\nGibbs sampling [6]. This iterative sampling process alternates\nbetween activating the hidden units based on the visible units\nand vice versa, gradually producing new visible configurations\nthat align with the learned data distribution.\nThis generative capability makes RBMs a powerful tool\nfor several applications. One of their primary uses is in\ndataset augmentation, where new data samples are synthesized\nto address challenges like limited data availability or class\nimbalance. For example, in highly imbalanced datasets, an\nRBM can generate synthetic samples for under-represented\nclasses, effectively increasing their prevalence and improving\nthe performance of downstream supervised learning models\n[14]. Unlike oversampling techniques that merely duplicate\nexisting data points, RBMs generate entirely new samples that\npreserve the statistical diversity of the dataset, reducing the\nrisk of overfitting."}, {"title": "C. Quantum Extensions", "content": "In quantum computing, the concept of generative modelling\nwith RBMs has been extended to QRBMs. QRBMs enhance\nthe generative capabilities of RBMs by leveraging quantum\neffects such as superposition and entanglement, enabling them\nto sample from high-dimensional and complex probability\ndistributions efficiently. These properties make QRBMs par-\nticularly suitable for generating synthetic data in large-scale,\ncomplex applications such as anomaly detection and advanced\ngenerative tasks [3].\nA key aspect of implementing QRBMs lies in mapping\nthe RBM energy function to the Ising model [5], a well-\nstudied model in quantum mechanics and statistical physics.\nAn energy function defines the Ising model:\n$H = -\\sum_i h_i s_i - \\sum_{i<j} J_{ij} s_i s_j,$ (7)\nwhere $s_i \\in \\{-1,+1\\}$ are spin variables, $h_i$ are local biases,\nand $J_{ij}$ are coupling coefficients between spins. The Ising\nmodel is the basis for quantum annealers, such as those\ndeveloped by D-Wave Systems. The RBM energy function\n(Equation 1) can be reformulated into an equivalent Ising\nHamiltonian by encoding the visible and hidden units as spin\nvariables, their biases as local fields, and their weights as\ncouplings between spins. This mapping enables the direct\nimplementation of RBM-like models on quantum hardware\n[2].\nQuantum annealing trains QRBM by finding low-energy\nconfigurations of the Ising Hamiltonian, which correspond\nto the most probable configurations in the RBM's probabil-\nity distribution. This is particularly advantageous in high-\ndimensional and multimodal distributions, where classical\nmethods often struggle due to local minima and high com-\nputational costs. By leveraging tunnelling, quantum annealers\ncan escape local minima and explore the solution space more\neffectively. In this way, QRBMs are particularly suitable for\ngenerating synthetic data in applications like class imbal-\nance correction, where they outperform classical oversampling\ntechniques by better preserving the statistical properties of\nminority classes [4]. Advanced quantum architectures, such as\nD-Wave's Pegasus topology, have enabled QRBMs to scale to\nlarger problem sizes. The Pegasus topology provides improved\nqubit connectivity, allowing for more efficient embeddings\nof the Ising Hamiltonian corresponding to large RBMs. This\nadvancement has opened new possibilities for tackling real-\nworld problems, such as optimizing supply chains, enhancing\ncybersecurity systems, and advancing generative modelling for\nmachine learning applications."}, {"title": "IV. METHODOLOGY", "content": "Fig. 1 illustrates the process for addressing the challenge of\ndataset imbalance using a QRBM. The workflow begins with\nthe initial dataset, divided into two key components: training\nand testing data.\nThe training data is split into attack datasets, and then the\nattack dataset undergoes a binarization process, converting the\ndata into a binary format suitable for analysis by the QRBM.\nThe QRBM is trained on the binarized attack data to generate\nsynthetic data, which mirrors the underlying distribution and\ncharacteristics of the original attack data.\nCombining synthetic data with the original training data\nproduces a balanced dataset, ensuring that classes or data cat-\negories are equally represented. This newly balanced dataset\nis then used to train classifiers to identify patterns and make\npredictions based on the input data.\nOnce the classifiers are trained, their performance is evalu-\nated using the testing data. The predictions generated by the\nclassifiers are compared against the actual outcomes in the\ntesting data to assess the model's accuracy, reliability, and\noverall effectiveness. The final step involves a comprehensive\nanalysis of the prediction results, which provides valuable\ninsights into the success of the balancing process and the\nperformance of the classifiers."}, {"title": "A. Dataset Preparation", "content": "The study utilized the CICIDS2017 dataset, a comprehen-\nsive benchmark dataset developed by the Canadian Institute\nfor Cybersecurity. CICIDS2017 is widely recognized for its\nrelevance in evaluating IDS due to its diversity and realistic\nsimulation of modern network traffic. The dataset captures a\nbalanced representation of regular activity and various attack\nscenarios, providing a robust foundation for developing and\ntesting advanced machine learning and quantum-based models\n[21].\nCICIDS2017 was created to address the shortcomings of\nearlier datasets, such as outdated attack types, lack of feature\ndiversity, and unrealistic traffic generation. It includes over 3\nmillion network flow records collected over 5 days, encom-\npassing a mix of benign and malicious traffic. The malicious\nactivities are categorized into six distinct attack types. The"}, {"title": "B. QRBM Implementation on Pegasus Topology", "content": "The Pegasus topology provides advanced qubit connectivity,\nwith each qubit connecting to up to 15 others, making it ideal\nfor embedding larger and more complex QRBMs \u00b9. This study\ndeveloped an algorithm to implement large QRBMs (up to\n172x120) on the Pegasus topology.\nOur embedding algorithm (Algorithm 1) maps the visible\nand hidden units of the QRBM to physical qubits. It allows"}, {"title": "C. Experimental Setup", "content": "The experiments were conducted using a hybrid setup.\nA Dell R705X server with 128 logical cores and 503 GiB\nof RAM was utilized to perform classical machine learn-\ning tasks. The server ran Ubuntu 22.04 and Python 3.10.9\nwith Scikit-learn 1.5.2, and minorminer 0.2.16. SMOTE and\nRandomOverSampler were implemented to address the class\nimbalance. The QRBM balanced the datasets by generating\n1,000 samples of attack class data in each anneal, with D-\nWave's default annealing time of 20\u00b5s and 124\u00b5s for readout.\nThe evaluation metrics adopted were precision, recall, and\nF\u2081 scores, calculated across several classifiers: Support Vector\nMachines (SVM), Logistic Regression, Na\u00efve Bayes, Decision\nTrees, Gradient Boosting, K-Nearest Neighbors (KNN), and\nRandom Forest.\n1) Data Preprocessing: The preprocessing involved con-\nverting the dataset into a binary format compatible with the\nQRBM. Steps included normalization, encoding, and feature\ntransformation to enhance the data's quality and compatibility.\nHandling missing values was crucial in preprocessing the\ndataset, as unaddressed gaps could lead to biased or inac-\ncurate models. Missing values in the dataset were addressed\nusing mean and median imputation techniques, with invalid\nor incorrect entries removed to improve data integrity. In\ntotal, 1,305 NaN values and 1,310 infinite values were iden-\ntified and eliminated from the dataset. A correlation analysis\nwas conducted to identify features with strong associations\nto address redundancy. Highly correlated features (absolute\ncorrelation \u2265 0.9) and features with zero standard deviation\nwere deemed redundant and removed. In total, 23 features\nwith high correlations and eight with zero variance were\nremoved, leaving 48 meaningful features. Duplicate records\nwere identified and removed to prevent bias in the models.\n307,078 duplicate entries were removed, leaving a final dataset\nof 2,524,847 records. This included 2,104,309 benign samples\nand 420,538 attack samples (Table IV).\nKey features were extracted based on their significance in\nintrusion detection tasks and encoded into binary form using\nequation 8:\n$Max(feature_value) \u2013 Min(feature_value) < 2^N$ (8)\nwhere N is the number of bits required for encoding.\nAt the end of this process, we created a 120-bit vector, and\nwe selected a QRBM with 120 visible and 120 hidden units."}, {"title": "D. Balancing the Dataset using the QRBM", "content": "The QRBM was trained using Algorithm 2, adapted from\n[9], and the resulting parameters (W,b,c) were utilized in the\nembedding process via Algorithm 1. Each quantum annealing\nstep produced 1,000 samples in milliseconds, with the visible\nunit states forming individual entries in the synthetic dataset.\nRepeating this process 1,700 times resulted in 1,683,771\nsynthetic attack samples, creating a perfectly balanced dataset\nof 4,208,618 samples. With each annealing process lasting\nonly 20\u00b5s, we rapidly generated high-quality synthetic data."}, {"title": "V. RESULTS", "content": "The results are tabulated and compared for four types of\ndatasets: without balancing, balancing using SMOTE, Rando-\nmOversampler, and QRBM.\nTable V shows the time taken to balance the dataset using\nthree different models: SMOTE, Random Over Sampler and\nQRBM. The SMOTE and Random Over Sampler methods\ntook a relatively short time (0.75 and 0.23 seconds, respec-\ntively), and only 0.33 seconds for the QRBM.\nTable VI shows the results of the evaluation metrics obtained\nwhen the dataset was trained without balancing the dataset.\nTable VII shows the results of the evaluation metrics\nobtained when the dataset was trained and modelled after\nbalancing it using SMOTE.\nTable VIII shows the results of the evaluation metrics obtained"}, {"title": "VI. DISCUSSION", "content": "QRBMs achieved superior performance compared to clas-\nsical methods, such as SMOTE and RandomOverSampler,\nby generating high-quality synthetic samples that improved\nevaluation metrics like precision, recall, and F\u2081 scores across\nmultiple ML models. Furthermore, QRBMs demonstrated re-\nmarkable time efficiency, completing the balancing process in\njust 0.33 seconds, as shown in Table V.\nQRBMs exhibit clear advantages over classical methods\nsuch as SMOTE and RandomOverSampler by efficiently gen-\nerating high-quality synthetic data using quantum annealing\nto model complex interactions. Unlike statistical approaches,\nQRBMS capture probabilistic dependencies across features, al-\nlowing for more nuanced sample generation. This capability is\nparticularly critical in IDS applications, where the distribution\nof attack vectors is intricate and highly non-uniform, requiring\nsophisticated modelling techniques to ensure effective data\nbalancing and robust detection.\nThe comparison of QRBMs with deep learning-based gen-\nerative models, such as GANs, further highlights their unique\nadvantages. While GANs can learn from data distributions and\ngenerate high-quality synthetic samples, they face challenges\nlike mode collapse and demand significant computational\nresources, particularly for large-scale datasets. In contrast,\nQRBMs leverage quantum hardware to model energy land-\nscapes directly, offering rapid sampling and scalability. Unlike\nGANs, QRBMs circumvent extensive training and hyperpa-\nrameter tuning, making them a more efficient choice in specific\nscenarios. However, QRBMs have limitations, as quantum\nhardware restricts scalability due to limited qubit counts\nand noise. Additionally, hybrid quantum-classical workflows\nintroduce complexities in preprocessing and postprocessing,\nrequiring further advancements in hardware and algorithms\nto realize their full potential. Future research should include\ndirect comparisons between QRBMs and GANs, focusing on\ncomputational efficiency, sample diversity, and downstream\nIDS performance.\nQPU sampling time (134.92 \u00b5s) and readout time (94 \u03bc\u03b5)\nillustrate QRBM's capability to generate synthetic samples\nrapidly. Unlike classical methods, QRBMs maintain consistent\nruntimes across varying dataset sizes, making them well-suited\nfor large-scale data preprocessing tasks.\nQRBMs face challenges due to the limitations of current\nquantum hardware. Issues such as limited qubit connectiv-\nity, short coherence times, and error rates can impact their\nperformance. Moreover, QRBM workflows' hybrid nature,\ncombining quantum and classical computations, introduces\nadditional data preprocessing and postprocessing complexity.\nThe introduction of the Zephyr topology, with its enhanced\nqubit connectivity and density, has the potential to mitigate\nmany of these hardware limitations, enabling the embedding\nof larger and more complex QRBMs while reducing chain\nbreaks and improving sampling quality. Additionally, Zephyr's\nimproved scalability and efficiency could streamline hybrid\nquantum-classical workflows by simplifying preprocessing and\nembedding steps, further unlocking the potential of QRBMs\nin real-world applications.\nFuture research should focus on overcoming these chal-\nlenges through advancements in quantum hardware and opti-\nmization techniques, in particular, developing more efficient\nembedding strategies to scale QRBM implementations for\neven larger and more complex datasets, exploring alternative\nquantum architectures and algorithms to optimize QRBM per-\nformance further and expanding the application of QRBMs in\ndomains like healthcare, finance, and other real-time anomaly\ndetection fields."}, {"title": "VII. CONCLUSION", "content": "This study has demonstrated the potential of quantum-\nrestricted Boltzmann Machines (QRBMs) as a powerful tool\nfor addressing dataset imbalances e.g., in intrusion detection\nsystems (IDS). By leveraging the advanced capabilities of the\nPegasus architecture, we successfully implemented one of the\nlargest QRBMs embedded on D-Wave's quantum processor\nto date, with 120 visible and 120 hidden units. Notably,\nthis scale of QRBM exceeds the capabilities of D-Wave's\ndefault embedding tools, highlighting the significance of this\nachievement in generative modelling and quantum computing.\nQRBMs generated high-quality synthetic data, significantly\nimproving IDS performance across multiple metrics, including\nprecision, recall, and F\u2081 scores. Compared to classical tech-\nniques such as SMOTE and RandomOverSampler, QRBMs\nprovided superior results while maintaining remarkable com-\nputational efficiency. QRBMs' generative modelling capabil-\nity, supported by the enhanced connectivity and flexibility of\nthe Pegasus topology, enabled efficient sampling from complex\nenergy landscapes, overcoming the limitations of classical\napproaches like Gibbs sampling.\nWe therefore contribute by demonstrate the practical fea-\nsibility of large-scale QRBM implementations, pushing the\nboundaries of what can be achieved on existing quantum\nhardware."}, {"title": "APPENDIX", "content": "A. Algorithms"}, {"title": "Algorithm 1 Embedding a QRBM on Pegasus", "content": "1: Input: n_visible, n_hidden, periodicity_v,\nperiodicity_h, n_periodicity\n2: Output: Logical-physical mappings for visible and\nhidden nodes, coupling matrix J\n3: H_V \u2190 n_visible/n_periodicity\n4: H_H \u2190 n_hidden/n_periodicity\n5: Initialize J_coupling, visible_nodes, hidden_nodes,\nand J_connections as empty lists\n6: startv\u2190 periodicity_v\n7: for 0 to H_V-1 do\n8: for x 0 ton_periodicity - 1 do\n9: n \u2190 startv + (n_periodicity \u00b7 x) + (z \u00b7 periodicity_v)\n10: Append n to visible_nodes\n11: for j 0 to H_H - 2 do\n12: Append (n + j, n + j + 1) to J_coupling\n13: end for\n14: end for\n15: end for\n16: starto \u2190 periodicity_h\n17: for 0 to H_H- 1 do\n18: for x 0 ton_periodicity - 1 do\n19:\np \u2190 starto + (n_periodicity \u00b7 x) + (z \u00b7 periodicity_h)\n20: Append p to hidden_nodes\n21: for j 0 to H_V - 2 do\n22: Append (p + j, p + j + 1) to J_coupling\n23: end for\n24: end for\n25: end for\n26: for x0 to H_H-1 do\n27: for y\u2190 0 to H_V - 1 do\n28: for t 0 ton_periodicity - 1 do\n29:\nn \u2190 startv + (n_periodicity\u00b7t) + y\u00b7 periodicity_v + x\n30: for k\u2190 0 to n_periodicity \u2013 1 do\n31:\np\u2190 starto +(n_periodicity\u00b7k) + x \u00b7 periodicity_h + y\n32: Append (n,p) to J_connections\n33: end for\n34: end for\n35: end for\n36: end for\n37: J0\n38: for Each (a, b) in J_coupling do\n39: J[a, b] -1\n40: end for\n41: return J, visible_nodes, hidden_nodes, J_coupling,\nJ_connections"}, {"title": "Algorithm 2 Training of a QRBM using quantum annealing\n(based on [9])", "content": "1: \u0454 \u2190 learning rate\n2: b, c, W\u2190 random number\n3: while not converged do\n4: Sample a mini-batch of m examples {x(1), ..., x(m)}\nfrom the training set\n5: V \u2190 {x(1), ..., x(m) }\n6: \u0397 \u2190 \u03c3(c + VW)\n7: {h, J} {b, c, W}\n8: (V', H') \u2190 quantum annealing(h, J)\n9: W + W + \u20ac  10:\nb+b+e(sum(V)  11:\ncc + E m1\n12: end while)m} \n1VTH \nsum(H)m (1sum(V')) \nsum(H')m1"}]}