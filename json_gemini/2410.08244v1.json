{"title": "RAB2-DEF: DYNAMIC AND EXPLAINABLE DEFENSE AGAINST ADVERSARIAL ATTACKS IN FEDERATED LEARNING TO FAIR POOR CLIENTS", "authors": ["Nuria Rodr\u00edguez-Barroso", "M. Victoria Luz\u00f3n", "Francisco Herrera"], "abstract": "At the same time that artificial intelligence is becoming popular, concern and the need for regulation is growing, including among other requirements the data privacy. In this context, Federated Learning is proposed as a solution to data privacy concerns derived from different source data scenarios due to its distributed learning. The defense mechanisms proposed in literature are just focused on defending against adversarial attacks and the performance, leaving aside other important qualities such as explainability, fairness to poor quality clients, dynamism in terms of attacks configuration and generality in terms of being resilient against different kinds of attacks. In this work, we propose RAB2-DEF, a resilient against byzantine and backdoor attacks which is dynamic, explainable and fair to poor clients using local linear explanations. We test the performance of RAB2-DEF in image datasets and both byzantine and backdoor attacks considering the state-of-the-art defenses and achieve that RAB2-DEF is a proper defense at the same time that it boosts the other qualities towards trustworthy artificial intelligence.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) is rapidly transforming many aspects of our lives, has silently crept in, and is already part of our lives. At the same time that we are still unable to even conceive the potential of AI in many societal contexts, there is growing concern about the possible negative impacts of AI. In this context, the concept of trustworthy AI [1, 2] arises, based on the pillars of legality, ethics and robustness. In addition, seven technical requirements are set out: (1) human agency and oversight, (2) technical robustness and safety, (3) privacy and data governance, (4) transparency, (5) diversity, non-discrimination and fairness, (6) social and environmental well-being, and (7) accountability.\nIn this context of regulation and concern about trustworthy requirements including data privacy led by the GDPR [3] and the governance proposals [4], Federated Learning (FL) emerges [5]. It is presented as a distributed machine learning paradigm in which the data is never shared with other devices. That way, it ensures data privacy, along with proved technical robustness and safety. However, although FL is designed to ensure data privacy and robustness, it is still vulnerable to adversarial attacks against both data [6] and model integrity [7].\nPoisoning adversarial attacks pose significant threats in FL scenarios. Substantial efforts have been made in the literature to effectively counter these attacks [8, 9]. This has led to the development of several defense mechanisms primarily aimed at enhancing the performance of the federated model and reducing the impact of these attacks. However, most existing strategies suffer from a series of weaknesses[10]:\n\u2022 They are designed to be resilient to just one kind of attack, remaining the federated scheme vulnerable to the rest of attacks.\n\u2022 They are designed based on some assumptions about the attack configuration, for example, the number of nature of attacks.\n\u2022 As they are based on performance metrics, they can not differ between clients with skewed data and adversarial ones. Resulting in the filtering of poor quality clients, which can harm the robustness of the global model against new data [11] and deprive these clients of the global learning model, which is unfair to them.\n\u2022 They are black-box methods and do not provide any explanation about the selection or filtering out of clients.\nWe hypothesize that it is possible to design a general defense mechanism able to address these weaknesses in a unique proposal. It has to be generalizable to different kinds of attacks, agnostic and dynamic to changing attack conditions, and show a fair and explainable filtering out of adversarial clients.\nThis work poses a step further the defense against adversarial attacks and propose defense which is resilient against byzantine and backdoor attacks, dynamic, explainable and fair to poor clients (RAB2-DEF). We design this defense mechanism inspired in [12] based on the use of eXplaniable"}, {"title": "2 Background", "content": "This section provides the required background to follow the rest of the work."}, {"title": "2.1 Federated Learning", "content": "FL represents a distributed machine learning paradigm that aims to build machine learning models without directly exchanging training data among participating entities [5, 17]. It operates within a network of clients or data owners, engaging in two primary phases:\n1. Model training phase: In this phase, each client collaborates by sharing information without revealing their raw data, thereby jointly training a machine learning model. This model may be hosted by a single client or distributed across multiple clients.\n2. Inference phase: Subsequently, clients work together to apply the jointly trained model to process new data instances.\nBoth phases can operate synchronously or asynchronously, depending on factors such as data availability and the status of the trained model.\nIt is crucial to note that while privacy preservation is central to this paradigm, another key aspect involves establishing a fair mechanism for distributing the profits generated from the collaboratively trained model.\nAfter introducing FL as a general idea, a formal FL scenario can be outlined as follows. We consider a group of clients or data owners, denoted as C1, ..., Cn, each having their own local training data D1, ..., Dn. Every client Ci has a local learning model Li, which is defined by the parameters L1,..., Ln. The primary goal of FL is to develop a global learning model G, leveraging the distributed data across clients through a repeated learning process referred to as a \"round of learning\".\nIn each round t, every client trains its local model using its corresponding local dataset D, which leads to the modification of the local parameters L, resulting in updated parameters L. Following this, the global parameters Gt are determined by combining the trained local parameters \u00ce,..., \u00cet using a predefined federated aggregation function A, and the local models are then updated based on the aggregated parameters:\n$G^{t+1} = \\mathcal{A}(\\hat{L}_1^t, \\hat{L}_2^t, ..., \\hat{L}_n^t) \\qquad L_i^{t+1} \\leftarrow G^t, \\quad \\forall i \\in 1,...,n.$\nThis exchange of updates between clients and the server continues until a predefined stopping criterion is reached. Ultimately, the final state of G will encapsulate the knowledge learned by the individual clients.\nIn Figure 1 we represent a genericl FL scheme, where model updates are uploaded to a central server and aggregated to yield a trained global model, which is then delivered downstream to the clients and combined with their local models. As a result, the combined local model leverages knowledge modelled by other client for the same task, while keeping local data private."}, {"title": "2.2 Attacks in Federated Learning: Backdoor and Byzantine Threats", "content": "FL is vulnerable to various adversarial attacks, which can be broadly classified into attacks on the model and privacy attacks [10, 18]. This section focuses on two significant types of model attacks: backdoor attacks and Byzantine attacks."}, {"title": "2.2.1 Backdoor attacks in Federated Learning", "content": "Backdoor attacks [19] involve embedding a hidden, secondary task within the model while pre- serving its performance on the primary task. These attacks can vary widely based on the specific backdoor task implemented [20]. One common approach is pattern-key backdoor attacks [21], where attackers introduce a pattern into certain data samples and label these tampered samples with a target label. To enhance the attack's impact and avoid mitigation during aggregation with benign clients' updates, backdoor attacks are often combined with model replacement techniques [19]. This approach amplifies the influence of the adversarial update to ensure it supersedes the benign updates.\nMathematically, let Gt and L denote the global model and the local model of the i-th client at round t, respectively, with n clients participating in the round and \u03b7 as the server learning rate. The global model update at round t is given by:\n$G^t = G^{t-1} + \\eta \\frac{1}{n} \\sum_{i=1}^{n} (L_i^t - G^{t-1}).$\nAssuming a single adversarial client is selected in round t, this client attempts to replace the global model Gt with its backdoored model Ladu, optimized for both the primary and backdoor"}, {"title": "2.2.2 Byzantine attacks in Federated Learning", "content": "Byzantine attacks [22] aim to degrade the model's performance by causing it to behave erratically. These attacks typically involve coordinated actions by adversarial clients to corrupt the learning process through data or model poisoning [23]. In data poisoning attacks [24], adversaries introduce harmful patterns into the data, leading to incorrect learning and poisoned models, with label-flipping being a common method [25]. Model poisoning attacks [26], on the other hand, involve random modifications to the model's weights, resulting in arbitrary outputs.\nGiven the potential varying proportion of adversarial clients, these attacks often utilize model replacement techniques [27] to ensure the adversarial models dominate the global model."}, {"title": "2.2.3 Defenses against adversarial attacks in Federated Learning", "content": "To counteract these threats, numerous defense mechanisms have been proposed [18, 22], primarily operating on the server due to limited access to client information. Robust aggregation methods such as MultiKrum [28], Bulyan [29], STYX [30], and DDaBA [12] are designed to filter out malicious updates. However, these defenses can also inadvertently exclude useful information from benign clients with skewed data distributions, thus compromising the principles of fairness and equity essential for trustworthy AI. This can adversely affect the overall performance of the federated model. Recently, advocacy mechanisms have shown promise in maintaining robustness even in highly heterogeneous environments with a significant presence of poor clients [31]."}, {"title": "2.3 Explainability in Federated Learning", "content": "The increasing complexity of AI models, particularly in machine learning and deep learning, underscores the necessity of explainability. Explainability, or the ability to understand AI models according to [32], is crucial for several reasons. It can allow stakeholders to understand how decisions are made based on explanations, which is essential for trust and accountability [32]. Models that can be easily explained and understood are more likely to be trusted by users, especially in sectors like healthcare and finance, where decisions can have significant consequences.\nSecondly, explainability aids in the detection and correction of biases within AI systems. Bias in training data can lead to biased outcomes, and without transparency, it is challenging to identify and mitigate these biases. XAI enables a better understanding of how models interpret data, making it easier to spot and address potential biases [32]. This is essential for developing fair and equitable AI systems that do not perpetuate existing societal inequalities.\nHowever, the distributed nature of FL complicates the process of ensuring explainability. Each client's data may vary significantly, leading to diverse local models that contribute to the global model. This heterogeneity can make it difficult to understand the global model's decision-making process, as it is influenced by a multitude of local datasets and training processes.\nDespite these challenges, integrating explainability into FL is essential. It helps in understanding the contributions of individual client models to the global model, ensuring that the aggregated model is robust and free from biases present in any single client's data. Moreover, explainability in FL can foster trust among participants, as they can gain insights into how their data is being used and how it influences the global model [33, 14]."}, {"title": "2.4 Fairness in Federated Learning", "content": "Fairness and FL are critical components in the advancement of responsible AI. Fairness ensures that AI assisted decision-making systems do not perpetuate historical biases or discriminate against minority groups, thereby promoting ethical and responsible use of technology [34]. Within the context of FL, significant research efforts are dedicated to addressing fairness concerns. This includes developing algorithms that ensure equitable performance across diverse data sources and demographic groups, as well as techniques to identify and mitigate bias during the federated training process [35]. Researchers are also exploring methods to measure and improve fairness in federated settings, such as fairness-aware aggregation techniques and bias correction mechanisms [36]. By prioritizing fairness in FL, we can ensure that these distributed models not only protect user privacy but also deliver equitable outcomes for all clients, considering that the clients can be affected by unfair decisions.\nIn this paper, we use the concept of the poor client as a client who has a skewed distribution of data. This skewed distribution can be in terms of features, or in terms of labels. Throughout this paper, we will refer to fairness in terms of participation in the model. In many situations, adversarial defense mechanisms filter out clients on the basis of their performance, even filtering out poor clients as well, which is unfair."}, {"title": "3 RAB\u00b2-DEF: Dynamic, explainable and fair to poor clients defense against byzantine and backdoor attacks", "content": "As stated in the introduction, the main motivation is to develop a defense against adversarial attacks that is:\n\u2022 Resilient against byzantine and backdoor attacks: As it does not depend on the performance of the clients.\n\u2022 Dynamic: As it is able to adapt to different number of adversarial clients.\n\u2022 Explainable: As it is able to explain why a client has been discarded or not.\n\u2022 Fair to poor clients: As it can differ between poor clients (with skewed data distributions) and adversarial clients, not discarding the poor ones.\nTo create a defense strategy that fulfills the criteria of generality, dynamism, explainability, and fairness, we design RAB2-DEF, a dynamic and explainable defense against byzantine and backdoor attacks fair to poor clients. For that purpose, we set a small test set located at the entral server to classify clients as either adversarial or non-adversarial based on XAI techniques. It includes the following components:\n1. LLEs-based induced ordering function for client model updates: This function ranks clients based on the LLEs over the server's test data. Our hypothesis is that this ordering not only sustains a good robustness against attacks, but also endows the server with the capability to explain why a certain client is identified as adversarial and hence filtered out from the aggregation. For that purpose, we employ the LLEs to measure how different the update of a specific client is from the rest of the clients.\n2. Dynamic linguistic quantifier for weighting the contribution of clients: This function assigns weights to each client's contribution, giving a weight of zero to those deemed adversarial, while distributing the remaining weights such that the top-performing clients have twice the contribution of the others. For that purpose, we define a step-wise function based on data distribution of the clients model updates sorted using the LLEs-based induced ordering function.\n3. Defense based on federated aggregation: The defense employs a weighted aggregation operator, with each client's contribution determined by the dynamic linguistic quantifier.\nLLEs-based induced ordering function Formally, for each client we define a LLEs ordering function for each local update Li as follows:\n$f_{LE}(L_i) = \\sum_{x_v \\in X_v} SC(A_{L_i}^{x_v}, A_{L_j}^{x_v}), \\quad \\forall L_j \\in L,$"}, {"title": "4 Experimental setup", "content": "In this section we detail the experimental setup employed to test our proposal. In the following, we detail the evaluation datasets (see Section 4.1), baselines (see Section 4.2) and poisoning attacks (see Section 4.3)."}, {"title": "4.1 Evaluation datasets", "content": "Since attacks and defenses are independent of the classification task, we can focus on image classification problems, which are the most common in studies of poisoning attacks, without losing generality. The considered datasets are:\n\u2022 The Fed-EMNIST dataset [37]. The EMNIST Digits contains a balanced subset of the DIGITS dataset containing 28,000 samples of each digit. The dataset consists of 280,000 samples, in which 240,000 are training samples and 40,000 test samples. We use its federated version by identifying each client with an original writer.\n\u2022 The Fashion MNIST [38], which contains a balanced subset of 10 different classes containing 7,000 samples of each class. Hence, the dataset consists of 70,000 samples, which 60,000 are training samples and 10,000 test samples. We fix the number of clients to 500.\n\u2022 The CIFAR-10 dataset is a labeled subset of the 80 million tiny images dataset [39]. It consists of 60,000 32\u00d732 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images, which correspond to 1,000 images of each class. We set the number of clients to 100.\nDue to the fact that we need some data in the server to apply the defense strategy \u2013 validation dataset X defined in (6) \u2013 we employ 20% of the test set for this. This yields the evaluation datasets as in Table 1."}, {"title": "4.2 Baselines", "content": "In order to test the resilience against adversarial attacks of our proposal, we use the following baselines:"}, {"title": "4.3 Poisoning attacks", "content": "In the following, we specify the poisoning attacks implemented for the experimental results. We employ both data and model poisoning attacks, and both byzantine and backdoor attacks.\nByzantine attacks These attacks consists of randomly poison some part of the data or model updates. In particular, we implement:\n\u2022 Label-flipping attack [45], which involves randomly altering the labels of the adversarial clients. Consequently, these clients learn from poisoned data, which they then transmit to the server for aggregation, thereby compromising the aggregated model.\n\u2022 Random weights [46], which is a model poisoning attack consisting of randomly producing the model updates assigned to each adversarial client.\nBackdoor attacks These attacks consists of injecting a secondary task. With this purpose, we implement pattern-key attacks, which are based on identifying the samples poisoned with some pattern with the target label. For the sake of showing that the performance of the defense is agnostic of the pattern-key, we employ different patterns:\n\u2022 A black cross of length 3 for Fed-EMNIST and Fashion MNIST.\n\u2022 A 5x5 white square for CIFAR-10.\nIn Figure 2 we show the selected patterns for the backdoor attacks. From left to right: (1) a black cross pattern of length 3 in the bottom-right corner for a Fed-EMNIST instance; a black cross pattern of length 3 for a Fashion MNIST instance; and (3) a white squared pattern in the bottom-right corner for a CIFAR instance."}, {"title": "4.4 Evaluation metrics", "content": "As the objectives of byzantine and backdoor attacks are different, we measure the performance of each kind of defense in different ways.\nEvaluation metrics in byzantine attacks As the goal of byzantine attacks is to impair the performance of the global model, we employ the average test accuracy (accuracy) of the global model. The higher it is, the better the defense, as the more it is mitigating the effect of the attack.\nEvaluation metrics in backdoor attacks As backdoor attacks have a double goal (to inject the secondary task while maintaining the performance in the original one) we use both metrics: (1) Original task test (Original), corresponding with the test accuracy in the original task; and (2) Backdoor task test (Backdoor), corresponding with the test accuracy in the backdoor task. Clearly, the best defense is the one which achieves the highest original accuracy and the lowest backdoor accuracy.\nIn addition, for the purpose of a more comprehensive analysis of the proposal, we use the following metrics:\nEvaluation metrics to explainability The explainability of the proposal is going to be measured using visual explanations according to the importance of each pixel.. For that reason, in Section 6 we depict images where the importance of each pixel is measured in a range of greys where total white represents maximum importance and black represents minimum importance.\nEvaluation metrics to fairness In order to measure the fairness of the proposal in Section 7, we count the minimum (Min), maximum (Max) and average (Avg) number of both adversarial and poor clients discarded along the rounds of learning. Fairness will be substantiated when the fewer the number of poor clients discarded."}, {"title": "5 Experimental results", "content": "In this section we discuss the experimental results obtained by RAB2-DEF in comparison with the baselines in the adversarial attacks specified. For the moment, we just focus on the precision in terms of accuracy, in the following we perform further analysis. We show the results against byzantine attacks in Section 5.1 and the results against backdoor attacks in Section 5.2. In the first row we also show the average accuracy of FedAvg without any attack. The best result for each of the scenarios is highlighted in bold."}, {"title": "5.1 Results against byzantine attacks", "content": "In the following we report the results obtained by RAB2-DEF and all the baselines considered un both the label-flipping (see Table 2) and random weights (see Table 3) byzantine adversarial attacks."}, {"title": "5.2 Results against backdoor attacks", "content": "In the following, we test if RAB2-DEF is a valid defense against backdoor attacks. We show the results in Table 4."}, {"title": "6 Analysis on explainability", "content": "The analysis on explainability delves into a fundamental characteristic of RAB2-DEF: the client selection process based on LLEs inherently provides an explanation for why a client is either included or excluded. Since LLEs rely on feature importance, we can illustrate the significance of each feature within an image and evaluate if the model is concentrating on image areas that intuitively correspond with the predicted categories. As explained in Section 3, RAB2-DEF utilizes the resemblance among these explanations to determine whether to retain or exclude a client's model from the aggregation process. Consequently, visually examining the explanations linked to various client models for a validation sample (or a collection of samples) can assist a client in comprehending why its model is either included in or excluded from the aggregation, thus revealing the aggregation criteria embedded within the proposed defense strategy."}, {"title": "7 Analysis on fairness", "content": "We begin our analysis by evaluating whether RAB2-DEF ensures fairness for all clients. Other accuracy-based baselines lacks fairness as its filtering criterion may exclude clients with a poor (skewed) distribution of data. This unfair exclusion can negatively impact both these disadvantaged clients and the global model, as such clients may hold relevant information for other clients. We perform two analyses: (1) we count the number of adversarial and poor clients discarded, and (2) we analyse the performance of the poor clients in the original task."}, {"title": "7.1 Comparison in terms of adversarial and poor clients discarded", "content": "To assess this, we count the number of adversarial and poor clients discarded in each learning round and report the minimum, maximum, and average number of discarded adversarial and poor clients in Tables 5 and 6. As the aim is to demonstrate that RAB2-DEF is an improvement in terms of fairness over DDaBA (based on accuracy), we only consider these two methods in the analysis."}, {"title": "7.2 Analysis on the performance of poor clients", "content": "In this section we analyze the impact of the proposal on the performance of poor clients as we state that RAB2-DEF improve the fairness in terms of poor clients' performance. To this end, we analyze the performance of these clients after the learning rounds on the test set. As in the previous section, we compare with DDaBA. In Table 7 we show the mean accuracy in the original task of poor clients for each dataset on each attack scenario. For that metrics, we consider that when a client is discarded, the local model is the one trained locally, but when it participate in the aggregation, the local model is the aggregated model assigned by the server.\nThe results show that the mean performance of poor clients, regardless of the type of the attack, is far higher when they are not discarded. This fact is justified because RAB2-DEF does not exclude poor clients (see Table 6), providing them with the opportunity to participate in the global model and thus benefit from the knowledge shared by all clients in the aggregated model. This analysis strongly supports the fairness to poor clients provided by our RAB2-DEF proposal without losing its qualities of defense and robust aggregator for the federated scheme."}, {"title": "8 Conclusions", "content": "Adversarial attacks pose a significant threat in FL scenarios. Although substantial efforts have been made in the literature, most existing strategies tend to prevent just against one kind of attack, unfairly exclude clients with low-quality local models and fail to provide explanations for the selection or exclusion of clients in the aggregation process. This work covers this gap with the proposed RAB2-DEF a dynamic, explainable and fair to poor clients defense mechanism against byzantine and backdoor attacks in FL. The results and further analysis show that:\n\u2022 RAB2-DEF maintains the performance in terms of accuracy and attack mitigation when compared to other baselines despite of not focusing on accuracy.\n\u2022 RAB2-DEF is able of dynamically selects the clients to filter out based on LLEs, thus being agnostic to the number of adversarial clients and able to adapt to a changing number of them.\n\u2022 RAB2-DEF, for being based on LLEs, provides visual explanations for the filtering out of clients.\n\u2022 RAB2-DEF is able to distinguish between poor and adversarial clients, then ensuring a fair selection of clients resulting in more robust results both in the global model and the local models of the poor clients.\nTo sum up, RAB2-DEF ensures robustness, data privacy, integrity and attack mitigation, and it also provides other desired requirements for trustworthy AI stressing explainability and fairness to poor clients."}]}