{"title": "TRANSPORT-EMBEDDED NEURAL ARCHITECTURE: REDEFINING THE LANDSCAPE OF PHYSICS AWARE NEURAL MODELS IN FLUID MECHANICS", "authors": ["Amirmahdi Jafari"], "abstract": "This work introduces a new neural model which follows the transport equation by design. A physical problem, the Taylor-Green vortex, defined on a bi-periodic domain, is used as a benchmark to evaluate the performance of both the standard physics-informed neural network and our model (transport-embedded neural network). Results exhibit that while the standard physics-informed neural network fails to predict the solution accurately and merely returns the initial condition for the entire time span, our model successfully captures the temporal changes in the physics, particularly for high Reynolds numbers of the flow. Additionally, the ability of our model to prevent false minima can pave the way for addressing multiphysics problems, which are more prone to false minima, and help them accurately predict complex physics.", "sections": [{"title": "Introduction", "content": "Transport processes are integral to a variety of flow problems, describing the movement of quantities like mass or energy within the flow. These equations are robust local representations of fundamental conservation laws in physics and appear in numerous applications, from heat transfer[Bergman et al., 2011] to drug delivery in biofluidic flows[Longest et al., 2019]. Depending on the specific quantity being transported, they are known by different names, such as convection-diffusion equations for species and energy transport or Navier-Stokes equations for momentum transport. With recent advancements in computational fluid dynamics (CFD), several methods, including finite element[Lewis et al., 2004], finite volume[Moukalled et al., 2016] and meshless techniques[Katz, 2009] have been developed. However, these methods struggle to effectively integrate real-world data into their frameworks. This limitation is particularly problematic for scenarios where existing theories do not fully capture flow behavior such as high-Reynolds number flows[Eivazi et al., 2022] or where data collection is difficult, costly, or noisy[Jin et al., 2021]. Additionally, they cannot solve inverse problems with relatively low computational costs. To overcome these challenges and improve accuracy in data-sparse conditions, physics-aware models have been proposed. These models leverage mathematical physics and data-driven techniques from machine learning to address more realistic physics problems compared to traditional numerical methods. These models are categorized into three groups: the first group includes models that are guided by physics without directly embedding it into their structure. They rely on data-dependent supervised optimization, which limits their ability to generalize or extrapolate their understanding of physics beyond the spatiotemporal data on which they were trained [Huang et al., 2021, Lee and Chen, 1993, Faroughi et al., 2022]. Also the majority of the real-world problems aren't big data-oriented and extracted datasets from experiments cannot support all possible conditions. The second group, known as physics-informed neural models, uses a loss function comprising residuals of the partial differential equations they aim to solve, along with boundary and initial conditions provided as supervised input data. These models compute the derivatives of the outputs with respect to their inputs using automatic differentiation, eliminating the need for discretization or mesh generation[Jin et al., 2021, Eivazi et al., 2022, Wang et al., 2017]. Both of the above groups are limited by issues related to stability, convergence, and the inability to perform successful optimization under certain constraints. The last group consists of models that directly embed physics formulations into their architecture. The capability to encode complex equations with non-linearities simplifies the training process, enabling these models to perform well in sparse data situations without concerns about generalizing physics beyond the spatiotemporal boundaries[Chen et al., 2018, Wang, 1994, Cranmer et al., 2020, Trask et al., 2022]. Currently, Physics-aware neural models are neither as accurate nor as fast as traditional computational fluid dynamics (CFD) techniques, however they offer better scalability and flexibility. This is primarily due to the non-convex, high-dimensional loss functions that arise in fluid dynamics applications. In this experiment, a standard benchmark for evaluating the accuracy and stability of numerical methods in solving the Navier-Stokes equations is used to assess the performance of standard physics-informed neural networks, as introduced by [Raissi et al., 2019] (referred to as vanilla PINN), and our physics-embedded neural architecture. We compare their accuracy across a range of flow scenarios."}, {"title": "Method", "content": ""}, {"title": "Problem Overview", "content": "The fundamental laws of mechanics, such as the second Newton's law which have been initially developed for particle systems can be modified to describe the same laws for continuum media through Reynold's transport theorem[Reynolds, 1900]. By this theorem, we can arrive at the scalar transport(or convection-diffusion) equations. Here we are encountered by a physical constraint on the model represented by a partial differential equation."}, {"title": null, "content": "$\\Theta_t \\varphi + \\partial_k(\\varphi u_k) = \\partial_k(\\Gamma \\partial_k \\varphi), \\quad \\varphi \\in \\mathbb{R}, u \\in \\mathbb{R}^n, k = 1, .., n$\nHere, $n$ refers to the spatial dimensions, and index k follows the Einstein summation convention, while $\\Gamma$ denotes the diffusion constant. The first two terms on the left-hand side (LHS) represent the rate of change within the system combined with the advection of quantities, which move along with the system (at velocity u). This should be equal to the diffusing quantity, considered stationary relative to the system, as described by Fick's law [Paul et al., 2014]. Such equations describe physical phenomena where quantities like energy or species are transported within a system."}, {"title": "Incompressible Navier-Stokes Equations", "content": "A famous subset of transport equations, are Navier-Stokes equations which demonstrate the momentum conservation in a viscous system."}, {"title": null, "content": "$\\begin{cases}  \\partial_t u_i + \\partial_j (u_i u_j) = -\\partial_i p + \\frac{1}{Re} \\partial_i \\partial_j u_i, \\quad u \\in \\mathbb{R}^n, i,j = 1, .., n \\\\  \\text{Div}_i \\\\  u_i = 0  \\end{cases}$\nwhere $u \\in H$ and $H$ is a space of divergence free vector fields. The solution of the equations is the flow's velocity and pressure fields at any moment in a time interval. It is usually studied in three spatial dimensions and one time dimension, although two (spatial) dimensional and steady-state cases are often used as models, and higher-dimensional analogues are studied in both pure and applied mathematics. Once the velocity field is calculated, other quantities of interest such as pressure or temperature may be found using dynamical equations and auxiliary relations. In the following section, we will introduce a problem which is used as benchmark to evaluate numerical models."}, {"title": "Transport on a T2", "content": "As a classical problem in fluid mechanics, investigating turbulence and transport on a bi-periodic domain has been of interest since the Taylor-Green vortex flow was first introduced by Taylor et al. [Taylor and Green, 1937] and later investigated by Brachet et al. [Brachet et al., 1983]. The Taylor-Green vortex problem is a canonical flow which has become a challenging benchmark test for numerical methods, as the geometry is simple while the phenomena represented are complex. The flow represents an initially vortex that decays with the formation of a cascade of progressively smaller vortex structures due to vortex stretching mechanisms caused by viscosity. This flow features transient and fully turbulent behaviour, and the velocity field remains anisotropic over a period of time depending on the Reynolds number. Similar to Taylor-Green experiment, our experiment, is a flow governed by Navier-Stokes equations, bounded inside a 1 by 1 periodic domain, also known as a unit Toroid, given a smooth initial condition as follows:"}, {"title": null, "content": "$u_0 = u(x)_{t=0} = \\begin{bmatrix} -\\cos(2\\pi x)\\sin(2\\pi y) \\\\  -\\cos(2\\pi y)\\sin(2\\pi x)  \\end{bmatrix}$"}, {"title": null, "content": "According to [Brze\u017aniak et al., 2016], no matter the initial conditions, as long as it is smooth, Navier-Stokes problem on a $T^2$ will always have a bounded and unique solution."}, {"title": "Vorticity Transport", "content": "The vorticity transport equation can be derived from the Navier-Stokes equation by applying the curl operator. Taking the curl of the Navier-Stokes equation yields the following form:"}, {"title": null, "content": "$\\frac{\\partial \\omega_i}{\\partial t} + u_i \\frac{\\partial \\omega_i}{\\partial x_i} - \\omega_i \\frac{\\partial u_j}{\\partial x_i} + \\omega_i \\frac{\\partial u_j}{\\partial x_i} + u_i \\frac{\\partial \\omega}{\\partial x_j} = -\\epsilon_{ijk} \\partial_j \\partial_k(\\frac{P}{\\rho}) + \\omega_j \\frac{\\partial u_i}{\\partial x_j} + \\Gamma \\frac{\\partial^2 \\omega_i}{\\partial x_j \\partial x_j}$"}, {"title": null, "content": "Here, $\\omega = \\nabla \\times u$ represents the vorticity, where $u$ is the velocity field, and $\\Gamma = \\frac{1}{Re}$ accounts for the diffusion effects, which are associated with the viscosity in the fluid. The vorticity equation describes the evolution of the vorticity vector $\\omega_i$ under the influence of fluid flow, viscous diffusion, and pressure gradients. Given that the velocity field is incompressible, which implies $\\nabla \\cdot u = 0$, certain terms simplify. In the case of a planar velocity field, the divergence conditions $\\omega_i = 0$ and $\\frac{\\partial u_i}{\\partial x_i} = 0$ hold, which lead to the cancellation of terms like $\\omega_i \\frac{\\partial u_j}{\\partial x_i} = 0$. Additionally, the curl of the pressure gradient term, $\\frac{P}{\\rho}$, vanishes under the assumptions of either a uniform density field or a condition where the pressure and density gradients are orthogonal. With these assumptions, the vorticity transport equation simplifies to:"}, {"title": null, "content": "$\\frac{\\partial \\omega_i}{\\partial t} + u_j \\frac{\\partial \\omega_i}{\\partial x_j} = \\omega_i \\frac{\\partial u_i}{\\partial x_j} = \\frac{\\partial^2 \\omega_i}{\\partial x_j \\partial x_j}$"}, {"title": null, "content": "For two-dimensional or planar flows, only the component of vorticity normal to the plane (i.e., perpendicular to the flow) remains non-zero. As a result, the term $\\omega_j \\frac{\\partial u_i}{\\partial x_j} = 0$ vanishes due to the lack of variation in the velocity components in the perpendicular direction. Taking all the above considerations into account, the vorticity transport equation reduces further to the scalar form:"}, {"title": null, "content": "$\\frac{\\partial \\omega}{\\partial t} + u_j \\frac{\\partial \\omega}{\\partial x_j} = \\Gamma \\frac{\\partial^2 \\omega}{\\partial x_j \\partial x_j}$"}, {"title": null, "content": "This form represents the scalar advection-diffusion equation for vorticity, where the left-hand side describes the advection of vorticity by the fluid motion, and the right-hand side corresponds to the viscous diffusion of vorticity. This simplified vorticity transport equation is particularly useful in analyzing two-dimensional incompressible flows, where the vorticity is confined to the plane and evolves due to both the flow dynamics and diffusion effects."}, {"title": "Embedding Scalar Transport into Neural Approximations", "content": "Let v represent the output vector predicted by a neural model for a given transport problem. We redefine the concept of divergence by incorporating the temporal component, denoted as Div. For an N-dimensional vector v, the divergence is expressed as Div(v) = $\\frac{\\partial v}{\\partial t}$ + $\\frac{\\partial v}{\\partial x_i}$. To embed convection, we construct the convection-embedding tensor A as follows:"}, {"title": null, "content": "$A_{ij} = \\epsilon_{ijk}u_k$\nwhere $\\epsilon_{ijk}$ is the Levi-Civita tensor. The row-wise divergence of A yields:"}, {"title": null, "content": "$T = \\partial_i A_{i;j} = \\partial_i(\\epsilon_{ijk}u_k) = \\epsilon_{ijk} \\partial_i v_k$"}, {"title": "Lemma 1: T is divergence-free", "content": "Proof: Taking the divergence of T, we obtain:"}, {"title": null, "content": "$Div(T) = \\epsilon_{ijk} \\partial_j \\partial_i u_k$\nInterchanging the indices i and j leaves the divergence unchanged, but applying this interchange results in:"}, {"title": null, "content": "$Div(T) = \\epsilon_{jik} \\partial_i \\partial u_k = -Div(T)$\nUsing the symmetry property of continuous functions, $\\partial_i \\partial_j = \\partial_i \\partial_i$, and the antisymmetric property of the Levi-Civita tensor, $\\epsilon_{jik} = -\\epsilon_{ijk}$, we conclude that Div(T) = 0."}, {"title": null, "content": "Next, considering the first component of T, denoted as $T_0$, we define the diffusion-embedding matrix D as:"}, {"title": null, "content": "$D_{ij} = \\delta_{ij}T_0$\nTaking the row-wise divergence of D and multiplying by a diffusion constant results in $R = \\Gamma \\partial_i D_{ij}$. We now demonstrate that M = T + R represents the desired output for training the neural model."}, {"title": "Lemma 2: M satisfies the transport equation", "content": "Proof: We express the output M for training as M = R + T. By defining the components of M as $M_0 = \\varphi$ and $M_i = \\partial_i u_i$ for i = 1, . . ., n, and noting that the first component of R is zero, we have $M_0 = T_0 = \\varphi$. Thus:"}, {"title": null, "content": "$Div(M) = Div(R + T) = Div(T) + Div(R) = $\\frac{\\partial \\varphi}{\\partial t}$ + $\\frac{\\partial (\\varphi u_i)}{\\partial x_i}$ + 0 + $\\frac{\\partial (\\Gamma \\partial_i\\frac{\\partial T_0}{\\partial x_i})}{\\partial x_i}$$\nFor matrix demonstration check appendix A."}, {"title": "Related Works", "content": "Enforcing periodicity Periodic boundary conditions are a mathematical approach used in mechanics and fluid mechanics to model infinite or repeating systems by assuming that the boundaries of a simulation domain are connected. This means that when a particle, wave, or flow exits one side of the domain, it re-enters from the opposite side as if the system is continuous and unbounded. In fluid mechanics, periodic boundary conditions are used to simulate flows in situations where the flow pattern is expected to repeat over a certain distance, such as in channels or across arrays of obstacles. This technique simplifies the computational effort by reducing the size of the computational domain while maintaining the overall characteristics of the flow. There has been numerous works on how to work model periodic functions in deep neural networks, whether as a boundary condition or prediction of a periodic output function. Some papers add a loss function to the overall loss as to making the condition on periodic axis identical, assuming the $x_i$ axis to be the periodic axis, $L_{PBC}:= ||y_{x_i=0} - y_{x_i=T}||_{\\partial \\Omega}$ with T being the periodic constant[Kumar et al., 2024, Shah and Anand, 2024, Krishnapriyan et al., 2021]. Another approach is to enforce boundary condition with prior knowledge to the neural model. In this method Putting periodic trigonometric[Peng et al., 2020, Kast and Hesthaven, 2024, Dong and Ni, 2021] or polynomial[Dong and Ni, 2021] feedforward in the middle between input and the rest of the neural model(cite) which guarantees periodicity has been discussed in literature. Also using periodic activation function has also been investigated as a way of helping neural models understand the nature of periodic functions."}, {"title": "Failures of Physics-informed neural models", "content": "The application of physics-informed neural networks (PINNs) to solve convection-diffusion problems has garnered significant interest since the concept was introduced by Raissi et al. in 2019 [Raissi et al., 2019]. While these models have achieved considerable success, certain challenges persist, particularly in specific training scenarios. These challenges often arise due to the highly non-convex nature of the loss functions in these models, leading to unbalanced gradients during training. Failures in training PINNs can generally be categorized into three types:"}, {"title": null, "content": "Trivial Solutions: In some cases, the models converge to trivial solutions such as $y(x,t) = 0$, which is technically a solution for convection-diffusion partial differential equations (PDEs), but not a useful one."}, {"title": null, "content": "Static Solutions: Other models converge to $y(x, t) = y(x, 0)$ or $\\frac{\\partial y}{\\partial t} = 0$, implying that the initial condition remains the solution throughout the time span, or the solution does not evolve over time, which is also uninformative."}, {"title": null, "content": "Insufficient Accuracy: A third category involves models that successfully capture the underlying physics and temporal progression of the problem but fail to predict the solution with sufficient accuracy."}, {"title": "Experiment", "content": "Physics-informed or physics-embedded models, enabled by advances in automatic differentiation, have become capable of approximating solutions to partial differential equations (PDEs), treating the physics as a requirement rather than a pattern learned through backpropagation. This implies that in an almost fully self-supervised model, the exact output of the network does not need to be known across the entire domain or even a part of it; yet, the network can still solve the PDE, given the initial conditions, similar to traditional PDE solvers. We examine the performance of transport-embedded neural networks and vanilla PINN in approximating solutions to the Navier-Stokes equations on a 2D biperiodic domain. The initial conditions are based on the Taylor-Green vortex formulation."}, {"title": null, "content": "$\\frac{\\partial u}{\\partial t} + (u.\\nabla)u = -\\frac{\\nabla p}{\\rho} + \\frac{1}{Re}\\nabla^2u$\n$u(x)_{t=0} = \\begin{bmatrix} \\cos(2\\pi x)\\sin(2\\pi y) \\\\ -\\cos(2\\pi y)\\sin(2\\pi x) \\end{bmatrix}$\n$\\nabla.u = 0$\nIn both models, a fully-connected neural network is designed, incorporating a prior dictionary to impose periodic boundary conditions, mapping the input (x, y, t) to the output vector. For the standard PINN model, the outputs include the velocity field and pressure field (ux, uy, p). In contrast, for the transport-embedded network, the outputs consist of the vorticity and velocity fields (w, ux, uy), as the vorticity transport equation, derived from the Navier-Stokes equation, involves different parameters (see Eq.6). Automatic differentiation is employed to calculate the partial derivatives of the outputs with respect to the inputs. The residual terms in the loss function are given by:"}, {"title": null, "content": "$L_{PDE} := ||\\frac{\\partial u}{\\partial t} + (u.\\nabla)u + \\frac{\\nabla p}{\\rho} + \\frac{1}{Re}\\nabla^2u||_{\\Omega}$, $L_{IC,Vanilla PINN}:= ||u - u(x)_{t=o}||_{\\Omega}$, $L_{Incmp} := ||div(u)||_{\\Omega}$\n$L_{Curl} := ||\\omega - \\nabla \\times u||$, $L_{IC,TENN} := ||u - u(x)_{t=0}|| + ||\\omega - \\omega(x)_{t=0}||_{\\Omega}$, $L_{Incmp} := ||div(u)||_{\\Omega}$\n$L_{Total} := \\alpha . [L_{PDE}, L_{Curl}, L_{Incmp}, L_{IC,Vanilla PINN, L_{IC,TENN}}]$\nEq.14a corresponds to the residual terms for vanilla PINN when solving the Taylor-Green vortex, while Eq.14b pertains to the residual terms for the transport-embedded neural network (TENN) for the same problem. Notably, the divergence-free term is the only common term between them. Eq.14c defines the final loss function as a weighted sum of each residual, where $\\alpha$ represents the vector of hyperparameters. When training vanilla PINN, the residual terms related to TENN have hyperparameters set to zero, and vice versa when training TENN, the vanilla PINN residual terms have zero as their hyperparameters. Also different activation functions are utilized, such as 'sin', 'tanh', and 'softplus'. We show that our model is capable of learning a spectrum of diffusion coefficients, unlike the vanilla PINN, which struggles to avoid static solutions. In all experiments, we used the ADAM (Adaptive Moment Estimation) optimizer, differing from many previous studies that relied on the L-BFGS optimizer, a second-order method. Although L-BFGS can often provide more precise results, it operates in batch mode, which prevents stochastic training. This limitation makes learning process more prone to getting trapped in local minima, particularly in problems where the likelihood of encountering such minima is high. In the case of the vanilla PINN, regardless of the Reynolds number, the network"}, {"title": "Conclusion", "content": "The proposed TENN architecture represents a step forward in neural network-based CFD modeling by embedding physical transport laws directly into the neural network structure. This approach enhances the model's ability to handle convection-driven phenomena and results in smoother and more convex loss functions compared to traditional PINN methods. However, the study also highlights the limitations of the TENN model, particularly in scenarios dominated by high diffusion effects, such as those encountered at low Reynolds numbers. In these cases, convergence was weak, and the model struggled to accurately capture the diffusion dynamics without additional data input. In summary, the paper successfully introduces a novel neural network architecture that improves upon standard PINNS for CFD applications. Yet, the results suggest that further improvements are necessary, particularly for high-diffusion scenarios, to ensure robust performance across a wide range of flow regimes."}]}