{"title": "Tell me what I need to know: Exploring LLM-based (Personalized) Abstractive Multi-Source Meeting Summarization", "authors": ["Frederic Kirstein", "Terry Ruas", "Robert Kratel", "Bela Gipp"], "abstract": "Meeting summarization is crucial in digital communication, but existing solutions struggle with salience identification to generate personalized, workable summaries, and context understanding to fully comprehend the meetings' content. Previous attempts to address these issues by considering related supplementary resources (e.g., presentation slides) alongside transcripts are hindered by models' limited context sizes and handling the additional complexities of the multi-source tasks, such as identifying relevant information in additional files and seamlessly aligning it with the meeting content. This work explores multi-source meeting summarization considering supplementary materials through a three-stage large language model approach: identifying transcript passages needing additional context, inferring relevant details from supplementary materials and inserting them into the transcript, and generating a summary from this enriched transcript. Our multi-source approach enhances model understanding, increasing summary relevance by ~9% and producing more content-rich outputs. We introduce a personalization protocol that extracts participant characteristics and tailors summaries accordingly, improving informativeness by ~10%. This work further provides insights on performance-cost trade-offs across four leading model families, including edge-device capable options. Our approach can be extended to similar complex generative tasks benefitting from additional resources and personalization, such as dialogue systems and action planning.", "sections": [{"title": "1 Introduction", "content": "Meeting summaries play a key role in professional settings (Zhong et al., 2021; Hu et al., 2023; Laskar et al., 2023), serving as references, updates for absentees, and reinforcements of key topics discussed. Major virtual platforms (e.g., Zoom\u00b9, Microsoft Teams\u00b2, Google Meet\u00b3) offer summarization systems already, highlighting their importance. Current methods rely solely on transcripts (Zhu et al., 2020; Zhong et al., 2021) and generate generic summaries, often failing to contextualize long discussions' content (Kirstein et al., 2024b) and to tailor information to individual preferences and productivity requirements. As such, there is a need for improved model comprehension and personalization in meeting summarization.\nAdditional content-related sources can be considered during the summarization process to enhance model comprehension, turning the task into multi-source summarization. However, traditional approaches of appending documents to transcripts are often limited by model context sizes (e.g., LED (Beltagy et al., 2020), DialogLED (Zhong et al., 2022), Llama (Touvron et al., 2023)). While hierarchical (Zhu et al., 2020) and graph-based methods (Pasunuru et al., 2021) have been explored, they struggle with handling redundant or contradicting information and maintaining coherence throughout the additional input (Ma et al., 2023). Recent advancements in question-answering, which face a conceptually close challenge when answering a query considering an arbitrarily large amount of sources(Chen et al., 2017), suggest Retrieval Augmented Generation (RAG) (Lewis et al., 2021) as a promising solution that efficiently filters relevant information from extensive document collections and uses language models to perform a task such as information inferring. As RAG is not designed to identify contextual gaps in transcripts, a targeted approach is needed to pinpoint specific information requirements within the transcript, using RAG for focused retrieval. Otherwise, language models, already challenged by meeting summarization complexities (e.g., omission, repetition, irrelevance)"}, {"title": "2 Methodology", "content": "Our multi-source RAG-based summarization pipeline (Figure 1) enriches meeting transcripts with inferred information from supplementary materials, turning the multi-source summarization into a single-source task. An optional personalization protocol tailors summaries to specific readers by extracting participant information from the transcript and providing this info as the target audience to the generating LLMs. Leveraging LLMs' zero-shot capabilities (Liu et al., 2023a), proven effective for meeting summarization (Laskar et al., 2023; Kirstein et al., 2024a), our approach is suitable for real-world applications lacking in-domain datasets. Prompt templates are detailed in Appendix F."}, {"title": "2.1 General Summary Pipeline", "content": "Our multi-source summarization pipeline enhances model comprehension through three stages mimicking the human summarization approach considering additional sources: identifying where additional context is required (gap identification), extracting and inferring relevant information from the additional resources (information inferring), and summarizing the transcript considering the new information (summarization).\nIn gap identification, an LLM uses chain-of-thought reasoning (Wei et al., 2023) to identify and prioritize context-deficient passages, inspired by research on knowledge gap detection in reasoning (Wang et al., 2024) and LLM knowledge (Yin et al., 2023; Feng et al., 2024). We further process the identified gaps by having the LLM generate questions about the missing context observed. A RAG framework then processes these questions, using similarity measures to determine content relevance (Lewis et al., 2021) and infer answers from relevant sources. These answers hold the information bits the summarizing system misses to fully comprehend the meeting content and are inserted into the original transcript as comments (see Appendix B for an example). Finally, an LLM produces an abstractive summary of the enriched transcript (Laskar et al., 2023; Kirstein et al., 2024a). This approach incorporates supplementary materials, distributing the additional challenges of multi-source summarization (i.e., additional content understanding, salient content extraction, linking to the original transcript) across multiple model instances, without requiring domain-specific training or few-shot examples."}, {"title": "2.2 Personalized Summary Pipeline", "content": "Meeting summaries are crucial for post-meeting processing and action planning, necessitating personalized, user-centric approaches. To improve personal efficiency and information retention, the summary should contain what content the reader is most interested in, considering factors such as project relevance or moments of distractions, ideally without the need to manually input constraints (Chen et al., 2023) or query the transcript (Jung et al., 2023). Our personalization protocol leverages an additional LLM to extract target reader details and generate a persona (Paoli), i.e., a description regarding personality traits, viewpoints, interests, and additional task-relevant information. We leverage zero-shot abilities to detect standpoints(Lan et al., 2024), personalities(Rahman and Halim, 2022; Yan et al., 2024) and knowledge levels (Baek et al., 2024; C\u00e2mara and El-Zein). An example is shown in Figure 3 (Appendix D.1). The LLM then embodies this persona (Serapio-Garc\u00eda et al., 2023; St\u00f6ckli et al., 2024; La Cava and Tagarelli, 2024) for gap identification to generate questions from the individual's perspective and informs the RAG and summarizer LLM about their target audience to accordingly tailor the summary."}, {"title": "3 Dataset", "content": "For our experiments, due to the lack of an established multi-source meeting summarization dataset, we introduce MS-AMI, an adapted version of AMI (Mccowan et al., 2005), comprising 125 staged business meetings with processed supplementary content (whiteboard drawings, slides, notes). Using GPT-40\u2075 for OCR and image description (Shahriar et al., 2024), and Aspose for document text extraction, we create a multi-source dataset compatible with language models. Each meeting's data is compiled into a JSON file, preserving original structures. We remove 12 samples from the initial 137 meetings due to processing errors. Dataset statistics are in Table 5, with quality assessment details in Appendix A."}, {"title": "4 Experiments", "content": "This section explores the quality of summaries generated by our general and personalized pipeline. We analyze the performance of different LLMs on"}, {"title": "4.1 Results and discussion on the general multi-source summarization pipeline", "content": "Baseline. We compare our multi-source pipeline (G-infer) against three baselines. G-none is a single GPT4 model without access to additional information. G-all is given all available additional sources appended to the transcript's end. G-top considers only the top 5 closest additional sources based on an RAG framework. GOLD refers to the huamn generated summary.\nStructured inclusion of inferred details enhances multi-source summarization quality."}, {"title": "4.2 Results and discussion on the personalized multi-source summarization pipeline", "content": "We follow the same setup as for the general pipeline, using GPT4 as the backbone model. Here we add the persona extraction stage to inform the subsequent stages about the participants' traits.\nBaseline. In addition to our full pipeline (P-infer+per) with RAG-based information insertion and persona consideration, we evaluate the infer, all, and none variants as additional baselines, named P-infer, P-all, and P-none. Additionally, we consider P-per, where a persona is extracted and provided to the summarization model, but without using the RAG stage. All variations are informed about the target participant. We exclude the previously tested G-top variation due to its weaker performance.\nDetailed persona inclusion improves personalization but complicates content handling."}, {"title": "4.3 Practical Application", "content": "After exploring multi-source summarization and personalization with GPT4, we investigate smaller, more efficient LLMs to assess the practical use of our concepts. We now evaluate our best pipeline setups ('infer' and 'infer+per') using Phi-3 mini 3.8b 128k (Phi3) (Abdin et al., 2024), Gemini Flash 1.5 (Gemini) (Team et al., 2024), and Llama 3 8b\u2078 (Llama3) on one-third of MS-AMI. For Llama3, which cannot fit most meetings into its 8k token limit, we employ a sequential chunking approach (Chang et al., 2024). Examples of generated summaries are shown in Appendix E.3."}, {"title": "5 Related Work", "content": "Personalized summarization. A recent consideration when producing high-quality summaries is related to the identification of saliency for the reader (Kirstein et al., 2024b), introduced as personalized meeting summarization by Khurana et al. (2023), which aims to identify reader-specific salient information. Unlike existing approaches leveraging"}, {"title": "6 Final Considerations", "content": "This paper presents a three-step RAG-based pipeline using multiple LLM instances to abstractly summarize Englisch business meeting transcripts, considering supplementary files. We also explored how to use personas extracted from transcripts to introduce personalization and preferences in summaries. Key findings show incorporating supplementary sources improves summary quality by at least 0.31 over the baseline (single-source), with an additional 0.11 improvement when distributing multi-source challenges (identifying, inferring, and linking salient content) across multiple sources. Persona-based personalization, using dynamically generated participant personas, enhances relevance by up to 0.44 compared to a baseline with only the target audience's role information. Our zero-shot setup performs well with significantly smaller models than GPT-4 turbo, revealing that Phi-3 mini 128k produces good-quality summaries under a low-resource environment. This study provides initial insights into multi-source and personalized meeting summarization using LLMs and RAG systems, leaving the development of more sophisticated approaches, such as multi-agent discussions for retrieval and personalization, and the development of a dynamic function to identify the best amount of resources to consider to future work."}, {"title": "Limitations", "content": "Although our proposed MultiSourceMeeting might seem small (125 samples), its size is comparable to the original AMI dataset (137 samples). We contribute to extending the original datasets with careful alignment and curation of additional resources where available. Another possible limitation in our work is the use of only GPT4 in our main experiments. We chose GPT4 because of its large context size (e.g., 128k tokens) and better initial robustness when exploring new concepts. Another potential drawback is that our pipeline faces challenges in jointly optimizing prompts across different model families, potentially leading to performance variations. We address this by adapting best practices for individual stage-informing methods and model-specific prompting techniques, translating methodological concepts to fit each backbone model prompt-wise. Pre-testing was conducted for each stage and model to refine prompts and mitigate obvious limitations before experiments."}, {"title": "Ethics Statement", "content": "Licenses: We adhered to licensing requirements for all tools used (OpenAI, Microsoft, Google, Meta, Huggingface).\nPrivacy: User privacy was protected by screening the dataset for personally identifiable information during quality assessment (Appendix A).\nIntended Use: Our pipelines are intended for business organizations to generate quick, personalized meeting overviews. While poor summary quality may affect user experience, it should not raise ethical concerns as summaries are based solely on given transcripts. Production LLMs will only perform inference, not re-training on live transcripts. Summaries will be accessible only to meeting participants, ensuring information from other meetings remains confidential."}, {"title": "A Dataset Quality Assessment", "content": "To ensure MS-AMI's integrity and usability, we conduct a quality assessment using three graduate students 10 with diverse academic backgrounds (e.g., computer science, psychology, communication science), English proficiency, and familiarity with meeting summarization. Each sample undergoes a dual-annotator review focusing on OCR quality, Aspose text extraction, and privacy concerns to assess the quality and perform corrections if necessary. For OCR, the annotators are asked to look for artifacts changing individual words, and if the generated image descriptions match the drawings. For Aspose, they assign a label according to if all text is extracted successfully and the original structure maintained. Regarding privacy concerns, the annotators check all sources to see if any personal information of participants is disclosed that should not be part of the dataset, marking instances. In cases of disagreement, a third annotator is consulted. The assessment reveals consistently accurate GPT-40 extractions, correct alignment across samples, and no privacy risks. This comprehensive evaluation process ensures MS-AMI's reliability and ethical compliance for multi-source meeting summarization research.\nStatistics on MS-AMI are listed in Table 5"}, {"title": "B Example of Comment in Transcript", "content": "The questions pointing out gaps in context are answered from supplementary files, inferring the required information. This information is injected into the original transcript as a comment enclosed in [] and placed after passages requiring additional context. Figure 2 provides an illustrative example of this format."}, {"title": "C Evaluation Details", "content": "We use AUTOCALIBRATE (Liu et al., 2023b) and GPT4 prompted to follow the concept of FACTE-VAL (Min et al., 2023) for evaluation. This choice is motivated by its scalability, as human evaluation of over 3000 summaries is infeasible, and because the LLM-based metrics do not require reference summaries, making evaluation of the personalization scenario easier. ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2020) are not reported as main metrics as they yield nearly identical scores"}, {"title": "D.1 Persona Extraction", "content": "Our persona extraction process builds on existing approaches for retrieving standpoints (Lan et al., 2024), personalities (Rahman and Halim, 2022; Yan et al., 2024), and knowledge levels (Baek et al., 2024; C\u00e2mara and El-Zein). We validate extraction"}, {"title": "D.2 Gap Identification and Question Generation", "content": "Our gap identification approach builds on work identifying gaps in LLM knowledge (Feng et al., 2024; Yin et al., 2023) and in texts forming the base to answer reasoning tasks (Wang et al., 2024). We find that the capabilities of the language models used there (e.g., Llama 2, GPT-3.5) also transfer to GPT4, which successfully generates questions on contextual gaps not directly covered in the transcript. These questions are often strategic (e.g., \"Has the team considered the implications of using speech recognition technology, and what are the arguments for and against its inclusion?\"), providing a global perspective and enhancing contextualization. For personalization, questions vary based on the persona embodied by the LLM, such as User Experience (\"What are the implications of omitting the numeric keypad in terms of user navigation and channel selection efficiency?\") versus Marketing (\"Can we clarify the specific consumer preferences regarding the importance of appearance over functionality for our remote control design?\"). This indicates successful persona embodiment and viewpoint-specific questioning, adapting to different roles and perspectives within the meeting context."}, {"title": "D.3 Information Inferring and Answer Generation", "content": "Answering questions based on a set of retrieved, related works, follows the core concept of RAG (Lewis et al., 2021). GPT4 performs well as generating model in such a setup (Ho et al., 2024), and also reliably answers questions in our pipeline using RAG-derived information, inferring required insights and determining question answerability. For personalization, explanations adapt to the targeted user level when the persona is provided, indicating information sources more clearly. For the general pipeline, the answering model maintains a high-level, neutral tone."}, {"title": "D.4 Summarization", "content": "We provide extended versions of Tables 1 and 2 in Tables 8 and 9, including the standard deviations of the averaged scores and the score deviation for the personalized scores."}, {"title": "E Summaries Examples", "content": "Following, we present model summaries of the first AMI meeting. The single-source summaries and gold summary are in Table 10. Summaries from the general pipeline are shown in Table 11, personalized pipeline summaries are listed in Table 2, and summaries stemming from the smaller models are stated in Appendix E.3."}, {"title": "E.1 General Pipeline Summaries", "content": "The summary examples of G-infer, G-top, and G-all are displayed in Table 11."}, {"title": "E.2 Personalized Summaries", "content": "In Table 12 we display summaries from the P-infer+per, P-per, P-infer, and P-all setups on the project manager role. Table 13 shows P-infer-per summaries for the four different target readers."}, {"title": "E.3 Practical Setup Summaries", "content": "In Tables 14 and 15 we show the summaries of the smaller models Gemini, Phi3, Llama3 on the first AMI meeting with the general G-infer and the personalized P-infer+per setups, respectively."}, {"title": "F Prompt Templates", "content": "In the following, we present the prompt templates used to identify gaps in a given transcript (Figure 4), infer information from a set of related documents (Figure 5), summarize the enriched transcript (Figure 6) and extract a persona (Figure 7). The persona related prompt-passages are optional and left out for the general summarization pipeline."}, {"title": "Gap Identification Prompt Template", "content": "For the following task, respond in a way that matches this description: <persona>.\nTake the role of a question generator that takes the role of a defined participant and points out unclarities and open questions in a transcript. Generate at most 5 questions. Only ask the 5 most relevant questions.\nIf you were participant <participant>, what open questions would you still have in regards to the following transcript: <transcript>?\nYour answer shall only contain a Python array of dictionaries: '[<question>, <insert>, <question>, <insert>, <question>, <insert>, ...]'. Each dict must contain an entry called 'question' containing the question itself and an entry called 'insert' containing an exact copy of the sentence from the transcript that is most relevant to the question."}, {"title": "Salient Information Inferring Prompt Template", "content": "Format your entire answer as a JSON object, with an entry named \"answer\" containing your answer and an entry \"able\" containing a binary value (true or false, all lower case) for whether you were actually able to answer the question.\nBase your answer strictly on information contained in the prompt, without speculating. Tailor your answer so it fits best to this persona: <persona>.\nThe answer should be a single running text string, not a list or dictionary.\nAnswer based on the following transcript and a supplemental file.\nTranscript: <transcript>\nSupplemental file: <file>"}, {"title": "Abstractive Summarization Prompt Template", "content": "You are a professional summarizer and have been tasked with creating an abstractive summary for a participant in a meeting. Your summary should be 250 tokens or less. Carefully analyze the following transcript and provide a detailed summary for the participant. Consider the target persona who will have to work with the summary: <persona>.\nThe generated summary should help the persona understand the meeting content even after a long time, and it should be the perfect source for the persona to post-process the meeting content and prepare for the next steps. Focus on what is relevant for the participant to know and add what the participant needs to know to best work with the meeting content.\nSummarize this transcript. Create an abstractive summary. Make the summary 250 tokens or less.\nTranscript: <enriched transcript>"}, {"title": "Persona Extraction Prompt Template", "content": "You are a professional profiler and have been tasked with creating a persona for a participant in a meeting. Carefully analyze the following transcript and provide a detailed persona for the participant.\nIn your answer, include the participant's role, personality traits from the Big Five, point of view, contributions, knowledge that they brought to the meeting, information that they did not know, and any other relevant information. Make sure to provide a detailed and comprehensive persona. Your answer should be a string containing a running text.\nCreate a persona for participant <participant> based on the following transcript: <transcript>."}, {"title": "Evaluation Prompt Template", "content": "You are an expert in the field of summarizing meetings and are tasked with evaluating the quality of the following summary. Score the summary according to the scoring criteria with a Likert score between 1 (worst) and 5 (best).\nTranscript: <transcript>\nSummary: <summary>\nCriteria: <criteria>\nYour task is to rank the summaries based on the criteria provided. Remember to consider the quality of the summaries and how well they capture the key points of the original transcript. First provide an argumentation for your ranking. Therefore, use chain-of-thought and think step by step. Return a json object with the ranking for the evaluation criteria. The output should be in the following format: <explanation, step-by-step>! <json object> The json object should follow the structure \u201cjson <evaluation criteria> : <Likert Score>\u201c\u201c The JSON object should only contain the single Likert score for the currently assessed criteria."}]}