{"title": "On Partial Prototype Collapse in the DINO Family of Self-Supervised Methods", "authors": ["Hariprasath Govindarajan", "Per Sid\u00e9n", "Jacob Roll", "Fredrik Lindsten"], "abstract": "A prominent self-supervised learning paradigm is to model the representations as clusters, or more generally as a mixture model. Learning to map the data samples to compact representations and fitting the mixture model simultaneously leads to the representation collapse problem. Regularizing the distribution of data points over the clusters is the prevalent strategy to avoid this issue. While this is sufficient to prevent full representation collapse, we show that a partial prototype collapse problem still exists in the DINO family of methods, that leads to significant redundancies in the prototypes. Such prototype redundancies serve as shortcuts for the method to achieve a marginal latent class distribution that matches the prescribed prior. We show that by encouraging the model to use diverse prototypes, the partial prototype collapse can be mitigated. Effective utilization of the prototypes enables the methods to learn more fine-grained clusters, encouraging more informative representations. We demonstrate that this is especially beneficial when pre-training on a long-tailed fine-grained dataset.", "sections": [{"title": "1 Introduction", "content": "Self-supervised learning (SSL) is an effective approach to learn representations from unlabelled datasets. SSL methods have progressed rapidly in recent years and even surpassed the performance achieved by supervised training on several downstream tasks Grill et al. [2020], Chen et al. [2021], Caron et al. [2021], Zhou et al. [2022], He et al. [2022]. Broadly, SSL methods can be categorized into contrastive and non-contrastive methods. In contrastive methods [Chen et al., 2020, He et al., 2020, Misra and Maaten, 2020], all data samples repel all other data samples resulting in an approximately uniform distribution of representations in the latent space [Wang and Isola, 2020]. Specialized techniques like memory banks [He et al., 2020, Misra and Maaten, 2020] have enabled such methods to perform well even without large batch sizes. Recent state-of-the-art SSL methods [Grill et al., 2020, Chen and He, 2021, Zhou et al., 2022, He et al., 2022] use Vision Transformers [Dosovitskiy et al., 2021] and non-contrastive training methods. The prototypical formulations used in the DINO family of methods [Caron et al., 2021, Li et al., 2022a, Zhou et al., 2022, Govindarajan et al., 2023, Oquab et al., 2024] enable data samples belonging to the same semantic cluster to concentrate while only repelling other clusters. Such methods learn representations that are effective at nearest neighbor tasks and few-shot learning.\nA common problem in this family of methods is the representation collapse. This originates from the simultaneous learning of the image representations as well as the clustering parameters. All existing methods regularize the marginal latent class distribution in order to prevent collapse. We show that these methods are still affected by a partial prototype collapse (i.e. some groups of prototypes converge to the same vector), resulting in much fewer unique prototypes compared to the initialized number (K). We consider a prototype to be unique if it is at least e distance away from all other"}, {"title": "2 Background", "content": "The DINO-family of methods [Caron et al., 2021, Zhou et al., 2022, Assran et al., 2022, 2023, Li et al., 2022a, Govindarajan et al., 2023] use the pretext task of assigning data to K latent classes with multi-view class consistency. Consider an encoder model that produces a L2-normalized representation y = ge(x) such that ||y|| = 1, for a data point \u00e6 using parameters \u03b8. The probability of assigning a data point to a latent class k under the assumption of a latent class prior \u03c0\u03ba is given by:\n$P_k(y) = Pr(z = k|y) = \\frac{\\pi_\\kappa Pr(y|z=k)}{\\sum_{j=1}^K \\pi_\\kappa Pr(y|z=j)}.$ With a uniform class prior \u03c0\u03ba = 1/K (which is true in most prior work [Assran et al., 2022]), Govindarajan et al. [2023] showed that the prototypical formulation in the DINO family corresponds to a von Mises-Fisher mixture model, with parameters {\u03bc\u03ba, \u03ba\u03ba} and a normalization constant Cp(kk)\n$P_k(y) = \\frac{C_p(\\kappa_k)\\text{exp}(\\kappa_k \\langle \\mu_k, y \\rangle)}{\\sum_{j=1}^K C_p(\\kappa_j)\\text{exp}(\\kappa_j \\langle \\mu_j, y \\rangle)}.$  Here, \u03bck is the mean vector (a.k.a prototype) with ||\u03bc\u03ba || = 1 and \u03ba\u03ba > 0 is the precision, which is a measure of concentration around the mean vector. The pre-training objective minimizes the KL-divergence between the latent class distributions of multiple views of each image. This task has a trivial solution where all data points can be mapped to the same representation. To prevent this collapse, it is essential to add some form of regularization to the training objective. The regularization"}, {"title": "3 Marginal latent class distribution", "content": "Before discussing a newly identified mode of collapse in the next section, we review and provide a unified understanding of some of the regularization techniques proposed in the literature to avoid collapse. We define the marginal latent class distribution (MLCD) as the probability vector with elements, Pk = Ex[Pk(go(x))]. To our knowledge, all existing methods avoid representation collapse by regularizing the MLCD. Specifically, the MLCD is encouraged to match a prescribed prior distribution. A uniform prior is the default choice except for Assran et al. [2023], who propose a power law distribution to better adapt the model to long-tailed data. In a self-distillation setup, the MLCD can be encouraged to match a prior distribution either by adjusting the teacher/target distributions or by adding a penalty on the online/student distributions.\nAdjusting the target distributions such that the MLCD matches a prior distribution can be posed as an entropy-regularized optimal transport problem, which can be solved using the Sinkhorn-Knopp (SK) algorithm [Cuturi, 2013]. SK is typically run for a few iterations and adds a small but noticeable computational overhead. Caron et al. [2021] proposed centering, a simpler and computationally efficient method to adjust the target distributions. A key distinction between Sinkhorn-Knopp and centering is that they adjust the target distributions $P_k^{(t)}(y)$ based on batch and moving average estimates of the MLCD, respectively. On the other hand, Assran et al. [2022, 2023] add a prior-matching penalty on the batch-estimates of MLCD obtained from the online distributions $P_k^{(o)}(y)$. The penalty is defined as the KL-divergence between the MLCD and the prior distribution. With a uniform prior, this is equivalent to maximizing the entropy of MLCD, known as mean entropy maximization.\nIs the centering adjustment ad-hoc? At first glance, the centering adjustment in DINO might appear somewhat ad-hoc. However, we find that probability centering (PC) [Govindarajan et al., 2023] is closely connected to SK. Consider a batch of B logit scores over K latent classes L \u2208 RB\u00d7K and corresponding probability distributions P. The SK adjusted (1 iteration) probability distributions are obtained as follows (derivation in A.2 of supplementary):\n$P_{b,k}^{(sk1)} = \\frac{\\text{exp}(L_{b,k} - \\text{log}(\\sum_b P_{b,k}))}{\\sum_{j=1}^K \\text{exp}(L_{b,j} - \\text{log}(\\sum_b P_{b,j}))}$  On the other hand, the probability centered distributions are obtained as follows, where the centering parameter ck is calculated as a moving average estimate with momentum rate m:\n$P_{b,k}^{(pc)} = \\frac{\\text{exp}(L_{b,k} - C_k)}{\\sum_{j=1}^K \\text{exp}(L_{b,j} - C_j)}; C_k \\leftarrow m c_k + (1 - m) \\text{log}(\\frac{1}{B} \\sum_{b=1}^B P_{b,k})$  Comparing Eq. (2) and Eq. (3), we observe that probability centering is equivalent to one iteration of SK with the key distinction that the logit adjustment is calculated as a moving average instead of a batch estimate."}, {"title": "4 Partial prototype collapse", "content": "Regularizing the MLCD enables the methods to meet the requirement of spreading data over clusters. However, the MLCD depends on both the data representations and the prototypes. Given a set of frozen data representations, a method can achieve MLCD matching a prior distribution simply by manipulating the prototypes (see Figure 1). This holds true for all existing methods in the DINO family, as they all regularize the MLCD in a similar manner, as discussed in the previous section. Sharpening prevents the extreme case when all prototypes collapse to the same vector. However, except for this limited guardrail, the existing regularization techniques do not ensure that the methods learn unique prototypes. We define the term partial prototype collapse, where only a significantly small proportion of the learned prototypes are unique.\nDefinition 4.1 (Partial prototype collapse). Consider the set W = {\u03bc\u03ba : k = 1,..., K} of K prototype vectors, \u03bc\u03b5 such that ||\u00b5k || = 1. A partial prototype collapse (of degree M and e distance) is said to have occurred if there exists a set of M disjoint partitions of prototype vectors Vm CW, m = 1, ..., M, and M representative prototype vectors vm \u2208 Vm, such that for all m = 1, ..., M, $1 - v_m^T \\mu_j  < \\epsilon$, for all \u00b5j \u2208 Vm. The set of M unique prototypes is defined as U = {vm}m=1. For each representative prototype, the redundancy factor rm is defined as the size of the corresponding set partition, rm = |Vm|.\nInvestigating learned MLCD and prototypes: When training with MLCD regularization, the DINO family of methods are prone to partial prototype collapse since it enables the method to spread probability mass associated with each unique prototype across its e-set of redundant prototypes. This acts as a shortcut to match the MLCD to the specified prior distribution. Govindarajan et al. [2023] make an empirical observation that the DINO models used significantly smaller number of unique prototypes compared to the hyperparameter K. However, this problem is neither studied further nor addressed by their proposed method. Based on our definition of partial prototype collapse and using a cosine distance metric, we investigate the prototypes learned by several self-supervised clustering methods that use a prototypical formulation, from SwAV [Caron et al., 2020] to iBOT [Zhou et al., 2022]. In Table 1 and Table 7 (in supplementary), we show that such a collapse exists in all the considered methods. We observe that prototypes with a higher redundancy factor tend to be assigned a larger proportion of the data samples (see Figure 2). Hence, the partial prototype collapse serves as a shortcut to achieve a MLCD closer to the specified uniform prior in these works. This shortcut is important to be aware of, if the intention is to encourage the MLCD to match a specific non-uniform distribution based on knowledge about the dataset domain. In addition, this means that the hyperparameter K does not play its intended role of controlling the number of clusters."}, {"title": "4.1 Regularizing prototype distribution", "content": "The number of latent classes is an important choice in clustering as this controls the fine-grainedness of the clusters. Firstly, this controls the difficulty of the self-supervision task. Secondly, more informative representations are required to discriminate between more fine-grained latent classes. With this motivation, we believe that the number of prototypes is an important design choice in SSL. However, prior works have found inconsistent results when ablating for this choice, likely because of the occurrence of partial prototype collapse. Given that we want the prototypes to be as diverse as possible, a meaningful choice is to encourage the prototypes W = {\u03bck}k=1K to be uniformly distributed in the latent space. We propose to achieve this by maximizing the differential entropy of the prototype vectors, obtained using the Kozachenko-Leonenko estimator [Kozachenko and Leonenko, 1987, Beirlant et al., 1997, Sablayrolles et al., 2019], $h_{k1}(W) = -\\frac{1}{K} \\sum_{k=1}^K log(d_k)$, where dk = mini\u2260k ||\u03bc\u03ba \u2013 \u03bc\u03af||. We efficiently compute an estimate of LKP = hkl(W) by randomly partitioning the prototypes into batches and we show in A.3 of supplementary that this adds negligible computational overhead. We verify in section 6.2 that this regularization can mitigate the partial prototype collapse. Then, we focus on our main goal of studying the downstream impact of effectively utilizing the initialized prototypes through various experiments that evaluate the learned representations."}, {"title": "5 Related Work", "content": "Connection to DINOv2: Our proposed KoLeo-proto regularization is formulated similar to Sablayrolles et al. [2019]. Recently, DINOv2 [Oquab et al., 2024] proposed the KoLeo-data regularization which uses a similar formulation but applied to spread the data representations instead of the pro-totypes. Hence, DINOv2 can be viewed as an interpolation between the uniformly distributed representations of contrastive learning and clustered representations of the DINO family. In contrast, KoLeo-proto preserves the clustered representations of DINO and encourages the method to learn diverse clusters. We illustrate this difference in Figure 1.\nRegularizations in clustering-based SSL: We provide an extended discussion of clustering based SSL methods in A.1 of supplementary and focus our discussion on the regularization methods in this section. A common limitation of simultaneously learning representations and clustering them is that there are degenerate solutions that perfectly solve the clustering task but fail to learn informative representations. Caron et al. [2018] proposed uniform pseudo-label sampling that is equivalent to weighting the loss contribution of an input by the inverse of its assigned cluster's size. Asano et al. [2020] and Caron et al. [2020] viewed the clustering task with MLCD regularization as an entropy regularized optimal transport problem and use the Sinkhorn-Knopp (SK) algorithm to assign pseudo-labels to data points [Cuturi, 2013]. Asano et al. [2020] and Caron et al. [2020] used the Sinkhorn-Knopp (SK) algorithm to regularize the MLCD [Cuturi, 2013]. This is shown by Assran et al. [2023] to encourage the MLCD to match a uniform prior. While SK requires multiple iterations for convergence, a simpler and computationally cheaper approach known as centering is proposed in DINO [Caron et al., 2021] and also used in EsViT [Li et al., 2022a] and iBOT [Zhou et al., 2022]. Govindarajan et al. [2023] proposed probability centering, that computed the centering parameter in the probability space instead of the logit space. MSN [Assran et al., 2022] and PMSN [Assran et al., 2023] proposed to add an explicit prior matching penalty to encourage the MLCD to align with prescribed prior distributions. Methods using the prior-matching penalty and SK depend on batch estimates of the MLCD. On the other hand, centering uses moving average estimates; we showed the connection of probability centering to SK in section 3. and investigate how methods using batch and moving average MLCD estimates compare at different batch sizes in section 6.1. While all the above methods regularize the MLCD, we show the occurrence of a partial prototype collapse by investigating the prototypes learned by existing pre-trained models. We propose a new KoLeo-proto regularization as a tool to prevent this collapse and study the downstream impact of effectively utilizing the prototypes.\nPre-training on long-tail datasets: Most SSL methods are evaluated by pre-training on ImageNet, a well-curated dataset with a uniform class distribution. To the contrary, real-world data collection often results in long-tailed distributions over visual concepts and pre-training on such datasets is of practical interest. We note that there is limited research on pre-training SSL methods on such long-tailed datasets. Caron et al. [2019] investigated pre-training on a large uncurated dataset. Recently,"}, {"title": "6 Experiments", "content": "To study the MLCD and prototype regularizations, we focus on iBOT, which is a strong recent baseline among the DINO family of methods and also used as the foundation for DINOv2 [Oquab et al., 2024]. We pre-train the models on the ImageNet-1K dataset [Deng et al., 2009] by modifying the public codebase of iBOT 1. We use the same hyperparameter settings as in iBOT for different ViT backbones (see configuration details in A.4.1 and additional experiment with a CNN backbone in A.5.4 of supplementary) and use the vMF normalized variants [Govindarajan et al., 2023], which are shown to produce stable trainings and improved performance. We start with ablation experiments to choose the MLCD regularization technique in section 6.1, evaluate the impact of adding our proposed prototype regularization in section 6.2 and finally perform full-scale pre-training experiments."}, {"title": "6.1 MLCD regularization", "content": "We conduct ablation experiments to select the method to regularize MLCD. We pre-train ViT-S/16 backbone with different MLCD regularization techniques - Sinkhorn-Knopp (SK), probability centering (PC) and mean entropy maximization (ME-MAX). For PC, we use the vMF normalized version of iBOT. For SK and ME-MAX, we chose to use a smaller teacher temperature based on a hyperparameter search (refer A.4.1 for details). We also consider three different compute budgets (2, 4 and 8 GPUs for 2 days), which allows us to evaluate the impact of batch size on these techniques. With more GPUs, we can accommodate a larger batch size. The number of epochs is adjusted such that the total number of iterations are the same for all the compute budgets. We do this to avoid the expensive process of optimizing the learning rates for each compute budget and regularization method. Overall, from Figure 3, we find that probability centering performs better than the other alternatives at different compute budgets. Interestingly, PC achieves performance on par or better than the alternatives, even at half of the compute budget (e.g. PC/4GPUs vs ME-MAX/8GPUs).\nThe methods discussed above have all been proposed in the literature as ways to regularize the MLCD. We argue that the main difference between them is whether the regularization is done over a single batch (SK, ME-MAX) or based on moving average statistics (PC). We observe that PC performs significantly better than the alternatives at the lowest compute budget, which uses a small batch size. As we increase the compute budget and thereby also the batch size, the gap is reduced. This indicates that PC is more robust to the choice of batch size. We conjecture that this is due to too noisy estimates of the MLCD when computed over a batch, which is not surprising considering that we estimate probability vectors in a high-dimensional space. Therefore, in all the experiments in the main paper, we use the vMF normalized iBOT with MLCD regularized using probability centering."}, {"title": "6.2 Prototype regularization", "content": "We add our proposed KoLeo-proto regularization to the iBOT-vMF baseline, resulting in the overall loss objective, L = LiboT + ALKP. These results are indicated by \"(kp)\". Similarly, we indicate the KoLeo-data regularization used by Oquab et al. [2024] as \"(kd)\". We use X = 0.1 and observe that such a small A is sufficient to mitigate partial prototype collapse (see ablation in A.5.2 of supplementary). In Figure 4, we compare the number of unique prototypes M when we vary the initialized number of prototypes hyperparameter K. With the baseline and KoLeo-data regularization, changing K has no impact on the number of unique prototypes learned by the method, which is significantly smaller than the initialized number of prototypes. This indicates the occurrence of partial prototype collapse. With KoLeo-proto regularization, we observe that M \u2248 K and hence the hyperparameter K reliably controls the number of learned clusters.\nWe observe that the baseline shows similar performance at different numbers of initialized prototypes. On the other hand, with KoLeo-data, the performance is worse than the baseline but continues to improve as the number of prototypes are increased. KoLeo-data encourages the data to spread on the hypersphere. Hence, data is assigned to more diverse prototypes compared to the baseline in the initial training phase. We conjecture that this initial training dynamic benefits from having more prototypes, even if many of these prototypes eventually collapse to the same vector. We limit the maximum number of prototypes to 10240 due to computational limitations. Computing probability distributions for all the tokens over more dimensions adds a large computational overhead. However, the KoLeo regularization itself only adds a negligible computational overhead (cf. A.3 in supplementary). With KoLeo-proto, we observe around 0.1% improvement in accuracy when adding every 2K additional prototypes. Overall, increasing the number of prototypes from 2K to 10K results in a 0.4% improvement. Further scaling of the number of prototypes can bring larger performance gains which should be feasible with the efficient implementation in DINOv2."}, {"title": "6.3 Pre-training with ImageNet", "content": "We pre-train iBOT-vMF with efficient prototype utilization using KoLeo-proto regularization for ViT-S/16 and ViT-B/16 backbones. To ensure fair comparison, we set the number of prototypes to 8192, similar to iBOT. Hence, any changes in performance can be associated to only the effective prototype utilization. In Table 2, we report the top-1 accuracies obtained using kNN and linear classification based on frozen backbone features, few-shot accuracies averaged over 3 different splits and the accuracy obtained after fine-tuning. For kNN, linear evaluation and finetuning, we follow the same protocol as in DINO and iBOT. We perform few-shot evaluation similar to Assran et al. [2022] and use the provided data splits. We compare against the iBOT-vMF baseline, MSN and the best performing models from WE-SSL [Ruan et al., 2023]. We observe on par or marginal improvements for kNN, linear and fine-tuned classification performance. The kNN performance improvement with respect to the baseline at 8192 prototypes after full-scale pre-training mirrors the improvement (+0.2%) observed after the small-scale ablation in Figure 4. This suggests that by using an even larger number of prototypes one can improve the performance further with efficient prototype utilization (cf. Figure 4), which we found to not be the case for the baseline in ablation experiments.\nWe find larger gains for few-shot learning (FSL) performance when adding KoLeo-proto to the baseline, even at 8192 prototypes. Note that the prediction head architecture and other hyperparam-eters are tuned in WE-SSL to achieve the best FSL performance with ViT-S/16. This explains the significantly better results achieved by WE-SSL with ViT-S/16. With ViT-B/16, iBOT-vMF (kp) outperforms WE-SSL at 1% and 5 img/cls settings. Note that iBOT-vMF can be tuned similar to WE-SSL but we have not investigated this. Instead, we focus on studying the impact of effective prototype utilization on general downstream performance and do not perform any task-specific tuning.\nTransfer learning: We conduct linear classification experiments on the standard suite of datasets trained using features extracted from a frozen pre-trained model. In Table 2, we report the accuracies averaged over all datasets. The detailed results and evaluation setup are provided in A.5.5 of supplementary. We observe on par or decreased transfer performance with effective prototype utilization compared to the iBOT-vMF baseline. Interestingly, we note that the transfer performance decreases also in other methods that improve few-shot learning performance such as MSN [Assran et al., 2022] and WE-SSL [Ruan et al., 2023] compared to their DINO baseline (note that transfer results for ViT-B/16 are not reported in WE-SSL). This indicates that tuning for few-shot learning performance can potentially harm transfer performance. We hypothesize that certain features that improve few-shot learning performance on the pre-training dataset could be too specific to the pre-training data and do not generalize to other datasets considered for transfer learning. Compared to these methods, our proposed regularization leads to better transfer performance. There appears to be a trade-off between few-shot learning performance on the pre-training dataset and transfer learning performance. Currently, it is unclear why such a trade-off exists and this requires further investigation. Note that DINOv2 constructs the LVD142M pre-training dataset by finding images similar to the suite of transfer datasets of interest, thus limiting the domain gap between the pre-training and transfer datasets."}, {"title": "6.4 Pre-training with iNaturalist-2018", "content": "Most SSL methods are pre-trained on ImageNet which is well-curated and contains uniformly distributed data across its classes. It is of practical interest to pre-train SSL methods on data collected in the wild, which is often long-tailed. We study pre-training the DINO family of methods on long-tailed datasets, which has gained limited attention. We consider the iNaturalist-2018 (iNat18) dataset 2 which is around 1/3rd of the size of ImageNet and contains a long-tail distribution of data from 8142 classes. We pre-train all the models for 300 epochs using the default publicly available hyperparameters. For MSN and PMSN [Assran et al., 2022, 2023], we choose the regularization strength A based on a hyperparameter search (see A.4.2 in supplementary for details). This analysis of regularization strength A indicated that weakly encouraging a uniform prior in MSN produces better performance than using a long-tailed prior as in PMSN. With this motivation, we retain the uniform prior assumption of the other methods. In Table 3, we report the top-1 classification accuracy obtained using a linear and a fine-tuned classifier. For linear classification, we follow a similar protocol as in the ImageNet experiments. For fine-tuning, we use longer trainings with a smaller learning rate as in"}, {"title": "7 Conclusion", "content": "We identified the occurrence of a previously unnoticed mode of collapse in the DINO family of methods, termed as partial prototype collapse that results in significant redundancies in the prototypes. As a consequence, the hyperparameter controlling the number of prototypes did not perform its intended role of controlling the number of clusters learned by the model. We proposed the KoLeo-proto regu-larization to encourage the model to learn diverse prototypes. By adding our proposed regularization, we showed that the initialized prototypes are effectively utilized. With effective prototype utilization, scaling the number of prototypes is useful in learning better image representations of the underlying dataset. Using the same moderate number of 8K prototypes as before, we showed that few-shot learning performance can be improved and full data trainings can be marginally improved. As indicated in our ablation experiments, it seems possible that further scaling the number of prototypes can result in more significant improvements. However, we observed a worse transfer performance and this trade-off is consistent with other methods that specifically improve few-shot learning. On the other hand, we found that learning fine-grained clusters on a long-tailed fine-grained dataset such as iNat-2018 is more beneficial, indicated by the larger performance gains achieved using a similar number of prototypes.\nWe have shown that the hyperparameter for the number of prototypes can be reliably controlled using our regularization. This has broad implications on applying methods from the DINO family. One can better understand the impact of using different numbers of clusters in the self-supervised pretext task for their own dataset and method of choice. This could vary depending on the domain of the dataset and how fine-grained the semantic concepts are in that domain. Computing probability distributions over a large number of latent classes comes at a significant computational cost (see A.5.3 in supplementary). If indeed a small number of clusters are sufficient for some dataset, effectively utilizing fewer prototypes can help in reducing computational expenses."}, {"title": "A Appendix", "content": "A.1 Extended related work\nClustering-based self-supervised learning: Self-supervised learning based on the clustering pretext task is a promising paradigm that has proven to be successful and grown tremendously in recent years. Initial works [Caron et al., 2018, 2019, Asano et al., 2020] used a two-stage process of assigning pseudo-labels by clustering the representations and then training the representations using the pseudo-labels as targets. Caron et al. [2018] proposed uniform pseudo-label sampling that is equivalent to weighting the loss contribution of an input by the inverse of its assigned cluster's size. Caron et al. [2020] used online assignment of pseudo-labels in every batch by clustering the representations over a small window of batches. By adapting this to ViTs, Caron et al. [2021] proposed a self-distillation framework where a teacher network produced the target latent classes. Govindarajan et al. [2023] demonstrated that this objective corresponds to learning a von Mises-Fisher mixture distribution. Li et al. [2022a] extended the DINO objective to patch tokens while also leveraging efficient architectures like Swin [Liu et al., 2021]. iBOT [Zhou et al., 2022] is a recent state-of-the-art method that poses the masked image modeling (MIM) task of BeIT [Bao et al., 2022] as a clustering task. Another branch of works have focused on improving the few-shot learning performance of these methods [Assran et al., 2022, Ruan et al., 2023]. Recently, DINOv2 [Oquab et al., 2024] built upon iBOT by making several modifications. By pre-training on the large LVD142M dataset, DINOv2 demonstrated performance surpassing many state-of-the-art visual benchmarks at image and pixel levels.\nA.2 Sinkhorn-Knopp and probability centering\nLet the batch of B logit scores be denoted as L \u2208 RB\u00d7K with corresponding probability distributions P. Then, the Sinkhorn-Knopp adjusted probability distributions P are obtained by alternating between normalizing the rows and columns of the matrix exp(L), so that they sum up to 1. Note that the exponent function is applied element-wise to the matrix. Let the elements of the matrix be denoted as Lb,k. Then, normalizing along the rows yields,\n$P_{b,k} \\leftarrow \\frac{\\text{exp}(L_{b,k})}{\\sum_k \\text{exp}(L_{b,k})} = \\frac{\\text{exp}(L_{b,k})}{\\frac{1}{B} \\sum_b \\text{exp}(L_{b,k})}  \\frac{1}{B} \\text{exp}(L_{b,k} - log(\\frac{1}{B} \\sum_b \\text{exp}(L_{b,k}))).\nNext, normalizing P along the columns we obtain,\n$P_{b,k} \\leftarrow \\frac{\\text{exp}(L_{b,k} - log(\\sum_b \\text{exp}(L_{b,k})))}{\\sum_{j=1}^K \\text{exp}(L_{b,j} - log(\\sum_b \\text{exp}(L_{b,j})))}.\nIf we consider the initial logit scores L\u2081 to be already normalized over the components K such that $\\sum_k \\text{exp}(L_{b,k}) = 1$, then the exponents within the inner sum can be replaced with probabilities. Thus, we obtain the probability distributions after 1 iteration of Sinkhorn-Knopp adjustment as,\n$P_{b,k}^{(sk1)} = \\frac{\\text{exp}(L_{b,k} - log(\\sum_b P_{b,k}))}{\\sum_{j=1}^K \\text{exp}(L_{b,j} - log(\\sum_b P_{b,j}))}.\nOn the other hand, the probability centered distributions proposed by Govindarajan et al. [2023] are obtained as follows, where the centering parameter ck is calculated as a moving average estimate with momentum parameter m:\n$P_{b,k}^{(pc)} = \\frac{\\text{exp}(L_{b,k} - C_k)}{\\sum_{j=1}^K \\text{exp}(L_{b,j} - C_j)}; C_k \\leftarrow m c_k + (1 \u2013 m) log(\\frac{1}{B} \\sum_{b=1}^B P_{b,k}).\nComparing the above expressions for $P_{b,k}^{(sk1)}$ and $P_{b,k}^{(pc)}$ (Eq. (2) and Eq. (3) in section 3 of the main paper), we observe that probability centering is equivalent to one iteration of Sinkhorn-Knopp with the key distinction that the logit adjustment is calculated as a moving average instead of a batch estimate."}, {"title": "A.3 KoLeo prototypes implementation", "content": "Given a set of K prototypes W \u2208 RK\u00d7D, to compute the KoLeo estimate of the differential entropy of the prototypes hk1(W), we require computing nearest neighbor distances for each of the prototypes. This can be memory intensive when a large number of prototypes are used. Note that this is not a problem in the case of DINOv2 [Oquab et al., 2024], as the KoLeo objective is computed between the B data representations in the batch (B is typically much smaller than K). Instead, we resort to a stochastic estimate when calculating the loss objective in each batch. For each batch, we randomly partition the prototypes into disjoint partitions containing 2048 prototypes each, W = {W1,..., WT}, Wt \u2208 ]R2048\u00d7D. Then, we compute the KoLeo estimate as follows:\n$h_{k1}(W) = \\sum_{t=1}^T h_{k1}(W_t)$. This efficient batched implementation adds negligible computational overhead, in terms of both memory and time (15 MB additional GPU memory when K = 8192 and unchanged image throughput)."}, {"title": "A.4 Experimental details", "content": "A.4.1 Hyperparameter settings\nThe complete hyperparameter configuration for full-scale iBOT-vMF pre-trainings on ImageNet using ViT-Small/16 and ViT-Base/16 models are provided in Table 4. For pre-training on iNaturalist-2018, we use a similar hyperparameter configuration except that we use pre-train both ViT-Small/16 and ViT-Base/16 models for 300 epochs. The complete hyperparameter configurations for MSN and PMSN pre-trainings on the iNaturalist-2018 dataset using the ViT-Small/16 model are provided in Table 5.\nA.4.2 MSN and PMSN discusion\nWhen pre-training"}]}