{"title": "Beyond Seen Data: Improving KBQA Generalization Through Schema-Guided Logical Form Generation", "authors": ["Shengxiang Gao", "Jey Han Lau", "Jianzhong Qi"], "abstract": "Knowledge base question answering (KBQA) aims to answer user questions in natural language using rich human knowledge stored in large KBs. As current KBQA methods struggle with unseen knowledge base elements at test time, we introduce SG-KBQA: a novel model that injects schema contexts into entity retrieval and logical form generation to tackle this issue. It uses the richer semantics and awareness of the knowledge base structure provided by schema contexts to enhance generalizability. We show that SG-KBQA achieves strong generalizability, outperforming state-of-the-art models on two commonly used benchmark datasets across a variety of test settings.", "sections": [{"title": "Introduction", "content": "Knowledge base question answering (KBQA) aims to answer user questions expressed in natural language with information from a knowledge base (KB). This offers user-friendly access to rich human knowledge stored in large KBs such as Freebase, DBpedia and Wikidata, and it has broad applications in QA systems, recommender systems, and information retrieval systems.\nState-of-the-art (SOTA) solutions often take a semantic parsing (SP)-based approach. They translate an input natural language question into a structured, executable form (AKA logical form), which is then executed to retrieve the question answer. Figure 1 shows an example. The input question, Who is the author of Harry Potter, is expressed using the S-expression (a type of logical form), which is formed by a set of functions (e.g., JOIN) operated over elements of the target KB (e.g., entity m.078ffw refers to book series Harry Potter, book.author a class of entities, and book.literary_series.author a relation in Freebase).\nA key challenge here is to learn a mapping between mentions of entities and relations in the input question to corresponding KB elements to form the logical form. Meanwhile, the mapping of KB element compositions has to adhere to the structural constraints (schema) of the KB. The schema defines entities' classes and the relationships between these classes within the KB. Take the KB subgraph in Figure 1 as an example, the relationship between the entity Harry Potter and the entity J.K. Rolwing is defined by the relation book.literary_series.author between their respective classes (i.e., class book.literary_series and class book.author).\nHowever, due to the vast number of entities, relations, classes, and their compositions, it is difficult (if not impossible) to train a model with all feasible compositions of the KB elements. For example, Freebase has over 39 million entities, 8,000 relations, and 4,000 classes. Furthermore, some KBs (e.g., NED) are not static as they continue to grow.\nA few studies consider model generalizability to non-I.I.D. settings, where the test set contains schema items (i.e., relations and classes) or compositions that are unseen during training (i.e., zero-shot and compositional generalization, respectively). In terms of methodology, these studies typically use ranking-based or generation-based models. Ranking-based models retrieve entities relevant to the input question and then, starting from them, perform path traversal in the KB to obtain the target logical form by ranking. Generation-based models retrieve relevant KB contexts (e.g., entities and relations) for the input question, and then feed these contexts into a Seq2Seq model together with the input question to generate the logical form.\nWe observe that both types of models terminate their entity retrieval prematurely, such that each entity mention in the input question is mapped to only a single entity before the logical form generation stage. As a result, the logical form generation stage loses the freedom to explore the full combination space of relations and entities. This leads to inaccurate logical forms (as validated in our study).\nTo address this issue, our strategy is to defer entity disambiguation \u2013 i.e., to determine the most relevant entity for an entity mention (Section 2) \u2013 to the logical form generation stage. This allows our model to explore a larger combination space of the relations and entities, and ultimately leads to stronger model generalizability because low-ranked (but correct) relations or entities would still be considered during generation. We call our approach SG-KBQA (schema-guided logical form generator for KBQA). Concretely, SG-KBQA follows the generation-based approach but with deferred entity disambiguation. As shown in Figure 2, it feeds the input question, the retrieved candidate relations and entities, plus their corresponding schema information (the domain and range of classes of relations and entities; Section 4) into a large language model (LLM) for logical form generation. The schema information reveals the connectivity between the candidate relations and entities, hence guiding the LLM to uncover their correct combination in the large search space.\nFurther exploiting the schema-guided idea, we propose a relation-guided module for SG-KBQA to enhance its entity mention detection from the input question. As shown in Figure 2, this module adapts a Seq2Seq model to generate logical form sketches based on the input question and candidate relations, where relations, classes, and literals are masked by special tokens, such that the entity mentions can be identified more easily without confusions caused by these elements.\nTo summarise:\n\u2022 We introduce SG-KBQA to solve the KBQA problem under non-I.I.D. settings, where test input contains unseen schema items or compositions during training.\n\u2022 We propose to defer entity disambiguation to logical form generation, and additionally guide this generation step with corresponding schema information, allowing us to explore a larger combination space of relations and entities to consider unseen relations, entities, and compositions. We further propose a relation-guided module to strengthen entity retrieval by generating logical form sketches.\n\u2022 We conduct experiments on two popular benchmark datasets and find SG-KBQA outperforming SOTA models on both datasets. In particular, on non-I.I.D GrailQA our model tops all three leaderboards for the overall, zero-shot, and compositional generalization settings, outperforming SOTA models by 3.3%, 2.9%, and 4.0% (F1) respectively."}, {"title": "Related Work", "content": "Knowledge Base Question Answering Most KBQA solutions use information retrieval-based (IR-based) or semantic parsing-based (SP-based) methods. IR-based methods construct a question-specific subgraph starting from the retrieved entities (i.e., the topic entities). They then reason over the subgraph to derive the answer. SP-based methods focus on transforming input questions into logical forms, which are then executed to retrieve answers. SOTA solutions are mostly SP-based, as detailed next.\nKBQA under I.I.D. Settings Recent KBQA studies under I.I.D. settings fine-tune LLMs to map input questions to rough KB elements and generate approximate logical form drafts. The approximate (i.e., inaccurate or ambiguous) KB elements are then aligned to exact KB elements through a subsequent retrieval stage. These solutions often fail over test questions that refer to KB elements unseen during training. While we also use LLMs for logical form generation, we ground the generation with retrieved relations, entities, and schema contexts, thus addressing the non-I.I.D. issue.\nKBQA under Non-I.I.D. Settings Studies considering non-I.I.D. settings can be largely classified into ranking-based and generation-based methods. Ranking-based methods start from retrieved entities, traverse the KB, and construct the target logical form by ranking the traversed paths. Gu et al. enumerate and rank all possible logical forms within two hops of retrieved entities, while Gu et al. incrementally expand and rank paths from retrieved entities.\nGeneration-based methods transform an input question into a logical form using a Seq2Seq model. They often use additional contexts beyond the question to augment the input of the Seq2Seq model and enhance its generalizability. For example, Ye et al. use top-5 candidate logical forms enumerated from retrieved entities as the additional context. Shu et al. further use top-ranked relations, disambiguated entities, and classes (retrieved separately) as the additional context. Zhang et al. use connected pairs of retrieved KB elements.\nOur SG-KBQA is generation-based. We use schema contexts (relations and classes) from retrieved relations and entities, rather than separate class retrieval which could introduce noise. We also defer entity disambiguation to the logical form generation stage, thus avoiding error propagation induced by premature entity disambiguation without considering the generation context, as done in existing works outlined below.\nKBQA Entity Retrieval KBQA entity retrieval typically has three steps: entity mention detection, candidate entity retrieval, and entity disambiguation. BERT-based named entity recognition is widely used for entity mention detection from input questions. To retrieve KB entities corresponding to entity mentions, the FACC1 dataset is commonly used, which contains over 10 billion surface forms (with popularity scores) of Freebase entities. Gu et al. use the popularity scores for entity disambiguation, while Ye et al. and Shu et al. adopt a BERT reranker."}, {"title": "Preliminaries", "content": "A graph structured-KB G is composed of a set of relational facts {(s, r, o)|s \u2208 E, r\u2208 R, o \u2208 EUL} and an ontology {{Cd,r,Cr)|Cd, Cr \u2208 C,r \u2208 R}. Here, & denotes a set of entities, R denotes a set of relations, and L denotes a set of literals, e.g., textual labels, numerical values, or date-time stamps. In a relational fact (s, r, o), s \u2208 E is the subject, O\u2208 EU L is the object, and r \u2208 R represents the relationship between the subject and the object.\nThe ontology defines the rules governing the composition of relational facts within G. In its formulation, C denotes a set of classes, each of which defines a set of entities (or literals) sharing common properties (relations). Note that an entity can belong to multiple classes. In an ontology triple (Cd, r, Cr), cd is called a domain class, and it refers to the class of subject entities that satisfy relation r; cr is called the range class, and it refers to the class of object entities or literals satisfying r. Each ontology triple can be instantiated as a set of relational facts. In Figure 1,  is an ontology triple. An instance of it is <Harry Potter, book.literary_series.author, J.K. Rowling>, where Harry Potter is an entity that belongs to class book.literary_series.\nProblem Statement Given a KB G and a question q expressed in natural language, i.e., a sequence of word tokens, knowledge base question answering (KBQA) aims to find a subset (the answer set) AC EU L of elements from G that with optional application of some aggregation functions (e.g., COUNT) answers q.\nLogical Form We solve the KBQA problem by translating the input question q into a structured query that can be executed on G to fetch the answer set A. Following previous works"}, {"title": "The SG-KBQA Model", "content": "As shown in Figure 3, SG-KBQA follows the common structure of generation-based models. It has two overall stages: relation and entity retrieval and logical form generation. We propose novel designs in both stages to strengthen model generalizability.\nIn the relation and entity retrieval stage (Section 4.1), SG-KBQA retrieves candidate relations and entities from KB G which may be relevant to the input question q. It starts with a BERT-based relation ranking model to retrieve candidate relations relevant to q. Together with q, the set of top-ranked candidate relations are fed into a novel, relation-guided Seq2Seq model to generate logical form sketches that contain entity mentions while masking the relations and classes. We harvest the entity mentions and use them to retrieve candidate entities from G. We propose a combined relation-based strategy to prune the entities (as there may be many). The remaining entities are ranked by a BERT-based model, indicating their likelihood of being the entity that matches each entity mention.\nLeveraging relations to guide both entity mention extraction and candidate entity pruning enhances the model generalizability over entities unseen during training. This in turn helps the logical form generation stage to filter false positive matches for unseen relations or their combinations.\nIn the logical form generation stage (Section 4.2), SG-KBQA feeds q, the top-ranked relations and entities (corresponding to each entity mention), and the schema contexts (i.e., domain and range classes of the relations and classes of the entities), into an adapted LLM to generate the logical form and produce answer set A.\nOur schema-guided logical form generation procedure is novel in that it takes (1) multiple candidate entities (instead of one in existing models) for each entity mention and (2) the schema contexts as the input. Using multiple candidate entities essentially defers entity disambiguation, which is usually done in the retrieval stage by existing models, to the generation stage, thus mitigating error propagation. This strat-"}, {"title": "Relation and Entity Retrieval", "content": "Relation Retrieval For relation retrieval, we follow the schema retrieval model of TIARA, as it has high accuracy. We extract a set Rq of top-kR (system parameter) relations with the highest semantic similarity to q. This is done by a BERT-based cross-encoder that learns the semantic similarity sim(q, r) between q and a relation r \u2208 R:\n$$sim(q, r) = LINEAR(BERT_{CLS}([q; r]))$$,\nwhere ';' denotes concatenation. This model is trained with the sentence-pair classification objective, where a relevant question-relation pair has a similarity of 1, and 0 otherwise.\nRelation-Guided Entity Mention Detection Given Rq, we propose a relation-guided logical form sketch parser to parse q into a logical form sketch s. Entity mentions in q are extracted from s. The parser is an adapted Seq2Seq model. The model input of each training sample takes the form of \\\u201cq  r1;r2; ...;rkr\\\u201d (ri \u2208 Rq, hence \\\u201crelation-guided\\\u201d). In the ground-truth logical form corresponding to q, we mask the relations, classes, and literals with special tokens \u2018', \u2018', and '' to form the ground-truth logical form sketch s. Entity IDs are also replaced by the corresponding entity names (entity mentions), to enhance the Seq2Seq model\u2019s understanding of the semantics of entities.\nAt model inference, from the output top-k\u2081 (system parameter) logical form sketches (using beam search), we extract the entity mentions.\nRelation-Guided Candidate Entity Retrieval We follow previous studies and use an entity name dictionary FACC1 to map extracted entity mentions to entities (i.e., their IDs in KB), although other retrieval models can be used. Since different entities may share the same name, the entity mentions may be mapped to many entities. For pruning, existing studies use popularity scores associated to entities.\nTo improve the recall of candidate entity retrieval, we propose a combined pruning strategy based on both popularity and relation connectivity. As Figure 4 shows, we first select the top-kE1 (system parameter) entities for each entity mention based on popularity and then extract kE2 (system parameter) entities from the remaining candidates that are connected to the retrieved relations Rq. Together, these form the candidate entity set Ec.\nEntity Ranking We follow existing works to score and rank each candidate entity in E by jointly encoding q and the context (entity name and its linked relations) of the entity using a cross-encoder (like Eq. 1). We select the top-kE3 (system parameter) ranked entities for each mention as the entity set Eq for each question."}, {"title": "Schema-Guided Logical Form Generation", "content": "Given relations Rq and entities Eq, we fine-tune an open-souce LLM (LLaMA3.1-8B by default) to generate the final logical form.\nBefore being fed into the model, each relation and entity is augmented with its schema context (i.e., class information) to help the model to learn their connections and generalize to unseen entities, relations, or their compositions. The context of a relation r is described by concatenating the relation's domain class ca and range class cr, formatted as \\\u201c[D] cd [N] r [R] cr\\\u201d. For an entity e, its context is described by its ID (\\\u201cide\\\u201d), name (\\\u201cnamee\\\u201d), and the intersection of its set of classes Ce and the set of all domain and range classes CR of all relations in Rq, formatted as \\\u201c[ID] ide [N] namee [C] class (Ce\u2229CR)\\\u201d.\nAs Figure 3 shows, we construct the input to the logical form generation model by concatenating q with the context of each relation in Rq and the context of each entity in Eq. The model is fine-tuned with a cross-entropy-based objective:\n$$L_{generator} = \\sum_{t=1}^{n} log \\ p(l_t | l_{<t}, q, K_q)$$,\nwhere I denotes a logical form of n tokens and lt is its t-th token, and Kq is the retrieved knowledge (i.e., relations and entities with contexts) for q. At inference, the model runs beam search to generate top-ko logical forms \u2013 the executable one with the highest score is selected as the output. See Appendix B for a prompt example used for inference.\nIt is possible that no generated logical forms are executable. In this case, we fall back to following Shu et al. and Ye et al. and retrieve candidate logical forms in two stages: enumeration and ranking. During enumeration, we search the KB by traversing paths starting from the retrieved entities. Due to the exponential growth in the number of candidate paths with each hop, we start from the top-1 entity for each mention and searches its neighborhood for up to two hops. The paths retrieved are converted into logical forms. During ranking, a BERT-based ranker scores q and each enumerated logical form l (like Eq. 1). We train the ranker using a contrastive objective:\n$$L=\\frac{exp(sim(q, l^*))}{exp(sim(q, l^*)) + \\sum_{l \\in C_l, l \\neq l^*} exp(sim(q, l))}$$,\nwhere l* is the ground-truth logical form and Ci is the set of enumerated logical forms. We run the ranked logical forms from the top and return the first executable one."}, {"title": "Experiments", "content": "We run experiments to answer: Q1: How does SG-KBQA compare with SOTA models in their accuracy for the KBQA task? Q2: How do model components impact the accuracy of SG-KBQA? Q3: How do our techniques generalize to other KBQA models?\nFollowing SOTA competitors , we use two benchmark datasets built upon Freebase.\nGrailQA is a dataset for evaluating the generalization capability of KBQA models. It contains 64,331 questions with annotated target S-expressions, including complex questions requiring up to 4-hop reasoning over Freebase, with aggregation functions including comparatives, superlatives, and counting. The dataset comes with training (70%), validation (10%), and test (20%, hidden and only known by the leaderboard organizers) sets. In the validation and the test sets, 50% of the questions include KB elements that are unseen in the training set (zero-shot generalization tests), 25% consist of unseen compositions of KB elements seen in the training set (compositional generalization tests), and the remaining 25% are randomly sampled from the training set (I.I.D. tests).\nWebQuestionsSP (WebQSP) is a dataset for the I.I.D. setting. While our focus is on non-I.I.D. settings, we include results on this dataset to show the general applicability of SG-KBQA. WebQSP contains 4,937 questions. More details of WebQSP are included in Appendix C.\nCompetitors We compare with both IR-based and SP-based methods including the SOTA models. On GrailQA, we compare with models that top the leaderboard, including RnG-KBQA, TIARA, DecAF, Pangu (previous SOTA as of 15th February, 2025), FC-KBQA, TIARA+GAIN, and RetinaQA. We also compare with few-shot LLM (training-free) methods: KB-BINDER (6)-R, Pangu, and FlexKBQA. These models are SP-based. On the non-I.I.D. GrailQA, IR-based methods are uncompetitive and excluded. On WebQSP, we compare with IR-based models SR+NSM, UNIKGQA, and EPR+NSM, plus SP-based models ChatKBQA (SOTA) and TFS-KBQA (SOTA), both of which use a fine-tuned LLM to generate logical forms. We also compare with TIARA, Pangu, and FC-KBQA as above, which represent SOTA models using pre-trained language models (PLMs). Appendix D details these models.\nImplementation Details All our experiments are run on a machine with an NVDIA A100 GPU and 120 GB of RAM. We fine-tuned three bert-base-uncased models for a maximum of three epochs each, for relation retrieval, entity ranking, and fallback logical form ranking."}, {"title": "Overall Results (Q1)", "content": "Tables 1 and 2 show the overall comparison of SG-KBQA with the baseline models for GrailQA and WebQSP, respectively. SG-KBQA shows the best results across both datasets.\nResults on GrailQA On the overall hidden test set of GrailQA, SG-KBQA outperforms the best baseline Pangu by 4.9% and 3.3% in the EM and F1 scores, respectively. Under the compositional and zero-shot generalization settings (both are non-I.I.D.), similar performance gaps are observed, i.e., 4.0% and 2.9% in F1 compared to the best baseline models, respectively. This validates that SG-KBQA can extract relations and entities more accurately from the input question, even when these are unseen in the training set, and it creates more accurate logical forms to answer the questions.\nThe fine-tuned baseline models do not use relation semantics to enhance entity retrieval, and they either omit the class contexts in logical form generation or use these classes separately for retrieval. As such, they do not generalize as well in the non-I.I.D. settings. The few-shot LLM-based competitors are generally not very competitive, especially under the non-I.I.D. settings. This suggests that the current generation of LLMs are unable to infer from a few input demonstrations the process of logical form generation from user questions. Fine-tuning is still required.\nResults on WebQSP On WebQSP, which has an I.I.D. test set, the performance gap of the different models are closer. Even in this case, SG-KBQA still performs the best, showing its general applicability. Comparing with TFS-KBQA (SOTA) and ChatKBQA, SG-KBQA improves the F1 score by 0.5%. Among IR-based methods, UniKGQA (SOTA) still performs substantially worse compared to SG-KBQA. The lower performance of IR-based methods is consistent with existing results."}, {"title": "Ablation Study (Q2)", "content": "Next, we run an ablation study with the following variants of SG-KBQA: w/o RG-EMD replaces our relation-guided entity mention detection with SpanMD which is commonly used in existing models; w/o RG-CER omits candidate entities retrieved from the top relations; w/o DED uses the top-1 candidate entity for each entity mention without deferring entity disambiguation; w/o SC omits schema contexts from logical form generation.\nTable 3 shows the results on the validation set of GrailQA and the test set of WebQSP. Only F1 scores are reported for conciseness, as the EM scores on GrailQA exhibit similar comparative trends and are provided in Appendix F.\nAll model variants have lower F1 scores than those of the full model, confirming the effectiveness of the model components. SG-KBQA w/o DED (with schema contexts) reduces the F1 scores across various generalization settings on both datasets, demonstrating the effectiveness of our DED strategy in reducing error propagation during the retrieval and generation stages. Furthermore, SG-KBQA w/o SC (with deferred entity disambiguation) has the most significant drops in the F1 score under the compositional (7.2) and zero-shot (14.0) generalization tests. It highlights the importance of schema contexts in constraining the larger search space introduced by DED and in generalizing to unseen KB elements and their combinations. Meanwhile, the lower F1 of SG-KBQA w/o RG-EMD emphasizes the capability of our relation-guided entity mention detection module in strengthening KBQA entity retrieval."}, {"title": "Module Applicability (Q3)", "content": "Our relation-guided entity retrieval (RG-EMD & RG-CER) module and schema-guided logical form generation (DED & SC) module can be applied to existing KBQA models. We showcase such applicability with the TIARA model. As shown in Table 4, by replacing the retrieval and generation modules of TIARA with ours, the F1 scores increase consistently for the non-I.I.D. tests.\nTable 4 further reports F1 scores of SG-KBQA when we replace LLaMA3.1-8B with T5-base (which is used by TIARA), and DeepSeek-R1-Distill-Llama-8B for logical form generation. We see that, even with the same T5-base model for the logical form generator, SG-KBQA outperforms TIARA consistently. This further confirms the effectiveness of our model design. As for DS-R1-8B, it offers accuracy slightly lower than that of the default LLaMA3.1-8B model. We conjecture that this is because DS-R1-8B is distilled from DeepSeek-R1-Zero, which focuses on reasoning capabilities and is not specifically optimized for the generation task."}, {"title": "Conclusion", "content": "We proposed SG-KBQA for the KBQA task. Our core innovations include: (1) using relation to guide the retrieval of entities; (2) deferring entity disambiguation to the logical form generation stage; and (3) enriching logical form generation with schema contexts to constrain search space. Together, we achieve a model that tops the leaderboard of a popular non-I.I.D. dataset GrailQA, outperforming SOTA models by 4.0%, 2.9%, and 3.3% in F1 under compositional generalization, zero-shot generalization, and overall test settings, respectively. Our model also performs well in the I.I.D. setting, outperforming SOTA models on WebQSP."}, {"title": "Limitations", "content": "First, like any other supervised models, SG-KBQA requires annotated samples for training which may be difficult to obtain for many domains. Exploiting LLMs to generate synthetic training data is a promising direction to address this issue. Second, as discussed in the error analysis in Appendix K, errors can still arise from the relation retrieval, entity retrieval, and logical form generation modules. There are rich opportunities in further strengthening these modules. Particularly, as we start from relation extraction, the overall model accuracy relies on highly accurate relation extraction. It would be interesting to explore how well SG-KBQA performs on even larger KBs with more relations."}, {"title": "Ethics Statement", "content": "This work adheres to the ACL Code of Ethics and is based on publicly available datasets, used in compliance with their respective licenses. As our data contains no sensitive or personal information, we foresee no immediate risks. To promote reproducibility and further research, we also open-source our code."}, {"title": "S-Expression", "content": "S-expressions use set-based semantics defined over a set of operators and operands. The operators are represented as functions. Each function takes a number of arguments (i.e., the operands). Both the arguments and the return values of the functions are either a set of entities or entity tuples (or tuples of an entity and a literal). The functions available in S-expressions are listed in Table 5, where a set of entities typically refers to a class (recall that a class is defined as a set of entities sharing common properties) or individual entities, and a binary tuple typically refers to a relation."}, {"title": "Prompt Example", "content": "We show an example prompt to our fine-tuned LLM-based logical form generator containing top-20 relations and top-2 entities per mention retrieved by our model in Figure 5."}, {"title": "Additional Details on the WebQSP Dataset", "content": "WebQuestionsSP (WebQSP) is an I.I.D. dataset. It contains 4,937 questions collected from Google query logs, including 3,098 questions for training and 1,639 for testing, each annotated with a target SPARQL query. We follow GMT-KBQA, TIARA to separate 200 questions from the training questions to form the validation set."}, {"title": "Baseline Models", "content": "The following models are tested against SG-KBQA on the GrailQA dataset:\n\u2022 RnG-KBQA enumerates and ranks all possible logical forms within two hops from the entities retrieved by an entity retrieval step. It uses a Seq2Seq model to generate the target logical form based on the input question and the top-ranked candidate logical forms.\n\u2022 TIARA shares the same overall procedure with RnG-KBQA. It further retrieves entities, relations, and classes based on the input question and feeds these KB elements into the Seq2Seq model together with the question and the top-ranked candidate logical forms to generate the target logical form.\n\u2022 TIARA+GAIN enhances TIARA using a training data augmentation strategy. It synthesizes additional question-logical form pairs for model training to enhance the model's capability to handle more entities and relations. This is done by a graph traversal to randomly sample logical forms from the KB and a PLM to generate questions corresponding to the logical forms (i.e., the \"GAIN\" module). TIARA+GAIN is first tuned using the synthesized data and then tuned on the target dataset, for its retriever and generator modules which both use PLMs.\n\u2022 Decaf uses a Seq2Seq model that takes as input a question and a linearized question-specific subgraph of the KG and jointly decodes into both a logical form and an answer candidate. The logical form is then executed, which produces a second answer candidate if successful. The final answer is determined from these two answer candidates with a scorer model.\n\u2022 Pangu formulates logical form generation as an iterative enumeration process starting from the entities retrieved by an entity retrieval step. At each iteration, partial logical forms generated so far are extended following paths in the KB to generate more and longer partial logical forms. A language model is used to select the top partial logical forms to be explored in the next iteration, under either fined-tuned models (T5-3B) or few-shot in-context learning (Codex).\n\u2022 FC-KBQA employs an intermediate module to test the connectivity between the retrieved KB elements, and it generates the target logical form using the connected pairs of the retrieved KB elements through a Seq2Seq model.\n\u2022 RetinaQA uses both a ranking-based method and a generation-based method (TIARA) to generate logical forms, which are then scored by a discriminative model to determine the output logical form.\n\u2022 KB-BINDER uses a training-free few-shot in-context learning model based on LLMs. It generates a draft logical form by showcasing the LLM examples of questions and logical forms (from the training set) that"}, {"title": "Implementation Details", "content": "All our experiments are run on a machine with an NVDIA A100 GPU and 120 GB of RAM. We fine-tuned three bert-base-uncased models for a maximum of three epochs each, for relation retrieval, entity ranking, and fallback logical form ranking. For relation retrieval, we randomly sample 50 negative samples for each question to train the model to distinguish between relevant and irrelevant relations.\nFor each dataset, a T5-base model is fine-tuned for 5 epochs as our logical form sketch parser, with a beam size of 3 (i.e., k\u2081 = 3) for GrailQA, and 4 for WebQSP. For candidate entity retrieval, we use the same number (i.e., kE1 + kE2 = 10) of candidate entities per mention as that used by the baseline models. The retrieved candidate entities for a mention consist of entities with the top-kE1 popularity scores and kE2 entities connected to the top-ranked relations in Rq, where ke1 = 1, kE2 = 9 for GrailQA, kE1 = 3, KE2 = 7 for WebQSP. We select the top-20 (i.e., KR = 20) relations and the top-2 (i.e., ke3 = 2) entities (for each entity mention) retrieved by our model. For WebQSP, we also use the candidate entities obtained from the off-the-shelf entity linker ELQ.\nFinally, we fine-tune LLaMA3.1-8B with LORA for logical form generation. On GrailQA, LLaMA3.1-8B is fine-tuned for 5 epochs with a learning rate of 0.0001. On WebQSP, it is fine-tuned for 20 epochs with the same learning rate (as it is an I.I.D. dataset where more epochs are beneficial). During inference, we generate logical forms by beam search with a beam size of 10 (i.e., Ko = 10). The generated logical forms are executed on the KB to filter non-executable ones. If none of the logical forms are executable, we check candidate logical forms from the fallback procedures, and the result of the first executable"}, {"title": "Full Ablation Study Results (GrailQA)", "content": "Table 6 presents the full ablation study results on the validation set of GrailQA. We observe a similar trend to that of the F1 score results reported earlier all ablated model variants yield lower EM scores compared to the full model.\nFor the retrieval modules, RG-EMD improves the F1 score by 3.2 points and the EM score by 3.8 points on GrailQA (i.e., SG-KBQA vs. SG-KBQA w/o RG-EMD for overall results), while achieving a 1.9-point increase in the F1 score on WebQSP (see Table 2 earlier). It achieves an increase of 3.4 points or larger in the F1 score on the compositional and zero-shot tests, which is larger than the 2.2-point improvement on the I.I.D. tests. This shows that relation-guided mention detection effectively enhances the generalization capability of KBQA entity retrieval. For the other module RG-CER, removing it (SG-KBQA w/o RG-CER) results in a 2.5-point drop in the F1 score for both the I.I.D. and compositional tests, while the impact is smaller on the zero-shot tests (1.6 points). This is because the lower accuracy in relation retrieval under zero-shot tests leads to error propagation into relation-guided candidate entity retrieval, reducing the benefits of this module.\nFor the generation modules, SG-KBQA w/o DED negatively impacts the F1 scores on both GrailQA and WebQSP, confirming that deferring entity disambiguation effectively mitigate error propagation between the retrieval and generation stages. For SG-KBQA w/o SC, it reduces the F1 score by 1.7 points and 3.2 points on the GrialQA I.I.D. tests and on WebQSP. The drop is more significant on the compositional and zero-shot tests, i.e., by 6.2 points and 14.0 points, respectively. This indicates that schema contexts can effectively guide the LLM to reason and identify the correct combinations of KB elements unseen at training.\nIn Table 6, we present an additional model variant, SG-KBQA w/o Fallback LF, which removes the fall back logical form generation strategy from SG-KBQA. We see that SG-KBQA has lower accuracy without the strategy. We note that this fallback strategy is not the reason why SG-KBQA outperforms the baseline models. TIARA also uses this fallback strategy, while RetinaQA uses the top executable logical form from the fallback strategy as one of the options to be selected by its discriminator to determine the final logical form output."}, {}]}