{"title": "BONE: BLOCK AFFINE TRANSFORMATION AS PARAMETER EFFICIENT FINE-TUNING METHODS FOR LARGE LANGUAGE MODELS", "authors": ["Jiale Kang"], "abstract": "As Large Language Models (LLMs) continue to grow in size, their computational and memory requirements increase correspondingly. Consequently, the exploration of cost-effective and efficient fine-tuning methods has become increasingly important. Low-Rank Adaptation (LoRA) has achieved remarkable training results by freezing the original weights and training only low-rank matrices, establishing itself as the predominant fine-tuning method for LLMs. In pursuit of performance closer to full-parameter training, a series of LoRA variants have emerged, such as LoRA+, PISSA, Olora, and LoRA-GA. However, these methods also make the fine-tuning initialization process more complex, and it remains challenging to surpass the performance ceiling of full fine-tuning. To address these issues, this paper introduces an innovative method called Bone (Block Affine), which not only reduces memory overhead but also emphasizes the internal connections between weights, leading to faster convergence and better data fitting. Experimental comparisons across two different LLM architectures (LLaMA2, RWKV6) and various parameter scales demonstrate that the Bone structure can achieve rapid convergence and superior data fitting without the need for complex initialization. For example, when fine-tuning LLaMA2-7B on the MetaMathQA dataset and validating on GSM8k and math benchmarks, Bone achieved fine-tuning scores of 49.36 and 8.8, respectively, outperforming PISSA by 5.84% and 1.96%.", "sections": [{"title": "1 INTRODUCTION", "content": "Large models have been integrated into various industries, revolutionizing many traditional technologies Radford et al. (2019); Raffel et al. (2020). However, general-purpose large models often struggle to meet the needs of all downstream tasks, making it necessary to fine-tune base models for specific scenarios. Full-scale fine-tuning of large models is extremely costly; for example, training the LLaMA2-7B Touvron et al. (2023) model with bfloat16 Wang & Kanwar (2019) precision requires around 60GB of VRAM. In contrast, by using PEFT (Parameter-Efficient Fine-Tuning)Xu et al. (2023) techniques, fine-tuning can be done with just a single 24GB VRAM GPU. As a result, numerous PEFT techniques and quantization methods have emerged to reduce the training costs of large models. LoRA (Low-Rank Adaptation) Hu et al. (2021) has become one of the most popular PEFT methods due to its small parameter size, its ability to optimize the model's main weights, and the fact that it doesn't add extra inference overhead.\nLORA significantly reduces memory usage by freezing the original weights W and updating two low-rank matrices A and B. Typically, either A or B is initialized to zero, ensuring that the initial state of LoRA is consistent with the pre-trained model, The Figure 2a illustrates the structure visualization. However, extensive experiments Ding et al. (2023); Liu et al. (2024b); Biderman et al. (2024) have shown that LoRA's convergence is significantly slower compared to full fine-tuning. This slow convergence is likely due to the small gradients caused by the zero initialization of either A or B. To address this issue, researchers have proposed several LoRA variants, such as LoRA+ Hayou et al. (2024), PISSA Meng et al. (2024), and LoRA-GAWang et al. (2024). Despite their excellent performance, these LoRA variants inevitably introduce complexity into the fine-tuning process and"}, {"title": "2 RELATED WORKS", "content": "The PEFT (Parameter-Efficient Fine-Tuning) techniques are diverse and include approaches like adapter tuningHoulsby et al. (2019), prefix tuningLiu et al. (2023), prompt tuningBrown (2020), LoRAHu et al. (2021), and layer-freezing methods such as LISA.\nThe adapter method does not require fine-tuning all the parameters of the pre-trained model. Instead, it introduces a small number of task-specific parameters to store knowledge related to that task, thereby reducing the computational demands of model fine-tuning. Prefix tuning is a lightweight fine-tuning method for generative tasks. It adds a continuous, task-specific vector sequence, called a prefix, to the input. Unlike prompts, prefixes are entirely composed of free parameters and do not"}, {"title": "3 BONE: BLOCK AFFINE", "content": "In this section, we will introduce the Bone method. Unlike the LoRA series of architectures, Bone employs a single learnable matrix, initialized to zero and named \"bone.\" This matrix interacts with the model's matrix W through Block Affine operations to represent AW. This approach ensures that the initial state remains unchanged while also facilitating the integration of information between the weights. Assuming the matrix W has dimensions (n, n), ideally, we train with a full-rank matrix bone (n, n). The specific calculation for Block Affine is as follows:\n\n$Y = X(W + \\Delta W)$ (1)\n\n$\\Delta W_{n,n} = W_{n,n} bone_{n,n} + bone_{n,n}$ (2)\nBlock Affine not only efficiently promotes information exchange between weights but also enhances the weight utilization of low-rank matrices. We conducted ablation experiments to verify the effectiveness of using a single learnable matrix structure. The comparison formulas are as follows:\n\n$\\Delta W_{n,n} = W_{n,n} bone\\_A_{n,n} + bone\\_B_{n,n}$ (3)\nWe know that in LLMs, the matrix W is typically very large, and in most cases, the number of rows and columns are not equal. Setting bone to the same dimensions as W would result in excessive computational overhead, which goes against the principles of fine-tuning. Therefore, we adjust bone to be a low-rank matrix. When the weight matrix W has dimensions (n, m), it is divided according to a block_size b into W(n/b,b, m/b,b), yielding (n/b) \u00d7 (m/b) blocks. Inspired by GQA(Grouped Query Attention)Ainslie et al. (2023), we group these blocks together to significantly reduce the number of trainable parameters without affecting performance. Different groups share a single Bone matrix for computation. Thus, the Bone matrix is set to (g, b, b), where g is the group size. The W matrix is reshaped to (gn, g, b, b), where gn represents the number of groups. The Bone structure is illustrated in Figure 2b. The final formula can be expressed as:\n\n$W_{g_n,g,b,b} = Reshape(W_{n,m})$ (4)\n\n$\\Delta W_{n,m} = Reshape(W_{g_n,g,b,b} bone_{g,b,b} + bone_{g,b,b})$ (5)\nDuring our comparative experiments between RWKV6 and LLaMA2, we found that the matrix W has several different dimensions. For example, in RWKV6-7B, the dimension of ffn's k is (4096 * 3.5, 4096), while in LLaMA2-7B, the dimension of gate_proj in the mlp part is"}, {"title": "4 EXPERIMENTS", "content": "The experiments were conducted on 4\u00d7NVIDIA 4090 24G GPUs.In our experiments, we fixed lora_r=32 to ensure consistent trainable parameters across different fine-tuning methods, using the AdamW optimizer with a batch size of 64, a learning rate of 2e-5 and a warmup ratio of 0.0, without any weight decay.\n\nThis subsection explores the upper limits of the Bone structure by varying the block_size in the Bone matrix. Notably, while we change the size of b, the value of n remains constant. Therefore, as b increases, the dimensions of Bone transform into the bone_g form, leading to a corresponding reduction in the number of blocks within each group. Comparative experiments were conducted by fine-tuning LLaMA2-7B on the MetaMathQA dataset and validating on GSM8K and Math benchmarks. The test results, as shown in Table 3, demonstrate that the fine-tuning performance improves as the value of b increases. Notably, when b reaches 16, the Bone structure, with only one-quarter of the trainable parameters compared to PiSSA, surpasses PiSSA's performance on the GSM8k benchmark. However, its performance on the Math benchmark is weaker, indicating that the value of b plays a crucial role in determining the model's generalization ability. As b increases, the model's generalization capacity strengthens.\n\nIn this subsection, we explore the impact of different grouping methods in the Bone structure on model fine-tuning performance. Due to structural differences in the weight matrix W, the Bone-free grouping requires manual configuration, which is inconvenient. Therefore, this subsection only compares row-wise and column-wise grouping, both of which can be easily extended to any structure. We fine-tuned LLaMA2-7B on the MetaMathQA dataset and validated the results on GSM8k"}, {"title": "5 RESOURCE AND EFFICIENCY", "content": "Table 6 compares the training resources and token throughput required for fine-tuning RWKV6-3B using Bone and LoRA on a single 4090 GPU. The specific fine-tuning settings are as follows: batchsize = 1, context length (ctx_len) = 512. Due to the structure of Bone, an additional intermediate variable for the W weight is created during training, which increases memory usage by 5GB compared to LoRA. Additionally, Bone's token throughput is only two-thirds of LoRA's. To address this, we applied checkpointing specifically to the Bone structure, eliminating the extra intermediate variable, which successfully reduced memory usage but also further decreased token throughput.\nDespite the fact that the Bone structure significantly outperforms LoRA variants in fine-tuning, we cannot ignore the efficiency issues. Therefore, a key focus of our future work will be improving the Bone operator to enhance token throughput and reduce memory usage."}, {"title": "6 CONCLUSION", "content": "This paper introduces a new PEFT technique called Bone(Block Affine), which employs block affine transformations on the weight matrix to capture the internal correlations within W, while also avoiding the complexity of initialization. Through extensive experiments, we found that Bone can achieve a rapid drop in loss during the early stages of training, similar to or even better than PISSA. Additionally, Bone demonstrates superior data fitting in the later stages of training and shows exceptional performance across different LLM architectures and parameter scales, proving its versatility. Bone brings new possibilities to existing LLM PEFT techniques. Instead of focusing solely on optimizing LoRA, we should shift our attention to innovative PEFT methods that are better suited to the architecture of LLMs."}, {"title": "7 LIMITATIONS AND FUTURE WORK", "content": "Due to limited training resources, this paper lacks comparative experiments on full fine-tuning. However, the extensive experiments presented demonstrate that Bone possesses strong data fitting and generalization capabilities, paving the way for new directions in PEFT development. However, there are still some issues that require further consideration and resolution:\n1. While the Bone architecture shows better training performance, its peak memory usage is higher compared to LoRA and its variants.\n2. It remains to be seen whether Bone is suitable for large models in other modalities.\nWe welcome the community to provide additional suggestions and conduct further tests."}]}