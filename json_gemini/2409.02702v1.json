{"title": "Incorporating Like-Minded Peers to Overcome Friend Data Sparsity in Session-Based Social Recommendations", "authors": ["Chunyan An", "Yunhan Li", "Qiang Yang", "Winston K.G. Seah", "Zhixu Li", "Conghao Yang"], "abstract": "Session-based Social Recommendation (SSR) leverages social relationships within online networks to enhance the performance of Session-based Recommendation (SR). However, existing SSR algorithms often encounter the challenge of \"friend data sparsity\". Moreover, significant discrepancies can exist between the purchase preferences of social network friends and those of the target user, reducing the influence of friends relative to the target user's own preferences. To address these challenges, this paper introduces the concept of \"Like-minded Peers\" (LMP), representing users whose preferences align with the target user's current session based on their historical sessions. This is the first work, to our knowledge, that uses LMP to enhance the modeling of social influence in SSR. This approach not only alleviates the problem of friend data sparsity but also effectively incorporates users with similar preferences to the target user. We propose a novel model named Transformer Encoder with Graph Attention Aggregator Recommendation (TEGAARec), which includes the TEGAA module and the GAT-based social aggregation module. The TEGAA module captures and merges both long-term and short-term interests for target users and LMP users. Concurrently, the GAT-based social aggregation module is designed to aggregate the target users' dynamic interests and social influence in a weighted manner. Extensive experiments on four real-world datasets demonstrate the efficacy and superiority of our proposed model and ablation studies are done to illustrate the contributions of each component in TEGAARec.", "sections": [{"title": "1. Introduction", "content": "Session-based recommendation (SR) is tasked with modeling a user's interest preferences based on historical data, such as the user's past purchases within a given timeframe, and predicting the next item of interest [18, 30, 31, 33, 39]. Previous studies have demonstrated that incorporating users' social relationships can enhance recommendation performance [6, 19, 26, 38]. When social relationships are integrated into the session-based recommendation task, it becomes known as session-based social recommendation (SSR) [2, 34].\nNumerous methods have been proposed to enhance the performance of SSR tasks, which can be broadly categorized into two groups, i.e., historical behavioral information based methods [4, 5, 17, 32, 33, 36, 37] and social/interactive information based methods [2, 11, 16, 25]. The first group focuses on leveraging historical behavioral information to predict the recommendation information of the target user. Early methods included collaborative filtering techniques [15], while more recent approaches focus on treating session data as sequences and modeling them using recurrent neural networks (RNNs) [8, 12, 15, 21]. The second group centers on information aggregation, wherein the target user's embedded representation is learned by integrating information from both the user-user social network and the user-item interaction network [2, 16, 25]. Many existing methods adopt Graph Neural Networks (GNNs) to construct their models, aiming to capture the intricate item transitions within sessions [21, 23, 27-29].\nHowever, real-world SSR algorithms encounter several challenges. Firstly, the issue of \"friend data sparsity\" arises, where the recommendation data may lack sufficient information about the friends of the target user, resulting in lost interaction data in the current or historical sessions. Secondly, the preferences of social network friends may differ significantly from those of the target user, and the influence of friends may be limited compared to the target user's own preferences especially facing the problem of friend data sparsity, potentially leading to inaccurate recommendation. Last but not the least, users' interests may evolve dynamically over time, posing a considerable challenge in accurately modeling interest changes of users [25, 28, 31].\nTo tackle these challenges, we propose the concept of \u201cLike-minded Peers\u201d (LMP) to represent users whose preferences are aligned with the target user's current session in their history sessions. We posit that users who exhibit similarity in interaction patterns with the target user, even if they are not social friends, can offer valuable insights. To the best of our knowledge, this is the first work using LMP to enhance SSR. It not only alleviates the problem of friend data sparseness but also effectively incorporates users with similar preferences to the target user. For example, consider a scenario where a mother purchases diapers and formula. In this case, she is more likely to share interests with other"}, {"title": "2. Related Work", "content": "Session-based recommendation tasks involve sequential recommendation, where early approaches predominantly utilized models from the Recurrent Neural Network (RNN) family to capture temporal order [31]. For example, Hidasi et al. [8] proposed to use a variant of RNN, namely the Gated Recurrent Unit (GRU) [3], to treat sessions as sequence prediction tasks for session-based recommendation. Li et al. [12] further improved upon this by incorporating attention mechanisms into the GRU to better capture users' sequential behaviors and personal intentions.\nGNN-based methods have been introduced to address the limitations of RNN models in capturing complex item transitions within sessions. Wu et al. [33] proposed the SR-GNN model, which represents sessions as directed graphs and learns item transitions using Gated Graph Neural Networks (GGNN) [14]. Xu et al. [35] proposed a GC-SAN model that introduces an attention mechanism to learn global dependencies on top of SR-GNN to enhance the session representation. Li et al. [13] proposed a HIDE model that goes a step further by introducing a hypergraph neural network approach to the SR task, which models possible interest transitions from different perspectives, and then uses a combination of micro- and macro-approaches to learn the intent behind each clicked item. However, the increasing complexity of GNN-based approaches has led to only marginal improvements. Zhang et al. [37] introduced the Attn-mixer, a multilevel attention mixer network that achieves multilevel reasoning for item transitions without relying on GNNs. Wang et al. [32] proposed the GCE-GNN model, which significantly enhances recommendation performance by constructing a global graph to learn item-transition information from other sessions.\nDespite their successes, these approaches often overlook the modeling of users' dynamic interests and lacks the modeling of long-term interests across sessions. In contrast, our approach integrates both long-term and short-term user interests to achieve a dynamic representation of users' interests."}, {"title": "2.1. Session-based Recommendation", "content": ""}, {"title": "2.2. Social Recommendation", "content": "User-based collaborative filtering (CF) models [9, 15, 20] are the classical algorithm to recommend items to the target user based on user similarity by finding other users with similar preferences or purchase history to the target user and recommending items to the target user that have been liked or purchased by these similar users. They operate under the assumption that users who have interacted with similar items in the past tend to share similar preferences, making information about such users valuable for recommendation. However, they often encounter problems of data sparsity and limited model expressiveness [24, 31]."}, {"title": "2.3. Session-based Social Recommendation", "content": "Several existing methods tried to address friend data sparsity problem by employing repeated friend selection [16, 25]. For instance, Song et al. [25] proposed the DGRec model, which utilized RNN-based models to capture the dynamic interests of users and their friends, while employing graph attention networks to model the influence of friends. Building upon DGRec, Liu et al. [16] introduced GNNRec, an enhanced model that constructed session graphs and leveraged Gated Graph Neural Networks (GGNN) to model complex item transitions, further improving recommendation performance. Additionally, Chen et al. [2] proposed SERec, a generalized social recommendation framework that utilized heterogeneous graph neural networks to learn user-item representations, effectively capturing item transitions across sessions. Feng et al. [6] proposed a Hierarchical Social Similarity-guided Model with Dual-mode Attention (HMDA) for Session-based Recommendation where the social influence exerted by friends are aggregated.\nAlthough these methods perform well, they are less effective when the user has fewer friends. To overcome this limitation, we introduce the concept of \u201cLike-minded Peers\u201d (LMP), which can enable the identification of users who genuinely share similar preferences with the target users."}, {"title": "3. Problem Definition", "content": "Before delving into the problem definition, we first give the basic definitions of concepts used throughout this paper and then the definitions relevant to users: Like-minded Peers (LMP), social friends, and neighbours.\nWe denote $G = (U, E)$ as the social network where $U$ and $E$ are the users and the social relations among users, respectively. $e(\\bar{u},u) = 1$ represents the existence of a social edge between the target user $\\bar{u}$ and user $u$, otherwise $e(\\bar{u},u) = 0$. $Q$ represents the set of all sessions of users, and $Q_u = \\{S_u^1, S_u^2, ..., S_u^T\\}$ denotes the set of sessions for a user $u \\in U$, where $S_u^t$ is the t-th session of user u. We denoted $S_u^t = \\{i_{u,1}^t, i_{u,2}^t,..., i_{u, n}^t\\}$ as the set of n items that the user u preferred at the t-th session. T is the number of historical sessions of user, and n is the number of items in the session. Each user's sessions are chronologically arranged.\nDefinition 1. Like-Minded Peers. Given a target user $\\bar{u}$ in the $T + 1$-th session, the Like-Minded Peers ($X_{\\bar{u}}^{T+1}$) of $\\bar{u}$ are the set of users ranging from the historical session interval [1,T] who have item intersections with the user $\\bar{u}$ in session $T + 1$. It is formally defined below:\n$X_{\\bar{u}}^{T+1} = \\{\\forall u, u \\in U \\}$\ns.t. $u \\neq \\bar{u}$\\\n$\\bigcup_{t=1}^T S_u^t \\bigcap S_{\\bar{u}}^{T+1} \\bigcup S_u^t \\neq \\emptyset,$\nDefinition 2. Social Friends. Given a target user $\\bar{u}$ in the $T + 1$-th session, social friends ($Z_{\\bar{u}}^{T+1}$) are the set of users ranging from the historical session interval [1,T] who have connection edge in the social network G with the target user $\\bar{u}$ and have interaction behaviors in the historical session. It is formally defined below:\n$Z_{\\bar{u}}^{T+1} = \\{\\forall u, u \\in U \\}$\ns.t. $u \\neq \\bar{u}$\\\n$e(\\bar{u},u) = 1, \\bigcup_{t=1}^T S_u^t \\bigcup S_u^t \\neq \\emptyset,$\nDefinition 3. Neighbours of Users. Traditionally, neighbours are the users who directly/indirectly connect the target user in the social network. In this paper, we give a broad definition of neighbour which is the union set of LMP users $X_{\\bar{u}}^{T+1}$ and social friends $Z_{\\bar{u}}^{T+1}$ for the target user $\\bar{u}$, denoted as:\n$N_{\\bar{u}}^{T+1} = X_{\\bar{u}}^{T+1} \\bigcup Z_{\\bar{u}}^{T+1}$\nDefinition 4. Session-based Social Recommendation (SSR). Given a new session $S_{\\bar{u}}^{T+1} \\bigcup S_u$ for target user $\\bar{u}$ and her/his neighbours $N_{\\bar{u}}^{T+1}$, the goal of SSR is to recommend a set of items from I that $\\bar{u}$ is likely to be interested in during next time step T + 1. Here, we employ the session information coming from the dynamic interests including the historical sessions $\\bigcup_{t=1}^T S_{\\bar{u}}^t$ and the current session $S_{\\bar{u}} \\bigcup S_{\\bar{u}}^{T+1}$ as well as the neighbour information including LMP users, i.e., $X_{\\bar{u}}^{T+1}$ and social friends i.e., $Z_{\\bar{u}}^{T+1}$."}, {"title": "4. Proposed Method", "content": "In this section, we present the detail of our proposed TEGAARec model. Fig. 2 illustrates the overall framework, which first generates neighbours by randomly sampling LMP users and social friends from the historical sessions, then encodes and merges the long- and short-term interests of the target user and neighbours with our proposed TEGAA module. Next, a GAT-based social aggregation layer is designed to weightedly aggregate the target users' dynamic interests and the social influence. Finally, a prediction layer is used to calculate the similarities between the item embeddings and target user embedding for selecting the top-k items as the recommendation."}, {"title": "4.1. Historical Session-based Neighbour Sampling", "content": "In this section, we introduce how to get the neighbours $N_{\\bar{u}}^{T+1}$, i.e., LMP users and social friends, of the target user $\\bar{u}$ given the historical sessions $\\bigcup_{t=1}^T S_{\\bar{u}}^t$. Particularly, for the LMP users acquisition, we first collect the items of $\\bar{u}$ at the current session $S_{\\bar{u}}^{T+1}$ and then check the users from the historical sessions ranging from the historical session interval [1, T] where users sharing the same items with $\\bar{u}$ are selected as the candidates. Then, we randomly choose users from candidates as the LMP users according to a predefined value called $L_1$. Note that if the size of LMP users is less than $L_1$, we randomly select duplicate LMP from the existing LMP set. Finally, the LMP users, i.e., $X_{\\bar{u}}^{T+1}$ are captured from the historical sessions. Note that our sampling strategy can be extended to the more complex one by replacing the random selection with the volume-based selection of shared items. Similarly, for social friends, we first extract users from the social networks in the historical sessions. The users directly connected to $\\bar{u}$ and having interaction behaviors in the historical session are treated as the candidates. Next, the random sampling strategy is employed to select users based on the predefined value $L_s$ to get $Z_{\\bar{u}}^{T+1}$ as the social users. We union the LMP users and social friends to acquire neighbours of the target user $\\bar{u}$."}, {"title": "4.2. Autoregressive Mask Sequence Construction", "content": "To augment the number of training examples [25, 33], we try to split the current session sequence $S_{\\bar{u}}^{T+1} \\bigcup S_u$ into several sub-sessions sequence $Masked Input_k$ and the corresponding labels $Target_k$. Specifically, for the sequence of interaction item sequence $S_{\\bar{u}}^{T+1} = \\{i_{\\bar{u},1}^{T+1}, i_{\\bar{u},2}^{T+1},..., i_{\\bar{u}, n}^{T+1}\\}$, we utilize an Autoregressive Mask Sequence Construction module to split it as follows:\n$Masked Input_k = \\{i_{\\bar{u},1}^{T+1}, ..., i_{\\bar{u},k}^{T+1}, 0, ..., 0\\},$\n(1)\n$Target_k = \\{i_{\\bar{u},k+1}^{T+1}\\}$\nwhere $Masked Input_k$ represents the k-th constructed masked input sequence and $Target_k$ represents the k-th item to be predicted, $k \\in [1, n \u2212 1]$. Different from existing methods which augment data in the data preprocessing step [32, 33], we implement it during the model's training phase. Our strategy can avoid the repeated sampling of neighbours in the process of historical session-based neighbour sampling."}, {"title": "4.3. Long- and Short-Term Interests Encoding and Fusing with TEGAA Module", "content": "Generally, interests of users may vary over time, therefore, it is essential to encode the long- and short-term interests, which can systemically learn the preference of target users. Given any user $u_m$ either from the neighbours $N_{\\bar{u}}^{T+1}$ or from the target user $\\bar{u}$, and her/his interacted item sequence $S_{u_m}^t = \\{i_{t,1}^{u_m}, i_{t,2}^{u_m} ... , i_{t,n}^{u_m}\\}$, we aim to learn the fused representations of long-term and short-term interests of the user $u_m$."}, {"title": "4.3.1. TEGAA Module", "content": "To capture the long-term and short-term interests and merge these interests of an user $u_m$, we propose a novel TEGAA module which consists of a Transformer Encoder, a user-embedding look-up table and a GAT-based Dynamic Interest Aggregator.\nLong- and Short-Term Interests Encoding. Given the item interaction sequence $S_{u_m}^t = \\{i_{t,1}^{u_m}, ... , i_{t, n}^{u_m}\\}$, we first use the Transformer Encoder to get the embeddings of $S_{u_m}^t$ as the short-term interests below:\n$h_{t}^{u_m} = Transformer\\_Encoder(S_{u_m}^t)$\n(2)\nwhere $Transformer\\_Encoder()$ is the function to get the encoded information of the interaction sequence of the session $S_{u_m}^t$. We employ a single vector from the user-embedded lookup table to learn the representation of the long-term interests of the user $u_m$ below:\n$emb_{u_m}= Embedding_{user}(u_m)$\n(3)\nwhere $Embedding_{user} \\in R^{|U| \\times d}$ denotes the user embedding lookup table, |U| is the number of all users, and d is the size of the embedding dimension.\nLong- and Short-Term Interests Fusing. Given the encoded information of items and users, we first modify the Multi-head Attention Mechanism to obtain the Multi-head Graph Attention Mechanism (MHGAT) by adjusting the query vectors to the center node, i.e., the target user embedding, and the key vectors to the neighbour embedding. MHGAT is utilzied to capture the short-term interest of the user $u_m$. Particularly, we treat the embeddings of target user as the center node and the embeddings of neighbours as the neighbouring nodes of multi-head attention. They are calculated as follows:\n$Q = emb_{u_m} W^Q, K = h_{t}^{u_m} W^K, V = h_{t}^{u_m} W^V,$\n(4)\n$head_i = Attention(Q, K, V) = SoftMax(\\frac{QK^T}{\\sqrt{d_k}})V$\n(5)\n$h_t = MHGAT(Q, K, V)$\n= Concat(head_1; ... ; head_z)W^o + b^o\n(6)\nwhere $head_1,...,head_z$ denotes the z heads of the multi-head attention mechanism, Concat denotes the vector splicing operation, $W^o \\in [R^{z d_k \\times d}, W^o \\in R^{d \\times d_k}, W^K \\in [R^{d \\times d_k}$ are the mapping parameter matrices, $b^o \\in R^d$ is the bias term, $\u2020$ denotes the transpose operation, and $d_k = d/h$.\nThen, we fuse the short-term interest result $h_t$, obtained from the Dynamic Interest Aggregator with the long-term interest representation $emb_{u_m}$ to obtain the final representation $h_{u_m}^t$ below:\n$h_{u_m}^t = TensorFusion(h_t, emb_{u_m})$\n= ReLU(Concat[h_t; emb_{u_m}]W^F + b^F)\n(7)\nwhere $ReLU(x) = max(0, x)$ is the nonlinear activation function, $W^F \\in R^{2d \\times d}$ is the mapping parameter matrix, $b^F \\in R^d$ is the bias term, and d is the size of the embedding dimension."}, {"title": "4.3.2. Neighbour and Target User Interests Encoding", "content": "Given the designed TEGAA module, we apply it for the neighbour interests encoding and the target user interesting encoding to get the fused long-term and short-term interests. Particularly, for the neighbour interest encoding, we use the TEGAA module to get the encoded results for all $|N_{\\bar{u}}^{T+1}|$ of the neighbours in the T-th session $H_{nm} = \\{h_{nm,1}, h_{nm,2},..., h_{nm|N_{\\bar{u}}^{T+1}|}\\}\nh_{nm,k}=TEGAA\\_Module(S_{nm,t})$\n(8)\nwhere $S_{nm,t}$ represents the sequence of interaction items for the k-th neighbour's T-th session, $h_{nm,k}$ represents the coded representation of neighbour k in the neighbour set through the TEGAA module, and $k \\in [1,|N_{\\bar{u}}^{T+1}|]$.\nSimilarly, for the target user interests encoding, we feed the constructed masked input sequences into the TEGAA module, and it outputs the encoded information corresponding to each masked sequence of the target user $\\bar{u}$. The encoded embeddings of n - 1 sub-sessions of the target user $\\bar{u}$ is denoted as $H_{\\bar{u}} = \\{h_{\\bar{u},1}, h_{\\bar{u},2},\u2026\u2026\u2026, h_{\\bar{u},n\u22121} \\}$. Each element $h_{\\bar{u}k} \\in H_{\\bar{u}}$ is calculated below:\n$h_{\\bar{u}k} = TEGAA\\_Module(Masked Input_k)$\n(9)\nwhere k is the length of the session."}, {"title": "4.4. GAT-based Social Aggregation Layer", "content": "Due to the difference of social influence for neighbours for target user, it is necessary to discern their contributions. In this section, we consider to use the MHGAT module (as mentioned in section 4.3) to weightedly aggregate the target users' dynamic interests and the social influence. On one hand, the neighbours of the target user $\\bar{u}$ in are encoded in session T with $H_{nm} = \\{h_{nm,1}, h_{nm,2},..., h_{nm|N_{\\bar{u}}^{T+1}|}\\}$ as neighbour nodes. On the other hand, the encoding result $H_{\\bar{u}} = \\{h_{\\bar{u},1}, h_{\\bar{u},2},..., h_{\\bar{u}n\u22121}\\}$ of each mask sequence in Session T + 1 for the target user $\\bar{u}$ is used as the target aggregation node. Node types of the target aggregation node and the neighbour nodes are both users, i.e., homogeneous nodes, so we add the self-attention mechanism to the aggregation. The final aggregated result is obtained as $H' = \\{h_{\\bar{u},1}', h_{\\bar{u},2}', ..., h_{\\bar{u},n\u22121}'\\}$, whose element is calculated below:\n$h_{\\bar{u},k}' = MHGAT(h_{\\bar{u},k}, w_k^Q W^K, h_{nm})$\n(10)\nwhere $h_{nm} = [h_{nm,1},..., h_{nm|N_{\\bar{u}}^{T+1}|}], h_{\\bar{u},k}$, and $h_{\\bar{u},k}$ represents the encoding result of the target user $\\bar{u}$ for the k-th mask sequence in session T + 1, $k \\in [1, n \u2212 1]$."}, {"title": "4.5. Prediction Layer", "content": "In this section, we introduce how to select the top relevant items that the target user preferred. We first encode all the items using an item-embedded lookup table. Given any item i with the id $i_v$, we get its embedding below:\n$h_i' = Embedding_{item}(i_v)$\n(11)\nThen, we use the SoftMax function to predict the probability of the next item as follows:\np(Target_k |Masked Input_k; \\{S_{nm,t}, nm \\in N_{\\bar{u}}^{T+1}\\})\n= \\frac{exp((h_{\\bar{u}k}')^T h_{Target}')}{\\sum_{j=1}^{|I|} exp((h_{\\bar{u}k}')^T h_j')}\n(12)\nwhere $S_{nm,t}$ represents the t-th session data of neighbour $nm,t \\in [1,T]$, and $N_{\\bar{u}}^{T+1}$ is the set of neighbours of target user $\\bar{u}$. Here, we denote $h_{\\bar{u}k}'$ as the encoding result of the k-th mask sequence of target user $\\bar{u}$ in Session T+1 into H, T as the transpose operation, and |I| as the total number of items."}, {"title": "4.6. Model Training", "content": "We treat the SSR task as a classification task that allows the model to learn which items the user is interested in. We use the maximum likelihood estimation to train the model and gradient descent to optimize the model:\n$-\\sum_{u\\in U}^{T_n-1} \\sum_{t=2} \\sum_{k=1}^{T_n-1} log(p(Target_k|Masked Input_k; \\{S_{nm,t}, nm \\in N_{\\bar{u}}^{T+1}\\})\n(13)"}, {"title": "5. Experiments", "content": "In this section, we describe the experimental setup including the used real-world datasets and the comparative baseline methods as well as the used evaluation metrics. We also introduce the implementation details to ensure the reproduction of our model. Finally, we provide the quantitative analysis of the results, the ablation study and the hyper-parameter analysis."}, {"title": "5.1. Experimental Setup", "content": ""}, {"title": "5.1.1. Datasets", "content": "We conducted experiments on the following four commonly used real-world datasets, for which descriptive statistics is shown in Table 1.\nDouban music and Douban movie\u00b9. Douban is a popular communication platform including the inforamtion of music and movie. Users can rate and communicate about music and movie. The dataset includes user's ratings of music and movie and the corresponding timestamps, as well as social network information.\nExtended epinions\u00b2. This dataset contains information about users' trust and distrust. Users usually rate other users based on the quality of their reviews of the item.\nYelp\u00b3. A very popular online review platform where users can review local businesses (e.g. restaurants and shops), providing content-rich social networking information.\nFollowing [25], we partitioned a dataset by segmenting user behavior into weekly sessions as the train, validation, and test sets. Specifically, sessions within the last s weeks were retained for evaluation. For the Douban music, Douban movie, Extended Epinions, and Yelp datasets, we selected s values of 26, 26, 14, and 24, respectively. Additionally, we filtered out items that did not appear in the training set to ensure consistency across sets. We randomly partitioned the sessions utilized for the testing and evaluation, and evenly distributed them."}, {"title": "5.1.2. Comparative Methods", "content": "We selected a number of representative works for comparison including:"}, {"title": "5.1.3. Evaluation Metrics", "content": "We used two popular ranking-based metrics for our evaluation[25]: R@K(Recall@K) and N@K(NDCG@K). For R@K, we use K={10, 20}, while for N@K, we set K=20 following [25]."}, {"title": "5.1.4. Implementation Details", "content": "We use the Pytorch framework [22] to build our models, and all experiments are run on two Tesla V100 GPUs. For faster model training convergence, we used the warm-up technique [7] to help the model parameters converge faster for learning rate and training epochs. Specifically, we use the grid search method to search for the following hyperparameters: learning rate in {0.01, 0.005, 0.0001, 0.00005}, number of neighbours in {25, 50}, number of LMP in {5, 15, 25}, number of layers of TEGAA modules in {1, 3, 5}, number of warm-up learning steps in {5, 10, 20}, and the number of early stop tolerances in {10, 20}. We used a uniform batch size of 50 and the Adam [10] optimiser. For all baseline model experiments, we used their default hyper-parameters, and we chose the highest value between our experimental results and those reported in the original paper for other baseline models."}, {"title": "5.2. Quantitative Results", "content": "Table 2 presents a performance comparison between our proposed TEGAARec model and selected baseline models. The experimental findings reveal that, apart from the Attn-mixer model, models incorporating social influence such as DGRec and GNNRec outperform those lacking social influence. This underscores the beneficial impact of social influence on recommendation performance, particularly evident in datasets like Yelp with numerous users and dense social networks. Moreover, GCE-GNN model gains a significant advantage by leveraging global graph construction using other session information, further validating the benefits of incorporating such data in recommendation tasks. In contrast, the Attn-mixer model, which enhances inference through multi-level user intent modeling, achieves competitive results, highlighting the advantages of employing attention mechanisms. Our proposed TEGAARec model effectively identifies users with genuinely similar preferences to the target user by leveraging Like-minded Peers (LMP) mining in the recommendation data. Additionally, it accurately models the dynamic interests of users. The results demonstrate a substantial performance improvement compared to other state-of-the-art works."}, {"title": "5.3. Ablation Studies", "content": "To assess the effectiveness of each key component in the TEGAARec model, we conducted ablation experiments from six different perspectives. These experiments were carried out on the Douban music and Yelp datasets, and the results are summarized in Table 4:\n\u2022 TEGAARec w/o LMP: This variant utilizes only the social relationships provided by the dataset and excludes the use of Like-minded Peers (LMP).\n\u2022 TEGAARec w/o GAL: It directly concatenates dynamic interest influences of different neighbours without employing graph attention aggregation.\n\u2022 TEGAARec w/o SF: It excludes the social friends of the target user while keeps the LMP only as the neighbours.\n\u2022 TEGAARec w/ PE: This variant incorporates position encoding into the TEGAA module.\n\u2022 TEGAARec w/o ULI: It excludes the target user's long-term interest embedding from the model.\n\u2022 TEGAARec w/o ALI: This variant removes all user long-term interest embeddings from the model.\nThe experimental results demonstrate that the presence of Like-minded Peers (LMP), the Graph Attention Aggregation Layer, and the inclusion of long-term interests of target users and their neighbours are pivotal factors influencing the model's performance. Removal or replacement of any of these components, as observed in TEGAARec w/o LMP, TEGAARec w/o GAL, and TEGAARec w/o ALI, respectively, leads to a significant degradation in performance. Interestingly, the absence of significant performance improvement in TEGAARec w/ PE suggests that users' short-term interests are more accurately captured through a collection of interaction items over a brief period, without the need for sequence and position information, which may be redundant. Also, the removal of social friends has a small influence on the model's performance. This proves the contribution of our designed LMP to some extent especially in the case of sparse social friends. Additionally, TEGAARec w/o ULI resulted in a slight performance degradation. This observation could be attributed to the fact that the target user's long-term interests are inherently incorporated when they act as a neighbour to other users."}, {"title": "5.4. Hyper-parameter Sensitivity Analysis", "content": "During the model training phase, we employed a grid search technique to optimize the hyperparameters. However, considering the large number of hyperparameters involved, we conducted the search on a reduced subset to maintain training efficiency. To further investigate the influence of hyperparameters on model performance, we selected specific key hyperparameters for sensitivity analysis. Consistent with the ablation experiments, we also performed hyper-parameter sensitivity experiments on the Douban music and Yelp datasets."}, {"title": "5.4.1. Impact of Embedding Dimension", "content": "To assess the effect of different embedding dimensions (utilized for both user and item embeddings) on model performance, we experimented with embedding dimensions ranging from 16 to 512. The results, depicted in Fig. 3, reveal that the model's performance improvement becomes limited when the embedding dimension exceeds 128 on both datasets, with a marginal decreasing effect thereafter."}, {"title": "5.4.2. Impact of Transformer Encoder Layer", "content": "To evaluate the impact of varying Transformer Encoder layers on model performance, we conducted experiments with different layer numbers ranging from 1 to 15. The results, as illustrated in Fig. 4, indicate that employing different Transformer Encoder layers does not substantially influence the model's performance on both datasets. Surprisingly, our findings suggest that a single layer of Transformer Encoder is adequate for achieving effective modeling."}, {"title": "5.4.3. Impact of the number of LMP users", "content": "To evaluate the impact of different LMP users numbers on model performance, we conducted experiments with the numbers ranging from 5 to 55. As depicted in Fig. 5, the results indicate that increasing the number of LMP users enhances the model performance when the number of samples is less than 25 on both datasets. However, when the number of LMP samples exceeds 25, a slight degradation in model performance is observed, possibly attributable to the introduction of noise from an excessive number of LMP users."}, {"title": "5.4.4. Impact of"}]}