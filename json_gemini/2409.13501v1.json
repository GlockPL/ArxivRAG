{"title": "HUT: A More Computation Efficient Fine-Tuning Method With Hadamard Updated Transformation", "authors": ["Geyuan Zhang", "Xiaofei Zhou", "Chuheng Chen"], "abstract": "Fine-tuning pre-trained language models for downstream tasks has achieved impressive results in NLP. However, fine-tuning all parameters becomes impractical due to the rapidly increasing size of model parameters. To address this, Parameter Efficient Fine-Tuning (PEFT) methods update only a subset of parameters. Most PEFT methods, such as LoRA, use incremental updates, which involve adding learned weight matrix increments to the original parameters. Although effective, these methods face limitations in capturing complex parameter dynamics and do not maintain a strong correlation between the original and updated parameters. To overcome these challenges, we propose the direct Updated Transformation (UT) paradigm, which constructs a transformation directly from the original to the updated parameters. This approach ensures that the correlation between the original and updated parameters is preserved, leveraging the semantic features learned during pre-training. Building on this paradigm, we present the Hadamard Updated Transformation (HUT) method. HUT efficiently updates the original weight matrix using the Hadamard transformation with two low-rank matrices, offering a more expressive and flexible update mechanism. This allows HUT to capture richer parameter features through functional transformations, reducing computational complexity while maintaining or improving model quality. Theoretical analysis and extensive experiments on ROBERTa and GPT-2 validate the effectiveness of HUT. Results show that HUT performs on par with or better than other PEFT methods in terms of model quality, while significantly reducing computational complexity.", "sections": [{"title": "1 Introduction", "content": "Pre-trained large language models have achieved great success in various natural language processing tasks. Typically trained on hyperscale cor-"}, {"title": "2 Related Work", "content": "Adapter (Houlsby et al., 2019) approach inserts bottleneck shaped modules (called Adapter) into the Transformer layers. The Adapter layer first uses a down-sampling matrix $W_{down} \\in R^{d \\times r}$ to project the input x from a higher dimension d to a smaller dimension r, followed by a nonlinear function f(.), and then uses an upsampling matrix"}, {"title": "3 Method", "content": "We describe the UT paradigm and the simple design of HUT. The principles outlined here apply to any dense layer in a deep learning model, although we only focused on certain weights in the Transformer language model as incentive use cases in our experiments."}, {"title": "3.1 Direct Updated Transformation (UT) paradigm", "content": "Assuming the new learned parameters are represented by $W_{new}$, and the initial parameters are represented by $W_o$. For other PEFT methods that use incremental updates, such as LoRA (Hu et al., 2022), $W_{new} = W_o + \\Delta W$ where $\\Delta W = sAB$. It can be seen that this $\\Delta W$ has no much correlation with the original parameters $W_o$ and it is not constrained by the value of $W_o$. We believe that this results in the semantic information encoded in $W_o$ not being fully leveraged during the fine-tuning process. To address this limitation, we propose a new paradigm, the direct UT paradigm, which uses a updated transformation U(\u00b7) to directly update $W_o$. The formulation is:\n\n$W_{new} = U(W_o)$ (5)\n\nFor comparison, $W_{new}$ can be further expressed as:\n\n$W_{new} = W_o + U'(W_o)$ (6)\n\nTherefore we can let $\\Delta W = U'(W_o)$. This formulation ensures a strong correlation between $\\Delta W$ and $W_o$, maintaining the semantic features learned during pre-training. We believe that preserving this relevance and constraint is crucial during the fine tuning stage, as it allows the model to better utilize the pre-trained semantic information encoded in $W_o$."}, {"title": "3.2 Hadamard Updated Transformation (HUT)", "content": "Hadamard Product. The most intuitive form of UT is to apply a linear transformation to the weight matrix $W_o \\in R^{d \\times k}$ using a transformation matrix:\nU($W_o$) = T \u00d7 $W_o$, where T \u2208 $R^{d \\times d}$ is a transformation and \u00d7 is matrix multiplication. But as mentioned before, matrix multiplication has high computation complexity, so we can use the Hadamard Product to implement the transformation:\n\n\nA B =\n$\\begin{bmatrix}\na_{11}b_{11}& a_{12}b_{12}&  ... & a_{1k}b_{1k} \\\\\na_{21}b_{21}& a_{22}b_{22}&  ... & a_{2k}b_{2k} \\\\\n:& :&   & : \\\\\na_{d1}b_{d1}& a_{d2}b_{d2}& ... & a_{dk}b_{dk}\n\\end{bmatrix}$ (7)\n\nwhere A \u2208 $R^{d \\times k}$, B \u2208 $R^{d \\times k}$ and indicates Hadamard product. From Eq.(7), we can find that the Hadamard product operation requires that both matrices have the same shape. Contrast with matrix multiplication, Hadamard product has lower"}, {"title": "Design of HUT.", "content": "Intrinsic SAID (Aghajanyan et al., 2021) finds that the pre-trained language models have a low \"instrisic dimension\" and can still learn efficiently despite a random projection to a smaller subspace. So we hypothesize that the linear tranformation of the weight matrices through Hadamard product have a low \"instrisic dimension\" during updating procedure in Eq.(5) and the transformation matrix T is also have a low \"intrisic rank\". Further, to improve the representation ability of the tranformation, we use two low-rank transformation matrices $M_A \\in R^{d \\times r}$ and $M_B \\in R^{r \\times k}$, where the rank r < min(d, k). The new updated transformation can be formulated by:\n\n$W_{new} = (M_A \\times 1_A) \\odot W_o  \\odot (1_B \\times M_B)$ (8)\n\nWhere $1_A \\in R^{r \\times k}, 1_B \\in R^{d \\times r}$, and they are used to map the shape of $M_A$ and $M_B$ to be the same with $W_o$. We call the parameter update form shown in Eq.(8) as the Hadamard Transformation Updated transformation(HUT). While in code implementation, the $\\odot$ operation can be replaced, and we will discuss it later.\nIn addition, according to (Lian et al., 2022), scaling and shifting the deep features can improve the performance of fine-tuning. Therefore, for the forward process h = $W_ox$, we add scaling and shifting to the input features and update the parameter with meta weights, our modified forward pass yields:\n\n$h = \\gamma(x \\odot W_{new}) + \\beta$ (9)\n\nWhere $\\gamma \\in R^{1 \\times k}$ and $\\beta \\in R^{1 \\times k}$ are the scale and shift factors. Though the shape of $\\beta$ and $\\gamma$ are not same with x, we can use broadcasting notation(van der Walt et al., 2011) to automatic expand their dimension during calculating."}, {"title": "Apply HUT.", "content": "There are many weight matrices in the Transformer architecture, including $W_q$, $W_k$, $W_u$, $W_o$ in self-attention module and $W_d$, $W_u$ in FFN module. In principle, we can apply HUT to any subset of weight matrices mentioned above."}, {"title": "3.3 Computation Complexity Analysis", "content": "We compare the most widely used PEFT method LORA with HUT in Floating Points Operations(FLOPs). Suppose that the input x \u2208 $R^{N \\times d}$"}, {"title": "Experiments", "content": "We evaluate the downstream task performance of HUT on ROBERTa-large (Liu et al., 2019) and GPT-2 (Radford et al., 2019). Our experiments cover a wide range of tasks, from natural language understanding (NLU) to generation (NLG). Specifically, we evaluate on the GLUE (Wang et al., 2019) benchmark for RoBERTa. We follow the setup of (Li and Liang, 2021) on GPT-2 for a direct comparison. We use NVIDIA RTX3090 for all experiments.\nWe compare our methods with these types of approaches as follows: full fine-tuning (FT), Bit-Fit (Zaken et al., 2022), HAdapter (Houlsby et al., 2019), LAdapter (Lin et al., 2020), PAdapter (Pfeiffer et al., 2021), LoRA (Hu et al., 2022), VeRA (Kopiczko et al., 2023) and FourierFT (Gao et al., 2024). See more details in Appendix A."}, {"title": "4.2 Natural Language Understanding", "content": ""}, {"title": "4.2.1 Models and Datasets.", "content": "We use GLUE (Wang et al., 2019) benchmark to evaluate the performence of our methds based on ROBERTa-large (Liu et al., 2019) model in natural language understanding tasks. GLUE benchmark is a wide-ranging collection of natural language understanding tasks. Dataset details are summarized in Appendix B. RoBERTa-large (Liu et al., 2019) consists of 357 millions parameters, and we take the pre-trained RoBERTa-large from HuggingFace Transformers library (Wolf et al., 2020)."}, {"title": "4.2.2 Implementation Details.", "content": "We apply HUT to query and value matrices {Wq, Wv} in self-attention module and set r = 8, using AdamW (Loshchilov and Hutter, 2019) optimizer to train it for all sub-tasks. For HAdapter (Houlsby et al., 2019), PAdapter (Pfeiffer et al., 2021) and LORA (Hu et al., 2022), we follow the original setup introduced in (Hu et al., 2022). While the other PEFT methods use a pre-trained model which is already adapted to MNLI to initialize the model for MRPC, RTE, and STS-B, we start with the original pre-trained RoBERTa large model. See Appendix for details on the hyperparameters used. We report the Matthew's correlation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. More details please refer to Appendix C."}, {"title": "4.2.3 Main Results.", "content": "Table 1 shows the experimental results on the GLUE validation dataset. We report mean of 5 runs using different random seeds. We can see that on four of the six datasets of the GLUE benchmark, we achieve"}, {"title": "4.3 Natural Language Generation", "content": ""}, {"title": "4.3.1 Models and Datasets.", "content": "HUT has been shown that it can get competitive results compared with other PEFT methods and full fine-tuning on NLU tasks, and we hope to answer if HUT still prevails on NLG tasks. So we use E2E NLG Challenge to evaluate the performence of our methods. It is first introduced in (Novikova et al., 2017) as a dataset for training end-to-end, data-driven natural language generation systems and is commonly used for data-to-text evaluation. And we use GPT-2 media as our base model which consists of over 354 millions parameters. We use the official evaluation script, which reports BLEU(Papineni et al., 2002), NIST(Belz and Reiter, 2006), \u039c\u0395\u03a4\u0395OR(Lavie and Agarwal, 2007), ROUGE-L(Lin, 2004), and CIDEr(Vedantam et al., 2015)."}, {"title": "4.3.2 Implementation Details.", "content": "We apply HUT to query and value matrices {Wq, Wv} in self-attention module and set r = 4, using AdamW(Loshchilov and Hutter, 2019) optimizer with a linear learning rate schedule for 5 epochs. We keep our setup as close as possible to (Li and Liang, 2021) for a direct comparison. The batch size of our methods is set to 4, learning rate is set to 0.002, and beam search beam size is set to 10. Accordingly, we also tune the above hyperparameters for HUT. We report the mean over 3 random seeds; the result for each run is taken from the best epoch. More details please refer to Appendix C."}, {"title": "4.3.3 Main Results.", "content": "We show in Table 3 the experimental results on the E2E NLG challenge after fine-tuning the base model GPT-2 Medium using HUT. As we can see"}, {"title": "4.4 Ablation Studies", "content": ""}, {"title": "4.4.1 Where to apply HUT.", "content": "We explored the effect of applying HUT to different attention weight matrices, and the experimental results are shown in Table 4. We conducted experiments on MRPC and CoLA datasets on the NLU task, and set different r for different subsets of weight matrix in Transformer attention module"}, {"title": "4.4.2 How to choose r.", "content": "Another important question is how to choose r to get better performance. We adapt {Wq, W\u03c5}, {Wq, Wk, Wu, Wo}, and just Wq for a comparison. We evaluate the effect of r on MRPC and COLA, and results are shown in Table 5. Surprisingly, we can find that HUT already performs competitively with a very small r(e.g., r=1), and this is ture for both just Wq and {Wq, Wv}. This suggests that the Hadamard updated transformation has a very small \"instrisic dimension\". As the dimension r goes up,"}, {"title": "4.4.3 Visualization.", "content": "To demonstrate the effectiveness of the proposed HUT, we conducted experiments on the SQUADv1.1 (Rajpurkar et al., 2016) dataset. We visualized the relationship between the output states of the final layer of the fine-tuned model and the inputs in Figure 4, where the varying shades of red indicate the model's attention to that word. From the figure, we can see that the HUT fine-tuned model accurately captures words related to the correct answers and provides the right responses. On the other hand, the LoRA fine-tuned model captures incorrect keywords, leading to wrong answers. Therefore, based on the comparison results, we can infer that the ability of our proposed HUT to capture key features is stronger. We believe this is due to the fact that our proposed UT paradigm can maintain a strong correlation between the learned AW and the original weight Wo, and fully leverage"}, {"title": "5 Conclusion", "content": "In this paper, we propose UT paradigm, which build a direct tranformation to map the original weights to the updated weights. UT paradigm maintain a strong correlation between the pre-trained weight Wo and the updated AW. Under this paradigm, we present an approach called HUT. HUT uses Hadamard Transformation which is a powerful feature transformation with only two low-rank matrices to update the original weight matrices. We conduct extensive experiments on NLU and NLG tasks. Results shows that, by using Hadamard transformation, our methods not only achieve on-par or SOTA performance on NLU and NLG tasks, but also reduce the computation complexity during training and inference procedure without introducing any inference latency. Our work demonstrates that the direct updated transformation paradigm of PEFT is feasible."}, {"title": "6 Limitations", "content": "Despite we proposed a new paradigm for efficiently updating parameters, but we only propose one concrete realization method, called HUT, to verify the effectiveness of this paradigm. Besides, as shown in this paper, our proposed approach has not achieve SOTA on certain datasets, and the exact reason is unknown. So further investigation is necessary to explore the underlying principles of HUT."}, {"title": "A Baselines", "content": "The details of baseline models are as follows:\n\n\u2022 Full fine-tuning is the most common approach for adaptation. During fine-tuning, the model is initialized with pre-trained weights and biases, and all model parameters undergo gradient updates.\n\n\u2022 BitFit (Zaken et al., 2022) is an effective parameter-efficient fine-tuning method. The method only fine-tunes bias vectors in the pretrained model.\n\n\u2022 HAdapter (Houlsby et al., 2019) as proposed to inserts adapter layers between the selfattention module and the MLP module with subsequent residual connection. There are two fully connected layers with biases in an adapter layer with a nonlinearity in between.\n\n\u2022 LAdapter (Lin et al., 2020) is a more efficient design with the adapter layer applied only after the FFN module and after a LayerNorm (Ba et al., 2016).\n\n\u2022 PAdapter (Pfeiffer et al., 2021) is similiar with LAdapter, adapters only applied after FFN modules and LayerNorm modules (Ba et al., 2016).\n\n\u2022 LoRA (Hu et al., 2022) is most widely used method for PEFT. The number of trainable parameter is controlled by the rank r and the number of adapted weight matrices n. We follow the original paper to apply LoRA to query and value projections only."}, {"title": "B Datasets Details", "content": ""}, {"title": "B.1 GLUE benchmark", "content": "is a wide-ranging collection of natural language understanding tasks. Dataset details are summarized in Appendix It includes MNLI (inference, (Williams et al., 2018)), SST-2 (sentiment analysis, (Socher et al., 2013)), MRPC (paraphrase detection, (Dolan and Brockett, 2005)), CoLA (linguistic acceptability, (Warstadt et al., 2019)), QNLI (inference, (Rajpurkar et al., 2018)), QQP(questionanswering), RTE (inference), and STS-B (textual similarity, (Cer et al., 2017))."}, {"title": "B.2 E2E NLG Challenge", "content": "dateset consists of roughly 42,000 training, 4,600 validation, and 4,600 test examples from the restaurant domain. Each source table used as input can have multiple references. Each sample input (x, y) consists of a sequence of slot-value pairs, along with a corresponding natural language reference text. The dataset is released under Creative Commons BY-NC-SA 4.0."}, {"title": "C Experiments Details", "content": ""}, {"title": "C.1 Code Implementation", "content": "We use PyTorch\u00b9 and peft\u00b2 to implement all experiments on NVIDIA RTX 3090 GPUs."}, {"title": "C.2 Hyperparameters", "content": "For NLU tasks, we train using AdamW with a linear learning rate decay schedule. We sweep learning rate, number of training epochs, and batch size for HUT. we use the hyperparameters presented in Table 6.\nAnd for NLG tasks, we train all of our GPT-2 models using AdamW(Loshchilov and Hutter, 2019) with a linear learning rate schedule for 5 epochs. We use the batch size, learning rate, and beam search beam size described in(Li and Liang, 2021). Accordingly, we also tune the above hyperparameters for HUT. We report the mean over 3 random seeds. The hyperparameters used for HUT in GPT-2 are listed in Table 7."}]}