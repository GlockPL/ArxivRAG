{"title": "Efficient Localized Adaptation of Neural Weather Forecasting: A Case Study in the MENA Region", "authors": ["Muhammad Akhtar Munir", "Fahad Shahbaz Khan", "Salman Khan"], "abstract": "Accurate weather and climate modeling is critical for both scientific advancement and safeguarding communities against environmental risks. Traditional approaches rely heavily on Numerical Weather Prediction (NWP) models, which simulate energy and matter flow across Earth's systems. However, heavy computational requirements and low efficiency restrict the suitability of NWP, leading to a pressing need for enhanced modeling techniques. Neural network-based models have emerged as promising alternatives, leveraging data-driven approaches to forecast atmospheric variables. In this work, we focus on limited-area modeling and train our model specifically for localized region-level downstream tasks. As a case study, we consider the MENA region due to its unique climatic challenges, where accurate localized weather forecasting is crucial for managing water resources, agriculture and mitigating the impacts of extreme weather events. This targeted approach allows us to tailor the model's capabilities to the unique conditions of the region of interest. Our study aims to validate the effectiveness of integrating parameter-efficient fine-tuning (PEFT) methodologies, specifically Low-Rank Adaptation (LORA) and its variants, to enhance forecast accuracy, as well as training speed, computational resource utilization, and memory efficiency in weather and climate modeling for specific regions. Our codebase and pre-trained models can be accessed at https://github.com/akhtarvision/weather-regional.", "sections": [{"title": "Introduction", "content": "The accurate modeling and prediction of weather and climate patterns hold prominent significance for both scientific research and societal well-being. Primarily, weather and climate modeling can be categorized into two types: numerical (NWP) methods and neural network-based models. The former, usually related to General Circulation Models (GCMs) [1,2], are designed to simulate the flow of energy and matter across the land, atmosphere, and ocean. However, the ability to perform detailed simulations to forecast weather and atmospheric variables in NWP models is restricted by computational resources and the time required for execution. As a consequence, there is a pressing need to address computational challenges and improve the precision of weather and climate modeling.\nNeural network-based models [3,4,5,6,7,8,9,10,11,12,13] are data-centric and can absorb large-scale data to enhance performance. These models undergo comprehensive training on globally accessible datasets, thereby providing a viable alternative for predicting weather and climate patterns. Despite the absence of explicit physics assumptions in these models, the datasets employed for predictive task training inherently incorporate implicit physics assumptions. Within data-driven approaches, transformer-based [3,7] and graph-based [6] models have been explored. One notable example of a transformer-based model is ClimaX [7], which focuses on developing a foundational model for climate and weather applications. ClimaX introduces a pre-training and fine-tuning paradigm, where during pre-training, it trains on data curated from physics-based models, enhancing its predictive"}, {"title": "Method", "content": "2.1 Preliminaries\nNotations: To input the neural network model, it takes the input I of shape D \u00d7 H \u00d7 W, where\nD is the number of variables spanning the atmospheric or climate ones. Let F be a neural network\noperator for gridded prediction tasks which takes input, F(I) and output O of shape D \u00d7 \u0124 \u00d7 \u0174.\nSpatial resolution H \u00d7 W determines the density of the grid, and here we operate with two levels of\nresolutions 5.625\u00b0 (32 \u00d7 64 grid points) and 1.40625\u00b0 (128 \u00d7 256 grid points).\nArchitecture: The Vision Transformer (ViT) architecture involves the partitioning of input data into\nfixed patches, followed by a linear transformation to generate patch embeddings, commonly referred\nto as tokens [19]. The ClimaX [7] model we use consists of two main features, which will be briefly\ndescribed in this paragraph. (i) Variable Tokenization: The proposed framework aims to mitigate a\nlimitation observed in ViT architecture, which inherently processes a specified number of channels in\nthe input. This approach independently addresses each input atmospheric and climate variable. (ii)\nVariable Aggregation: The variable tokenization approach presents challenges, notably an increase"}, {"title": "Fine tuning using PEFT", "content": "Several methods are considered under the PEFT paradigm. We extensively study LoRA and its\nvariants along with GLORA, to cater to several PEFT mechanisms in one place.\nLORA: It presents several advantages, including the capability to integrate multiple LoRA modules\nfor specific tasks without modifying the underlying base foundation model. By freezing the weights\nof the base model and optimizing low-rank learnable matrices, significant reductions in storage\nrequirements can be accomplished. Additionally, there is no added inference latency compared to\nusing a fully fine-tuned model. Because of these advantages, we also explore a variant of LoRA that\nincorporates residuals to retain information from previous blocks. However, empirical observations\nindicate that the simple LoRA method continues to outperform these variants. Neural network layers\ntypically possess full rank, while LoRA attempts to project weights into a smaller subspace. Let Wp\nrepresent the pretrained weight matrix, and its modification AW is replaced by low-rank decomposed\nmatrices, denoted as BA, resulting in the equation:\nWp + AW = Wp + BA\nHere, B \u2208 Rd\u00d7r and A \u2208 Rr\u00d7k decompose the matrix Wp \u2208 Rd\u00d7k, where r is the rank determined\nas r \u2264 min(d, k). To explain further, it is notable that gradients are not updated for Wp. For more\ndetails, interested readers are encouraged to consult [14].\nGLORA: This methodology presents a unified framework that integrates fine-tuning approaches\nwithin a singular formulation. The architecture features a supernet, which is efficiently optimized\nthrough evolutionary search techniques. These conventional methods often rely on resource-intensive\nhyperparameter searches, dependent upon data availability. Employing an implicit search mechanism\nprevents the requirement for manual hyperparameter tuning, relaying the simultaneous increase in\ntraining time. We refer to App. A for more details on GLORA.\nFlash-attention: In advancing large models like language models, the flash attention algorithm,\nintroduced by [20], optimizes attention computation by reducing memory usage. Its successor, flash\nattention-2 [21], further improves efficiency by minimizing non-matrix multiplication operations and\nparallelizing over sequence length."}, {"title": "Experiments and Results", "content": "The ERA5 reanalysis, developed by the European Center for Medium-Range Weather Forecasting\n(ECMWF), is utilized as a fundamental data source for training and evaluating weather forecasting\nsystems. ERA5 integrates state-of-the-art Integrated Forecasting System [22] model outputs with\nobservational data to generate comprehensive records of atmospheric, and land surface conditions.\nMore details regarding data and implementation can be found in the App. B and App. C respectively."}, {"title": "Experiments with Global and Regional Modeling", "content": "To evaluate our forecasting models, we employ the ERA5 dataset, which provides global atmospheric\nreanalysis data at different resolutions. We compare ClimaX with the settings of global forecasting as\nwell as in regional forecasting. We assess performance at both 5.625\u00b0 and 1.40625\u00b0 resolutions to\nunderstand the impact of spatial granularity on forecasting accuracy. The performance of each model\nis evaluated using latitude-weighted root mean squared error (RMSE) and latitude-weighted anomaly\ncorrelation coefficient (ACC), standard metrics in weather prediction literature, reflecting forecast\naccuracy and consistency with observed data. In addition to global forecasting, we extensively extend\nour analysis to regional forecasting focusing on Middle East and North Africa (MENA). By selecting\na regional dataset (ERA5-MENA) with the same set of variables, we assess ClimaX's ability to\nforecast weather conditions specifically within this region. We compare regional ClimaX with PEFT\nparadigms and a global version of ClimaX. Furthermore, we explore the impact of training ClimaX\non data at different resolutions and evaluate its performance in regional forecasting tasks.\nRegional vs Global: A comparative analysis between global and regional models unveils their\nrespective behaviors and notably, the regional model consistently demonstrates superior performance\nwhen tasked with regional prediction objectives (Table 1). The regional model is specialized to\npredict more accurate forecasts due to the localized features learned during training. Comparison\nwith regional model variants: In Table 2, our findings demonstrate that ClimaX with LoRA shows\nsuperior performance as compared to the global model and competitive performance compared to\nthe full fine-tune method in predicting key atmospheric variables indicating its efficacy in weather\nforecasting applications. The number of trainable parameters is reduced from 108M (fft) to 16.2M\n(LORA). Our study also presents findings obtained using GLora and a proposed modified version of\nLora, which integrates residuals from similar blocks while aggregating information into subsequent\ntransformer blocks. However, our observations indicate that while it does not exhibit superior\nperformance, it does deliver reasonable results. We show qualitative results in Fig. 2 for one of the\natmospheric variables. Bias is referred to as the difference between prediction and ground truth. We\nshow further experiments in App.D, which include predictions over ranges and ablations, primarily\nfocusing on the impact of rank r, LoRA's significance on attention and FC layers, memory and time\ncomparison, and more qualitative results."}, {"title": "Conclusion", "content": "Neural network based models offer a promising alternative as these data-driven approaches, such\nas transformer-based models, leverage comprehensive training on large-scale datasets to forecast\natmospheric variables with reasonable accuracy. Our study focuses on enhancing the transformer-\nbased forecasting model, particularly in the Middle East and North Africa (MENA) region, through"}, {"title": "More details on GLORA", "content": "GLORA overall formulates in Eq.2\nG = (Wo + W\u2030U + V)x + XW0 + Ybo + Z + bo\nThe tensors denoted as U, V, X, Y, and Z serve as trainable support structures for downstream tasks.\nNotably, during the entire fine-tuning process, the Wo and bo remain fixed. Among these tensors,\nU is responsible for scaling the weights, while V plays a pivotal role in both scaling the input and\nshifting the weights. X functions as a layer-wise prompt, akin to the prompt tuning paradigm. Y and\nZ are used in scaling and shifting the bias, respectively, contributing to the overall functionality of the\nGLORA model. For more technical details, we refer the reader to [17]."}, {"title": "Dataset", "content": "This dataset, available on a global 0.25\u00b0 \u00d7 0.25\u00b0 latitude-longitude grid, spans around 40 years, with\nhourly measurements encompassing 37 altitude levels and the Earth's surface. Comprising 721 \u00d7 1440\ngrid points, the dataset presents altitude levels in terms of pressure levels. ERA5 represents the fifth\ngeneration of ECMWF reanalysis, and offers improved spatial and temporal resolution. Reanalysis\nemploys data assimilation techniques to combine model forecasts with observations, yielding a\nconsistent and complete dataset that spans multiple decades. The incorporation of ensemble-based\nuncertainty estimates enhances the utility of ERA5 for climate-related applications. Furthermore,\npre-calculated monthly-mean averages facilitate analysis and interpretation. This comprehensive\ndataset highlights the critical role of reanalysis in advancing weather modeling and climate analysis."}, {"title": "Implementation Details", "content": "In our experiments, we operate with two levels of resolutions 5.625\u00b0 (32 \u00d7 64 grid points) and\n1.40625\u00b0 (128 \u00d7 256 grid points). The dataset is partitioned into training data spanning from\n1979 to 2015, validation data for the year 2016, and test data covering 2017 and 2018. All the\ndefault variables have been utilized as inputs in our experiments. For a 5.625\u00b0 resolution, a patch\nsize of 2, and for 1.40625\u00b0 resolution, a patch size of 4 is used. The learning rate adopted for\nour experiments is 1.0x10-5. The embedding dimension is set to 1024. The training process\nis conducted using four V100 GPUs, leveraging fp16 floating point precision. We present the\nresults for a wide range of atmospheric variables, including geopotential_500, temperature_850,\n2m_temperature, 10m_u_component_of_wind, 10m_v_component_of_wind, relative_humidity_850,\nand specific_humidity_850. We present results using the anomaly correlation coefficient (ACC) and\nroot mean square error (RMSE). For ACC, higher values indicate better performance, while for\nRMSE, lower values are preferable. These metrics together provide a comprehensive assessment of\nmodel performance. The regional setting is the MENA region which is a subset of global grid points.\nWith the utilization of a comprehensive set of input variables derived from atmospheric and sur-\nface data, we focus on forecasting future weather conditions given the current atmospheric state.\nSpecifically with a total of 48 input features, majorly includes geopotential, temperature, wind\ncomponents, relative humidity, and specific humidity at various pressure levels are crucial for weather\nprediction tasks due to their direct influence on atmospheric dynamics. The forecasting tasks involve\npredicting seven target variables: geopotential at 500hPa, 2-meter temperature, and eastward &\nnorthward components of the 10m wind, and temperature, relative humidity, & specific humidity at\n850hPa, Lead times ranging from 12 hours to 72 hours are considered, encompassing multiple range\nforecasting scenarios. For training the deep learning models, as described in ClimaX, we utilize a\nlatitude-weighted mean squared error (MSE) loss function and implement early stopping based on\nvalidation loss to prevent overfitting. For more details, we refer readers to [7]."}, {"title": "More Results", "content": "Predictions over ranges: For more accurate predictions, it was observed that a specialized model\nwith a specific lead time stands out. With this approach, we train multiple models for specific lead"}, {"title": "Ablations", "content": "In this section, we delve into two different mechanisms. Firstly, we explore the influence of the rank\nnumber on model behavior to understand how it changes with varying rank numbers. Secondly, we\ninvestigate the integration of LoRA with components other than the attention module, aiming to\nassess the impact of such integration on model performance.\nImpact of rank r: In investigating the impact of rank r in the Lora module, we have observed that\nwithin the range of Lora ranks, optimal results are achievable by integrating rank 16. Specifically, in\nthe context of low-resolution settings, our examination of rank r behavior for regional forecasting\nover a 72-hour prediction range is shown in Table 4.\nLora's significance on attention and fc layers: We also seek to investigate the effectiveness of the\nLORA module in conjunction with the feed-forward network, rather than the attention module only.\nSurprisingly, we observe a degradation in performance with the integration of LoRA into a model of\nthis size. Specifically, we aim to incorporate LoRA with the feed-forward network (Lf1 & Lf12)\nand the results of these experiments are detailed in Table 5.\nMemory and time comparison: Compared to full fine-tuning, LoRA in a parameter-efficient\nfine-tuning paradigm, significantly reduces the number of trainable parameters and accelerates\nconvergence. Furthermore, during training, the memory consumption on the GPU is noticeably lower"}]}