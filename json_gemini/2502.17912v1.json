{"title": "DECOUPLED GRAPH ENERGY-BASED MODEL FOR NODE OUT-OF-DISTRIBUTION DETECTION ON HET- EROPHILIC GRAPHS", "authors": ["Yuhan Chen", "Yihong Luo", "Yifan Song", "Pengwen Dai", "Jing Tang", "Xiaochun Cao"], "abstract": "Despite extensive research efforts focused on Out-of-Distribution (OOD) detection on images, OOD detection on nodes in graph learning remains underexplored. The dependence among graph nodes hinders the trivial adaptation of existing approaches on images that assume inputs to be i.i.d. sampled, since many unique features and challenges specific to graphs are not considered, such as the het- erophily issue. Recently, GNNSafe, which considers node dependence, adapted energy-based detection to the graph domain with state-of-the-art performance, how- ever, it has two serious issues: 1) it derives node energy from classification logits without specifically tailored training for modeling data distribution, making it less effective at recognizing OOD data; 2) it highly relies on energy propagation, which is based on homophily assumption and will cause significant performance degrada- tion on heterophilic graphs, where the node tends to have dissimilar distribution with its neighbors. To address the above issues, we suggest training Energy-based Models (EBMs) by Maximum Likelihood Estimation (MLE) to enhance data dis- tribution modeling and removing energy propagation to overcome the heterophily issues. However, training EBMs via MLE requires performing Markov Chain Monte Carlo (MCMC) sampling on both node feature and node neighbors, which is challenging due to the node interdependence and discrete graph topology. To tackle the sampling challenge, we introduce Decoupled Graph Energy-based Model (DeGEM), which decomposes the learning process into two parts-a graph encoder that leverages topology information for node representations and an energy head that operates in latent space. Additionally, we propose a Multi-Hop Graph encoder (MH) and Energy Readout (ERo) to enhance node representation learning, Condi- tional Energy (CE) for improved EBM training, and Recurrent Update for the graph encoder and energy head to promote each other. This approach avoids sampling adjacency matrices and removes the need for energy propagation to extract graph topology information. Extensive experiments validate that DeGEM, without OOD exposure during training, surpasses previous state-of-the-art methods, achieving an average AUROC improvement of 6.71% on homophilic graphs and 20.29% on heterophilic graphs, and even outperform methods trained with OOD exposure.", "sections": [{"title": "1 INTRODUCTION", "content": "Detecting Out-of-Distribution (OOD) data is crucial for enhancing AI models' robustness, reliability, and safety in real-world scenarios, where input data may deviate from the training data distribution. Many works (Hendrycks & Gimpel, 2016; Liang et al., 2018; Lee et al., 2018; Hendrycks et al., 2018; Liu et al., 2020) have been proposed for OOD detection tasks on i.i.d. data, e.g., images. Such an urgent need also exists in the domains that apply graph-format data, such as medical-diagnosis (Kukar, 2003) and autonomous driving (Dai & Van Gool, 2018). However, there has been a notable lack of work focusing on graph OOD node detection, i.e., detecting OOD nodes on which the model is expected to have low confidence (Amodei et al., 2016; Liang et al., 2018). Due to the interdependence among graph nodes, it is hard to apply the methods designed for i.i.d. inputs directly.\nRecently, GNNSafe (Wu et al., 2023a) adapted energy-based OOD detection for graph data mainly by using GNNs as the backbone, and output energy for nodes as confidence, achiev- ing current state-of-the-art performance. However, it is trained by classification loss and constructs node energy by classifica- tion logits without specifically designed training for modeling data distribution, limiting the performance of identifying OOD nodes. Moreover, the energy propagation technique, used in GNNSafe, heavily relies on the homophily assumption, i.e., that neighbors often belong to the same class. This will lead to significant performance degradation on heterophilic graphs, where neighbors do not share a similar distribution. To mitigate these issues, we suggest training Energy-based Models (EBMs) to detect OOD instances via Maximum Likelihood Estimation (MLE) to obtain better energy, and removing energy propagation to address the heterophily issue. However, training EBMs via MLE requires performing Markov Chain Monte Carlo (MCMC) sampling in training, which is notorious for graphs due to the complexity of graph topology.\nIn this paper, our proposed Decoupled Graph Energy-based Model (DeGEM) alleviates both the heterophily issue and sampling challenges of learning EBMs for large graphs. The key insight in our work is: GNNs can extract the topology information, forming latent space without interdependence. Hence, we can conduct MCMC sampling on latent space to train the energy head. This approach is computationally efficient, as it avoids sampling the adjacency matrix, and eliminates the need for energy propagation techniques, which could degrade performance on heterophilic graphs. Specifically, DeGEM decomposes the EBM into two components: graph encoder and energy head. First, the graph encoder is trained by the Graph Contrastive Learning (GCL) algorithm and classification loss for obtaining informative node representations, then the energy head, trained over latent space by MLE, outputs the energy scores of nodes, which are used as nodes' OOD scores. By doing so, DeGEM transposes operations inherently dependent on the graph structure into the latent representation domain, thereby decoupling subsequent steps from the graph structure's dependency. The benefits are twofold: 1) MCMC sampling can be efficiently conducted on low-dimensional latent space to sample node representations only, dramatically decreasing the computation cost; 2) the Energy head does not require a propagation operation to further extract topology information, avoiding performance degradation on heterophilic graphs.\nMoreover, to better unleash the effectiveness of DeGEM in node OOD detection, we propose several principled training designs: a Multi-Hop Graph encoder (MH) and Energy Readout (ERo) to enhance node representation learning, Conditional Energy (CE) to improve EBM training, and Recurrent Update to effectively update the CE and ERo jointly, enabling the graph encoder and energy head promote each other. Furthermore, we found that existing node OOD detection methods are evaluated only on homophilic graphs, with heterophilic graphs being overlooked. We therefore conduct a comprehensive evaluation of existing methods across homophilic and heterophilic graphs. The results show that existing graph-based methods deliver even worse performance on heterophilic graphs, compared to those graph-agnostic node OOD detection methods. Thanks to the powerful constructed DeGEM, our method can achieve state-of-the-art performance on OOD detection across both homophilic and heterophilic graphs without OOD exposure.\nOur key contributions can be summarized as follows:\n\u2022 Our proposed DeGEM decomposes EBM into two components, a graph encoder for extracting topology information and an energy head for estimating density, which avoids the notorious challenges of sampling the adjacency matrix when training via MLE and prevents serious performance degradation on heterophilic graphs.\n\u2022 We are the first to evaluate node OOD detection performance on both homophilic and heterophilic graphs and provide a comprehensive assessment of existing graph-based methods."}, {"title": "2 PRELIMINARY", "content": "Notations. We denote an undirected graph without self-loops as $G = {X, A}$, where $X = {x_i}_{i=1}^N \\in \\mathbb{R}^{N \\times d_0}$ is the initial node feature matrix, $d_0$ is the feature dimension, and $A \\in \\mathbb{R}^{N \\times N}$ is the adjacency matrix. $\\mathcal{N}(x_i)$ is the feature set of neighbors of node $x_i$. $D$ is a diagonal matrix standing for the degree matrix such that $D_{ii} = \\sum_{j=1}^N A_{ij}$. $H^{(l)} = {h_i}_{i=1}^N \\in \\mathbb{R}^{N \\times d}$ is the representation matrix in the $l$-th layer, where $d$ is the hidden dimension. We use $Y = {y_i}_{i=1}^N \\in \\mathbb{R}^{N \\times C}$ to denote the ground-truth node label matrix, where $C$ is the number of classes and $y_i$ is a one-hot vector.\nGraph Out-of-Distribution Node Detection. OOD detection refers to identifying data samples that do not conform to the distribution of the training data, while keeping the classification capability of in-distribution (ID) data. Formally, an OOD detection score function $\\mathcal{S}(\u00b7, \u00b7)$ should be defined to map the node and its neighbors to a scalar score, such that $\\mathcal{S}(x, \\mathcal{N}(x))$ yields a higher value for OOD nodes than for ID nodes. It can be seen that in the context of graph data, OOD detection becomes challenging due to the complex interplay between node features and graph topology. The OOD score for each node depends on itself and also the relational context provided by other nodes within the graph, which is distinct from OOD detection in the vision domain.\nHeterophily Issue. Most traditional GNNs are designed based on the homophily assumption (Kipf & Welling, 2016; Velickovic et al., 2018), where linked nodes tend to be similar. However, heterophilic graphs-where linked nodes often belong to different categories are prevalent in real-world appli- cations, on which the traditional GCNs perform poorly (Pei et al., 2020). While extensive works have been proposed to address the heterophily issue (Abu-El-Haija et al., 2019; Bo et al., 2021; Zhu et al., 2020a; Chien et al., 2021; Li et al., 2022a; Chen et al., 2023; Luo et al., 2024a) in node classification tasks, it has not received sufficient attention in the context of node OOD detection, resulting in performance degradation when applying models designed for node OOD detection to heterophily graphs.\nEnergy-based Model. A deep EBM models (Du & Mordatch, 2019) the data distribution $p_{\\theta}(x)$ using Boltzmann distribution $p_{\\theta}(x) = \\frac{exp(-E_{\\theta}(x))}{Z_{\\theta}}$ with the energy function $E_{\\theta}(x)$, where $Z_{\\theta}$ is the corresponding normalizing constant. EBM is trained by minimizing the negative log-likelihood (NLL) $\\mathcal{L}_{\\theta}$ of $p_{\\theta}(x)$, such that $\\mathcal{L}_{\\theta} = - \\mathbb{E}_{p_d(x)} [log p_{\\theta}(x)]$. The EBM loss can be reformulated as follows:"}, {"title": "3 METHODOLOGY", "content": "3.1 REVISITING GRAPH EBM FOR OOD DETECTION\nDifferent from image data, the graph data is neither continuous nor i.i.d. The inherent challenge is how to define and train an EBM over $(x, \\mathcal{N}(x))$. Since $\\mathcal{N}(x)$ is discrete, potentially huge, and includes arbitrary neighbor nodes, it is hard to sample $\\mathcal{N}(x)$ by MCMC for training EBM.\nTo avoid such challenge, GNNSafe (Wu et al., 2023b) proposes to train the EBMs by maximizing the conditional likelihood $log \\ p_{\\theta} (y|x) =  via node classification, where $E_{\\theta}(x,y) = \u2212 f_{\\theta}(x,\\mathcal{N}(x))[y]$ and $f_{\\theta}(\u00b7, \u00b7)$ is a classifier. Finally, the energy score of each node can be regarded as OOD score, i.e., $\\mathcal{S}(x, \\mathcal{N}(x)) = E_{\\theta}(x) = - log \\sum_y exp(-E_{\\theta}(x, y))$.\nThe EBMs trained in this way exhibit limitations in terms of capa- bility and a heavy reliance on sufficient labeled data. In particular, it has not been trained for modeling the marginal data distribution, resulting in an inferior ability to capture the data distribution. For instance, it may falter even when tasked with handling a simple 2D dataset, i.e., 8 Gaussians. This indicates that the energy constructed by the classification logits does not effectively capture the underlying data distribution. Consequently, such methods intuitively face challenges in addressing OOD detection for graph data, particularly when labeled data are scarce.\n3.2 OVERVIEW OF OUR METHOD: DEGEM\nThe framework of our proposed method is shown in Fig. 2. DeGEM first utilizes a graph en- coder trained by GCL algorithm (Veli\u010dkovi\u0107 et al., 2018) to extract node representations $h = g_{\\alpha}(x, \\mathcal{N}(x)) \\in \\mathbb{R}^{d}$. In this way, the topology information of the original graph is well-encoded into $h$, such that the follow-up steps can be free from $A$. Next, a $K$-step MCMC sampling is applied over the low-dimensional latent space $h \u223c q_{\\omega}(h)$ to learn an energy function $f_{\\varphi}$, which is defined as an MLP. Please see Appendix E.1 for detailed training algorithm.\n3.3 OUR DESIGN\nThe powerful capability of EBM comes from its minimal restrictions on modeling, but at the same time, this is also a double-edged sword that leads to difficult learning. To address the difficulty of defining EBMs on graphs, we propose to restrict EBM modeling formulation to some extent. Specifically, we suggest decomposing EBM into two parts: the first part focuses on extracting graph structural information, and the second part is dedicated to learning the energy.\nFor node x and its neighbor $\\mathcal{N}(x)$, we can define the EBM as $E_{\\theta}(x) = f_{\\omega} \u00b0 g_{\\alpha}(x, \\mathcal{N}(x))$, where $g_{\\alpha}(x, \\mathcal{N}(x)) = h \u2208 \\mathbb{R}^{d}$ is a graph encoder that focuses on extracting the graph structural information, and $f_{\\omega}(h) \u2208 \\mathbb{R}$ is an energy function that outputs the energy score.\nSuppose we have a fixed $g_{\\alpha}$ that learns graph structural information well, following Eq. (1), the EBM loss for learning $f_{\\omega}$ can be reformulated as:"}, {"title": "4 EXPERIMENTS", "content": "4.1 SETUP\nDataset and Evaluation Metrics. We evaluate DeGEM on seven benchmark datasets for node classification tasks (Yang et al., 2016; Shchur et al., 2018; Rozemberczki et al., 2021; Wang et al., 2020; Pei et al., 2020), including four homophily datasets (Cora, Amazon-Photo, Twitch, and ogbn-Arxiv) and three heterophily datasets (Chameleon, Actor, and Cornell). We mainly follow (Wu et al., 2021; 2023b) to adopt two established methods to simulate OOD scenarios. In the multi-graph context, OOD samples stem from distinct graphs or subgraphs not linked to the training nodes. Conversely, in the single-graph setting, OOD samples are part of the same graph as the training data but remain unseen during training. For Twitch, we treat one subgraph as in-distribution (ID) and others as OOD, using one for OOD exposure during training. In ogbn-Arxiv, we split the nodes by publication year for ID, OOD, and OOD exposure sets. For Cora, Amazon, Chameleon, Actor, and Cornell that have no clear domain information, we synthesize OOD data in three ways: i) Structure manipulation (S); ii) Feature interpolation (F); iii) Label leave-out (L). See detailed settings and splits in Appendix E.2. For the assessment of OOD detection performance, we employ standard metrics: Area Under the Receiver Operating Characteristic curve (AUC), Area Under the Precision-Recall curve (AUPR), and the False Positive Rate at 95% True Positive Rate (FPR95). In-distribution (ID) performance is quantified using the accuracy (Acc) metric on the testing nodes. In the following text, we use AUC to denote AUC with a little bit of abuse. Due to the space limits, we mainly present the results of AUC and Acc, the full results with AUPR and FPR95 can be found in Appendix I.\nBaseline Comparisons. Our model is benchmarked against two categories of baseline methods. The first category comprises models that handle i.i.d. inputs, which are predominantly used in computer vision. These include MSP (Hendrycks & Gimpel, 2016), ODIN (Liang et al., 2018), Mahalanobis (Lee et al., 2018), OE (Hendrycks et al., 2018), and Energy(-FT) (Liu et al., 2020). We also include ResidualFlow (Zisselman & Tamar, 2020), a density-based method capable of modeling data distribution. For a fair comparison, we substitute the CNN backbones in these models with a GCN encoder. The second category consists of methods tailored for graph- structured data, such as GKDE (Zhao et al., 2020b), GPN (Stadler et al., 2021b), OODGAT (Song & Wang, 2022) and GNNSafe (++) (Wu et al., 2023a). It is noteworthy that OE, Energy-FT and GNNSafe++ incorporate OOD samples during the training phase, i.e., trained with OOD exposure.\nImplemetation Details. We implement our model by PyTorch and conduct experiments on 24GB RTX-3090ti. Epoch number $E = 200$, MH layer number $L = 5$, hidden dimension $d = 512$, MCMC steps $K = 20$. We use Optuna (Akiba et al., 2019) to search hyper-parameters for our proposed model and baselines (see Appendix E.3 for detailed search space)."}, {"title": "4.2 EVALUATION RESULTS", "content": "We report the results of the proposed DeGEM and competing baselines on different datasets in Tabs. 1 and 2. Our finding indicates that the DeGEM consistently outperforms competing baselines without OOD exposure in terms of AUC and Acc across both homophilic and heterophilic datasets. Specifically, DeGEM increases the average AUC by 4.26% (resp. 20.63%) on homophilic (resp. heterophilic) graphs and improves the average Acc for homophilic (resp. heterophilic) graphs by 2.37% (resp. 13.95%). On the contrary, although the baselines tailored for graph inputs surpass those designed for i.i.d. data on homophilic graphs, they show a lower performance on heterophilic graphs. Additionally, after adding the Energy Propagation technique to Energy (i.e., GNNSafe), the average AUC decreases significantly from 70.01% to 55.23% and 56.42%, indicating that the Energy Propagation technique will cause severe performance decrease on heterophilic graphs.\nThe high performance of DeGEM is primarily attributed to the enhancement brought by the robust representation learned by the GCL algorithm and the classification loss, as well as the powerful data modeling capabilities of EBM trained via the MLE approach. Overall, these findings underscore the superiority of DeGEM over baseline approaches, on both homophilic and heterophilic graphs."}, {"title": "4.3 EVALUATION ON LIMITED LABELS", "content": "Given the time-consuming and labor-intensive na- ture of obtaining node labels on graphs in the real world, an interesting evaluation task is to evaluate the OOD detection performance of a model with limited category labels. We follow the data splits used in the experiments of Sec. 4.2 as a foundation and progressively reduce the proportion of available labels in the training set: from 100% to 50%, and then to 10%. To highlight the performance of our method, we select three baselines OE, Energy-FT and GNNSafe++ which incorporate OOD samples during the training phase, i.e., trained with OOD exposure. Note that we do not reduce the number of nodes used for OOD exposure training, but only decrease the number of category labels available within the distribution. We present the average results of each dataset in Tab. 3, and the average performance of each method in Fig. 4 for more intuitive comparison. We report the full results in Appendix I. The results show that our proposed DeGEM maintains high performance under different label proportions, with no significant decline; in contrast, all baselines exhibit a marked performance drop as the labels ratio decrease. In particular, when the available label rate is 100%, our method still outperforms baselines, highlighting the"}, {"title": "4.4 ABLATION STUDY", "content": "The Impact of Proposed Techniques. In what follows, we evaluate the impact of training the energy head via MLE (MLE-Energy) versus deriving node energy directly from classification logits (Classify-Energy). Additionally, we assess the use of DGI algorithm during graph encoder training (GCL), and explore the effectiveness of the Energy-Propagation technique (Eprop), the Multi-Hop Graph Feature Encoder (MH), Conditional Energy (CE), and Energy Readout (ERO). We present the results in Tab. 4. The baseline (Row 1) utilizes a GCN for node classification and obtains the node energy by the classification logits. Since Eprop is based on homophily assumption, the performance increases slightly (Avg +3.7%) on homophilic graphs but shows a dramatic degradation (Avg -16%) on heterophilic graphs (Row 2). Compared to the baseline (Avg 76.87%), solely utilizing the MLE for training the energy head (Avg 72.24%) or employing GCL algorithm for training the graph encoder (Avg 73.90%) does not work. It is the combination of MLE and GCL algorithm that brings positive effects for OOD detection tasks (Avg 87.82%). More detailed discussion can be found on Observation 1\u20134 below. With MH, DeGEM achieves a significant performance boost (Avg +11%, Row 9), demonstrating that combining local and global information simultaneously enhances performance on both homophilic and heterophilic graphs. CE alone can improve the average performance (Avg +0.63%, Row 10); however, using ERo on its own does not yield similar benefits (Avg -0.94%, Row 11). This is because CE takes the global view into account, producing better node energy, which enables ERo to generate a more optimized readout summary. Therefore combining them together via Recurrent Update can bring more positive effects (Avg +1.33%, Row 12).\nObservation 1: combining DGI with classification does not work. As shown in Tab. 4, although the Classifier-Energy variant shows 5.93% improvement of AUC on heterophilic graphs after adding DGI loss (GCL) into the classification task (Row 3), its performance on homophilic graphs dramatically decrease from 77.22% to 65.37%. This indicates that the integration of DGI algorithm and Classifier-Energy has an overall negative impact on its performance.\nObservation 2: naively training Energy Head via MLE does not work. As shown in Tab. 4, when decoupling the model into two parts (a graph encoder and an energy head), if we train the energy head via MLE but only employ classification loss for training the graph encoder (Row 4), the model will show a worse performance compared to the Classifier-Energy variant (Row 1).\nObservation 3: combining DGI and MLE works well As shown in Tab. 4, after adding DGI loss (GCL), the performance of MLE-Energy variant dramatically improves from 72.74% to 87.82% on average in terms of AUC, outperforming previous state-of-the-art method GNNSafe.\nObservation 4: combining other GCL and MLE not works well We try to replace the DGI algorithm with other widely used GCL algorithms: GRACE (Zhu et al., 2020b) and SUGRL (Mo et al., 2022) (without MH, CE, and ERO)."}, {"title": "5 RELATED WORK", "content": "EBMs on Graph. Some previous works (Hataya et al., 2021; Liu et al., 2021) apply EBMs on graphs that are trained with MLE have been proposed for graph generation. However, their works are performed on small graphs, where the over-computational issue of the sampling adjacency matrix is neglected, making them non-scalable. In contrast, our proposed CLEBM focuses on OOD detection, decomposing the EBM networks into informative representation extraction and output energy score based on given latent. The design enables us to move MCMC to latent space, which does not suffer from sampling adjacency matrix and therefore has excellent scalability.\nGraph OOD Detection. OOD detection for non-graph data by neural networks has garnered considerable attention in the literature (Hendrycks & Gimpel, 2016; Hendrycks et al., 2019; Bevandi\u0107 et al., 2018; Liu et al., 2020; Mohseni et al., 2020; Ren et al., 2019). However, these methods typically assume that instances (such as images) are i.i.d., overlooking scenarios with inter-dependent data that are common in many real-world applications. In contrast, Graph OOD Detection that inherently includes inter-dependent structures has not been explored well. Recently, some works (Li et al., 2022b; Bazhenov et al., 2022) focus on Graph OOD Detection on graph-level, i.e., detecting OOD graphs. These works treat each graph as an independent instance, while OOD detection on node-level presents unique challenges given the non-negligible inter-dependence between instances. To this end, Bayesian GNN models have been proposed that can detect OOD nodes within a graph by incorporating the inherent uncertainty in such inter-dependent data (Zhao et al., 2020a; Stadler et al., 2021a). OODGAT (Song & Wang, 2022) emphasizes the importance of node connection patterns for outlier detection, explicitly modeling node interactions and separating inliers from outliers during feature propagation. Energy-based Detection on graphs has been explored in GNNSafe (Wu et al., 2023b), by directly combing GNNs and Energy-based Detection on Images (Liu et al., 2020). However, their energy score is directly construed by classification, which is less effective. Moreover, they use energy propagation to enhance performance, which highly relies on the homophily assumption. Additionally, some methods train the model with OOD Exposure, i.e., training with both a known ID dataset and a known OOD dataset (Hendrycks et al., 2018; Liu et al., 2020; Wu et al., 2023b). In contrast, we decompose EBM learning into representation learning and energy learning, delivering better detection capability without OOD exposure (see Sec. 4.2). Additionally, benefits from powerful energy construction and effective graph encoder, we do not require energy propagation, thus keeping high performance across homophilic and heterophilic graphs."}, {"title": "6 CONCLUSION", "content": "We introduce a novel approach, DeGEM, for graph OOD node detection, overcoming the heterophily issue and the computational challenges associated with MCMC sampling in large graphs. By decoupling the learning process into a GNN-based graph encoder and an energy head, we managed to leverage the GCL algorithm and classification loss to learn robust node representations and perform efficient MCMC sampling in the latent space, circumventing the need to directly sample the adjacency matrix. The design of DeGEM, featuring a Multi-Hop Graph encoder and a Recurrent Update mechanism, facilitates the incorporation of topological information into node representations, which is crucial for OOD detection in graph-structured data. Extensive experimental evaluations have validated the effectiveness of DeGEM, which not only exhibits superior performance on both homophilic and heterophilic graphs compared to baselines with/without OOD exposure, but also outperforms methods trained with OOD exposure in a label-insufficient scenario."}, {"title": "A POTENTIAL BROADER IMPACT", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."}, {"title": "B LIMITATIONS", "content": "DeGEM has achieved excellent performance, but the results are nearly maxed out on some datasets. We need better and more challenging benchmarks to evaluate performance. Additionally, DeGEM shares the same drawback as EBMs trained with MLE: training requires MCMC sampling. However, it still shows superior performance. Besides, the cost of MCMC sampling can be reduced by cooperative learning (Xie et al., 2018; 2021; Luo et al., 2024b)."}, {"title": "C ADDITIONAL RELATED WORKS", "content": "Graph Contrastive Learning. Contrastive learning (CL) stands as a widely applied self-supervised learning method, aiming to derive informative sample representation solely from feature information. The main idea of CL is to align the representations of similar samples in close proximity while driving apart the representations of dissimilar samples. Witnessing the remarkable advancement of Graph Neural Networks (GNNs), a substantial amount of recent research has focused on Graph Contrastive Learning (GCL) (Veli\u010dkovi\u0107 et al., 2018; Peng et al., 2020; Hassani & Khasahmadi, 2020; Zhu et al., 2020b; Mo et al., 2022). DGI (Veli\u010dkovi\u0107 et al., 2018) learns by maximizing mutual information between node representations and corresponding high-level summaries of graphs. GRACE (Zhu et al., 2020b) maximizes the agreement of corresponding node representations in two augmented views for a graph. SUGRL (Mo et al., 2022) explores the complementary information from structural and neighbor information to maximize inter-class variation and minimize intra- class variation through triplet losses and an upper bound loss, while removing the need for data augmentation and discriminators. Our work builds upon the foundation of GCL, where we employ DGI as part of learning the graph encoder that extract graph topology information. We also establish the relationship between DGI and EBM: DGI can be understood as an EBM trained by noise contrastive estimation. To further enhance the capabilities of the proposed method, we design recurrent update to let the energy head and DGI promote each other in training (see Sec. 3.3), and experiments show that this to be greatly beneficial for OOD detection (see Sec. 4.4).\nNode Anomaly Detection (NAD). NAD is a binary classification task, that directly categorizes nodes into two different categories: normal and anomalous. Some works (Dong et al., 2025a; 2024; 2025b; Zhao & Akoglu, 2021; Gong et al., 2023) have been proposed to handle this problem. In contrast, node OOD detection requires balancing the ability to classify in-distribution nodes and detect out-of-distribution nodes."}, {"title": "D DERIVIATIONS", "content": "D.1 TRAINING EBMS\nGiven a Boltzmann distribution $p_{\\theta}(x) = \\frac{exp(-E_{\\theta}(x))}{Z_{\\theta}}$, its negative log-likelihood (NLL) $\\mathcal{L}_{\\theta}$ is:"}]}