{"title": "Tight Verification of Probabilistic Robustness in Bayesian Neural Networks", "authors": ["Ben Batten", "Mehran Hosseini", "Alessio Lomuscio"], "abstract": "We introduce two algorithms for computing tight guarantees on the probabilistic robustness of Bayesian Neural Networks (BNNs). Computing robustness guarantees for BNNs is a significantly more challenging task than verifying the robustness of standard Neural Networks (NNs) because it requires searching the parameters' space for safe weights. Moreover, tight and complete approaches for the verification of standard NNs, such as those based on Mixed-Integer Linear Programming (MILP), cannot be directly used for the verification of BNNs because of the polynomial terms resulting from the consecutive multiplication of variables encoding the weights. Our algorithms efficiently and effectively search the parameters' space for safe weights by using iterative expansion and the network's gradient and can be used with any verification algorithm of choice for BNNs. In addition to proving that our algorithms compute tighter bounds than the SoA, we also evaluate our algorithms against the SoA on standard benchmarks, such as MNIST and CIFAR10, showing that our algorithms compute bounds up to 40% tighter than the SoA.", "sections": [{"title": "1 Introduction", "content": "In recent years Neural Networks (NNs) have been proposed to perform safety-critical functions in applications such as automated driving, medical image processing and beyond (Abiodun et al., 2018; Anwar et al., 2018; Bojarski et al., 2016; Liu et al., 2021b). For machine learning methods to be adopted in these areas, assurance and certification are paramount."}, {"title": "Related Work", "content": "Mixed-Integer Linear Programming (MILP) and Bound Propagation (BP) are two common techniques used for the verification of standard NNs. In MILP-based approaches, the verification problem for NNs is encoded as an MILP problem (Lomuscio and Maganti, 2017; Hosseini and Lomuscio, 2023; Tjeng et al., 2019), which can be solved using MILP solvers. A common assumption in MILP-based approaches is that activations are piecewise-linear.\nPropagation-based approaches differ from MILP-based approaches in that they consider a relaxed network and propagate bounds on each node's output through the network's layers to solve the verification problem (Gowal et al., 2018; Singh et al., 2019, 2018; Wang et al., 2018).\nSoA approaches for the verification of NNs combine MILP and BP to achieve complete and scalable verification for NNs. For instance, (Kouvaros and Lomuscio, 2021) uses bound propagation alongside adaptive splitting (Henriksen and Lomuscio, 2020; Botoeva et al., 2020) to reduce the number of binary variables in the resulting MILP problem and thus further speed up the verification process. An important and well-studied safety specification in NN verification is robustness, i.e., a model's resilience in returning the same output for all values in a small neighbourhood of a given input. An example approach devoted only to checking robustness is (Cardelli et al., 2019b), where they compute upper bounds on the robustness of BNNs. Closely related to this, (Cardelli et al., 2019a) considers the problem of computing the probability of whether, for a given input to a BNN, there exists another input within a bounded set around the input such that the network's prediction differs for the two points and provides an approach to approximate this probability and compute statistical guarantees for it.\nIn the same line of research (Carbone et al., 2020) shows that the vulnerability of BNNs to gradient-based attacks arises as a result of the data lying on a lower-dimensional submanifold of the ambient space.\n(Wicker et al., 2020) considers a set-up similar to this paper, using a sampling-based approach to bound the probabilistic robustness of BNNs. In this approach, they sample orthotopes from the posterior distribution and use Interval Bound Propagation (IBP) and Linear Bound Propagation (LBP) to determine whether a given orthotope is safe. By repeating this process, they obtain a robustness certificate and a lower bound for the probabilistic robustness of the BNN.\nBerrada et al. (2021) consider the adversarial robustness of BNNs. They use the notion of Lagrangian duality, which replaces standard Lagrangian multipliers with functional multipliers. For an optimal choice of functional multipliers, this approach leads to exact verification, i.e., the exact computation of probabilistic robustness. Nevertheless, this may not be practical. The authors show that for specific classes of multipliers, however, one can obtain lower bounds for the probabilistic robustness.\nAdams et al. (2023) introduced an algorithm to verify the adversarial robustness of BNNs. They use Dynamic Programming alongside bound propagation and"}, {"title": "2 Preliminaries", "content": "Throughout this paper, we denote the set of real numbers by R, use italics, $w$, to refer to scalers in R, bold italics, $\\bf{w}$, for vectors in R"}, {"title": "2.1 Bayesian Neural Networks", "content": "Definition 2.1 (Bayesian Neural Network). Given a dataset, D, let $f_w : R^m \\rightarrow R^n$ be a Bayesian neural network with w a vector random variable consisting of the parameters of the network. Assuming a prior distribution over the parameters, $w \\sim p(w)$, training amounts to computing the posterior distribution, $p(w|D)$, by the application of Baye's rule. For a vector, w, sampled from the posterior, $p(w|D)$, we denote the deterministic neural network with parameters w by $f_w$. We use $n_w$ to refer to the dimension of the parameters space of the network.\nIn BNN literature, it is customary to assume the prior and the posterior both have normal distribution; nevertheless, our Pure Iterative Expansion (PIE) algorithm works for arbitrary distributions, and our Gradient-guided Iterative Expansion (GIE) algorithm works for all continuous distributions. In the case of normal distribution, training requires finding the mean $\\mu \\in R^{n_w}$ and covariance $\\Sigma \\in R^{n_w \\times n_w}$ of the parameters such that $p(w|D) \\approx q(w) \\sim N(\\mu, \\Sigma)$. We refer to the parameters' variance by $\\sigma^2 = diag(\\Sigma)$. When the parameters are independent, i.e., when the covariance"}, {"title": "2.2 The Probabilistic Robustness Problem for BNNS", "content": "Let us start by defining the probabilistic robustness problem for BNNs.\nDefinition 2.2 (Probabilistic Robustness Problem). Given a BNN $f_w : R^m \\rightarrow R^n$, a robustness specification for $f_w$ is a tuple $\\varphi = (X,S)$, where $X \\subseteq R^m$ is an e-ball $B_\\epsilon(x)$ around a given $x \\in R^m$ with respect to a norm $||\\cdot||_p$ and $S \\subset R^n$ is a half-space\n$S = \\{y : a^T \\cdot y \\geq b\\},$ (1)\nfor some $a \\in R^n$ and $b\\in R$. The probabilistic robustness problem concerns determining the probability that for all inputs in X the output of $f_w$ is in S, i.e.,\n$P_{safe}(X, S) = P_{w\\sim w}(\\forall x \\in X, f_w(x) \\in S)$.\nWe refer to X as the input constraint set and S as the output constraint set. The probabilistic robustness problem is sometimes referred to as the probabilistic safety problem (Cardelli et al., 2019b) or the adversarial robustness problem (Berrada et al., 2021) for BNNs; in the same light, is also referred to as the safety specification.\nFor a classification task, when the goal is to verify for a correct class c against a target class c', we fix $a_c = 1, a_{c'} = -1$, and $a_i = 0$ for, $i \\neq c, c'$ in Equation (1).\nDefinition 2.3 (Set of Safe Weights). The set of safe weights for a BNN with respect to a specification $\\varphi = (X, S)$, is defined as\n$C = \\{w \\in R^{n_w} : \\forall x \\in X, f_w(x) \\in S\\}.$ (2)\nProposition 2.1. Given a BNN $f_w$, such that $w \\sim q(w)$ for a probability density function q, and a safety specification (X, S), we have that\n$P_{safe}(X, S) = \\int_{C} q(w)dw.$"}, {"title": "3 Computing the Probabilistic Robustness for BNNS", "content": "In Subsection 2.2 we discussed why calculating $P_{safe}$ is computationally infeasible. Besides empirical sampling, the current approaches for computing $P_{safe}$ rely on sampling orthotopes $\\hat{C}_i \\subseteq R^{n_w}$ with fixed sizes, whose edges are proportional to the standard deviations of the weights of the network, and verifying whether $f_{\\hat{C}}(X) \\subseteq S$ through Linear Bound Propagation (LBP) (Wicker et al., 2020). This enables the computation of an under-approximation $\\hat{C} = \\bigcup \\hat{C}_i \\subseteq C$, which, in turn, allows computing the lower bound\n$P_{safe} = \\int_{\\hat{C}} q(w)dw < P_{safe}(X, S)$.\nOne weakness of this approach is that it uses orthotopes of the same size for all specifications. For example, whilst for a given input to a BNN it may be possible to fit large orthotopes around the weights's mean and use LBP to verify the safety of these orthotopes, for another input to the same network this may not necessarily be the case, and must resort to smaller orthotopes, which in high dimensions result in obtaining minuscule probabilistic robustness lower bounds, a phenomenon demonstrated in Example 3.2.\nTo mitigate this problem, we introduce two approaches that allow us to dynamically adjust the size of orthotopes $\\hat{C}_1,...,\\hat{C}_s$. The Pure Iterative Expansion (PIE) approach, which we introduce in Subsection 3.1, allows us to dynamically scale the size of and orthotope $\\hat{C}_i$, and therefore, cover a larger volume in C.\nThe more sophisticated Gradient-guided Iterative Expansion (GIE) approach, which we introduce in Subsection 3.2, not only allows the use of dynamic scaling, but also expands the orthotopes further in the dimensions that allow $\\hat{C}_i$'s to remain in C, according to the posterior's gradient, $\\nabla_w.f_w$.\nBoth PIE and GIE can be accompanied by LBP or other approaches. Since our iterative expansion approach depends on iteratively expanding $\\hat{C}_i$'s and verifying their safety, we use LBP, as described and implemented in (Wicker et al., 2020), because of its significant computational efficiency."}, {"title": "3.1 Pure Dynamic Scaling", "content": "Given a BNN, $f_w$, and safety specification, $\\varphi = (X,S)$, remember that we denote the set of safe weights of $f_w$ with respect to as C. The pure sampling and LBP approach (Wicker et al., 2020) starts by the under-approximation $\\hat{C} = \\{\\}$ of C and samples $w_0 \\sim q(w)$. It then defines $C_i = [w_i \\pm\\lambda \\sigma]$, where $\\lambda > 0$ is the scaling factor and $\\sigma$ is the parameters' standard deviation vector. Then, using IBP or LBP, it checks whether $C_i \\subseteq C$. If this is the case, $\\hat{C} = \\hat{C}\\bigcup C_0$; otherwise, $C_0$ is discarded. This process is repeated until a given number of weights s have been tried.\nAs we briefly discussed in the beginning of Section 3 and demonstrate in Section 4, the lower bounds computed using a pure sampling approach are not tight, because of the rigidity of the sampled orthotopes.\nThe first and most straightforward solution to this problem that we propose is to instead use dynamically-scaled orthotopes, as outlined in Algorithm 1. Wherein, after sampling an orthotope $\\hat{C}_i = [w_i\\pm\\lambda \\sigma] \\subseteq C$ and verifying it (line 8 in Algorithm 1), we expand the orthotope to $\\hat{C}_i = [w_i \\pm 2\\lambda \\sigma]$ rather than immediately moving on to the next sampled parameter vector, as in the pure sampling approach. After the initial expansion, we again check $\\hat{C}_i \\subseteq C$. If this is the case, we can repeat the same process for $\\hat{C}_i = [w_i \\pm 3\\lambda \\sigma]$. We repeat this, until $\\hat{C}_i \\nsubseteq C$.\nExample 3.1 demonstrates the advantage of the PIE approach over the pure sampling-based approach.\nExample 3.1. Consider a BNN with two Bayesian layers, $f_w(x) = ReLU(W_2 \\cdot ReLU(W_1 \\cdot x))$, where $W_1,W_2 \\sim N(0,1)$ and no bias terms. For the input constraint set $X = \\{1\\}$ and the output constraint set"}, {"title": "3.2 Gradient-Guided Dynamic Scaling", "content": "The goal of this subsection is to improve Algorithm 1 even further by using the gradient of the network, $\\nabla_wf_w$, to maximise our coverage of the safe set of weights, C, as summarised in Algorithm 2. Recall that for a given point in the parameter space, $w \\in C$, our goal is to explore C starting from w. Instead of expanding $C_i$ uniformly in all dimensions, we can use the parameter-wise BNN gradient, $\\nabla_wf_w$, to expand $C_i$ further in dimensions that do not violate $C_i \\subset C$.\nFrom Equation (2), we have that\n$C = \\{w \\in R^{n_w} : a^T \\cdot f_w(B_\\epsilon(x)) \\geq b\\},$ \nfor some $a \\in R, x \\in R^m, b \\in R, and $\\epsilon > 0$. To maximise the volume of a candidate orthotope, $\\hat{C}_i$, while staying within the bounds of C, we propose expanding further in directions with zero or positive gradients in $\\nabla_wa^T \\cdot f_w(x)$. Concretely, let\n$\\mathcal{E}^- = \\{e_i \\in R^{n_w} : \\nabla_w(a^T \\cdot f_w)(x) \\cdot e_i <0\\},$\\n$\\mathcal{E}^+ = \\{e_i \\in R^{n_w} : \\nabla_w(a^T \\cdot f_w)(x) \\cdot e_i > 0\\},$\\n$\\mathcal{E}^0 = \\{e_i \\in R^{n_w} : \\nabla_w(a^T \\cdot f_w)(x) \\cdot e_i = 0\\},$ \nwhere $\\{e_1,..., e_{n_w}\\}$ is the standard basis for the $R^{n_w}$.\nIntuitively, we can cover more of C by moving further in directions that take us away from the boundary at $a^T \\cdot f_w(x) = b$. In particular, when the activations considered are ReLU, where we have that $\\nabla_w(a^Tf_{w_i})(x) \\cdot e_i = 0$ for all i's corresponding to a weight feeding directly to an inactive node. Moreover, the value of a node is a locally linear function of the weights that are input to that node. Therefore, we have the freedom of moving in the directions corresponding to these weights without changing the output of the network, and hence, remaining inside C. We note that this assumption only holds locally, and with larger deviations in the parameter space, the activation pattern of the network will change. However, we observe that assuming local linearity yields a substantial increase in the volume of verifiable $\\hat{C}_i$'s."}, {"title": "3.3 Theoretical Comparison", "content": "Here we compare Algorithms 1 and 2 against the SoA sampling-based approach (Wicker et al., 2020) and show that both algorithms introduced here produce provably tighter lower bounds for the probabilistic robustness of BNNs. We start this by comparing Algorithms 1 and 2 against the static orthotopes approach of Wicker et al. (2020). We show that Algorithms 1 and 2 always provides tighter lower bounds than the static orthotopes approach.\nProposition 3.1. For every BNN, $f_w$, and specification, $\\phi = (X, S)$, let p be the probabilistic robustness of $f_w$ with respect to , and $p_s$, $p_p$, and $p_g$ be the probabilistic robustness lower bounds computed with the"}, {"title": "4 Evaluation", "content": "We evaluate the performance of our approach against the previously discussed SoA approach for computing lower bounds for probabilistic robustness (Wicker et al., 2020). As in previous studies, the approach is evaluated on the MNIST dataset (LeCun et al., 2010) and additionally the CIFAR10 dataset (Krizhevsky, 2009). All experiments were run on an AMD Ryzen Threadripper 3970X 32-core with 256GB RAM.\nThe evaluation is divided into three parts. (1) Benchmark against the SoA pure sampling approach in (Wicker et al., 2020). (2) Ablation study on the use of gradient-based dynamic scaling factors. (3) ablation study on the number of expansion iterations.\nFor the MNIST baselines, we use 6 fully-connected BNNs from (Wicker et al., 2020). However, we note that these networks have narrow distributions (s.d. of \\approx 10^{-5}). Therefore, we also use two networks trained for broader posterior distributions (s.d. of \\approx 10^{-3}) that achieve comparable clean accuracy and show greater natural resilience to adversarial attack, a key strength of BNNs (Bortolussi et al., 2022) we provide the train details in Appendix D. MNIST networks are identified using an $l\\times n$ nomenclature with l, the number of hidden layers, and n, the number of nodes per hidden layer.\nWe also include a Convolutional Neural Network (CNN) trained on the CIFAR10 dataset. The CNN consists of 5 deterministic convolutional layers followed by 3 fully-connected Bayesian layers with 256, 64, and 10 nodes, respectively. As in (Wicker et al., 2020), all parameter posteriors are Gaussian."}, {"title": "4.1 Comparison of Pure Sampling, PIE, and GIE Approaches", "content": "The performances of sampling-based methods are biased by the compute available and the maximum number of iterations. To remove this bias, we enforce an equal limit on the total number of LBP calls per image. The results are presented in Table 1. We note that the GIE approach must also compute network gradients a step that neither PIE nor the pure sampling approach require. Nevertheless, in practice, we find that the computation of network gradients is much faster than the LBP used in all approaches. We quantify and discuss the impact on runtime in Appendix C. Importantly, we noticed an error in the official implementation in (Wicker et al., 2020) affecting the computation of $p_{safe}$. We have corrected this error to generate the results presented in Table 1 and provide a detailed breakdown of the mistake in Appendix B.\nThe results show a significant improvement over the pure sampling method of (Wicker et al., 2020) in all cases with gradient-based scaling providing the tightest guarantees. Moreover, we remark that in obtaining these results the method from (Wicker et al., 2020) depends on careful selection of hyperparameters, namely the orthotope side length, where the use of suboptimal values yields trivially small guarantees - for the results in Table 1 we used grid-search to tune the hyperparameters in the pure sampling approach. Differently, by using a small value for the initial orthotope's side length, our approach is able to adapt to each image and each network with little consideration for tuning hyperparameters.\nIn summary, the approach here presented considerably outperforms the present SoA in estimating probabilistic robustness leading to bounds that often more than double those in (Wicker et al., 2020)."}, {"title": "4.2 Ablation Study on the Use of Gradient-Based Dynamic Scaling Factors", "content": "Here we explore the impact of using gradient-based dynamic scaling factors. In Figure 3, we plot the probabilistic certification, i.e., the mean $p_{safe}$, attained for a range of gradient-based scaling factors and networks. Each point is averaged over 50 images from the MNIST test set, same as (Wicker et al., 2020). In each case, we observe that the optimal scaling factor is larger than 0 - which represents no gradient-guided scaling and is marked by the red dashed line. The optimal gradient-based scaling factor varies between networks, as does its effect on the overall $p_{safe}$. Examining the results for individual images with a gradient-based scaling factor of 0 goes some way to explaining this behaviour.\nFor the 1x256 network, we find that 89.1% of the correctly classified images have $p_{safe}$ values greater than 99.9% or less than 0.1%. On these networks, most of the images are either trivially verifiable or prohibitively challenging, with only 10.9% in between. In such instances, the use of gradient-based scaling factors has little effect, as $p_{safe}$ is either very close to 1, or adversarial examples are found before $p_{safe}$ can reach substantial values.\nTo further illustrate this point, we examined the CNN network, which was most sensitive to the gradient-based scaling factor in the same way. We found that only 68.6% of the correctly classified images lay outside the 0.1% - 99.9% boundary. To remove this dilution effect, we examined how gradient-based dynamic scaling impacts a single image, for which we use the same example given in Figure 2. Using no gradient-based dynamic scaling on this image, we achieve $p_{safe}$ = 36.5 while using a gradient-based scaling factor of 0.2 we reach $p_{safe}$ = 97.8, highlighting the importance of gradient-based dynamic scaling on certain inputs."}, {"title": "4.3 Ablation Study on the Number of Iterations", "content": "To better understand the inner workings of the iterative expansion approaches presented in this paper, we calculated the number of iterations it took Algorithm 1 to reach the maximum probabilistic certification presented in Table 1. The results are available in Figure 4. We observe that Algorithm 1 takes 8 iterations to start computing a meaningful lower bound and at most 13 iterations to reach maximum probabilistic certification for the MNIST networks. The minimum number of iterations to obtain a meaningful lower bound can be reduced, e.g., to a single iteration, by increasing the value of A in line 8 of Algorithm 1; however, this is disadvantageous for two reasons: (1) one has to manually search for an optimal value for A similarly to the pure sampling approach, (2) this can reduce the certified robustness obtained in later iterations because the expansion rate of PIE increases with \u5165."}, {"title": "Conclusions", "content": "As we discussed in the introduction, BNNs have been proposed as an inherently robust architecture. This is particularly important in safety-critical applications, where robust solutions are essential, and models need to be shown to be highly robust before deployment.\nGiven this, it is essential to ascertain the robustness of BNNs in production. Differently from plain neural networks, where robustness verification can be recast into a deterministic optimisation problem, establishing the robustness of BNNs is a probabilistic problem. Finding an exact solution to this problem is infeasible even for small models. As reported, proposals have been put forward to approximate the probabilistic safety of BNNs by sampling regions in the weight space; however, such approximations are loose and underestimate the actual robustness of the model, thereby providing relatively little information. This limits the possibility of using BNNs in tasks where robustness needs to be verified before deployment.\nWe have put forward an alternative approach to search the weight space, and therefore, approximate the lower bounds of probabilistic safety in BNNs. The algorithms proposed provably produce tighter bounds than the SoA sampling-based approaches. This conclusion was validated in all experiments conducted. The approaches were evaluated on all the previously proposed benchmarks, as well as new benchmarks introduced in this paper, reporting a marked improvement on the bounds of all benchmarks. In some cases, we obtain bounds that are 50% higher than the previously known bound of just over 20%, opening the way to better evaluate the robustness of BNNs before deployment."}, {"title": "A Limitations", "content": "Methods for verifying the probabilistic robustness of BNNs are extremely sensitive to parameter count this remains true for both algorithms here proposed. Therefore scaling to larger networks is a challenge due to two primary effects: Firstly, as network size increases so does the parameter count and respective dimensionality of the probability space. The higher this dimensionality, the more important sampling large orthotopes becomes, and the less effective sampling multiple orthotopes becomes. This reliance on being able to verify single large orthotopes makes these algorithms vulnerable to loose verification techniques. Secondly, as network size increases it is well known that the tightness of bound propagation techniques suffers, thus impacting the ability of our algorithms to verify any orthotopes. Both of these limitations can be alleviated by using tighter verification procedures in place of bound propagation."}, {"title": "B Correction to (Wicker et al., 2020) implementation", "content": "We remarked in Section 4 that while generating the experimental results we identified an error in the implementation in (Wicker et al., 2020). In the following, we present more details of the issue identified. The results presented in the paper refer to the corrected version of their implementation.\nGiven a series of s potentially-overlapping orthotopes, $\\hat{C}_1,...,\\hat{C}_s$, we can calculate the probability captured in the series by computing the cumulative probability for each orthotope $\\hat{C}_i$ and summing over the series, while taking into account that we need to subtract the cumulative probabilities of the intersections and so on according to the inclusion-exclusion principle. Computing the cumulative probability of a single orthotope is trivial as the distributions in each dimension are independent; accordingly, we have\n$P(C_i) = \\prod_{d=1}^{n_w} \\frac{1}{2} ( erf(\\frac{\\mu_d - l_i}{\\sigma_d \\sqrt{2}}) - erf(\\frac{\\mu_d - u_i}{\\sigma_d \\sqrt{2}})).$ (3)\nwhere $\\mu_d$ and $\\sigma_d$ are the mean and standard deviation of the d-th parameter and $l_i$ and $u_i$ are the lower and upper bounds in the d-th dimension for the i-th orthotope. We can then obtain the probability of the union of all s orthotopes according to the inclusion-exclusion principle. For example, if there are no overlaps, we can simply sum $P(C_i)$'s for all $C_i$ and obtain\n$P_{safe} = \\sum_{i=1}^{s} P(C_i).$ (4)\nThis is correctly presented in Algorithm 1 from (Wicker et al., 2020).\nHowever, the code accompanying the paper implements this differently leading to large overestimates (i.e., incorrect values) of the $P_{safe}$ values as presented in (Wicker et al., 2020).\nThe implementation first computes the dimension-wise probability,\n$P(w_d) = \\frac{1}{2} (erf(\\frac{u_i - \\mu_d}{\\sigma_d \\sqrt{2}}) - erf(\\frac{l_i - \\mu_d}{\\sigma_d \\sqrt{2}})).$ (5)\nwhere $w_d$ is the d-th Bayesian parameter. The implementation then computes the product of $P(w_d)$ across the $n_w$ dimensions,\n$P_{safe} = \\prod_{d=1}^{n_w} P(w_d).$ (6)\nwhere $p_{safe}$ is marked with a hat to represent an invalid $p_{safe}$ value. The error comes from the fact that the operations in Equations (3) and (4) are noncommutative."}, {"title": "C Evaluation of timing for PIE algorithm", "content": "In Table 2 we examine the impact on runtime of generating the gradients required for the PIE algorithm. We observe that in each case there is an increase in runtime, peaking at 3.3% for the 1x128 network. The relative cost decreases as network size increases as a result of the gradient computation scaling better than the LBP computation with network size."}, {"title": "D Training Details", "content": "In addition to evaluating our approaches on the SoA benchmarks, we evaluated our approach on two fully connected BNNs and a convolutional BNN (only the fully connected layers are Bayesian). Similarly to the SoA, we used normal distributions for the prior (with mean 0 and standard deviation 1) and the posterior of the Bayesian layers. Using a wider standard deviation than the SoA, as expected, has resulted in networks that are more adversarially robust (cf. Table 1). We trained the networks using variation inference (Blundell et al.,"}, {"title": "E Comparison against Empirical Probabilistic Robustness", "content": "In Figure 5, we compare the pure sampling approach of (Wicker et al., 2020), as well as the PIE and GIE approaches, presented in this paper, against the empirical values we obtained for the probabilistic robustness of the networks in Table 1. We calculated the values for the empirical probabilistic robustness by sampling from the BNN posterior and using IBP to check whether classified correctly. Each value for the empirical accuracy shows the percentage of samples that resulted in correct classification.\nWe observe that PIE and GIE always perform favourably compared to pure sampling, and in some instances, such as for 1-layer networks, PIE and GIE significantly outperform pure sampling. Nevertheless, for most networks, there is still a gap between the certified robustness obtained by each of the approaches and the empirical probabilistic robustness."}]}