{"title": "Networked Agents in the Dark: Team Value Learning under Partial Observability", "authors": ["Guilherme S. Varela", "Alberto Sardinha", "Francisco S. Melo"], "abstract": "We propose a novel cooperative multi-agent reinforcement learning (MARL) approach for networked agents. In contrast to previous methods that rely on complete state information or joint observations, our agents must learn how to reach shared objectives under partial observability. During training, they collect individual rewards and approximate a team value function through local communication, resulting in cooperative behavior. To describe our problem, we introduce the networked dynamic partially observable Markov game framework, where agents communicate over a switching topology communication network. Our distributed method, DNA-MARL, uses a consensus mechanism for local communication and gradient descent for local computation. DNA-MARL increases the range of the possible applications of networked agents, being well-suited for real world domains that impose privacy and where the messages may not reach their recipients. We evaluate DNA-MARL across benchmark MARL scenarios. Our results highlight the superior performance of DNA-MARL over previous methods.", "sections": [{"title": "1 INTRODUCTION", "content": "Cooperative multi-agent reinforcement learning involves rational agents learning how to behave under uncertainty in a shared environment to maximize a single utility function. While distributed training has once been the dominating paradigm in learning for MARL [2] systems, the community's focus has shifted to centralized training and decentralized execution (CTDE) in recent years. The fundamental reason is that centralized training agents benefit from a either a single loss function to train a common policy, e.g., parameter sharing [10], or from agent-wise policies that can be factorized between agents, e.g., Q-MIX [27].\nCentralized training is ideal in settings where data is centralized. During training, agents benefit from sharing information, such as joint observations for partial observability mitigation, joint actions for modeling teammates and team rewards for cooperation. However, CTDE has its limitations. It assumes the existence of a central node (or entity) that actually trains the agents, knows all system information, performs all the necessary computations, and then distributes the resulting individual policies to agents for execution. Distributed training with decentralized execution has re-emerged, e.g, [38] and [4], as an alternative to CTDE systems in real world domains where there is no central entity capable of performing computations in behalf of the agents. For instance, in scenarios like distributed economic dispatch [34], where agents collaborate to determine optimal power generation, it is crucial to preserve the privacy of agents' observations-their power generation and cost curves. This privacy protection is essential for the fair bidding in the sale or purchase of energy. Hence agents collaborate to achieve a common goal, but they are less forthcoming about sharing their own observations. Another example of application is distributed packet routing in a dynamically changing networks [1]. Agents are nodes and by using local observations and collecting individual rewards, must balance the selection of routes that minimize the number of \"hops\" of a given packet, against the risk of overflowing links along popular routes. Finally, in wireless sensor networks [12] agents are endowed with limited processing and communication capacities, precluding exchanges of high precision (analog) data. Furthermore, randomness in the environment results in random packet dropouts.\nIn this work, we advance upon the decentralized training and decentralized execution (DTDE) [9] paradigm, wherein networked agents use peer-to-peer communication during training and operate in isolation during execution. Prior work has produced networked agents under relaxed assumptions: Zhang et al. [38] assumed a fully observable state while the rewards are kept private, and Chen et al. [4] proposed networked agents that choose when and to whom request observations. In contrast, we introduce a novel approach that is not bound by the same restrictions and our agents learn under partial observability. The key to our method is the use of a consensus mechanism to force agents to agree on a team value, resulting in cooperative value function learning under the partial observability setting.\nIn summary, our key contributions can be outlined as follows. First, we formalize the networked dynamic partially observable Markov game (ND-POMG), a specialized framework derived from partially observable Markov game where agents communicate over a switching topology network. Second, we present a novel approach, DNA-MARL, for solving ND-POMG problems with a team policy gradient. This approach is implemented in an actor-critic algorithm"}, {"title": "2 BACKGROUND", "content": "Partially observable Markov game (POMG): We define a partially observable Markov game [20] for N agents as the tuple:\n$(N, S, \\{A^i\\}_{i\\in N}, \\{O^i\\}_{i\\in N}, P, \\{r^i\\}_{i\\in N}, \\gamma)$,\nwhere $N = \\{1, ..., N\\}$ denotes a set of N agents. S represents the state space describing the system, which is not observed. Instead, at each time step t, each agent $i \\in N$ observes $o_t^i \\in O^i$, that depends on the true system state $s \\in S$. The set of actions available to agent i is denoted by $A^i$. The joint action set A is the Cartesian product of the individual action spaces, i.e., $A = A^1\\times\\dots\\times A^N$. The transition probability $P: S\\times A \\rightarrow \\triangle(S)$ denotes the probability distribution over the next state, they depend on the joint action $a \\in A$ and the current state s. The instantaneous individual reward for agent i is given by $r^i: S \\times A \\rightarrow \\mathbb{R}$; $y \\in [0, 1)$ is a discount factor.\nActor-critic: is a class of model-free reinforcement learning (RL) algorithms aimed at optimizing the policy $\\pi_{\\theta}$, parameterized by $\\theta \\in \\Theta$. Particularly, the actor component updates the policy $\\pi_{\\theta}$, and the critic component evaluates the actor's policy performance by using the Q-function:\n$Q(s, a; \\theta) = \\mathbb{E}_{\\pi_{\\theta}}[\\sum_{k=t}^{\\infty} \\gamma^{k-t}r_{k+1} | S_t = s, a_t = a ]$\nThe Q-function yields the expected discounted return by taking action a on state s at time t, and then following $\\pi_{\\theta}$ thereafter. In the single agent setting, the policy gradient theorem [33] prescribes the direction for the gradient updates to maximize the total discounted return:\n$\\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_{\\theta}log (\\pi_{\\theta}(a_t|s_t; \\theta))Q(s_t, a_t;\\theta)]$,\nActor-critic with advantage (A2C) [7], in the single agent episodic setting, the history of the interactions with the environment are collected into trajectories. A mini-batch is the concatenation of many trajectories, drawn from the same policy using parallel processing. A2C maintains one neural network for the actor, and one another for the critic, their weights are adjusted via gradient descent. The critic updates its parameters w by minimizing the least mean squares loss function:\n$\\mathcal{L}(\\omega; \\tau) = \\sum_{(s,a) \\in \\tau} ||A(s, a; \\omega)||^2$,\nwhere T is the length of an episode, $A(s, a; \\omega) = Q(s, a; \\omega) - V(s; \\omega)$ is the advantage function, and the value function\n$V(s; \\omega) = \\mathbb{E}_{\\pi_{\\theta}}[\\sum_{k=t}^{T}\\gamma^{k-t}r_{k+1} | S_t = s; \\omega ]$\ncaptures the discounted return for being on state s at time t, and then following $\\pi_{\\theta}$ thereafter until the episode's end at T. The advantage function reduces the variance of the actor-critic gradient updates.\nThe actor updates its parameters $\\theta$ by minimizing the loss function:\n$\\mathcal{L}(\\theta; \\tau) = \\sum_{(s,a) \\in \\tau} log(\\pi(a|s; \\theta))A(s, a; \\omega)$.\nConsensus: The goal of randomized consensus algorithms is to asymptotically reach an agreement on the global average of individual parameters held by nodes in a switching topology communication network through local communication. Formally, the switching topology communication network is defined by an undirected graph $G_k(N, \\mathcal{E}_k)$, where $N = \\{1, . . ., N\\}$ is the node set, and $\\mathcal{E}_k \\subseteq N \\times N$ denotes the time-varying edge set with respect to communication step k. Nodes n and m can communicate at communication step k, if and only if, $(n, m) \\in \\mathcal{E}_k$. Each node n, initially holding a parameter $\\phi_t^n(0)$, has the opportunity at each communication step k, to synchronously interact with its neighbors, updating its parameter value by replacing its own parameter with the average of its parameter and the parameters from neighbors. The distributed averaging consensus algorithm [36] prescribes the updates:\n$\\phi_t^n (k + 1) = \\sum_{m\\in N} w_{n,m}^k \\phi_t^m(k)$,\nwhere $N = \\{m|(n,m) \\in \\mathcal{E}_k\\}$ represents the neighborhood of agent n at time k. For a switching topology dynamic with random link dropouts, it is possible to show that in the limit, the values of the parameters for each node n converge to the network's average, i.e.:\n$\\lim_{k\\rightarrow\\infty} \\phi_t^n (k) = \\frac{1}{N} \\sum_{i=1}^{N} \\phi_t^i(0)$.\nMoreover, for an arbitrary graph $G_k(N, \\mathcal{E}_k)$, it is possible to derive the weights $w_{n,m}^k$ that guarantee consensus locally. For instance, the Metropolis weights matrix [36] in (Appendix A) is a matrix that guarantee consensus, requiring only that each node be aware of its closest neighbor degree.\nNetworked agents is a class of distributed reinforcement learning agents that combines consensus iterations in (4) for localized approximations and actor-critic updates in (3) and (2). Relevant previous works include:\nCritic consensus: Zhang et al. [38] introduce networked agents where the critic network $V (\\cdot, \\cdot; \\omega)$, parameterized with w, approximates the value-function $V^{\\pi} (\\cdot)$. The distributed critic emulates a central critic. Agents observe the transition $(s_t, a_t, s_{t+1})$, perform critic the update in (2), then agents average the parameter using consensus:\n$\\omega^i (k + 1) = \\sum_{j \\in N_i} w_{ij}^k \\omega^j (k) \\forall i\\in N$.\nPolicy consensus: Chen et al. [4] introduce the class of homogeneous Markov games wherein there is no suboptimality incurred by performing consensus on the actor parameters. Their motivation is to emulate parameter sharing under the decentralized setting, while minimizing the number of communication rounds. Agents perform the actor update in (3), then average the parameters using consensus:\n$\\theta^i (k + 1) = \\sum_{j \\in N_i} w_{ij}^k \\theta^j (k) \\forall i\\in N$."}, {"title": "3 NETWORKED DYNAMIC POMG", "content": "In this section, we present the first key contribution, which is a formalization of networked dynamic partially observable Markov game, ND-POMG. We define the ND-POMG as the septuple:\n$\\mathcal{M} = (G_k, S, \\{O^i\\}_{i\\in N}, \\{A^i\\}_{i\\in N}, P, \\{r^i\\}_{i\\in N}, \\gamma)$,\nwhere $G_k (N, \\mathcal{E}_k)$ represents a switching topology communication network, and the latter six elements represent the POMG elements.\nIn this work, we fix the agents set $N = |N|$, to ensure that no agent is added or removed from the network. We also introduce the hyperparameter $C = |\\mathcal{E}_k|$ for all k, that shapes the topology of the communication network by fixing the cardinality of every possible edge set $\\mathcal{E}_k$. Moreover, we let $\\mathcal{E}_k$ change according to an uniform distribution at each communication round. The uniform distribution over the edge sets is the least specific distribution that guarantees that over a sufficiently long round of communications agents will reach consensus (Appendix B.1)."}, {"title": "4 DOUBLE NETWORKED AVERAGING MARL", "content": "This section presents our second key contribution which is the DNA-MARL an approach to solve ND-POMG problems. Since our method requires an extra consensus iteration step, we call it double networked averaging MARL (DNA-MARL). Any single agent reinforcement learning algorithm can be cast as a DNA-MARL with our method, we elaborate the case for the A2C, an on-policy method (Sec. 4.1) and extend to the deep Q-network (DQN) (Sec. 4.2), an off-policy method."}, {"title": "4.1 Double Networked Averaging A2C", "content": "In order to make agents cooperate with decentralized training, we factorize the shared objective between agents. Hence, it is possible to maximize performance via local communication and local gradient descent updates.\nThe total discounted team return, $J(\\pi_{\\theta})$, serves as a measure of the joint policy $\\pi_{\\theta}$ performance:\n$J(\\theta) = \\mathbb{E}_{s_0 \\sim \\mu(\\cdot)}\\mathbb{E}_{a\\sim\\pi_{\\theta}(s)} [\\sum_{t=0}^{T} r_{t+1}]$,\nwhere $r_{t+1} = \\frac{1}{N} \\sum_{i\\in N} r_{t+1}^i$ is the instantaneous team reward. The expectation is taken by drawing the initial state $s_0$ from the initial state distribution $\\mu$ and taking actions from $\\pi_{\\theta}$, thereafter. For simplicity, we follow the convention of writing $J(\\pi_{\\theta})$ as $J(\\theta)$.\n4.1.1 Team Policy Gradient. To obtain the team policy gradient, we replace the team reward in (4.1) by the average of individual rewards.\n$\\begin{aligned} J(\\theta) & = \\mathbb{E}_{\\substack{s_0 \\sim \\mu(\\cdot)\\\\ a\\sim\\pi_{\\theta}(\\cdot|s)}} [\\sum_{t=0}^{T} \\frac{1}{N} \\sum_{i\\in N} r_{t+1}^i] \\\\ & = \\sum_{i\\in N}\\frac{1}{N}\\mathbb{E}_{\\substack{s_0 \\sim \\mu(\\cdot)\\\\ a\\sim\\pi_{\\theta}(\\cdot|s)}}[\\sum_{t=0}^{T} r_{t+1}^i] \\\\ & = \\sum_{i\\in N}\\frac{1}{N}J^i(\\theta) \\end{aligned}$\nThe result above suggests how the cooperative system's objective can be distributed across the participating agents, thus the total discounted team return is computed as the weighted sum of the discounted individual rewards. However, the behaviors of the agents are still coupled, depending on the joint policy parameterized by $\\theta$, and on the common system state $s_t$. Formally, the objective of the cooperative distributed system is to maximize the total discounted team return:\n$\\max_{\\theta} \\sum_{i\\in N} J^i (\\theta) \\text{ with } J^i(\\theta) = \\mathbb{E}_{\\tau \\sim P(\\cdot|\\theta)} [\\sum_{t=0}^{T} \\gamma^{t} r_{t+1}^i]$,\nWe drop the scaling constant N as it does not change the stationary points of the maximization. $\\mathbb{P}(\\tau|\\theta)$ is the short hand notation for the probability distribution of the trajectories,\n$\\tau = (s_0, a_0, s_1, s_1, a_1, r_2, . . ., s_T)$,\ngenerated from system dynamics P under the joint policy $\\pi_{\\theta}(s)$. The maximization can be achieved through iterative gradient search methods. More specifically, the policy gradient theorem (1) prescribes the direction of the parameter updates:\n$\\nabla_{\\theta}J(\\theta) = \\mathbb{E}_{\\tau \\sim P(\\cdot|\\theta)} [\\nabla_{\\theta}log (\\pi_{\\theta}(a|s))A_{\\theta}(s, a)]$,\nthat maximize the total discounted team return. We replaced the Q-function by the advantage function (2) to mitigate the variance on the weight updates. We note that the single agent policy gradient update in (1) serves as the policy gradient for a centralized agent in control of all agents. Departing from the centralized setting, we use the fact that the joint policy $\\pi_{\\theta}$ factorizes between agents, to set:\n$\\begin{aligned} \\nabla_{\\theta}J(\\theta) & = \\mathbb{E}_{\\tau \\sim P(\\cdot|\\theta)} [\\nabla_{\\theta}log (\\prod_{i\\in N}\\pi_{\\theta^i}(a^i|s))A_{\\theta}(s, a)] \\\\ & = \\mathbb{E}_{\\tau \\sim P(\\cdot|\\theta)} [\\sum_{i \\in N} \\nabla_{\\theta^i}log (\\pi_{\\theta^i}(a^i|s))A_{\\theta}(s, a)] \\\\ & = \\sum_{i \\in N}\\mathbb{E}_{\\tau \\sim P(\\cdot|\\theta)} [\\nabla_{\\theta^i}log (\\pi_{\\theta^i}(a^i|s))A_{\\theta}(s, a)]. \\end{aligned}$\nThere are two limitations in (10) preventing its use for conducting local updates. In the context of partial observability the states s are unavailable in trajectory $\\tau$. Second, the gradient update depends on global $\\theta \\in \\Theta$, and no agent has access to $\\theta$.\n4.1.2 Distributed Reinforcement Learning. We address the limitations in (10) by considering the information structure of the problem, or what do agents know [37]. We propose localized approximations that allow agents to perform local updates: considering a synchronous system where agents interact with the environment to collect their individual trajectories $\\tau^i$. Distributed learning requires a localized approximation $\\nabla_{\\theta^i}J^i(\\theta)$ for gradient of the discounted team return $\\nabla_{\\theta}J(\\theta)$ in (10). Moreover, each agent maximizes its policy $\\pi_{\\theta}$ which is parameterized by $\\theta^i$, i.e., $\\pi_{\\theta} = \\pi_{\\theta^i}$ and $\\nabla_{\\theta}J^i(\\theta) = \\nabla_{\\theta^i}J^i(\\theta)$. Combining the three facts together the localized approximation for (10) can be rewritten as:\n$\\nabla_{\\theta^i} J^i(\\theta) = \\mathbb{E}_{\\tau^i \\sim P(\\cdot|\\theta)} [\\nabla_{\\theta^i}log (\\pi_{\\theta^i}(a^i | o^i)) A_{\\theta}(o, a)]$,\nwhere $\\tau^i = (o_0^i, a_0^i, r_1^i, o_1^i, a_1^i, r_2^i, . . ., o_T^i)$ is available locally for agent i. The replacement $s_t$ with $o_t^i$ under the partially observability setting is standard practice in MARL literature [4, 27, 31]. The system's dynamics still depend on the joint behavior, parameterized by $\\theta$, but the gradient in (11) is locally defined. Straightforward application of the actor-critic updates in (3) and (2), with individual"}, {"title": "4.1.3 Distributed Cooperation", "content": "We propose a better approximation for the gradient of the discounted team return by performing the updates in the direction of the team advantage $A_{\\theta}(s, a)$ in (10), rather than the local advantage $A_{\\theta}(o^i, a^i)$ in (11). However, since the team advantage is unavailable, agents should instead perform local updates in the direction of the team advantage under the partially observable setting $A_{\\theta}(o, a)$. Since $o = [o_0^1, . . ., o_0^N]$ is the concatenation of observations, $A_{\\theta}(o, a)$ can be defined by:\n$A_{\\theta}(o, a) = Q(o, a; \\omega) - V(o; \\omega) \\approx r + \\gamma V(o'; \\omega^-) - V(o; \\omega)$,\nwhere, r and o' are respectively the rewards and the joint observations on the next time step. The parameter $\\omega^-$ is a periodic copy of the critic's parameters $\\omega \\in \\Omega$, which serves to stabilize learning. Decentralized learning agents neither observe o nor collect r, but may resort to local communication schemes to obtain factorized representations for $V (o, a; \\omega)$. The local critic update is a straightforward adaptation of the single agent critic in (2):\n$\\mathcal{L}(\\omega^i; \\tau^i) = \\frac{1}{T} \\sum_{t=0}^{T-1} (\\tilde{y} - V(o_t^i; \\omega^i))^2$,\nwith\n$\\tilde{y} = r_{t+1}^i + \\gamma V (o_{t+1}; \\omega^-)$.\nWe propose to use communication to combine $\\tilde{y}$ by performing team-V consensus:\n$\\tilde{y}(k + 1) = \\sum_{j\\in N} w_{ij}^k \\tilde{y}(k) \\forall i \\in N, k = 1, . . ., K$.\nAfter each training episode, agents concurrently approximate the team-V using $\\tilde{y}$ based on their individual rewards and observations. Then, we let K consensus updates per mini-batch aimed at approximating the team-V. At each communication round, a connected agent averages its team-V estimation with team-Vs from neighbors. Ideally, the following approximation will hold:\n$\\tilde{V}(o; \\omega) = \\sum_{i=1}^{N}[r_{t+1}^i + \\gamma V (o_{t+1}; \\omega)] = V(\\tilde{o}; \\omega)$.\nWe empirically test for suitable values of K. The consensus steps in (14) result in a flexible degree of cooperation: When K = 0, agents behave as independent learning agents. For a high enough values of K, the approximation error should be small enough, such that agents behave in fully cooperative mode.\nThis section concludes a core contribution to our method: distributed cooperation whereby agents produce localized approximations for a team-V (or team-Q) using consensus. Cooperation requires that each agent approximates the same critic, and this critic must evaluate the joint policy, so that the actor updates its parameters in the direction of the team-V, thus the best local actions for the team will be reinforced. Moreover, the updates in (15) do not require agents to be homogeneous. Previous networked agents works [4, 38] provide asymptotic convergence guarantees for linear function approximation on the state-action space. Under the linearity approximation on the critic and fully observable setting the update in (6) is sufficient to guarantee cooperation. Under partial"}, {"title": "4.1.4 Algorithm", "content": "To design our algorithm, we combine the localized approximations for the team-V in (15) with critic consensus in (6) [38]. And actor consensus in (7) [4] for improving sample efficiency. Hence, the updates comprising the double networked averaging actor critic with advantage are given by:\n$\\begin{aligned} \\tilde{y}(k + 1) & = \\sum_{j\\in N} w_{ij}^k \\tilde{y}^j(k) k = 1,..., K \\\\ \\tilde{y} & = \\tilde{y}(K + 1) \\end{aligned}$\nEvery agent interacts locally with the environment to collect the individual trajectories $\\tau^i$. Then, in (i) agents use localized approximation for team-V, $\\tilde{y}$, by performing K consensus steps; (ii) the final approximation for the team-V is defined; The next steps consist of local weight updates:\n$\\mathcal{L}(\\omega^i; \\tau^i, \\tilde{y}) = \\frac{1}{T} \\sum_{t=0}^{T-1} (\\tilde{y} - V(o_t^i; \\omega^i))^2$\n$\\mathcal{L}(\\theta^i; \\tau^i, \\tilde{y}) = - \\frac{1}{T} \\sum_{t=0}^{T-1} log\\pi(a^i|o_t^i; \\theta^i) (\\tilde{y} - V(o_t^i; \\omega^i))$\nIn (iii) the local critic updates its parameters using $\\tilde{y}$ instead of their own estimations $y_t^i$; (iv) Similarly, actor updates its parameters in the direction of team-V; Finally, periodically agents perform actor and critic parameter consensus, represented by steps (v) and (vi):\n$\\begin{aligned} \\omega^i (k + 1) & = \\sum_{j\\in N} w_{ij}^k \\omega^j (k) k = 1,..., K \\\\ \\theta^i (k + 1) & = \\sum_{j\\in N} w_{ij}^k \\theta^j (k) k = 1,..., K \\end{aligned}$\nWe note that actor-critic parameters can be concatenated to avoid extra communication rounds, and that (v) utilizes (6) and that (vi) utilizes (7). And the Listing 1 in (Appendix B.2) provides the pseudocode. The consensus updates in (v) require agents to have homogeneous observation spaces, while updates in (vi) require agents to have homogeneous action spaces."}, {"title": "5 EXPERIMENTS", "content": "We evaluate the performance of DNA-MARL following the methodology outlined by Papoudakis et al. [24] for benchmarking multi-agent deep reinforcement learning algorithms in cooperative tasks. This section presents the scenarios, baselines, and evaluation metrics.\n5.1 Scenarios\nMulti-agent environments are typically designed for the cooperative setting [22], but they can also be configured for the mixed setting. In the mixed setting, individual rewards are emitted, and the team reward is obtained by averaging all individual rewards. With minor adaptations which we outline briefly, the multi-agent particle environment (MPE) [18] scenarios were adjusted for partial observability and individual rewards. The scenarios include:\nAdversary\u00b3: The first MPE adaptation has two teammates protecting a target landmark from a third adversary agent. Teammates are rewarded the adversary's distance from the target and penalized with their negative distance to the landmark. The teammates observations include the position and color from the closest agent (either adversary or teammate), their relative distance to landmark, and the position of the two landmarks.\nSpread\u00b3: The second MPE adaptation has three agents that must navigate to three landmarks while incurring a penalty for collisions. We adapt the observation and reward for the partially observable and decentralized setting. Each agent's observation contains its own absolute location, the relative locations of the nearest agent, and the relative location of the nearest landmark. The reward is the negative distance of the agent to the closest landmark.\nTag\u00b3: The third MPE adaptation has three big predators (agents) that rewarded for catching a smaller and faster fleeing agent that follows a pre-trained policy. Additionally, two landmarks are placed as obstacles. Agents navigate a two-dimensional grid with continuous coordinates. The reward is sparse, and we adapt the environment for partial observability and decentralization. Each agent's observation includes its own position and velocity, the closest predator's position, and the prey's position and velocity. The reward is individual where the agent that catches the prey is the one receiving a reward of ten points.\nLevel-Based Foraging (LBF) [24]4: In this scenario, agents can move on a two-dimensional discrete position grid and collect fruits. Since both agents and fruits have associated levels, successful fruit loading occurs only if the total level of the agents attempting to load it exceeds the fruit's level. Observations consist of relative positions of agents and fruits within a two-block radius centered around the agent. The rewards are sparse, and only the agents that successfully load a fruit receive positive reward. We configure three instances in the partially observable setting, in increasing levels of difficulty: (i) Easy: 10 x 10 grid, 3 players, and 3 fruits (ii) Medium: 15 x 15 grid, 4 players, and 5 fruits (iii) Hard: 15 x 15 grid, 3 players, and 5 fruits.\n5.2 Baselines\nFollowing Papoudakis et al. [24], we divide our experiments into the on-policy and off-policy settings. Furthermore, for each scenario, we compare three different approaches: (i) individual learners (IL), (ii) decentralized training and fully decentralized execution (DTDE) and (iii) the centralized training and decentralized execution (CTDE) algorithms. ILs are always self interested and CTDE are always fully cooperative as they have access to the team reward. For the on-policy setting, the baselines include:"}, {"title": "5.3 Evaluation", "content": "In this section we establish the performance metric, the deviation metric and the hypothesis test to discriminate results.\nPerformance metrics: We evaluate the performance of the algorithms using the maximum average episodic returns [24] criteria."}, {"title": "6 RESULTS", "content": "Figure 2 presents a comparative analysis of DNA-MARL's performance for the on-policy and off-policy settings for two selected tasks from the LBF and MPE environments. The CTDE algorithms serve as an upper benchmark for other methods, while the IL algorithms establish a lower performance boundary. Notably, in three specific algorithm-task pairings - (a), (b), and (c) \u2013 DNA-MARL demonstrates superior results compared to its nearest competitors when utilizing decentralized training combined with decentralized execution strategies. These results indicate that our double networked averaging A2C (DNAA2C), an algorithm that learns using local observations, can indeed emulate a central critic that uses system-wide observations. In spite of having information loss due to randomized communication. For the off-policy setting, in the\n6.1 Ablations\nTo assess the impact of consensus steps within the DNA-MARL framework, we conducted ablations regarding the discounted return consensus outlined in (14), specifically focusing on both the critic parameters and actor parameters.\nFigure 3 presents the ablation results for DNAA2C in the tasks LBF Hard and MPE Tag. The bars represent the averages within a"}, {"title": "7 RELATED WORK", "content": "We relate our work with five other lines of research, two of which we present herein: the centralized training and decentralized execution under partial observability setting and networked agents in the decentralized training and fully decentralized execution (DTDE). Due to space restrictions, we further discuss related works in Appendix E.\nCentral Training and Decentralized Execution: CTDE is the prevailing approach in multi-agent reinforcement learning, where a central critic learns a system action-value function to mitigate the risk of non-stationarity. The policies are factorized and executed by individual actors, utilizing only local information to address the large state space problem. Examples from works that learn a central critic include MADDPG [18], COMA [8] and PIC [17]. Furthermore, actors benefit from parameter sharing as proposed by [10], wherein agents use a single neural network to approximate a policy trained with experiences collected from the behavior policies of all agents. Parameter sharing reduces wall clock time and increases sample-efficiency, enabling faster agent learning [11]. Another possibility is building utility functions that factorize into agent-wise function. Sunehag et al. [31] propose value decomposition networks, where the team-Q function is recovered by adding the agent-wise Q-functions. Finally, QMIX [27] extend VDN by proposing a mixing network, that has a dynamic set of parameters that vary according to the system state. The mixing network produces more expressive team-Q function decomposition, requiring that the joint action that minimizes the team Q-value be the same as the combination of the individual actions maximizing the agent-wise Q-values.\nNetworked agents with multi-agent reinforcement learning. Zhang et al. [38] is DTDE MARL system that apply the consensus mechanism over the critic's parameters to obtain a joint policy evaluation. However, their system requires full observability of both state and action spaces. In contrast Zhang and Zavlanos [39] propose DTDE MARL system that performs consensus on the actor's parameters while the critics are individual. As a limitation the policies must represent the joint action space. Chen et al. [4] apply networked"}, {"title": "A EXTENDED BACKGROUND", "content": "Consider a graph G(N", "cardinality": "d(n) = |$N_n$|.\nLet G(N", "where": "n$W(n", "36": ".", "a_{n,m}": 6, "by": "nB EXTENDED DOUBLE NETWORKED AVERAGING\nThis section is divided into two parts. The first part formalizes the graph model. The second part provides the pseudocode for the double networked agents.\nB.1 Graph Model\nLet Gk represent a switching topology communication network"}]}