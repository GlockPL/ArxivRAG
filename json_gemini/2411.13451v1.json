{"title": "AdaptAgent: Adapting Multimodal Web Agents with Few-Shot Learning from Human Demonstrations", "authors": ["Gaurav Verma", "Rachneet Kaur", "Nishan Srishankar", "Zhen Zeng", "Tucker Balch", "Manuela Veloso"], "abstract": "State-of-the-art multimodal web agents, powered by Multimodal Large Language Models (MLLMs), can autonomously execute many web tasks by processing user instructions and interacting with graphical user interfaces (GUIs). Current strategies for building web agents rely on (i) the generalizability of underlying MLLMs and their steerability via prompting, and (ii) large-scale fine-tuning of MLLMs on web-related tasks. However, web agents still struggle to automate tasks on unseen websites and domains, limiting their applicability to enterprise-specific and proprietary platforms. Beyond generalization from large-scale pre-training and fine-tuning, we propose building agents for few-shot adaptability using human demonstrations. We introduce the AdaptAgent framework that enables both proprietary and open-weights multimodal web agents to adapt to new websites and domains using few human demonstrations (up to 2). Our experiments on two popular benchmarks Mind2Web & VisualWebArena show that using in-context demonstrations (for proprietary models) or meta-adaptation demonstrations (for meta-learned open-weights models) boosts task success rate by 3.36% to 7.21% over non-adapted state-of-the-art models, corresponding to a relative increase of 21.03% to 65.75%. Furthermore, our additional analyses (a) show the effectiveness of multimodal demonstrations over text-only ones, (b) shed light on the influence of different data selection strategies during meta-learning on the generalization of the agent, and (c) demonstrate the effect of number of few-shot examples on the web agent's success rate. Overall, our results unlock a complementary axis for developing widely applicable multimodal web agents beyond large-scale pre-training and fine-tuning, emphasizing few-shot adaptability.", "sections": [{"title": "Introduction", "content": "Agents automating web-based tasks with minimal human intervention can significantly boost personal and workplace productivity [35, 36]. A prevalent interaction mechanism involves a human providing a natural language instruction (e.g., \"use delta.com to book a flight from JFK to Haneda on ... \"), and the agent autonomously executing the necessary webpage actions to complete the user-assigned task [62, 14, 22]. Large language models (LLMs) can understand instructions, plan, and predict structured outputs, serving as backbones for such agents [55]. Remarkable progress has been made in automating web-based tasks using LLM-based agents [30, 12, 20], employing careful prompting [62, 28] and extensive pre-training and fine-tuning [14] to predict actions using language instructions and HTML/DOM. With multimodal capabilities, these agents now process the graphical user interface's (GUI's) visual state to complement the HTML/DOM information [22]. In parallel with the methodological advancements, evaluating the generalizability of these multimodal web agents to new tasks, websites, and domains is a critical component to ensure their broad applicability."}, {"title": "Related Work", "content": "We categorize the related prior work along three dimensions: work on UI/Web agents, few-shot learning approaches with LLMs, and approaches to learn from demonstrations. \nUI/Web Agents: Controlling digital devices using AI and natural language input has been a long- standing goal [52, 23]. Before large language models (LLMs), approaches often used reinforcement learning on top of models like LSTM and BERT for language processing, combined with ResNet-like models for GUI state understanding [31, 25]. With the advent of multimodal LLMs, recent work has leveraged these models to build web agents that process user instructions and reason to generate actions on user interfaces [62, 20].\nMost state-of-the-art methods use pretrained LLMs, such as GPT-4, to build multimodal web agents. They provide the LLM with context like images of the GUI, prior actions, image annotations, and HTML/DOM information when available. Some works, like Pix2Act [51] and WebAgent [19], train LLMs to attend to parts of HTML code or generate the next action step through self-supervision, often using reinforcement learning techniques like behavioral cloning or REINFORCE. However, these approaches typically require large amounts of training data and resources, and are often limited to simpler environments [30]. They may not scale well to complex proprietary enterprise software, and agents requiring exploration during training may need human supervision to avoid risky outcomes. Methods that aim to make agents more adaptable to unseen settings, which is the focus of this work,"}, {"title": "Few-Shot Adaptation with Human Demonstrations", "content": "Methodological motivation. Learning from human demonstrations [46] has played a key role in many applications, notably helping robots generalize to new tasks or existing tasks under new environments and constraints [3]. Prior work has highlighted the limited generalizability of web agents to unseen tasks, websites, and domains [62, 22]. Agents that automate web tasks and robots that automate real- world tasks share strong analogies in desired capabilities (i.e., perception, reasoning, execution [55]), allowing for transfer of modeling strategies between these domains. This inspires us to adopt learning from human demonstrations for web agents to improve their adaptability to unseen settings. While it's possible to fine-tune web agents with a large number of human demonstrations covering new websites and domains, such approaches require tedious annotations and are expensive. Therefore, building highly adaptable web agents requires the ability to adapt them in a data-efficient manner.\nDespite the success of learning from demonstration in adapting robots and the strong analogies between physical robots and web agents, unique challenges remain for web agents. Traditionally, robot learning from human demonstrations exhibits limited generalizability; i.e., when a human demonstrates task A a few times, the robot learns to do the same task A or closely related tasks, akin to imitation learning [24, 43]. It remains to be seen how well web agents can generalize to unseen settings with few-shot human demonstrations, which is the primary focus of this work. In other words, can a handful of human demonstrations of specific tasks on certain websites (e.g., \"book a flight...\" on delta.com) lead the web agent to learn related tasks on similar websites (e.g., \"check visa requirements...\" on united.com), or even generalize to unrelated domains (e.g., \u201cbook a driving test appointment...\" on dol.wa.gov)? Our work proposes AdaptAgent, a framework to enable web agents to adapt with few-shot human demonstrations and evaluates their generalizability to unseen settings.\nMethods for learning with human demonstrations. Our proposed framework for adapting mul- timodal web agents with few-shot human demonstrations builds on advances in proprietary and open-weights multimodal LLMs. We use SeeAct [62], which employs a carefully crafted prompting strategy with GPT-40, as a representative proprietary model baseline and adapt it using multimodal in-context examples. As the representative baseline for SoTA open-weights models, we use CogA- gent [22] - an 18B multimodal LLM with a dedicated visual backbone to process GUI images. Given the success of meta-learning in efficient adaptation, we propose fine-tuning models like CogAgent with meta-learning instead of regular fine-tuning.  Next, we elaborate on the methodological details for in-context learning and meta-learning with human demonstrations for proprietary and open-weights models, respectively.\nIn-context learning with SeeAct + GPT-4o: SeeAct uses a carefully constructed prompt (using ReAct prompting [59]) to guide multimodal LLMs like GPT-40 in iteratively determining the next action based on the current GUI state to complete the user-assigned task. In-context learning (ICL) has proven to be an effective approach for adapting proprietary LLMs [4]. Consequently, we deconstruct the human demonstration of a task on the target website/domain into a sequence of (visual snapshot, HTML elements (filtered following [62]), human selection) and include them as an ICL example with the original SeeAct prompt; see Appendix A.6 and Figure 1 (left).\nMeta-learning with CogAgent: To overcome the limited abilities of general-purpose multimodal LLMs to process GUI snapshots \u2014 which involve complex layout understanding, OCR, and functional understanding of HTML elements, Hong et al. (2023) [22] pre-trained general-purpose MLLMs like CogVLM [56] on tasks involving GUI processing. Beyond extensive pre-training, fine-tuning on task-specific datasets showed notable performance boosts for CogAgent over several baselines. In this work, we consider the pre-trained CogAgent and further adapt it using model-agnostic meta-learning (MAML) [16] with few-shot human demonstrations; refer to Figure 1 (right) for a visual depiction.\nMeta-learning [48], often dubbed \u201clearning to learn\u201d, is a training strategy in which a model learns to adapt efficiently to unseen tasks by leveraging knowledge gained from updates across many related tasks. Model-agnostic meta-learning [16] is one such approach applicable to any model. Mathematically, the meta-learned model 9* is obtained via meta-updates $\\theta \\leftarrow \\theta - \\beta.\\nabla_{\\theta} \\Sigma_{T \\epsilon T} L_{T}(\\theta_{t})$ (outer loop update), where \u1e9e is the meta-learning step size, and the gradient is derived from the sum of losses $L_{T}(\\theta)$ across all tasks. Each \u03b8\u2081 is initialized from @ and fine-tuned on task Ti, via $\\theta_{t} \\leftarrow \\theta - \\alpha.\\nabla_{\\theta}L_{T_{i}}(\\theta)$ (inner loop update), with a being the step size. Thus, each meta-update involves meta-gradients (gradients through gradients). However, since our experiments involve LLMs with billions of parameters, computing meta-gradients is computationally challenging. Therefore, we consider the first-order approximation of model-agnostic meta-learning (FOMAML). FOMAML has demonstrated performance on par with MAML [16, 34], potentially due to the predominantly locally linear nature of neural networks [18, 42], making the second-order gradients negligible. Therefore, our meta-learning updates are represented as (derivation in Appendix A.2): $\\theta \\leftarrow \\theta - \\beta. \\Sigma_{i=1}^{N} \\nabla_{\\theta}L_{T_{i}}(\\theta_{i})$.\nIn other words, when adapting multimodal web agents with meta-learning, the inner loop involves fine-tuning the agent (0 \u2192 0\u2082) on web tasks Ti from a given website, with the training subset used for this inner loop denoted as Dtrain. Then, for the outer loop update, we update the parameters of the MLLM agent @ by backpropagating the gradients of the loss at \u03b8i, where the loss is computed on held-out web tasks from the same website/domain denoted as Diest. Importantly, the gradients being backpropagated are computed at 0\u2081 (rather than 0), ensuring the MLLM agent is not trained on both Dtrain & Dtest. Essentially, we train the MLLM agent 0 on Dtrain to obtain 0; and then update its original parameters 0 using penalties based on how well 0\u2081 performs on held-out D\u021best. A better"}, {"title": "Experimental Protocol and Details", "content": "Datasets: To evaluate the quick adaptation capabilities of our agents, we design experiments that require adaptation to unseen websites and domains. We consider two widely used benchmarks: Mind2Web [14] and VisualWebArena [28]. Mind2Web provides standardized train and test sets- across various websites and domains. The train set includes 1,009 tasks from 73 websites and 3 domains, while the test set is categorized into cross-task (174 tasks from 64 seen websites), cross- website (142 tasks from 10 unseen websites), and cross-domain (694 tasks from 2 unseen domains) subsets to evaluate different aspects of generalization. Since the cross-task evaluation set overlaps with the train set, we propose minor amendments to ensure proper evaluation of adaptability (details in Appendix A.3). VisualWebArena simulates a live environment with three different websites (Reddit, Classifieds, and Shopping) to evaluate task success rates of web agents. We use the entire VisualWebArena benchmark (910 tasks) to test the adaptability of our web agent to unseen websites. While some tasks have step-level ground truth, others provide only an overall task success signal based on the environment's state. More details about the datasets are presented in Appendix A.3.\nExperimental Protocol: Our experimental protocol for developing and evaluating the adaptability of web agents varies based on whether the underlying multimodal LLM is proprietary or open-weights. For the proprietary model (i.e., GPT-40), we use the prompting method proposed in SeeAct and add one ICL example from the website or domain to which the agent should adapt. This ICL example acts as the one-shot (n = 1) human demonstration (denoted as 1-ICMD for 1 in-context multimodal demonstration). We adopted a one-shot setting for ICL given the trade-off between time and incremental accuracy improvements; see Appendix 5. The selection of the ICL example ensures relevance to the cross-task, cross-website, and cross-domain evaluation setups. Specifically, for Mind2Web's cross-task and cross-website evaluation, we randomly sample one task from the same website (for cross-task) or from each unique website (for cross-website) in the test set and evaluate on the remaining examples from that website, maintaining website-level correspondence. For cross-domain evaluation, we randomly sample one task from each unique domain in the cross-domain test set and evaluate on the remaining examples from that domain. For VisualWebArena evaluation, we randomly choose one task as the in-context demonstration from the website being evaluated. For the open-weights model (i.e., CogAgent), during meta-learning, we sample 4 tasks per website from the 73 websites in the Mind2Web training set: 2 tasks for adaptation (Dtrain) and 2 tasks (Dtest) (1 from the same website and 1 from a different website within the same domain) for computing the adaptation loss and updating the agent's parameters as discussed in Section 3. After meta-learning, the meta-trained model adapts to new websites in the cross-website test set by fine-tuning on 2 tasks from each website and then evaluating on the remaining tasks. For cross-domain evaluation, we adapt on 2 tasks from each new domain and evaluate on the rest within that domain; see Figure 3 in the Appendix. We do not perform website adaptation for the cross-task test set, as all websites are seen during meta-learning. For VisualWebArena, we adapt the meta-trained model on the Mind2Web training set, using 2 tasks from each of the 3 websites and evaluate on the remaining tasks. To control for the effect of adaptation tasks, we report average results across 5 independent runs with different task selections. Overall, our approach involves meta-training the model with 292 tasks from Mind2Web (73 websites \u00d7 4 tasks) and adapting with 2 demonstrations to new websites/domains. Implementation details are available in Appendix A.4. We denote our meta-learned and adapted agent as CogAgent-FOMAML.\nWe compare the performance of adapted agents with existing SoTA agents as baselines. For the proprietary model, zero-shot SeeAct + GPT-40 serves as a baseline. We also include Set-of-Mark prompting (SoM) [58, 28] in the image input, giving us a slightly augmented baseline that we denote as SeeAct*. For the open-weights model, we consider the pre-trained CogAgent and another variant-CogAgent-FT\u2014that uses conventional fine-tuning on the entire train set of Mind2Web as baselines. Additionally, we consider CogAgent-FT (DE) as another baseline that maintains data equivalence (DE) with the proposed CogAgent-FOMAML method by using the same training subset for conventional fine-tuning. CogAgent-FOMAML and CogAgent-FT (DE) use 292 examples during meta-learning and fine-tuning, respectively, while CogAgent-FT uses ~3.4\u00d7 as many examples."}, {"title": "Results", "content": "Few-shot human demonstrations unlock complementary gains in agent's performance:  compares the baseline and few-shot adapted versions of proprietary (SeeAct, SeeAct*) and open-weights (CogAgent) models on (a) the Mind2Web dataset across cross-task, cross-website, and cross-domain evaluation settings, and (b) the VisualWebArena dataset across human trajectories and live environment settings. The proprietary models adapt through multimodal in-context demonstration, while CogAgent adapts via meta-learning. Recall that for CogAgent, we tested two baseline versions: one fine-tuned on the entire Mind2Web train set and another to ensure date-equivalence with CogAgent-FOMAML.\nWe observe that few-shot adaptation improved the performance of both proprietary and open-weights models across the two datasets and all settings involving adaptation to unseen websites or domains. Specifically, for Mind2Web's cross-website and cross-domain sets, few-shot adaptation using the AdaptAgent framework resulted in an absolute increase in overall success rate ranging from 4.18% to 7.21% over the corresponding unadapted counterparts, which corresponds to a relative increase of 21.03% to 45.40% over the current state-of-the-art. The trends are consistent across all the models, demonstrating the effectiveness of using only 1 or 2 human demonstrations via AdaptAgent. Similarly, on VisualWebArena, AdaptAgent led to an absolute increase in overall success rate ranging from 3.36% to 5.11%, which corresponds to 28.32% to 65.75% relative increase over the SOTA approaches.  In the following sections, we further investigate the advantage of multimodal in-context demonstrations compared to text-only demonstrations, the role of different data selection strategies during meta-learning, and the role of the number of in-context demonstrations used.\nMultimodal demonstrations are more effective than text-only demonstrations: In our ablation study, we examined the impact of in-context demonstration modalities-specifically text-only versus multimodal-on our top-performing models, SeeAct and SeeAct*."}, {"title": "Discussion and Limitations", "content": "We propose the AdaptAgent framework, which uses few-shot human demonstrations for efficient adaptation of web agents to unseen websites and domains, and demonstrated its efficacy for both pro- prietary and open-weights MLLM-based agents. More broadly, our results indicate that AdaptAgent provides a notable boost in the success rate of current SoTA web agents on unseen websites and domains in a cost-effective way, complementing the gains obtained by building larger pre-trained models or fine-tuning on larger datasets. Beyond the main result, we also demonstrate the benefits of using multimodal in-context demonstrations over text-only demonstrations. Furthermore, our ablations provide actionable recommendations for future research and practitioners to build efficiently adaptable web agents (i) the trade-offs associated with different data selection strategies for meta- learning can influence the generalizability of the adapted web agent, and (ii) while more in-context multimodal demonstrations boost the performance of proprietary agents, the gains tend to saturate with a higher number of examples.\nDespite the state-of-the-art performance achieved by AdaptAgent, the best-performing agent attained an overall task success rate of less than 25% on both Mind2Web and VisualWebArena. There remains significant room for improvement, particularly for tasks requiring long action sequences and websites with complex visual layouts (see A.5 for more details), underscoring the potential for future advancements in this area."}, {"title": "Appendix", "content": "A.1 Detailed Related Work\nA.1.1 UI/Web Agents\nAI-enabled digital device control \u2014 i.e., controlling digital devices using AI with natural language as input has been a long-standing ambition for large-scale automation of inherently useful tasks. The underlying problem involves mapping a language instruction from the user to a sequence of digital actions that AI agents can execute to successfully complete the task. Before LLMs, approaches to the problem involved using reinforcement learning on top of (often pre-trained) language models like LSTM and BERT for processing language input and HTML/DOM along with ResNet-like models for processing GUI states [23, 31, 25]. More recently, as multimodal LLMs have demonstrated success in modeling vision-and-language, they have lent themselves as strong backbones for building web agents that can process tasks specified by the user and engage in reasoning to output the best possible actions to be executed on a user interface such as a web browser. A majority of SoTA work [62, 20] use a pretrained, off-the-shelf LLM such as GPT-4(V/o) to build such multimodal web agents. The input information being provided as context to the LLM can include an image of the GUI, a series of prior actions, additional overlaid image annotations, as well as the HTML/DOM information assuming that the task is web interaction and access to HTML/DOM is possible. Work such as Pix2Act [51] and WebAgent [19] train LLMs to attend to parts of HTML code or generate the next action step through self-supervision, or combine the effectiveness of MLLMs with the promise of reinforcement learning train agents via Behavioral cloning or REINFORCE. However, these works were usually trained on simpler sandboxed environments and would require significant training resources as well as tedious curation of data samples [30]. A disadvantage of such an approach is that it cannot scale to tasks that are complex or that use proprietary enterprise software. Additionally, agents that require exploration as part of the training process would need constant human supervision to avoid risky outcomes. While there has been considerable progress in the success rate of agents on tasks that are encountered as part of their training, their performance in unseen settings has been lacking. To the best of our knowledge, prior work has not explicitly focused on methods that could make Web/GUI agents more adaptable to unseen settings.\nOur work proposes a framework where GUI/web agents are trained to efficiently adapt to unseen settings using few-shot human demonstrations. Data-efficient adaptation of web agents via human demonstrations will (a) avoid costly retraining processes/updates for unseen settings, (b) boost the generalizability of web agents to complex workflows and proprietary settings, and (c) enable web agents to learn from custom information provided by human experts as a part of the demonstrations.\nFew-shot learning with LLMs\nData-efficient alignment of LLMs to preferences and new tasks is an active area of research [26, 32]. In contrast to relatively data-hungry approaches like RLHF [38] and DPO [40] that often require hundreds of thousands paired comparisons, few-shot alignment and adaptation aims to use"}, {"title": "First-order approximation of MAML for Multimodal Web Agents", "content": "We present a derivation of the first-order approximation of MAML proposed by [16], while contex- tualizing it to our setting of updating multimodal LLMs. We begin with the original expression for updates using the MAML algorithm in Equation 1:\n$\\theta \\leftarrow \\theta - \\beta.\\nabla_{\\theta} \\Sigma_{T \\epsilon T} L_{T}(\\theta_{t})$.\n(1)\nUsing the chain rule, the derivative term can be expressed as $\\Sigma_{i=1}^{N} (\\nabla_{\\theta}\\theta_{i} \\times \\nabla_{\\theta_{i}} L_{T}(\\theta_{i}))$. The first component within the summation could be broken down further as,\n$\\nabla_{\\theta}\\theta_{i} = \\nabla_{\\theta}(\\theta \u2013 \\alpha.\\nabla_{\\theta}L^{train} (\\theta))$,"}]}