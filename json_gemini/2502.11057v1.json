{"title": "A Physics-Informed Machine Learning Framework for Safe and Optimal Control of Autonomous Systems", "authors": ["Manan Tayal", "Aditya Singh", "Shishir Kolathaya", "Somil Bansal"], "abstract": "As autonomous systems become more ubiquitous in daily life, ensuring high performance with guaranteed safety is crucial. However, safety and performance could be competing objectives, which makes their co-optimization difficult. Learning-based methods, such as Constrained Reinforcement Learning (CRL), achieve strong performance but lack formal safety guarantees due to safety being enforced as soft constraints, limiting their use in safety-critical settings. Conversely, formal methods such as Hamilton-Jacobi (HJ) Reachability Analysis and Control Barrier Functions (CBFs) provide rigorous safety assurances but often neglect performance, resulting in overly conservative controllers. To bridge this gap, we formulate the co-optimization of safety and performance as a state-constrained optimal control problem, where performance objectives are encoded via a cost function and safety requirements are imposed as state constraints. We demonstrate that the resultant value function satisfies a Hamilton-Jacobi-Bellman (HJB) equation, which we approximate efficiently using a novel physics-informed machine learning framework. In addition, we introduce a conformal prediction-based verification strategy to quantify the learning errors, recovering a high-confidence safety value function, along with a probabilistic error bound on performance degradation. Through several case studies, we demonstrate the efficacy of the proposed framework in enabling scalable learning of safe and performant controllers for complex, high-dimensional autonomous systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous systems are becoming increasingly prevalent across various domains, from self-driving vehicles and robotic automation to aerospace and industrial applications. Designing control algorithms for these systems involves balancing two fundamental objectives: performance and safety. Ensuring high performance is essential for achieving efficiency and task objectives under practical constraints, such as fuel limitations or time restrictions. For instance, a warehouse humanoid robot navigating to a destination must optimize its route for efficiency. At the same time, safety remains paramount to prevent catastrophic accidents or system failures. These two objectives, however, often conflict, making it challenging to develop control strategies that achieve both effectively.\n\nA variety of data-driven approaches have been explored to integrate safety considerations into control synthesis. Constrained Reinforcement Learning (CRL) methods [1], [2] employ constrained optimization techniques to co-optimize safety and performance where performance is encoded as a reward function and safety is formulated as a constraint. These methods often incorporate safety constraints into the objective function, leading to only a soft imposition of the safety constraints. Moreover, such formulations typically minimize cumulative constraint violations rather than enforcing strict safety at all times, which can result in unsafe behaviors.\n\nAnother class of methods involve safety filtering, which ensures constraint satisfaction by modifying control outputs in real-time. Methods such as Control Barrier Function (CBF)-based quadratic programs (QP) [3] and Hamilton-Jacobi (HJ) Reachability filters [4], [5] act as corrective layers on top of a (potentially unsafe) nominal controller, making minimal interventions to enforce safety constraints. However, because these safety filters operate independently of the underlying performance-driven controller, they often lead to myopic and suboptimal decisions. Alternatively, online optimization-based methods, such as Model Predictive Control (MPC) [6], [7] and Model Predictive Path Integral (MPPI) [8], [9], can naturally integrate safety constraints while optimizing for a performance objective. These methods approximate infinite-horizon optimal control problems (OCPs) with a receding-horizon framework, enabling dynamic re-planning. While effective, solving constrained OCPs online remains computationally expensive, limiting their applicability for high-frequency control applications. The challenge is further exacerbated when dealing with nonlinear dynamics and nonconvex (safety) constraints, limiting the feasibility of these methods for ensuring safety and optimality for real-world systems.\n\nA more rigorous approach to addressing the trade-off between performance and safety is to formulate the problem as a state-constrained optimal control problem (SC-OCP), where safety is explicitly encoded as a hard constraint, while performance is expressed through a reward (or cost) function. While theoretically sound, characterizing the solutions of SC-OCPs is challenging unless certain controllability conditions hold [10]. To address these challenges, [11] proposed an epigraph-based formulation, which characterizes the value function of an SC-OCP by computing its epigraph using dynamic programming, resulting in a Hamilton-Jacobi-Bellman Partial Differential Equation (HJB-PDE). The SC-OCP value function as well as the optimized policy is then recovered from this epigraph. However, dynamic programming suffers from the curse of dimensionality, making it impractical for high-dimensional systems with traditional numerical solvers. Furthermore, the epigraph formulation itself increases the problem's dimensionality, exacerbating computational complexity further.\n\nIn this work, we propose a novel algorithmic approach to co-optimize safety and performance for high-dimensional autonomous systems. Specifically, we formulate the problem as a SC-OCP and leverage the epigraph formulation in [11]. To efficiently solve this epigraph formulation, we leverage physics-informed machine learning [12], [13] to learn a solution to the resultant HJB-PDE by minimizing PDE residuals. This enables us to efficiently scale epigraph computation for higher-dimensional autonomous systems, leading to safe and performant policies. To summarize, our main contributions are as follows:\n\n\u2022 We propose a novel Physics-Informed Machine Learning (PIML) framework to learn policies that co-optimize safety and performance for high-dimensional autonomous systems.\n\n\u2022 We introduce a conformal prediction-based safety verification strategy that provides high-confidence probabilistic safety guarantees for the learned policy, reducing the impact of learning errors on safety.\n\n\u2022 We propose a performance quantification framework that leverages conformal prediction to provide high-confidence probabilistic error bounds on performance degradation.\n\n\u2022 Across three case studies, we showcase the effectiveness of our proposed method in jointly optimizing safety and performance, while scaling to complex, high-dimensional systems."}, {"title": "II. PROBLEM SETUP", "content": "Consider a nonlinear dynamical system characterized by the state $x \\in \\mathcal{X} \\subseteq \\mathbb{R}^n$ and control input $u \\in \\mathcal{U} \\subset \\mathbb{R}^M$, governed by the dynamics $\\dot{x}(t) = f(x(t), u(t))$, where the function $f: \\mathbb{R}^n \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$ is locally Lipschitz continuous. In this work, we assume that the dynamics model $f$ is known; however, it can also be learned from data if unavailable.\n\nWe are given a failure set $\\mathcal{F} \\subset \\mathcal{X}$ that represents the set of unsafe states for the system (e.g., obstacles for an autonomous ground robot). The system's performance is quantified by the cost function $C(t, x, u)$, given by:\n\n$C(t, x(t), u) = \\int_{s=t}^{T} l(x(s)) ds + \\phi(x(T))$,\n\nwhere $l : \\mathcal{X} \\rightarrow \\mathbb{R}_{>0}$ and $\\phi : \\mathcal{X} \\rightarrow \\mathbb{R}_{>0}$ are Lipschitz continuous and non-negative functions, representing the running cost over the time horizon $[t, T)$ and the terminal cost at time $T$, respectively. $u : [t,T) \\rightarrow \\mathcal{U}$ is the control signal applied to the system. Using this premise, we define the main objective of this paper:\n\nObjective 1. We aim to synthesize an optimal policy $\\pi^* : [t, T) \\times \\mathcal{X} \\rightarrow \\mathcal{U}$ that minimizes the cost function $C$ while ensuring that the system remains outside the failure set $\\mathcal{F}$ at all times."}, {"title": "A. State-Constrained Optimal Control Problem", "content": "To achieve the stated objective, the first step is to encode the safety constraint via a function $g : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ such that, $\\mathcal{F} := {x \\in \\mathcal{X} | g(x) > 0}$. Using these notations, the objective can be formulated as the following State-Constrained Optimal Control Problem (SC-OCP) to compute the value function $V$:\n\n$V(t, x(t)) = \\min_{u} \\int_{t}^{T} l(x(s))ds + \\phi(x(T))$\ns.t. $\\dot{x} = f(x, u)$,\n$g(x(s)) \\leq 0 \\quad \\forall s \\in [t,T]$\n\nThis SC-OCP enhances the system's performance by minimizing the cost, while maintaining system safety through the state constraint, $g(x) \\leq 0$, ensuring that the system avoids the failure set, $\\mathcal{F}$. Thus, the policy, $\\pi^*$, derived from the solution of this SC-OCP co-optimizes safety and performance."}, {"title": "B. Epigraph Reformulation", "content": "Directly solving the SC-OCP in (2) presents significant challenges due to the presence of (hard) state constraints. To address this issue, we reformulate the problem in its epigraph form [16], which transforms the constrained optimization into a more tractable two-stage optimization problem. This reformulation allows us to efficiently obtain a solution to the SC-OCP in (2). The resulting formulation is given by:\n\n$V(t, x(t)) = \\min z$\n$z \\in \\mathbb{R}_+$\ns.t. $\\hat{V}(t, x, z) \\leq 0$,\n\nwhere $z$ is a non-negative auxiliary optimization variable, and $\\hat{V}$ represents the auxiliary value function. Here, $\\hat{V}$ is defined as [11]:\n\n$\\hat{V}(t, x(t), z) = \\min_{u} \\max {\\int_{s=t}^{T} l(x(s)) ds + \\phi(x(T)) - z, \\max_{s \\in [t,T]} g(x(s))}$.\n\nNote that if $\\hat{V}(t, x, z) < 0$, it implies that $g(x(s)) < 0$ for all $s \\in [t,T]$. In other words, the system must be outside the failure set at all times; therefore, the system is guaranteed to be safe whenever $\\hat{V}(t, x, z) < 0$.\n\nIn this reformulated problem, state constraints are effectively eliminated, enabling the use of dynamic programming to characterize the value function, as we explain later in this section. Intuitively, optimal $z$ ($z^*$) can be thought of as the minimum permissible cost the policy can incur without compromising on safety. From Equation 3, it can be inferred that if $z > z^*$, the safety constraint dominates in the max term, resulting in a conservative policy. Conversely, if $z < z^*$, the performance objective takes precedence, leading to a potentially aggressive policy that might compromise safety.\n\nFurthermore, to facilitate solving the epigraph reformulation, $z$ can be treated as a state variable, with its dynamics given by $\\dot{z}(t) = -l(x(t))$. This implies that as the trajectory progresses over time, the minimum permissible cost, $z$, decreases by the step cost $l(x)$ at each time step. This allows us to define an augmented system that evolves according to the following dynamics:\n\n$\\dot{\\hat{x}} = \\hat{f}(t, \\hat{x}, u) := \\begin{bmatrix}f(t, x, u) \\\\ -l(x)\\end{bmatrix}$\n\nwhere $\\hat{x} := [x, z]^T$ represents the augmented state. With the augmented state representation, it has been shown that the auxiliary value function $\\hat{V}(t, x(t), z(t))$ is characterized as the unique continuous viscosity solution of the following Hamilton-Jacobi-Bellman (HJB) partial differential equation (PDE) [11]:\n\n$\\min_u {-\\partial_t \\hat{V} - \\min_u (\\nabla \\hat{V}(t, \\hat{x}), \\hat{f}(\\hat{x}, u)), \\hat{V} - g(x)} = 0,$\n\n$\\forall t \\in [0, T)$ and $\\hat{x} \\in \\mathcal{X} \\times \\mathbb{R}$, where $\\langle, \\rangle$ denotes the dot product of vectors. The boundary condition for the PDE is given by:\n\n$\\hat{V}(T, \\hat{x}) = \\max (\\phi(x) - z, g(x)), \\quad \\hat{x} \\in \\mathcal{X} \\times \\mathbb{R}.$"}, {"title": "III. METHODOLOGY", "content": "To solve the SC-OCP in Equation (2), we aim to compute the optimal value function $\\hat{V}$, which minimizes the cost while ensuring system safety. In this section, we outline a structured approach: first, we learn the auxiliary value function $\\hat{V}_\\theta$ using a physics-informed machine learning framework. Then, we apply a conformal prediction-based method to verify safety and correct for potential learning errors in $\\hat{V}_\\theta$. The final value function $\\hat{V}_\\epsilon$ is obtained from the safety-corrected $\\hat{V}_\\theta$ using the epigraph formulation in (3). Lastly, we assess the performance of $\\hat{V}$ through a second conformal prediction procedure. Figure 1 gives an overview of the proposed approach.\n\nThe following subsections provide a detailed explanation of each step, beginning with the methodology for learning $\\hat{V}$."}, {"title": "A. Training the Auxiliary Value Function (V)", "content": "The auxiliary value function, $\\hat{V}$, satisfies the HJB-PDE in Equation (6), as discussed in Section II-B. Traditionally, numerical methods are used to solve the HJB-PDE over a grid representation of the state space [17], [18], where time and spatial derivatives are approximated numerically. While grid-based methods are accurate for low-dimensional problems, they struggle with the curse of dimensionality - their computational complexity increases exponentially with the number of states - limiting their use in high-dimensional systems. To address this, we adopt a physics-informed machine learning framework, inspired by [19], which has proven effective for high-dimensional reachability problems.\n\nThe solution of the HJB-PDE inherently evolves backward in time, as the value function at time $t$ is determined by its value at $t + \\Delta t$. To facilitate neural network training, we use a curriculum learning strategy, progressively expanding the time sampling interval from the terminal time $[T, T]$ to the full time horizon $[0, T]$. This approach allows the neural network to first accurately learn the value function from the terminal boundary conditions, subsequently propagating the solution backward in time by leveraging the structure of the HJB-PDE.\n\nSpecifically, the auxiliary value function is approximated by a neural network, $\\hat{V}_\\theta$, where $\\theta$ denotes the trainable parameters of the network. Training samples, $(t_k, x_k, z_k)_{k=1}^{N}$, are randomly drawn from the state space based on the curriculum training scheme. The proposed learning framework utilizes a loss function that enforces two primary objectives: (i) compliance with the PDE in (6), using the PDE residual error given by:\n\n$\\mathcal{L}_{pde} (t_k, \\hat{x}_k | \\theta) = || \\min {-\\partial_t \\hat{V}_\\theta (t_k, \\hat{x}_k) - \\mathcal{H}(t_k, \\hat{x}_k),\\hat{V}_\\theta (t_k, \\hat{x}_k) - g (x_k)} ||,$\n\nwhere $\\mathcal{H}(t,\\hat{x}) = \\min_{u \\in \\mathcal{U}}(\\nabla \\nabla_\\theta(t, \\hat{x}), \\hat{f}(\\hat{x}, u))$ and (ii) satisfaction of the boundary condition in (7), using boundary condition loss, given by:\n\n$\\mathcal{L}_{bc} (t_k, x_k | \\theta) = ||\\max (\\phi(x_k) - z_k, g(x_k)) -\n\\hat{V}_\\theta (t_k, x_k) || \\mathbb{I}(t_k = T) .$\n\nThese terms are balanced by a trade-off parameter $\\Lambda$, leading to the overall loss function:\n\n$\\mathcal{L} (t_k, x_k | \\theta) = \\mathcal{L}_{pde} (t_k, x_k | \\theta) + \\Lambda \\mathcal{L}_{bc} (t_k, x_k | \\theta)$\n\nMinimizing the overall loss function provides a self-supervised learning mechanism to approximate the auxiliary value function."}, {"title": "B. Safety Verification", "content": "The learned auxiliary value function, $\\hat{V}_\\theta$, induces a policy, $\\pi_\\theta$, that minimizes the Hamiltonian term $\\mathcal{H}(t, \\hat{x})$ in the HJB-PDE. The policy is given by:\n\n$\\pi_\\theta (t, x) = \\arg \\min_{u \\in \\mathcal{U}} (\\nabla \\hat{V}(t, x), \\hat{f}(x, u)).$\n\nThe rollout cost corresponding to this policy is defined as:\n\n$\\hat{V}^{\\pi_\\theta} (t, \\hat{x}) = \\max {\\int_{t}^{T} \\hat{C}(t, x(t), u) - z, \\max_{s \\in [t,T]} g(x(s))} =$\n\nIdeally, the rollout cost from a given state under $\\pi_\\theta$ should match the value of the auxiliary value function at that state. However, due to learning inaccuracies, discrepancies can arise. This becomes critical when a state, $\\hat{x}$, is deemed safe by the auxiliary value function $(\\hat{V}_\\theta(t, \\hat{x}) < 0)$ but is unsafe under the induced policy $(\\hat{V}^{\\pi_\\theta}(t, \\hat{x}) > 0)$. To address this, we introduce a uniform value function correction margin, $\\delta$, which guarantees that the sub-$\\\\delta$ level set of the auxiliary value function remains safe under the induced policy. Mathematically, the optimal $\\delta$ $(\\delta^*)$ can be expressed as:\n\n$\\delta^* := \\min {\\hat{V}_\\theta(0, \\hat{x}) : \\hat{V}^{\\pi_\\theta} (0, \\hat{x}) \\geq 0}$\n\nIntuitively, $\\delta^*$ identifies the tightest level of the value function that separates safe states under $\\pi_\\theta$ from unsafe ones. Hence, any initial state within the sub-$\\delta^*$ level set is guaranteed to be safe under the induced policy, $\\pi_\\theta$. However, calculating $\\delta^*$ exactly requires infinitely many state-space points. To overcome this, we adopt a conformal-prediction-based approach to approximate $\\delta^*$ using a finite number of samples, providing a probabilistic safety guarantee. The following theorem formalizes our approach:\n\nTheorem III.1 (Safety Verification Using Conformal Prediction). Let $S_\\delta$ be the set of states satisfying $\\hat{V}_\\theta(0, \\hat{x}) \\leq \\delta$, and let $(0, \\hat{x}_i)_{i=1,...,N_s}$ be $N_s$ i.i.d. samples from $S_\\delta$. Define $\\alpha_\\delta$ as the safety error rate among these $N_s$ samples for a given $\\delta$ level. Select a safety violation parameter $\\epsilon_s \\in (0,1)$ and a confidence parameter $\\beta_s \\in (0,1)$ such that:\n\n$\\sum_{i=0}^{l-1} {N_s \\choose i} (1-\\epsilon_s)^i \\epsilon_s^{N_s-i} \\leq \\beta_s$\n\nwhere $l = \\lfloor (N_s+1)\\alpha_\\delta \\rfloor$. Then, with the probability of at least $1 - \\beta_s$, the following holds:\n\n$P_{\\hat{x} \\in S_\\delta} (\\hat{V}^{\\pi_\\theta} (0,\\hat{x}) \\leq 0) \\geq 1 - \\epsilon_s.$\n\nThe proof is available in Appendix A. The safety error rate $\\alpha_s$ is defined as the fraction of samples satisfying $\\hat{V}_\\theta \\leq \\delta$ and $\\hat{V}^{\\pi_\\theta} \\geq 0$ out of the total $N_s$ samples.\nAlgorithm 1 presents the steps to calculate $\\delta$ using the approach proposed in this theorem."}, {"title": "C. Obtaining Safe and Performant Value Function and Policy from $\\hat{V}_\\theta$", "content": "Using the $\\delta$-level estimate from Algorithm (1), we can finally obtain the safe and performant value function, $\\hat{V}_\\epsilon(t, x)$, by solving the following epigraph optimization problem:\n\n$\\hat{V}_\\epsilon(t, x) = \\min z$\n$z \\in \\mathbb{R}_+$\ns.t. $\\hat{V}_\\theta(t, x, z) < \\delta$.\n\nNote that $\\hat{V}_\\epsilon(t, x)$ is trivially $\\infty$ for the states where $\\hat{V}_\\theta(t, x,z) > \\delta$, since such states are unsafe and hence do not satisfy the safety constraint.\n\nIn practice, we solve this optimization problem by using a binary search approach on $z$. The resulting optimal state-feedback control policy, $\\pi_\\theta : \\mathcal{X} \\times [t,T) \\rightarrow \\mathcal{U}$, satisfying Objective (1), is given by:\n\n$\\pi_\\theta(t, x) = \\arg \\min_{u} (\\nabla \\hat{V}_\\epsilon(t, \\hat{x}^*), \\hat{f}(x^*, u)),$\n\nwhere $\\hat{x}^*$ is the augmented state associated with the optimal $z^*$ obtained by solving (16), i.e., $\\hat{x}^* = [x, z^*]^T$. Intuitively, we can expect $\\pi_\\theta$ to learn behaviors that best tradeoff the safety and performance of the system."}, {"title": "D. Performance Quantification", "content": "In general, the learning inaccuracies in the auxiliary value function $\\hat{V}_\\theta$, may lead to errors in the value function $\\hat{V}_\\epsilon$. These errors, in turn, can lead to performance degradation under policy $\\pi_\\theta$. To quantify this degradation, we propose a conformal prediction-based performance quantification method that provides a probabilistic upper bound on the error between the value function and the value obtained from the induced policy. The following theorem formalizes our approach:\n\nTheorem III.2 (Performance Quantification Using Conformal Prediction). Suppose $S^*$ denotes the safe states satisfying $\\hat{V}_\\theta(0,x) < \\infty$ (or equivalently $\\hat{V}_\\epsilon(0,x^*) < \\delta$) and $(0,x_i)_{i=1,...,N_p}$ are $N_p$ i.i.d. samples from $S^*$. For a user-specified level $\\alpha_p$, let $\\psi$ be the $\\lfloor (N_p+1)(1-\\alpha_p)\\rfloor th$ quantile of the scores $(p_i := |\\hat{V}_\\theta(0,x_i)-\\hat{V}^{\\pi_\\theta} (0,x_i)|)_{i=1,.., N_p}$ on the $N_p$ state samples. Select a violation parameter $\\epsilon_p \\in (0,1)$ and a confidence parameter $\\beta_p \\in (0,1)$ such that:\n\n$\\sum_{i=0}^{l-1} {N_p \\choose i} (\\epsilon_p)^{i} (1 - \\epsilon_p)^{N_p-i} \\leq \\beta_p$\n\nwhere, $l = \\lfloor (N_p + 1)\\alpha_p \\rfloor$. Then, the following holds, with probability $1 - \\beta_p$:\n\n$P_{x \\in S^*} (\\frac{|\\hat{V}_\\theta(0,x) - \\hat{V}^{\\pi_\\theta} (0,x)|}{C_{max}} < \\psi) \\geq 1 - \\epsilon_p.$\n\nwhere $C_{max}$ is a normalizing factor and denotes the maximum possible cost that could be incurred for any $x \\in S^*$.\n\nThe proof is available in Appendix B. Note that $C_{max}$ can be easily calculated by calculating the upper bound of the cost function $C(t, x(t), u) \\forall x \\in S^*$.\n\nIntuitively, the performance of the resultant policy is the best when the $\\psi$ value approaches 0, while the worst performance occurs at $\\psi = 1$. Algorithm 2 presents the steps to calculate $\\psi$ using the approach proposed in this theorem."}, {"title": "IV. EXPERIMENTS", "content": "The objective of this paper is to demonstrate the co-optimization of performance and safety. To achieve this, we evaluate the proposed method and compare them with baselines using two metrics: (1) Cumulative Cost: This metric represents the total cost for $\\int l(x(s))ds + \\phi(x(T))$, accumulated by a policy over the safe trajectories. (2) Safety Rate: This metric is defined as the percentage of trajectories that remain safe, i.e., never enter the failure region $\\mathcal{F}$ at any point in time.\n\nBaselines: We consider two categories of baselines: the first set of methods aim to enhance the system performance (i.e., minimize the cumulative cost) while encouraging safety, encompassing methods such as Constrained Reinforcement Learning (CRL) and Model Predictive Path Integral (MPPI) [8] algorithms. The second category prioritizes safety, potentially at the cost of performance. This includes safety filtering techniques such as Control Barrier Function (CBF)-based quadratic programs (QP) [3] that modify a nominal, potentially unsafe controller to satisfy the safety constraint. Additionally, we have presented the comparative study of offline and online computation times between the baselines in Appendix J."}, {"title": "A. Efficient and Safe Boat Navigation", "content": "In our first experiment, we consider a 2D autonomous boat navigation problem, where a boat with coordinates $(x_b, y_b)$ navigates a river with state-dependent drift to reach an island. The boat must avoid two circular boulders (obstacles) of different radii, which corresponds to the safety constraint in the system (see Fig. 3). The cost function penalizes the distance to the goal. The system state, $x$, evolves according to the dynamics:\n\n$\\dot{x} = [\\dot{x}_b, \\dot{y}_b]^T, \\quad \\dot{X} = [u_1 + 2 - 0.5y_b, u_2]^T$\n\nwhere $[u_1, u_2]$ are the bounded control inputs in the $x_b$ and $y_b$ directions, constrained by the control space $\\mathcal{U} = {[u_1, u_2] \\in \\mathbb{R}^2 | ||[u_1, u_2]|| \\leq 1}$. The term $2 - 0.5 y_b$ introduces a state-dependent drift, complicating the control task as the actions must counteract the drift while ensuring safety, which is challenging under bounded control inputs. The rest of the details about the experiment setup can be found in the Appendix"}, {"title": "C. Safety Guarantees and Performance Quantification", "content": "We use $N_s = 300K$ and $N_p = 300K$ samples for thorough verification, ensuring dense state space sampling. For this experiment, we set $\\epsilon_s = 0.001$ and $\\beta_s = 10^{-10}$, resulting in a $\\delta$-level of 0. This implies that, with $1 - 10^{-10}$ confidence, any state with $\\hat{V}_\\theta(t, x,z) \\leq 0$, is safe with at least 99.9% probability. For performance quantification, we set $\\epsilon_p = 0.01$ and $\\beta_p = 10^{-10}$, leading to a $\\psi$-level of 0.136. This ensures, with $1-10^{-10}$ confidence, that any state in $S^*$ has a normalized error between the predicted value and the policy value of less than 0.136 with 99% probability. Low $\\delta$ and $\\psi$ values with high confidence indicate that the learned policy closely approximates the optimal policy and successfully co-optimizes safety and performance."}, {"title": "Baselines", "content": "This being a 2-dimensional system, we compare our method with the ground truth value function computed by solving the HJB-PDE numerically using the Level Set Toolbox [17]. Additional baselines include: (1) MPPI, a sample-based path-planning algorithm with safety as soft constraints, (2) MPPI-NCBF, where safety is enforced using a Neural CBF-based QP with MPPI as the nominal controller [20], [21], and (3) C-SAC, a Lagrangian-based CRL approach using Soft Actor-Critic [22], incorporating safety as soft constraints."}, {"title": "B. Pursuer Vehicle tracking a moving Evader", "content": "In our second experiment, we consider an acceleration-driven pursuer vehicle, tracking a moving evader while avoiding five circular obstacles (see Fig. 4). This experiment involves an 8-dimensional system, with the state $x$ defined as $x = [x_p, y_p, v, \\Theta, x_e, y_e, v_{xe}, v_{ye}]^T$, where $x_p, y_p, v, \\Theta$, represent the coordinates, linear velocity, and orientation of the pursuer vehicle, respectively, and $x_e, y_e, v_{xe}, v_{ye}$ represent the coordinates and linear velocities of the evader vehicle. The pursuer vehicle is controlled by linear acceleration $(u_1)$ and angular velocity $(u_2)$. The control space is $\\mathcal{U} = {[u_1, u_2] \\in [-2,2]^2}$. The complexity of this system stems from the dynamic nature of the goal, along with the challenge of ensuring safety in a cluttered environment, which in itself is a difficult safety problem. More details about the experiment setup are in Appendix D."}, {"title": "Safety Guarantees and Performance Quantification", "content": "Similar to the previous experiment, we set $N_s = N_p = 300k$. We choose $\\epsilon_s = 0.01$ and $\\beta_s = 10^{-10}$, yielding a $\\delta$-level of -0.04 and a safety level of 99% on the auxiliary value function. For performance, we set $\\epsilon_p = 0.01$ and $\\beta_p = 10^{-10}$, leading to a $\\psi$-level of 0.137. These values indicate the learned policy maintains high safety with low-performance degradation in this cluttered environment."}, {"title": "Baselines", "content": "Similar to the previous experiment, we use MPPI and C-SAC with soft safety constraints as our baselines. For safety filtering, we apply a collision cone CBF (C3BF) [23]-based QP due to its effectiveness in handling acceleration-driven systems."}, {"title": "C. Multi-Agent Navigation", "content": "In our third experiment, we consider a multi-agent setting where each of the 5 agents, represented by $x_i = [x_{ai}, y_{ai}, x_{gi}, y_{gi}]$, tries to reach its goal while avoiding collisions with others. $(x_{ai}, y_{ai})$ denote the position of the $i$th agent, while $(x_{gi}, y_{gi})$ represent the goal locations for that agent. The system is 20-dimensional, with each agent controlled by its x and y velocities. The control space for each agent is $\\mathcal{U}_i = {[v_{xi}, v_{yi}] | ||[v_{xi}, v_{yi}]|| \\leq 1}$. The complexity of this system stems from the interactions and potential conflicts between agents as they attempt to reach their goals while avoiding collisions. The rest of the details about the experiment setup can be found in Appendix E."}, {"title": "Safety Guarantees and Performance Quantification", "content": "We set $N_s = N_p = 300k$, $\\epsilon_s = 0.001$, and $\\beta_s = 10^{-10}$, resulting in a $\\delta$-level of -0.09 with safety assurance of 99.9% for the auxiliary value function. For performance quantification, we set $\\epsilon_p = 0.01$ and $\\beta_p = 10^{-10}$, leading to a $\\psi$-level of 0.068. It is evident that the $\\delta$ and $\\psi$ values remain very low with high confidence, highlighting the effectiveness of our method in co-optimizing safety and performance for high-dimensional, multi-agent systems."}, {"title": "Baselines", "content": "Similar to previous experiments, we have used MPPI, C-SAC, and MPPI-NCBF as our baselines for this experiment too."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "In this work, we introduced a physics-informed machine learning framework for co-optimizing safety and performance in autonomous systems. By formulating the problem as a state-constrained optimal control problem (SC-OCP) and leveraging an epigraph-based approach, we enabled scalable computation of safety-aware policies. Our method integrates conformal prediction-based safety verification to ensure high-confidence safety guarantees while maintaining optimal performance. Through multiple case studies, we demonstrated the effectiveness and scalability of our approach in high-dimensional systems. In future, we will explore methods for rapid adaptation of the learned policies in light of new information about the system dynamics, environments, or safety constraints. We will also apply our method to other high-dimensional autonomous systems and systems with unknown dynamics."}, {"title": "APPENDIX", "content": "A. Proof of Theorem (III.1)\nBefore we proceed with the proof of the Theorem (III.1)", "prediction": "nLemma 1 (Split Conformal Prediction [24"}]}