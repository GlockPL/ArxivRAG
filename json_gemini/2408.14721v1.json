{"title": "PAT: Pruning-Aware Tuning for Large Language Models", "authors": ["Yijiang Liu", "Huanrui Yang", "Youxin Chen", "Rongyu Zhang", "Miao Wang", "Yuan Du", "Li Du"], "abstract": "Large language models (LLMs) excel in language tasks, especially with supervised fine-tuning after pre-training. However, their substantial memory and computational requirements hinder practical applications. Structural pruning, which reduces less significant weight dimensions, is one solution. Yet, traditional post-hoc pruning often leads to significant performance loss, with limited recovery from further fine-tuning due to reduced capacity. Since the model fine-tuning refines the general and chaotic knowledge in pre-trained models, we aim to incorporate structural pruning with the fine-tuning, and propose the Pruning-Aware Tuning (PAT) paradigm to eliminate model redundancy while preserving the model performance to the maximum extend. Specifically, we insert the innovative Hybrid Sparsification Modules (HSMs) between the Attention and FFN components to accordingly sparsify the upstream and downstream linear modules. The HSM comprises a lightweight operator and a globally shared trainable mask. The lightweight operator maintains a training overhead comparable to that of LORA, while the trainable mask unifies the channels to be sparsified, ensuring structural pruning. Additionally, we propose the Identity Loss which decouples the transformation and scaling properties of the HSMs to enhance training robustness. Extensive experiments demonstrate that PAT excels in both performance and efficiency. For example, our Llama2-7b model with a 25% pruning ratio achieves 1.33\u00d7 speedup while outperforming the LoRA-finetuned model by up to 1.26% in accuracy with a similar training cost.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) (Touvron et al. 2023a; Brown et al. 2020; Chowdhery et al. 2022) have transformed the field of NLP (Vaswani et al. 2017; Bahdanau, Cho, and Bengio 2014; Zhang, Zhao, and LeCun 2015; Yang et al. 2016) with their exceptional performance on various complex language benchmarks. Despite their success, these models often necessitate substantial computational resources and present challenges for practical deployment due to their billions of parameters. Their extensive scales result in high latency and complications in deployments (Pan et al. 2023; Zhang et al. 2024). To mitigate these issues, various techniques have been proposed, including model pruning (Ma, Fang, and Wang 2023; Ashkboos et al. 2024; Sun et al. 2023; Santacroce et al. 2023; Fang, Ma, and Wang 2023), knowledge distillation (Agarwal et al. 2023; Tunstall et al. 2023; Sun et al. 2019, 2020; Ma et al. 2020), and quantization (Liu et al. 2022; Yao et al. 2022; Bai et al. 2020; Zafrir et al. 2019) within the context of pre-trained language models (PLMs).\nNetwork pruning (Syed, Guo, and Sundarapandiyan 2023; Xu et al. 2021a; Liu et al. 2021; Guo et al. 2019), which reduces model size by eliminating specific weights, has gained significant attention. Especially for structural pruning (Ashkboos et al. 2024; Li et al. 2016; Wang et al. 2019b) which promises practical acceleration on current hardware architectures. However, as shown in Fig. 1, traditional pruning methods (Ma, Fang, and Wang 2023; Ashkboos et al. 2024) usually results in significant performance loss, whether applied before or after recovery model finetuning with Pre/Post-Trainig Pruning (P2F/F2P).\nOn the other hand, since the pretraining-fine-tuning pipeline has become standard practice in both academic and industrial scenarios, Parameter-Efficient Fine-Tuning (PEFT) methods (Xu et al. 2023a; Lin, Madotto, and Fung 2020; Mahabadi et al. 2021; Liu et al. 2024b), e.g., Low-Rank Adapter (LoRA) (Hu et al. 2021), have emerged as prevailing solutions for streamlined training. Meanwhile, since model fine-tuning can be seen as refining the universal and chaotic knowledge in the pre-trained model, thereby transforming the general LLM into a task-specific expert, combining structural pruning and PEFT for model efficiency and quick adaptation becomes a natural thought.\nDrawing inspiration from quantization methods that often work synergistically, including the training-free Post-Training Quantization (PTQ) (Dettmers et al. 2022; Frantar et al. 2022; Lin et al. 2023; Lee et al. 2023) and the performance-enhancing Quantization-Aware Training (QAT) (Liu et al. 2023; Kim et al. 2023; Dettmers et al. 2023), we aim to incorporate structure pruning into the fine-tuning process while further boosting the model performance. This prompts us to introduce a new Pruning-Aware Tuning (PAT) paradigm to facilitate efficient inference and practical deployment in real-world applications, such as autonomous vehicles which require fast and accurate model inference to make real-time decisions and avoid obstacles while a fine-tuned RAG model must quickly and precisely retrieve and generate relevant responses from a compact knowledge base for different customer support. Unlike traditional P2F/F2P methods that remove model weights based on fixed prior knowledge, our proposed PAT method enables simultaneous pruning and fine-tuning. This allows the model to adaptively learn which parameters are most redundant and should be pruned during the PAT process. As a result, we achieve an automatic, end-to-end structured pruning process that not only maximizes but can also enhance the capabilities of the fine-tuned model.\nSpecifically, we propose the integration of plug-in Hybrid Sparsification Modules (HSMs). These HSMs are strategically positioned between the Attention and FFN components. Initially, they are set as identity matrices to maintain stable gradients at the onset of the fine-tuning process. As fine-tuning progresses, the HSMs selectively attenuate the channel values of the hidden dimensions, resulting in the exclusion of the corresponding linear projection weights. However, directly integrating dense-structured HSMs introduces an excess of trainable parameters. To mitigate this issue, we leverage the Hybrid-Identity-Operator (HIO), which reduces the number of trainable parameters. Compared with other PEFT methods, our approach not only achieves parameter efficiency but also decreases the overall model complexity. Furthermore, we introduce the Identity Loss (IL) applied to the HSMs to enhance training robustness and efficacy. This technique regularizes the HSMs while delegating the scaling functionality to independent trainable parameters.\nIn addition, the pruning operation across all HSMs is governed by a single trainable Unified Sparsification Mask (USM), ensuring consistent retention of channel indices across modules. This approach standardizes and streamlines the transformer decoder structure. As the trainable mask gradually converges to the target sparsity, the knowledge encoded in weights from pruned channels are seamlessly updated and redistributed to the remaining active channels.\nExtensive experiments on widely recognized Large Language Models (LLMs) demonstrate the effectiveness of our proposed Pruning-Aware Tuning (PAT) compared to state-of-the-art baselines, including Parameter-Efficient Fine-Tuning (PEFT) and Pre/Post-Training Pruning (PTP) methods. Notably, on the Llama2-7B model, PAT surpasses the performance of LoRA-64 by 1.26% while achieving 25% weight pruning. The contribution of this paper can be summarized as follows:\n\u2022 We propose an innovative paradigm called Pruning-Aware Tuning (PAT). Unlike traditional pre- or post-training pruning methods, PAT achieves simultaneous structural pruning and fine-tuning, leading to improved model performance.\n\u2022 To decrease overall model complexity, we integrate plug-in Hybrid Sparsification Modules (HSMs) with the Hybrid-Identity-Operator. Additionally, we design an Identity Loss (IL) applied to the HSMs to further enhance fine-tuning efficiency and robustness.\n\u2022 We utilize a single Unified Sparsification Mask (USM) that governs all HSMs, ensuring consistent retention of channel indices across modules."}, {"title": "Related Work", "content": "Pruning\nNetwork pruning (LeCun, Denker, and Solla 1989) has long been recognized as an effective method for model compression and acceleration. Earlier research primarily focused on small-scale networks (Fang et al. 2023; Yang et al. 2023). However, with the advent of large-scale models, pruning techniques have increasingly been applied to large language models (LLMs). According to the pruning granularity, pruning methods can be broadly categorized into unstructured and structured pruning. In the realm of unstructured pruning (Frantar and Alistarh 2023; Sun et al. 2023), techniques such as SparseGPT (Frantar and Alistarh 2023) and Wanda (Sun et al. 2023) have been proposed. SparseGPT addresses the layer-wise reconstruction problem by utilizing Hessian inverses, while Wanda employs the product of weight magnitudes and input feature norms as its pruning criterion. Despite their effectiveness, these unstructured sparsification methods do not guarantee on-device speedup without hardware-specific support. In contrast, the structured pruning (Zafrir et al. 2021; Kurtic et al. 2022; Xia, Zhong, and Chen 2022; Yang, Wen, and Li 2019; Yang et al. 2023) removes organized patterns within the network, enabling significant acceleration in a hardware-agnostic manner. For instance, Shortened-LLaMA (Kim et al. 2024) introduces a block pruning method to remove Transformer blocks, resulting in depth pruning. Sheared-LLaMA (Xia et al. 2023) incorporates a mask learning phase to identify prunable components across both the network's width and depth. LLM-Pruner (Ma, Fang, and Wang 2023) and SliceGPT (Ashkboos et al. 2024) eliminates coupled structures in the aspect of network width while retaining the number of layers: LLM-Pruner sparsifies the intermediate dimension while SliceGPT focuses on the hidden dimension. However, many existing structured pruning models still suffer from accuracy loss, necessitating further exploration and improvement.\nParameter-Efficient Fine-Tuning\nCompared to full fine-tuning of LLMs, Parameter-Efficient Fine-Tuning (PEFT) can achieve comparable performance while significantly reducing the computation and memory cost. PEFT methods can be broadly classified into five categories: additive fine-tuning, partial fine-tuning, reparameterized fine-tuning, hybrid fine-tuning, and unified fine-tuning. Additive fine-tuning methods introduce new additional parameters into the model, including adapter-based (Hu et al. 2021; Zhang et al. 2023b; He et al. 2021; R\u00fcckl\u00e9 et al. 2020) and soft prompt-based (Li and Liang 2021; Wang et al. 2023; Vu et al. 2021) approaches. For example, LoRA (Hu et al. 2021), one of the most popular used PEFT method, freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. DoRA (Liu et al. 2024a), a successful variant of LoRA, achieves enhanced performance by decomposing the pre-trained weights into magnitude and direction for subsequent fine-tuning. Partial fine-tuning selects only the parameters that are important for the downstream task to be trained (Ben-Zaken, Ravfogel, and Goldberg 2021; Lawton et al. 2023; Xu et al. 2021b). Reparameterized fine-tuning methods (Edalati et al. 2022; Zhang et al. 2023a; Xu et al. 2023b) often use low-rank transformations to reduce the number of trainable parameters. Hybrid fine-tuning (Zhou et al. 2023; Hu et al. 2022) combines multiple PEFT methods together. Unified fine-tuning (He et al. 2022; Wang et al. 2022) integrates various fine-tuning methods into a unified structure, but only utilizes one of them during fine-tuning. In this study, we mainly employ LoRA and DoRA as the fine-tuning techniques to explore our proposed PAT paradigm."}, {"title": "Methodology", "content": "In this section, we detail the components of our proposed Pruning-Aware Tuning (PAT). Firstly, we introduce the foundational concept of the zero-preservation property inherent in the RMSNorm operation. Subsequently, we elaborate on the Hybrid Sparsification Module (HSM) and the Unified Sparsification Mask (USM). Furthermore, we outline the comprehensive process of PAT and introduce the innovative Identity Loss (IL). Finally, we expound on the overall optimization objective.\nPreliminary: Zero-Preservation of RMSNorm\nRMSNorm (Zhang and Sennrich 2019), an abbreviation for root mean square layer normalization, is widely used in LLMs, such as Llama (Touvron et al. 2023b), Gemma (Team et al. 2024), and Yi (Young et al. 2024). The general form of the RMSNorm is defined as the following:\n$Z_{i} = RMSNorm(x_{i}) = \\frac{x_{i}}{RMS(x)}g_{i},$ (1)\nwhere $x_{i}$ is the i-th value of vector $x \\in R^{d}$, and $g \\in R^{d}$ is the gain parameter. RMS(\u00b7) is the Root Mean Square operation, defined as:\n$RMS(x) = \\sqrt{\\frac{1}{d}\\sum_{i=1}^{d} x_{i}^{2}}$ (2)\nGiven the layer input $X \\in R^{d\\times n}$ with specific (e.g., 1st and 2nd) channels all equal to 0 :\n$X = \\begin{pmatrix} 0 & 0 & ... & 0\\\\ 0 & 0 & ... & 0\\\\ x^{(1)}_{3} & x^{(2)}_{3} & ... & x^{(n)}_{3}\\\\ : & : & ... & :\\\\ x^{(1)}_{d} & x^{(2)}_{d} & ... & x^{(n)}_{d} \\end{pmatrix},$ (3)\nwhere $x^{(j)}_{i}$ is the j-th value of the i-th vector in X. Referring to Eq. (1), the RMSNorm operation will preserve these zero values, thereby making it feasible to prune the corresponding channels.\nHybrid Sparsification Module (HSM)\nOur objective is to prune the hidden dimensions of LLMs during fine-tuning, which would involve selecting the channels to be pruned in a linear layer, and convert the knowledge of pruned weights into those remained. To achieve this, we design a specific module to be applied after a linear layer, namely Hybrid Sparsification Module (HSM). HSM consists of a trainable channel selection mask M and a knowledge transformation weight D. Specifically, the computation involving the HSM and the upstream linear layer with weight $W\\in R^{d_{o}\\times d_{i}}$ is formulated as follows:\n$Z = (MD) \\cdot WX = (M DW) \\cdot X = W_{D}X,$ (4)\nwhere $d_{i}$ and $d_{o}$ are the input and output dimension, respectively, $X \\in R^{d_{i}\\times n}$ is the input value, $Z \\in R^{d_{o}\\times n}$ is the output value, $M \\in R^{d_{o}}$ denotes the trainable mask whose values converge to either 0 or 1, $D \\in R^{d_{o}\\times d_{o}}$ is the HSM weight, $W\\in R^{d_{o}\\times d_{i}}$ is the upstream linear weight, and $W_{D} \\in R^{d_{o}\\times d_{i}}$ is the merged weight that replaces W after training. Notably, the zero values in M effectively cause the corresponding output channels of $W_{D}$ to be pruned.\nTo prune all linear layers in LLMs such as Llama2, which encompass the Q, K, V, and O projections in Attentions, as well as Up, Gate, and Down projections in FFNs, a straightforward approach is to apply the HSM after all linear layers. However, considering the sheer number of the linear layers in an LLM, this approach would incur significant overhead. We propose a novel and efficient alternative: placing pruning modules only between the Attention and FFN components, as illustrated in Fig. 2. The \u201cpruned\u00b9\u201d HSM's output, Z, will first undergo the addition with the residual connection, which has already been pruned by the previous HSM, and then be fed into the RMSNorm operator before the next Attention/FFN component. As demonstrated previously in the preliminary, the RMSNorm has no impact on zero-valued channels, and since the downstream linear projection receives input with certain channels set to zero, the input dimensions of the following block can be pruned accordingly. In cases where LLMs involve the LayerNorm which projects zero-valued channels to non-zero, we can convert it to the RMSNorm before incorporating HSMs. This transformation is mathematically equivalent, as described by SliceGPT (Ashkboos et al. 2024).\nAlthough inserting HSMs between Attention and FFN components reduces trainable parameters compared to directly applying them to each linear module, the overall training overhead remains significantly larger than that of PEFT methods. To mitigate this issue, we propose the Hybrid-Identity-Operator (HIO) as a replacement for the dense structure of HSMs, which is formulated as:\n$D = L_{1}L_{0} + I,$ (5)\nwhere $L_{0} \\in R^{r\\times d_{o}}$, $L_{1} \\in R^{d_{o}\\times r}$, r is the rank value of $L_{1}L_{0}$, and $I \\in R^{d_{o}\\times d_{o}}$ is the identity matrix with diagonal values set to 1 and other values set to 0. During fine-tuning, I is frozen, allowing gradients to flow through $L_{0}$ and $L_{1}$. HIO significantly reduces the number of trainable parameters. For example, a dense HSM consists of $d_{o} \\times d_{o}$ parameters, while the HIO consists of 2 \u00d7 $d_{o}$ \u00d7 r. By determining $r < d_{o}/2$, we can decrease the number of trainable parameters. In practice, we set r to approximately 5% of d, which in turn only accounts for 10% parameter of dense HSMs.\nUnified Sparsification Mask (USM)\nWe utilize a single trainable mask M as in Eq. (4) to adaptively set channel values of hidden states to zero. The mask acts uniformly across all HSMs, ensuring consistency in the pruned channel indices throughout the computation flow. This unified pruning mask is particularly necessary at the residual connections between Attention and FFN components, as it guarantees that the pruned channels are correctly aligned throughout the entire data flow.\nTo insure structural sparsity at the convergence of the model, we employ a continuous sparsification strategy with a tailored regularizer to ensure that the mask converges to discrete values of 0 or 1 and achieves the desired sparsity at the end of the training process. This involves applying a differentiable gating function, G(\u00b7), to the trainable proxy weights WM of the mask. The gating function utilizes a modified Sigmoid function with a variable temperature \u0442, which is defined as:\n$\\tau(s) = \\begin{cases} 1 & \\text{if } s < s_{0}, \\\\ -\\frac{1}{ln(s)} & \\text{otherwise}. \\end{cases}$ (6)\n$\\beta(s) = \\begin{cases} -\\frac{s}{s_{0}} +0.5 & \\text{if } s < s_{0}/2, \\\\ 0 & \\text{otherwise}. \\end{cases}$ (7)\n$M = G(s, W_{M}) = \\frac{1}{1+ e^{-\\tau(s) \\cdot W_{M}}} + \\beta(s),$ (8)\nwhere s denotes the current training step which dynamically determines the temperature, so is the milestone step which indicates that the temperature stay unchanged in the remaining training steps. In practice, we set so to 1/3 of the total training steps. \u03b2(\u00b7) denotes the offset which varies according to the step. Fig. 3 demonstrates some typical training stages. Initially, when s = 0, the gating function maps all proxy weights of the mask to 1. This is achieved by initializing WM to zero, which keeps the model weights unchanged, ensuring stable gradients at the beginning. As the temperature increases, the slope near 0 rises, and the offset term decreases. By halfway to the milestone step, the offset term reaches 0 and stops updating, while the slope continues to increase. At the milestone step, the slope near 0 becomes very steep, while the slope elsewhere approaches 0. At this point, the mask values will be enforced to either 0 or 1, where 0 refers to the channel being pruned. Moreover, to achieve the target sparsity, specifically the proportion of values equal to 0, we propose regularizing the number of active channels. This is achieved through the following regularization term:\n$L_{active} = || N_{target} - \\sum 1(m_{i}>0)||_{2},$ (9)\nwhere Ntarget denotes the target channel number of active channels, mi represents the i-th value of the proxy weight M, and 1 (condition) is the indicator function that equals 1 if the condition is true, and 0 otherwise.\nPruning-Aware Tuning\nWe perform model fine-tuning by updating the proposed HSM modules and applying LoRA on all linear layers (Hu et al. 2021). Besides the standard instruction fine-tuning loss LInstruct, we propose the innovative Identity Loss (IL) to decompose the scaling and rotation in the HSM transformations. Specifically, we alter the formulation of Eq. (5) into:\n$D = L_{1} diag(v) \\cdot L_{0} + I,$ (10)\nwhere v \u2208 R\" is the trainable scaling values, and Lo and L\u2081 are constrained to be orthogonal with the identity regularization\n$L_{Identity} = ||L_{0}L_{0}^{T} - I||_{2} + ||L_{1}^{T}L_{1} - I||_{2}$ (11)\nThe overall optimization objective is defined by a composite loss function L, which is expressed as follows:\n$L = L_{Instruct} + L_{active} + L_{Identity},$ (12)\nwhere Linstruct represents the loss associated with instruction fine-tuning.\""}, {"title": "Experiments", "content": "In this section, we present the experimental results and analysis. We begin by describing the experimental setup. Next, we showcase our main results across various Language Models (LLMs). We then delve into the efficiency and accuracy trade-off, examining memory and latency considerations. Finally, we conduct ablation studies on the trainable mask and identity loss.\nExperimental Setup\nModels. We utilize model frameworks and checkpoints from HuggingFace (Jain 2022; Wolf et al. 2019), which includes Llama-2 7B and 13B (Touvron et al. 2023b), Gemma 2B and 7B (Team et al. 2024), Yi-1.5-34B (Young et al. 2024).\nBaselines. The pruning baselines include LLM-Pruner (Ma, Fang, and Wang 2023), and SliceGPT (Ashkboos et al. 2024). We also involve the common LORA (Hu et al. 2021) approach with the rank set to 64. Unless otherwise stated, we adjust the number of trainable parameters in all fine-tuning approaches to match the number of the LoRA. Additionally, we conduct complementary tests by applying \u201cP\u2192FT\u201d (Pruning before Fine-Tuning) and \"FT\u2192P\" (Fine-Tuning before Pruning) strategies on LLM-Pruner and SliceGPT. The pruning ratios are set to 20%, 25%, and 30%, respectively.\nDatasets. We employ the LaMini-instruction dataset (Wu et al. 2023) for fine-tuning. To reduce training costs, we randomly drop 50% of the samples, resulting in a final dataset of 1 million samples. Unless otherwise stated, all experimental results are based on this setting. We conduct zero-shot evaluation on 14 datasets, including ARC-Challenge (Clark et al. 2018), ARC-Easy (Clark et al. 2018), BOOLQ (Wang et al. 2019a), COPA (Wang et al. 2019a), HellaSwag (Zellers et al. 2019), MMLU (Hendrycks et al. 2021), MultiRC (Wang et al. 2019a), OpenBookQA (Mihaylov et al. 2018), PIQA (Bisk et al. 2020), RTE (Wang et al. 2019a), SIQA (Sap et al. 2019), WIC (Wang et al. 2019a), WinoGrande (ai2 2019), WSC (Wang et al. 2019a). The accuracy is calulated by First-Capital-Word\u00b2 (Contributors 2023) method.\nImplementation Details. Experiments are conducted using A100 GPUs. The models are fine-tuned over 3 epochs using the Alpaca instruction template. The learning rate is set to 5 x 10-5 with a cosine schedule. The batch size is set to 128, and the sequence length is 256 tokens. The milestone step of our PAT, so, is set to 1/3 of the total training steps. The settings of our HIOs are derived to match the number of trainable parameters with LoRA-64, as detailed in the Appendix.\nExperimental Results and Analysis"}, {"title": "Appendix", "content": "This section serves as supplementary materials to the main paper. We give details on the settings of Hybrid Identity Operator. We also provide detailed results of zero-shot evaluation.\nHIO Settings\nWe meticulously select our PAT settings in alignment with the corresponding LoRA configurations. The specific settings are detailed in Tab. 4. For example, in the case of Gemma 2B, when the traditional LoRA method employs a rank value of 64, our PAT approach adjusts this to a rank value of 20 for LoRA and 300 for HIO. This configuration results in a smaller proportion of trainable parameters in PAT compared to LoRA, while still maintaining model accuracy even at high pruning ratios. We test several HIO settings in the Llama2 7B model to discuss the efficiency and accuracy trade-off. We find that the small number of trainable parameters in HIO will lead to poor accuracy. In practice, we choose a rank value of 200 for HIO in Llama2 and Yi-1.5 models, and 300 for that in Gemma models. The rank value for LORA can be therefore determined accordingly.\nMain Results\nThe zero-shot evaluation results of LORA (Hu et al. 2021), LLM-Pruner (Ma, Fang, and Wang 2023), SliceGPT (Ashkboos et al. 2024) and our PAT are shown in Tabs. 5 to 9. The evaluation tasks consist of ARC-C (Clark et al. 2018), ARC-E (Clark et al. 2018), BOOLQ (Wang et al. 2019a), COPA (Wang et al. 2019a), HS (Zellers et al. 2019), MMLU (Hendrycks et al. 2021), MULTIRC (Wang et al. 2019a), OBQA (Mihaylov et al. 2018), PIQA (Bisk et al. 2020), RTE (Wang et al. 2019a), SIQA (Sap et al. 2019), WIC (Wang et al. 2019a), WG (ai2 2019), and WSC (Wang et al. 2019a). We highlight our results in the orange background.\nLlama2 We employ Llama2 7B and 13B models. As demonstrated in Tabs. 5 and 6, the Llama2 models exhibit a higher tolerance to pruning compared to other models. Notably, at a 30% pruning ratio, our PAT method retains 98% of the zero-shot accuracy when compared to the LoRA-64 approach. Interestingly, our PAT method at 20% and 25% pruning ratios even outperforms LoRA-64.\nGemma We employ Gemma 2B and 7B models. The Gemma 2B encounters significant accuracy degradation when reaches a pruning ratio of 30%. The Gemma 7B exhibits linear degradation according to pruning ratios."}]}