{"title": "Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset", "authors": ["Tilahun Abedissa Taffa", "Debayan Banerjee", "Yaregal Assabie", "Ricardo Usbeck"], "abstract": "Existing Scholarly Question Answering (QA) methods typically target homogeneous data sources, relying solely on either text or Knowledge Graphs (KGs). However, scholarly information often spans heterogeneous sources, necessitating the development of QA systems that can integrate information from multiple heterogeneous data sources. To address this challenge, we introduce Hybrid-SQUAD (Hybrid Scholarly Question Answering Dataset), a novel large-scale QA dataset designed to facilitate answering questions incorporating both text and KG facts. The dataset consists of 10.5K question-answer pairs generated by a large language model, leveraging the KGs DBLP and SemOpenAlex alongside corresponding text from Wikipedia. In addition, we propose a RAG-based baseline hybrid QA model, achieving an exact match score of 69.65% on the Hybrid-SQUAD test set.", "sections": [{"title": "Introduction", "content": "Question Answering (QA) systems take as input a natural language question and provide an answer from a predefined set of sources (Zhang et al., 2023). These sources may have structured data, as found in Knowledge Graphs (KGs), or unstructured data, such as text documents (Dimitrakis et al., 2020). To leverage the knowledge in both KG and text, hybrid QA has emerged (Lehmann et al., 2024). Hybrid QA requires information from KG and text sources to generate the final answer (Zhang et al., 2023). The hybrid QA approach broadens the retrieval pieces of evidence across multiple sources, resulting in superior answer coverage compared to single-sourced QA models (Feng et al., 2022).\nIn the context of Scholarly QA, existing models are designed to answer questions based solely on bibliographic metadata information found in a KG (Taffa and Usbeck, 2023; Auer et al., 2023; Jaradeh et al., 2020) or in text (Lee et al., 2023; Saikh et al., 2022). Additionally, the existing Scholarly QA data sets focus on dealing with homogeneous data, as listed in Table 1. However, as shown in Figure 1, information about scholarly entities like authors, publications, or institutions is distributed over heterogeneous sources. For instance, different facts about a scholar appear in the KGs - DBLP\u00b9 (Digital Bibliography & Library Project) (Ley, 2002) and SemOpenAlex2 (Semantic Open Alex) (F\u00e4rber et al., 2023), as well as in Wikipedia pages (See Figure 1). Correspondingly, a question like \"What is the main research focus of the author of 'Your System Is Secure? Prove It!'?\" is only answerable by searching facts in both text and KG.\nTherefore, to fill this gap and to foster the development of Scholarly hybrid QA, we create Hybrid-SQUAD (Hybrid Scholarly Question Answering Dataset) using an LLM (Large Languge Model). This new large-scale QA dataset requires looking at multiple data sources to provide an answer. Each question is aligned with DBLP and SemOpenAlex"}, {"title": "Related Works", "content": "Common approaches in creating QA datasets are crowdsourcing, automatic generation, and collecting question-answer pairs from community-based QA platforms, such as Quora and Stack Exchange (Dzendzik et al., 2021). In crowdsourcing, from a given context, crowd-workers formulate question and answer pairs using in-house annotation tools or crowdsourcing annotation platforms like Amazon Mechanical Turk (Chen et al., 2020). On one hand, crowdsourcing allows for the creation of high-quality question-answer pairs, but it depends on the skill level of the workers. Crowdsourcing also typically incurs significant costs, especially for large-scale datasets. On the other hand, auto-generation approaches utilize language generation models, templates, or machine translation for the question-answer pairs formulation (Dzendzik et al., 2021).\nAs shown in Table 1, DBLP-QuAD, SciQA, and QASA are each derived from a single source. In contrast to these scholarly QA benchmarks, Hybrid-SQUAD introduces questions that require integrating structured knowledge from KGs with contextual understanding from text sources. Regarding their creation methods, DBLP-QuAD and SciQA use templates, while QASA relies on human annotators. Hybrid-SQuAD, however, employs an LLM to generate question-answer pairs.\nConversely, CompMix (Christmann et al., 2024) is a non-scholarly heterogeneous dataset that employs crowd-sourcing to generate question-answer pairs. This dataset capitalizes on the repetition of facts across sources like Wikipedia, Wikidata KG, Wikipedia tables, and info-boxes, allowing questions to be answerable by one or more of these underlying sources. Notably, the questions in the CompMix test set do not require reasoning across multiple sources. In contrast, our work highlights the complementary nature of three distinct sources: DBLP, SemOpenAlex, and Wikipedia text. For example, the KGs DBLP and SemOpenAlex lack personal details about authors, such as career milestones or institutional affiliations, which are exclusively found on Wikipedia pages dedicated to the authors and their institutions. On the other hand, KGs provide scholarly metadata like publication counts, citation numbers, and h-index metrics. Therefore, the three data sources used to construct Hybrid-SQUAD complement each other by offering a diverse array of information rather than reiterating the same data.\nFurthermore, GPT-3 (Brown et al., 2020) attains 50% accuracy on the CompMix test set in a zero-shot setting, while ChatGPT-3.5 achieves only a 2.6% exact match score on the test questions from Hybrid-SQUAD. This stark contrast highlights that Hybrid-SQUAD presents significantly more challenging questions, even for advanced LLMs."}, {"title": "Scholarly Hybrid QA", "content": "Contri(e)ve (Shivashankar and Steinmetz, 2024) presents a methodology that combines context extraction with prompt engineering. The context extraction process involves three stages: obtaining author information from the DBLP, retrieving data from SemOpenAlex using ORCID identifiers, and collecting additional details from Wikipedia. The collected data is refined to retain only relevant sentences with specific keywords, tackling issues associated with lengthy prompts that may degrade system performance. The subsequent prompt engineering phase organizes the prompts into four components-Instructions, Query, Context, and Output Indicator-ensuring they are concise yet informative for precise inference. Finally, the refined prompt is passed to the LLAMA3.1 8b-Instruct model to obtain the answer.\nSimilarly, Efeoglu et al. 2024 extract triples from DBLP and SemOpenAlex, as well as relevant text from the Wikipedia corpus. They utilize an algorithm to create a context for each question by identifying pertinent triples and sentences. The relevance of these triples and sentences is determined through cosine similarity using SBERT embeddings. This method allows for the selection of the most significant evidence by retaining only the essential components. The resulting evidence-matched context is then used to fine-tune the Flan-T5-Large model in a supervised setting, enabling it to effectively function as an answer extractor.\nLikewise, Fondi and Jiomekong 2024 collect data from all the three sources and structure it to facilitate pattern detection among queries through alphabetical ordering. They employ a divide-and-conquer strategy to group questions based on author identifiers and manually assigned topics. Ultimately, they generate context-specific predictions as answers using the BERT-base-cased-squad2 language model.\nContrary to these methodologies, we identify sub-question phrases, resolve the scholarly entities they contain, and replace each phrase with its corresponding entity. Subsequently search relevant evidence in KGs and textual sources. Finally, utilize a RAG (Retrieval-Augmented Generation) (Lewis et al., 2020) model to generate an answer."}, {"title": "The Dataset", "content": "This section outlines the process for collecting triples from the KGs along with their associated passages, followed by a description of the question generation method. Additionally, it provides various statistics related to Hybrid-SQuAD."}, {"title": "Data Collection", "content": "Our process for creating questions across multiple data sources starts by downloading the DBLP RDF dump. This choice is based on DBLP's focus on Computer Science publications, its manageable size, its well-defined schema, and the inclusion of Wikipedia URLs for authors, simplifying text retrieval from Wikipedia. We also use SemOpenAlex (F\u00e4rber et al., 2023), specifically downloading only the Authors and Institutions dump files due to the large size of the Publications file. Although SemOpenAlex lacks authors' Wikidata IDs or Wikipedia URLs, making Wikipedia page retrieval challenging, it provides valuable statistical information that complements DBLP data.\nWe crawl Wikipedia to collect textual data on authors and their institutions using the URLs from DBLP and SemOpenAlex, ensuring a comprehensive dataset. The data collection steps are:\nStep 1: Retrieval of DBLP Authors and their Information\nRetrieve authors who have a Wikipedia URI and an ORCID (Open Researcher and Contributor ID), along with their names and primary affiliations (see SPARQL-1 in Appendix A).\nFor each author extract publications (see SPARQL-2 in Appendix A).\nUse the authors' Wikipedia_uri to extract the corresponding Wikipedia text, then clean the text by utilizing an HTML and XML parser Python library - BeautifulSoup to remove HTML tags, references, extra spaces, and links."}, {"title": "Question-Answer Pair Generation", "content": "The question-answer pair generation involves three main steps: context preparation, prompt construction, and generation."}, {"title": "Context Preparation", "content": "For bridging questions over both DBLP & SemOpenAlex (KG-KG) or a combination of a KG and text (KG-Text), a record of an author is selected from the pool, along with its DBLP facts (as source 1) and the corresponding SemOpenAlex facts (as source 2). In the case of KG-Text questions, the first source is taken from DBLP or SemOpenAlex records, and the textual information of the DBLP entity taken from Wikipedia becomes the second source. Unlike bridging questions, for KG-KG comparison questions,, the data source pool is split into two distinct lists. A DBLP entity and its corresponding SemOpenAlex facts are chosen from one half and set as the first source. Another entity DBLP record is selected from the other half, along with SemOpenAlex data, and assigned as a second source. The first source for questions involving KG-KG-Text inference is an entity's DBLP records. The second source comprises the corresponding SemOpenAlex records, and the third is the textual information."}, {"title": "Question Generation", "content": "In the question generation, we first construct prompts that include toy data, instructions, examples, and data about an entity from two sources (see Listing 1). We then use ChatGPT-3.5 to process the prompts and format the generated question-answer pairs into JSON format. During a random check on the question-answer pairs, we found instances where some questions either lacked an answer or had responses consisting of only a single letter. The issue arose because we initially provided the entire text and the entity KG facts to the LLM in one go, resulting in truncated answers. We re-prompt the LLM to address this by supplying the question and its context. Then, replace the incomplete answers with these new, more complete responses. Finally, as shown in Appendix B, a single comprehensive JSON file compiles all 10,581 generated question-answer pairs and their unique IDs, Author DBLP URIs, source types, and question types."}, {"title": "Dataset Analysis", "content": "Our manual analysis of 100 randomly selected questions, presented in Table 2, revealed that 31 answers fall under the 'bibliometric numbers' category. These questions typically inquire about bibliometric metrics, including publication counts, citation numbers, the h-index, and the i10-index. The category of biographical information encompasses 21 answers that provide insight into a scholar's educational background, professional experience, and academic achievements. Furthermore, 19 questions are classified under the category of 'Organization', which includes the names of Universities, research centers, or laboratories. Answers about places are classified under the category of \u2018location\u2019 (11 questions). In comparison, those providing specific dates (8 questions) offer contextual information such as the establishment year of an institute or dates of notable scholarly events. Additionally, questions about research outputs and publications are placed under the 'research works' category, including seven questions highlighting inquiries into scholarly contributions. The remaining seven questions"}, {"title": "Question Evidence Traversal Paths", "content": "Table 3 analyzes the 700 test set questions in Hybrid-SQuAD, highlighting four paths for evidence traversal used in the test set questions. The analysis shows a significant reliance on the KG -> Text pathway, utilized in 55.55% of the questions. This highlights the need to effectively integrate KG data with textual information to generate accurate answers. The distribution of the remaining questions emphasizes the direct and comparative evidence traversal: KG -> KG (Bridge) accounts for 23.93%, KG -> KG -> Text for 10.54%, and KG -> KG (Comparison) for 9.97%. This distribution illustrates how most questions leverage structured and unstructured data, combining elements from KGs and texts to formulate comprehensive answers."}, {"title": "Baseline Model", "content": "As shown in Figure 2, the baseline model for answering questions comprises three phases: link, retrieve, and generate. The model identifies the bridging entity that connects various data sources in the link phase. This ensures that the different pieces of information are properly aligned by recognizing scholarly entities such as publications, author names, or institutions. Once a bridging entity is identified, the retriever module searches for relevant data about an entity from text sources and KGs. The retrieval phase allows comprehensive information from KG and text to be gathered. Finally, in the generate phase, the model fuses the heterogeneous inputs obtained from the retriever in the LLM and generates an answer."}, {"title": "Linking", "content": "The questions in Hybrid-SQuAD necessitate searching for evidence across multiple data sources and often include embedded sub-questions within the main query. For example, in the question, \"What is the main research focus of the author of \u2018Your System Is Secure? Prove It!'?\u201d, the phrase in italics represents the sub-question. Answering such queries requires the identification of these sub-question phrases and the scholarly entities involved, such as the publication titled 'Your System Is Secure? Prove It!' and the resolution of the author entity. These sub-question phrases and entities bridge different data sources, facilitating a comprehensive understanding and integration of information.\nThe sub-question phrase identification sub-component forms a prompt with a few examples and then prompts ChatGPT-3.5 to extract the sub-question. Subsequently, the scholarly entity identifier prompts the LLM to identify scholarly entities within the sub-question phrase, such as a publication, author, or institution. Once these scholarly entities are identified, the entity linker employs a SPARQL template to query DBLP, retrieving the relevant URLs and labels for these entities. If the entity linker successfully determines the label of the bridging entity, it updates the question by replacing the sub-question phrase with the resolved entity. This process is repeated recursively if multiple sub-question phrases exist, ensuring the question is thoroughly updated. The updated question, accompanied by the entity labels, is then forwarded to the retriever component. When the sub-question phrase and entity are not identifiable, the entity linker uses the author URI provided with the question to resolve the bridging entity. This rigorous linking process guarantees that the question is accurately transformed and enriched with precise entity information before being passed on to the retriever for context extraction."}, {"title": "Retrieve", "content": "The retrieval stage integrates both text search and KG query functionalities. When the evidence pathway involves KG-Text or KG-KG-Text sequences, the process begins with a text search. This search consists of extracting Wikipedia text related to the author or institution entity by utilizing the Wikipedia URL provided by the entity linker. The extracted text is parsed using BeautifulSoup, resulting in a plain text excerpt. In situations requiring a KG-KG, the KG query sub-component uses a SPARQL template from SemOpenAlex to gather all pertinent facts about the entity. This is achieved by leveraging the author's ORCID as a linkage criterion between DBLP and SemOpenAlex. The gathered texts, KG details, and the updated query are ultimately fed into the answer generation stage to produce the final response."}, {"title": "Generate", "content": "The answer generation process for questions involving KG-Text or KG-KG-Text evidence pathways leverages RAG, combining document retrieval with LLM to ensure precise responses. The RAG model loads the text, divides it into 200-word chunks with a 10-word overlap, generates embeddings using the 'BAAI/bge-small-en-v1.5\u2019 model, and stores these embeddings in a FAISS (Douze et al., 2024) vector store for efficient retrieval. The FAISS vector store retrieves top-5 relevant text chunks using the updated question as a query. The ChatGPT-3.5 model then processes these chunks alongside the KG triple labels to generate an answer. In contrast, for KG-KG evidence pathways, the LLM only receives the updated question and KG triple labels."}, {"title": "Evaluation", "content": "To evaluate the performance of our baseline model, we use Exact Match (EM) evaluation metrics that assess the proportion of predictions that match the gold answers, reflecting the model's ability to produce precise outputs. We also use F-Score evaluation metrics.\nAs shown in Table 4, the fine-tuned Flan-T5 Large by (Efeoglu et al., 2024) and Contri(e)ve (Shivashankar and Steinmetz, 2024) achieve an EM score of 48.9 and 32.0, respectively. In contrast, our baseline models demonstrate superior performance, with the RAG-based approach using ChatGPT-3.5 achieving the highest EM score of 69.65 and 74.91 F-score, underscoring the effectiveness of RAG strategies in enhancing accuracy through contextual information. The RAG-based model using LLAMA-3-8B also performs well with an EM and F-score of 61.1 & 68.92 respectively, significantly outperforming the other models and reinforcing the benefits of integrating retrieval mechanisms into language model frameworks."}, {"title": "Summary", "content": "This paper introduces Hybrid-SQuAD, a large-scale hybrid Scholarly QA dataset. Hybrid-SQUAD contains questions that need multiple data sources to be able to answer. Current LLMs, such as ChatGPT-3.5, perform poorly on this dataset, with results in the range of 3% accuracy, while a baseline QA system achieves 69.65 exact match and 74.91 F-Score. Additionally, hope our novel benchmark sparks further research on Scholarly hybrid QA."}]}