{"title": "Language Models Show Stable Value Orientations Across Diverse Role-Plays", "authors": ["Bruce W. Lee", "Yeongheon Lee", "Hyunsoo Cho"], "abstract": "We demonstrate that large language models (LLMs) exhibit consistent value orientations despite adopting diverse personas, revealing a persistent inertia in their responses that remains stable across the variety of roles they are prompted to assume. To systematically explore this phenomenon, we introduce the role-play-at-scale methodology, which involves prompting LLMs with randomized, diverse personas and analyzing the macroscopic trend of their responses. Unlike previous works that simply feed these questions to LLMs as if testing human subjects, our role-play-at-scale methodology diagnoses inherent tendencies in a systematic and scalable manner by: (1) prompting the model to act in different random personas and (2) asking the same question multiple times for each random persona. This approach reveals consistent patterns in LLM responses across diverse role-play scenarios, indicating deeply encoded inherent tendencies. Our findings contribute to the discourse on value alignment in foundation models and demonstrate the efficacy of role-play-at-scale as a diagnostic tool for uncovering encoded biases in LLMs. Our research code is at github.com/brucewlee/moral-value-bias.", "sections": [{"title": "1 Introduction", "content": "Recent advances in large language models (LLMs) have significantly enhanced their capabilities and applications in real-world scenarios, making them increasingly integral to our daily lives. However, one of the primary concerns of these models is their non-deterministic nature, which allows them to generate a variety of responses to the same input. This feature is particularly evident when responding to questions where multiple valid answers exist. For example, the question \u201cWhat are the benefits of democracy?\" can elicit different answers reflecting economic, political, and social dimensions (Kova\u010d et al., 2024). Such variability stems from the vast and heterogeneous datasets LLMs consume, enabling them to operate like mixture models that capture complex probability distributions for a single topic and encompass a multitude of viewpoints and opinions. Consequently, subtle nuances in the context or wording may yield different outcomes from a similar query (Ceron et al., 2024).\nPrompting and Model Inertia. To harness this characteristic effectively, users often employ prompting - an empirical method where the input to the model is carefully crafted to steer the output toward a desired direction (Louie et al., 2024; Magee et al., 2024; Ng et al., 2024; Stampfl et al., 2024; Tamoyan et al., 2024). Although the internal mechanisms of prompting remain opaque, users iteratively refine their prompts based on the responses, gradually guiding the model toward the preferred outcome.\nDespite the flexibility of LLMs in generating diverse responses, there is a tendency among these models to exhibit a sort of inertia. This inertia refers to a preference for specific phrasings, tones, or even content types, which could be seen as a central tendency within the model's outputs. For instance, LLMs often exhibit a consistent preference in their own responses Panickssery et al. (2024) and specific choices of languages or a tendency towards certain cultural perspectives (Adilazuarda et al.,"}, {"title": "2 Role-Play-at-Scale", "content": "One central motivation behind this research is to explore the largely uncharted territory of macroscopic LLM behavior under an overly random and diverse role-play setup, which differs from the more common approach of using role-play with a specific intention to steer LLM behavior in a certain direction (Shao et al., 2023; Wang et al., 2023a; Xu et al., 2024). While role-playing with a specific goal to elicit certain behaviors and evaluating the results is a valuable technique, it does not provide insights into the macroscopic trends that emerge when LLMs are subjected to diverse and randomized role-play scenarios repeatedly.\nTo address this gap in understanding, we propose the role-play-at-scale method for detecting inherent tendencies and inertia in LLMs. This approach involves engaging LLMs in a wide range of persona-driven interactions, where the models are prompted to respond from the perspective of randomly generated persons with diverse demographic backgrounds. These personas have random and diverse attributes, as also depicted in Table 1. By analyzing the consistency of the models' responses across these varied scenarios, we aim to identify stable tendencies and inertia that are inherent to the LLMs themselves rather than being influenced by any specific persona or situation."}, {"title": "3 Q1: Is there a response trend when exposed to repeated and randomized role-playing setups?", "content": "As an application of the role-play-at-scale framework, we present the experiments conducted on the Revised Portrait Values Questionnaire (PVQ-RR) (Schwartz et al., 2012) and Moral Foundations Questionnaire (MFQ-30) (Graham et al., 2008). The choice of these questionnaires is intentional, as both are directly ported from human-subject psychology tests and have been the subject of much debate in the field regarding the validity of such practices when applied to LLMs (Kova\u010d et al., 2023; Lu et al., 2024a).\nOur understanding of the main contested points is that: (A) the stability of the induced LLM response direction and (B) the validity of trusting LLM self-reports, which may be subject to AI deception, a widely reported phenomenon in the AI safety community (Lin et al., 2021; Park et al., 2024; Perez et al., 2022; Turpin et al., 2024). However, role-play-at-scale is designed to be more robust to the issue (B) by aggregating statistics rather than relying on individual LLM responses. In this section, we aim to demonstrate that (A) is achievable through our experiments.\nOur role-play prompt consists of two components: Question and Persona (Prompt Template A). This template instructs the language model to role-play as a specific person and answer a question from that perspective. It includes placeholders for {Persona} and {Question} and specifies that the response should always point to a specific letter option. The Question component is derived from two well-established psychological questionnaires: PVQ-RR, a 57-item questionnaire measuring ten basic value characteristics, and MFQ-30, a 32-item questionnaire assessing five foundational moral dimensions (Graham et al., 2008; Schwartz, 2012). Using these questionnaires does not imply that LLMs are equivalent to individual humans; rather, they serve as a means to characterize and understand these models' behavior and inherent tendencies in a more interpretable way."}, {"title": "4 Q2: Does the preference persist through different persona sets?", "content": "Having established the presence of consistent response trends in LLMs exposed to repeated and randomized role-playing setups, we now turn our attention to the second research question: Does the uncovered preference persist through different persona sets? This question is crucial for assessing the robustness and reliability of the role-play-at-scale method in uncovering stable tendencies within LLMs.\nIf the observed preferences remain consistent across multiple, distinct sets of randomly generated personas, it will provide strong evidence for the existence of inherent tendencies within the LLMs that"}, {"title": "5 Related Work", "content": "Human Values, though not universally defined, drive individual behavior and are key in comparative cultural studies. Schwartz's Theory of Basic Human Values Schwartz (2012) is particularly influential, proposing ten universal value types. The Moral Foundations Questionnaire assesses moral values based on five key dimensions: Harm, Fairness, Ingroup, Authority, and Purity (Graham et al., 2008). This tool helps measure how individuals prioritize these dimensions, offering insights into their moral reasoning (Borenstein et al., 2024; Osman and d'Inverno, 2024).\nEvaluation of LLMs with Human Values. As LLMs evolve, assessing them through human value systems is gaining attention (Ji et al., 2024; Qiu et al., 2024; Tlaie, 2024). This research area bridges human values and machine learning, evaluating LLMs' alignment with ethical frameworks. Santy et al. (2023) explore cultural tendencies in LLMs, Cao et al. (2023) use the Hofstede Culture Survey Hofstede (1984) to examine cultural patterns, and Abdulhai et al. (2023) apply traditional ethical frameworks Graham et al. (2008); Shweder et al. (2013) to probe moral alignments. Challenges remain, such as the 'agreeableness bias' discussed by Dorner et al. (2023), and variability in responses due to prompt phrasing highlighted by Gupta et al. (2023). These underscore the need for LLM-specific frameworks for accurate value alignment assessments (Biedma et al., 2024; Yao et al., 2024).\nRole-Play in LLMs. LLMs can mimic complex characteristics, leading to research on role-play simulations. Wang et al. (2023b) introduced a dataset with prompts for 100 diverse characters, and Zhou et al. (2023) created a large corpus of human-annotated role-playing data. Our research examines LLMs' consistency in character traits across scenarios, highlighting inherent tendencies and variations."}, {"title": "6 Limitations and Discussion", "content": "In this section, we discuss some limitations in our work that we identified. One limitation is the absence of human baseline data in our study. We acknowledge that comparing LLM responses to human responses could provide additional context for interpreting our results. However, our study focuses on uncovering inherent tendencies within LLMs rather than comparing them to human norms. The rationale for not including human baseline comparisons is rooted in the fundamental differences between human cognition and LLM processing (Johnson and Verdicchio, 2024). LLMs, unlike humans, do not have personal experiences, emotions, or cultural backgrounds that shape their responses (Lee and Lim, 2024; Zimmerman et al., 2024). Instead, their outputs are based on patterns learned from vast amounts of training data. Therefore, direct comparisons between LLM and human responses may not always be meaningful or appropriate.\nAnother limitation of our methodology is its reliance on multiple-choice questionnaires. While these questionnaires are well-established in psychological research and provide a structured way to assess values and moral foundations, they may not capture more nuanced or complex aspects of ethical reasoning and decision-making. The use of pre-defined options in multiple-choice questions may limit the range of responses and potentially miss subtle variations in LLM outputs. This limitation is particularly relevant when considering more nuanced types of tendencies that may not be easily categorized into discrete options.\nLastly, it's important to note that these findings may not fully reflect LLM behavior in real-world applications. While necessary for systematic analysis, the controlled nature of our experiments may not capture the full complexity of real-world interactions and contexts. Future research could focus on bridging this gap by conducting studies in more naturalistic settings or with real-world data, exploring how the observed tendencies manifest in different types of tasks."}, {"title": "7 Conclusion", "content": "Our research introduces a novel role-play-at-scale framework to uncover stable tendencies within LLMs. By systematically role-playing LLMs with diverse, randomized personas while responding to psychological questionnaires, we demonstrate consistent value and moral preferences that persist across various contexts. These tendencies appear deeply rooted in the models, not artifacts of specific scenarios.\nWhile our study has limitations, as discussed in the previous section, we believe that it opens up new avenues for research into AI behavior and ethics. Future work should address these limitations, expand the methodology to capture more nuanced tendencies and explore the implications of these findings for real-world AI applications."}]}