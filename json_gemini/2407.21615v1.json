{"title": "BETWEEN THE AI AND ME: ANALYSING LISTENERS' PERSPECTIVES\nON AI- AND HUMAN-COMPOSED PROGRESSIVE METAL MUSIC", "authors": ["Pedro Sarmento", "Jackson Loth", "Mathieu Barthet"], "abstract": "Generative AI models have recently blossomed, signifi-\ncantly impacting artistic and musical traditions. Research\ninvestigating how humans interact with and deem these\nmodels is therefore crucial. Through a listening and reflec-\ntion study, we explore participants' perspectives on AI- vs\nhuman-generated progressive metal, in symbolic format,\nusing rock music as a control group. AI-generated exam-\nples were produced by ProgGP [1], a Transformer-based\nmodel. We propose a mixed methods approach to assess\nthe effects of generation type (human vs. AI), genre (pro-\ngressive metal vs. rock), and curation process (random\nvs. cherry-picked). This combines quantitative feedback\non genre congruence, preference, creativity, consistency,\nplayability, humanness, and repeatability, and qualitative\nfeedback to provide insights into listeners' experiences. A\ntotal of 32 progressive metal fans completed the study. Our\nfindings validate the use of fine-tuning to achieve genre-\nspecific specialization in AI music generation, as listeners\ncould distinguish between AI-generated rock and progres-\nsive metal. Despite some AI-generated excerpts receiving\nsimilar ratings to human music, listeners exhibited a pref-\nerence for human compositions. Thematic analysis identi-\nfied key features for genre and AI vs. human distinctions.\nFinally, we consider the ethical implications of our work in\npromoting musical data diversity within MIR research by\nfocusing on an under-explored genre.", "sections": [{"title": "1. INTRODUCTION", "content": "Recently, advancements in AI have resulted in genera-\ntive models capable of creating remarkable musical pieces.\nThis has been particularly evident in the audio domain,\nwith models such as Jukebox (OpenAI) [2], MusicLM\n(Google) [3], AudioCraft/MusicGen (Meta) [4] and Sta-\nble Audio (Stability AI) [5], to name a few. In contrast\nto audio generative models, which can produce complete,\ndirectly perceivable music with limited user input beyond\nspecifying a prompt, symbolic music generation methods\nyield outputs that necessitate subsequent decoding and in-\nterpretation by performers and mixing by audio engineers\nbefore transforming them into music suitable for listening\nexperiences. Unless automated using synthesis and ma-\nchine mixing, by requiring human interpretation through\nperformance and mixing, symbolic music rendering opens\nthe door for the infusion of cultural and social elements.\nThese elements become integral aspects of the final musi-\ncal experience for listeners.\nOne major controversy surrounding AI music genera-\ntion models is their training on copyrighted data, often\nwithout consent nor royalty mechanisms. This raises con-\ncerns that Al-generated music could threaten artists' and\nmusicians' income streams, amongst others [6]. These is-\nsues apply to both symbolic and audio AI music genera-\ntion. However, symbolic approaches may pose less risk\nto artists' revenue streams since human musicians are still\nessential to the final product.\nIn this work, we focus on symbolic generative AI ap-\nplied to progressive metal, which is considered a sub-genre\nof metal. Building on progressive rock's complex phras-\ning and odd time signatures, it incorporates a heavier focus\non guitars and metal influences. The genre encompasses\nprominent bands such as Dream Theater, Between The\nBuried And Me \u00b9, and Meshuggah [7]. It is, however, rela-\ntively unexplored in academic literature, particularly in the\ncontext of AI music generation and MIR research [8] [9].\nGuitar tablature (see Figure 1) is a symbolic musical no-\ntation that translates guitar notes into fret and string num-\nbers. Due to the genre's emphasis on guitar, progressive\nmetal bands commonly use tablature to notate their com-\npositions. Given that technical complexity is a large appeal\nof the genre, artists often sell their music in the form of tab-\nlatures for learning purposes through tablature publishing\ncompanies \u00b2. Musicians from within the genre often use\ndigital representations of tablatures and software like Gui-\ntar Pro \u00b3 for dissemination of musical ideas and computer-\nassisted music making.\nWe conducted a listening and reflection study to ex-\nplore participants' perceptions of AI-generated progressive\nmetal music. We used examples generated by ProgGP [1],"}, {"title": "2. BACKGROUND", "content": "2.1 Symbolic Music Generation\nMusic generation has seen an increase in popularity due\nto recent advances in deep learning [10], with many re-\nsearchers utilizing techniques such as Recurrent Neural\nNetworks (RNNs) [11] [12], Variational Autoencoders\n(VAEs) [13], Generative Adversarial Networks (GANs)\n[14], and Transformers [15]. The Transformer model [16],\nknown for its performance in natural language processing\n(NLP) tasks, has been adapted for generating symbolic pi-\nano music in Huang et al.'s Music Transformer [15], with\nMusenet [17] and Pop Music Transformer [18] further im-\nproving the approach.\nThe field of guitar tablature generation gained signif-\nicant momentum with the release of the DadaGP dataset\n[19]. This dataset provides songs in two formats: Gui-\ntarPro, a popular tablature editing software, and a ded-\nicated textual token format. This allows researchers to\ndevelop AI models that can both represent and generate\nmusic in tablature format. GTR-CTRL [20] implements a\nTransformer-based model [18] for generating tablature that\nincorporates multiple instruments. It offers control over\ninstrumentation (inst-CTRL) and musical genre (genre-\nCTRL). ProgGP [1], the model used in this study, focuses\nspecifically on the progressive metal genre (see Figure 1\nand description in Section 3). LooperGP [21] adapts the\nmethod to generate loopable music excerpts, making it\napplicable e.g. for live coding performances. By fine-\ntuning the model on the music of four iconic guitar players,\nShredGP [22] demonstrates its ability to replicate specific\nstyles.\n2.2 Subjective Evaluation of AI-Generated Music\nObjective computational measures can provide an initial\nassessment of AI-generated music quality [23]. However,\noften they struggle to capture the subtleties needed to judge\ntheir aesthetic merit. The combination of objective compu-\ntational measures with subjective human evaluations pro-\nvides a more holistic understanding of AI-generated mu-\nsic. Listening tests typically involve ranking or scoring AI-\nand human-generated stimuli according to several metrics\nto gain a more comprehensive understanding of perceived\nquality. This often involves comparing outputs from dif-\nferent models with the established reference (the known\nideal or benchmark). Metrics used to assess Al music\nvary from general attributes such as musicality [15], lik-\ning [24] [25] [18] [26], pleasantness [27], richness [28], to\nmore specific qualities such as consistency [29], or struc-\ntural/stability properties [29]. Whereas ranking involves\nsorting the different stimuli along a given dimension, scor-\ning tasks commonly rely on 5- or 7-point Likert items [30]\n[31]. A musical Turing test, similar to the original Tur-\ning test, is designed to assess a machine's ability to exhibit\nhuman-level musical creation features. In such tests, par-\nticipants attempt to distinguish between human- and AI-\ngenerated music [32]. To assess AI-generated music, we\nemploy a mixed methods approach, combining quantita-\ntive and qualitative data from a listening and reflection\nstudy. This approach, common in music perception and\nHCI research (e.g. [33]), allows for a deeper understand-\ning of the problem. While listening tests enable us to better\nunderstand human perception of AI-generated music, they\nare not without limitations. These limitations include lis-\ntener fatigue, potential biases due to stimuli or participant\nselection. Additionally, they may lack sufficient statistical\npower to generalize the findings to a broader population."}, {"title": "3. METHODOLOGY", "content": "We used a mixed methods listening and reflective study to\nassess AI music, with an ethical approval from the Queen\nMary Ethics of Research Committee. All data was col-\nlected anonymously. The study took around 1h to com-\nplete and participants were compensated with a \u00a310 Ama-\nzon voucher.\nWe evaluated Al-generated progressive metal music\nfrom the ProgGP model [1], a Transformer-XL model\ntrained on the DadaGP dataset [19] and fine-tuned on\na progressive metal corpus, by comparing it to human-\ncomposed progressive metal pieces. We compare two\nways of choosing AI music: picking songs at random\nand subjectively choosing the \u201cbest\u201d ones (cherry-picking)\nthrough active listening. Additionally, we compare the\nProgGP model's outputs with rock music generated by the\ngenre-CTRL model [20], a similar model conditioned on\nthe rock genre. Human-composed rock music serves as\nanother control group in this comparison."}, {"title": "3.1 Hypotheses", "content": "Our study tests the following hypotheses:\n\u2022 H\u2081: Human-composed music obtains better\nscores than AI-generated music. We compare AI-\nand human-generated music along the following di-\nmensions: preference, creativity, consistency, playa-\nbility and repeatability.\n\u2022 H2: AI-generated and human-composed music\ncan be distinguished. This hypothesis is linked to\nthe musical Turing test.\n\u2022 H3: AI-generated music matches the genre used\nfor model conditioning. The ability of the model to\nspecialize in a specific genre (progressive metal).\n\u2022 H4: Cherry-picked AI-generated music is pre-\nferred to randomly chosen AI-generated music.\nWe hypothesize that picking examples by hand leads\nto better performance than random selection."}, {"title": "3.2 Stimuli", "content": "The stimuli were rendered using Guitar Pro 7, a soft-\nware for playing/editing digital guitar tablatures. The\nhuman-composed music was obtained using publicly avail-\nable transcriptions of progressive metal and rock songs\nhosted on Songsterr4, a website hosting Guitar Pro tab-\nlatures, as well as from the DadaGP dataset [19]. All\nthe examples were trimmed to 15 seconds, and rendered\nas WAV files using the default virtual instruments in\nGuitar Pro 7. They were further loudness-normalized\n[34]. The study comprised 60 stimulis broken down\ninto the following six groups with 10 examples per\ngroup: progcp (progressive metal examples generated us-\ning ProgGP cherry-picked), progrand (progressive metal\nexamples generated with ProgGP, randomly selected),\nproghum (progressive metal examples from the dataset\nused to fine-tune ProgGP, human-generated, randomly se-\nlected), rockcp (rock examples generated using genre-\nCTRL prompted for rock, cherry-picked), rockrand (rock\nexamples generated using genre-CTRL prompted for rock,\nrandomly selected), and rockhum (from rock examples in\nthe dataset used for genre-CTRL, human-generated, ran-\ndomly selected). The AI-generated stimuli were selected\nout of a corpus of 200 compositions from each genre."}, {"title": "3.3 Participants", "content": "We recruited participants familiar with progressive metal\nas we wanted to involve domain experts capable of iden-\ntifying differences between rock and progressive metal.\nTo this end, we advertised the call for participants on the\nr/progmetal sub-forum from the Reddit platform. This\ncommunity comprises progressive metal aficionados 6. 26\nparticipants were gathered from this forum. We recruited\nsix additional progressive metal fans within our depart-\nment, for a total of 32 participants. Their age distribution"}, {"title": "3.4 Procedure", "content": "Participants first went through a familiarization stage con-\ntaining two excerpts, followed by the main task during\nwhich musical excerpts were presented in random order\nto minimise potential order effects. Participants were in-\nstructed to focus on the quality of the composition and\nnot on the quality of the virtual instruments or the mu-\nsic production mix. For each excerpt, participants had to\nlisten to the stimulus, report their familiarity, and answer\nthe following questions using 7-point Likert items: Q1\n(\"This composition features the qualities of the progressive\nmetal genre.\"), Q2 (\"This composition features the quali-\nties of the rock genre.\"), Q3 (\"I like this composition.\"),\nQ4 (\"This composition is creative.\u201d), Q5 (\u201cThis composi-\ntion is consistent.", "This composition is playable.\"),\nQ7 (\\\"This composition was generated using AI.\") and Q8\n(\\\"This composition is repetitive.\"). Once the participants\nfinished rating all excerpts, they were presented with a\npost-task questionnaire to assess their reasoning when dis-\ntinguishing between genres as well as between AI- and\nhuman-composed excerpts (see Section 4.2).\"\n    },\n    {\n      \"title\"": "4. RESULTS"}, {"content": "4.1 Listening Test\nWe visualize Likert item answers using violin plots in Fig-\nure 2. We conducted statistical analyses investigating the\neffects of the music creation process (six levels: progcp,\nprogrand, proghum, rockcp, rockrand, rockhum) on\nseveral dependent variables (preference, creativity, consis-\ntency, playability, repeatability, humanness, genre congru-\nency, and AI curation method, where relevant). Because\nwe employed a within-participant design with repeated\nmeasures, and the collected data are ordinal, we used the\nnon-parametric Friedman test. We use a Type I error a of\n0.05; results are presented in Table 1. The Friedman test\nwas followed by post-hoc pairwise Wilcoxon tests, using\na Bonferroni-adjusted a level of .0033 (.05/15). This en-\nables us to compare two generation types (AI vs. human),\ntwo genres (progressive metal vs. rock), and two AI se-\nlection methods (random vs. cherry-picked). Results are\npresented in Table 2. For question about AI-generated mu-\nsic (Q7), we excluded responses (345 out of 1,920) where\nparticipants indicated prior song familiarity.\n4.2 Thematic Analysis\nWe performed a thematic analysis [36] of answers to post-\ntask questions to better understand the thought process of\nparticipants' decisions during the study. Multiple themes\nwere obtained from the responses, and results are ordered\nby number of codes within each theme (in parentheses next\nto each theme, indicating number of occurences)."}, {"title": "4.2.1 What features made you identify excerpts as\nprogressive metal?", "content": "Complexity (40): A huge emphasis was put on the com-\nplexity of a composition, particularly the rhythmic but also\nthe harmonic and melodic complexity. Uncommon and\nchanging time signatures were mentioned by roughly half\nof the participants. The difficulty of playing a composition\nwas also a very common answer.\nComposition/style (38): Many compositional and stylis-\ntic elements were seen as particularly relevant to the genre,\nsuch as aggressiveness, speed and atmosphere. A sense of\ncohesion is important, with \"clear and distinct ideas glued\ntogether\". The composition should be experimental, with\ncreative rhythms, unique segments and interesting har-\nmonic choices. Dissonant melodies, arpeggios, metal drum\npatterns and guitar specific techniques such as \"chugs\" are\nalso deemed as important.\nInstrumentation (7): Participants mention unique instru-\nments and extended range guitars being particularly indica-\ntive of the genre."}, {"title": "4.2.2 What features made you identify excerpts as Rock?", "content": "Musical structure/composition (24): These excerpts\nwere repetitive and had slower tempos, utilizing a question\nand answer structure and accents on beats two and four.\nThey were also generally soft and not particularly aggres-\nsive.\nSimple/straightforward (23): The excerpts identified\nas rock were seen as simplistic, using simple drums,\nmelodies, chord progressions and particularly 4/4 time sig-\nnatures. These songs had straightforward grooves and\ngeneric solos.\nGuitar techniques (14): Many techniques were seen as\nspecific to the rock genre such as the use of the pentatonic\nscale, open chords, power chords and double stops. Partic-\nipants noted a clear blues inspiration in the guitar playing.\nInstrumentation (11): The rock genre was seen as guitar\ndriven, with guitars and bass parts being separated. The\ndrums were generally synchronized with the guitar and\nemphasized the hi-hat cymbals."}, {"title": "4.2.3 What made you identify excerpts as being\ncomposed using AI?", "content": "Something \"off\" about the composition (40): A major\ntheme involved participants having some feeling of unease\nabout the composition. Preference for human-composed\nmusic might be attributed to a perceived lack of quali-\nties often associated with human creation, such as \"soul\"\nand creativity, or the inability to emulate human-like mu-\nsic performance (playability). Many participants noted\nthat some compositions lacked a sense of cohesiveness\nand consistency, or even sounded random, with odd note\nchoices and bass lines which did not make sense. Partici-"}]}