{"title": "Differentially Private Block-wise Gradient Shuffle for Deep Learning", "authors": ["David Zagardo"], "abstract": "Traditional Differentially Private Stochastic Gradient Descent (DP-SGD) introduces statistical noise on top of gradients drawn from a Gaussian distribution to ensure privacy. This paper introduces the novel Differentially Private Block-wise Gradient Shuffle (DP-BloGS) algorithm for deep learning. BloGS builds off of existing private deep learning literature, but makes a definitive shift by taking a probabilistic approach to gradient noise introduction through shuffling modeled after information theoretic privacy analyses. The theoretical results presented in this paper show that the combination of shuffling, parameter-specific block size selection, batch layer clipping, and gradient accumulation allows DP-BloGS to achieve training times close to that of non-private training while maintaining similar privacy and utility guarantees to DP-SGD. DP-BloGS is found to be significantly more resistant to data extraction attempts than DP-SGD. The theoretical results are validated by the experimental findings.", "sections": [{"title": "Introduction", "content": "Core to the theoretical underpinnings of DP-BloGS is a fact borne of modern necessity: deep learning libraries like PyTorch expose structured access to gradients through parameter groupings. The block-wise shuffle approach offers several nascent advantages compared to traditional DP-SGD in this regard. Specifically, by accounting for the parameter group and gradient dimensions in the sensitivity analysis, DP-BloGS is efficiently tailored to the modern architecture of deep learning models. The probabilistic noise approach through shuffling gradient components plays well with gradient accumulation, as shown through the perplexity metric in the utility experiments. Privacy guarantees are verified through the analysis of data extraction and membership inference vulnerability compared to traditional DP-SGD. These results are promising for machine learning practitioners in that they show DP-BloGS offers similar or better privacy guarantees, competitive performance, and faster runtimes than DP-SGD.\nThis paper introduces several important contributions to the differential privacy literature, particularly in the context of privacy-preserving deep learning:\nNovel Algorithm The paper introduces Differentially Private Block-wise Gradient Shuffle (DP-BloGS), a new algorithm for privacy-preserving deep learning. This algorithm takes a probabilistic approach to gradient noise introduction through shuffling modeled after information theoretic privacy analyses.\nTheoretical Foundations The paper provides a comprehensive theoretical analysis of DP-BloGS, including proofs of its privacy guarantees and utility bounds. This includes theorems on privacy composition, convergence analysis, and information-theoretic bounds.\nParameter-wise Privacy The paper introduces a parameter-wise approach to differential privacy, allowing for different privacy levels for different parts of the model. This is formalized through theorems on parameter-wise privacy loss and composition.\nImproved Efficiency DP-BloGS is shown to achieve training times close to that of non-private training while maintaining similar privacy and utility guarantees to DP-SGD. This addresses a key challenge in practical deployment of differentially private machine learning.\nEnhanced Privacy-Utility Trade-off The paper demonstrates that DP-BloGS can achieve better privacy-utility trade-offs compared to traditional DP-SGD in many scenarios, particularly in terms of resistance to data extraction attempts.\nScalability to Large Models The effectiveness of DP-BloGS is demonstrated on models with up to 1.1 billion parameters, showing its applicability to modern large language models.\nOptimal Parameter Selection The paper provides methods for optimally selecting key parameters of the DP-BloGS algorithm, such as block sizes and clipping thresholds, to balance privacy and utility.\nEmpirical Evaluation The paper includes experiments comparing DP-BloGS to DP-SGD across multiple model architectures and privacy levels, evaluating metrics such as perplexity, membership inference attack resistance, and data extraction rates.\nInformation-Theoretic Analysis The paper provides information-theoretic bounds on the privacy guarantees of DP-BloGS, connecting the algorithm to fundamental concepts in information theory.\nPrivacy Accounting The paper develops privacy accounting methods specific to DP-BloGS, allowing for precise tracking of privacy loss over the course of training.\nThe rest of this paper is structured as follows. Relevant work is briefly introduced and its importance stated in a related work section. The methodology section covers the experimental approach and the DP-BloGS algorithm in detail. Results are presented, with full tables and all plots saved for the appendix. The mathematical foundations for DP-BloGS are then laid out, building on prominent literature in differential privacy. The math section aims to provide each necessary mathematical tool in the order it is needed, proving DP-BloGS' privacy-enhancing qualities in rigorous detail. The paper concludes by summarizing key findings, commenting on limitations, and laying out future work."}, {"title": "Related Work", "content": "This work builds on prior Differential Privacy literature such as [1-4]. For implementation, notably, batched layer clipping [18] is a primary component of DP-BloGS' speed boost. DP-BloGS is compared to DP-SGD by implementing the code available on the AWS Fast DP repository [19].\n[21,22] also expand on shuffling models for differential privacy. Though this paper is categorically different in its approach and its privacy guarantees, it does draw explicit inspiration from the work done by Cheu et al."}, {"title": "Algorithm", "content": "The algorithm has 3 primary classes. The Trainer, the Generator, and the Accountant. The Trainer handles everything related to training, and is an extended version of the SFTTrainer from Hugging Face trl. The Generator is responsible for ingesting the gradients and shuffling during the post accumulation step before the model's weights are updated with the gradients. The Accountant is responsible for optimizing the block sizes and keeping track of privacy spend across the training process."}, {"title": "Initializing DPShuffleGenerator", "content": "The DPShuffleGenerator is initialized with several key parameters: model (the neural network model being trained), target_epsilon (the desired privacy budget), delta (the probability of privacy failure), steps (the number of training steps), clip_value (the maximum L2 norm for gradient clipping), and batch_size (the number of examples per training batch).\nDuring initialization, the generator creates a DPShufflePrivacy Accountant with these parameters. The accountant then performs the following optimization process:"}, {"title": "Privacy Accountant Optimization", "content": "The accountant computes the dimensions of all trainable parameters in the model. It then employs a nested binary search strategy to find the optimal block sizes for each parameter group:\nOuter binary search: This search finds the optimal target epsilon per group (epsilon_i) that will result in the overall target epsilon when summed across all groups and steps.\nInner binary search: For each parameter group, this search determines the largest block size that satisfies the current target epsilon_i.\nFor each parameter group with dimension $d_i$ and a given block size, the accountant computes two epsilon values:\n$\\epsilon_1 = 2 \\cdot log(1 + d_i \\cdot (e^{\\frac{2C}{\\sqrt{d_i}}} - 1))$ (1)\n$\\epsilon_2 = 2 \\cdot log(1 + \\frac{block\\_size}{d_i} \\cdot (e^{\\frac{2C}{\\sqrt{block\\_size/d_i}}} - 1))$ (2)\nWhere C is the clip_value. The smaller of these two values is chosen as the epsilon for this group and block size.\nThe total privacy spent across all groups and steps is computed using:\n$\\epsilon_{total} = \\sqrt{2 \\cdot steps \\cdot log(1/\\delta)} \\cdot \\epsilon_{total\\_per\\_step} + steps \\cdot \\epsilon_{total\\_per\\_step} \\cdot (e^{\\epsilon_{total\\_per\\_step}} - 1)$ (3)\nWhere $\\epsilon_{total\\_per\\_step}$ is the sum of epsilons for all parameter groups in a single step.\nThis process iteratively refines the estimate of the optimal target epsilon per group until the total privacy is as close as possible to the overall target epsilon.\nThe generator stores these optimal block sizes and initializes the spent privacy budget (epsilon_spent) to 0."}, {"title": "Training Process", "content": "When training begins and gradients have been accumulated, the generate function is called with the accumulated gradients. This function performs several key steps:"}, {"title": "Gradient Processing", "content": "For each gradient and its corresponding optimal block size, the gradient is clipped to the specified clip_value if necessary, and then shuffled using the optimal block size. These processed gradients are collected into a list called private_grads."}, {"title": "Gradient Shuffling", "content": "The shuffling process involves flattening the gradient into a 1D tensor, padding it if necessary to ensure it can be evenly divided into blocks, reshaping it into blocks of the specified size, randomly shuffling the blocks, flattening and trimming the shuffled blocks to the original gradient size, and finally reshaping the result to match the original gradient shape."}, {"title": "Privacy Accounting", "content": "After processing all gradients, the accountant computes the total privacy spent (epsilon) for this step."}, {"title": "Methodology", "content": "To aid in visualizing DP-BloGS, this paper presents two primary figures containing three plots in total. Figure 1 shows a simulation of parameter group gradients converted to 2D heatmaps for DP-BloGS at values of epsilon 1 and 100. Figure 2 shows layer gradients unprocessed for visual comparison.\nExperiments were performed on a single A100 GPU. The dataset used for training was the first 1,000 records of Tiny Orca. The dataset used for testing was the second 1,000 records of Tiny Orca. The models examined were sourced through the Hugging Face model hub. Specifically facebook/opt-350m, bigscience/bloom-560m, openai-community/gpt2, TinyLlama/TinyLlama-1.1B-step-50K-105b, and google-bert/bert-base-uncased.\nEach experiment held the majority of variables constant. The hyperparameters were kept as follows: batch size of 5, gradient accumulation steps of 10, learning rate of 2e-4, max gradient norm of 1.0, training epochs of 5, AdamW 32 bit optimizer, and a constant learning rate. Experiments were performed in full 32 bit precision."}, {"title": "Evaluation Process", "content": ""}, {"title": "Membership Inference Attack (MIA)", "content": "The membership inference attack was conducted using an approach that combines multiple machine learning models and advanced feature engineering.\nFeature extraction involved extracting features from both training and test datasets using the get_mia_features function. These features included model confidence, entropy, perplexity, data augmentation responses, memorization metrics, generalization indicators, contrastive learning features, out-of-distribution detection scores, and per-layer gradient norms.\nData preprocessing steps included imputation to handle missing values, standardization of features using StandardScaler, feature selection using mutual information, creation of polynomial features to capture non-linear relationships, and application of SMOTE for class balancing.\nFor model training and selection, multiple classifiers including Random Forest, Logistic Regression, and Neural Network were trained. Randomized SearchCV with StratifiedKFold cross-validation was used for hyperparameter tuning. A stacking ensemble combining the best models was implemented, and the best performing model was selected based on ROC AUC score.\nModel calibration and thresholding involved calibrating the best model's probabilities and determining an optimal classification threshold to balance precision and recall.\nEvaluation metrics included ROC AUC, precision-recall AUC, and Brier score, which are standard metrics for evaluating the effectiveness of membership inference attacks [20]. Accuracy, precision, recall, and F1-score were computed for both standard (0.5) and optimal thresholds. Error analysis was performed to understand misclassifications.\nVisualization included plots of the ROC curve, learning curves, and feature importance."}, {"title": "Data Extraction Risk Assessment", "content": "The data extraction risk was evaluated using a simulated extraction attack. The attack simulation involved 1000 extraction attempts. For each attempt, a randomly selected training example was used, with a substring of length 10 as a prompt to generate a completion of length 30 using the model.\nExtraction detection involved checking if the generated text matched any part of the training data. A match was considered successful if at least 20 consecutive characters matched.\nAnalysis of the extraction results included calculating the extraction rate (percentage of successful extractions), computing the average length of successfully extracted sequences, and identifying the top 10 most substantial extractions."}, {"title": "Perplexity Evaluation", "content": "Perplexity was evaluated to assess the model's predictive performance. The evaluation process involved iterating through the dataset batch by batch. For each batch, the input text was tokenized, target labels were created by shifting input tokens, model logits were computed, and cross-entropy loss was calculated, ignoring padding tokens.\nThe perplexity calculation involved accumulating total loss and total number of tokens across all batches, computing average loss per token, and calculating perplexity as the exponential of the average loss.\nInterpretation of perplexity results considered that lower perplexity indicates better predictive performance. Perplexity was compared across different model versions or privacy settings to assess impact on language modeling capability.\nThis comprehensive evaluation process provides a multi-faceted assessment of the model's privacy preservation, covering both the risk of membership inference and data extraction, while also measuring the model's overall language modeling performance through perplexity. The results from these evaluations can be used to analyze the trade-offs between privacy and utility in the trained models."}, {"title": "Results", "content": ""}, {"title": "DP-BloGS vs. DP-SGD", "content": "A normalized comparison of DP-BloGS and DP-SGD across multiple models reveals insights into their relative performance in terms of utility and privacy."}, {"title": "Utility Performance", "content": "In terms of perplexity, our key utility metric, DP-BloGS demonstrates a slight advantage. The average normalized difference of -0.0226 favors DP-BloGS, which outperforms DP-SGD in 67.69% of cases. DP-BloGS also exhibits a higher normalized Area Under the Curve (AUC) of 0.7557 compared to 0.0977 for DP-SGD. However, it is important to note that this difference is not statistically significant (p-value: 0.5523), indicating that the utility performance of both methods is comparable."}, {"title": "Privacy Performance", "content": "The privacy performance is assessed through two metrics: resistance to Membership Inference Attacks (MIA) and resistance to data extraction.\nFor MIA resistance, both methods perform similarly. The average normalized difference of 0.0013 slightly favors DP-SGD, but DP-BloGS still performs better in 50.77% of cases. The normalized AUC values are close (1484.56 for DP-BloGS vs 1493.59 for DP-SGD), and the difference is not statistically significant (p-value: 0.9094).\nIn terms of data extraction resistance, DP-BloGS significantly outperforms DP-SGD. The average normalized difference of -0.0514 favors DP-BloGS, which performs better in 69.23% of cases. This difference is statistically significant (p-value: 0.0024), indicating a consistent advantage for DP-BloGS in preventing data extraction across different models."}, {"title": "Key Findings", "content": "The comparative analysis reveals that DP-BloGS offers comparable or slightly better utility than DP-SGD, similar resistance to membership inference attacks, and significantly better protection against data extraction. These findings suggest that DP-BloGS may be the preferable method, particularly in scenarios where strong protection against data extraction is a priority. The balanced performance across utility and privacy metrics positions DP-BloGS as a robust choice for privacy-preserving machine learning applications."}, {"title": "Limitations and Future Work", "content": "This research was limited by budget and time. If more resources had been available, it would have been prudent to examine multiple datasets instead of training numerous model variants across a single dataset. Additional resources would have allocated for examining the interaction with even larger models, such as Llama 405b, Nemo, Falcon 180b, LoRA finetuning, and other methodologies.\nFuture research will refine parameter-wise budget allocation strategies, explore how DP-BloGS interacts with LoRA, apply DP-BloGS to different model architectures like convolutional neural networks, variational autoencoders, and diffusion models, and assess the parameter-specific factors that come into play when seeking to maximize utility while retaining privacy."}, {"title": "Math", "content": "We begin by describing and showing preservation and maintenance of gradient properties under shuffling."}, {"title": "Lemma 1", "content": "Lemma 1 (Gradient Properties Under Shuffling). Let $g \\in \\mathbb{R}^d$ be a gradient vector and $\\pi : \\{1, ..., d\\} \\rightarrow \\{1, ..., d\\}$ be a permutation function. Define the shuffled gradient $g_\\pi$ as $g_\\pi[i] = g[\\pi(i)]$ for $i \\in \\{1, ..., d\\}$. Then, the following properties hold:\n1. L2 Norm Preservation: $||g_\\pi ||_2 = ||g||_2$\n2. Mean Preservation: $\\Sigma_{i=1}^d g_\\pi[i] = \\Sigma_{i=1}^d g[i]$\n3. Variance Preservation: $Var(g_\\pi) = Var(g)$\nProof. Let $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be a probability space, where $\\Omega$ is the sample space, $\\mathcal{F}$ is a $\\sigma$-algebra on $\\Omega$, and $\\mathbb{P}$ is a probability measure.\n1. L2 Norm Preservation:\n$||g_\\pi ||_2^2 = \\sum_{i=1}^d (g_\\pi[i])^2 = \\sum_{i=1}^d (g[\\pi(i)])^2 = \\sum_{j=1}^d (g[j])^2$ (since $\\pi$ is a bijection) = $||g||_2^2$\nTaking the square root of both sides:\n$||g_\\pi ||_2 = ||g||_2$\n2. Mean Preservation:\n$\\frac{1}{d} \\sum_{i=1}^d g_\\pi[i] = \\frac{1}{d} \\sum_{i=1}^d g[\\pi(i)] = \\frac{1}{d} \\sum_{j=1}^d g[j]$ (since $\\pi$ is a bijection)"}, {"title": "Variance Preservation", "content": "Variance Preservation: First, let's recall the definition of variance for a vector:\n$Var(g) = \\frac{1}{d} \\sum_{i=1}^d (g[i] - \\mu)^2$, where $\\mu = \\frac{1}{d} \\sum_{i=1}^d g[i]$\nNow, let's prove that $Var(g_\\pi) = Var(g)$:\n$Var(g_\\pi) = \\frac{1}{d} \\sum_{i=1}^d (g_\\pi[i] - \\mu_\\pi)^2$, where $\\mu_\\pi = \\frac{1}{d} \\sum_{i=1}^d g_\\pi[i]$\nFrom (b), we know that $\\mu_\\pi = \\mu$. Therefore:\n$Var(g_\\pi) = \\frac{1}{d} \\sum_{i=1}^d (g_\\pi[i] - \\mu)^2 = \\frac{1}{d} \\sum_{i=1}^d (g[\\pi(i)] - \\mu)^2 = \\frac{1}{d} \\sum_{j=1}^d (g[j] - \\mu)^2$ (since $\\pi$ is a bijection) = $Var(g)$\nThus, we have proven that shuffling preserves the L2 norm, mean, and variance of the gradient vector."}, {"title": "Corollary 1.1", "content": "Corollary 1.1 (Preservation of Gradient Clipping). Let $clip_C(g)$ be a gradient clipping function that clips the L2 norm of g to a maximum of C. Then, for any permutation $\\pi$:\n$clip_C(g_\\pi) = (clip_C(g))_\\pi$\nProof. Case 1: If $||g||_2 \\leq C$, then $clip_C(g) = g$.\nIn this case, $||g_\\pi ||_2 = ||g||_2 \\leq C$ (from Lemma 1(a)). Therefore, $clip_C(g_\\pi) = g_\\pi = (clip_C(g))_\\pi$.\nCase 2: If $||g||_2 > C$, then $clip_C(g) = (\\frac{C}{||g||_2}) g$.\nLet $h = clip_C(g)$. Then:\n$h_\\pi[i] = h[\\pi(i)] = (\\frac{C}{||g||_2}) g[\\pi(i)]$\nOn the other hand:\n$clip_C(g_\\pi) = (\\frac{C}{||g_\\pi||_2}) g_\\pi = (\\frac{C}{||g||_2}) g_\\pi$ (from Lemma 1(a))\nTherefore, $clip_C(g_\\pi) = h_\\pi = (clip_C(g))_\\pi$"}, {"title": "Definition 2", "content": "This definition builds on the foundations of differential privacy [1] and incorporates shuffling techniques [9].\nDefinition 2 (DP-BloGS Mechanism). Let $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be a probability space, where $\\Omega$ is the sample space, $\\mathcal{F}$ is a $\\sigma$-algebra on $\\Omega$, and $\\mathbb{P}$ is a probability measure.\nLet $\\mathcal{X}$ be the data space and $\\mathcal{D} \\subset \\mathcal{X}^n$ be the set of all possible datasets of size n.\nLet $\\mathbb{R}^d$ be the d-dimensional real vector space representing the gradient space."}, {"title": "Define the gradient function", "content": "Define the gradient function $g : \\mathcal{D} \\rightarrow \\mathbb{R}^d$ that maps a dataset to its gradient.\nLet $S_d$ be the symmetric group on d elements, i.e., the set of all permutations of $\\{1,...,d\\}$.\nDefine a random variable $\\Pi : \\Omega \\rightarrow S_d$ that selects a permutation uniformly at random from $S_d$.\nThe DP-BloGS mechanism $M : \\mathcal{D} \\times \\Omega \\rightarrow \\mathbb{R}^d$ is defined as:\n$M(D,\\omega) = g_{\\pi}(D)$\nwhere $\\pi = \\Pi(\\omega)$ and $g_{\\pi}(D)[i] = g(D)[\\pi(i)]$ for $i \\in \\{1, ...,d\\}$.\nProperties of the DP-BloGS Mechanism:\n1. Uniform Selection: For any permutation $\\sigma \\in S_d$,\n$\\mathbb{P}(\\Pi = \\sigma) = \\frac{1}{|S_d|} = \\frac{1}{d!}$\n2. Invertibility: Given M(D,w) and $\\pi$, one can recover g(D) by applying $\\pi^{-1}$.\n3. Output Space: The output space of M is identical to the gradient space $\\mathbb{R}^d$."}, {"title": "Lemma 2.1", "content": "Lemma 2.1 (Conditional Probability of Output). For any dataset $D \\in \\mathcal{D}$ and any vector $v \\in \\mathbb{R}^d$,\n$\\mathbb{P}(M(D,\\omega) = v | g(D) = u) = \\begin{cases} \\frac{1}{|S_d|} & \\text{if } \\exists \\sigma \\in S_d \\text{ such that } v[i] = u[\\sigma(i)] \\forall i \\in \\{1, ...,d\\} \\\\ 0 & \\text{otherwise} \\end{cases}$\nProof:\nLet $E = \\{\\omega \\in \\Omega : M(D,\\omega) = v \\text{ and } g(D) = u\\}$.\nCase 1: $\\exists \\sigma \\in S_d \\text{ such that } v[i] = u[\\sigma(i)] \\forall i \\in \\{1, ...,d\\}$\nIn this case, $E = \\{\\omega \\in \\Omega : \\Pi(\\omega) = \\sigma^{-1}\\}$.\n$\\mathbb{P}(E) = \\mathbb{P}(\\Pi = \\sigma^{-1}) = \\frac{1}{d!}$\nCase 2: $\\nexists \\sigma \\in S_d \\text{ such that } v[i] = u[\\sigma(i)] \\forall i \\in \\{1, ...,d\\}$\nIn this case, $E = \\emptyset$.\n$\\mathbb{P}(E) = 0$\nTherefore,\n$\\mathbb{P}(M(D, \\omega) = v | g(D) = u) = \\mathbb{P}(E)/\\mathbb{P}(g(D) = u) = \\mathbb{P}(E)$\nThis completes the proof."}, {"title": "Lemma 2.2", "content": "Lemma 2.2 (Preservation of Gradient Properties). For any dataset $D \\in \\mathcal{D}$ and any $\\omega \\in \\Omega$,\n1. $||M(D,\\omega) ||_2 = ||g(D)||_2$\n2. $\\frac{1}{d} \\sum_{i=1}^d M(D,\\omega)[i] = \\frac{1}{d} \\sum_{i=1}^d g(D)[i]$\n3. $Var(M(D, \\omega)) = Var(g(D))$\nProof: This follows directly from Lemma 1, as M(D,w) is a shuffled version of g(D)."}, {"title": "Theorem 2.3", "content": "Theorem 2.3 (Unbiasedness of DP-BloGS). For any dataset $D \\in \\mathcal{D}$,\n$\\mathbb{E}[M(D,\\omega)] = g(D)$\nwhere the expectation is taken over the random permutation $\\Pi$.\nProof:\nFor any $i \\in \\{1, ...,d\\}$,\n$\\mathbb{E}[M(D, \\omega)[i]] = \\sum_{k=1}^d g(D)[k] \\cdot \\mathbb{P}(\\Pi(i) = k) = \\sum_{k=1}^d g(D)[k] \\cdot \\frac{1}{d} = \\frac{1}{d} \\sum_{k=1}^d g(D)[k] = g(D)[i]$\nThe last equality holds because the sum of all elements divided by d equals each element (since the expectation preserves each element).\nTherefore,\n$\\mathbb{E}[M(D,\\omega)] = g(D)$"}, {"title": "Theorem 3", "content": "Theorem 3 (Sensitivity of Shuffled Gradients). Let $D, D' \\in \\mathcal{D}$ be two adjacent datasets differing in at most one element. We analyze the sensitivity of the mechanism, a key concept in differential privacy [1]. Let M be the DP-BloGS mechanism as defined in Definition 2. Then, the $L_1$ and $L_2$ sensitivities of M are equal to the corresponding sensitivities of the original gradient function g:\n1. $\\Delta_1 M = \\Delta_1 g = \\sup_{D,D' \\text{ adjacent }} ||g(D) - g(D')||_1$\n2. $\\Delta_2 M = \\Delta_2 g = \\sup_{D,D' \\text{ adjacent }} ||g(D) - g(D')||_2$\nwhere $\\Delta_1 M$ and $\\Delta_2 M$ are the $L_1$ and $L_2$ sensitivities of M, respectively, and $\\Delta_1 g$ and $\\Delta_2 g$ are the $L_1$ and $L_2$ sensitivities of g, respectively.\nProof:\nLet $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be our probability space as defined in Definition 2.\n(a) $L_1$ Sensitivity:\n$\\Delta_1 M = \\sup_{D,D' \\text{ adjacent }} \\mathbb{E}_\\omega [||M(D, \\omega) - M(D', \\omega)||_1]$\nFor any $\\omega \\in \\Omega$, let $\\pi = \\Pi(\\omega)$ be the random permutation.\n$||M(D, \\omega) - M(D', \\omega)||_1 = \\sum_{i=1}^d |M(D, \\omega)[i] - M(D',\\omega)[i]| = \\sum_{i=1}^d |g(D)[\\pi(i)] - g(D')[\\pi(i)]| = \\sum_{j=1}^d |g(D)[j] - g(D')[j]|$ (since $\\pi$ is a bijection) = $||g(D) - g(D')||_1$\nThis equality holds for all $\\omega \\in \\Omega$, so:\n$\\mathbb{E}_\\omega [||M(D, \\omega) - M(D',\\omega)||_1] = ||g(D) - g(D')||_1$\nTherefore,\n$\\Delta_1 M = \\sup_{D,D' \\text{ adjacent }} ||g(D) - g(D')||_1 = \\Delta_1 g$\n(b) $L_2$ Sensitivity:\n$\\Delta_2 M = \\sup_{D,D' \\text{ adjacent }} \\sqrt{\\mathbb{E}_\\omega [||M(D, \\omega) - M(D', \\omega)||_2^2]}$\nFor any $\\omega \\in \\Omega$, let $\\pi = \\Pi(\\omega)$ be the random permutation.\n$||M(D, \\omega) - M(D', \\omega) ||_2^2 = \\sum_{i=1}^d (M(D, \\omega)[i] - M(D', \\omega)[i])^2 = \\sum_{i=1}^d (g(D)[\\pi(i)] - g(D')[\\pi(i)])^2 = \\sum_{j=1}^d (g(D)[j] - g(D')[j])^2$ (since $\\pi$ is a bijection) = $||g(D) - g(D')||_2^2$\nThis equality holds for all $\\omega \\in \\Omega$, so:\n$\\mathbb{E}_\\omega [||M(D, \\omega) - M(D',\\omega)||_2^2] = ||g(D) - g(D')||_2^2$\nTherefore,\n$\\Delta_2 M = \\sup_{D,D' \\text{ adjacent }} ||g(D) - g(D')||_2 = \\Delta_2 g$\nThis completes the proof."}, {"title": "Corollary 3.1", "content": "Corollary 3.1 (Sensitivity Preservation under Clipping). Let $clip_C(g)$ be a gradient clipping function that clips the L2 norm of g to a maximum of C. Gradient clipping, as introduced by [7], is a crucial technique in our analysis. Define Mc as the composition of clipping and DP-BloGS:\n$M_C(D,\\omega) = M(clip_C(g(D)), \\omega)$\nThen, the L2 sensitivity of Mc is bounded by 2C:\n$\\Delta_2 M_C \\leq 2C$\nProof:\nFor any adjacent datasets D and D':\n$||M_C(D,\\omega) - M_C(D', \\omega) ||_2 = ||M(clip_C(g(D)), \\omega) - M(clip_C(g(D')), \\omega) ||_2 = ||clip_C(g(D)) - clip_C(g(D'))||_2$ (from Theorem 3)\nNow, for any vectors u and v:\n$|| clip(u) - clip_C(v) ||_2 \\leq || clip_C(u) - clip_C(0) ||_2 + || clip_C(0) - clip_C(v) ||_2 < C + C = 2C$\nTherefore,\n$\\Delta_2 M_C = \\sup_{D,D' \\text{ adjacent }} \\mathbb{E}_\\omega [||M_C(D, \\omega) - M_C(D', \\omega)||_2] \\leq \\sup_{D,D' \\text{ adjacent }} \\mathbb{E}_\\omega [(2C)^2] = 2C$\nThis completes the proof."}, {"title": "Lemma 3.2", "content": "Lemma 3.2 (Expected Squared L2 Norm Difference). For any adjacent datasets D and D', the expected squared L2 norm difference between M(D,w) and M(D', w) is equal to the squared L2 norm difference between g(D) and g(D'):\n$\\mathbb{E}_\\omega [||M(D, \\omega) - M(D', \\omega) ||_2^2] = ||g(D) - g(D')||_2^2$\nProof:\n$\\mathbb{E}_\\omega [||M(D, \\omega) - M(D', \\omega) ||_2^2] = \\mathbb{E}_\\omega [\\sum_{i=1}^d (M(D,\\omega)[i] - M(D', \\omega)[i])^2] = \\sum_{i=1}^d \\mathbb{E}_\\omega [(M(D,\\omega)[i] - M(D',\\omega)[i])^2]$\nFor each i:\n$\\mathbb{E}_\\omega [(M(D, \\omega)[i] - M(D', \\omega)[i])^2] = \\mathbb{E}_\\omega [(g(D)[\\Pi(\\omega)(i)] - g(D')[\\Pi(\\omega)(i)])^2] = \\frac{1}{d} \\sum_{j=1}^d (g(D)[j] - g(D')[j])^2$\nTherefore,\n$\\mathbb{E}_\\omega [||M(D, \\omega) - M(D', \\omega) ||_2^2] = \\sum_{i=1}^d \\frac{1}{d} \\sum_{j=1}^d (g(D)[j] - g(D')[j])^2 = \\sum_{j=1}^d (g(D)[j] - g(D')[j])^2 = ||g(D) - g(D')||$\nThis completes the proof."}, {"title": "Lemma 4", "content": "Lemma"}]}