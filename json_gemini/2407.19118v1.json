{"title": "Large Language Models as Co-Pilots for Causal Inference in Medical Studies", "authors": ["Ahmed Alaa", "Rachael V. Phillips", "Emre K\u0131c\u0131man", "Laura B. Balzer", "Mark van der Laan", "Maya Petersen"], "abstract": "The validity of medical studies based on real-world clinical data, such as observational studies, depends on critical assumptions necessary for drawing causal conclusions about medical interventions. Many published studies are flawed because they violate these assumptions and entail biases such as residual confounding, selection bias, and misalignment between treatment and measurement times. Although researchers are aware of these pitfalls, they continue to occur because anticipating and addressing them in the context of a specific study can be challenging without a large, often unwieldy, interdisciplinary team with extensive expertise. To address this expertise gap, we explore the use of large language models (LLMs) as \u201cco-pilot\u201d tools to assist researchers in identifying study design flaws that undermine the validity of causal inferences. We propose a conceptual framework for LLMs as causal co-pilots that encode domain knowledge across various fields, engaging with researchers in natural language interactions to provide contextualized assistance in study design. We provide illustrative examples of how LLMs can function as causal co-pilots, propose a structured framework for their grounding in existing causal inference frameworks, and highlight the unique challenges and opportunities in adapting LLMs for reliable use in epidemiological research.", "sections": [{"title": "1 Introduction", "content": "Real-world data (RWD) play a vital role in advancing medical knowledge, from assessing correlates and causes of human disease to evaluating drug effectiveness and safety in routine clinical practice settings (Black (1996); Yuan et al. (2023)). RWD can come from electronic health records (EHRs); insurance claims and billing activities; disease and medical product registries; and other data sources that are informative of patient health status, such as digital health technologies. Studies that are designed to integrate RWD (such as observational studies (OS), pragmatic trials, and externally controlled trials) are often used when randomized controlled trials (RCTs) present ethical concerns or feasibility challenges. This can occur (i) when studying populations that are vulnerable or unable to give informed consent (such as children, cognitively impaired individuals, or severely ill patients); (ii) in studies of diseases with high mortality rates and no existing effective treatments, as withholding potential treatments for the sake of randomization could be seen as unethical; or (iii) in public health emergencies where delaying treatment for the sake of a randomized trial could result in significant harm or death. Studies also use RWD when it is of interest to better understand longer term outcomes or patient populations that are more representative of routine care (Concato (2012)).\nIn contrast to RCTs, studies with RWD have inherent limitations and complexities (such as confounding, selection bias and misalignment between measurements) that may introduce biases if they are not rigorously addressed (Stroup et al. (2000); Hannan (2008); Shrank et al. (2011); Fox et al. (2024)). Notably, without randomization of treatment, confounding persists; these studies are susceptible to identifying associations that do not necessarily imply causation (Gerstein et al. (2019)). Historical instances have demonstrated that inadequate study design, analysis, or misinterpretation of results have led to publications that could mislead public heath directives (Kral et al. (2005)). Such studies are even occasionally cited in the media, potentially fostering misconceptions among the public, particularly during health crises such as the COVID pandemic (Lyons (2011); Alexander et al. (2020); Jung et al. (2021)).\nOn the other hand, when studies using RWD are carefully designed and rigorously analyzed they offer the potential to advance medicine and improve clinical care. In particular, RWD studies that meet evidentiary standards can contribute to drug development and be used to support regulatory decision-making (Concato et al. (2020)). For example, in 2021, the Center for Drug Evaluation and Research (CDER) of the US Food and Drug Administration (FDA) granted approval to a medication following a study using RWD. This approval pertained to the use of tacrolimus (Prograf) along with other immunosuppressants to prevent organ rejection in lung transplants. The decision was based on an analysis comparing data from a well-established registry with data on historical controls. This marked the first time CDER acknowledged an OS as a sufficient, well-controlled investigation that provided primary support for a finding of substantial treatment effectiveness evidence (Concato & Corrigan-Curay (2022)). Legislation enabling the use of RWD in regulatory decision-making has now been passed across the globe.\nDespite increased recognition of the potential of RWD and the substantial efforts to use it to advance medicine, mechanisms for following best practices and controlling the quality of RWD studies have not kept pace. Indeed, it is an arduous task to design studies that adhere to these standards. It requires (i) knowledge of regulatory guidance documents; (ii) detailed and accurate descriptions of the RWD source and its reputability; (iii) objective comparisons of different methodologies for generating rigorous causal inferences in the absence of randomization.; (iv) respecting the RWD complexity via sophisticated and robust statistical and causal modeling; and (v) thorough discussion of the implications of the results, limitations, and any assumptions and their plausibility. To accomplish this, subject matter expertise across several domains is necessary, including medicine, public health, epidemiology, statistics, and causal inference. In practice this typically involves multiple iterations across teams of interdisciplinary experts. Key judgment calls may not be fully vetted or transparently reported, and the burden is placed entirely on the human investigator team with little to no technical support.\nIn this paper, we outline a new research direction exploring how large language models (LLMs) can collaborate with researchers to improve the quality of medical studies based on RWD. Our key thesis is that, while pitfalls in RWD studies are diverse and numerous, many are avoidable and can be addressed during study design and analysis. Because LLMs encode domain knowledge across various fields (Singhal et al. (2022)), they can emulate multidisciplinary expertise to help detect these pitfalls within the specific context of a study and engage with researchers through natural language interactions to refine study design and avoid these issues. Throughout this paper, we provide illustrative examples of how LLMs can function as causal co-pilots, propose a structured framework for integrating LLMs with existing causal inference methodologies, and highlight the unique challenges and opportunities in adapting LLMs for reliable use in epidemiological research.\nIn the next Section, we provide a brief overview of the different types of RWD studies and existing frameworks for RWD-based causal inference. Readers versed in this domain may opt to proceed directly to Section 3."}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Landscape of Medical Studies", "content": "Contemporary discourse often simplifies studies into \u201crandomized\u201d or \u201cobservational\u201d types, neglecting the wide variety of data and designs that exist. Regulatory agencies across the globe have worked to rectify these misunderstandings and provide a more nuanced categorization of studies involving RWD (ICH (2023)). In Figure 2A, we illustrate the spectrum of studies that use RWD and generate real-world evidence (RWE, clinical evidence about the usage and potential benefits or risks of a medical product derived from analysis of RWD (FDA (2018))). Figure 2A depicts the spectrum of medical studies along with the degree of RWD reliance across different types of studies."}, {"title": "2.2 Frameworks for RWE Generation", "content": "Designing RWD studies involves numreous complexities (Figure 2B) that make generating valid RWE challenging. These complexities also complicate the clear communication of findings in scientific publications, to the extent that many medical journals restrict the use of notions of causality to RCTs (Dahabreh & Bibbins-Domingo (2024)). To address these issues, researchers have proposed structured approaches to designing and communicating RWD studies to ensure the validity and clarity of their causal conclusions. These frameworks provide guidance on defining and reporting the causal question being studied, the causal quantity being estimated, the study design, the assumptions made, and the use of RWD to answer the causal question (Dang et al. (2023); Petersen & van der Laan (2014)).\nThe Causal Roadmap is an example for such a structured framework that we have previously proposed in Dang et al. (2023). The roadmap is an explicit, iterative step-by-step guide for investigators to follow in the pre-specification of study design, causal inference and statistical analysis plans. It addresses a wide range of questions and handles various RWD study types within a single framework. Using the Causal Roadmap helps in (i) detailing key elements of a study design and analysis plan likely to yield high-quality evidence; and (ii) reaching an objective conclusion that producing the required level of medical evidence is presently unfeasible, while also identifying what data is necessary for future credible evidence generation."}, {"title": "3 How can LLMs Help Avoid Flaws in RWD Studies?", "content": "TL;DR: LLMs possess the capability to analyze RWD study designs and results, both in textual and visual formats, while also discerning potential biases within these studies. However, off-the-shelf LLMs struggle to provide accurate and pertinent responses without explicit user instructions and explanations of general principles that guide their understanding of bias sources.\nDespite the availability of structured guidance for designing RWE studies, researchers often struggle to apply this abstract guidance to specific studies. This difficulty arises because each step in the study design process can present multiple potential flaws, and reasoning through these issues often requires context-specific and domain-specific knowledge. For instance, the initial step of a RWD study typically \u201cdefines a causal question & estimand\". As we will discuss in this section, there are many ways researchers can define an estimand that does not address the causal question of interest, or they might select a causal question that is not clinically relevant. In this section, we demonstrate how LLMs can, in principle, serve as \"causal co-pilot\" tools for contextualizing the application of structured frameworks for causal inference to the RWD study at hand. However, we also show that the zero-shot application of widely-used LLMs does not fulfill this role effectively.\nWe begin by motivating the causal co-pilot role of LLMs through a demonstration of how an LLM can collaborate with researchers to intercept problems in designing studies using RWD. We showcase how an LLM co-pilot can help identify and avert flaws, drawing on those from three well-known historical instances (Sections 3.1, 3.2, and 3.3) where findings of observational studies (OS) were later refuted. In this section, we focus on OS due to their historical prevalence in clinical research. We discuss the role of LLMs in other types of RWD studies in the next section."}, {"title": "3.1 Clarifying the Causal Question", "content": "Generating valid evidence from RWD requires specifying a precise causal question-a query regarding the cause-effect relationship between a possibly time-varying exposure or treatment on an outcome for a particular target population of interest. In practice, this process can involve multidisciplinary teams collaborating for months to pinpoint an estimand that (i) is clinically meaningful, and (ii) can be effectively answered using available data.\nAn LLM co-pilot can assist researchers in defining the causal question for the study at hand. The user-LLM interaction on the causal question can encompass various aspects of the causal estimand, including defining eligibility criteria, follow-up timing, outcome variables and treatment protocols, and definition of baseline study time. Responding to these questions, the co-pilot can leverage its encoded medical domain knowledge (Singhal et al. (2023)) to contextualize its responses in a way that mirrors a multidisciplinary collaboration. Below, we describe a notable instance of misleading findings from OS that were later attributed to poor formulation of the causal query, and then we discuss how an LLM co-pilot could have provided guidance to intercept the errors underlying the study design.\nDoes postmenopausal hormone therapy have cardiovascular benefits? In the 1980s and 1990s, several OS examined the impact of hormone replacement therapy (HRT) on future coronary heart disease (CHD) events in post-menopausal women, with results suggesting a cardiovascular benefit from HRT (Barrett-Connor & Grady (1998)). These findings were later contradicted by multiple RCTs, including the Women's Health Initiative study (Manson et al. (2003)), which even stopped early because of an increased risk of CHD and breast cancer in women treated with HRT (Boardman et al. (2015)). Medical guidelines and recommendations were updated in light of these RCT findings (National Institutes of Health (2016)).\nAn OS postmortem revealed that the biases exhibited where the result of a flawed conceptualization of the underlying \"causal question\" (Hern\u00e1n et al. (2008)). Firstly, the OS excluded women who initiated HRT and then suffered nonfatal myocardial infarction within the first 2 years, leading to selection bias that underestimates short-term risk of HRT on CHD event incidence. Additionally, the OS compared incidence of CHD events in women who were currently on HRT and those who were never on HRT (prevalent users), whereas the RCT compared HRT initiators and non-initiators (incident users). Comparing current users may be less clinically significant and is more prone to bias here, as women already on HRT for a long time are less likely to be susceptible to risk.\nAn LLM-based causal co-pilot can account for various aspects of the causal estimand and provides sound reasoning for its specification. The lesson learned from the OS on the effects of HRT is that an imprecise definition of the causal question can yield misleading or even clinically irrelevant findings. In Figure 3 (Panel A), we query an LLM model (GPT-4) about the appropriate causal question for estimating the effect of HRT on incidence of CHD events. We find that, with a careful choice of prompts, the model shows sound reasoning on how to specify the causal question in the context of the study. On the flip side, without user prompts providing cues on bias sources, an off-the-shelf LLM typically offers generic responses and fails to preemptively alert the researcher regarding relevant bias considerations for the study at hand."}, {"title": "3.2 Critiquing Study Design", "content": "In contrast to RCTs where interventions are randomized and follow strict protocols, OS often require retrospective decisions on defining treatments, follow-up time, and outcomes. Even with a sound causal question, poor design and analysis choices in OS can introduce problems in the definition of the estimand and biases in the corresponding estimates, ultimately producing potentially unreliable results that must be interpreted with caution.\nThe causal co-pilot can collaborate with researchers to enhance study design by critiquing initial design choices made by researchers and identifying potential sources of bias, thereby intercepting a chain of problems that may result from issues with the definition of the estimand itself. Below, we describe a historical instance of this chain effect; specifically, a flawed OS design which induced immortal time bias. We then discuss how the LLM causal co-pilot could have intercepted this issue at the study design phase.\nDoes statin use reduce risk of future cancer? Statins are medications that reduce LDL-cholesterol. In the early 2000s, several OS suggested a substantial decrease in the risk of various cancers, including colorectal and lung cancer, associated with statin use (Graaf et al. (2004); Poynter et al. (2005); Khurana et al. (2007)). These results were later contradicted by RCTs that found no impact of statins on cancer risk (Collaboration (2012); Dale et al. (2006)).\nA critical review of the OS identified that the bias originated primarily from a failure to establish a suitable \"time zero\" or \"baseline\" (starting point from which all measurements are taken). That is, there was a lack of synchronization between patient eligibility criteria and treatment assignment times (Dickerman et al. (2019)). This misalignment induces a form of selection bias known as \u201cimmortal time bias\" (a period during which the outcome cannot occur is incorrectly included in the analysis), which can lead to inflated effect estimates (Yadav & Lewis (2021)).\nAn LLM causal co-pilot can intercept bias and provide plausible explanation regarding its source. In Figure 3 (Panel B), we provide the LLM with the entire text of the OS in (Graaf et al. (2004)) and ask it to check for any aspects of the study design that may lead to immortal time bias. The LLM highlights the definition of statin use in the study as a potential issue, which is a plausible explanation of the source of bias based on the study details. However, similar to the previous example, the ability of the LLM to detect bias in this study relies on receiving cues on what to examine; otherwise, its responses are typically generic."}, {"title": "3.3 Interpreting Study Results", "content": "Another way the causal co-pilot can collaborate with researchers is by having the LLM critically analyze study results to uncover signs of residual confounding or other biases. Below, we illustrate how this can be done, drawing on various OS that examined the effectiveness of Nirmatrelvir/Ritonavir (brand name \"Paxlovid\") in the more recent omicron variant surges of the COVID-19 pandemic.\nDoes Paxlovid reduce the risk of hospitalization and mortality in COVID-19 patients? During the COVID-19 pandemic, the timeliness and urgency of policy decisions necessitated the use of RWD to gather evidence on pharmaceutical interventions. In 2021, Paxlovid received emergency use authorization from the FDA for COVID-19 treatment. The primary data supporting the authorization was based on the EPIC-HR trial-an RCT that studied the efficacy of Paxlovid in non-hospitalized adults with a confirmed SARS-CoV-2 infection (Hammond et al. (2022)). Following the EPIC-HR trial, various OS subsequently examined the effectiveness of Paxlovid in the more recent omicron variant surges. One OS found that the treatment was associated with improved 28-day outcomes compared to no antiviral treatment (Aggarwal et al. (2023)).\nA common indicator of residual confounding in OS of COVID-19 vaccines and drugs is the premature separation of survival curves for treated and control patients, occurring before the mechanism of action for the drug could influence outcomes (Mohyuddin & Prasad (2023)). In this study, the reported cumulative hazard curves separated at time zero, which is implausible given that the effect of Paxlovid takes place a few days after administration.\nAn LLM co-pilot can scrutinizes study results and uncovers biases based on visual inputs of scientific graphs. In Figure 3 (Panels C & D), we show the LLM images containing the results of the OS of Paxlovid in Aggarwal et al. (2023) (Panel C) and the results of the EPIC-HR trial, with the LLM blinded to the fact that it was an RCT (Panel D). The LLM is explicitly instructed to examine any raw data provided by the user and check for indications of residual confounding. The LLM responses to the images demonstrated an ability to process visual input, comprehend the displayed data, identify the intervention being studied, and draw conclusions based on its knowledge of the mechanism of action of Paxlovid. In this example, the LLM suspected a residual confounding bias in the results of the OS. Similar to the previous examples, we included instruction for the LLM on how to look for residual confounding in the study results (see Appendix). Notably, in Panel D the LLM recognized that the data comes from an RCT rather than an observational study as the user \u201cmistakenly\" indicated in the prompt. It did so by recognizing the existence of a placebo group in the input figure."}, {"title": "4 The Causal Co-Pilot Framework", "content": "The examples in Section 3 illustrate how LLMs can function as causal co-pilots by actively collaborating with researchers to refine study design, critique existing studies, or identify potential biases in study results. Importantly, the applications of these LLM co-pilots go beyond OS to various types of studies using RWD. In this section, we explore the potential role of LLMs as causal co-pilots in the current landscape of these studies (Section 4.1) and present their envisioned system architecture (Section 4.2)."}, {"title": "4.1 Role of LLM Co-Pilots in RWD Studies", "content": "The potential of LLMs to serve as co-pilots in medical studies hinges on two critical factors: (i) pretraining on diverse internet data, which enables LLMs to encode extensive medical and statistical domain knowledge (Jin et al. (2019); Singhal et al. (2023)), and (ii) the capability to reason and communicate with users in natural language, which enables LLMs to contextualize its encoded domain knowledge (Wei et al. (2022); Kojima et al. (2022); Pal et al. (2022); Nori et al. (2023a)). As depicted in the spectrum of studies in Figure 2, the increasing reliance on RWD introduces more design choices for researchers, which raises the likelihood of poor decisions and calls for the co-pilot's involvement in the study design process.\nIn the previous section, we explored how an LLM co-pilot can contribute to the design and interpretation of OS, which is positioned at the far end of the study spectrum. The co-pilot's applicability is more broad, however; studies that only partially rely on RWD can benefit from this framework. For example, in externally controlled trials, data for the control arm may be collected from RWD sources such as disease registries, electronic health records (EHRs), and medical claims (Jahanshahi et al. (2021)). This introduces additional design considerations, such as RWD population characteristics, treatment attributes, determination of \"time zero\u201d, and outcome assessment. Additionally, it raises data considerations, such as assessing the comparability of trial arms. The LLM co-pilot can assist researchers in selecting RWD for the control arm in an externally-controlled trial by using knowledge on the timing, frequency of data collection, and care patterns across different RWD sources (e.g., EHR vs. disease registries vs. medical claims)."}, {"title": "4.2 Causal Co-Pilot System Architecture", "content": "To fulfill the vision described above, we envision the architecture of the causal co-pilot resembling that of other assistant tools such as Github Copilot (Nguyen & Nadi (2022)) (Figure 1). The causal co-pilot should adeptly process user input prompts from diverse stakeholders, such as clinical and biopharmaceutical researchers, addressing causal questions with clinical equipoise. For instance, the user may inquire about the scenarios discussed in Section 3. Additionally, users can specify details about the RWD in use, such as its origin from an EHR or a claims database. Such key attributes of the study design (presented in Section 2) provide key inputs to the co-pilot as the relevant guidance, ethical concerns, and implications will vary depending on this context. The co-pilot will then refine the specificity of the user prompt by grounding it in clinical, causal and statistical domain knowledge, as well as potential in regulatory guidance for RWE studies, as we discuss in more detail in the next Section. This grounding ensures that the underlying LLM consistently delivers relevant and accurate responses that are useful for the researcher and tailored to the specific medical study. The co-pilot tool can integrate within commonly used tools for conducting and documenting statistical analyses, such as RStudio and Jupyter notebooks. In addition to assisting in study design steps, it can automatically generate documentation of study design parameters and rationale behind design choices. This could enhances the transparency of these studies and mitigate concerns regarding data dredging (Smith & Ebrahim (2002)).\nIn this framework, one can imagine the LLM co-pilot as a \u201cresearch agent\" that undertakes different \"actions\" throughout the research process, similar to Boiko et al. (2023). In this sense, the action space of the co-pilot would encompass typical atomic tasks within the research process, such as \"Search the web\", \"Write code\u201d, \u201cDocument\", and \u201cCheck FDA Guidance\u201d. The co-pilot can operate with a fixed system prompt, following a predetermined logic in selecting actions during user interactions.\""}, {"title": "5 Developing a Causal Co-Pilot: Research Directions & Challenges", "content": "TL;DR: To make LLMs effective as causal co-pilots, we must go beyond standard training methods. The challenge lies in the presence of flaws in many published studies that would be used for training, as well as the lack of real-world instruction-tuning data. Developing a causal co-pilot requires novel techniques grounded in causal inference frameworks and regulatory guidelines.\nIn the previous sections, we provided an overview of the medical studies landscape and outlined the potential use cases for LLM co-pilots in supporting the design of these studies. LLM-based assistance tools have found applications in various domains, including medical conversational models for diagnostic assistance (Thirunavukarasu et al. (2023)), co-pilots for programming (Barke et al. (2023)), and autonomous agents for research (Boiko et al. (2023)). This section underscores the unique aspects of developing a causal co-pilot with respect to its grounding, finetuning, alignment, human factors, and evaluation."}, {"title": "5.1 Grounding and Structured Decoding: Regulatory and Causal Frameworks", "content": "Despite the potential benefits of leveraging LLMs to enhance the design of medical studies, relying on an off-the-shelf LLM as a \u201czero-shot\u201d co-pilots is unlikely to offer reliable assistance to researchers. The examples in Figure 3 required meticulous and iterative prompting of GPT-4 to produce sensible responses. Often, the pretrained LLM generates generic responses lacking sufficient contextualization for the specific study. Consequently, a primary design challenge in developing causal co-pilots is in adapting an LLM to consistently provide relevant responses that genuinely assist researchers in designing the medical study at hand. This involves anchoring its responses in real-world knowledge and ensuring it maintains relevance to the context. Sources of domain-knowledge for grounding the co-pilot include: (i) causal frameworks developed by previous research for the design of medical studies, and (ii) regulatory guidance on the design, data and analysis considerations that should be addressed in such studies.\nCausal Frameworks. The Causal Roadmap is an explicit, iterative step-by-step guide for investigators to follow in the prespecification of study design and statistical analysis plans (e.g., Dang et al. (2023)). It addresses a wide range of questions and study types within a single framework. Following the Causal Roadmap leads to two outcomes: (i) detailing key elements of a study design likely to yield high-quality RWE; or (ii) reaching an evidence-based conclusion that producing the required level of RWE is presently unfeasible, while also identifying what data is necessary for future credible RWE generation. We refer the reader to Dang et al. (2023) for a thorough explanation.\nAnother popular causal framework is the Target Trial Emulation approach (e.g., proposed in Hern\u00e1n & Robins (2016)). This framework tries to emulate the conditions under which a clinical trial would have been conducted to answer the causal question researchers are trying to address with RWD. Similar to (and compatible with) the Roadmap Dang & Balzer (2023), Target Trial Emulation provides structured processes for critiquing medical studies based on RWD to help avoid common methodological pitfalls. This framework was used to critique the OS examples discussed in Section 3 (Dickerman et al. (2019)).\nRegulatory Guidance. Regulatory bodies, such as the FDA, Health Canada, and European Medicines Agency, have established frameworks outlining the criteria and standards for RWD in healthcare and RWE evaluation in regulatory decision-making. These guidelines encompass aspects like data quality, reliability, relevance, and the ethical implications of data use. There are also specific guidelines dedicated to the data or study types, such as externally controlled trials, non-interventional studies, data standards, and EHR and claims data. By incorporating these regulatory principles into LLMs, researchers can ensure that the generated insights adhere to the highest standards of scientific rigor and ethical compliance. This alignment not only bolsters the credibility and acceptability of the causal co-pilot but also offers the potential to facilitate smoother regulatory review processes. Moreover, grounding LLMs in these guidelines enhances their ability to provide contextually relevant, regulation-compliant advice, thereby optimizing the design, execution, and analysis of medical studies using RWD. This integration forms a critical bridge between cutting-edge AI technology and the stringent regulatory landscape governing medical research, paving the way for more effective, efficient, and ethically sound healthcare innovations.\nFuture research can investigate how causal and regulatory frameworks can be used to ground causal co-pilots and enforce a structured response to improve the reliability of LLM output. Causal frameworks can be used to ground the co-pilot by using the structured processes defined by these frameworks to systematically refine the user defined prompts and decoding constraints in order to improve the responses of the underlying LLM. These frameworks can also be used to design system prompts that determine the overall behavior of the causal co-pilot. For instance, one can inject a structured description of the different steps of the causal roadmap or the trial emulation framework. LLM decoding programs or queries can also imnprove reliability of the causal co-pilot by enforcing these steps and the careful, structured reasoning they suggest (Beurer-Kellner et al. (2023)). FDA guidance documents and reports can also be used as a knowledge base for retrieval augmentation methods that interface with the causal co-pilot to resurface design and analysis considerations relevant to the study at hand (Shuster et al. (2021))."}, {"title": "5.2 Finetuning: Instruction-Tuning Data for the Causal Co-Pilot", "content": "While grounding a general-purpose LLM using external sources of knowledge on study design can improve its utility in assisting researchers in designing medical studies, finetuning the LLM itself using high-quality training data for the problem of medical study design might be a more effective way to steer the LLM to function as causal co-pilots (Nori et al. (2023b); Hernandez et al. (2023)) and reduce \"hallucinations\" in research tasks related to medical study design. Ideally, we would like the causal co-pilot to learn from high-quality \u201cinstruction-tuning\" data, containing demonstrations of medical studies, researcher instructions, and ideal responses from the co-pilot. Similar instruction-tuning datasets are used to train LLMs, such as InstructGPT and FLAN-T5 (Chung et al. (2022)) as well as multimodal models such as LLaVA (Liu et al. (2023)).\nHowever, generating high-quality instruction-tuning data for the causal co-pilot is a challenging task because such data are not naturally occurring or collected in practice. A common approach to alleviate this problem is to use a \"self-instruct\u201d method: an approach which uses machine-generated, instruction-following data from a state-of-the-art instruction-tuned teacher LLMs (Wang et al. (2022); Peng et al. (2023)). For instance, the LLaVA model in Liu et al. (2023) was training using instruction-tuning data generated using a language-only GPT-4 by synthetically processing image captions to generate question-answer pairs associated with each image.\nDeveloping methods for generating instruction-tuning data for the causal co-pilot is an interesting direction for future research. This is a much more challenging task than the self-instruct approaches, where the instruction following data was tailored to common-sense question answering and visual reasoning tasks that easily can be automatically generated or annotated by non-experts. On the contrary, synthetic instruction following datasets that demonstrate the steps involved in the study design process must be grounded to a medical context and are non-trivial to generate. A potential approach for generating these datasets is to use the causal frameworks and regulatory guidance discussed in Section 5.1 to develop structured simulators of synthetic user-LLM interactions that encode knowledge about study design."}, {"title": "5.3 Alignment: Learning from Peer Review", "content": "One driver behind the recent success of LLMs is reinforcement learning from human feedback (RLHF) (Ouyang et al. (2022)). In this finetuning approach, the LLM generations are \"aligned\" with human preferences by gathering feedback on various responses to the same prompt. This human feedback is then used to create a reward model predicting preferences on new training data, which in turn refines the LLM through further finetuning. While gathering human feedback for general question-answering tasks is feasible through human annotators, obtaining expert feedback on medical study designs is a more challenging and less scalable process. Future research directions could explore alternative sources for constructing reward models that bypass the bottleneck of expert annotations. One approach is to view reward modeling as a scientific \"peer review\" process, where models are trained using peer reviews of submitted papers or regulatory evaluations of FDA applications, thereby aligning causal co-pilots with scientific and regulatory standards."}, {"title": "5.4 Human Factors: Interactions with Causal Co-pilots", "content": "A key research challenge in building a reliable causal co-pilot is creating a productive interaction between the human scientists and the co-pilot (Amershi et al. (2019)). Even simple design decisions, such as whether the co-pilot or the human scientists takes the initiative in executing analysis, whether critiques are presented as mere suggestions or definitive warnings, and details of assistance can strongly influence whether the human considers, trusts and validates, or over-relies on LLM outputs (Gu et al. (2023))."}, {"title": "5.5 Evaluation: Metrics and Benchmarks", "content": "Another challenge in developing causal co-pilots is evaluating their success. While it is possible to test the medical knowledge that LLMs encode using benchmarks like USMLE tests (Singhal et al. (2022)), there is currently no benchmark for medical study design. Creating a benchmark with truly held-out test cases is challenging because testing data from published medical studies might already be part of the LLM's training data. For instance, it is unclear whether the model responses in Figure 3 represent the its true capabilities or result from memorization of published studies. Future research could explore defining new metrics to assess LLM-guided study design in terms of alignment with regulatory standards and generating synthetic study design tasks for testing in truly unseen settings."}, {"title": "5.6 Related Work", "content": "Research agents in other domains. An example of adapting LLMs as research agents is the co-scientist framework in Boiko et al. (2023). The co-scientist is a system powered by GPT-4 that autonomously tackles complex experiments by interacting with various APIs and tools. While it streamlines workflows in fields like organic chemistry, the co-scientist model differs from the causal co-pilot. In natural sciences, experiments are confined to well-defined domains and are free from socio-technical biases. Medical study designs, on the other hand, involve unique contexts, domain-specific knowledge, and highly complex and imperfect data with potential biases, large numbers of latent factors and unmeasured determinants and errors. As a result, we suggest that an effective causal co-pilot for RWE generating in health will not serve to automate or streamline operation, but to collaborate with researchers through an interactive thought process throughout the study design.\nLLMs for causal inference and data analysis. Another line of work that is closely related to the causal co-pilot examines the capabilities of LLMs in engaging in causal reasoning. For instance, K\u0131c\u0131man et al. (2023) explores the capabilities of LLMs in various causal reasoning tasks and finds particular promise in LLMs as a source of domain knowledge to augment human domain experts in determining appropriate causal assumptions. Vashishtha et al. (2023) builds on this work, using LLMs for identifying causal estimands for causal effect inference. Lengerich et al. (2023) further finds that LLMs are capable of interpreting correlational relationships within datasets and linking them to potential mechanisms and identifying surprises. While the focus on causal reasoning capabilities of LLMs represents a distinct research direction from our proposed agenda, advancements in this area will inevitably impact our agenda. A causal co-pilots may build on such capabilities to identify causal assumptions critical to observational studies, as well as potential biases, and systematic measurement errors that may threaten validity of studies."}, {"title": "5.7 Potential Limitations and Harms", "content": "While an LLM-powered research pipeline holds promising prospects, there exists a potential downside to relying too heavily on systems that are not fully explainable or transparent in our research practices. Erroneous but convincing outputs from LLMs may lead to inaccuracies in defining the causal estimand or identifying relevant confounders. This in turn could result in"}]}