{"title": "Explainable Artificial Intelligence:\nA Survey of Needs, Techniques, Applications, and Future Direction", "authors": ["Melkamu Mersha", "Khang Lamb", "Joseph Wood", "Ali AlShami", "Jugal Kalita"], "abstract": "Artificial intelligence models encounter significant challenges due to their black-box nature, particularly in safety-critical domains\nsuch as healthcare, finance, and autonomous vehicles. Explainable Artificial Intelligence (XAI) addresses these challenges by\nproviding explanations for how these models make decisions and predictions, ensuring transparency, accountability, and fairness.\nExisting studies have examined the fundamental concepts of XAI, its general principles, and the scope of XAI techniques. However,\nthere remains a gap in the literature as there are no comprehensive reviews that delve into the detailed mathematical representa-\ntions, design methodologies of XAI models, and other associated aspects. This paper provides a comprehensive literature review\nencompassing common terminologies and definitions, the need for XAI, beneficiaries of XAI, a taxonomy of XAI methods, and\nthe application of XAI methods in different application areas. The survey is aimed at XAI researchers, XAI practitioners, AI model\ndevelopers, and XAI beneficiaries who are interested in enhancing the trustworthiness, transparency, accountability, and fairness of\ntheir AI models.\n\nKeywords: XAI, explainable artificial intelligence, interpretable deep learning, machine learning, neural networks, evaluation\nmethods, computer vision, natural language processing, NLP, transformers, time series, healthcare, and autonomous cars.", "sections": [{"title": "1. Introduction", "content": "Since the advent of digital computer systems, scientists have\nbeen exploring ways to automate human intelligence via com-\nputational representation and mathematical theory, eventually\ngiving birth to a computational approach known as Artificial\nIntelligence (AI). AI and machine learning (ML) models are\nbeing widely adopted in various domains, such as web search\nengines, speech recognition, self-driving cars, strategy game-\nplay, image analysis, medical procedures, and national de-\nfense, many of which require high levels of security, transpar-\nent decision-making, and a responsibility to protect information\n[1, 2]. Nevertheless, significant challenges remain in trusting\nthe output of these complex ML algorithms and AI models be-\ncause the detailed inner logic and system architectures are ob-\nfuscated by the user by design.\nAI has shown itself to be an efficient and effective way to\nhandle many tasks at which humans usually excel. In fact, it\nhas become pervasive, yet hidden from the casual observer, in\nour day-to-day lives. As AI techniques proliferate, the imple-\nmentations are starting to outperform even the best expectations\nacross many domains [3]. Since AI solves difficult problems,\nthe methodologies used have become increasingly complex. A\ncommon analogy is that of the black box, where the inputs are\nwell-defined, as are the outputs. However, the process is not\ntransparent and cannot be easily understood by humans. The\nAl system does not usually provide any information about how\nit arrives at the decisions it makes. The systems and processes\nused in decision-making are often abstruse and contain uncer-\ntainty in how they operate. Since these systems impact lives,\nit leads to an emerging need to understand how decisions are\nmade. Lack of such understanding makes it difficult to adopt\nsuch a powerful tool in industries that require sensitivity or that\nare critical to the survival of the species.\nThe black-box nature of AI models raises significant con-\ncerns, including the need for explainability, interpretability, ac-\ncountability, and transparency. These aspects, along with legal,\nethical, and safety considerations, are crucial for building trust\nin AI, not just among scientists but also among the wider pub-\nlic, regulators, and politicians who are increasingly attentive to\nnew developments. With this in mind, there has been a shift\nfrom just relying on the power of AI to understanding and in-\nterpreting how AI has arrived at decisions, leading to terms such\nas transparency, explainability, interpretability, or, more gener-\nally, eXplainable Artificial Intelligence (XAI). A new approach\nis required to trust the AI and ML models, and though much\nhas been accomplished in the last decades, the interpretability\nand black-box issues are still prevalent [4, 5]. Attention given\nto XAI has grown steadily, and XAI has attracted a\nthriving number of researchers, though there still exists a lack\nof consensus regarding symbology and terminology. Contri-\nbutions rely heavily on their own terminology or theoretical\nframework [6].\nResearchers have been working to increase the interpretabil-\nity of AI and ML models to gain better insight into black-box\ndecision-making. Questions being explored include how to ex-\nplain the decision-making process, approaches for interpretabil-\nity and explainability, ethical implications, and detecting and"}, {"title": "2. Background and Motivation", "content": "Black-box Al systems have become ubiquitous and are per-\nvasively integrated into diverse areas. XAI has emerged as a\nnecessity to establish trustworthy and transparent models, en-\nsure governance and compliance, and evaluate and improve the\ndecision-making process of AI systems.\n2.1. Basic Terminology\nBefore discussing XAI in-depth, we briefly present the basic\nterminology used in this work.\nAI systems can perform tasks that normally require human\nintelligence [25]. They can solve complex problems, learn from\nlarge amounts of data, make autonomous decisions, and under-\nstand and respond to challenging prompts using complex algo-\nrithms.\nXAI systems refer to Al systems that are able to provide\nexplanations for their decisions or predictions and give insight\ninto their behaviors. In short, XAI attempts to understand\n\"WHY did the AI system do X?\". This can help build compre-\nhensions about the influences on a model and specifics about\nwhere a model succeeds or fails [11].\nTrust is the degree to which people are willing to have con-\nfidence in the outputs and decisions provided by an AI system.\nA relevant question is: Does the user trust the output enough to\nperform real-world actions based upon it? [26].\nMachine learning is a rapidly evolving field within com-\nputer science. It is a subset of AI that involves the creation of\nalgorithms designed to emulate human intelligence by captur-\ning data from surrounding environments and learning from such\ndata using models, as discussed in the previous paragraph [27].\nML imitates the way humans learn, gradually improving accu-\nracy over time based on experience. In essence, ML is about en-\nabling computers to think and act with less human intervention\nby utilizing vast amounts of data to recognize patterns, make\npredictions, and take actions based on that data.\nModels and algorithms are two different concepts. How-\never, they are used together in the development of real-world\nAl systems. A model (in the context of machine learning) is a\ncomputational representation of a system whose primary pur-\npose is to make empirical decisions and predictions based on\nthe given input data (e.g., neural network, decision tree, or lo-\ngistic regression). In contrast, an algorithm is a set of rules or\ninstructions used to perform a task. The models can be simple\nor complex, and trained on the input data to improve their accu-\nracy in decision-making or prediction. Algorithms can also be\nsimple or complex, but they are used to perform a specific task\nwithout any training. Models and algorithms differ by output,\nfunction, design, and complexity [28].\nDeep learning refers to ML approaches for building multi-\nlayer (or \"deep\") artificial neural network models that solve\nchallenging problems. Specifically, multiple (and usually com-\nplex) layers of neural networks are used to extract features from\ndata, where the layers between the input and output layers are\n\"hidden\" and opaque [29].\nA black-box model refers to the lack of transparency and\nunderstanding of how an Al model works when making pre-\ndictions or decisions. Extensive increases in the amount of data\nand performance of computational devices have driven AI mod-\nels to become more complex, to the point that neural networks"}, {"title": "2.2. Need for Explanation", "content": "Black-box Al systems have become ubiquitous throughout\nsociety, extensively integrated in a diverse range of disciplines,\nand can be found permeating many aspects of daily activities.\nThe need for explainability in real-world applications is multi-\nfaceted and essential for ensuring the performance and reliabil-\nity of AI models while allowing users to work effectively with\nthese models. XAI is becoming essential in building trustwor-\nthy, accountable, and transparent AI models to satisfy delicate\napplication designs [31, 32].\nTransparency: Transparency is the capability of an AI sys-\ntem to provide understandable and reasonable explanations of a\nmodel's decision or prediction process [4, 33, 34]. XAI systems\nexplain how AI models arrive at their prediction or decision so\nthat experts and model users can understand the logic behind\nthe AI systems [17, 35], which is crucial for trustworthiness and\ntransparency. Transparency has a meaningful impact on peo-\nple's willingness to trust the AI system by using directly inter-\npretable models or availing XAI system explanations [36]. For\nexample, if on a mobile device, voice-to-text recognition sys-\ntems produce wrong transcription, the consequences may not\nalways be a big concern although it may be irritating. This may\nalso be the case in a chat program like ChatGPT if the questions\nand answers are \"simple\". In this case, the need for explainabil-\nity and transparency is less profound. In contrast, explainabil-\nity and transparency are crucial in critical safety systems such\nas autonomous vehicles, medical diagnosis and treatment sys-\ntems, air traffic control systems, and military systems [2].\nGovernance and compliance issues: XAI enables gover-\nnance in AI systems by confirming that decisions made by AI\nsystems are ethical, accountable, transparent, and compliant\nwith any laws and regulations. Organizations in domains such\nas healthcare and finance can be subject to strict regulations,\nrequiring human understanding for certain types of decisions\nmade by AI models [1, 37, 38]. For example, if someone is\ndenied a loan by the bank's Al system, he or she may have the\nright to know why the AI system made this decision. Simi-\nlarly, if a class essay is graded by an AI and the student gets\na bad grade, an explanation may be necessary. Bias is often\npresent in the nature of ML algorithms' training process, which\nis sometimes difficult to notice. This raises concerns about an\nalgorithm acting in a discriminatory way. XAI has been found\nto serve as a potential remedy for mitigating issues of discrim-\nination in the realms of law and regulation [39]. For instance,"}, {"title": "2.3. Stakeholders of the XAI\u0399", "content": "Broadly speaking, all users of XAI systems, whether direct\nor indirect, stand to benefit from AI technology. Some of the\nmost common beneficiaries of the XAI system are identified in\nFigure 3.\nSociety: XAI plays a significant role in fostering social col-\nlaboration and human-machine interactions [2] by increasing\nthe trustworthiness, reliability, and responsibility of AI systems,\nhelping reduce the negative impacts such as unethical use of AI\nsystems, discrimination, and biases. Hence, XAI promotes trust\nand the usage of models in society.\nGovernments and associated organizations: Governments\nand governmental organizations have become AI system users.\nTherefore, the government will be greatly benefited by X\u0391\u0399\nsystems. XAI can help develop the government's public pol-\nicy decisions, such as public safety and resource allocation, by\nmaking them transparent, accountable, and explainable to soci-\nety.\nIndustries: XAI is crucial for industries to provide trans-\nparent, interpretable, accountable, and trustable services and\ndecision-making processes. XAI can also help industries iden-\ntify and reduce the risk of errors and biases, improve regulatory\ncompliance, enhance customer trust and confidence, facilitate\ninnovations, and increase accountability and transparency.\nResearchers and system developers: The importance of\nXAI to researchers and AI system developers cannot be over-\nstated, as it provides critical insights that lead to improved\nmodel performance. Specifically, XAI techniques enable them\nto understand how AI models make decisions, and enable the\nidentification of potential improvement and optimization. X\u0391\u0399\nhelps facilitate innovation and enhance the interpretability and\nexplainability of the model. From a regulatory perspective, X\u0391\u0399\ncan help enhance compliance with legal issues, in particular\nlaws and regulations related to fairness, privacy, and security\nin the AI system. Finally, XAI can facilitate the debugging pro-\ncess critical to researchers and system developers, leading to\nthe identification and correction of errors and biases."}, {"title": "2.4. Interpretability vs. Explainability", "content": "The concepts of interpretability and explainability are dif-\nficult to define rigorously. There is ongoing debate and re-\nsearch about the best ways to operationalize and measure these\ntwo concepts. Even terminology can vary or be used in con-\ntradictory ways, though the concepts of building comprehen-\nsion about what influences a model, how influence occurs, and\nwhere the model performs well and fails, are consistent within\nthe many definitions of these terms. Most studies at least agree\nthat explainability and interpretability are related but distinct\nconcepts. Previous work suggests that interpretability is not a\nmonolithic concept but a combination of several distinct ideas"}, {"title": "3. Categories of Explainability Techniques", "content": "In this section, we introduce a taxonomy for XAI techniques\nand use specific criteria for general categorization. These ex-"}, {"title": "3.1. Local and Global Explanation Techniques", "content": "Local and global approaches refer to the scope of explana-\ntions provided by an explainability technique. Local explana-\ntions are focused on explaining predictions or decisions made\nby a specific instance or input to a model [2, 10]. This approach\nis particularly useful for examining the behavior of the model\nin relation to the local, individual predictions or decisions.\nGlobal techniques provide either an overview or a complete\ndescription of the model, but such techniques usually require\nknowledge of input data, algorithm, and trained model [44].\nThe global explanation technique needs to understand the\nwhole structures, features, weights, and other parameters. In\npractice, global techniques are challenging to implement since\ncomplex models with multiple dimensions, millions of param-\neters, and weights are challenging to understand."}, {"title": "3.2. Ante-hoc and Post-hoc Explanation Techniques", "content": "Ante-hoc and post-hoc explanation techniques are two dif-\nferent ways to explain the inner workings of AI systems. The\ncritical difference between them is the stage in which they\nare implemented [7]. The ante-hoc XAI techniques are em-\nployed during the training and development stages of an AI sys-\ntem to make the model more transparent and understandable,\nwhereas the post-hoc explanation techniques are employed af-\nter the AI models have been trained and deployed to explain\nthe model's prediction or decision-making process to the model\nusers. Post-hoc explainability focuses on models which are\nnot readily explainable by ante-hoc techniques. Ante-hoc and\npost-hoc explanation techniques can be employed in tandem to\ngain a more comprehensive comprehension of Al systems, as\nthey are mutually reinforcing [10]. Some examples of ante-\nhoc XAI techniques are decision trees, general additive models,\nand Bayesian models. Some examples of post-hoc XAI tech-\nniques are Local Interpretable Model-Agnostic Explanations\n(LIME) [26] and Shapley Additive Explanations (SHAP) [52].\nArrieta et al. [10] classify the post-hoc explanation tech-\nniques into two categories:\n\u2022 Model-specific approaches provide explanations for the\npredictions or decisions made by a specific AI model,\nbased on the model's internal working structure and de-\nsign. These techniques may not apply to other models with\nvarying architectures, since they are designed for specific\nmodels [4]. However, a model-specific technique provides"}, {"title": "3.3. Perturbation-based and Gradient-based \u03a7\u0391\u0399", "content": "Perturbation-based and gradient-based methods are two of\nthe most common algorithmic design methodologies for devel-\noping XAI techniques. Perturbation-based methods operate by\nmodifying the input data, while gradient-based methods calcu-\nlate the gradients of the model's prediction with respect to its\ninput data. Both techniques compute the importance of each\ninput feature through different approaches and can be used for\nlocal and global explanations. Additionally, both techniques are\ngenerally model-agnostic.\nPerturbation-based XAI methods use perturbations to deter-\nmine the importance of each feature in the model's prediction\nprocess. These methods involve modifying the input data, such\nas removing certain input examples, masking specific input fea-\ntures, producing noise over the input features, observing how\nthe model's output changes as a result, generating perturbations,\nand analyzing the extent to which the output is affected by the\nchange of the input data. By comparing the original output with\nthe output from the modified input, it is possible to infer which\nfeatures of the input data are most important for the model's\nprediction [26]. The importance of each feature value provides\nvaluable insights into how the model made that prediction [52]."}, {"title": "4. Detailed Discussions on XAI Techniques", "content": "XAI techniques differ in their underlying mathematical prin-\nciples and assumptions, as well as in their applicability and lim-\nitations. We classify the widely used XAI techniques based on\nperturbation, gradient, and the use of the Transformer [56] ar-\nchitecture. The Transformer has become a dominant architec-\nture in deep learning, whether it is in natural language process-\nsing, computer vision, time series data, or anything else. As a\nresult, we include a separate section on Transformer explain-\nability.\n4.1. Perturbation-based Techniques\nPerturbation-based XAI methods are used to provide local\nand global explanations of the black-box models by making\nsmall and controlled changes to the input data to gain insights\ninto how the model made that decision. This section discusses\nthe most predominant perturbation-based XAI techniques, such\nas LIME, SHAP, and Counterfactual Explanations (CFE), in-\ncluding their mathematical formulation and underlying assump-\ntions.\n4.1.1. LIME\nA standard definition of a black-box model $f$, where the in-\nternal workings are unknown, is $f: X \\rightarrow Y$, where $X$ and"}, {"title": "4.1.2. SHAP", "content": "SHAP [52] is a model-agnostic method, applicable to any\nML model, ranging from simple linear models to complex\nDNN. This XAI technique employs contribution values as\nmeans for explicating the extent to which features contribute\nto a model's output. The contribution value is then leveraged to\nexplain the output of a given instance $x$. SHAP computes the\naverage contribution of each feature through the subset of fea-\ntures by simulating the model's behavior for all combinations\nof feature values. The difference in output is computed when\na feature is excluded or included in that output process. The\nsubsequent contribution values give a measure of the feature\nrelevance, which is significant to the model's output [52, 58].\nAssume $f$ is the original or black-box model, $g$ is the expla-\nnation model, $M$ is the number of simplified input features, $x$ is\na single input, and $x'$ is a simplified input such that $x = h_x(x')$.\nAdditive feature attribution methods, such as SHAP, have a lin-\near function model explanation with binary variables.\n$g(x') = \\Phi_0 + \\sum_{i=1}^{M} \\Phi_i x'_i$(2)"}, {"title": "4.1.3. CFE", "content": "CFE [59] is used to explain the predictions made by the\nML model using generated hypothetical scenarios to under-\nstand how the model's output is affected by changes in input\ndata. The standard classification models are trained to find the\nthe optimal set of weights $w$:\n$\\underset{w}{\\operatorname{argmin}} \\zeta(f_w(x_i), y_i) + p(w),$(3)\nwhere $f$ is a model, $p$ is the regularizer to prevent overfitting\nin the training process, $y_i$ is the label for data point $x_i$, and $w$\nrepresents the model parameters to be optimized. The argument"}, {"title": "4.2. Gradient-based Techniques", "content": "Gradient-based techniques use the gradients of the output\nwith respect to the input features. They can handle high-\ndimensional input space, are scalable for large datasets, pro-\nvide deep insights into a model, and help detect errors and bi-\nases. Saliency Maps, Layer-wise Relevance BackPropagation\n(LRP), Class Activation Maps (CAM), and Integrated Gradi-\nents are the most common gradient-based XAI techniques and\nthey are good frameworks for building other techniques.\n4.2.1. Saliency \u041c\u0430\u0440\nSimonyan et al. [60] utilized a saliency map for a model\nexplanation for the first time in deep CNN. As a visualiza-\ntion technique, a saliency map, which is a model-agnostic tech-\nnique, highlights important features in the image classification\nmodel by computing the output's gradients for the input image"}, {"title": "4.2.2. LRP", "content": "The main goal of LRP [61] is to explain each input feature's\ncontribution to the model's output by assigning a relevance\nscore to each neuron. LRP, as visualized in Figure 7, propa-\ngates the relevance score backward through the network layers.\nIt assigns a relevance score to each neuron, which allows for\nthe determination of the contribution of each input feature to\nthe output of the model.\nLRP is subject to the conservation property, which means a\nneuron that receives the relevance score must be redistributed\nto the lower layer in an equal amount. Assume $j$ and $k$ are two\nconsecutive layers, where layer $k$ is closer to the output layer.\nThe neurons in layer $k$ have computed their relevance scores,\ndenoted as $(R_k)_k$, propagating relevance scores to layer $j$. Then,\npropagated relevance score to neuron $R_j$ is computed using the\nfollowing formula [62]:\n$R_j = \\sum_k \\frac{z_{jk}}{\\sum_j z_{jk}} R_k,$(8)\nwhere $z_{jk}$ is the contribution of neuron $j$ to $R_k$ and $\\sum j Z_{jk}$ is used\nto enforce the conservation property. In this context, a pertinent\nquestion arises as to how do we determine $z_{jk}$, which represents\nthe contribution of a neuron $j$ to a neuron $k$ in the network, is\nascertained? LRP uses three significant rules to address this\nquestion [61].\n\u2022 The basic rule redistributes the relevance score to the input\nfeatures in proportion to their positive contribution to the\noutput.\n\u2022 The Epsilon rule uses an $\\epsilon$ to diminish relevance scores\nwhen contributions to neuron $k$ are weak and contradic-\ntory.\n\u2022 The Gamma rule uses a large value of $\\gamma$ to reduces negative\ncontribution or to lower noise and enhance stability.\nOverall, LRP is faithful, meaning that it does not introduce\nany bias into the explanation [61]. This is important for en-\nsuring that the explanations are accurate and trustworthy. LRP\nis complex to implement and interpret, which requires a good\nunderstanding of the neural networks' architecture. It is compu-\ntationally expensive for large and complex models to compute\nthe backpropagating relevance scores throughout all layers of\nthe networks. LRP is only applicable to backpropagation-based\nmodels like neural networks. It requires access to the internal\nstructure and parameters of the model, which is sometimes im-\npossible if a model is proprietary. LRP is a framework for other\nXAI techniques. However, there is a lack of standardization,\nwhich leads to inconsistent explanations through different im-\nplementations."}, {"title": "4.2.3. CAM", "content": "CAM [63"}]}