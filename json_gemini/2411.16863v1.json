{"title": "Augmenting Multimodal LLMs with Self-Reflective Tokens for Knowledge-based Visual Question Answering", "authors": ["Federico Cocchi", "Nicholas Moratelli", "Marcella Cornia", "Lorenzo Baraldi", "Rita Cucchiara"], "abstract": "Multimodal LLMs (MLLMs) are the natural extension of large language models to handle multimodal inputs, combining text and image data. They have recently garnered attention due to their capability to address complex tasks involving both modalities. However, their effectiveness is limited to the knowledge acquired during training, which restricts their practical utility. In this work, we introduce a novel method to enhance the adaptability of MLLMs by integrating external knowledge sources. Our proposed model, Reflective LLaVA (ReflectiVA), utilizes reflective tokens to dynamically determine the need for external knowledge and predict the relevance of information retrieved from an external database. Tokens are trained following a two-stage two-model training recipe. This ultimately enables the MLLM to manage external knowledge while preserving fluency and performance on tasks where external knowledge is not needed. Through our experiments, we demonstrate the efficacy of ReflectiVA for knowledge-based visual question answering, highlighting its superior performance compared to existing methods. Source code and trained models are publicly available at https://github.com/aimagelab/ReflectiVA.", "sections": [{"title": "1. Introduction", "content": "While the past few years have seen a surge of Large Language Models (LLMs) with increasing fluency and reasoning capabilities [5, 55, 64], thanks to the availability of large-scale training data and novel training techniques [13, 29, 52], the Computer Vision community has recently started extending the capabilities of such models beyond pure text, with the inclusion of additional modalities like images, video, and 3D data. The resulting emergence of Multimodal Large Language Models (MLLMs) has been characterized by the development of models targeting multiple tasks [7, 22, 62] \u2013 ranging from visual dialogue to image generation \u2013, architectural innovations [1, 38, 70], and novel training recipes [32, 44].\nWhat most existing MLLMs share, though, is their exclusive reliance on the knowledge learned at training time an issue that severely limits their practical applicability to cases that adhere to the training distribution. While this issue is also common to LLMs, it becomes more pressing in the case of MLLMs, where obtaining high-quality and large-scale multimodal data becomes even more difficult. Ideally, indeed, an MLLM should be capable of engaging in dialogues concerning specific visual details, long-tail knowledge, fine-grained categories, and instances [11, 49]. However, this type of knowledge makes it hard for MLLMs to encode in their parameters because such long-tail information occurs rarely in the training data. Additionally, this lack of precise knowledge can also lead the MLLM to generate incorrect answers, thus again limiting their usage in practical cases.\nA viable solution to this issue is to rely on a non-parametric approach, where content from external knowledge sources is incorporated into the MLLM context, con-"}, {"title": "2. Related Work", "content": "Multimodal Large Language Models. Large Language Models (LLMs) [16, 28, 64] have shown remarkable performance and adaptability across diverse tasks [12, 63], with recent extensions incorporating multimodal capabilities, particularly integrating vision and language [7, 37, 48, 62]. A primary challenge in this domain is effectively combining LLMs with visual features and creating multimodal datasets for robust training. Architectural solutions for vision-to-language integration vary, including single linear projections or MLPs, as in the LLaVA models [43, 44], as well as Q-Former [14, 38] or Perceiver [33] modules to extract fixed-dimensional visual features. Models like Flamingo [1, 3] utilize cross-attention layers to directly integrate multimodal information. For training, multiple stages often leverage image captions for visual-text alignment [10, 18], and specialized datasets are developed for visual instruction tuning [14, 32, 43], with data quality and annotation specificity shown to significantly influence performance [15].\nKnowledge-based Visual Question Answering. This task involves answering questions that require external or specialized knowledge beyond the content of the image itself. Early datasets like OK-VQA [47, 57] and KVQA [58] introduced questions that require general and commonsense knowledge, which large-scale architectures such as MLLMs can increasingly handle within their current training scope. Newer datasets, such as Encyclopedic-VQA [49] and InfoSeek [11], present greater challenges by focusing on highly specific, Wikipedia-scale knowledge. These require understanding detailed information about specific entities and nuanced encyclopedic facts. As a result, MLLMs often struggle in these settings, as they lack comprehensive coverage of this in-depth knowledge without relying on external sources.\nTo tackle this, contrastive image-text encoders [35, 54, 61, 66, 68] are crucial for retrieving semantically aligned content based on image-question queries. Relevant passages from external knowledge sources are then accessed, with entities represented by both textual passages and images. Recent works [8, 53, 69] have combined these retrieval methods with LLMs and MLLMs to enhance knowledge-based VQA tasks. For example, Wiki-LLaVA [8] integrates external multimodal knowledge via a hierarchical retrieval pipeline within a contrastive embedding space [54]. RoRA-VLM [53], instead, introduces a visual token refinement to filter out query-irrelevant visual information from both retrieved and query images. Recently, inspired by advances in NLP [2, 56, 71], EchoSight [69] proposes a Q-Former based re-ranking module to reorder retrieved textual passages before feeding them into the LLM. In contrast, our approach follows a different path enhancing MLLMs with specialized tokens that help to determine when retrieval is needed and assess the relevance of retrieved external knowledge.\nRetrieval-Augmented Language Models. In recent years, retrieval-augmented generation has become increasingly popular in the field of LLMs, especially when training or fine-tuning models for specific tasks or domains is impractical. This has led to various techniques [4, 21, 26, 30, 65] for integrating additional information extracted from external sources to enhance the generation quality of frozen LLMs. Recent efforts in the NLP literature [2] align with our pro-"}, {"title": "3. Proposed Method", "content": "Task Definition. In retrieval-augmented generation, given a textual query q and a query image I, an MLLM is expected to generate an answer y by possibly leveraging additional snippets S retrieved from an external knowledge source as context. The objective of multimodal retrieval-augmented generation can therefore be written as\n$y = \\arg \\max_y \\text{MLLM}(y|I, q, S)$.\nIn our setting, the external knowledge source S is composed of multimodal documents, each endowed with metadata (e.g., title and summary), textual passages organized in sections, and possibly visual content. Formally, the external database can be defined as a collection\n$S = \\{(t_i, P_i, \\hat{\\iota}_i)\\}_{i=1}^n,$\nwhere $t_i$ represents the metadata of a multimodal document, $P_i$ the set of its textual passages, and $\\hat{\\iota}_i$ its visual content.\nSummary of the Approach. To address the limitations of existing retrieval-augmented methods, our approach introduces two innovative strategies. Firstly, we enable the model to determine the optimal timing for retrieval, specifically when generating with an empty retrieval set $S$ is advantageous because the query does not require external information. Secondly, after the retrieval process, we empower the model to identify the relevance of retrieved passages for generation. Both abilities are enabled through the incorporation"}, {"title": "3.1. Adding Reflective Tokens", "content": "Given a pre-trained MLLM, we augment its vocabulary $V_0$ with a set of four additional tokens, i.e. {<RET>,<NORET>,<REL>,<NOREL>}, which will enable the model to distinguish whether retrieval is needed (<RET>, <NORET>) and whether a retrieved sample is relevant to the input query (<REL>, <NOREL>).\nGeneration Protocol. At test time, the MLLM is prompted with an input image I and a query q, and is asked to produce either the <RET> or the <NORET> token. If the <NORET> token is sampled, the MLLM will be asked to directly generate the answer y without relying on additional snippets. In this case, the generation process follows a schema\n\"<NORET>\" ~ MLLM ([I, q], {<RET>,<NORET>},1),\ny ~ MLLM([I, q, <NORET>], $V_0$),\nwhere y ~ MLLM(p, V, t) indicates that a sequence of tokens y is sampled (e.g. through beam search) from the MLLM when prompted with an input p and constrained to emit tokens belonging to a vocabulary V and up to a length of t tokens\u00b9. Finally, [\u00b7, \u00b7] indicates concatenation.\nIf instead the <RET> token is sampled, we firstly retrieve a set of candidate textual passages $S_0 = {$s_0, ..., s_k$} from the external knowledge base, and then ask the MLLM to"}, {"title": "3.2. Training an In-Article Reflective Model", "content": "Clearly, a coarse-grained retrieval at the document level is not enough precise to retrieve the exact passage containing the answer. Indeed, this retrieval step is expected to have limited recall at lower values of k. Further, a more detailed examination of relevant documents is necessary to identify passages that can be utilized by the MLLM.\nTo this aim, we train the MLLM to emit the relevance tokens <REL> and <NOREL>. This is done with a two-stage, two-model training pipeline. Initially, we train an in-article reflective MLLM capable of distinguishing between relevant passages and negative passages from the same article. Subsequently, we employ predictions from that model to train the final MLLM with the ability to cope with negative passages taken from the same articles and from different articles.\nAutomatic Data Construction. As most of the datasets for knowledge-based VQA do not provide human-labeled"}, {"title": "3.3. Training the Overall Model", "content": "In the second stage, predictions from the in-article reflective model are employed to construct the dataset for training the overall model. The capabilities of the in-article model are indeed used to automatically annotate textual passages from existing datasets that require external knowledge. This is also complemented by negative passages taken from other pages of the knowledge base, plus samples that do not need an external knowledge base. The ultimate result of this stage is an MLLM capable of both answering questions and generating special tokens to assess whether additional information retrieval is necessary and whether it would be beneficial for answering.\nData Curation. The training split of the Encyclopedic-VQA [49] and InfoSeek [11] datasets are employed to create the data collection to train the second stage. Each sample (I, q) is expanded with three distinct passages: a positive, a hard negative coming from the ground-truth page, and a soft negative coming from a different page.\nTo construct the first two cases, each sample is processed by the in-article reflective model to label each passage as either relevant or not relevant. After this, the passage with the highest probability of containing the <REL> token is taken as positive. Instead, the hard negative sample is randomly chosen from one of the sections predicted as <NOREL>. For the soft negative case, the image I is used to retrieve inside the top-1 page, excluding the ground-truth page. From"}, {"title": "4. Experiments", "content": "4.1. Experimental Setup\nDatasets. Our experiments are conducted on Encyclopedic-VQA [49] and InfoSeek [11], which contain question-answer pairs linked to documents from an external knowledge base (e.g. Wikipedia). Encyclopedic-VQA consists of 221k pairs associated with 16.7k fine-grained entities (i.e. Wikipedia pages). Questions are divided into single-hop and two-hop types: the former indicates that a single Wikipedia page is required to answer them, while the latter requires a sequential retrieval process across multiple documents. Dataset samples are split into training, validation, and test sets with 1M, 13.6k, and 5.8k items respectively. Experiments are reported on the test set, where single-hop questions correspond to 4.8k samples. The InfoSeek dataset, instead, contains 1.3M image-question pairs associated with around 11k Wikipedia pages. The dataset comprises 934k training, 73k validation, and 348k test samples. Following existing literature [8, 69], experimental results are reported on the validation set which includes questions not contained in the training split and questions associated with unseen entities.\nExternal Knowledge Bases. Both datasets come with an external knowledge base composed of Wikipedia documents. In particular, Encyclopedic-VQA contains a knowledge base of 2M Wikipedia pages. Each page includes the Wikipedia title, the corresponding textual sections, and associated images. InfoSeek, instead, provides a knowledge base composed of 6M Wikipedia entities. In our experiments, we use the original 2M knowledge base for Encyclopedic-VQA, while we extract a subset of 100k pages from the original 6M for InfoSeek, following recent works [8, 69].\nEvaluation Metrics. We follow the evaluation protocol provided along with the datasets. Generated answers for"}, {"title": "4.2. Comparison with the State of the Art", "content": "Results on Encyclopedic-VQA and InfoSeek. We evaluate our model on the aforementioned datasets, comparing it to various zero-shot LLMs, MLLMs, and retrieval-augmented competitors. Specifically, we report results of three LLMs - Vicuna [12], LLaMA-3, and LLaMA-3.1 [16] each prompted with both the query question and a description of the query image generated by an image captioning model. Additionally, we assess the performance of BLIP-2 [38], InstructBLIP [14], and LLaVA-v1.5 [44], without external retrieval augmentation and using only the query image and question as input. As direct competitors, we include DPR [35], RORA-VLM [53], Wiki-LLaVA [8], and EchoSight [69], which all leverage external knowledge retrieval. To ensure a fair comparison with the considered methods, for both Wiki-LLaVA and EchoSight we develop"}, {"title": "5. Conclusion", "content": "We proposed ReflectiVA, a multimodal LLM with retrieval-augmented generation. Our method employs reflective tokens, trained in a two-stage two-model pipeline. Extensive experiments, conducted on both VQA datasets requiring external knowledge and standard datasets, demonstrate the efficacy of the proposed solution."}]}