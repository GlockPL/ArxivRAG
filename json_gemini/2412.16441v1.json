{"title": "LEARNING CROSS-TASK GENERALITIES ACROSS GRAPHS VIA TASK-TREES", "authors": ["Zehong Wang", "Zheyuan Zhang", "Tianyi Ma", "Nitesh V Chawla", "Chuxu Zhang*", "Yanfang Ye1*"], "abstract": "Foundation models aim to create general, cross-task, and cross-domain machine learning models by pretraining on large-scale datasets to capture shared patterns or concepts (generalities), such as contours, colors, textures, and edges in images, or tokens, words, and sentences in text. However, discovering generalities across graphs remains challenging, which has hindered the development of graph foundation models. To tackle this challenge, in this paper, we propose a novel approach to learn generalities across graphs via task-trees. Specifically, we first define the basic learning instances in graphs as task-trees and assume that the generalities shared across graphs are, at least partially, preserved in the task-trees of the given graphs. To validate the assumption, we first perform a theoretical analysis of task-trees in terms of stability, transferability, and generalization. We find that if a graph neural network (GNN) model is pretrained on diverse task-trees through a reconstruction task, it can learn sufficient transferable knowledge for downstream tasks using an appropriate set of fine-tuning samples. To empirically validate the assumption, we further instantiate the theorems by developing a cross-task, cross-domain graph foundation model named Graph generality Identifier on task-Trees (GIT). The extensive experiments over 30 graphs from five domains demonstrate the effectiveness of GIT in fine-tuning, in-context learning, and zero-shot learning scenarios. Particularly, the general GIT model pretrained on large-scale datasets can be quickly adapted to specific domains, matching or even surpassing expert models designed for those domains.", "sections": [{"title": "1 INTRODUCTION", "content": "Foundation models have gained prominence in the era of artificial general intelligence as a general-purpose, cross-task, and cross-domain machine learning approach. These models are exemplified by large language models (LLMs) for text (Achiam et al., 2023; Touvron et al., 2023) and large vision models (LVMs) for images (He et al., 2022; Yuan et al., 2021). Pretrained on large-scale datasets, they capture generalizable and transferable knowledge, including contours, colors, textures, and edges in images, as well as tokens, words, and sentences in text. These patterns and concepts represent modality-specific generalities, allowing the models to adapt rapidly to new downstream tasks via in-context learning (Xie et al., 2022) or zero-shot learning (Wei et al., 2021).\nDespite the success of foundation models across various modalities, their development of graph-structured data remains in its infancy (Liu et al., 2024). This is primarily due to the high degree of variability among graph-structured datasets (Mao et al., 2024), making it challenging to identify shared generalities across graphs. In particular, graphs from different domains typically represent distinct phenomena; for example, social networks capture relationships between people (Freeman, 2004), while molecular networks depict the structures of molecules (Zeng et al., 2022). These distinctions are evident not only in the differences between feature and label spaces (Huang et al., 2023; Liu et al., 2024) but also in structural heterogeneity (Qiu et al., 2020; Wang et al., 2024d). Additionally, different graph-related tasks often do not share basic learning instances such as nodes\nin node-level tasks and entire graphs in graph-level tasks\u2014making it difficult to use a single model for diverse tasks (Mao et al., 2024). The above challenges make it extremely difficult to develop a graph foundation model (GFM) that can identify generalities applicable across graphs.\nIs it possible to identify generalities across graphs? Despite the challenges, some researchers have attempted to uncover shared generalities across graphs, which can be mainly categorized into two groups. (1) One line of works draws on graph theory. In particular, they use the concept of graphon (Ruiz et al., 2020) to describe transferable patterns across graphs. If two graphs are generated from the same graphon, they are expected to share key topological properties, which can result in high transferability between them (Ruiz et al., 2020; Cao et al., 2023). However, the strong assumptions underlying graphon theory are often unrealistic for real-world graphs (Levie et al., 2021), and even when the assumptions hold, finding the graphon shared from a large set of graphs presents another significant challenge. (2) Another approach involves leveraging substructures that are present across graphs, such as triangles, stars, and k-cliques (Wang et al., 2024c; Zhao et al., 2023; Mao et al., 2024). For instance, triangle structures frequently appear in citation networks, social networks, and molecular networks, albeit with varying semantics. Based on this observation, some works (Sun et al., 2023; Liu et al., 2024) sample subgraphs consisting of these substructures (as shown in Figure 1, Left) and then use a graph neural network (GNN) to encode the subgraph-level embeddings for prediction. However, identifying which substructures are shared across distinct graphs remains difficult. Even if useful substructures can be identified, message-passing GNNs are proved to struggle with capturing such basic substructures in learning (sub)graph embeddings (Garg et al., 2020; Esser et al., 2021; Zhang et al., 2024a), limiting the practicality of these methods.\nGiven the limitations of exist-ing approaches, we propose a novel perspective to answer the question by focusing on the learning process of GNNs. In message-passing GNNs (Kipf &\nWelling, 2017; Hamilton et al.,\n2017), predictions are made based on the so-called task-relevant nodes in the graph. For node-level tasks, the task-relevant node is the node itself; for edge-level tasks, it includes the start and end nodes of the edge; and for graph-level tasks,\nall nodes in the graph are task-relevant. Regardless of the task, basic GNN models only aggregate the embeddings of task-relevant nodes for predictions (Srinivasan & Ribeiro, 2020). This aggregation process is analogous to introducing a virtual \"task node\" that connects all task-relevant nodes, and learn the embedding of the task nodes for prediction. We refer the computation tree surrounding the \"task node\" as task-tree, as illustrated in Figure 1 (Right). The task-tree formulation offers three distinct advantages. (1) Learnability: the information within task-trees can be fully captured by message-passing GNNs (Gupta et al., 2024), allowing the model to encode diverse structural information across domains. (2) Uniformity: task-trees are applicable to node-, edge-, and graph-level tasks, providing a unified task alignment paradigm across graphs (Sun et al., 2023). (3) Efficiency: encoding task-trees involves learning the embeddings of virtual nodes appended to the original graph, without extra time-consuming operations. Building on these benefits, it is feasible for task-trees to serve as the basic learning instances in graphs, similar to images in vision and sentences in language. Therefore, we propose that task-trees may maintain transferable patterns or concepts shared across graphs from different domains and tasks, leading to the following assumption:\nTask-Tree Generality Assumption: the generalities shared across graphs are (at least partially)\npreserved within the task-trees of the involved graphs.\nTo validate this assumption, we conduct a theoretical analysis to examine the properties of task-trees in terms of stability, transferability, and generalization (Verma & Zhang, 2019; Garg et al., 2020), showing the feasibility of building GFMs based on task-trees. Our key finding is that if a GNN model"}, {"title": "2 PRELIMINARY", "content": "We begin with a brief introduction to message-passing GNNs and some related concepts, where a comprehensive discussion on related works is presented in Appendix A. Let G = (V, E) represent a graph with node set V and edge set E, where each node $v \\in V$ is associated with a feature vector $x_v \\in R^d$. A GNN encoder $\\phi$ takes the graph as input and performs message passing to learn node embeddings Z = $\\phi$(V, E). Specifically, a GNN encoder can be defined as:\n$z_i^{(l)} = \\sigma(W_1 x_i + W_2 \\rho(\\sum_{j\\in N(i)} g(z_j^{(l-1)})))$, (1)\nwhere $N_i$ denotes the 1-hop neighbors of node i, $z_i^{(l)}$ represents the node embedding at the l-th GNN layer with $z_i^{(0)} = x_i$, and $W_1, W_2$ are learnable matrices. The functions $\\sigma, \\rho$, and g are the activation function, aggregation function and update function, respectively. To simplify the analysis, we assume $\\rho$ is an averaging operation and g is the identity function. Without loss of generality (WLOG), these functions can be replaced with any permutation-invariant and Lipschitz-continuous functions, respectively, without affecting the subsequent analysis. We now present some preliminary definitions before outlining the theoretical results.\nDefinition 2.1 (Task-Relevant Nodes). Graph tasks can be roughly categorized into node-level, edge-level, and graph-level tasks, where the basic learning instances are nodes, edges, and entire graphs, respectively. For node classification, the task-relevant node $v_i$ is the target node $v_i$ that needs to be classified. In edge classification, the task-relevant nodes are the start and end nodes $\\{v_s, v_e\\}$ of the target edge $e_{ij}$. For graph classification, the task-relevant nodes $\\{v_i\\}_{i=1}^{|V|}$ include all nodes in the target graph G.\nRemark 2.2. For any graph task instance, the prediction relies solely on the embeddings of the cor-responding task-relevant nodes. These node embeddings capture the surrounding subtree structures, also known as computation trees.\nDefinition 2.3 (Computation Trees (Chuang & Jegelka, 2022)). Given a node v in graph G, the L-layer computation tree $T_v^{\\downarrow L}$ is constructed by recursively expanding the subtrees of its neighboring nodes, starting with $T_v^{\\downarrow 0} = v$.\nThe learning process of GNNs involves recursively integrating information from the computation trees, progressing from the bottom to the top. Therefore, for a given node v, the node embedding produced by an L-layer GNN corresponds to the embedding of its computation tree $T_v^{\\downarrow L}$. Since the prediction for any graph task depends solely on the embeddings of task-relevant nodes, and these embeddings are determined by their respective computation trees, we can construct a task-tree for each instance\u2014whether it be a node, edge, or graph\u2014as illustrated in Figure 1 (Right).\nDefinition 2.4 (Task-Trees). For any graph-related instance\u2014whether a node, edge, or graph\u2014we have a set of task-relevant nodes $\\{v_1, ..., v_n\\}$ and their corresponding L-layer computation trees $\\{T_1, ..., T_n\\}$. These computation trees can be reformulated into a larger task-tree $T_t$ by introducing an additional task node that connects all task-relevant nodes.\nTask-Tree Encoding. We use a straightforward method to encode task-trees. Given a task-tree $T_t$, consisting of a virtual task node $v_t$ and a set of task-relevant nodes associated with compu-tation trees $T_1, ..., T_n$, we first apply a GNN $\\phi$ to encode each computation tree, obtaining node embeddings $z_1, ..., z_n$. We then aggregate these node embeddings into the task node, with the task node embedding serving as the embedding of the task-tree. Specifically, we use basic averaging for aggregation: $z_t = \\phi(T_t) = \\frac{1}{n} \\Sigma_{i=1}^{n} \\phi(T_i)$."}, {"title": "3 TASK-TREES SERVE AS BASIC LEARNING INSTANCES ON GRAPHS", "content": "In this section, we theoretically analyze the properties of task-trees to demonstrate their stability, transferability, and generalization in acting as the basic learning instances, thereby validating the Task-Tree Generality Assumption from a theoretical perspective. It is important to note that the theoretical analysis is not to prove the superiority of task-trees over other methods, such as graphon or subgraph. Rather, we aim to show the feasibility of building GFMs based on task-trees.\nWe begin by examining the stability of GNNs in learn-ing task-tree representations, showing that task-trees with similar subtree structures produce analogous embeddings. To facilitate this analysis, we first define the notation for describing subtree information:\n$x_i^{(l)} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} x_j^{(l-1)}$, (2)\nwhere $x_i^{(0)} = x_i$ denotes the original node feature, and $x_i^{(l)}$ denotes the subtree information of nodes in l-th layer, as illustrated in Figure 2. In this figure, for l = 1, only the nodes in the first layer of the tree are considered, and for l = 2, only the nodes in the second layer are considered.\nTheorem 3.1 (Stability on Task-Trees). Given two L-layer task-trees $T_1$ and $T_2$, with task-relevant nodes $\\{v_1,..., v_n\\}$ and $\\{v_1, ..., v_m\\}$, respectively. The distance between task-trees is defined as $\\Delta := ||\\phi(T_1) - \\phi(T_2)||$ with the following bound:\n$\\Delta = ||\\phi(T_1) - \\phi(T_2)|| = ||\\frac{1}{n} \\sum_{i=1}^{n} \\phi(T_i) - \\frac{1}{m} \\sum_{j=1}^{m} \\phi(T_j) || = ||\\frac{1}{nm} \\sum_{i=1}^{n} \\sum_{j=1}^{m} (C_1 ||x_i^{(0)} - x_j^{(0)}|| + C_1C_2 ||x_i^{(1)} - x_j^{(1)}|| + ... + C_1C_2^{L-1} ||x_i^{(L-1)} - x_j^{(L-1)} ||)|| \\leq 2 B_x \\cdot C_1 ... C_{L-1}$, (3)\nwhere $\\phi$ is the GNN encoder, $T_i$ is the computation tree corresponding to node i, and $C_1, C_2$ are constants related to the encoder, and $B_x$ represents the bounded norm of x.\nThe proof can be found in Appendix D.1. Theorem 3.1 suggests that two task-trees are likely to have similar representations if their subtrees are similar. This theorem highlights the significance of similarity between pairs of subtrees, while downplaying the impact of the number of subtrees (i.e., the width of the task-trees), despite having more subtrees could potentially increase diversity and thus magnify discrepancy. The theorem also implies that increasing the number of GNN layers may lead to a loose bound, which aligns with previous analyses (Garg et al., 2020; Ju et al., 2023).\nIllustration 3.2. This theorem provides theoretical support for using task-trees as basic learning instances in graph tasks. Consider two task-trees: one representing a node (with a single subtree) and the other representing a graph (with multiple subtrees). While the widths of these task-trees differ significantly, if their subtrees share some degree of similarity, they can produce similar repre-sentations. Thus, this theorem ensures that task-trees of nodes, edges, or graphs can potentially be similar, making it possible to use a GNN encoder to capture the shared patterns among them.\nFollowing the stability analysis, we now examine the transferability of task-trees in pretraining and fine-tuning scenario. Specifically, assuming a model is pretrained on a task-tree reconstruction task, we aim to quantify how the knowledge acquired during pretraining can be transferred to downstream tasks. The pretraining objective is defined as $L_P(g \\circ \\phi) := E_{(\\hat{T}, T) \\sim P} ||g(\\phi(T)) - \\phi(T)||^2$, where P represents the task-tree distribution used for pretraining, $\\phi \\in \\Phi$ and $g \\in G$ are the GNN encoder"}, {"title": "4 GIT: GRAPH GENERALITY IDENTIFIER ON TASK-TREES", "content": "The theoretical analysis demonstrates the feasibility of building graph foundation models based on task-trees. In this section, we apply these theorems to develop a cross-task, cross-domain GFM called GIT, with the aim of empirically validating the Task-Tree Generality Assumption.\n4.1 GENERAL MODEL: PRETRAINING TO ACQUIRE GENERAL KNOWLEDGE\nWe propose a task-tree reconstruction task as a pretext for pretraining. The key idea is to use two corrupted task-trees to reconstruct each other, thereby capturing corruption-invariant semantics of the task-trees. Given a set of task-trees $\\{T_1, ..., T_n\\}$ sampled from a graph database\u00b9, we apply cor-ruption techniques to generate two views of each task-tree, denoted as $\\{\\hat{T_1}, ..., \\hat{T_n}\\}$ and $\\{\\tilde{T_1}, ..., \\tilde{T_n}\\}$. For corruption, we use random edge masking and random attribute masking, as proposed by Zhu et al. (2020), due to its computational efficiency. We then use an encoder $\\phi$ to obtain embeddings for the corrupted task-trees, resulting in $\\{z_1, ..., z_n\\}$ and $\\{\\tilde{z_1},..., \\tilde{z_n}\\}$. Note that the task-tree embed-ding is defined as the average of the embeddings of task-relevant nodes, avoiding the introduction of inductive biases that could lead the model to learn incorrect patterns. The loss function is\n$L = \\frac{1}{2n} \\sum_{i=1}^{n} [||\\rho_g(\\tilde{z_i}) - sg[\\rho(\\hat{z_i})]||^2 + ||\\rho_g(\\hat{z_i}) - sg[\\rho(\\tilde{z_i})]||^2] + \\sum_{i=1}^{n} D_{KL}(h||z_i)$, (6)\nwhere g is a non-linear MLP projector, $\\rho(z) = (z/||z||)$ serves for normalization, sg is the stop-gradient operation, and h is the average of all instances z. The reconstruction loss captures the semantics of the task-trees in a predictive manner, while the regularizer ensures the embeddings are projected into a shared space by minimizing the KL divergence between individual instances and their center. Additional analysis is provided in Appendix C.\n4.2 SPECIALIZED MODEL: SPECIFICATION VIA INSTRUCTION TUNING\nTheorem 3.5 highlights the relationship between model generalization and the distribution gap be-tween pretraining data P and fine-tuning data T, showing that a smaller gap leads to better gener-alization. Based on this finding, it is feasible to develop a specialized model for specific domains from a pretrained general model. This is based on the mild assumption that graphs from the same domain have similar task-tree distributions $\\{T_1, .., T_n\\}$. If the pretrained model is post-trained on a task-tree distribution $P_{post}$ sampled from $\\{T_1, \u2026, T_m \\}$, the pretraining data distribution P can be ad-justed towards these task-tree distributions. This reduces the discrepancy $\\Sigma_{x\\in X_{\\phi}} ||T_{\\phi}(x) - P_{\\phi}(x)||$ in Theorem 3.5, thereby improving model generalization on the target domain. To achieve this, we propose an instruction-tuning method for post-training the pretrained model.\nInstruction tuning is a supervised fine-tuning (SFT) technique designed to enhance the capabilities of a pretrained model by post-training it on a small dataset. Our goal is to fine-tune the model using instructions to specialize it for a particular domain of interest. Given a pretrained model $\\phi^*$ and a set of task-trees $\\{T_1, ..., T_n\\}$ from the target domain, we post-train the model using the SFT loss:\n$L_{SFT} = \\frac{1}{n} \\sum_{i=1}^{n} \\kappa(\\phi^*(T_i), \\psi(T_i))$, (7)\nwhere $\\psi$ is the instruction generation function for each task-tree, and $\\kappa$ is the corresponding loss function. In this paper, as we use text-attributed graphs in our experiments, we define instructions as the embeddings of label descriptions encoded by a LLM, which is similar to Liu et al. (2024), and we use mean squared error as the loss function $\\kappa$."}, {"title": "5 EXPERIMENT", "content": "5.1 EXPERIMENTAL SETUP\nThe detailed experimental settings are provided in Appendix E, and the comprehensive experimental results and analysis are presented in Appendix F. In the following, we briefly introduce the datasets, baselines, and basic evaluation protocols.\nDatasets. Our experiments are based on text-attributed graphs due to data availability. Specifically, we include over 30 graphs spanning five domains: academic networks, e-commerce networks, knowledge graphs, molecular graphs, and temporal graphs. Detailed information is provided in Appendix E.1. For model pretraining, we use the citation network Arxiv, the e-commerce network Products, knowledge graphs WN18RR and FB15K237, and molecular graphs Chemblpre and PCBA. For supervised fine-tuning (SFT) during specialization, we use Arxiv for academic net-works, Products for e-commerce networks, FB15K237 for knowledge graphs, and PCBA for molecular networks. For temporal graphs, which are e-commerce temporal graphs, we also use Products for SFT to assess robustness under temporal distribution shifts.\nBaselines. We employ a wide range of baselines, including supervised GNNs (Kipf & Welling, 2017; Veli\u010dkovi\u0107 et al., 2018; Xu et al., 2019), self-supervised GNNs such as BGRL (Thakoor et al.,"}, {"title": "6 CONCLUSION", "content": "Conclusion. We introduce the concept of task-trees as basic learning instances for graphs and pro-vide both theoretical and empirical validation of the task-tree generality assumption: the generalities shared across graphs are (at least partially) preserved in the task-trees of the involved graphs. Build-ing on task-trees, we develop GIT, a cross-domain and cross-task GFM that can be quickly special-ized for specific domains. By pretraining on a small set of graphs, GIT improves performance across"}, {"title": "A RELATED WORK", "content": "Graph Neural Networks. GNNs are a class of learning models specifically designed to operate on graph-structured data and have demonstrated substantial success across a variety of domains. Their strength lies in their ability to perform relational learning, where information from neigh-boring nodes is aggregated and used to enhance node representations. For instance, GCN (Kipf &\nWelling, 2017) utilizes message-passing to aggregate information from neighboring nodes to central nodes. Building on this, models such as GraphSAGE (Hamilton et al., 2017) and GAT (Veli\u010dkovi\u0107 et al., 2018) introduce innovative techniques like neighborhood sampling and attention mechanisms, respectively, further advancing performance on graph learning tasks. However, these methods are limited to solving a single task by training from the scratch.\nTransferability of GNNs. Existing works that analyze the shared concepts (generalities) across dif-ferent graphs primarily follow two approaches. The first is graphon theory, which provides bounds on the distance between graphs generated from the same graphon. This method has been used to study transferability in pretraining and fine-tuning settings (Cao et al., 2023), to develop more expressive fine-tuning techniques (Sun et al., 2024), and to design new model architectures (Ruiz et al., 2020). However, despite its theoretical advantages, graphon-based approaches face practical challenges, particularly the strong assumptions required and the difficulty of identifying graphons in large-scale graphs, which limits their applicability in building graph foundation models. The sec-ond approach involves leveraging substructures within graphs to identify transferable patterns (Mao et al., 2024). This method focuses on extracting subgraphs composed of meaningful substructures for prediction tasks. While this approach offers theoretical insights into stability (Levie et al., 2019; Zhu et al., 2021), it struggles to fully capture substructures that are beneficial for downstream tasks (Zhang et al., 2024a).\nGraph Foundation Models. Foundation models are designed as general-purpose solvers capable of handling various tasks across different domains. For instance, LLMs, the foundation models in natural language processing, are capable of performing tasks such as summarization, translation, and entity recognition, as well as question-answering. However, building such versatile foundation models for graphs presents unique challenges due to the inherent feature, structural, and task het-erogeneity across different graph domains and tasks. To address these challenges, Qiu et al. (2020) pretrained GNNs using subgraphs as basic units, mitigating structural heterogeneity. Building on this, Sun et al. (2023) reformulated node-, edge-, and graph-level tasks into subgraph-level tasks, tackling task heterogeneity. Additionally, Huang et al. (2023) and Liu et al. (2024) applied LLMs to unify the feature spaces of cross-domain graphs, addressing feature heterogeneity. These approaches enable models to operate on cross-domain and cross-task graphs. Further advancements, such as He &\nHooi (2024) and Li et al. (2024), improve node embeddings by jointly optimizing GNN and LLM encoders, facilitating various downstream tasks like few-shot learning and zero-shot learning. Other efforts to resolve feature heterogeneity include methods like singular vector decomposition (SVD) (Zhao et al., 2024a; Yu et al., 2024) and non-parametric encoders (Zhao et al., 2024b). However, most of these approaches rely on subgraphs as the primary learning instances, which can result in inefficient training and reduced expressiveness, as discussed in the main paper.\nAnother line of research focuses on designing GFMs for single tasks or domains, thereby avoiding the complexities of feature, structural, or task heterogeneity. For example, Galkin et al. (2024) pro-pose a foundation model for reasoning tasks on knowledge graphs, using triplets as basic transferable patterns. Zhao et al. (2023) introduce a foundation model for molecular graphs, employing LLMs to align semantics between datasets and encode key motifs. In node classification, Li et al. (2024) propose a zero-shot learning foundation model, while Zhao et al. (2024a) present a feature align-ment method based on SVD for node-level graph foundation models. Recently, Zhao et al. (2024b) designed a foundation model for node classification using a non-parametric classifier. Meanwhile, Chen et al. (2024a), Tang et al. (2024), Guo et al. (2023), and Wang et al. (2024a) have explored using LLMs as graph reasoners to solve graph tasks, similar to their role in vision language mod-els. While these methods excel at specific tasks or domains, they are not suitable as general graph solvers across diverse tasks. In contrast to these approaches, our proposed GIT model is pretrained on diverse task trees to acquire general reasoning capabilities, allowing it to quickly specialize in specific domains through instruction tuning."}, {"title": "B POTENTIAL MODEL EXTENSIONS", "content": "B.1 PRETRAINING\nHow to Design Reconstruction Tasks? Theorem 3.5 suggests that a well-designed encoder, capable of effectively handling reconstruction tasks during pretraining, can improve the model's generaliza-tion ability. One approach is to use more powerful encoders to enhance reconstruction performance. Another approach is to introduce additional reconstruction losses to further refine the encoder. For example, methods such as those proposed by Qiu et al. (2020) and Hou et al. (2022), or designing more comprehensive reconstruction objectives could be explored.\nHow to Improve Transferability? The pretraining task, i.e., task-tree reconstruction, differs from the downstream task of task-tree classification, as the task heterogeneity may hinder model trans-ferability (Hu et al., 2020). To mitigate this, one could develop more effective adaptation methods, such as graph prompt learning (Sun et al., 2022), to reduce task heterogeneity.\nB.2 SPECIALIZATION VIA INSTRUCTION TUNING\nHow to Define Instructions? In this paper, as we focus on experiments with text-attributed graphs Wang et al. (2024b), we define instructions as label descriptions encoded by LLMs. However, this approach is not applicable to non-textual graphs. Other methods could be explored to define instructions, such as using proxy models (Hu et al., 2019) or graph heuristics (Jin et al., 2020) to generate instructions.\nHow to Choose SFT Data? We manually select graphs as supervised fine-tuning datasets for each domain, though the selected graphs may not be fully representative. Unlike textual data, evalu-ating the quality of graph datasets poses a challenge. Improved dataset selection methods could enhance the SFT process by identifying more representative or diverse data from graph databases. Additionally, while we perform instruction tuning over entire graphs, it is possible that only spe-cific subgraphs are beneficial (Hashemi et al., 2024). Developing data selection methods that focus on high-quality subgraphs within a single SFT dataset could improve task-tree selection. Another worthy research line is to select SFT data that aligns with user preferences (Song et al., 2024).\nHow to Leverage SFT Data? In scenarios with limited instructions, standard supervised fine-tuning may struggle to capture sufficient knowledge of the target domain. To address this, methods could be proposed to better utilize the unlabeled instances in the SFT dataset, thus enhancing model adaptation (Sohn et al., 2020; Wang et al., 2025).\nHow to Maintain General Inference Capability? While instruction tuning specializes the model for a specific domain, it may compromise the model's general inference capabilities across other domains. This could hinder the model's performance when it needs to function both as a domain expert and a general reasoner. To mitigate this, regularization techniques could be designed to preserve the general knowledge encoded in the model during the instruction tuning process.\nWhy SFT Works on Graphs? Instruction tuning is a common post-training process in modern large language models (e.g., LLAMA, GPT) that significantly improves instruction-following capabilities. The success of this method in LLMs may stem from the fact that natural language serves as an interface between humans and models (Wei et al., 2021). However, the reason instruction tuning works for graphs remains an open question and presents a potential direction for future research.\nB.3 MORE SCENARIOS.\nThe paper leverages text-attributed graphs to align node features. However, the pre-processing of TAGs can be time-consuming, raising the challenge of how to effectively apply the model to graphs without aligned node features. Furthermore, while we primarily focus on homogeneous graphs in this work, most real-world applications involve heterogeneous graphs. Addressing the question of how to design a single model capable of handling various types of graphs remains an open challenge. Finally, applying the model to specific applications Zhang et al. (2024c; 2025), which may exhibit unique characteristics, is another important consideration for future research."}, {"title": "C ADDITIONAL DISCUSSION", "content": "C.1 WHY DOES THE GENERAL MODEL NEED SPECIALIZATION?\nIt is challenging for a single graph model to handle tasks across various domains due to pattern conflicts, where the same structural pattern can have different meanings in different domains. To illustrate this issue, we provide an intuitive example\u00b3. Consider a pretraining process involving datasets from multiple domains, such as social networks, molecular networks, academic networks, and knowledge graphs. Suppose the model learns triangle structures during pretraining. In social networks, the semantic meaning of these triangles is stable, following the principle of \"the friend of my friend is my friend\". However, in molecular graphs, the meaning of triangle patterns may be unstable due to chemical properties. This pattern conflict can significantly degrade the performance of graph models (Cao et al., 2023; Mao et al., 2024). Specialization helps resolve this issue by aligning the meanings of certain structural patterns with the semantics specific to the target domain.\nC.2 MORE ANALYSIS ON DOMAIN REGULARIZER\nThe Necessity of Domain Alignment. Datasets from multiple domains are often projected into different subspaces, potentially due to misalignment of node attributes (Chen et al., 2024b) and the frequent patterns across domains. As a result, the model may \u201cmemorize\u201d information specific to each domain rather than learning transferable patterns. This can lead to misunderstandings when the same pattern appeared across different graphs is projected into different subspaces. Consequently, the model struggles to acquire transferable knowledge that would benefit unseen tasks and special-ized domains. Properly aligning the embedding spaces of different domains is crucial for obtaining transferable knowledge and improving performance on unseen graphs and specialized domains.\nHow to Regulate Domain Distances? We propose a domain regularizer to control domain distances by projecting cross-domain graphs with different characteristics into a shared embedding space."}, {"title": "D PROOF", "content": "D.1 PROOF OF THEOREM 3.1\nProof. We begin by introducing the basic GNN architecture used in the proof. Given a GNN encoder $\\phi(\u00b7)$ with parameters W = $(W_1", "abuse)": "n$z_i = \\phi(T_i) = \\sigma(W_1 x_i + W_2 \\frac{1"}, {"as": "n$\\Delta = ||\\phi(T_1) - \\phi(T_2)|| = ||\\frac{1}{n} \\sum_{i=1}^{n} \\phi(T_i) - \\frac{1}{m} \\sum_{j=1}^{m} \\phi(T_"}]}