{"title": "EVOLVING ALIGNMENT via ASYMMETRIC SELF-PLAY", "authors": ["Ziyu Ye", "Rishabh Agarwal", "Tianqi Liu", "Rishabh Joshi", "Sarmishta Velury", "Quoc V. Le", "Qijun Tan", "Yuan Liu"], "abstract": "Current RLHF frameworks for aligning large language models (LLMs) typically assume a fixed prompt distribution, which is sub-optimal and limits the scalability of alignment and generalizability of models. To address this, we introduce a general open-ended RLHF framework that casts alignment as an asymmetric game between two players: (i) a creator that generates increasingly informative prompt distributions using the reward model, and (ii) a solver that learns to produce more preferred responses on prompts produced by the creator. This framework of Evolving Alignment via Asymmetric Self-Play (eva), results in a simple and efficient approach that can utilize any existing RLHF algorithm for scalable alignment. eva outperforms state-of-the-art methods on widely-used benchmarks, without the need of any additional human crafted prompts. Specifically, eva improves the win rate of GEMMA2-9B-IT on Arena-Hard from 51.6% to 60.1% with DPO, from 55.7% to 58.9% with SPPO, from 52.3% to 60.7% with SimPO, and from 54.8% to 60.3% with ORPO, surpassing its 27B version and matching claude-3-opus. This improvement is persistent even when new human crafted prompts are introduced. Finally, we show eva is effective and robust under various ablation settings.", "sections": [{"title": "1 INTRODUCTION", "content": "Long-lived artificial intelligence must deal with an ever-evolving, open-ended world, yet currently face constraints in both the scale and quality of available data, and the growth rate at which new, useful information is created. High quality human data, crucial for scaling large language model (LLM) based intelligence, is projected to run out in the next few years (Villalobos et al., 2024); the quality of such data is also expected to stagnate: as LLMs become more capable, they need to solve increasingly complex or new challenges, requiring training data beyond abilities of humans to create. This necessitates a new fundamental mechanism for self-improving, where models can continuously self-generate and self-solve harder problems. We thereby investigate the research question below:\nCan language models self-create new, learnable tasks to work on,\nto self-improve to generalize better for human preferences alignment?"}, {"title": "2 PRELIMINARIES", "content": "We hereby review major concepts, which we later in \u00a7 3 use the regret and advantage function to identify informative prompts, leading to learning curricular implicitly maximizing contrastive ratio ."}, {"title": "Alignment by RLHF.", "content": "Classical RLHF (Ouyang et al., 2022) optimizes on a fixed distribution D:\n$\\max_{\\pi_{\\theta}} \\mathbb{E}_{x \\sim D, y \\sim \\pi_{\\theta}(y | x)} [r(x,y) ] - \\beta \\cdot D_{KL} [\\pi_{\\theta} (y | x) || \\pi_{\\text{ref}} (y | x) ]$,\nwhere x and y denote the prompts and responses, and $r(\\cdot, \\cdot)$ is the reward function."}, {"title": "Reward.", "content": "Let the optimal policy of Eq. 1 be $\\pi^*(\\cdot)$ and $Z(.)$ be the partition function, we have:\n$r(x, y) = \\beta \\cdot \\log \\frac{\\pi^*(y|x)}{\\pi_{\\text{ref}}(y|x)} + \\beta \\log Z(x)$."}, {"title": "Regret.", "content": "Let $r^* (x) = \\max_{y'} r(x, y')$ be the optimal reward achievable at x, the regret to take y is:\n$\\text{Regret}(x, y) = r^*(x) \u2013 r(x, y)$."}, {"title": "Advantage.", "content": "The advantage function quantifies how much better a response y is w.r.t. a baseline:\n$A(x, y) = r(x, y) \u2013 \\mathbb{E}_{y' \\sim \\pi(y'|x)} [r'(x, y')]$.\nVariants of advantage (e.g., the worst-case advantage $A_{\\min}$) are related to regret, as shown in Table 2."}, {"title": "Direct preference optimization.", "content": "The DPO (Rafailov et al., 2023) objective for RLHF is:\n$\\mathcal{L}^{\\text{DPO}}(\\pi_{\\theta}) = - \\mathbb{E}_{(y_+,y_-,x) \\in D} \\log \\sigma(\\beta \\cdot \\Delta_{\\theta; \\text{ref}})$, \nwhere we use +, \u2212 to denote chosen and rejected responses, and denote the contrastive ratio as:\n$\\Delta_{\\theta; \\text{ref}} := \\log \\frac{\\pi_{\\theta} (y^+ | x)}{\\pi_{\\text{ref}} (y^+ | x)} - \\log \\frac{\\pi_{\\theta} (y^- | x)}{\\pi_{\\text{ref}} (y^- | x)}$.\nBy reward reparameterization with Eq. 2, advantage also relates to contrastive ratio, as in \u00a7 3.4."}, {"title": "3 METHOD", "content": "Algorithm overview. At a high level, eva extends classical RLHF to open-ended RLHF via a creator that adapts prompt distributions with an easy-to-implement estimate, sample then evolve procedure, mimicking the minimax-regret policy of asymmetric self-play games, as detailed in \u00a7 3.3."}, {"title": "Algorithm 1", "content": "eva: Evolving Alignment via Asymmetric Self-Play\nInput: initial policy $\\pi_{\\theta_0}$, initial prompt set $X_0$\n1: for iteration $t = 1, 2, ...$ do\n2: /* creator step*/\n estimate informativeness:\n $X_t \\leftarrow X_t \\cup \\{ \\text{info}(x) \\}$\n sample subset:\n $X_{\\text{info}} \\leftarrow \\text{sample}(X_t)$\n self-evolve prompts:\n $X'_t \\leftarrow \\text{evolve}(X_{\\text{info}})$\n3: /* solver step */\n self-generate responses:\n $\\forall x_i \\in X'_t, \\text{ generate } \\{ y_i^{(i)}\\} \\sim \\pi_{\\theta_{t-1}}(\\cdot | x_i)$\n4: annotate rewards:\n $X'_t \\leftarrow X'_t \\cup \\{ (y^{(i)}, r^{(i)}) \\}$\n5: preference optimization:\n $\\theta_t \\leftarrow \\theta_{t-1} - \\eta \\nabla_{\\theta} \\mathcal{L}_{X'_t}(\\theta)$\n6: end for\n7: return final solver policy $\\pi_{\\theta_t}$"}, {"title": "3.1 THE PRINCIPLE: OPEN-EDNED RLHF FOR JOINT SELF-IMPROVEMENT", "content": "Intuition. Classical RLHF (cf., Eq. 1) optimizes over a static prompt distribution, meaning that the agent is only aligned to a fixed reference point, making it brittle when it is evaluated on new problems from the ever-changing real world. Our Open-Ended RLHF breaks away from this static framework, with the goal to develop an agent that generalizes well across unseen, novel environments (where the tasks entailed in prompts may not have been explicitly encountered during training). To achieve this, we must design a new objective that moves beyond optimizing over a fixed dataset D."}, {"title": "Formalization.", "content": "We thus formally introduce optimizable prompt generation policy $\\pi_{\\phi}(x)$, which is jointly optimized with the response policy $\\pi_{\\theta}(y | x)$, as follows:\nDefinition 1 (Open-ended RLHF) Let $\\pi_{\\phi,\\theta}(x,y) := \\pi_{\\phi}(x) \\cdot \\pi_{\\theta}(y |x)$ and $\\pi_{\\text{ref}}(x, y) :=  \\pi_{\\text{ref}}(x) \\pi_{\\text{ref}}(y | x)$. We define evolving alignment\" as the open-ended joint optimization on the prompt and response policy for alignment w.r.t the joint reference policy:\n$\\max_{\\phi,\\theta} \\mathbb{E}_{x \\sim \\pi_{\\phi}(x), y \\sim \\pi_{\\theta} (y|x)} [r(x,y) ] - \\beta \\cdot D_{KL} [\\pi_{\\phi,\\theta}(x, y) ||  \\pi_{\\text{ref}}(x, y)]$,"}, {"title": "3.2 THE MECHANISM: ASYMMETRIC SELF-PLAY via THE CREATOR V.S. SOLVER GAME", "content": "Intuition. It is hard to directly optimize Eq. 7, due to (i) the intractability of the unspecified reference (Dennis et al., 2020); (ii) the instability of joint differentiation (Goodfellow et al., 2014). We present an alternating optimization solution by casting it as an asymmetric creator-solver game.\nIntuitively, the creator can guide the solver by prompt curricula with increasing complexity, encouraging efficient and general learning to handle the diversity in the wild.\nMathematically, this resembles RL optimization via expectation-maximization (Dayan and Hinton, 1997; Singh et al., 2023), where $\\phi$ for the prompt distribution is fixed at each step."}, {"title": "Formalization.", "content": "We formalize the alternating optimization as an asymmetric game as follows:\nCreator: the prompt player $\\pi_{\\chi}$ that strategically generate prompts for the solver.\nSolver: the response player $\\pi_{\\Upsilon|\\chi}$ (or $\\pi$) that learn to generate preferred responeses.\nWe take the minimax regret strategy (Savage, 1951), where the solver minimizes and the creator maximizes regret (Hejna et al., 2023), i.e., the gap in the reward of the current and optimal policy:\n$\\text{Regret}(x, \\Upsilon) = \\mathbb{E}_{y' \\sim \\pi(y'|x)} [r'(x, y)] - \\mathbb{E}_{y \\sim \\pi^{*}(y'|x)} [r(x,y')]$.\nAt the Nash equilibrium (Nash et al., 1950), prior works (Dennis et al., 2020) have shown:"}, {"title": "Remark 1 (Minimax Regret)", "content": "If the solver-creator game reaches an equilibrium, the solver follows a minimax regret strategy, i.e., it optimizes to perform well under all cases:\n$\\pi^* \\in \\underset{\\pi \\in \\Pi_{\\Upsilon|X}}{\\text{arg min}} \\underset{\\pi_{\\chi} \\in \\Pi_{X}}{\\text{max}} \\mathbb{E}_{x \\sim \\pi_{\\chi}} [\\text{Regret}(x, \\pi)]$.\nHowever, without access to the true optimal policy, we must approximate the regret. Leveraging the stochastic policy and the reward signals, we design the advantage-based proxy:"}, {"title": "Definition 2 (Informativeness Proxy)", "content": "We measure the informativeness of a prompt by the (absolute) empirical worst-case optimal advantage, approximating the minimax regret:\n$\\text{info}(x) + A_{\\min} := | \\underset{y}{\\text{min}}r(x, y) -  \\underset{y'}{\\text{max}}r(x, y')|$.\nwhich is estimated by sampling multiple responses for x from the solver and calculating gap between the maximal reward (from the best y) and the minimal reward (from the worst y)."}, {"title": "3.3 THE PRACTICAL ALGORITHM", "content": "We now illustrate practical implementations of eva in Algo 1 (cf., Fig 3)."}, {"title": "3.3.1 THE CREATOR STEP: ESTIMATE, SAMPLE THEN EVOLVE", "content": "Plainly, the creator finds most useful prompts and generate variants of them for preference optimization. One may relate this to evolution strategies (Schwefel, 1977) which find the most promising species, then mutate and crossover, or to curriculum RL (Parker-Holder et al., 2022) which finds environments with high-regret levels, then edits within some distance. As in Section 3.2, we do not seek a differentiable creator in this work. The creator is implemented in three steps as in Figure 3."}, {"title": "Step 1", "content": "info() \u2013 estimate the informativeness. For each x in the prompt set $X_t$, we generate responses, annotate rewards and estimate a informativeness metric to x by Eq. 10 (see also Table 2)."}, {"title": "Step 2", "content": "sample(\u00b7) \u2013 weighted sampling for an informative subset. Using the informativeness metric as the weight, we sample a informative prompt subset $X_{info}$ to be evolved later."}, {"title": "Step 3", "content": "evolve(\u00b7) \u2013 evolving for a proximal region of high-advantage prompts. Our algorithm is agnostic to and does not rely on any specific evolving method. We take EvolInstruct (Xu et al., 2023a) as an off-the-shelf method, which conducts in-depth (i.e., adding constraints, deepening, concretising, complicating) and in-breadth evolving (i.e., mutation) for prompts. Specifically, we iterate over each prompt in the $X_{info}$, where each one is evolved to multiple variations, then optionally mix the newly generated prompts with a uniformly sampled buffer from $X_t$ to create $X'_t$."}, {"title": "3.3.2 THE SOLVER STEP: SOLVE THEN OP\u03a4\u0399\u039c\u0399\u0396\u0395", "content": "This step is the classical preference optimization (Rafailov et al., 2023), where responses are generated and the gradient descent is performed. Take the pointwise reward model setting as an example, for every prompt, we sample n responses with reward annotated for each; we take the responses with the maximal and the minimal reward to construct the preference pairs, then optimize upon.\nPut together, eva can unify existing iterative optimization pipeline (Tran et al., 2023) with a new creator module, which can either share the same network as the solver policy or operate independently."}, {"title": "3.4 UNDERSTANDING EVA IN DIFFERENT INTUITIVE WYAS", "content": "Learning potential. Our metric intuitively identifies the learning potential of a prompt by measuring the gap between the best and worst response from the solver. We reason, that prompts eliciting both high-reward and low-reward outcomes, reflect learnable tasks where the model is capable of improving but has not yet mastered, thereby implying learning potential (cf., Jiang et al. (2021b))."}, {"title": "Worst-case guarantees.", "content": "The minimax-regret objective, by design, leads to solvers that perform robustly across the prompt space, thus gives the wrost-case guarantee. While exact equilibrium may not be attainable with approximation, our empirical results in \u00a7 4.2.1 present robustness."}, {"title": "Auto-curricula for the players.", "content": "With the stochastic policy, the advantage may be heuristically understood as the reward difference between a base solver and a reference solver. Rather than optimizing separate solvers (Dennis et al., 2020), we sample multiple times from the same policy to create the pair. In this way, the creator is incentivized to produce new prompts that are just out of the comfort zone of solvers (Chaiklin et al., 2003):\nFor overly challenging prompts, both solutions perform poorly, leading to a low proxy.\nFor overly easy prompts, the base solution already performs well, again giving a low proxy.\nThe optimal strategy is to find prompts that are just beyond the solver's current capability."}, {"title": "Auto-curricula inherent to Contrastive Optimization.", "content": "Contrastive preference optimization generalizes DPO and a family of algorithms (c.f., Hejna et al. (2023); Rafailov et al. (2023); Tang et al. (2024)), many of whose losses monotonically decrease as the contrastive ratio increases. Here, by Eq. 2 and Eq. 6, the contrastive ratio can be written via the advantage-based proxy:\n$A_{\\min}(x) = \\beta \\cdot \\Delta_{\\theta; \\text{ref}}^* $.\nBy our proxy, we implicitly incentivize the creator to generate prompts that bring the most contrastive responses, which decrease the loss the most. This matches the curriculum learning literature, which prioritizes (in our case, generatively prioritizes) examples with smaller losses for better convergence and generalization (Bengio et al., 2009). We hereby suggest the Contrastive Curriculum Hypothesis: In contrastive preference optimization, prioritizing prompts with higher contrastive ratio improves sample efficiency and generalization. We show initial empirical results on this in \u00a7 4.2.1 and \u00a7 4.2.4."}, {"title": "4 EXPERIMENTS", "content": "Datasets and models for training. We use UltraFeedback (Cui et al., 2023) as the training dataset, which contains diverse high-quality prompts that are primarily human-generated. We use the instruction-finetuned GEMMA-2-9B (Team et al., 2024) as the primary model, which is a strong baseline for models of its size. Detailed experimental setting can be found in \u00a7 A.\nEvaluation settings. We choose: (i) AlpacaEval 2.0 (Dubois et al., 2024), which assesses general instruction following with 805 questions; (ii) MT-Bench (Zheng et al., 2023), which evaluates multi-turn instruction following with 80 hard questions in 8 categories; (iii) Arena-Hard (Li et al., 2024b), which is derived from 200K user queries on Chatbot Arena with 500 challenging prompts across 250 topics. We use gpt-4-1106 as the judge and gpt-4-0314 as the baseline for win rate.\nOptimization algorithms. We focus on direct preference optimization and consider the following:\nWith reference policy: DPO (Rafailov et al., 2023), SPPO (Wu et al., 2024).\nWithout reference policy: SimPO (Meng et al., 2024), ORPO (Hong et al., 2024).\nReward models as preference oracles. We use ARMORM-8B (Wang et al., 2024) as our default reward model as the human-preference proxy, and consider the following for ablation studies:\nPointwise: ARMORM-8B (Wang et al., 2024), SKYWORKRM-27B (Liu and Zeng, 2024).\nPairwise: PAIRRM-0.4B (Jiang et al., 2023), PAIRRM-8B (Dong et al., 2024)."}, {"title": "4.1 MAIN RESULTS", "content": "In general, eva brings notable gains in alignment without relying on any human-crafted data, thus offering more efficiency. In the base setup, building on the one-iteration finetuned model ($\\theta_{0 \\rightarrow 1}$), eva adds a creator to self-evolve the prompt set of the initial iteration and uses any preference optimization algorithm for an additional open-ended RLHF iteration, resulting in $\\theta_{1 \\rightarrow 1^1}$.\neva achieves self-improvement. As shown in red rows in Table 1, eva yields notable performance improvement over $\\theta_{0 \\rightarrow 1}$ across different optimization algorithms, especially on the harder Arena-Hard benchmark, which is recognized to be more challenging and distinguishable among others due to the complexity of its prompts and its fairer scoring system (Li et al., 2024b; Meng et al., 2024). Specifically, eva brings 8.4% gain with SimPO as the solver, and 8.5% gain with DPO as the solver, surpassing its 27B version and matching claude-3-opus-240229 as reported on the AH leaderboard, while using fully self-automated prompt generation for alignment.\neva can surpass human-crafted prompts. We further show that eva-prompt-trained models ($\\theta_{1 \\rightarrow 1^1}$) can match and even outperform those trained on additional new prompts from UltraFeedback ($\\theta_{1 \\rightarrow 2}$) (which we denoted as human prompts), while being much cheaper and more efficient. Additionally, on MT-Bench, training with new human prompts typically show decreased performance in the first turn and only modest gains in the second turn. In contrast, eva notably enhances second-turn performance. We hypothesize that eva evolves novel, learnable prompts that include characteristics of second-turn questions, reflecting emergent skills like handling follow-up interactions."}, {"title": "4.2 ABLATION STUDIES", "content": "We conduct in-depth ablation studies on eva, with findings below to be elaborated on later:\n\u00a7 4.2.1 - informativeness metric: our regret-based metric outperforms other alternatives.\n\u00a7 4.2.2 - sample-then-evolve procedure: our method outperforms greedy selection.\n\u00a7 4.2.3 - scaling w/ reward models: the alignment gain of eva scales with reward models.\n\u00a7 4.2.4 - continual training : our method has monotonic gain with incremental training; the evolved data and schedule by eva serves as an implicit regularizer for better local minima."}, {"title": "5 RELATED WORKS", "content": "Self-improving algorithms and iterative optimization. This line of work focuses on iteratively generating samples from the response policy and continuously re-training the policy by selected self-generated samples. Major works include ReST (Gulcehre et al., 2023; Singh et al., 2023), STaR (Zelikman et al., 2022), RFT (Yuan et al., 2023), RAFT (Dong et al., 2023), self-improving LLMs (Huang et al., 2022; Yuan et al., 2024); in the context of preference optimization, iterative DPO (Xu et al., 2023b; Tajwar et al., 2024; Tran et al., 2023; Xiong et al., 2024; Pang et al., 2024) has proven effective. Most works focus on self-training by improving in Y | X, while we jointly optimize both responses and prompts via generative exploration in the (X, Y) space. Among them, we also distinctly present a game-theoretic framework with the minimax-regret principle as the guidance.\nPrompt synthesis for language models. Existing works include Self-Instruct (Wang et al., 2022), WizardLM (Xu et al., 2023a; Luo et al., 2023), Self-Align (Sun et al., 2024), Glan (Li et al., 2024a), EvoPrompt (Guo et al., 2023), Magpie (Xu et al., 2024) and others (Long et al., 2024). eva is an orthogonal contribution since any synthesis method can be plugged in as the evolve() for the creator. Importantly, our work presents a new reward-related metric to endow prompt the notion of informativeness, with new implications as in \u00a7 3.4. We also focus on preference optimization algorithms, while those existing works primarily use synthesized prompts in an SFT-only way.\nSelf-play and open-ended curriculum RL. Agents trained on a fixed data distribution are often brittle and may struggle to adapt to the real world (Hughes et al., 2024a). Self-play (Samuel, 1959; Silver et al., 2016) addresses this by having the agent learn through self-interaction, thus creating more diverse experiences and automatic curricula. In asymmetric self-play, the paradigm centers on \u201cAlice proposing a task, and Bob doing it\" (Sukhbaatar et al., 2017; Samvelyan et al., 2023; Beukman et al., 2024; Anil et al., 2021). We revive the classical asymmetric self-play principle (Sutton et al., 2011) in optimizing language models. Unlike traditional curriculum RL (Parker-Holder et al., 2022), which usually renders environments from specified levels (Dennis et al., 2020), our approach is generative by nature, as we directly generate contexts from the auto-regressive language models.\nSelf-play in RLHF. A growing line of research frames RLHF as a symmetric self-play game, where both players are response players (Munos et al., 2023; Wu et al., 2024; Choi et al., 2024; Rosset et al., 2024). However, these methods still rely on a fixed prompt distribution thus is sub-optimal. In contrast, we solve this by asymmetric self-play, enabling evolving prompt distributions for more generalizable language agents. During our work, we notice one concurrent paper adopting the asymmetric two-player setup (Zheng et al., 2024), however (i) it applies to red teaming tasks instead of general alignment benchmarks, (ii) it is incompatible w/ direct preference optimization, and (iii) it relies on the maxmin principle (which is known to be producing unlearnable environments (Dennis et al., 2020)) instead of the minimax regret principle (Fan, 1953; Savage, 1951) as we do. We also first precisely defined the new problem of open-ended RLHF, that generalizes over classical RLHF."}, {"title": "6 CONCLUDING REMARKS", "content": "Limitations and future directionss. eva defines a new paradigm for alignment, opening up many new directions, e.g., (i) extending to differentiable creator policies, combining w/ other evolve(.) methods; (ii) evolving for more iterations w/ on-policy solvers like RLOO (Ahmadian et al., 2024); (iii) investigating exploration bonuses for diversity, coverage and extrapolation, and self-consuming loops (Gerstgrasser et al., 2024); (iv) extending the game with more players for full automation (e.g., rewarders, critics, correctors, verifiers, retrievers); (v) extending from alignment to reasoning (e.g., auto-conjecturing in theorem proving (Poesia et al., 2024) can be cast as asymmetric games), w/ process reward models and hierarchical tree search for creator and solver generations; (vii) exploring other metric like Fisher information for theoretical guarantees; (viii) scaling up w/ more data.\nConclusions. eva is a new, simple framework for aligning language models, and can be plugged into any existing alignment pipeline. The primary takeaway may be that RLHF can be made open-ended: (i) self-evolving joint data distributions can bring significant gain (as shown across various preference optimization algorithms), and (ii) reward advantage acts as an effective metric informing the collection and creation of future prompts for alignment. eva presents a new view of alignment by framing it as an asymmetric game between a creator generating new and learnable prompts and a solver producing preferred responses. eva also incentivizes agents to create problems rather than to simply solve problems, which is a key feature of intelligence, yet model trainers may often neglect."}, {"title": "APPENDIX", "content": "The appendix is organized as follows:\n\u00a7 A - Details on Reproducibility\n\u00a7 B - Plug-In Loss Functions Used in Main Results\n\u00a7 C - Additional Experimental Results\n\u00a7 D - Prompt and Response Examples\n\u00a7 E - Extended Literature Review on Open-Endedness"}, {"title": "A DETAILS ON REPRODUCIBILITY", "content": "Code release. As mentioned, we hope to open-source all the code and all the datasets (generated prompts and responses) and models (beyong the current model family), upon approval \u2013 before then, we are more than happy to provide any clarification requested to help re-implement eva and replicate our results. In general, our code base is made to be simple to use for practitioners, requiring only a creator module addition within the commonly adopted Alignment Handbook pipeline.\nHyperparameter settings. We follow the original hyperparameter settings as in (Hong et al., 2024; Meng et al., 2024; Wu et al., 2024), default to be:\nIterative Training Settings. By default (Tran et al., 2023; Yuan et al., 2024), we train with equal-size prompt subset in each iteration. Unless otherwise specified, we use 10K prompts from the UltraFeedback dataset (Cui et al., 2023) per iteration. The incremental training proceeds as follows:\n\\theta_0: Base SFT model.\n\\theta_{0 \\rightarrow 1}: initialize with \\theta_0; then train with the prompt split $X_1$ by self-generated responses from the initial model \\theta_0.\n\\theta_{1 \\rightarrow 2}: initialize with \\theta_{0 \\rightarrow 1}; trained with the prompt split $X_2$ via by self-generated responses from the initial model \\theta_{0 \\rightarrow 1}.\nFor evolving prompts (e.g., evolving $X_1$ to $X'_1$), with the calculated informativeness metric for each prompt, we normalize them as the weight to do weighted sampling for a 25% informative subset to get $X_{info}$. We then iterate over in $X_{info}$ and call EvolInstru (Xu et al., 2023a) as the plug-in evolving method (with the number of evolutions as 4) using the default mutation templates for (i) in-depth evolving (constraints, deepening, concretizing, increased reasoning steps) and (ii) in-breadth evolving (extrapolation) as implemented in tasks/evol_instruct/utils.py of distilabel==1.3.2. Next we uniformly select 80% prompts from this evolved dataset and 20% from the original dataset (i.e., the buffer) to form $X'_1$. We do not seek extensive parameter search (e.g., the number of evolutions, the evolving ratio) in this stage and encourage future works on exploring this and other plug-in evolving methods. For solver we generate 6 responses per prompt.\nSoftware environments. All our experiments are conducted on 8xNVIDIA H100 SXM GPUs. Our codebase primarily relies on transformers==4.40.0. For the response generation of GEMMA models at the training stage, we use vllm==0.5.4 with flashinfer backend for CUDA 12.4 and"}, {"title": "BPLUG-IN LOSS FUNCTIONS USED IN MAIN RESULTS", "content": "Table 6: Direct preference alignment algorithms used in the main experiments. In parameter tuning, we include an additional negative log-likelihood loss for chosen responses (i.e., $\\log \\sigma(\\beta \\cdot \\Delta_{\\theta; \\text{ref}}^* )$."}, {"title": "C ADDITIONAL EXPERIMENTAL RESULTS", "content": "In general, eva maintains the accuracy on downstream tasks and is robust on those reasoning-heavy tasks, and the scaling with reward models is more prominent on AlpacaEval, possibly due to the training sources for such reward models."}, {"title": "E EXTENDED LITERATURE REVIEW ON OPEN-ENDEDNESS", "content": "The design of our game-theoretic framework for language model post-training is inspired from many prior works in open-ended learning. As reflected in \u00a7 3, the central idea of open-ended learning is not to optimize for a specific, static distribution, but to develop an agent that can generalize well across unseen, novel environments, which are the environments that the agent has not been explicitly trained on. To achieve this, unsupervised environment design proposes to generate environments that present a curriculum of increasing complexity for the agent to evolve, which ensures that the agent's learning is not narrow, but broad enough to handle the diversity of complexity of future environments. In such curriculum, as the agent solves simpler environments, it moves on to more difficult ones, thus progressively builds more sophisticated strategies. Furthermore, by adopting a minimax regret framework, this approach adds a layer of robustness by minimizing the agent's performance gap in worst-case (i.e., most adversarial) environments. It is not just about generalizing to novel environments but also about ensuring that agents to handle the most challenging scenarios.\nIn addition to distinctions discussed in \u00a7 5, we here list several foundational works in this line, and encourage the LLM community to explore with more rigor and depth: Schmidhuber (1991) presents an initial investigation into open-ended learning via self-supervised curiosity-driven exploration; Wang et al. (2019) emphasize co-evolution of environments and agent policies by training a population of agents that adapt to and solve progressively complex challenges; Dennis et al. (2020) formally introduce the notion of Unsupervised Environment Design (UED), where a protagonist and antagonist agent pair simulates regret by competing in shared environments, driving the protagonist (the main learner) to adapt to increasingly challenging scenarios; Jiang et al. (2021b) introduce Prioritized Level Replay (PLR), which uses a rolling buffer of high-regret levels to dynamically adjust the training curriculum, and selects levels with the higher learning potential; Parker-Holder et al. (2022) further propose improvements by editing previously high-regret levels; Hughes et al. (2024b) present a formal definition for open-ended system with respect to novelty and learnability, which generalizes various systems, e.g., AlphaGo (Silver et al., 2016), AdA (Team et al., 2021), etc.\nOur focus here was on classical, seminal, and directly relevant works. We welcome suggestions for any additional references we may have missed that could enhance our citations \u2013 please feel free to reach out."}]}