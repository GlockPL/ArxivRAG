{"title": "SyncDiff: Synchronized Motion Diffusion for Multi-Body Human-Object Interaction Synthesis", "authors": ["Wenkun He", "Yun Liu", "Ruitao Liu", "Li Yi"], "abstract": "Synthesizing realistic human-object interaction motions is a critical problem in VR/AR and human animation. Unlike the commonly studied scenarios involving a single human or hand interacting with one object, we address a more generic multi-body setting with arbitrary numbers of humans, hands, and objects. This complexity introduces significant challenges in synchronizing motions due to the high correlations and mutual influences among bodies. To address these challenges, we introduce SyncDiff, a novel method for multi-body interaction synthesis using a synchronized motion diffusion strategy. SyncDiff employs a single diffusion model to capture the joint distribution of multi-body motions. To enhance motion fidelity, we propose a frequency-domain motion decomposition scheme. Additionally, we introduce a new set of alignment scores to emphasize the synchronization of different body motions. SyncDiff jointly optimizes both data sample likelihood and alignment likelihood through an explicit synchronization strategy. Extensive experiments across four datasets with various multi-body configurations demonstrate the superiority of SyncDiff over existing state-of-the-art motion synthesis methods.", "sections": [{"title": "1. Introduction", "content": "In everyday life, humans frequently manipulate objects to complete tasks, often using both hands or collaborating with others. For instance, an individual might use both hands to set a fallen chair upright, hold a brush in one hand to scrub a bowl held in the other, or work with another person to lift and position a heavy object. These activities are examples of multi-body human-object interactions (HOI) [13, 39, 40], where \"body\" may refer to objects, hands, or humans. Synthesizing such interactions is a prominent research area with significant applications in VR/AR, human animation, and robot learning [89].\nThe primary challenge in multi-body HOI synthesis is capturing the complex joint distribution of body motions and ensuring that the individual body motions are not only synchronized but also mutually aligned with meaningful interaction semantics. This challenge intensifies as the number of bodies increases with high-order motion relationships. Existing works usually focus on specific configurations of the bodies with limited body numbers such as a single hand interacting with an object [89], a person engaging with an object [40, 79], or two hands manipulating a single item [13, 69]. These works introduce configuration-specific priors and are restricted to particular setups, leading to a strong demand for a generic method that could handle the series of multi-body HOI synthesis problems in a unified manner without restrictions for body numbers.\nTo develop a generic solution, we draw two key insights. First, there exists a straightforward method of treating the motions of all individual bodies as a high-dimensional representation and using a single diffusion model to depict its distribution [13]. However, this diffusion model only estimates the data sample scores, which only implicitly depicts the correlations across individual body motions and is usually insufficient to guarantee the alignment between individual body motions with limited data amount. To solve this, we need to explicitly model a new set of alignment scores, which guides the model to synthesize synchronized motions. Second, once we estimate the sample and alignment scores from diffusion models, in inference, we can jointly optimize sample and alignment likelihoods by approaching multi-body HOI synthesis as a motion synchronization problem defined on a graphical model, whose nodes represent single bodies' individual motions, and edges describe the relative motions between pairwise bodies.\nTo realize the ideas above, we design Synchronized Motion Diffusion (SyncDiff) for generic multi-body HOI synthesis. SyncDiff is a diffusion model defined in the graphical model above, which simultaneously estimates the data sample scores and alignment scores by operating on a redundant motion representation consisting of all individual motions and relative motions. In particular, we propose frequency-domain motion decomposition to better estimate sample scores, which factorizes all individual and relative motions into different frequency components and supervises high-frequency components generated in the frequency domain independently. This approach prevents high-frequency, small-amplitude vibrations from being overshadowed by low-frequency, large-scale movements. For better synchronization, we design a set of alignment scores, which measures the direction where computed relative motion between two bodies' synthesized individual motions approaches the synthesized relative motion on the edge connecting them. At inference time, by leveraging both sample and alignment scores, we discover a simple explicit synchronization strategy, which is equivalent to maximum likelihood inference with a theoretical justification.\nIntegrating these motion synchronization and decomposition techniques, SyncDiff consistently improves multi-body interaction coordination and fine-grained motion fidelity. Experiments on four datasets involving different numbers of hands, humans, and objects demonstrate the strong generalizability of our method over many state-of-the-art methods in every specific setting.\nIn summary, we propose a newly adapted diffusion model(SyncDiff) defined on graphical models to synthesize synchronized motions. It incorporates a frequency-domain decomposition to capture fine-grained periodic motions. We also provide theoretical derivation for two synchronization mechanisms, namely the alignment scores and explicit synchronization in inference."}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Human-object Interaction Motion Synthesis", "content": "Synthesizing interactive motions between humans and objects has been an emerging research field supported by extensive HOI datasets [2, 3, 5, 20, 24, 36, 37, 45, 46, 77, 95, 99, 100]. A branch of works [25, 54, 87, 88, 110] aims at synthesizing human bodies interacting with objects, while others [4, 21, 50, 60, 62, 89, 104, 113] focus on hand-object interactions for learning more dexterous manipulation behaviors. For synthesizing human-object interaction motions, early works [22, 78, 108, 109] propose to leverage conditional variational auto-encoders [35, 72] to model distributions of human-object interaction motions and enable generalization ability, meanwhile using CLIP [61] to encode language features for text-based control. With the tremendous progress of diffusion models [28], diffusion-based methods [19, 39, 40, 56, 79, 90, 91, 93] have been widely proposed for superior generation qualities. Focusing on multi-person and object collaboration synthesis, recent approaches [14, 42, 56, 68] enhance feature exchange among different entities in diffusion steps for better cross-entity synchronization. For synthesizing hand-object interaction motions, one solution is to train dexterous hand agents [7, 10, 11, 84, 102, 103] using physical simulations [49, 83] and reinforcement learning [23, 67]. Another solution that is similar to ours is fully data-driven. Early methodologies [43, 44, 94, 101, 111, 114] apply representation techniques such as contact map or reference grasp, which better model the contact between hand joints or hand surface points and the relevant local regions of the object. Several recent works [4, 12, 69, 104, 107] have shifted the focus to multi-hand collaboration, while few works have addressed interactions involving more than one object. Compared to the above methods, our framework can handle both human-object and hand-object interactions, allowing any number of bodies in the scene, without the need for any delicately designed structures like contact guidance."}, {"title": "2.2. Injecting External Knowledge into Diffusion Models", "content": "Benefitting from high generation qualities, diffusion models [28, 73] are widely adopted in versatile synthesis tasks regarding images [64, 105], videos [29, 30], and motions [79, 106]. To further induce generation results to satisfy specific demands or constraints, injecting additional knowledge (e.g., expected image styles, human-object contact constraints) into diffusion processes is an emerging methodology in recent studies [19, 39, 56, 93]. For this purpose, one paradigm is explicitly improving intermediate denoising results in inference steps using linear fusions with conditions [9, 48, 79], optimizations with differential energy functions [17, 39, 56, 97], or guidance from learnable networks [32, 93, 98]. Another paradigm is to design additional diffusion branches with novel conditions [27, 92, 112] or representations [19] that comprise the knowledge, encompassing the challenge of fusing them with existing branches. Classifier-free guidance [27, 112] uses a linear combination of results from different branches in inference steps, Xu et. al. [92] propose blending and learnable merging strategies, while CG-HOI [19] presents additional cross-attention modules. Our method follows the second paradigm with relative representation designs and transforms the problem into a graphical model, optimizing it with an explicit synchronization strategy."}, {"title": "2.3. Motion Synchronization", "content": "Motion Synchronization aims at coordinating the motions of different bodies in a multi-body system to satisfy specific inter-body constraints. As technical bases, for specific parameterized closed-form constraints, the closed-form solutions in Euler angles, axis-angle, quaternions, and rotation matrices are derived in traditional approaches [1, 6, 8, 31, 34, 53, 66, 115]. To achieve global attitude synchronization under mechanical constraints in SE(3), several works adopt gradient flow [51, 52], lifting method [81], and matrix decomposition [82] techniques. In the trend of graph-based multi-body modeling, existing approaches present coordination control based on general undirected [63] and directed [47, 85, 86] graphs, while some works [26, 80] further extend to nonlinear configurations and dynamic topologies. These works do not adopt learning-based methods; by contrast, our method uses a generative model to enhance consistency, formulating data-driven motion synchronization."}, {"title": "2.4. Modeling Motions in Frequency Domain", "content": "Modeling motions in the frequency domain is a widely used strategy in diverse research areas. Animation approaches [70, 75, 76] leverage frequency domain characterization to generate periodic vibrations like fluttering of clouds, smoke, or leaves. Physical simulation methods [15, 16, 18] simulate physical motions by analyzing the underlying motion dynamics in the frequency domain. Adopting learning-based methods, GID [41] uses diffusion models to capture object motions in the frequency domain, generating complex scenes with realistic periodic motions."}, {"title": "3. Problem Formulation and Challenges", "content": "Considering an interaction scenario comprising $n$ articulated skeletons $h_1, h_2,..., h_n$ and $m$ rigid bodies $o_1, o_2,..., o_m$, where articulated skeletons are sets of joints allowing the skeletons' motion to be fully reconstructed based on joints' positions and intrinsic shape parameters. Our task is to synthesize their motions with known action labels, object categories, object models, and shape parameters $B_{i \\in [1, n]}$ of each skeleton. The skeletons are MANO [65] hands or SMPL-X [55] humans in our experiments, and we would like to highlight that our task releases the needs of either predefined object motions [40] or hand-object key frames [13] that are used in existing works.\nCompared to previous single-object interaction motion synthesis problems [13, 40, 69], two new challenges lie in synthesizing multi-skeleton multi-object interactions. Firstly, governed by abundant topologies, geometries, and versatile functionalities of objects, collaborations among multiple objects are more diverse and complex than those of one hand/human and one object [113], which typically follow constant grasping poses and can easily benefit from learnable priors (e.g., contact maps [3, 38, 44, 94], affordance maps [33, 50, 56, 96]). Secondly, the increase in bodies and objects puts forward higher demands for multi-entity motion synchronization. Slight temporal misalignment between any two entities could dramatically lead to unrealistic and failing cooperation behaviors."}, {"title": "4. Method", "content": "As mentioned in Section 1, in SyncDiff, representation factorization (Section 4.1) involves two aspects. On one hand, in the graphical model, we need individual motions for single bodies on nodes, and relative motions for pairwise bodies on edges. On the other hand, individual/relative representations are both decomposed into low- and high-frequency components. For our synchronization mechanisms, we will elaborate on both the implicit joint modeling (Section 4.2) and the explicit synchronization strategy during inference (Section 4.3). In order to produce hand mesh or human body mesh from predicted joints, the post processing algorithm of mesh construction is introduced (Section 4.4)."}, {"title": "4.1. Factorized Motion Representations", "content": "1. Individual and Relative representations. For a single body $b = h_{i \\in [1, n]}$ or $b = O_{j \\in [1, m]}$, we represent its motion as $X_{u,b}$ in the world coordinate system. Here $X_{u,h_i} \\in \\mathbb{R}^{T \\times D \\times 3}$, where $D$ is the number of its joints, represents 3D joint positions. $X_{u,O_j} = [t_j, q_j] \\in \\mathbb{R}^{T \\times 7}$, where $t_j \\in \\mathbb{R}^{T \\times 3}$ is its translation, and $q_j \\in \\mathbb{R}^{T \\times 4}$ is the quaternion representing its orientation.\nFor two bodies $b_1$ and $b_2$, we use $x_{b_1, b_2 \\rightarrow b_1}$ to denote the relative motion of $b_2$ in $b_1$'s coordinate system. We require that $b_1$ is a rigid body, and omit relative representation between articulated skeletons. If $b_2$ is also a rigid body, then $X_{b, b_2 \\rightarrow b_1} = [t_{b_2b_1}, q_{b_2b_1}]$, where $t_{b_2b_1} = q_{b_1}^{-1}(t_{b_2} - t_{b_1})$, and $q_{b_2b_1} = q_{b_1}^{-1} q_{b_2}$. If $b_2$ is an articu-"}, {"title": "4.2. Implicit Joint Modeling", "content": "We use one single diffusion model (Figure 2) to jointly model all individual and relative motions. The introduction will be carried out from two aspects: basic model backbone (Section 4.2.1), and loss functions corresponding to our derived scores (Section 4.2.2)."}, {"title": "4.2.1. Model Backbone", "content": "We adopt the latent diffusion [64] paradigm.\nFor action/object label features, unlike DiffH2O [13], which uses fully descriptive sentence annotations of data in GRAB [77], we do not assume that the given action labels and object labels can be efficiently and accurately concatenated into sentences, in order to improve the universality of our approach. Instead, we use pretrained CLIP [61] to extract features for each label individually (Specifically, the action label is expanded into the form of \"A video of [action]\".), and process these features through an independent 2-layer MLP.\nFollowing works like [39, 40], object geometry features are encoded by BPS [57]. They then pass through an independent 2-layer MLP.\nConcatenating the processed action/object label features, object geometry features, and shape parameters $B_{i \\in [1, n]}$ together, we finally obtain the condition vector. This condition vector is replicated and combined with $\\tilde{x}$ and $AC(\\tilde{x})$ derived from noised x respectively and passed through two separate convolutional layers into the latent space, and then concatenated to be denoised by a 4-layer, 8-head transformer. The denoised latent vectors are split and recovered with linear projectors as $\\tilde{x}_0$ and $AC(\\tilde{x}_0)$, the latter of which is reconstructed as $\\hat{x}_0$. The final denoised motion sequence is recomposed by $\\tilde{x}_0 = \\tilde{x}_0 - x_0 + x_0$."}, {"title": "4.2.2. Loss Functions", "content": "Suppose we need to supervise the final synthesis results $\\{\\tilde{x}, \\hat{x} \\}$, where $\\{\\tilde{x}, \\hat{x} \\}$ are ground-truth motions. Define $\\overline{x} = \\tilde{x} + \\hat{x}$. We need loss functions corresponding to both the data sample and alignment scores.\nFor data sample scores, our method is similar to the standard reconstruction loss, except that we supervise $\\tilde{x}$ and $\\hat{x}$ separately. We denote $L_{dc} = ||\\tilde{x} - \\tilde{x} ||_2^3$, $L_{ac} = ||\\hat{x} - \\hat{x} ||_2^2$ and $L_{norm} = \\sum_{j=1}^m ||1 - |q_j|||^2_2$. The last one is used to induce the norms of the quaternions representing rigid body rotations to be as close to 1, where $\\overline{\\tilde{x}}_{u, O_j} = [t_j, q_j]$.\nAs for our alignment scores for pairwise bodies, we can similarly derive the alignment loss\n$$L_{align} = \\sum_{1 < j_1 < j_2 < m} 2|| \\tilde{x}_{b, O_{j_2} \\rightarrow O_{j_1}} - rel(\\tilde{x}_{u, O_{j_1}}, \\tilde{x}_{u, O_{j_2}}) ||^2 + \\sum_{i \\in [1, n], j \\in [1, m]} || \\tilde{x}_{b, h_i \\rightarrow O_j} - rel(\\tilde{x}_{u, O_j}, \\tilde{x}_{u, h_i})||^2,$$\nwhere $rel(a, b)$ denotes $b$'s motion in $a$'s coordinate system. Finally, the total loss function is calculated as:\n$$L = \\lambda_{dc} L_{dc} + \\lambda_{ac} L_{ac} + \\lambda_{align} L_{align} + \\lambda_{norm} L_{norm}.$$"}, {"title": "4.3. Explicit Synchronization in Inference Time", "content": "Experiments show that after splitting the motion into $\\tilde{x}$ and $\\hat{x}$ for separate supervision, the issue of hand-object or human-object misalignment becomes more severe compared to simply using the mixed representation of $x = \\tilde{x} + \\hat{x}$. What's worse, relying solely on implicit joint modeling cannot fully mitigate this issue. Our strategy is to introduce an explicit synchronization process during inference time, aiming to leverage both data sample scores and alignment scores to address this problem. Since the synchronization step is time-consuming, to balance performance and efficiency, we perform synchronization operations every $t_0 (t_0 << T)$ steps, where $T$ is the total number of denoising steps, as is shown in Figure 2. In particular, for each synchronization step at inference time, let the current predicted motion be $\\hat{x}_t$. According to the diffusion formula [28], without synchronization, the next step would be:\n$$x_{t-1} = \\hat{\\mu}(x_t, t) + \\sigma \\epsilon (\\epsilon \\sim N(0, I)),$$\nwhere $\\hat{\\mu}$ is the predicted mean value, and $\\sigma_t$ is a predefined constant real number. For convenience, we denote the motion before synchronization as $\\tilde{\\mu}$ and the motion after synchronization as $\\hat{\\mu}'$. Let $\\sigma_t$ be abbreviated as $\\sigma$. For different parts of $\\hat{\\mu}'$, we handle them as follows:\n1. Individual Motions of Rigid Bodies. Let\n$$\\hat{\\mu}'_{u, O_j} = \\frac{\\sigma \\lambda}{1 + 2\\sigma^2 \\lambda} \\tilde{\\mu}_{u, O_j} + \\frac{2\\sigma^2 \\lambda}{1 + 2\\sigma^2 \\lambda} comb(\\tilde{\\mu}_{u, O_j}, \\hat{\\mu}_{b, O_{j'} \\rightarrow O_j}) + \\sigma' \\epsilon,\\quad \\qquad j' \\neq j,$$"}, {"title": "5. Experiments", "content": null}, {"title": "5.1. Datasets", "content": "To examine our method's generalizability across various multi-body interaction configurations, we utilize four datasets with different interaction scenarios: TACO [46] (two hands and two objects), CORE4D [100] (two people and one object), GRAB [77] (one or two hands and one object), and OAKINK2 [99] (two hands and one to three objects). We describe data splits for each dataset below.\n(1) TACO: We use the official split of TACO, with four testing sets, each representing different scenarios: 1) the interaction triplet (action, tool category, target object category) and the object geometries are all seen in the training set, 2) unseen object geometry, 3) unseen triplet, and 4) unseen object category and geometry.\n(2) CORE4D: We divide 875 motion sequences into one training set and two testing sets, where the two testing sets represent seen and unseen object geometries, respectively. The (action, object category) pairs from testing sets are all involved in the training set.\n(3) GRAB: We use an existing data split of unseen subjects from IMoS [22] and that of unseen objects from DiffH2O [13]. Please refer to the two papers for details.\n(4) OAKINK2: We utilize the train, val, and test divisions stated in the TaMF task of their paper.\nToward a fair comparison with DiffH2O [13], we follow the setting of DiffH2O for GRAB, where methods are required to generate hand-object manipulations after grasping objects. For the other three datasets, all methods need to synthesize complete multi-body interaction motion sequences without direct guidance."}, {"title": "5.2. Evaluation Metrics", "content": "To evaluate the qualities of synthesized motion sequences comprehensively, we present two types of evaluation metrics focusing on fine-grained physical plausibility and general motion semantics, respectively.\n(1) Physics-based metrics measure the physical plausibility of hand-object/human-object interactions and the extent of motion coordination among different bodies. We use the Contact Surface Ratio (CSR) for hand-object settings and the Contact Root Ratio (CRR) for human-object settings to denote the proportion of motion sequences where hand-object/human-object contact occurs. Contact is defined as the hand mesh being within 5mm of at least one object for CSR, and the two hand roots of a human consistently being within 3cm of at least one object for CRR. When there are multiple hands or humans, we take the average among all of them. We label the frames based on whether hand-object contact occurs for ground-truth and synthesized motions and then compute their Intersection-over-Union (IoU), denoted as CSIoU. Besides, Interpenetration Volume (IV) and Interpenetration Depth (ID) are incorporated to assess penetration between different bodies.\n(2) Motion semantics metrics evaluate high-level motion semantics and its distributions, comprising Recognition Accuracy (RA), Fr\u00e9chet Inception Distance (FID), Sample Diversity (SD), and Overall Diversity (OD). Following existing evaluations for motion synthesis [39, 40], we train a network to extract motion features and predict action labels using ground-truth motion data. For better feature semantics, the network is trained on the combination of all train, val, and test splits. RA denotes the action recognition accuracy of the network on synthesized motions. FID measures the difference in feature distributions of generated and ground-truth motions. Following [13], SD represents the mean Euclidean distance between multiple generated wrist trajectories in a single sample, and OD refers to the mean distance between all generated trajectories in a dataset split."}, {"title": "5.3. Comparison to Existing Methods", "content": "Hand-Object Interaction. For hand-object interaction motion synthesis, we compare our SyncDiff to two state-of-the-art approaches MACS [69] and DiffH2O [13]. For MACS, we first generate the motions of all objects in their object trajectory synthesis phase, and then directly use their hand motion synthesis phase to get overall results. For DiffH2O, we use the version without grasp reference input."}, {"title": "5.4. Ablation studies", "content": "We examine the effect of our three key designs (frequency-domain motion decomposition, the alignment loss $L_{align}$, and the explicit synchronization) separately. The results after removing each of these three components individually are shown as \u201cw/o decompose\u201d, \u201cw/o $L_{align}$\u201d, and \u201cw/o exp sync\u201d. Two additional ablations are to remove the two synchronization mechanisms and all three designs, denoted as \"w/o $L_{align}$, exp sync\u201d and \u201cw/o all\u201d, respectively. Results in Table 1, 3, and 4 show that removing any of the above three components can lead to varying extents of performance decline. Compared to the method without any key design, simply incorporating frequency-domain decomposition obtains coherent performance drops under nearly all physics-based metrics due to more challenging multi-body synchronization. Nevertheless, the two synchronization strategies consistently enhance the effects and finally achieve the best results on nearly all evaluation aspects. Removal of Decomposition. As shown in Figure 8, af-"}, {"title": "6. Conclusions", "content": "We present SyncDiff, a universal framework for synchronized motion synthesis of multi-body HOI interaction by estimating both data sample scores and alignment scores, and jointly optimizing sample and alignment likelihoods in inference. We also introduce a frequency-domain decomposition to better capture high-frequency, small-amplitude motion details. Experiments on four datasets demonstrate that SyncDiff can be adapted to synthesize synchronized multi-body human-object interaction sequences with any number of humans, hands, and rigid objects. Comparative experiments also demonstrate that in each specific setting, our method achieves better physical accuracy and action semantics than a range of state-of-the-art works. Some limitations of our work and their potential solutions are discussed in the appendix."}, {"title": "Appendix", "content": null}, {"title": "A. Details of Our Two Synchronization Mechanisms", "content": null}, {"title": "A.1. Diffusion Model Basics", "content": "Diffusion models [71] and its variants [28, 74], especially latent diffusion, have been widely applied in different tasks, such as high-resolution image generation [64] or video generation [30]. They simulate the data distribution by introducing a series of variables $\\{x_t\\}_{t=1}^T$ with different noise levels. The forward noise process can be represented as\n$$q(x_t|x_{t-1}) = N(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I),$$\nwhere $0 < \\beta_t < 1$. We can derive that\n$$x_t = \\sqrt{\\overline{\\alpha}_t} x_0 + \\sqrt{1 - \\overline{\\alpha}_t} \\epsilon,$$\nwhere $\\epsilon \\sim N(0, I)$, and $\\overline{\\alpha}_t = \\prod_{i=1}^t (1 - \\beta_i)$.\nFor the inference process, from $T$ that is large enough so that $p(x_T) \\approx N(0, I)$, do reverse sampling step by step, following\n$$p_\\theta(x_{t-1}|x_t) = N(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2 I),$$\nwhere $\\mu_\\theta$ is the mean distribution center predicted by network, and $\\sigma_t^2$ is pre-defined constant variance. Then\n$$x_{t-1} = \\mu_\\theta(x_t, t) + \\sigma_t z (z \\sim N(0, I)).$$\nFinally we can get $x_0$, which is the denoised sample."}, {"title": "A.2. Complete Proof of Our Two Synchronization Mechanisms", "content": "First let's derive some commonly used formulas in diffusion models that we will need in our proof, which the readers might not be familiar with. If you are familiar with the step-by-step denoising formula of diffusion, you can skip directly to Eq 8 and 9.\nSuppose the noise-adding process has a total of T steps, with each step's amplitude denoted by $\\beta_t (t \\in [T])$, we define $\\alpha_t = 1 - \\beta_t$, and $\\overline{\\alpha}_t = \\prod_{i=1}^T \\alpha_i$.\nThen by basic principles of diffusion models, we have\n$$x_t \\sim N(\\sqrt{\\alpha_t} x_{t-1}, (1 - \\alpha_t) I)$$\n$$x_{t-1} \\sim N(\\sqrt{\\overline{\\alpha}_{t-1}} x_0, (1 - \\overline{\\alpha}_{t-1}) I)$$\n$$x_t \\sim N(\\sqrt{\\overline{\\alpha}_{t}} x_0, (1 - \\overline{\\alpha}_{t}) I)$$\nAccording to Bayesian's Formula,\n$$q(x_{t-1}| x_t, x_0) = \\frac{q(x_t | x_{t-1}) q(x_{t-1} | x_0)}{q(x_t | x_0)}$$\nTaking negative logarithmic,\n$$-\\log q(x_{t-1} | x_t, x_0) = \\frac{(x_t - \\sqrt{\\alpha_t} x_{t-1})^2}{2(1 - \\alpha_t)} + \\frac{(x_{t-1} - \\sqrt{\\overline{\\alpha}_{t-1}} x_0)^2}{2(1 - \\overline{\\alpha}_{t-1})} - \\frac{(x_t - \\sqrt{\\overline{\\alpha}_t} x_0)^2}{2(1 - \\overline{\\alpha}_t)} + Const$$\n$$= \\frac{\\alpha_t}{2(1 - \\alpha_t)} x_{t-1}^2 - [\\frac{\\sqrt{\\alpha_t}}{1 - \\alpha_t} x_t + \\frac{\\sqrt{\\overline{\\alpha}_{t-1}}}{1 - \\overline{\\alpha}_{t-1}} x_0] x_{t-1} = A x_{t-1}^2 + B x_{t-1} + C$$\n$$= A (x_{t-1} + \\frac{B}{2A})^2 + C'$$\nSo\n$$x_{t-1} \\sim N(\\frac{B}{2A}, \\frac{1}{2A} I)$$\nwhere\n$$\\mu_t = \\frac{B}{2A} = \\frac{\\frac{\\sqrt{\\alpha_t}}{1 - \\alpha_t} x_t + \\frac{\\sqrt{\\overline{\\alpha}_{t-1}}}{1 - \\overline{\\alpha}_{t-1}} x_0}{\\frac{\\alpha_t}{1 - \\alpha_t} + \\frac{1}{1 - \\overline{\\alpha}_{t-1}} }$$"}]}