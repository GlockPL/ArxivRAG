{"title": "Environment-Driven Online LiDAR-Camera Extrinsic Calibration", "authors": ["Zhiwei Huang", "Jiaqi Li", "Ping Zhong", "Rui Fan"], "abstract": "LiDAR-camera extrinsic calibration (LCEC) is the core for data fusion in computer vision. Existing methods typically rely on customized calibration targets or fixed scene types, lacking the flexibility to handle variations in sensor data and environmental contexts. This paper introduces EdO-LCEC, the first environment-driven, online calibration approach that achieves human-like adaptability. Inspired by the human perceptual system, EdO-LCEC incorporates a generalizable scene discriminator to actively interpret environmental conditions, creating multiple virtual cameras that capture detailed spatial and textural information. To overcome cross-modal feature matching challenges between LiDAR and camera, we propose dual-path correspondence matching (DPCM), which leverages both structural and textural consistency to achieve reliable 3D-2D correspondences. Our approach formulates the calibration process as a spatial-temporal joint optimization problem, utilizing global constraints from multiple views and scenes to improve accuracy, particularly in sparse or partially overlapping sensor views. Extensive experiments on real-world datasets demonstrate that EdO-LCEC achieves state-of-the-art performance, providing reliable and precise calibration across diverse, challenging environments.", "sections": [{"title": "I. INTRODUCTION", "content": "We have long envisioned robots with human-like intelli-gence, enabling them to understand, adapt to, and positively impact the world [1]\u2013[4]. This dream is becoming increasingly attainable with the advent of LiDAR-camera data fusion systems. LiDARs provide accurate spatial information, while cameras capture rich textural details [5]. The complementary strengths of these two modalities, when fused, provide robots with powerful environmental perception capabilities [6], [7]. LiDAR-camera extrinsic calibration (LCEC), which estimates the rigid transformation between the two sensors, is a core and foundational process for effective data fusion."}, {"title": "II. RELATED WORK", "content": "Target-based methods achieve high accuracy using cus-tomized calibration targets (typically checkerboards). However, they require offline execution and are significantly limited in dynamic or unstructured environments where such targets are unavailable [29], [30]. Recent studies have shifted to online, target-free methods to overcome these limitations. Pioneering works [14], [15], [31]\u2013[34] estimate the relative pose between the two sensors by aligning the cross-modal edges or mutual information (MI) extracted from LiDAR pro-jections and camera RGB images. While effective in specific scenarios with abundant features, these traditional methods heavily rely on well-distributed edges and rich texture, which largely compromise calibration robustness. To circumvent the challenges associated with cross-modal feature matching, sev-eral studies [29], [35]\u2013[37] have explored motion-based meth-ods. These approaches match sensor motion trajectories from visual and LiDAR odometry to derive extrinsic parameters through optimization. While they effectively accommodate heterogeneous sensors without requiring overlap, they demand precise synchronized LiDAR point clouds and camera images to accurately estimate per-sensor motion, which limits their applicability.\nAdvances in deep learning techniques have driven signif-icant exploration into enhancing traditional target-free algo-rithms. Some studies [5], [16]\u2013[19], [27], [30], [38] explore attaching deep learning modules to their calibration framework as useful tools to enhance calibration efficiency. For instance, [16] accomplishes LiDAR and camera registration by aligning road lanes and poles detected by semantic segmentation. Similarly, [19] employs stop signs as calibration primitives and refines results over time using a Kalman filter. A recent study"}, {"title": "III. METHODOLOGY", "content": "Given LiDAR point clouds and camera images, our goal is to estimate their extrinsic matrix ET, defined as follows:\n$E_T = \\begin{bmatrix} R & t\\\\ 0 & 1 \\end{bmatrix} \\in SE(3),$ (1)\nwhere R\u2208 SO(3) represents the rotation matrix, Et denotes the translation vector, and 0 represents a column vector of zeros. We first give an overview of the proposed method, as shown in Fig. 2. It mainly contains three stages:\n\u2022 We first utilize a scene discriminator to understand the environment through image segmentation and depth esti-mation, generating virtual cameras that project LiDAR in-tensity and depth from multiple viewpoints (Sect. III-A).\n\u2022 The image segmentation outputs are processed along two pathways (spatial and textural), then input into a dual-path correspondence matching module to establish reliable 3D-2D correspondences (Sect. III-B).\n\u2022 The obtained correspondences are used as inputs for our proposed spatial-temporal relative pose optimization, which derives and refines the extrinsic matrix (Sect. III-C).\n A. Generalizable Scene Discriminator\nOur environment-driven approach first employs a general-izable scene discriminator to interpret the surroundings by generating virtual cameras to project LiDAR point cloud intensities and depth. This discriminator configures both an intensity and a depth virtual camera from the LiDAR's per-spective. This setup yields a LiDAR intensity projection (LIP) image YI \u2208 RH\u00d7W\u00d71 (H and W represent the image height and width) and a LiDAR depth projection (LDP) image I. To align with the LDP image, the input camera RGB image I is processed using Depth Anything V2 [40] to obtain estimated depth images I\u00b9. To take advantage of semantic information,"}, {"title": "B. Dual-Path Correspondence Matching", "content": "Given the segmented masks and detected corner points, dual-path correspondence matching leverages them to achieve dense and reliable 3D-2D correspondences. Inspired by human visual system, DPCM consists of two pathways, one for cor-respondence matching of LIP and RGB images, and the other for LDP and depth images. For each pathway, DPCM adopted the approach outlined in [5] to obtain mask matching result A = {(MiV, MC) | i = 1, ..., m} from a cost matrix M\u00b9.\nEach matched mask pair can estimate an affine transformation [sRA,tA] \u2208 SE(2) to guide the correspondence matching. Specifically, we update the corner points cV in the virtual image to a location \u0109 that is close to its true projection coordinate using this affine transformation, as follows:\n$\\hat{c} = sR_A(c^V) + t_A$. (5)\nTo determine optimum corner point matches, we construct a cost matrix MC, where the element at x = [i, j]T, namely:\n$M^C(x) = $ (6)\ndenotes the matching cost between the i-th corner point of a mask in the LiDAR virtual image and the j-th corner point of a mask in the camera image. (6) consists of structural and textural consistency. The structural consistency measures the structural difference of corner points in the virtual and real image, where L(A) denotes the average perimeter of the matched masks and H(cik, cjk) represents the similarity of the neighboring vertices between current and target corner point. The textural consistency derives the relative textural similarity of the b neighboring zone. After establishing the cost matrix, a strict criterion is applied to achieve reliable matching. Matches with the lowest costs in both horizontal and vertical directions of M(x) are determined as the optimum corner point matches. Since every cV can trace back to a LiDAR 3D point p+l = [x, y, zL]T, and every cf is related to a pixel pr = [u, v] (represented in homogeneous coordinates as pr) in the camera image, the final correspondence matching result of DPCM is C = {(p+l, pr) | i = 1, . . ., q}."}, {"title": "C. Spatial-Temporal Relative Pose Optimization", "content": "EdO-LCEC treats the sensor's operational environment as a spatial-temporal flow composed of N multiple scenes across different time instances. In situations with incomplete or sparse point clouds, single-view methods such as [5], [14] are constrained by the limited number of high-quality corre-spondences. Our environment-driven approach addresses this limitation by integrating multi-view and multi-scene optimiza-tion. By merging the optimal matching results from multiple"}, {"title": "IV. EXPERIMENT", "content": "A. Experimental Setup and Evaluation Metrics\nIn our experiments, we compare our proposed EdO-LCEC with SoTA online, target-free LCEC approaches on the public dataset KITTI odometry [26] (including 00-09 sequences) and MIAS-LCEC [5] (including target-free datasets MIAS-LCEC-TF70 and MIAS-LCEC-TF360). To comprehensively evaluate calibration accuracy, we quantify LCEC performance using the Euler angle error magnitude er and the translation error magnitude et.\nNotably, sequences in KITTI odometry, aside from 00, were included in the training datasets for the learning-based methods [20], [21], [23], [25]. To ensure a fair comparison, we reproduced calibration results for both the left and right cameras on sequence 00 when the authors provided their code; otherwise, we used the reported results for the left camera from their papers. Since most learning-based methods lack APIs for custom data, our comparison with these methods is limited to the KITTI odometry 00 sequence. For experiments on the MIAS-LCEC dataset, as the results for the compared methods [5], [14]\u2013[16], [27] are reported in [5], we directly use the values presented in that paper.\n B. Comparison with State-of-the-Art Method\nIn this section, quantitative comparisons with SoTA ap-proaches on three datasets are presented in Fig. 6, Tables I\u00b2, II, III, and IV. Additionally, qualitative results are illustrated in Figs. 5 and 4.\n Evaluation on KITTI odometry. The results shown in Table I and II suggest that, with the exception of sequences 01 and 04, our method achieves SoTA performance across the ten sequences (00-09) in KITTI odometry. Specifically, in the 00 sequence, EdO-LCEC reduces the er by around 35.2-99.8% and the et by 11.8-98.7% for the left camera, and reduces the er by around 46.9-99.7% and the et by 13.9-97.6% for the right camera. Additionally, according to Fig. 4, it can be observed that the point cloud of a single frame in KITTI is so sparse that the other approaches behave poorly. In contrast, our proposed method overcomes this difficulty and achieves high-quality data fusion through the calibration result. We attribute these performance improvements to our spatial-temporal rela-tive pose optimization. Merging the optimal matching results from multiple views and scenes maximizes the number of reliable correspondences and ultimately improves overall cal-ibration accuracy.\n Evaluation on MIAS-LCEC Dataset. Compared to the sparse point clouds in the KITTI dataset, the point clouds in the MIAS-LCEC datasets are significantly denser, which facilitates feature matching and allows us to test the upper limits of the algorithm calibration accuracy. The results shown in Table III demonstrate that our method outperforms all other SoTA approaches on MIAS-LCEC-TF70. It can also be observed that our method dramatically outperforms CRLF, UMich, DVL, HKU-Mars, and is slightly better than MIAS-LCEC across the total six subsets. In challenging conditions that are under poor illumination and adverse weather, or when few geometric features are detectable, EdO-LCEC performs significantly better than all methods, particularly. This impres-sive performance can be attributed to the generalizable scene discriminator. The multiple virtual cameras generated by the scene discriminator provide a comprehensive perception of the calibration scene from both spatial and textural perspectives, which largely increases the possibility of capturing high-quality correspondences for the PnP solver. Furthermore, the data fusion results in Fig. 5, obtained using our optimized extrinsic matrix, visually demonstrate perfect alignment on the checkerboard. This highlights the high calibration accuracy achieved by our method."}, {"title": "V. CONCLUSION", "content": "In this paper, we explore to extend a new definition called \"environment-driven\" for online LiDAR-camera extrinsic cal-ibration. Unlike previous methods, our approach demonstrates human-like adaptability across diverse and complex environ-ments. Inspired by human perceptual systems, we designed a scene discriminator that actively analyzes the calibration scenes from spatial and textural perspectives. By leveraging structural and textural consistency between LiDAR projections and camera images, our method achieves reliable 3D-2D correspondence matching, overcoming the challenge of cross-modal feature matching in sparse scenes encountered by earlier approaches. Additionally, we modeled the calibration process as a spatial-temporal joint optimization problem, achieving high-precision and robust extrinsic matrix estimation through multi-view optimization within individual scenes and joint op-timization across multiple scenarios. Extensive experiments on real-world datasets demonstrate that our environment-driven calibration strategy achieves state-of-the-art performance."}]}