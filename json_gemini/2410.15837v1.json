{"title": "Long-distance Geomagnetic Navigation in GNSS-denied Environments with Deep Reinforcement Learning", "authors": ["Wenqi Bai", "Xiaohui Zhang", "Shiliang Zhang", "Songnan Yang", "Yushuai Li", "Tingwen Huang"], "abstract": "Geomagnetic navigation has drawn increasing attention with its capacity in navigating through complex environments and its independence from external navigation services like global navigation satellite systems (GNSS). Existing studies on geomagnetic navigation, i.e., matching navigation and bionic navigation, rely on pre-stored map or extensive searches, leading to limited applicability or reduced navigation efficiency in unexplored areas. To address the issues with geomagnetic navigation in areas where GNSS is unavailable, this paper develops a deep reinforcement learning (DRL)-based mechanism, especially for long-distance geomagnetic navigation. The designed mechanism trains an agent to learn and gain the magnetoreception capacity for geomagnetic navigation, rather than using any pre-stored map or extensive and expensive searching approaches. Particularly, we integrate the geomagnetic gradient-based parallel approach into geomagnetic navigation. This integration mitigates the over-exploration of the learning agent by adjusting the geomagnetic gradient, such that the obtained gradient is aligned towards the destination. We explore the effectiveness of the proposed approach via detailed numerical simulations, where we implement twin delayed deep deterministic policy gradient (TD3) in realizing the proposed approach. The results demonstrate that our approach outperforms existing metaheuristic and bionic navigation methods in long-distance missions under diverse navigation conditions.", "sections": [{"title": "1 Introduction", "content": "Global navigation satellite systems (GNSS) estimate the position of an object by measuring the time delay of signals, known as pseudoranges transmitted from multiple satellites. GNSS is widely adopted for long-range navigation with its provision of precise location information without cumulative error [1-3], as it recalculates position continuously based on satellite signals. The global coverage of GNSS allows it to work reliably across diverse terrains without the need for local navigation infrastructure. However, in environments e.g., tunnels or dense forests, GNSS signals are highly susceptible to environmental disruptions that can lead to degraded signal strength or complete signal loss [4,5]. Accurate GNSS positioning typically requires signals from at least four satellites. When fewer than four satellites are available, positioning becomes infeasible, resulting in GNSS outages [6]. In underwater environments, GNSS signals are completely unavailable [7], rendering it impossible to conduct long distance navigation using GNSS services.\nGeomagnetic navigation has drawn increasing attention with its using the ubiquitous geomagnetic signals for navigation [8-10]. Geomagnetic approaches are free from dependence on GNSS service or inertia navigation equipment and are with confined drift and cumulative errors [9,11], making it promising especially for long-distance navigation missions in unexplored areas or underwater missions where GNSS service is inaccessible. One of the most extensively investigated geomagnetic approaches carries out navigation via geomagnetic matching [12-14]. Geomagnetic matching compares and matches real-time geomagnetic measurements with a geomagnetic map to determine the location based on matching result. However, geomagnetic matching requires a pre-established geomagnetic map with optimal integrity and high precision, which is impossible for navigation across unexplored areas.\nNumbers of studies have shown that animals like pigeons and turtles can use the geomagnetic field for long-distance migration and homing [15\u201319], without any pre-stored maps in their brain. Furthermore, researchers have found that there exists an almost one-to-one correlation between the geomagnetic field vector and a specific location in near-earth space [20]. Inspired by the magnetic orientation behavior observed in animals, bionic geomagnetic navigation has gained momentum to derive the navigation based on real-time measured geomagnetic signals [21]. The problem of bionic geomagnetic navigation is framed as the autonomous exploration of a navigation path in response to the geomagnetic environment during the navigation [8].\nSeveral bionic geomagnetic navigation approaches have been studied. Liu et al. [22] introduced a bio-inspired geomagnetic navigation method for unmanned vehicles. They characterized the navigation process as the convergence of geomagnetic measurements towards the destination, and proposed a stress evolution search method to achieve such convergence. Their experimental results demonstrate efficient navigation without any prior geomagnetic map. However, they use a random searching strategy for the navigation that leads to significant fluctuations in heading angles and zigzag path patterns. A segmented searching approach is proposed to optimize and expedite trajectory convergence [23]. This approach employs the probability evolution strategy (PES) for the initial navigation search and can compensate the random searching. Taylor et"}, {"title": "2 Fundamentals", "content": "al. [24] explored the potential of magnetic inclination in guiding long-distance navigation. They employed an agent-based modeling approach and succeeded in trans-equatorial navigation missions using magnetic inclination measurements. Zhou et al. [15] proposed a differential evolution (DE) algorithm to improve the searching efficiency in geomagnetic navigation, where they use enhanced mutation in the evolution strategy to attain superior individuals for the searching. Qi et al. [3] utilized geomagnetic gradients of real-time measurements to predict the heading direction towards the destination. One of the requirements of this approach is that the gradients of geomagnetic intensity and inclination are not parallel in the navigation areas to guarantee an efficient gradient calculation, while such conditions might not hold in navigation practice. Overall, bionic geomagnetic navigation approaches iteratively generate populations to explore a solution space, from where an optimal solution is expected to lead to successful navigation. Nevertheless, the exploration process inevitably involves the execution of suboptimal solutions, which renders high cost or even failures especially in long-range navigation, making it a typical expensive optimization problem [10].\nDeep reinforcement learning (DRL) has been adopted to solve optimization problems in various applications, including visual navigation [25,26] and radar navigation [27,28] where DRL trains agents to learn the optimal navigation strategy through interactions with the environment. Instead of random or heuristic searching, DRL emphasizes the role of environment in shaping the learning of agent towards the optimal solution. Through a learning process based on trial and error, DRL agents continuously refine and converge to the optimized solution without the need to repeatedly assess suboptimal choices [29\u201331]. The DRL features in optimization make DRL well-suited for long-distance geomagnetic navigation, where real-time decision-making is essential in unexplored navigation areas with unknown geomagnetic conditions. The key challenge in applying DRL to geomagnetic navigation lies in accurately modeling the complex and non-linear decision-making processes in the dynamic navigation environment, while sustaining the efficiency and adaptability of the DRL agent in navigation missions.\nIn this work, we propose a DRL-based approach for long-distance geomagnetic navigation in GNSS-denied and unexplored environments. Particularly, we adopt the DRL framework of twin delayed deep deterministic policy gradient (TD3), and design a geomagnetic gradient-guided TD3 (Gradient-Guided TD3) algorithm to learn the optimal navigation solution for long-distance missions. We leverage the continuous action space of TD3 to capture the subtle variations in the magnetic field during navigation, to maintain stable and efficient navigation progress. To the best of our knowledge, we are the first to develop a DRL-based approach for long-distance geomagnetic navigation. We summarize the contributions of our work as follows.\n1. We develop a DRL-based geomagnetic navigation strategy where we propose a Gradient-Guided TD3 algorithm, leveraging ubiquitous geomagnetic information of the earth for navigation decisions in long-range missions where GNSS service can be unavailable and the geomagnetic features unknown during the navigation.\n2. To guarantee the exploration efficiency of the DRL agent in navigation, we design a dense reward based on geomagnetic gradient information. This encourages the agent to choose"}, {"title": "2.1 Mathematical Description of the Geomagnetic Field", "content": "The geomagnetic field is a fundamental physical field of the earth, theoretically corresponding to the geographical position of any point in near-earth space [32]. The strength of the geomagnetic field is typically measured in nanoteslas (nT). The average intensity on the ground is 5 \u00d7 10^{4} nT, reaching only about 7 \u00d7 10^{4} nT at the two poles, indicating that the geomagnetic field is considered a weak magnetic field.\nTo describe the spatial distribution of the geomagnetic field, the geomagnetic vector is de-composed into geomagnetic parameters (BF, BH, Bx, By, Bz, D, I) in the geographic coordinate"}, {"title": "2.2 Problem Formulation", "content": "The goal of geomagnetic navigation is to identify an optimal or suboptimal path from an origin to a destination in a two-dimensional (2D) or three-dimensional (3D) environment through geomagnetic information. Considering that the vehicle relies solely on geomagnetic information, magnetic sensors measure geomagnetic parameters in real-time, and use the disparity between geomagnetic parameters at the present position and the destination to determine the movement distance and yaw angle for movement [34]. We regard the vehicle as a particle and only consider the problem in a two-dimensional Cartesian coordinate system, the movement of the vehicle is\n\\begin{equation}\\begin{cases}x_{j+1} = x_j + L_{j+1} \\cos(\\theta_{j+1})\\\\y_{j+1} = y_j + L_{j+1} \\sin(\\theta_{j+1})\\end{cases},\\end{equation}\nwhere xj, yj is the coordinate position of the vehicle in time step j, Lj and \\theta_j respectively represent the movement distance and angle of the vehicle in the Cartesian coordinate system."}, {"title": "3 Gradient-Guided TD3 algorithm for Geomagnetic navigation", "content": "In this article, the angular displacement of the vehicle \\theta_j is defined as the yaw angle \\psi_j after previous time step. Accordingly, the motion equation of the vehicle is represented as\n\\begin{equation}\\begin{cases}x_{j+1} = x_j + L_{j+1} \\cos(\\theta_j + \\psi_{j+1})\\\\y_{j+1} = y_j + L_{j+1} \\sin(\\theta_j + \\psi_{j+1})\\end{cases}\\end{equation}\nFrom a reinforcement learning perspective, a strong and constrained relationship exists between geomagnetic parameters and the navigation path. This enables the agent to develop a \"magnetotactic\" capability similar to that seen in migrating animals. By leveraging the sensitivity of geomagnetic trends to the destination, the agent establishes a convergence with geomagnetic parameters throughout the navigation process.\nThe agent taking one or more of the geomagnetic parameters as shown in (1) at its current position and destination as perceptual information, which can be denoted by an i-dimensional vector\n\\begin{equation}B = \\{B_1, B_2, . . . , B_i\\}.\\end{equation}\nThe agent iteratively selects navigation actions including the yaw angle \\psi_j and the movement distance Lj to maximize rewards, guiding the vehicle to converge from the current position to the destination. When the geomagnetic parameters measured at the current position are equal to those at the destination, it signifies that the vehicle has reached the destination and completed the navigation process [22], that is\n\\begin{equation}\\lim_{k\\to\\infty} F(B, k) \\rightarrow 0,\\end{equation}\nwhere F(B, j) is the normalized objective function at the j-th time step, which can be expressed as\n\\begin{equation}F(B,j) = \\frac{f(B, j)}{\\sum_{i=1}^{n} f_i(B,j)} = \\frac{\\sum_{i=1}^{n} (B_i - B_i^*)^2}{\\sum_{i=1}^{n} (B_i - B_i^*)^2}.\\end{equation}\nB_i^j and B_i^* are the current value and the target value of the i-th geomagnetic parameter, respectively, f_i(B, j) represents the objective sub-function of the i-th geomagnetic parameter at the j-th time step.\nIn practice, the vehicle is considered to have successfully navigated to the destination when it is within a certain distance. This can be equivalent to the total objective function being less than the set threshold \\zeta\n\\begin{equation}F(B, j) < \\zeta.\\end{equation}\nTherefore, we regard the navigation of autonomous mobile vehicles in unknown environments without prior information as a reinforcement learning task. This task involves training the agent to select actions in the navigation space to maximize cumulative rewards and successfully complete the navigation task."}, {"title": "3.1 Solution of DRL-Based Geomagnetic Navigation", "content": "Deep reinforcement learning is a branch of machine learning that focuses on how artificial agents make decisions in an environment to maximize cumulative future rewards. In this section, we introduce the Markov decision process (MDP), which serves as the theoretical foundation of DRL, to describe our DRL-based solution. Subsequently, we introduce the selected DRL framework TD3 algorithm, along with the optimized Gradient-Guided TD3 algorithm proposed in this article to address the geomagnetic navigation task.\nDeep reinforcement learning is typically described using MDP. Specifically, for the geomagnetic navigation problem, the current position of the vehicle depends only on its previous position and navigation actions. Consequently, we interpret the geomagnetic navigation problem as an MDP, with the definitions of its state, action, and reward outlined below.\nThe MDP is typically characterized by a 4-tuple (S, A, P,R), where S represents the state space, comprising a set of environmental states, A is the action space, representing a set of available actions, P : S \u00d7 A \u00d7 S \u2192 [0, 1] is the dynamics transition function from the current state sj to the next state s'; under action aj, and R : S \u00d7 A \u2192 R is the reward function, providing immediate rewards following state transitions [35]. Details on the MDP for the geomagnetic navigation problem are as follows:\n1. State sj: The state is composed of information available at each time step j, which consists of real-time geomagnetic parameters measured by magnetic sensors, movement distance, and yaw angle in the previous time step. Considering that any three-dimensional parameters allow us to obtain all geomagnetic parameters. Therefore, in this article, we select only three geomagnetic parameters: the magnetic deviation D, the magnetic inclination I, and the horizontal component BH. The state sj is defined as\n\\begin{equation}s_j = \\{D_I^j, I_I^j, B_H^j, D_T^j, I_T^j, B_{H_T}^j, L_j, \\theta_j\\},\\end{equation}\nwhere \\{D_I^j, I_I^j, B_H^j\\} and \\{D_T^j, I_T^j, B_{H_T}^j\\} respectively denote the three-dimensional geomagnetic parameters measured at the current position and the destination, Lj and \\theta_j is the movement distance and yaw angle of the vehicle.\n2. Action aj: The agent takes action aj after obtaining the state sj at time step j. The action aj is defined as\n\\begin{equation}a_j = \\{L_{j+1}, \\psi_{j+1}\\},\\end{equation}\nIt is noteworthy that research in the field of geomagnetic navigation typically considers the yaw angle \\psi_j as the output of the navigation algorithm, while the movement distance Lj is usually pre-set to a fixed value. However, the navigation trajectory is influenced by"}, {"title": "3. Reward rj: We design a composite reward function, which is set as", "content": "the chosen value of the movement distance. Zhang et al. [36] underscore the complexities associated with determining the movement distance Lj in long distance navigation. When Lj is relatively small, substantial changes in gradient suggest a localized influence, making it challenging to represent the overall gradient of the entire area effectively, as Lj increases, the gradient tends to stabilize. Nevertheless, in geomagnetic navigation, due to the absence of location information, the navigation is deemed successful when the geomagnetic parameters at the current position fulfill the condition in (7). Setting an excessively large movement distance may result in the solution not converging within the convergence domain.\n3. Reward rj: We design a composite reward function, which is set as\n\\begin{equation}r_j = R_{\\text{destination}} + R_{\\text{proximity}} + R_{\\text{alignment}},\\end{equation}\nthe reward function consists of three parts, the destination reward R_{\\text{destination}} is set to encourage the agent to reach the destination and is represented by a positive value \\zeta_1. When the vehicle reaches the destination, the agent acquires the destination reward. The proximity reward R_{\\text{proximity}} is defined as\n\\begin{equation}R_{\\text{proximity}} = -\\zeta_2 (F(B_{j}, j) - F(B_{j-1}, j - 1)),\\end{equation}\nTo gauge the distance between the current position and the destination using geomagnetic parameters, we employ the objective function F(B_j, j) defined in (6). Rewards are given for actions that move toward the destination, and penalties are imposed for those that move away, based on the comparison of the objective function between the current and previous time steps. We implement a reward decay mechanism for the R_{\\text{proximity}}, when the current time step exceeds half of the maximum time step n_{\\text{max}}, an additional penalty term proportional to the R_{\\text{proximity}} and nj is applied. R_{\\text{proximity}} is set to encourage action that converges geomagnetic parameters from the current position toward the destination between adjacent time steps, prevent the agent from adopting an excessively conservative speed.\nThe alignment reward R_{\\text{alignment}} in this article is shown in (12):\n\\begin{equation}R_{\\text{alignment}} = \\zeta_3 (\\pi/4 - |\\delta_j - \\delta'_j|),\\end{equation}\nwhere \\delta_j represents the angular displacement of the vehicle, accumulated from the yaw angle \\theta_j at each time step, and \\delta'_j represents the theoretical heading angle calculated using the parallel approach based on magnetic gradients, with specific calculations to be introduced in Section 3.3. The R_{\\text{alignment}} is designed to constrain the heading deviation.\n\\zeta_1, \\zeta_2, and \\zeta_3 are three adjustable parameters, where \\zeta_1 should be set to the maximum. \\zeta_2 and \\zeta_3 are the weights for two subsidiary rewards. Notably, due to the non-uniform distribution of the magnetic field, a large value of \\zeta_3 may decrease the efficiency of geomagnetic navigation in areas with magnetic field anomalies. Therefore, setting the impact"}, {"title": "3.2 Basic Principle of the TD3 Algorithm", "content": "smooth and efficient navigation actions and also improves generalizability of the trained DRL agent.\n3. We evaluate the proposed Gradient-Guided TD3 method in simulations under long-distance navigation scenarios with diverse conditions. We conduct comprehensive assessments, and compare our approach with existing heuristic geomagnetic navigation algorithms to demonstrate the performance of the developed approach.\nThe remaining of this article is organized as follows. Section 2 describes the fundamentals for geomagnetic navigation. Section 3 details the proposed Gradient-Guided TD3 method. Section 4 carries out simulations to compare the performance of Gradient-Guided TD3 with heuristic algorithms, and analyzes the results in details and demonstrates the effectiveness of our approach. We conclude this work in Section 5.\nDRL operates on an iterative 'trial and error' learning process, where the agent learns to maximize rewards through interactions with the environment [37]. As illustrated in Fig. 2, the agent selects actions based on the current state, and the environment provides feedback in the form of new state and reward signals, transitioning to the next state. Through this iterative process, the agent converges on an optimal policy that maximizes cumulative rewards within the environment.\nIn DRL, the network is updated through temporal difference learning, utilizing a secondary frozen target network Qe(s, a) to preserve an objective y across multiple updates as\n\\begin{equation}y = r + \\gamma Q_{\\theta'} (s', a')\\end{equation}\n\\begin{equation}a' \\sim \\pi_{\\phi'}(s'),\\end{equation}\nwhere the actions are selected from a target actor network \\pi_{\\phi'}, \\gamma is the discount factor, s' and a' are the next state and action respectively. The weights of a target network are updated either periodically to precisely match the weights of the current network or by a given proportion \\tau at each time step, as shown by \\theta' \\leftarrow \\tau \\theta + (1 - \\tau)\\theta', \\phi' \\leftarrow \\tau \\phi + (1 - \\tau)\\phi'. The update can be implemented in an off-policy manner, involving the random sampling of mini-batches of transitions from an experience replay buffer [38].\nFor the overestimation problem of Q value, the TD3 algorithm is utilized to realize the navigation strategy of vehicles. TD3 belongs to actor-critic algorithms, it mitigates the overestimation problem of Q value by integrating the DDPG and DDQN. The architecture of TD3 is shown in Fig. 3, involving the copies of neural networks for one actor network and two critic networks as target networks. Two critic networks are employed to evaluate the Q value, and the smaller one is selected to update the target Q value as shown in (15). The loss function is defined as the squared difference between the selected target Q value and the output of the neural network, as expressed in (16):\n\\begin{equation}y = r + \\gamma \\min_{i=1,2} Q_{\\theta_i'} (s', a')\\end{equation}\n\\begin{equation}L(\\theta_i) = E[(Q_i(s, a) \u2013 y)^2].\\end{equation}\nIn addition, TD3 introduces the regularization of parameter updates to reduce the deviation generated by the estimation of the Q function. The noise \\varepsilon is added to the target action as a regularization in (17) to facilitate a smoother update process and mitigate the risk of overfitting.\n\\begin{equation}a'(s') = \\text{clip}(\\pi_{\\phi'} (s') + \\varepsilon, a_{\\text{Low}}, a_{\\text{High}}),\\end{equation}\nwhere \\varepsilon \\in \\text{clip}(\\mathcal{N}(0, \\sigma), -c, c), \\varepsilon represents the noise, \\sigma denotes the standard deviation of the noise, \\mathcal{N} refers to the standard normal distribution, and c defines the noise bound. The detailed pseudocode is provided in Algorithm 1."}, {"title": "3.3 Optimized Gradient-Guided TD3 Method", "content": "For long-distance geomagnetic navigation, the smoothness of the navigation trajectory is crucial as it reflects the efficiency of the navigation strategy. To efficiently guide the agent in navigating through an unfamiliar environment, we draw inspiration from the bionic navigation approach proposed by Wang et al. [42]. The gradient of geomagnetic parameters is introduced to calculate the theoretical heading angle approximately pointing toward the destination by parallel approach. Considering the overall stability of geomagnetic field distribution, encouraging the agent to choose navigation actions that align closely with the theoretical heading angle can enhance exploration efficiency in practice.\nAs depicted in Fig. 4, B^0 denotes the vector of geomagnetic parameters at the origin, BT denotes the vector at the destination, and the vector at the j-th step is denoted as B_j^T. The parallel approach allows different geomagnetic parameters to approach their respective target BT with the same ratio as follows:\n\\begin{equation}(B_{j+1}^i - B_j^i) \\times (B_j^i - B_i^T).\\end{equation}\nProjecting vectors (B_{j+1}^i - B_j^i) and (B_j^i - B_i^T) onto the geographical coordinate system, the"}, {"title": "4 Experiments", "content": "following results can be obtained:\n\\begin{equation}\\frac{B_{1,j+1}^i - B_{1,j}^i}{D_i^i} = \\frac{B_{2,j+1}^i - B_{2,j}^i}{D_i^i},\\end{equation}\nwhere geomagnetic parameters at two adjacent time steps satisfy the relationship as\n\\begin{equation}\\begin{cases}B_{1,j+1}^i = B_{1,j}^i + g_{i_{1,x}}^i \\cos \\theta_k + g_{i_{1,y}}^i \\cdot \\sin \\theta_k\\\\B_{2,j+1}^i = B_{2,j}^i + g_{i_{2,x}}^i \\cos \\theta_k + g_{i_{2,y}}^i \\sin \\theta_k\\end{cases}\\end{equation}\nwhere g_{1,i,x}^i, g_{1,i,y}^i, g_{2,i,x}^i, g_{2,i,y}^i are the gradients of B_j^i and B_j^i. By substituting (20) into (19), the theoretical heading angle can be calculated as\n\\begin{equation}\\chi'_j = \\text{arctan}(\\frac{(B_{1,j}^i-B_i^T) g_{i_{2,x}}^i - (B_{2,j}^i-B_i^T) g_{i_{1,x}}^i}{(B_{1,j}^i-B_i^T) g_{i_{1,y}}^i + (B_{2,j}^i-B_i^T) g_{i_{2,y}}^i}).\\end{equation}\nThe theoretical heading angle \\chi'_j can be estimated from previous calculations. However, while the geomagnetic field distribution is generally stable, magnetic anomalies can cause inaccuracies in gradient calculations. This makes it problematic to directly use the theoretical heading angle in navigation tasks. As a more effective strategy, we set a low-weight alignment reward to encourage the yaw angle to closely match the theoretical heading angle.\nIn this section, we outline the experimental setup, detailing the simulation environment, evaluation metrics, and implementation procedures. We then conduct an in-depth analysis of the algorithm's performance in geomagnetic navigation, using visual comparisons of long-distance navigation trajectories to emphasize effectiveness of algorithms. Finally, we present quantitative results to offer a thorough understanding of the performance metrics."}, {"title": "4.1 Experimental Setup", "content": "In the remaining sections, we present a breakdown of the experimental setup, provide an analysis of the training efficiency, discuss the navigation trajectory, and then show qualitative results of the navigation in unknown environments. In detail, the experimental setup introduces how the simulation environment generates the geomagnetic information, lists baselines for comparisons, sets evaluation metrics for the quantitative analysis and also specifies details on the implementation.\nFor long-distance geomagnetic navigation, the smoothness of the navigation trajectory is crucial as it reflects the efficiency of the navigation strategy. To efficiently guide the agent in navigating through an unfamiliar environment, we draw inspiration from the bionic navigation approach proposed by Wang et al. [42]. The gradient of geomagnetic parameters is introduced to calculate the theoretical heading angle approximately pointing toward the destination by parallel approach. Considering the overall stability of geomagnetic field distribution, encouraging the agent to choose navigation actions that align closely with the theoretical heading angle can enhance exploration efficiency in practice.\nAs depicted in Fig. 4, B^0 denotes the vector of geomagnetic parameters at the origin, BT denotes the vector at the destination, and the vector at the j-th step is denoted as B_j^T. The parallel approach allows different geomagnetic parameters to approach their respective target BT with the same ratio as follows:\n\\begin{equation}(B_{j+1}^i - B_j^i) \\times (B_j^i - B_i^T).\\end{equation}\nProjecting vectors (B_{j+1}^i - B_j^i) and (B_j^i - B_i^T) onto the geographical coordinate system, the"}, {"title": "4.1.1 Simulation Environment", "content": "Geomagnetic parameters were sourced from the IGRF model. The region selected spans from (10\u00b0S, 160\u00b0E) to (0\u00b0N,170\u00b0E), encompassing an area of ocean north of Australia, as shown in Fig. 5, the yellow, blue dashed, and green lines represent the contours of D, I, and BH, respectively. The red square marks the starting position, while the red triangle indicates the destination.\nIn Fig. 5, we randomly simulated a navigation task with the origin at (2\u00b0S, 162\u00b0E) near Nauru, where the geomagnetic parameters are D = 8.019\u00b0, I = \u221216.150\u00b0, and BH = 35467.990nT. The destination is set at (8\u00b0S, 164\u00b0E) near the Solomon Islands, with geomagnetic parameters of D = 9.228\u00b0, I = \u221226.923\u00b0, and BH = 35199.415nT. The vehicle starts at the origin, using the geomagnetic parameters of the current position and destination as perceptual information. It navigates based on the selected heading angle and movement distance determined by the navigation algorithms."}, {"title": "4.1.2 Baselines", "content": "To comprehensively evaluate the overall performance of the proposed Gradient-Guided TD3 algorithm in unknown environments, we selected four popular methods as baselines: TD3 [41], genetic algorithm (GA) [43], particle swarm algorithm (PSO) [44], and artificial fish swarm algo-"}, {"title": "4.1.4 Implementation Details", "content": "rithm (AFSA) [45]. These baselines highlight two distinct strategies for geomagnetic navigation without prior magnetic maps: TD3 serves as the foundation for the proposed Gradient-Guided TD3 and represents reinforcement learning-based methods, while GA, PSO, and AFSA exemplify metaheuristic approaches commonly employed in bionic geomagnetic navigation.\nTo select the movement distance Lj and yaw angle \\psi_j for navigation based on the predicted actions a'_j at each time step, constraints are applied: the movement distance is limited to a range of 0 to 50 km, and the yaw angle is restricted to -\\pi/2 to \\pi/2. Both the action vector aj and state vector sj undergo min-max normalization. The hyperparameters for the proposed"}, {"title": "4.2 Analysis of Training Efficiency", "content": "4.1.3 Evaluation Metrics\nOur experiments adopted a set of evaluation metrics to assess algorithm performance. Initially, we report four key metrics\u2014success rate (SR) [46], trajectory length (TL) [47], success weighted by path length (SPL) [48], and total navigation time (TNT) [27] \u2014to provide an overall evaluation of each algorithm's success rate and navigation efficiency. SR measures the ratio of successful episodes, TL calculates the average length of the trajectories, SPL assesses the efficiency relative to the shortest path, and TNT indicates the average time taken to complete the navigation, with lower values reflecting greater efficiency.\nFollowing this, we further analyzed the algorithms using four additional metrics-path smoothness [49], mean absolute error (MAE) of heading deviation [50], root mean square error (RMSE) of heading deviation [50], and navigation error (NE) [47]. path smoothness evaluates the continuity of the agent's path, MAE and RMSE capture the accuracy and variance in heading angles, and NE measures the final distance between the agent and the target. These metrics were statistically analyzed and presented using box plots to offer deeper insights into task-specific performance.\nGradient-Guided TD3 algorithm are outlined in Table. 1. For a fair comparison, 100 navigation tasks were randomly generated within the selected region, with straight-line distances ranging from 300 to 500 km. Each baseline model then sequentially executed all assigned navigation tasks.\nFig. 6 presents the cumulative reward curves for the two DRL-based models following 20,000 training iterations in the simulation environment. The gray and light blue lines show the cumulative rewards for each episode. Given the typical fluctuations in episode rewards during reinforcement learning, especially in complex tasks with dense reward structures, it can be challenging to accurately gauge the performance of agent. To address this, we utilize a sliding window technique to smooth the episode rewards. The red and blue lines depict the average rewards, \\bar{r_i}, calculated as \\bar{r_i} = \\frac{1}{N} \\sum_{j=i-N+1}^{i} r_j, where N is the sliding window size (set to 200) and rj represents the reward for the j-th episode during training.\nAs the DRL-based model continues to train with replay buffer samples, it progressively improves its ability to navigate toward the destination using geomagnetic information, even with random origin and destination points. This improvement is reflected in the steady increase in average rewards as the number of iterations grows. The proposed Gradient-Guided TD3 model exhibits more pronounced fluctuations in the reward curve during the initial training phase due to the added alignment reward R_{\\text{alignment}}. However, with sufficient training, the Gradient-Guided TD3 model gradually stabilizes.\nMoreover, Fig. 7 presents the average success rate of the DRL models across 20,000 training iterations. It shows the success rate based on the 200 most recent training tasks for each"}, {"title": "4.3 Analysis of Navigation Trajectory", "content": "4.4 Qualitative Results of Navigation in Unknown Environment\niteration. Both models experience an increase in success rate around the 2,900-th iteration. However, the Gradient-Guided TD3 developed in this article demonstrates a more rapid increase in success rate and exhibits less fluctuation. This suggests that the Gradient-Guided TD3 is more effective at learning robust navigation strategies, achieving stability more quickly and consistently compared to the TD3 model.\nThe navigation trajectories generated by different algorithms for a typical navigation task are shown in Fig. 8, the trajectories produced by both DRL algorithms are noticeably smoother overall compared to those generated by the metaheuristic algorithms. This disparity arises because metaheuristic algorithms rely heavily on random search strategies, leading to less predictable and more erratic trajectories. On the other hand, DRL algorithms benefit from a training phase that builds a strong association between geomagnetic states and navigation actions, resulting in more stable and coherent trajectories.\nMoreover, the enhanced Gradient-Guided TD3 algorithm shows superior linearity in its trajectories compared to the standard TD3. This improvement highlights the advantage of using real-time geomagnetic gradient information to calculate theoretical heading angles, which significantly boosts the navigation algorithm's efficiency. By incorporating dynamic geomagnetic features, the Gradient-Guided TD3 algorithm adapts more effectively to environmental changes, producing trajectories with improved straight-line characteristics.\nThe convergence curves of the three geomagnetic parameters are compared between the proposed Gradient-Guided TD3 and baseline methods in Fig. 9, These curves demonstrate how the geomagnetic parameters gradually converge from their initial values towards the target values over time. The results indicate that PSO and the proposed Gradient-Guided TD3 exhibit the fastest convergence, reflecting the highest navigation efficiency. TD3 and GA follow in terms of convergence, while AFSA shows the slowest convergence. The variations in geomagnetic parameters D and I between the origin and destination are relatively uniform, whereas BH presents non-uniform changes. This non-uniformity introduces deviations in the theoretical heading angles calculated based on the geomagnetic gradient. Further analysis in Fig. 9(c) reveals that during navigation, the vehicle, influenced by the non-uniform geomagnetic field strength, does"}, {"title": "5 Conclusion", "content": "not mechanically follow the gradient descent direction calculated using the parallel approach. Instead, it intelligently identifies a trajectory that is closer to the destination. This highlights the ability of Gradient-Guided TD3 to effectively integrate TD3 with the parallel approach, showcasing its adaptability in navigating through non-uniform geomagnetic field distributions.\nFig. 9(d) shows the relationship between movement distance and time step across different methods. Both DRL models generally opt for larger movement distances when far from the destination, shifting to smaller distances as they approach the destination. This behavior is characteristic of geomagnetic navigation, which depends on meeting the threshold condition in (7). This threshold indicates sufficient proximity to the target without needing additional localization data. Choosing smaller movement distances near the destination helps guide the vehicle into the solution space defined by (7), reducing the risk of overshooting. This adaptability demonstrates the strength of DRL models in adjusting navigation strategies based on proximity, addressing uncertainties in geomagnetic navigation, and proving their effectiveness in real-world scenarios.\nTo evaluate the accuracy and generalizability of the proposed algorithm in navigating unknown environments, 100 random sets of origins and destinations were independently generated within the selected simulation region. The performance comparison of the different methods across various evaluation metrics is summarized in Table. 2.\nExamining the RMSE and MAE of heading deviation in Fig. 10(b) and Fig. 1"}]}