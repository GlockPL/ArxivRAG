{"title": "The Roles of English in Evaluating Multilingual Language Models", "authors": ["Wessel Poelman", "Miryam de Lhoneux"], "abstract": "Multilingual natural language processing is getting increased attention, with numerous models, benchmarks, and methods being released for many languages. English is often used in multilingual evaluation to prompt language models (LMs), mainly to overcome the lack of instruction tuning data in other languages. In this position paper, we lay out two roles of English in multilingual LM evaluations: as an interface and as a natural language. We argue that these roles have different goals: task performance versus language understanding. This discrepancy is highlighted with examples from datasets and evaluation setups. Numerous works explicitly use English as an interface to boost task performance. We recommend to move away from this imprecise method and instead focus on furthering language understanding.", "sections": [{"title": "Introduction", "content": "With the increase of in-context, prompt-based evaluation of auto-regressive languages models (LMs, Brown et al., 2020), choices have to be made on how prompts are created. Specifically in multilingual evaluation, a crucial choice is in which language(s) prompts are written. In practice, English tends to be mixed with a target language with the explicit goal of increasing task performance. We argue this goal is different from furthering language understanding. In this position paper, we outline two roles of English at the core of this discrepancy and their implications.\nSeveral works have highlighted methodological issues in multilingual evaluation setups (Artetxe et al., 2020; Ploeger et al., 2024). The dominance of English in natural language processing (NLP) has also been discussed repeatedly (Joshi et al., 2020; Ruder et al., 2022). With the increase of prompt-based evaluations of models, a new issue has appeared: English being used as an interface, rather than a natural language.\nIn recent work, Zhang et al. (2023) propose a taxonomy of prompt-based multilingual LM evaluations. They conclude that \u201c[the model] achieves higher performance when the task is presented in English.\" This finding is consistent among a large number of papers (Shi et al., 2022; Huang et al., 2022; Fu et al., 2022; Lin et al., 2022; Asai et al., 2024; Etxaniz et al., 2024, inter alia). Resorting to using English like this is hardly surprising given that instruction tuning datasets are expensive to create and not readily available for most languages. Less surprising still is the finding that English performs well, as it is included in virtually all LMs. It does bring into question: what is being evaluated and what do we learn from this?\nTo illustrate: MaLa-500 (Lin et al., 2024) is a Llama 2-based model (Touvron et al., 2023) that underwent continued pre-training in over 500 languages. It is partially evaluated on a news topic classification task using SIB-200 (Adelani et al., 2024a), a dataset of (sentence, topic) pairs in 205 languages. The model is prompted as follows:\nThe topic of the news {sentence} is {topic}\nUsing the prompt with a Turkish\u00b9 example gives:\nThe topic of the news Bu oteller g\u00fcn\u00fcn zenginlerinin ve \u00fcnl\u00fclerinin kalaca\u011f\u0131 yerlerdi ve \u00e7o\u011fu zaman kaliteli yemeklere ve gece hayat\u0131na sahipti. is entertainment\nThis format is used across all 205 languages in few-shot setups from one to ten. This mixture of English and a target language is, arguably, not very 'natural'. We refer to this role of English as an interface, rather than a natural language. In the next sections, we outline these roles and why they are important to consider in multilingual evaluation."}, {"title": "Evaluation Goals", "content": "Language understanding. We take the common perspective that evaluation concerns a task which is used as a proxy for understanding. This is exemplified by the natural language understanding (NLU) label many datasets and models adhere to (including SIB-200). A news topic classification task shows that the model (arguably) 'understands' some of the differences between news categories. A model that rewrites, translates or summarizes 'understands' both task instructions and target passages. In a multilingual setting, the understanding of interest is generalizability across languages; a model performing a task in a target language supposedly understands something about that language. This is then applied to multiple languages. We refer to this as 'multilingual natural language understanding' (MLU). Specifically, we use MLU to mean \u2018understanding a target language is part of multilingual natural language understanding.'\nUnderstanding English by itself and understanding a natural mix of English and another language are both part of MLU. The latter enters the domain of code-switching: the phenomenon where a speaker fluently switches between multiple different languages during the same conversational turn (Milroy and Muysken, 1995).\nThe MaLa-500 prompt mixes English and a target language. However, it is hard to classify this as code-switching, as the switch is hardly natural, especially in a few-shot setup. Rather than a natural language that tells something about language understanding, English is used as an interface to the LM with the goal of increasing task performance. We refer to this mixing as a mixed-prompt.\nTask performance. Another widespread perspective on evaluation in (multilingual) NLP considers performance on a task as an end in itself. If we want to classify news topics in a practical application operating in a multilingual setting, what a model supposedly understands or how well it models a particular language is of little value. What matters is the system performing its task adequately across languages. Without using English, the system might not even work at all. This is a common justification; mixing in English is arguably better than not having a system at all.\nWhile practical, this perspective is seemingly at odds with the many tasks and datasets that present themselves under the aforementioned label of language understanding. Additionally, task performance as the sole goal introduces a usability issue. Auto-regressive LMs are increasingly meant to be directly interacted with (a natural language interface). If we have to resort to a mixed-prompt for the system to even function, it means the user has to be able to write English and get familiar with this unnatural mixing of languages.\nFigure 1 summarizes our argument and terminology. Next, we provide more details regarding the discrepancies between using English as an interface versus using it as a natural language."}, {"title": "Evaluation Methods", "content": "As mentioned in \u00a71, a large body of contemporary research in multilingual NLP focuses on prompting methods. Common evaluation setups range from (i) prompts fully in a target language, to (ii) English instructions with task-specific passages in the target language, to (iii) translating all text into English before presenting it to a model. None of these works refer to this mixture as being code-switched text. All conclude that a mixture of English and a target language (a mixed-prompt) generally results in the best task performance. In this section we show why a mixed-prompt is an inherently imprecise method to use in evaluation, even if maximizing task performance is the goal.\nIf we use a prompt fully in a target language, we are clearly evaluating part of MLU. A mixed-prompt introduces additional factors that are evaluated that are neither the task nor MLU. We illustrate this from two angles: the representation of the prompt and fortuitous issues from unnaturally mixing English and a target language.\nConsider how to evaluate a multilingual masked language model on the news classification task. A classification layer is added to a pre-trained model to predict the topic labels; it sees label indices that are consistent across languages. The labels are language-agnostic for the model (i.e., detached from natural language). The evaluation method and goal are clear: mapping a target language sequence to one of these indices. There are no additional signals influencing this process.\nIn a prompting setup, the representation of the labels can either be language-agnostic (numbers, letters, symbols, etc.), or not (English words, target language words, etc.). These options result in any number of tokens, which will have different representations within the model, unless specifically accounted for. In many multilingual evaluation prompts, the classification labels are English words (such as in the MaLa-500 example). Without target language words or (to an extent) language-agnostic labels, the evaluation method and goal will be inherently imprecise.\nIn addition to the different representation, more than just the task is evaluated with a mixed-prompt setup. To illustrate this, consider the following setup from the AfriMMLU subtask of IrokoBench (Adelani et al., 2024b):\nYou are a highly knowledgeable and intelligentartificial intelligence model answers multiple-choicequestions about {subject}Question: {question}Choices:A: {choice1}B: {choice2}C: {choice3}D: {choice4}Answer:\nThe prompt and subject are always in English, the question and choices in the target language. With this setup, more is tested than just a task in a target language:\n\u2022 Code-switching, if this is considered natural, or unnatural 'mixed-prompt' switching.\n\u2022 Script-switching, if the target language uses a non-Latin script (which applies to Amharic in IrokoBench, using the Ge'ez script).\n\u2022 Instruction following in English.\n\u2022 Grammatical error correction in English.\n\u2022 Answering high-school level exam questions in the target language.\nWith these mixed-prompts, we arguably do not test MLU, as that would entail a native target language prompt. At the same time, we test more than just the task, even though that is the explicit goal of using English in this way.\nWhile we only discussed classification tasks until now, our argument also applies to other types of tasks. Consider the following zero-shot machine translation prompt from Hendy et al. (2023):\nTranslate this sentence from {source} to {target}Source: {source_sentence}Target:\nThe prompt is always in English, the source and target are English words referring to the languages, and the source sentence is in the target language. Filled in, it looks like this:\n# DE \u2192 NLTranslate this sentence from German to DutchSource: Du gehst mir auf den KeksTarget:#NL \u2192 DETranslate this sentence from Dutch to GermanSource: tijd voor een bakje koffieTarget:"}, {"title": "Why does this matter?", "content": "Interacting with computers in a natural manner is arguably the ultimate goal of numerous sub-fields of computer science. Work on natural language interfaces to information systems dates back decades (Winograd, 1972; Waltz, 1978). LMs bring us ever closer to this goal. However, in a multilingual setting, it is important to consider what natural language is, what is being evaluated, and what promises are sold. Next, we outline the implications of the interface versus natural language roles on evaluation practices.\nInterface. Let us start with the role in which English is akin to a programming language. We need an interface to communicate with a system, in a way the system can understand. We have seen that mixed-prompts are used to get the system to perform better on a given task. Given the scarcity of instruction tuning datasets and the costs involved in creating these, it is understandable that this is a common (albeit sometimes implicit) perspective. English becomes the 'programming' language that glues target language passages together and makes the system perform a task. Programming languages also predominantly use English labels for their keywords. However, if the keyword for a while loop happens to be mientras or kjsdfk is irrelevant for its function. These are natural language-agnostic as the meaning (as interpreted by a compiler or interpreter) does not change. Variable names and keywords can be chosen arbitrarily. This is not the case with prompting, which is sensitive to slight changes, both in English (Sclar et al., 2023) and multilingual setups (Zhang et al., 2023; Asai et al., 2024).\nAdditionally, evaluation setups that use English as an interface introduce knowledge leakage from English to the target language. This is, again, with the explicit goal of improving task performance. Being able to understand English instructions is not the same as being able to understand target language instructions. If English truly was a programming language, this would not matter, as the meaning of the instructions would be separate from the meaning of the target language passages. Given that English is a natural language, this de facto means more is evaluated than just the task. Consequently, such evaluations are imprecise at best, as shown in \u00a73.\nPrompt-based evaluations should extend MLU to the instruction domain. A mixed-prompt setup claiming to test \u201cmultilingual understanding\u201d might more accurately be described as \"understanding English instructions interleaved with passages from target language(s), albeit not in a natural code-switching setup.\"\nNatural language. When we consider the other role of English in multilingual prompt-based evaluation, we should treat it the same as any other language. The 'Multilingual Exemplars' setup from Shi et al. (2022) is a creative interpretation of this perspective. In this few-shot setup, the model sees various examples, all in different languages. The final question is asked in the target language. A setup like this extends the definition of 'multilingual language understanding' to the extreme. It becomes harder to interpret what a multilingual model knows about any individual language in this context, but English is certainly not an interface, it is a natural language like all others.\nA less extreme setup would simply use native, target language prompts or natural code-switched prompts. This is costly, but it aligns much better with the goal of multilingual natural language understanding. Indeed, several works specifically explore this direction (K\u00f6pf et al., 2023; Singh et al., 2024). This approach clearly tests multilingual language understanding, including the instruction domain. If performance on a particular task in a particular language is lagging behind, or not working at all, it means focus should be put on addressing the core of these issues (e.g., data or modeling). Ideally, we should not resort to imprecise methods to boost task performance."}, {"title": "Conclusion", "content": "In this position paper we outline two roles of English in multilingual language model evaluation: as an interface, with the goal of task performance, and as a natural language, with the goal of language understanding. We (i) list works that incorporate English with the explicit goal of boosting task performance, even in tasks such as translation where it is neither the source nor target, underlining the interface role, (ii) show that mixing English with a target language in a mixed-prompt is unnatural (i.e., not code-switching), and (iii) outline why the interface role is an imprecise choice when evaluating multilingual language understanding of language models.\nAdditionally, we argue that using a mixed-prompt tests more than just performance on a certain task. Because English is a natural language and not a programming language, using it in a mixed prompt will inherently lead to fortuitous factors such as (un)natural switching between languages or scripts, grammatical error correction, and more. This all results in imprecise or misleading evaluations, even if the ultimate goal was to evaluate and improve task performance.\nWe finally contrast the implications of the two roles on evaluation practices. We recommend to move away from using English as an interface in multilingual evaluations and ultimately advocate for the goal of language understanding."}]}