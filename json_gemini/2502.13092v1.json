{"title": "TEXT2WORLD: Benchmarking Large Language Models for Symbolic World Model Generation", "authors": ["Mengkang Hu", "Tianxing Chen", "Yude Zou", "Yuheng Lei", "Qiguang Chen", "Ming Li", "Yao Mu", "Hongyuan Zhang", "Wenqi Shao", "Ping Luo"], "abstract": "Recently, there has been growing interest in leveraging large language models (LLMs) to generate symbolic world models from textual descriptions. Although LLMs have been extensively explored in the context of world modeling, prior studies encountered several challenges, including evaluation randomness, dependence on indirect metrics, and a limited domain scope. To address these limitations, we introduce a novel benchmark, TEXT2WORLD, based on planning domain definition language (PDDL), featuring hundreds of diverse domains and employing multi-criteria, execution-based metrics for a more robust evaluation. We benchmark current LLMs using TEXT2WORLD and find that reasoning models trained with large-scale reinforcement learning outperform others. However, even the best-performing model still demonstrates limited capabilities in world modeling. Building on these insights, we examine several promising strategies to enhance the world modeling capabilities of LLMs, including test-time scaling, agent training, and more. We hope that TEXT2WORLD can serve as a crucial resource, laying the groundwork for future research in leveraging LLMs as world models.", "sections": [{"title": "1 Introduction", "content": "The significance of world models for intelligent behavior has been historically acknowledged in early psychological theories, which posited that organisms employ internal representations of the external world for prediction and planning (Craik, 1967). Furthermore, LeCun (2022) extends this concept by highlighting world modeling as a core component of autonomous machine intelligence. In this paper, we primarily study symbolic world models (also known as domain models), which are formal representations of an environment's dynamics and constraints. In recent years, Large Language Models (LLMs) (OpenAI, 2022; Yang et al., 2024; Meta AI, 2024) have showcased their understanding of common-world knowledge, making them promising candidates for generating symbolic world models, which requires inferring action dynamics and constraints from solely natural language description. Some works have already explored this across numerous tasks, including planning (Hu et al., 2024b; Guan et al., 2023), game design (Wang et al., 2023a, 2024), reinforcement learning (Tang et al., 2024) among others.\nDespite extensive exploration, previous work for evaluating symbolic world model generation suffers from several key limitations: (i) Limited Domain Scope: These studies are often confined to a narrow set of domains (typically fewer than 20), which limits the generalizability and applicability of their findings (Oswald et al., 2024; Silver et al., 2024; Wong et al., 2023). (ii) Evaluation Randomness: Some works rely on LLM-based evaluation methods, which may introduce additional margins of error (Wang et al., 2023a). Preliminary experiments in Section 3.6 demonstrate that the LLM-based evaluation exhibits a low inter-annotator agreement with human annotators (Cohen's \u043a = 0.10). (iii) Indirect Evaluation: Some studies evaluate world models based on end-to-end success rates in model-based planning, making it difficult to identify specific failure modes (Guan et al., 2023; Dainese et al., 2024).\nMotivated by these issues, this paper introduces a novel benchmark TEXT2WORLD based on the Planning Domain Definition Language (PDDL) as illustrated in Figure 1. Specifically, to address the first issue, we initially gathered a broad set of domains, which were then filtered through an automated pipeline and manually curated to ensure their quality, ultimately resulting in a collection of hundreds of diverse domains. Furthermore, to tackle the second issue, we designed multi-criteria, execution-based metrics to ensure a more robust assessment."}, {"title": "2 Preliminary", "content": "We formally define a symbolic world model as\nD = (F, A), where F represents the set of fluents\n(state variables represented as predicates) and A\nis the set of possible actions. Each fluent f \u2208 F\nis a predicate of the form p(x1, ..., xn), where p is\nthe predicate name and x1, ..., In are typed vari-\nables. Each action a \u2208 A is defined as a tuple\n\u03b1 = (\u03b1, \u03a1,\u03c6,\u03b5) where: i) a denotes the action"}, {"title": "2.2 Task Definition", "content": "The task is formally defined as: M : N \u2192\nD, D = N, where M is a mapping function (im-\nplemented by an LLM) that generates world model\nD from the natural language description N. \u3151\ndenotes semantic satisfaction. Each N contains\nthe following components: i) A general descrip-\ntion describing the overall objective of the domain;\nii) A set of predicates NF = {f1,..., fn} where\neach predicate is described with its signature (e.g.,\n\u201c(conn ?x ?y)", "Indicates\na connection between two places ?x and ?y\"); iii)\nA set of actions Na = {a1, ..., am} where each\naction is described with: its signature (e.g., \u201cmove\n<?curpos> <?nextpos>\") and an explanation (e.g.,\n\u201cAllows the robot to move from place <?curpos> to\nplace <?nextpos>\"). Note that to evaluate LLMs'\ninherent world modeling capabilities, action de-\nscriptions in NA are intentionally kept at a high\nlevel, without explicit specifications of precondi-\ntions and effects E. This design choice allows\nus to assess how well LLMs can infer the under-\nlying world dynamics and constraints from purely\ndescriptive text. A comparative analysis of model\nperformance conditioned on different description\nstyles is presented in Section 6.5.\"\n    },\n    {\n      \"title\"": "2.3 Evaluation Metrics"}, {"content": "We directly evaluate generated world models, ad-\ndressing the ambiguity associated with indirect\nevaluations (Guan et al., 2023; Dainese et al., 2024).\nIn addition, we propose using execution-based met-\nrics, overcoming the randomness of LLM-based\nevaluation (Wang et al., 2023a). Specifically, we\nestablished the following evaluation metrics: (i)\nExecutability (EXEC.): Measures whether the gen-\nerated PDDL can be successfully parsed and vali-\ndated by standard PDDL validators. (ii) Structural\nSimilarity (SIM.): Quantifies the textual similarity\nbetween the generated and ground truth PDDL us-\ning normalized Levenshtein ratio. (iii) Component-\nwise F1 Scores: When generated PDDL achieves\nexecutability (EXEC. = 1), we perform fine-grained\nanalysis by calculating the macro-averaged F1\nscore for each component type (predicates, actions,"}, {"title": "3 Benchmark Construction", "content": "The overall process of benchmark construction is\nshown in Figure 2. In this section, we provide a\ndetailed explanation of each stage."}, {"title": "3.1 Data Acquisition", "content": "Our benchmark construction process began with\ncollecting PDDL files from various public repos-\nitories and planning competitions. Through this\ninitial collection phase, we accumulated 1,801 raw\nPDDL files. We performed several preprocessing\nsteps to standardize the data format (e.g., convert\nfiles with BOM encoding to standard UTF-8). The\nprocessed files served as the foundation for our\ndataset construction."}, {"title": "3.2 Data Filtering and Manual Selection", "content": "To ensure the quality and reliability of\nTEXT2WORLD, we implemented a compre-\nhensive filtering pipeline: (i) Validation: We\nemployed a PDDL domain parser to perform\nsyntax validation on each file; (ii) Similarity\nDeduplication: We eliminated duplicate entries by\ncomputing pairwise cosine similarity on TF-IDF\nvectorized PDDL content, removing files with\nsimilarity scores exceeding 0.9; (iii) Complexity"}, {"title": "3.3 Data Annotation", "content": "After obtaining the high-quality PDDL domains,\nwe manually annotated natural language descrip-\ntions for each domain. To ensure the quality of\nannotations, we recruited 6 computer science grad-\nuates as annotators. The annotated description\nfollowed the structured format described in Sec-\ntion 2.2, and annotators were required to follow the\nannotation criteria: (i) Descriptive Completeness:\nAnnotations must contain all required components;\n(ii) Action Abstraction: Action descriptions should\navoid explicit references to formal preconditions\nand effects; (iii) Inference-Enabling: Descriptions\nshould contain sufficient contextual information to\nallow models to infer the underlying dynamics; (iv)\nNatural Language Priority: Technical terminol-\nogy should be minimized in favor of natural lan-\nguage explanations. Examples of TEXT2WORLD"}, {"title": "3.4 Quality Assurance", "content": "Manual Recheck To maintain rigorous quality\nstandards throughout the annotation process, we es-\ntablished a review system supervised by two senior\nexperts. These experts conducted regular inspec-\ntions of the annotations, ensuring accuracy and\nconsistency. Inspectors must verify all data twice\nto determine if the annotated examples meet the\nspecified annotation standards. Examples are ac-\ncepted only if both inspectors approve them. The\nverification results showed \"almost perfect agree-\nment\" with a Fleiss Kappa (Landis and Koch, 1977)\nscore of 0.82. Through this comprehensive quality\ncontrol process, we compiled a final curated dataset\nof 103 domains with gold-standard descriptions.\nData Contamination As shown by Carlini et al.\n(2021), LLMs can memorize training data rather\nthan truly model the world. To assess potential\ncontamination between LLMs' training data and\nTEXT2WORLD, we generated complete PDDL do-\nmains from the first 20 tokens using GPT-4 (Ope-\nnAI, 2023) and calculated contamination rates\nbased on tokenized 10-grams with up to 4 mis-\nmatches (Touvron et al., 2023), excluding PDDL-\nspecific keywords and variables. We also com-\npared these results with previous studies (Guan\net al., 2023; Smirnov et al., 2024). Figure 3 shows\nthat TEXT2WORLD has a lower contamination rate\n(\u03bc = 0.04 vs. \u03bc = 0.47), suggesting its perfor-\nmance reflects domain understanding rather than\nmemorization. However, the complete elimina-"}, {"title": "3.5 Data Analysis", "content": "This section provides some detailed data analysis\nto better understand TEXT2WORLD.\nCore Statistics We designated 2 domains as in-\ncontext exemplars (train set), with the remaining\n101 samples forming our test set.\nSemantic Analysis We use LLMs to extract high-\nlevel domain characteristics to better understand\nthe conceptual distribution of TEXT2WORLD, As\nshown in Figure 4 (Bottom), common themes such\nas path planning, constraint satisfaction, and task\nallocation, among others, emerge.\nRequirements Analysis A PDDL requirement\nspecifies a formal capability needed to express a do-\nmain, often reflecting its complexity. For instance,\n: typing stands for allowing the usage of typing for\nobjects. As shown in Figure 4 (Top), there are eight\ndifferent requirement type in TEXT2WORLD. We"}, {"title": "3.6 Preliminary Experiment", "content": "In previous works, LLMs have been employed\nto evaluate the action dynamics of world mod-\nels generated by LLMs themselves (Wang et al.,\n2023a). To further assess the ability of LLMs\nto detect errors in world models, we conducted\na preliminary experiment where we first used\nclaude-3.5-sonnect for TEXT2WORLD. Sub-\nsequently, human annotators and the LLM inde-\npendently evaluated the generated action dynamics\nto identify potential errors. The inter-annotator\nagreement between human ratings and LLM rat-\nings, measured using Cohen's k, was 0.10, indicat-\ning a low level of agreement. This suggests that\npredicting the correctness of PDDL domains using\nan LLM is particularly challenging, highlighting\nthe need for more discriminative evaluation metrics."}, {"title": "4 Experiments", "content": "We evaluate several state-of-the-art LLMs, in-\ncluding GPT-4 (OpenAI, 2023), GPT-3.5 (Ope-\nnAI, 2022), Claude-3.5 (Anthropic), and LLaMA-\n3.1 (Meta AI), DeepSeek-v3 (Liu et al., 2024),"}, {"title": "4.2 Experimental Results", "content": "Several conclusions can be drawn from Table 1:\n(i) The most advanced LLMs still struggle with\nTEXT2WORLD. For example, the best-performing\nmodel, DeepSeek-R1, achieves F1 scores below\n60% for both preconditions (F1PRECOND) and effects\n(F1EFF) under the without error correction setting.\nThis highlights the limitations of current LLMs in\nworld modeling tasks. (ii) Large reasoning models\ntrained with reinforcement learning exhibit supe-\nrior world modeling capabilities. These models,\nsuch as DeepSeek-R1 (DeepSeek-AI et al., 2025),\noutperform others in executability, structural simi-\nlarity, and component-wise performance, indicat-\ning that RL-based training enhances the ability of\nmodels to generate structured and valid world mod-\nels. (iii) The ability of models to benefit from\nerror correction is evident. For instance, GPT-4\n(gpt-4o-mini) demonstrates a notable improve-\nment in executability, increasing from 48.5% to\n72.3% after three correction attempts."}, {"title": "5 Analysis", "content": "We conducted a one-way ANOVA (Girden, 1992)\nto evaluate the impact of correction attempts on\nmodel performance, excluding anomalous zero val-\nues. The results showed a significant improvement\nwith three correction attempts (F = 27.48,p ="}, {"title": "6 Exploration", "content": "In addition to the zero-shot CoT evaluation in\nSection 4.2, we further evaluate the models on\nTEXT2WORLD with five different strategies: (1)\nTest-time Scaling; (2) In-Context Learning; (3)\nFine-tuning; (4) Agent Training; (5) Inference with\nConcrete Description."}, {"title": "6.1 Test-time Scaling", "content": "Recently, test-time scaling has demonstrated re-\nmarkable potential (OpenAI, 2024; DeepSeek-AI\net al., 2025). We use the error information from the\nsyntax parser as feedback and assess whether in-\ncreasing the test-time compute budget can enhance\nthe LLM's performance. As shown in Figure 6,\nthe model exhibits consistent improvement with\nincreased test-time computation. More advanced"}, {"title": "6.2 In-Context Learning", "content": "We also perform a few-shot evaluation in Sec-\ntion 6.2, where we carefully select demonstration\n\"gripper\" and \"blocks\" that are structurally sim-\nilar but semantically distinct from the test cases\nto prevent data leakage. As shown in Table 2,\nwe observe that different models exhibit varying\ndegrees of improvement from in-context learning.\nFor instance, claude-3.5-sonnect demonstrates\na substantial enhancement, achieving over a 20%\nincrease in the component-wise F1 score. However,\nfor gpt-4o-mini, incorporating few-shot exam-\nples resulted in a decrease in model performance."}, {"title": "6.3 Fine-tuning", "content": "We leverage the AgentGen (Hu et al., 2024b) frame-\nwork to synthesize 601 PDDL domains and their\ncorresponding descriptions for fine-tuning LLaMA-\n3.1 (Meta AI) to investigate potential improvements\nin their world modeling capabilities. As shown\nin Table 2, fine-tuning can lead to significant im-"}, {"title": "6.4 Agent Training", "content": "Many studies have demonstrated that supervised\nfine-tuning on agent trajectories can enhance a\nmodel's performance on agentic tasks (Hu et al.,\n2024b; Zeng et al., 2023) (i.e., agent training).\nSome previous works also discussed that a good\nagent model requires a sufficiently strong internal\nworld representation (LeCun, 2022). Therefore, in\nthis section, we explore whether agent training can\nimprove the model's world modeling capabilities.\nMore specifically, we trained LLaMA-2-70B model\non AgentInstruct (Zeng et al., 2023). As shown in\nTable 2, the model's world modeling capabilities\nare enhanced post-agent training, indicating a pos-\nitive correlation between performance on agentic\ntasks and the model's world modeling abilities."}, {"title": "6.5 Inference with Concrete Description", "content": "As is discussed in Section 2.2, we intentionally\nmake the natural language description of a world\nmodel at a high level. We refer to these high-level\ndescriptions as \"abstract descriptions,\" in contrast\nto more detailed \"concrete descriptions\" that ex-\nplicitly specify preconditions and effects. Exam-\nples of both description types can be found in the\nAppendix A.1.2. Using concrete descriptions sim-\nplifies the task by requiring the model to directly\nmap the provided text to a world specification, by-\npassing the need to infer symbolic action dynamics.\nThe observed consistent improvement (as shown"}, {"title": "7 Related Work", "content": "Neural world modeling is a long-standing research\ntopic with widespread applications across various\nfields, including reinforcement learning (Ha and\nSchmidhuber, 2018b,a), robotics (Wu et al., 2023),\nand autonomous driving (Guan et al., 2024), among\nothers. In recent years, LLMs trained on massive\ndatasets have demonstrated zero-shot capabilities\nacross a variety of tasks, including planning (Zhao\net al., 2023; Qin et al., 2024; Huang et al., 2022;\nHu et al., 2024a), robotics (Mu et al., 2024; Chen\net al., 2024a), analog design (Lai et al., 2024), and\nmore. Preliminary studies propose directly using"}, {"title": "8 Conclusion", "content": "We present TEXT2WORLD, a novel benchmark\nconsisting of hundreds of domains designed to\nevaluate the world modeling capabilities of large\nlanguage models (LLMs). Developed through a\nmeticulous and thorough process, TEXT2WORLD\nprovides a robust foundation for analysis. Addition-\nally, we conducted an extensive evaluation involv-\ning 16 different LLMs from 9 model families based"}, {"title": "A Benchmark Construction", "content": ""}, {"title": "A.1 Example", "content": ""}, {"title": "A.1.1 Domain Example", "content": ""}, {"title": "A.1.2 Abstract Description", "content": "General. This domain models a robot navigating a grid environment with the objective of unlocking\ndoors and moving through the grid. The robot can carry keys that match the shape of locks to unlock\ndoors. The environment includes places, keys with specific shapes, and doors (locks) with corresponding\nshapes that need to be unlocked.\nPredicates. The following predicates are used in the domain:\n\u2022 (conn ?x ?y): Indicates a connection between two places ?x and ?y, allowing movement between\nthem.\n\u2022 (key-shape ?k ?s): Indicates that key ?k has shape ?s.\n\u2022 (lock-shape ?x ?s): Indicates that lock (or door) at place ?x has shape ?s.\n\u2022 (at ?r ?x): Indicates that key ?r is at place ?x.\n\u2022 (at-robot ?x): Indicates that the robot is at place ?x.\n\u2022 (place ?p): Indicates that ?p is a place in the grid.\n\u2022 (key ?k): Indicates that ?k is a key.\n\u2022 (shape ?s): Indicates that ?s is a shape.\n\u2022 (locked ?x): Indicates that the place ?x is locked.\n\u2022 (holding ?k): Indicates that the robot is holding key ?k.\n\u2022 (open ?x): Indicates that the place ?x is open.\n\u2022 (arm-empty): Indicates that the robot's arm is empty."}, {"title": "A.1.3 Concrete Description", "content": "General. This domain models a robot navigating a grid environment with the objective of unlocking\ndoors and moving through the grid. The robot can carry keys that match the shape of locks to unlock\ndoors. The environment includes places, keys with specific shapes, and doors (locks) with corresponding\nshapes that need to be unlocked.\nPredicates. The following predicates are used in the domain:\n\u2022 (conn ?x ?y): Indicates a connection between two places ?x and ?y, allowing movement between\nthem.\n\u2022 (key-shape ?k ?s): Indicates that key ?k has shape ?s.\n\u2022 (lock-shape ?x ?s): Indicates that lock (or door) at place ?x has shape ?s.\n\u2022 (at ?r ?x): Indicates that key ?r is at place ?x.\n\u2022 (at-robot ?x): Indicates that the robot is at place ?x.\n\u2022 (place ?p): Indicates that ?p is a place in the grid.\n\u2022 (key ?k): Indicates that ?k is a key.\n\u2022 (shape ?s): Indicates that ?s is a shape.\n\u2022 (locked ?x): Indicates that the place ?x is locked.\n\u2022 (holding ?k): Indicates that the robot is holding key ?k.\n\u2022 (open ?x): Indicates that the place ?x is open.\n\u2022 (arm-empty): Indicates that the robot's arm is empty."}, {"title": "A.2 Preliminary Experiment", "content": "The experimental results show that LLM's effectiveness in detecting PDDL semantic errors is limited,\nwith an accuracy of 55.0%, a precision of 56.2%, a recall rate of 45.0%, an F1 score of 50.0%, and a ROC\nAUC of 55.0. ROC AUC indicates that the model is close to random performance, making it difficult to\nreliably distinguish between correct and incorrect PDDL domains. Below is the prompt used for LLMs to\ndetect semantic errors in generated PDDL domains:"}, {"title": "A.3 More Details on Data Analysis", "content": ""}, {"title": "B More Details on Experiments", "content": ""}, {"title": "B.1 Evaluation Metrics", "content": "Levenshtein Ratio. The Levenshtein Ratio is a value between 0 and 1 that quantifies the similarity\nbetween two strings, such as a predicted PDDL domain and a golden PDDL domain. It is derived from the\nLevenshtein distance, which calculates the minimum number of character-level operations\u2014insertions,\ndeletions, or substitutions\u2014needed to convert one string into the other. The ratio is then computed by\ndividing the Levenshtein distance by the length of the longer string, providing a measure of how closely\nthe two strings match, where a value closer to 1 indicates high similarity and a value closer to 0 indicates\nsignificant differences.\nComponent-wise F1 Scores. The F1 score is mainly used to measure the similarity between the predicted\nPDDL domain and the golden PDDL domain, specifically including predicate F1 and action F1. The\nrange of this score is from 0 to 1, which is the harmonic mean of precision and recall."}]}