{"title": "Are Large Language Models Ready for Travel Planning?", "authors": ["Ruiping Ren", "Xing Yao", "Shu Cole", "Haining Wang"], "abstract": "While large language models (LLMs) show promise in hospitality and tourism, their ability to\nprovide unbiased service across demographic groups remains unclear. This paper explores\ngender and ethnic biases when LLMs are utilized as travel planning assistants. To investigate this\nissue, we apply machine learning techniques to analyze travel suggestions generated from three\nopen-source LLMs. Our findings reveal that the performance of race and gender classifiers\nsubstantially exceeds random chance, indicating differences in how LLMs engage with varied\nsubgroups. Specifically, outputs align with cultural expectations tied to certain races and\ngenders. To minimize the effect of these stereotypes, we used a stop-word classification strategy,\nwhich decreased identifiable differences, with no disrespectful terms found. However,\nhallucinations related to African American and gender minority groups were noted. In\nconclusion, while LLMs can generate travel plans seemingly free from bias, it remains essential\nto verify the accuracy and appropriateness of their recommendations.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) are advanced AI systems trained on extensive datasets,\ncapable of understanding, generating, and interacting with human language in a meaningful\nmanner (Dong et al., 2023). LLMs have significantly transformed the hospitality and tourism\nindustries. By facilitating seamless dialogue-based interactions, LLMs can offer highly relevant\ntourism information to travelers, influencing their decision-making processes and enhancing\ntheir overall experiences (Wong et al., 2023). Additionally, LLMs can analyze hotel reviews,\nproviding aspect-based feedback to help better understand service shortfalls in the hospitality\nsector (Jeong & Lee, 2024). Moreover, they can efficiently capture tourists' emotional needs,\nenabling the creation of products tailored to meet those needs (Lu et al., 2024). LLMs are\nreshaping traditional practices within hospitality and tourism, driving greater efficiency and\nfostering innovation.\nHowever, inherent biases in the training datasets and the architectures of LLMs can result\nin potentially harmful outputs (Dong et al., 2023; Ouyang et al., 2022). As LLMs become more\npervasive across various domains, exerting significant social influence, concerns over these\nbiases have become more pronounced (Dong et al., 2023; Zhao et al., 2024). Specifically, in the\nhealthcare sector, Zhang et al. (2023) observed that ChatGPT-3.5 shows gender and racial biases\nin managing acute coronary syndrome (ACS), with female, African American, and Hispanic\npatients receiving less guideline-recommended care, diagnosis, and symptom management.\nThese disparities may contribute to the gender- and race-based differences in ACS and coronary\nartery disease morbidity and mortality outcomes. Similarly, in the employment sector, Salinas et\nal. (2023) identified similar biases in LLMs, noting that these models frequently recommended\nlow-paying jobs for Mexican workers and secretarial roles for women. Moreover, Kotek et al."}, {"title": "2. Literature review", "content": ""}, {"title": "2.1 Large language models", "content": "A language model (LM) defines a probability distribution over sequences of words. For\nexample, given the phrase \u201cThe cat sat on the,\u201d the model assigns a higher probability to \u201cmat\u201d\nas the subsequent token compared to \u201cthe.\u201d The primary objective in language model training is\nto minimize prediction errors for the next word based on preceding words. Ideally, the target\ndistribution would encompass all discourse throughout a language's history, including spoken\nand written forms, natural and artificial languages, and both documented and undocumented\nsources. In practical terms, however, only a subset of human discourse is readily accessible for\ntraining purposes, primarily sourced from the Internet. This includes materials such as textbooks,\ncode repositories, academic papers, social media posts, and transcriptions of movies and\ndialogues. Despite the limitations in data scope, the volume of text used for training can still be\nenormous; it is increasingly common to train language models on corpora containing trillions of\nwords. This extensive process of modeling large-scale human-produced text is called pretraining.\nPretrained models are frequently fine-tuned on task-specific corpora to enhance their\nperformance on specific applications. For example, to adapt a pretrained language model for\nEnglish French translation, it is advisable to fine-tune it using a parallel corpus composed of\naligned English and French texts. These texts ideally include task-specific indicators, such as"}, {"title": "2.2 LLMs study in tourism and hospitality industry", "content": "LLMs have significantly impacted the tourism and hospitality industry, prompting\nextensive research into their applications (Gursoy et al., 2023; Dogru et al., 2023). These studies\nhighlight LLMs' profound influence on tourist experiences, demonstrating their ability to\nprovide highly relevant tourism information through simple dialogue interactions. This capability\nenhances tourists' decision-making and overall experiences during various travel stages (Wong\net al., 2023; Xu & Wang, 2023; Feng, 2023). Furthermore, LLMs have revolutionized traditional\nmethods in product design and marketing by improving efficiency and fostering innovation. For\ninstance, Lu et al. (2024) leveraged text-generative AI (GPT-4.0) to analyze tourists' comments\nand elucidate their emotional needs, while utilizing image-generative AI (Midjourney) to create\nproducts that align with these identified needs. Remountakis et al. (2023) emphasized ChatGPT\u2019s\ncapacity for comprehending and generating human-like text, thereby facilitating more accurate\nand context-sensitive recommendations. Their experimental findings indicated that the\nintegration of ChatGPT with persuasive technology can significantly enhance guest experiences\nand improve business performance. Jeong & Lee (2024) employed ChatGPT to conduct an\nanalysis of hotel reviews, focusing on service failures. Their research demonstrated that\nChatGPT can provide enhanced accuracy in aspect-based analysis, offering deeper insights into\ncustomer experiences and perceptions."}, {"title": "2.3 Gender and racial/ethnic bias in LLMs", "content": "LLMs are trained on vast corpora of textual data, enabling them to generate coherent and\ncontextually appropriate text that closely mimics human-produced content (Dong et al., 2023).\nHowever, these models are susceptible to inherent biases stemming from their training datasets\nand architectural design, potentially leading to problematic outputs. As LLMs increasingly\npermeate various sectors and exert significant societal influence, the implications of these biases\nhave garnered substantial attention (Dong et al., 2023; Zhao et al., 2024). Consequently, the\ninvestigation of inherent biases in LLMs has emerged as a critical area of research, attracting\nconsiderable focus across the field (Hu et al., 2023).\nScholars have explored the inherent biases in LLMs and identified various strategies for\nmitigating these biases. Hu et al. (2023) examined ingroup-positive and outgroup-negative biases\nacross 51 LLMs, finding that most exhibited social biases comparable to humans. Haim et al.\n(2024) assessed gender and racial biases in GPT-4, GPT-3.5, and PaLM-2 across multiple\ndomains, confirming their presence. Rhue et al. (2024) identified gender-based biases in LLM\nresponses to factual inquiries, while Zhao et al. (2024) reported significant gender biases in\nLLMs tailored for various languages. Dong et al. (2023) observed persistent explicit and implicit\ngender biases in LLaMA models, noting that increasing model size did not reliably improve\nfairness. To address these issues, researchers have explored potential solutions. Kaneko et al.\n(2024) demonstrated the effectiveness of Chain-of-Thought prompting in reducing unconscious\nsocial biases in LLMs, promoting fairer predictions.\nBesides, extensive research has been conducted on gender and racial/ethnic biases within\nthe deployment of LLMs across diverse sectors, including healthcare, the workplace, and\neducation, producing mixed outcomes. In healthcare, some studies have found no significant"}, {"title": "2.4 Gender and race/ethnicity study in tourism", "content": "Existing tourism studies have demonstrated that gender identity significantly influences\ntravel motivations, preferences, and behaviors. Men generally exhibit a propensity for physical\nactivities and entertainment, favoring challenging nature-based activities, including canoeing,\nhiking, skiing, horseback riding, hunting, and fishing (Meng & Uysal, 2008). In contrast, female\ntravelers tend to seek vacations that offer relaxation and restoration showing a broader range of\ndestination attributes. They often engage in activities such as attending festivals and museums,\nvisiting historical sites, sightseeing, and shopping (Meng & Uysal, 2008). Women also tend to\nplace higher value on family-oriented holidays and are more likely to seek unique souvenirs,\nemphasizing the importance of shopping and culinary experiences during their vacations (Ryan\net al., 1998). Gender identity-related constraints further differentiate travel behaviors. Women\nexpress greater concerns regarding security risks, which influences their willingness to engage in"}, {"title": "3. Methodology", "content": "To address the research questions, we collected data from three open source LLMs and\nused machine learning to analyze the data produced by these LLMs. This analysis determined if\nthere are any ethnic and gender biases present when utilizing LLMs for travel planning\nassistance."}, {"title": "3.1 Data collection", "content": "In this study, we selected three leading open-source LLMs that have demonstrated state-\nof-the-art performance on various benchmarks: Gemma-2-9b, Llama-3-8b, and Llama-3-70b.\nThese models will be utilized to generate text data (travel suggestions) for our analysis. Gemma-\n2-9b, developed by Google, was trained on a diverse dataset comprising 8 trillion tokens. The\ntraining corpus encompasses a wide variety of sources, with key components including web\ndocuments, mathematics, and code (Google, 2024). Llama 3, developed by Meta, is a collection\nof pretrained and instruction-tuned generative text models available in 8 billion and 70 billion\nparameter sizes. The pretraining phase utilized over 15 trillion tokens of data sourced from\npublicly available repositories. The subsequent fine-tuning process incorporated publicly\navailable instruction datasets, supplemented by over 10 million human-annotated examples\n(Meta, 2024).\nFirstly, we crafted a straightforward system prompt to steer these three LLMs to provide\nvirtual travel planning service. The system prompt reads: \u201cYou are a helpful, respectful, and\nhonest travel assistant.\u201d Then, to create synthetic queries, we used three common scenarios\n(travel attractions, accommodations and dining) encountered in virtual travel planning services.\nWe targeted four racial/ethnic groups (African American, Hispanic, Asian, Caucasian) and three\ngender categories (man, woman, gender minority group) for our study. Considering that various\nfactors can influence a tourist's planning decisions, we also included age (young adult, middle\nage, elderly), educational background (no bachelor, bachelor, postgraduate), income (low,\nmiddle, high), budget (low, middle, high), duration of stay (1-3 days, 4-7 days, more than 7\ndays), destination (New York, Chicago, Miami, Los Angeles), time of year (Spring, Summer,\nFall, Winter), and previous travel experience (first-time visitor, repeating visitor). The queries\nwere structured as follows: \u201cI will be traveling alone. Please help plan my [three common\nscenarios]. Here are my details: [ethnicity, gender, age, educational background, income, budget,\nduration of stay, destination, time of year, and previous travel experiences].\u201d\nLastly, we assembled queries and solicited responses from language models. Using the\nLlama-3-8b model as an example, we constructed a query by selecting a random factor from\neach influencing category and integrating these factors into a predefined conversation template\n(refer to Figure 1, upper panel). The resulting prompt is fed to Llama-3-8b, whose outputs are\ndecoded with a straightforward strategy (see Figure 1 lower panel). For each model, we repeat\nthe described process 2,000 times. We refer you to our repository for technical details."}, {"title": "3.2 Data analysis", "content": "To detect ethnic and gender biases in the responses from these three open source LLMs,\nwe introduce a machine learning method termed \u201cfairness probing\u201d, enlightened by existing\nstudies (Alain and Bengio, 2016; Conneau et al., 2018; Li et al., 2022). This approach is based\non a straightforward premise: the more distinctive the groups of inputs are, the more easily they\ncan be distinguished from each other by a basic classifier. Applied to our case, the greater the\ndisparities in language use across different ethnic and gender groups in the three open source\nLLMs outputs, the higher the accuracy a basic classifier can achieve. If the three open source\nLLMs provide unbiased travel planning service, we would expect a basic classifier to perform no\nbetter than random guessing. To conduct the investigation, we use the frequencies of words in\nthree open source LLMs outputs as features and gender or ethnic groups as labels. We train the\nclassifier using all outputs except for a portion held out for testing. If the test accuracy (defined\nas the proportion of correctly predicted cases. Accuracy= correct predicted cases/ total predicted\ncases) matches chance levels, such as 33.3% for gender with three categories and 25.0% for\nethnicity with four groups, this indicates that three open source LLMs outputs are no gender and\nethnic bias. And we can argue that three open source LLMs delivered unbiased travel planning\nservice to each of the ethnic and gender groups.\nOtherwise, a classifier would pick up on the cues of differences in language use among\ndifferent groups to perform better than a chance guess. In this case, we will examine the weights\nof the classifier to determine which words are contributing to the ethnic and gender bias. The\nmost influential tell-tale signs will be investigated to ascertain whether they are expressions of\nbias, stereotypes, or occasional noise."}, {"title": "3.2.1 Classifier & feature engineering", "content": "We chose a standard logistic regression model with l2 regularization (i.e., regularization\nstrength $\\lambda = 1.0$) as our classifier, given its effectiveness and ease of interpretation. Our analysis\nwas conducted at the word level, treating each word as an individual feature. For feature"}, {"title": "4. Result", "content": ""}, {"title": "4.1 Racial/ethnic bias", "content": "The results demonstrate that the race/ethnicity test accuracy (correct predicted race cases/\ntotal predicted race cases) using TF-IDF representations is 50.08%, exceeding the random\nchance threshold of 25% (with four groups). Subsequently, the top 20 features with the highest\nabsolute weights for each class (as presented in Table 1) are analyzed to further examine\npotential ethnic biases.\nAnalysis of Table 1 reveals that the primary features associated with the African\nAmerican group include: \u201csoul,\u201d \u201charlem,\u201d \u201cleimert,\u201d \u201cchicken,\u201d \u201chistory,\u201d \u201cfried,\u201d \u201chyde,\u201d\n\u201cdusable,\u201d \u201camericans,\u201d \u201cwaffles,\u201d \u201cfood,\u201d \u201cim,\u201d \u201csylvia's,\u201d \u201ccultural,\u201d \u201cblack,\u201d \u201cculture,\u201d and\n\"caldwell's\". Among these features, several geographic names emerge, demonstrating strong\ncorrelations with African American communities. \u201charlem,\u201d a neighborhood in Upper\nManhattan, New York City, is renowned for its African American cultural heritage. \u201cleimert\"\ntypically refers to Leimert Park, a Los Angeles neighborhood that has become a focal point for\nboth historical and contemporary African American art, music, and culture. \u201chyde\u201d is commonly\nassociated with Hyde Park, a neighborhood on Chicago's South Side, while \u201cdusable\u201d is linked\nto the DuSable Museum of African American History located in Hyde Park. The prevalence of\nfood-related terms such as \u201csoul,\u201d \u201cchicken,\u201d \u201cfried,\u201d \u201cwaffles,\" \"food,\" and \"sylvia's\" reflects\nthe significance of soul food in African American cuisine. Soul food represents the ethnic\nculinary traditions of African Americans, with fried chicken and waffles being notable examples.\n\"Sylvia's\" refers to a well-known Harlem establishment offering traditional soul food at\naccessible prices."}, {"title": "4.2 Gender bias", "content": "The gender test accuracy (correct predicted gender cases/ total predicted gender cases) of\nTF-IDF representations is 60.83%, exceeding the chance level of 33.3% (with three groups). \u03a4\u03bf\nfurther investigate potential biases, we analyzed the top 20 features with the highest absolute\nweights for each class (see Table 3).\nKey features for the gender minority group include: \u201cindividual,\u201d \u201ci'm,\u201d \u201clgbtq,\"\n\"welcoming,\" \"needs,\u201d \u201cbackground,\" \"safety,\u201d \u201cdiverse,\u201d \u201crespectful,\u201d \u201cmember,\u201d \u201cdiversity,\"\n\u201cinclusive,\u201d \u201caccessible,\u201d \u201ccater,\u201d \u201caccessibility,\u201d \u201ctravel,\u201d \u201c2019,\u201d and \u201ccommunity.\u201d\n\"individual\" frequently appears in safety-related recommendations, such as: \u201cBe mindful of your\nsafety: As a gender minority individual, it's essential to be aware of your surroundings and take\nnecessary precautions to ensure your safety.\u201d \u201clgbtq\u201d is used interchangeably with \"gender\nminority group.\u201d \u201cmember\u201d is consistently associated with the gender minority group and\nLGBTQ+ community, often describing the traveler's affiliation with these groups. \u201cdiverse\u201d,\n\"diversity\u201d and \u201cinclusive\u201d are commonly used in contexts related to the gender minority group.\n\"safety\" and \"welcoming\u201d appear in recommendations specific to this group, with LLMs\nsuggesting destinations with welcoming atmospheres and emphasizing safety considerations.\n\"accessibility\u201d and \u201caccessible\u201d occur more frequently in recommendations for older adults\nwithin the gender minority group. \"background\u201d and \u201crespectful\u201d are often found in prompts and\nopening sentences of responses. \u201ccater\u201d and \u201cneeds\u201d indicate that the LLMs provide tailored\nsuggestions based on the traveler's characteristics.\nThe term \u201c2019\" frequently appears as a hallucination in outputs from Llama-3-70B,\nwith most occurrences found in the gender minority group--totaling 106 instances: five times\nwithin the women's group, four times within the men's group, and 95 times within the gender\nminority group. This usage is extraneous and does not contribute meaningfully to the sentences.\nFor example, one output suggests, \u201cI recommend making reservations in advance, 2019,\nespecially for The Purple Pig and Girl & the Goat, as they can get quite busy during the summer\nmonths.\" Another states, \u201cConsidering your preferences, 2019, the high-end experiences you're\nlooking for.\" In these contexts, the insertion of \u201c2019\u201d is superfluous and disrupts the clarity of\nthe message.\nThe primary features associated with male travelers include: \u201cgentleman,\u201d \u201cmale,\u201d\n\u201cwelcome,\u201d and \u201cclassic.\u201d \u201cgentleman\u201d and \u201cmale\u201d are used interchangeably with \u201cman\u201d in the\""}, {"title": "5. Conclusions", "content": "This study employed an experimental design to investigate gender and racial/ethnic\nbiases in large language models (LLMs) when used as travel planning assistants. The research\nutilized machine learning techniques to detect potential biases present in outputs from three\nopen-source LLMs: Gemma-2-9b, Llama-3-8b, and Llama-3-70b. Following TF-IDF\nclassification, both gender and race test accuracies surpassed the random chance threshold.\nAnalysis of the top 20 features in the TF-IDF representations revealed that LLMs consistently\nlinked specific racial groups to their traditional cuisines and culturally significant landmarks. For\ninstance, the models suggested soul food for the African American group, noodles and sushi for\nthe Asian group, burgers for the Caucasian group, and tacos and carnitas for the Hispanic group.\nSimilarly, landmarks like Harlem, Leimert Park, and the DuSable Museum of African American\nHistory were recommended for the African American group; Chinatown for the Asian group;"}, {"title": "6. Limitations and future research", "content": "This study serves as an initial exploration, focusing on three prominent open-source\nLLMs. It is essential to expand our examination to include other open-source LLMs to enhance\nour understanding. Moreover, future research should aim to compare the performance of closed-\nsource LLMs with their open-source counterparts, particularly in the realm of travel planning"}]}