{"title": "LLAMA-BERRY: PAIRWISE OPTIMIZATION FOR 01-LIKE OLYMPIAD-LEVEL MATHEMATICAL REASONING", "authors": ["Di Zhang", "Jianbo Wu", "Jingdi Lei", "Tong Che", "Jiatong Li", "Tong Xie", "Xiaoshui Huang", "Shufei Zhang", "Marco Pavone", "Yuqiang Li", "Wanli Ouyang", "Dongzhan Zhou"], "abstract": "This paper presents an advanced mathematical problem-solving framework, LLAMA-Berry, for enhancing the mathematical reasoning ability of Large Language Models (LLMs). The framework combines Monte Carlo Tree Search (MCTS) with iterative Self-Refine to optimize the reasoning path and utilizes a pairwise reward model to evaluate different paths globally. By leveraging the self-critic and rewriting capabilities of LLMs, Self-Refine applied to MCTS (SR-MCTS) overcomes the inefficiencies and limitations of conventional stepwise and greedy search algorithms by fostering a more efficient exploration of solution spaces. Pairwise Preference Reward Model (PPRM), inspired by Reinforcement Learning from Human Feedback (RLHF), is then used to model pairwise preferences between solutions, utilizing an Enhanced Borda Count (EBC) method to synthesize these preferences into a global ranking score to find better answers. This approach addresses the challenges of scoring variability and non-independent distributions in mathematical reasoning tasks. The framework has been tested on general and advanced benchmarks, showing superior performance in terms of search efficiency and problem-solving capability compared to existing methods like ToT and rStar, particularly in complex Olympiad-level benchmarks, including AIME24 and AMC23.", "sections": [{"title": "1 INTRODUCTION", "content": "Mathematical reasoning represents a great challenge in artificial intelligence, with broad applications across automated theorem proving, mathematical problem solving, and scientific discovery (Ahn et al., 2024). Recently, significant strides have been made by Large Language Models (LLMs) like GPT-4 (Achiam et al., 2023) in general mathematical tasks involving arithmetic or geometric problem-solving (Cobbe et al., 2021; Sun et al., 2024; Ying et al., 2024). However, complex mathematical reasoning remains challenging, especially at the Olympiad-level benchmarks such as AIME (MMA). One intuitive way is to break the complex problem-solving solutions into step-by-step thoughts, known as reasoning paths, as shown in Chain-of-Thought (CoT) (Wei et al., 2022). Although prompt-based methods can directly construct such reasoning paths, they remain limited in efficiency and accuracy. As a result, researchers have begun exploring search-based optimization algorithms to enhance the generation of effective reasoning paths.\nOne line of research focuses on generating solutions based on complete reasoning paths. Tree search techniques with backtracking capabilities are proposed to address the inefficiencies of naive methods, such as Tree-of-Thought (ToT) (Yao et al., 2024) and Breadth-First Search (BFS) based on greedy strategies (Zhang et al., 2024). Heuristic tree search methods like RAP (Hao et al., 2023) and Q* (Wang et al., 2024) treat each step in a reasoning path as a state and the generation of steps as actions, framing the search for reasoning paths as a Markov Decision Process (MDP) problem."}, {"title": "2 \u041c\u0415\u0422\u041dODOLOGY", "content": null}, {"title": "2.1 PRELIMINARY", "content": "In the realm of mathematical problem-solving, we face significant challenges that require systematic approaches to derive accurate and efficient solutions. A core issue in this field is generating and op-timizing reasoning paths, which can be formalized as a path-wise Markov Decision Process (MDP) framework. This framework allows us to define the state space S, where each state s represents a complete solution to a given problem, and the action space A consists of all feasible rewriting actions a that make transitions between states.\nThe central issues we aim to address include the development of the value function Q(s, a), which quantifies the expected rewards from executing action a at state s, defined as:\n$Q(s, a) = E[R(s')|s' = T(s, a)]$\nWhere T(s, a) indicates the transition to another solution s' from solution s with rewriting operation denoted as a. Our primary objective is to identify the optimal state s*, which represents the best solution to the problem. This optimal state can be achieved through the selection of actions that maximize the value function, guiding us toward the most desirable outcomes:\n$S* = arg max_{s' \\in S} E[Q(s)]$"}, {"title": "2.2 SELF-REFINE APPLIED TO MONTE CARLO TREE SEARCH", "content": "Monte Carlo Tree Search (MCTS) is an effective reinforcement learning method within the Markov Decision Processes (MDP) framework, employing states, actions, and cost functions through sam-pling. The algorithm follows four key steps: selection, expansion, simulation/evaluation, and back-propagation. In the selection phase, the root node is expanded using the Upper Confidence Bound applied to Trees (UCT) algorithm, which selects a node s by balancing exploration and exploitation, that is,\n$a = arg max_{\\alpha \\in A(s)} (Q(s,a) + c \\sqrt{\\frac{ln N(s)}{N(s,a)}}),$\nwhere N(s) is the visitation count of node s, N(s, a) is the action frequency, and c is a parameter controlling exploration. In the expansion phase, node s generates subsequent states s', added as"}, {"title": "2.3 PAIRWISE PREFERENCE REWARD MODEL", "content": "In mathematical problem-solving tasks, reliable evaluation of solutions is crucial, as it leads to bet-ter estimation of Q-values, thereby offering improved guidance. Existing reward models typically evaluate solutions by giving absolute scores, such as (PRM) (Lightman et al., 2023) and outcome reward mode (ORM) (Yu et al., 2023a). However, the score-based reward models may fall short in leveraging the instruction following capability of LLMs or effectively handling the variations in data distributions and exhibit volatility, especially when the differences between different solu-tions are subtle. To address this issue, we propose the Pairwise Preference Reward Model (PPRM), which leverages a comprehensive preference dataset incorporating substantial samples from both ap-proaches to learn preference relationships among mathematical solutions. Please refer to Appendix for details about training and inference of PPRM.\nFor two answers to a given mathematical problem, denoted as a\u2081 > a2 where a\u2081 and a2 denote the preferred and dispreferred completion, PPRM predicts their relative quality using a pairwise partial ordering, represented by the following probability formula:\n$P(\\alpha_1 \\succ \\alpha_2 | \\Phi) = \\frac{e^{\\phi(a_1)}}{e^{\\phi(a_1)} + e^{\\phi(a_2)}}$ \nWhere (a\u2081 > a2) denotes the feature representation of a partial ordering relation between solution a1 and a2, with \u03a6 representing the parameters of the model. In our method, a\u2081 > a2 are represented by tokens of the LLM, and $P(a_1 \\succ a_2 | \\phi)$ is estimated using the logits value of tokens calculated by the LLM.\nThen, inspired by advancements in Language Interface Fine-Tuning (LIFT) (Dinh et al., 2022), we frame the training process of PPRM as a question-answering task. The model is tasked with answer-ing the question, \"For Question Q, is solution a\u2081 better than solution a\u2082?\" as shown in Figure 2. To form a robust training objective, the predicted token labels \u0177 ('Yes' or 'No') are evaluated using the indicator function I:\n$I(\\hat{y}, y) = \\begin{cases} 1, & \\text{if } \\hat{y} = y \\\\ 0, & \\text{if } \\hat{y} \\neq y \\end{cases}$\nFinally, a pairwise preference dataset D that contains millions of mathematical problem-solving so-lution pairs is converted into a dataset D' suitable for a question-answering task. We employ RLHF techniques to train the model to improve its performance in the partial-order prediction question-answering task. Subsequently, the Direct Preference Optimization (DPO) (Rafailov et al., 2024) method is utilized to find the optimal $P_\\Phi$ by maximizing the objective $arg \\max_{P_{\\Phi}}, E_p[I(\\hat{y}, y)]$."}, {"title": "2.4 ENHANCED BORDA COUNT METHOD", "content": "Although PPRM allows us to directly compare the quality of two answers, we still need to convert these local partial order relations into a cohesive global ranking to gain a comprehensive evaluation for the answers. This conversion process can be formalized as the global optimal ranking aggre-gation (GORA) problem. To address this problem, we propose the Enhanced Borda Count (EBC) method based on a transitivity assumption of mathematical problem-solving solutions (detailed in Figure 3), which integrates the naive Borda Count Algorithm with a transitive closure of partial order relations calculated by the Floyd-Warshall (Cormen et al., 2009) Algorithm. For formalized proof, please refer to Appendix E.\nLocal Preference Calculation: First, as shown in Figure 3 (1), the PPRM generates a win-loss matrix M \u2208 $R^{n \\times n}$ for all n problem-solving solutions, where M[i, j] = 1 indicates that solution ai is superior to solution aj, and M[i, j] = 0 otherwise. This process can be represented as:\n$M[i, j] = \\begin{cases} 1, & \\text{if } P(a_i > a_j) \\geq 0.5 \\\\ 0, & \\text{if } P(a_i \\n \\\\ Solutions & 1500- & \\\\\\ Critiques & 1000- & \\\\\\ Overall & 500- & \\\\\\ & 0 &  \\\\ & \\end{cases}$Critiques & Overall"}, {"title": "5 CONCLUSION", "content": "The research aims to address the challenges in complex mathematical reasoning, especially at the Olympiad level, where generating accurate and efficient reasoning paths for problem-solving. By combining iterative optimization methods Self-Refine applied to Monte Carlo Tree Search (SR-MCTS), the LLAMA-Berry framework improves the efficiency of solution generation. The Pairwise Preference Reward Model (PPRM) further enhances performance by modeling prefer-ences between solutions rather than just scoring outcomes. Experimental results demonstrate that LLAMA-Berry outperforms baseline approaches on benchmarks like GSM8K and MATH, and demonstrates a competitive level with GPT-4 Turbo on difficult benchmarks without additional train-ing."}, {"title": "6 Related work", "content": null}, {"title": "6.1 PRM&ORM", "content": "The mathematical reasoning capabilities of LLMs exhibit continuous enhancement. OpenAI o1 (OpenAI, 2024) even achieves ability comparable to graduate students on challenging GPQA benchmarks. Recent studies mainly focus on collecting high quality data or domain-specific knowledge (Liao et al., 2024; Huang et al., 2024; Toshniwal et al., 2024; Lu et al., 2024; Yu et al., 2023b; Zhao et al., 2024) or training external reward models (Kang et al., 2024; Wang et al., 2023; Havrilla et al., 2024; Lightman et al., 2023; Ma et al., 2023). There are primarily two types of reward models. The first one is Outcome-supervised Reward Models (ORMs), which are trained from the final CoT answer of the model, while ORMs assign a reward to a complete answer. This kind of feedback is not precise enough, and it is challenging to align the reward. The second one is process-supervised reward models (PRMs), which receive feedback at each step in CoT. PRMs are considered to be more effective (Lightman et al., 2023). However, PRMs face significant challenges such as the need for extensive manually annotated data (Luo et al., 2024a; Havrilla et al., 2024), and the inefficiency in guiding LLMs to generate answers (Feng et al., 2023; Ding et al., 2023). To address these issues, we propose the Pairwise Preference Reward Model (PPRM), which uses the following instructions and the learning ability of LLMs in context. Our approach effectively improves the efficiency and accuracy of answer search by directly comparing the quality of two answers and globally ranking all generated answers. The whole process saves the cost of collecting much human-annotated data."}, {"title": "6.2 TREE SEARCH REASONING", "content": "Sampling diverse reasoning paths (Brown et al., 2024) can significantly enhance the probability of finding the correct answers. Self-Consistency (Wang et al., 2022) sample a complete path each time while tree search methods like Tree-of-Thought (ToT) (Yao et al., 2023) and MCTS (Chen et al., 2024a;b; Luo et al., 2024b; Feng et al., 2023; Xie et al., 2024; Xu, 2023; Liu et al., 2023; Tian et al., 2024; Ding et al., 2023) extend numerous steps to optimize step answers and ultimately obtain the optimal solution. Additionally, Self-refine (Madaan et al., 2023b) method has become a recent focus. Self-verification (Gero et al., 2023; Weng et al., 2022) and rStar (Qi et al., 2024) utilize the inherent capabilities of the model to iteratively explore and refine answers. However, the performance of self-refine is typically constrained by the inherent capabilities of the model, especially for small language models (SLMs) with significantly weaker self-refine abilities (Madaan et al., 2023a). Our approach explores the answers by combining MCTS with self-refine. Ultimately, we construct a global win-loss matrix of answers following a directed graph process."}]}