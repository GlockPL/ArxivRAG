{"title": "COGS: Causality Constrained Counterfactual Explanations using goal-directed ASP*", "authors": ["Sopam Dasgupta", "Joaqu\u00edn Arias", "Elmer Salazar", "Gopal Gupta"], "abstract": "Machine learning models are increasingly used in areas such as loan approvals and hiring, yet they often function as black boxes, obscuring their decision-making processes. Transparency is crucial, and individuals need explanations to understand decisions, especially for the ones not desired by the user. Ethical and legal considerations require informing individuals of changes in input attribute values (features) that could lead to a desired outcome for the user. Our work aims to generate counterfactual explanations by considering causal dependencies between features. We present the CoGS (Counterfactual Generation with s(CASP)) framework that utilizes the goal-directed Answer Set Programming system s(CASP) to generate counterfactuals from rule-based machine learning models, specifically the FOLD-SE algorithm. CoGS computes realistic and causally consistent changes to attribute values taking causal dependencies between them into account. It finds a path from an undesired outcome to a desired one using counterfactuals. We present details of the CoGS framework along with its evaluation.", "sections": [{"title": "1 Introduction", "content": "Predictive models are widely used in automated decision-making processes, such as job-candidate filtering or loan approvals. These models often function as black boxes, making it difficult to understand their internal reasoning for decision making. The decisions made by these models can have significant consequences, leading individuals to seek satisfactory explanations, especially for an unfavorable decision. This desire for transparency is crucial, whether the decision is made by an automated system or humans. Explaining these decisions presents a significant challenge. Additionally, users want to know what changes they must make to flip an undesired (negative) decision into a desired (positive) one.\nIn this work we follow Wachter et al.'s [19] approach where counterfactuals are employed to explain the reasoning behind a prediction by a machine learning model. Counterfactuals help answer the following question: \u201cWhat changes"}, {"title": "2 Background", "content": "Counterfactual Reasoning: Explanations help us understand decisions and inform actions. Wachter et al. [19] advocated using counterfactual explanations (CFE) to explain individual decisions, offering insights on how to achieve desired outcomes. For instance, a counterfactual explanation for a loan denial might state: If John had good credit, his loan application would be approved. This involves imagining alternate (reasonably plausible) scenarios where the desired outcome is achievable. For a binary classifier given by f : X \u2192 {0,1},\nwe define a set of counterfactual explanations \u00ee for a factual input x \u2208 X as\nCFf(x) = {x \u2208 X\\f(x) \u2260 f(x)}.\nThis set includes all inputs a leading to different predictions than the original input x under f. We utilize the s(CASP) query-driven predicate ASP system [3] for counterfactual reasoning, incorporating causal dependency between features. s(CASP) computes dual rules (section 2), allowing negated queries and constructing alternate worlds/states that lead to counterfactual explanations while considering causal dependencies.\nCausality Considerations: Causality relates to cause-effect relationship among predicates. P is the cause of Q, if (P \u21d2 Q) ^ (\u00acP \u21d2 \u00acQ) [15]. We say that\nQ is causally dependent on P. For generating realistic counterfactuals, causal dependencies among features must be taken into account. For example, in a loan approval scenario, increasing the credit score to be 'high' while still being"}, {"title": "3 Overview", "content": "When an individual (represented as a set of features) receives an undesired negative decision (loan denial), they can seek necessary changes to flip it to a positive"}, {"title": "3.1 The Problem", "content": "Predictive models are widely used in automated decision-making processes, such as job-candidate filtering or loan approvals. These models often function as black boxes, making it difficult to understand their internal reasoning for decision making. The decisions made by these models can have significant consequences, leading individuals to seek satisfactory explanations, especially for an unfavorable decision. This desire for transparency is crucial, whether the decision is made by an automated system or humans. Explaining these decisions presents a significant challenge. Additionally, users want to know what changes they must make to flip an undesired (negative) decision into a desired (positive) one."}, {"title": "3.2 Solution: CoGS Approach", "content": "Inspired by the planning problem, the CoGS approach casts the solution as traversing from an initial state to a goal state, represented as feature-value pairs (e.g., credit score: 600; age: 24). There can be multiple goal states that represents the positive outcome (set of goal states G). The objective is to turn a negative decision (initial state i) into a positive one (goal state g) through necessary changes to feature values, so that the query goal '?- not reject_loan(john)' will succeed for g\u2208 G. As mentioned earlier, the CoGS framework involves solving a variant of the planning problem due to the dependence between features.\nCoGS models two scenarios: 1) the negative outcome world (e.g., loan denial, initial state i), and 2) the positive outcome world (e.g., loan approval, goal state g) achieved through specific interventions. The initial state i and goal state g are defined by specific attribute values (e.g., loan approval requires a credit score >=600). CoGS symbolically computes the necessary interventions to find a path to the goal state g, representing a flipped decision.\nGiven an instance where the decision query (e.g., \u2018?- reject_loan/1') succeeds (negative outcome), CoGS finds the state where this query fails (i.e., the decision query \u2018?- not reject_loan/1' succeeds), which then constitutes the goal state g. In terms of ASP, the problem of finding interventions can be cast as follows: given a possible world where a query succeeds, compute changes to the feature values (while taking their causal dependencies into account) that will reach a possible world where negation of the query will succeed. Each of the intermediate possible worlds we traverse must be viable worlds with respect to the rules. We use the s(CASP) query-driven predicate ASP system [3] for this purpose. s(CASP) automatically generates dual rules, allowing us to execute negated queries (such as '?- not reject_loan/1') constructively.\nCoGS employs two kinds of actions: 1) Direct Actions: directly changing a feature value, and 2) Causal Actions: changing other features to cause the target feature to change, utilizing the causal dependencies between features. These actions guide the individual from the initial state i to the goal state g through (consistent) intermediate states, suggesting realistic and achievable changes. Unlike CoGS, Wachter et al.'s approach [19] can output non-viable solutions."}, {"title": "4 Methodology", "content": "We next outline the methodology used by CoGS to generate paths from the initial state (negative outcome) to the goal state (positive outcome). Unlike traditional planning problems where actions are typically independent, our approach involves interdependent actions governed by causal rules C. This ensures that the effect of one action can influence subsequent actions, making interventions realistic and causally consistent. Note that the CoGS framework uses the FOLD-SE RBML algorithm [20] to automatically compute causal dependency rules. These rules have to be either verified by a human, or commonsense knowledge must be used to verify them automatically. This is important, as RBML algorithms can identify a correlation as a causal dependency. CoGS uses the former approach. The latter is subject of research. We next define specific terms.\nDefinition 1 (State Space (S)). S represents all combinations of feature val- ues. For domains D1, ..., Dn of the features F1, ..., Fn, S is a set of possible states s, where each state is defined as a tuple of feature values V1,..., Vn\ns \u2208 S where S = {(V1, V2, ..., Vn)|Vi \u2208 Di, for each i in 1,..., n} (1)\nFor example state s can be : s = (31 years, $5000, $40000, 599 points).\nDefinition 2 (Causally Consistent State Space (Sc)). Sc is a subset of S where all causal rules are satisfied. C represents a set of causal rules over the features within a state space S. Then, 0c : P(S) \u2192 P(S) (where P(S) is the power set of S) is a function that defines the subset of a given state sub-space S'S that satisfy all causal rules in C.\n0c(S') = {s \u2208 S' | s satisfies all causal rules in C} (2)\nSc = 0c(S) (3)\nE.g., if a causal rule states that if debt is 0, the credit score should be above 599, then instance s\u2081 = (31 years, $0, $40000, 620 points) is causally consistent, and instance s2 = (31 years, $0, $40000, 400 points) is causally inconsistent."}, {"title": "4.1 Algorithm", "content": "We next describe our algorithm to find the goal states and compute the solution paths. The algorithm makes use of the following functions: (i) not_member: checks if an element is: a) not a member of a list, and b) Given a list of tuples, not a member of any tuple in the list. (ii) drop_inconsistent: given a list of states [s0,..., sk] and a set of Causal rules C, it drops all the inconsistent states resulting in a list of consistent states with respect to C. (iii) get_last: returns the last member of a list. (iv) pop: returns the last member of a list. (v) is_counterfactual: returns True if the input state is a causally consistent counterfactual solution (see supplement for details [2])."}, {"title": "4.2 Soundness", "content": "Definition 10 (CFG Implementation). When Algorithm 1 is executed with the inputs: Initial State i (Definition 4), States Space S (Definition 1), Set of Causal Rules C (Definition 2), Set of Decision Rules Q (Definition 3), and Set of Actions A (Definition 5), a CFG problem (Sc, SQ, I, d) (Definition 7) with causally consistent state space Sc (Definition 2), Decision consistent state space SQ (Definition 3), Initial State i (Definition 4), the transition function \u03b4 (Definition 6) is constructed.\nDefinition 11 (Candidate path). Given the counterfactual (Sc, SQ, I, d) con- structed from a run of algorithm 1, the return value (candidate path) is the re- sultant list obtained from removing all elements containing states s' \u2260 Sc.\nDefinition 10 maps the input of Algorithm 1 to a CFG problem (Definition 7). Candidate path maps the result of Algorithm 1 to a possible solution (Definition 9) of the corresponding CGF problem. From Theorem 1 (proof in supplement [2]), the candidate path (Definition 11) is a solution to the corresponding CFG problem implementation (Definition 10).\nTheorem 1 Soundness: Given a CFG X = (Sc, SQ, I, d), constructed from a run of Algorithm 1 & a corresponding candidate path P, P is a solution path for X. Proof: Given in the supplemental document [2]."}, {"title": "5 Experiments", "content": "We applied the CoGS methodology to rules generated by the FOLD-SE algorithm (code on GitHub [2]). Our experiments use the German dataset [11], Adult dataset [5], and the Car Evaluation dataset [7]. These are popular datasets found in the UCI Machine Learning repository [1]. The German dataset contains de- mographic data with labels for credit risk ('good' or 'bad'), with records with the label 'good' greatly outnumbering those labeled 'bad'. The Adult dataset in- cludes demographic information with labels indicating income (\u2018=< $50k/year'"}, {"title": "6 Related Work and Conclusion", "content": "Various methods for generating counterfactual explanations in machine learn- ing have been proposed. Wachter et al. [19] aimed to provide transparency in automated decision-making by suggesting changes individuals could make to achieve desired outcomes. However, they ignored causal dependencies, resulting in unrealistic suggestions. Utsun et al. [18] introduced algorithmic recourse, of- fering actionable paths to desired outcomes but assuming feature independence, which is often unrealistic. CoGS rectifies this by incorporating causal depen- dencies. Karimi et al. [12] focused on feature immutability and diverse coun- terfactuals, ensuring features like gender or age are not altered and maintain model-agnosticism. However, this method also assumes feature independence, limiting realism. Existing approaches include model-specific, optimization-based approaches [17,16]. White et al. [21] showed how counterfactuals can enhance model performance and explanation accuracy. Karimi et al. [13] further em- phasized incorporating causal rules in counterfactual generation for realistic and achievable interventions. However their method did not use the 'if and only' prop- erty which is vital in incorporating the effects of causal dependence. Bertossi and Reyes [6] rectified this by utilizing Answer Set Programming (ASP) but they re- lied on grounding, which can disconnect variables from their associations. In contrast, CoGS does not require grounding as it leverages s(CASP) to generate"}, {"title": "7 Supplementary Material", "content": "7.1 Methodology: Details"}, {"title": "7.2 Proofs", "content": "Theorem 1. Soundness Theorem\nGiven a CFG X = (Sc, SQ, I, d), constructed from a run of algorithm 1 and a corresponding candidate path P, P is a solution path for X.\nProof. Let G be a goal set for X. By definition 11, P = 50,..., Sm, where m \u2265 0. By definition 9 we must show P has the following properties.\n1) so = I\n2) sm \u2208 G\n3) sj \u2208 Sc for all j\u2208 {0,..., m}\n4) 80, ..., Sm-1 \u00a3 G\n5) Si+1 \u2208 d(si) for i \u2208 {0,..., m \u2013 1}\n1) By definition 4, i is causally consistent and cannot be removed from the can- didate path. Hence I must be in the candidate path and is the first state as per line 2 in algorithm 1. Therefore so must be i.\n2) The while loop in algorithm 1 ends if and only if is_counter factual(s, C, Q) is True. From theorem 2, is_counter factual(s,C,Q) is True only for the goal state. Hence sm \u2208 G.\n3) By definition 11 of the candidate path, all states sj \u2208 Sc for all j\u2208 {0,..., m}.\n4) By theorem 4, we have proved the claim 80, ..., Sm\u22121 \u2209 G.\n5) By theorem 3, we have proved the claim si+1 \u2208 d(si) for i \u2208 {0, ..., m \u2013 1}. Hence we proved the candidate path P (definition 11) is a solution path (defini- tion 9).\nTheorem 2. Given a CFG X = (Sc, SQ, I, 8), constructed from a run of algo- rithm 1, with goal set G, and s \u2208 Sc; is_counter factual(s, C, Q) will be TRUE if and only if s \u2208 G.\nProof. By the definition of the goal set G we have\nG = {s \u2208 Sc\\s \u00a3 SQ} (12)\nFor is_counter factual which takes as input the states, the set of causal rules C and the set of decision rules Q (Algorithm 3), we see that by from line 1 in algorithm 3, it returns TRUE if it satisfied all rules in C and no rules in Q. By the Definition 3, s \u2208 SQ if and only if it satisfies a rule in Q. Therefore, is_counter factual(s,C,Q) is TRUE if and only if s \u2209 S\u0105 and since s \u2208 Sc and s & SQ, then s \u2208 G.\nTheorem 3. Given a CFG X = (Sc, SQ, I, \u03b4), constructed from a run of algo- rithm 1 and a corresponding candidate path P = 80, \u2026\u2026\u2026, Sm; Si+1 \u2208 d(si) for i \u2208 {0, ..., m - 1}"}, {"title": "7.3 Experimental Setup", "content": "Dataset: Cars The Car Evaluation dataset provides information on car pur- chasing acceptability. We relabelled the Car Evaluation dataset to 'acceptable' and 'unacceptable' in order to generate the counterfactuals. We applied the CoGS"}]}