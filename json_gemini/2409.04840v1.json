{"title": "Sample- and Oracle-Efficient Reinforcement Learning for MDPs with Linearly-Realizable Value Functions", "authors": ["Zakaria Mhammedi"], "abstract": "Designing sample-efficient and computationally feasible reinforcement learning (RL) algorithms is particularly challenging in environments with large or infinite state and action spaces. In this paper, we advance this effort by presenting an efficient algorithm for Markov Decision Processes (MDPs) where the state-action value function of any policy is linear in a given feature map. This challenging setting can model environments with infinite states and actions, strictly generalizes classic linear MDPs, and currently lacks a computationally efficient algorithm under online access to the MDP. Specifically, we introduce a new RL algorithm that efficiently finds a near-optimal policy in this setting, using a number of episodes and calls to a cost-sensitive classification (CSC) oracle that are both polynomial in the problem parameters. Notably, our CSC oracle can be efficiently implemented when the feature dimension is constant, representing a clear improvement over state-of-the-art methods, which require solving non-convex problems with horizon-many variables and can incur computational costs that are exponential in the horizon.", "sections": [{"title": "1 Introduction", "content": "The field of reinforcement learning (RL) is advancing rapidly, making the development of sample-efficient algorithms increasingly important as the dimensionality of modern problems continues to grow. Traditional RL methods used in practice often lack sample complexity guarantees, meaning there are no bounds on the number of interactions with the environment needed to find a near-optimal policy. Designing RL algorithms with provable guarantees is particularly challenging, especially in applications involving large state and action spaces. While theoretical algorithms that offer such guarantees exist, they often rely on strong assumptions about the environment and are typically computationally infeasible for practical use. In this paper, we take a step toward bridging this gap by focusing on a classical environment assumption that has been widely studied in recent years in the RL context, yet has lacked computationally efficient algorithms.\nSpecifically, we consider MDPs where the state-action value functions of any policy are linear in some known feature map, which we refer to as linearly Q^{\\pi}-realizable MDPs (Lattimore et al., 2020; Du et al., 2020). This assumption is particularly interesting because it does not impose direct constraints on the dynamics of the MDP, unlike classic linear MDPs (Jin et al., 2020; Yang and Wang, 2019, 2020), where the transition operator is assumed to have a low-rank structure. However, with less structure to exploit, learning in linearly Q^{\\pi}-realizable MDPs is significantly more challenging. Consequently, these MDPs have primarily been studied in settings where a local simulator is available, allowing the learner to revisit previously encountered state-action pairs (Hao et al., 2022; Weisz et al., 2021a; Li et al., 2021; Yin et al., 2022; Weisz et al., 2022). While sample- and computationally efficient algorithms are known in the local simulator setting, it was not until recently that Weisz et al. (2024) developed a sample-efficient algorithm for linearly Q^{\\pi}-realizable MDPs without relying on a local simulator, though this approach is not computationally efficient. Weisz et al. (2024) left open the question of whether a computationally efficient algorithm could be developed for this setting.\nContributions. In this paper, we present an RL algorithm that finds a near-optimal policy in linearly Q^{\\pi}-realizable MDPs, using a number of episodes and calls to a cost-sensitive classification (CSC) oracle over policies that are both polynomial in the problem parameters. Additionally, we show that, due to the nature"}, {"title": "2 Setup and Preliminaries", "content": "In Section 2.1, we outline the reinforcement learning (RL) setting considered in this paper and introduce the notation used throughout. In Section 2.2, we present key structural results for linearly Q^{\\pi}-realizable MDPs, which are essential to our analysis. Finally, in Section 2.3, we describe our computational oracle and its properties."}, {"title": "2.1 Online Reinforcement Learning in Linearly Realizable MDPs", "content": "A Markov Decision Process (MDP) is a tuple M = (X, A, P, P_{init}, R, H), where X is a (large/potentially infinite) state space, A is the action space, which we assume is finite and abbreviate A = |A|, H \\in N is the horizon, R: X \u00d7 A \u2192 [0,1] is the reward function, P : X \u00d7 A \u2192 \u2206(X) is the transition distribution, and P_{init} \\in \u2206(X) is the initial state distribution. A (Markovian) policy \u03c0 is a map \u03c0 : X \u2192 \u2206(A); we use \u03a0 to denote the set of all such maps. When a policy is executed, it generates a trajectory (x_1,a_1,r_1),..., (x_h, a_h,r_h) via the process a_h ~ \u03c0(x_h), r_h ~ R(x_h, a_h), x_{h+1} ~ P(\u00b7 | x_h, a_h), initialized from x_1 ~ P_{init} (we use X_{H+1} to denote a terminal state with zero reward). We write P^\u03c0 [.] and E^\u03c0 [.] to denote the law and expectation under this process. We assume (following, e.g., (Jiang et al., 2017; Mhammedi et al., 2023; Weisz et al., 2024)) that the MDP M is layered so that X = X_1 \u222a\u2026 \u222a X_H for X_i \u2229 X_j = \u00d8 for all i \u2260 j, where X_h \u2286 X is the subset of states in X that are reachable at layer h \\in [H].\nThe goal in an MDP is to find a policy \u03c0 \\in \u03a0 such that\nJ(\u03c0^*) \u2013 E[J(\u03c0)] \u2264 \u03b5,\nwhere J(\u03c0) := E^\u03c0 [\\sum_{h=1}^{H} r_h] is expected reward of policy \u03c0, and \u03c0^* \u2208 arg max_{\u03c0 \\in \u03a0} J(\u03c0) is the optimal policy. Since the state space X can be very large or even infinite, achieving the goal in efficiently can be very challenging in general without any additional assumptions on the MDP. In this paper, we assume that the MDP is linear Q-realizable.\nLinearly Q-realizable MDPs. To present our main assumption, we need to define the state-action value functions (or Q-functions); for a policy \u03c0 \u2208 \u03a0, the Q-function Q_h^{\\pi} : X_h \u00d7 A \u2192 R at layer h \u2208 [H] is defined as\nQ_h^{\\pi}(x, \u03b1) := E^\u03c0 [\\sum_{l=h}^{H} r_e|x_h = x, a_h = a],\nfor all (x, a) \u2208 X_h \u00d7 A. Our main assumption in this paper asserts that the Q-functions are linear with respect to some known feature map \u03c6 : X \u00d7 A \u2192 R^d.\nAssumption 2.1 (Linear-Q^\u03c0). We assume that for all h \u2208 [H], \u03c0 \u2208 \u03a0, there exists \u03b8_h^{\\pi} \u2208 B_d(H) \u2286 R^d such that\n\u2200(x, \u03b1) \u2208 X \u00d7 A, Q_h^{\\pi}(x,a) = \u03c6(x,\u03b1)^T\u03b8_h^{\\pi}.\nFurthermore, the feature map \u03c6 is known to the learner and satisfies ||\u03c6(x,a)|| \u2264 1, for all (x, a) \u2208 X \u00d7 A.\nResults of this paper can easily be extended to the setting where the linear Q^\u03c0 assumption holds approximately; that is, when there is a \u03b5_{lin} > 0 such that for all h \u2208 [H] and \u03c0 \u2208 \u03a0, there exists \u03b8_h^{\\pi} \u2208 B_d(H) such that\n\u2200(x, \u03b1) \u2208 X_h \u00d7 A, ||Q_h^{\\pi}(x,a) \u2013 \u03c6(x,a)^T\u03b8_h^{\\pi}|| \u2264 \u0190_{lin}.\nHowever, to simplify the presentation, we focus on the exact linear Q^\u03c0 assumption in Assumption 2.1.\nWe refer to an MDP that satisfies Assumption 2.1 as linearly Q^\u03c0-realizable. Linearly Q^\u03c0-realizable MDPs are particularly interesting because, unlike many other common assumptions in the RL context, the assumption in does not directly impose any constraints on the dynamics (which are captured by the transition operator P) of the MDP. For instance, in classic linear MDPs, the transition operator P is typically assumed to have a low-rank structure.\nOnline Reinforcement Learning. To achieve the goal in, we consider the standard online reinforcement learning framework, where the learner/algorithm repeatedly interacts with an (unknown) MDP by executing a policy and observing the resulting trajectory, with the goal of maximizing the total reward. Formally,"}, {"title": "2.2 Range of States and Valid Preconditioning", "content": "In this section, we introduce key concepts and structural results for linearly Q^{\\pi}-realizable MDPs. Most of the results presented here are based on the works of (Weisz et al., 2024; Tkachuk et al., 2024). The proofs for these statements can be found in Appendix E.\nCentral to our analysis is the notation of range of states.\nDefinition 2.1 (Range of state). For h \u2208 [H], the range of a state x \u2208 X_h is defined as\nRg(x) := sup_{\u03b1, \u03b1'\\in A} sup_{\u03b8_h \\in \u0398_h} (\u03c6(x, \u03b1) \u2013 \u03c6(x, \u03b1'),\u03b8_h),\nwhere we recall that \u0398_h is the set of all parameter vectors corresponding to Q-functions; see ."}, {"title": "2.3 Benchmark Policies and Computational Oracle", "content": "In this subsection, we describe the computational Oracle our algorithm requires. For this, we need to introduce a class of benchmark policies.\nBenchmark policies. We consider the set Bench of benchmark policies such that \u03c0 \\in \u03a0_{Bench} if and only if there exist \u03b3 > 0, \u03c0',\u03c0'' \u2208 \u03a0_{Base} := {x \u2192 arg max_{a \u2208 A} \u03c6(x, a)^T\u03b8 | \u03b8 \u2208 B(H)} and \u03b8_1,...,\u03b8_{\\tilde{d}} \u2208 B(H) with \\tilde{d} = 4d log log d + 16 such that\n\u03c0(\u00b7) = I\\Bigg{\\langle \\underset{a,a'\u2208A,i\u2208[d]}{max} \\phi(\u22c5,a,a')^T \u03b8_i \\rangle \u2265 \u03b3\\Bigg}\u00b7\u03c0'(\u00b7) + I\\Bigg{\\langle \\underset{a,a'\u2208A,i\u2208[d]}{max} \\phi(\u22c5,a,a')^T \u03b8_i \\rangle < \u03b3\\Bigg}\u00b7\u03c0''(\u00b7),\nwhere \u03c6(\u00b7, \u03b1, \u03b1') := \u03c6(\u00b7, \u03b1) \u2013 \u03c6(\u00b7, a'). Note that from the definition of the design range in Definition 2.3, there exist \u03b8_1,..., \u03b8_{\\tilde{d}} \u2208 B(H) such that Rg^D(\u00b7) = max_{a,a'\\in A,i\u2208[d]} (\u03c6(\u00b7, \u03b1) \u2013 \u03c6(\u00b7, \u03b1'), \u03b8_i), and so we have that\n\u2200\u03c0', \u03c0'' \u2208 \u03a0_{Base}, \u2200 > 0, I{Rg^D(\u00b7) < \u03b3}\u00b7\u03c0'(\u00b7) + I{Rg^D(\u00b7) \u2265 \u03b3}\u00b7\u03c0''(\u00b7) \u2208 \u03a0_{Bench}.\nThe growth function of the policy class \u03a0_{Bench} at layer h \u2208 [H] is defined as\nG_h (Bench, n) := \\underset{(x^{(1)},...,x^{(n)})\u2208X_h}{sup} |{(\\pi(x^{(1)}),...,\\pi(x^{(n)})) |\\pi\u2208 \u03a0_{Bench}}|.\nThe growth function of \u03a0_{Bench} is defined as G(\u03a0_{Bench}, n) := max_{h\u2208[H]} G_h(\u03a0_{Bench}, n). The growth function is a key concept in the study of statistical generalization (Mohri et al., 2012). Since we will be using the benchmark class Bench to learn a good policy, it is essential to bound the growth function of Bench to ensure the sample efficiency of our algorithm. The next lemma shows that the logarithm of the growth function of Bench is polynomial in the problem parameters, which is sufficient to meet our sample efficiency requirements.\nLemma 2.8. For any n \u2208 N, the growth function G(\u03a0_{Bench}, n) is at most (92nA^2/d)^{(d+1)^2}.\nWe now describe our computational Oracle and how it uses the class Bench.\nComputational Oracle. Our algorithm requires a Cost-Sensitive Classification (CSC) Oracle over policies in \u03a0_{Bench} such that given any n\u2208N, h\u2208 [H], and tuples (c^{(1)}, x^{(1)}, a^{(1)}), ..., (c^{(n)}, x^{(n)}, a^{(n)}) \u2208 R \u00d7 X_h \u00d7 A, the Oracle returns\n\u03c0' \u2208 arg min_{\u03c0 \u2208 \u03a0_{Bench}} \\sum_{i=1}^{n} c^{(i)}\u00b7 I{\\pi(x^{(i)}) = a^{(i)}}."}, {"title": "3 Main Result: Guarantee of Optimistic-PSDP", "content": "The following is the main guarantee of Optimistic-PSDP (Algorithm 1):\nTheorem 3.1. Let \u03b5, \u03b4\u2208 (0,1) be given and consider a call to Optimistic-PSDP(\u03a0_{Bench},\u03b5,\u03b4) (Algorithm 1) with \u03a0_{Bench} as in Section 2.3. Then, with probability at least 1 \u2013 2\u03b4, we have\nJ(\u03c0^*) \u2013 J(\u03c0_{1:H}) \u2264 \u03b5,\nwhere \u03c0_{1:H} is the policy returned by Algorithm 1. Furthermore, the number of episodes Algorithm 1 uses is at most poly(A, d, 1/\u025b,log(1/\u03b4)) and the number of calls to the CSC Oracle in Section 2.3 is at most \\tilde{O}(d^2H^6).\nAs desired, the sample complexity of Optimistic-PSDP is polynomial in the problem parameters. However, we note that our approach incurs a poly(A) factor in the sample complexity compared to the non-computationally efficient method of Weisz et al. (2024), which is based on global optimism. It is unclear whether this factor can be eliminated when using a local optimism-based approach like ours.\nOracle complexity. As mentioned earlier, our algorithm requires calls to a CSC Oracle (see Section 2.3 for the Oracle's definition). As stated in Theorem 3.1, the number of Oracle calls made by Optimistic-PSDP does not depend on the desired suboptimality \u025b; crucial it does not grow with O(1/\u03b5).\nComputational complexity and practicality. We now revisit a few key points regarding the complexity and practicality of the CSC Oracle (see Section 2.3):\n\u2022 The policy optimization Oracle we require can be implemented efficiently when the feature dimension is constant (see Lemma 2.9).\n\u2022 While implementing the Oracle is NP-hard in general (for non-constant feature dimension), it can be reduced to binary classification, allowing the use of well-established machine learning algorithms (see Section 2.3).\nComparison to previous algorithms. We note that our result strictly improves upon those of Weisz et al. (2024) in terms of computational complexity. The algorithm of Weisz et al. (2024) relies on global optimism, which involves solving non-convex optimization problems in R^{dH}, leading to a computational complexity exponential the horizon in the worst case. In contrast, the computational complexity of Optimistic-PSDP is polynomial in the horizon, as reflected by Theorem 3.1 and Lemma 2.9.\nIt is also worth noting that certain algorithms based on global optimism such as OLIVE (Jiang et al., 2017) (which is similar to the algorithm in (Weisz et al., 2024)) are known to be incompatible with Oracle-efficient implementation for various common RL Oracles, including the CSC Oracle considered in this paper (see Dann et al. (2018)). Therefore, Theorem 3.1 separates these computationally intractable algorithms from our"}, {"title": "4 Algorithm, Intuition, and Challenges", "content": "In this section, we describe our algorithm, Optimistic-PSDP, offer some intuition behind its design (Section 4.1), and outline the challenges that motivated our approach (Section 4.2)."}, {"title": "4.1 High-Level Algorithm Description and Intuition", "content": "Our main algorithm, Optimistic-PSDP (Algorithm 1), builds on the classical Policy Search by Dynamic Programming (PSDP) algorithm (see, e.g., Bagnell et al. (2003)) by incorporating bonuses into the rewards. In a nutshell, the algorithm learns a policy in a dynamic programming fashion by fitting optimistic value functions for each layer h = H, ..., 1. It is well known in RL, that adding the right bonuses helps in driving exploration, which is what we use them for in our algorithm. Optimistic-PSDP consists of three subroutines: FitOptValue (Algorithm 2), DesignDir (Algorithm 4), and Evaluate (Algorithm 3). Note that Algorithm 2 and Algorithm 4 are simplified (asymptotic) versions of the full algorithms in Appendix C; Algorithm 5 and Algorithm 6, respectively.\nBefore delving into the specifics of Optimistic-PSDP, we first provide an overview of the key variables in Algorithm 1."}, {"title": "4.1.1 Key Variables in Optimistic-PSDP (Algorithm 1)", "content": "Optimistic-PSDP runs for T = \\tilde{O}(d) iterations, where at each iteration t \u2208 [T], the algorithm maintains the following variables:"}, {"title": "4.1.2 FitOptValue (Algorithm 2)", "content": "In each iteration t \u2208 [T], starting from h = H and progressing down to h = 1, Optimistic-PSDP invokes FitOptValue_h with the input (\\Psi_{h-1}^{(t)},\\pi_{h+1:H}^{(t)}, U_{h+1:H}^{(t)}, W_{h+1:H}^{(t)}), returning the pair ((\\theta_h^{(t)},w_h^{(t)}),W_{h+1:H}^{(t)}). The vectors w_h^{(t,h)} are then used in Line 5 of Algorithm 1 to update the preconditioning matrices (W_{1:H}^{(t)}). The FitOptValue_h subroutine ensures, with high probability, that (W_{1:H}^{(t)}) are valid preconditionings (see Lemma H.5 and recall the definition of a valid preconditioning in Definition 2.5). Consequently, using Lemma 2.7-which bounds the maximum length of a sequence of non-zero preconditioning vectors-it can be shown that w_{h}^{(t)} is non-zero only on \\tilde{O}(d) iterations; these are the iterations where the preconditioning matrix W_{1:H}^{(t)} actually changes (see Line 5). Intuitively, W_{1:H}^{(t)} does not update too frequently.\nOn iterations t where the preconditioning matrix is not updated (which is the case for most iterations as just argued), FitOptValue_h, ensures that (\\theta_h^{(t)},\\pi_h^{(t)}) is a good approximation of the optimistic value function Q_t in under trajectories generated using policies in \\Psi_{1}. More specifically, the subroutine FitOptValue_h guarantees that on most iterations t \u2208 [T]: for all (\\pi,v) \u2208 \\Psi_{1} and \u03c0 \\in \\Pi_{Bench} (see Lemma H.5),\n|E_{\\pi \\circ \\pi} [[I{\\langle \\phi(x_{h-1}, a_{h-1})^T v \\rangle \u2265 0} \u00b7 (Q_h^{(t)}(x_h, a_h) - Q_t(x_h, a_h))]| \u2264 O\\bigg(\\frac{\\epsilon}{\\delta^{7/4}H\\sqrt{d}}\\bigg),\nwhere Q_t(x, \u03b1) = \u03c6(x, a)^T\u03b8_h^{(t)} + b_h^{(t)} (x), for all (x, \u03b1) \u2208 X_h \u00d7 A, and b_h^{(t)} is as in. The bound in is a core result for the analysis of Optimistic-PSDP. Note that this bound is weaker than the typical least-squares error bounds in reinforcement learning, which bound the squared approximation error of the (optimistic) value function Q_h. However, this bound suffices for our purposes. In our setting, bounding the squared"}, {"title": "4.1.3 Evaluate (Algorithm 3)", "content": "Before introducing the second main component of Optimistic-PSDP, the DesignDir subroutine, we first describe the Evaluate subroutine in Algorithm 1. The Evaluate subroutine is used to evaluate the performance of the policy at iteration t by calculating the average sum of rewards across n_{traj} trajectories. This subroutine is invoked at the end of each iteration to evaluate the policy's performance. Optimistic-PSDP ultimately returns the best-performing policy after T rounds. Our analysis of Optimistic-PSDP relies on showing that for T = \u03a9(d), there will be at least one O(\u03b5)-optimal policy among (\\pi_{1:H}^{(t)})_{t\u2208[T]}."}, {"title": "4.1.4 DesignDir (Algorithm 4)", "content": "Optimistic-PSDP uses DesignDir to update the policy sets (\\Psi_{1}^{(t)}) and the design matrices (U_h^{(t)}). Notice that in , we bound the difference between Q_t^{(t)} and Q_h^{(t)} in expectation under trajectories generated using policies from \\Psi_{1}. Thus, it is desirable for the policies in \\Psi_{1} to have good coverage over the state space; informally, policies with good coverage would allow us to transfer the guarantee in to any other policy (i.e., perform a change of measure) with minimal cost. Taking inspiration from the classical linear MDP setting, one way we can measure the quality of the coverage of a set of policies {\\pi^{(1)},...,\\pi^{(m)}} is by looking at the \"diversity\" of the expected feature vectors they induce. For example, suppose the policies \\pi^{(1)}, ..., \\pi^{(m)}\ninduce a G-optimal design in the space of expected features (E^\u03c0[\u03c6(x_h, a_h)]|\u03c0 \u2208 \u03a0); that is, suppose that\n\\underset{i=1}{m} E_{\\pi^{(i)}}[\\underset{a\u2208 A}{max} \\left\\langle \u03c6(x_h, a),\u03b8 \\right\\rangle] \u2264 \u221a{2d}, \\text{ where } U=\\sum_{i=1}^{m}E_{\\pi^{(i)}}[\\phi(x_h,a)] E_{\\pi^{(i)}}[\\phi(x_h,a)]^T.\nSuch a set is guaranteed to exist for m = \\tilde{O}(d) (see e.g. Todd (2016)). In this case, for any vector 0 \u2208 R^d and any policy \u03c0 \u2208 \u03a0, we can perform the following change of measure (see proof of Lemma J.1):\n|E^{\u03c0} [(x_h, a_h)^T\u03b8]| \u2264\u221a{d} E^{\u03c0} [\\underset{i\u2208 [m]}{max} \\sqrt{{\\phi(x_h,a)}^T\\Sigma^{-1}{\\phi(x_h,a)}} ] \\left|E^{\\pi^{(i)}} [(x_h,a_h)^T\u03b8]\\right|,\n\u2264\u221a{2d}.\\sum_{i\u2208 [m]}E_{\\pi^{(i)}} [(x_h,a_h)^T\u03b8].\nThis implies that if |E^{\\pi^{(i)}} [(x_h, a_h)^T\u03b8]| is small for all i \u2208 [m], it will also be small for |E^{\u03c0} [(x_h, a_h)^T\u03b8]|,\nfor any \u03c0 \u2208 \u03a0. Thus, one might hope to construct a set \\Psi_{h-1}^{(t)} with policies \u03c0^{(1)},...,\u03c0^{(m)} satisfying , which would allow us to transfer the guarantee in from the policies in \\Psi_{h-1}^{(t)} to any other policy with minimal overhead. However, there are two challenges to achieving this:\n1. Although a set of policies satisfying always exists with m = \\tilde{O}(d), there is no straightforward way to compute such a set in our setting. Even in the much simpler linear MDP setting, finding such a set would require solving a non-convex optimization problem.\n2. Even if \\Psi_{h-1}^{(t)} consisted of policies satisfying , we would not necessarily be able to perform a change of measure in as we did in . This is because (Q_h^{(t)} - Q_t)(x, a) is not necessarily linear in the feature map \u03c6(x,a), as it would be in the standard linear MDP setting."}, {"title": "4.2 Challenge: Non-Linearity of Optimistic Value Functions", "content": "The second challenge described in Item 2\u2014the non-linearity of Q_h^{(t)} - Q_t\u2014is much more serious. We cannot perform a change of measure (as described in Section 4.1) unless the error in is linear in \u03c6; the analysis of Optimistic-PSDP (and essentially any other RL algorithm) requires performing a change of measure at some step. To understand how we can resolve this issue, we need to closely examine the step in the analysis that requires a change of measure. Similar to typical analyses of algorithms that employ local optimism, our approach involves showing via backward induction that for all l = H + 1, ..., 1, (x, \u03b1) \u2208 X_e \u00d7 A and \u03c0\\in Bench:\nQ_l^*(x,a) \u2264 Q_t(x,a),\nV_l^*(x) \u2264 V_t^{(T)}(x),\nIn the RL literature, it is known that on-policy guarantees are sufficient when employing optimism."}, {"title": "4.2.1 First Key Idea: Regaining Linearity by Going One Layer Back", "content": "The key to resolving this issue is to perform the change of measure in a slightly different way (which ultimately comes at the cost of requiring the CSC Oracle in the call to FitOptValue in Line 12). First, for the backward induction, we are now going to aim at showing that for all l = H + 1, . . ., 1, (x, \u03b1) \u2208 X_e \u00d7 A, and \u03c0\\in Bench:\nQ_l^*(x,a) \u2264 Q_t(x,a),\nV_l^*(x) \u2264 V_t^{(T)}(x) + (Q_l^{(T)} \u2013 Q_t)(x,\\pi(x)) + (Q_l^{(T)} \u2013 Q_t)(x,\\pi_t(x)).\nSuppose that and hold for all l \u2208 [h + 1 .. H], and we want to show that they hold for l = h. Once we show , will follow easily by a standard decomposition as we did in (see also the proof of Theorem 3.1). Let's examine how we can prove for l = h by using the fact that holds for l = h + 1. To do this, we will focus on the comparator policy \\tilde{\\pi} = \u03c0^* that satisfies:\n\\tilde{\\pi}(\u00b7) = I{Rg^D(\u00b7) \u2265 \u03b3}\u00b7 \u03c0^*(\u00b7) + I{Rg^D(\u00b7) < \u03b3}\u00b7\\pi_t(\u00b7),"}, {"title": "4.2.2 Second Key Idea: Skipping Over Low Range States", "content": "To deal with the issue that the bonus in is missing the indicator I{\\phi(\u00b7; W_l^{(t)})\\geq \u03bc}, we will use a special value decomposition (generalizing ) which essentially involves the value functions over different layers and treats low-range states in a special way. Again, we need to slightly modify the target inequalities for our induction; We modify and so that the goal is to show via backward induction over l = H + 1, ..., 1 that for all (x, \u03b1) \u2208 X_e \u00d7 A:\nQ_l^*(x,a) \u2264 Q_t(x, a) + b_l^{(t)} (x) \u00b7 I{\\Vert \\phi(x; W_l^{(t)}) \\Vert < \u03bc},\nV_l^*(x) \u2264 V_t^{(T)}(x) + (x,\\pi(x)) - (x,\\pi_t(x)) + b_l^{(t)} (x) \u00b7 I{\\Vert \\phi(x; W_l^{(t)}) \\Vert < \u03bc},\nwhere for (\\tilde{x},\\tilde{a}) \u2208 X_e \u00d7 A, \\xi(x,\\tilde{a}) := Q(x,\\tilde{a}) - Q(x,\\tilde{a}). We note that because of the term b_l^{(t)}(x)\u00b7 I{\\Vert \\phi(x; W_l^{(t)}) \\Vert < \u03bc}, which is not necessarily admissible (Definition 2.4), we cannot easily recover for l = h from with l = h + 1. Instead, we will show for l = h by leveraging for all l\u2208 [h + 1.. H] and the following \u201cskip-step\" decomposition (aimed at replace the steps in e.g. which we prove in Lemma M.8: for all (x, \u03b1) \u2208 X_h \u00d7 A:\nE[V_{l+1}^*(x_{l+1}) - V_{h+1}^{(T)}(x_{h+1}) | x_h = x, a_h = a] \\\\\n\u2264 E \\Bigg[\\sum_{l=h+1}^{H} \\Big(I{Rg^D(x_l) \u2265 \u03b3} \\underset{k=h+1}{\\overset{l-1}{\\Pi}} I{Rg^D(x_k) < \u03b3}\\Big) \\Big( \\xi^D(x_l, \\pi^*(x_l)) - \\xi^D(x_l, \\pi_t(x_l)) \\Big) \\Bigg| x_h=x, a_h=a\\Bigg]\\\\ + E \\Bigg[\\sum_{l=h+1}^{H} I{Rg^D(x_l) < \u03b3} \\underset{k=h+1}{\\overset{l-1}{\\Pi}} I{Rg^D(x_k) < \u03b3}\\I{||\\phi(x_l; W_l^{(t)})|| < \u03bc}b_l^{(t)}(x_l) \\Bigg| x_h=x, a_h=a\\Bigg]\nThe advantage of this decomposition, compared to say , is that it eliminates any inadmissible terms on the right-hand side. In fact, by Lemma 2.4 in Section 2.2 we have that all the terms on the right-hand side of are linear in (x,a), allowing us to perform a change of measure as in , which in turn enables us to prove for l = h.\nFinally, coming back to some earlier points from Section 4.1, the reason we needed for all l\u2208 [0.. h \u2212 1] instead of just l = 0 is precisely because we are using the skip-step decomposition in (see the proof sketch in Section 5 for more detail). Additionally, the policy optimization step in FitOptVal is needed because we are bounding the conditional expectation:\nE[(Q_{l}^{(T)} - Q_l^{(T)}(x_{h+1},\\pi^*(x_{h+1})) + (Q_{l}^{(T)} - Q_t(x_{h+1},\\pi_t(x_{h+1})) | | x_h=x, a_h=a];\nthat is, we are bounding the regression error \u201cone layer back\u201d (as reflected by the title of Section 4.2.1), and so we need to ensure that we measure the error for all policies \u03c0\\in \u03a0_{Bench}."}, {"title": "5 Proof Sketch of the Main Theorem (Theorem 3.1)", "content": "We now provide a proof sketch of Theorem 3.1, with the full proof deferred to Appendix J. We recommend that readers first review Section 4.1.\nLet \u03c4\u2208 [T] be an iteration such that for all h\u2208 [H], l \u2208 [0 .. h \u2212 1], and (\u03c0,\u03c5) \u03b5 \\Psi_h^{(\u03c4)},\n\u03b5\n|E_{\\pi \\circ \\pi} [[I{\\langle \u03c6(x_{h-1}, a_{h-1})^T v \\rangle \u2265 0} \u00b7 (Q_h^{(t)}(x_h, a_h) - Q_h^*(x_h, a_h))]| \u2264 \\frac{\\epsilon}{16H^2T^2d^2},\nand\n\\|E_{\\pi \\circ \\pi} [max_ \u03b1 \u03c6(x_h, a) || (BI+U_h^{(t)})^{-1}]| \u2264 2\u221a{d}.\nWe gave a high-level explanation of why such a \u03c4 exists in Section 4.1; see also the formal statements for these bounds in Lemma J.2 (the bound we display in is merely a slightly tighter version of the one we presented earlier in ). Further, define\n\\tilde{\\pi}(\u00b7) := I{Rg^D(\u00b7) < \u03b3}\u00b7 \u03c0_t^{(\u03c4)}(\u00b7) + I{Rg^D(\u00b7) \u2265 \u03b3}\u00b7\u03c0^*(\u00b7),"}, {"title": "6 Conclusion, Limitations, and Future Work", "content": "In this paper, we presented an algorithm for the linear Q-realizable setting that is both sample-efficient and makes a polynomial number of calls to a cost-sensitive classification oracle over policies. We also showed that this oracle can be implemented efficiently when the feature dimension is constant. The techniques we used to achieve our results may be of independent interest and could be applicable to other RL settings beyond linear function approximation.\nHowever, this work leaves open several important questions. First, it remains unclear whether a com- putationally efficient algorithm can be developed for non-constant feature dimension without relying on a computational oracle. Second, we have not yet determined whether the poly(A) factor in the sample complexity can be eliminated when using a local optimism-based approach like ours. Addressing these questions could lead to more broadly applicable and computationally feasible algorithms in reinforcement learning."}]}