{"title": "Condensed Sample-Guided Model Inversion for Knowledge Distillation", "authors": ["Kuluhan Binici", "Shivam Aggarwal", "Cihan Acar", "Nam Trung Pham", "Karianto Leman", "Gim Hee Lee", "Tulika Mitra"], "abstract": "Knowledge distillation (KD) is a key element in neural network compression that allows knowledge transfer from a pre-trained teacher model to a more compact student model. KD relies on access to the training dataset, which may not always be fully available due to privacy concerns or logistical issues related to the size of the data. To address this, \"data-free\" KD methods use synthetic data, generated through model inversion, to mimic the target data distribution. However, conventional model inversion methods are not designed to utilize supplementary information from the target dataset, and thus, cannot leverage it to improve performance, even when it is available. In this paper, we consider condensed samples, as a form of supplementary information, and introduce a method for using them to better approximate the target data distribution, thereby enhancing the KD performance. Our approach is versatile, evidenced by improvements of up to 11.4% in KD accuracy across various datasets and model inversion-based methods. Importantly, it remains effective even when using as few as one condensed sample per class, and can also enhance performance in few-shot scenarios where only limited real data samples are available.", "sections": [{"title": "Introduction", "content": "Knowledge distillation (KD) (Hinton, Vinyals, and Dean 2015) is an effective technique for reducing the resource demand of large pre-trained models. It functions by transferring the task knowledge of a large pre-trained model called the \"teacher\" to a more compact \u201cstudent\" architecture. One restrictive aspect of KD is its dependence on the training dataset of the target task. Such data might not be available in full scale, making distillation ineffective or even infeasible. For example, the training data could be privacy sensitive or large in memory size, aggravating its relocation. Addressing this issue, several works examine methods to perform knowledge distillation even when no sample from the training set is available at the compression stage. These are called data-free distillation techniques and are mainly based on generating synthetic samples to substitute the real training set (Lopes, Fenu, and Starner 2017; Yu et al. 2023; Liu et al. 2024). To generate informative samples that are aligned with the real data distribution, the information encapsulated in the teacher model is leveraged, which is called model inversion."}, {"title": "Related Work", "content": "Knowledge Distillation (KD) KD (Hinton, Vinyals, and Dean 2015) trains a compact \"student\" neural network model to approximate the decision space of a more complex one called the \u201cteacher\u201d. The inclusion of the soft guidance supplied by the teacher enriches the limited information the student receives from the one-hot encoded class labels. As a result, the student can achieve better performance than supervised training alone can provide. While most commonly, the logit scores or the softmax probabilities of the teacher are considered for regularization (Romero et al. 2014), activation maps or attention scores can also be used (Zagoruyko and Komodakis 2016a).\nFew-Shot KD In few-shot KD, only a small subset of real training samples are accessible. To avoid over-fitting, available methods typically reduce the number of parameters required to train the student network. (Bai et al. 2020) feeds activation of teacher and student networks to the layers of one another for cross-correction. FSKD (Li et al. 2020b) obtains the student architecture from the teacher itself via pruning, freezes it, and only learns 1x1 convolutions added after each layer. NetGraft (Shen et al. 2021) performs distillation one layer at a time by grafting the student's layer-to-be-trained to the teacher's architecture.\nModel Inversion for Data-Free KD If the training dataset is entirely inaccessible, conventional distillation methods can not operate. Data-free methods use the textit Model Inversion (MI) technique to infer data samples that the teacher model had observed during training and use them for distillation. Some early works consider the confidence of teachers' predictions as supervision for sample generation (Chen et al. 2019). Some others generate samples that maximize the information gain to the student (Micaelli and Storkey 2019). Following these, DeepInversion (Yin et al. 2020) proposed taking advantage of the batch normalization statistics gathered while training the teacher model. CMI (Fang et al. 2021) improved on this method by diversifying sample synthesis with the help of contrastive learning. Fast-Datafree (Fang et al. 2022) proposed a technique to reduce the significant amount of time that data-free distillation takes. PRE-DFKD (Binici et al. 2022) introduced a method to eliminate the trade-off between the large memory footprint and the robustness of the data-free KD process. Recent works have focused on further addressing scalability and efficacy issues (Yu et al. 2023; Liu et al. 2024).\nDataset Condensation Dataset condensation was introduced by (Zhao, Mopuri, and Bilen 2021) to reduce the training time required for large-scale datasets. It optimizes small batches of synthetic samples to carry almost equal information content as real batches of much larger size. The resulting samples are typically not visually realistic and can better protect data privacy than communicating real samples (Dong, Zhao, and Lyu 2022). To quantitatively assess the privacy benefits, (Zhou, Nezhadarya, and Ba 2022) exercised membership inference attacks (MIA) using condensed samples and showed they yield only around 0.52 attack AUC, which is almost the same value as random guessing,"}, {"title": "Condensed Sample-Guided Model Inversion", "content": "Model inversion techniques (Yin et al. 2020; Chen et al. 2019; Fang et al. 2021) typically use variants of the following loss function to guide the synthetic data generation.\n$L_{MI} = E_{x,y \\sim p_s} [y_{ps} \\log(\\hat{y}_T)] - E_x [D_{KL}(y_T||y_S)]$ (1)\nThe first term targets to maximize the softmax score ($\\hat{y}(T)$) (confidence) that the synthetic samples $\\hat{x}$ receive from the teacher T for the pseudo-classes ($y_{ps} \\sim u(0,c)$) assigned to them. Generally, $\\hat{x}$ is obtained using a generative model parameterized by $\\theta_g$ ($x \\sim p_{\\theta_g}(x|z)$). The last term encourages the synthesis of samples that provide high information gain to the student. As this objective is strictly guided by the knowledge that teacher embodies, supplementary information cannot be incorporated to provide additional guidance.\nFeature alignment mechanism: To incorporate the supplementary information about the training distribution in the model inversion objective, we add the constraint of aligning the feature distribution of generated synthetic samples with that of the condensed ones. This constraint is enforced through the inclusion of a feature discriminator in our model inversion framework. As illustrated in Figure 2, first, the generator outputs synthetic samples. Then, the teacher model encodes both these synthetic batches and condensed samples into feature representations. Later, discriminator classifies these features as condensed or synthetic samples. This classification results in a feature alignment loss that quantifies the gap between synthetic and condensed feature distributions. This can be viewed as a minimax game in which the feature discriminator competes against the generator for distinguishing real features from synthetic ones. In equilibrium, the generator will be able to provide samples that can yield similar features as the real ones to trick the discriminator. Essentially, the condensed samples serve as prototypes, guiding the generation of new samples that reflect the training distribution.\nThe final optimization objective for the generator is constructed by combining the feature alignment loss with the objective of any base model inversion method $L_{MI}$ as shown in Equation 2."}, {"title": null, "content": "$\\\\\\min L_G = L_{MI} + L_{FA}$ (2)\n$\\\\max L_D = E_{(x,y)}[\\log D(\\phi_l(x), y)] + E_{(\\hat{x}, y_{ps})}[1 - \\log D(\\phi_l(\\hat{x}), y_{ps})]$ (3)\nWhere $L_{FA} = E_{\\hat{x}} [1 - \\log D(\\Phi_l(\\hat{x}))]$ stands for feature alignment loss. As the dimension of feature vectors is high with respect to the limited availability of condensed samples, our method is prone to over-fitting. Therefore we use a simple discriminator architecture introduced by (Li et al. 2020a) that contains very few parameters. To further address the risk of over-fitting, we perform differentiable data augmentation (Zhao et al. 2020) on both the synthetic images output by the generator and the condensed samples before feeding them to the discriminator.\nIn deciding on the layer index at which the feature alignment will be employed, we considered the type of image features encoded by different parts of the teacher model. Typically, for image inputs, early layers of neural networks encode structural patterns that are commonly shared across natural images (e.g., edges). In contrast, the image features occurring at the later layers contain semantical information. As our objective is to produce diverse views of objects from the same semantical classes as the condensed samples, we considered features at late layers, specifically the penultimate layer, of the teacher as alignment targets.\nAdditionally, we posit that simply aligning the cumulative distribution of synthetic features from all classes with the real feature distribution is not ideal. Rather, we provide class-specific alignment using a conditional discriminator (Mirza and Osindero 2014). The contrast between these two alternatives can be seen in the figure given in the appendix. This further changes our discriminator objective to the following.\n$\\\\max L_D = E_{(x,y)}[\\log D(\\phi_l(x), y)]$\n$+ E_{(\\hat{x}, y_{ps})}[1 - \\log D(\\phi_l(\\hat{x}), y_{ps})]$ (3)\nHere, the discriminator not only predicts if a feature is associated with a condensed or synthetic sample but also determines the class it belongs to. To enforce this, we present three different types of inputs to the discriminator. First, we"}, {"title": null, "content": "construct \"real\" inputs by pairing real features with their labels. Later, we use the teacher to assign labels to the synthetic features and obtain \"fake\" inputs. Lastly, to prevent the discriminator from neglecting the class information, we construct additional \u201cfake\u201d inputs by pairing the same real features with the wrong class labels. Formally, our real (R) and fake (F) sets can be defined as,\n$R = {(\\hat{x},y)|(\\hat{x}, y) \\in X}$ (4)\n$F = {(\\hat{x}, y_{ps})} \\cup {(\\hat{x}, c)|(\\hat{x}, y) \\in X, c \\neq y}$\nCombining condensed and synthetic samples: After establishing our feature-alignment strategy to improve model inversion with the available data samples, we discuss how we can join condensed and generated synthetic samples for KD. Some alternatives included pre/post-training the student with condensed samples with respect to model inversion. However, as these methods can cause the student to be biased towards one data type, we avoided them. Instead, we extended the condensed dataset by adding the iteratively refined synthetic samples (through model inversion) and trained the student with randomly sampled batches from such union. Our distillation objective involves minimizing the distance between the predictions of the teacher and the student models, which can be summarized as,\n$\\theta_{S}^*:= arg\\\\ \\underset{\\theta_{S}}{min}  E_{\\hat{x}}[D_{KL}(\\\\\\hat{y}_{S}\\\\|\\hat{y}_{T})] + E_{x}[D_{KL}(y_{S}\\\\|y_{T})]$ (5)\nIn Equation 5, $\\hat{x}$ and x denote synthetic samples and condensed samples respectively. The exact procedure we follow in generating synthetic samples and distilling the student is summarised in Algorithm 1. First, we initialize our synthetic dataset X with the available condensed samples. Later at each epoch, we generate a new synthetic batch via our condensed sample-guided model inversion and add it to X. Later, within the same epoch, we randomly draw a data batch from X and use it to transfer knowledge from the teacher to the student."}, {"title": "Experimental Evaluation", "content": "To assess the effectiveness of our method, we incorporate condensed data guidance to the model inversion procedures of three state-of-the-art data-free KD methods and record the improvement. These methods are Fast, CMI, and PRE-DFKD. Moreover, we also experiment with using limited real samples for feature regularization and observe the advantage against few-shot KD methods. For this comparison, we selected NetGraft and FSKD as baselines. All the results we report on the performance of our baseline methods are either directly taken from the papers or obtained by running the official implementations based on the hyper-parameter configurations shared in the papers or GitHub pages. To standardize the evaluation, we use fixed random seeds borrowed from the official implementations of the baselines.\nDatasets We use three image classification datasets, which are CIFAR-10/100 (Krizhevsky and Hinton 2009), and ImageNet-200 (Deng et al. 2009). We conducted our experiments using condensed samples generated by three different methods, including those provided by (Zhao and Bilen 2021), (Cazenavette et al. 2022) and (Zhao and Bilen 2023). While the results reported throughout our experiments primarily utilize the condensed samples from (Zhao and Bilen 2021), we also include an ablation study to compare the effectiveness of each condensation method.\nImplementation details We used the same generator architectures as shared in the official implementations of the data-free methods that we couple our method with (Fast, CMI, and PRE-DKD). Further details on generator and discriminator architectures as well as how we couple our method with individual MI methods can be found in the appendix.\nCondensed-data guided sample synthesis\nTables 1 and 4 show the student accuracies upon coupling our approach with Fast, CMI, and PRE-DFKD. The results achieved by this coupling are indicated by the asterisk symbol with the annotation \"*(ours w/ CS)\". To eliminate the advantage of having access to condensed samples, we also configure baselines where we combine these samples with the synthetic datasets generated by model inversion methods. These baselines are denoted with \"+ CS\" notations. Further, the row \"Train w/ full real data\" represents the accuracy of full-scale training of students on the target dataset and constitutes the upper bound. \"Train w/ cond. samples\" reflects the accuracy achieved by only using condensed samples for student training, which is the lower bound. In all experiments, we assume 50 spc (500 samples) and 10 spc (1000 samples) are available for CIFAR-10 and CIFAR-100, respectively. For ImageNet-200, we consider the availability of 10 spc (2000 samples). These represent 1%, 2%, and 2% of the total samples in their respective datasets. First, we note that the performance of Fast method notably diminishes for pairs with low structural similarity (e.g. ResNet-34 (He et al. 2016) & MobileNet-v2 (Howard et al. 2017)). The improvement was especially significant for WRN-40-2 (Zagoruyko and Komodakis 2016b) & MobileNet-v2 pairs reaching up to 11.44% on CIFAR-100. CMI also has a considerable performance gap with respect to the upper bound for heterogeneous model pairs. Our method again achieves consistent advantage, with substantial accuracy improvements reaching up to 8.43% (WRN-40-2 & MobileNet-v2). Since PRE-DFKD achieves almost the same student accuracy as the upper limit for homogeneous pairs (e.g. ResNet-"}, {"title": "Conclusion", "content": "In conclusion, our study addresses the challenge of enhancing data-free knowledge distillation by incorporating supplementary information available from the target dataset-something traditional model inversion methods fail to leverage. Recognizing the potential of condensed samples to serve as this supplementary information, we developed a method that refines the synthetic data generation process within the model inversion framework. This integration helps align the synthetic data more closely with the real data distribution, effectively reducing the domain gap that often undermines KD performance. Our experiments demonstrate the effectiveness of this approach, showing accuracy improvements of up to 11.4% across various datasets and model configurations. The method is also applicable in environments where only a few real samples are accessible, such as in few-shot learning scenarios."}, {"title": "Appendix A: Computing Infrastructure", "content": "The specifications of the system used to run computational experiments are:\n\u2022 Ubuntu 22.04.4 LTS\n\u2022 NVIDIA A100 80GB GPUs\n\u2022 Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz"}, {"title": "Appendix B: Implementation Details", "content": "Generator architectures The Fast and CMI baselines share the same generator architecture, which is detailed in Table 6. The PRE-DFKD generator differs slightly, with the inclusion of spectral normalization following the convolution operations, as outlined in Table 7. In addition, the final activation function is hyperbolic tangent (Tanh), as opposed to sigmoid. Both tables utilize \"nz\" to represent the dimension of the random vector that is drawn from a standard normal distribution. In PRE-DFKD, the value of \"nz\" is set to 1000, while in Fast and CMI, it is set to 256. Lastly, the described architectures are cofigured to produce 32x32 RGB images (for CIFAR-10 and CIFAR-100). The activation shapes vary for datasets with samples of higher resolution (e.g. ImageNet-200).\nCoupling with model inversion methods Fast and CMI use only a single generator, so we simply attach our discriminator as shown in Figure 1(a) in the main paper. In addition, we notice that both methods reset the generator periodically by randomly re-initializing its weights. To prevent any instability in our adversarial feature regularization setup, we also reset the discriminator with the same frequency. We employ the Fast-2 configuration on CIFAR-10 and CIFAR-100 datasets, which update the generator two times for every update that the student receives. Meanwhile, for ImageNet-200, we adhere to the Fast-50 configuration, which is consistent with the original paper's approach for complex datasets. As the PRE-DFKD framework contains 2 generators, namely \u201cnovel\u201d and \u201cmemory\u201d generators, attaching our method requires further consideration. While the \"novel\" generator is responsible for producing new/informative samples in the framework, the memory generator memorizes the cumulative output distribution of the novel generator. As conditioning the novel generator also implicitly affects the memory generator, we only pair the discriminator with it. We run CMI, Fast, and PRE-DFKD for 200, 220, and 220 epochs, respectively. All results reflect the maximum student accuracy observed during distillation.\nMoreover, PRE-DFKD does not store generated samples and instead distills the student on-the-fly with newly generated samples at each epoch. This prevents us from directly applying Algorithm 1 to combine condensed and synthetic samples, and the other alternatives discussed in Section 3 impair student accuracy. For this reason, we only use the condensed samples to guide synthetic data generation and exclude them while training the student. Even without utilizing condensed samples in a direct way in distillation, we observe accuracy improvement on both datasets."}, {"title": "Appendix C: Condensed Datasets", "content": "The basic dataset condensation process uses a reference model and optimizes condensed samples to match the gradients that larger real batches produce for that model. In our experiments, we used condensed samples retrieved from the official GitHub pages of (Zhao, Mopuri, and Bilen 2021) and (Cazenavette et al. 2022). Both works, used the same 3-layer CNN architecture as the reference model to generate these datasets. For CIFAR-10, we used samples generated by"}, {"title": "Appendix D: Time complexity of model inversion", "content": "To show that our method does not sacrifice significant speed while enhancing the performance, we share the model inversion times in table 9."}, {"title": "Appendix E: Class-specific Feature Alignment", "content": "When the discriminator is configured to only differentiate between the generated synthetic samples and the condensed ones, the generator can achieve its optimization objective by merely aligning the two distributions, as demonstrated in Figure 6(a). However, for optimal results, it is preferable to align the synthetic samples with the condensed ones based on their respective classes, as shown in Figure 6(b). For instance, a synthetic car image should exhibit similar semantic features to a condensed car image rather than a condensed sunflower image."}]}