{"title": "Condensed Sample-Guided Model Inversion for Knowledge Distillation", "authors": ["Kuluhan Binici", "Shivam Aggarwal", "Cihan Acar", "Nam Trung Pham", "Karianto Leman", "Gim Hee Lee", "Tulika Mitra"], "abstract": "Knowledge distillation (KD) is a key element in neural network compression that allows knowledge transfer from a pre-trained teacher model to a more compact student model. KD relies on access to the training dataset, which may not always be fully available due to privacy concerns or logistical issues related to the size of the data. To address this, \"data-free\" KD methods use synthetic data, generated through model inversion, to mimic the target data distribution. However, conventional model inversion methods are not designed to utilize supplementary information from the target dataset, and thus, cannot leverage it to improve performance, even when it is available. In this paper, we consider condensed samples, as a form of supplementary information, and introduce a method for using them to better approximate the target data distribution, thereby enhancing the KD performance. Our approach is versatile, evidenced by improvements of up to 11.4% in KD accuracy across various datasets and model inversion-based methods. Importantly, it remains effective even when using as few as one condensed sample per class, and can also enhance performance in few-shot scenarios where only limited real data samples are available.", "sections": [{"title": "Introduction", "content": "Knowledge distillation (KD) (Hinton, Vinyals, and Dean 2015) is an effective technique for reducing the resource demand of large pre-trained models. It functions by transferring the task knowledge of a large pre-trained model called the \"teacher\" to a more compact \u201cstudent\" architecture. One restrictive aspect of KD is its dependence on the training dataset of the target task. Such data might not be available in full scale, making distillation ineffective or even infeasible. For example, the training data could be privacy sensitive or large in memory size, aggravating its relocation. Addressing this issue, several works examine methods to perform knowledge distillation even when no sample from the training set is available at the compression stage. These are called data-free distillation techniques and are mainly based on generating synthetic samples to substitute the real training set (Lopes, Fenu, and Starner 2017; Yu et al. 2023; Liu et al. 2024). To generate informative samples that are aligned with the real data distribution, the information encapsulated in the teacher model is leveraged, which is called model inversion.\nIn certain situations, some information from the target dataset can be communicated. However, as model inversion technique is primarily designed to facilitate knowledge distillation when no training data is available, even when such information is supplied, existing methods cannot leverage it for enhancing KD performance. In this work, we explore the potential of condensed samples (Zhao, Mopuri, and Bilen 2021) from target data as a form in which supplementary information can be available, and introduce a method to refine the synthetic data generation process in model inversion whenever they are available. Our method uses condensed sampled as prototypes to propagate new samples from the training distribution as illustrated in Figure 1. Condensed samples are synthetic samples that summarize large batches from target dataset. They provide certain privacy benefits, as studied in (Dong, Zhao, and Lyu 2022) and can be produced with modest memory and time resources through recent methods (Zhou, Nezhadarya, and Ba 2022; Zhao and Bilen 2023; Feng, Vedantam, and Kempe 2023). These qualities render condensed samples suitable for scenarios in which the privacy considerations prohibit exposure of individual training samples.\nA trivial approach is simply combining the condensed samples with those obtained by model inversion to distill the student. However, preliminary experiments suggest that this does not improve accuracy, which is likely caused by the domain gap between synthetic and condensed samples. With this observation, we propose using them to refine the"}, {"title": "Related Work", "content": "Knowledge Distillation (KD) KD (Hinton, Vinyals, and Dean 2015) trains a compact \"student\" neural network model to approximate the decision space of a more complex one called the \u201cteacher\u201d. The inclusion of the soft guidance supplied by the teacher enriches the limited information the student receives from the one-hot encoded class labels. As a result, the student can achieve better performance than supervised training alone can provide. While most commonly, the logit scores or the softmax probabilities of the teacher are considered for regularization (Romero et al. 2014), activation maps or attention scores can also be used (Zagoruyko and Komodakis 2016a).\nFew-Shot KD In few-shot KD, only a small subset of real training samples are accessible. To avoid over-fitting, available methods typically reduce the number of parameters required to train the student network. (Bai et al. 2020) feeds activation of teacher and student networks to the layers of one another for cross-correction. FSKD (Li et al. 2020b) obtains the student architecture from the teacher itself via pruning, freezes it, and only learns 1x1 convolutions added after each layer. NetGraft (Shen et al. 2021) performs distillation one layer at a time by grafting the student's layer-to-be-trained to the teacher's architecture.\nModel Inversion for Data-Free KD If the training dataset is entirely inaccessible, conventional distillation methods can not operate. Data-free methods use the textit Model Inversion (MI) technique to infer data samples that the teacher model had observed during training and use them for distillation. Some early works consider the confidence of teachers' predictions as supervision for sample generation (Chen et al. 2019). Some others generate samples that maximize the information gain to the student (Micaelli and Storkey 2019). Following these, DeepInversion (Yin et al. 2020) proposed taking advantage of the batch normalization statistics gathered while training the teacher model. CMI (Fang et al. 2021) improved on this method by diversifying sample synthesis with the help of contrastive learning. Fast-Datafree (Fang et al. 2022) proposed a technique to reduce the significant amount of time that data-free distillation takes. PRE-DFKD (Binici et al. 2022) introduced a method to eliminate the trade-off between the large memory footprint and the robustness of the data-free KD process. Recent works have focused on further addressing scalability and efficacy issues (Yu et al. 2023; Liu et al. 2024).\nDataset Condensation Dataset condensation was introduced by (Zhao, Mopuri, and Bilen 2021) to reduce the training time required for large-scale datasets. It optimizes small batches of synthetic samples to carry almost equal information content as real batches of much larger size. The resulting samples are typically not visually realistic and can better protect data privacy than communicating real samples (Dong, Zhao, and Lyu 2022). To quantitatively assess the privacy benefits, (Zhou, Nezhadarya, and Ba 2022) exercised membership inference attacks (MIA) using condensed samples and showed they yield only around 0.52 attack AUC, which is almost the same value as random guessing,"}, {"title": "Condensed Sample-Guided Model Inversion", "content": "Model inversion techniques (Yin et al. 2020; Chen et al. 2019; Fang et al. 2021) typically use variants of the following loss function to guide the synthetic data generation.\n$\\mathcal{L}_{MI} = \\mathbb{E}_{x,y \\sim p_s} [Y_{ps}log(\\hat{y}_T)] - \\mathbb{E}_{x} [D_{KL}(Y_T||Y_S)] $  (1)\nThe first term targets to maximize the softmax score ($\\hat{y}_{(T)}$) (confidence) that the synthetic samples $\\hat{x}$ receive from the teacher T for the pseudo-classes ($\\mathbb{Y}_{ps} \\sim u(0,c)$) assigned to them. Generally, $\\hat{x}$ is obtained using a generative model parameterized by $\\theta_g$ ($x \\sim p_{\\theta_g}(x|z)$). The last term encourages the synthesis of samples that provide high information gain to the student. As this objective is strictly guided by the knowledge that teacher embodies, supplementary information cannot be incorporated to provide additional guidance.\nFeature alignment mechanism: To incorporate the supplementary information about the training distribution in the model inversion objective, we add the constraint of aligning the feature distribution of generated synthetic samples with that of the condensed ones. This constraint is enforced through the inclusion of a feature discriminator in our model inversion framework. As illustrated in Figure 2, first, the generator outputs synthetic samples. Then, the teacher model encodes both these synthetic batches and condensed samples into feature representations. Later, discriminator classifies these features as condensed or synthetic samples. This classification results in a feature alignment loss that quantifies the gap between synthetic and condensed feature distributions. This can be viewed as a minimax game in which the feature discriminator competes against the generator for distinguishing real features from synthetic ones. In equilibrium, the generator will be able to provide samples that can yield similar features as the real ones to trick the discriminator. Essentially, the condensed samples serve as prototypes, guiding the generation of new samples that reflect the training distribution.\nThe final optimization objective for the generator is constructed by combining the feature alignment loss with the objective of any base model inversion method $\\mathcal{L}_{MI}$ as shown in Equation 2."}, {"title": "", "content": "$\\min \\mathcal{L}_{G} = \\mathcal{L}_{MI} + \\mathcal{L}_{FA}$  (2)\n$\\max \\mathcal{L}_{D} = \\mathbb{E}_{x} [log D(\\phi_l(x))] + \\mathbb{E}_{\\hat{x}} [1 - log D(\\phi_l(\\hat{x}))]$\nWhere $\\mathcal{L}_{FA} = \\mathbb{E}_{\\hat{x}} [1 - log D(\\Phi_l(\\hat{x}))]$ stands for feature alignment loss. As the dimension of feature vectors is high with respect to the limited availability of condensed samples, our method is prone to over-fitting. Therefore we use a simple discriminator architecture introduced by (Li et al. 2020a) that contains very few parameters. To further address the risk of over-fitting, we perform differentiable data augmentation (Zhao et al. 2020) on both the synthetic images output by the generator and the condensed samples before feeding them to the discriminator.\nIn deciding on the layer index at which the feature alignment will be employed, we considered the type of image features encoded by different parts of the teacher model. Typically, for image inputs, early layers of neural networks encode structural patterns that are commonly shared across natural images (e.g., edges). In contrast, the image features occurring at the later layers contain semantical information. As our objective is to produce diverse views of objects from the same semantical classes as the condensed samples, we considered features at late layers, specifically the penultimate layer, of the teacher as alignment targets.\nAdditionally, we posit that simply aligning the cumulative distribution of synthetic features from all classes with the real feature distribution is not ideal. Rather, we provide class-specific alignment using a conditional discriminator (Mirza and Osindero 2014). The contrast between these two alternatives can be seen in the figure given in the appendix. This further changes our discriminator objective to the following.\n$\\max \\mathcal{L}_{D} = \\mathbb{E}_{(x,y)} [log D(\\phi_l(x), y)] \\newline + \\mathbb{E}_{(\\hat{x}, Y_{ps})} [1 - log D(\\phi_l(\\hat{x}), Y_{ps})]$ (3)\nHere, the discriminator not only predicts if a feature is associated with a condensed or synthetic sample but also determines the class it belongs to. To enforce this, we present three different types of inputs to the discriminator. First, we"}, {"title": "", "content": "construct \"real\" inputs by pairing real features with their labels. Later, we use the teacher to assign labels to the synthetic features and obtain \"fake\" inputs. Lastly, to prevent the discriminator from neglecting the class information, we construct additional \u201cfake\u201d inputs by pairing the same real features with the wrong class labels. Formally, our real (R) and fake (F) sets can be defined as,\n$\\mathcal{R} = \\{(x,y)|(x, y) \\in X\\}$ (4)\n$\\mathcal{F} = \\{(\\hat{x}, Y_{ps})\\} \\cup \\{(\\hat{x}, c)|(x, y) \\in X, c \\neq y\\}$\nCombining condensed and synthetic samples: After establishing our feature-alignment strategy to improve model inversion with the available data samples, we discuss how we can join condensed and generated synthetic samples for KD. Some alternatives included pre/post-training the student with condensed samples with respect to model inversion. However, as these methods can cause the student to be biased towards one data type, we avoided them. Instead, we extended the condensed dataset by adding the iteratively refined synthetic samples (through model inversion) and trained the student with randomly sampled batches from such union. Our distillation objective involves minimizing the distance between the predictions of the teacher and the student models, which can be summarized as,\n$\\theta_s^{*}:= arg min \\mathbb{E}_\\hat{x} D_{KL}(\\hat{u}_S||\\hat{u}_T) + \\mathbb{E}_x D_{KL}(Y_S||Y_T)$ (5)\nIn Equation 5, $\\hat{x}$ and x denote synthetic samples and condensed samples respectively. The exact procedure we follow in generating synthetic samples and distilling the student is summarised in Algorithm 1. First, we initialize our synthetic dataset X with the available condensed samples. Later at each epoch, we generate a new synthetic batch via our condensed sample-guided model inversion and add it to X. Later, within the same epoch, we randomly draw a data batch from X and use it to transfer knowledge from the teacher to the student."}, {"title": "", "content": "repeat for number of epochs do\n   $\\hat{X}_{new}$ invert_model(T, S, G, D, X)\n  $\\mathbb{X} \\gets \\mathbb{X} \\cup \\hat{X}_{new}$\n   $(x, y) \\sim \\mathbb{X}$\n   $\\mathcal{L}_{KD} \\gets \\sum D_{KL}(\\hat{u}_S||\\hat{u}_T)$\n   optimizer.step(backward($\\mathcal{L}_{KD}), \\theta_s$)\nend for"}, {"title": "Experimental Evaluation", "content": "To assess the effectiveness of our method, we incorporate condensed data guidance to the model inversion procedures of three state-of-the-art data-free KD methods and record the improvement. These methods are Fast, CMI, and PRE-DFKD. Moreover, we also experiment with using limited real samples for feature regularization and observe the advantage against few-shot KD methods. For this comparison, we selected NetGraft and FSKD as baselines. All the results we report on the performance of our baseline methods are either directly taken from the papers or obtained by running the official implementations based on the hyper-parameter configurations shared in the papers or GitHub pages. To standardize the evaluation, we use fixed random seeds borrowed from the official implementations of the baselines.\nDatasets We use three image classification datasets, which are CIFAR-10/100 (Krizhevsky and Hinton 2009), and ImageNet-200 (Deng et al. 2009). We conducted our experiments using condensed samples generated by three different methods, including those provided by (Zhao and Bilen 2021), (Cazenavette et al. 2022) and (Zhao and Bilen 2023). While the results reported throughout our experiments primarily utilize the condensed samples from (Zhao and Bilen 2021), we also include an ablation study to compare the effectiveness of each condensation method.\nImplementation details We used the same generator architectures as shared in the official implementations of the data-free methods that we couple our method with (Fast, CMI, and PRE-DKD). Further details on generator and discriminator architectures as well as how we couple our method with individual MI methods can be found in the appendix.\nCondensed-data guided sample synthesis\nTables 1 and 4 show the student accuracies upon coupling our approach with Fast, CMI, and PRE-DFKD. The results achieved by this coupling are indicated by the asterisk symbol with the annotation \"*(ours w/ CS)\". To eliminate the advantage of having access to condensed samples, we also configure baselines where we combine these samples with the synthetic datasets generated by model inversion methods. These baselines are denoted with \"+ CS\" notations. Further, the row \"Train w/ full real data\" represents the accuracy of full-scale training of students on the target dataset and constitutes the upper bound. \"Train w/ cond. samples\" reflects the accuracy achieved by only using condensed samples for student training, which is the lower bound. In all experiments, we assume 50 spc (500 samples) and 10 spc (1000 samples) are available for CIFAR-10 and CIFAR-100, respectively. For ImageNet-200, we consider the availability of 10 spc (2000 samples). These represent 1%, 2%, and 2% of the total samples in their respective datasets. First, we note that the performance of Fast method notably diminishes for pairs with low structural similarity (e.g. ResNet-34 (He et al. 2016) & MobileNet-v2 (Howard et al. 2017)). The improvement was especially significant for WRN-40-2 (Zagoruyko and Komodakis 2016b) & MobileNet-v2 pairs reaching up to 11.44% on CIFAR-100. CMI also has a considerable performance gap with respect to the upper bound for heterogeneous model pairs. Our method again achieves consistent advantage, with substantial accuracy improvements reaching up to 8.43% (WRN-40-2 & MobileNet-v2). Since PRE-DFKD achieves almost the same student accuracy as the upper limit for homogeneous pairs (e.g. ResNet-"}, {"title": "", "content": "34 & ResNet-18), we only experiment with heterogeneous ones (e.g. ResNet-34 & MobileNet-v2), where there is still room for improvement. The results in Table 4 shows that our method effectively improves acuracy also for this baseline method.\nIn all experiments, the simple inclusion of condensed samples in the distillation set (\u201c+ CS\u201d) neither mitigated this issue nor caused any substantial performance improvement in most cases. On the other hand, our condensed sample-guided model inversion (\u201c*\u201d) consistently increased student accuracy across different datasets and teacher-student pairs, which ensures that the benefit of our approach is not simply due to exposing the student to condensed samples during training.\nVisual results We examine the impact of our method on the visual quality of the generated samples. Figure 4 contains synthetic CIFAR100 images obtained by CMI and CMI*. We note that CMI* samples are significantly more realistic and exhibit common class-distinctive patterns across images from the same categories, which is not observed in CMI. This strengthens the claim that feature alignment effectively conditions the synthetic data to contain realistic semantics that is consistent among samples from the same"}, {"title": "", "content": "classes. Additionally, this conditioning does not compromise the diversity of the synthetic set, as demonstrated by the varied object views and scales in CMI*. We also note that"}, {"title": "How does student accuracy scale with the condensed", "content": "data size? After establishing that our proposed method can boost performance using condensed samples, we analyze how the scale of the available data affects the improvement. For this, we consider condensed datasets of 3 different sizes (1 spc, 10 spc, 50 spc) for CIFAR-10 and 2 different sizes (1 spc, 10 spc) for CIFAR-100. These amounts correspond to 0.02%, 0.2% and 1% of the samples contained in CIFAR-10, and 0.2% and 2% for CIFAR-100. The plots in Figure 5 show that Fast and CMI baselines do not benefit from the mere inclusion of condensed samples in their synthetic distillation sets, irrespective of the number of samples available. However, when they are equipped with our feature alignment module, the student accuracies scale up with increasing data availability.\nDoes the condensation method affect the quality of model inversion? We study the impact of the dataset condensa-"}, {"title": "", "content": "tion method used to produce the condensed samples on the effectiveness of our approach and display the results in Table 2. DSA, DM and MTT refer to (Zhao and Bilen 2021), (Zhao and Bilen 2023), and (Cazenavette et al. 2022) respectively. The results indicate that while the choice of condensation method can impact the final student accuracy, our method consistently improves performance across all types of condensed samples tested. This suggests that its effectiveness is not dependent on any single condensation approach."}, {"title": "Real-data guided sample synthesis", "content": "Our method is also inherently capable of utilizing limited real samples from the target dataset when provided. To show this, we repeated the experiments in Table 4 by replacing the condensed samples with real ones. We assume the availability of the same amount of samples randomly drawn from the target datasets as we used in experiments with condensed data (50 spc for CIFAR10, 10 spc for CIFAR100 and 10 spc for ImageNet-200). The results are displayed in Table 4 as PRE-DFKD* (ours w/ RS). Similar to condensed sample experiments, the improvements observed for heterogeneous pairs are again greater than the homogeneous ones, i.e. ResNet-34 & ResNet-18. The overall improvement yielded by our method upon utilizing few real samples is comparable that using condensed ones. This highlights the effectiveness of our approach in few-shot KD scenarios.\nComparison with few-shot methods We benchmarked the effectiveness of our method in few-shot KD against FSKD and Netgraft baselines using the same experiment setups reported in their papers. To ensure a fair evaluation of our work against these few-shot baselines, we selected identical teacher-student pairs to those in the original papers. We"}, {"title": "Conclusion", "content": "In conclusion, our study addresses the challenge of enhancing data-free knowledge distillation by incorporating supplementary information available from the target dataset-something traditional model inversion methods fail to leverage. Recognizing the potential of condensed samples to serve as this supplementary information, we developed a method that refines the synthetic data generation process within the model inversion framework. This integration helps align the synthetic data more closely with the real data distribution, effectively reducing the domain gap that often undermines KD performance. Our experiments demonstrate the effectiveness of this approach, showing accuracy improvements of up to 11.4% across various datasets and model configurations. The method is also applicable in environments where only a few real samples are accessible, such as in few-shot learning scenarios."}]}