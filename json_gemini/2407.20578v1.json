{"title": "Comparison of Large Language Models for Generating Contextually Relevant Questions", "authors": ["Ivo LODOVICO MOLINA", "Valdemar \u0160V\u00c1BENSK\u00dd", "Tsubasa MINEMATSU", "Li CHEN", "Fumiya OKUBO", "Atsushi SHIMADA"], "abstract": "This study explores the effectiveness of Large Language Models (LLMs) for Automatic Question Generation in educational settings. Three LLMs are compared in their ability to create questions from university slide text without fine-tuning. Questions were obtained in a two-step pipeline: first, answer phrases were extracted from slides using Llama 2-Chat 13B; then, the three models generated questions for each answer. To analyze whether the questions would be suitable in educational applications for students, a survey was conducted with 46 students who evaluated a total of 246 questions across five metrics: clarity, relevance, difficulty, slide relation, and question-answer alignment. Results indicate that GPT-3.5 and Llama 2-Chat 13B outperform Flan T5 XXL by a small margin, particularly in terms of clarity and question-answer alignment. GPT-3.5 especially excels at tailoring questions to match the input answers. The contribution of this research is the analysis of the capacity of LLMs for Automatic Question Generation in education.", "sections": [{"title": "1 Introduction", "content": "The rapid growth of Large Language Models (LLMs) has revolutionized numerous sectors, including education [8]. LLMs can focus on and transform relevant parts of a given text, making them suitable also for Automatic Question Generation (QG). Generating educational questions is beneficial for teachers and students, as it supports comprehension, skill development, and assessment [6], [4]. Moreover, automating QG provides scalability and personalizes the learning path.\nProviding an appropriate context for QG is crucial in order to produce ques-tions that are relevant to the educational material. Before LLMs appeared, re-lated work [10], [5], [9] implemented context-specific QG models using well-known datasets. Other work focused on educational environments, such as in [4], where resources like school repositories, Wikipedia, or other websites provided context. However, the limitation of the past work is that the context was difficult to provide\nand remained static once set. With the recent advancements, LLMs can facilitate more flexible QG that is contextually relevant.\nIn university education, slide-based teaching materials are prevalent, and slide text contains valuable content usable for QG. Slide-based QG can be beneficial in numerous ways, for example, if a student has only the slide material available and needs to challenge their knowledge and understanding using an automated tool. In another case, an instructor can automatically generate stimulating questions based on their slides without any intermediate steps, emphasizing what they consider important for the subject. Depending on the purpose of the generated questions, a lot of previous work has been reviewed in [4]. However, no works were found using LLMs with slide text as context.\nThis paper evaluates and compares the effectiveness of different LLMs in gener-ating contextually relevant educational questions, using only slide text as context. We investigate the advantages and weaknesses of deploying three advanced LLMs: GPT-3.5 Turbo [1] by OpenAI, Flan T5 XXL [2], and Llama 2-Chat 13B [7]. For this purpose, we create the slide-based automatic question generator and subse-quently analyze the quality of the generated questions.\nThe challenge in utilizing slide text arises from the complexity of extracting meaningful answer phrases. The grammar, structure, and the amount of informa-tion in the slides can vary from slide to slide.\nThis paper focuses on the student perspective, because in our use case, students are the intended users of the generated questions. For example, students may use the questions for their knowledge reinforcement or diagnostic quizzes. Therefore, we pose the following research question: How do the three LLMs differ in terms of student evaluations of the generated questions?"}, {"title": "2 Research Methods", "content": "2.1 Answer Extraction/Generation (Step 1)\nA crucial step in our method is to obtain relevant concepts (called \"answers\" in this work) that are used to guide the QG later. From the three LLMs, only Llama 2-Chat 13B was always used for the answer generation. Its performance was very good: better than Flan T5 and about the same as GPT 3.5, which is, however, not free, so we decided to use the open model. The input for the LLM in this step is the corresponding prompt for this task and the context, which is the slide text from which the answer was extracted. It is important to separate the text used to obtain the answers, and the context that only gives more information to the model. The final output is a list of LLM-generated answers.\n2.2 Question Generation by the LLMs (Step 2)\nAll three compared models (GPT-3.5 Turbo, Flan T5 XXL, and Llama 2-Chat 13B) were used for this QG step. The input is the corresponding prompt for this task,"}, {"title": "2.3 Question Evaluation", "content": "For this study, we generated a total of 246 questions from 21 different slides from the course Pattern Recognition taught at Kyushu University. These slides were selected from 7 different units of the course because they had a sufficient amount of meaningful content, especially including the crucial concepts in the subject. The slides explain topics related to machine learning (in English language).\nWe sought to understand different aspects of the generated questions, the strengths and weaknesses of the LLMs, and the models' behavioral scenarios that commonly manifested during the QG. To achieve this, we examined five features of a question, which were based on the metrics defined in [3].\nClarity: evaluates how precise and easily understandable a question is. It should be clear and unambiguous to prevent student confusion.\nRelevance: assesses how closely a question-answer pair is tied to the subject unit's core topic. It measures how well a question-answer pair contributes to understanding the subject matter.\nDifficulty: evaluates how challenging it is to answer a question and how explicit the answer is in the context.\nSlide relation: evaluates how well a question-answer pair aligns with the content presented in the one specifically provided slide.\nQuestion-answer (QA) alignment: evaluates how accurate the provided answer is in relation to the question.\nTo analyze the generated questions, we conducted a survey with 46 under-graduate and graduate students of computer science degree programs at Kyushu University. The study participants were self-selected; they responded to a call for taking part in the research. There was reward for participation (gift cards).\nTo conduct the survey, we developed a custom web application in which the students can see the slides from the dataset in order to better understand the subject. The 21 slides were randomly distributed to the students to maintain a similar amount of evaluations for each slide. Some students evaluated more than one slide because not all the slides have the same amount of text, and the amount of extracted answers may differ as well.\nDuring the evaluation, the generated question and answer are presented, along with the five features and a scale from 1 (lowest score) to 5 (highest score) to"}, {"title": "3 Results and Discussion", "content": "A total of 246 different questions were rated by students, yielding 5339 evaluations. Between 18 and 24 students evaluated each question. The confidence level used for filtering the evaluations considerably modifies each distribution. When only the evaluations with higher confidence are analyzed, the distributions peak near the low or high ends of the scale. This indicates that the students felt more confi-dent about the evaluations when the question features were more evident to them. Moreover, with increasing confidence thresholds, the distributions across different metrics become very similar to each other.\nWhether the distributions differ was statistically analyzed using Kruskal-Wallis H-tests, with a p-value cut-off of 0.05. \n3.1 Analysis of the Five Individual Question Features\nOverall, the student ratings for the five evaluated metrics (clarity, relevance, diffi-culty, slide relation, and QA alignment) were very good.\n1. Concerning the generation of clear questions, both Llama 2-Chat 13B and GPT 3.5 Turbo performed well, with 80% of questions rated 4 or above. The Flan T5 XXL model encountered slightly more challenges, as some of its questions were rated lower for clarity by the students.\n2. In terms of relevance, all models had similar results. However, GPT 3.5 Turbo was slightly better.\n3. No major differences between the models were observed for question difficulty. GPT 3.5 Turbo tends to generate slightly easier questions, while Flan T5 XXL leans towards more difficult ones, making the overall range quite extensive.\n4. All LLMs scored similarly on slide relation with analogous results to the first two features; most output questions appropriately related to the slide content. However, GPT 3.5 Turbo again showed slightly better results.\n5. Interestingly, when it comes to the consonance between the generated questions and the provided answers, differences were more evident. Questions by GPT 3.5 Turbo aligned better with the answers compared to Llama 2-Chat 13B, and Flan T5 XXL presented the most issues in this aspect.\n3.2 Benefits and Limitations for Educational Applications\nIt is remarkable that even without fine-tuning, the models mostly scored high in the evaluations of the analyzed metrics. This indicates that the LLMs can be used for a satisfactory QG quickly and without specific knowledge about technical aspects of the models or specific training.\nHowever, the LLMs still have a few weaknesses, mainly in ensuring the align-ment between the output questions and the provided answers, and also in limiting bias towards certain types of questions (e.g., avoiding generic \"what is\u201d questions). Another constrain is that if the LLM output was to be integrated to educational software for practical deployment, so that the instructors would not have to inter-act with the LLM directly, the best-performing LLM is not freely available. Thus, the associated costs might limit the scalability of the solution.\nDespite these limitations, the evaluated LLMs are advisable for educational applications, especially the support of students' personalized learning. This includes knowledge reinforcement or quick diagnostic quizzes, where occasional problems"}, {"title": "4 Conclusion", "content": "This research evaluated three LLMs in facilitating QG for educational applications. We aimed to gain deeper understanding of how Generative AI can be used to produce contextually relevant questions using slide text as context. The proposed pipeline to extract the concepts or \"answers\" resulted in a unique and effective method to process slides and obtain more granular information for the question generation task. This method can be applied for other text-based educational ma-terials besides slides, such as textbooks and websites. The resulting questions may contribute to effective learning, inspire content creation, and allow for assessment and knowledge reinforcement.\nAll the evaluated LLMs demonstrated capability in QG, scoring high in clar-ity, relevance, and slide relation. The models performed well without fine-tuning, making them immediately applicable. Despite some limitations in answer align-ment and occasional biases that complicate question interpretation, these models have potential for educational applications. Nevertheless, for tasks requiring high precision and QA alignment, fine-tuning and further improvements are necessary."}]}