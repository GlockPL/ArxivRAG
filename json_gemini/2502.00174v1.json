{"title": "THE ROLE OF POSITIONAL ENCODINGS IN THE ARC BENCHMARK", "authors": ["Guilherme H. Bandeira Costa", "Miguel Freire", "Arlindo L. Oliveira"], "abstract": "The Abstraction and Reasoning Corpus challenges AI systems to perform abstract reasoning with minimal training data, a task intuitive for humans but demanding for machine learning models. Using CodeT5+ as a case study, we demonstrate how limitations in positional encoding hinder reasoning and impact performance. This work further examines the role of positional encoding across transformer architectures, highlighting its critical influence on models of varying sizes and configurations. Comparing several strategies, we find that while 2D positional encoding and Rotary Position Embedding offer competitive performance, 2D encoding excels in data-constrained scenarios, emphasizing its effectiveness for ARC tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "With the recent competition surrounding the Abstraction and Reasoning Corpus (ARC) challenge (Chollet et al., 2024), more participants have attempted to tackle its unique tasks, pushing the boundaries of what Artificial Intelligence (AI) systems can achieve. Designed by Chollet (2019), ARC serves as a benchmark for evaluating an AI system's ability to perform abstract reasoning by requiring models to transform input grids into output grids based on minimal training examples, each illustrating a specific reasoning process. Although these tasks are intuitive for humans, who can easily generalize from a few examples, they pose significant challenges for machine learning algorithms (Mitchell et al., 2023), which excel at curve fitting but struggle with reverse engineering diverse implicit rules.\nRecent advances, such as OpenAI's O3 model, have shown promise in the ARC benchmark (Chollet, 2024). However, these successes come at an immense computational cost, with O3 spending thousands of dollars to solve a single task. We posit that such performance could be achieved more efficiently by reconsidering key components of transformer-based models (Vaswani et al., 2017), particularly their positional encoding mechanism. If positional encoding was designed to better align with ARC's spatial reasoning requirements, similar performance might be attainable with far fewer resources.\nIn this work, we investigate the role of positional encoding in influencing the reasoning of a model in ARC tasks. Using a simple example, we show how CodeT5+ (Wang et al., 2023), a Large Language Model (LLM), fails due to limitations in its positional encoding mechanism. We further evaluate the impact of different positional encodings across various transformer architectures, demonstrating that effective encoding significantly enhances performance regardless of model size or structure. This is achieved by comparing the original positional encoding proposed in Vaswani et al. (2017) and its 2D extension (Wang & Liu, 2021), which incorporates the x and y positions of a token to calculate its encoding. Finally, we compare several positional encoding strategies, including 2D encodings, Rotary Position Embedding (RoPE) (Su et al., 2024), and Learned Embeddings, a trainable alternative to fixed encodings first introduced in Vaswani et al. (2017). Our results indicate that in data-constrained scenarios like ARC, 2D positional encoding outperforms other approaches,"}, {"title": "2 BACKGROUND", "content": "Positional Encoding. Incorporating positional information is essential for transformer models, enabling them to capture order and spatial relationships that would otherwise be missing in their inherently order-agnostic architectures. The original sinusoidal encoding introduced by Vaswani et al. (2017) uses fixed sine and cosine functions to provide smooth transitions across sequence lengths. Learned embeddings, a trainable alternative, adapt positional representations during training but often yield limited improvements.\nFor tasks involving grids, such as ARC, 2D sinusoidal encodings (Wang & Liu, 2021) extend the original concept by independently encoding horizontal and vertical positions, effectively capturing 2D spatial relationships. Relative positional encodings (Shaw et al., 2018) further enhance this by encoding token-to-token distances rather than absolute positions, excelling in sequences but proving less effective in grid-based structures. RoPE (Su et al., 2024) encodes relative positions by applying rotational transformations to query and key vectors, making it particularly effective in capturing long-range dependencies. Tailored approaches such as Abacus Embeddings (McLeish et al., 2024) demonstrate that task-specific encodings can surpass general-purpose methods in domains requiring precise positional awareness.\nRelated Work. Evidence provided by Li et al. (2024) demonstrates that a two-dimensional visual representation significantly enhances the reasoning performance of vision transformers, with positional information further improving their capabilities in spatially complex tasks. However, to the best of our knowledge, no specific experiments have evaluated the impact of positional encoding on ARC using the vanilla transformer architecture."}, {"title": "3 HOW DOES POSITIONAL ENCODING IMPACT LLM REASONING", "content": "Positional encoding is a fundamental component of transformer-based models such as CodeT5+, enabling them to process sequential data and understand the relationships between tokens. To demonstrate its influence, we utilize a simple ARC task whose goal involves connecting lines between pixels. Although this task presents the same reasoning challenges for humans regardless of whether it is presented in a horizontal or vertical orientation positional encoding can significantly influence the performance of an LLM in the two distinct views.\nTo demonstrate this, we used an ARC generator (Hodel, 2024) to create horizontal examples for the ARC task and rotated them 90 degrees to generate vertical examples, ensuring that they have the same level of difficulty. Using these examples, we trained two CodeT5+ models: one exclusively on horizontal examples and the other on vertical examples. Despite the identical structure of the task, the model performed considerably better on horizontal examples. Given that the attention mechanism in transformer-based models is designed to attend to all positions, this disparity suggests that the positional encoding used by CodeT5+ plays a critical role in shaping its ability to handle different spatial relationships.\nIn fact, upon analyzing the CodeT5+ positional encoding mechanism, we find that it employs relative positional encoding, which prioritizes tokens that are closer together in the sequence. This explains the observed disparity: In horizontal examples, the relevant tokens are naturally closer together, making it easier for the model to establish the necessary relationships. In contrast, in vertical examples, the relevant tokens are farther apart, spanning multiple rows, making it more difficult for the model to capture these relationships."}, {"title": "4 EXPERIMENTS", "content": "Since CodeT5+ was pre-trained with a deeply integrated positional encoding mechanism, modifying it to effectively investigate the impact of positional encoding posed significant challenges. To address this, we develop several custom transformer models from scratch, enabling a systematic evaluation of how different positional encoding strategies influence performance across various model architectures."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "We trained the models on 100,000 examples for each of 10 distinct ARC tasks independently to ensure that the results were not biased towards any single task. The evaluation is performed every two epochs, following the method proposed in Chollet (2019), where an answer is considered correct only if all pixels are in the correct position. The results are presented as aggregated averages across all tasks.\nThe experiments were designed to systematically assess the impact of different positional encoding strategies and input formats. First, we compared various model architectures using the original positional encoding proposed in Vaswani et al. (2017), which we refer to as 1D, with two distinct tokenization formats: raw and bracketed (see Appendix A.1). In the raw format, the grid is presented as a simple tokenized list without explicit spatial awareness. In contrast, the bracketed format introduces spatial awareness through the use of special bracket tokens that indicate the start and end of each row in the grid. These two input formats were then compared against models trained using the raw format combined with 2D positional encoding, referred to as 2D in our experiments, where spatial information is encoded explicitly at the positional encoding level rather than within the input.\nSubsequently, we evaluated the sample efficiency of 2D positional encoding against more modern approaches, such as RoPE and Learned Embeddings, using a single model architecture to ensure a consistent and fair comparison."}, {"title": "4.2 DIFFERENT ARCHITECTURES", "content": "To evaluate the robustness of positional encoding strategies across varying model configurations, we investigated their performance in different architectural setups. Specifically, we examined the impact of positional encoding on:\n\u2022 Model Size: Small, Medium, and Large transformers (see Appendix A.2 for architecture details).\n\u2022 Decoder-Only Architecture: A Medium model without encoder layers, focusing solely on the decoder's capacity.\nOur results reveal a consistent trend: positional encoding plays a crucial role in determining model performance across all architectures (see Figure 6). Among the encoding strategies tested, 2D positional encoding stands out, demonstrating a clear advantage over 1D encoding, irrespective of model size or architectural configuration. This superiority holds even in models with a single transformer layer, where 2D encoding achieves competitive performance against larger models equipped with less effective positional encoding. Notably, even in a decoder-only architecture, where the model lacks the encoder's ability to view the entire grid, 2D encoding still performs remarkably well, achieving around 90% accuracy."}, {"title": "4.3 MODERN APPROACHES", "content": "We provide a comparative analysis of several positional encoding strategies using a Medium model in two distinct scenarios: one with 100,000 training examples, where ROPE demonstrates a slight performance advantage over other methods (see Figure 7a), and another with 10,000 training examples, where 2D encoding consistently achieves the best performance (see Figure 7b). These results indicate that a clear and well-defined positional encoding has a direct impact on how effectively these models reason on ARC tasks. In particular, 2D positional encoding appears to be a more suitable choice given the limited data availability and the inability to generate additional examples during test time."}, {"title": "5 CONCLUSION", "content": "This work highlights the significant impact of positional encoding on the performance of transformer models in solving ARC tasks. Our findings suggest that while 2D positional encoding consistently outperforms other methods in data-constrained scenarios, it may not necessarily be the optimal choice if a robust dataset generator is available. However, it remains highly effective for training smaller models or achieving more efficient learning, making it a practical choice for scenarios with limited computational resources or data availability.\nOne limitation of this study is the treatment of ARC examples as independent, overlooking the reasoning often required between groups of related examples. Addressing this limitation could provide a deeper understanding of how positional encoding influences more complex relational reasoning. Future work should explore group-based reasoning while also expanding the analysis to include a broader range of tasks, building on the insights provided here to develop more effective positional encoding strategies for complex reasoning challenges."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 TOKENIZATION", "content": "For these experiments, we used both raw and bracketed tokenization formats.  Figure 8 illustrates how this mechanism works."}, {"title": "A.2 MODEL SIZE", "content": "During this work, we used several transformer architectures to systematically evaluate the impact of positional encoding across different configurations. The specifications for these architectures, including Feedforward layer size, number of layers, and attention heads, are summarized in Table 1. Additionally, all models were trained using consistent hyperparameters, such as batch size, learning rate, and vocabulary size, as detailed in Table 2. These configurations were chosen to ensure a fair comparison while maintaining computational efficiency."}, {"title": "A.3 DATASETS", "content": "For experiments involving CodeT5+, we trained exclusively on the task 22eb0ac0, restricting the grid sizes to 15 by 15. Each model was trained on 1,000 examples generated for this task. For the custom transformer models developed in this work, we utilized a broader set of tasks, including 22eb0ac0, 36d67576, 3aa6fb7a, 08ed6ac7, e8593010, e21d9049, 39e1d7f9, 913fb3ed, and 68b16354. To manage memory and efficiency constraints, we restricted the grid sizes such that the sum of the dimensions of an example did not exceed 40."}]}