{"title": "t-READi: Transformer-Powered Robust and Efficient Multimodal Inference for Autonomous Driving", "authors": ["Pengfei Hu", "Yuhang Qian", "Tianyue Zheng", "Ang Li", "Zhe Chen", "Yue Gao", "Xiuzhen Cheng", "Jun Luo"], "abstract": "Abstract\u2014Given the wide adoption of multimodal sensors (e.g., camera, lidar, radar) by autonomous vehicles (AVs), deep analytics to fuse their outputs for a robust perception become imperative. However, existing fusion methods often make two assumptions rarely holding in practice: i) similar data distributions for all inputs and ii) constant availability for all sensors. Because, for example, lidars have various resolutions and failures of radars may occur, such variability often results in significant performance degradation in fusion. To this end, we present t-READi, an adaptive inference system that accommodates the variability of multimodal sensory data and thus enables robust and efficient perception. t-READi identifies variation-sensitive yet structure-specific model parameters; it then adapts only these parameters while keeping the rest intact. t-READi also leverages a cross-modality contrastive learning method to compensate for the loss from missing modalities. Both functions are implemented to maintain compatibility with existing multimodal deep fusion methods. The extensive experiments evidently demonstrate that compared with the status quo approaches, t-READi not only improves the average inference accuracy by more than 6% but also reduces the inference latency by almost 15\u00d7 with the cost of only 5% extra memory overhead in the worst case under realistic data and modal variations.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous driving, with its worldwide developments [1], [2], [3], promises to achieve greater safety, less harmful emissions, increased lane capacity, and reduced travel time [4], [5], [6]. At the core of autonomous driving, the perception capability of autonomous vehicles (AVs) leverages the data collected from various sensors (e.g., camera, lidar, and radar) to better understand AVs' surrounding [7], [8], [9], thus enabling applications such as object detection [10], [11], [12] and semantic segmentation [13], [14], [15]. To power the perception capability, deep neural networks (DNN) are adopted to process and fuse the multimodal data [16], [17], [18] into a unified representation. Therefore, maintaining the effectiveness of these AV-DNN models is crucial to autonomous driving.\nThe lifecycle of an AV-DNN model starts with its design, then it gets pre-trained model by individual manufacturers, before being deployed to respective AVs equipped multimodal sensors for performing perception-related inferences. Although DNN-based multimodal fusion for on-device inference in general has attracted attention from both academia [19], [20], [21], [22] and industry (e.g., Google [23], as well as Intel and Ford [24]), only part of them have targeted multimodal DNN for autonomous driving (e.g., [25], [26]). Since this latter batch of proposals largely focuses on DNN architecture design at the manufacturer side, they often disregard the per-vehicle variations in terms of input data and modality, resulting in a missing link, pertaining to in-vehicle inference, between DNN models and their practical adoptions.\nOne critical issue with existing proposals is the fixed set of multimodal sensors and environment conditions used for model training. In practice, both sensor modalities and environmental conditions may experience drastic variations [27], [28], [29], [30]. For example, different lighting conditions may force the camera to change its exposure, different velocities can result in motion blurs to various degrees, and adverse weather conditions (e.g., rain, fog, and snow) often scatter laser light, thus forcing lidar to perform more intensive scans. Consequently, those two modalities introduce more drastic variations. Moreover, sensor occlusion and/or malfunction may lead to missing modalities that compromise the multimodal fusion. All these realistic scenarios yield variations in the input data and modality, as illustrated in Figure 1. They often significantly degrade the performance of well-designed DNN models, causing erroneous decision-making and even traffic accidents. Therefore, it is crucial to address adverse external conditions and internal sensor malfunctions carefully. As highlighted in the survey by [31], adverse weather conditions and faulty sensors are common contributors to autonomous vehicle accidents, such as the 2019 Tesla Model 3 incident [32]. A naive solution to mitigate performance degradation is to pre-train a large set of models for different scenarios and switch among them during runtime based on the available data and modality variations. However, this solution is not robust against continuously varying sensor and environment conditions since we may only have a countable number of models. Besides, model switching incurs excessive memory footprints and intolerable inference latency.\nWhereas, it is non-trivial to realize robust and efficient multimodal inference for autonomous driving. First of all, as robustness and efficiency are often at odds, designing a multimodal system that copes with variations without consuming excessive resources remains an uncharted area. Secondly, it's hard to strike an adaptive balance between these two objectives since it depends on continuously varying conditions. For instance, the latency requirements for DNN models in high-speed cruising and low-speed maneuvering scenarios are different because of the DNN responsiveness needed to match the vehicle speed. Last but not least, since DNN models embedded in vehicles are typically compiled low-level codes optimized for specific DNN architecture [25], [26], [33], it is largely impossible to modify the architecture in response to, for example, missing modalities and in turn their sensory data as part of the input.\nTo address these challenges, we propose t-READi (i.e., transformer-powered Robust and Efficient multimodal inference for Autonomous Driving) as a novel AV inference system; it adaptively accommodates the variations in multimodal sensory data and missing modalities. Specifically, t-READi employs a variation-aware model adaptation algorithm to handle data and modality variations under memory constraints while producing multiple variants of models deviated from the pre-trained model. For each variant, we exploit the parameter-efficient fine-tuning techniques of Large Language Models (LLMs) [34], but extend to widely-used modules in AV-DNN models in addition to transformer modules in LLMs [35], [36], [37]. For example, t-READi injects trainable rank decomposition matrices into residual blocks leading to a significantly decreased number of trainable parameters. With multiple variants of models and the pre-trained model, we can switch different variants according to current input indications, such as weather conditions.\nMoreover, t-READi leverages a contrastive learning framework to overcome the issue of missing modalities, while maintaining a comparable performance as using complete modalities. In particular, by contrasting the samples with and without a potential missing modality, t-READi drives the latent representation under missing modality towards semantically correlated with that under full modalities; this renders the DNN model robust to missing modalities without altering its architecture. Our key contributions can be summarized as follows:\n\u2022 We design t-READi to address the variations of sensory data and modality in autonomous driving so that t-READi adapts to various run-time environments with both robust and efficient manners.\n\u2022 We propose a variation-aware model adaptation algorithm to perform model adaptations with sparse parameter updates. The technique minimizes memory footprints, hence allowing for loading a large number of model variations into memory and in turn eliminating extra latency caused by model reloading.\n\u2022 We design a cross-modal contrastive learning method to compensate for input data loss due to missing modalities. It avoids runtime modifications on model architecture and thus preserves the overall efficiency.\n\u2022 We implement a prototype of t-READi and evaluate it with extensive experiments. The promising results confirm that t-READi indeed offers robust and efficient multimodal inference for autonomous driving.\nThough proposals on multimodal fusion for on-device inference do exist [19], [21], [24], t-READi is still the first to design such a system for autonomous driving with practical considerations, i.e., robustness and efficiency in driving environments. The rest of the paper is organized as follows. \u00a7 II introduces the background and motivation. The detailed system design and implementation are described in \u00a7 III and \u00a7 IV, respectively. \u00a7 V reports the evaluation results. Related works are presented in \u00a7 VI, along with limitations and future directions of t-READi. Finally, \u00a7 VIII concludes our paper."}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "In this section, we first investigate the impact of sensors' parameter changes (e.g., lidar and camera) on DNN inference performance. We then show how missing modalities significantly degrade the performance of DNN for autonomous driving. Finally, we explain why model reloading is not feasible for mitigating these negative impacts due to high inference latency and memory usage."}, {"title": "A. Sensor Parameter Variation", "content": "Most existing DNN-powered perception systems use a pre-trained model assuming that inference and training data follow the same probability distribution [38], [25], [26]. However, this assumption does not always hold true under AV scenarios: the camera of an AV may capture video with varying exposure and motion blur; the lidar of the AVs may retrieve data streams with a dynamic density of point cloud. Consequently, the parameter variation will result in a non-iid (independent and identically distributed) distribution that cannot be well handled by the AV-DNN model and hence lead to degraded performance.\nLidar is a vital sensor used in AVs due to its depth estima-tion capability. Currently, the automotive industry primarily uses rotating lidar sensors, which are powered by mechanical rotation [39]. For lidar sensors, the spinning rate refers to the number of revolutions in a second, and a higher spinning rate results in lower azimuth/elevation angular resolution. For example, when the spinning rate sweeps from 5 Hz to 20 Hz, the azimuth angular resolution deteriorates from 0.09\u00b0 to 0.36\u00b0. However, the pre-setting spinning rate cannot accommodate varying driving speed of the AV on run-time, resulting in a dynamic density of point cloud. The reason is shown in Figure 2(a) that lidar cannot catch point cloud timely, leading to depth difference in each scanning period (inverse of spinning rate). Figure 2(b) and Figure 2(c) demonstrate a road sign lies in front of the view, and the reflected lidar points are delaminated significantly, as the fixed spinning rate cannot catch different driving speeds of the AV.\nThe camera is another sensor widely used in AVs but much more affordable, which can provide richer semantic information than lidar. However, its performance can be significantly affected by surrounding environments. For example, a high or low intensity of light can cause cameras to malfunction. A common issue is the flare effect. Generally, a model's inference performance will degrade for unseen and bad environments with high possibility. As shown in Figure 2(d) and 2(e), even though scenarios of two images are similar, the pre-trained object detection model can work well on its training dataset in Figure 2(d), but fails to detect the target in Figure 2(e) due to flare effect.\nIn a nutshell, input variations of multiple modalities severely affect AV-DNN performance."}, {"title": "B. Missing Modality", "content": "In the realm of multimodal sensing data fusion, most existing research presumes the availability of all modalities during both the training and inference stages [40], [41]. However, this assumption does not always hold true, as sensors can malfunction or become obstructed during inference, leading to missing modalities, as demonstrated for the RGB-camera modality in [42]. Such failures pose significant challenges to DNN-based perception tasks in autonomous driving, primarily due to mismatches between the partial input data and the model architecture. A common solution to this problem is data imputation, which involves filling in missing modalities with predefined values, such as zeros [43], [44]. However, this approach introduces bias during inference, resulting in non-iid issues.\nWe demonstrate this in Figure 3 by showing the mAP (mean average precision) and NDS (details of the metrics will be introduced in \u00a7 IV) of BEVFusion [25] under both full and missing modalities of the nuScene Dataset [45] with zero-filling. The results reveal that BEVFusion inference with all modalities outperforms inference with missing camera data (filled with zeros) by over 15%. Moreover, training with missing lidar modality results in significantly lower mAP and NDS, due to the absence of rich geometric information provided by 3D lidar point cloud. These findings confirm that merely filling missing modalities with zeros falls short. They highlight the need for novel methods that can effectively harness the complementary information from multiple modalities, thereby reducing the performance gap between inference with full and missing modalities.\nHowever, designing an effective mechanism to compensate for missing modalities without additional computational overhead, while maintaining comparable performance to using complete modalities, is challenging. To address this, t-READi introduces a novel method that effectively utilizes complementary information from multiple modalities to narrow the performance gap between full and missing modalities. This method will be presented in \u00a7 III-C."}, {"title": "C. Model Reloading is Impractical", "content": "To address the performance degradation caused by environment variance, sensor parameter changes, and missing modalities, a straightforward solution is to pre-train a set of distinct DNN models for different data input variations and missing modalities. These models can be reloaded as needed in corresponding scenarios. However, changing environments can lead to time-varying parameter changes and missing modalities, requiring frequent model reloading to accommodate these variations. This places a heavy burden on AV embedded systems in terms of memory and latency, making this solution impractical. To illustrate the memory and latency demands imposed by frequent DNN model reloading, we present the memory and latency requirements with and without model reloading in Figure 4 using aforementioned pre-trained models.\nAs Figure 4(a) shows, model reloading from hard drives can increase the inference latency by up to 40% compared to not reloading the model. However, as reported in [46], the median driving reaction time in urban street is 370 ms, which makes hundreds of millisecond latency unacceptable indeed. In fact, if we can fit all pre-trained models into memory instead of reloading them from hard drives, the inference latency will be comparable to the case of not reloading. However, as Figure 4(b) demonstrates, each time the AV reloads the model, it requires an additional 160 MB of hard disk space (i.e., typical size of our trained model) to store the new model parameters compared to not reloading model. More importantly, the memory capacity is still limited (e.g., Tesla Model 3 has 8 GB RAM), and the memory needs to run multiple applications simultaneously.\nFurthermore, contemporary AV inference engines heavily rely on neural network processors, which use SRAM with capacities comparable to L3 cache at best. The prolonged latency and additional memory requirements are unacceptable for real-time, memory-constrained AV embedded systems. Thus, a more efficient solution for DNN model adaptation is needed to handle variations in input data and modalities. Although parameter-efficient fine-tuning methods [47], [34], [48] have succeeded in LLMs for adaptation, these strategies are not directly applicable to AV-DNN models. Adapting such fine-tuning approaches to address the specific data variation issues in AV-DNN models remains an unresolved challenge. AV-DNN models incorporate diverse modules, such as transformers, convolutional layers, and residual convolution blocks. The challenge lies in developing a versatile fine-tuning method that integrates seamlessly with these diverse structures. To this end, we will present t-READi in Section III and tackle the challenge in \u00a7 III-B."}, {"title": "D. Transformer for Autonomous Driving", "content": "DNN in computer vision has long been dominated by CNN (convolutional neural networks), and these architectures are enhanced with greater scale, more extensive connection, and more sophisticated form of convolution. Recently, the Transformer architecture [49] is adapted from NLP (natural language processing) to vision community [50], [51]. Vision transformer provides the capability to encode distant dependencies or heterogeneous interactions, which is crucial for autonomous driving scenario, and is qualified to be a powerful backbone as achieves better performance with similar complexity against convolutional-based backbone counterparts. The attention module, as a component of transformer, plays a critical role in modeling the interactive relation. Mathematically speaking, it is computed as:\n$Ax = XW_Q W_K X^T / \\sqrt{d}$  (1)\nWhere $X \\in \\mathbb{R}^{f \\times n}$ denotes f-dimensional n features, usually as the intermediate results translated from sensor data with encoders. $W_Q, W_K \\in \\mathbb{R}^{d \\times f}$ are feature projection matrices which project vectors to d-dimensional ones. Many attention modules split relative large projected dim d into pieces as known as \"multi-head attention\" to obtain effective performance, which implies $d \\ll n$.\nTransformer-powered DNNs are notoriously difficult to train from scratch, particularly in the presence of noisy data, often due to ill-conditioned attention modules. t-READi effectively tunes these AV-DNNs by addressing and correcting issues within the attention modules."}, {"title": "III. SYSTEM DESIGN", "content": "This section introduces the design of t-READi. First we give an overview, then we introduce the variation-aware model adaptation and cross-modal contrastive learning. Finally, we put everything together and summarize the training strategy."}, {"title": "A. Overview", "content": "Motivated by the observation in \u00a7 II, we design t-READi, a system consisting of two key components: i) a variation-aware model adaptation mechanism for efficient multimodal inference under memory and latency constraints, and ii) a cross-modal contrastive learning algorithm that addresses missing modalities and improves inference robustness. As shown in Figure 5, t-READi maintains compatibility with existing multimodal DNNs for AVs, while incorporating two aforementioned components that will be elaborated in \u00a7 III-B and \u00a7III-C, respectively. Given a multimodal DNN (e.g., BEVFusion) and sensory data (e.g., lidar and camera), the first component identifies, selects and injects the variation-aware model parameters. The second component compensates for information loss due to missing modalities.\nThe design of t-READi resolves three key technical challenges. (i) Adapting techniques from LLM to AV-DNN to manage data variations (\u00a7III-B), as introduced in \u00a7 II-C. (ii) Narrowing the performance gap between full and missing modalities (\u00a7III-C), as discussed in \u00a7 II-B. (iii) As most training samples are collected in a normal environment, we may only have limited types of data variations for fine-tuning the pre-trained model. However, the AV-DNN models always encounter unseen variations in reality, which may result in unexpected performance drop. Therefore, our final challenge is to expand the capability of fine-tuned models to deal with unseen variations (\u00a7III-D)."}, {"title": "B. Variation-Aware Model Adaptation", "content": "Existing parameter-efficient fine-tuning techniques [47], [34], [52], [48] are mostly designed for downstream tasks of LLMs, and how to effectively apply those approaches to AV-DNN models is barely studied. To investigate the effectiveness of the parameter-efficient fine-tuning method for AV-DNN models, we start with the models with the same transformer modules as LLMs, and then extend to other widely-used modules in AV-DNN models. Drawing inspirations from Bit-Fit [48], which focuses on fine-tuning lightweight inductive-bias terms only, we begin by tuning all the normalization layers and task-specific heads in the AV-DNN model. This foundational operation of t-READi is elaborated in \u00a7 V-H, and results demonstrate that it significantly outperforms the conventional practice of only fine-tuning task-specific heads.\na) Low-Rank Adaptation: Our exploration continues into the transformer module, where the attention mechanism is a critical component. We conduct an experiment by setting $n = 200, d = 16$ in Equation 1, calculating the cumulative sum of eigenvalues of Ax for each sample, and averaging them to gauge its rank. As Figure 6(a) shows, it is noticeable that even on clean data that the attention module is most familiar with, it already exhibits a low-rank instance as the top 3% eigenvalues account for more than 85% energy. This bias amplifies as sensor data distortion intensifies. In other words, the matrix Ax, inherently low-rank due to multi-head operations, degrades when processing distorted data. However, this is not a consequence of low-quality input data since comprehensive fine-tuning can substantially mitigate this effect. The observed rank collapse phenomenon was first recognized in NLP, specifically when fine-tuning the pre-trained model for distinct tasks. To overcome this, a Low-Rank Adaptation method [34] is proposed by of injecting rank-decomposition matrices to transformer blocks. Drawing on this concept, we design low-rank modules to fine-tune transformer blocks in AV-DNNs based on sensory data variations. Figure6(b) presents its detailed architecture. In general, the low-rank module comprises a pair of matrices A, B, which coexist parasitically for each inherent projection weight matrix. Each matrix has a low rank, bounded by a hyperparameter r, and is transparent due to their near-identity initialization. During finetuning, we keep the bulky pre-trained weights frozen, allowing only updates to these low-rank modules. As our objective is to restore the pre-trained matrix, and its rank is relatively low in optimal scenarios (as indicated in Figure 6(a)), we can efficiently bound r by a small integer. Additionally, the two distinct paths that are demonstrated in Figure 6(b) can operate in parallel, causing no distinctive latency overhead.\nb) Generalization to Non-Tranformer Modules: It is worth noting that not all AV-DNNs rely on transformers. To make our adaptation approach general and not specific to particular models, we take a closer look at the similarities and distinctions between transformer-heavy DNNs and others. Our key insight is that transformers or residual convolution blocks widely adopted cutting-edge AV-DNNs are deeply interconnected. For example, they both utilize the skip connection, significantly mitigates the rate of rank degeneration [53]. Furthermore, it has been demonstrated that a convolution-only network with a relatively large kernel size can achieve performance on par with its transformer-based counterparts [54], suggesting that the attention mechanism of transformers can be emulated with larger convolution kernels. Moreover, we find parallels in viewing the attention module as an ensemble of shallow networks, as studies of ResNet point out [55].\nBased on these insights, with the intention of reviving convolution kernels, we inject a module with a similar structure as the low-rank module into residual blocks, resulting in a different style of adaptation in the attention module [47]. This module, demonstrated in Figure 7, is placed before batch nor-malization layers with skip connection, a decision informed by a similar layer arrangement in transformers. We complement this setup with an activation layer, given the absence of an apparent low-rank attention matrix. By harmonizing the above three techniques, we ensure our adaptation method remains efficient and universally applicable."}, {"title": "C. Cross-Modal Contrastive Learning", "content": "Recall that the preliminary experiments in \u00a7II-B showed that the conventional data imputation methods, such as filling the missing modalities with zeros, incur information loss and even introduce biases into the DNN model. To compensate for the information loss caused by missing modality, we resort to representation learned from full modalities (e.g., camera and lidar) to guide uni-modal data (e.g., camera) towards a unified multimodal representation space. To this end, we design a cross-modal contrastive learning framework. The key idea behind the this framework is that even when some modalities are missing due to sensor occlusion or malfunction, the latent representation learned from missing modalities should be as similar as possible to the representation extracted from complete modalities. For example, if only one camera on an AV, which are equipped with multiple cameras and lidars, is missing, the inference performance can be preserved if the missing camera's latent representation can be compensated by the remaining sensors, which typically have overlapping fields of view (FoV).\nTo make the latent representations with and without missing modalities as similar as possible, follow work [56], [57], t-READi employs four components (i.e., data augmentation, feature extractor, projection head, and contrastive loss) in its contrastive learning framework, as shown in the right side of Figure 5. First, t-READi uses a stochastic data augmentation module to remove each sample of some modalities with a probability of 10%, resulting in one missing-modality dataset paired with the original full-modality one. The full-modality dataset and the one with missing modality are denoted as x and a(x), respectively. We consider $(x_i, a(x_i)) \\in \\{(x_i, a(x_i)) : i \\in |x|\\}$ as a positive pair, and $(x_i, a(x_j)) \\in \\{(x_i, a(x_j)) : i, j \\in |x|, \\text{and } i \\neq j\\}$ as a negative pair since xi and a(xj) describe different scenes. Then, t-READi leverages a neural network based encoder f(\u00b7) to extract representation vectors from a pair of samples from x and a(x). Our framework allows various choices of network architecture without any constraints, and we opt for simplicity, and reuse the feature extractor used in the object detection network. Thereafter, we use a multilayer perceptron with one hidden layer as g(\u00b7), which is used to map representations to the latent space where the contrastive loss is applied. Last but not least, we use a contrastive loss function, termed NT-Xent [58], i.e., the normalized temperature-scaled cross-entropy loss, to train the encoder f(.). The loss function enforces that the features between a positive pair as similar as possible, while enlarging the distance between the features of a negative pair.\nWe randomly sample a mini-batch of N samples from x and one sample from a(x), i.e., $a(x_i)$ and $x_i \\in \\{x_1,...,x_N\\}$ resulting in N + 1 samples. Given a positive pair $(x_i, a(x_i))$, we consider the other N \u2013 1 samples from x as negative samples with regard to $a(x_i)$. Then the contrastive loss for a positive pair $(x_i, a(x_i))$ can be formally expressed as:\n$L_i = - log \\frac{exp(sim(z_i, \\hat{z_i}) /\\tau)}{\\Sigma_{k=1}^{N}1[k \\neq i] exp (sim (z_k, \\hat{z_i}) /\\tau)}$  (2)\nwhere $z_i = g(f(x_i))$ is the projected feature for scene $x_i$ and $\\hat{z_j} = g(f(x(x_j)))$ is the projected feature for scene $x_j$ where some missing modality happens, sim(\u00b7) is a function that calculates cosine similarity of two latent representations, $1[k\\neq i] \\in \\{0,1\\}$ is an indicator function evaluating to 1 if k \u2260 i, and $\\tau$ denotes a temperature hyper-parameter, whose appropriate tuning can help the model learn from hard negatives as it controls the penalties on hard negative sample. The final loss is computed across all positive pairs in a N size mini-batch. Minimizing the contrastive fusion loss will force the projected features from the same scene but modality-missing conditions are different (i.e., $(z_i, \\hat{z_i})$ pair) together, while pushing projected features from different scenes (i.e., $(z_k, \\hat{z_i}), k\\neq i$ pairs) apart. A previously trained but not robust model is incorporated into the contrastive learning framework, during the subsequent adaptation phase, these additional components are removed. The contrastive learning framework approximately doubles the forward propagation time, however, we find that a relatively small number of epochs is sufficient, and it remains completely transparent during inference. Therefore, we consider it introduces an affordable computational cost."}, {"title": "D. Putting Everything Together", "content": "We summarize the training strategy of the t-READi, and present the overall workflow as follows. With the strategy presented in \u00a7III-C, t-READi re-trains the model under missing modality settings, allowing each sensor to fail independently. During the following tuning phase, which addresses various scenarios with variation, t-READi is constrained by two parameters, namely rank bound k and projection squeeze ratio r, as we have discussed their insights in \u00a7III-B above.\nIn reality, a cocktail of domain-specific variations muddles the situation, rendering the tuning of the entire domain space anything from unmanageable to impossible. Yet, by employing tuned parameters from individual variations, we squeeze the combined domain's tuning scope from M \u00d7 N to a more manageable M + N (respecting lidar and camera modalities). Furthermore, we find unseen domains like nighttime often share traits with seen domains, such as underexposure. We propose a conditional melding of two tuned variants $L_C$ and $L_l$: $(U_{i \\in NL_C'}) \\oplus (U_{j \\in NL_{l}'})$, where $\\oplus$ indicates a symmetric difference operation, we opt for a straightforward update. More complicated are the overlapping layers $(U_{i \\in NL_C'}) \\cap (U_{j \\in NL_{l}'})$, for which we choose interpolation using $\u03bb_C * P_C(t) + \u03bb_l * P_l(t)$, where t denotes overlapping layers and $P_{C,l}(t)$ represents the model-specific parameter set for t, with the additional constraint $\u03bb_C + \u03bb_l = 1$.\nWhile it's possible to overwrite the tuned layers, t-READI takes a different approach for the sake of efficiency. It simultaneously loads all the layers that are tuned under different variations, and further refines the less significant bits within the \"shared\" layers through pruning. These layers are organized as the values in a map. During inference, t-READi utilizes the information provided by various sensors (e.g., brightness) to encode the key that switches to the desired layer set. While a more self-contained method that only uses input data to switch parameters might be ideal, it can be more specific to certain modalities and less general. For example, it might be easier to implement for cameras but more challenging for lidar. Simple solutions like a lightweight filter are capable of rating images, mapping the ratings to keys, and selecting the tuned parameters to switch. However, it is not as straightforward when dealing with lidar data. The complexity of lidar data, with its 3D point cloud representation, makes it challenging to use the same methodology to rate, map, and switch parameters as easily as we do with images. Therefore, we consider the development of a more self-contained approach to be part of our future work. This would enhance t-READi's adaptability to different variations in sensory data and modalities."}, {"title": "IV. IMPLEMENTATION AND EXPERIMENT SETUP", "content": "In this section, we first present the details of t-READi's implementation, then we apply t-READi to develop two widely-used applications. Finally, we describe the metrics that we use to comprehensively evaluate the performance of t-READi."}, {"title": "A. Implementation", "content": "We implement the vehicle detection application on a server equipped with an Intel Xeon Gold 6226 CPU [59], 128 GB RAM, and NVIDIA GeForce RTX 3080 Ti GPU [60]. As for the software, Python 3.7 and PyTorch 1.9.1 [61] are used for implementing the application. Our object detection and segmentation model is built upon mmDetection [62], which is an open-source toolbox that provides state-of-the-art OD models. In particular, the model components and settings for t-READi are as follows:\n\u2022 The encoder f(.) consists of two modality-specific encoders. For the camera and lidar modalities, Swin Transformer-T [51] and VoxelNet [63] are used as the encoders, which is transformer intensive and residual block intensive respectively.\n\u2022 We choose to project both camera's and lidar's data to a unified bird's-eye view. For lidar, we flatten the sparse lidar features along the height dimension, hence not creating geometric distortion. For camera, we cast each camera feature pixel back into a ray in the 3D space, which can result in a feature map that retains full semantic information from the cameras.\n\u2022 Even though all sensory inputs are converted to a unified representation, the lidar features and camera features can still be spatially misaligned to some extent due to the inaccurate depth in the view transformer. To this end, we apply a fully convolutional encoder (with a few residual blocks) to compensate for such local misalignments.\n\u2022 In the tuning process, the AdamW optimizer, which employs the decoupled weight decay regularization [64] is used by setting a fixed learning rate of 5 \u00d7 10-5 and weight decay as 0.01. We also enabled gradient clip, which confines the L2 norm to be less than 35."}, {"title": "B. Tasks and Dataset", "content": "We perform two tasks to evaluate of t-READi, i.e., object detection and semantic segmentation, which are primary tasks for autonomous driving perception. We evaluate our scheme on two large-scale public datasets for autonomous driving: nuScenes [45] and DENSE [65]. Details of the two datasets are given in Table I.\n\u2022 nuScenes includes 1000 driving scenes under different weather and illumination conditions in Boston and Singapore, which are known for dense traffic and challenging driving situations. There are approximately 1.4M camera images and 390k lidar frames which are annotated with 1.4M accurate bounding boxes for 23 classes.\n\u2022 DENSE is captured during two test drives in February and December 2019 for two weeks, each under different weather (i.e., rain, snow, light/dense fog). There are approximately 104K camera images and 104K lidar frame which are annotated with 13.5K accurate bounding boxes."}, {"title": "C. Evaluation Metrics", "content": "To evaluate the performance of object detection, follow the previous works, we consider the widely used metric mean Average Precision (mAP), along with the specialized nuScenes detection score (NDS) tailored for the nuScenes. Since semantic segmentation can be considered as a pixel-wise classification task, we employ Intersection over Union (IoU) to measure the overlap between pixel set pairs of ground-truth and prediction.\n\u2022 Average Precision (AP): Our predictions consists of 4 categories: True Positive (TP), True Negative (TN), False Positive (FP) and False Negative (FN). Based on whether our prediction agrees with the corresponding ground truth (T/F) and the condition of our prediction (P/N). Precision (prec) is then calculated as $\\frac{|TP|}{|TP|+|FP|}$ to evaluate the likelihood of making false positive reports. Consequently, AP is defined as the integration over recall: $AP = \\int_0^1 prec(r) dr$.\n\u2022 mAP: mAP is calculated by averaging the AP values across different thresholds and categories. These thresholds are based on IoU typically (like DENSE). However in NuScenes, the thresholds are center-distance based represented by a set of thresholds D. The overall object category space, denoted as C, is heavily biased and consists of 10 categories. The mAP can be expressed as:\n$MAP = \\frac{\\sum_{c \\in C} \\sum_{d \\in D} AP_{c,d}}{|C||D|}$  (3)\n\u2022 NDS: NDS is designed to address the limitations of mAP in capturing all aspects of general detection tasks, such as vehicle velocity. To overcome these limitations, it decomposes the detection error into individual normalized metric components, such as translation, orientation, etc. All metrics are bounded between 0 and 1, with higher values indicating better performance. We refer readers to the original papers [45], [65] for more dataset-specific metric details."}, {"title": "V. EVALUATION", "content": "In this section, we evaluate t-READi under various sensory variation and modality-missing scenarios."}, {"title": "A. Benchmark", "content": "One primary motivation of t-READi is to accommodate various distortion inputs. We now summarize several common distortion types which prevail in daily driving conditions:\n\u2022 Fog-induced distortion. Fog affects lidar systems by distorting point clouds at short distances and reducing intensity information. We adopt the approach from [66"}]}