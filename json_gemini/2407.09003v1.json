{"title": "Enhancing Few-Shot Stock Trend Prediction with Large Language Models", "authors": ["Yiqi Deng", "Xingwei He", "Jiahao Hu", "Siu-Ming Yiu"], "abstract": "The goal of stock trend prediction is to forecast future market movements for informed investment decisions. Existing methods mostly focus on predicting stock trends with supervised models trained on extensive annotated data. However, human annotation can be resource-intensive and the annotated data are not readily available. Inspired by the impressive few-shot capability of Large Language Models (LLMs), we propose using LLMs in a few-shot setting to overcome the scarcity of labeled data and make prediction more feasible to investors. Previous works typically merge multiple financial news for predicting stock trends, causing two significant problems when using LLMs: (1) Merged news contains noise, and (2) it may exceed LLMs' input limits, leading to performance degradation. To overcome these issues, we propose a two-step method 'denoising-then-voting'. Specifically, we introduce an 'Irrelevant' category, and predict stock trends for individual news instead of merged news. Then we aggregate these predictions using majority voting. The proposed method offers two advantages: (1) Classifying noisy news as irrelevant removes its impact on the final prediction. (2) Predicting for individual news mitigates LLMs' input length limits. Our method achieves 66.59% accuracy in S&P 500, 62.17% in CSI-100, and 61.17% in HK stock prediction, outperforming the standard few-shot counterparts by around 7%, 4%, and 4%. Furthermore, our proposed method performs on par with state-of-the-art supervised methods.", "sections": [{"title": "1 Introduction", "content": "Stock trend prediction aims to forecast future movements as upward or downward for specific stocks or indexes. Accurate projections on stocks benefit investors or consultants in their trading activities like designing profitable trading strategies (Sawhney et al., 2021a,b), portfolio optimization (Du and Tanaka-Ishii, 2020; Niu et al., 2022), and risk management (Zhu et al., 2022). Typically, stock trend prediction entails the analysis of historical market data, such as price and volume. Financial news also plays an important role in influencing how markets move (Tetlock, 2007; Ashtiani and Raahmei, 2023), for instance, economic reports reflecting the financial health of one company, announcements of mergers and acquisitions, and declarations of leadership changes. In addition, by analyzing the sentiment, tone, and keywords in news texts, it is possible to identify patterns that may indicate future stock price movements.\nTo extract information from financial news for stock trend prediction, supervised methods have emerged as a prevalent choice (Yang et al., 2018; Chen et al., 2019; Li et al., 2021). However, supervised methods heavily rely on labeled data. Unfortunately, it is challenging to obtain labeled data due to the laborious and time-consuming annotation process, which hinders the practical applicability of supervised models. Recent works (Wei et al., 2022; He et al., 2023b) have highlighted the remarkable few-shot ability of large language models (LLMs) in various downstream NLP tasks. Motivated by this, we propose harnessing the power of LLMs, especially ChatGPT, to alleviate the need for labeled financial data and make prediction more feasible for investors in real-world financial practices.\nTo adopt LLMs in stock prediction, two problems arise if we follow the previous work (Huang et al., 2022; Deng et al., 2023) to merge multiple news as inputs and directly predict stock trends. First, the merged news contains noise, which may cause incorrect predictions. In Figure 1(A), features ('helped', 'starts', 'grows') from news (News #1, #4) that is poorly related to the trend can lead to an incorrect prediction of \u2018Up' trend. Previous supervised works (Ding et al., 2019; Huang et al., 2018) can effectively capture features from news, but it is challenging to discern whether news being analyzed is relevant to trends or not. Second, the merged news may exceed the maximum input length of LLMs. Simply truncating or shortening the merged news can result in a loss of crucial information about stock trends, leading to a degraded performance in the final prediction. To address these issues, we propose a two-step method \u2018denoising-then-voting' as shown in Figure 1(B). We first identify and filter out the irrelevant news through a denoising strategy. Then we aggregate all relevant news and predict stock trends via majority voting. The proposed \u2018denoising-then-voting' method allows us to focus on denoised news without losses of useful information relevant to the stock trend, which helps to improve prediction accuracy.\nTo summarize, our contributions are as follows: (1) We propose to forecast stock trends using LLMs which include ChatGPT and GPT-3 models under few-shot settings. This enables investors to forecast with minimal annotated financial data, making it more accessible to make timely precise decisions. (2) To facilitate few-shot stock trend prediction with LLMs, we further propose a two-step method \u2018denoising-then-voting' to reduce the adverse impact of noisy news and alleviate the input limitation of LLMs. (3) We conduct extensive experiments on three datasets including S&P 500 index, CSI-100 index, and HK stocks. Experimental results demonstrate the superior performance of our proposed method over few-shot LLM baselines. More importantly, when compared to state-of-the-art fully"}, {"title": "2 Preliminary", "content": "Following the previous work (Du and Tanaka-Ishii, 2020; Huang et al., 2022), we formulate stock trend prediction as a binary classification task. Given a target stock index s and a specific date k, we aim to predict price movements Yk \u2208 {Up, Down} for stock s on the k-th day based on the k - 1-th day's news information Dk\u22121, which contains Dk-1 pieces of news. The predicted labeled yk = Up indicates the adjusted closing price of stock s on the k-th day will be greater than that on the previous day (i.e., k \u2212 1-th day), while yk = Down represents the stock price will be less or equal than the previous day.\nRecent work (Brown et al., 2020; Wei et al., 2022) has shown that pre-trained LLMs are strong in-context learners in few-shot scenarios across various NLP tasks even without additional fine-tuning or gradient updates. To conduct in-context learning, LLMs are supplied with a prompt comprising natural language instructions and a small set of exemplars pertaining to the target task."}, {"title": "3 Approach", "content": "Figure 2 depicts an overview of three different prompts used by LLMs. Our proposed\u2018denoising-then-voting' method is outlined on the right. It contains two components: a denoising strategy and a majority voting strategy. The denoising strategy (please see \u00a73.2) reduces irrelevant news and the majority voting strategy (please see \u00a73.3) effectively aggregates relevant news for prediction. By comparison, we also set up a standard prompt on the left and a voting prompt without denoising in the middle. To predict stock trends, the standard way is to adopt a similar input manner as in supervised methods, which merge multiple news from the previous days and directly predict a final result for merged news. Therefore, in a standard prompt, each exemplar consists of financial news paired with a corresponding label of stock trend."}, {"title": "3.2 Denoising Strategy", "content": "As previously mentioned, features from noise would hinder making accurate decisions (Huang et al., 2022; Du et al., 2024). To filter out irrelevant news (i.e., reducing noise), we introduce an additional category named 'Irrelevant' and predict on each news individually instead of on the merged news. Specifically, LLMs are required to classify each piece of news as 'Irrelevant', 'Up', or 'Down', depending on whether it is unrelated to stock trends or contains relevant information that affects stock prices positively or negatively. By processing each news separately, we can effectively identify and exclude irrelevant news, ensuring it does not interfere with our final stock trend prediction."}, {"title": "3.3 Majority Voting Strategy", "content": "To utilize the remaining relevant news, one straight approach is to consolidate all these news into a single merged news and use LLMs to predict on the merged news. However, merging news may exceed the maximum token limits of LLMs, while cutting the combined news to fit the input token limits can lead to a loss of useful information. Furthermore, adopting this strategy necessitates using LLMs twice to obtain the final prediction \u2013 once during the denoising stage and once in the final prediction stage, which introduces additional costs and latency. To bypass these problems, we opt to reuse the predicted results of individual news in the denoising stage and obtain a final prediction through an effective and efficient majority voting mechanism. As illustrated in the right part of Figure 2, the number of news classified as \u2018Down' (Ndown) outweighs those classified as 'Up' (Nup) after excluding irrelevant news. Here we set a threshold \u03bb, when Ndown/(Nup+Ndown) > \u03bb, the final predicted stock trend is 'Down', otherwise it is 'Up'."}, {"title": "4 Experiments", "content": "We conduct experiments on three datasets: U.S. S&P 500 involves 553,666 financial news texts (i.e., news titles and articles) released by Ding et al. (2015). The news texts sourced from Bloomberg and Reuters are related to the U.S. stock market between 10/2006 to 11/2013. Following Huang et al. (2022), we collect historical prices of the Standard & Poor's 500 (S&P 500) index within the same period from Yahoo finance, and derive the U.S. S&P 500 dataset. CSI-100 & HK includes 90,361 financial news (i.e., news titles) of 78 A-share stocks in CSI 100 and 13 Hong Kong stocks over the period from January 2015 to December 2015. Following Huang et al. (2018), we separately collect prices of the CSI-100 index, HK stocks from Wind, and obtain the CSI-100 and HK dataset."}, {"title": "4.1.2 Evaluation Metrics.", "content": "In line with the previous work (Huang et al., 2018, 2022), we employ four established classification metrics: Precision (P), Recall (R), F1-score (F1), and Accuracy (Acc) to evaluate the performance of the stock trend prediction task."}, {"title": "4.1.3 Baselines.", "content": "We compare our proposed 'denoising-then-voting' method with the few-shot baselines and state-of-the-art supervised baselines. Few-Shot Baselines. We take the standard prompt that predicts stock trends for the merged news, and the voting prompt that predicts on each individual news instead of the merged news to vote. The 'Irrelevant' label (for denoising purpose) is not introduced in these baselines. Both few-shot baselines and our proposed method use the same LLMs for fair comparisons. Supervised Baselines. We include several state-of-the-art supervised baselines, which utilize the merged news as input to forecast stock trends. In S&P 500 evaluations, we compare models with different news representations. CNN+KB (Ding et al., 2016) uses event embeddings enhanced with knowledge bases and CNNs as the event sequence model. Later, LSTM-RGCN (Li et al., 2021) takes LSTM to encode news texts and Relational Graph Convolutional Network (RGCN) to learn node representations for each stock. The Noise Equity State representation of news (NES) in Huang et al. (2022) integrates representations of equity state and noise effects into their model. For CSI-100 & HK, we compare our proposed method with: TeSIA (Li et al., 2015), the tensor-based information framework in predicting stock movements, GDR+TeSIA (Li et al., 2016) and SMC+TeSIA (Huang et al., 2018), which reduces input dimensionality for TeSIA using Global Dimensionality-Reduction (GDR) and Sub-mode Coordinate (SMC), and SMC+LSTM (Huang et al., 2018), a variant of SMC+TeSIA that utilizes LSTM to predict stock trends."}, {"title": "4.1.4 Implementation Details.", "content": "For both the few-shot baselines and our proposed \u2018denoising-then-voting' method, we utilize ChatGPT (i.e., gpt-3.5-turbo-0301) as the backbone in-context model and set the temperature to 0. Following previous works (Huang et al., 2018, 2022), we leverage the title of news (i.e., news titles) to predict stock trends. Please refer to \u00a74.4.4 for the effect of different news inputs (e.g., news titles or articles). To set up our \u2018denoising-then-voting' method, we adopt a 9-shot (3 examples per class) prompt for U.S. S&P 500 dataset, and a 6-shot (2 examples per class) prompt for CSI and HK datasets. For the standard and voting prompts, we also use the same settings. (please refer to Appendix A for more details.) We choose threshold \u03bb that performs best in the validation set and evaluate it on the test set. Here, we take \u03bb on the three datasets."}, {"title": "4.2 Overall Performance", "content": "We show the overall results of our proposed method for S&P 500 index in Table 2, and CSI-100 index and HK stocks in Table 3. In Table 2, our method yields a prediction accuracy of 66.59% for U.S. S&P 500 forecasting, with a 6.84% enhancement over the ChatGPT with the standard prompt (i.e., 6-shot ChatGPT). Similarly, Table 3 shows that our approach achieves prediction accuracy of 62.17% and 61.17% for the CSI-100 and HK stock respectively, demonstrating 4% improvements compared to the standard prompt (i.e., 4-shot ChatGPT). In addition, our method obtains better performance compared with the voting prompt, with 3%-4% elevations in accuracy across three datasets. All improvements are statistically significant compared with the standard and voting few-shot methods, with p-values less than 0.05 under the paired t-test.\nThe major improvements of our proposed \u2018denoising-then-voting' method come from two aspects. The denoising strategy helps remove noise, enhancing performance by focusing on relevant information. Besides, the majority voting strategy effectively aggregates financial news. In Table 2 and 3, much more news can be utilized in prediction than the standard prompt. So, the majority voting strategy mitigates the loss of useful information about stock trends and further elevates accuracy.\nOverall, our proposed 'denoising-then-voting' method outperforms both standard and voting few-shot approaches on different datasets. Furthermore, it yields results comparable to the state-of-the-art supervised methods across three datasets, highlighting the effectiveness of our \u2018denoising-then-voting' method for stock trend prediction tasks under few-shot settings."}, {"title": "4.3 Ablation Study", "content": "To assess the effects of different components within our proposed method, in this section, we conduct an ablation study and obtain results in Table 4. Compared to the standard prompt, the majority voting strategy yields a significant improvement in accuracy (2.61%) on the S&P 500 dataset, by mitigating the loss of news information. However, the benefits of majority voting are more modest on the CSI-100 and HK datasets because these two datasets contain more irrelevant news. Therefore, we propose a denoising strategy by introducing an 'Irrelevant' label to distinguish noise from informative news, deriving the proposed 'denoising-then-voting' method. Table 4 demonstrates that the 'denoising-then-voting' method outperforms the voting prompt by a large margin, achieving an accuracy increase of 3%-4% on all three datasets. These improvements highlight the benefits of the denoising strategy, as it effectively identifies irrelevant news, and mitigates its adverse impacts on the final prediction. To summarize, both the denoising and the majority voting are indispensable to our proposed method for stock trend prediction."}, {"title": "4.4 Further Analysis", "content": "We further evaluate our 'denoising-then-voting' method in various experimental settings such as using different LLMs, inputting different number of news titles, etc. By default, we use ChatGPT and employees 60 news titles under 9-shot setting (3 examples per class) in S&P 500 prediction, 40 titles in 6-shot setting (2 examples per class) for CSI and HK predictions."}, {"title": "4.4.1 Performance on Different LLMs.", "content": "Besides ChatGPT, we experiment with three different sizes of GPT-3 models (Brown et al., 2020): GPT-3 Ada (350 million parameters), GPT-3 Babbage (3 billion parameters), and GPT-3 curie (13 billion parameters). As shown in Figure 3, the 'denoising-then-voting' method consistently outperforms the voting prompt across various LLMs on all three datasets, demonstrating the robustness and superior performance of our proposed approach."}, {"title": "4.4.2 Effects of the Number of Exemplars.", "content": "To assess the effect of the number of exemplars in our proposed \u2018denoising-then-voting' method, we experiment on prompts with various numbers of exemplars (i.e., k-shot in-context learning). As shown in Table 5, our proposed \u2018denoising-then-voting' method performs best under 9-shot (3 examples per class) setting in S&P 500 index prediction and 6-shot (2 examples per class) setting for CSI-100 and HK stock forecast. With few exemplars (e.g., 0-shot), LLMs are not provided with sufficient examples, which may limit their capability of capturing information. When given too many exemplars (12-shot for S&P 500, 9-shot for CSI and HK), however, the performance of our proposed method may become saturated or even decrease. Therefore, in this work, we adopt a 9-shot prompt for U.S. S&P 500 dataset and a 6-shot prompt for both CSI-100 and HK datasets."}, {"title": "4.4.3 Effects of the Number of News Titles.", "content": "We input different numbers of news titles in predicting S&P 500 movement for evaluation. As depicted in Figure 4, when inputting various numbers of news titles, our method exhibits a consistent enhancement compared to the standard and voting prompts. The standard prompt can only accommodate up to 40 news titles at one time for the input token limits of ChatGPT. In contrast, the voting prompt can accept much more news titles. Accordingly, the voting prompt outperforms the standard prompt by a large margin (63.23% vs. 59.75%). More importantly, our \u2018denoising-then-voting' method further significantly improves the accuracy (66.59% vs. 63.23%) over the voting prompt, showing the effectiveness of denoising strategy."}, {"title": "4.4.4 Effects of Inputting Titles or Articles.", "content": "Typically, a complete news article is quite lengthy. As news titles highly summarize the news articles, in this paper, we mostly use news titles as input to predict trends. In this section, we explore the effects of using news titles or articles as input for S&P 500 forecast. Given that inputting a complete news article into LLMs would be costly, we propose four methods to condense a news article into a 100-token summary for prediction. Article-first-100, Article-middle-100, and Article-last-100 respectively represent taking the first, the middle, and the last 100 tokens of the news article as the summary. Article-summary-100 refers to compressing the given news article into a summary of 100 tokens using ChatGPT. The average length of each news title is around 10 tokens. We also use Article-first-10, which takes the first 10 tokens of the news article as the summary, to compare with using 10-token news titles as input.\nAs shown in the first block of Table 7, for the standard prompt, inputting with the first 100 tokens (Article-first-100) performs best than those inputting with the middle 100 tokens, the last 100 tokens and the summary generated by ChatGPT. That is because news typically has lead bias (Zhang et al., 2020; Zhu et al., 2021), where the early parts of an article often contain the most salient information. In contrast, using the article's last 100 tokens as input performs the worst, indicating that the last 100 tokens contain less useful information.\nFor 'denoising-then-voting', we find that employing the first 10 tokens (Article-first-10) as input performs worse than using news titles for stock trend prediction, which clearly demonstrates that news titles are highly condensed and high-quality summaries. Conversely, using the first 100 tokens (Article-first-100) as input brings a significant improvement to the proposed 'denoising-then-voting' over using the news title as input. Since Article-first-100 takes much longer inputs than news titles, it includes more useful information for stock trend prediction. It is worth noting that inputting a lengthier summary also incurs higher API usage costs, which is often prohibitive in practice. Therefore, using news titles as input strikes a good balance between performance and cost efficiency."}, {"title": "4.5 Case Study", "content": "We present two cases of stock trend prediction in Table 6. In case #1, both the standard and voting prompts fail to identify noise (News 1, 3, 4), thus resulting in an incorrect final prediction. In contrast, our proposed method can exclude irrelevant news (News 1, 3, 4) and mitigate the adverse impacts of noise, leading to a correct final prediction based on the valuable information (News 2). Case #1 highlights the denoising strategy. For case #2, both voting and \u2018denoising-then-voting' methods correctly predict the stock trends, while the standard prompt does not. To fit the input limits of LLMs, meaningful information (News 3 and 4) is excluded when truncating the merged news, leading to a wrong prediction with the standard prompt. Yet, the majority voting strategy allows us to effectively aggregate more news for prediction analysis. Therefore, it can include more information relevant to stock trends (News 3 and 4), elucidating the effectiveness of the majority voting strategy."}, {"title": "5 Related Work", "content": "News-based Stock Prediction investigates the relationship between news data and stock market fluctuations. To capture meaningful news information, various efficient news representations are explored, e.g., tensor-based method (Huang et al., 2018), stock embeddings (Du and Tanaka-Ishii, 2020), etc. In addition, deep learning methods (Hu et al., 2021; Zhao and Yang, 2023) have been developed to extract relevant patterns within financial data. Trained with extensive annotated data, these works accumulate multiple news together to predict their influence on the future market. However, the labeled data is not always available. Instead, LLMs show remarkable few-shot capability while merging multiple news as input will cause input overflow of LLMs and introduce significant noise, harming prediction results. To handle these issues, this paper proposes a \u2018denoising-then-voting' method and predicts trends with LLMs under a few-shot setting.\nLarge Language Models (LLMs) have demonstrated remarkable capability in various text generation tasks, such as machine translation (Jiao et al., 2023), question-answering (Lin et al., 2022) and factual error correction (He et al., 2023a, 2024). In financial domains, the applications of LLMs still remain relatively untapped. Wu et al. (2023) proposes BloombergGPT, a domain-specific LLM for finance. Lopez-Lira and Tang (2023) scores news headlines using ChatGPT and conducts different trading strategies based on the scores to predict stock returns. Different from prior studies, this work investigates various few-shot methods. We contribute to alleviating the noise and input limitations with LLMs when processing thousands of pieces of news in stock trend prediction."}, {"title": "6 Conclusion", "content": "In this work, we propose to use large language models (LLMs) such as ChatGPT and GPT-3 in stock prediction with financial news. It addresses the issue of sparse data annotation in supervised learning. More importantly, the proposed two-step method 'denoising-then-voting' effectively deals with noisy news and alleviates the constraints of LLMs in handling large input news streams. The results show significant improvements compared with few-shot baselines and are highly comparable to the state-of-the-art fully supervised methods. This study demonstrates the potential feasibility of adopting LLMs in financial forecasting with our proposed method, paving the way for truly enabling LLM applications in real-world trading."}]}