{"title": "V-VIPE: Variational View Invariant Pose Embedding", "authors": ["Mara Levy", "Abhinav Shrivastava"], "abstract": "Learning to represent three dimensional (3D) human pose given a two dimensional (2D) image of a person, is a challenging problem. In order to make the problem less ambiguous it has become common practice to estimate 3D pose in the camera coordinate space. However, this makes the task of comparing two 3D poses difficult. In this paper, we address this challenge by separating the problem of estimating 3D pose from 2D images into two steps. We use a variational autoencoder (VAE) to find an embedding that represents 3D poses in canonical coordinate space. We refer to this embedding as variational view-invariant pose embedding (V-VIPE). Using V-VIPE we can encode 2D and 3D poses and use the embedding for downstream tasks, like retrieval and classification. We can estimate 3D poses from these embeddings using the decoder as well as generate unseen 3D poses. The variability of our encoding allows it to generalize well to unseen camera views when mapping from 2D space. To the best of our knowledge, V-VIPE is the only representation to offer this diversity of applications. Code and more information can be found at https://v-vipe.github.io/.", "sections": [{"title": "1. Introduction", "content": "Learning to represent three dimensional (3D) human pose given a two dimensional (2D) image of a person, is a challenging problem with several important downstream applications such as teaching a person to mimic a video, action recognition and imitation learning for robotics. The key challenge arises from the fact that different camera viewpoints observing the same 3D pose lead to very different projections in a 2D image. The common practice is to circumvent this challenge by estimating 3D pose in the camera coordinate space [7, 15, 32]. However, this leads to differences in scale and rotation between the estimated 3D representations from images of the same 3D pose from different camera viewpoints. Without the knowledge of camera parameters, it is not possible to establish correspondence between these 3D representations. This is important as we move towards environments where we have very little control over the camera viewpoint, such as photos taken with a phone or AR glasses. In such scenarios, we can make very few assumptions about the camera space.\nIn this paper, we address this challenge by separating the problem of estimating 3D pose from 2D images into two steps. First, we learn an embedding to represent 3D poses in canonical coordinate space. Next, we learn to encode 2D poses, from different camera viewpoints, to the embedding from the first step. This leads to a canonical 3D pose embedding that is invariant to camera viewpoints. This view-invariant pose embedding is highly flexible, allowing us to do 3D pose retrieval, 3D pose generation, and most importantly, estimating consistent 3D pose from different 2D viewpoints 1.\nIn our approach we use a variational autoencoder (VAE) to learn an embedding for 3D human poses. This VAE is trained to reconstruct 3D poses and has two key benefits: (a) we can leverage loss functions to ensure similar 3D poses are close in the embedding space, and (b) we learn embeddings that can generalize better to unseen 3D poses due to the variational training paradigm. Next, we learn a map-"}, {"title": "2. Related Work", "content": "Human Pose Estimation. There are two family of approaches for human pose estimation. One is to directly estimate 3D poses from an 2D images [19, 24], and the other is to lift pre-detected 2D poses to 3D poses [15, 23, 30]. In recent years, state-of-the-art approaches have almost exclusively focused on the lifting strategy.\nOur goal is to specifically find correspondence between 2D poses in images from different camera viewpoints without any knowledge of camera parameters or temporal context. Recent works have explored how temporal informa-tion can improve 3D pose estimation [3, 31], typically by processing a sequence of images using a transformer [31]. However, our focus is 3D pose estimation using a single 2D image which is similar to [26].\nThe key distinction between our approach and prior works in estimating 3D poses using 2D images is view-invariant embedding that can be estimated from a monocular viewpoint. Several works have attempted to address view invariant estimation by leveraging many viewpoints [14, 20, 25] because it is much easier to place a person in canonical coordinate space when you have access to many views. However, access to multiple viewpoints of the same scene is an unrealistic assumption in the canonical settings. Therefore, these approaches can only be used in environments that have multiple cameras observing the same scene. In contrast, our approach can be applied to any arbitrary 2D image. Some works only use a single viewpoint during inference time, but still require multiple views for each pose during training [11]. Whereas our method is more flexible and can be trained on any dataset with both 2D and 3D information, even if there is only one camera viewpoint available. Similar to our work, [26] performs view-invariant pose estimation from one view, but their method requires localized transformations that fundamentally change the 3D pose and must be reversed at the end to get the final pose. Our approach, on the other hand, requires only one global rotation to a canonical camera viewpoint that does not change the integrity of the pose.\n3D Pose Generation. Training a model capable of generating new 3D poses is important for representing unseen data in addition to training data. There are two main types of generators that can be used, Generational Adversarial Networks (GANs) and Variational Auto Encoders (VAEs).\nSeveral works have used GANS [26, 27, 29] to generate training data for 3D poses. However, they are not well suited for our task which also requires encoding 3D poses in an embedding space. VAEs, on the other hand, are better suited for learning embedding by auto-encoding 3D poses. [17] learns a latent network, where they go directly from 2D to 3D without using the 3D data as input to the model, whereas [22] learns a latent representation using a variant of VAE and generate 3D poses using 2D pose as a precondition to their decoder. [10] employs a basic autoencoder instead of a VAE, which leads to an inconsistent embedding space that is harder to map to 2D inputs. [6] also learns an autoencoder instead of a VAE, but additionally, they choose to regress on the embedding and perform little normalization prior to training which leads to a poorly regularized output space."}, {"title": "3. Proposed Method", "content": "Our method consists of three main parts. In 3.1 we review the input data pre-processing to ensure that the output is"}, {"title": "3.1. Data Processing", "content": "Before we pass any data through our model we perform two key steps. First, we modify the global rotation of the image; second, we scale the keypoints so that the original size does not affect the model.\nGlobal Rotation Realignment. Predicting 3D pose in canonical space is extraordinarily difficult as mentioned in [15]. We believe this is mostly due to the global rotation\u00b9 of any 3D pose. Global rotation is hard to estimate due to its ambiguity. We can see in Figure 2 that a pose in global space can have a very different appearance in camera space. Without any information, such as a ground truth pose, which we can align the output to or any camera parameters, it would be difficult to determine that any two of these poses are the same.\nWe argue that global rotation is irrelevant for human pose comparison. Specifically, when we are trying to determine if two poses are the same we do not need to understand how those are oriented in relation to the world they are in. If one pose is facing the x-axis and the other is facing the y-axis, it is still possible that their overall pose is the same. We thus remove rotation dependence by aligning the coordinates of the left hip, right hip and the spine to the same points in every pose of the dataset. This can be visualized in Figure 3. In order to achieve such alignment we find the rotation that minimizes the equation:\n$$L(C) = \\frac{1}{2} \\sum_{i=1}^{n} ||a_i - Cb_i||^2$$\nwhere a1, a2, a3 equal the 3D points representing the left hip, right hip and spine respectively and b1, b2, b3 equal [[0, -1, 0], [0, 1, 0], [0, 0, 1]]. Aligning to these points causes the hips to align to the y axis and the spine to the z axis. We specifically align the hips because they are in a straight line so it is easy to align to one axis and the spine because it is directly above the root and therefore can be easily aligned to a perpendicular axis. In order to minimize Equation 1, we use the Kabsch algorithm [9].\nScaling and Pose Normalization. In this work, we are only concerned with estimating pose such that it is easy to compare how similar two poses are. This is because pose comparison is what is needed for downstream tasks such as action recognition. To account for this, we scale and normalize the input, such that it becomes independent from factors that should not affect the pose similarity estimation.\nWe use the universal skeleton provided by the dataset to remove the size factor. In this representation all joints are scaled to the same proportions. This makes the size of the 3D output independent of the inputted 2D image or the original 3D pose.\nMoreover, to complete the normalization of the data we use a process similar to [1] where we center the root joint and scale all of the other joints, accordingly."}, {"title": "3.2. 3D Pose VAE", "content": "The proposed model consists of two parts, a 3D Pose VAE Network and a 2D Mapping Network. The 3D Pose VAE Network, Figure 4.a, consists of an encoder network and a decoder network, which make up the VAE model. To stay consistent with other papers we choose [15] as the backbone for both our encoder and our decoder.\nThe benefit of using a VAE for the 3D Pose VAE Network is its ability to generalize to new poses. This is because the goal of a VAE is to synthesize unseen poses. Although this is not our main goal, we do want our network to potentially be able to represent unseen poses, which is a realistic setting in real world applications."}, {"title": "3.3. 2D Mapping Network", "content": "Once we have trained the 3D Pose VAE Network we utilize its embedding space to learn a 2D Mapping Network (see Figure 4.b). In particular, we take the 3D Pose VAE Network decoder model and we freeze it so that it translates from the pre-defined V-VIPE space to 3D coordinates. Next, we train a new encoder Enc2p for 2D coordinates. The new encoder takes in input S2D = {pi \u2208 R2|i = 1...N} and outputs a V-VIPE, e \u2208 Rn. We pass e through the frozen decoder to get what the embedding represents in 3D space according to the model trained in the previous phase, S3D = {pi \u2208 R\u00b3 i = 1...N}.\nTo train the 2D Mapping Network we use two losses. Given the input, S2D, the output $3D and the ground truth 3D keypoints, S3D, we compute MSE(S3D, S3D). We combine this loss with a triplet loss, which we compute similarly as in Section 3.2. The main difference is that we use the output from the 2D encoder and the ground truth 3D keypoints. We then back-propagate this loss through the whole network, but do not apply the gradient losses to the decoder network. This is because we do not want to change"}, {"title": "4. Experiments and Results", "content": "4.1. Experimental Setup\nThe model uses a backbone network described described in [15]. We stack 2 blocks of this network together for both the encoder and the decoder network of both the 3D Pose VAE Network and the 2D Mapping Network. We set the linear size to 1024, and we use a 0.1 dropout. The dimension of a V-VIPE is 32 and the margin for the triplet loss is 1.0. Any 2D keypoint detector could be used, but we chose AlphaPose [5, 12, 13, 28]. We use COCO keypoints because they are widely used for 2D detectors. We implemented the model in PyTorch and we trained it on 1 GPU.\n4.2. Metrics\nWe evaluate the model using two metrics. The first is a hit metric, inspired from [26], which we use to measure how often we are able to retrieve a pose that is similar to a query pose. Given two normalized keypoints Sp and S3p we first apply a Procrustes alignment [21] between the two to get A(SD) and A(S3D). Given a dataset with many views we select two camera views. We find all embeddings for the 2D poses from the selected cameras. Then, we query each embedding from camera 1 and find the k nearest neighbors from the set of embeddings for camera 2. We consider a"}, {"title": "4.5. Quantitative Results", "content": "Similar Pose Retrieval Experiments. We compare our model for hit metrics against 3 baselines. The first baseline is the PR-VIPE model, which attempts to define an embedding space without reconstructing the 3D pose; we adopted their open source code and re-trained their model so we would have results on the same 2D pose detector, i.e., AlphaPose. The second baseline is simply finding the nearest neighbor of the detected 2D keypoints. The third baseline uses Epipolar Pose [11] to detect 3D keypoints. In this case, Procrustes alignment is performed between all poses and the closest aligned pose is selected as the match.\nWe show the hit metrics for the different k values in Table 1. The top section of the table shows the results of our method and of PR-VIPE when trained and tested with ground truth (GT) 3D keypoints. The left part of the table reports the results on the test set of H3.6M. We can see that our approach is slightly worse than the PR-VIPE approach. This is because we are testing on very similar data to the original training set. Our model, however, is designed to generalize. The generalization of the model is demonstrated in the middle part of the table, where we report the performance on the 3DHP dataset when considering all available cameras. In this case, our model gets higher values for all values of k. Moreover, when we pair one chest camera with a camera that is not at chest height, i.e., unseen cameras with respect to the training data(right part of the table), we can see that the gap is even larger. For example, when considering k = 1, the gap between the two models is about 4.2 percent for unseen cameras and 2.7 percent for all cameras. This demonstrates that the latent space we acquired during the VAE training is able to generalize to unseen camera viewpoints better than existing models.\nIn the bottom section of the table, we show results when the keypoints are automatically detected(D). For PR-VIPE and our model we use AlphaPose. Epipolar Pose detects its own keypoints. Again our method outperforms the PR-VIPE model when generalizing to data different from the training set, 3DHP, as well as to unseen cameras. For example, when k = 10 our method outperforms PR-VIPE by about 5.6 percent for all 3DHP cameras, and by about 7 percent for the unseen category.\nIn this section we also show results for detected keypoints plus additional training data generated by augmenting the 3D poses. We see an increase from just our detection model for 3DHP because we have introduced new camera viewpoints to the training data. We see an improvement over PR-VIPE when they use augmented data, although we do not get as much of a boost from augmentation because our model already generalizes better than theirs. For k = 1 our model outperforms theirs by 1.5 percent.\nAdditionally, in the table we report the 2D keypoints and Epipolar Pose results. We can observe that using the 2D keypoints is not effective, as demonstrated by the low hit metric for all k values. The Epipolar Pose# method performs better than both our method and the PR-VIPE method before any augmentation is applied to the data because it is trained on the 3DHP dataset and does not need to generalize. When you try to run the Epipolar Pose* model on 3DHP data the output does not resemble human pose. We do not report generalized results for Epipolar pose because of this. Despite the fact that Epipolar Pose# is trained specifically for detection on the 3DHP dataset when we add augmentation of the data to our model we are able to beat their results by about 2 percent.\n3D Pose Estimation Experiments. In addition to calculating the hit metric described above our model also outputs the predicted 3D pose. We find that the average error of this model is 62.1 millimeters. We calculated this number using a model trained on keypoints detected by the Cascaded Pyramid Network [2] as this is commonly [4, 19, 26] used for 3D Pose Estimation. We find that while this number is not competitive with current methods for pose estimation that use more complex models or take in more information, such as sequences, it is similar to the error found in [15], which we use as the backbone for our network."}, {"title": "4.6. Qualitative Results", "content": "2D to 3D Pose Estimation. Figure 5 shows examples of our 3D estimations given a 2D image as input. We show examples of 4 different poses each with 4 different camera angles. In the two examples on the left we have very accurate retrievals. All of the cameras have similar retrievals that allow us to determine that the person is in the same pose despite the very different original camera angles. The examples on the right are the ones where our model struggles to find the whole pose. In the example on the top we are able to find the hand position because the hands are visible in every image, however our model struggles to detect that the body is slightly angled. This is likely because the difference in 2D keypoints between an angled and not angled body are very small and our 2D keypoint detector is not accurate enough. In the example on the bottom our model succeeds with the arms, except for one camera viewpoint where the arm is not visible in the image at all. The other way our model struggles is with the head tilt. This is likely because this is difficult to visualize from most camera angles.\n3D Pose Retrieval. We show how our model is able to retrieve similar poses from different view points. In Figure 6 you can see the query pose as well as the pose that is retrieved from a different view point. Ideally, the two poses will be identical. This is the visualization of what the Hit metric represents. If the queried pose is sufficiently close to the retrieved pose then we have a hit.\nVisualizing V-VIPE. Figure 8 shows a t-SNE visualization, which we use to show the smoothness of the learned V-VIPE space, where each dot represents a V-VIPE. In order to properly show the clustering we select 10 visually different 3D poses and color our visualization based on which of the 10 poses is the most similar to the pose that each point represents. It is easy to see from this graph that similar colors are typically found in clusters. This means that the space well represents the notion of similarity between poses. We can see this even clearer in the expansion of the visualization where we show three poses and their locations in the cluster. The two poses on the right are colored the same and are very close together. These are slightly different, but the overall pose is very similar. We then select a point that is very far away and here we can see that the pose is quite different.\n3D Pose Generation. Our model is able to generate new poses by adding noise to the embedding space of an existing pose. In Figure 7 we define a noise array z and add it to an embedding with increasing magnitudes. The pose continues to move in one direction as we increase magnitude showing that our embedding space is smooth."}, {"title": "5. Ablation Study", "content": "We performed an ablative analysis in order to understand which of our design choices best contributed to our results.\nTriplet Loss. First we examine how important it is that we include the triplet loss term in our method. We remove it from the loss term and find that the new Hit@1 value is 17.41 with no augmented data. This is a drop of 6.1 from the Hit@1 value when triplet loss is included. Therefore the triplet loss value is important to the overall loss term.\nData Processing. We examine how important it is for us to rotate the 3D pose before training on our model. This step is important because it enables us to compare the similarity of poses with two different global rotations without needing to do a time consuming Procrustes Alignment between every pair of poses. We find that the Hit@1 value on 3DHP with no augmentation obtained when using non rotated points is 18.0 percent, a 5.5 percent decline from our approach.\nPretraining the Decoder. Finally, we studied whether or not pretraining a VAE and using a defined embedding space contributed to our final hit metric. We found that the Hit@1 value for the model with no pretraining is 23.4 versus the 23.5 we obtained by completing the pretraining step. However, this step is important anyways because it enables the model to do 3D Pose Retrieval. Without it we would not be able to map our 3D poses to our embedding space. Therefore we would not be able to generate similar poses to a given 3D pose or query a 3D pose to find a similar 2D pose from a set of images."}, {"title": "6. Conclusion", "content": "In this work we showed that by using only 3D poses to define a V-VIPE space we can define a better camera invariant space than if we were to only use 2D poses. We defined a procedure made of two steps: first we train a VAE model to learn a latent space of 3D poses; then, we train a 2D keypoints encoder that is linked to the VAE decoder to allow 3D reconstructions of 2D images. We adopted a VAE model as it creates a smooth latent space that can generalize better towards unseen poses during training. In order to achieve this goal, we train a VAE with a three component loss function. We performed an extensive experimental evaluation, by using two datasets, i.e., Human3.6M and MPI-INF-3DHP. We demonstrated that the latent space is modeling a meaningful notion of similarity of the embeddings. This is reflected in the Pose Retrieval experiments where we improve about 2.5 percent in the Hit@1 metric when considering unseen cameras. We also showed qualitative examples demonstrating the capability of our embedding space to capture the notion of similarity of poses. This is important in downstream tasks. In the future we believe that this approach has a lot of promise for application to downstream tasks such as action segmentation and detection."}]}