{"title": "V-VIPE: Variational View Invariant Pose Embedding", "authors": ["Mara Levy", "Abhinav Shrivastava"], "abstract": "Learning to represent three dimensional (3D) human\npose given a two dimensional (2D) image of a person, is\na challenging problem. In order to make the problem less\nambiguous it has become common practice to estimate 3D\npose in the camera coordinate space. However, this makes\nthe task of comparing two 3D poses difficult. In this pa-\nper, we address this challenge by separating the problem\nof estimating 3D pose from 2D images into two steps. We\nuse a variational autoencoder (VAE) to find an embedding\nthat represents 3D poses in canonical coordinate space. We\nrefer to this embedding as variational view-invariant pose\nembedding (V-VIPE). Using V-VIPE we can encode 2D and\n3D poses and use the embedding for downstream tasks, like\nretrieval and classification. We can estimate 3D poses from\nthese embeddings using the decoder as well as generate un-\nseen 3D poses. The variability of our encoding allows it\nto generalize well to unseen camera views when mapping\nfrom 2D space. To the best of our knowledge, V-VIPE is\nthe only representation to offer this diversity of applica-\ntions. Code and more information can be found at https://v-\nvipe.github.io/.", "sections": [{"title": "1. Introduction", "content": "Learning to represent three dimensional (3D) human pose\ngiven a two dimensional (2D) image of a person, is a chal-\nlenging problem with several important downstream appli-\ncations such as teaching a person to mimic a video, action\nrecognition and imitation learning for robotics. The key\nchallenge arises from the fact that different camera view-\npoints observing the same 3D pose lead to very different\nprojections in a 2D image. The common practice is to cir-\ncumvent this challenge by estimating 3D pose in the camera\ncoordinate space [7, 15, 32]. However, this leads to differ-\nences in scale and rotation between the estimated 3D rep-\nresentations from images of the same 3D pose from differ-\nent camera viewpoints. Without the knowledge of camera\nparameters, it is not possible to establish correspondence\nbetween these 3D representations. This is important as we\nmove towards environments where we have very little con-\ntrol over the camera viewpoint, such as photos taken with a\nphone or AR glasses. In such scenarios, we can make very\nfew assumptions about the camera space.\nIn this paper, we address this challenge by separating\nthe problem of estimating 3D pose from 2D images into\ntwo steps. First, we learn an embedding to represent 3D\nposes in canonical coordinate space. Next, we learn to en-\ncode 2D poses, from different camera viewpoints, to the\nembedding from the first step. This leads to a canonical 3D\npose embedding that is invariant to camera viewpoints. This\nview-invariant pose embedding is highly flexible, allowing\nus to do 3D pose retrieval, 3D pose generation, and most\nimportantly, estimating consistent 3D pose from different\n2D viewpoints 1.\nIn our approach we use a variational autoencoder (VAE)\nto learn an embedding for 3D human poses. This VAE is\ntrained to reconstruct 3D poses and has two key benefits: (a)\nwe can leverage loss functions to ensure similar 3D poses\nare close in the embedding space, and (b) we learn embed-\ndings that can generalize better to unseen 3D poses due to\nthe variational training paradigm. Next, we learn a map-"}, {"title": "2. Related Work", "content": "Human Pose Estimation. There are two family of ap-\nproaches for human pose estimation. One is to directly es-\ntimate 3D poses from an 2D images [19, 24], and the other\nis to lift pre-detected 2D poses to 3D poses [15, 23, 30]. In\nrecent years, state-of-the-art approaches have almost exclu-\nsively focused on the lifting strategy.\nOur goal is to specifically find correspondence between\n2D poses in images from different camera viewpoints with-\nout any knowledge of camera parameters or temporal con-\ntext. Recent works have explored how temporal informa-\ntion can improve 3D pose estimation [3, 31], typically by\nprocessing a sequence of images using a transformer [31].\nHowever, our focus is 3D pose estimation using a single 2D\nimage which is similar to [26].\nThe key distinction between our approach and prior\nworks in estimating 3D poses using 2D images is view-\ninvariant embedding that can be estimated from a monoc-\nular viewpoint. Several works have attempted to ad-\ndress view invariant estimation by leveraging many view-\npoints [14, 20, 25] because it is much easier to place a per-\nson in canonical coordinate space when you have access\nto many views. However, access to multiple viewpoints of\nthe same scene is an unrealistic assumption in the canoni-\ncal settings. Therefore, these approaches can only be used\nin environments that have multiple cameras observing the\nsame scene. In contrast, our approach can be applied to any\narbitrary 2D image. Some works only use a single view-\npoint during inference time, but still require multiple views\nfor each pose during training [11]. Whereas our method is\nmore flexible and can be trained on any dataset with both\n2D and 3D information, even if there is only one camera\nviewpoint available. Similar to our work, [26] performs\nview-invariant pose estimation from one view, but their\nmethod requires localized transformations that fundamen-\ntally change the 3D pose and must be reversed at the end to\nget the final pose. Our approach, on the other hand, requires\nonly one global rotation to a canonical camera viewpoint\nthat does not change the integrity of the pose.\n3D Pose Generation. Training a model capable of generat-\ning new 3D poses is important for representing unseen data\nin addition to training data. There are two main types of\ngenerators that can be used, Generational Adversarial Net-\nworks (GANs) and Variational Auto Encoders (VAEs).\nSeveral works have used GANS [26, 27, 29] to gen-\nerate training data for 3D poses. However, they are not\nwell suited for our task which also requires encoding 3D\nposes in an embedding space. VAEs, on the other hand,\nare better suited for learning embedding by auto-encoding\n3D poses. [17] learns a latent network, where they go di-\nrectly from 2D to 3D without using the 3D data as input to\nthe model, whereas [22] learns a latent representation us-\ning a variant of VAE and generate 3D poses using 2D pose\nas a precondition to their decoder. [10] employs a basic\nautoencoder instead of a VAE, which leads to an inconsis-\ntent embedding space that is harder to map to 2D inputs. [6]\nalso learns an autoencoder instead of a VAE, but addition-\nally, they choose to regress on the embedding and perform\nlittle normalization prior to training which leads to a poorly\nregularized output space."}, {"title": "3. Proposed Method", "content": "Our method consists of three main parts. In 3.1 we review\nthe input data pre-processing to ensure that the output is"}, {"title": "3.1. Data Processing", "content": "Before we pass any data through our model we perform two\nkey steps. First, we modify the global rotation of the image;\nsecond, we scale the keypoints so that the original size does\nnot affect the model.\nGlobal Rotation Realignment. Predicting 3D pose in\ncanonical space is extraordinarily difficult as mentioned in\n[15]. We believe this is mostly due to the global rotation\u00b9\nof any 3D pose. Global rotation is hard to estimate due\nto its ambiguity. We can see in Figure 2 that a pose in\nglobal space can have a very different appearance in cam-\nera space. Without any information, such as a ground truth\npose, which we can align the output to or any camera pa-\nrameters, it would be difficult to determine that any two of\nthese poses are the same.\nWe argue that global rotation is irrelevant for human pose\ncomparison. Specifically, when we are trying to determine\nif two poses are the same we do not need to understand how\nthose are oriented in relation to the world they are in. If one\npose is facing the x-axis and the other is facing the y-axis, it\nis still possible that their overall pose is the same. We thus\nremove rotation dependence by aligning the coordinates of\nthe left hip, right hip and the spine to the same points in\nevery pose of the dataset. This can be visualized in Figure 3.\nIn order to achieve such alignment we find the rotation that\nminimizes the equation:\n\\(L(C) = \\frac{1}{2} \\sum_{i=1}^{n} ||a_i - Cb_i||^2\\) (1)\nwhere \\(a_1, a_2, a_3\\) equal the 3D points representing the\nBy global rotation we mean how a human is rotated in relation to the\ncanonical space.\nleft hip, right hip and spine respectively and \\(b_1, b_2, b_3\\)\nequal [[0, -1, 0], [0, 1, 0], [0, 0, 1]]. Aligning to these points\ncauses the hips to align to the y axis and the spine to the\nz axis. We specifically align the hips because they are in a\nstraight line so it is easy to align to one axis and the spine\nbecause it is directly above the root and therefore can be\neasily aligned to a perpendicular axis. In order to minimize\nEquation 1, we use the Kabsch algorithm [9].\nScaling and Pose Normalization. In this work, we are\nonly concerned with estimating pose such that it is easy to\ncompare how similar two poses are. This is because pose\ncomparison is what is needed for downstream tasks such as\naction recognition. To account for this, we scale and nor-\nmalize the input, such that it becomes independent from\nfactors that should not affect the pose similarity estima-\ntion.\nWe use the universal skeleton provided by the dataset\nto remove the size factor. In this representation all joints\nare scaled to the same proportions. This makes the size of\nthe 3D output independent of the inputted 2D image or the\noriginal 3D pose.\nMoreover, to complete the normalization of the data we\nuse a process similar to [1] where we center the root joint\nand scale all of the other joints, accordingly."}, {"title": "3.2. 3D Pose VAE", "content": "The proposed model consists of two parts, a 3D Pose VAE\nNetwork and a 2D Mapping Network. The 3D Pose VAE\nNetwork, Figure 4.a, consists of an encoder network and a\ndecoder network, which make up the VAE model. To stay\nconsistent with other papers we choose [15] as the back-\nbone for both our encoder and our decoder.\nThe benefit of using a VAE for the 3D Pose VAE Net-\nwork is its ability to generalize to new poses. This is be-\ncause the goal of a VAE is to synthesize unseen poses. Al-\nthough this is not our main goal, we do want our network\nto potentially be able to represent unseen poses, which is a\nrealistic setting in real world applications."}, {"title": "3.3. 2D Mapping Network", "content": "Once we have trained the 3D Pose VAE Network we uti-\nlize its embedding space to learn a 2D Mapping Network\n(see Figure 4.b). In particular, we take the 3D Pose VAE\nNetwork decoder model and we freeze it so that it trans-\nlates from the pre-defined V-VIPE space to 3D coordinates.\nNext, we train a new encoder Enc2p for 2D coordinates. The\nnew encoder takes in input \\(S_{2D} = \\{p_i \\in \\mathbb{R}^2 | i = 1...N\\}\\)\nand outputs a V-VIPE, \\(e \\in \\mathbb{R}^n\\). We pass e through the\nfrozen decoder to get what the embedding represents in 3D\nspace according to the model trained in the previous phase,\n\\(\\hat{S}_{3D} = \\{p_i \\in \\mathbb{R}^3 i = 1...N\\}\\).\nTo train the 2D Mapping Network we use two losses.\nGiven the input, \\(S_{2D}\\), the output \\(\\hat{S}_{3D}\\) and the ground truth\n3D keypoints, \\(S_{3D}\\), we compute MSE(\\(\\hat{S}_{3D}, S_{3D}\\)). We\ncombine this loss with a triplet loss, which we compute sim-\nilarly as in Section 3.2. The main difference is that we use\nthe output from the 2D encoder and the ground truth 3D\nkeypoints. We then back-propagate this loss through the\nwhole network, but do not apply the gradient losses to the\ndecoder network. This is because we do not want to change"}, {"title": "4. Experiments and Results", "content": "4.1. Experimental Setup\nThe model uses a backbone network described described\nin [15]. We stack 2 blocks of this network together for both\nthe encoder and the decoder network of both the 3D Pose\nVAE Network and the 2D Mapping Network. We set the\nlinear size to 1024, and we use a 0.1 dropout. The dimen-\nsion of a V-VIPE is 32 and the margin for the triplet loss\nis 1.0. Any 2D keypoint detector could be used, but we\nchose AlphaPose [5, 12, 13, 28]. We use COCO keypoints\nbecause they are widely used for 2D detectors. We imple-\nmented the model in PyTorch and we trained it on 1 GPU.\n4.2. Metrics\nWe evaluate the model using two metrics. The first is a hit\nmetric, inspired from [26], which we use to measure how\noften we are able to retrieve a pose that is similar to a query\npose. Given two normalized keypoints \\(S_{2D}\\) and \\(S_{3D}\\) we first\napply a Procrustes alignment [21] between the two to get\n\\(A(S_{2D})\\) and \\(A(S_{3D})\\). Given a dataset with many views we\nselect two camera views. We find all embeddings for the\n2D poses from the selected cameras. Then, we query each\nembedding from camera 1 and find the k nearest neighbors\nfrom the set of embeddings for camera 2. We consider a"}, {"title": "4.3. Datasets", "content": "In all the experiments we train on the standard training set\nof the Human3.6M dataset (H3.6M) [8]. For our hit metric\nwe use the test set of H3.6M as the validation set and show\nresults on the MPI-INF-3DHP dataset [16] (3DHP).\nHuman3.6M. The H3.6M dataset [8] contains 3.6 million\nhuman poses taken from 4 different cameras. All of these\ncameras are at chest level. The standard training set for this\ndataset is made up of subjects 1,5,6,7 and 8. The standard\ntest set contains poses from subjects 9 and 11. For the eval-\nuation of the hit metric, we follow the method described\nin [23], where they remove poses that are similar.\nMPI-INF-3DHP. 3DHP [16] contains 14 different camera\nangles. For our tasks we remove the overhead cameras,\nwhich leaves us with 11 cameras. Of these cameras, 5 are\nat chest height and the others have a slight vertical angle.\nThis dataset is used to show whether or not our method will\ngeneralize to data that is different from the training data.\n4.4. Augmentation\nIn order to improve the model's ability to generalize we\nintroduce camera augmentation similar to the work done\nin [23]. To calculate this augmentation we take the ground\ntruth 3D pose and randomly rotate it. We then project\nthis pose into 2D. We add augmented poses to each of our\nbatches during training time. We found that it was best to\nadd augmented poses for half of the poses in each batch."}, {"title": "4.5. Quantitative Results", "content": "Similar Pose Retrieval Experiments. We compare our\nmodel for hit metrics against 3 baselines. The first base-\nline is the PR-VIPE model, which attempts to define an\nembedding space without reconstructing the 3D pose; we\nadopted their open source code and re-trained their model\nso we would have results on the same 2D pose detector, i.e.,\nAlphaPose. The second baseline is simply finding the near-\nest neighbor of the detected 2D keypoints. The third base-\nline uses Epipolar Pose [11] to detect 3D keypoints. In this\ncase, Procrustes alignment is performed between all poses\nand the closest aligned pose is selected as the match.\nWe show the hit metrics for the different k values in\nTable 1. The top section of the table shows the results of\nour method and of PR-VIPE when trained and tested with\nground truth (GT) 3D keypoints. The left part of the table\nreports the results on the test set of H3.6M. We can see that\nour approach is slightly worse than the PR-VIPE approach.\nThis is because we are testing on very similar data to the\noriginal training set. Our model, however, is designed to\ngeneralize. The generalization of the model is demonstrated\nin the middle part of the table, where we report the perfor-\nmance on the 3DHP dataset when considering all available\ncameras. In this case, our model gets higher values for all\nvalues of k. Moreover, when we pair one chest camera with\na camera that is not at chest height, i.e., unseen cameras\nwith respect to the training data(right part of the table), we\ncan see that the gap is even larger. For example, when con-\nsidering k = 1, the gap between the two models is about\n4.2 percent for unseen cameras and 2.7 percent for all cam-\neras. This demonstrates that the latent space we acquired\nduring the VAE training is able to generalize to unseen cam-\nera viewpoints better than existing models.\nIn the bottom section of the table, we show results when\nthe keypoints are automatically detected(D). For PR-VIPE\nand our model we use AlphaPose. Epipolar Pose detects\nits own keypoints. Again our method outperforms the PR-\nVIPE model when generalizing to data different from the\ntraining set, 3DHP, as well as to unseen cameras. For ex-\nample, when k = 10 our method outperforms PR-VIPE by\nabout 5.6 percent for all 3DHP cameras, and by about 7\npercent for the unseen category.\nIn this section we also show results for detected key-\npoints plus additional training data generated by augment-\ning the 3D poses. We see an increase from just our detection\nmodel for 3DHP because we have introduced new camera\nviewpoints to the training data. We see an improvement\nover PR-VIPE when they use augmented data, although we\ndo not get as much of a boost from augmentation because\nour model already generalizes better than theirs. For k = 1\nour model outperforms theirs by 1.5 percent.\nAdditionally, in the table we report the 2D keypoints and\nEpipolar Pose results. We can observe that using the 2D\nkeypoints is not effective, as demonstrated by the low hit\nmetric for all k values. The Epipolar Pose# method per-\nforms better than both our method and the PR-VIPE method\nbefore any augmentation is applied to the data because it\nis trained on the 3DHP dataset and does not need to gen-\neralize. When you try to run the Epipolar Pose* model\non 3DHP data the output does not resemble human pose.\nWe do not report generalized results for Epipolar pose be-\ncause of this. Despite the fact that Epipolar Pose# is trained\nspecifically for detection on the 3DHP dataset when we add\naugmentation of the data to our model we are able to beat\ntheir results by about 2 percent.\n3D Pose Estimation Experiments. In addition to calculat-\ning the hit metric described above our model also outputs\nthe predicted 3D pose. We find that the average error of this\nmodel is 62.1 millimeters. We calculated this number us-\ning a model trained on keypoints detected by the Cascaded\nPyramid Network [2] as this is commonly [4, 19, 26] used\nfor 3D Pose Estimation. We find that while this number is\nnot competitive with current methods for pose estimation\nthat use more complex models or take in more information,\nsuch as sequences, it is similar to the error found in [15],\nwhich we use as the backbone for our network."}, {"title": "4.6. Qualitative Results", "content": "2D to 3D Pose Estimation. Figure 5 shows examples of\nour 3D estimations given a 2D image as input. We show\nexamples of 4 different poses each with 4 different camera\nangles. In the two examples on the left we have very ac-\ncurate retrievals. All of the cameras have similar retrievals\nthat allow us to determine that the person is in the same pose\ndespite the very different original camera angles. The exam-\nples on the right are the ones where our model struggles to\nfind the whole pose. In the example on the top we are able\nto find the hand position because the hands are visible in\nevery image, however our model struggles to detect that the\nbody is slightly angled. This is likely because the difference\nin 2D keypoints between an angled and not angled body\nare very small and our 2D keypoint detector is not accurate\nenough. In the example on the bottom our model succeeds\nwith the arms, except for one camera viewpoint where the\narm is not visible in the image at all. The other way our\nmodel struggles is with the head tilt. This is likely because\nthis is difficult to visualize from most camera angles.\n3D Pose Retrieval. We show how our model is able to\nretrieve similar poses from different view points. In Figure\n6 you can see the query pose as well as the pose that is\nretrieved from a different view point. Ideally, the two poses\nwill be identical. This is the visualization of what the Hit\nmetric represents. If the queried pose is sufficiently close to\nthe retrieved pose then we have a hit.\nVisualizing V-VIPE. Figure 8 shows a t-SNE visualiza-\ntion, which we use to show the smoothness of the learned\nV-VIPE space, where each dot represents a V-VIPE. In\norder to properly show the clustering we select 10 visually\ndifferent 3D poses and color our visualization based on\nwhich of the 10 poses is the most similar to the pose\nthat each point represents. It is easy to see from this\ngraph that similar colors are typically found in clusters.\nThis means that the space well represents the notion of\nsimilarity between poses. We can see this even clearer in\nthe expansion of the visualization where we show three\nposes and their locations in the cluster. The two poses on\nthe right are colored the same and are very close together.\nThese are slightly different, but the overall pose is very\nsimilar. We then select a point that is very far away and\nhere we can see that the pose is quite different.\n3D Pose Generation. Our model is able to generate new\nposes by adding noise to the embedding space of an existing\npose. In Figure 7 we define a noise array z and add it to an\nembedding with increasing magnitudes. The pose continues\nto move in one direction as we increase magnitude showing\nthat our embedding space is smooth."}, {"title": "5. Ablation Study", "content": "We performed an ablative analysis in order to understand\nwhich of our design choices best contributed to our results.\nTriplet Loss. First we examine how important it is that we\ninclude the triplet loss term in our method. We remove it\nfrom the loss term and find that the new Hit@1 value is\n17.41 with no augmented data. This is a drop of 6.1 from\nthe Hit@1 value when triplet loss is included. Therefore the\ntriplet loss value is important to the overall loss term.\nData Processing. We examine how important it is for us to\nrotate the 3D pose before training on our model. This step is\nimportant because it enables us to compare the similarity of\nposes with two different global rotations without needing to\ndo a time consuming Procrustes Alignment between every\npair of poses. We find that the Hit@1 value on 3DHP with\nno augmentation obtained when using non rotated points is\n18.0 percent, a 5.5 percent decline from our approach.\nPretraining the Decoder. Finally, we studied whether or\nnot pretraining a VAE and using a defined embedding space\ncontributed to our final hit metric. We found that the Hit@1\nvalue for the model with no pretraining is 23.4 versus the\n23.5 we obtained by completing the pretraining step. How-\never, this step is important anyways because it enables the\nmodel to do 3D Pose Retrieval. Without it we would not be\nable to map our 3D poses to our embedding space. There-\nfore we would not be able to generate similar poses to a\ngiven 3D pose or query a 3D pose to find a similar 2D pose\nfrom a set of images."}, {"title": "6. Conclusion", "content": "In this work we showed that by using only 3D poses to de-\nfine a V-VIPE space we can define a better camera invariant\nspace than if we were to only use 2D poses. We defined a\nprocedure made of two steps: first we train a VAE model to\nlearn a latent space of 3D poses; then, we train a 2D key-\npoints encoder that is linked to the VAE decoder to allow 3D\nreconstructions of 2D images. We adopted a VAE model as\nit creates a smooth latent space that can generalize better to-\nwards unseen poses during training. In order to achieve this\ngoal, we train a VAE with a three component loss function.\nWe performed an extensive experimental evaluation, by us-\ning two datasets, i.e., Human3.6M and MPI-INF-3DHP. We\ndemonstrated that the latent space is modeling a meaningful\nnotion of similarity of the embeddings. This is reflected in\nthe Pose Retrieval experiments where we improve about 2.5\npercent in the Hit@1 metric when considering unseen cam-\neras. We also showed qualitative examples demonstrating\nthe capability of our embedding space to capture the no-\ntion of similarity of poses. This is important in downstream\ntasks. In the future we believe that this approach has a lot of\npromise for application to downstream tasks such as action\nsegmentation and detection."}]}