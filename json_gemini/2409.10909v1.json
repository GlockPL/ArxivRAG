{"title": "GenCRF: Generative Clustering and Reformulation Framework for Enhanced Intent-Driven Information Retrieval", "authors": ["Wonduk Seo", "Haojie Zhang", "Yueyang Zhang", "Changhao Zhang", "Songyao Duan", "Lixin Su", "Daiting Shi", "Jiashu Zhao", "Dawei Yin"], "abstract": "Query reformulation is a well-known problem in Information Retrieval (IR) aimed at enhancing single search successful completion rate by automatically modifying user's input query. Recent methods leverage Large Language Models (LLMs) to improve query reformulation, but often generate insufficient and redundant expansions, potentially constraining their effectiveness in capturing diverse intents. In this paper, we propose GenCRF: a Generative Clustering and Reformulation Framework to capture diverse intentions adaptively based on multiple differentiated, well-generated queries in the retrieval phase for the first time. GenCRF leverages LLMs to generate variable queries from the initial query using customized prompts, then clusters them into groups to distinctly represent diverse intents. Furthermore, the framework explores to combine diverse intents query with innovative weighted aggregation strategies to optimize retrieval performance and crucially integrates a novel Query Evaluation Rewarding Model (QERM) to refine the process through feedback loops. Empirical experiments on the BEIR benchmark demonstrate that GenCRF achieves state-of-the-art performance, surpassing previous query reformulation SOTAs by up to 12% on nDCG@10. These techniques can be adapted to various LLMs, significantly boosting retriever performance and advancing the field of Information Retrieval.", "sections": [{"title": "1 Introduction", "content": "Query reformulation is a well-known problem in Information Retrieval (IR) to enhance search effectiveness by automatically modifying the initial query into well-formed one(s) (Carpineto and Romano, 2012). Traditional Pseudo-Relevance Feedback (PRF) based methods, such as RM3, improve the initial query by selecting terms from relevant documents (Robertson, 1991; Lavrenko and Bruce, 2001). Similarly, researchers expand initial queries by incorporating semantically similar terms with pre-trained word embeddings (Kuzi et al., 2016; Roy et al., 2016; Zamani and Croft, 2016). With the advent of Large Language Models (LLMs), query reformulation has re-emerged as a prominent research area within the field of information retrieval (Zhao et al., 2023). In contrast to past methods that relied on using existing related terms in the retrieval system for expansion, the current approaches to query reformulation harness the exceptional generative understanding abilities of LLMs (Wang et al., 2023a; Li et al., 2023). They leverage foundational LLM techniques such as prompt engineering and Chain-of-Thought (CoT) to enhance initial queries by generating keywords and detailed descriptions (Wei et al., 2022; Jagerman et al., 2023). However, these methods often face limitations in enriching information capacity through single expansions.\nMore recently, ensemble approaches utilizing multiple prompts to generate various keywords have emerged, demonstrating improved performance compared to earlier single expansion methods (Li et al., 2023; Dhole and Agichtein, 2024; Dhole et al., 2024). Although these methods demonstrate the benefits of utilizing various expansions to enrich original queries and improve retrieval effectiveness, these methods face several challenges: \u2460 The variations in their prompts tend to be simplistic and homogeneous prompt variations, lacking effective methods to capture the diverse user intents from multiple perspectives, \u2461 These methods primarily lack of dynamic assessment of intent importance and query relevance, \u2462 There is a lack of effective mechanisms to detect generation quality, potentially introducing negative biases in query performance.\nTo overcome these limitations, we propose GenCRF: a Generative Clustering and Reformulation Framework. Unlike previous methods that gen-"}, {"title": "2 RELATED WORK", "content": "Numerous methods have been applied for query reformulation, which has significantly evolved over the years, adapting to new methodologies in information retrieval (IR). Early approaches relied on classical retrieval models such as BM25 (Robert-son et al., 1994), which focused on exact matching statistical features including term frequency and document length to assess relevance. These methods often utilize techniques such as RM3 and query logs for pseudo-relevance feedback (Robertson, 1991; Lavrenko and Bruce, 2001; Jones et al., 2006; Craswell and Szummer, 2007). Neural networks have provided a new perspective on developing more sophisticated methods for query reformulation. Grbovic et al. (2015) proposed a rewriting method based on a query embedding algorithm, and Nogueira et al. (2017) also explored reinforcement learning-based models. Dense neural networks further advanced the field of query reformulation, with pre-trained embeddings, capturing complex semantics and facilitating transfer learning in IR tasks (Devlin et al., 2019; Xiong et al., 2021).\nMore recently, Large Language Models (LLMs) have significantly transformed query reformulation strategies. Weller et al. (2024) emphasized the potential of LLMs to utilize their ability including query reformulation, and showed that LLMs outperform traditional methods for query expansion. Generating keywords and pseudo documents such as Query2Doc (Q2D) (Wang et al., 2023a) and Query2Expansion (Q2E) (Jagerman et al., 2023) have shown their effectiveness in improving retrieval quality (Nogueira et al., 2019; Claveau, 2022; Wang et al., 2023b). Although those methods have shown promise in query reformulation to some extent, they often rely on a single model or prompt. To address the limitations of previous query reformulation methods, recent studies found that applying multiple different prompts to generate various keywords or documents could further boost the overall quality of query reformulation, as they can provide a certain degree of information gain for retrieval queries more closely, thereby effectively capturing a broader range of user intent. (Li et al., 2023; Dhole and Agichtein, 2024).\nDespite their improvements, above methods still face formidable challenges. They often tend to employ simplistic prompt variations that may not adequately capture the breadth of diverse user intents, resulting in redundant keyword generations that undermine the effectiveness of query reformulation. Moreover, their ensemble techniques frequently fall short in appropriately emphasizing the significance of various intents and in dynamically weighting the relevance between initial queries and reformulated ones. There is also a noticeable absence of robust mechanisms to evaluate the quality"}, {"title": "3 METHODOLOGY", "content": "In this section, we first provide a comprehensive overview of our innovative Generative Clustering and Reformulation Framework (GenCRF) (Section 3.1), followed by our specific Generation & Clustering settings and a comparative analysis with existing methods (Section 3.2). We then present weighted aggregation and fine-tuning strategies to optimize retrieval performance (Section 3.3). Finally, we introduce our Query Evaluation Rewarding Model (QERM), desgined to further enhance GenCRF's performance through intent-driven query capture and critical feedback for query re-generation and re-clustering (Section 3.4)."}, {"title": "3.1 Overview of GenCRF", "content": "To construct the GenCRF, we first utilize LLMs to reformulate the initial query $q_{init}$, into a new form $I_{new}$. This process generates N queries for each of the 3 diverse customized prompts in set P:\n$Q_{gen} = \\bigcup_{prompt \\in P}{R(q_{init}, prompt)}$\nIn this equation, $Q_{gen}$ represents the set of all generated queries. The reformulation LLM, denoted as R, applies each prompt in P to the initial query. To reduce information redundancy and capture diverse intents, we introduce a clustering step in our framework. This step dynamically clusters generated queries into several intentional groups and produces a representative, comprehensive query for each cluster:\n$Q_{final} = G(q_{init}, Q_{gen})$\nThe function G clusters the set of generated queries $Q_{gen}$ into 1 to 3 groups dynamically. It then generates a new representative query for each cluster, resulting in the set $Q_{final}$. This procedure ensures comprehensive coverage of derived query intents beyond the original query, while eliminating similar or redundant queries. Following the clustering step, the framework proceeds with a retrieval process. This process combines various weighted aggregation strategies designed to effectively capture both $q_{init}$ and $Q_{final}$:\n$D_{retrieval} = Retrieve(Q_{final}, q_{init}, W)$\nwhere W represents the weighting parameters used in the aggregation strategies and $D_{retrieval}$ is the set of final retrieved documents. To further enhance its performance, the GenCRF framework incorporates a novel Query Expansion Rewarding Model (QERM), which detects the superiority of clustered intent-driven queries and provides effective feedback to LLMs, signaling when re-generation and re-clustering are necessary. The overall pipeline of the GenCRF is shown in Figure 1."}, {"title": "3.2 Generation and Clustering", "content": "Prompts used by current baselines, such as Query2Doc (Q2D) and Query2Expansion (Q2E), typically instruct models to produce relevant keywords or documents without considering the inherent intent and underlying value of the query."}, {"title": "3.3 Weighted Aggregation Strategies", "content": "In order to optimize retrieval performance by effectively capturing both $q_{init}$ and reformulated queries from $Q_{final}$, we introduce two distinct weighted aggregation strategies and a fine-tuning process.\nSimilarity Dynamic Weights (SimDW). This novel strategy dynamically adjusts the weights of reformulated queries based on their similarity to $q_{init}$, while incorporating a filtering mechanism to ensure relevance. After assigning a fixed weight to the initial query, the method considers only those reformulated queries exceeding a predefined similarity threshold in the dynamic weighted aggregation. The aggregation equation is given by:\n$Q^{simDW}_{agg} = W_o \\cdot q_{init} + \\sum_{i=1}^{L_{qf} \\atop sim>\\theta} sim(q_{init}, q_{f,i}) \\cdot q_{f,i}$\nwhere $w_o$ is the fixed weight for the initial query; sim represents a dynamic weight estimating the relative magnitude of $q_{f,i}$, calculated as the cosine similarity between the embeddings of the initial query and the i-th reformulated query $q_{f,i}$ using a sentence embedding model; and $\\theta$ is the similarity threshold for filtering irrelevant queries.\nScore Dynamic Weights (ScoreDW). Building upon the SimDW approach, the ScoreDW strategy offers a more comprehensive evaluation of reformulated queries by employing a multidimensional scoring system to assess query quality, using these scores as dynamic weights in the aggregation process. The method retains the fixed weight for the initial query and the filtering mechanism from SimDW, but enhances the evaluation criteria. The aggregation equation for ScoreDW is expressed as:\n$Q^{scoreDW}_{agg} = w_o \\cdot q_{init} + \\sum_{i=1}^{L_{qf}} \\sum_{score \\ge 0} score(q_{init}, q_{f,i}) \\cdot q_{f,i}$\nSpecifically, score is a dynamic weight representing the estimated importance of each $q_{f,i}$, derived from an LLM's evaluation of the reformulated query relative to the initial query. The evaluation considers five key dimensions: Relevance, Specificity, Clarity, Comprehensiveness, and Usefulness for retrieval. The threshold $\\theta$ ensures that only high-scoring, pertinent reformulations contribute to the final aggregated query.\nFine-Tuning for ScoreDW. For the purpose of optimizing the ScoreDW strategy, we implement a fine-tuning process for the LLMs to enhance their precision in evaluating and scoring reformulated queries. The process begins with the generation of a diverse set of query pairs $(q_{init}, q_{ref})$ using each LLM, where $q_{ref}$ is the reformulated query. These pairs are then evaluated by GPT-40, serving as a high-quality benchmark, to produce reference scores. The fine-tuning objective is formulated as:\n$\\Phi^* = arg \\min_\\Phi \\sum_{i=1}^{N} C(LLM(q_{init,i}, q_{ref,i}), S_i)$\nwhere $\\Phi^*$ denotes the optimal LLM parameters, $(q_{init,i}, q_{ref,i})$ represents the i-th query pair, $s_i$ is the"}, {"title": "3.4 Query Evaluation Rewarding Model", "content": "To further improve the performance of GenCRF, we also introduce a novel approach: the Query Evaluation Rewarding Model (QERM). This innovative model functions as a multi-intent gain detection model that assesses the quality and effectiveness of queries generated by GenCRF, focusing on their alignment with diverse, intent-driven clusters. QERM evaluates how well generated queries capture user intent and contribute to meaningful query clusters. It provides feedback to LLMs for re-generation and re-clustering if necessary, addresses limitations in initial scoring."}, {"title": "4 EXPERIMENTS", "content": "We detail the experimental configuration, including datasets, baseline methods, and model specifications. We also detail the prompts used and specific parameters for each component of our framework."}, {"title": "4.1.1 Experimental Datasets", "content": "We conduct our main experiments on six datasets from the BEIR benchmark (Thakur et al., 2021) to evaluate retrieval performance: SciFact, TREC-COVID, SciDOCS, NFCorpus, DBPedia-entity, and FiQA-2018. For ablation studies and parameter analysis, we used two additional datasets: ArguAna and CQADupStack-English. The Quora dataset is utilized for both constructing scoring data in the Fine-Tuning process and training our Query Evaluation Rewarding Model (QERM)."}, {"title": "4.1.2 Models Used", "content": "LLMs: We employ Mistral-7B-Instruct-v0.3 and Llama-3.1-8B-Instruct models (Jiang et al., 2023; Touvron et al., 2023), with temperature 0.8 and top_p 0.95 for diverse outputs. GPT-40 (OpenAI, 2024) is used to generate high-quality reference scores for fine-tuning. We apply full-parameter fine-tuning to both models, using a learning rate of 1e - 5 for 5 epochs, with a batch size of 16.\nSimilarity Model: SentenceBERT (all-mpnet-base-v2) (Reimers and Gurevych, 2019) is used to generate embeddings of initial query and generated queries. These embeddings are then used to calculate the cosine similarity between them within the GenCRF framework. We set a similarity threshold 0 = 0.2 to filter out irrelevant queries.\nRetrieval Model: MSMARCO-DistilBERT-base-TAS-B model is used for our retrieval step, which is specifically designed for Dense Passage Retrieval and trained on the MSMARCO passage dataset (Campos et al., 2016), featuring 6-layer DistilBERT architecture optimized for retrieval."}, {"title": "4.1.3 Baseline Methods", "content": "We compare our method against several established competitive baselines. For non-fusion methods, queries are structured as \"initial query [SEP] generated query\", where [SEP] is a separator token. The baseline methods include: Query2Doc (Q2D): Generate pseudo-documents for query expansion (Wang and andFuru Wei, 2023); Query2Expansion (Q2E): Expand queries with relevant keywords (Jagerman et al., 2023); Query2CoT (Q2C): Apply Chain of Thoughts for query reformulation (Wei et al., 2022); GenQREnsemble (GenQRE): Use multiple prompts to generate and concatenate keywords with initial query(Dhole and Agichtein, 2024); GenQRFusion: Extend GenQREnsemble with keyword fusion method."}, {"title": "4.1.4 Prompts Used", "content": "Baseline methods utilize varying numbers of prompts: Q2D, Q2E, and Q2C each use four few-shot prompts, GenQRE uses ten (Dhole and Agichtein, 2024), and GenQR-Fusion randomly selects three prompts with a fusion strategy (Dhole et al., 2024). Our framework utilizes five types of prompts: three for diverse query generation (Contextual Expansion, Detail Specific, Aspect Specific), one for Clustering-Generation, and one for Scoring. The Scoring prompt evaluates generated queries based on Relevance, Specificity, Clarity, Comprehensiveness, and Usefulness, assigning scores from 1 to 100, with a threshold 0 = 60 to ensures only high-scoring reformulations contribute to the final aggregated query. Detailed descriptions of all prompts are provided in the appendix."}, {"title": "4.2 Experimental Analysis", "content": "In our experiments, we evaluated the performance of our proposed GenCRF framework across six datasets from the BEIR benchmark, as shown in Table 1. Ensemble-based methods such as GenQRE and GenQRFusion outperforms single prompted methods on average, with GenQRFusion demonstrating particularly strong results. This indicates that ensemble based apporaches using multiple prompts to expand retrieval queries enhance information gaining and improve retrieval performance. However, our proposed GenCRF methods, such as SimDW and ScoreDW, further improve upon these ensemble-based approachs. Our strategies consistently outperform GenQRE and GenQRFusion across all datasets. This result demonstrates"}, {"title": "4.3 Ablation Studies", "content": "To validate GenCRF's robustness, we conduct ablation studies on key parameters using ArguAna and CQADupStack-English datasets. For comparison with other methods, particularly those that do not use weighted aggregation, we introduce Direct Concatenation (DC) method:\n$Q^{DC}_{agg} = q_{init} + [SEP] + \\sum_{i=1}^{\\lvert Q_{final}\\rvert} (q_{final,i} + [SEP])$\nDC combines the initial query $q_{init}$ with all reformulated queries using [SEP] tokens as separators. Also, we introduce Fixed Weights (FW) method for ablation study:\n$Q^{FW}_{agg} = w_o \\cdot q_{init} + \\frac{1 - w_o}{\\lvert Q_{final}\\rvert}  \\sum_{i=1}^{\\lvert Q_{final}\\rvert} q_{f, i}$\nHere, $w_o$ is the fixed weight for the initial query, and $(1 - w_0)/|Q_{final}|$ is the equal weight applied to each reformulation query."}, {"title": "4.3.1. Initial Query Weight. To determine the optimal weight for Weighted Aggregation, we investigate on both Fixed Weights (FW) and ScoreDW/FT strategies. We vary $w_o$ from 0.3 to 0.9 in 0.1 increments, evaluating nDCG@10 for both strategies. As shown in Figure 2, $w_o$ = 0.7 achieves optimal performance across both strategies and datasets. For fairness in building baseline, we applied this optimal $w_o$ = 0.7 to GenQRFusion method as well.", "content": "4.3.1. Initial Query Weight. To determine the optimal weight for Weighted Aggregation, we investigate on both Fixed Weights (FW) and ScoreDW/FT strategies. We vary $w_o$ from 0.3 to 0.9 in 0.1 increments, evaluating nDCG@10 for both strategies. As shown in Figure 2, $w_o$ = 0.7 achieves optimal performance across both strategies and datasets. For fairness in building baseline, we applied this optimal $w_o$ = 0.7 to GenQRFusion method as well."}, {"title": "4.3.2. Impact of Prompt Quantity. We investigate the effect of varying the number of prompt types on retrieval performance, measured by nDCG@10 scores. The prompts included \"contextual expansion,\" \"detail specific,\" \"aspect specific,\" and \"clarity enhancement.\" We evaluated all possible combinations of these prompts and calculated the average performance for each number of prompts used. As shown in Figure 3, performance typically improves when increasing from 1 to 3 prompts, but adding a fourth prompt does not lead to further enhancements and conversely decreases performance. Thus, we selected 3 prompt types as the optimal configuration for maximizing retrieval performance in this study.", "content": "4.3.2. Impact of Prompt Quantity. We investigate the effect of varying the number of prompt types on retrieval performance, measured by nDCG@10 scores. The prompts included \"contextual expansion,\" \"detail specific,\" \"aspect specific,\" and \"clarity enhancement.\" We evaluated all possible combinations of these prompts and calculated the average performance for each number of prompts used. As shown in Figure 3, performance typically improves when increasing from 1 to 3 prompts, but adding a fourth prompt does not lead to further enhancements and conversely decreases performance. Thus, we selected 3 prompt types as the optimal configuration for maximizing retrieval performance in this study."}, {"title": "4.3.3. Impact of Generated Query Count. We explore the effect of varying the number of generated queries N per prompt on retrieval performance, as described in Section 3.2. Experiment were conducted with N ranging from 1 to 4 for both SimDW and ScoreDW-FT strategies across ArguAna and CQADupstack-English dataasets. As shown in Table 2, generating 2 queries per prompt consistently yields the best performance across both datasets and strategies. The performance decline for N suggests that additional generation may introduce noise or redundancy, which may be attributed to the excessive length of single-prompt generated responses or their mutual interference.", "content": "4.3.3. Impact of Generated Query Count. We explore the effect of varying the number of generated queries N per prompt on retrieval performance, as described in Section 3.2. Experiment were conducted with N ranging from 1 to 4 for both SimDW and ScoreDW-FT strategies across ArguAna and CQADupstack-English dataasets. As shown in Table 2, generating 2 queries per prompt consistently yields the best performance across both datasets and strategies. The performance decline for N suggests that additional generation may introduce noise or redundancy, which may be attributed to the excessive length of single-prompt generated responses or their mutual interference."}, {"title": "4.3.4. Iterative Optimization with QERM. We examine the impact of iterative optimization using the Query Evaluation Rewarding Model (QERM) on our ScoreDW-FT-QERM framework, with iteration counts ranging from 1 to 4, as shown in table 3. We observe that the best iteration count is 2, significantly surpassing the score where iteration count is 4. The result indicate that the integration of QERM with two iterations achieve an optimal result, allowing the ScoreDW-FT-QERM framework to adaptively optimize query generation and clustering, resulting in more precise and relevant retrieval outcomes across diverse datasets.", "content": "4.3.4. Iterative Optimization with QERM. We examine the impact of iterative optimization using the Query Evaluation Rewarding Model (QERM) on our ScoreDW-FT-QERM framework, with iteration counts ranging from 1 to 4, as shown in table 3. We observe that the best iteration count is 2, significantly surpassing the score where iteration count is 4. The result indicate that the integration of QERM with two iterations achieve an optimal result, allowing the ScoreDW-FT-QERM framework to adaptively optimize query generation and clustering, resulting in more precise and relevant retrieval outcomes across diverse datasets."}, {"title": "5 Generation study and Discussions", "content": "Our GenCRF has demonstrated superior performance in baseline comparisons, effectively capturing query intent compared to existing methods. Figure 4 reveals GenCRF outperforming GenQR methods, even with basic aggregation strategies such as Direct Concatenation (DC) and Fixed Weights (FW). While GenQRFusion relies on keyword-based methods that often fails to capture the underlying query intent, GenCRF's prompts explore various query facets, resulting in more comprehensive reformulations that capture nuances keyword-based methods neglect.\nAs shown in Figure 5, our individual prompts (Contextual Expansion, Detail Specific and Aspect Specific) outperform baseline methods such as Q2E, Q2D and CoT. Our prompts capture deeper query semantics, contrasting with conventional methods' focus on surface-level information. Notably, our Cluster-Generated method, which combines diverse insights from various prompts, achieves the best results, demonstrating the effectiveness of integrating multiple perspectives in query reformulation-an approach absent in single-prompt methods."}, {"title": "6 Conclusion", "content": "We present the Generative Clustering and Reformulation Framework (GenCRF), which demonstrates significant advancements over existing competitive baseline methods, achieving up to 12% increase on BEIR benchmark. Our approach combines diverse prompting strategies and clustering refinement to accurately capture and reformulate query intents. We introduced our optimization techniques including weighted aggregation methods: SimDW, ScoreDW, ScoreDW-FT and the evaluation rewarding model QERM, enhancing GenCRF's performance and offering a more precise, user-centric information retrieval experience. Extensive ablation studies have confirmed the reasonableness and robustness of the GenCRF framework by exploring key parameters and settings across datasets from the BEIR benchmark. Future work could explore GenCRF's application to real-world search scenarios, potentially enhancing its effectiveness in practical information retrieval contexts."}, {"title": "A Appendix A. Overview", "content": "This appendix provides a comprehensive overview of the methodologies and experimental setups employed in our study. We detail the prompts used in our baseline models and our GenCRF framework, including those used for ablation studies. Additionally, we present our methods for finding optimal simlilarity and score thresholds, also conduct a cluster anaylsis within the GenCRF framework."}, {"title": "B Appendix B. Prompts", "content": "We share five prompts utilized in our experiments, including: Query2Doc (Q2D): Generate pseudo-documents and expands queries (Wang and andFuru Wei, 2023); Query2Expansion (Q2E): Expand queries with relevant keywords (Jagerman et al., 2023); Query2CoT (Q2C): Reformulate queries based on Chain of Thoughts prompting (Wei et al., 2022); GenQREnsemble (Gen-QRE): Applies multiple prompts to generate various keyword sets concatenated within the initial query(Dhole and Agichtein, 2024) and GenCRF (Ours)."}, {"title": "B.1 Q2D, Q2E, Q2C", "content": "B.1 Q2D, Q2E, Q2C"}, {"title": "B.2 GenQRE", "content": "B.2 GenQRE"}, {"title": "B.3 GenCRF", "content": "B.3 GenCRF"}, {"title": "C Appendix C. Similarity and Score Threshold for Dynamic Weights", "content": "We conducted comprehensive experiments to determine the optimal similarity and score thresholds."}, {"title": "C.1 Similarity Threshold", "content": "We experimented with similarity ranging from 0.1 to 0.3 to identify the optimal threshold. As illustrated in Figure 6, a threshold of 0.2 yielded the highest nDCG@10 score across all the ablation datasets. This finding demonstrates that an appropriate threshold can effectively enhance the"}, {"title": "C.2 Score Threshold", "content": "We also investigated the impact of various score thresholds on the performance of our model. Figure 7 shows the results of experiments with score thresholds ranging from 40 to 70. For both datasets, a score threshold of 60 resulted in the highest nDCG@10 scores, which suggests that our threshold effectively filters our lower-quality generated queries while retaining those that contribute most to improved retrieval performance."}, {"title": "D Appendix D. Cluster Analysis", "content": "We analyze how the GenCRF framework clusters data across our main datasets, focusing on the distribution of cluster counts and the similarity between clusters."}, {"title": "D.1 Cluster Counts", "content": "As shown in Figure 8, the GenCRF framework predominantly form three clusters across all datasets, followed by two clusters, with a small proportion of single clusters. The result indicates that our framework often identifies multiple distinct aspects of query intents. This multi-faceted clustering approach likely contributes to the framework's ability"}, {"title": "D.2 Similarity between Clusters", "content": "Figure 9 illustrates the similarity between clusters when two or three clusters are formed. We observe that 2 cluster formation consistently show higher similarity scores compared to three cluster formations, and the similarity scores for both 2 cluster and 3 cluster formations are relatively high, indicating greater diversity, and making these clusters more effective at capturing different aspects of query intents."}]}