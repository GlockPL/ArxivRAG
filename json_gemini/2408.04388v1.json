{"title": "MM-Forecast: A Multimodal Approach to Temporal Event Forecasting with Large Language Models", "authors": ["Haoxuan Li", "Zhengmao Yang", "Yunshan Ma", "Yi Bin", "Yang Yang", "Tat-Seng Chua"], "abstract": "We study an emerging and intriguing problem of multimodal temporal event forecasting with large language models. Compared to using text or graph modalities, the investigation of utilizing images for temporal event forecasting has not been fully explored, especially in the era of large language models (LLMs). To bridge this gap, we are particularly interested in two key questions of: 1) why images will help in temporal event forecasting, and 2) how to integrate images into the LLM-based forecasting framework. To answer these research questions, we propose to identify two essential functions that images play in the scenario of temporal event forecasting, i.e., highlighting and complementary. Then, we develop a novel framework, named MM-Forecast. It employs an Image Function Identification module to recognize these functions as verbal descriptions using multimodal large language models (MLLMs), and subsequently incorporates these function descriptions into LLM-based forecasting models. To evaluate our approach, we construct a new multimodal dataset, MidEast-TE-mm, by extending an existing event dataset MidEast-TE-mini with images. Empirical studies demonstrate that our MM-Forecast can correctly identify the image functions, and further more, incorporating these verbal function descriptions significantly improves the forecasting performance. The dataset, code, and prompts are available at https://github.com/LuminosityX/MM-Forecast.", "sections": [{"title": "1 Introduction", "content": "Temporal event forecasting aims to predict future events according the observed events in history. The forecasting of critical events, such as pandemic outbreak, civil unrest, and international conflicts, can help shape policies in advance and minimize potential impacts [50]. Due to its great potential application value, temporal event forecasting [14, 23, 28, 30] has garnered increasing attention from both the academic and industrial community. Despite promising progress, current methods have ignored the rich multimodal information, e.g., images, leaving this an unexplored research gap.\nWith the enormous success of LLMs, an increasing number of studies [4, 16, 24, 27, 44, 46, 49] have been exploring LLMs to tackle the temporal event forecasting problem. These pioneering works explore the application of LLMs in the task of temporal event forecasting, leveraging techniques such as in-context learning (ICL) [16, 49], instruction tuning [27, 44], and retrieval-augmented generation (RAG) [4, 37]. Compared to LLM-based methods, traditional methods have several shortcomings in terms of effectiveness, flexibility, and scalability. Specifically, traditional non-LLM methods [15, 22, 29, 30, 33], whether based on structured or unstructured data, typically require large-scale well-annotated datasets. Moreover, model selection is often a challenge for these traditional methods due to high computational costs. Additionally, traditional methods generally require separate training for different datasets, as a result, they often struggle to make fast adaptation w.r.t. frequent changing in dataset and temporal shifts. Therefore, applying LLMs to the task of temporal event forecasting is a worthwhile direction to explore [16]. However, all of the existing LLM-based methods only consider a single modality, such as text [16] or graph [27], while ignoring the prevalent visual modality, i.e., images. Some previous works [19, 20] have justified that images are helpful in multimodal event detection [20] and extraction [21, 41], while none of them investigate images\u2019 utility in temporal event forecasting.\nTo bridge this gap, we aim to integrate images into temporal event forecasting and construct multimodal temporal event forecasting models. However, it is a non-trivial objective due to the following challenges. First, it is necessary to clarify the function between visual information and other modal information, i.e., the interplay between visual and non-visual modalities. Next, we need to figure out how the function between two modalities can contribute to the task of temporal event forecasting. Second, previous works [41] that explores the image function typically require large amounts of labeled training data. Additionally, images serve different functions for different specific tasks, so these methods often struggle to generalize effectively to other task definitions. Therefore, there is a pressing need to design an effective method to identify the function between modalities and seamlessly integrating them into LLM-based forecasting models.\nTo address the aforementioned challenges, we propose a novel framework for multimodal temporal event forecasting, named as MM-Forecast. Specifically, we identify two essential functions of images, i.e., highlighting and complementary. As illustrated in Figure 1, when the function of associated image is highlighting, the image plays the role of emphasizing the key events. In contrast, when the function of associated image is complementary, the image provides supplementary information that complements the textual content. In order to recognize these two types of functions, we propose an Image Function Identification module that is based on Multimodal LLMs (MLLMs) due to their superior multimodal understanding and reasoning capabilities in zero-shot settings [18]. This proposed module is designed to recognize the function of images in historical events, and then transform this information into verbal descriptions that can be seamlessly integrated into the LLM-based event forecasting model. Equipping this Image Function Identification module into the overall framework, we integrate it into two distinct LLM-based forecasting models, i.e., one based on the in-context learning (ICL) method [16], and the other based on the retrieval-augmented generation (RAG) technique [17]. In order to evaluate our approach, we construct an exploratory dataset by incorporating images into an existing dataset MidEast-TE-mini [4]. We name this new dataset MidEast-TE-multimodal (short as MidEast-TE-mm). In the final evaluation, with the enhancement of visual information, the temporal event forecasting task achieves superior forecasting accuracy compared to the unimodal approach. The experimental results illustrate that our method accurately recognizes the function of images in various aspects. Furthermore, the findings demonstrate that multimodal temporal forecasting represents a potential and promising research direction worthy of further exploration. The main contributions are as follows:\n\u2022 To the best of our knowledge, this is the first comprehensive study of exploring visual information for temporal event forecasting in the era of LLMs.\n\u2022 We identify two main functions that images play in temporal event forecasting, and design a framework to recognise and integrate visual information into LLM-based forecasting models.\n\u2022 Extensive experiments justify that our framework is able to identify the functions of images and visual information can enhance the performance of temporal event forecasting. Furthermore, these findings have led to several noteworthy and promising directions for future research."}, {"title": "2 Related Works", "content": "We survey the related works of temporal event forecasting and LLMs for event analysis."}, {"title": "2.1 Temporal Event Forecasting", "content": "Temporal event forecasting centers on predicting future event occurrences based on historical events, and the typical approaches can be categorized by event format: time series, structured, and unstructured events. Regarding the time series paradigm, existing works [2, 23, 31] typically represent events as an ordered sequence of data points that describe the progression of actions or occurrences. However, this paradigm inherently fails to represent multiple relationships among entities. Alternatively, another branch of works [8, 15, 22, 29, 33, 36, 38, 45] focus on the prediction of structured events, i.e., using graph to represent events, which is known as temporal knowledge graph (TKG). Recent works[29, 30] introduce context into temporal event forecasting models, enhancing the prediction performance by elaborating the event\u2019s occurrence situation. In addition, several studies have explored the use of unstructured textual representations of temporal events, where each atomic event is generated from multi-document summaries [11] or event chains [13]. Nonetheless, all of them design forecasting models relying on single modality data. Some works [21, 41] explore the image utility in event extraction task, while none of them investigate images\u2019 utility in temporal event forecasting."}, {"title": "2.2 LLMs for Event Analysis", "content": "The tremendous success of LLMs in recent years, exemplified by ChatGPT and its numerous successors [5, 6, 42, 48], has inspired researchers to explore the application of these powerful models to various event-related tasks [4, 7, 16, 24, 46, 49]. One area of research focuses on temporal understanding, where LLMs are tested for the task of temporal event ordering or storyline understanding [32, 47, 51]. More works focus on leveraging LLMs to tackle the typical task of temporal reasoning [39, 43], while the task of forecasting receives much less attention. Deng et al. [7] surveyed the recent advances in event modeling, ranging from graph neural networks to LLMs. Specifically, GENTKG [24] improves the selection of historical event inputs by a temporal logical rule-based retrieval strategy. Beyond specific methods, more works are focusing on benchmarking LLMs\u2019 capability in temporal event forecasting. Zhang et al. [49] propose a method to evaluate the proficiency of LLMs in handling temporal dynamics and understanding extensive text through three distinct tasks. And Ye et al. [46] introduce a novel benchmarking environment designed to rigorously assess and advance the capabilities of LLM agents for international event forecasting over time. Furthermore, Chang et al. [4] propose a unified dataset of structured and unstructured data, and systematically evaluates LLM-based methods on the task of text-involved temporal event forecasting. However, these existing LLM-based methods still solely rely on single-modality data, potentially missing valuable information from other modalities, such as images. With the success of LLMs, MLLMs, such as LLaVA [25], and Gemini [40], have emerged as promising means for unifying visual and textual modalities. These MLLMs have demonstrated impressive performance gain across various visual-language tasks [1, 3, 10, 25], suggesting their potential in the task of temporal event forecasting by leveraging visual information."}, {"title": "3 Our Approach: MM-Forecast", "content": "The overall framework of our proposed approach is depicted in Figure 2. We first formally define the multimodal temporal event forecasting task in Section 3.1. Second, we specifically introduce the key module of Image Function Identification in Section 3.2. Finally, we elaborate on how to integrate the recognized image functions into LLM-based forecasting models in Section 3.3."}, {"title": "3.1 Problem Formulation", "content": "To give formal definition of the problem, we separate it into two subtasks given the different representations of historical information.\nStructured Event Forecasting (Graph\u00b9). This formulation defines each event as a quadruple $(s, r, o, t)$, which is also called an atomic event, where $s, r, o, t$ corresponds to the subject, relation\u00b2, object, and timestamp. At each timestamp $t$, all the quadruples form an event graph, denoted as $G_{t}=\\{(s, r, o, t)\\}_{N}^{i=1}$, where $N$ is the number of events at timestamp $t$. Recent work[30] introduces the concept of complex event (CE) into the structured event representation by document clustering, elaborating the event\u2019s occurrence situation or context. Specifically, each atomic event is extended from a quadruple to a quintuple, i.e., $(s, r, o, t, c)$, where $s \\in \\mathcal{E}, r \\in \\mathcal{R}, o \\in \\mathcal{E}$, and $c \\in \\mathcal{C}$ represent the subject, relation, object, and CE, respectively; $\\mathcal{E}, \\mathcal{R}$ and $\\mathcal{C}$ are the entity set, relation set and complex context set. Correspondingly, the event graph at each timestamp will be extended as $G_{t}=\\{(s, r, o, t, c)\\}_{N}^{i=1}$. Furthermore, in addition event graph, there are images associated with structured events, denoted as $V_{t}=\\{v_{1}, v_{2}, ..., v_{m}\\}_{m=1}^{M}$, where $M$ is the number of images at timestamp $t$. Finally, the structured event forecasting task can then be formulated as follows: given the historical event graphs $G_{<t} = \\{G_{0}, G_{1}, ..., G_{t-1}\\}$ and associated images $V_{<t} = \\{V_{0}, V_{1}, ..., V_{t-1}\\}$ before timestamp $t$, and a query $(s, r, t)$ or $(s, o, t)$, the goal is to predict the missing object $o$ or relation $r$.\nUnstructured Event Forecasting (Text\u00b3). In addition to the structured event representation, we also consider the unstructured event representation, where the historical information is represented in the form of textual sub-events, i.e., $A_{t} = \\{a_{1}, a_{2}, ..., a_{k}\\}_{k=1}^{K}$ and $A_{t} \\in \\mathcal{A}$, where $a_{k}$ denotes the $k$-th textual sub-events and $\\mathcal{A}$ denotes the corpus of textual sub-events. The textual sub-events are obtained by summarizing the content of news articles. Similar to structured event forecasting, textual sub-events have associated images, denoted as $V_{<t} = \\{V_{0}, V_{1}, ..., V_{t-1}\\}$. The unstructured event forecasting task can be formulated as: given the historical textual sub-events $A_{<t} = \\{A_{0}, A_{1}, ..., A_{t-1}\\}$ and associated images $V_{<t} = \\{V_{0}, V_{1}, ..., V_{t-1}\\}$ before timestamp $t$, and a query $(s, r, t)$ or $(s, o, t)$, the goal is to predict the missing object $o$ or relation $s$."}, {"title": "3.2 Image Function Identification", "content": "Identifying the function of images in the temporal event forecasting is the key to utilize multimodal visual information. In news articles, images play a vital role not only in attracting readers but also in completing and enriching the textual content, especially key event content. We will identify the image functions into three categories, i.e., highlighting, complementary, and irrelevant, during the dataset construction stage. Excluding the irrelevant images, the others serve distinct roles in the temporal event forecasting task. We propose an Image Function Identification module to recognize these functions as verbal descriptions using MLLMs, and subsequently incorporate these function descriptions into LLM-based forecasting models. Specifically, when the function of associated image is highlighting, the visual elements directly support and highlight the key sub-events described in the text. These \"highlighting\" sub-events, substantiated by corroborating information across modalities, can be identified as key events. To determine which sub-event is a key event, we leverage the MLLMs to analyze the images and sub-events from multiple aspects, including main objects, celebrities, activities, environment, and labeled items. In cases where the function of associated image is complementary, the visual content contains information that supplements and extends what is covered in the news text. To more effectively extract the relevant supplementary information, we consider the following aspects: 1) identify the main subject of the image as the central point, 2) directly relate the extracted information to the news event in the article, 3) prioritize the most newsworthy visual elements, 4) ensure all information comes directly from the provided news article without fabrication, and 5) aim for a concise summary using clear language. By analyzing the interplay between visual images and textual content within news articles, we can gain a more comprehensive understanding of the underlying events and better contextualize the temporal evolution of historical events. Ultimately, the prompts utilized in making predictions are shown below:\nSYSTEM:\nYou are an assistant to perform event forecasting with the following rules:\n1. The atomic event is the basic unit describing a specific event, typically presented in the form of a quadruple (S, R, O, T), where S represents the subject, R represent the relation, O represents the object, and T represents the relative time.\n2. When formulating the ultimate prediction, the preeminent factor to be meticulously weighed and scrutinized is the [Key Events]. Complementing this paramount consideration is the [Related events], which, though ancillary in nature, serves as a valuable adjunct, furnishing pertinent contextual details and auxiliary insights to fortify the predictive analysis.\n3. Given a query of (S, R, T) in the future and the list of historical events until t, event forecasting aims to predict the missing object.\nUSER:\n[Query]: (S, O/R, T)\n[Key Events]: xxx.\n[Related Events]: xxx.\n[Options]: A.xxx B.xxx C.xxx D.xxx E.xxx\nThe key events are explicitly highlighted within the prompts, while complementary information is provided as additional relevant events."}, {"title": "3.3 Forecasting Framework", "content": "Given there are few established studies of using LLMs for event forecasting, we consider two representative approaches, i.e., In-context Learning (ICL) [16] and Retrieval Augmented Generation (RAG) [17]. Each of these two methods can accept both structured and unstructured historical input, and answer the structured forecasting questions."}, {"title": "3.3.1 In-context Learning (ICL)", "content": "In-context learning leverages both intrinsic and extrinsic factors to construct historical events. Specifically, the intrinsic factors of an event are related to its inherent elements, particularly the subject. In contrast, the extrinsic factors are driven by the contextual environment surrounding the event. Therefore, whether the data is structured or unstructured, we construct the historical events based on the subject and the complex event, separately. The details are as follows:"}, {"title": "4 Experiments", "content": "We conduct experiments to evaluate the proposed approach, and answer the following research questions:\n\u2022 RQ1: What is the overall performance of temporal event forecasting methods by including visual information?\n\u2022 RQ2: How do the highlighting and complementary functions of images affect the forecasting performance?\n\u2022 RQ3: How do different LLM backbones as well as fine-tuning affect the performance?"}, {"title": "4.1 Experimental Settings", "content": "We introduce the experimental settings, including the dataset, the methods compared, and the implementation details."}, {"title": "4.1.1 Dataset", "content": "We build our dataset based on MidEast-TE-mini [4], which includes structured atomic events and news articles. We aim to add images that correspond to the events in the dataset, hence we will have the data in visual modality. An intuitive way is to download the web page according to the URL provided by the original dataset. However, the original web page always contains a lot of irrelevant images, such as advertisement images, that are cumbersome and difficult to be accurately filtered out. Instead of directly solving this problem, we propose an alternative solution that we use Google Image Search\u2074 to search the images using the news article title as the query. Among the returned images, we select the top-ranked ones as the associated images of the news article. In order to further filter out irrelevant images, we instruct the Gemini-1.0-Pro-Vision model to determine the relevance of images to news articles. We give three options: highlighting, complementary and irrelevant. Highlighting means that the images and the content of the news are highly matched, and complementary means that the image has supplementary meaning to the content of the news. Images beyond these two are regarded as irrelevant. We further remove images that are classified as irrelevant. Finally, we name our dataset as MidEast-TE-multimodal, short as MidEast-TE-mm."}, {"title": "4.1.2 Compared Methods", "content": "The compared methods are categorised into non-LLM-based methods and LLM-based methods. For non-LLM-based methods, only text or graph modalities are involved, since these methods architecture are fixed. We train the models on the training set, selecting the best-performing model based on the validation set results, and obtain the final results of the testing set. For LLM-based methods, we use the proprietary LLMs due to their superior performance compared to open-source LLMs. Therefore, testing is generally done in a zero-shot manner, i.e., directly test them on the testing set. The specific methods are shown below:\n\u2022 ConvTransE [36]: This method employs a convolutional neural network (CNN) and a translational operation to capture the relational patterns within triplet data.\n\u2022 RGCN [35]: RGCN leverages a graph convolutional neural network (GCN) to capture the diverse relations between entities.\n\u2022 RE-GCN [22]: RE-GCN utilizes a combination of GCN and recurrent neural network (RNN) to capture both the relational patterns and temporal dynamics.\n\u2022 LoGo [30]: This method models relationships within and between complex events from both local and global perspectives.\n\u2022 GPT-3.5-Turbo7: The GPT-3.5-turbo model is the prevalent iteration of the GPT (Generative Pre-trained Transformer) language model developed by OpenAI5."}, {"title": "4.1.3 Implementation Details", "content": "To ensure the reproducibility, we fixed the temperature parameter of the proprietary LLMs used to 0 and set the seed parameter to a constant value. When making forecasting, we limit the maximum output token length to 256 to prevent invalid responses. To ensure fairness across the experiments, the history that can be retrieved is set to 30 days. Notably, the retrieval models that we employ include: BM25 [34], Contriever [12], and LlamaIndex [26]. Additionally, considering the limitation of the context window, we further restrict the maximum number of sub-events in the historical context to 50. Following previous methods [4], we employ the Accuracy (Acc) as the evaluation metric."}, {"title": "4.2 Performance Comparison (RQ1)", "content": "We analyze our model\u2019s performance, by comparing various baseline methods on different experimental settings, different input forms, and different retrieval models."}, {"title": "4.2.1 Performance w.r.t. Various Settings", "content": "The overall performance comparison is presented in the Table 1. To comprehensively explore and evaluate methods, we conduct experiments across multiple dimensions, including the format of data representation (Text or Graph), the construction of historical information (RAG-based or ICL-based), and the prediction objective (Object or Relation). Clearly, we have the following observations."}, {"title": "4.2.2 Performance w.r.t. Directly Using Images", "content": "To illustrate the limitations of existing MLLMs in the task of temporal event forecasting, we also conduct experiments using the Gemini-1.0-Pro-Vsion model [40] and directly consuming the images in the sub-events. Specifically, this approach leverages the inherent image understanding capabilities of the Gemini-1.0-Pro-Vision model, which embeds image patches as features and seamlessly concatenates thes image features with textual features. From Table 1, we can observe that the accuracy of using images directly is not only lower than our MM-Forecast, but also even worse than the method using only textual data (Uni-modal methods). This illustrates that existing proprietary MLLMs still struggle to make effective event forecasting with multiple images, and reflects the superiority of our MM-Forecast."}, {"title": "4.2.3 Performance w.r.t. Various Retrieval Models", "content": "The choice of retrieval model may have a significant impact on forecasting. The experiments here involve only unstructured event forecasting, since the structured approach employs retrieval based on keyword search techniques. To explore the effect of retrieval model, we adopt three different retrieval models, i.e., BM25 [34], Contriver [12], and LlamaIndex [26], then equip them into our forecasting framework, and obtain the forecasting results. From the results in Table 2, we can observe that the performance progressively improves by using stronger retrieval models, with LLamaIndex performing the best, followed by Contriver, and then BM25. These results verify that stronger retrieval capabilities lead to better forecasting performance, suggesting that retrieval-oriented method design is a promising direction for future research. This phenomenon is consistent with the observation concluded from recent works [4]."}, {"title": "4.3 Study of the Image Functions (RQ2)", "content": "4.3.1 Effects of Image Functions. We conduct ablation experiments for the highlighting and complementary function of images. The results are shown in Figure 3. First, the model that leverages both the highlighting of key events and the complementary information performs the best across the experimental settings. In addition, the performance of the model with only key events highlighted is sub-optimal. This illustrates the effectiveness of the highlighting function of images, and it elicit the fact that highlighting and complementary reinforce each other to achieve even better prediction results. Second, we can observe that in some settings (Text-ICL, Text-RAG), the performances of the model with only complementary information are even worse than the baseline model. The possible reason for this is that the offering of complementary information also introduces more noise and therefore leads the degradation of performance. Third, the performance of RAG-based method is obviously worse than the ICL-based method in the relation prediction task, meanwhile, such performance gap does not exist in the entity prediction task. This is may because that relation prediction is easier than object entity prediction, as mentioned in section 4.2.1. As a result, ICL-based historical events may already contain enough information to make accurate relation prediction, whereas the retrieval model may not retrieve relevant information instead."}, {"title": "4.3.2 Analysis of the Image Function Identification", "content": "In addition to overall forecasting performance analysis, we conduct in-depth study to directly assess the efficacy of highlighting and complementary function. Specifically, we design additional experiments at the data level and prompt level to further verify the function of images. At the data level, we randomly sample 100 images of two categories respectively, and then judge the correctness of the classification by the powerful MLLM GPT-4-Vision 8. As shown in Table 3, both classification of highlighting and complementary functions show high accuracy. Furthermore, we can observe that the accuracy of highlighting is lower than that of complementary on all settings, which should be due to its more strict definition. The high accuracy of image functions in both LLM and human identification indicate that the images we used can indeed play the highlighting and complementary functions. In addition to direct assessment of the quality of image function identification, we conduct another ablation study by replacing our identified functions with randomly selected sub-events. Looking into the forecasting results in Table 4, random selection of sub-events leads to a decrease in forecasting accuracy, indicating that correct image function identification is crucial to the forecasting."}, {"title": "4.4 Performance on Open-source and Fine-tuned LLMs (RQ3)", "content": "All of the above LLM-based forecasting backbones are implemented using proprietary LLMs in the zero-shot manner without any fine-tuning. We are interested in how our method performs on open-source LLMs, especially finetuned open-source LLMs. Addressing this intriguing question, we select one of the most popular open-source LLMs, i.e., Vicuna-7b, to replace the forecasting backbone LLM in our framework, with both zero-shot manner and fine-tune following typical instruction tuning with QLORA [9]. The results of object entity prediction are presented in Table 5, which also includes the best results for proprietary LLMs and non-LLM methods. We observe that the zero-shot performance of Vicuna-7B is worse than its corresponding performance on proprietary LLMs, owing to the inherent capacity gap. However, after fine-tuning, Vicuna-7B achieves substantial performance gains, not only surpassing the proprietary LLMs but also outperforming all the non-LLM methods. In addition to fine-tuning the LLMs on object entity prediction, we also fine-tuning on the relation prediction task, as shown in Table 6. In both the text and the graph settings, the relation prediction results are consistent with the entity prediction, i.e., fine-tuned LLMs achieve the best performance. These results demonstrate the significant potential of fine-tuning LLMs for the temporal event forecasting task."}, {"title": "5 Conclusion and Future Work", "content": "In this paper, we studied an emerging and interesting problem of multimodal temporal event forecasting. We identified two essential image functions in the scenario of temporal event forecasting, i.e., highlighting and complementary. Then, we introduced MM-Forecast, a novel framework that leverages visual information to enhance temporal event forecasting. By recognizing the highlighting and complementary functions of images and translating them into verbal descriptions, we were able to seamlessly integrate this visual information into LLM-based forecasting models. Ultimately, this enabled the integration of visual information to enhance temporal event forecasting task.\nLooking ahead, there are numerous avenues for future work to address the key challenges. In particular, we would like to highlight three distinct aspects that warrant further exploration. First, multi-images relationship need to be considered. There are inherent relationships between images in related historical events, and these relationships are also important for event forecasting. Second, seeing is believing. Images have significant effects on the event forecasting task rather than accuracy improvement, that is credibility or trustability. Third, our current solution is still a multi-step pipeline, while devising an end-to-end approach using MLLMs is intriguing to explore in the future."}, {"title": "Acknowledgments", "content": "This work is partially supported by the National Natural Science Foundation of China under grant 62220106008, U20B2063 and 62102070. This work is also partially supported by Sichuan Science and Technology Program under grant 2023NSFSC1392. This research is also supported by Asian Institute of Digital Finance and NEXT Research Center."}, {"title": "A Appendix", "content": "A.1 Prompts: Image Function\nIn this section, we show all the prompts that need to be used in the image function identification module. As show in Table 7, the first row is the prompt for image function recognition, which is mainly from the perspective of the subject background and the specific event to judge the function of the image. The last two rows are the prompts of the different functions of the images to achieve their respective functions and transform their information into verbal descriptions. Eventually, the verbal information will be integrated into the LLM-based event forecasting model.\nA.2 Case study: Image Function\nTo further illustrate that our approach does indeed identify truly key events and the required complementary information, we provide additional examples. In the first example of the highlighting function, the image directly depicts Ocasio-Cortez, with the background appearing to be the Congressional sites, thereby emphasizing the relevant key event. Correspondingly, the key event also mentions the relationship between Congress and Ocasio-Cortez. Consequently, an accurate prediction is achieved. In the second example of the highlighting function, the key event highlighted by the image directly mentions the disqualification of Ali Larijani from the election, which perfectly aligns with the results that need to be predicted and the information provided to present those results. For the first example of complementary functions, the image provides information about the signing of a free trade agreement between Turkey and the United Kingdom. While enhanced trade has the potential to lead to employment and economic growth, the image offers complementary information on the role of labor. Therefore, an accurate forecast is achieved. In the second example about the complementary function, the image shows Bernie Sanders who is a democratic progressive socialist like Ocasio-Cortez. They share many commonalities and connections to Congress, which can provide supplementary information to more accurately predict the outcome. Through these examples, the distinct functions of highlighting key events and providing complementary information are elucidated, substantiating the effectiveness of our approach in leveraging multimodal information for accurate temporal event forecasting."}]}