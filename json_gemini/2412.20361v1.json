[{"title": "Safe Multiagent Coordination via Entropic Exploration", "authors": ["Ayhan Alp Aydeniz", "Enrico Marchesini", "Robert Loftin", "Christopher Amato", "and Kagan Tumer"], "abstract": "Many real-world multiagent learning problems involve safety concerns. In these setups, typical safe reinforcement learning algorithms constrain agents' behavior, limiting exploration-a crucial component for discovering effective cooperative multiagent behaviors. Moreover, the multiagent literature typically models individual constraints for each agent and has yet to investigate the benefits of using joint (team) constraints. In this work, we analyze these team constraints from a theoretical and practical perspective and propose entropic exploration for constrained multiagent reinforcement learning (E2C) to address the exploration issue. E2C leverages observation entropy maximization to incentivize exploration and facilitate learning safe and effective cooperative behaviors. Experiments across increasingly complex domains show that E2C agents match or surpass common unconstrained and constrained baselines in task performance while reducing unsafe behaviors by up to 50%.", "sections": [{"title": "1 INTRODUCTION", "content": "Training agents to operate in real-world scenarios requires adhering to safety specifications. This is particularly challenging in multiagent environments (e.g., search-and-rescue missions [29] and remote exploration tasks [18]), where agents must discover highly coordinated behaviors while satisfying the safety requirements. However, learning these behaviors is difficult as it requires optimizing for (at least) two objectives-a team objective aiming to solve the task and the safety objective(s). Hence, extending single-agent safe reinforcement learning (RL) algorithms to multiagent settings often fails to cope with the dynamics and challenges of cooperative systems.\nIn particular, prior safe RL work has explored constrained algorithms using trust region-based methods to match the safety specifications [1, 32, 38, 42, 43]. These constrained RL methods aim to maximize task performance-modeled as reward signal(s)-while adhering to safety specifications in the form of constraints. Despite being effective in single-agent settings, their extensions to multiagent RL (MARL) model the problem by defining separate (individual) constraints for each agent [14, 20]. However, we argue that safety in cooperative MARL is inherently a team-level concept (e.g., a collision between two agents can cause the entire team to fail). Additionally, introducing constraints inherently limits exploration, which is critical for avoiding local optima [17]. Without effective exploration, constrained algorithms tend to prioritize safety at the expense of learning behaviors that achieve strong task performance. This detrimental trade-off further exacerbates in multiagent cooperative systems, where exploration is crucial for discovering joint behaviors that are necessary to solve a task [9, 23, 30, 47]. To improve discovery of such behaviors, constrained MARL often considers policy entropy in the optimization process [15, 45]. However, we note that this technique increases the \"randomness\" of policy decisions and could further hinder constraint satisfaction and the team's performance in complex tasks.\nIn this paper, we address these issues by first analyzing the impact of defining team constraints from a theoretical perspective. Then, we introduce entropic exploration for constrained MARL (E2C) to enhance exploration and learn safe behaviors with good team performance for cooperative agents (Figure 1). E2C employs observation entropy maximization (OEM) [8, 22, 36] to balance the team objective of agents and the safety objective in the system without employing policy entropy. Specifically, E2C leverages a count-based and k-nearest neighbor (knn) approximations to estimate the observation entropy and reward the agents. Finally, we apply E2C to a constrained MARL algorithm with both individual and team constraints to analyze their practical impact. The contributions of this work are to:\n\u2022 Introduce team constraints for cooperative agents, providing a lower bound on policy improvement and showing their practical impact on performance.\n\u2022 Propose a novel constrained MARL algorithm leveraging entropy maximization to balance task performance and safety constraints effectively.\nOur experiments show the efficacy of E2C in addressing safety and cooperation in a variety of multiagent setups. We evaluate our method across six well-known MARL domains, including variations of the cooperative multi-rover exploration domain [2], three coordination environments from the multiagent particle suite [23], and two multiagent locomotion tasks from the safe MaMuJoCo problems [13]. Notably, E2C algorithms achieve superior or comparable task performance to baseline unconstrained MARL algorithms, while satisfying constraints. Our method also successfully satisfies"}, {"title": "2 PRELIMINARIES AND RELATED WORK", "content": "Cooperative multiagent tasks can be modeled as a decentralized Markov decision processes (DecMDPs) [28]. We represent a Dec-MDP with a tuple $(N, S, U, T, r, O, \\gamma)$, where N is a finite set of agents; S is the set of states of the environment; $U = \\{U^i\\}_{i \\in N}$ is the set of all possible joint actions. At a time step, t, each agent i selects an action, $u_t^i$, forming joint action $u_t = \\{u_t^i\\}_{i \\in N}$, which transitions the environment from state $s_t$ to $s_{t+1}$ via $T(s_t, u_t, s_{t+1}) = P(s_{t+1} | s_t, u_t)$ and yields a joint reward r(st, ut). In a DecMDP, each agent i receives an observation $o_t^i$ according to an observation transition function $O(o_t^i, s_t, u_t) = P(o_t^i | s_t, u_t)$. This function defines the probability distribution over the joint observations $o_t = \\{o_t^1, ..., o_t^N\\}$ based on the state and joint action. In a DecMDP, the state at each time step can be uniquely determined by the joint observation. The objective of a cooperative team of agents is to learn a joint policy $\\pi(u_t | s_t)$ that maximizes the expected discounted return defined as:\n$J_r(\\tau) := E_\\pi[\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, u_t)] $  (1)\nwhere $\\gamma \\in [0, 1)$ is a discount factor."}, {"title": "2.1 Multiagent Reinforcement Learning", "content": "Motivated by the promises of collaborative multiagent systems, MARL has received significant research attention. A popular approach is the centralized training with decentralized execution (CTDE) paradigm, which centralizes information during training while maintaining decentralized execution [12, 44, 47]. In discrete action spaces, value factorization techniques have been used to estimate a joint value function as a global optimization signal [25, 30, 44] in a CTDE fashion. However, these algorithms are unsuitable for multiagent systems with continuous control.\nTo address this limitation, extensions of well-known single-agent algorithms to MARL (e.g., proximal policy optimization (PPO) [35] to multiagent PPO (MAPPO) [47]) have shown promising performance in cooperative games with both discrete and continuous action spaces, by estimating centralized value functions [12, 23, 47]. Crucially, the centralized component in these approaches is required only during training, ensuring a principled CTDE method.\nGiven MAPPO's strong performance in decentralized multiagent problems, we build E2C on top of it. Next, we discuss how to incorporate safety specifications within this learning paradigm."}, {"title": "2.1.1 Constrained Reinforcement Learning", "content": "Constrained RL has successfully modeled safety specifications for single-agent tasks [31, 32, 46]. These constrained methods typically extend trust region-based algorithms by incorporating additional sets of cost functions, denoted as $C := \\{c^i_j\\}_{i\\in N}^{j\\in m^i}$, where each agent i has $m^i$ cost functions. Each cost function is of the form $c^i_j : S \\times U^i \\rightarrow \\{0,1\\}$ with corresponding hard-coded cost-limiting values (i.e., thresholds) $l := \\{l^i_j\\}_{i\\in N}^{j \\in m^i}$. After performing the joint action in the environment, each agent i also receives costs $c^i_j(s_t, u^i_t) \\forall j = 1, ..., m^i$. In addition to maximizing the expected discounted return, the agent aims to satisfy its safety constraints, defined as:\n$J_c^j(\\pi) := E_\\pi[\\sum_{t=0}^{\\infty} \\gamma^t c^i_j(s_t, u_t)] \\le l^i_j, \\forall j = 1, ..., m^i$. (2)\nConstraint-satisfying (feasible) policies $\\Pi_c$ (where $\\Pi$ are stationary policies), and optimal policies $\\pi^*$ are thus defined as:\n$\\Pi_c := \\{\\pi \\in \\Pi : J_c^j(\\pi) \\le l^i_j, \\forall i \\in N, j = 1, ..., m^i\\}$,\n$\\pi^* = arg\\underset{\\pi \\in \\Pi_c}{max} J_r(\\pi)$. (3)\nHowever, enforcing strict thresholds leads to a significant performance trade-off due to the detrimental effects of constraints on exploration [24, 31, 46]. Such drawbacks are further exacerbated in multiagent settings that we discuss in the following."}, {"title": "2.1.2 Constrained Multiagent Reinforcement Learning", "content": "In cooperative MARL, good exploration is pivotal to learning a joint policy that can successfully solve a cooperative task. In particular, introducing constraints in MARL typically leads to having three competing objectives: agent-specific behaviors (e.g., learning how to navigate in an environment), joint task performance (e.g., cooperating to rescue a target), and constraints (e.g., avoiding collisions).\nDespite its importance, constrained MARL has received marginal research attention [14, 20, 21] and there is much room to develop novel safe MARL algorithms. For example, the works [20, 21] propose approaches voted to improve cost-value estimation and credit assignment. However, these methods extend the single-agent case by: (i) using individual constraints for each agent, (ii) disregarding the negative impact of constraints and policy entropy used during optimization by the algorithms to learn cooperative behaviors. We thus analyze the benefit of incorporating safety specifications at a team level from a theoretical and practical perspective. On the theory side, we extend the work of Gu et al. [14], deriving cost improvement bounds for trust-region-based methods (on which MAPPO builds [39]) using team constraints. On the practical side, we conduct an extensive evaluation of constrained MAPPO employing individual and team constraints. Moreover, to tackle the limited exploration of constrained algorithms and the potential issues of employing policy entropy, by integrating observation entropy maximization for which we provide a brief overview in the following."}, {"title": "2.2 Entropy Maximization", "content": "A popular technique to improve exploration in RL algorithms is to generate diversity that enables the policy search to cover larger areas in the policy space. Many RL algorithms achieves this search by maximizing the policy entropy [10, 16, 35, 48] within their optimization processes or as their objective. This entropy maximization naturally increases stochasticity while agents take actions and has been used within constrained RL frameworks [15, 45]. In this work, we show that maximizing policy entropy is not a good practice when cooperative agents are under strict safety requirements.\nConversely, observation (or state) entropy maximization (OEM) is used to incentivize visiting new states by rewarding agents based on the novelty of their observations. To this end, designing a count-based reward has been a popular approach for mapping the agent's decisions to the entropy measured over the observed"}, {"title": "3 TRUST REGION BOUNDS FOR TEAM CONSTRAINTS", "content": "In this section, we extend the cost improvement bounds derived by the works [14, 39] for trust region MARL with individual constraints to the team settings. In particular, we note that the work [14] considers a Markov game, assuming fully cooperative agents with a joint reward, and Sun et al. [39] relies on a Dec-MDP formalization assuming local observations capture \"sufficient\" information about the state.\u00b9 Hence, we follow the same assumptions of such previous works and extend their stateful lower bound on the cost improvement to team constraints.\nWhen cooperative agents use joint (team) constraints, we define a set of cost functions $C := \\{c_j\\}_{j \\in m}$ (the team has m cost functions). These functions take the form $c_j : S \\times U \\rightarrow \\{0,1\\}$ with cost-limiting values $l := \\{l_j\\}_{j \\in m}$. After performing the joint action in the environment, the agents receive joint costs $c_j(s_t, u_t) \\forall j = 1, ..., m$. On top of maximizing the expected discounted return, the agents now also try to satisfy a joint constrained objective:\n$J_j(\\pi) := E_\\pi[\\sum_{t=0}^{\\infty} \\gamma^t c_j(s_t, u_t)] \\le l_j, \\forall j = 1,...,m$. (4)\nfor which optimal policies are defined similarly to Equation 3 by replacing the cost objectives in the space of feasible policies $\\Pi_c$.\nTo derive the cost improvement bound for team constraints, we define the corresponding joint cost value functions. For the jth cost function, we define the jth (stateful) value functions as follows:\n$Q^{\\pi}_{j,r}(s,u) := E_\\pi[\\sum_{t=0}^{\\infty} \\gamma^t c_j(s_t, u_t) | s_0 = s, u_0 = u]$,\n$V_j^\\pi(s) := E_{u \\sim \\pi}[Q^{\\pi}_{j,r}(s, u)]$,\n$A_j^\\pi(s, u) = Q^{\\pi}_{j,r}(s, u) \u2013 V_j^\\pi(s)$. (5)\nIn trust region-based methods, Equation 4 is difficult to optimize directly when considering a joint policy $\\pi$ and some other policy $\\tilde{\\pi}^i$ of agent i. Hence, we define the surrogate objective for team constraints following the individual constraint case of Gu et al. [14]."}, {"title": "4 ENTROPIC EXPLORATION FOR CONSTRAINED MARL", "content": "After introducing the team constraints problem, we present our E2C method that addresses the limited exploration arising from constrained MARL settings (with both individual and team constraints). In particular, we remark that constrained MARL algorithms typically result in a detrimental trade-off between safety and task performance. Exploration plays a crucial role for discovering joint behaviors and balance this trade-off. Before introducing E2C, this section formalizes the constrained MARL problem for both individual and team constraint cases.\nThe typical goal of constrained MARL is to maximize a reward function while satisfying constraints modeling safe behaviors. Recalling the cost and reward-based objective notations of Sections 2.1 and 3, constrained problems can be defined as maximizing the joint reward objective-max Jr(\u03c0)-while satisfying constraints- $J_c^j(\\pi) \\le l^i_j \\forall i \\in N, j = 1,...,m^i$ for individual constraints, or $J_j(\\pi) \\le l_j \\forall j = 1, ..., m$ for team ones.\nIn the safe RL literature, the Lagrangian method [27] is commonly used to transform the constrained problem into an equivalent unconstrained one $\\forall i \\in N$, using a dual variable as follows:\n$L^{\\pi}(\\lambda) = J_r \u2013 L(\\lambda)$,\n$L(\\lambda) = \\{ \\begin{array}{ll} \\sum_j \\lambda_j^i (J_c^j(\\pi) \u2013 l_j^i) & \\forall j = 1,..., m^i \\text{ individual} \\\\ \\sum_j \\lambda_j (J_j(\\pi) \u2013 l_j) & \\forall j = 1, ..., m \\text{ team} \\end{array}$ (6)\nwhere $\\lambda$ are the so-called Lagrangian multipliers and act as a penalty in the optimization objective of each agent. The goal is thus to solve the resulting max min problem:\n$\\underset{\\pi}{max} \\underset{\\lambda \\ge 0}{min} L^\\pi(\\lambda)$. (7)\nA typical solution to Equation 7 is to iteratively take gradient ascent steps in $\\pi$ and descent in $\\lambda$. We first update the multipliers following $\\nabla_\\lambda L^\\pi(\\lambda)$, noting the multipliers must be $\\ge 0$ because they act as a penalty when the constraints are not satisfied (i.e., $\\lambda$ increases) while decreasing to 0 (i.e., removing any penalty) when the"}, {"title": "4.1 E2C", "content": "As discussed in Section 2, we build E2C on top of the Lagrangian MAPPO-a strong baseline across a variety of scenarios [14, 47].2 The resultant E2C-MAPPO algorithms address the challenges of using constraints in multiagent systems by using entropy enhanced agents. In this section, we start by deriving the constrained MAPPO algorithm for individual and team constraints and then present the entropic exploration method based on OEM.\nFollowing the MAPPO baseline, we learn a centralized advantage estimator $A_{\\phi}(s, u)$ parametrized by $\\phi$, while each agent $i \\in N$ learns a policy $\\pi_{\\theta^i}$; parametrized by $\\theta^i$. Policies' parameters are updated using the following clipped objective:\n$\\underset{\\theta}{max} \\underset{\\lambda \\ge 0}{min} E_{\\pi_{\\theta}}[\\sum_{s,u}$\n$\\Big( q(\\theta^i, \\theta_i) A_{\\phi}(s, u),$\\n$\\text{clip} \\Big(q(\\theta^i, \\theta_i), 1 \u2013 \\epsilon, 1 + \\epsilon\\Big) A_{\\phi}(s,u)\\Big)$+q(\\theta^i, \\theta_i) L_{\\theta^i}(\\lambda)]$ (8)\nwhere the centralized advantage measures the overall effect of selecting a joint action, and $q(\\theta^i, \\theta_i) = \\frac{\\pi_{\\theta^i}(u^i | s^i)}{\\pi_{\\theta^i}(u^i | s^i)}$. In more detail,\n$L_{\\theta^i}$ depends on the nature of constraints (i.e., individual or team as in Equation 6). For example, consider the simplified case with one individual constraint for each agent and the corresponding one team constraint case. In the case of an individual constraint $c_i$ (with threshold $l_i$), each agent i learns a separate multiplier $\\lambda^i$ and a cost-advantage estimator based on local information. Conversely, when considering a team constraint $c_1$ with threshold $l_1$, we learn a single team multiplier $\\lambda_1$ and a joint cost-advantage estimator. Both approaches have pros and cons that follow the benefits and drawbacks of using decentralized (i.e., with local information) and centralized (i.e., with joint information) estimators in MARL. Specifically, individual constraints scale better as the size of local information used by cost-advantage estimators does not depend on the number of agents. However, using local information can hinder performance [40]. In contrast, the centralized case does not scale well in the number of agents due to the cardinality of the joint observation and action spaces but leveraging joint information typically improves value estimates and performance [23]."}, {"title": "4.1.1 Entropic Exploration", "content": "We use observation entropy maximization to design an exploration-driven reward that incentivizes agents' exploration. For OEM, we employ quantization [3] to cluster similar observation vectors together for the experiments in multi-rover exploration [2] due to the low cardinality of the observations. In the more complex state spaces of particle environments and safe MaMuJoCo tasks [13, 26], we use knn estimate [37] of entropy to deal with higher dimensional observations. The resultant OEM reward is presented in Algorithm 1. Specifically, each agent receives a reward bonus based on the novelty of its current observation. OEM thus aims to improve the search in the observation space, which is key to learning good joint policies in multiagent systems [3]. To emphasize observations that might have a contribution to the overall team task more prominent (for an efficient search), we incorporate a value, $\\beta(o)$, as described in [3] in multi-rover domain. In safe MaMuJoCo tasks, we incorporate the OEM reward into the extrinsic task reward, $r_{ex}(o)$, via a hard-coded weight (0.3), $\\psi$, as applied in [36]. In particle environments, we observe that using pure OEM rewards is sufficient to unearth safe and cooperative behaviors."}, {"title": "4.1.2 E2C-MAPPO", "content": "After introducing all the components required to design E2C-MAPPO in the previous sections, Algorithm 2 shows a general template for our method. For simplicity, we show the procedure using team constraints, but the individual constraint case follows by replacing the team components with the individual ones as previously discussed. In detail, after defining the desired constraints thresholds and initializing agents' policies, value functions, and multipliers (lines 1-2), we follow the training loop of MAPPO algorithm (highlighted in italics), where agents interact in the environment to collect training data (lines 3-7). After each episode, for each agent, we perform the following steps:\n\u2022 Compute the OEM reward (line 10), following Algorithm 1.\n\u2022 Update the Lagrangian multipliers as described in Section 4.1 (line 11).\n\u2022 Compute the (return) centralized advantage and the cost-advantages, using the advantage functions parameterized by $\\phi$, $\\{$c_j$\\}_{j=1,...,m}$ (line 12)."}, {"title": "5 EXPERIMENTS", "content": "Our experiments aim to answer the following questions: (i) How well does E2C-MAPPO solve standard cooperative tasks compared to the unconstrained (unsafe) and constrained (safe) baseline? (ii) How do different definitions of constraints (i.e., individual and team) affect performance? (iii) Is policy entropy detrimental to constrained MARL performance when compared to our observation entropy?\nTo answer these questions, we first evaluate E2C-MAPPO, the unconstrained MAPPO, and two constrained MAPPO baselines with and without policy entropy in increasingly complex variations of the well-known multi-rover domain [2]. We then compare our framework and the two constrained baselines across three particle environment tasks [26] and two safe MaMuJoCo locomotion scenarios [13], outlined below and depicted in Figure 2.3 Overall, in the navigation-based scenarios (i.e., multi-rover and particle environment), the safety requirement is collision avoidance, while in locomotion tasks is a velocity limit. Hence, when agents collide or"}, {"title": "5.1 Environments", "content": "In this section, we briefly introduce the environments considered in our evaluation. We refer to the original works for more details regarding observation and action spaces, and rewards [2, 13, 26]."}, {"title": "5.1.1 Multi-Rover Exploration", "content": "This continuous and sparse reward (i.e., the team reward is only given at the end of an episode) domain (Figure 2d) [2] consists of multiple agents (rovers), and points of interest (POIs). Each rover must learn cooperative navigation skills to observe a POI simultaneously in the environment. The team size required to observe a POI is determined by a coupling factor. A higher coupling translates into a more complex coordination"}, {"title": "5.1.2 Particle Environments", "content": "We consider three particle environments [23, 26] (Figures 2: a, b, c). For all these tasks, we consider 3 cooperative (good) agents and 1 adversary. The latter learns a policy using the unconstrained PPO [35] algorithm, updating the same type of policy network as the cooperative agents. The episodic team reward totals the cooperative agents' rewards throughout an episode, and the cost functions model collisions to achieve collision-free team behaviors. In more detail, we consider the following environments:\n\u2022 Physical deception: Good agents and an adversarial agent compete to reach a single landmark. Good agents cooperate to reach the landmark and receive the negative of the closest good agents' distance and the distance of the adversarial agent's distance to the landmark as their reward.\n\u2022 Keep away: Good agents have to reach a landmark (randomly chosen from a set of 2 landmarks) and are rewarded with their negative distance to the landmarks. An adversarial agent tries to push away the agents from their target.\n\u2022 Predator prey: Landmarks are used as obstacles, and good agents (with lower speed) need to capture a faster adversarial agent. When the good agents touch the adversarial agent, the agents receive positive rewards, whereas the adversary gets a penalty."}, {"title": "5.1.3 Safe MaMuJoCo", "content": "These tasks extend the well-known single-agent locomotion benchmark to multiagent settings. Each agent controls different parts of the robot and receives a joint reward for forward movements, incentivizing learning good locomotion behaviors. We consider two tasks [13] (Figures 2: d, e):\n\u2022 Ant 2x4: Two agents control four joints of an ant and each has to learn how to run in a corridor. The agents receive a positive cost when an ant topples over or gets too close to the wall.\n\u2022 HalfCheetah 2x3: Two agents control three joints of a cheetah and each has to learn how to run in a corridor. There are moving bombs in the corridor and the agents receive a positive cost when the cheetah gets too close to the bombs."}, {"title": "5.2 Implementation Details", "content": "Data collection is performed on Xeon E5-2650 CPU nodes with 64GB of RAM. Considering the twofold nature of E2C, we call E2C-MAPPO (T) the entropy maximizing algorithm using team constraints, and E2C-MAPPO the one with individual constraints. For a fair comparison, the threshold for each agent in the individual constraint case equals the team threshold divided by the number of agents (detailed in the following section).\nThe following results show the average return smoothed over the last hundred episodes of 10 runs per method with shaded regions representing the standard error. As in previous work on individual constraints [39], we incorporate a GRU [6] layer into the agents' networks to address the partially observable nature of some tasks"}, {"title": "5.3 Results", "content": "This section aims at showing how well the proposed approach solves the cooperative tasks and how different definitions of constraints impact the training process."}, {"title": "5.3.1 Multi-Rover Exploration", "content": "The constraint thresholds for E2C-MARL (T) is set to 20, 20, 40, 30, and 75 for the task variations listed in Figure 3. Each row of the figure shows experiments with increasing number of agents and coupling. The first column highlights the average reward and the second column shows the average cost.\nIn general, the E2C-MAPPO variations have comparable task-objective performance (i.e., average reward) to the unconstrained version, but reduce the cost by half. Crucially, when the number of rovers increases, using team constraints leads to better performance than using individual constraints. We also perform experiments in a more challenging setup with 10 rovers and a coupling of 4, which confirms the superior performance of E2C-MAPPO (T). This supports our claims regarding the potential benefits of team constraints in fully cooperative scenarios."}, {"title": "5.3.2 Experiments with Policy Entropy", "content": "To test our claims on the detrimental effect of policy entropy, we compare the impacts of observation entropy against policy entropy. We perform the tests using: (i) the Lagrangian MAPPO with individual constraints and policy entropy proposed by Gu et al. [14] (C-MAPPO (w. policy entropy)); and (ii) a variation not employing any entropy (C-MAPPO). For a fair comparison over these previous individual constraint baselines, we compare them against E2C-MAPPO.\nAs claimed in Section 2, policy entropy is detrimental under restrictive constraint thresholds. When we set the coupling and the"}, {"title": "5.3.3 Particle environments", "content": "After this preliminary experiments, we use the constrained methods, E2C-MAPPO (T), E2C-\u041c\u0410\u0420\u0420\u041e, C-MAPPO, and C-MAPPO (w. policy entropy) for the remaining evaluations. Figure 5 shows the results in the particle environments, where the learning adversarial agent makes the good agents' performance brittle. Overall, all the methods achieve comparable performance in physical deception. However, E2C-MAPPO (T) converges to its peak performance in approximately half of the steps required by the C-MAPPO baselines, while improving sample efficiency over E2C-MAPPO. Cost plots show that E2C-MAPPO (T) also satisfies the constraint threshold in fewer steps, motivating its higher sample efficiency in discovering safe collaborative behaviors with higher payoffs. Results in keep away and predator prey show that the performance of C-MAPPO agents is significantly more brittle than E2C agents, as its policy entropy negatively affects performance in tasks with high uncertainty. Especially, C-MAPPO's limited exploration during the early stages of training causes agents to remain still to avoid collisions, finally lead to task failure. In contrast, E2C-MAPPO algorithms learn high-reward, low-cost behaviors without significant differences between individual and team constraints.\nOverall, we note that team constraints improve performance in tasks with less uncertainty and higher cooperation while matching the performance of individual constraints under higher uncertainty."}, {"title": "5.3.4 Safe MaMuJoCo", "content": "Finally, we compare the methods in two safe MaMuJoCo tasks. In this set, we only consider E2C-MAPPO (T) as different agents use the joints of the same robot; thus, our two constraint formalizations are equivalent. We set the constraints thresholds to 5 for Ant and 20 for HalfCheetah. Figure 6 shows the"}, {"title": "6 CONCLUSION", "content": "We introduce entropic exploration to address the challenges of real-world applications of multiagent systems requiring cooperation and safety. We highlight the limitations of existing constrained MARL methods, which often rely on individual constraints and policy entropy maximization. These approaches can compromise task performance due to limited exploration and increased randomness in action selection. Our approach tackles these issues by: (i) investigating team constraints, which better capture the specifics of cooperative multiagent tasks, from a theoretical and practical perspective; and (ii) leveraging observation entropy maximization (OEM) to encourage diversity in observations upon satisfying constraints. The OEM-based rewards effectively balance the competing objectives of task performance and constraint satisfaction, avoiding the pitfalls of overly conservative behavior. By prioritizing observation diversity, our E2C algorithms foster exploration even within the boundaries of strict safety constraints, enhancing both learning cooperative strategies and adherence to safety specifications.\nOur experiments spanned multiple challenging environments confirms E2C's ability to consistently satisfy both individual and team constraints. Our results show the superior performance of E2C in achieving higher task performance while maintaining or improving constraint satisfaction compared to traditional baselines. These results validate the potential of OEM as a crucial mechanism to promote safe, coordinated behaviors in multiagent systems.\nIn conclusion, E2C offers a novel, effective solution to the exploration and safety trade-offs inherent in constrained MARL. By shifting the focus from policy randomness to observation entropy driven exploration, we provide a more principled approach to balancing performance and safety in cooperative settings. Future work could extend this framework to more complex environments, examine scalability in larger agent teams, and explore the potential of E2C in real-world applications."}, {"title": "APPENDIX", "content": "LEMMA 2. Let $\\pi$ and $\\tilde{\\pi"}, "be joint policies. Let $i \\in N$ be an agent, and $j = 1, ..., m$ be one of the joint cost indexes. The following inequality holds:\n$J_j(\\tilde{\\pi}) \\le J_j(\\pi) + L(\\tilde{\\pi}^i) + v_j \\sum_{h=1}^{|N|}D^{KL}_{max}(\\pi^h, \\tilde{\\pi}^h)$,  where $v_j = \\frac{4\\gamma max_{s,u} |A_j^{\\pi}(s, u)|}{(1 \u2013 \\gamma)^2}$\nPROOF. The proof closely follows the one provided by Gu et al. [14"], "that": "n$J_j(\\tilde{\\pi"}, {"following": "n$J_j(\\tilde{\\pi"}, {"holds": "n$D^{KL}_{max}$  =  max $D_{KL}(\\pi(\\cdot|s), \\tilde{\\pi}(\\cdot|s))$\n$s \\in N$\n$ = max  \\sum_{s \\in N}  D_{KL}(\\pi_i(\\cdot|s_i), \\tilde{\\pi}_i(\\cdot|s_i)) $\n$ = max  \\sum_{s \\in N}  D_{KL}(\\pi^i(s), \\tilde{\\pi}^i(s)) $\n$ \\sum_{i=1}^{|N|}D^{KL"}]