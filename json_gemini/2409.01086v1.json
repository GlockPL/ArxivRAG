{"title": "DPDEdit: Detail-Preserved Diffusion Models for Multimodal Fashion Image Editing", "authors": ["Xiaolong Wang", "Zhi-Qi Cheng", "Jue Wang", "Xiaojiang Peng"], "abstract": "Fashion image editing is a crucial tool for designers to convey their creative ideas by visualizing design concepts interactively. Current fashion image editing techniques, though advanced with multimodal prompts and powerful diffusion models, often struggle to accurately identify editing regions and preserve the desired garment texture detail. To address these challenges, we introduce a new multimodal fashion image editing architecture based on latent diffusion models, called Detail-Preserved Diffusion Models (DPDEdit). DPDEdit guides the fashion image generation of diffusion models by integrating text prompts, region masks, human pose images, and garment texture images. To precisely locate the editing region, we first introduce Grounded-SAM to predict the editing region based on the user's textual description, and then combine it with other conditions to perform local editing. To transfer the detail of the given garment texture into the target fashion image, we propose a texture injection and refinement mechanism. Specifically, this mechanism employs a decoupled cross-attention layer to integrate textual descriptions and texture images, and incorporates an auxiliary U-Net to preserve the high-frequency details of generated garment texture. Additionally, we extend the VITON-HD dataset using a multimodal large language model to generate paired samples with texture images and textual descriptions. Extensive experiments show that our DPDEdit outperforms state-of-the-art methods in terms of image fidelity and coherence with the given multimodal inputs.", "sections": [{"title": "1 Introduction", "content": "The purpose of fashion image editing is to manipulate fashion images according to the user's creative vision, thereby materializing their fashion concepts. This approach provides a seamless interface for both designers and non-experts to explore and visualize their fashion ideas. Furthermore, fashion image editing algorithms hold significant promise for e-commerce, advertising, and social networks. As computer vision increasingly intersects with the fashion industry(Zhu et al. 2023; Gou et al. 2023; Sarkar et al. 2023), there is growing research interest in this emerging field(Pernu\u0161 et al. 2023; Baldrati et al. 2023; Wang and Ye 2024).\nPrevious works(Zhu et al. 2017; Jiang et al. 2022; Pernu\u0161 et al. 2023) has attempted to use GAN-based methods to generate and edit fashion images based on textual descriptions. Although GANs have shown potential, they are often plagued by issues related to training instability and struggle to produce high-quality generated images with abundant details. In contrast, Diffusion Models(Dhariwal and Nichol 2021; Nichol and Dhariwal 2021a; Rombach et al. 2022) have emerged as a promising alternative for image editing tasks, recognized for their ability to produce high-quality results and provide more stable and controllable generation mechanisms. TexFit(Wang and Ye 2024) introduces a straightforward text-driven fashion image editing method based on diffusion models. It is user-friendly and generates impressive results. However, relying exclusively on textual input poses challenges in accurately capturing the user's design specifications, including garment styles, patterns, and fabric textures. This limitation often results in discrepancies between the generated images and the user's intended vision.\nAs a result, introducing multimodal approaches in fashion image editing is essential for meeting user requirements. MGD(Baldrati et al. 2023) integrates text, human pose, and garment sketch modalities for fashion image editing using text inversion techniques. Ti-MGD(Baldrati et al. 2024) further incorporates clothing texture control. Although Ti-MGD incorporates multimodal conditional control to generate garment texture information, relying exclusively on CLIP(Radford et al. 2021) for extracting texture image features hinders the accurate restoration of complex and detailed textures. Additionally, these methods lack an empha-"}, {"title": "2 Related Works", "content": "sis on the precise localization of the editing region, limiting their effectiveness as a general-purpose solution. Tex-Fit proposes a Editing Region Location Module (ERLM), which generates corresponding editing region masks using an encoder-decoder architecture. However, this approach of combining text descriptions with image features and computing the difference proves inadequate for fashion images involving complex human poses and diverse clothing styles.\nTo address the aforementioned drawbacks, we introduce Detail-Preserved Diffusion Edit(DPDEdit) method, which integrates multiple modalities within a latent diffusion model for fashion image editing. DPDEdit leverages multimodal inputs, including text, human densepose(G\u00fcler, Neverova, and Kokkinos 2018), region mask and texture images to guide the garment editing process. To locating editing regions in complex scenarios, we utilize the latest research advancement, Grounded-SAM, for garment region segmentation. Grounded-SAM leverages its powerful segmentation capabilities to accurately generate a mask for the editing region based on the user's text prompt. In order to align the generated garment with the input texture image, we propose a texture injection and refinement mechanism. This mechanism employ a decoupled cross-attention layer to effectively guide the diffusion process under the joint control of texture image and textual description.To preserve intricate garment textures and enhance fine details, we employed a pre-trained auxiliary U-Net, named Detail-Preserved U-Net(DP-UNet), to extract high-frequency features from the texture images and integrate them into the denoising-UNet. DP-UNet supplements the texture image details, ensuring that the generated garments closely align with the input texture patterns (Figure 2).\nTo the best of our knowledge, there is no publicly available dataset that includes both garment texture images and corresponding text descriptions. To address this gap and meet the requirements of our task, we have extended the VITON-HD dataset (Choi et al. 2021). Specifically, we extracted fabric texture images from the garment images in the original dataset. Using the Multimodal Large Language Model LLaVA (Liu et al. 2024), we generated appropriate captions for these fabric texture images, thereby creating a paired text-image dataset suitable for training and evaluation.\nIn summary, our contributions are as follows:\n\u2022 We propose the DPDEdit framework for fashion image editing, which leverages multimodal inputs to guide the diffusion model. This approach generates high-quality images that are consistent with the input modalities and allows for fine-grained control over the fabric texture of the clothing.\n\u2022 We employ Grounded-SAM to accurately identify the editing region and introduce texture injection and refinement mechanism to preserve the intricate details of the garment texture, aligning with the specific requirements of our task.\n\u2022 To support our task, we have extended the VITON-HD dataset to include fabric texture images of garments along with corresponding text captions, providing a valuable resource for future research in this domain."}, {"title": "2.1 Text-to-Image Generation", "content": "The process of text-to-image generation involves creating a visual representation from a given textual description. Early approaches in this field are primarily based on GANs(Zhang et al. 2017, 2018a; Zhu et al. 2019). StackGAN(Zhang et al. 2017) and StackGAN++(Zhang et al. 2018a) utilize a multi-stage, iterative methodology to gradually improve the resolution of the generated images. Recent advancements have increasingly focused on the application of diffusion models. GLIDE(Nichol et al. 2021) pioneered the use of text to directly guide image generation from high-dimensional pixel data, replacing the labels in class-conditioned diffusion models. Similarly, Imagen(Saharia et al. 2022) employs a cascaded framework to generate high-resolution images more efficiently within the pixel space. Another research direction involves projecting the image into a lower-dimensional space and then applying diffusion models in this latent space. Notable works in this area include Stable Diffusion (SD)(Rombach et al. 2022), VQ-diffusion (Gu et al. 2022), and DALL-E 2(Ramesh et al. 2022). Building on these foundational studies, numerous subsequent works(Podell et al. 2023; Meng et al. 2023; Dai et al. 2023), have further advanced the field over the past two years."}, {"title": "2.2 Image Editing with Diffusion models", "content": "Editing real images has long been a crucial task in the field of image processing, and recent advancements in image editing have garnered significant attention. This task can be categorized into two distinct types based on the editing region.\nGlobal text-driven Editing These methods globally stylize real images or edit specific objects within an image based on textual descriptions.Prompt2Prompt(Hertz et al. 2022) modifies words in the original prompts to enable both local and global editing using cross-attention control. Null Text Inversion(Mokady et al. 2023) removes the need for the original caption during editing by optimizing the inverted diffusion path of the input image. Imagic(Kawar et al. 2023) optimizes a text embedding that corresponds to the input image and then interpolates it with the target description, producing varied images for editing purposes.\nLocal text-driven Editing Another line of research focuses on utilizing masked regions and corresponding regional descriptions for local editing. SDEdit(Meng et al. 2021), introduces intermediate noise to an image and then denoises it using a diffusion process conditioned on the desired edits. DiffEdit (Couairon et al. 2022b) streamlines semantic editing by automatically generating masks that isolate specific regions for modification, ensuring that unedited regions retain their semantic integrity. In the domain of fashion image editing, MGD(Baldrati et al. 2023) employs text inversion to integrate multimodal conditions for guiding fashion garment generation, while Ti-MGD(Baldrati et al."}, {"title": "3 Methodology", "content": "In this section, we propose a novel task to automatically edit fashion images conditioned on multiple modalities. Specifically, given the model image xo, the name of the garment to be edited Yo, Densepose xp of the model image, fabric texture image xe, and the corresponding caption s, we aim to generate a new image xg that retains the information of the input model while replacing the target garment according to the multimodal inputs. An overview of our model is illustrated in Figure 4."}, {"title": "3.1 DPDEdit Framework", "content": "We introduce the DPDEdit framework, which integrates Grounded-SAM for precise localization of editing regions and a main denoising U-Net for image generation.\nGrounded-SAM To achieve high-quality fashion image editing, precise identification and segmentation of the editing regions are essential. We utilize Grounded-SAM, integrating Grounding-DINO (Caron et al. 2021) and SAM (Segment Anything Model) (Kirillov et al. 2023), to ensure accurate localization. Grounding-DINO processes the input image xo and garment description Yo using vision transformers and text embeddings to generate relevant bounding boxes. SAM refines these boxes into a segmented mask $M\\in \\{0,1\\}^{H\\times W}$ (Figure 3), we slightly extending M for smoother edges. This two-step approach ensures robust initial bounding boxes and accurate mask refinement, crucial for handling complex garment style.\nDenoising-UNet Denoising-UNet employs a latent diffusion model within the latent space of a variational autoencoder (VAE) comprising an encoder E and a decoder D (Kingma and Welling 2013). Starting with the latent representation of person image E(x0), noise is added through the diffusion model's forward process, resulting in z\u03c4. Using the mask M from Grounded-SAM, the person image with the garment removed is represented as xm = (1 - M) xo, where denotes element-wise multiplication. Additionally, the input to Denoising-UNet includes the latent representation of human densepose image p = E(xp), a garment texture image xc, and a textual description of the texture s. The training loss function is formulated as:\n$E_{zr,t,M,p,E(x_m),x_c,s,\\epsilon~N(0,1)} [||\\epsilon - \\epsilon_{\\theta}(z_T, t, x_c, s)||_2] \\qquad(1)$\nwhere z = [z\u0442, M, p, E(xm)]. These latents are concatenated along the channel dimension, and the convolutional layers of the UNet are expanded to accommodate 13 channels, initialized with zero weights.\nTo preserve the identity of the person and maintain the integrity of the unedited regions in the fashion image, we"}, {"title": "3.2 Texture Injection and Refinement Mechanism", "content": "To inject and preserve the intricate texture details in the generated garments, we propose a texture injection and refinement mechanism. This approach begins with a decoupled cross-attention mechanism that preliminarily aligns the textures of the input image with those of the generated output. Additionally, we introduce DP-UNet, specifically designed to further enhance and refine these texture details.\nDecoupled Cross-Attention Mechanism Inspired by the Image Prompt Adapter(Ye et al. 2023), we use a decoupled cross-attention mechanism for multimodal prompt control. Specifically, we decoupled the attention heads for text and image embeddings, allowing independent control over text and visual prompts. Let Q represent the query matrices derived from the main UNet's intermediate representation, while K and V denote the key and value matrices obtained from the text embeddings ct. The output of the cross-attention layer is given by:\n$Attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d}}) V, \\qquad(3)$\nwhere $K = c_tW_k$ and $V = c_tW_v$. Similarly, let K' and V' represent the key and value matrices derived from the image embeddings ci, with $K' = c_iW_k'$ and $V' = c_iW_v'$. Here, Wk, Wv, $W_k'$ and $W_v'$ are the weight matrices of the trainable linear projection layers. By adjusting the parameter A during inference, the final formulation of the decoupled cross-attention mechanism is expressed as:\n$Z = Attention(Q, K, V) + \\lambda \\cdot Attention(Q, K', V'), \\qquad(4)$\nWhen \u03bb = 0, the model reverts to the original text-to-image diffusion model. We initialize the feature projection and cross-attention layers using IP-Adapter-Plus\u00b9.\nDP-UNet To preserve intricate garment textures and refine details, we introduce DP-UNet, addressing the limitations of the original Denoising-UNet in handling high-frequency details. For complex garment patterns, relying solely on CLIP to extract image features is insufficient.\nSpecifically, DP-UNet enhances these details by incorporating a refinement step that focuses on high-frequency features. Starting with the latent representation of the texture image E(xc), we first pass it through a frozen, pre-trained U-Net. During the downsampling process, the encoder of the pre-trained U-Net extracts detailed features fc from the texture image. These features are subsequently concatenated with the corresponding features from the same layer of the denoising-UNet, facilitating the model's ability to accurately reconstruct the texture. Let Q represents the query matrix, K the key matrix, and V the value matrix. For texture feature fi from the decoupled cross-attention layer and detail feature fc, we define:\n$Q = f_iW_q, K = [f_i; f_c]W_k, V = [f_i; f_c]W_v, \\qquad(5)$\nThe self-attention is computed on the combined features as Equation 3. Which Wq, Wk, and W as the weights of the self-attention layer in the denoising-UNet. We use the"}, {"title": "3.3 DPDEdit Datasets", "content": "The current fashion datasets lack the necessary multimodal information for the task we aim to address. To address this limitation, we extend the virtual try-on domain dataset VITON-HD to better align with our specific requirements. VITON-HD is a high-resolution dataset specifically designed for fashion applications, containing image pairs with a resolution of 1024 \u00d7 768 pixels. Each pair consists of a garment image and a corresponding model image wearing the garment. The dataset comprises 11,647 items for the training set and 2,032 items for the test set. For each garment C and its corresponding mask Mc in the dataset, we extract fabric textures using a sliding window of 128 x 128 pixels, selecting only patches X that are fully contained within the garment mask Mc. To prevent resampling of specific regions of the garment, we use a stride of 64 pixels (128/2) in both horizontal and vertical directions. For garments with limited fabric area, where no suitable patch can be found within Mc, we reduce the window size to 64 x 64 pixels to ensure at least one patch X can be extracted for each garment C. Then, we input the extracted fabric texture images into the multimodal large language model LLaVA to generate a textual description of the texture pattern image. We employ LLaVa v1.6-34b3 for this annotation task. The process for extending the dataset is shown in Figure 5."}, {"title": "4 Experiments", "content": "4.1 Experimental Settings\nBaselines. We select four diffusion model-based image editing methods as our comparison baselines. For text-only inputs, we employ the Stable Diffusion inpainting"}, {"title": "4.2 Comparison to SOTA Methods", "content": "presents the quantitative comparison between our proposed DPDEdit and the baseline method on the extended VITON-HD test dataset. The text-only conditioned method, TexFit, demonstrates competitive performance in FID (12.63) and LPIPS (0.211) metrics when compared to multimodal approaches MGD and IP-Adapter. This indicates that with accurate localization of the editing region, text-only editing method can also produce high-quality im-"}, {"title": "4.3 Ablation Study", "content": "We performed an ablation study on the Grounded-SAM and DP-UNet components of the proposed method to evaluate their effectiveness in localizing the garment editing regions and preserving the fine-grained details of garment textures. The qualitative results of Grounded-SAM are shown in Figure 8. Grounded-SAM exhibits greater accuracy in identifying editing regions compared to TexFit, especially in cases involving complex body poses and varied garment styles. The qualitative results of DP-UNet can be referenced Figure 9. Images on the left in each pair are generated with DP-UNet, demonstrating improved pattern accuracy and consistency across different designs, while images on the right are without DP-UNet, showing less precise alignment.Furthermore, we conduct a quantitative evaluation on extended VITON-HD test dataset in Table 2, we see that using Grounded-SAM(replaces TexFit) and DP-UNet quantitatively improves Image fidelity and multimodal coherence, which is aligned with our qualitative results."}, {"title": "5 Conclusion", "content": "In this paper, we introduce DPDEdit, a novel method for fashion image editing guided by multimodal conditions. Our approach integrates textual descriptions, human poses, and garment textures to achieve localized editing in fashion images. DPDEdit utilize Grounded-SAM to ensures precise localization of garment regions. The proposed texture injection and refinement mechanism enables fine-grained control over the generated images. To address the challenges posed"}, {"title": "A Supplementary Details", "content": "A.1 Methods Preliminaries\nDiffusion Models Inspired by the principles of nonequilibrium thermodynamics (Sohl-Dickstein et al. 2015), diffusion models(Ho, Jain, and Abbeel 2020) are a sophisticated class of probabilistic generative models designed to perturb data by systematically introducing noise through a forward process and then learning to reverse this process to generate new samples. The fundamental concept of these models is to start with a randomly sampled noise image $x_T ~ N(0, I)$, and iteratively refine it in a controlled manner until it is transformed into a photorealistic image xo. Each intermediate sample xt (for t \u2208 {0,...,T}) satisfies $x_t = \\sqrt{a_t}x_o + \\sqrt{1 \u2013 a_t}e_t$, with $0 = \u03b1\u03c4 < &T\u22121 <\u2026 < \u03b11 < \u03b1\u03bf = 1$ being the hyperparameters of the diffusion schedule, and et ~ N(0, I). Each refinement step involves applying a neural network fo(xt, t) to the current sample xt, followed by a random Gaussian noise perturbation to obtain Xt-1. The network is trained using a simple denoising objective, aiming for fo(xt,t) \u2248 \u20act. This process results in a learned image distribution that closely approximates the target distribution, thereby facilitating exceptional generative performance.\nStable Diffusion introduces Latent Diffusion Models (LDMs) (Rombach et al. 2022), which represent a significant advancement in generative modeling. LDMs efficiently compress image data into a latent space using a pre-trained autoencoder, substantially reducing computational demands while preserving essential image details. Unlike traditional autoencoders, Stable Diffusion optimizes the latent space to capture higher fidelity image details with minimal regularization. The effectiveness of LDMs is quantified through the following equation:\n$L_{LDM} := E_{x(x),y,\\epsilon~N(0,1),t} [||\\epsilon - E_o (z_t, t, T_o(y))||_2], \\qquad(6)$\nwhere eo (zt, t, \u0442\u04e9(y)) denotes the model's prediction of noise at time t, and the training process aims to minimize the discrepancy between this prediction and the actual noise \u0395.\nClassifier-Free Guidance Classifier-Free Guidance(Ho and Salimans 2022) is a technique employed to enhance the quality of diffusion models. It leverages both conditional and unconditional data during training, allowing the model to be trained in a manner that integrates these two forms of data. During the generation phase, the outputs from both the conditional and unconditional branches are combined to improve the fidelity and diversity of the generated samples. Given a time step t and a generic condition c, the predicted diffusion process is governed by the following equation:\n$\\epsilon_{\\theta}(Z_t C) = \\epsilon_{\\theta}(Z_t\u00d8) + \u03b1 \\cdot (E_{\\theta}(Z_t|C) \u2013 E_{\\theta}(Z_t0)), \\qquad(7)$\nwhere 60 (ztc) represents the predicted noise at time t given the condition c, and 60(zt|(\u00d8) denotes the predicted noise at time t under the null condition. The guidance scale a serves as a hyperparameter that controls the degree of extrapolation towards the specified condition."}, {"title": "A.2 Training and Inference Details", "content": "DPDEdit is trained on the extended VITON-HD(Choi et al. 2021) dataset, which consists of 11,647 texture image-text pairs. For data augmentation(Kim et al. 2024), we apply horizontal flipping with a probability of 0.5 and random affine transformations, including shifting and scaling (limited to 0.2, with a probability of 0.5) to the multimodal inputs. The model is trained on a single machine equipped with 8 A6000 GPUs for 65k steps, with a batch size of 8 per GPU. We employ the AdamW(Diederik 2014) optimizer with a fixed learning rate of le-5 and a weight decay of 0.01. To facilitate classifier-free guidance(Ho and Salimans 2022), we use a probability of 0.05 to drop either the text or the texture image individually, and a probability of 0.05 to drop both simultaneously. During inference, we utilize the DDIM(Nichol and Dhariwal 2021b) sampler with 30 steps, setting the guidance scale to 5.0, which has been found effective in practice. When only the texture image prompt is used, the text prompt is left empty, and A is set to 1.0. Additionally, a batch size of 2 is used during inference to efficiently manage GPU memory. To ensure reproducibility across different inference runs, we use a random seed of 42."}, {"title": "A.3 Datasets Construction", "content": "To create a paired dataset of garment texture images and text descriptions, we utilized LLaVA1.6-34B(Liu et al. 2024) to annotate the fashion texture images. Due to the low resolution of the texture images extracted from garments, we upscaled the garment texture to 256x256 to display more detailed patterns. To diversify the model's responses, we employed various types of instructions during the dialogue. Considering the distinctive features of fashion garment images, it's crucial for the model to concentrate on key attributes like color, texture, fabric material, and pattern. To achieve accurate annotations for the texture images, we specifically highlighted these elements in our instructions. The instructions as shown in Table 3. The dataset generated using this strategy is shown in Figure 10. This method facilitates the creation of a diverse set of garment textures paired with detailed text descriptions, providing robust support for our task."}, {"title": "B Additional Qualitative Results of DPDEdit", "content": "In this section, we present supplementary qualitative results to further demonstrate the effectiveness of DPDEdit. Figure 11 showcases results on the extended VITON-HD test set, where the use of precise editing region masks generated by Grounded-SAM(Ren et al. 2024) enables DPDEdit to seamlessly modify the color, texture, and patterns of target garments while maintaining the original design. Additionally, Figure 12 illustrates DPDEdit's performance on a broader range of datasets, including fashion images from open-world scenarios and other datasets such as Dresscode(Morelli et al. 2022). These results highlight DPDEdit's ability to edit fashion garments across various backgrounds and human poses, as well as its effectiveness in modifying different parts of garments, including the lower body and dresses."}]}