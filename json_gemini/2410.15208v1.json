{"title": "Low-cost Robust Night-time Aerial Material Segmentation through Hyperspectral Data and Sparse Spatio-Temporal Learning", "authors": ["Chandrajit Bajaj", "Minh Nguyen", "Shubham Bhardwaj"], "abstract": "Material segmentation is a complex task, particularly when dealing with aerial data in poor lighting and atmospheric conditions. To address this, hyperspectral data from specialized cameras can be very useful in addition to RGB images. However, due to hardware constraints, high spectral data often come with lower spatial resolution. Additionally, incorporating such data into a learning-based segmentation framework is challenging due to the numerous data channels involved. To overcome these difficulties, we propose an innovative Siamese framework that uses time series-based compression to effectively and scalably integrate the additional spectral data into the segmentation task. We demonstrate our model's effectiveness through competitive benchmarks on aerial datasets in various environmental conditions.", "sections": [{"title": "1 Introduction", "content": "Hyperspectral cameras capture the electromagnetic spectrum across multiple continuous bands, creating images with rich spectral information. This imaging technology has numerous important applications, including anomaly detection and remote sensing, where detailed spectral data can reveal insights that are not possible with conventional RGB imaging. For instance, Xu et al. [27] use low-rank and sparse representations to decompose hyperspectral data into background and anomalous components. Similarly, Ojha et al. [20] leverage hyperspectral data from the CRISM instrument to identify spectral signatures of hydrated salts on Mars, enabling them to detect and confirm the presence of these salts in recurring slope lineae.\nHowever, hardware limitations often impose a trade-off between spatial and spectral resolutions, typically resulting in low spatial resolution for hyperspectral images (HSI). Despite this, applications like material segmentation can significantly benefit from low-resolution HSI. To address this challenge, we develop a novel framework that leverages low spatial resolution hyperspectral images (HSI)"}, {"title": "2 Background and related work", "content": "Material segmentation is a well-established task that extends image classification from the image level to the pixel level. A significant amount of research has been conducted to address this problem in RGB images using various techniques: conditional random fields [4], convolutional network [18], structured prediction module [33,15,5], receptive field enlargement [25], usage of boundary information [13], the contextual information refinement[30], attention modules [26], and zero-shot generalization [10].\nWhile these methods focus exclusively on RGB images, there are few material segmentation works that extend beyond traditional RGB channels. Among these, [22] consider object-based segmentation with an additional channel near-infrared (NIR), while Liang et al. [14] introduce the MCubeS dataset, that include only two additional channels, polarization and NIR. In their work, Liang et al. incorporate a region-guided filter selection (RGFS) layer to optimize the use of imaging modalities for each material class, resulting in effective classification."}, {"title": "2.2 Previous work on combining RGB and HSI images", "content": "Several techniques have been proposed to combine RGB and HSI images to comprehensively understand the full SRI images underlying these pairs of RGB-HSI inputs. One class of methods [17,3] aims to enhance the spatial resolution of hyperspectral images through pansharpening techniques. Another approach, called hyperspectral unmixing, utilizes the underlying low-rank structure of the full SRI [28,11], as well as other prior information on the SRI's structure, such as spectral correlation [6], sparsity information [1], or spatial degradation [9,29,7].\nHowever, these methods are limited because they only handle individual images and are difficult to integrate into a deep learning framework for segmentation tasks. To address this problem, [12] discusses a framework to train a model jointly on both HSI images and RGB images so that the auxiliary RGB super-resolution can provide additional supervision and regulate network training. Additionally, RGB images have been spectrally super-resolved with hyperspectral images using class-based backpropagation neural networks (CBPNNs) [8]. Despite these advancements, these methods treat RGB and HSI images as separate entities, highlighting the need for a method that uses paired RGB-HSI inputs to enhance segmentation tasks. To this end, we develop a scalable technique that takes individual RGB-HSI pairs as inputs and outputs a desirable segmentation map."}, {"title": "3 Proposed approach", "content": "We formally describe the problem as follows:"}, {"title": "3.1 Problem description", "content": "Our dataset D consists of pairs of RGB and HSI images D = {(Zi, Yi)}^N_{i=1} with a total of N image pairs. Here each RGB image Zi has dimension 3 \u00d7 H \u00d7 W, where 3 corresponding to the standard red, green, blue channels, and H and W are the (high resolution) height and width of the image. Similarly, each HSI image Yi has the dimension C \u00d7 h \u00d7 w with (low resolution) height h and width w, and the number of channels C. Here we assume that H \u226b h, W \u226b w, C \u226b 3.\nWe want to learn a material segmentation model defined by\nS: (Zi, Yi) \u2192 Mi,\n(1)\nwhere Mi is the predicted segmentation map for the ith input pair with the dimension L \u00d7 H \u00d7 W such that Mi(x,y) gives the predicted logits for the probabilities of occurrence (as an L-dimensional vector) over all material classes. We suppose that there are L possible classes at each spatial pixel location (x, y)."}, {"title": "3.2 General method", "content": "Our framework (see Figure 1) includes two main steps:\n1. First we use the technique from [2] in order to extract the relevant information across C channels to a smaller of size 6 \u00d7 h \u00d7 w: X\u2081 = MC(Yi) (see Section 3.3), where MC is the function corresponding the extraction process.\n2. Then we feed the pair (Xi, Zi) into a Siamese network S. This network is comprised of an IWCA module I\u2084 (see Section 3.4), an encoder with 2 branches ERGB and EHSI, and a decoder D\u2084 (see Section 3.5 for encoder and decoder details). The final output has the form:\nM\u2081 = S(Xi, Zi) = D\u2084(EHSI (I\u2084(Xi)), ERGB (Zi))\n(2)"}, {"title": "3.3 Selective learning for hyperspectral imaging", "content": "While HSI images are valuable for improving material prediction, their large number of channels makes deep learning training computationally intensive. To address this, our framework begins by reducing and condensing the relevant information for the segmentation task. Instead of learning from all C channels, we select only the most informative channels for our hyperspectral input. Specifically, we first convert the original image data into time series data by considering pixel values across all channels. In other words, for each image in the training set and for each of the h \u00d7 w pixel locations in that image, we create a time series of length C, where the values represent the corresponding pixel intensity. Each time series is then assigned a label based on the material class of its pixel (see Figure 2).\nWe then randomly select n out of all possible time series. Next, we apply the time series analysis technique known as Motion Code from [2] to extract the"}, {"title": "3.4 IWCA module", "content": "Before entering the Siamese model, our extracted Xi from the original HSI image Yi passes through a module called Importance Weighted Channel Attention (IWCA). Inspired by attention mechanisms [24], the IWCA module re-weights the channels based on their importance and consists of two branches. The first branch uses regular 3 \u00d7 3 convolutions to integrate spatial and channel information. The second branch applies the group convolution operator to aggregate channel importance statistics, followed by average pooling and a sigmoid layer. Finally, the importance weight vector from the second branch is element-wise multiplied with the output of the first branch to produce the final output."}, {"title": "3.5 Siamese U-net", "content": "Our model's Siamese component leverages the strengths of both hyperspectral and RGB images to achieve high-quality segmentation results. The architecture is built on a modified U-Net structure, incorporating specialized blocks for down-sampling, feature extraction, and upsampling.\nThe Siamese encoder is specifically designed to process both the low-resolution hyperspectral image (HSI) and the RGB image simultaneously. It features two parallel branches, each containing a series of convolutional layers that progressively downsample the input images and extract relevant features. For each input pair, the HSI image is processed by the IWCA module (see Section 3.4) before entering one branch, while the RGB image is fed directly into the other branch. Each block in the encoder comprises a 1\u00d71 convolutional unit, followed by a 3 \u00d7 3 convolutional unit, and another 1\u00d71 unit. The 1 x 1 units handle channel"}, {"title": "4 Experiments", "content": "Datasets and experiment setup\nWe consider two aerial datasets Jasper Ridge and Urban [34]. For each dataset, we extract pairs of HSI and RGB images through the following 4 steps:\n1. We first obtain 16\u00d716 sub-views from the dataset's global scene by extracting 16 \u00d7 16 sub-images from the larger global scene. Each extracted sub-image represents an aerial view of a specific area within the captured regions. For example, in the Jasper Ridge dataset, one sub-image might capture an area near the river, while another might show a section deep in the jungle."}, {"title": "4.2 Adversities effects", "content": "Low-Light effect Using Gamma Correction The low-light effect algorithm modifies a hyperspectral image by applying gamma correction to each spectral band. This involves normalizing the pixel values, applying the gamma transformation, and scaling them back to their original range. The process ensures that the image appears darker, simulating low-light conditions.\nAtmospheric scattering We adjust images based on a atmospheric scattering model that incorporates haze formation within images [32] with the following equation:\nI(x) = J(x)t(x) + A(1 \u2013 t(x))\n(4)"}, {"title": "4.3 Evaluation results", "content": "We select two other baselines: U-Net [21] and convolutional neural network (CNN) models trained solely on RGB images to assess the impact of our approach in incorporating low-resolution hyperspectral data. We have 2 evaluation metrics:\n1. Per-class mean IOU (MIOU), where for each material class, we compute the mean of all IOU [14] over all pixels.\n2. The generalized dice score (gDice) metric across all material classes. [23]\nOur method outperforms other baseline models, as demonstrated in Table 1, when only the atmospheric scattering effect (flight effect on day time) is applied. More importantly, we present results showcasing our method's performance under low-light night-time scenarios, with and without contrast enhancement. Specifically, our method significantly surpasses the other baselines, not only in terms of gDice but also in mIOU for each class, across both the Jasper Ridge and Urban datasets (see Table 2 and Table 3).\nContrast Enhancement Images taken in low-light conditions often suffer from poor contrast, making it difficult to extract meaningful information. In addition to the regular setting, we also allow contrast enhancement through a series of processing steps [31]. The preprocessing includes Gaussian filtering for anisotropic propagation controlled by the standard deviation \u03c3, also known as the conductivity coefficient C, and computing local statistics (minimum, maximum, and average) within a sliding window. We also apply a parabolic transfer function to each pixel, adjusting the contrast based on its intensity relative to the local minimum, maximum, and average."}, {"title": "4.4 Night-time effect analysis", "content": "Our model significantly outperforms conventional RGB-based models like CNN and U-Net in low-light conditions. RGB-only models struggle under night-time scenarios due to their reliance on visible light, which is severely limited in such conditions. As a result, these models show poor segmentation accuracy, particularly in distinguishing material boundaries like roads or water surfaces.\nIn contrast, our Siamese network consistently maintains high accuracy by incorporating hyperspectral data alongside RGB inputs. Hyperspectral channels, especially those in the infrared (IR) spectrum, capture crucial material properties that remain visible even in low-light environments. These channels allow the model to effectively differentiate between materials based on their reflectance and surface textures, which are otherwise indistinguishable in darkness using RGB data alone. This capability gives our model a significant advantage over traditional approaches, allowing it to \"see through\" low-light conditions and provide more accurate material segmentation."}, {"title": "4.5 Visualization analysis", "content": "Visualization of the segmentation maps generated by our model versus the baseline models clearly highlights the strengths of our approach . In challenging areas, both CNN and U-Net models struggle to maintain the topological structure of the scene. In contrast, our model preserves the geometric integrity of the materials being segmented.\nThis is especially evident in regions with mixed materials, such as transitions between road and dirt or water and tree areas. The U-Net and CNN models introduce substantial noise and errors in these regions, resulting in significant misclassifications. The visualizations highlight our model's effectiveness in maintaining structural coherence in the segmentation maps. This capability is critical for applications where understanding the topology and material distribution in aerial imagery is essential."}, {"title": "5 Conclusion", "content": "In this paper, we developed a framework that combines traditional RGB images with hyperspectral data to tackle the material segmentation task on aerial data, particularly under night-time and challenging atmospheric conditions. We demonstrated our framework's capabilities and robustness in improving segmentation results through the efficient use of hyperspectral data. In the future, we aim to extend our work to more general datasets and other critical tasks beyond material segmentation.\nThe implementation is available at https://github.com/CVC-Lab/HSI-MSI-Image-Fusion."}]}