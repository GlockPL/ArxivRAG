{"title": "DISTRL: AN ASYNCHRONOUS DISTRIBUTED REINFORCEMENT LEARNING FRAMEWORK FOR ON-DEVICE CONTROL AGENTS", "authors": ["Taiyi Wang", "Zhihao Wu", "Jianheng Liu", "Jianye Hao", "Jun Wang", "Kun Shao"], "abstract": "On-device control agents, especially on mobile devices, are responsible for operating mobile devices to fulfill users' requests, enabling seamless and intuitive interactions. Integrating Multimodal Large Language Models (MLLMs) into these agents enhances their ability to understand and execute complex commands, thereby improving user experience. However, fine-tuning MLLMs for on-device control presents significant challenges due to limited data availability and inefficient online training processes. This paper introduces DistRL, a novel framework designed to enhance the efficiency of online RL fine-tuning for mobile device control agents. DistRL employs centralized training and decentralized data acquisition to ensure efficient fine-tuning in the context of dynamic online interactions. Additionally, the framework is backed by our tailor-made RL algorithm, which effectively balances exploration with the prioritized utilization of collected data to ensure stable and robust training. Our experiments show that, on average, DistRL delivers a 3x improvement in training efficiency and enables training data collection 2.4x faster than the leading synchronous multi-machine methods. Notably, after training, DistRL achieves a 20% relative improvement in success rate compared to state-of-the-art methods on general Android tasks from an open benchmark, significantly outperforming existing approaches while maintaining the same training time. These results validate DistRL as a scalable and efficient solution, offering substantial improvements in both training efficiency and agent performance for real-world, in-the-wild device control tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "The integration of Large Language Models (LLMs) into agents capable of complex tasks has gained momentum with initiatives like AutoGPT Yang et al. (2023a), HuggingGPT Shen et al. (2024), and MetaGPT Hong et al. (2023), AutoUI Zhan & Zhang (2023), etc. These LLM-based agents extend beyond language processing to perform sophisticated functions, including software development and gaming, leveraging their reasoning abilities to interact with and manipulate environments effectively.\nOne of the key factors driving this trend is the advent of Multimodal Large Language Models (MLLMs), which can process diverse inputs such as text, images, audio, and video, thereby significantly expanding the scope of LLM applications Alayrac et al. (2022); Achiam et al. (2023); Zheng et al. (2024); Li et al. (2023). This versatility enables MLLM-based on-device control agents-intelligent systems embedded within mobile devices that manage and operate applications to execute user commands seamlessly to interact more naturally and efficiently with their surroundings, completing more complex tasks that require a deeper understanding of context and the ability to learn from interactions. For instance, agents designed to operate smartphone applications can interpret screenshots from the operating system, demonstrating flexibility and adaptability that make them valuable tools in a wide range of scenarios Yang et al. (2023b); Wang et al. (2024). These agents are essential for tasks such as automating app interactions, managing settings, and enhancing user productivity by providing intuitive control over device functionalities."}, {"title": null, "content": "However, a gap remains between LLMs' general reasoning capabilities and their effectiveness in GUI-based device control. While LLMs can process information, they struggle with rational behavior and error recovery in real-world environments. Previous solutions use complex wrappers,\nbut without updating model weights, their performance remains limited Bai et al. (2022a). Consequently, prior work in building device agents often relies on constructing complex wrappers around these models, combining them with prompting, search, or tool use. However, without updating the model's weights, the effectiveness of these agents remains inherently limited by the capabilities of the base model.\nTo bridge this gap, fine-tuning(type of light training) aforementioned agents on demonstrations via imitation learning Zhan & Zhang (2023); Nakano et al. (2021); Yang et al. (2023b) channels pretrained abilities into actionable behaviors suitable for device control. However, the dynamic nature of devices and the web renders models trained on static data sub-optimal as ecosystems evolve. Such agents struggle to recover from mistakes in changing environments, limiting their practical utility. Therefore, building robust and reliable device-control agents necessitates an interactive approach that enables MLLMs to adapt and learn from their own experiences on devices and the Internet.\nMoreover, MLLMs are prone to confidently generating incorrect content that deviates from human preferences Ouyang et al. (2022); Ziegler et al. (2019); Bai et al. (2022a); Stiennon et al. (2020). To address these challenges, introducing reinforcement learning (RL)-based fine-tuning methods, such as Reinforcement Learning from Human Feedback (RLHF), becomes essential. RLHF leverages human feedback to align model outputs with desired behaviors and preferences. By incorporating RL-based methods, models can learn to optimize policies that not only perform tasks effectively but also adhere to human expectations. Details of the problem formulation as an RL-based fine-tuning approach will be discussed in Sections 2.2 and 3.\nOne significant challenge in RL-based fine-tuning of mobile agents is the lack of support for efficient online fine-tuning. Existing offline datasets, such as AitW Rawles et al. (2024b) and AndroidControl Li et al. (2024), provide static data that do not capture the dynamic and evolving nature of mobile apps. Frequent updates and new elements like advertisements cause distribution shifts that offline-trained agents struggle to handle, leading to failures in real-world deployments.\nThe second challenge is the need for Reinforcement Learning (RL) algorithms that can operate efficiently within a distributed framework. Asynchronous data collection introduces algorithmic difficulties: non-stationary data distributions hinder convergence, and delays between policy updates and data collection can cause agents to act on outdated policies, degrading performance. Ensuring consistency and stability becomes more complex when dealing with distributed agents collecting data at different rates and times, necessitating robust mechanisms to handle delayed or out-of-order updates.\nTo further demonstrate these challenges and the non-trivial nature of bringing MLLMs into mobile device control scenarios, our extensive case studies reveal that even the most advanced proprietary Multimodal Large Language Models (MLLMs) like GPT-4V Achiam et al. (2023), agents such as AutoUI with Supervised Fine-Tuning (SFT), and state-of-the-art mobile control agents like DI-"}, {"title": null, "content": "GIRL Bai et al. (2024) fail in numerous scenarios. A detailed analysis of these failure modes is provided in Appendix A.1.\nThese challenges motivate us to develop DistRL, as illustrated in Figure 1. DistRL is a novel and scalable reinforcement learning (RL) fine-tuning pipeline specifically designed for on-device mobile control agents on Android, featuring Centralized Training and Decentralized Data Acquisitions. Our main contributions are:\n1. Scalable and Asynchronous Architecture for Data Acquisitions: DistRL introduces a decoupled and asynchronous architecture that deploys a tailor-made RL fine-tuned agent across a variety of heterogeneous worker devices and environments for remote data collection. Each worker asynchronously sends real-time interaction data back to the central learner, which continuously updates the agent. This design improves training efficiency and scalability, making DistRL highly effective for adapting control agents to dynamic environments.\n2. Advanced RL Algorithm Tailored for Centralized Training: DistRL leverages a novel offpolicy reinforcement learning algorithm, A-RIDE (detailed in Section 5), specifically designed for distributed and asynchronous data utlizations. Our algorithm prioritizes significant experiences to enhance sample efficiency, ensuring that the learning process focuses on the most informative data while simultaneously encouraging exploratory behavior among workers.\nIn practice, we validate our framework using a T5-based multimodal generation model with 1.3B parameters (details in Appendix A.5.1) to efficiently handle both vision and language inputs. To the best of our knowledge, DistRL is the first work to scale autonomous, online RL fine-tuning for mobile device control in a distributed environment."}, {"title": "2 RELATED WORKS", "content": null}, {"title": "2.1 MULTI-MODAL ON-DEVICE CONTROL AGENTS", "content": "Recent advancements in pre-trained Large Language Models (LLMs) and Multimodal LLMs (MLLMs) have revolutionized on-device control agents, moving beyond early methods like behavioral cloning or reinforcement learning Osa et al. (2018); Mnih et al. (2015); Shi et al. (2017).\nEarly agents simulated mouse clicks and typing Shi et al. (2017); Humphreys et al. (2022) but faced scalability and adaptability challenges.\nModern approaches use pre-trained models with zero or few-shot prompting and fine-tuning for enhanced capabilities. WebGPT Nakano et al. (2021) employ fine-tuned models for web browsing, while WebAgent Gur et al. (2023) generates web code using T5. AppAgent Yang et al. (2023b) and MobileAgent Wang et al. (2024) act as drivers, enabling the LLMs to explore and act on mobile device environments. Training multimodal device control agents poses challenges like pixel-level interactions and variability in device ecosystems. Many rely on proprietary Vision-Language Models (VLMs) and wrappers for GUI visual grounding Driess et al. (2023); Reid et al. (2024), but without fine-tuning, they are limited by the base models Driess et al. (2023).\nSome works fine-tune VLMs with demonstration data, e.g., AutoUI, CogAgent Kapoor et al. (2024);\nZhan & Zhang (2023), but static data-trained models may struggle with real-world variability Jiang et al. (2023). Others use filtered imitation learning with autonomously collected data Pan et al. (2024); Lai et al. (2024), While DIGIRL Bai et al. (2024) supports on-device RL fine-tuning, it encounters significant inefficiencies in parallel environments. Specifically, DigiRL's multi-machine setup relies on a fully synchronous data acquisition process, causing faster workers to idle while waiting for slower ones. This approach is impractical in real-world scenarios where task durations can vary by up to 100 times, ranging from seconds to over ten minutes. Additionally, DigiRL lacks support for efficient distributed learning algorithm designs, further hindering its scalability and performance in dynamic, parallel settings. To address this, our scalable and asynchronous RL fine-tuning pipeline, DistRL, offers an efficient solution for distributed mobile control agent training."}, {"title": "2.2 REINFORCEMENT LEARNING FOR ON-DEVICE AGENT FINE-TUNING", "content": "Reinforcement Learning from Human Feedback (RLHF) is widely used to fine-tune LLMs to align with human preferences Stiennon et al. (2020); Ouyang et al. (2022). In device control tasks, similar approaches use imitation learning from human-labeled evaluations, but RLHF is labor-intensive due to the need for human annotations Ouyang et al. (2022); Bai et al. (2022a)."}, {"title": null, "content": "Recent advances in MLLMs Alayrac et al. (2022); Driess et al. (2023); Reed et al. (2022); Li et al. (2023); Zhang et al. (2023) show impressive multimodal capabilities but often produce incorrect outputs that deviate from human preferences Ouyang et al. (2022); Ziegler et al. (2019); Bai et al. (2022a); Stiennon et al. (2020). Reinforcement Learning from AI Feedback (RLAIF), using AI labelers as proxies, offers an alternative Bai et al. (2022b); Yu et al. (2024); Lee et al. (2023).\nFor on-device tasks, AI evaluators assess task completion using prompts and screenshots Bai et al. (2024); Lee et al. (2023); Yu et al. (2024).\nPrevious RL research focused on single-turn tasks, limiting their effectiveness for multi-step problems Reed et al. (2022); Liang et al. (2023); Driess et al. (2023). To address this, we developed a simplified off-policy multi-turn RL algorithm, A-RIDE, which learns from suboptimal online interactions, reducing complexity and accelerating convergence compared to previous value-based methods Driess et al. (2023); Nakano et al. (2021); Reed et al. (2022); Yao et al. (2023). This approach is effective for large-scale applications like Android device control."}, {"title": "2.3 SCALABLE AND DISTRIBUTED RL FRAMEWORK", "content": "Scalable reinforcement learning frameworks like Ray RLlib Liang et al. (2018) enable distributed training by parallelizing policy learning across CPUs and GPUs using the Ray engine Liang et al. (2017). Ray RLlib supports various algorithms and efficiently manages neural network training, but it assumes that data collection can be simulated or parallelized within the same infrastructure. This assumption limits its applicability to real-world on-device control tasks involving actual mobile devices, where data collection must occur across distributed, heterogeneous devices with varying task durations and network conditions.\nMoreoever, frameworks such as IMPALA Espeholt et al. (2018) and IMPACT Luo et al. (2019) are optimized for fast simulations, enabling efficient data collection and policy updates in highly parallelized environments. Applying these frameworks to on-device control introduces non-trivial challenges due to the stochastic nature of real-world interactions and the scalability constraints of mobile devices, including heterogeneous device capabilities, variable task execution times, and unreliable communication.\nOur approach builds upon the foundational ideas of IMPALA and IMPACT, effectively extending their concepts to accommodate the unique challenges of on-device control. We introduce a decoupled and asynchronous architecture that supports distributed data collection from heterogeneous, real-world devices by implementing sophisticated communication protocols and optimized data utilization strategies. Furthermore, we develop a novel reinforcement learning algorithm specifically designed to handle the stochasticity and scalability inherent in mobile device environments. This algorithm enhances and maximizes the utilization of collected data while maintaining the exploration behavior of workers, addressing critical aspects that prior work utilizing A3C Mnih (2016) have largely overlooked."}, {"title": "3 PROBLEM SETUP AND PRELIMINARIES", "content": "As presented in Figure 2, we model the ondevice control problem as a finite-horizon Markov Decision Process (MDP) $\\mathcal{M} = {\\mathcal{S}, \\mathcal{A}, \\mathcal{T}, \\mathcal{R}, \\mu_0, H}$. Here, $\\mathcal{S}$ denotes the set of GUI states, represented by screenshots or visual observations of the device screen. $\\mathcal{A}$ represents the set of actions available to the agent, such as touch events at specific screen coordinates. The state transition function $\\mathcal{T} : \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0, 1]$ defines the probability of transitioning from one state to another given an action. The reward function $\\mathcal{R} : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ provides sparse rewards, typically positive upon task completion. $\\mu_0$ is the initial state distribution, and $H$ is the finite horizon of the episode.\nAt each timestep $t$, the mobile agent observes a state $s_t \\in \\mathcal{S}$, selects an action $a_t \\in \\mathcal{A}$ according to its policy $\\pi(a_t | s_t)$, receives a reward $r_t = \\mathcal{R}(s_t, a_t)$, and transitions to the next state $s_{t+1}$. The agent's"}, {"title": null, "content": "objective is to maximize the expected cumulative reward $E_{\\pi} \\left[\\sum_{t=0}^{H} \\gamma^t r_t \\right]$ over the episode. Given the asynchronous nature of distributed data generation in DistRL, trajectories are collected under behavior policies $\\pi_l$ and used to optimize a target policy $\\pi$. This setup requires robust off-policy learning algorithms to correct for discrepancies between $\\pi_l$ and $\\pi$.\nA critical component of our RL framework is the ability to obtain reliable reward signals in realtime. To achieve this, we utilize Gemini-1.5-pro Reid et al. (2024) as an autonomous evaluator to assess whether the agent has successfully completed the task at each state. The evaluator receives the current observation, composed of the task description and a screenshot of the device, and outputs a reward signal. Specifically, the evaluator assigns a reward $r_t = 1$ if the screenshot indicates successful task completion and $r_t = 0$ otherwise. This effectively transforms the problem into a Partially Observable MDP (POMDP), where the evaluator helps determine the termination condition based on the agent's observation. Additionally, we applied a reward penalty on unexpected behaviors like repetition which we observed many times in collected trajectories. Details of how we implemented the auto-evaluation can be found in Appendix A.2."}, {"title": "4 SYSTEM DESIGN", "content": "DistRL is an asynchronous distributed reinforcement learning framework for scalable and efficient training of mobile agents. By decoupling trajectory collection from policy learning and doing both in parallel, it leverages distributed working machines for CPU-intense agent-environment interactions and GPU servers for policy training. This separation optimizes efficiency, scalability, and resource utilization by aligning tasks with appropriate hardware.\nThis decoupled and asynchronous design offers several key advantages: it improves scalability as data collection scales naturally and linearly with more working machines providing the mobile environment, even if there are large performance gaps between them; it optimizes resource utilization by assigning tasks to suitable hardware; and it improves policy quality through richer, more diverse datasets from multiple devices, enhancing robustness and generalization capabilities. The details of our system are presented as follows:\nAs illustrated in Figure 3, DistRL employs a host-worker architecture consisting of a central Host Learner (Left side in Figure 3) and multiple Workers (Right side in Figure 3) which can be heterogeneous devices: i.e. machines of various specifications, running android emulators or being connected with mobile devices, providing the android interaction environments. These components work together to train agents through asynchronous data collection and distributed policy updates.\nHost Learner: Host Learner orchestrates the policy training process using powerful GPUs. It maintains a Circular Replay Buffer (details in Appendix A.4) that stores the trajectories collected from the workers. The training loop processes this data by applying reinforcement learning algorithms to update the policy. To manage incoming data efficiently, a FIFO Trajectory Queue receives experiences from the workers and organizes them for training.\nThe host learner updates the policy with tailored regularization controls (details in Section 5) to encourage workers to explore a broader range of potential actions during task execution. This approach ensures diversity in the collected data, preventing convergence towards homogeneous behaviors, which is especially crucial in dynamic and complex mobile device environments. Additionally, to maximize the use of the large-scale and diverse data collected, and to avoid excessive learning on redundant or similar data, the learner employs priority-based sampling techniques (details in Section 5). These carefully curated design choices not only ensure training efficiency but also enhance the generalization capability of the policy. After updating the policy, the host learner distributes the latest version to the workers, allowing them to interact with their environments based on the latest learning updates. The training process runs continuously, with new experiences from the workers refining the policy, and the updated policy enhancing the workers' performance.\nWorkers: Workers operate in parallel, each managing its own Android environments with Android Emulators or actual Android devices through multi-threading. Each thread in the workers executes the policy received from the host learner and interacts with the environment through an Agent. The agent queries the environment, receives observations, and generates actions based on the current policy. Each worker collects trajectories-sequences of actions, observations, and rewards-during its interaction with the emulator."}, {"title": null, "content": "To facilitate efficient simulation, workers use Environment Snapshots, allowing them to reset the emulator to specific states. The result trajectories from the collecting threads are asynchronously sent back to the host learner to be added to the replay buffer for training. This asynchronous design allows heterogeneous worker machines with different specifications and performance levels to collaborate naturally in improving the data collection efficiency. It prevents interference between the workers and minimizes the impact of different threads within each worker. As a result, each worker can fully contribute their performance gains as expected.\nOn the whole, DistRL employs asynchronous RL to address the challenges of online RL in dynamic, real-world settings. Each thread in workers operates independently, executing tasks and generating learning trajectories at its own pace, which accommodates variability in task durations and system latencies. Data produced by the working threads is queued and processed by the host learner, which updates the global policy based on the collected trajectories. The updated policy is asynchronously distributed back to the workers, allowing for independent and non-blocking policy updates. This approach effectively manages temporal discrepancies between the workers and the host learner, ensuring smooth and effective learning across distributed workers. Further details regarding the communication mechanism between the host and workers are elaborated in Appendix A.3.2.\nThe asynchronous framework design also ensures scalability. With two 96 vCPU machines, it supports up to 32 emulators operating concurrently, and it can scale almost linearly with worker performance to handle large workloads."}, {"title": "5 METHODOLOGY", "content": "In this section, we introduce the core reinforcement learning algorithm employed in DistRL to finetune RL agents for device control tasks. The inherent issues of limited on-device resources, asynchronous data generation, and distributed constraints necessitate an efficient and scalable framework.\nReinforcement learning in distributed device control environments encounters significant challenges related to policy stability, convergence, and effective exploration. Stable convergence is essential for reliable agent performance, while robust exploratory behavior is crucial for discovering effective control policies in dynamic and complex settings. On-policy algorithms such as Proximal Policy Optimization (PPO) Schulman et al. (2017) and Advantage Actor-Critic (A2C) Mnih (2016) are limited by their reliance on synchronous data collection and policy updates, leading to sample inefficiency and delayed learning in asynchronous, multi-agent environments.\nTo overcome these challenges, we introduce A-RIDE, an off-policy reinforcement learning algorithm tailored for distributed environments. A-RIDE stands for Advantage-based Retrace Improved by Distributed Prioritized Experience Replay. It enhances exploration efficiency, maintains policy robustness, and improves training efficiency by promoting robust explorative behavior, ensuring policy stability, and prioritizing informative experiences. These advancements enable DistRL to achieve stable and efficient learning in real-world device control tasks."}, {"title": "5.1 A-RIDE: THE BACKBONE OF DISTRL", "content": "Our method employs advantage-based estimations to refine policy gradient updates, as an extension of Generalized Advantage Estimation (GAE) Schulman et al. (2015), effectively balancing exploration and exploitation in the learning process. By introducing a trace decay parameter, A-RIDE manages the bias-variance trade-off in advantage calculations, optimizing the stability and convergence of the policy. A-RIDE incorporates enhancements tailored to distributed, asynchronous environments, ensuring robust policy stability and efficient learning in complex device control tasks."}, {"title": null, "content": "While GAE has proven highly effective in synchronous environments, its reliance on synchronized data collection makes it less suitable for asynchronous settings, such as distributed control tasks on devices. To handle asynchronous trajectory generation in device control tasks, A-RIDE leverages an enhanced Retrace($\\lambda$) method for robust off-policy corrections inspired by Espeholt et al. (2018); Munos et al. (2016). Unlike traditional on-policy algorithms that require synchronous data collection, A-RIDE is designed for distributed environments with asynchronous data.\nThe Retrace($\\lambda$) update is defined as: $Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\delta_t$, where the correction term $\\delta_t$ is calculated as:\n$\\delta_t = \\sum_{k=t}^{H} \\gamma^{k-t} \\left(\\prod_{i=t+1}^{k} c_i\\right) [r_k + Q(s_{k+1}, a_{k+1}) - Q(s_k, a_k)]$,\nHere, $Q(s_t, a_t)$ is the estimated action-value function; $\\gamma \\in [0, 1]$ is the discount factor; $H$ is the time horizon; $c_i = \\text{Amin} \\left(1, \\frac{\\pi(a_i|s_i)}{\\mu(a_i|s_i)}\\right)$ with $\\lambda\\in [0, 1]$ being the trace decay parameter; $\\frac{\\pi(a_i|s_i)}{\\mu(a_i|s_i)}$ is the importance sampling ratio between the target policy $\\pi$ and the behavior policy $\\mu$.\nTo ensure effective exploration within the action space and prevent the generation of nonsensical or invalid commands, we incorporate entropy regularization into the actor loss function. This addresses the challenge inherent in Vision-Language Models (VLMs) where purely random exploration may lead to semantically incoherent actions Li et al. (2017). The actor loss is defined as:\n$\\mathcal{L} = -\\mathbb{E}_\\mu [\\mathcal{P}_t A(s_t, a_t) \\log \\pi(a_t|s_t)] - \\beta \\mathbb{E}_\\mu [\\mathcal{H}(\\pi(a_t|s_t))] + \\lambda \\mathbb{E}_\\mu [\\mathcal{P}_{invalid}(a_t)]$,\nwhere $\\mathcal{P}_{invalid}(a_t)$ imposes a penalty on actions deemed invalid based on task-specific criteria, $\\beta$ controls the strength of entropy regularization, and $\\Lambda$ modulates the penalty's influence. The penalty is assigned using validation through pre-trained LLMs like Gemini Reid et al. (2024)), ensuring that only contextually appropriate actions are penalized. The hyperparameters $\\beta$ and $\\Lambda$ are optimized through empirical studies to balance exploration and policy robustness effectively. This formulation encourages the agent to explore a diverse set of actions while constraining it to generate valid and meaningful commands, thereby enhancing both exploration and policy robustness."}, {"title": "5.2 DISTRIBUTED PRIORITIZED EXPERIENCE REPLAY (DPER)", "content": "To improve sample efficiency, we employ Distributed Prioritized Experience Replay (DPER). For each trajectory $\\mathcal{T} = \\{(s_t, a_t, r_t, s_{t+1})\\}_{t=0}^{H}$, we compute the priority $p(\\tau)$ as: $p(\\tau) = w_1|\\delta| + w_2\\rho + w_3H$, where $|\\delta|$ is the average absolute temporal-difference (TD) error over the trajectory, calculated as $\\delta_t = r_t + V(s_{t+1}) - V(s_t)$; $\\rho$ is the average importance sampling ratio $\\rho_t$; and $\\mathcal{H}$ is the average policy entropy, $\\mathcal{H}_t = -\\log \\pi(a_t|s_t)$, encouraging exploration by encouraging policy uncertainty, thus avoiding early convergence to suboptimal policies during training in dynamic environments. The weights $w_1$, $w_2$, and $w_3$ balance the contributions of each component, which is selected by grid-search. Trajectories with higher priorities are replayed more frequently, focusing learning on the most informative experiences. Priorities are periodically updated based on the latest policy, recalculating them to focus learning on the most informative experiences, ensuring continual adaptation to evolving behavior policies. Details can be found in Appendix A.4."}, {"title": "5.3 DISTRL PIPELINE IMPLEMENTATION", "content": "As illustrated in Figure 4, DistRL adopts a distributed asynchronous setup where multiple worker agents generate trajectories under the behavior policy $\\mu$ and send them to a central learner. The trajectory reward is computed using the Monte Carlo estimate:\n$\\mathcal{L}(\\mathcal{V}_{traj}) = -\\mathbb{E}_\\tau [r(s_H, a_H) \\log \\mathcal{V}_{traj} (s_H, a_H) + (1 - r(s_H, a_H)) \\log (1 - \\mathcal{V}_{traj} (s_H, a_H))]$.\nThe actor is updated using policy gradients based on advantage estimates, and enhanced Retrace corrections are applied for off-policy learning. This process is distributed asynchronously across worker nodes, ensuring efficient fine-tuning in environments with sparse rewards and distributed delays."}, {"title": "6 EXPERIMENTS", "content": "To evaluate the performance of DistRL on challenging Android device control tasks, we conducted extensive experiments. Our primary goal is to determine whether DistRL can produce agents that effectively learn from autonomous online interaction. We describe the experimental environment in Section 6.1, the baseline and benchmarks in Section 6.2, training performance in Section 6.3, and on-device task evaluations in Section 6.4. Additionally, we present ablation studies on our approach's components in Section 6.5."}, {"title": "6.1 EVALUATION ENVIRONMENT", "content": "Our evaluation environment consists of a host learner with 4 NVIDIA V100 GPUs for intensive policy training and two worker machines with 8 NVIDIA Tesla T4 GPUs and 96 vCPUs each, supporting parallel emulation. This setup leverages 192 vCPUs to run multiple emulators concurrently, enabling scalable distributed reinforcement learning experiments. The worker machines handle inference and data collection, asynchronously communicating with the host learner to exchange trajectories and updated model weights. This configuration allows us to assess DistRL's scalability and performance in a realistic large-scale distributed environment."}, {"title": "6.2 BENCHMARKS AND BASELINE METHODS", "content": "To comprehensively validate our approach, we utilize both the General and web shopping tasks for training and testing. Specifically, our training set is derived from enhanced online task instructions, which are composed of AitW Rawles et al. (2024b), AndroidWorld Rawles et al. (2024a), and expert-curated task sets. We fine-tune our model on this combined training set and evaluate performance on the corresponding test subsets derived from AitW. Our analysis focuses on training efficiency using the General Tasks, which include fundamental application operations and information retrieval tasks. Additionally, we assess the agent's performance on both General Tasks and Web Shopping Tasks to evaluate its capability in handling domain-specific instructions, addressing the significant task distribution gap. Detailed descriptions of the datasets used are provided in Appendix A.5. Our baseline methods include:\n\u2022 DIGIRL Bai et al. (2024): The state-of-the-art framework prior to our work, which integrates RL fine-tuning with visual language models (VLMs) and provides a reproducible training process. We consider its both single and multi-machine settings in online mode.\n\u2022 AutoUI Zhan & Zhang (2023): A simple explorative mobile agent equipped with VLMs under supervised fine-tuning.\n\u2022 GPT-4V OpenAI et al. (2024) and Gemini 1.5 Pro Reid et al. (2024): Equipped with exploration drives-AppAgent Yang et al. (2023b) to facilitate learning from the environments."}, {"title": "6.3 TRAINING PERFORMANCE", "content": "Training efficiency is crucial in reinforcement learning, particularly in complex environments, and is measured by the rate of improvement over time. We compared DistRL with the existing DIGIRL framework. Our results show that DistRL significantly boosts training efficiency with its distributed, asynchronous design, leveraging multiple machines and GPUs."}, {"title": "6.4 ON-DEVICE END-TO-END AGENT PERFORMANCE EVALUATION", "content": "We evaluate the end-to-end performance of agents trained with DistRL against other frameworks, including on-device control agents, using subsets of both the AitW training and test sets. The primary metric for evaluation is the success rate across General and Web Shopping tasks. To ensure a fair comparison, we allocate extensive fine-tuning time for DIGIRL in single-machine and synchronous multi-machine configurations, typically allowing 2 times the convergence time required by our asynchronous DistRL multi-machine setup. Despite this generous tuning period, baseline"}, {"title": "6.5 ABLATION STUDIES", "content": "To understand the contributions of different components in DistRL, we conduct ablation studies by systematically removing or altering key elements of the algorithm, such as the enhanced Retrace algorithm and Distributed Prioritized Experience Replay (DPER). The results, summarized in Figure 6.(b), demonstrate the significant impact of each component on the task success rate.\nDistributed Prioritized Experience Replay (DPER) is crucial for accelerating training convergence. Removing DPER results in an 8% decrease in the success rate, indicating that prioritizing trajectories with higher TD errors and smaller policy discrepancies enables faster and more efficient learning by focusing updates on the most informative experiences. With the entropy term, the pri-"}, {"title": null, "content": "oritization mechanism promotes exploration based on the evolving policy distribution, preventing stagnation during training.\nRetrace Algorithm is essential for maintaining training stability. Ablating the Retrace algorithm leads to a 6% drop in success rate and causes sharp decreases in performance during training. This instability arises because Retrace provides off-policy correction, ensuring stable updates even when the agent receives a large number of diverse trajectories.\nOverall, the ablation results confirm that both DPER and the Retrace algorithm are integral to the efficiency and robustness of DistRL."}, {"title": "7 CONCLUSION AND FUTURE WORK", "content": "In this paper, we introduce DistRL, an efficient distributed reinforcement learning framework tailored for mobile-based agents tasked with user instructions. Our primary contribution is the development of a robust and scalable pipeline that seamlessly bridges the gap between real-time interactions on mobile devices or emulators and distributed training infrastructures, ensuring efficient and adaptive learning.\nFor future work, we aim to extend the generalization capabilities of DistRL to a broader range of tasks, focusing on enhancing both the training pipeline and the underlying algorithmic architecture. Additionally, we envision evolving DistRL into a core backbone for integrating many more Multimodal Large Language Models (MLLMs), allowing for a wider range of applications and evaluations on diverse benchmarks."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 CASE STUDY", "content": "Figure 7 illustrates a type of common case where baseline methods always fails", "error": "DIGIRL mistakenly opens Google Photos instead of the Play Store. Since the two apps share similar icon features. After opening the wrong app, DIGIRL continues to operate within the incorrect environment, ultimately getting stuck in the settings menu of Google Photos. This reveals a significant limitation of offline-training-only agent in adapting to updated environments, especially when visual similarities between app icons lead to misclassification. AutoUI shares a similar issue where it struggles to correctly identify the target application. In this case, it opens Google Maps directly instead of navigating through the Play Store. Its lack of adaptability to new tasks or novel instructions results in failure.\nThe AppAgent with GPT-4V takes an alternate route by resorting to web searching, which diverges from the intended method of updating the app. Eventually, this leads to the agent becoming stuck within the Google Maps application itself, indicating that while GPT-4V was able to explore different avenues to achieve the goal, it did not follow the expected approach due to the lack of app-specific knowledge.\nWhile the DistRL, which was actively trained on the real-time newly-updated environment through"}]}