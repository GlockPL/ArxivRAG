{"title": "Tractable and Provably Efficient Distributional Reinforcement Learning with General Value Function Approximation", "authors": ["Taehyun Cho", "Seungyub Han", "Kyungjae Lee", "Seokhun Ju", "Dohyeong Kim", "Jungwoo Lee"], "abstract": "Distributional reinforcement learning improves performance by effectively capturing environmental stochasticity, but a comprehensive theoretical understanding of its effectiveness remains elusive. In this paper, we present a regret analysis for distributional reinforcement learning with general value function approximation in a finite episodic Markov decision process setting. We first introduce a key notion of Bellman unbiasedness for a tractable and exactly learnable update via statistical functional dynamic programming. Our theoretical results show that approximating the infinite-dimensional return distribution with a finite number of moment functionals is the only method to learn the statistical information unbiasedly, including nonlinear statistical functionals. Second, we propose a provably efficient algorithm, SF-LSVI, achieving a regret bound of $\\tilde{O}(d_EH^{3/2} \\sqrt{K})$ where H is the horizon, K is the number of episodes, and $d_E$ is the eluder dimension of a function class.", "sections": [{"title": "1 Introduction", "content": "Distributional reinforcement learning (DistRL) [7, 13, 30, 41] is an advanced approach to reinforcement learning (RL) that focuses on the entire probability distribution of returns rather than solely on the expected return. By considering the full distribution of returns, distRL provides deeper insight into the uncertainty of each action, such as the mode or median. This framework enables us to make safer and more effective decisions that account for various risks [14, 22, 29, 46], particularly in complex real-world situations, such as robotic manipulation [10], neural response [36], stratospheric balloon navigation [8], algorithm discovery [19], and several game benchmarks [6, 34].\nAlthough distRL has a solid theoretical basis and demonstrates effective empirical performance, its efficiency and tractability are still not fully understood. In practice, distributions contain an infinite amount of information, and we should resort to approximations using a finite number of parameters or statistical functionals, such as categorical [7] and quantile representations [16]. However, not all statistical functionals can be learned through the Bellman operator, as the meaning of statistical functionals is not typically preserved after updates. For example, the median is not preserved under the Bellman updates, as the median of a mixture of two distributions does not necessarily equate to the mixture of their medians. Hence, a distinct analysis is required to determine whether there exists a corresponding Bellman operator for each statistical functional that ensures commutativity. Rowland et al. [41] defined Bellman closedness as a property of statistical functionals that can be exactly learned by the existence of a corresponding Bellman operator."}, {"title": "2 Related Work", "content": "Distributional RL. In classical RL, the Bellman equation, which is based on expected returns, has a closed-form expression. However, it is questionable whether any statistical functionals of return distribution always have their corresponding closed-form expressions. Rowland et al. [41] introduced the notion of Bellman Closedness for collections of statistical functionals that can be updated in a closed form via Bellman update. They showed that the only Bellman-closed statistical functionals in the discounted setting are the moments $E_{z \\sim \\eta}[Z^k]$. More recently, Marthe et al. [35] proposed a general framework for distRL, where the agent plans to maximize their own utility functionals instead of expected return and formalized the property of Bellman Optimizability. They proved that the only $W_1$-continuous and linear Bellman Optimizable statistical functionals are shown to be exponential utilities $\\psi = \\log E_{z \\sim \\eta}[\\exp(\\lambda Z)]$ in the undiscounted setting.\nIn practice, C51 [7] and QR-DQN [16] are notable distributional RL algorithms where the convergence guarantees of sampled-based algorithms are proved [40, 42]. Dabney et al. [15] expanded the class of policies on arbitrary distortion risk measures by taking the based distribution non-uniformly and improve the sample efficiency from their implicit representation of the return distribution. Cho et al. [12] highlighted the drawbacks of optimistic exploration in distRL, introducing a randomized exploration method that perturbs the return distribution when the agent selects their next action.\nRL with General Value Function Approximation. Regret bounds have been studied for a long time in RL, across various domains such as bandit [1, 32, 43], tabular RL [3, 26, 28, 38, 39], and linear function approximation [27, 51, 52]. In recent years, deep RL has shown significant performance using deep neural networks as function approximators, and attempts have been made to analyze whether it is efficient in terms of general function approximation. Wang et al. [50] established a provably efficient RL algorithm with general value function approximation based on the eluder dimension $d_E$ [43] and achieves a regret upper bound of $\\tilde{O}(\\text{poly}(d_E H) \\sqrt{K})$. To circumvent the intractability from computing the upper confidence bound, Ishfaq et al. [25] injected the stochasticity on the training data and get the optimistic value function instead of upper confidence bound, enhancing computationally efficiency. Beyond risk-neutral setting, several prior works have shown regret bounds under risk-sensitive objectives (e.g., entropic risk [20, 33], CVaR [5]), which align with our approach in that they are built on a distribution framework. Liang and Luo [33] achieved the regret upper bound of $\\tilde{O}(\\exp(H)|S|^2|A|H^2 K)$ and the lower bound of $\\Omega(\\exp(H) \\sqrt{|S||A|HK})$ in tabular setting.\nDistRL with General Value Function Approximation. Recently, only few efforts have aimed to bridge the gap between two fields. Wang et al. [49] proposed a distributional RL algorithm, O-DISCO, which enjoys small-loss bound by using a log-likelihood objective. Similarly, Chen et al. [11] provided a risk-sensitive reinforcement learning framework with static lipschitz risk measure. While these studies analyze within a distributional framework, they do not address the intractability"}, {"title": "3 Preliminaries", "content": "Episodic MDP. We consider a episodic Markov decision process which is defined as a M = (S, A, H, P, r) characterized by state space S, action space A, horizon length H, transition kernels P = {$P_h$}$_{h \\in [H]}$, and rewardr = {$r_h$}$_{h \\in [H]}$ at steph $\\in$ [H]. The agent interacts with the environment across K episodes. For each k $\\in$ [K] andh $\\in$ [H], $H^k$ = ($s^k_1, a^k_1, ..., s^k_h, a^k_h, ..., s^k_H, a^k_H$) represents the history up to step h at episode k. We assume the reward is bounded by [0, 1] and the agent always transit to terminal state $s_{\\text{end}}$ at step H + 1 with $r_{H+1} = 0$.\nPolicy and Value Functions. A (deterministic) policy $\\pi$ is a collection of H functions {$\\pi_h: S \\rightarrow A$}$_1$. Given a policy $\\pi$, a step h $\\in$ [H], and a state-action pair (s,a) $\\in$ S $\\times$ A, the Q and V-function are defined as $Q_h^{\\pi}(s,a) (: S \\times A \\rightarrow \\mathbb{R}) := E_{\\pi} \\left[ \\sum_{h'=h}^H r_{h'}(s_{h'}, a_{h'}) | s_h = s, a_h = a, a_{h'}=\\pi_{h'}(s_{h'}) \\right]$ and $V_h^{\\pi}(s) (: S \\rightarrow \\mathbb{R}) := E_{\\pi} \\left[ \\sum_{h'=h}^H r_{h'}(s_{h'}, a_{h'}) | s_h = s \\right]$.\nRandom Variables and Distributions. For a sample space $\\Omega$, we extend the definition of the Q-function into a random variable and its distribution,\n$Z_h^{\\pi}(s, a) (: S \\times A \\times \\Omega \\rightarrow \\mathbb{R}) := \\sum_{h'=h}^H r_{h'}(s_{h'}, a_{h'}) | s_h = s, a_h = a, a_{h'}=\\pi_{h'}(s_{h'}),$", "mathcal{P}(\\mathbb{R}))": "text{law}(Z_h^{\\pi}(s,a))$.\nAnalogously, we extend the definition of V-function by introducing a bar notation.\n$\\bar{Z}_h^{\\pi}(s) (: S \\times \\Omega \\rightarrow \\mathbb{R}) := \\sum_{h'=h}^H r_{h'}(s_{h'}, a_{h'}) | s_h = s, a_{h'}=\\pi_{h'}(s_{h'}),"}, {"mathcal{P}(\\mathbb{R}))": "text{law}(\\bar{Z}_h^{\\pi}(s))$.\nNote that $Z_h^{\\pi}(s) = Z_h^{\\pi}(s,\\pi_h(s))$ and $\\eta_h^{\\pi}(s) = \\eta_h^{\\pi}(s,\\pi_h(s))$. We use $\\pi^*$ to denote an optimal policy and denote $V_h^*(s) = V_h^{\\pi^*}(s)$, $Q_h^*(s, a) = Q_h^{\\pi^*}(s, a)$, $\\eta_h^*(s, a) = \\eta_h^{\\pi^*}(s,a)$, and $\\eta_h^*(s) = \\eta_h^{\\pi^*}(s)$. For notational simplicity, we denote the expectation over transition, $[P_h V_{h+1}](s,a) = E_{s'\\sim P_h(\\cdot|s,a)} V_{h+1}(s')$, $[P_h Z_{h+1}](s,a) = E_{s'\\sim P_h(\\cdot|s,a)} Z_{h+1}(s')$, and $[P_h \\eta_{h+1}](s, a) = E_{s'\\sim P_h(\\cdot|s,a)} \\eta_{h+1}(s')$. 3 For brevity, we refer to $\\bar{\\eta}^{\\pi}$ simply as $\\eta$.\nIn the episodic MDP, the agent aims to learn the optimal policy through a fixed number of interactions with the environment across a number of episodes. At the beginning of each episode $k(\\in [K])$, the agent starts at the initial state $s_1^k$ and choose a policy $\\pi^k$. In step $h(\\in [H])$, the agent observes $s_h^k(\\in S)$, takes an action $a_h^k(\\in A) \\sim \\pi_h^k(\\cdot|s_h^k)$, receives a reward $r_h(s_h^k, a_h^k)$, and the environment transits to the next state $s_{h+1}^k \\sim P_h(s_{h+1}^k|s_h^k, a_h^k)$. Finally, we measure the suboptimality of an agent by its regret, which is the accumulated difference between the ground truth optimal and the return received from the interaction. The regret after K episodes is defined as $Reg(K) = \\sum_{k=1}^K V_1^*(s_1^k) - V_1^{\\pi^k}(s_1^k)$.\nDistributional Bellman Optimality Equation. Recall that $\\eta^*$ satisfies the distributional Bellman optimality equation:\n$\\eta_h^*(s, a) = (T_h \\eta_{h+1}^*)(s, a) = E_{s'\\sim P_h(\\cdot|s,a), a'\\sim \\pi^* (\\cdot|s')} [(\\mathcal{B} r_h) \\# \\eta_{h+1}^*(s', a')]$ = $(B r_h)\\#[P_h \\eta_{h+1}^*](s, a)$\nwhere $\\mathcal{B}: \\mathbb{R} \\rightarrow \\mathbb{R}$ is defined by $\\mathcal{B}r(x) = r + x$, and $g\\#\\eta \\in \\mathcal{P}(\\mathbb{R})$ is the pushforward of the distribution $\\eta$ through g, i.e., $g\\#\\eta(A) = \\eta(g^{-1}(A))$ for any Borel set A $\\subseteq$ R.\nNote that $E_{s'\\sim P_h(\\cdot|s,a)} \\eta_{h+1}^*(s')$ is a mixture distribution."}, {"title": "4 Statistical Functionals in Distributional RL", "content": "4.1 Bellman Unbiasedness\nWe revisit the definitions of statistical functionals and illustrate the property of Bellman closedness described in Bellemare et al. [9]. Then we introduce another pivotal property, Bellman unbiasedness, required for learning in terms of statistical functionals, rather than approximated distribution.\nDefinition 4.1 (Statistical functionals, Sketch; [9]). A statistical functional is a mapping from a probability distribution to a real value $\\psi: \\mathcal{P}(\\mathbb{R}) \\rightarrow \\mathbb{R}$. A sketch is a vector-valued function $\\Psi_{1:N}: \\mathcal{P}(\\mathbb{R}) \\rightarrow \\mathbb{R}^N$ specified by an N-tuple where each component is a statistical functional,\n$\\Psi_{1:N}(\\cdot) = (\\psi_1(\\cdot),..., \\psi_N(\\cdot))$.\nWe denote the domain of sketch as $\\mathcal{P}_{\\Psi_{1:N}} (\\mathbb{R})$ and its image as $\\mathcal{I}_{\\Psi_{1:N}} = {\\psi(\\eta) : \\eta \\in \\mathcal{P}_{\\Psi_{1:N}}(\\mathbb{R})}$. We further extend to state return distribution functions $\\psi_{1:N} (\\eta) = (\\psi_{1:N} (\\eta(s)) : s \\in S)$.\nDefinition 4.2 (Bellman closedness; [41]). A sketch $\\Psi_{1:N}$ is Bellman closed if there exists an operator $\\mathcal{T}_{\\Psi_{1:N}}: \\mathcal{I}_{\\Psi_{1:N}} \\rightarrow \\mathcal{I}_{\\Psi_{1:N}}$ such that\n$\\psi_{1:N}(\\mathcal{T} \\eta) = \\mathcal{T}_{\\Psi_{1:N}} \\psi_{1:N} (\\eta)$ for all $\\eta \\in \\mathcal{P}(\\mathbb{R})^S$\nwhich is closed under the distributional Bellman operator $\\mathcal{T}: \\mathcal{P}(\\mathbb{R})^S \\rightarrow \\mathcal{P}(\\mathbb{R})^S$.\nBellman closedness is the property that a sketch are exactly learnable when updates are performed from the infinite-dimensional distribution space to the finite-dimensional embedding space. Notably, Rowland et al. [41] showed that the only finite linear statistical functionals that are Bellman closed are given by the collections of statistical functionals where its linear span is equal to the set of exponential-polynomial functionals where $\\varphi_o$ is the constant functional equal to 1.\nRemark 4.3. Rowland et al. [41] discussed only linear statistical functionals when defining a sketch, leaving open questions about the Bellman closedness of nonlinear statistical functionals such as variance, central moments, or quantiles. However, it is noteworthy that nonlinear statistical functionals, such as maximum or minimum, can also be Bellman closed. In their proof, there might be an ambiguity regarding the assumption that the corresponding sketch Bellman operator for quantile is linear. In this paper, we show the non-existence of sketch Bellman operator for quantile and discuss in detail in Appendix B.1."}, {"title": "5 Algorithms", "content": "In this section, we propose SF-LSVI for distributional RL framework with general value function approximation. Leveraging the result from Theorem 4.7, we introduce a moment least square regression. This allows us to capture a finite set of moment information from the distribution, which can be unbiasedly estimated, thereby leading to the truncated moment problem. Unlike previous work [11, 49] that estimates in infinite-dimensional distribution spaces, our method enables tractable distribution estimation in finite-dimensional embedding spaces without approximation error."}, {"title": "6 Theoretical Analysis", "content": "In this section, we provide the theoretical guarantees for SF-LSVI under assumption 4.8. We apply proof techniques from Wang et al. [50] and extend the result to a statistical functional lens. First, we generalize the concept of eluder dimension [43] to the vector-valued function, which has been widely used in RL literatures [4, 27, 50] to measure the complexity of learning with the function approximators.\nDefinition 6.1 ($\\epsilon$-dependent, $\\epsilon$-independent, Eluder dimension for vector-valued function). Let $\\epsilon \\geq 0$ and $Z = {(s_i, a_i)}_{i=1}^t \\subseteq S \\times A$ be a sequence of state-action pairs."}]}