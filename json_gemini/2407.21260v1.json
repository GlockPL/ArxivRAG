{"title": "Tractable and Provably Efficient Distributional Reinforcement Learning with General Value Function Approximation", "authors": ["Taehyun Cho", "Seungyub Han", "Kyungjae Lee", "Seokhun Ju", "Dohyeong Kim", "Jungwoo Lee"], "abstract": "Distributional reinforcement learning improves performance by effectively cap- turing environmental stochasticity, but a comprehensive theoretical understanding of its effectiveness remains elusive. In this paper, we present a regret analysis for distributional reinforcement learning with general value function approximation in a finite episodic Markov decision process setting. We first introduce a key notion of Bellman unbiasedness for a tractable and exactly learnable update via statistical functional dynamic programming. Our theoretical results show that approximating the infinite-dimensional return distribution with a finite number of moment func- tionals is the only method to learn the statistical information unbiasedly, including nonlinear statistical functionals. Second, we propose a provably efficient algorithm, SF-LSVI, achieving a regret bound of \u00d5(dEH\u00b3 \u221aK) where H is the horizon, K is the number of episodes, and de is the eluder dimension of a function class.", "sections": [{"title": "1 Introduction", "content": "Distributional reinforcement learning (DistRL) [7, 13, 30, 41] is an advanced approach to reinforce- ment learning (RL) that focuses on the entire probability distribution of returns rather than solely on the expected return. By considering the full distribution of returns, distRL provides deeper insight into the uncertainty of each action, such as the mode or median. This framework enables us to make safer and more effective decisions that account for various risks [14, 22, 29, 46], particularly in complex real-world situations, such as robotic manipulation [10], neural response [36], stratospheric balloon navigation [8], algorithm discovery [19], and several game benchmarks [6, 34]. Although distRL has a solid theoretical basis and demonstrates effective empirical performance, its efficiency and tractability are still not fully understood. In practice, distributions contain an infinite amount of information, and we should resort to approximations using a finite number of parameters or statistical functionals, such as categorical [7] and quantile representations [16]. However, not all statistical functionals can be learned through the Bellman operator, as the meaning of statistical functionals is not typically preserved after updates. For example, the median is not preserved under the Bellman updates, as the median of a mixture of two distributions does not necessarily equate to the mixture of their medians. Hence, a distinct analysis is required to determine whether there exists a corresponding Bellman operator for each statistical functional that ensures commutativity. Rowland et al. [41] defined Bellman closedness as a property of statistical functionals that can be exactly learned by the existence of a corresponding Bellman operator."}, {"title": "2 Related Work", "content": "Distributional RL. In classical RL, the Bellman equation, which is based on expected returns, has a closed-form expression. However, it is questionable whether any statistical functionals of return distribution always have their corresponding closed-form expressions. Rowland et al. [41] introduced the notion of Bellman Closedness for collections of statistical functionals that can be updated in a closed form via Bellman update. They showed that the only Bellman-closed statistical functionals in the discounted setting are the moments Ez\u223cn[Zk]. More recently, Marthe et al. [35] proposed a general framework for distRL, where the agent plans to maximize their own utility functionals instead of expected return and formalized the property of Bellman Optimizability. They proved that the only W\u2081-continuous and linear Bellman Optimizable statistical functionals are shown to be exponential utilities = log Ez~n[exp(AZ)] in the undiscounted setting. In practice, C51 [7] and QR-DQN [16] are notable distributional RL algorithms where the convergence guarantees of sampled-based algorithms are proved [40, 42]. Dabney et al. [15] expanded the class of policies on arbitrary distortion risk measures by taking the based distribution non-uniformly and improve the sample efficiency from their implicit representation of the return distribution. Cho et al. [12] highlighted the drawbacks of optimistic exploration in distRL, introducing a randomized exploration method that perturbs the return distribution when the agent selects their next action.\nRL with General Value Function Approximation. Regret bounds have been studied for a long time in RL, across various domains such as bandit [1, 32, 43], tabular RL [3, 26, 28, 38, 39], and linear function approximation [27, 51, 52]. In recent years, deep RL has shown significant performance using deep neural networks as function approximators, and attempts have been made to analyze whether it is efficient in terms of general function approximation. Wang et al. [50] established a provably efficient RL algorithm with general value function approximation based on the eluder dimension de [43] and achieves a regret upper bound of \u00d5(poly(dEH)\u221aK). To circumvent the intractability from computing the upper confidence bound, Ishfaq et al. [25] injected the stochasticity on the training data and get the optimistic value function instead of upper confidence bound, enhancing computationally efficiency. Beyond risk-neutral setting, several prior works have shown regret bounds under risk-sensitive objectives (e.g., entropic risk [20, 33], CVaR [5]), which align with our approach in that they are built on a distribution framework. Liang and Luo [33] achieved the regret upper bound of \u00d5(exp(H)|S|2|A|H2K) and the lower bound of \u03a9(exp(H) \u221a|S||A|HK) in tabular setting.\nDistRL with General Value Function Approximation. Recently, only few efforts have aimed to bridge the gap between two fields. Wang et al. [49] proposed a distributional RL algorithm, O-DISCO, which enjoys small-loss bound by using a log-likelihood objective. Similarly, Chen et al. [11] provided a risk-sensitive reinforcement learning framework with static lipschitz risk measure. While these studies analyze within a distributional framework, they do not address the intractability"}, {"title": "3 Preliminaries", "content": "Episodic MDP. We consider a episodic Markov decision process which is defined as a M = (S, A, H, P, r) characterized by state space S, action space A, horizon length H, transition kernels P = {Ph}h\u2208[H], and rewardr = {rh}h\u2208[H] at steph \u2208 [H]. The agent interacts with the environment across K episodes. For each k \u2208 [K] andh \u2208 [H], H = (s\u0131, a\u00a6, ..., s\u0127, a, ..., sh, a) represents the history up to step h at episode k. We assume the reward is bounded by [0, 1] and the agent always transit to terminal state send at step H + 1 with rh+1 = 0.\nPolicy and Value Functions. A (deterministic) policy is a collection of H functions {\u03c0\u03b7: S \u2192 A}1. Given a policy \u03c0, a step h \u2208 [H], and a state-action pair (s,a) \u2208 S \u00d7 A, the Q and V-function are defined as Q(s,a)(: S \u00d7 A \u2192 R) := \u0395\u03c0 [\u2211h'=h rh'(Sh', ah') | Sh = S, ah = a] and V\u2081(s)(: S \u2192 R) := E [\u2211h'=h rh'(sh', ah') | Sh = s] .\nRandom Variables and Distributions. For a sample space \u03a9, we extend the definition of the Q-function into a random variable and its distribution,\nZ(s, a)(: S \u00d7 A \u00d7 \u03a9 \u2192 R) := \u2211h'=h rh'(sh', ah') | Sh = s, ah = a, \u03b1\u03c0' = \u03c0\u03b7' (Sh'),\n\u03b7 (s, a)(: S \u00d7 A \u2192 P(R)) := law(Z(s,a)).\nAnalogously, we extend the definition of V-function by introducing a bar notation.\nZ(s): S \u00d7 n \u2192 R) := \u2211h'=h rh'(sh', ah') | Sh = s, \u03b1\u03b7' = \u03c0\u03b7' (Sh'),\n(s)(: S \u2192 P(R)) := law(\u017d(s)).\nNote that Z(s) = Z(\u03b4,\u03c0(s)) and (s) = n(s,\u03c0(s)). We use \u03c0* to denote an op- timal policy and denote Vt(s) = V(s), Q(s, a) = Q (s, a), (s, a) = *(s,a), and(s) = *(s). For notational simplicity, we denote the expectation over transi- tion, [PhVh+1](s,a) = Es'~Ph(:\\s,a) Vn+1(s'), [PhZn+1](s,a) = Es'~Ph(:\\s,a) Zn+1(s'), and [Phn+1](s, a) = Es'~Ph(:\\s,a)+1(s'). For brevity, we refer to \u1fc6\u201d simply as \u1fc6.\nIn the episodic MDP, the agent aims to learn the optimal policy through a fixed number of interactions with the environment across a number of episodes. At the beginning of each episode k(\u2208 [K]), the agent starts at the initial state sf and choose a policy \u03c0k. In step h(\u2208 [H]), the agent observes sh(\u2208 S), takes an action a(\u2208 A) ~ \u33a1(\uc774\u33a2), receives a reward rh(sh, ah), and the environment transits to the next state sh sh+1~Ph(sh, a). Finally, we measure the suboptimality of an agent by its regret, which is the accumulated difference between the ground truth optimal and the return received from the interaction. The regret after K episodes is defined as Reg(K) = \u2211=1V (st) \u2013 V\u2081**(st).\nDistributional Bellman Optimality Equation. Recall that n satisfies the distributional Bellman optimality equation:\nn(s, a) = (Thn+1)(s, a) = Es'~Ph(:\\s,a), a'~\u3160 (-\\s') [(Brn) #Nh+1(s', a')] = (Brn)#[Phn+1](s, a)\nwhere B: R \u2192 R is defined by Br(x) = r + x, and g#n \u2208 P(R) is the pushforward of the distribution \u03b7 through g, i.e., g#n(A) = n(g\u00af\u00b9(A)) for any Borel set A \u2286 R.\nNote that Es'~Ph(\u00b71s,a)+1 (s') is a mixture distribution."}, {"title": "4 Statistical Functionals in Distributional RL", "content": "We revisit the definitions of statistical functionals and illustrate the property of Bellman closedness described in Bellemare et al. [9]. Then we introduce another pivotal property, Bellman unbiasedness, required for learning in terms of statistical functionals, rather than approximated distribution.\nDefinition 4.1 (Statistical functionals, Sketch; [9]). A statistical functional is a mapping from a probability distribution to a real value 4 : P(R) \u2192 R. A sketch is a vector-valued function V1:N: P(R) \u2192 RN specified by an N-tuple where each component is a statistical functional,\n\u03a81:N(\u00b7) = (\u03c81(\u00b7),\u2026\u2026\u2026, \u03c8\u2116(\u00b7)).\nWe denote the domain of sketch as P41:N (R) and its image as I41:N = {\u03c8(\u1fc6) : \u1f22 \u0395\u03a1\u03a8\u0399.\u039d(R)}. We further extend to state return distribution functions \u03c81:N (7) = (\u03c81:N (\u1fc6(s)) : s \u2208 S).\nDefinition 4.2 (Bellman closedness; [41]). A sketch 41:N is Bellman closed if there exists an operator\n\u03a4\u03c8\u03b9:\u039d: \u0399\u03b4 1$1:N \u2192 IS1 \u03a81:\u039d\nsuch that\n\u03c81:\u039d(\u03a4\u1fc6) = \u03a4\u03c8\u03b9:\u039d \u03a81:N (\u1fc6) for all \u1fc6 \u2208 P(R)S\nwhich is closed under the distributional Bellman operator T : P(R)S \u2192 P(R)S.\nBellman closedness is the property that a sketch are exactly learnable when updates are performed from the infinite-dimensional distribution space to the finite-dimensional embedding space. Notably, Rowland et al. [41] showed that the only finite linear statistical functionals that are Bellman closed are given by the collections of statistical functionals where its linear span is equal to the set of exponential-polynomial functionals where o is the constant functional equal to 1.\nRemark 4.3. Rowland et al. [41] discussed only linear statistical functionals when defining a sketch, leaving open questions about the Bellman closedness of nonlinear statistical functionals such as variance, central moments, or quantiles. However, it is noteworthy that nonlinear statistical functionals, such as maximum or minimum, can also be Bellman closed. In their proof, there might be an ambiguity regarding the assumption that the corresponding sketch Bellman operator for quantile is linear. In this paper, we show the non-existence of sketch Bellman operator for quantile and discuss in detail in Appendix B.1."}, {"title": "4.1 Bellman Unbiasedness", "content": "We revisit the definitions of statistical functionals and illustrate the property of Bellman closedness described in Bellemare et al. [9]. Then we introduce another pivotal property, Bellman unbiasedness, required for learning in terms of statistical functionals, rather than approximated distribution.\nDefinition 4.5 (Bellman unbiasedness). A sketch 4 is Bellman unbiased if there exists an integer k and a corresponding vector-valued estimator \u03c6\u03c8 = \u03c6\u03c8(\u03c8(x1),\u2026\u2026,\u03c8(xk)) : (I)k \u2192 I where the sketch of expected distribution (Br)#Es'\u223cP(\u00b7|s,a)[\u1fc6(s')] can be unbiasedly estimated by \u0444\u2193 using the k samples from the sketch of the sample distribution (Br)#\u1fc6(s'), i.e.,\n\u03c8  ((Br)#Es'((Br)#\u1fc6(s1)),\u2026\u2026\u2026, \u03c8 ((Br)#\u1fc6(sk))s\u2032~P(\u00b7|s,a)[\u1fc6(s\u2032)]\n= (Es'~P(\u00b7|s,a)[\u1fc6(s\u2032)])\n\u03a4\u03c8\u03c8(\u03b7)\n\u03c8\\[\u03c8((Br)#\u1fc6(s1)),\u2026\u2026\u2026, \u03c8 ((Br)#\u1fc6(sk))] = Es'~P(\u00b7|s,a)[\u03c8((Br)#\u1fc6(s\u2032))]\nBellman unbiasedness is another natural definition, similar to Bellman closedness, which takes into account a finite number of samples for the transition. As a simple example, median functional is not Bellman unbiased as the sample median is not an unbiased estimate of the median functional. In general, there is no unbiased estimator for the median, implying that the median functional cannot be unbiasedly estimated within an embedding space represented by a finite number of statistical functionals. On the other hand, variance functional is a nonlinear statistical functional but can be unbiasedly estimated. Then, the following question naturally arises;\n\"Which sketches are unbiasedly estimatable under the sketch-based Bellman update?\"\nThe following lemma answers this question.\nLemma 4.6. Let F\u0127 be a CDF of the probability distribution \u1f22 \u2208 PV(R)S. Then a sketch is Bellman unbiased if and only if the sketch is homogeneous over P\u2084(R)S of degree k, i.e., there exists some vector-valued function h = h(x1,\uff65\uff65\uff65,xk) : Xk \u2192 RN such that\n\u03c8(n) = \u222b\u2026\u2026\u2026\u222b h(x1,..., xk)dFy(x1)...dFy(xk).\nLemma 4.6 states that in statistical functional dynamic programming, the unbiasedly estimatable embedding of a distribution can only be structured in the form of functions that are homogeneous of finite degree. (e.g., variance functional is a homogeneous of degree 2 [23]). Taking this concept further and combining it with the results on Bellman closedness, we prove that even when including a nonlinear statistical functional, the only sketch that can be unbiasedly estimated in a finite-dimensional embedding space is the moment functional."}, {"title": "4.2 Statistical Functional Bellman Completeness", "content": "We consider distributional reinforcement learning with value function approximation. For successful TD learning, reinforcement learning with function approximation commonly requires the assumption, Bellman Completeness, that after applying Bellman operator, the output lies in the function class F [4, 25, 50]. On the other hand, our approach receives a tuple of function class FN \u2286 {f : S \u00d7 A \u2192 RN} as input to represent N-moment of distribution. Building on this, we assume that for any \u1fc6 : S \u2192 P([0, H]), the sketch of target function that results from applying sketch Bellman operator lies in the function class FN In the seminal works, Wang et al. [49] and Chen et al. [11] assumed that the function class HC {\u03b7 : S \u00d7 A \u2192 P([0, H])} follows the distributional Bellman Completeness assumption, i.e., if \u03b7 \u2208 H for all \u03c0, h\u2208 [H], then \u03a4\u03b7 \u2208 H. However, since the distributional Bellman operator mixes distributions for the next state transition, it implies that a function class H must be closed under mixture. Due to this closedness under mixture, both previous studies assumed a discretized reward MDP, which requires the prior knowledge that outcomes of return distribution are additively closed. In general, representing a distribution with a fixed number of parameters inevitably increases the required number of parameters due to the emergence of a mixture distribution. Consequently, trying to represent it with a limited number of parameters leads to approximating the learning outcome, thereby causing model misspecification which generally introduces an additional linear regret. Precisely, a regret lower bound with model misspecification error ( is known as \u03a9(CK) in a linear bandit setting [52]. Therefore, assuming Bellman Completeness on the distribution space is invalid as it significantly constrains the MDP structure or causes linear regret.\nTo circumvent this issue, we revisit the assumption of distributional Bellman completeness through the statistical functional lens. We propose a new framework that matches a finite number of all statistical functionals to the target function, rather than the entire distribution itself.\nAssumption 4.8 (Statistical Functional Bellman Completeness). For any distribution \u1fc6 : S \u2192 P([0, H]) and h \u2208 [H], there exists f\u1fc6 \u2208 FN which satisfies\nf\u1fc6(s, a) = \u03c81:N ((Brn)#[Ph\u1fc6](s,a)) \u2200(s, a) \u2208 S \u00d7 A\nAssumption 4.9 (Model Misspecification). There exists a set of function FN and a real number ( > 0, such that for any \u1fc6 : S \u2192 P([0, H]) and h \u2208 [H], there exists f\u0127 \u2208 FN which satisfies\nmax || f(s, a) \u2013 41:N ((Brn)#[Ph\u012b](s,a)) || \u2264 \u2200(s, a) \u2208 S \u00d7 A.(s,a)ESXA\nWe call the misspecification error."}, {"title": "5 Algorithms", "content": "In this section, we propose SF-LSVI for distributional RL framework with general value function approximation. Leveraging the result from Theorem 4.7, we introduce a moment least square regression. This allows us to capture a finite set of moment information from the distribution, which can be unbiasedly estimated, thereby leading to the truncated moment problem. Unlike previous work [11, 49] that estimates in infinite-dimensional distribution spaces, our method enables tractable distribution estimation in finite-dimensional embedding spaces without approximation error."}, {"title": "6 Theoretical Analysis", "content": "In this section, we provide the theoretical guarantees for SF-LSVI under assumption 4.8. We apply proof techniques from Wang et al. [50] and extend the result to a statistical functional lens. First, we generalize the concept of eluder dimension [43] to the vector-valued function, which has been widely used in RL literatures [4, 27, 50] to measure the complexity of learning with the function approximators.\nDefinition 6.1 (e-dependent, e-independent, Eluder dimension for vector-valued function). Let \u0454 \u2265 0 and Z = {(si, ai)}=1 \u2286 S \u00d7 A be a sequence of state-action pairs."}, {"title": "7 Conclusions", "content": "We describe the sources of approximation error inherent in distribution-based updates and introduce a pivotal concept of Bellman unbiasedness, which enables to exactly learn the information of distribu- tion. We also present a provably efficient distRL algorithm, SF-LSVI, with general value function approximation. Notably, our algorithm achieves a near-optimal regret bound of \u00d5(deH\u00ba \u221a\u221aK), matching the tightest upper bound achieved by non-distributional framework [24, 53]. One interesting future direction would be to reformulate the definition of regret as discrepencies in moments rather than the expected return, and to show the sample-efficiency of distRL. We hope that our work sheds some light on future research in analyzing the provable efficiency of distRL."}, {"title": "A Notation", "content": ""}, {"title": "B Related Work and Discussion", "content": ""}, {"title": "B.1 Technical Clarifications on Linearity Assumption in Existing Results", "content": "Bellman Closedness and Linearity. Rowland et al. [41] proved that quantile functional is not Bellman closed by providing a specific counterexample. However, their discussion based on coun- terexamples can be generalized as it assumes that the sketch Bellman operator for the quantile functional needs to be linear.\nThey consider an discounted MDP with initial state so with single action a, which transits to one of two terminal states 81, 82 with equal probability. Letting no reward at state 80, Unif([0, 1]) at state 81, and Unif ([1/K,1+1/K]) at state 82, the return distribution at state so is computed as mixture Unif([0,1]) + Unif([\u03b3/\u039a, \u03b3+ \u03b3/K]). Then the 2-quantile at state so is . They proposed a counterexample where each quantile distribution of state 81, 82 is represented as =1 824-1 and 1 2 2K \u03a3\u03ba\u03b9 \u03b4(2k+1) +1 respectively, the 2k-quantile of state so is \u03c842K (2\u039a \u03a3\u039a-18(2 k=1) + \u03b4(2k+1)) = 3. However, this example does not consider that the mixture of quantiles is not a quantile of the mixture distribution (i.e., \u03c8\u2084(\u03bb\u03b7\u2081 + (1 \u2212 1)\u03b72) \u2260 \u03bb\u03c8q(71) + (1 \u2212 1)\u03c8q(72)), due to the nonlinearity of the quantile functional. Therefore, this does not present a valid counterexample to prove that quantile functionals are not Bellman closed.\nBellman Optimizability and Linearity. Marthe et al. [35] proposed the notion of Bellman optimiz- able statistical functional which redefine the Bellman update by planning with respect to statistical functionals rather than expected returns. They proved that W\u2081-continuous Bellman Optimizable statistical functionals are characterized by exponential utilities = log Ez\u223cn[exp(AZ)]. However, their proof requires some technical clarification regarding the assumption that such statistical functionals are linear.\nTo illustrate, they define a statistical functional f and consider two probability distributions 7\u2081 = (\u03b4\u03bf + \u03b4\u03b7) and 72 = 8(h) where (h) = f-1 ((f(0) + f(h))). Using the translation property, they lead \u03c8f(71) = $f(72) to (f(x) + f(x + h)) = f(x + $(h)) for all x \u2208 R. However, this equality f ((x+8x+h)) = (f(x)+ (f(x+h)) holds only if &f is linear, which is not necessarily a valid assumption for all statistical functionals."}, {"title": "B.2 Existence of Nonlinear Bellman Closed Sketch.", "content": "The previous two examples may not have considered the possibility that the sketch Bellman operator might not necessarily be linear. However, some statistical functionals are Bellman-closed even if they are nonlinear, so it is open question whether there is a nonlinear sketch Bellman operator that makes the quantile functional Bellman-closed. In this section, we present examples of maximum and minimum functionals that are Bellman-closed, despite being nonlinear.\nIn a nutshell, consider the maximum of return distribution at state 81, 82 is y, y + \u03b3/K respectively. Beyond linearity, the maximum of return distribution at state so can be computed by taking the maximum of these values;\nmax(max((81)), max(\u1fc6(82))) = max(\u03b3, \u03b3 + \u03b3/K) = y + \u03b3/\u039a\nwhich produces the desired result. This implies the existence of a nonlinear sketch that is Bellman closed. More precisely, by defining maxs\u2032~P(\u00b7|s,a) and mins\u2032~P(\u00b7|s,a) as the maximum and minimum of the sampled sketch ((Br)#7(s')) with the distribution P(. s, a), we can derive the sketch Bellman operator for maximum and minimum functionals as follows;\n\u2022 Tmax (max(7(s))) = maxs\u2032~P(-\\s,a) (max((Br)#7(s')))\n\u2022 Tamin (min ((8))) = ming'~P(-\\s,a) (min ((Br)#\u1fc6(s')))."}, {"title": "B.3 Non-existence of sketch Bellman operator for quantile functional", "content": "In this section, we prove that quantile functional cannot be Bellman closed under any additional sketch. First we introduce the definition of mixture-consistent, which is the property that the sketch of a mixture can be computed using only the sketch of the distribution of each component.\nDefinition B.1 (mixture-consistent). A sketch \u03c8 is mixture-consistent if for any A \u2208 [0, 1] and any distributions \u03b71, N2 \u2208 P\u2084(R), there exists a corresponding function h\u0173, such that\n\u03c8(\u03bb\u03b71 + (1 \u2212 1)\u03b72) = h\u03c8 (\u03c8(71), \u03c8(72), \u03bb).\nNext, we will provide some examples of determining whether a sketch is mixture-consistent or not.\nExample 1. Every moment or exponential polynomial functional is mixture-consistent.\nProof. For any n \u2208 [N] and \u03bb\u03b5C,\n\u0395\u0396~\u03bb\u03b71+(1-1)72 [Z\" exp(AZ)] = \u03bb\u0395\u0396\u223c71 [Z\" exp(AZ)] + (1 \u2212 1)Ez\u223c72 [Z\" exp(AZ)].\nExample 2. Variance functional is not mixture-consistent.\nProof. Let X = and Z, Y be the random variables where Z ~ 8\u03bf + 82 and Y ~ \u03ba + \u03b4\u03ba+2. Then, Var(Z) = Var(Y) = 1. While RHS is constant for any k, LHS is not a constant for any k, i.e.,\nVarx~(80+2)+(x+k+2) (X) = (k\u00b2 + 5).\nWhile variance functional is not mixture consistent by itself, it can be mixture consistent with another statistical functional, the mean.\nExample 3. Variance functional is mixture-consistent under mean functional.\nProof. Notice that mean functional is mixture-consistent. We need to show that variance functional is mixture-consistent under mean functional.\nVarz~\u03bb\u03b71+(1-1)\u03b7\u03c2 [2]\n= \u0395\u0396~\u03bb\u03b71+(1-1)\u03b72 [22] \u2013 (\u0395\u0396~\u03bb\u03b71+(1-1)72 [Z])2\n= \u03bb\u0395\u0396~ [\u0396\u00b2] + (1 \u2212 1)Ez~72 [Z\u00b2] \u2013 (XEz~n\u2081 [Z] + (1 \u2212 1)Ez~72 [Z])2\n= \u03bb(Varz~71 [Z] + (Ez~n\u2081 [Z])\u00b2) + (1 \u2212 1)(Varz~72 [Z] + (Ez~72 [Z])2) - (\u03bb\u0395\u0396~71 [Z] + (1 \u2212 \u03bb)Ez~72 [Z])2.\nThis means that to determine whether it is mixture-consistent or not, we should check it on a per-sketch basis, rather than on a per-statistical functional basis.\nExample 4. Maximum and minimum functional are both mixture-consistent.\nProof.\nmax [Z] = max(max [Z], max [Z])\u0396\u03c5\u03bb\u03b71+(1-1)\u03b7\u03c2 \u0396~71 \u0396~72\nand\nmin [Z] = min(min [Z], min [Z])\u0396\u03c5\u03bb\u03b71+(1-1)\u03b7\u03c2 \u0396~71 \u0396~72"}, {"title": "C Proof", "content": "Theorem (4.4). Quantile functional cannot be Bellman closed under any additional sketch.\nProof. See Lemma B.2 and Lemma B.3.\nLemma (4.6). Let F\u0127 be a CDF of the probability distribution \u1fc6 \u2208 P(R)S. Then a sketch is Bellman unbiased if and only if the sketch is a homogeneous of degree k, i.e., there exists some vector-valued function h = h(x1,\u2026, xk) : Xk \u2192 RN such that\n\u03c8(\u03b7) = \u222b\u2026\u2026\u2026\u222b h(x1,..., xk)dF(x1)...dF(x).\nProof. (\u21d2) Consider an two-stage MDP with a single action a, and an initial state so which transits to one of terminal state {$1,\u2026\u2026,SK} with transition kernel P(\u00b7|so, a). Assume that the reward r(so) = 0. Then \u1fc6(so) = \u03a3\u03ba=1P(sk)dr(sk). Note that s\u2081,..., s are independent and identically distributed random variable in distribution P(\u00b7|s, a).\nEs~P(-150,a) [\u03a6\u03c8 (\u03c8((Br)#\u00f1(s1)),\u2026\u2026, \u03c8 ((Br)#7(sk)))] = V1:N (B+)#Es'~P(-\\so,a) [7(s')]) \u21d2 Es~P(:180,a) [\u03a6 ((dr(s)),\u00b7\u00b7\u00b7, (dr(s)))] = (Es'~P(:\\80,a) [fr(s')]) Es~P(\\50,a) [(g(81),\u2026\u2026,9(sk))] = \u03c8((so)) \u21d2\n\u222b\u2026\u222b h(s\u00ed,\u2026\u2026,sk)dF(81)\u2026\u2026dF(sk) = ((so)).\n(\u2190) ((Br)#Es'~P(-\\s,a) [\u00f1(s')])\n=/\u2026/ h(x1,...,xk)dF(Br+)#Es\u2032~P(-\\8,4) [\u00f1(8')] (X1), ..., dF(Br+)#Es\u2032~P(-\\s,a) [\u1fc6(s')](xk)\n=/\u2026/ h(x1+r,...,xk+r)d(Es~P(\\s,a) F\u00f1(s') (x1)), ..., d (Es~P(\\s,a) Fi(s') (Tk))\n= Es'~P(./s,a)\n[/\u2026/ h(x1+r,...,xk + r)dFn(s') (x1)...dF(s') (xk)] = Es'~P(\\s,a) [((Br)#[\u00f1(s')])] Theorem (4.7). The only finite statistical functionals that are Bellman unbiased and closed are given by the collections of \u03c81,..., \u03c8N where its linear span {2=0 AnYn|an \u2208 R, \u2200N} is equal to the set of exponential polynomial functionals {\u03b7 \u2192 Ez\u223cn[Z' exp (AZ)]| 1 = 0, 1, . . ., L, \u00c0 \u2208 R}, where 4o is the constant functional equal to 1. In discount setting, it is equal to the linear span of the set of moment functionals {\u03b7 \u2192 Ez~n[Z\u00b9\u00b2]| l = 0, 1, . . ., L} for some L < N.\nProof. Our proof is mainly based on the proof techniques of Rowland et al. [41] and we describe in an extended form. Since their proof also considers the discounted setting, we will define Br,y(x) = r + \u03b3\u00e6 for discount factor \u03b3 \u2208 [0, 1). By assumption of Bellman closedness, \u03c8\u03b7 ((Br,r)#\u1fc6(s')) will"}]}