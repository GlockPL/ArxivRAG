{"title": "On Vanishing Gradients, Over-Smoothing, and Over-Squashing in GNNs: Bridging Recurrent and Graph Learning", "authors": ["\u00c1lvaro Arroyo", "Alessio Gravina", "Benjamin Gutteridge", "Federico Barbero", "Claudio Gallicchio", "Xiaowen Dong", "Michael Bronstein", "Pierre Vandergheynst"], "abstract": "Graph Neural Networks (GNNs) are models that leverage the graph structure to transmit information between nodes, typically through the message-passing operation. While widely successful, this approach is well-known to suffer from the over-smoothing and over-squashing phenomena, which result in representational collapse as the number of layers increases and insensitivity to the information contained at distant and poorly connected nodes, respectively. In this paper, we present a unified view of these problems through the lens of vanishing gradients, using ideas from linear control theory for our analysis. We propose an interpretation of GNNs as recurrent models and empirically demonstrate that a simple state-space formulation of a GNN effectively alleviates over-smoothing and over-squashing at no extra trainable parameter cost. Further, we show theoretically and empirically that (i) GNNs are by design prone to extreme gradient vanishing even after a few layers; (ii) Over-smoothing is directly related to the mechanism causing vanishing gradients; (iii) Over-squashing is most easily alleviated by a combination of graph rewiring and vanishing gradient mitigation. We believe our work will help bridge the gap between the recurrent and graph neural network literature and will unlock the design of new deep and performant GNNs.", "sections": [{"title": "1. Introduction", "content": "Graph Neural Networks (GNNs) (Sperduti, 1993; Gori et al., 2005; Scarselli et al., 2008; Bruna et al., 2014; Defferrard et al., 2017) have become a widely used architecture for processing information on graph domains. Most GNNs operate via message passing, where information is exchanged between neighboring nodes, giving rise to Message-Passing Neural Networks (MPNNs). Some of the most popular instances of this type of architecture include GCN (Kipf & Welling, 2017), GAT (Veli\u010dkovi\u0107 et al., 2018), GIN (Xu et al., 2018), and GraphSAGE (Hamilton et al., 2017).\nDespite its widespread use, this paradigm also suffers from some fundamental limitations. Most importantly, we highlight the issue of over-smoothing (Nt & Maehara, 2019; Cai & Wang, 2020; Rusch et al., 2023a), where feature representations become exponentially similar as the number of layers increases, and over-squashing (Alon & Yahav, 2021; Topping et al., 2021; Di Giovanni et al., 2023), which describes the difficulty of propagating information across faraway nodes, as the exponential growth in a node's receptive field results in many messages being compressed into fixed-size vectors. Although these two issues have been studied extensively, and there exists evidence that they are trade-offs of each other (Giraldo et al., 2023), there is no unified theoretical framework that explains why architectures which solve these problems work and whether there exists a common underlying cause that governs these problems.\nIn this work, we analyze over-smoothing and over-squashing from the lens of vanishing gradients. In particular, we ask several questions about the appearance and consequences of this phenomenon in GNNs: (i) How prone are GNNs to gradient vanishing? (ii) What is the effect of gradient vanishing on over-smoothing? (iii) Can preventing vanishing gradients effectively mitigate over-squashing? (iv) Can methods used in the non-linear (Hochreiter & Schmidhuber, 1997; Pascanu et al., 2013; Beck et al., 2024) and more recently linear (Gu & Dao, 2023; Orvieto et al., 2023) recurrent neural network (RNN) literature be effective at dealing with over-smoothing and over-squashing? With the latter questions, we aim to fill the gaps and open questions left in the over-squashing analysis of Di Giovanni et al. (2023). Further, we hope the point on over-smoothing will help provide a theoretical explanation for why certain architectural choices have led to the design of deep and performant GNNs."}, {"title": "Contributions and outline.", "content": "In summary, the contributions of this work are the following:\n\u2022 In Section 3, we explore the connection between GNNs and sequence models and demonstrate how graph convolutional and attentional models are susceptible to a phenomenon we term extreme gradient vanishing. We propose GNN-SSM, a GNN model that is written as a state-space model, allowing for a better control of the spectrum of the Jacobian.\n\u2022 In Section 4, we show how vanishing gradients contribute to over-smoothing, providing a much more precise explanation for why over-smoothing occurs in practice in GNNs by studying the spectrum of the layer-wise Jacobians. We show that GNN-SSMs are able to exactly control the rate of smoothing.\n\u2022 In Section 5, we show how vanishing gradients are related to over-squashing. We argue that over-squashing should therefore be tackled by approaches that both perform graph rewiring and mitigate vanishing gradients.\nOverall, we believe that our work provides a new and interesting perspective on well-known problems that occur in GNNs, from the point of view of sequence models. We believe this to be an important observation connecting two very wide - yet surprisingly disjoint \u2013 bodies of literature."}, {"title": "2. Background and Related Work", "content": "We start by providing the required background on graph and sequence models. We further discuss the existing literature on over-smoothing and over-squashing in GNNs and vanishing gradients in recurrent sequence models."}, {"title": "2.1. Message Passing Neural Networks", "content": "Let a graph G be a tuple (V, E) where V is the set of nodes and E is the set of edges. We denote edge from node u \u2208 V to node v \u2208 V with (u, v) \u2208 E. The connectivity structure of the graph is encoded through an adjacency matrix defined as A \u2208 Rnxn where n is the number of nodes in the graph. We assume that G is an undirected graph and that there is a set of feature vectors {h}vev \u2208 Rd, with each feature vector being associated with a node in the graph. Graph Neural Networks (GNNs) are functions fe : (G, {h}) \u2192 y, with parameters trained via gradient descent and y being a node-level or graph level prediction label. These models typically take the form of Message Passing Neural Networks (MPNNs), which compute latent representation by composing K layers of the following node-wise operation:\nh(k) = \u03c6(k) (h(k-1), \u03c8(k) ({h(k-1) : (u, v) \u2208 E})),                                                                                                   (1)\nfor k = {1, ..., K}, where \u03c8(k) is a permutation invariant aggregation function and \u03c6(k) combines the incoming messages from one's neighbors with the previous embedding of oneself to produce an updated representation. The most commonly used aggregation function takes the form\n\u03c8(k) ({h(k-1) : (u, v) \u2208 E}) = \u2211Auvh(k-1),                                                                                  (2)\nu\nwhere A = D-AD\u00af\u00bd, and D\u2208 Rn\u00d7n is a diagonal matrix where Dii = \u2211j Aij. One can also consider a matrix representation of the features H(k) \u2208 Rn\u00d7dk. Throughout the paper, we will use the terms GNN and MPNN interchangeably, and will generally consider the most widely used instance of GNNs, which are Graph Convolutional Networks (GCNs) (Kipf & Welling, 2017) whose matrix update equation is given by:\nH(k) = \u03c3(AH(k-1)W(k-1)),                                                                                                                (3)\nwhere \u00c2 = (D + I)-1/2 (A + I) (D + I)-1/2 is the adjacency matrix with added self connections through the identity matrix I, and \u03c3(\u00b7) is a nonlinearity. Our analysis also applies to Graph Attention Networks (GATs) (Veli\u010dkovi\u0107 et al., 2018), where the fixed normalized adjacency is replaced by a learned adjacency matrix which dynamically modulates connectivity while preserving the key spectral properties used in our analysis."}, {"title": "2.2. Recurrent Neural Networks", "content": "A Recurrent Neural Network (RNN) is a function ge : XH y, where x = (x(1), x(2),...,x(K)) and y = (y(1),y(2), ..., y(K)), where x(k) \u2208 Rd is the input vector at time step k and y(k) \u2208 Rm is the output vector at time step k, and are learnable parameters. RNNs are designed to handle sequential data by maintaining a hidden state h(k) \u2208 Rdh that captures information from previous time steps. This hidden state\u00b9 allows the network to model sequential dependencies in the data. The update equations for the hidden state of the RNN are as follows:\nh(k) = \u03c3(Whh(k-1) + Wxx(k)).                                                                                                                (4)"}, {"title": "2.3. The Vanishing and Exploding Gradient Problem", "content": "Both RNNs and GNNs are trained using the chain rule. One can backpropagate gradients w.r.t. the weights at ith layer of a K-layer GNN or RNN as\n(H(k))\n=\nK\n\u220fJ(H(k))          (5)\nk=i+1"}, {"content": "where matrix H(k) in an RNN will contain a single state vector. As identified by Pascanu et al. (2013), a major issue in training this type of models arises from the product Jacobian, given by:\nJ =\n(H(k))\n(H(i))\n\u220f\u2202(H(k\u22121))\n\u220fJk.\n(6)\nk=i+1 k=i+1\nIn general, we have that if ||Jk||2 \u2248 \u03bb for all layers then ||J||2 \u2264 \u03bbK-i. This means that we require \u03bb \u2248 1 for gradients to neither explode nor vanish, a condition also known as edge of chaos."}, {"title": "2.4. Over-smoothing, Over-squashing, and Vanishing Gradients in GNNS", "content": "Over-smoothing. Over-smoothing (Cai & Wang, 2020; Oono & Suzuki, 2020; Rusch et al., 2023a) describes the tendency of GNNs to produce smoother representations as more and more layers are added. In particular, this has been related to the convergence to the 1-dimensional kernel of the graph Laplacian and equivalently as a minimization process of the Dirichlet energy (Di Giovanni et al., 2022). In Section 4, we study this issue from the lens of vanishing gradients and show that over-smoothing has a much more simple explanation: it occurs due to the norm-contracting nature of GNN updates.\nOver-squashing. Over-squashing (Alon & Yahav, 2021; Topping et al., 2021; Di Giovanni et al., 2023; Barbero et al., 2024) was originally introduced as a bottleneck resulting from 'squashing' into node representations amounts of information that are growing potentially exponentially quickly due to the topology of the graph. It is often characterized by the quantity ||h(0)-h(0)||| being low, implying that\n(H(i))"}, {"title": "3. Connecting Sequence and Graph Learning through State-Space Models", "content": "In this section, we study GNNs from a sequence model perspective. We show that the most common classes of GNNS are more prone to vanishing gradients than feedforward or recurrent networks due to the spectral contractive nature of the normalized adjacency matrix. We then propose GNN-SSMs, a state-space-model-inspired construction of a GNN that allows more direct control of the spectrum."}, {"title": "3.1. Similarities and differences between learning on sequences and graphs", "content": "The GNN architectures that first popularized deep learning on graphs (Bruna et al., 2014; Defferrard et al., 2017) were initially presented as a generalization of Convolutional Neural Networks (CNNs) to irregular domains. GCNs (Kipf & Welling, 2017) subsequently restricted the architecture in Defferrard et al. (2017) to a one-hop neighborhood. While this is still termed \u201cconvolutional\" (due to weight sharing across nodes), the iterative process of aggregating information from each node's neighborhood can also be viewed as recurrent-like state updates.\nIf we consider an RNN unrolled over time, it forms a directed path graph feeding into a state node with a self-connection-making it a special case of a GNN. Conversely, node representations in GNNs can be stacked using matrix vectorization, allowing us to interpret GNN layer operations\""}, {"title": "3.2. Graph convolutional and attentional models are prone to extreme gradient vanishing", "content": "Based on the previously introduced notion of stacking node representations using the matrix vectorization operation, we now analyze the gradient dynamics of GNN. In particular, we focus on the gradient propagation capabilities of graph convolutional and attentional models at initialization, given their widespread use in the literature. Specifically, we demonstrate that the singular values of the layer-wise Jacobian in these models form a highly contractive mapping, which prevents effective information propagation beyond a few layers. We formalize this claim in Lemma 3.1 and Theorem 3.2, and we refer the reader to Appendix A.1 for the corresponding proofs.\nLet H(k) = \u00c3 H(k-1) W be a linear GCN layer, where A has eigenvalues {1,...,\u03bbn} and W WT has eigenvalues {\u00b51,...,\u00b5dk}. Consider the layer-wise Jacobian\n J = \u2202 vec (H(k)) /\u2202 vec(H(k-1)), Then the squared singular values of J are given by the set\n{\u03bb\u03bc; | i = 1,...,n, j = 1,...,dk}.\nAssume the setting of Lemma 3.1, and let W \u2208 Rdk\u22121\u00d7dk be initialized with i.i.d. N(0, \u03c3\u00b2) entries. Denote the squared singular values of the Jacobian by Yi,j. Then, for sufficiently large dk the empirical eigenvalue distribution of WWT converges to the Marchenko-Pastur distribution. Then, the mean and variance of each Vi,j are\n\u0395[i,j] = \u03bb \u03c3\u00b2,                                                                                                                            (7)\nVar[i,j] = \u03bb \u03c3,                                                                                                                       (8)"}, {"title": "3.3. GNN-SSM: Improving the training dynamics of GNNs through state-space models", "content": "To allow direct control of the signal propagation dynamics of any GNN, we can rewrite its layer-to-layer update as a state-space model. Concretely, we express the update as\nH(k+1) = \u039b\u0397(k) + BX(k)\n\u039b\u0397(k) + BF0(H(k), k),                                                                                                              (9)\nwhere we refer to \u039b as the state transition matrix and B as the input matrix,\u00b2 and Fo(H(k), k) as a time-varying coupling function which connects each node to some neighborhood. We refer to the model defined in Equation (9) as GNN-SSM. From an RNN perspective, \u039b plays the role of"}, {"title": "4. How is Over-smoothing linked to Vanishing Gradients?", "content": "In this section, we study the practical implications of the mechanism causing vanishing gradients in GNNs in relation to the over-smoothing phenomenon. We show theoretically how over-smoothing is a consequence of GNN layers acting as contractions, which make node features collapse to a zero fixed point under certain conditions. We experimentally validate our points by analyzing Dirichlet energy, node feature norms, and node classification performance for increasing numbers of layers. Overall, we believe this section provides a more practical and general understanding of over-smoothing, by analyzing any GNN from the point of view of its layer-wise Jacobians."}, {"title": "4.1. Over-smoothing secretly occurs due to contractions in the Jacobian", "content": "Over-smoothing describes the tendency of node features to become too similar to each other as more layers are added in GNNs (Cai & Wang, 2020; Rusch et al., 2023a). This phenomenon is largely due to the nature of the operations that are performed by GNN layers and their relationship with heat equations over graphs. Consequently, a common way of measuring over-smoothing in GNNs is via the graph Dirichlet energy E(H). Given a graph node signal H \u2208 Rn\u00d7d on a graph G, E(H) takes the form:\nE(H) = tr (\u0397\u0394\u0397) = \u2211 ||hu - hv||2,                                                                                                              (11)\n(\u03ba,\u03c5) \u0395\u0395\nwhere \u0394 is the (unweighted) graph Laplacian (Chung, 1997). The graph Dirichlet energy measures the smoothness of a signal over a graph and will be minimized when the signal is constant over each node \u2013 at least when using the non-normalized Laplacian. Notably, models like GCNs can be seen as minimizers of the Dirichlet energy\u00b3 (Bodnar et al., 2022; Di Giovanni et al., 2022).\nWe consider in our analysis GNN layers as in Equation 3. We view a GNN layer as a map fk : Rnd \u2192 Rnd and construct a deep GNN f via composition of K layers, i.e. f = fK \u00b0\u2026 f1. We also require the condition of our"}, {"title": "5. The Impact of Vanishing Gradients on Over-squashing", "content": "In this section, we study the connection between vanishing gradients and over-squashing in GNNs."}, {"title": "5.1. Mitigating over-squashing by combining increased connectivity and non-dissipativity", "content": "Over-squashing is typically measured via the sensitivity of a node embedding after k layers with respect to the input of another node using the node-wise Jacobian.\n(k)\ndh(u)\n(0)\n< (cowd)k (Ok)vu,                                                                                        (15)\nwith O = cII+ Ca A \u2208 Rnxn is the message passing matrix adopted by the MPNN, with cr and ca are the contributions of the self-connection and aggregation term.\nTheorem 5.1 shows that the sensitivity of the node embedding arises from a combination of (i) a term based on the graph topology and (ii) a term dependent on the model dynamics, with over-squashing occurring when the right-hand side of Equation (15) becomes too small. We highlight that this differs from the standard product Jacobian which arises in RNNs. This is because in MPNNs, messages are scaled by the inverse node degree, incurring an extra information dissipation step. Consequently, while recurrent architectures only need to adjust their dynamics to ensure long memory, MPNNs must simultaneously enhance graph connectivity and modify their dynamics to mitigate vanishing gradients.\nEven though the sensitivity bound in Theorem 5.1 is controlled by two components, the majority of the literature has typically focused on addressing only the topological term via graph rewiring (Gasteiger et al., 2019; Topping et al., 2021; Karhadkar et al., 2022; Barbero et al., 2023; Finkelshtein et al., 2024), with some methods also targeting the model dynamics (Gravina et al., 2023; 2025; Heilig et al., 2025). In fact, Di Giovanni et al. (2023) explicitly discourages increasing the model term in Theorem 5.1 and claims that doing so could lead to over-fitting and poorer generalization. However, we argue that increasing the model term directly linked to vanishing gradients as discussed in Section 4 is essential to mitigate over-squashing. Rather than harming performance, boosting this term helps prevent over-smoothing, since even in a well-connected graph where information can be reached in fewer hops, unaddressed vanishing gradients due to the model term will cause the target node's features to decay to zero during message passing."}, {"title": "5.2. Empirical validation of claims", "content": "We focus our empirical validation on answering the following questions: (i) What is the result of combining an effective rewiring scheme with vanishing gradient mitigation? (ii) Will this result in similar state-of-the-art results? To investigate this, we construct a minimal model that combines high connectivity with non-dissipativity. In particular, we make of the GNN-SSM model and employ a k-hop aggregation scheme for the coupling function Fe, which we term KGNN-SSM (more details are provided in Appendix B)."}, {"title": "6. Conclusion", "content": "In this work, we revisit the well-known problems of over-smoothing and over-squashing in GNNs from the lens of vanishing gradients, by studying GNNs from the perspective of recurrent and state-space models. In particular, we show that GNNs are prone to a phenomenon we term extreme gradient vanishing, which results in ill-conditioned signal propagation with few layers. As such, we argue that it is important to control the layerwise Jacobian and propose a state-space-inspired GNN model, termed GNN-SSM, to do so. We then uncover that vanishing gradients result in a very specific form of over-smoothing in which all signals converge exactly to the 0 vector, and support this claim empirically. Finally, we theoretically argue and empirically show that mitigation of over-squashing is best achieved through a combination of strong graph connectivity and non-dissipative model dynamics.\nLimitations and Future Work. We believe our work opens up a number of interesting directions that aim to bridge the gap between graph and sequence modeling. In particular, we hope that this work will encourage researchers to adapt vanishing gradient mitigation methods from the sequence modeling community to GNNs, and conversely explore how graph learning ideas can be brought to recurrent models. In our work, we mostly focused on GCN and GAT type updates, but we believe that our analysis can be extended to understand how different choices of updates and non-linearities affect training dynamics, which we leave for a future work."}, {"title": "A. Theoretical Results", "content": "A.1. Proofs of Jacobian Theorems\nDefinition A.1 (Vectorization and Kronecker product). Let X \u2208 Rm\u00d7n be a real matrix. The vectorization of X, denoted vec(X), is the (mn)-dimensional column vector obtained by stacking the columns of X:\nX:,1\nX:,2\nvec(X) =\n\u2208 Rmn.\nX:,n\nOne key property of the vectorization operator is its relationship to the Kronecker product. In particular, for compatible matrices A, B, C, we have\nvec(ABC) = (CTA) vec(B).\nHere, denotes the Kronecker product.\nDefinition A.2 (Wishart matrix). Let X \u2208 Rn\u00d7p be a matrix with i.i.d. entries Xij ~ N(0,\u03c3\u00b2). The random matrix XTX \u2208 RP\u00d7p is called a Wishart matrix (up to a scaling factor). In particular, such a matrix follows the Wishart distribution Wp(\u03b7, \u03c3\u00b2) in certain parametrizations.\nDefinition A.3 (Marchenko-Pastur distribution. (Marchenko & Pastur, 1967)). In the high-dimensional limit (n, p \u2192 \u221e at a fixed ratio p/n \u2192 c), the empirical eigenvalue distribution of the (properly normalized) Wishart matrix XTX converges to the Marchenko-Pastur distribution. Concretely, if X \u2208 Rn\u00d7p has entries N(0, 1), then the eigenvalues of XTX lie within [(1 - \u221ac)\u00b2, (1 + \u221ac)\u00b2] for large n, p, and their density converges to\nfMP(x) =\n1\n\u221a(x \u2212 amin) (amax \u2212 x), X\u2208 [amin, amax],\n2\u03c0\u03bf\u03c7\nwith amin = (1 \u2212 \u221ac)2 and amax = (1 + \u221ac)\u00b2. If the entries of X have variance \u03c3\u00b2 \u2260 1, then the support is rescaled by \u03c32.\nLet H(k) = \u00c3 H(k-1) W be a linear GCN layer, where A has eigenvalues {1,...,\u03bbn} and W WT has eigenvalues {\u00b51,...,\u00b5dk}. Consider the layer-wise Jacobian J = \u2202vec (H(k)) /\u2202vec(H(k-1)), Then the squared singular values of J are given by the set\n{\u03bb\u03bc; | i = 1,...,n, j = 1,...,dk}.\nProof. By the property of vectorization (Definition A.1), we have\nvec (AH(k-1) W) = (WTA) vec(H(k-1)).\nHence\nJ = WT\u00c3.\nBy properties of the Kronecker product, the eigenvalues of J JT are the products of the eigenvalues of WTW and \u00c3\u00b2. Equivalently,\nspec(JJ) = spec(WTW) spec(\u00c3\u00b2),\nwhere spec is the vectorized version of the set of eigenvalues of a matrix. If WTW has eigenvalues {\u00b5j}dj=1 and \u00c3\u00b2 has eigenvalues {\u03bbi}ni=1, then the squared singular values of J are precisely \u03bb \u03bc; for i\u2208 {1, . . ., n}, j \u2208 {1,...,dk}.\nAssume the setting of Lemma 3.1, and let W \u2208 Rdk\u22121\u00d7dk be initialized with i.i.d. N(0, \u03c3\u00b2) entries. Denote the squared singular values of the Jacobian by Yi,j. Then, for sufficiently large dk the empirical eigenvalue distribution WWT converges to the Marchenko-Pastur distribution. Then, the mean and variance of each Vi,j are\n\u0395[i,j] = \u03bb \u03c3\u00b2,                                                                                                                          (16)\nVar[i,j] = 104dk\u03c34/dk\u22121.                                                                                                                       (17)"}, {"title": "A.2. Proofs to Smoothing Theorems", "content": "Definition A.7 (Lipschitz continuity). A function f : Rn \u2192 Rm is Lipschitz continuous if there exists an L \u2265 0 such that for all x, y \u2208 Rn, we have that:\n||f(x) - f(y) || \u2264 L ||x -y||,\nwhere we equip Rn and Rm with their respective norms. The minimal such L is called the Lipschitz constant of f.\nThe notion of Lipschitz continuity is effectively a bound on the rate of change of a function. It is therefore not surprising that one can relate the Lipschitz constant to the Jacobian of f. In particular, we state a useful and well-known result (Hassan et al., 2002) that relates the (continuous) Jacobian map Jf of a continuous function f : Rn \u2192 Rm to its Lipschitz constant L \u2265 0. In particular, the Lipschitz constant is is the supremum of the (induced) norm of the Jacobian taken over its domain.\nLet f : Rn \u2192 Rm be continuous, with continuous Jacobian J f. Consider a convex set U = R\" If there exists L \u2265 0 such that ||Jf(x)|| < L for all x \u2208 U, then || f(x) \u2212 f(y)|| < L ||x \u2212 y||. In particular, we have that the Lipschitz constant of f L is:\nL = sup ||J f (x)||.\nXEU"}, {"title": "B. kGNN-SSM: A simple method to combine high connectivity and non-dissipativity.", "content": "To test our assumption on more complex downstream tasks, we construct a minimal model that combines high connectivity with non-dissipativity. To guarantee high connectivity, we employ a k-hop aggregation scheme. In particular, each node i at layer k will aggregate information as\na = k({h(k)k) : j\u2208 Nk(i)}),                                                                                                                    (21)\nwhere\nNk(i) := {j \u2208 V : dg(i, j) = k}\nand dg: V \u00d7 V \u2192 R\u22650 is the length of the minimal walk connecting nodes i and j. This approach avoids a large amount of information being squashed into a single vector, and is more in line with the recurrent paradigm. We note that this scheme is similar to (Ding et al., 2024), but in this case we do not consider different block or parameter sharing, and our recurrent mechanism is based on an untrained SSM layer.\nWe denote a GNN endowed with this rewiring scheme and wrapped with our SSM layer as kGNN-SSM."}, {"title": "C. Experimental Details", "content": "In this section, we provide additional experimental details, including dataset and experimental setting description and employed hyperparameters.\nOver-smoothing task. In this task, we aim to analyze the dynamics of the Dirichlet energy across three different graph topologies: Cora (Yang et al., 2016), Texas (Pei et al., 2020), and a grid graph. The Cora dataset is a citation network consisting of 2,708 nodes (papers) and 10,556 edges (citations). The Texas dataset represents a webpage graph with 183 nodes (web pages) and 499 edges (hyperlinks). Lastly, the grid graph is a two-dimensional 10 \u00d7 10 regular grid with 4-neighbor connectivity. For all three graphs, node features are randomly initialized from a normal distribution with a mean of 0 and variance of 1. These node features are then propagated over 80 layers (or iterations) using untrained GNNs to observe the energy dynamics.\nGraph Property Prediction. This experiment consists of predicting two node-level (i.e., eccentricity and single source shortest path) and one graph-level (i.e., graph diameter) properties on synthetic graphs sampled from different distribution, i.e., Erd\u0151s-R\u00e9nyi, Barabasi-Albert, grid, caveman, tree, ladder, line, star, caterpillar, and lobster. Each graph contains between 25 and 35 nodes, with nodes assigned with input features sampled from a uniform distribution in the interval [0, 1). The target values correspond to the predicted graph property. The dataset consists of 5,120 graphs for training, 640 for validation and 1,280 for testing."}, {"title": "D. Additional empirical results", "content": "In this section, we propose additional empirical results on over-smoothing and over-squashing, as well as the eigendistribution of the layerwise Jacobians of various standard GNNs."}, {"title": "D.1. Additional MPNN Jacobians", "content": "Here, we present in Figure 6 the eigendistribution of the layerwise Jacobians of GCN, GIN (Xu et al., 2018) and Gated-GCN (Bresson & Laurent, 2017). Across the board, we observe similar contraction effects in the Jacobian as those presented in the main paper, with a long number of eigenvalues accumulating at zero, with no significant changes in the distribution during training. However, the maximum eigenvalues for both GIN and Gated-GCN are much larger than those of GCN. We also compare a nonlinear feedforward network and a nonlinear GCN in Figure 7."}, {"title": "D.3. Link between delay and vanishing gradients", "content": "Here, we show how the delay term in (Gutteridge et al., 2023) is directly related to preventing vanishing gradients. We do so by showing that adding the delay term to a GCN is effective at preventing over-smoothing, see Figure 12, as well as by checking the histogram of eigenvalues of the Jacobian, see Figure 14."}, {"title": "D.5. Additional comments on LRGB tasks", "content": "In our experiments with the LRGB tasks, we observe that the peptides-func task exhibits significantly longer-range dependencies than the peptides-struct task. Notably, the peptides-struct task performs best when the model is not initialized at the edge of chaos and requires fewer layers. Conversely, on peptides-struct the model performs best when it is set to be at the edge of chaos, and shows a monotonic performance increase with additional layers, with optimal results achieved when using forty layers.\nFurthermore, we highlight that while our experiments with a small hidden dimension adhere to the parameter budget established in (Dwivedi et al., 2022), increasing the hidden dimension (d \u2191) to 256 causes us to exceed the 500k parameter budget limit, even though our model maintains the same number of parameters as a regular GCN. While this budget is a useful tool to benchmark different models, we highlight that this restriction results in models running with fewer layers and small hidden dimensions. However, a large number of layers is crucial for effective long-range learning in graphs that are not highly connected, while increasing the hidden dimension also directly affects the bound in Theorem 5.1. As such, we believe that this parameter budget indirectly benefits models with higher connectivity graphs, inadvertently hindering models that do not perform edge addition."}, {"title": "E. Supplementary Related Work", "content": "Long-range propagation on GNNs. Learning long-range dependencies on graphs involves effectively propagating and preserving information across distant nodes. Despite recent advancements, ensuring effective long-range communication between nodes remains an open problem (Shi et al., 2023). Several techniques have been proposed to address this issue, including graph rewiring methods, such as (Gasteiger et al., 2019; Topping et al., 2021; Karhadkar et al., 2022; Barbero et al., 2023; Gutteridge et al., 2023; Black et al., 2023), which modify the graph topology to enhance connectivity and facilitate information flow. Similarly, Graph Transformers enhance the connectivity to capture both local and global interactions, as demonstrated by (Ying et al., 2021; Dwivedi & Bresson, 2021; Shi et al., 2021; Kreuzer et al., 2021; Ramp\u00e1\u0161ek et al., 2022; Wu et al., 2023). Other approaches incorporate non-local dynamics by using a fractional power of the graph shift operator (Maskey et al., 2024), leverage quantum diffusion kernels (Markovich, 2023), regularize the model's weight space (Gravina et al., 2023; 2025), or exploit port-hamiltonian dynamics (Heilig et al., 2025).\nVanishing gradients in sequence modelling and deep learning. One of the primary challenges in training recurrent neural networks lies in the vanishing (and sometimes exploding) gradient problem, which can hinder the model's ability to learn and retain information over long sequences. In response, researchers have proposed numerous architectures aimed at preserving or enhancing gradients through time. Examples include Unitary RNNs (Arjovsky et al., 2016), Orthogonal RNNs (Henaff et al., 2016), coRNNs (Rusch & Mishra, 2020), Linear Recurrent Units (Orvieto et al., 2023), and Structured State Space Models (Gu et al., 2021; Gu & Dao, 2023"}]}