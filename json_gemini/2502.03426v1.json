{"title": "TRUEPOSE: Human-Parsing-guided Attention Diffusion for Full-ID Preserving Pose Transfer", "authors": ["Zhihong Xu", "Dongxia Wang", "Peng Du", "Yang Cao", "Qing Guo"], "abstract": "Pose-Guided Person Image Synthesis (PGPIS) generates images that maintain a subject's identity from a source image while adopting a specified target pose (e.g., skeleton). While diffusion-based PGPIS methods effectively preserve facial features during pose transformation, they often struggle to accurately maintain clothing details from the source image throughout the diffusion process. This limitation becomes particularly problematic when there is a substantial difference between the source and target poses, significantly impacting PGPIS applications in the fashion industry where clothing style preservation is crucial for copyright protection. Our analysis reveals that this limitation primarily stems from the conditional diffusion model's attention modules failing to adequately capture and preserve clothing patterns. To address this limitation, we propose human-parsing-guided attention diffusion, a novel approach that effectively preserves both facial and clothing appearance while generating high-quality results. We propose a human-parsing-aware Siamese network that consists of three key components: dual identical UNets (TargetNet for diffusion denoising and SourceNet for source image embedding extraction), a human-parsing-guided fusion attention (HPFA), and a CLIP-guided attention alignment (CAA). The HPFA and CAA modules can embed the face and clothes patterns into the target image generation adaptively and effectively. Extensive experiments on both the in-shop clothes retrieval benchmark and the latest in-the-wild human editing dataset demonstrate our method's significant advantages over 13 baseline approaches for preserving both facial and clothes appearance in the source image.", "sections": [{"title": "1. Introduction", "content": "Images of well-dressed individuals are widely used in shopping stores for advertising. To achieve a high-quality image, a model must wear specific clothing and adopt a designated pose. Once these images are captured and deployed, it becomes difficult for stores to change the model's pose. Otherwise, they must invite the model to wear the same clothes and undergo the photography process again, which is time-consuming and costly. The well-defined task, i.e., pose-guided person image synthesis (PGPIS), could easily meet the above requirement efficiently. PGPIS aims to generate an image that matches the specified pose condition (e.g., skeleton) while retaining the same appearance (person and clothing) as the source image (See the inputs in Fig. 1) [12, 17, 39]. Such a task is challenging due to the potential significant discrepancy between the source and target poses.\nPrevious works formulate the task as the deep generation problem and employ generative adversarial network (GAN) [25, 27, 30, 34, 35, 38, 39] and variational autoencoder (VAE) [2] to achieve the goal. However, GAN-based methods struggle with unstable training and generating high-quality images. VAE-based methods usually suffer from some artifacts like blur.\nRecently, diffusion models have demonstrated powerful capabilities in generating high-quality images [7, 29] with flexible control conditions [24]. Researchers have developed diffusion-based PGPIS methods, achieving impressive results [1, 15, 26]. In particular, the state-of-the-art (SOTA) methods, e.g., coarse-to-fine latent diffusion (CFLD) [15] and progressive conditional diffusion (PCDM) [26], can generate realistic images with preserved human pose and face ID (See in Fig. 1). However, they struggle to transfer clothing patterns and textures (i.e., clothing ID) from the source image to the target image. We display four cases in Fig. 1 and observe that: \u25cf CFLD and PCDM can hardly preserve clothing patterns and textures. The two methods fail to transfer the texts (1st and 4th cases), regular texture (2nd case), and irregular textures (3rd case). As the discrepancy between the source and target poses increases, it becomes more challenging to preserve the clothing IDs. For example, with similar source images (1st case vs. 4th case), PCDM can reproduce some words in the 1st case but loses all words in the 4th case. We calculate the feature attention map of baseline methods for each case, which shows that both methods pay less attention to the clothing regions. We will conduct a statistical analysis in Sec. 3.2 for further discussion.\nThese observations inspire us to design a novel PGPIS framework that preserves both facial and clothing patterns. Our approach leverages the person-parsing map of the source image to guide the encoding process, ensuring the features focus on both face and clothing regions. We propose a human-parsing-aware Siamese network with three main modules: First, we design a dual idenitical UNets (i.e., SourceNet and TargetNet). TargetNet is used for diffusion denoising and SourceNet is designed for extracting source image embedding. Second, we introduce the human-parsing-guided fusion attention, which corrects the attention of TargetNet's embedding on the facial and clothes regions according to the guidance of the source parsing map and source image embeddings. Third, we propose CLIP-guided attention alignment to further refine the corrected TargetNet embedding based on the consistent constraints across different semantic regions. The proposed modules could be inserted into different layers of the TargetNet. We validate our method on the in-shop clothes retrieval benchmark with two resolutions (512 \u00d7 352, 256 \u00d7 176) and compare with 13 SOTA baselines. As shown in Fig. 1, our method can generate high-quality images with well-preserved facial appearance and clothing patterns."}, {"title": "2. Related Work", "content": "Pose-guided person image synthesis. PGPIS achieves pose transformation in a complex spatial context while maintaining consistency with the detailed features of the source image, which was proposed by Ma et al. [17]. Early approaches [2, 17] employ conditional generative adversarial networks (CGANs) [3, 20] to guide the generation of target images using the source image as conditional information [17, 19]. However, due to the significant spatial information differences between the source image and the target image, as well as the sparse guidance from the skeleton map, directly achieving pose transfer is highly challenging. To address the challenges, some works decouple and separately optimize pose and appearance information[18], or use attention mechanisms to better establish the mapping relationship between pose and appearance[23, 35, 37]. On the other hands, some approaches introduce more detailed and dense supplementary information, such as UV [25], dense-pose [4], and parsing maps [16, 19, 34, 38], to alleviate inconsistencies between poses. Recent approaches adopt diffusion as the generative model[1, 4, 15, 26]. By incorporating conditional information into iterative forward-backward denoising procedures [7], diffusion-based methods outperform GAN-based approaches in terms of both image generation quality and pose control effectiveness. PIDM is the pioneering endeavor to integrate the diffusion model into PGPIS, which designs a conditional texture diffusion model and a disentangle classifier-free guidance mechanism [6]. PoCoLD and PCDM aim to achieve pose alignment through a pose-constrained attention mechanism and progressive multi-stage refinement, respectively [4, 26]. CFLD [15] endeavors to attain coarse-to-fine generation by extracting coarse prompt information from the source image, followed by refinement through a learnable Perception-Refined Decoder and Hybrid-Granularity Attention to achieve fine-grained results.\nConditional diffusion models. The significant potential of diffusion models in traditional unconditional generative tasks has recently been evidenced [7, 28, 29]. In contrast to conventional single-step generative models such as Generative Adversarial Networks (GANs) [20] and Variational Autoencoders (VAEs)[9], diffusion relies on multi-step denoising sampling of initial noise to generate high-quality data, thereby enabling the production of detailed and diverse outputs. The advancement of various conditional diffusion models further enhances its practicality and versatility. Classifier-free methodologies devise implicit classifiers to guide the weights of controllable and uncontrollable processes, thus facilitating conditional generation[6]. Furthermore, Latent Diffusion Model(LDM)[24] utilize denoising process by encoding original images into a low-dimensional latent space and integrate cross-attention mechanisms to introduce multi-modal conditions, thereby significantly expanding the diversity of control information. Prior LDM-based approaches [4, 15, 26] rely on pre-trained encoders to extract appearance information from the source image. We argue that such encoders are ineffective in capturing fine-grained details, which hinders the generation of complex images. In contrast, our proposed framework utilizes guided attention to capture complex detail features, improving the preservation of full identities and ensuring person consistency. Our extensive experimental analysis confirms the limitations of traditional approaches. (Sec. 3.2)."}, {"title": "3. Preliminaries and Limitations", "content": "3.1. Diffusion-based PGPIS\nGiven a source image $I_s$ containing a person with pose $p_s$ (represented as a skeleton map) and a target pose $p_t$, PGPIS aims to generate a target image $I_t$ that preserves the person's appearance (should contain both facial and clothing patterns) while adopting the target pose. The SOTA diffusion-based PGPIS methods are designed based on the stable diffusion (SD) [24] that involves a forward diffusion process and a backward denoising process of $T$ steps. The forward diffusion process progressively add random Gaussian noise $\\epsilon \\in \\mathcal{N}(0, I)$ to the initial latent $z_0$. At the $t$th timestep, we can formulate it as\n$z_t = \\sqrt{\\bar{\\alpha}_t}z_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon, t \\in [1, T]$\nwhere $\\alpha_1, \\alpha_2, ..., \\bar{\\alpha}_t$ are calculated from a fixed variance schedule. The denoising process uses an UNet to predict the noise, i.e., $\\epsilon_{\\theta}(z_t, t, C, O_{\\beta})$ and remove the noise from the latent. The set $C$ involves related conditions and the function $O_{\\beta}(.)$ defines the way of embedding condition features into the UNet. For PGPIS task, we can set $C = \\{X_s, X_{tp}, X_{sp}\\}$ where $X_s$ denotes the features of source image and $X_{tp}$ and $X_{sp}$ contains the features of target pose and source pose. To train the UNet $\\epsilon_{\\theta}(.)$, the predicted noise should be the same as the sampled noise during the forward process\n$\\mathcal{L}_{mse} = \\mathbb{E}_{z_0, C, \\epsilon, t}(\\|\\epsilon - \\epsilon_{\\theta}(z_t, t, C, O_{\\beta})\\|_2)$.\nDuring the inference with trained $\\epsilon_{\\theta}$ and $O_{\\beta}$, we use image encoder and pose encoder to extract features of the inputs (i.e., $I_s$, $p_s$, $p_t$) and get $C = \\{X_s, X_{tp}, X_{sp}\\}$. Then, given the latent $z_t$, we perform the backward denoising process iteratively to generate the predicted target image.\nCoarse-to-fine latent Diffusion (CFLD) [15]. The SOTA method CFLD utilizes the pre-trained Swin-B [14] to extract multi-layer features (i.e., $X_s$) of the source image and the Adapter to extract multi-layer features [21] (i.e., $X_{tp}$) of the target pose. Moreover, CFLD proposes two new modules as the $O_{\\beta}$ to embed the condition features effectively.\nProgressive conditional Diffusion models (PCDM) [26]. Shen et al. proposed three-stage diffusion models to progressively generate high-quality synthesized images. The first stage is to predict the global embedding of the target image; the second stage is to get a coarse target estimation; the final stage is to refine the coarse result. Each stage relies on a diffusion model that could be approximately formulated with Eq. (1) and Eq. (2) with different condition setups. In particular, all three stages use the pre-trained image encoders, i.e., CLIP and DINOv2."}, {"title": "3.2. Empirical Study", "content": "As shown in Fig. 1, the state-of-the-art methods CFLD [15] and PCDM [26] can hardly preserve the clothing patterns and textures. The feature attention maps in Fig. 1 demonstrate a superficial reason, that is, the SOTA methods' features did not focus on the clothing. To further validate this observation, we perform a statistical analysis. Specifically, we randomly collect 50 examples from the in-shop clothes retrieval benchmark [13] and use the two SOTA methods to handle these"}, {"title": "4. Human-Parsing-aware Siamese Network", "content": "4.1. Overview\nFollowing the task definition of PGPIS, we have similar inputs including source image $I_s$ and target pose $p_t$. We begin by acquiring person-parsing maps. For the source image $I_s$, we utilize the person-parsing approach of [11] to generate parsing map $H_s$, which semantically segments the human body into categories such as arm, leg, dress, and skirt. With the parsing map and inputs, we also use the diffusion model to generate the target image and the key problem is how to set the condition $C$ and the fusion function $O_{\\beta}$ in Eq. (2). Different from previous works using pre-trained encoders to extract embeddings of source, pose, and parsing images, we design a human-parsing-aware Siamese network. First, we set an encoder denoted as $\\phi(.)$ to extract the latent of source images and the noisy target image during the diffusion process, which can be formulated as $z_s = \\phi(I_s)$.\nSecond, we build a Siamese network containing two UNets denoted as TargetNet (i.e., $\\epsilon_{\\theta}$) and SourceNet (i.e., $\\epsilon_{\\theta'}$). TargetNet is the UNet in the stable diffusion, taking the noisy latent as input $z_t$, predicting the noise amount at tth timestamp for inverse denoising. SourceNet has the same architecture as the TargetNet and takes $z_s = \\phi(I_s)$ as the input and output L embeddings from L layers of $\\epsilon_{\\theta'}$,\n$F' = \\{F'_l\\}_{l=1}^{L} = \\epsilon_{\\theta'}(z_s)$.\nMeanwhile, we leverage the parsing map to split the source image into M regions corresponding to their M categories and get $\\{R_i\\}_{i=1}^{M}$. Each region $R_i$ is a rectangle region wrapping the $i$th category. Then, we use the pre-trained CLIP encoder to extract the embeddings of all regions\n$F_{clip} = \\{F_{clip}^i\\}_{i=1}^{M} = \\{CLIP(R_i)\\}_{i=1}^{M}$.\nWe leverage SourceNet and CLIP encoder embeddings of the source image as conditions in our diffusion. The design is motivated by two key advantages: \u2460 The SourceNet, sharing identical architecture and initial weights with the TargetNet, naturally produces embeddings that are well-aligned with TargetNet's feature representations across all layers. This alignment facilitates effective transfer of source image information during the diffusion process. \u2461 The CLIP encoder provides rich semantic embeddings that effectively capture region-specific features, enhancing semantic consistency across different areas of the generated image.\nWe formulate the noise prediction for diffusion by\n$\\hat{\\epsilon} = \\epsilon_{\\theta}(z_t, t, C, O_{\\beta}), s.t., C = \\{F', F_{clip}\\}$,\nwhere $\\hat{\\epsilon}$ is the predicted noise at tth timestamp. The function $O_{\\beta}$ is the way to fuse conditions $C$ into the TargetNet, and the key problem is how to design the function $O_{\\beta}$.\nWe propose a two-stage fusion strategy with the above two conditions, respectively. In Sec. 4.2, we introduce human-parsing-guided fusion attention that enhances the embedding's attention on the body, clothes, and facial regions in the target image according to the source parsing map and target pose. However, the output embedding still presents low scores in some regions and different source images have different lower-score regions. To fix this problem, we further propose a CLIP-guided attention alignment in Sec. 4.3 that can enhance the regions with lower attention scores automatically."}, {"title": "4.2. Human-Parsing-guided Fusion Attention", "content": "At the lth layer of the Siamese network, we obtain embeddings from both the TargetNet and SourceNet, denoted as $F_l$ and $F'_l$ respectively. Our method generates a refined embedding $F$ by fusing $F_l$ with $F'_l$. For notational simplicity, we will omit the layer index $l$ in subsequent discussions.\nTarget-oriented source parsing selection. The given target pose $p_t$ contains the coordinates of the skeleton points and the categories of all points. We denote the category set of the target pose as $A_t = \\{a_i\\}$. Meanwhile, we have the category set of the parsing map $H_s$, and denote it as $A_s = \\{a_i\\}$. Each category $a_i \\in A_s$ corresponds to a mask indicating the category's region in the source image and is denoted as $M_a$. Then, we can get a mask map $M$ that is the combination of all categories contained in the target pose, that is, we have $M = \\bigcup \\{M_a | a \\in A_t\\}$.\nSelected-parsing-reweighed attention. Previous works in pose-guided generation methods [8, 33] have demonstrated that modifying self-attention can help regularize the identity of images. They normally concatenate $F$ and $F'$ and input them into the self-attention layers of TargetNet for fusion and then decompose. We argue that such a fusion method is not suitable for tasks involving significant pose transformations, as it can lead to distortion of image generation(more details in Sec. 5.3). This limitation arises from the inability to effectively guide the model's focus toward the interest region. In response, we propose human-parsing-guided fusion attention, which leverages binary mask M to reweight the embeddings of $F'$.\nSpecifically, we resize the $M$ using interpolation to match the corresponding size of $F' \\in \\mathbb{R}^{h \\times w \\times c}$. Then, we calculate the query, key, and value of $F'$ through $(Q', K', V') = (W_Q'F', W_K'F', W_V'F')$, and get the attention map $A' = (Q'K'^T)/\\sqrt{d}$ that indicates the focusing regions within $F'$. Then, we derive the mask weight matrix $M'$ based on the attention map $A'$:\n$\\begin{equation}\nM'_{i,j} = \\begin{cases}\n1 + \\delta & \\text{if } A'_{i,j} > 0 \\text{ and } M_{i,j} = 1 \\\\\n\\delta & \\text{if } A'_{i,j} \\leq 0 \\text{ and } M_{i,j} = 1 \\\\\n\\sigma & \\text{if } A'_{i,j} > 0 \\text{ and } M_{i,j} = 0 \\\\\n1 + \\sigma & \\text{if } A'_{i,j} \\leq 0 \\text{ and } M_{i,j} = 0\n\\end{cases} \\\\\n\\text{s.t., } i \\in \\{0,...,h\\}, j \\in \\{0,...,w\\},\n\\end{equation}$\nwhere $\\sigma > 0$ and $\\delta > 0$ are hyperparameters. Intuitively, the Eq. (6) assigns higher weights to the embeddings with higher attention through 1+$\\delta$ and 1+$\\sigma$ for the masked region and unmasked region, respectively. In our experiments, we empirically set $\\sigma$ and $\\delta$ as 0.3 and 0.6 to avoid overfitting and distortion of the background. Then, we compute the hidden states via reweighted attention :\n$H' = RwSelfAtt(Q', K', V', M'),\n= SoftMax((Q'K'^T/\\sqrt{d}) \\odot M')V'$.\nWe further fuse $H'$ and $F$ via cross attention:\n$\\begin{aligned}\nF &= CrossAtt(Q, K_h, V_h),\n&= SoftMax(QK_h/\\sqrt{d})V_h\n\\end{aligned}$\nwhere $(Q, K_h, V_h) = (W_QF, W_{H'}H', W_{VH'}H')$. Then, we can compare the attention map of the original embedding $F$ and the one of the new embedding $F$. As shown in Fig. 3, we find that the model focuses more on the areas indicated by the mask during the diffusion process, allowing for better feature extraction and effective pose transfer. We can insert the proposed attention module into different layers."}, {"title": "4.3. CLIP-guided Attention Alignment", "content": "Human-parsing-guided fusion attention facilitates the correction of the embedding F to highlight regions that would appear in the target image, but some regions still have low attention scores. To address this limitation, we propose leveraging CLIP embeddings of local image regions to further refine the attention in F.\nWe first split the attention map A' into M regions according to the parsing map H, and calculate the average attention scores of each parsing region. Then, we select K regions with the lowest average attention scores. We have the K selected CLIP embeddings $\\{F_{clip}^k\\}_{i=1}^{K}$ via Eq. (4), concatenate them, and get the embedding $F_{clip}$. We perform the cross-attention with F and output the final refined embedding\n$F = CrossAtt(Q, K_{clip}, V_{clip}),\n= SoftMax(QK_{clip}^T/\\sqrt{d})V_{clip},\n$\\text{where } Q, K_{clip}, V_{clip} = (W_{\\tilde{Q}}F, W_{K_{clip}} F_{clip}, W_{V_{clip}} F_{clip})$.\nThe emebdding $\\tilde{F}$ is calculated by Sec. 4.2 with Eq. (8). It can be observed that while $\\tilde{F}$ has extracted the majority of fine-grained features, some areas, particularly the face, remain underrepresented. A comparison between $\\tilde{F}$ and $\\overline{F}$ reveals that our use of CLIP embeddings effectively enhances features in these local regions.\nWe can insert the proposed attention module and the alignment module into different layers of the TargetNet and enhance the embeddings across multiple layers."}, {"title": "4.4. Implementation Details", "content": "Optimization and sampling. During the training process, we also adopt classifier-free guidance [6], which is a strategy widely used in diffusion models to enhance the quality and control of the generated images. To achieve that we set conditions C to 0 with a random probability of 7%. The final training loss function is rewritten as\n$\\mathcal{L} = \\mathbb{E}_{z_0, C, \\epsilon, t}(\\|\\epsilon - \\epsilon_{\\theta}(z_t, t, C, O_{\\beta})\\|_2)$.\nDuring inference, given a randomly initialized Gaussian noise, we utilize TargetNet (i.e., $\\epsilon_{\\theta}(.)$) to predict the noise at the tth timestep and employ classifier scale $\\omega$ to regulate the strength of guidance. The sampling formula is as follows:\n$\\hat{\\epsilon} = \\epsilon_{\\theta}(z_t, t, \\varnothing, O_{\\beta})+\\omega\\cdot(\\epsilon_{\\theta}(z_t, t, C, O_{\\beta}) - \\epsilon_{\\theta}(z_t, t, \\varnothing, O_{\\beta}))$,\nTarget latents can be obtained through multi-step denoising process, and mapping them back to the original image space we can acquire the target images.\nModel details. Our framework is derived by modifying the structure and weights of Stable Diffusion v1.5, based on the Hugging Face Diffusion library. PoseEncoder is a lightweight network with four convolutional layers. For training, we run on 4 NVIDIA A800 GPUs, with a batch size of 12 for 512 \u00d7 352 and batch size of 60 for 256 \u00d7 176. The training process consists of 50 epochs and uses the AdamW optimizer with a fixed learning rate of 1e\u22125. Classifer-free dropout probability \u03b7 is 30% and k is empirically set to 2. For inference, We employ the DDIM[7] scheduler with a sampling step of 35 and set classifier scale \u03c9 to 3.5."}, {"title": "5. Experiments", "content": "5.1. Setup\nDataset. We follow [15, 26] to conduct extensive experiments on In-Shop Clothes Retrieval benchmark from DeepFashion dataset and evaluated on 512\u00d7352 and 256\u00d7176 resolutions respectively. The dataset is comprised of 52,712 high-resolution images featuring clean backgrounds and diverse fashion models. Furthermore, we evaluate the model trained on the DeepFashion dataset using the high-resolution in-the-wild dataset WPose[10], which consists of 2,304 pairs. This dataset features a more diverse and complex range of backgrounds and human poses.\nObjective metrics. We use four commonly employed metrics, namely Structure Similarity Index Measure (SSIM) [31], Peak Signal to Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS) [36], and Fr\u00e9chet Inception Distance (FID) [5], to assess quality & fidelity of generated images. To better assess the authenticity and quality of the generations, we introduce the widely adopted LLM-based metric, Q-Align [32], which contains both Image Quality Assessment (IQA) and Image Aesthetic Assessment (IAA).\nSubjective metrics. We employ three subjective metrics in [1]: R2G [17], G2R [17], and J2b [1, 27]. R2G and G2R represent the ratios of real images classified as generated images and generated images classified as real images, respectively. Higher values for R2G and G2R indicate a greater similarity to real images. Meanwhile, J2b reflects user preferences by comparing the proportion of users selecting the best image from a group of images generated by different methods."}, {"title": "5.2. Quantitative and Qualitative Comparison", "content": "We conduct a comprehensive comparison with SOTA approaches including 13 methods encompass GAN-based, flow-based, attention-based and diffusion-based, along with a qualitative evaluation of the latest nine SOTA methods.\nQuantitative comparison. We conducted a comprehensive quantitative comparison of our method with 13 SOTAs across two datasets, with the results in Table 1. Upon analyzing these results, it is clear that our method significantly outperforms existing techniques across nearly all metrics, with the exception of the FID score at the resolution of 256\u00d7176. Notably, we achieved substantial improvements in metrics reflecting human preferences, such as LPIPS and LLM-based evaluations, compared to previous GAN-based and diffusion-based methods. This enhancement can be attributed to our proposed model's ability to better capture fine-grained clothing details, resulting in more realistic and higher-quality images. Furthermore, in the WPose dataset, which features complex backgrounds and poses, our method surpasses the two latest diffusion-based approaches(CFLD,PCDM) across all objective metrics. This finding underscores the superior generalization and robustness of our model in handling diverse and intricate scenarios. It is noteworthy that although our method has a slightly lower FID than PIDM, this may be attributed to PIDM's overfitting to the dataset distribution, as mentioned in previous studies [4, 15, 26].\nQualitative comparison. In Fig 4, we compare the visualization our method with nine SOTAs. We observe that: \u2460 Previous methods, whether GAN-based or diffusion-based, struggle to retain complex clothing patterns featuring designs or text, even in simple pose transformation scenarios, as demonstrated in rows 1-2. In contrast, our method significantly outperforms existing approaches in terms of clothing consistency by incorporating a mask-guided attention fusion mechanism, which effectively captures fine-grained details of garments. \u2461 In observing the more extreme pose transformation scenarios in rows 3, we attribute our ability to consistently generate images while effectively preserving the clothing details of the source image to the designed local region enhancement and mask-guided overlapping area attention mechanisms. Although the latest diffusion-based methods, such as CFLD and PCDM can generate a rough skeletal representation of the person, they either result in deformations or fail to retain fine-grained patterns. \u2462 The last three rows illustrate scenarios where the target pose necessitates the visualization of areas that are not visible in the source image. The results indicate that our method does not overfit and is capable of reasonably generating images of the unseen regions, while also demonstrating superior visual consistency compared to other approaches.\nUser study. To better illustrate the advantage of our method, we further conduct a user study with 30 computer science volunteers, following the experimental setup of PIDM [1]. As shown in Fig. 5, we have the following observations:\n\u2022 For the R2G and G2R metrics, we randomly select 30 images from each generation method and the real dataset to form a test dataset. Volunteers discern whether each image is generated or real. Our method significantly outperforms other methods in G2R metric, with over half of the images perceived as real, while no significant differences are found in R2G metric. We also conduct two Jab experiments. In one set, we select 30 images with complex patterns from each method. In the other set, we randomly select 30 images for fair comparison. Our method significantly outperformed others on both sets, achieving scores of 70.2 and 53.6 respectively. These results demonstrate that our method excels in preserving consistency in both complex and normal images while aligning more closely with human aesthetics."}, {"title": "5.3. Ablation Study", "content": "To demonstrate the effectiveness of our contributions, we conduct ablation study on DeepFashion with Table 2 and Fig. 6. B1 represents our adoption of pre-trained encoders (i.e. CLIP) to extract features from the source images like before methods. We design a stacket multi-layer perceptron to map the features to the shape of the l-th layer in TargetNet, thereby implementing the fusion mechanism proposed in our method. B2 utilizes the original architecture of [8], and fine-tuning it for 10 epochs on the DeepFashion dataset for a fair comparison. B3 and B4 stands for the removal of two proposed attention mechanism. The results of B1 in Sec. 3.2 indicate that directly utilizing CLIP fails to preserve clothing details. Furthermore, the naive concatenate fusion like B2 results in significant distortion of the characters during large-scale pose transformations. The proposed HPFA module incorporates parsing masks as constraints on attention, enhancing the capability to capture relevant features while minimizing distortions caused by irrelevant regions. Consequently, B3 achieves a higher SSIM score compared to B2, indicating improved structural consistency. Due to the presence of certain parsing regions that occupy a small proportion of the overall image or have low attention scores, resulting in poor detail generation, our proposed CAA module adaptively enhances features in local regions to achieve finer-grained generation, as illustrated in Fig. 6. In summary, the proposed modules demonstrate complementary effects compared to the baseline, achieving significant improvements in both quantitative and qualitative aspects."}, {"title": "5.4. Application", "content": "Our method inherits the flexibility of diffusion models, allowing for fashion-related tasks without additional training.\nStyle Transfer. We select the region of interest from the reference image $y^{ref}$ to obtain a binary mask m. During sampling, the relation $y_t = m \\odot y'_t + (1 - m) \\odot y_{ref}$ is applied to obtain the noise at each step t, where $y'_t$ and $y_{ref}$ represent the predicted noise for the source image and reference image at step t based on the reference image's pose. As shown in Fig 7, our method preserves the region of interest in $y^{ref}$ while generating image details that are visually consistent with the source image."}, {"title": "6. Conclusion", "content": "In this paper, we introduced a novel human-parsing-aware Siamese network framework for pose-guided person image synthesis (PGPIS) that effectively preserves both facial features and clothing patterns. Our approach comprises three key innovations: the introduction of a dual identical UNet architecture (SourceNet and TargetNet), a human-parsing-guided fusion attention module, and a CLIP-guided attention alignment module. Through extensive experiments on the in-shop clothes retrieval benchmark at multiple resolutions, our method significantly outperformed 13 SOTA baselines, successfully generating high-quality images that maintain both facial identity and intricate clothing details - including text patterns, regular textures, and irregular patterns - even under challenging pose transformations. This advancement addresses critical limitations of existing approaches and offers practical value for the retail industry by enabling the efficient generation of realistic product images in various poses while maintaining precise clothing details, thereby reducing the need for additional photo shoots."}]}