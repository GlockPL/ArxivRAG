{"title": "LEVERAGING SCENE GEOMETRY AND DEPTH INFORMATION FOR ROBUST IMAGE DERAINING", "authors": ["Ningning Xu", "Jidong J. Yang"], "abstract": "Image deraining holds great potential for enhancing the vision of autonomous vehicles in rainy conditions, contributing to safer driving. Previous works have primarily focused on employing a single network architecture to generate derained images. However, they often fail to fully exploit the rich prior knowledge embedded in the scenes. Particularly, most methods overlook the depth information that can provide valuable context about scene geometry and guide more robust deraining. In this work, we introduce a novel learning framework that integrates multiple networks: an Au- toEncoder for deraining, an auxiliary network to incorporate depth information, and two supervision networks to enforce feature consistency between rainy and clear scenes. This multi-network design enables our model to effectively capture the underlying scene structure, producing clearer and more accurately derained images, leading to improved object detection for autonomous vehicles. Extensive experiments on three widely-used datasets demonstrated the effectiveness of our proposed method.", "sections": [{"title": "1 Introduction", "content": "Image deraining is a critical preprocessing step in computer vision applications due to its significant impact on visual clarity and accuracy. Rain on images can obscure the visibility of objects, leading to substantial degradation in image quality. This can adversely affect the performance of object detection[1], recognition[2], and tracking algorithms[3], which are essential in various domains such as surveillance and navigation. In autonomous driving[4], clear vision is paramount for safety and robust decision-making; rain-induced artifacts can compromise the accuracy of perception systems, potentially leading to hazardous situations. Therefore, effective image deraining techniques are vital to enhance the reliability and functionality of vision-based systems.\nIn general, a rainy image can be represented as a superimposition of two layers: a clean image layer and a rain layer. The rain layer encompasses various artifacts such as rain streaks, raindrops, and fog, which make rain removal a challenging task. These rain-induced artifacts obscure objects and scenes, not only blurring visual data but also partially concealing critical features necessary for accurate image interpretation. Moreover, spatial factors further complicate the process. For objects are closer to the camera, rain is the primary occluding element, making its removal relatively easier. In contrast, distant objects are more challenging to recover due to additional occlusions from fog and other atmospheric conditions. This intricacy underscores the challenges of deraining in computer vision. Garg and Nayar[5] illustrated this phenomenon by demonstrating how the intensity of rain effects transitions into fog as the distance to the scene increases. Recent deep learning approaches for single-image rain removal have predominantly concentrated on the removal of rain streaks, often neglecting the broader physical characteristics of rain itself. The existing training datasets for rain removal typically include images featuring artificial rain streaks, raindrops, or a combination of both, with some datasets even containing indoor scenes. This limitation significantly hampers the effectiveness of these methods when"}, {"title": "2 Related Work", "content": "Image deraining methods can be broadly categorized into model-based methods[6, 7, 8] and deep learning methods[9, 10, 11, 12]. Model-based methods often approach deraining as a filtering problem, using various filters to restore a rain-free image. While this can effectively remove rain effects, it also tends to eliminate important structures within the image. Many model-based approaches develop various image priors based on the statistical properties of rain and clear images. These methods include image decomposition[3], low-rank representation[6, 13], discriminative sparse coding[14], and Gaussian mixture models[15]. Although these techniques have achieved improved results, they still struggle to effectively handle complex and varying rainy conditions.\nIn contrast, deep learning-based methods have significantly advanced image deraining by learning data-driven rep- resentations of rain and clear images. These approaches utilize powerful architectures and novel mechanisms to achieve superior performance. Early works such as [16] demonstrated substantial improvements in rain removal across benchmark datasets using convolutional neural networks (CNNs). Generative adversarial networks (GANs) [17] have also been employed to restore perceptually superior rain-free images, as demonstrated by [18]. The introduction of transformers, such as [19], enabled effective modeling of non-local dependencies, further enhancing image reconstruc- tion quality. Inspired by the success of recent diffusion models[20] in generating high-quality images, diffusion-based approaches[21] have shown great potential in improving image deraining performance across complex scenarios. Recent advancements include the integration of additional data modalities and novel priors into the learning process. For instance, Hu et al. [22] introduced depth information via an attention mechanism, achieving promising results on synthetic rainy datasets. Zhang et al. [9] exploited both stereo images and semantic information for improved image deraining performance. Guo et al. [23] proposed the use of Fourier priors to improve model generalization in rain removal tasks.\nIn summary, model-based methods have historically provided a solid foundation for image deraining, emphasizing handcrafted priors and optimization frameworks. However, their limitations in handling complex rainy conditions and preserving image details have led to a growing focus on deep learning approaches. Deep learning methods, driven by CNNs, GANs, transformers, and diffusion models, continue to achieve state-of-the-art results by leveraging large datasets, powerful architectures, and innovative priors. With the rapid evolution of data-driven techniques, deep learning is poised to dominate future advancements in image deraining, offering scalable solutions for complex and diverse real-world scenarios."}, {"title": "3 Methods", "content": "In this section, we introduce our multi-network approach for effective image deraining. The core of this framework is the Deraining AutoEncoder (DerainAE), which serves as the primary network for the deraining task. To enhance its performance, we introduce a supplementary Depth Network (DepthNet) that integrates depth information to assist in rain removal. Additionally, we utilize pretrained networks to provide supervisory signals, ensuring multiscale feature consistencies between clear and rainy images. The detailed architecture and loss functions are discussed subsequently."}, {"title": "3.1 Network architecture", "content": "For image deraining, the commonality between clear and rainy images lies in their depiction of the same scene, meaning the depth map should remain consistent between them. Apart from the rain artifacts, the feature map of the clear and rainy images should also be identical. Therefore, our approach employs two forms of supervision: one from the depth map and one from the feature map. This dual supervision ensures that the model not only learns to remove the rain but also retains the intrinsic features of the scene, leveraging the consistency between the depth and feature information to enhance the deraining process.\nOur DerainAE model adopts an autoencoder architecture to tackle the image deraining task by learning both the latent representation and the restored derained image. The autoencoder is designed to effectively capture the underlying structure and intrinsic features of rain-affected images through an encoding-decoding process. The encoder compresses the input image into a lower-dimensional latent space, extracting critical high-level information necessary for rain removal while filtering out irrelevant noise. The decoder then reconstructs the derained image from this latent representation, ensuring the preservation of essential details and textures. This dual functionality enables the model to efficiently map rain-degraded images to their rain-free counterparts."}, {"title": "3.2 Loss function", "content": "To jointly train DerainAE and DepthNet, we introduce a composite loss function that considers multiple complementary loss components. Building on the perceptual loss $L_{perceptual}$ proposed by Johnson et al. [24], we measure the discrepancy between clear images and corresponding rain images in a manner more consistent with human visual perception. Specifically, we utilize a pretrained VGG16 network to capture discrepancies at various feature levels, computed by Equation 1.\n$L_{perceptual} = \\sum \\lambda_{l} \\cdot |\\Phi_{l}(Y) \u2013 \\Phi_{l}(\\hat{Y})|^{2}$\nwhere $\\Phi_{l}$ denotes the activation map of the l-th layer in VGG16.\nWe employ cosine similarity losses (Equations 2 and 3) to measure the consistency of latent representations of clear images and corresponding rain images for both DerainAE and DepthNet.\n$L_{depth_consist} = cos(D_{R}, D_{C})$\n$L_{derain_consist} = cos(R_{L}, C_{L})$\nwhere cos(,) denotes the cosine similarity function.\nAdditionally, we use mean squared error (MSE) losses for reconstruction of the depth map D and the derained image C:\n$L_{derain} = MSE(\\hat{C}, C)$\n$L_{depth} = MSE(\\hat{D}, D)$\nThe loss function used to train our model is a weighted sum of these individual loss terms by Equation (6):\n$L = \\lambda_{1}L_{perceptual} + \\lambda_{2}L_{depth_{consist}} + \\lambda_{3}L_{derain_{consist}} + \\lambda_{4}L_{derain} + \\lambda_{5}L_{depth}$\nwhere $\\lambda_{1}, ..., \\lambda_{5}$ are hyperparameters that control the relative importance of each loss component during training. This hybrid loss function enables the joint optimization of DerainAE and DepthNet, ensuring robust performance across both deraining and depth estimation tasks."}, {"title": "4 Experimental results", "content": "In this section, we begin by introducing the datasets and evaluation metrics, followed by a discussion of the implemen- tation details and results. Ablation studies are conducted to evaluate the contributions of key components. Additionally, the effectiveness of our model is validated through an object detection task, highlighting the benefits of deraining for enhanced vision."}, {"title": "4.1 Datasets and evaluation metrics", "content": "Due to the challenge of obtaining paired rain and clear images, various rain models have been developed to synthetically generate rain streaks from clear images. In the linear model proposed by [15], the observed rain image $O \\in R^{MXN}$ is represented as a combination of a desired background layer $B \\in R^{M \\times N}$ and a rain streak layer $R \\in R^{M \\times N}$, such that $O = B + R$. Building upon this model, [25] proposed a more generalized formulation: $O = B + R\\tilde{R}$, where $\\tilde{R}$ is a new region-dependent variable that indicates the locations of individually visible rain streaks. The elements of $\\tilde{R}$ take binary values, with 1 indicating rain regions and 0 indicating non-rain regions. Further, [26, 22] modeled a rain image as a composite of a rain-free image, a rain layer, and a fog layer, formulating the observed rain image as below,\n$O = B(1 \u2013 R \u2013 F) + R + f_{o}F$\nwhere F is a fog layer, $f_{o}$ is the atmospheric light, which is assumed to be a global constant following[27]."}, {"title": "4.2 Implementation details", "content": "In model training, we set the hyperparameters $\\lambda1, \\lambda2, \\lambda3, \\lambda4, \\lambda5$ to [1, 0.5, 0.5, 10, 2], respectively. For the pretrained Variational Autoencoder (VAE) model, we adopt the VAE component from the Stable Diffusion framework [20], employing Mean Squared Error (MSE) as the loss function. The latent space of the VAE is sampled to produce a latent vector of the same size (length 150) as that used in our DerainAE model. During training, we keep the VAE model weights frozen and only fine-tune the final output layer. For depth reconstruction, we use the pretrained VGG16 model as the encoder, which is frozen during training, and train the decoder from scratch. Our entire model is implemented in PyTorch [32] and is trained on a workstation with a NVIDIA RTX A6000 GPU. All datasets in our experiments share the same training configuration: a batch size of 4, and the ADAM optimizer [33] with an initial learning rate of 5 \u00d7 10-3 and a weight decay of 0.9."}, {"title": "4.3 Evaluation on Different Datasets", "content": "Table 1 presents the evaluation metrics for the three datasets. The SSIM demonstrates that our model can restore most of the clear image's information, while the PSNR indicates better overall clarity in the predictions. Figure 3 shows results of exemplar images from the RainCityScapes and RainKITTI2012 datasets. It is clear that besides rain streaks, the foggy effect has been removed as well."}, {"title": "4.4 Comparsion with other methods", "content": "We evaluate two additional deraining models, DID-MDN [34] and PReNet [10], on the RainCityscapes testing dataset. For the DID-MDN model, we utilize the pretrained weights provided by the authors on GitHub. Since the DID-MDN model accepts an input size of 512 \u00d7 512, while RainCityscapes images are sized at 2028 \u00d7 1024, we resize the RainCityscapes images to 512 \u00d7 512 for processing, then resize the derained outputs back to the original resolution for evaluation. For PReNet, we leverage all the pretrained models available for Rain100H, Rain100L, and Rain1400 datasets, selecting the best-performing results on the RainCityscapes testing dataset as the final outputs. As seen in the Table 2, our model can perform better that other methods. Table 3 presents the inference times of DID-MDN, PReNet, and our method on an NVIDIA RTX A6000 GPU. As shown, our method achieves greater efficiency compared to the other approaches, attributed to its simpler backbone architecture."}, {"title": "4.5 Ablation studies", "content": "All ablation studies are performed on the RainCityscapes, RainKITTI2012, and RainKITTI2015 datasets. To evaluate the effectiveness of our model architecture, we calculate PSNR and SSIM on the respective testing sets. These metrics provide a quantitative assessment of the quality of the generated images, with higher PSNR and SSIM values indicating better image restoration and alignment with ground truth. By comparing different configurations of our model, referred"}, {"title": "GT depth and depth feature concatenation", "content": "Table 9 presents the results when both the ground truth depth map and depth feature concatenation are removed from the model during training. The performance is notably impacted across all datasets, as reflected by the lower PSNR and SSIM values compared to the full model."}, {"title": "4.6 Vehicle Detection", "content": "Image deraining can be integrated into outdoor vision systems to enhance object visibility during complex weather conditions, which is particularly beneficial for autonomous driving. By improving visibility, it can aid in critical tasks like vehicle detection and navigation, making autonomous vehicles safer and more reliable, especially in regions prone to heavy rainfall. For this evaluation, the focus is on detecting other vehicles in the scene from the ego vehicle perspective. We implemented YOLOv11[35] on both rainy and derained images. Figure 4 shows that derained images significantly improve vehicle detection accuracy on the RainKITTI2015 dataset. Similarly, Figure 5 demonstrates the ability of our model in enhancing vehicle detection performance under more challenging rainy scenarios in the RainCityscapes dataset, which closely approximate real-world rainy and foggy conditions.\nThe vehicle detection performance metrics are summarized in Table 10, showing that our deraining model significantly improves vehicle detection recall. This demonstrates enhanced visibility with significantly reduced false negative (missed) detections, which is critical for safe driving of autonomous vehicles, particularly in low-visibility environments."}, {"title": "5 Conclusions", "content": "In this study, we introduced a novel learning framework that integrates multiple networks, including an AutoEncoder for deraining, an auxiliary network to incorporate depth information, and two supervision networks to enforce feature consistency between rainy and clear scenes. Our approach demonstrates that even with a design based solely on simple convolutional layers, the integration of depth information and feature consistency constraints enables the network to produce high-quality derained images. Our method was evaluated on three public datasets, with results demonstrating its efficacy and robustness under diverse rainy conditions. Further, applying our model to an object detection task revealed significantly improved recall when using derained images. It is important to note that the primary focus of this work was not on identifying the optimal model architecture but rather on assessing the impact of different supervisory signals and training strategies. For future work, we plan to explore more advanced network architectures to further enhance deraining performance, particularly for autonomous driving applications."}]}