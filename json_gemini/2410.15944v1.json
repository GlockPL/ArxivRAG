{"title": "Developing Retrieval Augmented Generation (RAG) based LLM Systems from PDFs: An Experience Report", "authors": ["Ayman Asad Khan", "Md Toufique Hasan", "Kai Kristian Kemell", "Jussi Rasku", "Pekka Abrahamsson"], "abstract": "This paper presents an experience report on the development of Retrieval Augmented Generation (RAG) systems using PDF documents as the primary data source. The RAG architecture combines generative capabilities of Large Language Models (LLMs) with the precision of information retrieval. This approach has the potential to redefine how we interact with and augment both structured and unstructured knowledge in generative models to enhance transparency, accuracy and contextuality of responses. The paper details the end-to-end pipeline, from data collection, preprocessing, to retrieval indexing and response generation, highlighting technical challenges and practical solutions. We aim to offer insights to researchers and practitioners developing similar systems using two distinct approaches: OpenAI's Assistant API with GPT Series and Llama's open-source models. The practical implications of this research lie in enhancing the reliability of generative AI systems in various sectors where domain specific knowledge and real time information retrieval is important. The Python code used in this work is also available at: GitHub.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) excel at generating human like responses, but base AI models can't keep up with the constantly evolving information within dynamic sectors. They rely on static training data, leading to outdated or incomplete answers. Thus they often lack transparency and accuracy in high stakes decision making. Retrieval Augmented Generation (RAG) presents a powerful solution to this problem. RAG systems pull in information from external data sources, like PDFs, databases, or websites, grounding the generated content in accurate and current data making it ideal for knowledge intensive tasks.\nIn this report, we document our experience as a step-by-step guide to build RAG systems that integrates PDF documents as the primary knowledge base. We discuss the design choice, development of system, and evaluation of the guide, providing insights into the technical challenges encountered and the practical solutions applied. We detail our experience using both proprietary tools (OpenAI) and open-source alternatives (Llama) with data security, offering guidance on choosing the right strategy. Our insights are designed to help practitioners and researchers optimize RAG models for precision, accuracy and transparency that best suites their use case."}, {"title": "Background", "content": "This section presents the theoretical background of this study. Traditional generative models, such as GPT, BERT, or T5 are trained on massive datasets but have a fixed internal knowledge cut off based on their training data. They can only generate black box answers based on what they know, and this limitation is notable in fields where information changes rapidly and better explainability and traceability of responses is required, such as healthcare, legal analysis, customer service, or technical support."}, {"title": "What is RAG?", "content": "The concept of Retrieval Augmented Generation (RAG) models is built on integrating two core components of NLP: Information Retrieval (IR) and Natural Language Generation (NLG). The RAG framework, first introduced by Lewis et al. [5] combines dense retrieval methods with large scale generative models to produce responses that are both contextually relevant and factually accurate. By explicitly retrieving relevant passages from a large corpus and augmenting this information in the generation process, RAG models enhance the factual grounding of their outputs from the up-to-date knowledge.\nA generic workflow of Retrieval Augmented Generation (RAG) system, showcasing how it fundamentally enhances the capabilities of Large Language Models (LLMs) by grounding their outputs in real-time, relevant information is illustrated in the Fig[1]. Unlike static models which generate responses based only on closed-world knowledge, the RAG process is structured into the following key steps:\n1. Data Collection:\nThe workflow begins with the acquisition of relevant, domain specific textual data from various external sources, such as PDFs, structured documents, or text files. These documents represent raw data important for building a tailored knowledge base that the system will query during the retrieval process"}, {"title": "Data Preprocessing:", "content": "The collected data is then preprocessed to create manageable and meaningful chunks. Preprocessing involves cleaning the text (e.g., removing noise, formatting), normalizing it, and segmenting it into smaller units, such as tokens (e.g., words or group of words), that can be easily indexed and retrieved later. This segmentation is necessary to ensure that the retrieval process is accurate and efficient."}, {"title": "Creating Vector Embeddings:", "content": "After preprocessing, the chunks of data are transformed into vector representations using embedding models (e.g., BERT, Sentence Transformers). These vector embeddings capture the semantic meaning of the text, allowing the system to perform similarity searches. The vector representations are stored in a Vector Store, an indexed database optimized for fast retrieval based on similarity measures."}, {"title": "Retrieval of Relevant Content:", "content": "When a Query is input into the system, it is first transformed into a vector embedding, similar to the documents in the vector store. The Retriever component then performs a search within the vector store to identify and retrieve the most relevant chunks of information related to the query. This retrieval process ensures that the system uses the most pertinent and up-to-"}, {"title": "Augmentation of Context:", "content": "By merging two knowledge streams the fixed, general knowledge embedded in the LLM and the flexible, domain-specific information augmented on demand as an additional layer of context, aligns the Large Language Model (LLM) with both established and emerging information."}, {"title": "Generation of Response by LLM:", "content": "The context-infused prompt, consisting of the original user query combined with the retrieved relevant content is provided to a Large Language Model (LLM) like GPT, T5 or Llama. The LLM then processes this augmented input to generate a coherent response not only fluent but factually grounded."}, {"title": "Final Output:", "content": "By moving beyond the opaque outputs of traditional models, the final output of RAG systems offer several advantages: they minimize the risk of generating hallucinations or outdated information, enhance interpretability by clearly linking outputs to real-world sources, enriched with relevant and accurate responses.\nThe RAG model framework introduces a paradigm shift in Generative AI by creating glass-box models. It greatly enhanced the ability of generative models to provide accurate information, especially in knowledge-intensive domains. This integration has become the backbone of many advanced NLP applications, such as chatbots, virtual assistants, and automated customer service systems. [5]"}, {"title": "When to Use RAG: Considerations for Practitioners", "content": "Choosing between fine-tuning, using Retrieval Augmented Generation (RAG), or base models can be a challenging decision for practitioners. Each approach offers distinct advantages depending on the context and constraints of the use case. This section aims to outline the scenarios in which each method is most effective, providing a decision framework to guide practitioners in selecting the appropriate strategy."}, {"title": "Fine-Tuning: Domain Expertise and Customization", "content": "Fine-tuning involves training an existing large language model (LLM) on a smaller, specialized dataset to refine its knowledge for a particular domain or task. This method excels in scenarios where accuracy, tone consistency, and deep understanding of niche contexts are essential. For instance, fine-tuning has been shown to improve a model's performance in specialized content generation, such as technical writing, customer support, and internal knowledge systems.\nAdvantages: Fine-tuning embeds domain specific knowledge directly into the model, reducing the dependency on external data sources. It is particularly"}, {"title": "RAG: Dynamic Information and Large Knowledge Bases", "content": "Retrieval-Augmented Generation (RAG) combines LLMs with a retrieval mechanism that allows the model to access external data sources in real-time, making it suitable for scenarios requiring up-to-date or frequently changing information. RAG systems are valuable for handling vast knowledge bases, where embedding all the information directly into the model would be impractical or impossible.\nAdvantages: RAG is ideal for applications that require access to dynamic information, ensuring responses are grounded in real-time data and minimizing hallucinations. It also provides transparency, as the source of the retrieved information can be linked directly.\nDrawbacks: RAG requires complex infrastructure, including vector databases and effective retrieval pipelines, and can be resource-intensive during inference."}, {"title": "When to Use Base Models", "content": "Using base models(without fine-tuning or RAG) is appropriate when the task requires broad generalization, low-cost deployment, or rapid prototyping. Base models can handle simple use cases like generic customer support or basic question answering, where specialized or dynamic information is not required."}, {"title": "Understanding the Role of PDFs in RAG", "content": "PDFs are paramount for RAG applications because they are widely used for distributing high-value content like research papers, legal documents, technical manuals, and financial reports, all of which contain dense, detailed information essential for training RAG models. PDFs come in various forms, allowing access to a wide range of data types from scientific data and technical diagrams to legal terms and financial figures. This diversity makes PDFs an invaluable resource for extracting rich, contextually relevant information. Additionally, the consistent formatting of PDFs ensures accurate text extraction and context preservation, which is fundamental for generating precise responses. PDFs also include metadata (like author, keywords, and creation date) and annotations (such as"}, {"title": "Challenges of Working with PDFs", "content": "In RAG applications, accurate text extraction from PDFs is essential for effective retrieval and generation. However, PDFs often feature complex layouts such as multiple columns, headers, footers, and embedded images that complicate the extraction process. These complexities challenge RAG systems, which rely on clean, structured text for high-quality retrieval. Text extraction accuracy from PDFs decreases dramatically in documents with intricate layouts, such as multi-column formats or those with numerous figures and tables. This decline necessitates advanced extraction techniques and machine learning models tailored to diverse document structures.\nMoreover, the lack of standardization in PDF creation, including different encoding methods and embedded fonts, can result in inconsistent or garbled text, further complicating extraction and degrading RAG model performance. Additionally, many PDFs are scanned documents, especially in fields like law and academia, requiring Optical Character Recognition (OCR) to convert images to text. OCR can introduce errors, particularly with low-quality scans or hand-written text, leading to inaccuracies that are problematic in RAG applications, where precise input is essential for generating relevant responses. PDFs may also contain non-textual elements like charts, tables, and images, disrupting the linear text flow required by most RAG models. Handling these elements requires specialized tools and preprocessing to ensure the extracted data is coherent and useful for RAG tasks."}, {"title": "Key Considerations for PDF Processing in RAG Application Development", "content": "Processing PDFs for Retrieval Augmented Generation (RAG) applications requires careful handling to ensure high-quality text extraction, effective retrieval, and accurate generation. Below are key considerations specifically tailored for PDF processing in RAG development.\n1. Accurate Text Extraction:\nSince PDFs can have complex formatting, it is essential to use reliable tools and methods to convert the PDF content into usable text for further processing.\nAppropriate Tool for Extraction: There are tools and libraries for extracting text from PDFs for most popular programming languages (i.e: pdfplumber or PyMuPDF (fitz) for Python). These libraries handle most common PDF structures and formats, preserving the text's layout and structure as much as possible.\nVerify and Clean Extracted Text: After extracting text, always verify it for completeness and correctness. This step is essential for catching any extraction errors or artifacts from formatting."}, {"title": "Effective Chunking for Retrieval:", "content": "PDF documents often contain large blocks of text, which can be challenging for retrieval models to handle effectively. Chunking the text into smaller, contextually coherent pieces can improve retrieval performance.\nSemantic Chunking: Instead of splitting text arbitrarily, use semantic chunking based on logical divisions within the text, such as paragraphs or sections. This ensures that each chunk retains its context, which is important for both retrieval accuracy and relevance.\nDynamic Chunk Sizing: Adjust the chunk size according to the content type and the model's input limitations. For example, scientific documents might be chunked by sections, while other types of documents could use paragraphs as the primary chunking unit."}, {"title": "Preprocessing and Cleaning:", "content": "Preprocessing the extracted text is key for removing noise that could affect the performance of both retrieval and generative models. Proper cleaning ensures the text is consistent, relevant, and ready for further processing.\nRemove Irrelevant Content: Use regular expressions or NLP-based rules to clean up non-relevant content like headers, footers, page numbers, and any repeating text that doesn't contribute to the document's meaning.\nNormalize Text: Standardize the text format by converting it to lowercase, removing special characters, and trimming excessive whitespace. This normalization helps create consistent input for the retrieval models."}, {"title": "Utilizing PDF Metadata and Annotations:", "content": "PDFs often contain metadata (such as the author, title, and creation date) and annotations that provide additional context, which can be valuable for retrieval tasks in RAG applications.\nExtract Metadata: You can use tools specific to programming languages like PyMuPDF or pdfminer.six for Python to extract embedded metadata. This metadata can be used as features in retrieval models, adding an extra layer of context for more precise search results.\nUtilize Annotations: Extract and analyze annotations or comments within PDFs to understand important or highlighted sections. This can help prioritize content in the retrieval process."}, {"title": "Error Handling and Reliability:", "content": "Reliability in processing PDFs is essential for maintaining the stability and reliability of RAG applications. Implementing proper error handling and logging helps manage unexpected issues and ensures smooth operation.\nImplement Error Handling: Use try-except blocks to manage potential errors during PDF processing. This ensures the application continues running smoothly and logs any issues for later analysis."}, {"title": "Use Logging for Monitoring:", "content": "Implement logging to capture detailed information about the PDF processing steps, including successes, failures, and any anomalies. This is important for debugging and optimizing the application over time.\nBy following these key considerations and best practices, we can effectively process PDFs for RAG applications, ensuring high-quality text extraction, retrieval, and generation. This approach ensures that your RAG models are strong, efficient, and capable of delivering meaningful insights from complex PDF documents."}, {"title": "Study Design", "content": "This section presents the methodology for building a Retrieval Augmented Generation (RAG) system that integrates PDF documents as a primary knowledge source. This system combines the retrieval capabilities of information retrieval (IR) techniques with the generative strengths of Large Language Models (LLMS) to produce factually accurate and contextually relevant responses, grounded in domain-specific documents.\nThe goal is to design and implement a RAG system that addresses the limitations of traditional LLMs, which rely solely on static, pre-trained knowledge. By incorporating real-time retrieval from domain-specific PDFs, the system aims to deliver responses that are not only contextually appropriate but also up-to-date and factually reliable.\nThe system begins with the collection of relevant PDFs, including research papers, legal documents, and technical manuals, forming a specialized knowledge base. Using tools and libraries, the text is extracted, cleaned, and preprocessed to remove irrelevant elements such as headers and footers. The cleaned text is then segmented into manageable chunks, ensuring efficient retrieval. These text segments are converted into vector embeddings using transformer-based models like BERT or Sentence Transformers, which capture the semantic meaning of the text. The embeddings are stored in a vector database optimized for fast similarity-based retrieval.\nThe RAG system architecture consists of two key components: a retriever, which converts user queries into vector embeddings to search the vector database, and a generator, which synthesizes the retrieved content into a coherent, factual response. Two types of models are considered: OpenAI's GPT models, accessed through the Assistant API for ease of integration, and the open-source Llama model, which offers greater customization for domain-specific tasks.\nIn developing the system, several challenges are addressed, such as managing complex PDF layouts (e.g., multi-column formats, embedded images) and maintaining retrieval efficiency as the knowledge base grows. These challenges were highlighted during a preliminary evaluation process, where participants pointed out the difficulty of handling documents with irregular structures. Feedback from the evaluation also emphasized the need for improvements in text extraction and chunking to ensure coherent retrieval."}, {"title": "Results: Step-by-Step Guide to RAG", "content": "This section walks you through the steps required to set up a development environment for Retrieval Augmented Generation (RAG) on your local machine. We will cover the installation of Python, setting up a virtual environment and configuring an IDE (VSCode)."}, {"title": "Setting Up the Environment", "content": "Installing Python If Python is not already installed on your machine, follow the steps below:\n1. Download and Install Python\nNavigate to the official Python website: https://www.python.org/downloads/\nDownload the latest version of Python for your operating system (Windows, macOS, or Linux).\nDuring installation, ensure that you select the option Add Python to PATH. This is important to run Python from the terminal or command line.\nFor Windows users, you can also:\n\u2022 Click on Customize Installation.\n\u2022 Select Add Python to environment variables.\n\u2022 Click Install Now.\n2. Verify the Installation\nOpen the terminal (Command Prompt on Windows, Terminal on macOS/Linux).\nRun the following command to verify that Python is installed correctly: python --version\nIf Python is installed correctly, you should see output similar to Python 3.x.x."}, {"title": "Setting Up an IDE", "content": "After installing Python, the next step is to set up an Integrated Development Environment (IDE) to write and execute your Python code. We recommend Visual Studio Code (VSCode), however you are free to choose editor of your own choice. Below are the setup instructions for VSCode."}, {"title": "Download and Install VSCode", "content": "Visit the official VSCode website: https://code.visualstudio.com/.\nSelect your operating system (Windows, macOS, or Linux) and follow the instructions for installation."}, {"title": "Install the Python Extension in VSCode", "content": "Open VSCode.\nClick on the Extensions tab on the left-hand side (it looks like a square with four pieces).\nIn the Extensions Marketplace, search for Python.\nInstall the Python extension by Microsoft. This will allow VSCode to support Python code."}, {"title": "Setting Up a Virtual Environment", "content": "A virtual environment allows you to install libraries and dependencies specific to your project without affecting other projects on your machine.\n1. Open the Terminal in VSCode\nPress Ctrl + ` (or Cmd + ` on Mac) to open the terminal in VSCode. Alternatively, navigate to View - Terminal in the menu.\nIn the terminal, use the mkdir command to create a new folder for your project. For example, to create a folder named my-new-project, type: mkdir my-new-project\nUse the cd command to change directories and navigate to the folder where your project is located. For example: cd path/to/your/project/folder/my-new-project\n2. Create a Virtual Environment\nFor Windows, run the following commands: python -m venv my_rag_env  my_rag_env\\Scripts\\activate\nFor Mac/Linux, run the following commands: python3 -m venv my_rag_env  source my_rag_env/bin/activate\n3. Configure VSCode to Use the Virtual Environment\nOpen the Command Palette by pressing Ctrl + Shift + P (or Cmd + Shift + P on Mac).\nType Python: Select Interpreter in the Command Palette.\nSelect your virtual environment, my_rag_env, from the list."}, {"title": "Two Approaches to RAG: Proprietary and Open source", "content": "This section introduces a structured guide for developing Retrieval Augmented Generation (RAG) systems, focusing on two distinct approaches: using OpenAI's Assistant API (GPT Series) and an open-source Large Language Model (LLM) Llama and thus divided into two subsections [4.2.1][4.2.2]. The objective is to equip developers with the knowledge and practical steps necessary to implement RAG systems effectively, while highlighting common mistakes and best practices at each stage of the process. Each subsection is designed to provide practical insights into setup, development, integration, customization and optimization to generate well-grounded and aligned outputs.\nIn addition to the two primary approaches discussed in this guide there are several alternative frameworks and methodologies for developing Retrieval Augmented Generation (RAG) systems. Each of these options such as Cohere, AI21's Jurassic-2, Google's PaLM, and Meta's OPT have their merits and trade-offs in terms of deployment flexibility, cost, ease of use, and performance.\nWe have selected OpenAI's Assistant API (GPT Series) and Llama for this guide based on their wide adoption, proven capabilities, and distinct strengths in developing RAG systems. As highlighted in comparison Table[2] OpenAI's Assistant API provides a simple and developer-friendly black-box, allowing quick integration and deployment without the need for extensive model management or infrastructure setup with high quality outputs. In contrast, as an open-source model, Llama allows developers to have full control over the model's architecture, training data, and fine-tuning process, allowing for precise customization to suit specific requirements such as demand control, flexibility, and cost-efficiency. This combination makes these two options highly valuable for diverse RAG system development needs."}, {"title": "Using OpenAI's Assistant API : GPT Series", "content": "While the OpenAI Completion API is effective for simple text generation tasks, the Assistant API is a superior choice for developing RAG systems. The Assistant API supports multi-modal operations (such as text, images, audio, and video inputs) by combining text generation with file searches, code execution, and API calls. For a RAG system, this means an assistant can retrieve documents, generate vector embeddings, search for relevant content, augment user queries with additional context, and generate responses all in a seamless, integrated workflow. It includes memory management across sessions, so the assistant remembers past queries, retrieved documents, or instructions. Assistants can be configured with specialized instructions, behaviors, parameters other than custom tools that makes this API far more powerful for developing RAG systems.\nThis subsection provides a step-by-step guide and code snippets to utilize the OpenAI's File Search tool within the Assistant API, as illustrated in"}, {"title": "Environment Setup and API Configuration", "content": "Setting up your environment and configuring access to OpenAI's API is the foundational step.\n(a) Create an OpenAI Account.\n(b) Once logged in, navigate to the OpenAI API dashboard. Generate a New Project API Key.\n(c) Depending on your usage and plan, OpenAI may require you to set up billing information. Navigate to the Billing section in the dashboard to add your payment details. Refer to Appendix [7] for cost estimations.\nStore your API key securely. A .env file is used to securely store environment variables, such as your OpenAI API key.\n(a) Set Up a New Project Folder and Virtual Environment: First, create a new folder for your project. Ensure that a virtual environment is already set up in this folder, as described in section[4.1.3]."}, {"title": "Create a .env File:", "content": "Inside your new folder, make a file called .env. This file will store your OpenAI API key.\n(c) Add Your API Key: Open the .env file and paste your OpenAI API key in this format: OPENAI_API_KEY=your_openai_api_key_here\nBe sure to replace  with your actual API key.\n(d) Save the .env File: After adding your key, save the .env file in the same folder where you'll keep your Python files.\n(e) Install Necessary Python Packages: To make everything work, you need two tools: openai and python-dotenv. Open a terminal (or Command Prompt) and run this command to install them: pip install python-dotenv openai\nIf you need specific version of these tools used for the code in GitHub repository, you can install them like this: pip install python-dotenv==1.0.1 openai==1.37.2\n(f) Create the Main Python File: In the same folder, create a new file called main.py. All the code snippets attached in this entire section[4.2.1] should be implemented within this file.\nTo interact with the OpenAI API and load environment variables, you need to import the necessary libraries. The dotenv library will be used to load environment variables from the .env file."}, {"title": "Set OpenAI API Key and LLM", "content": "# Load environment variables from .env file load_dotenv()\n# Check if OPENAI_API_KEY is set openai_api_key = os.getenv(\"OPENAI_API_KEY\") if not openai_api_key: raise EnvironmentError(\"Error: OPENAI_API_KEY is not set in the environment. Please set it in the .env file.\")\n# Set OpenAI key and model openai.api_key = openai_api_key client = openai.OpenAI(api_key=openai.api_key) model_name = \"gpt-4o\" # Any model from GPT series"}, {"title": "Understanding the Problem Domain and Data Requirements", "content": "To develop an effective solution for managing and retrieving information, it's important to understand the problem domain and identify the specific data requirements and not just provide any data. For a deeper insight into the challenges of handling PDFs, refer to Section[2.3.1]. Given that this paper focuses on working with PDFs, it is important to emphasize the significance of having relevant and clean data within these documents.\nOrganize the Knowledge Base Files: After selecting the PDF(s) for your external knowledge base, create a folder named Upload in the project directory, and place all the selected PDFs inside this folder."}, {"title": "Upload PDF(s) to the OpenAI Vector Store", "content": "def upload_pdfs_to_vector_store(client, vector_store_id, directory_path):\n try:\n if not os.path.exists(directory_path):\n raise FileNotFoundError(\"Error: Directory '{directory_path}' does not exist.\")\n if not os.listdir (directory_path):\n raise ValueError(\"Error: Directory '{directory_path}' is empty. No files to upload.\")\n file_ids = {}\n # Get all PDF file paths from the directory\n file_paths = [os.path.join(directory_path, file)  for file in os.listdir (directory_path) if file .endswith(\".pdf\")]\n # Check if there are any PDFs to upload\n if not file_paths:\n raise ValueError(\"Error: No PDF files found  in directory '{directory_path}'.\")\n # Iterate through each file and upload to vector store\n for file_path in file_paths:\n file_name = os.path.basename(file_path)\n # Upload the new file\n with open(file_path, \"rb\") as file:\n uploaded_file = client.beta.vector_stores.files.upload(vector_store_id= vector_store_id, file=file)\n print (f\"Uploaded file: {file_name} with ID : {uploaded_file.id}\")\n file_ids[file_name] = uploaded_file.id\n print (f\"All files have been successfully uploaded to vector store with ID: {vector_store_id}\")\n return file_ids\n except Exception as e:\n print(\"Error uploading files to vector store: {e}\")\n return None"}, {"title": "Creating and Managing Vector Stores in OpenAI", "content": "OpenAI Vector stores are used to store files for use by the file search tool in Assistant API. This step involves initializing a vector store for storing vector embeddings of documents and retrieving them when needed."}, {"title": "Initialize a Vector Store for RAG", "content": "# Get/Create Vector Store def get_or_create_vector_store(client, vector_store_name):\n if not vector_store_name:\n raise ValueError(\"Error: 'vector_store_name' is not set. Please provide a valid vector store name.\")\n try:\n # List all existing vector stores\n vector_stores = client.beta.vector_stores.list()\n # Check if the vector store with the given name already exists\n for vector_store in vector_stores.data:\n if vector_store.name == vector_store_name:\n print (f\"Vector Store '{vector_store_name}' already exists with ID: {vector_store .id}\")\n return vector_store\n # Create a new vector store if it doesn't exist\n vector_store = client.beta.vector_stores.create( name=vector_store_name)\n print (f\"New vector store '{vector_store_name}' created with ID: vector_store.id}\")\n # Upload PDFs to the newly created vector store ( assuming 'Upload' is the directory containing PDFs)\n upload_pdfs_to_vector_store(client, vector_store. id, 'Upload')\n return vector_store\n except Exception as e:\n print (f\"Error creating or retrieving vector store: {e}\")\n return None"}, {"title": "Creating Vector Store Object", "content": "vector_store_name = \"\" # Ensure this is set to a valid name vector_store = get_or_create_vector_store(client, vector_store_name)"}, {"title": "Creating Assistant with Specialized Instructions", "content": "After setting up the vector store, the next step is to create an AI assistant using the OpenAI API. This assistant will be configured with specialized instructions and tools to perform RAG tasks effectively. Set the assistant name, description and instructions properties accordingly. Refer to the"}, {"title": "Create and Configure Assistant", "content": "# Get/Create Assistant def get_or_create_assistant(client, model_name, vector_store_id):\n assistant_name = \"\" # Ensure this is set to a valid name description = \"\" # Ensure Purpose of Assistant is set here instructions = \"\" # Ensure Specialized Instructions for Assistant and Conversation Structure is set here)\n try:\n assistants = client.beta.assistants.list() for assistant in assistants.data:\n if assistant.name == assistant_name:\n print(\"AI Assistant already exists with ID :\" + assistant.id)\n return assistant\n assistant = client.beta.assistants.create( model=model_name, name=assistant_name, description=description, instructions=instructions, tools= [{\"type\": \"file_search\"}], tool_resources= {\"file_search\": {\" vector_store_ids\": [vector_store_id]}},\n temperature=0.7, # Temperature for sampling top_p=0.9 # Nucleus sampling parameter )\n print(\"New AI Assistant created with ID:\" + assistant.id)\n return assistant\n except Exception as e:\n print (f\"Error creating or retrieving assistant: {e }\")\n return None\n assistant = get_or_create_assistant (client, model_name, vector_store.id)"}, {"title": "Creating Conversation Thread", "content": "Creating a thread, initializes a context-aware conversation session where the AI assistant can interact with the user, retrieve relevant information from the vector store, and generate responses based on that context. Additionally, tool resources can be attached to the Assistant API threads that are made available to the assistant's tools in this thread.\nThis capability is essentially important when there is a need to use the same AI assistant with different tools for different threads. They can be dynamically managed to suit the requirements for topic-specific threads, reusing the same Assistant across different contexts or overwriting assistant tools for a specific thread."}, {"title": "Initialize a thread for conversation", "content": "# Create thread thread_conversation = {\n \"tool_resources\": {\n \"file_search\": {\n \"vector_store_ids\": [vector_store.id]\n }\n }\n }"}, {"title": "Initiating a Run", "content": "A Run represents an execution on a thread. This step involves sending user input to the assistant, which then processes it using the associated resources, retrieves information as needed, and returns a response that could include dynamically fetched citations or data from relevant documents.\nThis following code allows a user to ask questions to an assistant in a loop. It sends the user's question, waits for the assistant to think and respond, and then displays the response word by word. The process repeats until the user types \"exit\" to quit."}, {"title": "Interact with the LLM", "content": "# Interact with assistant while True:\n user_input = input(\"Enter your question (or type 'exit', to quit): \") if user_input.lower() == 'exit':\n print(\"Exiting the conversation. Goodbye!\") break\n # Add a message to the thread with the new user input\n message_conversation = {\n \"role\": \"user\",\n \"content\": [\n {\n \"type\": \"text\",\n \"text\": user_input\n }\n ]\n }\n message_response = client.beta.threads.messages.create(thread_id=message_thread.id, ** message_conversation)\n run = client.beta.threads.runs.create( thread_id=message_thread.id, assistant_id=assistant.id )"}, {"title": "Using Open-Source LLM Model: Llama", "content": "We will utilize Ollama, an open-source framework that implements the Llama model, to incorporate Llama-based question generation capabilities within our application. By employing Ollama, we can process user input and generate contextually relevant questions directly through the terminal, offering an efficient and scalable solution for natural language processing tasks in a local environment. This integration will enable seamless question generation without relying on external API services, ensuring both privacy and computational efficiency."}, {"title": "Install the following Python libraries", "content": "pip install pymupdf langchain-huggingface faiss-cpu pip install oLlama sentence-transformers sentencepiece pip install langchain-community"}, {"title": "Converting PDFs to Text Files", "content": "Save this script as pdf_to_text.py. This script converts PDF files in a given folder into text files. You have to create one folder at the same directory. The folder name should be Data. You have to keep your PDF files in this Data folder for the further process. Check the GitHub Link of the Code."}, {"title": "pdf_to_text.py", "content": "import os import fitz # PyMuPDF for reading PDFs\n def convert_pdfs_to_text(pdf_folder, text_folder):\n if not os.path.exists(text_folder):\n os.makedirs (text_folder)\n for file_name in os.listdir(pdf_folder):\n if file_name.endswith(\".pdf\"):\n file_path = os.path.join(pdf_folder, file_name )"}, {"title": "Creating the FAISS Index", "content": "Save this script as txt_to_index.py. This script generates a FAISS index from the text files in the DataTxt folder. Check the GitHub link for the Code."}, {"title": "txt_to_index.py", "content": "import os from langchain_huggingface import HuggingFaceEmbeddings"}]}