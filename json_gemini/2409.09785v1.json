{"title": "LARGE LANGUAGE MODEL BASED GENERATIVE ERROR CORRECTION:\nA CHALLENGE AND BASELINES FOR SPEECH RECOGNITION, SPEAKER TAGGING,\nAND EMOTION RECOGNITION", "authors": ["Chao-Han Huck Yang", "Taejin Park", "Yuan Gong", "Yuanchao Li", "Zhehuai Chen", "Yen-Ting Lin", "Chen Chen", "Yuchen Hu", "Kunal Dhawan", "Piotr \u017belasko", "Chao Zhang", "Yun-Nung Chen", "Yu Tsao", "Jagadeesh Balam", "Boris Ginsburg", "Sabato Marco Siniscalchi", "Eng Siong Chng", "Peter Bell", "Catherine Lai", "Shinji Watanabe", "Andreas Stolcke"], "abstract": "Given recent advances in generative AI technology, a key\nquestion is how large language models (LLMs) can enhance\nacoustic modeling tasks using text decoding results from\na frozen, pretrained automatic speech recognition (ASR)\nmodel. To explore new capabilities in language modeling\nfor speech processing, we introduce the generative speech\ntranscription error correction (GenSEC) challenge. This chal-\nlenge comprises three post-ASR language modeling tasks: (i)\npost-ASR transcription correction, (ii) speaker tagging, and\n(iii) emotion recognition. These tasks aim to emulate future\nLLM-based agents handling voice-based interfaces while\nremaining accessible to a broad audience by utilizing open\npretrained language models or agent-based APIs. We also\ndiscuss insights from baseline evaluations, as well as lessons\nlearned for designing future evaluations.", "sections": [{"title": "1. INTRODUCTION", "content": "Early statistical ASR systems based on the noisy channel\nmodel were conceived as utilizing two model components:\nacoustic model and language model (LM) [1]. First-pass\ndecoding results could be subjected to postprocessing, or\nrescoring, to apply more powerful LMs or additional knowl-\nedge sources [2]. With the introduction of end-to-end (E2E)\nASR systems in the early 2020s, language modeling for post-\nASR has become more complex, e.g., by also modeling the\nimplicit internal LM of E2E ASR systems [3-6]). With the\nadvent of LLMs, however, post-ASR processing has become\nvery attractive again, given the capacity of LLMs to model\nlinguistic patterns, contextual influences, and even world\nknowledge as reflected in language.\nMore recently, LLMs and speech/language-model align-\nment methods have sparked considerable interest in new\nmethods, such as cascaded LLM correction [7], for ASR and\nspeech translation. LLM-based text-to-text generative error\ncorrection [7,8] has shown accuracy improvements over base-\nline rescoring methods, even surpassing n-best oracle perfor-\nmance, by bringing external knowledge to bear in ASR [9],\nspeech translation [10], and image captioning [11, 12].\nWhile a text-based ASR-LLM interface limits the richness\nof information utilized in post-processing, such as acoustic-\nprosodic expressions of speaker identity and paralinguistic\nproperties, ASR outputs will still be sensitive to such infor-\nmation, especially when multiple hypotheses are output, ef-\nfectively providing a text-based feature map that weakly re-\nflects acoustic information [7].\nInspired by this observation, as well as by the early LLM-\nbased studies cited, our challenge task aims to push research\nin two directions: (1) how large performance gains could be\nachieved by applying cascaded ASR-LLMs, and (2) how well\ncascaded LLMs could perform tasks beyond word-level tran-\nscription, such as recovering speaker and paralinguistic infor-\nmation. In other words, by leveraging LLMs, even text-only\noutput from first-pass ASR system (such as available from\na black-box API) might be enriched with paralinguistic and\nmeta-information that is commonly thought to be encoded\nprincipally in acoustic-prosodic features. Fig. 1 illustrates the\nthree challenge tasks, highlighting the assumption that ASR\nhypotheses in textual form contain sufficient implicit acoustic\ninformation to perform these tasks.\nThe short-term goal of the challenge is to introduce new\nASR-LM tasks to the speech community that leverage the lat-\nest developments in LLM-based post-ASR text modeling, po-\ntentially benefiting the design of voice-interface LLM agents\nthat use only text-based encodings. Through this initiative, we"}, {"title": "2. BACKGROUND", "content": "This challenge considers an agent-based LLM application\nscenario with a fixed ASR interface. The focus is on char-\nacterizing how LLMs can enhance speech processing by\nleveraging the textual N-best hypotheses without explicit en-\ncoding of acoustic information. Additionally, we encourage\nparticipants to \"push the limits of language modeling for\nASR\" given a setup based on a black-box ASR interface,\nwhich would be readily accessible through APIs, at modest\ncost.\nThe success of probabilistic language modeling systems\nin ASR can be traced back to several influential tools and\nframeworks, including SRILM [13], CNTK [14], and the LM\ncomponents within Kaldi [15]. These tools have significantly\ncontributed to the development and enhancement of speech\nrecognition technology by providing robust methods for han-\ndling the complexities of speech processing. LLMs, on the\nother hand, have also benefited from democratized model in-"}, {"title": "2.1. Post-ASR Text Modeling", "content": "ference (e.g., Claude) and open-source models (e.g., LLaMA)\nto establish end-to-end agent learning-based interfaces, such\nas AudioGPT [16]. For instance, work on task-activating\nprompting (TAP) [7] illustrates that instruction-prompted\nLLMS for ASR can correct recognition errors by inferring\nphonetic confusions or grammatical variants from the ASR-\ndecoded text.\nTo investigate this form of ASR-LLM pipeline, we in-\ntroduce three tasks based on ASR-decoded text, which have\nbeen studied previously and were shown to benefit from com-\nbined acoustic and language modeling. In this challenge, par-\nticipants can explore a training-free setup by optimizing in-\nstruction prompts for the speech tasks, or by hosting LLMS\nin their own compute environment. To examine the limits\nof the text-only modality for speech processing, we limit the\nfirst SLT challenge to text-to-text modeling without access\nto acoustic embeddings. The acoustic information will be\naccessed through ASR hypotheses ranked by acoustic confi-\ndence scores and error word-level or utterance-level error at-\ntributions. We hope this simple setup will entice researchers\nwithout a speech background to become active in speech pro-\ncessing through language modeling."}, {"title": "2.2. Open Topics in LLM-based Speech Modeling", "content": "To avoid test set data that may have leaked into the pretrained\nLLMs, we prepare a non-public test set for each challenge\nsubtask. While LLMs hold promise for post-ASR correction,\nthey are not without problems. One concern is the potential\nfor introducing biases reflected in the training data, which\ncould affect the accuracy and fairness of the corrected tran-\nscripts. Additionally, LLMs could produce enriched text that\ndiverges from the intended meaning or introduces new types\nof errors, necessitating novel error analysis methodologies.\nThese potential issues highlight the need for ongoing research"}, {"title": "3. CHALLENGE DESCRIPTION", "content": "The GenSEC challenge at IEEE SLT 2024 consists of three\ntasks for post-ASR language modeling:\n\u2022 Task 1: Post-ASR Output Correction by LLM\nThe goal of this task is to map from N-best Hypotheses to\nground Truth transcriptions (H2T), similar to the setup in\nYang et al. [7]. The training set includes recognition scores\nfrom various pretrained end-to-end ASR models and N-\nbest hypotheses. Participants are allowed to use N-best hy-\npotheses and their scores for re-ranking or generative cor-\nrection to produce final transcriptions.\n\u2022 Task 2: Post-ASR Speaker Tagging Correction\nThis task aims at correcting the speaker tags in the out-\nput of a speaker-attributed (multi-speaker) ASR system.\nSpeaker tagging in Task 2 refers to the speaker indices or\nanonymized speaker names (e.g., \"speaker-A\", \"speaker-\n2\") used to identify who spoke which words. We will\nprovide errorful speaker-attributed transcripts produced\nby a multi-speaker ASR system. Participants in Task 2\nare asked to submit corrected versions of the transcripts\nwith accurate speaker tagging. A metric that gauges both\nspeaker tagging and ASR accuracy will be used for evalu-\nation. Similar to the other tasks, the current version of the\nTrack-2 challenge allows use of the text modality only.\n\u2022 Task 3: Post-ASR Speech Emotion Recognition\nThis task aims to achieve utterance-level speech emotion\nrecognition (SER) based on errorful ASR transcripts. Par-\nticipants will develop ASR error correction methods com-\nbined with traditional deep-learning-based SER models,\ndesign novel prompt templates utilizing LLMs for SER,\nor utilize any other methods based on text input. Partici-\npants are encouraged to use the conversation as context to\npredict the emotion of a target utterance."}, {"title": "4. TASK DESCRIPTION", "content": "Background: Language model (LM) rescoring has been\nemployed widely and for a variety of ASR technologies to"}, {"title": "4.1. Task 1 on LLM for Post-ASR Output Correction", "content": "improve ASR results, usually achieving good performance\ngains [2, 17-19]. In this approach, an external LM is trained\nseparately and used to re-score the N-best hypotheses gener-\nated by the ASR system. While text error correction (TEC)\nhas been explored [7, 20], ASR error correction is distinct\ndue to the variability and distinct patterns of spoken lan-\nguage [21]. Neural models have been used widely with E2E\nmodels for text error correction or normalization [22-24].\nThese models often use beam search to generate new esti-\nmates, and can usually handle text normalization and denor-\nmalization of spelling errors.\nMotivation: As shown in Fig. 2, with Task 1 we aim to\nexplore the limits of ASR-LLM error correction, as well as\nhow best to utilize the ambiguity conveyed by N-best output.\nN-best dataset: The N-best open source corpus HyPo-\nradise [9] will be made open-source under the MIT license.\nThis includes HyPoradise training sets (316.8k pairs), de-\nvelopment sets such as Librispeech-test-clean (2.6k pairs)\nand WSJ-dev93 (503 pairs), and evaluation sets including\nLibrispeech-test-other (2.9k pairs) and WSJ-dev93 (333\npairs).\nBaseline: We provide pretrained 1st-pass and 2nd-pass\nmodels. The details of existing engineering pipelines are\nlisted below. Training code has been released\u00b9 and the pre-\ntrained LLaMA2-7B model2 has been released.\nEvaluation: The challenge participants are allowed to ap-\nply their own 2nd-pass model to the provided ASR hypothe-\nses decoded by beam search using Whisper.\nThe WER of the corrected hypotheses is used for eval-\nuation. This WER is compared to two \"oracle\" WERs cal-\nculated from the N-best inputs, namely, 1) the lowest WER\nachievable by picking the best hypothesis from each N-best\nlist, and 2) the compositional oracle method ocp: the achiev-\nable WER using \"all tokens\" in the N-best hypothesis list.\nThe former can be viewed as a lower bound on re-ranking\nmethods, while the latter denotes the lower bound using ele-\nments already occurring in the list. To understand the effect of\ntext normalization (punctuation and capitalization, P&C) on\nASR performance, both normalized and unnormalized (P&C)\nWERs are reported."}, {"title": "4.2. Task 2: Post-ASR Speaker Tagging Correction", "content": "Background: While the use of lexical cues in speaker di-\narization, speaker turn detection, and speaker segmentation\nhas been explored previously, it is still less commonly used\nthan acoustic-only speaker diarization. Early studies in this\narea, such as those presented in [25, 26], utilized linguistic\npatterns to identify speakers during the diarization process.\nSeveral studies have enhanced speaker segmentation and\nclustering accuracy by integrating ASR output to leverage\nlexical cues [27-29]. Furthermore, lexical cues can be incor-\nporated into speaker diarization by combining speaker turn\nprobabilities based on both audio and text during the clus-\ntering phase [30]. Alternatively, spoken words and speaker\nchannels can be decoded jointly, thus utilizing lexical cues\nimplicitly for diarization [31,32].\nMore recently, the study presented in [33] introduced\nsemantic information through neural embeddings generated\nby a spoken language processing (SLP) unit. Subsequently,\na multimodal (audio-text) speaker change detector was pro-\nposed [34], along with a speaker error correction (SEC)\nsystem [35] based on a pretrained language model.\nDue to the recent popularity of LLMs, the multi-speaker\nASR and speaker diarization community has also begun em-"}, {"title": "", "content": "ploying LLMs to enhance performance. One framework es-\ntablished for this purpose fine-tunes PaLM 2-S [36] to cor-\nrect speaker diarization errors from GCP's Universal Speech\nModel [37]", "39].\nMotivation": "As discussed for Task 1", "system.\nDatasets": "The DiPCo [41]", "Huggingface.3\nBaseline": "We generated the speaker-annotated transcripts\nfrom the system proposed in [45] as a baseline system"}, {}, {"title": "", "content": "Table 3 shows the accuracy of the baseline for development\n3https://huggingface.co/datasets/GenSEC-LLM/\nSLT-Task2-Post-ASR-Speaker-Tagging\n4https://github.com/tango4j/llm_speaker_tagging"}, {"title": "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]", "content": ""}, {"title": "Fig. 4. Dataflow for the Task 2 baseline. Note that the\nacoustic-only diarization probability values are set to fixed\nvalues.", "content": ""}, {"title": "", "content": "Evaluation: We employ concatenated minimum permu-\ntation word error rate (cpWER), as presented in [47]. \u0441\u0440-\nWER is calculated by concatenating the speaker-wise tran-\nscripts for every label permutation and selecting the permuta-\ntion that results in the lowest WER, using the open-source and\npublicly available MeetEval [48] multi-speaker ASR evalua-\ntion toolkit. Additionally, we provide a Hugging Face-style\nleaderboard for challenge participants to upload and evalu-\nate their submissions."}, {"title": "4.3. Task 3: Post-ASR Speech Emotion Recognition", "content": "Background: Text-based SER has advanced significantly\nover the past decade. However, its use in real-world applica-"}, {"title": "", "content": "tions remains rare. One reason is that the majority of SER\nresearch relies on human annotation, i.e., manual transcripts.\nIn contrast, even for elicited emotion corpora, transcripts from\na state-of-the-art ASR system can result in high WERs [49],\nmeaning that few findings obtained in the lab can be repli-\ncated in the wild. Moreover, SER on ASR transcripts is an\nunderstudied topic. Traditionally, researchers have consid-\nered confidence scores of recognized words [50], ASR error\ncorrection [51, 52], as well as fusion with audio informa-\ntion [53, 54] to mitigate the side effects of ASR errors. Still,\nthere is a lack of comprehensive studies covering diverse\nsituations (i.e., corpora, metrics, WERs, fusion techniques).\nWith the rise of LLMs, it has become feasible to perform\nSER on ASR transcripts with simple prompting [55]. This\nemerging approach, however, has not been established as a\nreliable solution given the uneven performance with different\nprompting templates and the general lack of explainability of\nLLM outputs.\nMotivation: Using text input only, without access to\nacoustic information, is a good starting point for explor-\ning LLMs for SER, especially given that most LLMs are\ntext-based. Insights gained about text-based SER, including\nhandling of ASR errors, can be a foundation for future evalu-\nations incorporating acoustic features. Potential future tasks\ncould include ASR-error-robust multimodal fusion, enhanc-\ning word embeddings from ASR transcripts with discrete\nspeech units, and developing ASR-integrated multimodal\nLLMs based on spoken language.\nDataset: We use the public IEMOCAP dataset [56].\nSpeech transcripts from eleven ASR models (Wav2vec2,\nHuBERT, Whisper, etc.) are provided for each audio seg-\nment [57]. We ask participants to predict four emotion\nclasses: angry, happy (combined with excited), neutral, and\nsad, for each segment. All segments are presented in the order"}, {"title": "", "content": "5https://huggingface.co/spaces/GenSEC-LLM/task2_\nspeaker_tagging_leaderboard"}, {"title": "Fig. 5. Example of a data entry of Task 3.", "content": ""}, {"title": "", "content": "of the conversation based on the timestamp. Participants are\nencouraged to use conversational context to improve emotion\nprediction.\nAn exemplary data entry is shown in Fig. 5, where\nneed prediction indicates whether this utterance should be\nincluded in the prediction procedure. \"yes\" denotes the utter-\nances labeled with the four emotion classes and \"no\" denotes\nall other utterances. Note that we have removed the utterances\nthat have no human annotations. The key emotion indicates\nthe emotion label of the utterance. The key id indicates the\nutterance ID, which is also the name of the audio file in\nIEMOCAP dataset. The ID is exactly the same as the raw ID\nin IEMOCAP. The key speaker indicates the speaker of the\nutterance. The key groundtruth indicates the original human\ntranscription provided by IEMOCAP while the remaining ten\nkeys indicate the ASR transcription generated by the various\nASR models.\nBaseline: We provide two performance baselines with\nASR transcripts from Whisper-tiny used as the text input:\none with an LLM-based approach using GPT-3.5-turbo, the\nother with a traditional approach based on a deep learning\nmodel. For the GPT-3.5-turbo approach, we performed zero-\nshot prediction with a context window of three (only previous\nutterances allowed), with code available to participants as\na reference. For the deep learning-based model, a two-layer\nfeed-forward network was trained following the standard five-\nfold cross-validation of IEMOCAP. The first layer encodes\nROBERTa output of dimension 768 into hidden states of di-\nmension 128, and the second further encodes it into a dimen-\nsion of 16. ReLU is used as the activation function between\nthe layers. The dataset statistics and our baseline results are\ngiven in Table 4.\nFor the method based on GPT3.5-turbo, the accuracy is\n44.70% on the training set and 55.18% on the test set. This"}, {"title": "", "content": "significant discrepancy is reasonable since the training set\ncontains scripted dialogs (which may not align with emotion\nlabels), while the test set is more spontaneous. For the tra-\nditional deep learning approach, the accuracy is 62.34% on\nthe training set and 51.08% on the test set. This discrepancy\nis also plausible, considering duplicate textual scripts in the\ntraining set, which results in overlap between the training and\ndevelopment subsets of the training set.\nEvaluation: We use unweighted four-class accuracy\n(number of correctly predicted samples / total number of\nsamples). We will release a training set and a test set. Par-\nticipants can use the training set to develop their methods\nand tune hyperparameters. The test set does not come with\nemotion labels or ground-truth transcription and is strictly\ndisallowed for use in model development. We will rank the\nmodels based on their accuracy on the test set, but will also\nfurther evaluate the models with a separate unpublished test\nset to assess generalization. Participants are free to use any\nLLM (such as GPT or LLaMA) or non-LLM methods (tradi-\ntional text-based emotion classifiers) based on the provided\nASR transcriptions. For fairness considerations, participants\nshould not use any audio data (including audio waveforms or\nacoustic features) or transcribe speech using their own ASR\nmodel. Participants are allowed to use additional training\ndatasets, as long as they are specified and publicly available,\nbut they must not include IEMOCAP. To encourage innova-\ntion, we do not place any other restrictions on the methods\nused, as long as they are automated."}, {"title": "5. CONCLUSION", "content": "We have created the GenSEC challenge to probe the capa-\nbilities of large language models for post-processing of ASR\noutputs. By standardizing the tasks, datasets and metrics, we\nhope to create a community of researchers that will advance\nthe state of the art in speech processing systems by loose cou-\npling of off-the-shelf ASR systems and techniques based on\ngenerative LMs, such as instruction prompting, text genera-\ntion and in-context learning. We propose three tasks that go\nbeyond speech transcription correction and include text-based\nspeaker diarization correction and emotion recognition. Un-\nlike traditional models, LLMs provide the potential for im-\nproving these tasks by leveraging linguistic and world knowl-\nedge learned during pretraining, and by taking advantage of\nlong conversational context."}]}