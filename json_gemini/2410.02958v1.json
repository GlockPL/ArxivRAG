{"title": "AUTOML-AGENT: A MULTI-AGENT LLM FRAMEWORK FOR FULL-PIPELINE AUTOML", "authors": ["Patara Trirat", "Wonyong Jeong", "Sung Ju Hwang"], "abstract": "Automated machine learning (AutoML) accelerates AI development by automat- ing tasks in the development pipeline, such as optimal model search and hyper- parameter tuning. Existing AutoML systems often require technical expertise to set up complex tools, which is in general time-consuming and requires a large amount of human effort. Therefore, recent works have started exploiting large lan- guage models (LLM) to lessen such burden and increase the usability of AutoML frameworks via a natural language interface, allowing non-expert users to build their data-driven solutions. These methods, however, are usually designed only for a particular process in the AI development pipeline and do not efficiently use the inherent capacity of the LLMs. This paper proposes AutoML-Agent, a novel multi-agent framework tailored for full-pipeline AutoML, i.e., from data retrieval to model deployment. AutoML-Agent takes user's task descriptions, facilitates collaboration between specialized LLM agents, and delivers deployment-ready models. Unlike existing work, instead of devising a single plan, we introduce a retrieval-augmented planning strategy to enhance exploration to search for more optimal plans. We also decompose each plan into sub-tasks (e.g., data prepro- cessing and neural network design) each of which is solved by a specialized agent we build via prompting executing in parallel, making the search process more efficient. Moreover, we propose a multi-stage verification to verify executed re- sults and guide the code generation LLM in implementing successful solutions. Extensive experiments on seven downstream tasks using fourteen datasets show that AutoML-Agent achieves a higher success rate in automating the full AutoML process, yielding systems with good performance throughout the diverse domains.", "sections": [{"title": "1 INTRODUCTION", "content": "Automated machine learning (AutoML) has significantly reduced the need for technical expertise and human labors in developing effective data-driven solutions by automating each process in the AI development pipeline (Yao et al., 2018; Ren et al., 2020; He et al., 2021), such as feature engi- neering, model selection, and hyperparameter optimization (HPO). However, current AutoML sys- tems (Gijsbers et al., 2024) often necessitate programming expertise to configure complex tools and resources, potentially creating barriers for a larger pool of users with limited skills and knowledge.\nTo make AutoML frameworks more accessible, recent studies (Trirat et al., 2021; Viswanathan et al., 2023; Li et al., 2023; Hollmann et al., 2023b; Liu et al., 2024a; Zhang et al., 2023; Shen et al., 2023; Zhang et al., 2024a; Hong et al., 2024a; Guo et al., 2024a; Yang et al., 2024) have suggested to use natural language interfaces with large language models (LLM) for machine learning (ML) and data science (DS) tasks. Nevertheless, these previous LLM-based AutoML frameworks only considered a limited number of tasks due to their restricted designs, either only for a process in the pipeline (e.g., feature engineering (Hollmann et al., 2023b; Li et al., 2024; Malberg et al., 2024), HPO (Liu et al., 2024a; Zhang et al., 2024a), and model selection (Zhang et al., 2023; Shen et al., 2023)) or for a specific group of downstream tasks (e.g., natural language processing (Viswanathan et al., 2023) and computer vision (Yang et al., 2024)). In addition, most methods overlook the inherent capability"}, {"title": "2 RELATED WORK", "content": "Automated machine learning (AutoML) is a transformative approach for optimizing ML workflows, enabling both practitioners and researchers to efficiently design models and preprocessing pipelines with minimal manual intervention (Ren et al., 2020; He et al., 2021; Gijsbers et al., 2024). Despite several advancements in AutoML (Jin et al., 2019; Feurer et al., 2022; Tang et al., 2024), most of them are designed only for particular elements of the ML pipeline. Only a few works (Bisong, 2019; Mukunthu et al., 2019; Microsoft, 2021) support multiple steps of the pipeline. Also, due to the traditional programming interfaces, these systems often have complex configuration procedures and steep learning curves that require substantial coding expertise and an understanding of the under- lying ML concepts, limiting their accessibility to non-experts and being time-consuming even for experienced users. These limitations hinder the widespread adoption of traditional AutoML systems.\nLarge language models (LLM), e.g., GPT-4 (Achiam et al., 2023) and LLaMA (Touvron et al., 2023), have recently shown promise in addressing these limitations with the complex problem-solving skills across disciplines via human-friendly language interfaces, including AI problems (Xi et al., 2023). This shift towards natural language-driven interfaces democratizes access and allows users to artic- ulate their needs in a more intuitive manner. However, existing LLM-based frameworks can only assist in a specific step of the ML pipeline, such as feature engineering (Hollmann et al., 2023b), model search (Shen et al., 2023; Hong et al., 2024a; Guo et al., 2024a), or HPO (Liu et al., 2024a; Zhang et al., 2024a). A few attempts (Viswanathan et al., 2023; Yang et al., 2024) support the entire ML production pipeline, yet only for a specific type of downstream tasks. Besides, these methods either naively use the LLMs or overlook the inherent capabilities, making their search processes time-consuming for the AutoML pipeline that requires sophisticated planning and verification.\nIn contrast to the existing studies, our framework aims to overcome these limitations by incorporat- ing a new retrieval-augmented planning strategy, coupled with plan decomposition and prompting- based execution techniques, alongside structure-based prompt parsing and multi-stage verification. Through these enhancements, we can increase plan execution efficiency and support diverse ML tasks with more accurate pipeline implementation."}, {"title": "3 A MULTI-AGENT LLM FRAMEWORK FOR FULL-PIPELINE AUTOML", "content": "This section presents details of the proposed multi-agent framework, AutoML-Agent, including agent specifications, a prompt parsing module, a retrieval-augmented planning strategy, a prompting-based plan execution, and a multi-stage verification. As depicted in Figure 2, all agents are coordinated by an Agent Manager to complete the user's instructions by delivering the deployment-ready model."}, {"title": "3.1 AGENT SPECIFICATIONS", "content": "We now provide brief descriptions of the agents in our multi-agent AutoML framework.\nAgent Manager (Amgr) acts as the core interface between users and other LLM agents in the frame- work. It is responsible for interacting with the user, devising a set of global plans for subsequent processes with retrieved knowledge, distributing tasks to corresponding agents, verifying executed results with feedback, and tracking the system progress.\nPrompt Agent (Ap) is an LLM specifically instruction-tuned for parsing the user's instructions into a standardized JSON object with predefined keys. The parsed information is then shared across agents in the framework during the planning, searching, and verification phases.\nData Agent (Ad) is an LLM prompted for doing tasks related to data manipulation and analysis. The analysis results from the Data Agent are used to inform the Model Agent about data characteristics during the model search and HPO.\nModel Agent (Am) is an LLM prompted for doing tasks related to model search, HPO, model profiling, and candidate ranking. The results produced by the Model Agent are sent back to the Agent Manager for verification before proceeding to the Operation Agent.\nOperation Agent (Ao) is an LLM prompted for implementing the solution found by the Data and Model Agents that passes the Agent Manager's verification. The Operation Agent is responsible for writing effective code for actual runtime execution and recording the execution results for final verification before returning the model to the user.\nAfter we define all agents with their corresponding profiles as described above (see \u00a7B.1 for detailed prompts), the Amgr then assigns relevant tasks to each agent according to the user's input. Note that we can implement Ad and Am with more than one agent per task based on the degree of parallelism."}, {"title": "3.2 FRAMEWORK OVERVIEW", "content": "We present an overview of our AutoML-Agent framework in Figure 2 and Algorithm 1. In the (1) initialization stage, the Agent Manager (Amgr) receives the user instruction and checks its valid- ity through request verification (Figure 2(c) and Line 3). In the (2) planning stage, the Prompt Agent (Ap) parses the verified user instruction into a standardized JSON object. Then, Amgr gen- erates plans to solve the given AutoML task using retrieval-augmented planning (Figure 2(a) and Line 11). In the (3) execution stage, the Data (Ad) and Model (Am) Agents decompose these plans and execute them via plan decomposition (PD) and prompting-based plan execution (Figure 2(b)"}, {"title": "3.3 INSTRUCTION DATA GENERATION AND PROMPT PARSING", "content": "N\nData Generation For Ap to generate accurate JSON objects, we need to instruction-tune the LLM first because it can output a valid JSON object but with incorrect keys that are irrelevant to subse- quent processes. Following Xu et al. (2024), we first manually create a set of high-quality seed instructions then automatically generate a larger instruction dataset D = {(Ii, Ri)};i=1, having N instruction-response pairs. Here, Ii is the i-th instruction with the corresponding response Ri. We use the JSON format substantially extended from Yang et al. (2024) for response Ri with the fol- lowing top-level keys to extract the user's requirement from various aspects of an AutoML pipeline.\n\u2022 User. The user key represents the user intention (e.g., build, consult, or unclear) of the given instruction and their technical expertise in AI.\n\u2022 Problem. The problem key indicates the characteristics and requirements of the given task, in- cluding area (e.g., computer vision), downstream task (e.g., image classification), application or business domain, and other constraints like expected accuracy and inference latency.\n\u2022 Dataset. The dataset key captures the data characteristics and properties, including data modality, requested preprocessing and augmentation techniques, and potential data source.\nPrompt Parsing Then, we can use the generated dataset D to train an LLM and use it as Ap. Note that these standardized keys are important for a better control over the LLM agents' behavior within our framework and necessary for effective communication between agents. Moreover, these keys provide contextual information for generating a high-quality AutoML pipeline from various perspectives. After the instruction tuning, we use the Ap to parse the user's instructions (or task descriptions) and return the parsed requirements R = Ap(I) to Amgr, as shown in \u00a7C.1."}, {"title": "3.4 RETRIEVAL-AUGMENTED PLANNING", "content": "Recent studies (Guo et al., 2024b; Huang et al., 2024; Masterman et al., 2024; Zhang et al., 2024b; Hu et al., 2024) highlights that effective planning and tool utilization are essential for solving com- plex problems with LLMs, especially in a multi-agent framework. By bridging two techniques in a single module, we propose a retrieval-augmented planning (RAP) strategy to effectively devise a robust and up-to-date set of diverse plans for the AuotML problems.\nLet P = {p1,..., pp} be a set of plans. Based on past knowledge embedded in the LLM, knowl- edge retrieved via external APIs (such as arXiv papers), and R, RAP generates P multiple end- to-end plans for the entire AutoML pipeline having different scenario pi. This strategy enables AutoML-Agent to be aware of newer and better solutions. Specifically, we first use the parsed re- quirements R to acquire a summary of the relevant knowledge and insights via API calls, including web search and paper summary. Amgr then uses this information to devise P different plans, i.e., P = Amgr (RAP(R)). Note that Amgr devises each plan independently to make the subsequent steps parallelizable. The benefit of this strategy is that it enhances exploration for better solutions while allowing parallelization. Examples of generated plans are provided in \u00a7C.2."}, {"title": "3.5 PROMPTING-BASED PLAN EXECUTION AND IMPLEMENTATION", "content": "Given the generated P, we now describe how Ad and Am execute each pi using prompting tech- niques without actually executing the code. Examples of the execution results are in \u00a7C.4.\nPlan Decomposition Due to the high complexity of the end-to-end plan, we first need to adap- tively decompose the original plan pi \u2208 P into a smaller set of sub-tasks si relevant to the agent's roles and expertise to increase the effectiveness of LLMs in solving and executing the given plan (Khot et al., 2023). The plan decomposition (PD) process involves querying the agents about their understanding of the given plan specific to their roles. Formally, sa = PD(R, Ad, pi), where sa is the decomposed plan for Data Agent, containing sub-tasks for the given plan pi. Then, the agent executes the decomposed plan towards the user's requirements instead of the original lengthy plan. We define the sub-tasks sm of Am below due to its reliance on Data Agent's outcomes. Examples of decomposed plans are in \u00a7C.3.\nPseudo Data Analysis In AutoML-Agent, Ad handles sub-tasks in sa, including the retrieval, pre- processing, augmentation, and analysis of the specified dataset. During the data retrieval phase, if the dataset is not directly supplied by the user, we initiate an API call to search for potential datasets in repositories, such as HuggingFace and Kaggle, using the dataset name or description. Upon lo- cating a dataset, we augment the prompt with metadata from the dataset's source; if no dataset is found, we rely on the inherent knowledge of the LLM. We then prompt Ad to proceed by acting as if it actually executes si, according to the dataset characteristics and user requirements R. The summarized outcomes of these sub-tasks, O, are then forwarded to the Am. Hence, O = Aa(sa).\nTraining-Free Model Search and HPO Like Ad, Am uses API calls to complete all sub-tasks sm, instead of direct code execution. However, in contrast to Ad, the plan decomposition for Am"}, {"title": "3.6 MULTI-STAGE VERIFICATION", "content": "Verification, especially with refinement or feedback, is essential for maintaining the correct trajec- tory of LLMs (Baek et al., 2024; Madaan et al., 2023; Gou et al., 2024). Our framework incorporates three verification steps to guarantee its accuracy and effectiveness: request verification, execution verification, and implementation verification.\nRequest Verification Initially, we assess the clarity of the user's instructions to determine if they are relevant and adequate for executing ML tasks and addressing the user's objectives. If the in- structions prove insufficient for progressing to the planning stage, Amgr will request additional in- formation, facilitating multi-turn communication. This request verification (ReqVer in Algorithm 1 Line 3) step, however, often overlooked in existing studies, placing an undue burden on users to for- mulate a more detailed initial prompt a challenging task particularly for those who are non-experts or lack experience. Prompts for ReqVer are shown in \u00a7B.4.1.\nExecution Verification After executing the plans in \u00a73.5, Amgr then verifies whether any of the pipelines produced by Ad and Am (i.e., O) satisfy the user's requirements via prompting (see \u00a7B.4.2). If the results are satisfied, the suggested solution is selected as a candidate for imple- mentation. This execution verification (ExecVer) step effectively mitigates computational overhead in the search process by allocating resources exclusively to the most promising solution.\nImplementation Verification This implementation verification (ImpVer) phase closely resembles the execution verification; however, it differs in that it involves validating outcomes derived from the code that has been executed and compiled by Ao. We present the prompt for this verification in \u00a7B.4.3. If the outcomes meet the user's requirements, Amgr provides the model and deployment endpoint to the user.\nNote that if any execution or implementation fails to satisfy the user requirements (i.e., does not pass the verification process), these failures are systematically documented. Subsequently, the system transitions to the plan revision stage. During this stage, Amgr formulates a revised set of plans, incorporating insights derived from the outcomes of the unsuccessful plans."}, {"title": "4 EXPERIMENTS", "content": "We validate the effectiveness of our full-pipeline AutoML framework by comparing AutoML-Agent with handcrafted models, state-of-the-art AutoML variants, and LLM-based frameworks across mul- tiple downstream tasks involving different data modalities."}, {"title": "4.1 SETUP", "content": "Downstream Tasks and Datasets As summarized in Table 2, we select seven downstream tasks from five different data modalities, including image, text, tabular, graph, and time series. These"}, {"title": "4.2 MAIN RESULTS", "content": "We report the average scores from five independent runs for all evaluation metrics in Figure 3.\nSuccess Rate Figure 3(left) and Table 5 present the results for the SR metric. For the constraint- free cases, which can be considered easier tasks, all methods have higher SR than ones in the constraint-aware setting. Notably, AutoML-Agent also consistently outperforms the baselines in the constraint-aware setting, achieving an average SR of 87.1%, which underscores the effectiveness of the proposed framework. We conjecture that the knowledge retrieved during the planning process helps the agents identify which areas to focus on in order to meet the given constraints. Regarding DS-Agent, although we use the provided example cases for the relevant tasks, DS-Agent appears to fail on certain tasks due to its heavy reliance on curated case banks and the inclusion of partially completed code, which is unavailable in our setting.\nDownstream Performance We present the performance comparison between the successfully built models in Figure 3(center) and Table 6. To ensure meaningful results and to examine how the performance of LLM-generated models compares to state-of-the-art AutoML techniques and manual ML pipelines crafted by experienced experts, we select top-performing models by evaluat- ing results reported in Papers with Code benchmarks and Kaggle notebooks for the same tasks and datasets, where applicable, as the Human Models baselines. From the results, we can observe that AutoML-Agent significantly outperforms other agents, including Human Models, in the NPS metric. In particular, AutoML-Agent achieves the best performance across all tasks under the constraint- aware setting. These findings highlight the superiority of AutoML-Agent in adapting to various sce- narios, attributed to the retrieval-augmented planning (RAP) strategy. This approach enables agents to discover effective pipelines for given constraints. These empirical observations substantiate the efficacy of the proposed RAP, providing up-to-date solutions for various tasks.\nComprehensive Score Figure 3(right) and Table 7 present the weighted quality of each agent based on the CS metric. Overall, AutoML-Agent outperforms all other baselines, especially in more complicated tasks. Interestingly, it is evident that general-purpose LLMs still works relatively well on classical tasks like tabular classification and regression, while more sophisticated methods, such as DS-Agent and our AutoML-Agent work significantly better in complex tasks. This finding aligns with previous research (Guo et al., 2024a), which suggests that tabular tasks typically involve straightforward function calls from the sklearn library (Pedregosa et al., 2011), and therefore do not demand advanced reasoning or coding abilities from LLM agents, unlike more complex tasks."}, {"title": "4.3 RESOURCE COST", "content": "As we primarily use closed-source LLMs in this paper, we analyze the resource costs in terms of time and money. Figure 4 presents the average time and monetary costs across different tasks and datasets for a single run, under the constraint-free (upper) and constraint-aware (lower) set- tings. On average, it takes around 525 sec- onds and costs 0.30 USD (using GPT-4o) to search for a single model that will be deployable after training. The significant amount of time spent in the planning stage also suggests the difficulty in devising plans for full-pipeline AutoML."}, {"title": "4.4 ABLATION AND HYPERPARAMETER STUDIES", "content": "To validate the effectiveness of each component in AutoML-Agent, we conduct the following ab- lation studies. The results are presented in Figure 5a and Table 8. First, we investigate retrieval- augmented planning (RAP) alone, where retrieved knowledge from external APIs is directly used without plan decomposition and multi-stage verification. As expected, this ablation leads to a de- cline in performance, and in some cases, even fails to generate a runnable model. This outcome highlights the importance of the decomposition and verification modules. Second, we evaluate RAP with plan decomposition, where the plan is decomposed for each specific agent. While this variant demonstrates better downstream performance, it still fails to produce runnable models in certain downstream tasks due to the lack of code verification. Finally, we assess the full framework with multi-stage verification, which provides feedback to agents, thereby enhancing both their planning and coding capabilities. Integrating all components significantly empowers LLM agents to effec- tively incorporate external knowledge from various sources to build a full-pipeline AutoML system.\nTo further verify the effectiveness of devising multiple plans in our retrieval-augmented planning strategy (\u00a73.4), we conduct a hyperparameter study by varying the number of plans P in the constraint-free setting. As shown in Figure 5b and Table 9, the number of plans does not signif- icantly affect the success rate, likely due to GPT-4's robust planning capabilities. However, based on the NPS and CS metrics, we observe that the number of plans has a notable impact on downstream task performance. Also, these results also suggest that adding more plans does not necessarily lead to better results, as the model may generate multiple similar plans, resulting in similar outcomes. Consequently, we select 3 as the default number of plans."}, {"title": "5 CONCLUSION", "content": "This paper presents AutoML-Agent, a novel LLM-based multi-agent framework designed for Au- toML, covering the entire pipeline from data retrieval to model deployment. AutoML-Agent tack- les the full-pipeline planning complexity and implementation accuracy challenges in the LLMs for task-agnostic AutoML by leveraging the newly proposed retrieval-augmented planning strategy and multi-stage verification. In addition, we enhance the plan execution efficiency by integrating role- specific decomposition and prompting-based execution techniques into the framework. Our experi- ments on seven ML tasks demonstrate that AutoML-Agent outperforms existing methods in terms of success rate and downstream task performance.\nLimitations and Future Work Even though we offer a flexible module to accommodate various ML tasks and data modalities, the absence of a skeleton code for completely new tasks could increase the risk of code hallucination problems. Additionally, in the current version, there is still a gap in code generation quality when using different backbones, e.g., GPT-4 vs. GPT-3.5, which is not unique to our approach but a common challenge faced by existing LLM-based frameworks. Developing a more robust framework that can effectively provide reasonable solutions with less reliance on the LLM backbone is very promising future work.\nIn addition, our work still faces code generation failures when applied to machine learning tasks that require significantly different development pipelines from those tested in our experiments, which focused on general supervised and unsupervised settings. Tasks such as reinforcement learning and recommendation systems pose particular challenges. Consequently, extending AutoML-Agent to these tasks will require the development of additional agents to handle specific steps in the target pipeline, such as actor-environment interaction and reward modeling in reinforcement learning."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "We present the complete prompts and showcase results in \u00a7B and \u00a7D to facilitate reproduction. More experimental and implementation details are provided in \u00a7A, along with detailed results in \u00a7E."}, {"title": "ETHICS STATEMENT", "content": "We expect AutoML-Agent to offer significant advantages by promoting AI-driven innovation and enabling individuals with limited AI expertise to effectively utilize AI capabilities. However, we acknowledge the potential misuse of AutoML-Agent by malicious users, such as generating offen- sive content, malicious software, or invasive surveillance tools when exposed to harmful inputs. This vulnerability is not unique to our approach but represents a common challenge faced by ex- isting LLMs with substantial creative and reasoning capabilities, which can occasionally produce undesirable outputs.\nAlthough we strictly instruct the LLM to focus on generating positive results for machine learning tasks, there is a possibility of unforeseen glitches that could introduce security issues within the system. Therefore, we recommend running AutoML-Agent within a Docker container to ensure isolation from the host's file system. Additionally, due to its integration with external services for retrieval-augmented generation and API-based LLMs like GPT-4, privacy concerns may arise. Users should carefully review any data included in API prompts to prevent unintended data disclosures."}, {"title": "A DETAILS OF EXPERIMENTAL SETUP", "content": "This section outlines the detailed experimental setup used in this paper, including the complete instruction prompts for both constraint-free (Table 3) and constraint-aware (Table 4) settings, a full- pipeline skeleton script (\u00a7A.1), dataset and baseline descriptions, as well as evaluation metrics."}, {"title": "A.1 SKELETON PYTHON SCRIPT", "content": "Skeleton Python Script (e.g., text_classification.py)\n# The following code is for \"text classification\" task using PyTorch.\nimport os, random, time, json\n# define GPU location\nos.environ [\"CUDA_DEVICE_ORDER\"]\n= \"PCI_BUS_ID\"\nos.environ [\"CUDA_VISIBLE_DEVICES\"] = \"3\"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport gradio as gr\n# TODO: import other required library here, including libraries for datasets and (pre- trained) models like HuggingFace and Kaggle APIs. If the required module is not found, you can directly install it by running 'pip install your_module'.\nfrom torchtext import datasets, data, vocab\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import accuracy_score, f1_score\nSEED = 42\nrandom.seed (SEED)\ntorch.manual_seed (SEED)\nnp.random.seed (SEED)\n# Define device for model operations\ndevice = torch.device (\"cuda\" if torch.cuda.is_available() else \"cpu\")\nDATASET_PATH \"_experiments/datasets\" # path for saving and loading dataset (s) (or the user's uploaded dataset) for preprocessing, training, hyperparamter tuning, deployment and evaluation\n# Data preprocessing and feature engineering\ndef preprocess_data():\n# TODO: this function is for data preprocessing and feature engineering\n# Run data preprocessing\n# Should return the preprocessed data\nreturn processed_data\ndef train_model (model, train_loader):\n# TODO: this function is for model training loop and optimization on 'train' and 'valid' datasets\n# TODO: this function is for fine-tuning a given pretrained model (if applicable)\n# Should return the well-trained or finetuned model.\nreturn model\ndef evaluate_model (model, test_loader):\n# In this task, we use Accuracy and F1 metrics to evaluate the text classification performance.\n# The 'performance_scores' should be in dictionary format having metric names as the dictionary keys\n# TODO: the first part of this function is for evaluating a trained or fine-tuned model on the 'test' dataset with respect to the relevant downstream task's performance metrics\n# Define the 'y_true' for ground truth and 'y_pred for the predicted classes here.\nperformance_scores\n}\n{\n'ACC': accuracy_score (y_true, y_pred),\n'F1': fl_score (y_true, y_pred)\n# TODO: the second part of this function is for measuring a trained model complexity on a samples with respect to the relevant complexity metrics, such as inference time and model size\n# The 'complexity_scores' should be in dictionary format having metric names as the dictionary keys\n# Should return model's performance scores\nreturn performance_scores, complexity_scores\ndef prepare_model_for_deployment():"}, {"title": "A.2 DATASET DESCRIPTIONS", "content": "As presented in Table 2, we select seven representative downstream tasks, covering five data modal- ities. We describe the datasets their statistics as follows.\n\u2022 Butterfly Image (Butterfly). This dataset includes 75 distinct classes of butterflies, featuring over 1,000 labeled images, including validation images. Each image is assigned to a single"}, {"title": "A.3 BASELINES", "content": "Human Models We select top-performing models based on evaluations from Papers with Code benchmarks or Kaggle notebooks, where the similar tasks and datasets are applicable. The chosen models for relevant downstream tasks are described below.\n\u2022 Image Classification. The human models for image classification tasks are obtained from a Kaggle notebook available at https://www.kaggle.com/code/mohamedhassanali/ butterfly-classify-pytorch-pretrained-model-acc-99/notebook, utiliz- ing a pretrained ResNet-18 model.\n\u2022 Text Classification. For text classification tasks, two models are employed. A Word2Vec-based XGBoost model is applied to the e-commerce text dataset https://www.kaggle.com/ code/sugataghosh/e-commerce-text-classification-tf-idf-word2vec# Word2Vec-Hyperparameter-Tuning, while the XLM-ROBERTa model is used for the textual entailment dataset https://www.kaggle.com/code/vbookshelf/ basics-of-bert-and-xlm-roberta-pytorch.\n\u2022 Tabular Classification. Due to the absence of a similar model in the repository, we use the state-of-the-art TabPFN model (Hollmann et al., 2023a) designed for tabular classification tasks.\n\u2022 Tabular Regression. For tabular regression tasks, we adopt two models specif- ically designed for the given datasets, which are available at https://www. kagle.com/code/shatabdi5/crab-age-regression for the crab age dataset and at https://www.kaggle.com/code/mahmoudmagdyelnahal/ crop-yield-prediction-99/notebook for the crop yield dataset.\n\u2022 Tabular Clustering. For unsupervised clustering tasks, we use manually hyperparameter-tuned KMeans clustering, following the approach outlined in https://www.kaggle.com/code/ samuelcortinhas/tps-july-22-unsupervised-clustering, as the baseline.\n\u2022 Time-Series Forecasting. In this task, we use the state-of-the-art iTransformer (Liu et al., 2024b), which is designed for the same task and datasets as the baseline model.\n\u2022 Node Classification. For node classification tasks, we also employ a state-of-the-art graph neural network-based model, PMLP (Yang et al., 2023), as the handcrafted baseline for both datasets.\nAutoGluon We adopt AutoGluon as the baseline because it is a state-of-the-art AutoML frame- work capable of handling various downstream tasks and data modalities, with the exception of graph data. There are three variants of AutoGluon: AutoGluon-TS (Shchur et al., 2023) for time series, AutoGluon-Tabular (Erickson et al., 2020) for tabular machine learning, and AutoGluon- Multimodal (Tang et al., 2024) for computer vision and natural language processing tasks.\nGPT-3.5 and GPT-4 For GPT-3.5 and GPT-4, we use the gpt-3.5-turbo-0125 and gpt-4-2024-05- 13 models via the OpenAI API. We implement the zero-shot baselines using the prompt below.\nDS-Agent We reproduce the DS-Agent (Guo et al., 2024a) baseline using the official source code. However, it is important to note that our framework encompasses the entire process from data re- trieval/loading to deployment, whereas DS-Agent focuses solely on the modeling aspect, assuming complete data and evaluation codes are provided. In this paper, we utilize the deployment stage of DS-Agent along with its collected case banks and Adapter prompt for the same tasks, as the source code for manual human insights collection during the development stage is unavailable."}, {"title": "A.4 EVALUATION METRICS", "content": "Success Rate (SR) We employ the success rate (Guo et al., 2024a; Hong et al., 2024a), which eval- uates whether the models built by an LLM agent are executable in the given runtime environment. Success rate is used to assess code execution.\nFor the constraint-free setting, we apply a three-level grading scale as follows.\n\u2022 0.00: Code cannot be executed.\n\u2022 0.50: Code provides a runnable ML/DL model.\n\u2022 1.00: Code provides a runnable model and an accessible deployment endpoint (e.g., Gradio).\nFor the constraint-aware setting, we use a five-level grading scale to evaluate whether the code executes successfully and satisfies the given constraints. The grading criteria are as follows.\n\u2022 0.00: Code cannot be executed.\n\u2022 0.25: Code provides a runnable ML/DL model.\n\u2022 0.50: Code provides a runnable model and an accessible deployment endpoint (e.g., Gradio).\n\u2022 0.75: Code provides a deployed, runnable model that partially meets constraints (e.g., target per- formance, inference time, and model size).\n\u2022 1.00: Code provides a deployed, runnable model that fully meets constraints.\nNormalized Performance Score (NPS) In this paper, each downstream task is associated with a specific evaluation metric, which may vary between tasks. These metrics include accuracy, F1- score, and RMSLE. For metrics such as accuracy and F1-score, we present the raw values to facilitate comparison across identical data tasks. For performance metrics where lower values indicate better performance, such as loss-based metrics, we normalize all performance values s using the following transformation: NPS = 1/(1+s). This transformation ensures that metrics like RMSLE are scaled between 0 and 1, with higher NPS values indicating better performance.\nNote that achieving downstream task performance (NPS) requires a runnable model, i.e., SR > 0. If the model cannot run, the NPS is zero by default as it cannot make any predictions.\nComprehensive Score (CS) To evaluate both the success rate and the downstream task perfor- mance of the generated AutoML pipelines simultaneously, we calculate CS as a weighted sum of SR and NPS, as follows: CS = 0.5 \u00d7 SR + 0.5 \u00d7 NPS."}, {"title": "B PROMPTS FOR AUTOML-AGENT", "content": "B.1 AGENT SPECIFICATIONS\nThis subsection provides the system prompt design for agent specifications in AutoML-Agent, in- cluding Agent Manger (B.1.1), Prompt Agent (B.1.2), Data Agent (B.1.3), Model Agent (B.1.4), and Operation Agent (B"}]}