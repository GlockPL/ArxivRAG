{"title": "Arabic Music Classification and Generation using Deep Learning", "authors": ["Mohamed Elshaarawy", "Ashrakat Saeed", "Mariam Sheta", "Abdelrahman Said", "Asem Bakr", "Omar Bahaa", "Walid Gomaa"], "abstract": "This paper proposes a machine learning approach for classifying classical and new Egyptian music by composer and generating new similar music. The proposed system utilizes a convolutional neural network (CNN) for classification and a CNN autoencoder for generation. The dataset used in this project consists of new and classical Egyptian music pieces composed by different composers.\nTo classify the music by composer, each sample is normalized and transformed into a mel spectrogram. The CNN model is trained on the dataset using the mel spectrograms as input features and the composer labels as output classes. The model achieves 81.4% accuracy in classifying the music by composer, demonstrating the effectiveness of the proposed approach.\nTo generate new music similar to the original pieces, a CNN autoencoder is trained on a similar dataset. The model is trained to encode the mel spectrograms of the original pieces into a lower-dimensional latent space and then decode them back into the original mel spectrogram. The generated music is produced by sampling from the latent space and decoding the samples back into mel spectrograms, which are then transformed into audio. In conclusion, the proposed system provides a promising approach to classifying and generating classical Egyptian music, which can be applied in various musical applications, such as music recommendation systems, music production, and music education.", "sections": [{"title": "I. INTRODUCTION", "content": "This project aims to leverage deep learning techniques for the classification and generation of Arabic music. The project encompasses several phases, starting with an extensive data collection process. A dataset comprising music files from various genres was assembled. Subsequently, rigorous pre-processing techniques were employed to optimize the data for the subsequent modeling stages. The pre-processing phase involved exploring different approaches to identify the most effective methods for enhancing model performance. This involved techniques such as audio normalization, fea- ture extraction, and data augmentation. By carefully refining the pre-processing pipeline, the project sought to maximize the quality and relevance of the input data for subsequent modeling steps. The next phase involved training and fitting deep learning models for both music classification and music generation tasks. The classification models were subjected to iterative testing and refinement to achieve satisfactory accuracy levels. For the music generation aspect, the fitted model was used to generate new musical compositions. Post-processing techniques were employed to refine and enhance the gener- ated music, ensuring that it adhered to the desired musical characteristics and structure. These post-processing steps may have included melody harmonization, rhythm adjustment, and tempo normalization. By employing advanced deep learning techniques and thorough pre-processing methodologies, this project aims to advance the field of Arabic music classification and generation. The results obtained from this research can potentially contribute to the broader domain of music analysis, synthesis, and creative expression."}, {"title": "II. RELATED WORK", "content": "In recent years, there has been a growing interest in the field of music classification and generation. Several studies have explored various techniques and methodologies to address the challenges associated with this domain. In this section, the relevant literature on music composer classification, and generation highlighting the key approaches and achievements are reviewed.\nOne notable paper in this field is titled \"Composer Classi- fication Models for Music-Theory Building\u201d by Herremans, Martens, and Sorensen (2015)[1]. In their study, the re- searchers adopted a data-driven approach by extensively ana- lyzing a large database of existing music. The main objectives of their work were twofold: first, to develop an automated system capable of accurately distinguishing between compo- sitions from different composers, and second, to identify the significant musical attributes that contribute to this distinction.\nA recent Kaggle project by Holst[2] focused on music composer classification using a machine learning approach. The project involved building a model to classify musical compositions based on the underlying composer's style. Holst extracted various features from the audio recordings and employed a classification algorithm to differentiate between composers. The results obtained in the Kaggle project provide valuable insights into the application of machine learning techniques for music composer classification.\nThe paper titled \"A Comprehensive Survey on Deep Music Generation\" [3] published in the Journal of Artificial Intelli- gence and Data Analytics is a valuable resource that we uti- lized to deepen our understanding of Variational Autoencoders (VAEs) and explore their potential for sound generation. We"}, {"title": "III. DATASET", "content": "Our classification and generation models were trained on a diverse data set of Arabic music. The data set was carefully curated to include a variety of genres, including classical, modern, piano, and oud songs, to ensure the model was exposed to a wide range of musical styles. The classical category consisted of compositions by renowned Egyptian artists such as Mohamed Abdelwahab, Reyad ElSonbaty, and Balegh Hamdy. The modern category included songs by popu- lar composers like Waleed Saad, Mohamed Rahem, and others.\nMust mention that our data size was 256 Arabic songs(MUSIC ONLY), ranging from 2.5 minutes to 50 minutes in length.\nOverall, the data set used for training the music generation model was carefully selected to provide a diverse set of musical styles and genres while maintaining consistency in length. The data set and preprocessing methods used in this study can serve as a foundation for future music generation research in Arabic music and related fields."}, {"title": "IV. METHODOLOGY", "content": "A. Composers Classification Model\n1) Preprocessing: This study aimed to develop an optimal model for music classification, specifically in the context of composer identification. To achieve this goal, a thorough in- vestigation was conducted through a series of five experiments. The ultimate objective was to identify and establish the most effective approach that yields accurate results in composer classification.\nIn the first experiment, A dataset comprising compositions from 11 different composers, with unevenly distributed data between them, representing diverse musical styles and genres, was selected.\nTo ensure consistency of results, a standardized prepro- cessing pipeline was followed. The steps in this pipeline included the conversion of the audio files into WAV format and segmenting and normalizing the compositions. Segmentation involved dividing the compositions into smaller segments of 10 seconds each, with a 2-second hop-length. Normalization was performed to ensure consistent volume levels across all segments.\nThe next step involved extracting the MFCC features from the normalized audio segments. This feature extraction tech- nique captures essential acoustic characteristics of the audio signals. To achieve uniform dimensions across all segments, padding techniques were applied. The segments were padded to match a target shape of (20, 938), ensuring that the input data has the same dimensions, which is necessary for training the model consistently."}, {"title": "2) Network Architecture:", "content": null}, {"title": "B. Generation Model", "content": "1) Preprocessing: The dataset used for preprocessing consisted of 75 WAV files, which were segmented into a total of 8756 segments. Each audio file was divided into smaller durations of 5.94 seconds, a duration that was determined through multiple iterations and experimentation. The goal was to find a duration that would allow the models to capture sufficient features while maintaining a suitable input size of 256x512.\nTo determine the optimal duration, the models were tested during each epoch using various durations. The performance of the models was evaluated based on the generated results and the loss errors obtained. Through this iterative process, the duration of 5.94 seconds was found to be effective in capturing relevant audio features while ensuring the models' input requirements were met.\nThe preprocessing steps involved in preparing these seg- ments for the subsequent model training were as follows: Segmentation: Each audio file was segmented into smaller durations of 5.94 seconds.\nPadding: If necessary, the segmented audio segments were padded with zeros to make them uniform in size. Padding helps maintain consistency in the input dimensions and ensures compatibility with the subsequent processing steps.\nLog Spectrogram Extraction: From each segmented audio segment, a log-scaled spectrogram was extracted using the short-time Fourier transform (STFT) method. The STFT di- vides the audio signal into small overlapping frames and calculates the magnitude of the complex-valued STFT. The magnitude spectrogram was then transformed to the log scale using the logarithmic compression provided by the Librosa library. This conversion enhances the representation of the audio data by emphasizing perceptually relevant features.\nMin-Max Normalization: The extracted log spectrograms were normalized using min-max normalization. This process scales the values of the spectrograms to a predefined range, typically between 0 and 1. Min-max normalization ensures that all spectrograms have consistent scaling, enabling the model to learn from a uniform data distribution.\nSaving the Preprocessed Data: The preprocessed log spec- trograms were saved in the specified paths to facilitate further model training and evaluation. These saved representations serve as the input to the subsequent steps of the music generation pipeline.\nBy performing these preprocessing steps on the 75 WAV files, the data was effectively transformed and prepared for training the variational autoencoder (VAE) model. The pre- processing pipeline ensures that the audio data is properly formatted, normalized, and ready for subsequent music gener- ation tasks.\n2) Network Architecture: The proposed Arabic music gen- eration model utilizes a Convolutional Variational Autoen- coder (CVAE) for the generation process. A CVAE is a type of generative model that combines the power of variational autoencoders with convolutional neural networks to capture the spatial and temporal dependencies in the mel-spectrogram data."}, {"title": "3) Postprocessing:", "content": "After using the CVAE model to generate new mel spectrograms, we need to convert them back to audio. This is done mainly by using the Griffin-Lim algorithm. The Griffin-Lim algorithm is an iterative algorithm that esti- mates the phase of a spectrogram given its magnitude. The algorithm is described in the paper \"Signal estimation from modified short-time Fourier transform\" [5]. The algorithm is implemented in the Librosa library.\nThe steps for the post-processing are as follows. First, we first reshape the output mel spectrogram to the original shape (removing the additional dimension that we added to fit into the model). Next, we apply denormalization to the mel spectrogram, returning it to its original range. To complete this step, we use the minimum and maximum values of the training data obtained during the preprocessing step.\nAfter that, we convert the mel spectrogram to a linear spectrogram using the inverse mel transform. Finally, we apply the Griffin-Lim algorithm to the linear spectrogram using the hop length set earlier to obtain the audio signal. The post- processed signals are saved in a specified directory. These steps are like the inverse of the preprocessing steps that we applied to the data before feeding it into the model. It's necessary to follow this order of steps to get the correct audio signal. To test the model, we apply the post-processing algorithm to the output of the model and compare it to the original audio signal (post-processing the original input data mel spectrograms)."}, {"title": "V. RESULTS", "content": "A. Composers Classification Model\nIn the first experiment, upon analyzing the confusion matrix, shown in figure 9, it became evident that the model struggled to make accurate predictions for most of the composers. This was reflected in the validation accuracy of 0.7370 and the test accuracy of 0.5489.\nThe noticeable difference between the validation accuracy and the test accuracy suggests that the model is overfitting the training data- Overfitting occurs when a model becomes too specialized in learning from the training data and fails to generalize well to unseen data. In this case, the overfitting can be attributed to the uneven distribution of data among the composers, which may have resulted in biased learning."}, {"title": "B. Generation Model", "content": "After training our generation model on multiple files, we observed a gradual decrease in the loss function throughout the training process. However, the results were not as promising as we had hoped. Upon listening to the output, it became apparent that while it bore a resemblance to the input, it also contained a slight amount of noise. Despite our rigorous efforts during the preprocessing phase and throughout model training, the outcomes fell short of our expectations. Our experiments have led us to the conclusion that the variational autoencoder struggled to capture the intricate compositions inherent in our original dataset. To solidify this observation, we recognize the need for additional testing and evaluations. It is noteworthy that this aspect represents our most significant contribution, utilizing the data we originally collected, in the realm of preprocessing and model selection."}, {"title": "VI. CONCLUSION", "content": "In this study, we investigated the task of classifying different types of composers in the context of Arabic music using deep learning techniques. Through a series of experiments and rigorous data pre-processing, we aimed to achieve the highest possible accuracy in classifying composers based on their musical compositions. Additionally, we explored the generation of Arabic music using deep learning models.\nTo achieve our classification goals, we conducted five dis- tinct experiments employing various approaches. We identi- fied the best set of pre-processing steps that maximized the accuracy of our models, ensuring optimal representation and encoding of the input data.\nFor the generation aspect, we experimented with various approaches to generate Arabic music compositions. Our goal was to produce high-quality output that closely resembled the characteristics of the input data. We devoted considerable ef- fort to pre-processing techniques to ensure the generated sound quality. While our current results are a beginning for further improvement, we acknowledge that further refinement and investigation are necessary to achieve even more compelling and accurate generation capabilities.\nOverall, this project contributes to the field of Arabic music classification and generation by providing insights into the effective utilization of deep learning techniques. Our experiments highlight the importance of data pre-processing and the impact of different approaches on the accuracy of composers classification. Furthermore, we demonstrate the potential of deep learning models in generating Arabic music compositions. The findings presented here lay the foundation for future research and development in the domain of Arabic music analysis, classification, and generation."}], "equations": ["DKL(N(\u03bc,\u03c3)||(N(0,1)) = 1/2 \u2211(1 + log \u03c3\u00b2 \u2013 \u03bc\u00b2 \u2013 \u03c3\u00b2)   (1)", "Loss = \u03b1 * RMSE + DKL   (2)"]}