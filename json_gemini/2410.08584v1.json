{"title": "ZIPVL: EFFICIENT LARGE VISION-LANGUAGE MOD- ELS WITH DYNAMIC TOKEN SPARSIFICATION AND KV CACHE COMPRESSION", "authors": ["Yefei He", "Feng Chen", "Jing Liu", "Wenqi Shao", "Hong Zhou", "Kaipeng Zhang", "Bohan Zhuang"], "abstract": "The efficiency of large vision-language models (LVLMs) is constrained by the computational bottleneck of the attention mechanism during the prefill phase and the memory bottleneck of fetching the key-value (KV) cache in the decoding phase, particularly in scenarios involving high-resolution images or videos. Visual content often exhibits substantial redundancy, resulting in highly sparse attention maps within LVLMs. This sparsity can be leveraged to accelerate attention computation or compress the KV cache through various approaches. However, most studies focus on addressing only one of these bottlenecks and do not adequately support dynamic adjustment of sparsity concerning distinct layers or tasks. In this paper, we present ZipVL, an efficient inference framework designed for LVLMs that resolves both computation and memory bottlenecks through a dynamic ratio allocation strategy of important tokens. This ratio is adaptively determined based on the layer- specific distribution of attention scores, rather than fixed hyper-parameters, thereby improving efficiency for less complex tasks while maintaining high performance for more challenging ones. Then we select important tokens based on their normalized attention scores and perform attention mechanism solely on those important tokens to accelerate the prefill phase. To mitigate the memory bottleneck in the decoding phase, we employ mixed-precision quantization to the KV cache, where high-bit quantization is used for caches of important tokens, while low-bit quantization is applied to those of less importance. Our experiments demonstrate that ZipVL can accelerate the prefill phase by 2.6\u00d7 and reduce GPU memory usage by 50.0%, with a minimal accuracy reduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively enhancing the generation efficiency of LVLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "With the recent advancement of large language models (LLMs) (Achiam et al., 2023; Team et al., 2023; Vavekanand & Sam, 2024), many studies have extended their capabilities to comprehend and generate visual content. These models, commonly known as large vision-language models (LVLMs), have demonstrated remarkable performance in tasks such as image captioning and visual question answering (Ge et al., 2024b; Liu et al., 2024b; Team, 2024; Ge et al., 2024c; Lin et al., 2023). Typically, to remain compatible with the next-token-prediction generation scheme of LLMs, images or videos are encoded into visual tokens through a pre-trained visual encoder, and concatenated with text tokens for input into the model. For instance, LLaVA (Liu et al., 2024b) employs a pre-trained CLIP-ViT-L-336px model (Radford et al., 2021), which encodes an image of size 336\u00d7336 pixels to 576 visual tokens. However, for high-resolution images or videos, the visual encoder generates excessive sequences of visual tokens, significantly limiting the generative efficiency of LVLMs. Specifically, the prefill phase suffers from the quadratic complexity of the attention mechanism, resulting in computational bottleneck and prolonged time-to-first-token (TTFT). In the decoding phase, each new token interacts with all preceding tokens, requiring to fetch the full key-value (KV) cache from memory. This process slows down decoding due to memory bottleneck. Improving generative efficiency in both phases is essential for the practical deployment of LVLMs.\nTo address computational complexity in the prefill phase, sparse attention (Pagliardini et al., 2023; Jiang et al., 2024; Zhu et al., 2024) has emerged as an effective strategy, particularly suitable for LVLMs where visual information exhibits considerable redundancy, leading to highly sparse attention maps (Wan et al., 2024; Chen et al., 2024). This sparsity can be implemented at various levels of granularity. Some studies pre-define several sparse patterns and assign them to the attention mask during inference (Jiang et al., 2024; Zhu et al., 2024). However, these predefined patterns are not compatible with efficient attention implementations such as FlashAttention (Dao et al., 2022) and require custom GPU kernels for each pattern. Alternatively, other approaches adopt token-level sparsity by identifying and discarding less important tokens (Chen et al., 2024; Arif et al., 2024), allowing seamless integration with off-the-shelf efficient attention implementations. However, the optimal retention ratio of important tokens may vary across different layers or tasks due to distinct attention patterns, as illustrated in Figure 1. These methods rely on a fixed token retention ratio and do not dynamically adjust based on task difficulty, leading to suboptimal performance on complex tasks.\nTo alleviate memory bottleneck, various efforts have been made to reduce KV cache size, including token dropping (Wan et al., 2024), token merging (Yang et al., 2024a), and quantization (Hooper et al., 2024; He et al., 2024b). However, these methods often rely on fixed compression ratios that are uniformly applied across all layers, failing to account for the distinct characteristics of attention maps in different layers. Moreover, despite the necessity of identifying important tokens for both sparse attention and KV cache compression, a unified inference optimization framework has yet to be developed.\nIn this paper, we present ZipVL, an efficient inference framework tailored for LVLMs that jointly optimizes the prefill and decoding phases with a unified ratio of important tokens, as shown in Figure 2. To start with, we introduce a layer-wise adaptive ratio assignment scheme for important tokens. This ratio is adaptively determined based on the distribution of attention scores in each layer, rather than relying on predefined hyper-parameters (Chen et al., 2024; Arif et al., 2024; He et al., 2024b; Zhang et al., 2023). This adaptive approach allows the ratio to be adjusted according to task complexity, enhancing efficiency for simpler tasks while preserving performance for more complex ones. After determining the ratio, we then select important tokens with the highest normalized attention scores,"}, {"title": "2 RELATED WORK", "content": "2.1 SPARSE ATTENTION FOR LLMS\nAttention scores have been widely observed to exhibit high sparsity in both LLMs and LVLMS (Xiao et al., 2024; Wan et al., 2024; Zhu et al., 2024; Zaheer et al., 2020; Beltagy et al., 2020). This sparsity allows sparse attention to overcome the quadratic computational complexity of the standard attention mechanism by restricting each token to focus on only a subset of tokens within the input sequence (Zhu et al., 2024; Jiang et al., 2024; Pagliardini et al., 2023; Ribar et al., 2024). Depending on the granularity of sparsity, sparse attention can be categorized into unstructured, semi-structured, and structured schemes. The unstructured scheme (Lefaudeux et al., 2022; He et al., 2024a) employs sparse attention masks without a fixed structure, making it hardware-unfriendly and challenging to achieve practical inference acceleration. The semi-structured sparse attention uses attention masks with predefined sparse patterns (Jiang et al., 2024; Pagliardini et al., 2023; Zhu et al., 2024) or introduces N:M sparsity to attention weights (Chen et al., 2023). However, it requires customized computational kernels for each sparse pattern or specific hardware to achieve acceleration. Structured sparse attention (Chen et al., 2024; Arif et al., 2024) directly prunes tokens before the attention computation, enabling acceleration without the need for custom kernels. However, due to its coarse granularity, the pruning sparsity and the selection of tokens to prune significantly impact model performance. For instance, HiRED (Arif et al., 2024) selects patches with the highest responses based on the feature maps of the visual encoder without considering the input text prompt, leading to suboptimal performance. FastV (Chen et al., 2024) empirically retains all tokens in the first two layers and prunes 50% of the visual tokens in all subsequent layers, resulting in performance degradation in challenging tasks such as ChartQA (Masry et al., 2022). In contrast, our approach achieves superior performance through an adaptive layer-wise ratio assignment scheme for important tokens.\n2.2 KV CACHE COMPRESSION\nKV cache prevents re-computation in the decoding phase by storing the key and value states of previous tokens, but with a significant memory bottleneck in long-context scenarios. Previous efforts to compress the KV cache can be broadly categorized into three types: token dropping-based (Ge et al., 2024a; Ren & Zhu, 2024; Zhang et al., 2023), token merging-based (Wang et al., 2024; Wan et al., 2024; Liu et al., 2024d), and quantization-based approaches (Hooper et al., 2024; He et al., 2024b; Yang et al., 2024b; Kang et al., 2024; Liu et al., 2024c). Both token dropping-based and merging- based methods aim to reduce the number of tokens stored in the KV cache by evicting or merging less important tokens. However, the information that is evicted or merged cannot be recovered, potentially leading to risks such as contextual incoherency or hallucination (Yang et al., 2024b), especially in multi-round dialogue scenarios. Conversely, quantization-based approaches retain all tokens in the KV cache and apply quantization to the cached values. To preserve performance, mixed-precision quantization further assigns higher bit-width to recent tokens (Liu et al., 2024c) or important tokens (Yang et al., 2024b) in the KV cache. In this paper, we apply mixed-precision quantization to compress the KV cache, leveraging the proposed layer-wise adaptive ratio assignment scheme to achieve a higher compression ratio."}, {"title": "3 PRELIMINARY", "content": "Attention block is the key module of Transformer-based LLMs. Each attention block contains three weight matrices $W_Q, W_K, W_V \\in \\mathbb{R}^{d\\times d}$, where d is the dimension of the input data. Here, we use a single attention head and omit the output projection for clarity. In the prefill phase, the input data $X \\in \\mathbb{R}^{n\\times d}$ with a sequence length of n is first multiplied with three weight matrices to obtain the query, key and value states:\n$Q = XW_Q, K = XW_K, V = XW_V$.\nThen the attention output is calculated as follows:\n$A = Softmax(\\frac{QK^T}{\\sqrt{d}}+M), O = AV$.\nHere, computing the product of $QK^T$ has a quadratic complexity $O(n^2)$, which makes the prefill phase compute-bound. $M\\in \\mathbb{R}^{n\\times n}$ is a lower triangular causal mask to ensure that each token can only attend to itself and previous tokens. Unstructured and semi-structured sparse attention introduce sparsity in the attention mask M with dynamic or fixed sparse pattern. With custom computing kernels, tokens in certain positions can be skipped when computing $QK^T$, thus accelerating the computation. On the other hand, structured sparse attention only computes attention scores for a subset of tokens $X' \\in \\mathbb{R}^{n'\\times d}$, reducing computational complexity to $O(n'^2)$ and seamlessly integrating with existing fast attention implementations."}, {"title": "4 METHOD", "content": "4.1 LAYER-WISE ADAPTIVE RATIO ASSIGNMENT FOR IMPORTANT TOKENS\nPrior studies (Arif et al., 2024; Zhang et al., 2023; He et al., 2024b; Liu et al., 2024c; Wan et al., 2024) typically adopt a fixed ratio of important tokens across all layers. However, as analyzed by the preceding study (Chen et al., 2024) and demonstrated in Figure 1(a) and (b), there are substantial variations in the attention map patterns across different layers. Moreover, Figure 1(b) and (c) illustrate that, even within the same layer, attention maps can differ depending on the task and input. In scenarios involving complex tasks, a limited, static ratio for important tokens can impair model performance. This raises the question:\ncan the model dynamically determine the number of tokens required to solve a task?\nIntuitively, for simpler tasks, the model needs to concentrate on fewer tokens, leading to a more focused distribution of attention scores. Conversely, more demanding tasks require the model to engage with a broader array of tokens, resulting in a more uniform distribution of attention scores. Prior work (Xiao et al., 2024) also highlights the criticality of preserving significant attention scores during inference within a constrained attention window. Building on these insights, we introduce a layer-wise adaptive scheme for assigning ratio of important tokens, ensuring the majority of significant attention scores are maintained within each layer.\nConsider an attention layer with n input tokens, where the full attention score matrix is denoted as $A \\in \\mathbb{R}^{n\\times n}$. The accumulated attention score for each token j is calculated by summing the corresponding column:\n$A_j = \\sum_{c=1}^{n}A_{c,j}$.\nThese accumulated attention scores are subsequently sorted in descending order, such that $a_{sorted}(j)$ represents the j-th highest attention score. The number of important tokens p is determined by preserving the majority of attention scores with minimal number of tokens, which can be expressed as:\n$p = min\\{p \\in \\mathbb{Z} | \\sum_{j=1}^{p}A_{sorted}(j) \\geq \\tau\\times n\\}$.\nHere, \u03c4 is the threshold dictating the retention of attention scores and the sum of the attention scores in A is equal to n due to the row-wise Softmax operation. As shown in Figure 3, our method can dynamically adjust the ratio of important tokens across distinct layers and tasks, thereby enhancing performance in complex tasks while improving efficiency in simpler tasks. Additional experimental results can be found in Section 5.2.1 and Figure 4."}, {"title": "4.2 INFERENCE OPTIMIZATION WITH UNIFIED TOKEN RATIO", "content": "After determining the number of important tokens p for each layer, we partition all tokens into two sets: set T of important tokens with a size of p and the set U for less important tokens with a size of n \u2212 p. Following prior work (Ren & Zhu, 2024; He et al., 2024b), we use normalized attention scores to assess token importance, calculated as follows:\n$\\bar{a}_j = \\frac{\\sum_{c=1}^{n}A_{c,j}}{nnz(A_{:,j})}$\nHere, $nnz(A_{:,j})$ denotes the number of non-zero elements in the j-th column. Important tokens are then selected using the top-k indexing method, while the remainder are considered less important:\n$T = topk\\_index(\\bar{a}_j,p)$,\n$U = \\{j \\in \\{1, 2, ..., n\\} | j \\notin T\\}$.\nThe inference optimization is then performed based on the split of tokens. Specifically, to address the computational bottleneck in the prefill phase, the attention mechanism is performed solely on these important tokens, thereby enhancing efficiency through token-level sparsity. Tokens excluded from this computation have their outputs padded to maintain the number of tokens consistent for subsequent layers. By leveraging token-level sparsity, our approach seamlessly integrates with off-the-shelf, fast attention implementations (Dao et al., 2022) to expedite the prefill process.\nTo tackle the memory bottleneck, we implement mixed-precision quantization for the KV cache based on the same token split in Eqs. (8) and (9). The KV cache for important tokens is quantized at a higher bit-width to retain information, whereas the cache for less critical tokens is quantized at a lower bit-width to significantly reduce KV cache size. In comparison to prior method (He et al., 2024b), quantizing KV cache with our adaptive layer-wise token ratio leads to a higher compression ratio with even stronger performance. Further details will be provided in Section 5.3.1.\nEfficient approximation of full attention scores. To integrate our method with fast attention implementation (Dao et al., 2022) and circumvent the computation of full attention scores in Eqs. (5) and (7), we selectively compute and accumulate the attention scores for a subset of tokens, following previous literature (He et al., 2024b; Jiang et al., 2024). The size of this subset is small and fixed, ensuring that the computational burden for these tokens remains minimal in long-context scenarios. The accumulated and normalized attention scores for each token can then be approximated with partial attention scores. Details can be found in Appendix A."}, {"title": "5 EXPERIMENTS", "content": "5.1 IMPLEMENTATION DETAILS\nTo assess the effectiveness of our proposed method, we conduct experiments on both image and video understanding tasks. For image understanding, we utilize three widely adopted LVLMs: LLaVA (Lin et al., 2023), LLaVA-Next (Liu et al., 2024a), and QWen-VL (Bai et al., 2023). These models are evaluated against five rigorous benchmarks: VQAv2 (Goyal et al., 2017), TextVQA (Singh et al., 2019), GQA (Hudson & Manning, 2019), MME (Fu et al., 2023), and ChartQA (Masry et al., 2022). For video understanding, evaluations are conducted using the LongVA (Zhang et al., 2024) model on the Video-MME (Fu et al., 2024) benchmark. To ensure reproducibility, all reported results are obtained using the Evaluation Suite of Large Multimodal Models (Li et al., 2024). For mixed- precision quantization, the KV cache of important tokens was quantized to 4-bit, while the KV cache of other tokens was quantized to 2-bit."}, {"title": "5.2 MAIN RESULTS", "content": "5.2.1 EVALUATION ON IMAGE BENCHMARKS\nWe begin our evaluation on five image comprehension benchmarks and compare our results against well-established methods with token-level sparsity: FastV (Chen et al., 2024) and HiRED (Arif et al., 2024). The results are presented in Table 1. Notably, HiRED determines the importance of patches through the feature map of the visual encoder, without considering the semantic information of the input prompt, resulting in a significant accuracy drop. In contrast, both FastV and our approach assess token importance via attention maps in the LVLMs. However, FastV employs a fixed token ratio and exhibits severe performance degradation on challenging tasks such as ChartQA (Masry et al., 2022). By implementing layer-wise adaptive ratio assignment, our proposed ZipVL consistently surpasses FastV across all five benchmarks and three model architectures, while maintaining a smaller overall ratio of important tokens. As illustrated in Figure 4, our method dynamically adjusts the ratio across various tasks and models, slightly increasing the ratio of important tokens for difficult tasks to preserve performance and enhancing efficiency on simpler tasks. Moreover, the performance gap between our method and FastV becomes more pronounced over the LLaVA-Next-13B model. This discrepancy can be attributed to the varying attention maps across different models and that FastV's predefined hyperparameters are not universally applicable, whereas our dynamic approach demonstrates high robustness.\n5.2.2 EVALUATION ON VIDEO BENCHMARKS\nWe also assess the performance of our method on the Video-MME benchmark (Fu et al., 2024) over the LongVA model (Zhang et al., 2024), which supports a maximum multimodal input length of 224K tokens. We compare our approach with semi-structured sparse attention methods such as MInference (Jiang et al., 2024) and QK-sparse (Pagliardini et al., 2023), as well as the structured sparse attention method FastV (Chen et al., 2024). The results are summarized in Table 2. Among these sparse attention methods, FastV (Chen et al., 2024) consistently retains a fixed proportion of tokens while MInference (Jiang et al., 2024) retains a fixed number of sparse blocks. Notably, our approach not only achieves the highest overall performance but also exhibits superior reductions in FLOPs within the attention module compared to other sparse attention methods. This demonstrates the effectiveness of employing dynamic token-level sparsity to accelerate the attention module in LVLMs. Furthermore, long videos inherently contain significant redundancy, and our method dynamically allocates the ratio of important tokens by analyzing the sparse attention maps, resulting in a higher FLOPs reduction ratio when processing 128-frame videos compared to 64-frame videos."}, {"title": "5.3 ABLATION STUDY", "content": "5.3.1 EFFECT OF THE LAYER-WISE ADAPTIVE RATIO\nIn this subsection, we evaluate the efficacy of the proposed adaptive ratio assignment scheme by integrating it with sparse attention and KV cache compression, as detailed in Table 3. Initially, we implement a fixed sparse attention scheme on LongVA-7B model over Video-MME benchmark. In this scheme, the ratio for important tokens remains constant across all attention layers and is fixed. Although this approach shares the same overall important token ratio and FLOPs reduction ratio as our method, it suffers from significant performance degradation (51.1% vs. 52.6%) due to its failure to account for the varying attention maps across layers. In contrast, our method achieves nearly lossless performance (52.4% vs. 52.6%) while reducing the FLOPs of attention mechanism by 82.3%.\nTo assess the efficacy of our method specifically for KV cache compression, we apply it to compress the KV cache of LLaMA3-8B (Meta, 2024) and evaluate its performance on the GSM8k dataset. The baseline method (He et al., 2024b) also utilizes mixed-precision quantization for KV cache but employs a fixed ratio for important tokens. For both the baseline and our method, important tokens are quantized to 4-bit, while other tokens are quantized to 2-bit. Notably, by adaptively determining the ratio of important tokens, our method achieves a significantly higher compression ratio (6.18 \u00d7 vs. 4.69\u00d7) while maintaining superior accuracy (54.06% vs. 53.75%). This demonstrates that our method also sets a new state-of-the-art for KV cache compression of LLMs.\n5.3.2 EFFECT OF THE THRESHOLD \u03c4\nWe further investigate the impact of the attention retention threshold \u03c4 on both the ratio of important tokens and model performance. The results are illustrated in Figure 5. Intuitively, a lower retention threshold leads to a reduced ratio for important tokens, thereby enhancing generation efficiency at the cost of performance degradation. Notably, the ratio decreases significantly as \u03c4 decreases but remains above 0.97, with minimal performance deterioration. Conversely, when \u03c4 falls below 0.97, substantial performance loss is observed, despite a gradual reduction in the ratio of important tokens. This indicates that the optimal range for \u03c4 lies around 0.97."}, {"title": "5.4 DEPLOYMENT EFFICIENCY", "content": "In this subsection, we present the prefill phase latency and GPU memory usage in Figure 6 to illustrate the real efficiency improvements achieved by ZipVL. Specifically, we first compare the prefill phase latency of ZipVL with that of the well-established semi-structured sparse attention method, MInference (Jiang et al., 2024), as shown in Figure 6a. Notably, MInference exhibits significant additional overhead when the sequence length is short and is notably slower than FlashAttention (Dao et al., 2022) for sequence lengths below 32K. In contrast, ZipVL achieves comparable latency to FlashAttention with short input sequences, while significantly reducing the prefill phase latency as the sequence length exceeds 32K. This can be attributed to the fact that the attention module's latency becomes the dominant factor in the total latency with long sequences. With an input sequence length of 200K, ZipVL achieves a 2.6\u00d7 reduction in prefill-phase latency.\nMoreover, MInference is not designed to reduce the KV cache size, while the proposed ZipVL jointly optimizes the attention computation in the prefill phase and the KV cache through the dynamic token ratio. Consequently, ZipVL presents a 50.0% reduction in GPU memory usage with an input sequence length of 64K (26,230MB vs. 52,514MB), as illustrated in Figure 6b."}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "In this paper, we have proposed ZipVL, an efficient inference framework tailored for LVLMs. ZipVL jointly optimizes both the prefill and decoding phases by assigning an adaptive ratio of important tokens. This ratio is dynamically adjusted based on the distribution of attention scores across each layer, ensuring that the majority of attention scores are preserved. After identifying important tokens through normalized attention scores, less significant tokens are excluded from attention computation during the prefill phase to alleviate the computational bottleneck. Additionally, their KV cache is quantized to a lower bit-width, mitigating the memory bottleneck in the decoding phase. Extensive experiments have demonstrated that ZipVL significantly enhances the generation efficiency of LVLMs, achieving up to a 2.6\u00d7 reduction in prefill phase latency and a 50% reduction in GPU memory usage. However, a limitation of our approach is its focus on sparse attention during the prefill phase only, while attention during the decoding phase and the multi-layer perceptron (MLP) modules in both phases remain dense. Future efforts may explore extending sparse computations to MLP modules or the attention mechanism in the decoding phase to further reduce computational complexity."}, {"title": "A EFFICIENT APPROXIMATION OF FULL ATTENTION SCORES", "content": "ZipVL requires accumulated attention scores to adaptively assign the ratio of important tokens and normalized attention scores to identify token importance. However, attention scores are not accessible in fast attention implementations such as FlashAttention (Dao et al., 2022). To integrate our method with FlashAttention, we follow prior literature (He et al., 2024b; Jiang et al., 2024) and select a subset of tokens, referred to as \u201cprobe tokens\" (He et al., 2024b), and explicitly compute their attention scores:\n$A_{probe} = Softmax(\\frac{Q_{probe} K^T}{\\sqrt{d_k}})$.\nThe approximate accumulated and normalized attention scores for each token can then be obtained accordingly based on $A_{probe}$. Prior work (He et al., 2024b) selects 10% of the tokens as probe tokens, which still yields quadratic complexity in Eq. 10. In contrast, we select only 64 recent tokens and 64 randomly positioned tokens, which incurs negligible computation overhead in long-context scenarios."}]}