{"title": "TensorTEE: Unifying Heterogeneous TEE Granularity for Efficient Secure Collaborative Tensor Computing", "authors": ["Husheng Han", "Xinyao Zheng", "Yuanbo Wen", "Yifan Hao", "Erhu Feng", "Ling Liang", "Jianan Mu", "Xiaqing Li", "Tianyun Ma", "Pengwei Jin", "Xinkai Song", "Zidong Du", "Qi Guo", "Xing Hu"], "abstract": "Heterogeneous collaborative computing with NPU and CPU\nhas received widespread attention due to its substantial per-\nformance benefits. To ensure data confidentiality and in-\ntegrity during computing, Trusted Execution Environments\n(TEE) is considered a promising solution because of its com-\nparatively lower overhead. However, existing heterogeneous\nTEE designs are inefficient for collaborative computing due\nto fine and different memory granularities between CPU and\nNPU. 1) The cacheline granularity of CPU TEE intensifies\nmemory pressure due to its extra memory access, and 2) the\ncacheline granularity MAC of NPU escalates the pressure on\nthe limited memory storage. 3) Data transfer across hetero-\ngeneous enclaves relies on the transit of non-secure regions,\nresulting in cumbersome re-encryption and scheduling.\nTo address these issues, we propose TensorTEE, a unified\ntensor-granularity heterogeneous TEE for efficient secure\ncollaborative tensor computing. First, we virtually support\ntensor granularity in CPU TEE to eliminate the off-chip\nmetadata access by detecting and maintaining tensor struc-\ntures on-chip. Second, we propose tensor-granularity MAC\nmanagement with predictive execution to avoid computa-\ntional stalls while eliminating off-chip MAC storage and ac-\ncess. Moreover, based on the unified granularity, we enable\ndirect data transfer without re-encryption and scheduling\ndilemmas. Our evaluation is built on enhanced Gem5 and\na cycle-accurate NPU simulator. The results show that Ten-\nsorTEE improves the performance of Large Language Model\n(LLM) training workloads by 4.0x compared to existing work\nand incurs only 2.1% overhead compared to non-secure train-\ning, offering a practical security assurance for LLM training.", "sections": [{"title": "1 Introduction", "content": "The heterogeneous collaborative computing with NPU and\nCPU has demonstrated significant performance and energy\nimprovement in various fields [12, 30, 36, 44, 64, 69-71],\nespecially when dealing with the challenges posed by the\nlimited DRAM capacity of NPUs, such as LLM training [70],\nautonomous driving [82], and AlphaFold2 [46], hence het-\nerogeneous collaborative architectures emerge as a trend\nin state-of-the-arts: Nvidia Grace Hopper [20] and AMD\nInstinct MI300 [5]. The improvement stems from the het-\nerogeneous collaborative computing system that can effec-\ntively leverage the characteristics of different architectures.\nFor example, ZeRO-offload [70] of DeepSpeed [68] offloads\nforward and backward computations to an NPU, while the\nCPU manages high-precision weights and optimizer states.\nSuch a heterogeneous collaborative way effectively alleviates\nDRAM storage pressure, thus enabling the training of a 10B\nparameter GPT-2 on a single V100 GPU [70].\nGiven that collaborative computing is equally important in\nterms of computing efficiency and data privacy, the outstand-\ning performance of heterogeneous collaborative computing\nhas increasingly sparked people's pursuit of security. This\ntrend makes Trusted Execution Environments (TEE), which\nhas been developed for CPU [13, 16, 53] and extended to NPU\narchitectures [13, 43, 56, 78, 84], gain significant attention, as\nit can provide secure enclaves with robust privacy guarantees\nand lower overhead than alternative solutions (such as Fully\nHomomorphic Encryption and Multiple Party Computation).\nSpecifically, TEE considers only the on-chip hardware trust-\nworthy, so both CPU/NPU off-chip memory and inter-chip\ncommunication require additional protection mechanisms\nagainst attacks from OS or physical attacks. In this paper,\nwe focus on discrete NPUs with off-chip GDDR memory fol-\nlowing the same configuration with recent works [2, 34, 62].\nHowever, existing heterogeneous TEE designs [34, 78],\nwith different memory protection granularities between CPU\nand NPU, incur significant performance overhead and are\nnot feasible for collaborative computing. They face efficiency\nissues from three aspects: 1) The cacheline granularity of\nVersion Number (VN) employed in CPU TEE generates sub-\nstantial additional memory accesses for VN and Merkle Tree\ntraversal, intensifies memory pressure in memory-intensive\nworkloads, and results in significant overhead. 2) Although\nVN is tensor-wise managed in NPUs, their cacheline granu-\nlarity MAC management brings extra storage overhead, fur-\nther escalating the strain on the already constrained memory\nstorage. 3) Due to the granularity differences in heteroge-\nneous TEEs, data transfer across heterogeneous enclaves\nrelies on the transit of non-secure regions, necessitating re-\nencryption and decryption to maintain data security. This\nre-encryption and decryption process introduces a substan-\ntial amount of extra memory access, which in turn leads to\na competition between data transfer and computation for\nmemory bandwidth, preventing parallel execution.\nTo address these issues, our work creates a unified TEE\nto efficiently support secure heterogeneous collaborative\ncomputing, eliminating VN overhead of intensive batching\ndata accesses in CPU, computation stalls caused by large-\ngranularity MAC in NPU, and re-encryption overhead of\nsecure communication channel. 1) For CPU, TensorTEE vir-\ntually supports the on-chip tensor-granularity VN manage-\nment (compatible with cacheline granularity), thus elimi-\nnating the off-chip VN access and Merkle Tree traversal\noverhead. TensorTEE efficiently detects and manages on-\nchip tensor structures by leveraging the dimensional-wise\nstreaming access pattern and flexible entries merging op-\neration. 2) For NPU, TensorTEE propose tensor-wise MAC\nmanagement with delayed verification to reduce storage\noverhead and eliminate computation stalls. It maintains data\nintegrity with the proposed tensor poison tracing and verifi-\ncation barrier mechanism to ensure that data tampering is\nlimited in the NPU enclave and can soon be detected before\ncommunication. To ensure code integrity, we restrict instruc-\ntion memory requests following the normal non-delayed\nverification dataflow. 3) For heterogeneous communication,\nthe unified granularity of TensorTEE brings the compatibil-\nity of ciphertext across heterogeneous enclaves, avoiding\ncumbersome re-encryption and scheduling in data transfer.\nThe main contributions of the paper are as follows:\n\u2022 We analyze the overhead stems from different granu-\nlarities between CPU and NPU in existing heteroge-\nneous TEE designs, which inspires our unified tensor\ngranularity heterogeneous TEE designs.\n\u2022 We propose TensorTEE, a unified granularity heteroge-\nneous TEE architecture, including the hardware-based\ntensor-granularity CPU TEE, the tensor-wise MAC\nmanagement with delayed verification, and a direct\ndata transfer protocol between secure enclave memory\nwithout re-encryption.\n\u2022 We evaluated TensorTEE with enhanced Gem5 and a\ncycle-accurate NPU simulator. Our evaluation results\nshow that TensorTEE improves the performance of\nLLM training by 4.0x compared to existing work and\nincurs only 2.1% overhead compared to non-secure\ntraining."}, {"title": "2 Background", "content": "2.1 Collaborative Computing\nDue to the distinct computational model and limitations\nof CPUs and NPUs, collaborative computing is becoming\nincreasingly popular [46, 70, 82].\nLLM CPU-NPU collaborative computing: Large lan-\nguage model training has tended to heterogeneous collab-\norative computing with NPU and CPU due to the limited\nNPU DRAM capacity [12, 30, 70]. This paper focuses on the\nZeRO-Offload [70] for evaluation study, which is a prevalent\nframework in collaborative training for LLM. The main rea-\nson is that ZeRO-Offload could offload phases with lower\ncomputational loads but high memory usage from the NPU\nto the CPU which effectively mitigates memory storage pres-\nsure on the NPU, thus enabling larger model training. For\nexample, by offloading the gradients and optimizer states to\nthe CPU while keeping forward and backward computation\non NPU, ZERO-Offload enables a 10x increase in training\nmodel size. As illustrated in Figure 1, the computation flow\nof ZeRO-Offload mainly involves three stages. First, the NPU\nperforms forward and backward computations, passing the\nupdated gradients to the CPU during backpropagation. Then,\nthe CPU conducts optimizer iterations to update optimizer\nstates and weights. Finally, the CPU transfers the updated\nweights back to the NPU.\n2.2 Memory Protection\nIn TEE, memory protection encompasses ensuring data con-\nfidentiality through encryption, integrity via Message Au-\nthentication Code (MAC) verification, and freshness with\nthe use of a monotonic counter."}, {"title": "Memory encryption", "content": "TEE typically employs counter\nmode AES for memory encryption, which introduces a counter\nthat consists of a physical address (PA) and a version number\n(VN). The VN increases with each write-back, against replay\nattack. Let \\(K_{AES}\\), P and C denote the AES encryption key,\nplaintext, and ciphertext respectively. Then, the encryption\ncan be formulated as \\(C = AES(K_{AES}, (PA, VN))\u2295P\\), where \u2295\nrepresents XOR. The decryption follows the same procedure\ndue to the XOR nature."}, {"title": "Mismatched granularity", "content": "Figure 2 illustrates two distinct\ngranularities for VN management. Firstly, the CPU holds a\ndedicated VN for each cacheline to support random memory\naccess (Figure 2 (a)). However, the fine granularity, such as\na 56-bit VN for a 64B data block, introduces large storage\noverhead (11%) and memory access overhead [13]. Moreover,\noff-chip VNs rely on the Merkle Tree to ensure their integrity.\nSecondly, recent works [34, 78] embrace tensor-wise VN for\nNPU, leveraging the regular memory access patterns preva-\nlent in NPU workloads (Figure 2 (b)). With tensor granularity,\na single VN is used for all cachelines within the tensor. This\napproach significantly reduces storage pressure and enables\non-chip storage of VNs, thereby reducing memory access for\nVNs and eliminating the need for Merkle Tree."}, {"title": "Limitations of existing work", "content": "SoftVN [83] preserves tensor\nstructure within the on-chip VN Table by explicitly declar-\ning it in the software code. It performs efficiently in simple\nstatic scenarios with minimal hardware overhead. However,\nSoftVN has certain limitations: 1) Inapplicable in complex and\ndynamic scenarios: SoftVN [83] assumes a static and simple\ndata flow in programs. However, in LLM training, various\nruntime parallel strategies and further complex operations\nlike tensor splitting and merging [67] exist, making it chal-\nlenging to trace the data flow of tensors and specify VN at\neach writeback in the top-level user code. 2) Dilemma for\nimproving practicability: Since the acquisition of VNs occurs\non critical paths (cache access) in SoftVN, increasing the\nmanageable number of tensor entries results in performance\ndegradation. This puts SoftVN in a dilemma between prac-\nticality and high performance under complex applications.\n3) Wastage of Entries: In scenarios where a single tensor is\nused in parallel across multiple cores, SoftVN leads to each\nsubtensor occupying a VN Table entry in the correspond-\ning core. This contributes to the exacerbation of capacity\npressure, underscoring a critical limitation of the SoftVN."}, {"title": "Integrity verification", "content": "To ensure data integrity, the MAC\nof the data is calculated and stored in DRAM during write-\nback. In subsequent reads the MAC is recalculated and val-\nidated to ensure that it is identical to the one from off-\nchip storage. The MAC calculation can be represented as\n\\(MAC = Hash(K_{MAC}, (C, PA, VN))\\). To prevent replay at-\ntacks, the MAC should be stored on-chip. However, due to\nlimited on-chip storage, VN and Merkle Tree are employed to\nreduce on-chip storage overhead. BMT [72] further proposes\nthat the Merkle Tree only needs to protect the VN, reducing\nthe width of the Merkle Tree. Eventually, the root node of\nthe Merkle Tree is securely stored on-chip while VNs and\nMACs are stored off-chip. The integrity verification for each\nmemory access relies on recursive layer-by-layer verification\nof the Merkle Tree, resulting in significant overhead. For the\nNPU, the VNs are securely stored on-chip, eliminating the\nneed for iterative access to the Merkle Tree."}, {"title": "2.3 Heterogeneous NPU TEE", "content": "Recent works have extended CPU TEE to heterogeneous\nTEE for mission-critical computations within NPU. Heteroge-\nneous NPU TEE can be classified into integrated and discrete\ntypes based on their connectivity with the CPU.\nIntegrated NPU TEE: Integrated architecture places both\nthe CPU and NPU on the same chip and shares the same\nDRAM, thus with no problem for data transfer between\nCPU and NPU [15, 56]. However, The integrated NPU TEE's\nperformance is limited due to power and area constraints,\nwhich makes it insufficient for demanding applications such\nas LLM training that involve large computations.\nDiscrete NPU TEE: Discrete heterogeneous architecture\nhas separate CPU and NPU chips and connects them with\ndata buses like GDDR. The discrete NPU, featuring dedicated\ncontrollers and memory, provides enhanced computational\npower to support LLM training. Existing works on NPU\nmemory protection prefer tensor-wise VN management for\nperformance: Common Counter [62] scans memory during\nkernel switches, saving metadata on the chip for data re-\ngions with the same VN, which reduces additional memory\naccess overhead for metadata. MGX [34] and Securator [78]\non the other hand generate corresponding VNs for each\nmemory access based on on-chip execution status during\nkernel execution, minimizing VN storage and access over-\nhead. Since the discrete heterogeneous architecture is more\nwidely adopted in the product environment, we will focus\non the discrete heterogeneous TEE."}, {"title": "2.4 Threat Model", "content": "As a heterogeneous system, our Trusted Computing Base\n(TCB) only consists of the on-chip architectures (like CPU,\nNPU, and caches). The off-chip host memory and NPU GDDR\nmemory reside outside the TCB. For NPU, following the\nsame configuration with recent works [2, 34, 62], we con-\nsider NPUs with off-chip connected GDDR memory. The\noff-chip memory is vulnerable to physical attacks [88] like\nbus snooping attack [6, 35, 37, 77] with mature toolkits al-\nready developed [10] and cold boot attack as assumed by\nIntel TDX [42], AMD SEV [48].\nWe assume the adversary has full control over the OS\nor privileged software and can carry out physical attacks.\nThe adversary can snoop the bus (memory bus or PCIe\nbus) [35, 37] or employ cold boot attacks [1, 42, 56] to by-\npass security isolation for data theft. They can also ma-\nnipulate the bus signals for data corruption or relay at-\ntacks. We do not consider other side-channel attacks such\nas timing-based, power-based, and electromagnetic-based\nattacks [39, 47, 49, 52, 63], as well as denial-of-service attacks\nand adversarial attacks [9, 27, 28]. Model extraction attacks\ncan be mitigated by orthogonal approaches [80]."}, {"title": "3 Motivation", "content": "3.1 Inefficient CPU TEE Computation with\nCacheline-Wise Metadata\nIn heterogeneous collaborative LLM training, the CPU is re-\nsponsible for optimizer updates like the Adam optimizer step.\nFigure 3 shows that up to 3.7x slowdown is caused when\nTEE is introduced. In the SGX environment, as the number\nof threads increases, the performance benefits gradually di-\nminish, indicating a transition from compute-intensive to\nmemory-intensive tasks. The increased memory access is\nmainly attributed to metadata such as VN and MAC.\nFor the Adam update, the tensor data are element-wisely\naccessed. The regular access pattern enables memory protec-\ntion at the tensor granularity. Figure 4 illustrates the number\nand size of tensors during optimizer updates for different\nmodels. We observe that the tensor sizes grow to MBytes,\nbut the growth rate of tensor numbers is slow, reaching only\na few hundred. The data characteristic of small numbers and\nlarge sizes of tensors means that the memory protection of\ntensor granularity will achieve great performance gain with\na small overhead. However, CPUs always access memory at\nthe granularity of cachelines, and the memory organization\nor instructions do not have tensor types.\n3.2 Cacheline-Wise MAC of NPU Leads to Large\nStorage and Performance Overhead\nThe memory resources on the NPU are limited, compared to\nCPU memory of up to 512GB on server CPUs, NPU memory\ncapacity is usually only 40GB. The storage of enclave meta-\ndata further exacerbates the shortage of memory resources.\nRecent NPU works with tensor-granularity VN [34, 79] re-\nduce VN's storage overhead, yet close to 10% of the storage\noverhead and 12% performance overhead from MAC remains\nunresolved. Plutus [2] employs predictive execution to miti-\ngate additional memory accesses for MAC, but it does not\naddress the storage overhead. MGX [34] and GuardNN [33]\nelevate the granularity of MAC from 64B to 512B, but they\ndo not consider the computation stall issue resulting from\nthe increased granularity (see Section 4.3). Therefore, an in-\ntegrity verification method that can reduce both storage and\nperformance overhead is urgently needed.\nTo address the above challenges, we propose leveraging\ntensor-wise MAC with delayed verification and offer the\nfollowing assurances for integrity with delayed verification:\n1) Tampering can only occur on data but not code and tam-\npered data can be soon detected. 2) Tampered data on the"}, {"title": "3.3 Inefficient Data Transfer Protocols", "content": "Figure 5 shows that the communication only occupies 12%\noverhead in non-secure mode, but significantly increases to\n53% in the baseline system where CPU with SGX-like TEE\nand NPU with MGX-like TEE. The reasons are as follows:\nMismatched protection granularity of CPU and NPU in-\ncurs extra re-encryption. CPU and NPU have a mismatched\ngranularity in that CPU performs protection on cacheline\ngranularity while NPU on tensor granularity. Due to the\ngranularity incompatibility, the encrypted data of CPU (or\nNPU) TEE can not be correctly decrypted by NPU (or CPU)\nTEE even with the same secret key since their fixed hardware\nengine and different metadata format. Therefore, communi-\ncation relies on non-secure memory as a relay (Figure 6 (a)).\nThe sender needs to decrypt the data of the secure memory\nregion and save it in the non-secure area. After communica-\ntion, the receiver loads the data into the secure region. To\nprotect the privacy of the data, the data in the non-secure\nregion need to be re-encrypted.\nThe bandwidth contention caused by communication (re-\nencryption) and computation (IO read/write) results in both\ntasks being forced to execute sequentially (Figure 7), even\nwhen the encryption engines are sufficient. For resource-\nconstrained NPUs, the hardware cost limits the deployment\nof multiple encryption engines, resulting in the contention\nof encryption bandwidth [90]. Recent work SecureLoop [55]\nreports that a fully-pipelined cryptographic engine accounts\nfor nearly 35% of the logic gates in an Eyeriss-like NPU.\nIn our simulation, one AES engine cannot provide enough\nbandwidth even for NPU computing (8 GB/s provided, but\nat least 20 GB/s is needed), thus communication data have\nto be delayed and serialized with NPU computing. Even\nfor large NPUs with sufficient AES engines, re-encryption\nduring communication exacerbates the pressure of memory\nbandwidth and blocks the IO relevant to computation. The\nre-encryption process is extremely memory-intensive and\noccupies the most DRAM bandwidth (84%) in our simulation,\nhindering its parallelization with NPU computation. Both of\nthe above indicate inefficient dataflow caused by the sequen-\ntial execution of computation and communication. In this\npaper, we assume each channel has its dedicated encryption\nengine to balance the trade-off between hardware overhead\nand bandwidth requirements."}, {"title": "3.4 Optimization Goal", "content": "These observations inspire a unified tensor-granularity het-\nerogeneous TEE architecture for several benefits: 1) The\ntensor-wise CPU TEE with on-chip VN reduces memory ac-\ncess for metadata. 2) Tensor-wise MAC management of NPU\nmitigates storage and performance overhead while ensuring\nNPU data security. 3) The unified heterogeneous tensor-wise\nTEE enables direct data transfer between secure memory."}, {"title": "4 TensorTEE Design", "content": "4.1 Overview\nOur overall architecture, as shown in Figure 8, introduces\nthree optimizations:\nTensor-wise memory protection on CPU: To unify the pro-\ntection granularity and improve the CPU TEE performance\non collaborative computing, we propose the hardware-based\nTensor-Analyzer (TenAnalyzer) component for tensor-wise\nmemory protection. TenAnalyzer receives and analyzes mem-\nory access requests from the CPU cores, constructing and\nmaintaining the Meta Table data structure in run-time. Each\nentry of the Meta Table holds shared metadata (like VN) for\nall cachelines within a tensor, eliminating the overhead of\nmetadata memory access in subsequent memory requests.\nOur design is optional and complementary to the traditional\nprotection method like SGX [13], enabling or disabling tensor\nmanagement through the Enable Tensor-wise Management\nFlag (EnTMF) without affecting the software programming.\nIn non-tensor applications, tensor-wise management is dis-\nabled with negligible overhead.\nTensor-wise MAC with delayed verification on NPU: To im-\nprove both the performance and storage efficiency, Tensor-\nTEE proposes tensor-wise MAC management with delayed\nverification for only NPU tensor data. Delayed verification\nenables parallelization between verification and computa-\ntion, eliminating stalls caused by coarse-grained MAC verifi-\ncation. To ensure integrity, TensorTEE 1) restricts code access\nfollowing the normal non-delayed verification dataflow to\nmaintain code integrity and prevent any potential attacks\nand 2) ensures the communication data integrity when they\nleave the NPU enclave by proposed tensor poison tracing and\nverification barrier mechanism.\nImplementation of direct data transfer protocol: Leveraging\na unified tensor granularity, we enable an efficient data trans-\nfer protocol that eliminates the transit through non-secure\nregions and enables the parallel between data transfer and\ncomputation. The protocol contains two channels: A trusted\nchannel is used for the encrypted transfer of on-chip meta-\ndata and a direct channel is for the transfer of ciphertext\ntensors between heterogeneous secure enclaves."}, {"title": "4.2 CPU TEE with Tensor Granularity", "content": "To increase practicability and alleviate the burden on pro-\ngrammers, a hardware-based tensor-wise CPU memory man-\nagement mechanism is required. It should incorporate tensor\nmanagement capabilities while maintaining low overhead.\nChallenges of hardware-based tensor data structure\ndetection: To address the entry wastage issue, the tensor\nmanagement structure should be located outside the cores,\nsuch as in the Memory Controller (MC). However, there are\nchallenges with tensor detection and maintenance in the\nMC: 1) Cache interference: As shown in Figure 9, the pro-\ncess switches lead to random cache line evictions, disrupting\nmemory access patterns. Additionally, cache prefetchers like\nstride prefetchers shuffle the access order when the wrong\nprefetch request is issued. 2) MC receives physical addresses,\nand their contiguity is limited by the page size. 3) Long\nlatency: The access of the large-capacity cache before the\nrequest reaches the MC introduces significant memory ac-\ncess latency, resulting in long latency for maintaining and\naccessing tensor data structures.\nTensor structure management in CPU TEE: To ad-\ndress the aforementioned challenges, we propose TenAna-\nlyzer, located within the MC, which directly receives memory\naccess requests from the cores, as shown in Figure 8. TenAn-\nalyzer offers the following advantages: 1) It receives memory\naccess requests directly from the cores, avoiding cache inter-\nference from evicting and prefetching as shown in Figure 9\n(a). 2) Leveraging virtual addresses from the cores maximizes\nthe utilization of tensor contiguity as shown in Figure 9\n(b). To avoid the same VA problems, each enclave uses its\ndedicated key and the Meta Table is saved and restored for\ncontext-switching cases. 3) Detection, maintenance, and ac-\ncess of tensor data structures can be parallelized with multi-\nlevel caches, effectively hiding their access latency. Thus,\nTenAnalyzer overcomes the limitations of SoftVN [83] and\nprovides an efficient and scalable solution for tensor-based\nmemory management."}, {"title": "Tensor Filter", "content": "TenAnalyzer mainly contains two components: Meta Ta-\nble and Tensor Filter, as shown in Figure 10. 1) Meta Table\nmaintains the tensor structure and can be constantly up-\ndated by boundary hit. Specifically, the Meta Table receives\nthe access request of the core, and when it hits, the address\nand VN are passed to the MEE for encryption, which avoids\nthe extra access for metadata in memory. 2) Tensor Filter\nhandles the Meta Table misses, collecting and analyzing the\nrequest addresses. When a specific entry in the Tensor Filter\nreaches its collection address limit (4 in our setting), it checks\nif the entry meets the tensor condition: having the same VN\nand a consistent pattern between addresses. The successfully\nchecked entry is then populated into the Metadata Table as\nthe initialization structure for tensors. Besides, other com-\nponents, a bitmap and two flags for indicating the cacheline\nupdate states and the EnTMF flag for enabling or disabling\nTenAnalyzer, are used."}, {"title": "Dataflow for reading (or detection)", "content": "As shown in Fig-\nure 10, TenAnalyzer has three cases when a request arrives:\nhit in, hit boundary, and miss. 1) Hit in means that the re-\nquest hits the address range of an entry in the Meta Table,\nand then the corresponding VN will be transmitted to MEE\nto start the encryption/decryption. 2) Hit boundary means\nthat the request hits the boundary of the address range of an\nentry (request address == last address + stride) in the Meta\nTable that the VN cannot guarantee to be correct. At this\ntime, assuming that this VN is correct, the VN is still sent to\nMEE, and at the same time, the VN access in DRAM is started.\nIf the VN in DRAM is consistent with the assumed VN, the\nrelevant Meta Table entry is updated and its address range is\nexpanded, which leads to gradual coverage of tensor detection.\n3) Miss means that there is no matching in the Meta Table,\nthen the VN will be obtained by the DRAM access and this\nrequest is sent to the Tensor Filter for subsequent pattern de-\ntection and filtering. For reading, the on-chip tensor-wise VN\nalways remains equivalent to the off-chip cacheline-wise VN\nwhile efficiently providing VN to minimize off-chip memory\naccess and Merkle Tree traversal.\nTenAnalyzer efficiently detects tensors with complex ac-\ncess patterns through entries merging. For streaming opera-\ntions, such as Adam's update, a single entry is sufficient for\nthe detection and management of large tensors (Figure 11\n(a)). However, for tensor operations requiring tiling, such as\nmatrix multiplication, detecting complete tensor information\nwith a single entry becomes challenging. Nevertheless, Ten-\nAnalyzer can still manage each short streaming data like a\nrow of a tile and infer the tile dim (e.g. the first blue row with\na size of d1) based on the dimensional-wise streaming access\npattern. Next, TenAnalyzer attempts to merge a few recently\nupdated entries when creating new entries. The merge op-\neration is not restricted to address-adjacent tiles but allows\nmerging in multiple directions, enabling more timely merges\nand more efficient on-chip storage (2 directions for 1D tensor,\n4 directions for 2D tensor and 6 directions for 3D tensor). To\nmerge tiles with non-adjacent addresses, it's required that\nthe tile dims, stride, and VN match. The successful merge\nallows inferring the tensor dimension (e.g. merging the two\nblue rows infers the tensor dimension D1). The inferred di-\nmension can serve as a constraint for subsequent merges\nto improve the merge accuracy. By iterating this process,\nunified management of the tiled tensors can be achieved as\nshown in Figure 11 (b).\nTenAnalyzer gradually completes the tensor structure but\nwith extra memory access to verify its correctness. Since\nthe data transfer instructions from NPU typically include\ntensor structure information, such as address, size, and stride,\nTensorTEE utilizes them to update the Meta Table entry with\na large overlapped address range to speed up the tensor\nstructure creation in the CPU."}, {"title": "Dataflow for writing (or update)", "content": "During tensor writ-\ning, TenAnalyzer efficiently maintains the correct updates\nof tensor VN by confirming each cacheline within a tensor\nmust update only once within a tensor update using a bitmap\n(BM), Updating Flag (UF) and Bit State (BS). The bitmap and\naddress range log cacheline updates for a tensor, flipping\nbits at addresses upon write requests. UF denotes an updat-\ning tensor, and BS shows the pre-update bitmap bit value.\nTenAnalyzer increments the VN after updating all tensor\ncachelines (all relevant bitmap bits flip). Hardware isolation\nmechanism secures the full bitmap in DRAM, and a small\non-chip cache enhances bitmap access efficiency [21].\nTenAnalyzer encompasses 3 scenarios in tensor writing as\nillustrated in Figure 12: Hit edge, Hit in and Miss. 1) Hit edge\nmeans the request hits the first address (start updating) or\nlast address (finish updating) which matches the common\ntensor-based application and allows complex memory access\nlike tiling. The bitmap bit flips after checking with BS to be\nequal, marking the cacheline updated. When the last cache-\nline address arrives, all bitmap bits in the address range are\nchecked to be equal to the flipped BS to indicate the tensor\nupdating finish; then the on-chip tensor increments, BS flips,\nand UF resets. 2) Hit in means the request hits the address\nrange but not the edge. After checking the UF and bitmap\nstate, we flip the bitmap bit. 3) Miss indicates out-of-range\naddresses, requiring only off-chip VN updates. Note that the\nwriting addresses from cores are filtered by LLC to match the\noff-chip data update. Writing is performed on background,\nand not on the critical path.\nThe assertions (Assert1, 2 and 3) ensure that each cacheline\nmust be updated only once before the tensor update finishes\nto guarantee the correct update of on-chip tensor-wise VN.\nAssert1 ensures that each cacheline can not be updated be-\nfore the tensor update starts, while Assert2 ensures that\neach cacheline must be updated once when the tensor up-\ndate finishes. The access pattern is common in tensor-based\napplications. When the assertion is violated, the Meta Table\nentry is invalidated, preventing inconsistencies between the\ntensor VN and the off-chip VN in the following corner cases:\n1) The detected tensor is mixed with non-tensor variables."}, {"title": "4.3 Delayed Verification for Tensor-Wise MAC", "content": "The granularity of MAC trades off the storage overhead and\nverification overhead. Cacheline-wise MAC results in large\nstorage overhead in the NPU, which further hinders the de-\nployment of LLM workloads on the NPU due to the limited\nmemory capacity. Recent studies [33, 34], have opted for a\nMAC granularity of 512 bytes, thus establishing a balance\nbetween storage overhead and verification cost. However,\nthe use of large granularity MAC results in later verifica-\ntion, leading to computation stalls for already decrypted\ncachelines as shown in Figure 13 (b). Specifically, despite\nhaving the same time for MAC re-generation at both granu-\nlarities, the blocked computation by later MAC verification\nstages results in many pipeline bubbles, causing the ineffi-\ncient pipeline (13% overhead with 4KB granularity).\nTo reduce storage overhead while avoiding computing\nstalls, leveraging the characteristics of LLM collaborative\ncomputing, we propose a tensor-wise MAC management\nmethod with delayed integrity verification. We parallel the\nkernel computation for decrypted data and re-generation\ntensor MAC as shown in Figure 13 (c). After completing the\ntensor-wise MAC, we perform the integrity verification for\nthe entire tensor. In this way, we significantly reduce the\nstorage overhead and memory traffic while avoiding large\nbubbles caused by the long stalls as shown in Figure 13 (b).\nFor tensor-wise MAC calculations, we adopt a simple and\nefficient algorithm that involves XORing the MAC values of\nall cachelines which is similar to the MEE [24]. The algorithm\ncan be formulated as follows where \\(MAC_{i}\\) means the MAC\nfor i-th cacheline in the tensor and n is the number of cache\nlines: \\(MAC_{tensor} = \u2295 MAC_{i} = MAC_{0}\u2295 MAC_{1}...\u2295 MAC_{n-1}\\). The XOR-\nbased algorithm is insensitive to order, allowing various\noptimizations in NPU computing like tensor tiling."}, {"title": "Integrity guarantee", "content": "We ensure the integrity of tensor\ndata on NPU computation with a strict integrity guarantee\non code"}]}