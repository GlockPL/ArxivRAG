{"title": "TensorTEE: Unifying Heterogeneous TEE Granularity for Efficient Secure Collaborative Tensor Computing", "authors": ["Husheng Han", "Xinyao Zheng", "Yuanbo Wen", "Yifan Hao", "Erhu Feng", "Ling Liang", "Jianan Mu", "Xiaqing Li", "Tianyun Ma", "Pengwei Jin", "Xinkai Song", "Zidong Du", "Qi Guo", "Xing Hu"], "abstract": "Heterogeneous collaborative computing with NPU and CPU has received widespread attention due to its substantial performance benefits. To ensure data confidentiality and integrity during computing, Trusted Execution Environments (TEE) is considered a promising solution because of its comparatively lower overhead. However, existing heterogeneous TEE designs are inefficient for collaborative computing due to fine and different memory granularities between CPU and NPU. 1) The cacheline granularity of CPU TEE intensifies memory pressure due to its extra memory access, and 2) the cacheline granularity MAC of NPU escalates the pressure on the limited memory storage. 3) Data transfer across heterogeneous enclaves relies on the transit of non-secure regions, resulting in cumbersome re-encryption and scheduling. To address these issues, we propose TensorTEE, a unified tensor-granularity heterogeneous TEE for efficient secure collaborative tensor computing. First, we virtually support tensor granularity in CPU TEE to eliminate the off-chip metadata access by detecting and maintaining tensor structures on-chip. Second, we propose tensor-granularity MAC management with predictive execution to avoid computational stalls while eliminating off-chip MAC storage and access. Moreover, based on the unified granularity, we enable direct data transfer without re-encryption and scheduling dilemmas. Our evaluation is built on enhanced Gem5 and a cycle-accurate NPU simulator. The results show that TensorTEE improves the performance of Large Language Model (LLM) training workloads by 4.0x compared to existing work and incurs only 2.1% overhead compared to non-secure training, offering a practical security assurance for LLM training.", "sections": [{"title": "1 Introduction", "content": "The heterogeneous collaborative computing with NPU and CPU has demonstrated significant performance and energy improvement in various fields [12, 30, 36, 44, 64, 69-71], especially when dealing with the challenges posed by the limited DRAM capacity of NPUs, such as LLM training [70], autonomous driving [82], and AlphaFold2 [46], hence heterogeneous collaborative architectures emerge as a trend in state-of-the-arts: Nvidia Grace Hopper [20] and AMD Instinct MI300 [5]. The improvement stems from the heterogeneous collaborative computing system that can effectively leverage the characteristics of different architectures. For example, ZeRO-offload [70] of DeepSpeed [68] offloads forward and backward computations to an NPU, while the CPU manages high-precision weights and optimizer states. Such a heterogeneous collaborative way effectively alleviates DRAM storage pressure, thus enabling the training of a 10B parameter GPT-2 on a single V100 GPU [70]. Given that collaborative computing is equally important in terms of computing efficiency and data privacy, the outstanding performance of heterogeneous collaborative computing the unified granularity of TensorTEE brings the compatibility of ciphertext across heterogeneous enclaves, avoiding cumbersome re-encryption and scheduling in data transfer. The main contributions of the paper are as follows: \n\u2022 We analyze the overhead stems from different granularities between CPU and NPU in existing heterogeneous TEE designs, which inspires our unified tensor granularity heterogeneous TEE designs. \n\u2022 We propose TensorTEE, a unified granularity heterogeneous TEE architecture, including the hardware-based tensor-granularity CPU TEE, the tensor-wise MAC management with delayed verification, and a direct data transfer protocol between secure enclave memory without re-encryption. \n\u2022 We evaluated TensorTEE with enhanced Gem5 and a cycle-accurate NPU simulator. Our evaluation results show that TensorTEE improves the performance of LLM training by 4.0x compared to existing work and incurs only 2.1% overhead compared to non-secure training."}, {"title": "2 Background", "content": "2.1 Collaborative Computing Due to the distinct computational model and limitations of CPUs and NPUs, collaborative computing is becoming increasingly popular [46, 70, 82]. LLM CPU-NPU collaborative computing: Large language model training has tended to heterogeneous collaborative computing with NPU and CPU due to the limited NPU DRAM capacity [12, 30, 70]. This paper focuses on the ZeRO-Offload [70] for evaluation study, which is a prevalent framework in collaborative training for LLM. The main reason is that ZeRO-Offload could offload phases with lower computational loads but high memory usage from the NPU to the CPU which effectively mitigates memory storage pressure on the NPU, thus enabling larger model training. For example, by offloading the gradients and optimizer states to the CPU while keeping forward and backward computation on NPU, ZERO-Offload enables a 10x increase in training model size. As illustrated in Figure 1, the computation flow of ZeRO-Offload mainly involves three stages. First, the NPU performs forward and backward computations, passing the updated gradients to the CPU during backpropagation. Then, the CPU conducts optimizer iterations to update optimizer states and weights. Finally, the CPU transfers the updated weights back to the NPU. 2.2 Memory Protection In TEE, memory protection encompasses ensuring data confidentiality through encryption, integrity via Message Authentication Code (MAC) verification, and freshness with the use of a monotonic counter."}, {"title": "Memory encryption:", "content": "TEE typically employs counter mode AES for memory encryption, which introduces a counter that consists of a physical address (PA) and a version number (VN). The VN increases with each write-back, against replay attack. Let $K_{AES}$, P and C denote the AES encryption key, plaintext, and ciphertext respectively. Then, the encryption can be formulated as $C = AES(K_{AES}, (PA, VN)) \\oplus P$, where $\\oplus$ represents XOR. The decryption follows the same procedure due to the XOR nature."}, {"title": "Mismatched granularity:", "content": "Figure 2 illustrates two distinct granularities for VN management. Firstly, the CPU holds a dedicated VN for each cacheline to support random memory access (Figure 2 (a)). However, the fine granularity, such as a 56-bit VN for a 64B data block, introduces large storage overhead (11%) and memory access overhead [13]. Moreover, off-chip VNs rely on the Merkle Tree to ensure their integrity. Secondly, recent works [34, 78] embrace tensor-wise VN for NPU, leveraging the regular memory access patterns prevalent in NPU workloads (Figure 2 (b)). With tensor granularity, a single VN is used for all cachelines within the tensor. This approach significantly reduces storage pressure and enables on-chip storage of VNs, thereby reducing memory access for VNs and eliminating the need for Merkle Tree."}, {"title": "Limitations of existing work:", "content": "SoftVN [83] preserves tensor structure within the on-chip VN Table by explicitly declaring it in the software code. It performs efficiently in simple static scenarios with minimal hardware overhead. However, SoftVN has certain limitations: 1) Inapplicable in complex and dynamic scenarios: SoftVN [83] assumes a static and simple data flow in programs. However, in LLM training, various runtime parallel strategies and further complex operations like tensor splitting and merging [67] exist, making it challenging to trace the data flow of tensors and specify VN at each writeback in the top-level user code. 2) Dilemma for improving practicability: Since the acquisition of VNs occurs on critical paths (cache access) in SoftVN, increasing the manageable number of tensor entries results in performance degradation. This puts SoftVN in a dilemma between practicality and high performance under complex applications. 3) Wastage of Entries: In scenarios where a single tensor is used in parallel across multiple cores, SoftVN leads to each subtensor occupying a VN Table entry in the corresponding core. This contributes to the exacerbation of capacity pressure, underscoring a critical limitation of the SoftVN."}, {"title": "Integrity verification:", "content": "To ensure data integrity, the MAC of the data is calculated and stored in DRAM during write-back. In subsequent reads the MAC is recalculated and validated to ensure that it is identical to the one from off-chip storage. The MAC calculation can be represented as $MAC = Hash(K_{MAC}, (C, PA, VN))$. To prevent replay attacks, the MAC should be stored on-chip. However, due to limited on-chip storage, VN and Merkle Tree are employed to reduce on-chip storage overhead. BMT [72] further proposes that the Merkle Tree only needs to protect the VN, reducing the width of the Merkle Tree. Eventually, the root node of the Merkle Tree is securely stored on-chip while VNs and MACs are stored off-chip. The integrity verification for each memory access relies on recursive layer-by-layer verification of the Merkle Tree, resulting in significant overhead. For the NPU, the VNs are securely stored on-chip, eliminating the need for iterative access to the Merkle Tree."}, {"title": "2.3 Heterogeneous NPU TEE", "content": "Recent works have extended CPU TEE to heterogeneous TEE for mission-critical computations within NPU. Heterogeneous NPU TEE can be classified into integrated and discrete types based on their connectivity with the CPU. Integrated NPU TEE: Integrated architecture places both the CPU and NPU on the same chip and shares the same DRAM, thus with no problem for data transfer between CPU and NPU [15, 56]. However, The integrated NPU TEE's performance is limited due to power and area constraints, which makes it insufficient for demanding applications such as LLM training that involve large computations. Discrete NPU TEE: Discrete heterogeneous architecture has separate CPU and NPU chips and connects them with data buses like GDDR. The discrete NPU, featuring dedicated controllers and memory, provides enhanced computational power to support LLM training. Existing works on NPU memory protection prefer tensor-wise VN management for performance: Common Counter [62] scans memory during"}, {"title": "2.4 Threat Model", "content": "As a heterogeneous system, our Trusted Computing Base (TCB) only consists of the on-chip architectures (like CPU, NPU, and caches). The off-chip host memory and NPU GDDR memory reside outside the TCB. For NPU, following the same configuration with recent works [2, 34, 62], we consider NPUs with off-chip connected GDDR memory. The off-chip memory is vulnerable to physical attacks [88] like bus snooping attack [6, 35, 37, 77] with mature toolkits already developed [10] and cold boot attack as assumed by Intel TDX [42], AMD SEV [48]. We assume the adversary has full control over the OS or privileged software and can carry out physical attacks. The adversary can snoop the bus (memory bus or PCIe bus) [35, 37] or employ cold boot attacks [1, 42, 56] to bypass security isolation for data theft. They can also manipulate the bus signals for data corruption or relay attacks. We do not consider other side-channel attacks such as timing-based, power-based, and electromagnetic-based attacks [39, 47, 49, 52, 63], as well as denial-of-service attacks and adversarial attacks [9, 27, 28]. Model extraction attacks can be mitigated by orthogonal approaches [80]."}, {"title": "3 Motivation", "content": "3.1 Inefficient CPU TEE Computation with Cacheline-Wise Metadata In heterogeneous collaborative LLM training, the CPU is responsible for optimizer updates like the Adam optimizer step. Figure 3 shows that up to 3.7x slowdown is caused when TEE is introduced. In the SGX environment, as the number of threads increases, the performance benefits gradually diminish, indicating a transition from compute-intensive to memory-intensive tasks. The increased memory access is mainly attributed to metadata such as VN and MAC. For the Adam update, the tensor data are element-wisely accessed. The regular access pattern enables memory protection at the tensor granularity. Figure 4 illustrates the number and size of tensors during optimizer updates for different models. We observe that the tensor sizes grow to MBytes, but the growth rate of tensor numbers is slow, reaching only a few hundred. The data characteristic of small numbers and large sizes of tensors means that the memory protection of tensor granularity will achieve great performance gain with"}, {"title": "3.2 Cacheline-Wise MAC of NPU Leads to Large Storage and Performance Overhead", "content": "The memory resources on the NPU are limited, compared to CPU memory of up to 512GB on server CPUs, NPU memory capacity is usually only 40GB. The storage of enclave metadata further exacerbates the shortage of memory resources. Recent NPU works with tensor-granularity VN [34, 79] reduce VN's storage overhead, yet close to 10% of the storage overhead and 12% performance overhead from MAC remains unresolved. Plutus [2] employs predictive execution to mitigate additional memory accesses for MAC, but it does not address the storage overhead. MGX [34] and GuardNN [33] elevate the granularity of MAC from 64B to 512B, but they do not consider the computation stall issue resulting from the increased granularity (see Section 4.3). Therefore, an integrity verification method that can reduce both storage and performance overhead is urgently needed. To address the above challenges, we propose leveraging tensor-wise MAC with delayed verification and offer the following assurances for integrity with delayed verification: 1) Tampering can only occur on data but not code and tampered data can be soon detected. 2) Tampered data on the"}, {"title": "3.3 Inefficient Data Transfer Protocols", "content": "Figure 5 shows that the communication only occupies 12% overhead in non-secure mode, but significantly increases to 53% in the baseline system where CPU with SGX-like TEE and NPU with MGX-like TEE. The reasons are as follows: Mismatched protection granularity of CPU and NPU incurs extra re-encryption. CPU and NPU have a mismatched granularity in that CPU performs protection on cacheline granularity while NPU on tensor granularity. Due to the granularity incompatibility, the encrypted data of CPU (or NPU) TEE can not be correctly decrypted by NPU (or CPU)"}, {"title": "3.4 Optimization Goal", "content": "These observations inspire a unified tensor-granularity heterogeneous TEE architecture for several benefits: 1) The tensor-wise CPU TEE with on-chip VN reduces memory access for metadata. 2) Tensor-wise MAC management of NPU mitigates storage and performance overhead while ensuring NPU data security. 3) The unified heterogeneous tensor-wise TEE enables direct data transfer between secure memory."}, {"title": "4 TensorTEE Design", "content": "4.1 Overview Our overall architecture, as shown in Figure 8, introduces three optimizations: Tensor-wise memory protection on CPU: To unify the protection granularity and improve the CPU TEE performance on collaborative computing, we propose the hardware-based Tensor-Analyzer (TenAnalyzer) component for tensor-wise memory protection. TenAnalyzer receives and analyzes memory access requests from the CPU cores, constructing and maintaining the Meta Table data structure in run-time. Each entry of the Meta Table holds shared metadata (like VN) for all cachelines within a tensor, eliminating the overhead of metadata memory access in subsequent memory requests. Our design is optional and complementary to the traditional protection method like SGX [13], enabling or disabling tensor management through the Enable Tensor-wise Management Flag (EnTMF) without affecting the software programming. In non-tensor applications, tensor-wise management is disabled with negligible overhead. Tensor-wise MAC with delayed verification on NPU: To improve both the performance and storage efficiency, TensorTEE proposes tensor-wise MAC management with delayed verification for only NPU tensor data. Delayed verification enables parallelization between verification and computation, eliminating stalls caused by coarse-grained MAC verification. To ensure integrity, TensorTEE 1) restricts code access following the normal non-delayed verification dataflow to maintain code integrity and prevent any potential attacks and 2) ensures the communication data integrity when they leave the NPU enclave by proposed tensor poison tracing and verification barrier mechanism. Implementation of direct data transfer protocol: Leveraging a unified tensor granularity, we enable an efficient data transfer protocol that eliminates the transit through non-secure regions and enables the parallel between data transfer and computation. The protocol contains two channels: A trusted channel is used for the encrypted transfer of on-chip metadata and a direct channel is for the transfer of ciphertext tensors between heterogeneous secure enclaves."}, {"title": "4.2 CPU TEE with Tensor Granularity", "content": "To increase practicability and alleviate the burden on programmers, a hardware-based tensor-wise CPU memory management mechanism is required. It should incorporate tensor management capabilities while maintaining low overhead. Challenges of hardware-based tensor data structure detection: To address the entry wastage issue, the tensor management structure should be located outside the cores, such as in the Memory Controller (MC). However, there are challenges with tensor detection and maintenance in the MC: 1) Cache interference: As shown in Figure 9, the process switches lead to random cache line evictions, disrupting memory access patterns. Additionally, cache prefetchers like stride prefetchers shuffle the access order when the wrong prefetch request is issued. 2) MC receives physical addresses, and their contiguity is limited by the page size. 3) Long latency: The access of the large-capacity cache before the request reaches the MC introduces significant memory access latency, resulting in long latency for maintaining and accessing tensor data structures. Tensor structure management in CPU TEE: To address the aforementioned challenges, we propose TenAnalyzer, located within the MC, which directly receives memory access requests from the cores, as shown in Figure 8. TenAnalyzer offers the following advantages: 1) It receives memory access requests directly from the cores, avoiding cache interference from evicting and prefetching as shown in Figure 9 (a). 2) Leveraging virtual addresses from the cores maximizes the utilization of tensor contiguity as shown in Figure 9 (b). To avoid the same VA problems, each enclave uses its dedicated key and the Meta Table is saved and restored for context-switching cases. 3) Detection, maintenance, and access of tensor data structures can be parallelized with multi-level caches, effectively hiding their access latency. Thus, TenAnalyzer overcomes the limitations of SoftVN [83] and provides an efficient and scalable solution for tensor-based memory management."}, {"title": "TenAnalyzer mainly contains two components: Meta Table and Tensor Filter, as shown in Figure 10.", "content": "1) Meta Table maintains the tensor structure and can be constantly updated by boundary hit. Specifically, the Meta Table receives the access request of the core, and when it hits, the address and VN are passed to the MEE for encryption, which avoids the extra access for metadata in memory. 2) Tensor Filter handles the Meta Table misses, collecting and analyzing the request addresses. When a specific entry in the Tensor Filter reaches its collection address limit (4 in our setting), it checks if the entry meets the tensor condition: having the same VN and a consistent pattern between addresses. The successfully checked entry is then populated into the Metadata Table as the initialization structure for tensors. Besides, other components, a bitmap and two flags for indicating the cacheline update states and the EnTMF flag for enabling or disabling TenAnalyzer, are used."}, {"title": "Dataflow for reading (or detection):", "content": "As shown in Figure 10, TenAnalyzer has three cases when a request arrives: hit in, hit boundary, and miss. 1) Hit in means that the request hits the address range of an entry in the Meta Table, and then the corresponding VN will be transmitted to MEE to start the encryption/decryption. 2) Hit boundary means that the request hits the boundary of the address range of an entry (request address == last address + stride) in the Meta Table that the VN cannot guarantee to be correct. At this time, assuming that this VN is correct, the VN is still sent to MEE, and at the same time, the VN access in DRAM is started. If the VN in DRAM is consistent with the assumed VN, the relevant Meta Table entry is updated and its address range is expanded, which leads to gradual coverage of tensor detection. 3) Miss means that there is no matching in the Meta Table, then the VN will be obtained by the DRAM access and this request is sent to the Tensor Filter for subsequent pattern detection and filtering. For reading, the on-chip tensor-wise VN always remains equivalent to the off-chip cacheline-wise VN"}, {"title": "TenAnalyzer efficiently detects tensors with complex access patterns through entries merging.", "content": "For streaming operations, such as Adam's update, a single entry is sufficient for the detection and management of large tensors (Figure 11 (a)). However, for tensor operations requiring tiling, such as matrix multiplication, detecting complete tensor information with a single entry becomes challenging. Nevertheless, TenAnalyzer can still manage each short streaming data like a row of a tile and infer the tile dim (e.g. the first blue row with a size of d1) based on the dimensional-wise streaming access pattern. Next, TenAnalyzer attempts to merge a few recently updated entries when creating new entries. The merge operation is not restricted to address-adjacent tiles but allows merging in multiple directions, enabling more timely merges and more efficient on-chip storage (2 directions for 1D tensor, 4 directions for 2D tensor and 6 directions for 3D tensor). To merge tiles with non-adjacent addresses, it's required that the tile dims, stride, and VN match. The successful merge allows inferring the tensor dimension (e.g. merging the two blue rows infers the tensor dimension D1). The inferred dimension can serve as a constraint for subsequent merges to improve the merge accuracy. By iterating this process, unified management of the tiled tensors can be achieved as shown in Figure 11 (b). TenAnalyzer gradually completes the tensor structure but with extra memory access to verify its correctness. Since the data transfer instructions from NPU typically include tensor structure information, such as address, size, and stride, TensorTEE utilizes them to update the Meta Table entry with a large overlapped address range to speed up the tensor structure creation in the CPU."}, {"title": "Dataflow for writing (or update):", "content": "During tensor writing, TenAnalyzer efficiently maintains the correct updates of tensor VN by confirming each cacheline within a tensor must update only once within a tensor update using a bitmap (BM), Updating Flag (UF) and Bit State (BS). The bitmap and address range log cacheline updates for a tensor, flipping"}, {"title": "4.3 Delayed Verification for Tensor-Wise MAC", "content": "The granularity of MAC trades off the storage overhead and verification overhead. Cacheline-wise MAC results in large storage overhead in the NPU, which further hinders the deployment of LLM workloads on the NPU due to the limited memory capacity. Recent studies [33, 34], have opted for a MAC granularity of 512 bytes, thus establishing a balance between storage overhead and verification cost. However, the use of large granularity MAC results in later verification, leading to computation stalls for already decrypted cachelines as shown in Figure 13 (b). Specifically, despite having the same time for MAC re-generation at both granularities, the blocked computation by later MAC verification stages results in many pipeline bubbles, causing the inefficient pipeline (13% overhead with 4KB granularity). To reduce storage overhead while avoiding computing stalls, leveraging the characteristics of LLM collaborative computing, we propose a tensor-wise MAC management method with delayed integrity verification. We parallel the kernel computation for decrypted data and re-generation tensor MAC as shown in Figure 13 (c). After completing the tensor-wise MAC, we perform the integrity verification for the entire tensor. In this way, we significantly reduce the storage overhead and memory traffic while avoiding large bubbles caused by the long stalls as shown in Figure 13 (b). For tensor-wise MAC calculations, we adopt a simple and efficient algorithm that involves XORing the MAC values of all cachelines which is similar to the MEE [24]. The algorithm can be formulated as follows where $MAC_i$ means the MAC for i-th cacheline in the tensor and n is the number of cache lines: $MAC_{tensor} = MAC_0 \\oplus MAC_1 ... \\oplus MAC_{n-1}$. The XOR-based algorithm is insensitive to order, allowing various optimizations in NPU computing like tensor tiling."}, {"title": "Integrity guarantee:", "content": "We ensure the integrity of tensor data on NPU computation with a strict integrity guarantee on code access and communication data. This allows for the delayed verification of intermediate tensors during computations, resulting in performance improvements without compromising security. Delayed verification allows temporary tampering on intermediate tensors, but can detect them by MAC soon. The temporary tampering of NPU intermediate tensors is acceptable as it cannot be exploited to carry out more powerful attacks, such as data theft. This is because the NPU, being a pure accelerator, lacks the privileged software (like OS) and file I/O abilities. In contrast, the NPU code and communication data that leave the NPU and GDDR memory need strict integrity guarantees. TensorTEE guarantees code integrity by restricting code access requests following normal non-delayed MAC verification dataflow. The non-delayed verification of NPU code disables attacks for delayed verification based on code tampering, such as pointer conversion, binary search, or disclosing kernel [76]. We differentiate code and data requests in the memory controller and disable delay verification for code requests. The desired differentiation can be easily accomplished by adding an isInst flag in the instruction request packet during issuance by the fetch component or last-level instruction cache. Specifically, in this paper, the indicator flag is provided by the instruction buffer (as assumed TPU-like architecture [45]). TensorTEE guarantees the integrity of NPU communication data by ensuring the completion of verification on the involved tensors before the communication. This is accomplished through two steps: tensor poison tracing and verification barrier. Firstly, drawing inspiration from [57], we incorporate a poison bit for tensors to track poison propagation as shown in Figure 14 (c). The poison bit of unverified tensors is set to 1, and the potential poison effect propagates to the output tensors. The poison bit is cleared after the verification is finished. Secondly, we introduce the pragma"}, {"title": "4.4 Direct Data Transfer Protocol Implementation", "content": "In this section, we present the implementation of an efficient heterogeneous TEE data transfer protocol that eliminates the need for re-encryption and decryption and enables the parallel between computation and data transfer. 4.4.1 Optimizations Based on Unified Tensor Granularity. Existing heterogeneous CPU and NPU TEE have different memory protection granularity, which leads to significant overheads in data transfer between the enclaves: First, the data transfer relies on a non-secure memory region relay, which incurs re-encryption overhead as shown in Figure 6 (a). Second, since both computation and data transfer (with re-encryption) are memory-intensive, limited AES/memory bandwidth makes data transfer not parallel with computation as shown in Figure 7. In this work, we reduce the data transfer overhead based on our proposed unified tensor management granularity. On the one hand, CPU and NPU enclaves have a unified tensor data structure, which means the off-chip encrypted memory could be decrypted on both sides. Thus, data transfer between the two does not need to rely on the transfer in non-secure regions, which makes it possible for direct data transfer between secure regions as shown in Figure 6 (b)."}, {"title": "4.4.2 Implementation.", "content": "The protocol primarily consists of two phases: authentication and data transfer. Authentication phase: The CPU and NPU build trust by remote attestation. Firstly, at the beginning of the execution of the workload, the CPU starts the process of enclave creation which copies code and data copy from non-secure memory to secure memory and computes the report of the enclave. Secondly, the CPU enclave sends the enclave creation request to NPU to create an NPU enclave with a similar process. Thirdly, the two enclaves authorize each other based on the report. To enable low-cost data transfer, the CPU and NPU TEE hold the same key for encryption/decryption. After the attestation, the two enclaves perform a key-exchange protocol like the Diffie-Hellman which enables the same key in both enclaves without leaking the key in the communication process. After the exchange, the key is always kept in the on-chip memory to ensure security. Communication phase: There are two types of transfers: trusted metadata transfer and direct tensor transfer. We take the example of CPU-to-NPU data transfer to describe the process. As shown in Figure 8, in the metadata transfer phase, the NPU sends a data transfer request containing the tensor's address range. The CPU memory controller queries the Meta Table for metadata. The obtained tensor VN, MAC, and address are transmitted to the NPU through a trusted encrypted channel for subsequent decryption and verification. Simultaneously, the tensor data corresponding to the address range is directly transferred from the CPU DRAM to the NPU's DRAM via a direct channel, without involving the CPU and NPU. It is important to note that the two stages can be executed in parallel, requiring only the synchronization mechanisms upon completion of the transfers."}, {"title": "5 Experimental Methodology", "content": "5.1 Simulation Infrastructures We perform detailed simulations with the enhanced CPU simulator Gem5 [8], an implemented cycle-accurate NPU simulator integrated with Ramulator [51], and the modeled system communication protocols with measured bandwidth. Further, we conduct performance alignment with SGX CPU and A100 GPU separately to ensure the correctness of the time models of our customized Gem5 and NPU simulator. CPU Simulator: To evaluate the performance of CPU workloads on baseline SGX and TensorTEE, we extend the Gem5 [8] simulator with the baseline SGX-like and our proposed tensor-wise memory protection mechanism. The"}, {"title": "5.2 Workload", "content": "TensorTEE is suitable for any tensor-based applications. In this paper, we take one of the most representative and challenging workloads, LLM training, for the evaluation study. For LLM training, we select several popular models as our workloads, with parameter numbers ranging from 100M to 7B. The models and their configurations are presented in Table 2. We employ the scheduling strategy of the DeepSpeed framework with offload optimization for collaborative heterogeneous computing. In this paper, we focus on the LLM training scenario with a host multi-core CPU and a single NPU. To accommodate the memory capacity of NPU, we choose different batch sizes for different models. Besides, we choose 2D matrix multiplication with tiling optimization (GEMM) as CPU workload to show the effectiveness of TensorTEE on workloads with complex access patterns. We compare three types of configurations in our experiments: 1) Non-Secure: In this setup, both the CPU and NPU do not utilize isolation and memory protection mechanisms like TEE. This configuration is used as a performance reference. 2) CPU SGX + NPU MGX configuration: In this setup, the CPU employs an SGX-like cacheline granularity protection mechanism, while the NPU utilizes an MGX-like tensor granularity protection mechanism. The communication between the CPU and NPU requires additional encryption and decryption due to granularity mismatch. 3) TensorTEE(ours): In this setup, we leverage the unified tensor-based management approach and optimization techniques proposed in our paper to achieve enhanced performance while maintaining security."}, {"title": "6 Experiment Results", "content": "In this section, we will first present the overall performance and performance breakdown of TensorTEE. Subsequently, we will delve into a detailed analysis of the benefits and effects of proposed optimizations."}, {"title": "6.1 System Performance Improvement", "content": "Figure 16 showcases the overall performance comparison of the three methods across different model setups, while Figure 17 provides a breakdown of the performance. The following observations can be made from these two figures: 1) Figure 16 reveals that the baseline SGX+MGX method incurs a considerable overhead. Additionally, Figure 17 depicts that this overhead primarily stems from the communication process of weights and gradients. 2) TensorTEE exhibits a significant performance improvement compared to the baseline. As seen in Figure 16, TensorTEE achieves a maximum performance boost of up to 5.5x and an average improvement of 4.0x compared to the baseline. Furthermore, when compared to the non-secure configuration, the average performance overhead of TensorTEE is only 2.1%. 3) The Figure 16 demonstrates a clear increasing trend of performance improvement for TensorTEE as the model size increases. The large models result in increased communication volume and CPU computation, as indicated by the breakdown in Figure 17 which provides TensorTEE with greater optimization opportunities, leading to large performance gains."}, {"title": "6.2 Improvement of Tensor-Wise CPU TEE", "content": "The LLM training needs thousands of iterations to update the parameters. Figure 18 illustrates the two types of hits in the Meta table when the CPU performs the Adam optimizer"}, {"title": "Figure 19 presents the average performance overhead of TensorTEE's proposed VN tensor-based management compared to SGX and SoftVN at different iterations.", "content": "For SoftVN, meticulous management of the VN table is required, including timely release and careful creation due to the limited number of entries. The following observations can be made: 1) The baseline SGX method incurs significant overhead, reaching a factor of 3.65x at 8 threads. The encryption and decryption overhead of SGX, along with additional memory accesses, significantly impact the memory-intensive Adam workload on the CPU. Furthermore, as the number of threads increases, the memory pressure intensifies, leading to further performance overhead. 2) TensorTEE demonstrates significant performance benefits once the Meta table is established, as the number of iterations increases. It can be observed that, except for the initial iteration with higher overhead, the performance overhead decreases to 11% of the 8-thread configuration after 10 iterations. Given that LLM training"}, {"title": "6.3 Improvement of Delayed Verification", "content": "Figure 20 illustrates the performance comparison and storage overhead of the NPU TEE at different MAC granularities. It can be observed that the storage overhead decreases as the MAC granularity increases. However, although there is a performance gain at smaller granularities due to fewer extra memory accesses, the overhead keeps increasing with larger granularity than 256B due to the stall problem. The performance overhead reaches 13.0% when MAC granularity is set to 4KB, which is unacceptable. In contrast, our proposed MAC tensor-based management reduces the storage overhead to negligible levels. Additionally, benefiting from delayed verification, the performance loss is only 2.5%. Since MAC regeneration and computation are executed in parallel, the overhead caused by the verification barriers refers to the overhead of MAC comparison. This operation can be completed in just a few cycles, making the overhead negligible."}, {"title": "6.4 Improvement of Data Transfer Protocol", "content": "Figure 21 presents the communication breakdown for gradient transfer. The following observations can be made from the figure: 1) Due to granularity mismatch, re-encryption and decryption are introduced before and after communication to align the granularities. 2) The presence of re-encryption"}, {"title": "TensorTEE eliminates the need for re-encryption and decryption, allowing parallel execution of communication and computation.", "content": "As a result, the communication performance is improved by 18.7x."}, {"title": "6.5 Hardware Overhead", "content": "We assume that the Meta Table consists of 512 entries to accommodate complex and dynamic applications. Each entry consists of an address range (64 bits for address and 92 bits for dimensions)"}]}