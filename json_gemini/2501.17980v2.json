{"title": "Limits to Al Growth: The Ecological and Social Consequences of Scaling", "authors": ["ESHTA BHARDWAJ", "ROHAN ALEXANDER", "CHRISTOPH BECKER"], "abstract": "The accelerating development and deployment of AI technologies depend on the continued ability to scale their infrastructure. This has implied increasing amounts of monetary investment and natural resources. Frontier Al applications have thus resulted in rising financial, environmental, and social costs. While the factors that AI scaling depends on reach its limits, the push for its accelerated advancement and entrenchment continues. In this paper, we provide a holistic review of Al scaling using four lenses (technical, economic, ecological, and social) and review the relationships between these lenses to explore the dynamics of AI growth. We do so by drawing on system dynamics concepts including archetypes such as \"limits to growth\" to model the dynamic complexity of AI scaling and synthesize several perspectives. Our work maps out the entangled relationships between the technical, economic, ecological and social perspectives and the apparent limits to growth. The analysis explains how industry's responses to external limits enables continued (but temporary) scaling and how this benefits Big Tech while externalizing social and environmental damages. To avoid an \"overshoot and collapse\" trajectory, we advocate for realigning priorities and norms around scaling to prioritize sustainable and mindful advancements.", "sections": [{"title": "1 Introduction", "content": "In the past decade, the scaling of AI models has accelerated rapidly due to a combination of foundational research advances, improvements in technical infrastructure, access to huge amounts of Internet-based data, and significant capital investment. Data storage alone requires considerable amounts of energy in order to maintain data centers, equivalent to the energy usage of small towns [37, 116] and predicted to increase to the usage of entire countries [26, 29]. The amount of capital invested into driving further advancements in the AI industry yields numerous social implications. Large Language Models (LLMs) in particular have become pervasive and their environmental impact has become a significant concern. On the other hand, recent debates have raised doubts over the feasibility of continued scaling including concerns over the end of training data [173], industry profitability [11], and the industry's mounting climate impact [137]. Critics have variously examined the social harms [7, 67, 100, 152, 168], emphasized the environmental damages wrought by generative AI [18, 112], and questioned industry commitments to ethical principles [6, 22, 178, 180]. It is thus an important moment to ask whether this scaling can and should continue, at what cost, and to whom [182, 191]. This paper asks: \u201cHow can we characterize the dynamics of AI growth to identify and analyze its limits?\". We use 'Al' as an encompassing term including LLMs, generative AI, applications of machine learning models including deep learning, and we use 'AI industry' to refer to large or frontier companies in the development of AI technologies and hardware.\nWe first establish a baseline by reviewing recent discussions about AI scaling and introduce the basic modeling constructs of system dynamics (Section 2). We then examine Al scaling from four perspectives \u2013 technical, economical, ecological, and social (Section 3). For each, we highlight barriers that are surfacing. This allows us to model how these\""}, {"title": "2 Background", "content": "Related Work. In 2021, Bender et al. asked \"can language models be too big?\". They highlighted that \"... increasing the environmental and financial costs of these models doubly punishes marginalized communities that are least likely to benefit from the progress achieved by large LMs and most likely to be harmed by negative environmental consequences of its resource consumption.\" [20, p. 610]. The uneven distribution of benefits and harms of large models has been acknowledged in discussions of intrinsic and extrinsic harms [106], labour and wage inequality [24], surveillance through data collection [202], centralization of the development of large models [190], and increased usage of decision-making systems that may cause a \"monoculture\" [95, 136]. It has also been observed that these models exacerbate societal and social concerns like the deterioration of content quality through misinformation [129, 198, 201], harmful content [181], and security and privacy [65]. When a technology is introduced, it does not exist in a vacuum that only advances that technology or frontier, but rather has multi-dimensional impacts to both society and the environment. Several taxonomies of harms or risks within the AI landscape exist, (e.g., [24, 39, 148, 188]). Each of these outline repercussions that occur beyond the confines of the surroundings that the AI technology or model is produced in. Particularly, a social-ecological-technical (SET) framework to analyze the outcomes of algorithmic technologies has been proposed \"...to [study] complex dynamical systems and their constitutive relations...\" [141, p. 3]. Thus by examining technological systems in larger social, economical, and ecological contexts, we can better understand how they interact with and depend on non-technical and mathematical aspects and permeate into our broader ecosystem.\nOther works have similarly highlighted that a wider lens is required to address \"algorithmic injustice\" such as using relational ethics as a framework to better understand and act on \"how we can re-examine our underlying working assumptions... interrogate hierarchical power asymmetries, and... consider the broader, contingent, and interconnected background that algorithmic systems emerge from (and are deployed to) in the process of protecting the welfare of the most vulnerable\" [23, p. 2].\nA recent paper [182] examines the scaling hype in LLMs and outlines three negative consequences: environmental unsustainability, data quality issues at scale, and a centralization of power with harmful consequences on innovation and society. Highlighting that \"small can also be beautiful\", the authors advocate for increased transparency about size and cost and a focus on progress in terms of resource efficiency. This is a timely call considering that the global scale of LLM R&D is such that it now seriously impacts planetary ecosystems on a global scale.\nSystem Dynamics. In 1972, the well-known Limits to Growth report developed a system dynamics model of key global parameters and relationships connecting population dynamics, industrialization, agriculture, CO2 emissions, and temperature increases. The conclusions are well known: the planet would reach its limit for unlimited growth within the next 100 years after which there would be a drastic decline, but it was possible to achieve global equilibrium if ecological and economic stability was attained, and there was greater likelihood of success the sooner we strived for this stability [120]. While it was originally criticized and doubted for its technical feasibility, the 30-year update demonstrated its"}, {"title": "3 Mapping the Perspectives of Al Scaling", "content": "AI scaling can be examined from several perspectives [41]. The trajectory of each and how they intersect evolves rapidly. In compiling an evidence base for modeling dynamic relationships, we triangulate between academic literature that provides robust arguments on a comparably slow timescale; company reports and industry conversations that illustrate important facets rapidly but lack the rigor and representativeness of peer-reviewed research; and investigative journalism that, for example, demonstrates the intentional opacity of omissions in reports [53]. Here we provide a distilled synthesis as basis for the dynamic models below."}, {"title": "3.1 Technical", "content": "Language models and transformer architectures continue computing's history of exponential growth curves [130] described in a succession of \"laws\" for integrated circuits, disk storage, and network bandwidth [183]. Scaling laws for Transformer architecture follow a power law relationship [35, 85], i.e., a model's performance depends on the scale of its parameters, volume of data, and amount of compute [85]. Scaling laws dictate that either the size of model, training data, or compute budget must continually be scaled to improve [14, 40, 176]. With current architectures, larger models outperform bespoke models, so technical advances have focused on increasing size over all else [128, 182]. Fig. 2 shows how training compute has doubled every 18 months between 1952-2010 (pre deep learning era), 6 months between 2010-2022 (deep learning era) and 10 months between 2015-2022 (large scale era) [161]. However, an increase in model size and complexity does not yield an equivalent increase in model performance [160]. The larger a model is, robustness and generalizability become difficult to attain while overfitting becomes more likely [160].\nBarrier 1: Diminishing returns. While efficiency improvements and data center size enable further scaling, these imply economic and environmental impacts, and marginal returns diminish: \u201cincreasing the computing budget from $10 million to $100 million increased the pass rate for Al-generated computer programs from about 65% to about 75%\u201d and \"a trillion-dollar model would increase those odds to just over 91%\u201d [109]. Even with a trillion-dollar model, \"record performance\" is not guaranteed. In other words, scaling alone may not lead to gains in model performance [109].\nBarrier 2: No more data. The exploding volumes of extracted data available for training were a driving force for model improvements, growing at 0.22 orders of magnitude per year for language datasets (see Fig. 3a) but seem to be"}, {"title": "3.2 Economic", "content": "Technical advances are tied to capital investments. Fig. 3b shows how billions of dollars in investments have poured into the AI industry since 2013, with exponential growth in 2020. Similarly, the global semiconductor market is forecast to double from $590 billion USD by 2030 [27]. The undisputed winners in that market so far are hardware companies like NVidia [126]. In total, Alphabet, Amazon, Apple, Meta and Microsoft have budgeted to spend approximately $400 billion on Al capital expenditures in 2024 [5]. But, OpenAI expected to lose $5 billion in 2024 to rapid expansion efforts [78, 125]. It has been predicted that developing a model in 2027 could cost $100 billion [194]. There are also operational costs which are dependent on query length, number of queries, and response speeds [109]. For example, ChatGPT's increasing website visits from 2022 to 2024 shown in Fig. 3c, accordingly scaled OpenAI's operational costs.\nBarrier 3: Returns on investments. Investors increasingly notice a gap between the revenue prospects generated by Al models and the capital required to power it [74]. Genuine business adoption rates for AI of 5% in the U.S. are much lower than the widespread rate of experimentation often cited [5], and comparisons are made to the dot-com boom when over-production without genuine demand led to short-term collapse [74]. While investments upwards of trillions of dollars are being sought to further scale the semiconductor industry [86], \"some of the largest tech companies' current spending on AI data centers will require roughly $600 billion of annual revenue to break even, of which they are currently about $500 billion short.\u201d [194]. While many see disruptive potential in generative AI, \u201cno one knows what its main uses will be, or how it will make money.\u201d [3]."}, {"title": "3.3 Ecological", "content": "While the list of concerns is broad and includes waste, air pollution, minerals, and the impact of mining, below we focus on energy, water, and CO2 emissions to illustrate the ecological perspective.\nAl requires energy and materials to manufacture, power, run, and cool hardware. Dedicated AI data centers are increasingly specialized for training (data processing and ML model training) and inference (deployed models used in applications) [158]. While inference is tiny when compared to training, it occurs much more frequently [84, 155] and its energy demand varies based on task complexity (e.g., image generation versus text classification [113]). In the case of the BLOOMz-7B model, approximately 590 million inferences would be needed for the energy cost of inference to match that of training [113]. But for popular models like GPT with 100 million monthly users [115], inference is so frequent that it heavily outweighs the energy impact of training [113]. About 50% of energy consumption of data centers is used to power the hardware and another 40% to cool it [114]. For example, the energy demand of the LLaMa 3.1 405B model (released in July 2024) for powering and cooling peaks at 1.7kW per GPU [4]. At 16,000 GPUs, this amounted to roughly 27.2MW [162]. Newer chip generations have also steadily increased power consumption (Fig. 3d shows the tripling of maximum power consumption enabled by NVidia chips within four years).\nWhile clean water is needed to cool data centers and to generate electricity (through water-intensive thermoelectric plants) [114, 165], chip manufacturing requires \u201cultrapure\u201d water [73, 105] and one facility \u201ccan use 37 million litres of ultrapure water per day", "of the total 50 tonnes of CO2eq emissions emitted during model training, only half was due to the energy consumption of the GPUs used for training\"\n    },\n    {\n      \"title\": \"3.4 Social\",\n      \"content\": \"Al's Scale of Impact. The widespread adoption of AI poses old questions in new forms as well as new ethical challenges [31, 54, 56]. Its scale of impact is larger than ever before in the history of computing ethics due to the opacity of the models, the number of people it can impact, its potential for damage, and the feedback loops that increase their scale of harm [136]. A major consequence of the prevalence of AI in our society is the standardization and lack of autonomy in deciding whether you want to be part of the data economy [121] as power centralizes in the hands of monopolies driving the \\\"social quantification sector\\\" [32, 121, 122]. Datafication enables surveillance capitalism in a \\\"transformation of human life so that its elements can be a continual source of data\\\" [121, p. 2], [202]. The ability to datafy remains largely with the big AI companies. \\\"Inequality resides... in having or not having the power to decide what kind of data is being generated... by whom, for what purpose, and for whose benefit.\\\" [49, p. 831]. This also enables the exploitation of labour. Millions of gig workers, often employed from India, Kenya, Philippines, or Mexico, are paid $1.46/hour to enable the development of AI models [76]. The poor labour conditions of the workers include monitoring through automated tools [193]. The lack of equity in the actors that can afford and access data, and the monopoly they exercise signals that \\\"...capitalism is premised upon the preservation of unequal power: of the enforcement of the racial, gendered, and other social hierarchies which enable the extraction of labor, and therefore value, from the many for the profit of the few.": 94, "6": "Organized resistance. In response to these environmental injustices, the erosion of worker autonomy, and injustices resulting from data colonialism [32], organized public resistance has increased [117], including residents mobilizing against data centers in many locations [42, 134], calls to defund Big Tech [16], environmental activism by Amazon workers [59], campaigns against military projects by Google employees [174], legal action by content moderators [25], and calls to democratize decision-making about how Al is developed and deployed [82]."}, {"title": "3.5 Dynamics of Al Scaling", "content": "The four perspectives illustrated in the previous sections intersect with each other and result in the complex dynamics of AI scaling. We present a simplified version of this in Fig. 4. We define and justify each causal relationship of Fig. 4 and subsequent CLDs in Appendix B.\nThe reinforcing loops in the CLD demonstrate how technical scaling is reinforced by socially recognized and documented model performance, i.e., that scaling leads to improved performance, which spurs more capital investment (R1). Competitive pressure (R2), AI hype (R3), and model performance (R4) drive investment cycles which continue to power scaling of AI models and hardware (R5) which are also mutually reinforcing (R6). The development of GPUs and"}, {"title": "4 Limits to Al Scaling", "content": "In this section, we revisit three of the barriers surfaced previously and map a (non-comprehensive) set of responses by the Al industry to each of them using system dynamics archetypes."}, {"title": "4.1 Limits to Performance: Rebound Effects", "content": "Continuous growth has yielded diminishing returns to performance. The industry response has been to force further performance scaling by investments into data centers and GPU technology. While this addresses the technical barrier (that new scale must be reached in order to improve performance), it has significant ecological and social implications and relies on economic scaling.\nIt is estimated that there will be increased power-efficiency of hardware, more efficient hardware usage, and longer training periods which distribute the energy consumption over time [161]. \u201cGiven all of the above, we expect training runs in 2030 will be 4x (hardware efficiency) * 2x (FP8) * 3x (increased duration) = 24x more power-efficient than the"}, {"title": "4.2 Limits to Data: Fixes that Fail", "content": "The \"data cliff\" is considered a soon-impending barrier to further scaling of AI models, particularly LLMs, as publicly available, human-generated data on the Internet runs out [184]. To avoid this stagnation, many are exploring the creation of synthetic data as a way of ensuring continued performance scaling of LLMs [103, 110]. One perspective on synthetic data is that it enables the creation of endless amounts of high quality, error-free, complete data while simultaneously democratizing the landscape of proprietary data. It has fewer privacy concerns and can be less biased because of the ability to ensure greater diversity, and prevents copyright issues [103]. Others have argued that continued progress is possible through improvements to the quality of data by having diverse, unique, and more smartly selected"}, {"title": "4.3 Limits to Energy Supply: Shifting the Burden", "content": "Given the push towards larger and more complex models, due to the need to continue scaling in order to meet revenue demands and the ingrained industry norm of exponential growth, resource consumption is further increasing. This is a limiting factor because energy and water requirements are becoming staggering and companies are facing backlash for their GHG emissions. In response, there is \"energy hunger\" [68, 69]: AI companies continue to invest in gas, coal, and nuclear [102] sources, invest in renewable energies because it is cheaper to scale, upgrade or create new electricity grids, and try to make AI development more efficient.\nThis trajectory follows the shifting the burden archetype (see Fig. 7) [159]. The temporary solution of developing more energy capacity (e.g., nuclear) to meet energy demands (B1) creates a dependence on scaling energy sources (R1) to fuel more powerful Al models and diverts attention (R2) from the better solution of more responsible energy consumption through frugal AI (B2)."}, {"title": "5 Discussion: Progress beyond Growth", "content": "Limits to performance, data, and energy supply currently prevent AI from scaling further and are therefore prioritized for innovation and advancements. However, it is the barriers that do not represent current limits to growth and are externalized by the AI industry which have the most concerning consequences. Below, we outline these in turn and then discuss how to avoid an overshoot and collapse trajectory by redefining what progress in Al means."}, {"title": "5.1 Unlimited Capital Expenditure has Consequences", "content": "While the lack of revenue generated by the Al industry as compared to the scale of capital expenditure can pose a barrier, there are no apparent limits to growth in investments at the moment. Investments continue to scale despite the lack of profitability of AI companies. We observe some dynamics and feedback loops of the scaling \"hype\" [182]. Industry shapes narratives by controlling access to information to build hype and market AI as \"tech-positive\" [190] and transformative. National AI strategies \u201ctalk AI into being\" by framing it as inevitable and revolutionary, and necessary for \"future societal welfare\u201d [15]. The rebranding of LLMs as foundation models is considered a tactic to distance from the negative discourse that surrounds them [190]. The compute divide between academia and industry is also to be noted as industry was responsible for training 81% of large ML models as of 2022 [21]. This means these models are developed based on industry motivations and incentives and continue to reify the narrative of AI from industry's perspective. Prominent actors and groups have superficially advocated for an \"AI pause\" in order to make AI systems and development safer [52]. But the display of longtermism [56] places importance on possible future harms instead of highlighting and recognizing the reality that current Al systems already exhibit and exacerbate inequities and discrimination [55]. The main effect of these claims is that their \"extreme risk\" framing reinforces the Al hype by underscoring its narrative as a powerful, transformative, and inevitable technology."}, {"title": "5.2 Ethics Capture Invisibilizes Social and Ecological Harms", "content": "It is worth noting that the social and ecological costs of continued scaling do not currently feed back into limits to growth, despite increased recognition and organizing. Various forms of capture have been identified [190].\nRegulatory capture, \u201cthe practice whereby private industry professionals or lobbyists overtake regulatory agencies to serve their own interests\" [154, p. 1], has been discussed since the 1950s. Capture occurs when industry is successfully able to influence policy processes and outcomes. Methods in which capture is performed can be categorized as direct and indirect [187]. Al companies engage in advocacy, agenda-setting, and information management [187] to shape policy content and enforcement. Governments often provide tax breaks to attract data center investments similar to factory investments. Unlike factories, however, data centers require few workers [153] and provide limited gains for local communities, which instead face cutbacks to energy capacities. For example, AWS data centers claim to benefit local communities such as adding $6.4 billion to Oregon's GDP, creating over 5000 jobs, and providing cooling water from"}, {"title": "5.3 Limits to Growth, Overshoot, and Collapse", "content": "Returning to the theme of Limits to Growth, consider Fig. 8. The only difference to Fig. 1 is an added link between the state of the system and the carrying capacity. This link is based on the recognition that system activity itself can affect the capacity of the containing ecosystem to support continued activities [120], such as ecosystem viability and planetary life support systems [64, 147, 149]. In the anthropocene, computing has reached activity levels with measurable impact on a planetary scale [34]. The crucial difference is that unlike a limited growth scenario in which the activity level of a system stagnates, like a saturated market, the erosion of carrying capacity undermines the continued stability of the entire ecosystem and leads either to system collapse or an unstable oscillation trajectory.\nOvershoot and collapse patterns can apply in at least two ways. First, model collapse already has been proven with respect to deteriorating data quality, as discussed earlier. In this case, resource adequacy would refer to the availability of high-quality training data for LLMs. The proliferation of generated text online erodes that availability as already illustrated by the closure of Wordfreq noticed above [96]. The collapse of model quality by continued erosion of data"}, {"title": "6 Conclusion", "content": "Above, we have mapped out the accelerating growth of AI along the lines of technical, economic, ecological, and social dimensions and their interactions. We have illustrated how the mathematical laws of scaling translate into technical development with economic implications of scale subject to capitalist and market dynamics. The resulting economies of scale cause ecological destruction, which is ultimately an ethical concern too.\nWe explored the dynamics of how these perspectives interact through causal modelling. Drawing on system dynamics archetypes, we argue that the AI industry's responses to barriers typically attempt to overcome apparent limits in one perspective but fail to account for resulting damages in other perspectives. These damages cause social and ecological harms that are externalized by the AI industry but demonstrate a vital need to realign our priorities around scaling. Lastly, we emphasize the need for refocusing on sufficient Al practices to avoid an overshoot and collapse trajectory.\nGrowth always ends - either by design, or by disaster. Which path we choose has not been decided yet. In Fall 2024, the United Nations Pact for the Future enshrined a commitment to develop measures of progress beyond GDP growth in future agendas and sustainable development policy [177]. As the world comes to finally recognize that pursuing relentless growth is the path to assured disasters, it is time that the AI industry catches on and changes course."}]}