{"title": "OSCAR: OPERATING SYSTEM CONTROL VIA STATE-AWARE REASONING AND RE-PLANNING", "authors": ["Xiaoqiang Wang", "Bang Liu"], "abstract": "Large language models (LLMs) and large multimodal models (LMMs) have shown great potential in automating complex tasks like web browsing and gaming. However, their ability to generalize across diverse applications remains limited, hindering broader utility. To address this challenge, we present OSCAR: Operating System Control via state-Aware reasoning and Re-planning. OSCAR is a generalist agent designed to autonomously navigate and interact with various desktop and mobile applications through standardized controls, such as mouse and keyboard inputs, while processing screen images to fulfill user commands. OSCAR translates human instructions into executable Python code, enabling precise control over graphical user interfaces (GUIs). To enhance stability and adaptability, OSCAR operates as a state machine, equipped with error-handling mechanisms and task-driven re-planning, allowing it to efficiently adjust to real-time feedback and exceptions. We demonstrate OSCAR's effectiveness through extensive experiments on diverse benchmarks across desktop and mobile platforms, where it transforms complex workflows into simple natural language commands, significantly boosting user productivity. Our code will be open-source upon publication.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) have demonstrated exceptional performance on tasks requiring complex reasoning, particularly when combined with advanced planning techniques and external tools. These model-centric agents show revolutionary potential for automating real-world tasks such as web browsing, gaming, and software development. However, despite impressive results, these agents struggle to generalize across different applications due to variations in observation and action spaces. In real-world scenarios, workflows often involve switching between applications and interacting with diverse graphical or command-line interfaces. This raises an intriguing and practical question: can we build a generalist agent capable of following user instructions across various applications using standardized operating system (OS) controls like mouse and keyboard inputs, while processing screen outputs?\nRecent work has explored graphical user interface (GUI) control on mobile devices, with a focus on smartphone GUI understanding and task automation. For desktop computers, existing approaches simulate tasks in black-box systems like AAA games and office workflows. Some methods extend this to general OS control via visual question answering and human action trajectories. However, these systems often lack real-time feedback from the OS and struggle to adapt dynamically when task execution fails. Without a grounded executable environment, these methods fall short in real-world scenarios, where real-time feedback and adaptive action adjustment are crucial for navigating new GUI environments, similar to human behavior. Recently,\nIn this paper, we introduce OSCAR, a general-purpose agent designed to autonomously interact with dynamic OS environments through code-centric control. OSCAR generates executable Python code to directly interface with the OS, enabling semantically clear and precise actions, ensuring broad applicability across diverse tasks. To enhance GUI understanding, OSCAR augment screen"}, {"title": "METHODOLOGY", "content": "In this section, we introduce OSCAR, an intelligent agent designed for general-purpose control and navigation within operating systems. As illustrated in Figure 2, OSCAR operates as a state machine, enabling it to handle dynamic OS environments through systematic state transitions. This framework allows OSCAR to efficiently process user instructions, observe the environment, plan and execute actions, and verify outcomes, while managing potential OS exceptions. We now detail the state transition process, highlighting how OSCAR integrates GUI grounding, task-driven re-planning, and code-centric control in each operational state."}, {"title": "2.1 FORMULATION OF STATE TRANSITIONS", "content": "[Init \u2192 Observe]. In the [Init] state, OSCAR awaits user instructions. Upon receiving a command, the system transitions to the [Observe] state to begin processing the input. This is the starting point for each task, and the agent returns to this state after completing or terminating a task.\n[Observe \u2192 Plan]. After receiving the user's request, OSCAR captures a screenshot of the current environment and interprets it by performing GUI grounding detailed in Section 2.2. This involves identifying screen elements, such as buttons and input fields, to understand the user interface context. The system then transitions to the [Plan] state.\n[Plan \u2192 Execute, Plan \u2192 Verify]. In the [Plan] state, OSCAR generates an action plan based on the current screenshot, user instructions, context memory, and any previous verification feedback from the OS (if available). As detailed in Section 2.3, it utilizes task-driven re-planning to invoke the model backend and determine the next action.\n\u2022 If more actions are needed to complete the task, OSCAR stores the planning results and generated actions in the context memory and transitions to the [Execute] state to interact with the operating system via executable Python code.\n\u2022 If no further actions are necessary (the whole task completion is indicated), OSCAR transitions directly to the [Verify] state.\n[Execute \u2192 Plan, Execute \u2192 Observe \u2192 Plan]. In the [Execute] state, the Python code is executed to interact with the operating system. There are two possible outcomes:\n\u2022 If execution fails due to invalid code (e.g., syntax errors or attempts to access non-existent GUI elements), OSCAR transitions back to the [Plan] state, incorporating the interpreter's error message for re-planning.\n\u2022 If execution succeeds, OSCAR first transitions to the [Observe] state to capture a new screenshot, reflecting the updated state of the environment. Subsequently, OSCAR moves to the [Plan] state to plan the next action based on the new context.\n[Verify Success, Verify \u2192 Plan, Verify \u2192 Fail]. In the [Verify] state, OSCAR runs evaluation scripts to validate the outcomes of the executed actions. These scripts check system or application settings and analyze file content to confirm that the intended tasks were successfully completed. Based on the results, OSCAR either transitions to the [Success] state if verification passes or returns to the [Plan] state if it fails. If the failure exceeds the allowed maximum number of attempts, OSCAR transitions to the [Fail] state.\n[Success \u2192 Init]. If the task verification passes, OSCAR enters the [Success] state, signaling successful task completion and notifying the user. The system then transitions to the [Init] state, ready to process the next user query.\n[Fail \u2192 Reset]. If the task cannot be completed after the maximum number of allowed attempts, OSCAR transitions to the [Fail] state, notifying the user of the failure and then transitioning to the [Reset] state.\n[Plan \u2192 Error, Execute \u2192 Error, Verify Error, Error \u2192 Reset]. OSCAR transitions to the [Error] state when a critical system exception or crash occurs, such as a local model backend failure or when too many files or processes are open in the OS. In this state, the task is terminated, and the user is notified of the error. User intervention may be required to resolve the issue before OSCAR transitions to the [Reset] state.\n[Reset \u2192 Init]. In the [Reset] state, OSCAR restores the operating system to its pre-query configuration by terminating processes and closing file handlers. Once the reset is complete, OSCAR returns to the [Init] state, ready to process the next user query.\nIn a nutshell, the state machine architecture of OSCAR introduces continuous feedback loops, enabling dynamic interaction and error recovery, which enhances its robustness in dynamic OS environments. Additionally, unlike previous methods that relied on linear action sequences and re-planning from scratch, OSCAR's state machine integrates real-time verification feedback for fine-grained, task-driven re-planning, significantly improving efficiency and adaptability. Most importantly, its modular state transitions allow for flexible generalization across diverse OS environments, such as desktop and smartphone OS."}, {"title": "2.2 GUI-GROUNDED OBSERVATION", "content": "While LLMs exhibit strong capabilities in understanding general visual information and grounding in broad domains, feeding a screenshot into the model to facilitate planning and output control over the screen remains insufficient. This insufficiency stems from the fact that GUI images differ significantly from natural images, as they are densely packed with text and diverse interaction elements, such as icons and widgets, often rendered at a small scale relative to high-resolution screens. As a result, it is difficult for models to accurately locate all interaction elements and understand GUI semantics. For instance, both and could represent a settings icon, depending on the application.\nTo address this, we introduce a dual-grounding observation approach to enhance GUI understanding, i.e. incorporating both visual grounding and explicit semantic grounding. Firstly, we leverage a Set-of-Mark (SOM) prompting technique to enhance GUI visual grounding. SoM prompting, a visual prompting technique that adds marks to image regions to significantly improve LMM performance on fine-grained vision tasks. Specifically, we utilize native window API to extract the Accessibility (A11Y) tree, a kind of structural representation providing the location, properties, and states of UI components. Based on the A11Y tree, we extract precise numerical coordinates of UI elements and map them into bounding boxes to generate SoM visual prompts. The A11Y tree offers greater precision and robustness than the commonly adopted detection+OCR pipeline, particularly in complex screens with numerous UI elements where OCR often fails (see Section 3.1 for ablation analysis).\nIn addition to visual grounding, we further enhance GUI understanding through explicit semantic grounding by adding descriptive labels to key elements, such as: (ID: 14, Label: Start, X\u2081: 0.35, Y\u2081: 0.95, X\u2082: 0.38, Y\u2082: 1.00). These labels not only offer semantic descriptions of UI components but also facilitate code-centric control by allowing precise references to elements (e.g. by element ID).\nBy combining the screenshot with dual-grounding observations, OSCAR can not only grasp the overall layout and context of the GUI, but also focus on relevant areas of the screen, while flexibly referring to specific elements when needed. This approach significantly enhances GUI understanding, ensuring robust and efficient task execution in dynamic OS environments."}, {"title": "2.3 TASK-DRIVEN RE-PLANNING", "content": "Interacting with dynamic environments for open-ended tasks has been well-studied in domain-specific agents, such as those agents in Minecraft and data analysis. Iterative planning with exploration in self-instructed task curricula has proven effective, as agents adjust their plans based on environmental feedback. These methods typically involve two stages: exploration and deployment. During the exploration phase, agents comprehen-"}, {"title": "2.4 CODE-BASED ACTION", "content": "As portrayed in Figure 4, leveraging the textualized SoM from observed screenshots, OSCAR can easily refer interaction elements on the screen using element ID or numerical coordinates. This allows OSCAR to generate code to control these elements with logically clear semantics. To operationalize OSCAR's action space, we employ the widely-used PyAutoGUI library \u00b9 for mouse and keyboard control. This library enables various mouse behaviors (movement, left-click, right-click,"}, {"title": "3 EXPERIMENTS", "content": "Benchmarks. We evaluate OSCAR on real-world workflow automation benchmarks involving complex user requests. The first benchmark is GAIA, which consists of 466 question-answering (QA) structured into three levels: Level 1 includes simple tasks requiring no more than five steps; Level 2 involves more complex tasks with 5-10 steps and multiple tools; and Level 3 presents advanced tasks requiring over 10 actions and tool usage. The second benchmark is OSWorld, an interactive dynamic environment with real-time OS feedback. It includes 369 tasks covering OS settings, office software, daily applications (e.g. Chrome), professional tools (e.g. VSCode), and multi-application tasks. Without a gold-standard reference action sequence, the environment allows for multiple valid solutions, which are evaluated through dynamic execution testing-verifying modified files or displayed text content in windows. Additionally, similar to OSWorld, AndroidWorld provides a dynamic smartphone OS environment with 116 tasks spread across 20 diverse applications, and human annotated difficulty level: easy, medium, hard. Please refer to Appendix D and Appendix E for more experiments on the GUI understanding and static navigation benchmark.\nBaselines. We compare OSCAR with seven generalist agents designed to handle dynamic OS feedback. For the desktop OS environment, we include Cradle, UFO, FRIDAY, and MMAC. For the smartphone OS environment, we evaluate against M3A, AppAgent, and Mobile Agent. Implementation details of OSCAR and these baselines are provided in Appendix B.\nResults. Table 1 summarizes the results on the GAIA benchmark, where OSCAR achieves the best performance across all three levels of workflow complexity. In particular, for Level 3 tasks, OSCAR significantly outperforms previous methods, achieving 13.5% compared to MMAC's 6.1%, demonstrating the effectiveness of OSCAR's task-based planning. Additionally, as shown in Table 2, OSCAR consistently surpasses other methods across various applications in dynamic desktop OS environments. In challenging tasks involving multiple applications, OSCAR achieves a 12.9% success rate, outperforming the multi-agent baseline, UFO, which leverages dual agents to coordinate workflow decomposition and execution. When adapting OSCAR's action space to a mobile environment, as shown in Table 3, it achieves better average performance than the two-phase approach (comprehensive exploration followed by execution) of AppAgent, particularly in the medium and hard subsets, highlighting the effectiveness and efficiency of OSCAR's task-driven re-planning."}, {"title": "3.1 ABLATION ANALYSIS", "content": "We conduct ablation analysis on the individual components of OSCAR, including GUI-grounded observation and various planning techniques. Specifically, we first compare our GUI-grounded observation against baseline that omits the dual-grounding input, i.e. feeding raw screenshots as input. Additionally, we replace A11Y tree-based extraction with a Detection+OCR pipeline.\nFor the baselines in planning techniques, we replace our task-driven re-planning with state-of-the-art methods used in multi-step decision-making tasks, particularly for long action sequences. These include ReAct, plan-and-solve, and chain-of-action.\nThe results of different baselines on the OS-World benchmark are illustrated in Figure 5. We have the following observation: 1) Both GUI-grounding and task-driven re-planning significantly enhance performance. Specifically, raw screen input without GUI grounding and direct prompts without fine-grained planning achieve only 70% and 80% of OSCAR's full performance, respectively. 2) The Detection+OCR pipeline is less effective than the original A11Y tree-based method, particularly on the subset of professional tools with numerous UI elements, where it only marginally outperforms raw screenshot input. Furthermore, the Detection+OCR method introduces additional processing time, reinforcing the A11Y tree as the superior choice for dynamic OS environments. 3) Advanced planning strategies can significantly enhance workflow performance. For instance, ReAct and Chain-of-Action achieve results that are comparable to OSCAR in daily application and office software scenarios. 4) However, without considering real-time OS feedback and efficient re-planning, ReAct and Chain-of-Action struggle in professional software and multi-application scenarios, highlighting OSCAR's advantage in adapting to dynamic OS environments."}, {"title": "3.2 IN-DEPTH ANALYSIS", "content": "Instance-level analysis on planning efficiency. To better understand why OSCAR achieves superior performance, particularly in dynamic OS environments, we take a closer look at the final success rate results and conduct an instance-level analysis for both successful and failed user requests on the OSWorld benchmark. Specifically, for the successful cases with OSCAR, we track the number of re-planning occurrences before verification failures exceed the allowed maximum number of attempts i.e. the upper bound for re-planning is the maximum number of allowed attempts. We also track the total action steps taken and the ratio of the successful action path length to the total steps, serving as a proxy for the action matching score in dynamic environments, where no reference action path exists as it does in static environments. It is used to quantify the planning and execution efficiency in the fail-and-re-planning setting, is also referred as process score (PS) by Wang et al. (2024a), or as completion rate (CR) by Zhang et al. (2024a).\nFor failed cases, following Xu et al. (2024), we categorize failures into three classes: False Completion (FC), where the agent incorrectly believes the task is completed; Reach Step Limit (RSL), where the agent reaches the maximum step limit without completing the task; and Invalid Action (IA), where the agent produces outputs that do not follow instructions, including invalid formats, nonexistent actions, or incorrect action parameters. Since OSCAR can handle invalid actions and false completions through execution and verification feedback, i.e. [Execute] \u2192 [Plan] and [Verify] \u2192 [Plan] state transitions, FC and IA errors do not occur in OSCAR. We further analyze a subclass of RSL, where re-planning generates the same task list or action trajectory that has already been marked as a verification failure in previous attempts. We refer to this subclass as Redundant Re-plan (RR). For comparison, we also analyze these metrics for FRIDAY, the most competitive baseline in dynamic OS environments, as shown in Table 2."}, {"title": "4 RELATED WORKS", "content": "GUI agents. LLM and LMM-based agents, equipped with advanced planning module, have been developed across various environments, including robotics, web browsing, gaming, software development, automating benchmark construction, data analysis, and AI for science. Although recent all-in-one agent development platforms have been released, most of these agents operate within specific domains, limiting their broader applicability. Among them, GUI agents-capable of interacting with various desktop and smartphone GUIs like human users offer broader applicability in automating real-world workflows. Some agents are continually pre-trained or fine-tuned on GUI-specific data. Others simulate GUI control in sandbox environments, such as AAA games or office workflows, which require internal application-specific APIs to interact with the environment. In a broader context, some agents interact with basic OS APIs but are often designed for static, pre-defined environments without grounding in real-time executable environments. Other agents follow linear action sequences and perform re-planning from scratch when verification fails, lacking fine-grained re-planning strategies, which makes them less efficient in real-world scenarios. Motivated by these limitations, we design OSCAR to handle real-time dynamic OS feedback using an efficient, state-aware, task-driven re-planning strategy.\nSynergizing LLMs and LMMs with OS. Beyond GUI agents, another line of work explores integrating LLMs and LMMs with OSs in two key areas: 1) optimizing or tuning traditional OS functions using LLMs, and 2) integrating LLMs into OS kernels (LLM as OS) to serve as system-level interfaces, facilitating local agent operations and deployment. The former includes optimizing CPU load balancing, improving storage access, and identifying and repairing code vulnerabilities. The latter focuses on OS-level hardware adaptation and resource management as well as agent-level resource scheduling and sharing, such as managing agent memory and enabling efficient communication among multiple heterogeneous agents sharing the same model back-end. Unlike these approaches, OSCAR functions as a generalist GUI agent, acting as an OS co-pilot to enhance user experience and productivity."}, {"title": "5 CONCLUSION", "content": "In this work, we introduced OSCAR, a generalist agent designed to autonomously navigate and interact with dynamic OS environments using a code-centric control framework. By leveraging task-driven re-planning and GUI-grounded observations, OSCAR demonstrates robust adaptability and effectiveness across both desktop and smartphone OS tasks. Our experiments on real-world workflow automation benchmarks, including GAIA, OSWorld, and AndroidWorld, showed that OSCAR outperforms existing methods, achieving significant improvements in task success rates, particularly for complex, multi-step tasks. Ablation studies further confirmed the importance of key components"}]}