{"title": "RoCo: Robust Collaborative Perception By Iterative Object Matching and Pose Adjustment", "authors": ["Zhe Huang", "Shuo Wang", "Yongcai Wang", "Wanting Li", "Deying Li", "Lei Wang"], "abstract": "Collaborative autonomous driving with multiple vehicles usually requires the data fusion from multiple modalities. To ensure effective fusion, the data from each individual modality shall maintain a reasonably high quality. However, in collaborative perception, the quality of object detection based on a modality is highly sensitive to the relative pose errors among the agents. It leads to feature misalignment and significantly reduces collaborative performance. To address this issue, we propose RoCo, a novel unsupervised framework to conduct iterative object matching and agent pose adjustment. To the best of our knowledge, our work is the first to model the pose correction problem in collaborative perception as an object matching task, which reliably associates common objects detected by different agents. On top of this, we propose a graph optimization process to adjust the agent poses by minimizing the alignment errors of the associated objects, and the object matching is re-done based on the adjusted agent poses. This process is carried out iteratively until convergence. Experimental study on both simulated and real-world datasets demonstrates that the proposed framework RoCo consistently outperforms existing relevant methods in terms of the collaborative object detection performance, and exhibits highly desired robustness when the pose information of agents is with high-level noise. Ablation studies are also provided to show the impact of its key parameters and components. The code is released at https://github.com/HuangZhe885/RoCo.", "sections": [{"title": "1 Introduction", "content": "Collaborative perception can significantly enhance perception performance by sharing information from different sensors among agents. It can overcome the inherent limitations of single-agent-based perception, such as invisibility caused by occlusion or long-range issues. Recent research [14, 16, 21, 45, 48] has spurred the widespread attention in the fields such as autonomous driving, metaverse, multi-robot systems, and multimedia application systems [1, 10, 17, 22, 25, 43, 46, 49]. As an emerging topic, collaborative 3D object detection is of significance but also faces many challenges, including model-agnostic and task-agnostic formulation [26], communication bandwidth constraints [16, 40], time delay [41], adversarial attacks [24, 42], and multimodal fusion [27, 29].\nIn collaborative 3D object detection, regardless of the modality of the employed sensors, the detection module of each agent transmits the obtained object detection features to the Ego agent. The Ego agent gathers the features of the same object and then carries out further information fusion. Therefore, the object detection features received from other agents need to be accurately aligned in the Ego agent for further processing. However, in practical applications, the pose estimated for each agent contains errors, which can in turn cause misalignment of the positions of the same objects across agents. For example, as illustrated in Figure 1(a), this misalignment may result in the incorrect fusion of Object 3 detected by agent i (red box) with Object 2 detected by agent j (green box). This issue becomes more pronounced when the pose error is large or high traffic is presented. This paper will focus on addressing the object object-matching-guided pose adjustment which can fundamentally prevent the introduction of additional noise into the features.\nWe conduct extensive experiments on both simulation and real-world datasets, including V2XSet[45] and DAIR-V2X[49]. Results show that RoCo consistently achieves the best performance in the task of collaborative 3D object detection with the presence of pose errors. In summary, the main contributions of this work are:\n\u2022 We propose RoCo, a novel robust multi-agent collaborative LiDAR-based 3D object detection framework that addresses the matching errors and pose inaccuracies between agents and objects. To the best of our knowledge, RoCo is the first to model the pose correction problem in collaborative perception as an object matching task.\n\u2022 The proposed RoCo establishes matching relationships between targets based on distance and neighborhood structural consistency using a graph matching approach. On top of this, RoCo iteratively adjusts agent poses based on global observation consistency, effectively filtering out incorrect object matches.\n\u2022 Extensive experiments have shown that RoCo achieves more accurate and robust 3D object detection performance even in the scenarios with vehicle congestion and significant noise."}, {"title": "2 Related work", "content": ""}, {"title": "2.1 Collaborative Perception", "content": "Collaborative perception is a promising mechanism in multi-agent systems that can overcome the limits of single-agent perception. To support research in this field, various strategies have been devised to address practical challenges[44][39][27][19][18]. To reduce communication bandwidth, Where2comm[16] selects spatially sparse but perceptually significant regions by using a spatial confidence map. V2X-ViT[45] includes a multi-agent attention module that aggregates information from various agents, improving detection performance. To handle communication delays, SyncNet [30] uses historical multi-frame information to compensate for current information, employing feature attention co-symbiotic estimation and time modulation techniques. CoBEVFlow [41] creates a synchrony-robust collaborative system for Bird's Eye View (BEV) flow that aligns asynchronous collaboration messages sent by various agents using motion compensation. HEAL [27] introduces a pioneering and adaptable collaborative perception framework designed to seamlessly integrate continually emerging heterogeneous agent types into collaborative perception tasks. However, in crowded and noisy road scenarios, information often becomes inconsistent and misaligned, leading to mismatches between objects detected by different agents and resulting in performance degradation. Therefore, in this work, we specifically considered the robustness to pose errors."}, {"title": "2.2 Noisy Pose Issue", "content": "Due to performance discrepancies between hardware acquisition devices and model estimations, the poses of agents are susceptible to interference, leading to misalignment and inconsistencies during fusion processes. Therefore, many methods attempt to design robust networks to correct error influences. The first category of methods primarily involves proposing robust network architectures and introducing specific modules to learn the influence of poses, such as V2VNet[40], MASH[13], V2X-ViT[45] and FeaCo[14]. However, these methods require additional pose supervision signals. The second category is to use the coarse observations as prior knowledge to adjust the poses of objects[28, 55]. CoAlign[28] introduced a method for modeling and optimizing agent-object pose graphs to enhance the consistency of relative poses, thereby improving model robustness. Nevertheless, in circumstances with crowded roadways, this approach\u2019s use of clustering to create connections between objects could result in mistakes and fail pose graph optimisation. In contrast, our proposed RoCo models the reduction in pose errors as a matching and optimization problem for objects, accurately identifying relationships between objects detected by multiple agents. Through iterative pose optimization based on the correct matching graph, RoCo improves the accuracy of 3D detection."}, {"title": "2.3 Object Matching", "content": "SLAM[3, 4, 8, 32, 33, 37, 38, 55] (Simultaneous Localization and Mapping) is a crucial research approach in scene understanding, primarily used by autonomous robots (such as robots and autonomous vehicles) to estimate their positions and build maps of the environment simultaneously during motion, without prior environmental information. Object matching in SLAM refers to associating detection results of the current frame with known objects in the scene. Most methods use metrics such as distances between different objects or Intersection over Union (IoU), followed by matching using the Hungarian algorithm[6, 34, 36, 52, 54], which is classical and high-performing but impractical for large-scale deployment in real environments. Therefore, researchers have explored many methods to facilitate its application[2][5][7]. To our knowledge, we are the first to model the pose correction problem of multiple agents in collaborative perception as an object matching and optimization task. This work marks the first application of SLAM-based object matching in collaborative perception, expanding its scope of application, and investigation into the impact of object matching on multi-agent collaborative perception systems in challenging environments."}, {"title": "3 Collaborative Perception and the Issue of Pose Error", "content": "Following the literature of collaborative perception [14, 28, 41], the commonly used model can be described as below.\nIt is assumed that there are N agents (i.e., collaborators) in the scene. Let \\(X_i\\) and \\(O_i\\) be the raw observation and the perception output of Ego agent i. \\(M_{j\\rightarrow i}\\) denotes the collaboration message sent from agent j to agent i. The goal of collaborative perception is to enhance the 3D object detection capability of the ego agent through cooperation. This process can be formally expressed as:\n\\begin{align}\nF_i &= \\Phi_{\\text{Enc}} (X_i), \\quad i = 1,\\ldots, N, \\tag{1a} \\\\\nM_{j\\rightarrow i} &= \\Phi_{\\text{Proj}} (\\xi_i, (F_j, \\bar{\\xi}_j)), \\quad j = 1, \\cdot \\cdot \\cdot, N; j \\neq i \\tag{1b} \\\\\nF &= \\Phi_{\\text{Agg}} (F_i, \\{M_{j\\rightarrow i}\\}_{j=1,2,...,N;j\\neq i}), \\tag{1c} \\\\\nO_i &= \\Phi_{\\text{Dec}} (F). \\tag{1d}\n\\end{align}\nFirst, for each agent, Step (1a) extracts feature \\(F_i\\) from the raw observation \\(X_i\\) via an encode network \\(\\Phi_{\\text{Enc}} (\\cdot )\\). After that, each agent j projects, via an projection module \\(\\Phi_{\\text{Proj}} (\\cdot )\\), its feature \\(F_j\\) to the agent i's coordinate system based on \\(\\xi_i\\) and \\(\\bar{\\xi}_j\\) which represent the poses of the two agents, respectively. The projected feature is then sent to agent i as a message \\(M_{j\\rightarrow i}\\), and this completes Step (1b). After receiving all the \\((N-1)\\) messages, agent i will fuse its feature \\(F_i\\) with these messages via a fusion network \\(\\Phi_{\\text{Agg}} (\\cdot )\\), producing a fused feature \\(F\\) as in Step (1c). Finally, Step (1d) uses a decode network \\(\\Phi_{\\text{Dec}} (\\cdot )\\) to convert \\(F\\) to the final perception output \\(O_i\\).\nAs seen above, when the pose of agent i is assumed to be given, the projection in Step (1b) will critically rely on the accuracy of the pose of agent j, \\(\\bar{\\xi}_j\\). However, the pose estimated by each agent cannot be perfect in practice and often come with noise, i.e., \\(\\{\\bar{\\xi}_j, j = 1,..., N\\}\\) have errors. This kind of error adversely affects the accuracy of the projected feature to be sent via the message \\(M_{j\\rightarrow i}\\). This leads to the misalignment of features when the message is processed by agent i and finally hurts the performance of 3D object detection.\nTo correct agent poses, existing work [28] innovatively uses a clustering based method to determine association among the bounding boxes detected by different agents. After finding object associations, the agent poses are adjusted by graph optimization accordingly. Nevertheless, when the error of agent pose becomes significant or when objects become close in crowded scenes, such a clustering based matching method will be prone to producing large matching errors. The matching errors will in turn lead to large pose adjusting errors, causing poor collaborative perception."}, {"title": "4 Our Proposed Method", "content": "To improve this situation, this paper proposes an unsupervised, iterative object matching and pose adjusting framework, called RoCo. It can maintain accurate matching relationships even in scenarios with high pose errors and traffic congestion, thereby enhancing object detection performance.\nOur solution consists of two key ideas. Firstly, finding reliable matching relationship between two objects depends not only on their spatial similarity but also on the similarity of the object configuration in the neighborhood of the two objects. Secondly, we take an iterative approach to conduct object matching and pose adjustment. That is, after using the current matching result to adjust the agent poses, object matching will be conducted again upon the updated poses. This process repeats until convergence, that is, the object matching does not change. This approach is able to improve the matching accuracy and in turn lead to better adjustment for the agent poses.\nIt is worth noting that the accuracy of object detection from each individual agent also affects the quality of object matching. In this work, we assume that every agent can independently and accurately detect objects within its own range. This allows us to focus on the issue of feature misalignment caused by pose errors."}, {"title": "4.1 Object Detection", "content": "Following the literature, the collaborative perception in this work uses LIDAR to perform 3D object detection. The input point cloud is in the dimensions of \\((n \\times 4)\\), where n denotes the number of points. Each point has its intensity and the 3D position in a world coordinate system. All agents use the same 3D object detector \\(\\Phi_{\\text{Enc}}\\)."}, {"title": "4.2 Graph-guided Object Matching", "content": "After performing object detection, agent j will share a message \\(M_{ji}\\) to the ego agent i. This message contains three pieces of information including i) its feature map \\(F_j\\); ii) the set of bounding boxes of the detect objects \\(B_j\\), in which each element has the form of \\((x, y, z, w, l, h, \\theta)\\), denoting the 3D centre position, the size in different dimensions, and the yaw angle, respectively, for each bounding box; iii) the pose of agent j denoted by \\(\\bar{\\xi}_j = (x_j, y_j, \\theta_j)\\), representing its centre position and the heading angle on the 2D space. Once the ego agent i receives the messages from all the other agents, it will start performing object matching by processing the messages one by one. Formally, the task can be described as follows: given the information \\(\\{B_i, \\xi_i\\}\\) and \\(\\{B_j, \\bar{\\xi}_j\\}\\), agent i needs to find the associations among the bounding boxes in \\(B_i\\) and \\(B_j\\).\nModeling this task as a bipartite graph matching problem [15], we seek an optimal matching relationship that maximizes the similarity between the objects detected by agents i and j. This can be formulated as\n\\begin{equation}\n\\hat{A}_{i,j} = \\mathop{\\text{arg max}}\\limits_{A_{i,j}} \\sum_{p \\in B_i} S (p, A_{i,j} (p)), \\tag{2}\n\\end{equation}\nwhere \\(A_{ij}\\) denotes a list of one-to-one matching from the elements in \\(B_i\\) to those in \\(B_j\\). \\(A_{i,j} (p)\\) represents the object, denoted by q, found in \\(B_j\\) by querying the matching relationship \\(A_{i,j}\\) using an object p in \\(B_i\\). Let \\(S(p, q)\\) denote the similarity of p and q, where \\(q = A_{i,j} (p)\\). A threshold \\(\\tau_1\\) is applied to select the reliable matching, i.e., \\(S(p, q) = 0\\), if \\(S(p, q) < \\tau_1\\). Those selected ones will constitute \\(\\hat{A}_{i,j}\\), representing the obtained optimal matching relationship."}, {"title": "4.2.1 Graph construction and Initial Matching", "content": "The proposed object matching method is graph-guided. First, for each object p in \\(B_i\\), we construct a star graph \\(G_p\\). Its central node is p and the spatial neighbors of p form other nodes of this graph, respectively. Each node has a feature vector consisting of the 3DoF pose \\((x, y, \\theta)\\) of the object bounding box. In the same way, for each object q in \\(B_j\\), a star graph \\(G_q\\) is constructed. They are illustrated in Figure 3(a).\nTo conduct initial matching, we transform \\(B_j\\) from agent j's coordinate frame into agent i's and establish the initial association using a distance-based method. For the two graphs \\(G_p\\) and \\(G_q\\), we evaluate the spatial distance between their central nodes p and q, denoted by \\(\\text{dis} (p, q)\\). When the distance is shorter than a threshold \\(\\tau_2\\), \\(G_p\\) and \\(G_q\\) will be considered for matching. Note that multiple objects in \\(B_j\\) could have distances to p shorter than \\(\\tau_2\\). The one with the minimum distance is chosen to be the initial match as\n\\begin{equation}\n\\hat{A}_{i,j} (p) = \\mathop{\\text{arg min}}\\limits_{p \\in B_i,q \\in B_j} \\text{dis} (p,q); \\quad \\text{where } \\text{dis} (p,q) \\le \\tau_2 \\tag{3}\n\\end{equation}"}, {"title": "4.2.2 Graph Structure Similarity", "content": "Logically, if p and q correspond to the same object, then the graphs \\(G_p\\) and \\(G_q\\) constructed from the two objects should have high similarity. Considering this, we design two types of similarity, edge similarity and distance similarity, to jointly measure the closeness of graphs \\(G_p\\) and \\(G_q\\) to assess whether p and q are from the same object.\nEdge Similarity. In graph \\(G_p\\), the edge between the central node p and one of its spatial neighbors m is denoted by \\(e_{pm}\\). It can be associated with a relative pose transformation from p to m, denoted by \\(T_{pm}\\). \\(T_{pm}\\) is a \\(3 \\times 3\\) matrix obtained through the detection output of the objects p and m. In the same way, in graph \\(G_q\\), the edges \\(e_{qn}\\) between node q and its neighbor n can be associated with a relative pose transformation \\(T_{qn}\\). If p and q correspond to the same object, i.e., if \\(G_p\\) and \\(G_q\\) have a high similarity, then the edges in two graphs should be consistent. In this case, \\(T_{qn}\\) should be consistent with \\(T_{pm}\\). We define the following criterion to evaluate the edge consistency between \\(e_{pm}\\) and \\(e_{qn}\\):\n\\begin{equation}\nl(e_{pm}, e_{qn}) = \\exp(-\\|T_{pm}(T_{qn})^{-1} - I\\|_F) \\tag{4}\n\\end{equation}\nwhere \\((T_{qn})^{-1}\\) is the inverse transformation of \\(e_{qn}\\), I denotes the identity matrix, \\(\\| \\cdot \\|_F\\) represents the Frobenius norm. The overall edge consistency score between \\(G_p\\) and \\(G_q\\) can be defined as:\n\\begin{equation}\nS_{\\text{edge}}(P,q) = \\frac{1}{\\|\\mathcal{N}_p\\|} \\sum_{m\\in \\mathcal{N}_p} l(e_{pm}, e_{qn}), \\quad \\text{s.t. } n = \\hat{A}_{i,j} (m) \\tag{5}\n\\end{equation}\nwhere \\(\\mathcal{N}_p\\) contains the leaf vertices in \\(G_p\\) that have corresponding vertices in \\(G_q\\). Note that the object n is the one found to correspond to object m in the list \\(\\hat{A}_{i,j}\\).\nDistance Similarity. In addition to edge similarity, we can also further quantify the closeness of the two graphs by using distance similarity. This is because if the central nodes p and q correspond to the same object, the spatial distance between the detection boxes should be sufficiently close, even in the presence of noise and or in a crowded situation. We define the distance similarity of p and q as:\n\\begin{equation}\nS_{\\text{dis}} (p, q) = \\exp (-\\text{dis} (p, q)) \\tag{6}\n\\end{equation}\nwhere \\(\\text{dis}(\\cdot , \\cdot )\\) is implemented as a Euclidean distance between the coordinates of the centers of the corresponding bounding boxes. Therefore, the overall graph similarity is obtained by combining the edge consistency and the distance consistency as\n\\begin{equation}\nS(p, q) = S_{\\text{edge}} (q, p) + \\lambda S_{\\text{dis}} (p, q) \\tag{7}\n\\end{equation}\nwhere \\(\\lambda\\) is the hyperparameter for balancing.\nFinally, we utilize the overall similarity \\(S\\) to implement the maximization problem in Equation (7) to find the optimal matching. Essentially, this is a bipartite graph problem, where one graph corresponds to all bounding boxes p detected by agent i, and the other corresponds to all bounding boxes q detected by agent j. The previously defined similarity \\(S(p, q)\\), serves as the edge weight in the bipartite graph. Solving this maximum matching problem can be accomplished using the Kuhn-Munkres algorithm [53]."}, {"title": "4.3 Robust Pose Graph Optimization", "content": "After performing object matching, the elements in the pair of object detection box sets \\(B_i\\) and \\(B_j\\) will establish one-to-one correspondences. As shown in Figure 3(b), we then construct a pose graph \\(G_{\\mathcal{G}} = (V_{\\text{agent}}, V_{\\text{agent}}, V_{\\text{object}}, \\mathcal{E})\\) for all the agents and the detected objects in the whole scene and use this correspondence to adjust their poses. The node set \\(V_{\\text{agent}}\\) comprises all \\(N\\) agents, while the node set \\(V_{\\text{object}}\\) consists of all detected objects, the edge set \\(\\mathcal{E}\\) represents the detection relationships between agents and objects. As mentioned at the beginning of Section 4, there could still be incorrect matching relationships between objects, even after the process proposed in Section 4.2, this will affect the adjustment of the poses of agents. Therefore, we develop an iterative process as what follows in order to achieve robust object matching and pose adjustment.\nFirstly, after obtaining the object matching relationship \\(\\hat{A}_{i,j}\\), as in Section 4.2, we will perform pose adjustment guided by this matching relationship; After this, the elements of \\(B_i\\) and \\(B_j\\) will be updated; We then use the updated \\(B_i\\) and \\(B_j\\) to perform a new round of object matching as in Section 4.2, and then a new round of pose adjustment will be conducted. This iterative matching and adjustment process repeats until the matching result converges, yielding the optimal matching result \\(\\hat{A}_{i}\\) and the optimized poses \\(\\bar{\\xi}_j\\) and \\(X_k\\). The process is illustrated in Figure 3(a)(b). We model this task as a graph optimization problem. In this pose graph, each node is associated with a pose. The pose of the jth agent is denoted as \\(\\bar{\\xi}_j\\), and the pose of the kth object is denoted as \\(X_k\\). Let us assume that after the step in Section 4.2, p and q form a matching pair (\\(\\hat{A}_{i,j} (p) = q\\)) and they are the bounding boxes of the same object.\nLet \\(T_{ip}\\) be the measured relative transformation of the pth object from the perspective of the ith agent, which is naturally obtained through the ith agent's detection output; \\(T_{jq}\\) is the measured relative transformation of the qth object from the perspective of the jth agent. From agent i's observation, we have \\(T_{ip} = \\bar{\\Xi}_i^{-1}X_p\\), so \\(X_p = \\bar{\\Xi}_i T_{ip}\\). Note that \\(\\bar{\\Xi}_i\\) is a matrix constructed from \\(\\bar{\\xi}_i\\) which denotes the pose of agent i 1 . Similarly, we can get \\(X_q = \\bar{\\Xi}_j T_{jq}\\). In the optimization process, the variables to optimize in this scenario are \\(\\{\\bar{\\xi}_j, X_k\\}\\). The matched objects are hoped to converge to a consistent result \\(X_p = X_q = X_k\\). An example is given in Figure 4 to show how the residue errors are set up. We can set up the residue error functions:\n\\begin{align}\ne_{ik} &= (\\bar{\\Xi}_i T_{ip})^{-1}X_k = T_{ip}^{-1} \\bar{\\Xi}_i^{-1}X_k \\tag{8} \\\\\ne_{jk} &= (\\bar{\\Xi}_j T_{jq})^{-1}X_k = T_{jq}^{-1} \\bar{\\Xi}_j^{-1}X_k \\tag{9} \\\\\ne_{ji} &= T_{ji} \\bar{\\Xi}_j^{-1} \\bar{\\Xi}_i \\tag{10}\n\\end{align}\nThe overall optimization objective can be given as:\n\\begin{equation}\n\\{\\bar{\\xi}_j, X_k\\} = \\mathop{\\text{arg min}}\\limits_{\\{\\bar{\\xi}_j, X_k\\}} \\sum_{(j,k)} (\\e_{ik}^T \\Omega \\e_{ik} + \\e_{jk}^T \\Omega \\e_{jk} + \\e_{ji}^T \\Omega \\e_{ji}) \\tag{11}\n\\end{equation}\nwhere \\(\\Omega = \\text{diag} (\\sigma^{-2}_x, \\sigma^{-2}_y, \\sigma^{-2}_{\\theta}) \\in \\mathbb{R}^{3\\times 3}\\) is the information matrix and its diagonal elements come from the uncertainty estimates of the bounding box, which are part of the detection output from agent j. By solving (11) using Levenberg-Marquardt algorithm [23], we get \\(\\{\\bar{\\xi}'_j, j = 1 : N\\}, \\{X'_k, k = 1 : K\\}\\). Then we re-do the object matching result:\n\\begin{equation}\n\\hat{A}_{ij} = \\text{Match} (\\{B_i \\bar{\\xi}'_i\\}, \\{B_j \\bar{\\xi}'_j\\}) \\tag{12}\n\\end{equation}\nIn the iterative matching (12) and pose optimization (11) process, we update the position of \\(B_i\\) using \\(\\bar{\\xi}_j\\). Our goal is to gradually improve object matching results by minimizing the overall graph optimization error. During the iterative matching process, we fix the poses of self-agent and update the poses of other agents and the objects."}, {"title": "4.4 Aggregation and Detection", "content": "After the pose calibration, the corrected relative pose from agent j to agent i is denoted as \\(\\bar{\\xi}_{ji} = \\bar{\\xi}_i^{-1}\\bar{\\xi}_j\\). With the corrected relative pose \\(\\bar{\\xi}_{ji}\\), we can adjust the contents of \\(M_{j\\rightarrow i}\\), feature \\(F_j\\) from agent j are synchronized to the ego pose which has the same coordinate system with its ego feature \\(F_i\\). After the spatial alignment, each agent aggregates \\(\\Phi_{\\text{Agg}} (\\cdot )\\) other agents' collaboration information and obtain a more informative feature. This aggregating function can be any common fusion operation. All our experiments adopt Multi-scale feature fusion. After receiving the final fused feature maps, we decode them into the final detection layer \\(\\Phi_{\\text{Dec}} (\\cdot )\\) to obtain final detections \\(O_i\\). The regression output is \\((x, y, z, w, l, h, \\theta)\\), denoting the position, size, yaw angle of the anchor boxes, respectively. The classification output is the confidence score of being an vehicle or background for each anchor box."}, {"title": "5 Experimental Result", "content": "We conduct extensive experiments on both simulated and real-world scenarios. The task is point-cloud-based 3D object detection. Following the literature, the detection results are evaluated by Average Precision (AP) at Intersection-over-Union (IoU) threshold of 0.50 and 0.70."}, {"title": "5.1 Dataset", "content": "DAIR-V2X [49]. DAIR-V2X is a public real-world collaborative perception dataset. It contains two agents: vehicle and road-side-unit (RSU) with image resolution 1080 \u00d7 1920. The perception range is x \u2208 [-100m, 100m], y \u2208 [-40m, 40m]. RSU LiDAR is 300-channel while the vehicle's is 40-channel. V2XSet [45]. V2XSet is a large-scale dataset designed for Vehicle-to-Infrastructure (V2X) communication. The data in V2XSet is collected using the simulator CARLA [11] and OpenCDA [31]. It includes LiDAR data captured from multiple autonomous vehicles and roadside infrastructure in various scenarios. The dataset consists of a total of 11,447 frames, with train, validation, test splits of 6,694/1,920/2,833 frames, respectively."}, {"title": "5.2 Implementation Details", "content": "For encoder backbone, we use PointPillar [22] with the voxel resolution to 0.4m for both height and width. To simulate pose errors, we follow the noisy settings in CoAlign [28] during the training process. We add Gaussian noise \\(\\mathcal{N} (0, \\sigma^2)\\) on x, y and \\(\\mathcal{N} (0, \\sigma^2)\\) on \\(\\theta\\), where x, y, \\(\\theta\\) are the coordinates of the 2D centers of a vechicle and the yaw angle of accurate global poses. We use the bounding boxes output by the detection framework in reference [28] for each individual agent. In the bipartite graph matching, we set \\(\\lambda = 1\\) for Eq.(7) and similarity threshold \\(t_1 = 0.5\\) for Eq.(2). In the pose graph optimization, the Levenberg-Marquardt algorithm[23] is employed to solve the least squares optimization problem, with the maximum number of iterations set to 1000. To better contrast the performance of RoCo, we retrained other methods. We use the Adam [20] with an initial learning rate 0.001 for detection and 0.002 for feature fusion. The batchsizes we set for the DAIR-V2X and V2XSet datasets are 4 and 2, respectively. All models are trained on six NVIDIA RTX 2080Ti GPUs with epoch number 30."}, {"title": "5.3 Quantitative evaluation", "content": "To validate the overall performance of RoCo in 3D object detection, we compare it with a series of previous methods on two datasets. For a fair comparison, all models take 3D point clouds as input data, RoCo is compared with seven state-of-the-art methods: F-Cooper [9], FPV-RCNN[51], V2VNet [40], V2X-ViT[45], Self-ATT, CoAlign [28] and CoBEVFlow [41]. Table 1 shows the AP at IoU threshold of 0.5 and 0.7 in DAIR-V2X and V2XSet dataset. We reference the results from the literatures [28, 41] in the DAIR-V2X and implement them in the V2XSet, and we also implement noise level of \\(\\sigma^2_T / \\sigma^2_R = 0.8m/0.8^\\circ\\) on both datasets. We see that RoCo significantly outperforms the previous methods at various noise levels across both datasets. In the real-world dataset DAIR-V2X, RoCo outperforms CoAlign across all noise settings. In the case of noise levels of 0.0m/0.0\u00b0, our approach achieves 1.7% (76.3% vs. 74.6%) and 1.6% (62.0% vs. 60.4%) improvement over CoAlign for AP@0.5/0.7. When the noise level becomes as high as 0.6m/0.6\u00b0, our approach achieves 1.9% (71.9% vs. 70.20%) and 1.2% (58.2% vs. 57.0%) improvement over CoAlign for AP@0.5/0.7. In the case of noise levels of 0.8/0.8, our approach still achieves 2.3% (71.5% vs. 69.2%) and 0.9% (57.6% vs. 56.9%) improvement over CoAlign for AP@0.5/0.7. We conduct experiments on a simulated dataset V2XSet which involves more agents. RoCo still performs well across all noise settings, as shown in Table 1. As the level of noise increases, both methods experience performance degradation. However, our method maintains a higher level of accuracy even under high-level of noise. In the case of noise level of 0.4m/0.4\u00b0, our approach achieves 1.9% (90.0% vs. 88.1%) and 4.3% (77.3% vs. 73.0%) improvement over CoAlign for AP@0.5/0.7. When the noise level reaches 0.8m/0.8\u00b0, our approach still achieves 1.4% (84.1% vs. 82.7%) and 1.6% (68.9% vs. 67.3%) improvement over CoAlign for AP@0.5/0.7."}, {"title": "5.4 Qualitative evaluation", "content": "Figure 5 and Figure 6 show the 3D detection results in the BEV format on the V2XSet. Red and green boxes denote the detection results and the ground-truth, respectively. The degree of overlapping of these boxes reflects the performance of the testing methods. Figure 5 depicts the detection results of V2X-ViT, V2VNet, CoAlign, and the proposed RoCo at an"}]}