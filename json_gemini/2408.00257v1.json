{"title": "RoCo: Robust Collaborative Perception By Iterative Object Matching and Pose Adjustment", "authors": ["Zhe Huang", "Shuo Wang", "Yongcai Wang", "Wanting Li", "Deying Li", "Lei Wang"], "abstract": "Collaborative autonomous driving with multiple vehicles usually requires the data fusion from multiple modalities. To ensure effective fusion, the data from each individual modality shall maintain a reasonably high quality. However, in collaborative perception, the quality of object detection based on a modality is highly sensitive to the relative pose errors among the agents. It leads to feature misalignment and significantly reduces collaborative performance. To address this issue, we propose RoCo, a novel unsupervised framework to conduct iterative object matching and agent pose adjustment. To the best of our knowledge, our work is the first to model the pose correction problem in collaborative perception as an object matching task, which reliably associates common objects detected by different agents. On top of this, we propose a graph optimization process to adjust the agent poses by minimizing the alignment errors of the associated objects, and the object matching is re-done based on the adjusted agent poses. This process is carried out iteratively until convergence. Experimental study on both simulated and real-world datasets demonstrates that the proposed framework RoCo consistently outperforms existing relevant methods in terms of the collaborative object detection performance, and exhibits highly desired robustness when the pose information of agents is with high-level noise. Ablation studies are also provided to show the impact of its key parameters and components. The code is released at https://github.com/HuangZhe885/RoCo.", "sections": [{"title": "1 Introduction", "content": "Collaborative perception can significantly enhance perception performance by sharing information from different sensors among agents. It can overcome the inherent limitations of single-agent-based perception, such as invisibility caused by occlusion or long-range issues. Recent research [14, 16, 21, 45, 48] has spurred the widespread attention in the fields such as autonomous driving, metaverse, multi-robot systems, and multimedia application systems [1, 10, 17, 22, 25, 43, 46, 49]. As an emerging topic, collaborative 3D object detection is of significance but also faces many challenges, including model-agnostic and task-agnostic formulation [26], communication bandwidth constraints [16, 40], time delay [41], adversarial attacks [24, 42], and multimodal fusion [27, 29].\nIn collaborative 3D object detection, regardless of the modality of the employed sensors, the detection module of each agent transmits the obtained object detection features to the Ego agent. The Ego agent gathers the features of the same object and then carries out further information fusion. Therefore, the object detection features received from other agents need to be accurately aligned in the Ego agent for further processing. However, in practical applications, the pose estimated for each agent contains errors, which can in turn cause misalignment of the positions of the same objects across agents. For example, as illustrated in Figure 1(a), this misalignment may result in the incorrect fusion of Object 3 detected by agent i (red box) with Object 2 detected by agent j (green box). This issue becomes more pronounced when the pose error is large or high traffic is presented. This paper will focus on addressing the object object-matching-guided pose adjustment which can fundamentally prevent the introduction of additional noise into the features.\nWe conduct extensive experiments on both simulation and real-world datasets, including V2XSet[45] and DAIR-V2X[49]. Results show that RoCo consistently achieves the best performance in the task of collaborative 3D object detection with the presence of pose errors. In summary, the main contributions of this work are:\n\u2022 We propose RoCo, a novel robust multi-agent collaborative LiDAR-based 3D object detection framework that addresses the matching errors and pose inaccuracies between agents and objects. To the best of our knowledge, RoCo is the first to model the pose correction problem in collaborative perception as an object matching task.\n\u2022 The proposed RoCo establishes matching relationships between targets based on distance and neighborhood structural consistency using a graph matching approach. On top of this, RoCo iteratively adjusts agent poses based on global observation consistency, effectively filtering out incorrect object matches.\n\u2022 Extensive experiments have shown that RoCo achieves more accurate and robust 3D object detection performance even in the scenarios with vehicle congestion and significant noise."}, {"title": "2 Related work", "content": ""}, {"title": "2.1 Collaborative Perception", "content": "Collaborative perception is a promising mechanism in multi-agent systems that can overcome the limits of single-agent perception. To support research in this field, various strategies have been devised to address practical challenges[44][39][27][19][18]. To reduce communication bandwidth, Where2comm[16] selects spatially sparse but perceptually significant regions by using a spatial confidence map. V2X-ViT[45] includes a multi-agent attention module that aggregates information from various agents, improving detection performance. To handle communication delays, SyncNet [30] uses historical multi-frame information to compensate for current information, employing feature attention co-symbiotic estimation and time modulation techniques. CoBEVFlow [41] creates a synchrony-robust collaborative system for Bird's Eye View (BEV) flow that aligns asynchronous collaboration messages sent by various agents using motion compensation. HEAL [27] introduces a pioneering and adaptable collaborative perception framework designed to seamlessly integrate continually emerging heterogeneous agent types into collaborative perception tasks. However, in crowded and noisy road scenarios, information often becomes inconsistent and mis-aligned, leading to mismatches between objects detected by different agents and resulting in performance degradation. Therefore, in this work, we specifically considered the robustness to pose errors."}, {"title": "2.2 Noisy Pose Issue", "content": "Due to performance discrepancies between hardware acquisition devices and model estimations, the poses of agents are susceptible to interference, leading to misalignment and inconsistencies during fusion processes. Therefore, many methods attempt to design robust networks to correct error influences. The first category of methods primarily involves proposing robust network architectures and introducing specific modules to learn the influence of poses,"}, {"title": "2.3 Object Matching", "content": "SLAM[3, 4, 8, 32, 33, 37, 38, 55] (Simultaneous Localization and Mapping) is a crucial research approach in scene understanding, primarily used by autonomous robots (such as robots and autonomous vehicles) to estimate their positions and build maps of the environment simultaneously during motion, without prior environmental information. Object matching in SLAM refers to associating detection results of the current frame with known objects in the scene. Most methods use metrics such as distances between different objects or Intersection over Union (IoU), followed by matching using the Hungarian algorithm[6, 34, 36, 52, 54], which is classical and high-performing but impractical for large-scale deployment in real environments. Therefore, researchers have explored many methods to facilitate its application[2][5][7]. To our knowledge, we are the first to model the pose correction problem of multiple agents in collaborative perception as an object matching and optimization task. This work marks the first application of SLAM-based object matching in collaborative perception, expanding its scope of application, and investigation into the impact of object matching on multi-agent collaborative perception systems in challenging environments."}, {"title": "3 Collaborative Perception and the Issue of Pose Error", "content": "Following the literature of collaborative perception [14, 28, 41], the commonly used model can be described as below.\nIt is assumed that there are N agents (i.e., collaborators) in the scene. Let \\(X_i\\) and \\(O_i\\) be the raw observation and the perception output of Ego agent i. \\(M_{j \\rightarrow i}\\) denotes the collaboration message sent from agent j to agent i. The goal of collaborative perception is to enhance the 3D object detection capability of the ego agent through cooperation. This process can be formally expressed as:\n\\begin{align}\nF_i &= \\Phi_{\\text{Enc}} (X_i), i = 1,..., N, \\tag{1a}\\\\\nM_{j\\rightarrow i} &= \\Phi_{\\text{Proj}} (\\mathbf{z}_i, (F_j, \\mathbf{\\xi}_j)), j = 1, \\cdot \\cdot \\cdot, N; j \\neq i \\tag{1b}\\\\\nF &= \\Phi_{\\text{Agg}} (F_i, \\{M_{j\\rightarrow i}\\}_{j=1,2,...,N;j\\neq i}),\\tag{1c}\\\\\nO_i &= \\Phi_{\\text{Dec}} (F).\\tag{1d}\n\\end{align}\nFirst, for each agent, Step (1a) extracts feature \\(F_i\\) from the raw observation \\(X_i\\) via an encode network \\(\\Phi_{\\text{Enc}} (\\cdot )\\). After that, each agent j projects, via an projection module \\(\\Phi_{\\text{Proj}} (\\cdot )\\), its feature \\(F_j\\) to the agent i's coordinate system based on \\(\\mathbf{z}_i\\) and \\(\\mathbf{\\xi}_j\\) which represent the poses of the two agents, respectively. The projected feature is then sent to agent i as a message \\(M_{j \\rightarrow i}\\), and this completes Step (1b). After receiving all the \\((N-1)\\) messages, agent i will fuse its feature \\(F_i\\) with these messages via a fusion network \\(\\Phi_{\\text{Agg}} (\\cdot )\\), producing a fused feature \\(F\\) as in Step (1c). Finally, Step (1d) uses a decode network \\(\\Phi_{\\text{Dec}} (\\cdot )\\) to convert \\(F\\) to the final perception output \\(O_i\\).\nAs seen above, when the pose of agent i is assumed to be given, the projection in Step (1b) will critically rely on the accuracy of the pose of agent j, \\(\\mathbf{\\xi}_j\\). However, the pose estimated by each agent cannot be perfect in practice and often come with noise, i.e., \\(\\{\\mathbf{\\xi}_j, j = 1,..., N\\}\\) have errors. This kind of error adversely affects the accuracy of the projected feature to be sent via the message \\(M_{j \\rightarrow i}\\). This leads to the misalignment of features when the message is processed by agent i and finally hurts the performance of 3D object detection.\nTo correct agent poses, existing work [28] innovatively uses a clustering based method to determine association among the bounding boxes detected by different agents. After finding object associations, the agent poses are adjusted by graph optimization accordingly. Nevertheless, when the error of agent pose becomes significant or when objects become close in crowded scenes, such a clustering based matching method will be prone to producing large matching errors. The matching errors will in turn lead to large pose adjusting errors, causing poor collaborative perception."}, {"title": "4 Our Proposed Method", "content": "To improve this situation, this paper proposes an unsupervised, iterative object matching and pose adjusting framework, called RoCo. It can maintain accurate matching relationships even in scenarios with high pose errors and traffic congestion, thereby enhancing object detection performance.\nOur solution consists of two key ideas. Firstly, finding reliable matching relationship between two objects depends not only on their spatial similarity but also on the similarity of the object configuration in the neighborhood of the two objects. Secondly, we take an iterative approach to conduct object matching and pose adjustment. That is, after using the current matching result to adjust the agent poses, object matching will be conducted again upon the updated poses. This process repeats until convergence, that is, the object matching does not change. This approach is able to improve the matching accuracy and in turn lead to better adjustment for the agent poses.\nIt is worth noting that the accuracy of object detection from each individual agent also affects the quality of object matching. In this work, we assume that every agent can independently and accurately detect objects within its own range. This allows us to focus on the issue of feature misalignment caused by pose errors."}, {"title": "4.1 Object Detection", "content": "Following the literature, the collaborative perception in this work uses LIDAR to perform 3D object detection. The input point cloud is in the dimensions of \\((n \\times 4)\\), where n denotes the number of points. Each point has its intensity and the 3D position in a world coordinate system. All agents use the same 3D object detector \\(\\Phi_{\\text{Enc}}\\)."}, {"title": "4.2 Graph-guided Object Matching", "content": "After performing object detection, agent j will share a message \\(M_{j \\rightarrow i}\\) to the ego agent i. This message contains three pieces of information including i) its feature map \\(F_j\\); ii) the set of bounding boxes of the detect objects \\(B_j\\), in which each element has the form of \\((x, y, z, w, l, h, \\theta)\\), denoting the 3D centre position, the size in different dimensions, and the yaw angle, respectively, for each bounding box; iii) the pose of agent j denoted by \\(\\mathbf{\\xi}_j = (x_j, y_j, \\theta_j)\\), representing its centre position and the heading angle on the 2D space. Once the ego agent i receives the messages from all the other agents, it will start performing object matching by processing the messages one by one. Formally, the task can be described as follows: given the information \\(\\{B_i, \\mathbf{\\xi}_i\\}\\) and \\(\\{B_j, \\mathbf{\\xi}_j\\}\\), agent i needs to find the associations among the bounding boxes in \\(B_i\\) and \\(B_j\\).\nModeling this task as a bipartite graph matching problem [15], we seek an optimal matching relationship that maximizes the similarity between the objects detected by agents i and j. This can be formulated as\n\\[\nA^* = \\underset{A_{i,j}}{\\text{arg max}} \\sum_{p \\in B_i} S (p, A_{ij} (p)),\n\\]\nwhere \\(A_{ij}\\) denotes a list of one-to-one matching from the elements in \\(B_i\\) to those in \\(B_j\\). \\(A_{ij} (p)\\) represents the object, denoted by q, found in \\(B_j\\) by querying the matching relationship \\(A_{i,j}\\) using an object p in \\(B_i\\). Let \\(S(p, q)\\) denote the similarity of p and q, where \\(q = A_{ij}(p)\\). A threshold \\(\\tau_1\\) is applied to select the reliable matching, i.e., \\(S(p, q) = 0\\), if \\(S(p, q) < \\tau_1\\). Those selected ones will constitute \\(A_{ij}^*\\), representing the obtained optimal matching relationship."}, {"title": "4.2.1 Graph construction and Initial Matching", "content": "The proposed object matching method is graph-guided. First, for each object p in \\(B_i\\), we construct a star graph \\(G_p\\). Its central node is p and the spatial neighbors of p form other nodes of this graph, respectively. Each node has a feature vector consisting of the 3DoF pose (x, y, \\(\\theta\\)) of the object bounding box. In the same way, for each object q in \\(B_j\\), a star graph \\(G_q\\) is constructed. They are illustrated in Figure 3(a).\nTo conduct initial matching, we transform \\(B_j\\) from agent j's coordinate frame into agent i's and establish the initial association using a distance-based method. For the two graphs \\(G_p\\) and \\(G_q\\), we evaluate the spatial distance between their central nodes p and q, denoted by \\(\\text{dis} (p, q)\\). When the distance is shorter than a threshold \\(\\tau_2\\), \\(G_p\\) and \\(G_q\\) will be considered for matching. Note that multiple objects in \\(B_j\\) could have distances to p shorter than \\(\\tau_2\\). The one with the minimum distance is chosen to be the initial match as\n\\[\nA_{ij} (p) = \\underset{q \\in B_j}{\\text{arg min }} \\text{dis} (p,q); \\text{ where dis} (p,q) \\leq \\tau_2\n\\]"}, {"title": "4.2.2 Graph Structure Similarity", "content": "Logically, if p and q correspond to the same object, then the graphs \\(G_p\\) and \\(G_q\\) constructed from the two objects should have high similarity. Considering this, we design two types of similarity, edge similarity and distance similarity, to"}, {"title": "4.3 Robust Pose Graph Optimization", "content": "After performing object matching, the elements in the pair of object detection box sets \\(B_i\\) and \\(B_j\\) will establish one-to-one correspondences. As shown in Figure 3(b), we then construct a pose graph \\(\\mathcal{G} \\triangleq \\mathcal{G}(\\mathcal{V}_{\\text{agent}}, \\mathcal{V}_{\\text{agent}}, \\mathcal{V}_{\\text{object}}, \\mathcal{E})\\) for all the agents and the detected objects in the whole scene and use this correspondence to adjust their poses. The node set \\(\\mathcal{V}_{\\text{agent}}\\) comprises all N agents, while the node set \\(\\mathcal{V}_{\\text{object}}\\) consists of all detected objects, the edge set \\(\\mathcal{E}\\) represents Let \\(T_{ip}\\) be the measured relative transformation of the pth object from the perspective of the ith agent, which is naturally obtained through the ith agent's detection output; \\(T_{jq}\\) is the measured relative transformation of the qth object from the perspective of the jth agent. From agent i's observation, we have \\(T_{ip} = \\Xi_i^{-1}X_p\\), so \\(X_p = \\Xi_iT_{ip}\\). Note that \\(\\Xi_i\\) is a matrix constructed from \\(\\mathbf{\\xi}_i\\) which denotes the pose of agent i. Similarly, we can get \\(X_q = \\Xi_jT_{jq}\\). In the optimization process, the variables to optimize in this scenario are \\(\\{\\mathbf{\\xi}_j, X_k\\}\\). The matched objects are hoped to converge to a consistent result \\(X_p = X_q = X_k\\). An example is given in Figure 4 to show how the residue errors are set up. We can set up the residue error functions:\n\\begin{align}\ne_{ik} &= (\\Xi_iT_{ip})^{-1}X_k = T_{ip}^{-1}\\Xi_i^{-1}X_k\\\\ \ne_{jk} &= (\\Xi_jT_{jq})^{-1}X_k = T_{jq}^{-1}\\Xi_j^{-1}X_k \\\\\ne_{ji} &= T_{jq}^{-1}\\Xi_j^{-1}\\Xi_i\n\\end{align}\nThe overall optimization objective can be given as:\n\\begin{equation}\n\\{\\mathbf{\\xi}_{j},X_{k}\\} =  \\underset{\\{\\mathbf{\\xi}_{j},X_{k}\\}}{\\text{arg min }} \\sum_{(j,k)}(\\Omega_{ik}e_{ik}^2 + \\Omega_{jk}e_{jk}^2 + \\Omega_{ji}e_{ji}^2)\n\\end{equation}\nWhere \\(\\Omega = \\text{diag} (\\sigma_x^{-2}, \\sigma_y^{-2}, \\sigma_{\\theta}^{-2}) \\in \\mathbb{R}^{3\\times 3}\\) is the information matrix and its diagonal elements come from the uncertainty estimates of the bounding box, which are part of the detection output from agent j. By solving (11) using Levenberg-Marquardt algorithm [23], we get \\(\\{\\mathbf{\\xi}'_{j}, j = 1 : N\\}, \\{X'_{k}, k = 1 : K\\}\\). Then we re-do the object matching result:\n\\[\nA_{ij} = \\text{Match} (\\{B_i\\mathbf{\\xi}'_{i}\\}, \\{B_j\\mathbf{\\xi}'_{j}\\})\n\\]\nIn the iterative matching (12) and pose optimization (11) process, we update the position of \\(B_i\\) using \\(\\mathbf{\\xi}'_{i}\\). Our goal is to gradually improve object matching results by minimizing the overall graph optimization error. During the iterative matching process, we fix the poses of self-agent and update the poses of other agents and the objects."}, {"title": "4.4 Aggregation and Detection", "content": "After the pose calibration, the corrected relative pose from agent j to agent i is denoted as \\(\\mathbf{\\xi}_{ji} = \\mathbf{\\xi}^{-1}_{i} \\oplus \\mathbf{\\xi}_{j}\\). With the corrected relative pose \\(\\mathbf{\\xi}_{ji}\\), we can adjust the contents of \\(M_{j \\rightarrow i}\\). The feature \\(F_j\\) from agent j are synchronized to the ego pose which has the same coordinate system with its ego feature \\(F_i\\). After the spatial alignment, each agent aggregates \\(\\Phi_{\\text{Agg}} (\\cdot )\\) other agents' collaboration information and obtain a more informative feature. This aggregating function can be any common fusion operation. All our experiments adopt Multi-scale feature fusion. After receiving the final fused feature maps, we decode them into the final detection layer \\(\\Phi_{\\text{Dec}} (\\cdot )\\) to obtain final detections \\(O_i\\). The regression output is \\((x, y, z, w, l, h, \\theta)\\), denoting the position, size, yaw angle of the anchor boxes, respectively. The classification output is the confidence score of being an vehicle or background for each anchor box."}, {"title": "5 Experimental Result", "content": "We conduct extensive experiments on both simulated and real-world scenarios. The task is point-cloud-based 3D object detection. Following the literature, the detection results are evaluated by Average Precision (AP) at Intersection-over-Union (IoU) threshold of 0.50 and 0.70."}, {"title": "5.1 Dataset", "content": "DAIR-V2X [49]. DAIR-V2X is a public real-world collaborative perception dataset. It contains two agents: vehicle and road-side-unit (RSU) with image resolution 1080 \u00d7 1920. The perception range is x \u2208 [-100m, 100m], \u0443 \u0454 [-40m, 40m]. RSU LiDAR is 300-channel while the vehicle's is 40-channel. V2XSet [45]. V2XSet is a large-scale dataset designed for Vehicle-to-Infrastructure (V2X) communication. The data in V2XSet is collected using the simulator CARLA [11] and OpenCDA [31]. It includes LiDAR data captured from multiple autonomous vehicles and roadside infrastructure in various scenarios. The dataset consists of a total of 11,447 frames, with train, validation, test splits of 6,694/1,920/2,833 frames, respectively."}, {"title": "5.2 Implementation Details", "content": "For encoder backbone, we use PointPillar [22] with the voxel resolution to 0.4m for both height and width. To simulate pose errors, we follow the noisy settings in CoAlign [28] during the training process. We add Gaussian noise \\(N (0, \\sigma^2)\\) on x, y and \\(N (0, \\sigma^2)\\) on \\(\\theta\\), where x, y, \\(\\theta\\) are the coordinates of the 2D centers of a vechicle and the yaw angle of accurate global poses. We use the bounding boxes output by the detection framework in reference [28] for each individual agent. In the bipartite graph matching, we set \\(\\lambda = 1\\) for Eq.(7) and similarity threshold \\(t_1 = 0.5\\) for Eq.(2). In the pose graph optimization, the Levenberg-Marquardt algorithm[23] is employed to solve the least squares optimization problem, with the maximum number of iterations set to 1000. To better contrast the performance of RoCo, we retrained other methods. We use the Adam [20] with an initial learning rate 0.001 for detection and 0.002 for feature fusion. The batchsizes we set for the DAIR-V2X and V2XSet datasets are 4 and 2, respectively. All models are trained on six NVIDIA RTX 2080Ti GPUs with epoch number 30."}, {"title": "5.3 Quantitative evaluation", "content": "To validate the overall performance of RoCo in 3D object detection, we compare it with a series of previous methods on two datasets. For a fair comparison, all models take 3D point clouds as input data, RoCo is compared with seven state-of-the-art methods: F-Cooper [9], FPV-RCNN[51], V2VNet [40], V2X-ViT[45], Self-ATT, CoAlign [28] and CoBEVFlow [41]. Table 1 shows the AP at IoU threshold of 0.5 and 0.7 in DAIR-V2X and V2XSet dataset. We reference the results from the literatures [28, 41] in the DAIR-V2X and implement them in the V2XSet, and we also implement noise level of \\(\\sigma^2_T/\\sigma^2_R = 0.8m/0.8^\\circ\\) on both datasets. We see that RoCo significantly outperforms the previous methods at various noise levels across both datasets. In the real-world dataset DAIR-V2X, RoCo outperforms CoAlign across all noise settings. In the case of noise levels of 0.0m/0.0\u00b0, our approach achieves 1.7% (76.3% vs. 74.6%) and 1.6% (62.0% vs. 60.4%) improvement over CoAlign for AP@0.5/0.7. When the noise level becomes as high as 0.6m/0.6\u00b0, our approach achieves 1.9% (71.9% \u03c5\u03c2. 70.20%) and 1.2% (58.2% vs. 57.0%) improvement over CoAlign for AP@0.5/0.7. In the case of noise levels of 0.8/0.8, our approach still achieves 2.3% (71.5% vs. 69.2%) and 0.9% (57.6% \u03c5\u03c2. 56.9%) improvement over CoAlign for AP@0.5/0.7. We conduct experiments on a simulated dataset V2XSet which involves more agents. RoCo still performs well across all noise settings, as shown in Table 1. As the level of noise increases, both methods experience performance degradation. However, our method maintains a higher level of accuracy even under high-level of noise. In the case of noise level of 0.4m/0.4\u00b0, our approach achieves 1.9% (90.0% vs. 88.1%) and 4.3% (77.3% \u03c5\u03c2. 73.0%) improvement over CoAlign for AP@0.5/0.7. When the noise level reaches 0.8m/0.8\u00b0, our approach still achieves 1.4% (84.1% \u03c5\u03c2. 82.7%) and 1.6% (68.9% vs. 67.3%) improvement over CoAlign for AP@0.5/0.7."}, {"title": "5.4 Qualitative evaluation", "content": "Figure 5 and Figure 6 show the 3D detection results in the BEV format on the V2XSet. Red and green boxes denote the detection results and the ground-truth, respectively. The degree of overlapping of these boxes reflects the performance of the testing methods. Figure 5 depicts the detection results of V2X-ViT, V2VNet, CoAlign, and the proposed RoCo at an intersection to validate the effectiveness of our method. We set the noise level of 0.4m/0.4\u00b0 and 0.8m/0.8\u00b0 to produce high pose errors to make the perception"}, {"title": "5.5 Ablation Studies", "content": "Selection of the threshold value t2. The selection of the initial candidate set in graph matching is crucial. To determine the optimal threshold \\(\\tau_2\\) in Eq.(3) during the graph initialization, we conduct ablation experiments on different datasets, as shown in Table 2. In the DAIR-V2X dataset, we found that the detection results reach their optimum when \\(\\tau_2\\) is set to 3. As the value of \\(\\tau_2\\) increases, the detection performance will decline. This is because vehicles usually maintain a safe distance from other vehicles in real-world scenarios, and too small a threshold would affect the initial matching. Furthermore, we conducted the same experiments on the simulated dataset. We observed that when the noise level is below 0.8m/0.8\u00b0, the optimal threshold \\(\\tau_2\\) becomes 2 meters. As the noise level increases to 0.8m/0.8\u00b0, the optimal threshold becomes \\(\\tau_2\\) = 3. This is because in the V2XSet dataset, there are more vehicles and they are densely distributed. As a result, the distances between vehicles are smaller compared to real-world scenarios.\nContribution of Object Matching and Robust Graph Optimization. Table 4 assesses the effectiveness of the proposed module at various noise levels, including object matching and robust graph optimization. We evaluate the impact of each module on the final 3D detection by incrementally adding i) object matching and ii) robust graph optimization. We see that both modules can improve the performance. To validate the impact of object matching on pose error, Figure 7 plots the distribution of relative pose errors across all samples. The horizontal axes \"trans error\" and \"rot error\" denote the translational and rotational relative pose errors (RPE), respectively. The vertical axis represents the density value of the distribution. The orange dashed line represents the density when applying the proposed object matching method, while the blue line represents case without the object matching. The closer the distribution is to the Delta distribution centered at zero, the better the performance because this means most of the pose errors have smaller values. It can be observed from figure that using object matching leads to the desired distribution and significantly reduces relative pose errors.\nContribution of graph similarity. Now we investigate the contributions of different similarities defined in Eq.(5) and (6) to object matching on DARI-V2X, the result is shown in Table 3. We evaluate the impact of each similarity score on the final graph matching by incrementally adding i) edge similarity and ii) distance similarity. The first row in Table 3 represents the baseline model which does not use any matching methods. The experimental results indicate that both designs of similarity matching contribute to improving detection accuracy, especially at the noise level of 0.8m/0.8\u00b0."}, {"title": "6 Conclusion", "content": "This paper proposes a novel robust collaborative perception framework, RoCo, for 3D object detection. This framework addresses issues such as object mismatch and misalignment caused by pose errors among multiple agents. It enhances the precision and reliability of this modality and better prepare it for the potential multimodal fusion step in the sequel. The core idea of RoCo is based on graph-based object matching, which reliably associates common objects detected by different agents and reduces pose errors of agents and objects using iterative robust optimization. Additionally, RoCo does not require any ground truth pose supervision during training, making it highly practical. Comprehensive experiments demonstrate that RoCo achieves outstanding performance across all settings and exhibits superior robustness under extreme noise conditions. We believe that this work has the potential to contribute to the development of the multimodal field. In future work, we will apply our method to multimodal collaborative perception to further improve the safety and reliability of autonomous driving."}]}