{"title": "TOWARDS AUTOMATED SOLUTION RECIPE GENERATION FOR INDUSTRIAL ASSET MANAGEMENT WITH LLM", "authors": ["Nianjun Zhou", "Shuxin Lin", "Dhaval Patel", "Fearghal O'Donncha"], "abstract": "This study introduces a novel approach to Industrial Asset Management (IAM) by incorporating Conditional-Based Management (CBM) principles with the latest advancements in Large Language Models (LLMs). Our research introduces an automated model-building process, traditionally reliant on intensive collaboration between data scientists and domain experts. We present two primary innovations: a taxonomy-guided prompting generation that facilitates the automatic creation of AI solution recipes and a set of LLM pipelines designed to produce a solution recipe containing a set of artifacts composed of documents, sample data, and models for IAM. These pipelines, guided by standardized principles, enable the generation of initial solution templates for heterogeneous asset classes without direct human input, reducing reliance on extensive domain knowledge and enhancing automation. We evaluate our methodology by assessing asset health and sustainability across a spectrum of ten asset classes. Our findings illustrate the potential of LLMs and taxonomy-based LLM prompting pipelines in transforming asset management, offering a blueprint for subsequent research and development initiatives to be integrated into a rapid client solution.", "sections": [{"title": "1 Introduction", "content": "Industrial asset management has evolved from a rigid, schedule-based approach to comprehensive practices, exemplified by frameworks like ISO 550012, which requires organizations to establish policies, objectives, and processes for risk management, financial management, and continuous improvement. These rigid approaches struggle to adapt to the evolving demands of modern environments. The emergence of data-rich ecosystems, pulsating with interconnected sensors and the industrial Internet of Things (IoT), necessitates a paradigm shift towards conditional asset management (CBM), a cornerstone of Industry 4.0 Fernandes et al. [2021]."}, {"title": "2 Related Work", "content": "Our work is motivated by two streams of research: embedding domain-specific problem-solving logic into a tree-structured semantic hierarchy, and leveraging large language models (LLMs) as a guide in driving content generation for a particular aspect or topic."}, {"title": "2.1 Taxonomy-aware Prompting", "content": "This section reviews the work in the domain of taxonomy and prompts associated with taxonomy. Taxonomy is a tree-like structure that enables end-users to organize related concepts (or terms) via relationships, which facilitates information seeking, retrieval, or behavior for downstream applications.\nExisting work has largely focused on building taxonomy Shah et al. [2023], Xu et al. [2023], extending taxonomies Xu et al. [2022], Shen et al. [2020], Liu et al. [2021], and generating a taxonomy for prompt Oppenlaender [2023]. These approaches, by and large, use similar mechanisms to build a large language model using data of interest and demonstrate its benefits. In our case, we aim to leverage the given taxonomy to generate appropriate prompts for extracting knowledge from LLMs. In this context, Promptiverse Lee et al. [2022] is closely related to our work, as they employed a knowledge graph to traverse and generate prompts. However, they did not utilize LLMs, and the paper's focus was on learning the knowledge graph from available information, such as videos and notes. Additionally, Promptiverse did not cover the area of writing textual inputs or instructions for prompting generative pre-trained models. Our work aims to address this gap by leveraging the given taxonomy to generate appropriate prompts for extracting domain knowledge, and utilizing LLMs to guide the content generation process as well as the prompt generation process.\nIn the biomedical domain, a method called HIPrompt Lu et al. [2023] was proposed to organize domain-specific terms into a hierarchy for knowledge fusion. The objective was to use LLMs to fuse knowledge by inputting the terms in the hierarchy. However, instead of generating textual inputs to query LLMs, the authors used a ranker module to obtain the ranking of terms from the hierarchy. It's not clear why the authors chose to use a ranker module instead of generating prompts from the hierarchy. Additionally, LLMs can also be used to generate taxonomies for specific problems, such as user intent taxonomies. For example, Shah et al. Shah et al. [2023] proposed a method for generating a user intent taxonomy using LLMs. Their approach involves using LLMs to generate a set of candidate terms that represent different user intents and then using a ranker module to rank the terms based on their relevance to the target task. This approach can be helpful in situations where the hierarchy of terms is not well-defined or is too complex to be generated manually. Clearly, taxonomy is an interesting way to encode the problem, and its application using LLMs brings value to industrial applications."}, {"title": "2.2 Knowledge Extraction from LLM", "content": "This section reviews work centered around prompting techniques, multi-step reasoning, and factual verification of LLM output. LLM has been trained on the vast majority of data from Wikipedia, common-crawl books, etc. On top of this learning, the instruction version of LLM has been trained on additional demonstration to bring a logical reasoning capability on understanding user's request and responding appropriately using internal knowledge Cohen et al. [2023]. Such capabilities encourage communities and industries to use LLMs as Knowledge Gurus in various downstream tasks, such as the use of LLM as a web-crawler to discover factual information Cohen et al. [2023]."}, {"title": "3 Problem Formulation", "content": "The problem we are addressing in our research is how to create a comprehensive solution using LLM that automatically generates a recipe for a specific industrial asset based on one Key Performance Indicator (KPI)."}, {"title": "3.1 Key Terms: KPIs, Metrics and Sensors", "content": "Key Performance Indicators (KPIs) are the strategic measures closely linked to overarching business goals. Metrics are a broader range of operational measurements detailing various aspects of asset performance. Both are essential for evaluating and enhancing asset performance, implementing maintenance strategies, and guiding investment decisions. The KPIs/metrics, such as mean-time-between-failure (MTBF), maintenance costs, and asset availability, provide tangible measures of an asset's operational efficiency and performance. Meanwhile, The KPIs/metrics, like failure probability, component condition monitoring, and greenhouse gas equivalent emissions, offer predictive insights, enabling the assessment of asset risk, asset health, and their environmental impact. These indicators guide informed decisions about maintenance practices, resource distribution, and long-term asset strategy formulation. Table 1 lists eight KPIs commonly used in industrial asset management sectors.\nIoT sensors play a critical role to estimate asset KPIs. For instance, vibration sensors play a pivotal role in assessing a machine's Health Score, whereas temperature sensors are crucial for maintaining performance efficiency. Similarly, energy consumption meters are instrumental in calculating metrics like Greenhouse Gas Equivalent Emissions. Given the diverse nature of industrial assets, including rotary equipment and fixed assets, the types of sensors required for their monitoring differ significantly. It is in this context that Large Language Models (LLMs) become valuable."}, {"title": "3.2 KPI-centric Taxonomy", "content": "A KPI-centric taxonomy is a tree-like structure that encapsulates the domain knowledge of a specific KPI. The root node of the taxonomy tree is the KPI, while the child nodes are factors that influence the KPI."}, {"title": "3.3 Solution Recipe", "content": "To support the function of predicting/estimating a KPI for an industrial asset, we generate a solution entity named recipe. For an industrial asset class, a recipe is a collection of documentation and programming codes denoted by $R = {a_1,a_2,..., a_n}$. Each recipe R is designed to handle a specific KPI and metric. Currently, we break down a recipe into five artifacts, including:\n\u2022 Knowledge Document for KPI/metric Analysis: is a markdown file that provides a comprehensive introduction to the asset, including its definition and a detailed overview of the factors and sensors or another measurement such as inspection, sampling, and historical record and asset profile that could affect the KPI/metric estimation or forecasting.\n\u2022 KPI/metric Modeling Configuration(s): one or more YAML file(s) that specifies the detailed information regarding each sensor and historical records, including description, unit if required, and value range when the value is in a normal or abnormal state or situation, etc. and the assignment of the scale to the value.\n\u2022 Sample Dataset: Typically, it is a CSV (comma-separated value) dataset including identified sensor time series, variables from historical records, or asset profiles. They can be real-world industry data or synthetic data generated based on a simulated program (itself could be parts of the recipe).\n\u2022 Modeling Code: programming classes or functions that predict or estimate a KPI/metric from sensors' or other monitored data and historical or profile information. These models can be either data-driven or traditional engineering models.\n\u2022 Recipe Wrapper: is an artifact that encapsulates the code for integrating inputs from both configuration settings and the dataset, and subsequently invoking the model code to facilitate model generation."}, {"title": "4 Solution Outline", "content": "This work proposes a framework for AutoRecipe, an automated solution for creating a Solution Recipe for an industrial asset. Figure 3 illustrates the system architecture.\nThe first crucial task in AutoRecipe is to generate a series of meaningful prompts, hereinafter referred to as PromptSequence, using a KPI-centric taxonomy. The prompt constitutes a critical element in the architecture of a high-performing Large Language Model (LLM), yet the organization of prompts into sequences also demands careful consideration. This aspect has been discussed by research into methodologies like chain-of-thought, tree-of-thought, and graph-of-thought, underscoring the significance of structured prompt sequences in enhancing LLM performance Wu et al. [2023b], Yao et al. [2023], Besta et al. [2023]. In Section 4.1, we discuss the KPITaxo2Promt methodology, a two-step hierarchical planning-aware ADaPT approach to generate a variable-length PromptSequence employing LLM."}, {"title": "4.1 \u039a\u03a1\u0399\u03a4axo2Prompt Generation", "content": "In the proposed prompt KPITaxo2Prompt, we aim to utilize an instruction-tuned LLM to generate a prompt sequence that supports KPI-centric taxonomy use cases. Building upon the ADAPT approach Prasad et al. [2023], our proposed prompt includes an expanded version of the taxonomy introduction section, which lays the foundation for a context-aware taxonomy. Additionally, we have included a materialized taxonomy (specifically, asset health) to provide a concrete example. Unlike ADaPT, our approach does not rely on a fixed goal that can be evaluated each time. Instead, we have implemented a planner that comprehends the taxonomy and generates a comprehensive plan. Our goal is to calculate asset health using components, just one example of how our proposed prompt can be applied.\nThe remainder of the prompt consists of a sequence of Think-Step(s)-Think cycles, where we demonstrate how the LLM can generate prompt sequence Step 1 \u2192 Step 2 \u2192 Step 3 \u2192 \u2192 Step X \u2192 Step Y. In each cycle, the Think step serves as information collectors from the taxonomy, while the Step P steps involve the LLM generating question/code which will be part of PromptSequence.\nIn our approach, we adopted three principles to guide the generation of steps: (i) starting with generic information and gradually adding specific details (compare Step 1 vs Step Y); (ii) capturing forward-looking information in questions using parent-child relationships inferred from the taxonomy (see Step 1); and (iii) repeating questions to improve quality. By following these principles, we can generate an effective interrogation of the entire taxonomy. However, we noticed a weakness in the LLM's reasoning ability to generate conditional questions that require accounting for distant parent-child nodes of the taxonomy. To overcome this limitation, we leveraged the AI4Code approach, where questions that require conditional evaluation are generated as Python code first (see Step Y), which are later converted into questions in a second round of execution. Essentially, KPITaxo2Prompt is a two-step approach where step 1 generates a sequence of prompts, some of which may be code, and the execution of these prompts in the second round generates the final questions."}, {"title": "4.2 PromptSequence Execution Engine", "content": "Given the PromptSequence, which is a guided pipeline, the first task we execute is to generate the knowledge document. We defined five distinct approaches to generate outputs using PromptSequence:\n\u2022 Last Question (Lasto): Only the last question in PromptSequence is executed to capture the Zero-shot capability of LLM as a baseline.\n\u2022 All Questions Concatenated (Allq): We combined all questions from PromptSequence into one extended query, testing the effect of presenting the full question context in a single prompt.\n\u2022 All Questions with Chain of Thought (AllQCOT): This method enhances the Allo approach by incorporating a \"think step-by-step\" comment at the end of last questions in the prompt.\n\u2022 All Questions with ReAct (AllQREACT): This method enhances the Allq approach by incorporating a ReAct agent that enables LLM think-act-observe each questions in PromptSequence before answering the last question.\n\u2022 Guided Iterative Thought (GITQ): This approach simulates a dynamic Q/A session, where each question and its subsequent answer lead to the next query, mirroring a real-world interaction pattern. In this process, questions are already generated in PromptSequence and its answers help to understand how the final knowledge is generated.\nThese methods aim to optimize the extraction of domain-specific knowledge for a particular KPI, evaluating their effectiveness in contributing to a rich, accurate knowledge base. In the context of Asset Health KPIs for Wind Turbine, the expected content of the knowledge document discusses the wind turbine's components, specific sensors, and their corresponding health monitoring responsibilities. This document also establishes criteria for health status labeling across diverse sensor outputs, categorizing them into distinct classes, such as normal, warning, or critical. This initial stage lays the groundwork for a tailored monitoring framework, precisely tailored to the wind turbine's unique requirements."}, {"title": "4.3 Quality Assurance of Generated Recipe", "content": "As the knowledge document is the base for the generation of artifacts, we employ a continuous booster mechanism to improve the knowledge document. This process, iterated N times, is designed to enhance the quality of results and incrementally increase the content tied to the knowledge extraction outputs. We incorporate factual scores alongside references to external sources, thereby grounding our solutions in verifiable information. This method of including confidence from external resources, with references linked directly within our outputs, ensures an increased degree of accuracy and reliability."}, {"title": "4.3.1 Iterative Information Discovery Flow", "content": "In this subsection, we introduce a technique called Iterative Information Discovery Flow to enhance the information discovery process using LLM (See Figure 4). We leverage LLM's built-in capability to generate a confidence score (C) for the output. For each question (Q) we pose to LLM, we not only produce an answer (A) but also a confidence score ranging from 0 to 100 percent. The process iterates up to ten times, and after each round, we evaluate the quality of the generated text. The question and answer from the current round are appended to the System Prompt of the agent, and the original question Q is supplemented with the additional text: \"generate a better result.\" Unlike LLRetrival, we don't rely on external sources to calculate the score. If the confidence score of the generated text in the current round falls below a certain threshold, we terminate the process to avoid repetition. Note that, confidence score act as quality score and if quality score is poor the answer is dropped."}, {"title": "4.3.2 Reference Generator Pipeline", "content": "Our task is formalized as follows: Given a knowledge document (kd) and a corpus of extracted text passages from web D, the pipeline is required to return an output S, which consists of n passages, $P_1, P_2, \u2026, P_n$, and each passage $p_i$ cites a list of passage $D_i = {d_{i,1}, d_{i,2}, ..., d_{i,N}}$; where $d_{i,1} \u2208 D$. In this work, we segment LLMs output into passages by parts in knowledge documents. While LLMs output may include tables, numbers, etc, we noticed that all the passages have valuable information and need citation. In this work, we generate citations per part, not by sentences.\nNote that, we do not have a web corpus D available upfront. Thus, one objective is to construct it for a kd selectively. As we have divided the knowledge document (kd) into passages $p_i$, we use the instruction"}, {"title": "5 Experiment", "content": "To test our approach, we first created a taxonomy for two key performance indicators (KPIs): Asset Health and Sustainability. Then, we used Asset Health to prepare a KPITaxo2Prompt, as discussed in Section 3. To validate that KPITaxo2Prompt would work for the Sustainability KPI, we generated a prompt sequence without making any modifications. As expected, we found that just one internal thought was enough to transfer knowledge from one demonstration to another, confirming our initial hypothesis of \"write once and apply multiple times\" on different KPIs. Finally, we used the generated prompt sequences to create knowledge documents for 10 assets."}, {"title": "5.1 Knowledge Extraction Improvement", "content": "To demonstrate how the technique of Section 4.3.1 works, we conducted a small-scale experiment involving ten asset classes to obtain asset descriptions as knowledge. Figure 5 shows the answer length obtained in the last iteration for each asset class. We can see that the answer length varies across asset classes. We recorded how these answer lengths vary over different iterations in Figure 6. The incremental trend of the document length over the iterations shows the effectiveness of reinforcement in helping complete the knowledge extraction. We observed that in most cases, the second iteration significantly increased the content length compared to the remaining rounds."}, {"title": "5.2 Knowledge Extraction Factual Validation", "content": "We apply Reference Generator Pipeline (See Section 4.3.2) on two asset classes and KPIs (See Table 2). Since, the knowledge document is spitted into three parts (asset description, KPI specific explanation, and measurement approaches), we tracked the claims at part level. For example, (3,2,3) for Wind turbine's"}, {"title": "5.3 Pipeline Execution Engine", "content": "We also applied the all 5 pipelines to obtain the different depth of documents. We use popular TTR (text to token ration - which is the ratio of the number of unique types to the square root of the total number of tokens) to measure the obtain the first level of understanding for generated document. The high value of TTR is preferred. Table 10 shows the comparison among existing Prompt method."}, {"title": "5.4 Appendix Summary", "content": "Appendix A discuss various prompt used as a part of experimental work. We have used Myxtral7x13b as a LLM for all the experimental work. The Sustainability KPI taxonomy is listed in Appendix B. The entire system prompt to generate the prompt sequence from taxonomy is given in Appendix B including prompt sequence. The sample react prompt for pipeline ALLQREACT is given in Appendinx C. The generated knowledge doc is provided in Figure 13 and 14."}, {"title": "6 Conclusion", "content": "Our solution and experiment have systematically explored the capability of LLMs to generate detailed and relevant solution recipes (docs, sample data, and codes) across a spectrum of industrial asset classes through a structured approach using tailored prompts that could be even auto-created using KPI-centric taxonomy."}, {"title": "A Prompt Design", "content": "This section outlines the critical design considerations for selecting prompts essential for artifact generation. Our methodology involves precise fine-tuning of both the system and user prompts. The system prompt is tailored to define the role of the Large Language Model (LLM), while the user prompt focuses on task specification."}, {"title": "A.1 System Prompt for Different Roles", "content": "Our solution features a variety of roles, such as domain expert, data scientist or system integration architect, each carefully emulated by the LLM. In different situations, we formulate unique system prompts and user prompts to guide the LLM precisely. This specialized approach ensures the generation of diverse artifacts (shown in Figure 11), each tailored to meet the needs of the specific role and objectives of the session. For instance, consider the example of a system prompt designed for extracting knowledge pertinent to estimating the health score from an asset's component health conditions."}, {"title": "A.2 Prompts for Artifacts Generation", "content": "We present examples of several user prompts. The expected responses from LLM to these prompts constitute the artifacts that form the basis of our solution recipe. The example used mainly comes from the asset health score scenario. For the following prompts, the asset class and its description are replaced with a specific asset class at the time of execution."}, {"title": "A.2.1 Knowledge Document Generate", "content": "We have a series of Q&As to help generate the final knowledge document. Using the health score estimation, we have for the last question from the user."}, {"title": "A.2.2 Asset Configuration Generate", "content": "For health score estimates from the health component, we utilize two distinct approaches: the analytic hierarchical process (AHP) and a weighted approach. These methodologies will be integrated into the system configuration, which specific customers can tailor during deployment.\nThe AHP method de FSM Russo and Camanho [2015] is particularly advantageous due to its user-friendly configuration. It is designed to be more easily adjustable by customers or end-users, making it a preferable choice for its accessibility and adaptability in diverse application settings.\nThere are two configuration files generated. The first one defines the health indicator for each sensor. The second one defines the parameters required for aggregating the sensor or measurement values for the final health score."}, {"title": "A.2.3 Sample Dataset Generate", "content": "The generation of the sample dataset is a two-step process. Initially, a concise dataset is created based on the information within the knowledge document. Following this, the initial dataset serves as a template to produce simulated code, which can then be used to generate synthetic data of any desired size.\nThis is an example of the sample dataset generation for health score estimated using component quality."}, {"title": "A.2.4 Model Generate for KPI of a Given asset class", "content": "The model has to be aligned to the scikit learn. We have both the engineering and ML models. The guideline for generation model with user prompt."}, {"title": "A.3 Improvement of Knowledge Extraction with Iterative Prompting", "content": "The following system and user prompt technique applied to help to improve the quality of knowledge extraction. We simple iterative ask the LLM to repeatedly generate the same answer with the requirements of improvement itself quality. In following example, we only display one example (one-shot) due to the limitation of the paper, however, in the experiment, we choose two-shot approach in the system prompt. We also ask the output coming out the confidence level as part of outputs. However, we felt that this confidence level might not be truly reflects the confidence of the context generated rather than a hallucination."}, {"title": "B KPI-centric Taxonomy to Prompt Auto-Generation", "content": "In this section, we are showing examples for how KPITaxo2Prompt auto-generates prompt/step sequences with various pre-defined KPI taxonomy using LLM. The prompt consists of System Prompt and User Prompt. In System Prompt shown in Table 4, we are providing context, the taxonomy descriptions, and one goal along with its Think-Step-ExecOrder answer. The goal is considered as one-shot example.\nNext, in User Prompt 1, we demonstrate how LLM responds to a new goal where its KPI, asset health, is identical, but a different method is requested for analysis. In User Prompt 2, we demonstrate how LLM responds to a new goal where its KPI, asset sustainability in this case, differs from the one-shot example. Note that the user prompt is independent of asset types. We replace asset class and description with specific strings in the downstream tasks."}]}