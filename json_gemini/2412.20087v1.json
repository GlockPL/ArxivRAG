{"title": "On the Validity of Traditional Vulnerability Scoring Systems for Adversarial Attacks against LLMs", "authors": ["Atmane Ayoub MANSOUR BAHAR", "Ahmad Samer WAZAN"], "abstract": "Purpose - This research investigates the effectiveness of established vulnerability metrics, such as the Common Vulnerability Scoring System (CVSS), in evaluating attacks on Large Language Models (LLMs), with a focus on Adversarial Attacks (AAs). The study explores the influence of both general and specific metric factors in determining vulnerability scores, providing new perspectives on potential enhancements to these metrics.\nApproach - This study adopts a quantitative approach, calculating and comparing the coefficient of variation of vulnerability scores across 56 adversarial attacks on LLMs. The attacks, sourced from various research papers, and obtained through online databases, were evaluated using multiple vulnerability metrics. Scores were determined by averaging the values assessed by three distinct LLMs.\nFindings - The results indicate that existing scoring-systems yield vulnerability scores with minimal variation across different attacks, suggesting that many of the metric factors are inadequate for assessing adversarial attacks on LLMs. This is particularly true for context-specific factors or those with predefined value sets, such as those in CVSS. These findings support the hypothesis that current vulnerability metrics, especially those with rigid values, are limited in evaluating AAs on LLMs, highlighting the need for the development of more flexible, generalized metrics tailored to such attacks.\nValue - This research offers a fresh analysis of the effectiveness and applicability of established vulnerability metrics, particularly in the context of adversarial attacks on Large Language Models, both of which have gained significant attention in recent years. Through extensive testing and calculations, the study underscores the limitations of these metrics and opens up new avenues for improving and refining vulnerability assessment frameworks specifically tailored for LLMs.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have recently become a cornerstone in artificial intelligence (AI) research and application, thanks to their remarkable ability to understand and generate human-like text (Brown et al., 2020). LLMs such as GPT (Radford, 2018), BERT (Devlin, 2018), and others have achieved widespread adoption in a variety of fields, including Natural Language Processing (NLP), machine translation, and conversational AI, due to their capacity to generalize across diverse tasks (Vaswani, 2017). However, this surge in popularity has also exposed LLMs to a myriad of vulnerabilities, becoming an attractive target for various security threats (Goodfellow et al., 2014; Abdali et al., 2024; Liu and Hu, 2024).\nOne of the most significant threats to LLMs is Adversarial Attacks (AAs) (Shayegani et al., 2023; Peng et al., 2024; Zhao et al., 2024), which are typically designed to fool Machine Learning (ML) models by modifying input data or introducing carefully-crafted inputs that cause the model to behave inappropriately (Szegedy, 2013; Goodfellow et al., 2014). These attacks often remain indistinguishable to humans but significantly impact the model's decision-making process, posing a significant threat to LLMs, as they can compromise the integrity, reliability, and security of applications that rely on these models (Carlini and Wagner, 2017b). One significant example is the Crescendo attack (Russinovich et al., 2024). This sophisticated method manipulates LLMs by gradually escalating a conversation with benign prompts that evolve into more harmful requests, effectively bypassing safety mechanisms. Therefore, protecting LLMs has become a critical concern for researchers and practitioners alike (Zou et al., 2024; Kumar et al., 2023a).\nTo effectively secure LLMs against AAs, it is crucial to assess and rank these threats based on their severity and potential impact on the model. For instance, some attacks, like Prompt Injection (Liu et al., 2024b), are easy to execute and widely applicable, making them higher-priority threats. Others, like Backdoor attacks (Li et al., 2021), may require greater sophistication but can cause significant long-term damage (Greshake et al., 2023a). This prioritization allows security teams to focus on the most dangerous attacks first for mitigation efforts. Existing vulnerability metrics, such as the Common Vulnerability Scoring System (CVSS) (Schiffman and Cisco, 2005) and OWASP Risk Rating (Williams, 2023), are commonly used to evaluate the danger level of attacks on traditional systems, taking into account factors such as attack vector, attack complexity, and impact. However, their applicability to LLMs remains questionable.\nMost existing vulnerability metrics are tailored for assessing technical vulnerabilities in software or network systems. In contrast, AAs on LLMs often target the model's decision-making capabilities and may not result in traditional technical-impacts, such as data breaches or service outages (Zhang et al., 2020). For example, attacks like Jailbreaks (Chu et al., 2024), which manipulate the model's outputs to bypass ethical or safety constraints, cannot easily be classified as technical vulnerabilities. These attacks focus on manipulating the model's behavior rather than exploiting system-level weaknesses. In other terms, the context-specific factors used in existing metrics, such as CVSS, do not adequately account for the unique characteristics of LLMs or the nature of AAs. Consequently, they may be ill-suited for assessing the risk posed by these attacks on LLMs.\nIn this study, we aim to evaluate the suitability of known vulnerability metrics in assessing Adversarial Attacks against LLMs. We hypothesize that: 'the factors used by traditional metrics may not be fully applicable to attacks on LLMs', because many of these factors are not designed to capture the nuances of AAs.\nTo test this hypothesis, we evaluated 56 different AAs across four widely used vulnerability metrics. Each attack was assessed using three distinct LLMs, and the scores were averaged to provide a final assessment. This multi-faceted approach aims to provide a nuanced understanding of how well current metrics can distinguish between varying levels of threat posed by different adversarial strategies, as relying solely on human judgment for security assessments"}, {"title": "2. Methods", "content": "In this section, we outline the methodology adopted to evaluate vulnerabilities in attacks tar-geting Large Language Models using established metrics such as DREAD (Michael and Steve, 2006), CVSS (Schiffman and Cisco, 2005), OWASP Risk Rating (Williams, 2023), and Stakeholder-Specific Vulnerability Categorization (SSVC) (Spring et al., 2021).\nOur approach involves three key steps depicted below in Figure 1: data collection, assessment, and statistical interpretation."}, {"title": "2.1. Data collection", "content": "The first step in our methodology was to gather a comprehensive dataset of AAs targeting LLMs. To ensure a thorough and systematic approach, we began by reviewing the literature on these attacks, exploring existing types and classifications. This step provided a broad under-standing of the main categories of attacks commonly observed in the context of ML and NLP systems.\nFollowing this foundational review, we focused on identifying recent AAs specifically tar-geting LLMs. These attacks were grouped into seven primary types: Jailbreaks (White-box and Black-box) (Xu et al., 2024b), Prompt Injections (Liu et al., 2023a), Evasion attacks (Wang et al., 2023), Model-Inference (Membership Inference) attacks (Hu et al., 2022), Model-Extraction at-tacks (Gen\u00e7 et al., 2023), and Poisoning/Trojan/Backdoor attacks (Tian et al., 2022; Li et al., 2021; Liu et al., 2020b). For each type, we selected eight representative attacks, prioritizing those published in recent research or demonstrated in practical scenarios. This effort resulted in a list of 56 attacks, covering a diverse range of threat vectors and methodologies.\nTo enable a systematic ranking of these attacks based on their potential danger, we decided to assess each attack using vulnerability metrics. By applying multiple metrics, we aimed to provide a multi-faceted evaluation of each attack's severity and to ensure that the dataset would serve as a robust basis for further analysis and interpretation."}, {"title": "2.2. Score assessments", "content": "To evaluate the severity and danger level of the 56 gathered attacks, we began by identifying widely recognized vulnerability assessment metrics to ensure a comprehensive analysis. After careful consideration, we selected four metrics: DREAD (Michael and Steve, 2006), CVSS (Schiffman and Cisco, 2005), OWASP Risk Rating (Williams, 2023), and SSVC (Spring et al., 2021). These metrics were chosen for their broad adoption and their focus on different factors, enabling a more nuanced understanding of the vulnerabilities. Since the Adversarial Attacks we collected are recent and not yet assessed in the literature, calculating their scores became essential to address this gap.\nManually assessing 56 attacks across four metrics is a daunting task, requiring extensive ex-pertise from security analysts, system administrators, and other domain experts. The process involves interpreting complex scenarios, considering varying factors for each metric, and ensur-ing consistency between all evaluations. Completing such an effort manually could take months or even years, which is impractical given the fast-evolving nature of adversarial threats.\nTo overcome this challenge and accelerate the process, we leveraged the capabilities of LLMS to perform semi-automated scoring. Specifically, we utilized three state-of-the-art models: GPT-40 (OpenAI, 2024), LLAMA3.2-90b (Dubey et al., 2024), and Perplexity AI (Inc., 2022). Each model operated independently, assessing the attacks and vulnerabilities according to the factors defined by the selected metrics. For each scoring factor, we calculated the average score provided by the three LLMs, rounded to the closest unit.\nThis approach offers several advantages. First, it enables rapid assessments. Second, using multiple LLMs increases the robustness of the results by minimizing biases or errors from any single model. Furthermore, the models' advanced text-processing capabilities allow them to analyze the contextual details of each attack and provide scores that align with the logic of the vulnerability metrics.\nA recent work of Chopra et al. (2024) proves that LLMs are able to identify and analyze software vulnerabilities; but that they can lead to misinterpretations or oversights in understand-ing complex vulnerabilities. To address such potential inconsistencies in the assessments, we incorporated a Human-in-the-Loop (HitL) verification process. We reviewed the logic and rea-soning behind each LLM-provided score to ensure its accuracy and reliability. This step was essential to mitigate any errors or misinterpretations that might arise from the LLMs, especially when handling complex scenarios.\nTo validate this methodology, we tested it on a set of Common Vulnerabilities and Exposures (CVEs) that already have human validated scores with both CVSS and SSVC (Spring et al., 2021) to measure the gap, as shown in Table 1. The details of each factor are further explained in Section 5.2."}, {"title": "2.3. Results interpretations", "content": "Our approach provided a multi-dimensional analysis of Adversarial Attacks against LLMs by leveraging four distinct vulnerability assessment metrics: DREAD, CVSS, OWASP Risk Rating, and SSVC. This comprehensive evaluation allowed us to gain a broad perspective on how these metrics reflect the severity and impact of attacks, as well as their usefulness in ranking and understanding vulnerabilities in the LLM context.\nTo assess the utility and added value of each factor within the metrics, we analyzed their variability across the 56 attacks, grouped by attack type. For the quantitative metrics (DREAD and OWASP Risk Rating), we calculated the coefficient of variation (CV) for each factor to measure the relative dispersion of scores. For the qualitative metrics (CVSS and SSVC), we used entropy (Shannon, 1948) to quantify the diversity or uniformity of categorical values."}, {"title": "3. Adversarial Attacks", "content": "The rise of AAs in the field of Machine Learning has posed significant security challenges, especially for Large Language Models. These attacks exploit the vulnerabilities inherent in AI models by manipulating inputs to achieve unintended or harmful outputs. This section provides a detailed exploration of AAs, beginning with their formal definition and an analysis of why they are considered particularly dangerous to LLMs. Then it introduces various types and classifi-cations of AAs, offering insight into the range of attack strategies used to compromise LLMs. Understanding these elements is crucial for designing more robust defenses and enhancing the security of AI-driven systems."}, {"title": "3.1. Definition", "content": "Adversarial Attacks are intentional manipulations of input data designed to exploit vulner-abilities in ML models (Finlayson et al., 2019). The concept of adversarial examples was first introduced in the domain of Image Recognition by Szegedy (2013), and it has since been widely explored across different ML tasks, including NLP (Qiu et al., 2022; Dong et al., 2022; Zhang et al., 2020). In the context of LLMs, adversarial inputs are carefully crafted to cause the model to produce incorrect, biased, or harmful outputs (Kumar, 2024). Unlike traditional errors, AAs are not random; but are strategically designed to exploit the decision boundaries of models by altering inputs in ways imperceptible to humans and effective against ML models (Carlini and Wagner, 2017a). These attacks can involve minimal changes, such as swapping words, insert-ing seemingly harmless phrases, or restructuring sentences, that lead to dramatically different responses from the model, often having severe real-world consequences (Ibitoye et al., 2019; Kumar et al., 2023b), particularly in safety-critical applications such as autonomous driving, healthcare diagnostics, and security systems (Papernot et al., 2016). For example, an Adversarial Attack could lead an autonomous vehicle to misinterpret road signs, resulting in catastrophic accidents (Eykholt et al., 2018; Zhou et al., 2024).\nOn top of that, AAs can come in various forms, each exploiting different aspects of LLMs. These attacks can be broadly categorized based on the attacker's knowledge, the nature of the perturbations, and the model's vulnerability. The following section will explore the different"}, {"title": "3.2. Classifications of AAs", "content": "Adversarial Attacks have been classified in various ways in the literature, offering different perspectives on how AAs operate and their potential impact on Machine Learning models. In this section, we have gathered the most common classifications of AAs, based on criterias such as their purpose, target, the attacker's knowledge and strategy, life-cycle stages, CIA\u00b9 triad, and the type of data and control involved. We depict these classifications in Figure 2, and each one will be discussed in detail in the following subsections."}, {"title": "3.2.1. Based on the Purpose", "content": "One of the most widely used ways to classify AAs is by analyzing their intended purpose (de Morais, 2023; Boesch, 2023; Wintel, 2020). Attacks can be designed either to evade detec-tion by a model or to cause intentional misclassifications, thereby compromising the system's integrity or exploiting its weaknesses. Based on these overarching objectives, several distinct types of AAs have emerged, including: Evasion attacks (Wang et al., 2023; Li et al., 2021; Liu et al., 2020b), Jailbreak attacks (Xu et al., 2024b), Prompt Injections (Liu et al., 2023a), Model Inference attacks (Hu et al., 2022), Model Extraction (Stealing) attacks (Gen\u00e7 et al., 2023), and Poisoning/Trojan/Backdoor attacks (Tian et al., 2022; Liu et al., 2020b; Li et al., 2021). Each type targets different aspects of an ML system, posing unique challenges to the robustness and security of the models.\nEvasion attacks:. In evasion attacks, adversaries craft inputs that evade detection or mislead the model into making incorrect classifications. For instance, small changes in an image may lead a computer vision model to misclassify it, while adversarial inputs in LLMs can bypass content filters (Chen et al., 2019; Ayub et al., 2020; Badr et al., 2023)."}, {"title": "3.2.2. Based on the Target", "content": "Adversarial attacks can also be classified based on their target, which refers to whether the attack is aimed at causing a specific or arbitrary misclassification (de Morais, 2023).\nTargeted:. In targeted attacks, the attacker aims to manipulate the model into misclassifying an input into a specific, incorrect class (Carlini and Wagner, 2018a). For example, an attacker might craft an input to make a stop sign consistently classified as a yield sign.\nNon-targeted:. In non-targeted attacks, the goal is to cause the model to misclassify the input, but the specific incorrect class is irrelevant to the attacker (Wu et al., 2019). For instance, an adversarial input could cause a stop sign to be classified as any incorrect traffic sign."}, {"title": "3.2.3. Based on the Attacker's Knowledge", "content": "Another existing classification is categorizing AAs by the amount of knowledge the attacker has about the target model (Oprea and Vassilev, 2023; Wintel, 2020). These categories typically include white-box, black-box, and sometimes grey-box attacks, although grey-box is not always explicitly classified."}, {"title": "3.2.4. Based on the Life-Cycle", "content": "Adversarial attacks can be categorised also by when they occur in the machine learning pipeline, with some references focusing on the training and deployment phases only (Oprea and Vassilev, 2023), and others adding phases such as pre-training, post-training, and inference phase (Wu et al., 2023).\nPre-training:. Pre-training attacks are conducted before the model training begins, often during the data collection phase. For example, poisoned-data injection into a dataset to compromise the model's integrity once training commences (Li et al., 2020b; Liu et al., 2022).\nTraining:. Training-phase attacks occur during the actual model training process. A notable example is backdoor injection, where adversaries embed specific triggers in the training data to manipulate the model's behavior later (Xie et al., 2019; Du et al., 2022).\nPost-training:. Post-training attacks take place immediately after the training process concludes, before the model is deployed. These attacks might involve modifying the model parameters in a way that alters its predictions without detection (Qi et al., 2021; Zhao et al., 2019).\nDeployment:. Deployment-phase attacks are executed after the model has been deployed on a hardware device, such as a server or mobile device. An example includes modifying model pa-rameters in memory through techniques like bit-flipping, which can lead to unexpected behaviors (Chen et al., 2021; Bai et al., 2021).\nInference:. Inference attacks are performed by querying the model with test samples. A specific instance is backdoor activation, where an adversary triggers the model's malicious behaviors by providing inputs that match the previously embedded backdoor conditions (Dong et al., 2018; Kurakin et al., 2017)."}, {"title": "3.2.5. Based on the CIA Violation", "content": "A fifth classification of AAs is made according to the targeted aspect of the CIA triad, which encompasses confidentiality, integrity, and availability violations (Sadeghi et al., 2020; Oprea and Vassilev, 2023)."}, {"title": "3.2.6. Based on the Type of Control", "content": "Adversarial attacks are classified in other sources based on the type of control the attacker exerts over various elements of the ML model, with some highlighting the control of training and testing data (Sadeghi et al., 2020), and others (Oprea and Vassilev, 2023) proposing more aspects of control, such as the control of the model, source code, and queries, as well as a limited control on the data labels.\nTraining data:. In training data attacks, the attacker manipulates the training dataset by inserting or modifying samples. An example is data poisoning attacks, where malicious inputs are added to influence the model's learning process (Wang and Chaudhuri, 2018).\nTesting data:. Testing data attacks involve altering the input samples during the model's de-ployment phase. Backdoor poisoning attacks serve as an example, where specific triggers are embedded in the testing data to manipulate the model's predictions under certain conditions (Saha et al., 2020).\nModel:. Model attacks occur when the attacker gains control over the model's parameters, often by altering the updates applied during training. This can happen in Federated Learning (FL) envi-ronments, where malicious model updates are sent to compromise the integrity of the aggregated model (Tolpegin et al., 2020).\nSource code:. Source code attacks involve modifying the underlying code of the model, which can include changes to third-party libraries, especially those that are open source. This allows attackers to introduce vulnerabilities directly into the model's functionality (Zhang et al., 2024b).\nQueries:. Query-based attacks allow the attacker to gather information about the model by sub-mitting various inputs and analyzing the outputs. Black-box evasion attacks exemplify this, as adversaries attempt to craft inputs that evade detection while learning about the model's behavior through its responses (Fan et al., 2021).\nLabel limit:. In label limit attacks, the attacker does not have control over the labels associated with the training data. An example is clean-label poisoning attacks, where the adversary influ-ences the model without altering the labels themselves, making detection more difficult (Shafahi et al., 2018)."}, {"title": "3.2.7. Based on the Type of Data", "content": "An seventh classification of Adversarial attacks is based on the type of data they target, high-lighting the diverse methodologies employed across different modalities. Some underline attacks targeting data types as images, text, tabulars, cybersecurity, and even multimodal (Oprea and Vas-silev, 2023), while other works mention attacks on audio data (Carlini and Wagner, 2018b), and graph-based data (Dai et al., 2018).\nImage:. In image-based attacks, the attacker crafts adversarial images designed to cause mis-classification. An example includes perturbing images to deceive object detectors or image clas-sifiers, leading to incorrect identification (Subramanya et al., 2019).\nText:. Text attacks involve modifying text inputs to mislead NLP models. For instance, an adversary might introduce typos or antonyms to trick sentiment analysis tools or text classifiers into generating false outputs (Garg and Ramakrishnan, 2020).\nTabular:. Tabular data attacks target models that operate on structured data, often seen in ap-plications like finance or healthcare. A common example is poisoning attacks, where malicious entries are inserted into tabular datasets to manipulate model behavior (Cartella et al., 2021).\nAudio:. Audio-based attacks involve crafting adversarial noise or altering audio inputs to cause misclassification in systems like voice recognition. For example, specific sound patterns can be designed to mislead voice-activated systems, resulting in incorrect command interpretations (Li et al., 2020c).\nGraphs:. Graph-based attacks manipulate graph structures and attributes to deceive Graph Neu-ral Networks (GNNs). An attacker might alter edges or node features to induce misclassification or misleading outputs from graph-based models (Mu et al., 2021).\nCybersecurity:. In the cybersecurity domain, AAs target systems like malware detection or in-trusion detection systems. An example is poisoning a spam email classifier, where attackers introduce deceptive emails to degrade the model's performance (Wang et al., 2021).\nMultimodal:. Multimodal attacks involve exploiting systems that integrate multiple data types. In these cases, attackers might gain insights by submitting queries that encompass different modalities, such as text and image combinations (Wu et al., 2024)."}, {"title": "3.2.8. Based on the Strategy", "content": "Last but not least, Adversarial attacks can also be categorized based on the strategy employed by the attacker, distinguishing between passive and active approaches (Sadeghi et al., 2020).\nPassive:. In passive attacks, the attacker seeks to gather information about the application or its users without actively interfering with the system's operation. An example is reverse engineering, where an adversary analyzes a black-box classifier to extract its functionalities and gain insights into its behavior (Chiang et al., 1994).\nActive:. Active attacks are designed to disrupt the normal functioning of an application. The attacker may implement poisoning attacks that introduce malicious inputs, aiming to trigger misclassifications or degrade the model's performance during operation (Huang et al., 2020)."}, {"title": "4. Adversarial Attacks on LLMs", "content": "In recent years, LLMs have been increasingly targeted by AAs (Shayegani et al., 2023; Kumar, 2024; Yao et al., 2024), posing various threats to their reliability, safety, and security. These attacks can take multiple forms and serve distinct purposes, each exploiting different vulnerabil-ities within the model or its deployment. In this section, we present a comprehensive taxonomy of 56 recent AAs targeting LLMs, following the purpose-based classification of AA (refer to Section 3). We consider 7 types of AAs: White-box Jailbreak attacks, Black-box Jailbreak attack, Prompt Injection, Evasion Attacks, Model Extraction, Model Inference, and Poison-ing/Trojan/Backdoor. Each attack type includes 8 prominent examples, which are detailed in the following subsections."}, {"title": "4.1. Jailbreak Attacks", "content": "The type of AAs that we begin with are model Jailbreaking attacks, which are designed to bypass safety measures. We consider two approaches in jailbreak attacks according to the targeted model: White-box, and Black-box model jailbreaking."}, {"title": "4.1.1. White-box attacks", "content": "The first type are White-box Jailbreak attacks, where the attacker has complete access to the model's architecture, parameters, and training data. This level of knowledge allows the attacker to design specific inputs that exploit vulnerabilities in the model, often related to the model gradients, in order to bypass its restrictions or safety measures."}, {"title": "4.1.2. Black-box attacks", "content": "The second type of attacks are Black-box Jailbreak attack, in which, in contrast to white-box attacks, the attacker has no access to the model's internal workings or training data. Instead, the attacker can only interact with the model by providing inputs and observing the outputs, often relying on trial and error to discover effective prompts able to bypass the model's safeguards. We present in Table 2 a list of recent white-box and black-box jailbreak attacks existing in the literature (Sitawarin, 2023), and if they are Open Source (OS) or not, as each has a different strategy and implementation."}, {"title": "4.2. Prompt Injection", "content": "The third type of attacks that we illustrate are Prompt injections, where the adversary manip-ulates the input prompts and queries to deceive the model into producing unintended or harmful outputs. This technique is ranked among the most dangerous attacks against LLMs by OWASP (2023). To illustrate the diverse strategies attackers employ to exploit LLMs with PIs, we have gathered eight different attacks, utilizing both direct injections, where the attacker append a ma-licious input to a prompt, and indirect injection methods, where the attacker append malicious prompts through file or external inputs. These attacks are presented in Table 3"}, {"title": "4.3. Evasion Attacks", "content": "The forth type of attacks we illustrate are Evasion attacks, in which attackers aim to deceive language models by crafting inputs designed to bypass detection or classification. These attacks often target sentiment analysis and text classification models, seeking to manipulate their outputs"}, {"title": "4.4. Model Extraction", "content": "Model extraction attacks are the fifth type we illustrate in this section. These attacks aim to recreate or steal a language model's functionality by querying it and using the responses to reconstruct the model, this poses a significant threat as they allow adversaries to duplicate propri-etary models without access to their internal details. We present below in Table 5, eight examples of Model Extraction attacks, showcasing different methods adversaries use to probe black-box LLMs and either extract training data of the model, or precise personal information of users."}, {"title": "4.5. Model Inference", "content": "Model inference (or Membership Inference) are the sixth type of attacks we focus on in this study. These attacks determine whether specific data samples, especially sensitive information, were part of the training set of an LLM. These attacks can compromise the privacy of users or organizations by revealing training data patterns. We gathered in Table 6 eight examples of model inference attacks, which demonstrate how attackers exploit LLMs to infer confidential training data and gain insights into the model's behavior."}, {"title": "5. Classification of Adversarial attacks on LLMs based on their danger level", "content": "After presenting the existing classifications of AAs and some of the most-recent attacks against LLMs, we propose in this section a new criterion for classifying AAs on LLMs. We present the idea and methodology in the following subsections."}, {"title": "5.1. Principle", "content": "Seeing the list of AAs on LLMs presented in Section 4 and how frequent they are, one question that comes across the mind is what attacks should be mitigated first to secure LLMs? In order to answer this question, we need to rank the available attacks based on their danger level against LLMs in order to know what attacks is a model most-vulnerable to. This can be done by calculating the vulnerability score those of attacks using Vulnerability-assessment metrics (Shah and Mehtre, 2015)."}, {"title": "5.2. Vulnerability-assessment Metrics", "content": "Vulnerability assessment metrics are critical tools for evaluating and ranking potential secu-rity threats based on their severity and likelihood of exploitation. Various methodologies, such as DREAD (Michael and Steve, 2006), CVSS (Schiffman and Cisco, 2005), OWASP Risk Rating (Williams, 2023), and SSVC (Spring et al., 2021), provide frameworks for assessing vulner-abilities by considering different factors, including technical attributes, potential impacts, and"}, {"title": "5.2.1. DREAD (Michael and Steve, 2006)", "content": "Originally developed by Microsoft, DREAD is a qualitative risk assessment model that ranks, prioritizes, and evaluates the severity of vulnerabilities and potential threats based on five factors: Damage potential (D), Reproducibility (R), Exploitability (E), Affected users (A), and Discover-ability (D) of the attack.\nCalculations. The vulnerability score is calculated with DREAD as an average score of the five factors, each assessed with a value out of 10. The details of each factor and their values are shown in Table 8 below."}, {"title": "5.2.2. CVSS (Common Vulnerability Scoring System) (Schiffman and Cisco, 2005)", "content": "Created by the FIRST2 (Forum of Incident Response and Security Teams), the CVSS is an industry-standard scoring system for rating the severity of software vulnerabilities out of 10. It is encompasses three main metrics:\nBase Metrics: Represent the vulnerabilities that are constant over time. It contains factor related to the exploitability of an attack (how easy it is to exploit the vulnerability) like the Attack Vector (AV), Attack Complexity (AC), Privileges Required (PR), User Interaction (UI), and the Scope (S) of the attack. And factors related to the impact of an attack on the CIA triad, such as Confidentiality Impact (C), Integrity Impact (I), and Availability Impact (A).\nTemporal Metrics (Optional): Represent the vulnerabilities that might change over time in order to update the base score, it encompasses three factors, Exploit Code Maturity (E), Remediation Level (RL), and Report Confidence (RC).\nhttps://www.first.org/"}, {"title": "5.2.3. OWASP Risk Rating (Williams, 2023)", "content": "Developed by the Open Web Application Security Project (OWASP)4, it is a risk assessment methodology that evaluates vulnerabilities and categorizes security risks in web applications by assessing likelihood (based on threat agent and vulnerability characteristics) and impact (consid-ering technical and business factors) to produce an overall risk score.\nLikelihood: Calculates the probability of the attack to be exploited based on two compo-nents:\nThreat Agent (TA): Quantifies the skill level, motivation, opportunity, and size of the threat-agent population\nVulnerability (V): Quantifies the ease of discovery, ease of exploit, awareness, aware-ness of the system administrators, and the intrusion detection level.\nhttps://owasp.org/"}, {"title": "5.2.4. SSVC (Stakeholder-Specific Vulnerability Categorization) (Spring et al., 2021)", "content": "The SSVC is a framework that prioritizes vulnerabilities based on qualitative decision trees tailored to specific stakeholder roles, instead of numerical severity scores. The main two stake-holders represented are:\nSuppliers: They decide how urgent it is to develop and release patches for their systems based on reports about potential vulnerabilities. Their decision tree is based on factors such as Exploitation, Technical Impact, Utility, and Safety Impact.\nDeployers: They decide when and how to deploy the patches developed by the suppliers. Their decision tree is based on similar factors such as Exploitation, System Exposure, Automation, and Human Impact.\nCalculations. In our case, we consider LLMselves as Suppliers trying to assess the potential vulnerabilities impacting their LLM. Table 13 below show the different factors used in evaluating vulnerabilities using SSVC as a supplier."}, {"title": "6. Assessment of AAs on LLMs with Vulnerability Metrics", "content": "In this section", "types": "White-box Jailbreak", "logic": "nFor CVSS Factors:\nIf they have four values (eg. AV), they are represented with values from 1 to 4.\nIf have three values (eg. PR, C, I, A), they are represented with values from 1 to 3.\nIf they have two values (eg. AC, UI, S), they are represented with the values 2 and 4.\nFor"}]}