{"title": "Reinforcement Learning Enhanced LLMs: A Survey", "authors": ["Shuhe Wang", "Shengyu Zhang", "Jie Zhang", "Runyi Hu", "Xiaoya Li", "Tianwei Zhang", "Jiwei Li", "Fei Wu", "Guoyin Wang", "Eduard Hovy"], "abstract": "This paper surveys research in the rapidly growing field of enhancing large language models (LLMs) with reinforcement learning (RL), a technique that enables LLMs to improve their performance by receiving feedback in the form of rewards based on the quality of their outputs, allowing them to generate more accurate, coherent, and contextually appropriate responses. In this work, we make a systematic review of the most up-to-date state of knowledge on RL-enhanced LLMs, attempting to consolidate and analyze the rapidly growing research in this field, helping researchers understand the current challenges and advancements. Specifically, we (1) detail the basics of RL; (2) introduce popular RL-enhanced LLMs; (3) review researches on two widely-used reward model-based RL techniques: Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF); and (4) explore Direct Preference Optimization (DPO), a set of methods that bypass the reward model to directly use human preference data for aligning LLM outputs with human expectations. We will also point out current challenges and deficiencies of existing methods and suggest some avenues for further improvements.", "sections": [{"title": "Introduction", "content": "Large language models (Jiang et al., 2023; OpenAI, 2023; Dubey et al., 2024) are sophisticated language models pre-trained on extensive text data, allowing them to produce coherent and fluent responses to diverse inputs. However, the interaction capabilities of these pre-trained LLMs can be inconsistent, sometimes leading to responses that, while technically correct, may be harmful, biased, misleading, or irrelevant to users' needs. Therefore, it is crucial to align the outputs of pre-trained LLMs with human preferences before they can be effectively applied to various natural language tasks (Wang et al., 2023b; Wan et al., 2023; Sun et al., 2023c,b; Giray, 2023; Zhang, 2023; Long, 2023; Sun, 2023; Gao et al., 2023; Paranjape et al., 2023; Sun et al., 2023a; Diao et al., 2023; Wang et al., 2023a; Zhang et al., 2023b; Sun et al., 2023d; Liu et al., 2024d; Yao et al., 2024; Liu et al., 2024c; Lee et al., 2024; Kambhampati, 2024; Wang et al., 2024c).\nPreviously, a widely adopted approach for aligning the outputs of pre-trained LLMs with human preferences has been supervised fine-tuning (SFT) (Hu et al., 2021; Mishra et al., 2021; Wang et al., 2022; Du et al., 2022; Dettmers et al., 2023; Taori et al., 2023; Zhang et al., 2023a; Chiang et al., 2023; Xu et al., 2023; Peng et al., 2023; Mukherjee et al., 2023; Li et al., 2023; Ding et al., 2023; Luo et al., 2023; Wang et al., 2024d; Zhou et al., 2024). This method further trains LLMs on (Instruction, Answer) pairs, where \"Instruction\" represents the human prompt given to the model, and \"Answer\" is the target output that follows the instruction. SFT helps guide LLMs to produce responses that adhere to specific characteristics or domain knowledge, making it possible for humans to interactive with LLMs. Despite its effectiveness, SFT has limitations: during training, the model is constrained to learn specific answers we provide, with metrics like perplexity (PPL) penalizing synonym use. On one hand, this can hinder the LLM's ability to generalize, as tasks like writing and summarization have multiple valid phrasings. On the other hand, it may cause poor performance in aligning with human preferences, as no direct human feedback is incorporated into the training process.\nTo alleviate the above issues, reinforcement learning (RL) is adopted in aligning the outputs of LLMs with human preferences, which can be decomposed into three steps: (1) First, before fine-tuning, a reward model (or reward function) is trained to approximate human preferences and score different LLM outputs; (2) Then, during each fine-tuning iteration, given a single instruction, the LLM generates multiple responses, each of which is scored by the trained reward model; (3) Finally, policy optimization, an RL optimization technique, updates the LLM's weights to improve predictions based on these preference scores. Fine-tuing LLMs with RL tackles the aforementioned issues simultaneously. In one line, rather than being restricted to learning a specific answer, RL adjusts the LLM based on various preference scores, rewarding any valid, well-phrased responses. In the other line, the reward model is designed to approximate human preferences, enabling direct training on human preferences and fostering the LLM's capacity for impressive creativity.\nIn this paper, we organize the most up-to-date state of knowledge on reinforcement learning (RL) in large language models (LLMs), attempting to consolidate and analyze the rapidly growing research in this field, helping researchers understand the current landscape, challenges, and advancements. Specifically,\n\u2022 Section 2 presents the basics of reinforcement learning (RL) along with key terminologies, and outlines how the RL pipeline is adapted for LLMs.\n\u2022 Section 3 introduces popular and powerful LLMs enhanced by reinforcement learning.\n\u2022 Section 4 outlines the process of reinforcement learning from human feedback (RLHF), a training method that integrates reinforcement learning with human feedback to align LLMs with human values, preferences, and expectations.\n\u2022 Section 5 reviews research on reinforcement learning from AI feedback (RLAIF), which presents a promising alternative or complement to RLHF by utilizing AI systems to provide feedback on the outputs of the LLM being trained, offering advantages in scalability, consistency, and cost-effectiveness.\n\u2022 Section 6 provides an analysis of the challenges associated with RLHF and RLAIF.\n\u2022 Section 7 discusses research on direct preference optimization (DPO), a series of methods that bypasses the reward model and directly utilizes human preference data to align LLM outputs with human expectations.\n\u2022 Section 8 summarizes the current challenges and discusses opportunities for further improvement.."}, {"title": "Basics: Reinforcement Learning for LLMS", "content": "In this section, we first detail the basics of reinforcement learning (RL) along with key terminologies, and then outline how the RL pipeline is adapted for LLMs."}, {"title": "Basics of Reinforcement Learning", "content": "Reinforcement Learning (RL) is a key approach in machine learning, focusing on how an agent engages with its environment to maximize cumulative rewards. Unlike supervised learning, which depends on labeled data, and unsupervised learning, which uncovers patterns in unlabeled data, RL emphasizes learning through direct feedback via trial and error. Below, we sequentially describe basic definitions and general pipeline of RL."}, {"title": "Basic Definitions", "content": "Here, we use the training example in Figure 1 to illustrate the full process of RL. In this example, our goal is to train a robot to move from the bottom-left corner of a square to the top-right corner. Additionally, each grid cell has a reward score, and we aim to maximize the robot's total score. Before delving into the training process, we first introduce some relevant terms:\n\u2022 Agent: An agent is the entity we train to make correct decisions. In this example, our goal is to train the robot to make movement decisions, so the robot is the agent.\n\u2022 Environment: The environment is the external system that the agent interacts with. For our example, as the trained robot (agent) moves within the grid, the grid serves as the environment.\n\u2022 State: The state represents the agent's position at each time t. For instance, at the beginning, at time $t_0$, the robot (agent) starts at the bottom-left corner, so the state at time $t_0$ is the bottom-left corner, represented by the coordinates (0, 0).\n\u2022 Action(s): Actions represent the possible choices available to the agent within the environment at each time t. For example, at the start, at time $t_0$, the robot (agent) can choose to move right or up, making these two actions available to the agent at $t_0$.\n\u2022 Reward(s): Rewards are the signals or feedback provided by the environment to the agent based on the action it takes at each time t. For instance, at time $t_0$, the robot (agent) would receive a reward of +5 points for moving right, or a penalty of -1 point for moving up.\n\u2022 Policy: A policy is a set of decision-making strategies that helps the agent choose an action at each time t. In practice, at time $t_0$, the policy represents a probability distribution that directs the robot (agent) to move right or up in order to maximize its cumulative rewards."}, {"title": "General Pipeline of RL", "content": "We have defined key terminologies used in RL, and in this section, we will continue to detail the general pipeline of RL.\nAs illustrated in Figure 1, the general reinforcement learning (RL) pipeline can be represented as a Markov Decision Process (MDP). Formally, the agent begins in an initial state $s_0$, and at each time step t, it selects an action $a_t$ based on its current state $s_t$. In response, the environment transitions to a new state $s_{t+1}$, and the agent receives a reward $r_t$. This cycle continues, with the agent's objective being to maximize the cumulative rewards it accumulates over time.\nMapping into the specific example in Figure 1, at the initial time $t_0$, the robot starts at the bottom-left corner, denoted by the position (state) $s_0$. As time progresses, at each time step t, the robot chooses an action $a_t$ (either moving up or moving right). This action causes the robot to transition from its current position $s_t$ to a new position $s_{t+1}$, while earning a reward $r_t$. This cycle of movement and reward collection continues until the robot reaches the desired position (state) at the top-right corner, achieving the goal of maximum cumulative rewards."}, {"title": "RL for LLMs", "content": "We have outlined the general framework of RL above; now we will delve into the process of fine-tuning LLMs using RL. This approach aims to align LLMs with desired behaviors, enhance their performance, and ensure that their outputs are both effective and dependable.\nIn reinforcement learning (RL), there are six key components:agent, environment, state, action, reward, and policy. To apply RL for fine-tuning large language models (LLMs), the first step is to map these components to the LLM framework. LLMs are highly proficient at next-token prediction, where they take a sequence of tokens as input and predict the next token based on the given context. From an RL perspective, we can view the LLM itself as the policy. The current textual sequence represents the state, and based on this state, the LLM generates an action\u2014the next token. This action updates the state, creating a new state that incorporates the newly added token. After generating a complete textual sequence, a reward is determined by assessing the quality of the LLM's output using a pre-trained reward model.\nFigure 2 illustrates the specific RL framework for LLMs as proposed by (Ouyang et al., 2022). Ouyang et al. (2022) starts with an instruction-tuned model trained through supervised learning, enabling it to generate structured responses to human instructions. Then, Ouyang et al. (2022) applies the following two steps:\nStep 1: Collect comparison data, and train a reward model. Ouyang et al. (2022) collects a dataset of comparisons between outputs of the instruction-tuned model, where labelers indicate which output they prefer for a given input. Then, the collected dataset is used to train a reward model (RM) to predict the human-preferred output.\nStep 2: Optimize a policy against the reward model using PPO. Ouyang et al. (2022) leverages the output of the RM as a scalar reward, and fine-tunes the instruction-tuned model to optimize this reward using the PPO algorithm (Schulman et al., 2017)."}, {"title": "Popular LLMs Enhanced by RL", "content": "Recent popular LLMs with strong capabilities almost all leverage reinforcement learning (RL) to further enhance their performance during the post-training process. The RL methods adopted by these models can be typically divided into two main lines: 1. Traditional RL approaches, such as Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF). These methods require training a reward model and involve a complex and often unstable process, using algorithms like Proximal Policy Optimization (PPO) (Schulman et al., 2017) to optimize the policy model. Models like InstructGPT (Ouyang et al., 2022), GPT-4 (OpenAI, 2023), and Claude 3 (Anthropic, 2024) follow this approach. 2. Simplified approaches, such as Direct Preference Optimization (DPO) (Rafailov et al., 2024) and Reward-aware Preference Optimization (RPO) (Adler et al., 2024). These methods discard the reward model, offering a stable, performant, and computationally efficient solution. Models like Llama 3 (Dubey et al., 2024), Qwen 2 (Yang et al., 2024a), and Nemotron-4 340B (Adler et al., 2024) follow this approach. In this section, we provide a detailed description of each model, starting with a brief overview of these RL enhanced LLMs and followed by an explanation of how RL is applied in their post-training process. An overview of these RL Enhanced LLMs is shown in Tab 1."}, {"title": "InstructGPT", "content": "InstructGPT (Ouyang et al., 2022) is a series of language models fine-tuned from GPT-3 (Brown et al., 2020) by OpenAI, using human feedback to better align with human intent. The series includes models in three sizes: 1.3 B, 6 B, and 175 B parameters. The model is first fine-tuned using supervised learning with prompts collected from the OpenAI API or written by labelers and corresponding labeler demonstrations, then further refined using reinforcement learning from human feedback (RLHF). Human evaluations reveal that InstructGPT outputs are preferred over GPT-3. Notably, the 1.3B parameter InstructGPT model is favored over the 175B GPT-3, despite having 100 times fewer parameters. Additionally, InstructGPT demonstrates improved truthfulness and reduced toxic outputs, with minimal performance trade-offs on public NLP datasets.\nBefore applying reinforcement learning (RL), the authors train a 6B reward model (RM) initialized from the supervised fine-tuned (SFT) model, with the final unembedding layer removed. This RM is trained using comparison data ranked by labelers. During the RL phase, they fine-tune the SFT model to optimize the scalar reward output from the RM using the PPO algorithm (Schulman et al., 2017). To address performance regressions on public NLP datasets, they experiment with mixing pre-training gradients with PPO gradients, resulting in models known as PPO-ptx."}, {"title": "GPT-4", "content": "GPT-4 (OpenAI, 2023), developed by OpenAI, is a large multimodal model that can process both image and text inputs to produce text outputs. It excels at understanding and generating natural language, particularly in complex and nuanced scenarios. Evaluations show that GPT-4 performs exceptionally well on a range of human-designed exams, often surpassing the majority of human test takers. Additionally, it outperforms earlier large language models and most state-of-the-art systems, which frequently rely on benchmark-specific training or hand-engineered solutions.\nGPT-4 leverages RLHF methods, as outlined in InstructGPT (Ouyang et al., 2022) which we have describe in Sec 3.1, in the post-training alignment stage. To steer the models more effectively towards appropriate refusals at a finer level, the authors further use a zero-shot GPT-4 classifier as the rule-based reward model (RBRM). This RBRM provides an additional reward signal to the GPT-4 policy model during PPO fine-tuning on a subset of training prompts. The RBRM takes a prompt (optional), the policy model's output, and a human-written rubric (e.g., a set of rules in multiple-choice style) as input, then classifies the output according to the rubric. Through this approach, GPT-4 is rewarded for refusing harmful content and for appropriately responding to known-safe prompts."}, {"title": "Gemini", "content": "Gemini (Team et al., 2023) represents a family of advanced multimodal models developed by Google, distinguished by their impressive capabilities. The initial version, Gemini 1.0, comes in three sizes-Ultra, Pro, and Nano-ranging from large to small in terms of performance. Each size is tailored to address specific computational constraints and application needs. Notably, Gemini Ultra, the most powerful variant, achieves state-of-the-art results in 30 out of 32 benchmarks and is the first model to attain human expert-level performance on MMLU (Hendrycks et al., 2020), while setting new records across all 20 multimodal benchmarks.\nGemini implements a post-training process that utilizes an optimized feedback loop, collecting human-AI interactions to drive continuous improvement in key performance areas. During the post-training's RLHF phase, an iterative approach is adopted wherein reinforcement learning (RL) incrementally enhances the reward model (RM). Concurrently, the RM undergoes continuous refinement through systematic evaluation and data collection. This dynamic interplay promotes ongoing advancement in both RL and RM, leading to progressively improved performance over time."}, {"title": "InternLM2", "content": "InternLM2 (Cai et al., 2024) is an open-source series of large language models developed by Shanghai AI Laboratory, available in three sizes: 1.8B, 7B, and 20B. The model demonstrates superior performance across six dimensions and 30 benchmarks, including long-context modeling and open-ended subjective evaluations, thanks to innovative pre-training and optimization techniques.\nTo further enhance alignment, InternLM2 employs a novel strategy called Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) with the use of PPO. This approach addresses two key challenges. The first is preference conflict, where it is difficult to satisfy two preferences, such as helpfulness and harmlessness, simultaneously. The second challenge is reward hacking, which becomes more problematic as the model's scale increases and its policy becomes more powerful. COOL RLHF introduces a Conditional Reward mechanism that reconciles diverse preferences by allowing a single reward model to dynamically adjust its focus based on specific conditional prompts, effectively integrating multiple preferences. Additionally, COOL RLHF incorporates a multi-round Online RLHF strategy with two distinct pathways: a Fast Path for immediate, targeted improvements and a Slow Path for long-term, comprehensive refinement of the reward model. This approach enables the model to quickly adapt to new human feedback while reducing the risk of reward hacking."}, {"title": "Claude 3", "content": "Claude 3 (Anthropic, 2024) is a family of large multimodal models developed by Anthropic, which demonstrates strong performance across benchmark evaluations. It comprises three models with varying abilities and speeds: the largest, Claude 3 Opus; the mid-sized, Claude 3 Sonnet; and the smallest, Claude 3 Haiku. The Claude 3 models show strong benchmark performance, setting new standards in reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations such as GPQA (Rein et al., 2023), MMLU (Hendrycks et al., 2020), and MMMU (Yue et al., 2024). Claude 3 Haiku matches or surpasses Claude 2 in most text tasks, while Sonnet and Opus perform significantly better.\nThe authors use a technique called Constitutional AI (Bai et al., 2022) to align Claude 3 with human values during reinforcement learning (RL). In the RL stage, Constitutional AI follows a process similar to RLHF, but instead of human preferences for harmlessness, it uses AI feedback, known as RLAIF. Specifically, it distills language model interpretations of a set of rules and principles into a hybrid human/AI preference model (PM), using human labels for helpfulness and AI labels for harmlessness. Afterwards, they fine-tune the supervised learning model using RL with this PM, resulting in a policy trained by RLAIF."}, {"title": "Zephyr 141B-A39B", "content": "Zephyr 141B-A39B (HuggingFaceH4, 2024) is the newest addition to the Zephyr (Tunstall et al., 2023) series of language models, developed through a collaboration between Argilla, KAIST, and Hugging Face. This model is a Mixture of Experts (MoE) with a total of 141 billion parameters, 39 billion of which are active, fine-tuned from Mixtral-8x22B-v0.1 (Mistral AI, 2024).\nZephyr 141B-A39B employs a novel alignment algorithm known as Odds Ratio Preference Optimization (ORPO) (Hong et al., 2024). ORPO is a straightforward, unified alignment approach that discourages the model from adopting undesired generation styles during supervised fine-tuning. Notably, ORPO does not require an SFT warm-up phase, a reward model, or a reference model, making it highly resource-efficient. The method works by adding an odds ratio-based penalty to the standard SFT negative log-likelihood loss, enabling the model to distinguish between preferred and non-preferred response styles."}, {"title": "DeepSeek-V2", "content": "DeepSeek-V2 (Liu et al., 2024a), developed by DeepSeek-AI, is a powerful Mixture-of-Experts (MoE) language model designed for economical training and efficient inference. It features innovative architectures such as Multi-head Latent Attention (MLA) and DeepSeekMoE. With 236 billion total parameters, of which 21 billion are activated per token, it supports a context length of up to 128K tokens. The model is pre-trained on a high-quality, multi-source corpus of 8.1 trillion tokens. Evaluations show that DeepSeek-V2, along with its chat versions, maintains top-tier performance among open-source models, despite having only 21 billion activated parameters.\nDeepSeek-V2 is optimized using Group Relative Policy Optimization (GRPO) (Shao et al., 2024) during the RL phase to reduce training costs. Unlike traditional RL methods that use a critic model of similar size to the policy model, which increases training expenses, GRPO foregoes the critic model and estimates the baseline from scores computed on a group of outputs for the same question. Additionally, a two-stage RL training strategy is employed: the first stage focuses on reasoning alignment, and the second on human preference alignment, as the authors find these stages exhibit distinct characteristics."}, {"title": "ChatGLM", "content": "ChatGLM (GLM et al., 2024), developed by Zhipu AI, represents an evolving series of large language models. The latest version in this series is GLM-4, which includes variants such as GLM-4, GLM-4-Air, and GLM-4-9B. These models are pre-trained on a dataset of over 10 trillion tokens, predominantly in Chinese and English, and are subsequently post-trained through a combination of supervised fine-tuning (SFT) and RLHF to achieve advanced alignment quality. Evaluation results indicate that GLM-4 rivals or even surpasses GPT-4 (OpenAI, 2023) on general benchmarks like MMLU, and demonstrates superior performance in Chinese-specific alignments as measured by AlignBench (Liu et al., 2023b).\nThe reinforcement learning phase involves the ChatGLM-RLHF (Hou et al., 2024) pipeline, which enhances alignment with human preferences. This pipeline comprises three primary components: gathering human preference data, training a reward model, and optimizing policy models. To support large-scale training, ChatGLM-RLHF includes methods to reduce reward variance for stable training, leverages model parallelism with fused gradient descent, and applies regularization constraints to prevent catastrophic forgetting in large language models. Experimental results confirm that ChatGLM-RLHF yields substantial improvements in alignment-focused tasks compared to the supervised fine-tuned version of ChatGLM."}, {"title": "Nemotron-4 340B", "content": "Nemotron-4 340B (Adler et al., 2024) is a family of models released by NVIDIA, consisting of Nemotron-4-340B-Base, Nemotron-4-340B-Instruct, and Nemotron-4-340B-Reward. The Nemotron-4-340B-Base model is trained on 9 trillion tokens from a high-quality dataset. In the alignment process to develop Nemotron-4-340B-Instruct, over 98% of the data used is synthetically generated by the model. Evaluations demonstrate that these models perform competitively with open-access models across a broad range of evaluation benchmarks.\nDuring the preference fine-tuning phase, both DPO (Rafailov et al., 2024) and a new alignment algorithm, Reward-aware Preference Optimization (RPO), are employed to improve the model through multiple iterations. RPO addresses a limitation in DPO, where the quality difference between selected and rejected responses is not considered, leading to overfitting and the forgetting of valuable responses. RPO uses an implicit reward from the policy network to approximate this gap, enabling the model to better learn from and retain superior feedback."}, {"title": "Llama 3", "content": "Llama 3 (Dubey et al., 2024), developed by Meta, is a collection of open-source foundational language models available in sizes of 8 billion, 70 billion, and 405 billion parameters. It is trained on a significantly larger corpus consisting of approximately 15 trillion multilingual tokens, a notable increase compared to the 1.8 trillion tokens used for Llama 2 (Touvron et al., 2023). Extensive empirical evaluations demonstrate that Llama 3 achieves performance comparable to leading models, such as GPT-4 (OpenAI, 2023), across a diverse range of tasks.\nThe post-training process for aligning Llama 3 with human feedback involves six rounds of iterative refinement. Each round includes supervised fine-tuning (SFT) followed by DPO, with the final model being an average of the outputs from all rounds. For each round, a reward model (RM) is trained on newly collected preference annotation data, targeting a wide range of capabilities built upon the pre-trained checkpoint. After SFT, DPO is applied to further optimize the SFT models, using recent preference data batches obtained from the best-performing models of previous rounds. To enhance the stability of DPO training, two key adjustments are implemented: masking out formatting tokens in the DPO loss and introducing regularization via an NLL (negative log-likelihood) loss."}, {"title": "Qwen2", "content": "Qwen2 (Yang et al., 2024a), developed by Alibaba, is a series of large language models ranging from 0.5 billion to 72 billion parameters in dense configurations, as well as a Mixture-of-Experts variant with 57 billion parameters, of which 14 billion are activated per token. It is pre-trained on a high-quality, large-scale dataset containing over 7 trillion tokens, covering a wide array of domains and languages. Extensive evaluations show that Qwen2 outperforms most prior open-weight models, including its predecessor Qwen1.5, and delivers competitive results across a range of benchmarks, including language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning.\nThe preference fine-tuning process for Qwen2 consists of two main stages: offline and online learning. In the offline stage, Qwen2 is optimized using DPO, which aims to maximize the likelihood difference between two responses to the same prompt, based on a pre-compiled preference dataset. In the online stage, the model improves continuously in real-time by utilizing preference pairs selected by the reward model from multiple responses generated by the current policy model. Additionally, the Online Merging Optimizer (Lu et al., 2024) is employed to minimize alignment costs."}, {"title": "Gemma 2", "content": "Gemma 2 (Team et al., 2024b), developed by Google, is the latest addition to the Gemma family of lightweight, state-of-the-art open models, with sizes ranging from 2 billion to 27 billion parameters. The model incorporates several well-established modifications to the Transformer architecture, including interleaving local-global attentions (Beltagy et al., 2020) and group-query attention (Ainslie et al., 2023). Experiments demonstrate that these models deliver the best performance for their size and even provide competitive alternatives to models 2-3 times larger.\nSimilar to Gemma 1.1 (Team et al., 2024a), during the post-training RLHF phase, the authors use a high-capacity model as an automatic rater to tune hyperparameters and mitigate reward hacking (Amodei et al., 2016; Skalse et al., 2022). However, unlike Gemma 1.1, they employ a reward model that is an order of magnitude larger than the policy model. This reward model is specifically designed to focus on conversational capabilities, with an emphasis on multi-turn interactions."}, {"title": "Starling-7B", "content": "Starling-7B (Zhu et al., 2024) is a strong 7-billion-parameter chat model developed by UC Berkeley, focused on alignment with human preferences for helpfulness and harmlessness. It is fine-tuned from Openchat-3.5 (Wang et al., 2024a) using RLAIF on a high-quality preference dataset called Nectar, which comprises 3.8 million pairwise comparisons generated by prompting GPT-4 to rank responses. As a result, the model's score on MT-Bench improves from 7.81 to 8.09, its score on AlpacaEval increases from 88.51% to 91.99%, and its human evaluation ELO on Chatbot Arena (Chiang et al., 2024) rises from 1072 to 1087.\nThe authors introduce several improvements to the PPO algorithm during the RLAIF process to enhance training stability and robustness. First, they introduce a constant positive reward for length control to prevent excessive verbosity. This adjustment helps address the issue where a highly negative reward from the reward model during the early stages can cause the policy model to become overly verbose after only a few gradient updates. Second, they pretrain the critic model to reduce early performance drops due to a randomly initialized critic. Third, they conduct full parameter tuning on both the actor and critic models, as opposed to tuning only the top four layers, to maximize performance improvements during the reinforcement learning stage."}, {"title": "01", "content": "OpenAI's o1 (OpenAI, 2024b) is a newly developed large language model optimized for complex reasoning, utilizing reinforcement learning for its training. Before producing responses, o1 engages in an extensive internal thought process, enabling it to excel across various reasoning tasks. The model significantly surpasses GPT-4o (OpenAI, 2024a) in many challenging tasks: ranks in the 89th percentile on Codeforces for competitive programming, places among the top 500 participants in the AIME for mathematics, and surpasses PhD-level accuracy in scientific benchmarks such as GPQA.\nThe training of o1 involves a large-scale reinforcement learning algorithm that emphasizes productive thinking through a detailed chain of thought (CoT) (Wei et al., 2023), implemented with high data efficiency. To preserve the model's unfiltered reasoning ability, no policy compliance or user preference training is applied to its internal thought processes, which also provides a unique opportunity to understand the model's raw thought process. This approach allows o1 to refine its strategies, correct errors, and deconstruct complex problems during training. Notably, the model's performance improves with increased training compute and with more extensive test-time computation."}, {"title": "Others", "content": "Reka Core", "Edge": "Team et al. (2024c) are powerful multimodal language models developed from scratch by Reka. Reka Edge and Reka Flash are dense models with 7B and 21B parameters", "further.\nPhi-3": "Abdin et al. (2024) is a series of language models introduced by Microsoft", "rejected": "esponses.\nAthene-70B: Nexusflow (2024) is a powerful chat model fine-tuned from Llama-3-70B (Dubey et al.", "3": "Teknium et al. (2024) is a series of neutrally-aligned generalist instruction and tool-use models with advanced reasoning and creative capabilities, developed by Nous Research. It is finetuned from Llama 3.1 (Dubey et al., 2024) in 8B, 70B, and 4"}]}