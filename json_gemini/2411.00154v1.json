{"title": "Scaling Up Membership Inference: When and How Attacks Succeed on Large Language Models", "authors": ["Haritz Puerto", "Martin Gubri", "Sangdoo Yun", "Seong Joon Oh"], "abstract": "Membership inference attacks (MIA) attempt to verify the membership of a given data sample in the training set for a model. MIA has become relevant in recent years, following the rapid development of large language models (LLM). Many are concerned about the usage of copyrighted materials for training them and call for methods for detecting such usage. However, recent research has largely concluded that current MIA methods do not work on LLMs. Even when they seem to work, it is usually because of the ill-designed experimental setup where other shortcut features enable \"cheating.\" In this work, we argue that MIA still works on LLMs, but only when multiple documents are presented for testing. We construct new benchmarks that measure the MIA performances at a continuous scale of data samples, from sentences (n-grams) to a collection of documents (multiple chunks of tokens). To validate the efficacy of current MIA approaches at greater scales, we adapt a recent work on Dataset Inference (DI) for the task of binary membership detection that aggregates paragraph-level MIA features to enable MIA at document and collection of documents level. This baseline achieves the first successful MIA on pre-trained and fine-tuned LLMs.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are trained on vast datasets, which providers typically keep private. Data owners are concerned that their copyrighted data might be used in LLM training without explicit consent. Membership Inference Attacks (MIAs) attempt to determine if a specific data sample was used to train a model (Shokri et al., 2017). These methods are now being applied to LLMs to address the question of potential data misuse (Shi et al., 2024; Meeus et al., 2024a; Zhang et al., 2024b; Wang et al., 2024a; Xie et al., 2024).\nThe application of MIAs on LLMs has faced discouraging results so far. Initial claims of success (Shi et al., 2024; Meeus et al., 2024a; Carlini et al., 2021; Mattern et al., 2023) were later disproven by Duan et al. (2024); Das et al. (2024); Maini et al. (2024); Meeus et al. (2024b). Before their work, it was common practice to select true non-members from documents created after the LLM's cut-off date. They showed that this approach allowed MIA methods to exploit temporal cues, rather than identifying membership through the model's inherent response characteristics. Duan et al. (2024); Das et al. (2024); Maini et al. (2024) introduced a new evaluation method based on an independent, identically distributed (IID) split between true members and non-members. Since adopting this method, no further MIA studies on LLMs have shown performance significantly better than random. Reported membership detection has remained below 60% AUROC, close to the random-chance level of 50% AUROC (Duan et al., 2024; Das et al., 2024; Maini et al., 2024; Xie et al., 2024; Zhang et al., 2024b).\nWe argue that MIA can still be effective on LLMs, provided it is applied to much longer token sequences than previously considered. Earlier MIA approaches often focused on short sequences of tokens, typically ranging from 128 to 256 tokens (Xie et al., 2024; Duan et al., 2024; Shi et al., 2024; Wang et al., 2024b). This use of n-grams faced criticism because of the significant overlap between members and non-members, making the MIA problem poorly defined (Duan et al., 2024). Some later studies suggested analysing entire documents instead of n-grams (Shi et al., 2024; Meeus et al., 2024b), but even then, performance remained near random (Meeus et al., 2024b).\nIn this work, we demonstrate that MIA approaches begin to show meaningful performance only when applied to much longer token sequences, such as 10K tokens. To show this, we introduce four scales of token sequences: sentences, paragraphs, documents, and datasets, as shown in Figure 1. We propose MIA evaluation protocols and benchmarks for the binary detection of training data samples given at four different scales. As a baseline, we adapt the Dataset Inference (Maini et al., 2024) method for the MIA task at multiple scales. As a result, our aggregation-based MIA demonstrates significant performance improvements for document sets, achieving AUROC scores of 80% or higher.\nTo explore additional scenarios, we investigate the performance of MIA on fine-tuning data. Our findings show that continual learning MIA is effective collection of documents (AUROC > 88%), while CoT-based fine-tuning works at both the sentence and collection levels.\nOur contributions are summarized as:\n1. We introduce a novel evaluation benchmark and protocol for MIA, covering multiple scales of token sequences.\n2. We extend and adapt the first successful \u039c\u0399\u0391 Maini et al. (2024) to any data scale, allowing us to conduct a comprehensive analysis of MIA performance in LLMs.\n3. We provide additional MIA benchmarks for various LLM fine-tuning scenarios, demonstrating that our method achieves even stronger performance in these contexts."}, {"title": "2 Background & Related Work", "content": "Membership inference attacks (MIA) aims to prove that a certain data sample belongs to the training set of a model. Yeom et al. (2018) hypothesize that members have a lower loss (perplexity for LLMs) than non-members and based on this propose to use the loss to infer membership. Carlini et al. (2021) build on top of it and propose to use the ratio of the perplexity and the zlib entropy. Shi et al. (2024) propose to compute the average log-likelihood of the tokens with the lowest probabilities based on the assumption that members would have higher probabilities than non-members.\nSince most LLMs do not have training-test set splits, initial benchmarks and recent works use the knowledge cut-off date to define members and non-members (Shi et al., 2024; Meeus et al., 2024a). Works using these evaluation benchmarks show positive results (Shi et al., 2024; Meeus et al., 2024a; Zhang et al., 2024b; Wang et al., 2024a; Xie et al., 2024). However, Duan et al. (2024); Das et al. (2024); Maini et al. (2024) showed that the evaluation method based on cut-offs is flawed as these cut-offs introduce temporal biases that are easily captured by a bag of words. Therefore, they proposed to use LLMs trained on datasets that contain a random train-test split for members and non-members, like The Pile dataset (Gao et al., 2020). In this setup, however, MIA only achieves performance barely above random guessing.\nMaini et al. (2024) show that aggregating MIA scores across multiple documents can yield successful dataset-level MIA. However, their work does not provide standard performance metrics like AUROC, leaving the precise effectiveness of their method unclear compared to standard paragraph-level MIA. In this study, we extend their evaluation by applying bootstrapping to compute AUROC for collection of documents and adapt their method to any data scale, allowing us to conduct a comprehensive analysis of MIA performance in LLMs.\nMIA performance across data scales in LLMs has not yet been explored despite its importance for copyright disputes. Current lawsuits against LLM developers primarily concern the use of entire documents in training sets (Pope, 2024). However,"}, {"title": "3 \u039c\u0399\u0391 Evaluation", "content": "We introduce novel evaluation benchmarks for membership inference attacks (MIA) across various scales and multiple LLM training scenarios.\nMIA Evaluation Overview. At a high level, following previous MIA work (Shi et al., 2024; Meeus et al., 2024a; Zhang et al., 2024b; Wang et al., 2024a; Xie et al., 2024), we frame the problem as a binary detection task: determining whether a given token sequence t was used during the training of a target large language model M. Specifically, MIA methods are expected to produce scores $(s^1, \\dots, s^n)$ for each token sequence in a set of instances $(t^1, \\dots, t^n)$. With knowledge of the actual membership status $(m^1, \\dots, m^n)$, indicating whether each sequence was part of the training set, we can evaluate detection performance using the area under the receiver-operating-characteristic curve (AUROC) across different thresholds.\nHowever, creating a binary detection task for long token sequences is non-trivial. Depending on the LLM type, sequences of 10K tokens or more often exceed the context window of current models. This requires assessing membership across a set of sequences, demanding datasets with known members and non-members at the sequence set level. In this work, we propose four MIA benchmarks on the PILE dataset (Gao et al., 2020), each targeting a different sequence length scale. We also extend previous MIA benchmarks, traditionally focused on detecting pre-training data, to three popular LLM training paradigms, including fine-tuning. In total, we introduce 4 \u00d7 3 = 12 MIA benchmarks, covering realistic application scenarios.\nData Scales. We define four scales of the \u039c\u0399\u0391 tasks: i) sentence, ii) paragraph, iii) document, and iv) collection, as shown in Table 1.\nSentence-level MIA. We define a sentence as a natural sequence of words ending in a full stop. The average sentence in the Pile contains 43 tokens. This granularity is important because it is used to detect whether specific data points in benchmarks (e.g., questions in question-answering tasks) are contaminated, ensuring fair model evaluation. Due to its short nature, it can be extremely challenging to perform MIA successfully. For instance, Duan et al. (2024) shows large overlaps between member and non-member sentences, which blurs the decision boundary. Additionally, sentence-level MIA can probe privacy leakage inferring membership of personally identifiable information (Kim et al., 2023).\nParagraph-level MIA. We define a paragraph as a sequence of tokens that fits within the context window of a large language model (LLM). Thus, the length of a \"paragraph\" depends on the specific LLM in use. In our experiments, we use paragraphs of up to 512, 1024, and 2048 tokens, aligning with the context window sizes of the target models. In practice, paragraph-level copyrighted materials are particularly relevant to user-generated content on social media platforms and online forums, where short-form content is most common.\nDocument-level MIA. We define a document in the conventional sense, such as a single arXiv paper. In The Pile dataset, the average document contains 14,222 tokens. Documents typically consist of multiple paragraphs, often exceeding the context window of many LLMs, such as Pythia, which has a 2048-token limit. MIA approaches must handle these long documents by splitting the token sequence into smaller chunks, or \u201cparagraphs\u201d (as defined earlier), that fit within the model's context length and then aggregating the model's responses across these chunks. Performing MIA at the document level is essential for addressing copyright and data ownership concerns, particularly for copyrighted materials like novels, news articles, research papers, and book chapters.\nCollection-level MIA. We define a collection as a set of documents. The attacker can choose the collection's size. For instance, with 100 documents, a collection contains approximately 1.4 million tokens. Collection-level MIA is crucial, as one may question whether a collection of articles, such as those provided by an internet service provider, has been crawled and used in LLM training. Aggregating individual signals can amplify the detection of the collection's usage. This is also relevant when examining contaminated benchmarks, as entire datasets may be unintentionally included in the training, leading to an overestimation of model performance.\nMIA at scales beyond the document level is legally and practically significant, as copyright disputes often focus on individual articles. Moreover, as we will show in \u00a75, MIA only achieves strong performance at these larger scales.\nLLM Training Scenarios. Previous MIA approaches for LLMs have primarily focused on detecting pre-training data. While pre-training poses significant concerns regarding data scraping and exploitation by large tech companies with vast resources, other forms of training, such as fine-tuning, are much more common and affordable for entities worldwide. Although fine-tuning operates on a smaller scale, it may still involve copyrighted materials, and models fine-tuned in this way could be deployed in widely used applications. In such cases, copyright infringement could have serious consequences. Therefore, we consider MIA under three different LLM training scenarios."}, {"title": "4 Method", "content": "We extend the Dataset Inference (DI) methods introduced by Maini et al. (2024) to multiple MIA scales and adapt them for the binary detection task. While the original DI focused solely on determining whether a single dataset is a member or not, we adapt this methodology for finer granularity, such as document-level MIA. Additionally, we explain how to derive the detection scores necessary for evaluating AUROC performance.\nWe begin by explaining the Dataset Inference (DI) process. Given a token sequence t, we divide it into smaller paragraphs, $t_{p,1}, \\dots, t_{p,K}$, each small enough to fit within the context window of the large language model (LLM) M under investigation. Following the DI approach, we compute various membership inference attack (MIA) features for each token sequence, using existing sentence-level MIA methods. These include perplexity (Yeom et al., 2018), lowercase perplexity, zlib compression scores (Carlini et al., 2021), and Min-K statistics (with thresholds at 5%, 10%, ..., 60%; Zhang et al. 2024b). These MIA features, denoted by $A_1, \\dots, A_L, (L = 10)$ are functions that take the likelihood output of M on a sequence as input and return a scalar representing the membership information for that sequence. Thus, for a given token sequence t, we produce a K \u00d7 L array of MIA features, $A_l(M(t_{p,k}))$.\nWe employ a two-stage aggregation process to reduce the K \u00d7 L array into a single statistic representing membership likelihood. In the first stage, we aggregate the L MIA features into a single score using a learned linear map $f: \\mathbb{R}^L \\rightarrow [0, 1]$. This linear map is trained on a dataset consisting of 1,000 known members and 1,000 known non-members (see \u00a73), with the objective of predicting the membership status m of the token sequence t. After aggregation, we obtain a set of K \u039c\u0399\u0391 scores: $\\{f(t_{p,1}), \\dots, f(t_{p,K})\\}$. These scores are treated as an unordered set, as the criteria for determining membership becomes largely independent of the ordering of documents within a dataset or paragraphs within a document. Following the DI approach, we perform statistical testing by comparing the set of K MIA scores against a baseline set of K scores from known non-members. Specifically, we use the Student's t-test for collection-level MIA since we aggregate more than 30 samples and the Mann-Whitney U-test for document-level M\u0399\u0391, since we aggregate less than 30 samples from a non-normal distribution.\n$t-score(t) = \\frac{\\mu - \\mu_{n-m}}{\\sqrt{s^2 + s_{n-m}^2}}$\n$U-score(t) = \\sum_{k=1}^{K} rank(f(t_{p,k}))$\nHere, \u00b5 and s represent the mean and sample standard deviation of the K MIA scores, with the subscript n-m indicating the scores from the non-members. The term $rank(f(t_{p,k}))$ refers to the rank of the MIA feature for the query sequence t in the combined set of 2K MIA features, which"}, {"title": "5 Experiments", "content": "With our experiments, we aim to answer the following questions: i) How effective is the aggregation of MIA scores for larger textual units, such as documents and collection of documents? (\u00a75.1), ii) What are the requirements for a successful aggregation? (\u00a75.2), iii) What is the nature of the compounding effect observed in MIA score aggregation? (\u00a75.3), iv) How much does the MIA aggregation benefit from fine-tuning scenarios (\u00a75.4)?"}, {"title": "5.1 Aggregating Text Subunits is Effective", "content": "Table 2 and Figure 3 demonstrate the effectiveness of aggregating multiple text units (e.g., sentences or paragraphs) to perform successful Membership Inference Attacks (MIA) at larger textual scales (e.g., documents or collections). The figure illustrates a clear trend: MIA performance improves as we increase the number of aggregated text units.\nThe left plot focuses on collection MIA for arXiv. It reveals a stark contrast between small and large collections. While collections with only dozens of documents yield AUROC scores barely above random chance, those with 500 or more documents achieve significantly higher AUROC. Notably, using the 6.9B model and despite employing only a few MIA methods in our ensemble, we attain a remarkable collection MIA AUROC of 0.9 for arXiv.\nThe right plot shows the performance of document MIA on arXiv, where we observe our highest results. This exceptional performance can be attributed to two factors: i) the considerable length of the documents (averaging around 15K tokens), allowing for the aggregation of numerous MIA scores, and ii) a paragraph MIA AUROC exceeding 0.53. The combination of these factors yields an impressive document MIA AUROC of 0.75. To the best of our knowledge, this marks the first successful application of MIA to entire documents."}, {"title": "5.2 Requirements for Successful Aggregations", "content": "Figure 4 illustrates common scenarios where the aggregation method fails to achieve MIA performance. The left plot demonstrates that collection-MIA can be ineffective when the base AUROC is close to random chance. Even with collection of 500 documents, the collection-MIA for the 2.8B model barely achieves an AUROC of 0.55. The central plot presents a case where the base ROC is sufficiently high, but the documents are too short (consisting of only 3 paragraphs of 2048 tokens each), providing insufficient paragraphs for effective aggregation. The right plot shows the opposite situation: while the documents are long enough (comparable to the successful arXiv case), the base AUROC is only 0.5, rendering the aggregation ineffective. These examples highlight the importance of both base performance and sufficient text units for successful MIA score aggregation. Lastly, aggregating sentences to conduct paragraph MIA becomes extremely challenging because sentence-level MIA remains unachievable and the works of Maini et al. (2021); Duan et al. (2024) suggest it might even be impossible. We provide the plots for all collection, document, and paragraph aggregations on Appendix D."}, {"title": "5.3 Compounding Effect in MIA Score Aggregation", "content": "Our experiments reveal a powerful compounding effect in the aggregation of MIA scores from smaller to larger textual units. This relationship follows an approximately square root function, where small improvements in paragraph-level MIA performance lead to substantial gains at the collection or document level. Figure 5 illustrates this relationship across all our experiments, encompassing various sources and hyperparameters.\nThe compounding effect is particularly striking when examining specific thresholds. For instance, a paragraph-level MIA with an AUROC of 0.51 can result in collection-level MIA AUROCs ranging from 0.5 to 0.65. However, a modest improvement in paragraph-level performance to an AUROC of 0.53 can dramatically boost collection-level AUROCs to between 0.6 and 0.9. This demonstrates the potential for significant gains in MIA effectiveness through relatively small improvements in paragraph level.\nLastly, our analysis also indicates that our aggregation method remains effective as long as the base AUROC exceeds 0.51 and establishes a threshold for when aggregation becomes a viable strategy."}, {"title": "5.4 MIA Aggregation Benefits from Fine-tuning Scenarios", "content": null}, {"title": "5.4.1 Continual Learning Fine-tuning", "content": "In this section, we investigate the performance of MIA on LLMs that have undergone a continual learning process to adapt to specific domains. Figure 6 and Table 3 show the MIA performance on Pythia 2.8B after it was further trained on the validation sets of Wikipedia and GitHub (independently) from the Pile dataset.\nOur results reveal a significant increase in \u039c\u0399\u0391 effectiveness in this continual learning scenario."}, {"title": "5.4.2 End-Task Fine-tuning", "content": "We reuse the fine-tuned Phi-2 for reasoning from Puerto et al. (2024) for our fine-tuning experiments. They fine-tuned the model on multiple question-answering datasets so that the model responded with a chain of thoughts. This task allows us to know if MIA could be used to evaluate the contamination of fine-tuned models at the sentence level, for example, if the model used the evaluation questions for training.\nTable 4 presents the performance of dataset and sentence-level MIA. For dataset-MIA, we use 100 datasets of 20 datasets each from 10k unique questions using a non-member known partition of 10 questions for the statistical comparison. For the evaluation of sentence-MIA, we use 5980 questions. In both cases, we run the experiments five times with different random seeds. Notably, sentence-MIA achieves an AUROC of 0.793 \u00b1 0.024, while dataset-MIA is 0.99 for small datasets of just 20 data points. This suggests that MIA could serve as strong evidence in legal cases to prove the use of data for fine-tuning LLMs, in contrast to the claims made by Zhang et al. (2024a).\nWe further explored scenarios where collections contain a mix of both member and non-member questions to evaluate the robustness of our collection-MIA method in the presence of noise. As shown in Figure 8 in Appendix D, with a 20% contamination rate, the method continued to classify the collection as members, but at a contamination rate of 50%, the method correctly assigned membership status in 50% of cases. These results confirm the robustness of our approach to handling noisy collections."}, {"title": "6 Conclusion", "content": "In this paper, we have shown the importance of evaluating membership inference attacks (MIA) across different data scales, from sentences to collections of documents. Each text granularity represents a different and valuable use case that requires investigation. In contrast to prior works that suggest that MIA does not work for LLMs, we have empirically shown that MIA can work for document and collection levels with currently available attacks. We have further explored the performance of MIA across different training stages of an LLM and show that while small continual training remains robust towards sentence-level MIA, end-task fine-tuning is vulnerable, making MIA a suitable method to analyse test-set contamination.\nIn this work, we use simple MIA baselines to test the effectiveness of their aggregation and statistical testing. We believe the addition of more baselines would improve the overall results and leave that for future work."}, {"title": "Limitations", "content": "The lack of training-validation-test set splits for training data of LLMs limits the range of models to evaluate. In this work, we only focus on Pythia as in (Duan et al., 2024; Maini et al., 2024). Furthermore, we only employed 2.8B and 6.9B, the smallest models where MIA has been seen to perform a bit better than random chance. Using larger models could boost the reported results. Similarly, we only use three baselines as membership inference attacks. The use of more baselines could further improve our results."}, {"title": "Ethics and Broader Impact Statement", "content": "This work adheres to the ACL Code of Ethics. In particular, The Pile and Pythia we used have been shown by prior works to be safe for research purposes. They are not known to contain personal information or harmful content. We also fulfill with their licenses MIT for The Pile and Apache 2.0 for Pythia. Similarly, the model used in Section 5.4 also uses Apache 2.0 and is known to be safe for research purposes. Our method aims to provide data providers with tools to use their rights for the hypothetical cases where an LLM developer uses their data without consent.\nUnderstanding when and how membership inference attacks (MIA) succeed on large language models (LLMs) can provide insights into developing strategies to train LLMs to be robust against these attacks. This poses the risk of making MIA outdated in the future, and therefore, data providers could be back in their current position without tools to use their rights."}]}