{"title": "Integrate the Essence and Eliminate the Dross: Fine-Grained Self-Consistency for Free-Form Language Generation", "authors": ["Xinglin Wang", "Yiwei Li", "Shaoxiong Feng", "Peiwen Yuan", "Boyuan Pan", "Heda Wang", "Yao Hu", "Kan Li"], "abstract": "Self-consistency (SC), leveraging multiple samples from LLMs, shows significant gains on various reasoning tasks but struggles with free-form generation due to the difficulty of aggregating answers. Its variants, UCS and USC, rely on sample selection or voting mechanisms to improve output quality. These methods, however, face limitations due to their inability to fully utilize the nuanced consensus knowledge present within multiple candidate samples, often resulting in suboptimal outputs. We propose Fine-Grained Self-Consistency (FSC) to addresses these limitations by extracting and integrating segment-level commonalities from candidate samples, enhancing the performance of LLMs both in open-ended and reasoning tasks. Based on this, we present two additional strategies: candidate filtering, which enhances overall quality by identifying highly similar candidate sets, and merging, which reduces input token requirements by combining similar samples. The effectiveness of FSC is demonstrated through extensive experiments on various tasks, including summarization, code generation, and mathematical reasoning, using GPT-3.5-turbo and GPT-4. The results indicate significant improvements over baseline methods, showcasing the potential of FSC to optimize output quality by effectively synthesizing fine-grained consensus knowledge from multiple samples\u00b9.", "sections": [{"title": "1 Introduction", "content": "The remarkable success of large-scale language models (LLMs) have transformed the landscape of natural language processing, showcasing significant improvements across a wide range of tasks, from reasoning tasks (Wei et al., 2022) with distinct answers like arithmetic and commonsense reasoning to free-form generation tasks like code generation (Austin et al., 2021) and summarization (Goyal et al., 2022).\nHowever, LLMs may still generate suboptimal samples in challenging tasks. Efforts to improve output quality involve selecting the best response from multiple samples based on specific criteria. This includes using trained models for reranking outputs (Ravaut et al., 2023) and employing LLMs to evaluate the responses (Liu et al., 2023b). However, both approaches require additional models and overlook the knowledge present among the candidates. Wang et al. (2023) introduce self-consistency (SC) to improve performances without additional models, which mitigates noise from individual sampling by employing a voting mechanism across multiple samples. Unfortunately, SC is limited to tasks where the final answer can be aggregated through precise matching. How to aggregate the answers for free-form problems remains unclear.\nRecently, some works seek to evolve the idea of self-consistency into open-ended generative tasks. UCS (Jain et al., 2023) calculates the overlap of unigrams between candidates and then selects the final answer with highest value. Alternatively, USC (Chen et al., 2023) leverages the capabilities of LLMs instead of rule-based criteria to choose the most consistent one. However, both approaches still rely on selection or voting mechanisms, which do not align well with the nature of free-form generation tasks, i.e., the final quality is determined by the entirety of the output content, rather than specific individual tokens. Therefore, sample-level selection methods only yield suboptimal outputs, primarily due to two reasons: (1) They are unable to incorporate consensus knowledge from unselected samples, which, despite their lower overall quality, may contain locally valuable information to enhance the quality of selected samples. (2)"}, {"title": "2 Related Work", "content": "Consistency-Based Response Selection Approaches. The literature presents a variety of consistency-based response selection approaches, typically incorporating a voting procedure to select the most frequently occurring response (Wang et al., 2023; Zhou et al., 2022; Portillo Wightman et al., 2023; Yue et al., 2023; Li et al., 2023b). The self-consistency approach proposed by Wang et al. (2023) demonstrates that by generating multiple responses for the same task and selecting the reasoning path leading to the most common final answer, the performance of chain-of-thought reasoning can be improved. Candidate responses can also be derived from different prompt variants corresponding to the same problem (Zhou et al., 2022; Portillo Wightman et al., 2023; Yue et al., 2023). For open-ended generation tasks, Jain et al. (2023) propose the n-gram consistency score to measure the pairwise similarity between candidate responses. The consistency score for each response is calculated as the sum of the pairwise similarity scores. Chen et al. (2023) propose leveraging LLMs themselves to directly select the most consistent answer among multiple candidates without an explicit definition of the pairwise similarity. In this work, we take a closer look into the coarse-grained limitations faced by selection based self-consistency methods on open-ended generation tasks, and propose Fine-Grained Self-Consistency to fully leverage the segment-level consensus knowledge extracted within multiple samples so as to gain better consistency output.\nResponse improvement with multiple candidates. Recent studies show that the LLM can enhance its own prediction output based on candidate responses. Zheng et al. (2023) demonstrate"}, {"title": "3 Method", "content": "The core idea behind fine-grained self-consistency is to measure and extract the commonality of each response generated by LLM at the segment level, and then fuse to generate the final response with superior commonality. Figure 2 illustrates the idea of select-based self-consistency and the proposed fine-grained self-consistency (FSC). Specifically, select-based self-consistency ranks the responses generated by LLM according to specific consistency criteria (Jain et al., 2023; Chen et al., 2023), and the top-ranked response is selected as the output. However, these methods do not assess the consistency at a more fine-grained level within the response. On the other hand, FSC first measures the commonality of each segment of the responses generated by LLM, and extracts the corresponding common part, then fuses the common part and generates the final output, achieving a more refined consistency sample output.\nWe present the overall workflow of Fine-grained self-consistency (FSC) in Figure 3, Including FSC with two plugins, candidates filtering and merge."}, {"title": "3.1 Fine-Grained Self-Consistency", "content": "Considering that it is difficult to divide and measure the consistency of the segments in the responses based on prior knowledge, we seek to utilize the comparative and integrative capabilities of LLMs to inherently extract the fine-grained common part and integrate consistency knowledge from different responses. As shown in Figure 3, for the multiple input responses, we concatenate them all and construct a prompt with an instruction asking the LLM to extract the major consensus of generated responses, then integrate and generate the final response. In this way, FSC measures commonality at a finer granularity, utilizing, and integrating the consistency knowledge from different responses, alleviating the coarse-grained limitations of the selection based self-consistency."}, {"title": "3.2 Candidates filtering", "content": "As shown in Figure 3, considering the varying quality of responses generated by LLM, we propose candidates filtering strategy to measure the consistency of generated responses at the sample level, eliminating responses with low consistency to ensure the overall quality of the candidate responses. Specifically, given the similarity function Sim, we can define a sample-level self-consistency score $SC_{sim}(i)$ for each response i, given by $SCSim(i) = \\frac{1}{N-1} \\sum_{j=1,j \\neq i}^{N}Sim(j, i)$. Here, N represents the number of responses, and we use ROUGE (Lin, 2004) for our similarity function Sim. In the end, we take the Top-k responses according to $SC_{sim}$ as the filtered candidates set, where k is a hyperparameter."}, {"title": "3.3 Merge", "content": "As shown in Figure 3, to reduce the computation cost of FSC, we propose merging semantically similar candidates to decrease the number of responses to be integrated by the LLM. Furthermore, due to the limitations of the LLM input length, the number of samples inputted to the LLM for consistency extraction is limited. By merging, we can provide the LLM with samples that contain more diverse knowledge, broadening the LLM's knowledge field of view. specifically, To merge similar responses, we employ the K-Medoids clustering algorithm based on their semantic similarity. We categorize all responses into c clusters, each encompassing a set of similar results. Then we select the centroids of each cluster as representative responses and discard the remaining ones. It ensures the selected response has diverse knowledge and reduces the cost of FSC."}, {"title": "4 Experiment", "content": "4.1 Evaluation setup"}, {"title": "4.1.1 Benchmarks", "content": "We evaluate FSC on the following variety of tasks:\nCode generation We conduct experiments on three widely used code generation benchmarks, including HumanEval (Chen et al., 2021), HumanEval+ (Liu et al., 2023a) for Python code generation and BIRD-SQL dataset (Li et al., 2023a) for text-to-SQL generation. HumanEval (Chen et al., 2021) is a hand-written Python programming problem, which is further enhanced by HumanEval+ (Liu et al., 2023a) through the addition of more unit tests. BIRD-SQL (Li et al., 2023a) is a much more challenging dataset consisting of text-to-SQL tasks across 37 professional domains, derived from 95 databases with a total size of 33.4 GB.\nText summarization We conduct our experiments on two widely used text summarization datasets: DailyMail for short text summarization (Nallapati et al., 2016) and SummScreen for long text summarization (Chen et al., 2022). In the DailyMail dataset, each input is an ~ 800 words news article, and each reference output is a human-written summary of the article with ~ 55 words. In SummScreen, every input is a transcript of a TV show episode with ~ 5,600 words, and each reference output is a ~ 100 words human-written recap of the episode. We follow Nallapati et al. (2016) and measure ROUGE 1, ROUGE 2, and ROUGE-L which measure n-gram overlap with the reference summary, and we also measure BERTScore F1\u00b3 (Zhang et al., 2019).\nMathematical reasoning We introduce the widely used GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) datasets to verify the generalizability of the proposed method on tasks with answer of fixed form. GSM8K consists of 8,500 grade school math word problems, and MATH consists of 12,500 challenging mathematics problems from high school competitions."}, {"title": "4.1.2 Baselines", "content": "We compare FSC to the following self-consistency methods: (1) Random selects one answer randomly from multiple responses with temperature > 0; (2) UCS (Jain et al., 2023)\u2074 calculates the overlap of unigrams between candidates and then selects the final answer with highest value; (3) USC (Chen et al., 2023) utilizes LLMs to choose the most consistent one as the final answer. (4) SC (Wang et al., 2023) is the standard self-consistency decoding with answer extraction, which mitigates noise from individual sampling by employing a voting mechanism across multiple samples. Specifically, random select represents the performance of the LLM itself when the temperature > 0, while UCS and USC are two strong baselines for selection based self-consistency methods. Since the outputs of code generation and summarization tasks are in free-form, we evaluate SC on mathematical reasoning benchmarks where the final answers can be compared through exact match."}, {"title": "4.1.3 Implementation details", "content": "We conduct experiments using GPT-3.5-turbo and GPT-4 models. We set the temperature as 0.8 for both GPT-3.5-turbo (ChatGPT)\u2076 and GPT-4 (Achiam et al., 2023) models to generate 50 initial responses for all benchmarks. For summarization and python code generation, the initial samples are generated with zero-shot prompting, thus the output formats are diverse. For BIRD-SQL, we used the 1-shot chain-of-thought prompt following Li et al. (2023a), which improves the performance. Considering the cost of experiments, we randomly select 1,000 samples from the test splits of DailyMail and SummScreen respectively to form our text summarization benchmarks. We set the temperature of FSC and USC as 0 to ensure the reproducibility of the results. Unless otherwise specified, we set the default number of input responses as 5 for all baselines. All experiments are repeated five times and the average performance is reported."}, {"title": "4.2 Main results", "content": "4.2.1 Code generation\nAs shown in Table 2, we present the results on HumanEval, HumanEval+ and BIRD-SQL respectively. Besides the execution accuracy, we follow Li et al. (2023a) to also evaluate the valid efficiency score on BIRD-SQL, which measures the efficiency of the generated SQL queries. We show that FSC outperforms all baselines on execution accuracy by a significant margin across all datasets, while also generating more efficient SQL code."}, {"title": "4.2.2 Text summarization", "content": "Table 3 presents the results for summarization benchmarks. In both datasets FSC consistently improves over the baselines across all metrics, which demonstrates that FSC can improve performance in both short and long text summarization tasks simultaneously. Considering that LLMs demonstrate better consistency with humans in evaluation tasks (Liu et al., 2023b; Yuan et al., 2023), we employ GPT-4 as an evaluator to assess the generated summaries, following Liu et al. (2023b). As shown in Table 4, the experimental results indicate that FSC is superior to both UCS and USC."}, {"title": "4.2.3 Mathematical reasoning", "content": "As shown in Table 5, besides selection based self-consistency methods, we compare FSC against the standard self-consistency (SC). For SC, we employ a regular expression matching to extract the final answer on GSM8K, and reuse the answer parsing code from Li et al. (2023b) for MATH. Overall, FSC consistently improves over the selection based self-consistency methods UCS and USC, and the performance is generally comparable to SC, which needs answer parsing to perform the voting. surprisingly, FSC outperform SC on GPT-4, which demonstrates that FSC is not simply dependent on statistical measures of final reasoning answers, and its analysis and integration of various reasoning"}, {"title": "4.3 Effect of candidates filtering", "content": "As shown in Table 6, we compare the performance of FSC combined with candidates filtering strategy (denoted as FSC+Filter) with FSC itself and selection based self-consistency baselines. Specifically, FSC+Filter performs candidate filtering on the initial N responses to obtain $\\frac{N}{2}$ filtered responses, and then applies FSC to the filtered responses to get the final output. The results show that candidates filtering consistently improves FSC performance across all test benchmarks, indicates that candidates filtering obtains a higher-quality response candidate set through screening. On the other hand, the performance of FSC+Filter surpasses UCS and USC under the same number of response inputs on all test datasets (except for BertScore on DailyMail), demonstrating the superiority of FSC combined with candidates filtering strategy."}, {"title": "4.4 Effect of merge", "content": "We present the results of FSC combined with merge strategy in Table 7. The results show that the Merge strategy can significantly reduce the number of FSC input responses (20.0% on HumanEval and HumanEval+; 26.5% on Daily Mail), with minimal performance loss. Compared to saving costs by reducing the number of input responses (N =$\\frac{N}{4}$ for FSC), the Merge strategy saves more costs and maintains better performance (except for Rouge2 and RougeL on DailyMail), demonstrating its effectiveness. Furthermore, we combine FSC with both Filter and Merge strategies and achieve the best performance on HumanEval and HumanEval+, saving 40.0% of the cost. Besides, we save 53.2% of the cost on DailyMail with minimal performance drop. The results demonstrate the superiority of the combination of two proposed strategies."}, {"title": "4.5 Discussion", "content": "4.5.1 Different number of input responses\nAs shown in Figure 4, we examine the effect of using different numbers of responses (denoted as n) in FSC on GPT-4 model.\u2077 The results show that FSC consistently benefits from more input responses with n \u2264 8. However, the performance of FSC decreases with n = 16, which could be due to the difficulty in understanding long-context when the prompt includes a larger number of input responses, as well as the length constraint of LLMs. Nevertheless, we believe that using a limited number of input responses (e.g., 5) strikes an ideal balance between task accuracy and computational cost. In such case, FSC reliably enhances performance across all benchmarks.\n4.5.2 Robustness to noise in input responses"}, {"title": "4.5.3 Generalizability on open-source small model", "content": "As shown in Table 9, We validate the generalization of FSC on the open-source small model Mistral-7B-Instruct-v0.2\u2078 in code generation tasks. We set the temperature for baseline sampling to 0.2 and kept the rest of the implementation completely consistent with the main experiment. The experimental results indicate that FSC has the potential to work effectively on smaller models."}, {"title": "4.5.4 Segment-level consensus of FSC", "content": "To provide additional evidence that FSC can incorporate a higher level of segment-level consensus, we carry out quantitative experiments. For all the generated candidates, we construct a pool of 4-grams (as representations of segments), and then calculate the overlap between the 4-grams of the final sample and the pool. We compare our method"}, {"title": "5 Conclusion", "content": "In this work, we propose Fine-Grained Self-Consistency (FSC), which fully leverages the segment-level consensus knowledge extracted within multiple samples to overcome the coarse-grained limitations faced by selection based self-consistency methods. To improve performance and minimize costs, we further propose two strategies called candidates filtering and merge. Extensive experiments demonstrate that FSC notably boosts the performance on diverse range of tasks, exhibits superior robustness against noise in input responses, and can be generalized to those tasks where answer extraction is feasible through voting. Additional experiments confirm that the proposed candidates filtering and merge strategies can further enhance the performance of FSC while reducing the required computational cost."}, {"title": "Limitations", "content": "Despite the remarkable performance gain on variety of tasks, the current implementation of FSC still suffers from the following limitation: As illustrated in Section 4.5.1, While self-consistency can be applied to any number of samples, the number of samples supported by FSC is bounded by the context length of the underlying LLM. That is to say, FSC would be limited in tasks that require lengthy responses, such as story generation, long text translation, etc."}, {"title": "Ethics Statement", "content": "All of the datasets used in this study were publicly available, and no annotators were employed for our data collection. We confirm that the datasets we used did not contain any harmful content and was consistent with their intended use (research). We have cited the datasets and relevant works used in this study."}, {"title": "Acknowledgements", "content": "This work is supported by the Beijing Natural Science Foundation, China (Nos. 4222037, L181010). We thank the anonymous reviewers for their constructive comments."}, {"title": "A Appendix", "content": "A.1 Performance of FSC under different temperatures\nTo verify the generalizability of FSC under different temperature settings, we conduct experiments based on GPT-3.5-turbo under different temperature settings on HumanEval+ dataset. As shown in Table 11, the results show that FSC exhibits consistent generalization under different temperature settings, with more significant performance improvements compared to the baselines when the temperature is higher. We hypothesize that this could be due to the fact that as the temperature increases, the diversity of the samples also increases, thereby enriching the knowledge from the input samples that FSC is able to integrate.\nA.2 Sampling cost of FSC\nUnder the setting of input sample size N=10, we conduct a statistical analysis of the token cost on the HumanEval+ dataset based on GPT-3.5-turbo. As shown in Table 12, the results show that the token cost of FSC+filter+merge is comparable to that of USC, while FSC achieves a significant performance improvement.\nA.3 Case study of FSC\nTo gain a more intuitive understanding of the working mechanism of FSC, and to conduct a qualitative analysis of the consistency of FSC's final output, we present the case of FSC on HumanEval benchmark. Figure 5 shows the final output of FSC on HumanEval_130 when the input responses are all wrong. Specifically, for all error input responses, FSC incorporates consensus knowledge from each input and eliminates low-quality segments, ultimately recovering the correct solution. Our analysis indicates that FSC is capable of achieving fine-grained commonality extraction and obtaining outputs with better consistency compared to selection based self-consistency methods.\nA.4 Comparision with Minimum Bayes Risk Decoding (MBDR)\nSuzgun et al. (2023) propose MBDR method, achieving sample selection by calculating the BertScore between the generated samples. As shown in Table 13, we reproduce the MBDR according to the original paper, and compare it with FSC on our code generation benchmark.\nA.5 Input document for Figure 1\nInput: \"The wedding of the year in Scotland takes place on Saturday when British No 1 and two-time Grand Slam champion Andy Murray marries Kim Sears, his girlfriend of almost 10 years, in his hometown of Dunblane. Murray and Sears, both aged 27, met when the pair were teenagers during the US Open in 2005. Murray was playing in only his second Grand Slam tournament, while Sears was travelling with her tennis coach father Nigel. The couple got back together in 2010 following a brief split and after having to field constant questions over the years on when he would propose, their engagement was confirmed last November. Andy Murray kisses his new girlfriend Kim Sears in the crowd after winning his first ATP World Tour title in San Jose in February 2006. Murray pictured walking alongside Sears on the streets of London during the Wimbledon Championships in June 2006. Murray watches his brother Jamie in action at London's Queen's Club in June 2007 alongside Sears and mother Judy (left) Murray watches his brother in action at Wimbledon in 2007 alongside Sears and Carlos Mier (right), who will be one of Murray's three best men. Murray and Sears watching British boxer Amir Khan in action during a title fight at the ExCel Arena in London in February 2008. Murray and Sears attend the exhibition match held to mark the launch of the new Wimbledon Centre Court roof in May 2009. Murray and Sears attend a Burberry fashion show alongside Serena Williams (second left) and Sarah Jessica Parker (left) in September 2010. Murray was the best man"}]}