{"title": "TIMEMIXER++: A GENERAL TIME SERIES PATTERN MACHINE FOR UNIVERSAL PREDICTIVE ANALYSIS", "authors": ["Shiyu Wang", "Jiawei Li", "Xiaoming Shi", "Zhou Ye", "Baichuan Mo", "Wenze Lin", "Shengtong Ju", "Zhixuan Chu", "Ming Jin"], "abstract": "Time series analysis plays a critical role in numerous applications, supporting tasks such as forecasting, classification, anomaly detection, and imputation. In this work, we present the time series pattern machine (TSPM), a model designed to excel in a broad range of time series tasks through powerful representation and pattern extraction capabilities. Traditional time series models often struggle to capture universal patterns, limiting their effectiveness across diverse tasks. To address this, we define multiple scales in the time domain and various resolutions in the frequency domain, employing various mixing strategies to extract intricate, task-adaptive time series patterns. Specifically, we introduce TIMEMIXER++, a general-purpose TSPM that processes multi-scale time series using (1) multi-resolution time imaging (MRTI), (2) time image decomposition (TID), (3) multi-scale mixing (MCM), and (4) multi-resolution mixing (MRM) to extract comprehensive temporal patterns. MRTI transforms multi-scale time series into multi-resolution time images, capturing patterns across both temporal and frequency domains. TID leverages dual-axis attention to extract seasonal and trend patterns, while MCM hierarchically aggregates these patterns across scales. MRM adaptively integrates all representations across resolutions. TIMEMIXER++ achieves state-of-the-art performance across 8 time series analytical tasks, consistently surpassing both general-purpose and task-specific models. Our work marks a promising step toward the next generation of TSPMs, paving the way for further advancements in time series analysis.", "sections": [{"title": "1 INTRODUCTION", "content": "Time series analysis is crucial for identifying and predicting temporal patterns across various domains, including weather forecasting (Bi et al., 2023), medical symptom classification (Kiyasseh et al., 2021), anomaly detection in spacecraft monitoring (Xu, 2021), and imputing missing data in wearable sensors (Wu et al., 2020). These diverse applications highlight the versatility and importance of time series analysis in addressing real-world challenges. A key advancement in this field is the development of time series pattern machines (TSPMs), which aim to create a unified model architecture capable of handling a broad range of time series tasks across domains (Zhou et al., 2023; Wu et al., 2023).\nAt the core of TSPMs is their ability to recognize and generalize time series patterns inherent in time series data, enabling the model to uncover meaningful temporal structures and adapt to varying time series task scenarios. A line of research (Lai et al., 2018b; Zhao et al., 2017) has utilized recurrent neural networks (RNNs) to capture sequential patterns. However, these methods often struggle to capture long-term dependencies due to limitations like Markovian assumptions and inefficiencies."}, {"title": "2 RELATED WORK", "content": "Time Series Analysis. A pivotal aspect of time series analysis is the ability to extract diverse patterns from various time series while building powerful representations. This challenge has been explored across various model architectures. Traditional models like ARIMA (Anderson & Kendall, 1976) and STL (Cleveland et al., 1990) are effective for periodic and trend patterns but struggle with non-linear dynamics. Deep learning models, such as those by (Lai et al., 2018b) and (Zhao et al., 2017), capture sequential dependencies but face limitations with long-term dependencies. TCNs (Franceschi et al., 2019) improve local pattern extraction but are limited in capturing long-range dependencies. TimesNet (Wu et al., 2023) enhances long-range pattern extraction by treating time series as 2D signals, while MLP-based methods (Zeng et al., 2023; Ekambaram et al., 2023) offer simplicity and effectiveness. Transformer-based models like PatchTST (Nie et al., 2023) and iTransformer (Liu et al., 2024) leverage self-attention to model long-range dependencies, demonstrating good forecasting performance. Given the strengths and limitations discussed above, there is a growing need for a TSPM capable of effectively extracting diverse patterns, adapting to various time series analytical tasks, and possessing strong generalization capabilities. As illustrated in Figure 1, TIMEMIXER++ meets this requirement by constructing robust representational capabilities, thereby demonstrating its potential for universal time series analysis.\nHierarchical Time Series Modeling. Numerous methodologies have been advanced utilizing specialized deep learning architectures for time series analysis, with an emphasis on the decomposition and integration of temporal patterns. For example, several studies (Wu et al., 2021; Wang et al., 2024; Zhou et al., 2022b; Wang et al., 2024) utilize moving averages to discern seasonal and trend components, which are subsequently modeled using attention mechanisms (Wu et al., 2021; Zhou et al., 2022b), convolutional networks (Wang et al., 2023), or hierarchical MLP layers (Wang et al., 2024). These components are individually processed prior to aggregation to yield the final output. Nonetheless, such approaches frequently depend on predefined and rigid operations for the disentanglement of seasonality and trends, thereby constraining their adaptability to complex and dynamic patterns. In contrast, as depicted in Figure 2, we propose a more flexible methodology that disentangles seasonality and trend directly within the latent space via dual-axis attention, thereby enhancing adaptability to a diverse range of time series patterns and task scenarios. Furthermore, by adopting a multi-scale, multi-resolution analytical framework (Mozer, 1991; Harti, 1993), we facilitate hierarchical interaction and integration across different scales and resolutions, substantially enhancing the effectiveness of time series modeling."}, {"title": "3 \u03a4\u0399\u039c\u0395\u039cIXER++", "content": "Building on the multi-scale and multi-periodic characteristics of time series, we introduce TIMEMIXER++, a general-purpose time series pattern machine that processes multi-scale time series using an encoder-only architecture, as shown in Figure 2. The architecture generally comprises three components: (1) input projection, (2) a stack of Mixerblocks, and (3) output projection."}, {"title": "3.1 STRUCTURE OVERVIEW", "content": "Multi-scale Time Series. We approach time series analysis using a multi-scale framework. Given an input time series $x_0 \\in \\mathbb{R}^{T \\times C}$, where T represents the sequence length and C the number of variables, we generate a multi-scale representation through downsampling. Specifically, the input time series $x_0$ is progressively downsampled across M scales using convolution operations with a stride of $2^1$, producing the multi-scale set $X_{init} = {x_0,\\ldots,x_M}$, where $x_m \\in \\mathbb{R}^{[\\frac{T}{2^m}] \\times C}$. The downsampling process follows the recursive relationship:\n$x_m = Conv(x_{m-1}, stride = 2), m\\in {1,\\ldots, M}.$\nInput Projection. Previous studies (2023; 2023) employ a channel-independence strategy to avoid projecting multiple variables into indistinguishable channels (Liu et al., 2024). In contrast, we adopt channel mixing to capture cross-variable interactions, which are crucial for revealing comprehensive patterns in time series data. The input projection has two components: channel mixing and embedding. We first apply self-attention to the variate dimensions at the coarsest scale $x_M \\in \\mathbb{R}^{[\\frac{T}{2^M}] \\times C}$, as it retains the most global context, facilitating the more effective integration of information across variables. This is formulated as follows:\n$X_M = Channel-Attn(Q_M, K_M, V_M),$\nwhere Channel-Attn denotes the variate-wise self-attention for channel mixing. The queries, keys, and values $Q_M, K_M,V_M \\in \\mathbb{R}^{C \\times [\\frac{T}{2^M}]}$ are derived from linear projections of $x_M$. Then, we embed all multi-scale time series into a deep pattern set $X^0$ using an embedding layer, which can be expressed as $X^0 = {x^0,\\ldots, x^0_M} = Embed(X_{init})$, where $x^0_m \\in \\mathbb{R}^{[\\frac{T}{2^m}] \\times d_{model}}$ and $d_{model}$ represents the dimensionality of the deep patterns.\nMixerBlocks. Next, we apply a stack of L Mixerblocks with the goal to capture intricate patterns across scales in the time domain and resolutions in the frequency domain. Within the MixerBlocks, we convert multi-scale time series into multi-resolution time images, disentangle seasonal and"}, {"title": "3.2 MIXERBLOCK", "content": "We organize a stack of MixerBlocks in a residual way. For the (l + 1)-th block, the input is the multi-scale representation set $X^l$, and the forward propagation can be formalized as:\n$X^{l+1} = LayerNorm(X^l + MixerBlock(X^l)),$\nwhere LayerNorm normalizes patterns across scales and can stabilize the training. Time series exhibits complex multi-scale and multi-periodic dynamics. Multi-resolution analysis (Harti, 1993) models time series as a composite of various periodic components in the frequency domain. We introduce multi-resolution time images, converting 1D multi-scale time series into 2D images based on frequency analysis while preserving the original data. This captures intricate patterns across time and frequency domains, enabling efficient use of convolution methods for extracting temporal patterns and enhancing versatility across tasks. Specifically, we processes multi-scale time series using (1) multi-resolution time imaging (MRTI), (2) time image decomposition (TID), (3) multi-scale mixing (MCM), and (4) multi-resolution mixing (MRM) to uncover comprehensive time series patterns.\nMulti-Resolution Time Imaging. At the start of each MixerBlock, we convert the input $X^l$ into $(M + 1) \\times K$ multi-resolution time images via frequency analysis (Wu et al., 2023). To capture representative periodic patterns, we first identify periods from the coarsest scale $X_M^l$, which enables global interaction. Specifically, we apply the fast fourier transform (FFT) on $x_M^l$ and select the top-K frequencies with the highest amplitudes:\n$A, {f_1,\\ldots, f_K}, {p_1,\\ldots,p_K} = FFT(x_M^l),$\nwhere $A = {A_{f_1},\\ldots, A_{f_K} }$ represents the unnormalized amplitudes, ${f_1,\\ldots, f_K}$ are the top-K frequencies, and $p_k = \\frac{T}{f_k}, k \\in {1, ..., K}$ denotes the corresponding period lengths. Each 1D time series $x_m^l$ is then reshaped into K 2D images as follows:\n$MRTI(X^l) = {\\{Z_{m}^{l,k}\\}_{k=1}^{K}\\}_{m=0}^{M} = {\\{z_{m}^{(l,k)} \\}}_{1D \\rightarrow 2D}, $\nTime Image Decomposition. Time series patterns are inherently nested, with overlapping scales and periods. For example, weekly sales data reflects both daily shopping habits and broader seasonal trends. Conventional methods (Wu et al., 2021; Wang et al., 2024) use moving averages across the entire series, often blurring distinct patterns. To address this, we utilize multi-resolution time"}, {"title": "4 EXPERIMENTS", "content": "To verify the effectiveness of the proposed TIMEMIXER++ as a general time series pattern machine, we perform extensive experiments across 8 well-established analytical tasks, including (1) long-term forecasting, (2) univariate and (3) multivariate short-term forecasting, (4) imputation, (5) classification, (6) anomaly detection, as well as (7) few-shot and (8) zero-shot forecasting. Overall, as summarized in Figure 1, TIMEMIXER++ consistently surpasses contemporary state-of-the-art models in a range of critical time series analysis tasks, which is demonstrated by its superior performance across 30 well-known benchmarks and against 27 advanced baselines. The detailed experimental configurations and implementations are in Appendix A."}, {"title": "4.1 MAIN RESULTS", "content": "4.1.1 LONG-TERM FORECASTING\nSetups. Long-term forecasting is pivotal for strategic planning in areas such as weather prediction, traffic management, and energy utilization. To comprehensively assess our model's effectiveness over extended periods, we perform experiments on 8 widely-used real-world datasets, including the four subsets of the ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2), as well as Weather, Solar-Energy, Electricity, and Traffic, consistent with prior benchmarks set by Zhou et al. (2021b); Wu et al. (2021); Liu et al. (2022a)\n4.1.2 UNIVARIATE SHORT-TERM FORECASTING\nSetups. Univariate short-term forecasting is crucial for demand planning and marketing. We evaluate our model using the M4 Competition dataset Makridakis et al. (2018), comprising 100, 000 marketing time series with six frequencies from hourly to yearly, enabling comprehensive assessment across varied temporal resolutions.\n4.1.3 MULTIVARIATE SHORT-TERM FORECASTING\nSetups. We further evaluate the short-term forecasting performance in multivariate settings on the PeMS benchmark (Chen et al., 2001), which includes four publicly available high-dimensional traffic network datasets: PEMS03, PEMS04, PEMS07, and PEMS08. These datasets feature a large number"}, {"title": "4.1.4 IMPUTATION", "content": "Setups. Accurate imputation of missing values is crucial in time series analysis, affecting predictive models in real-world applications. To evaluate our model's imputation capabilities, we use datasets from electricity and weather domains, selecting ETT (Zhou et al. (2021b)), Electricity (UCI), and Weather (Wetterstation) as benchmarks."}, {"title": "4.1.5 FEW-SHOT FORECASTING", "content": "Setups. Transformer-based models excel in various forecasting scenarios, especially with limited data. To evaluate their transferability and pattern recognition, we test across 6 diverse datasets, training each model on only 10% of available timesteps. This approach assesses adaptability to sparse data and the ability to discern general patterns, which is crucial for real-world predictive analysis where data is often limited."}, {"title": "4.1.6 ZERO-SHOT FORECASTING", "content": "Setups. We explore zero-shot learning to evaluate models' ability to generalize across different contexts. As shown in Table 6, models trained on dataset $D_a$ are evaluated on unseen dataset $D_b$ without further training. This direct transfer ($D_a \\rightarrow D_b$) tests models' adaptability and predictive robustness across disparate datasets."}, {"title": "4.2 CLASSIFICATION AND ANOMALY DETECTION", "content": "Setups. Classification and anomaly detection test models' ability to capture coarse and fine-grained patterns in time series. We use 10 multivariate datasets from UEA Time Series Classification Archive (2018) for classification. For anomaly detection, we evaluate on SMD (2019), SWaT (2016), PSM (2021), MSL, and SMAP (2018)."}, {"title": "4.3 MODEL ANALYSIS", "content": "Ablation Study. To verify the effectiveness of each component of TIMEMIXER++, we conducted an ablation study by removing individual components (w/o). The results are in Table 7. TIMEMIXER++ with all components\u2014channel mixing, time image decompose, multi-scale mixing, and multi-resolution mixing-achieves the best performance. On datasets like ECL, Traffic, and Solar, channel-mixing improves performance by 5.36%. Time image decomposition yields an 8.81% improvement, especially on seasonal datasets like ECL and Traffic. Multi-scale mixing provides a 6.25% improvement, particularly for less predictable datasets like ETT. Multi-resolution mixing adds a 5.10% improvement, highlighting the importance of a multi-resolution hybrid ensemble. We provide more ablation studies in Appednix C.\nRepresentation Analysis. Our analysis, depicted in Figure 4, present the original, seasonality, and trend images across two scales and three resolutions (periods: 12,8,6; frequencies: 16, 24, 32). \u03a4IMEMIXER++ demonstrates efficacy in the separation of distinct seasonality and trends, precisely capturing multi-periodicities and time-varying trends. Notably, the periodic characteristics vary across different scales and resolutions. This hierarchical structure permits the simultaneous capture of these features, underscoring the robust representational capabilities of TIMEMIXER++ as a pattern machine."}, {"title": "5 CONCLUSION", "content": "In this paper, we present TIMEMIXER++, a novel framework designed as a universal time series pattern machine for predictive analysis. By leveraging multi-resolution imaging, we construct time images at various resolutions, enabling enhanced representation of temporal dynamics. The use of dual-axis attention allows for effective decomposition of these time images, disentangling seasonal and trend components within deep representations. With multi-scale and multi-resolution mixing techniques, TIMEMIXER++ seamlessly fuses and extracts information across different hierarchical levels, demonstrating strong representational capabilities. Through extensive experiments and comprehensive evaluations, TIMEMIXER++ consistently outperforms existing general-purpose and task-specific time series models, establishing itself as a state-of-the-art solution with significant potential for broad applications in time series analysis. Limitations and directions for future research are discussed in Appendix K."}, {"title": "B DETAILS OF MODEL DESIGN", "content": "In this section, we present a comprehensive exposition of our model design, encompassing five key components: channel mixing and embedding (input projection), multi-resolution time imaging, time image decomposition, multi-scale mixing, and multi-resolution mixing. To enhance comprehension, we provide visual illustrations that afford an intuitive understanding of our structural design.\nChannel Mixing and Embedding. We employ a channel mixing approach to effectively capture inter-variable dependencies crucial for uncovering rich temporal patterns. Our method first applies variate-wise self-attention, as formulated in Equation 2, at the coarsest temporal scale $x_M \\in \\mathbb{R}^{\u25142^M\u300d\\times C}$, ensuring the preservation of global context. This mechanism fuses information across variables, enabling the extraction of inter-variable patterns. Subsequently, the multivariate time series is projected into an embedding space via the function $Embed(\u00b7) : \\mathbb{R}^{L\u300d\\times C} \\rightarrow \\mathbb{R}^{[2^M] \\times d_{model}}$, capturing temporal structure at different scales and facilitating comprehensive pattern learning across the input time series."}, {"title": "C ADDITIONAL ABLATION STUDIES", "content": "To verify the effectiveness of each component of TIMEMIXER++, we conducted a detailed ablation study by performing experiments that remove individual components (w/o) across various tasks, including univariate short-term forecasting, multivariate short-term forecasting, and anomaly detection. Based on the Table 11 12 provided, the conclusions are as follows: TIMEMIXER++ outperforms other configurations in short-term forecasting with the lowest average SMAPE and MAPE scores, indicating the importance of each component. As for PEMS datasets with large variable dimensions, the most significant improvement is observed with channel mixing, showing a 14.95% improvement. In anomaly detection, TIMEMIXER++ achieves the highest average F1 score, with time image decomposition contributing the most to performance, showing a 9.8% improvement. Other components like channel mixing, multi-scale mixing, and multi-resolution mixing also enhance performance. Overall, each component plays a crucial role in the effectiveness of TIMEMIXER++ for all tasks."}, {"title": "D ADDITIONAL REPRESENTATION ANALYSIS", "content": "To evaluate the representational capabilities of TIMEMIXER++, we selected three datasets: Traffic, Electricity, and ETTm1, each exhibiting distinct periodic and trend characteristics. We conducted comprehensive analyses across various scales and resolutions. Initially, 1D convolution was employed to downsample the original time series, yielding different scales. Subsequently, frequency spectrum analysis was utilized to identify the primary frequency components within the time series, selecting the top three components by magnitude as the primary resolutions. This process transformed the original time series into a multi-scale time image. Time image decomposition was then applied to disentangle seasonality and trends from the original time image, resulting in distinct seasonality and trend images. Hierarchical mixing was performed to facilitate interactions across different scales. The visualization of the resulting representations is depicted in Figures 10, 11, and 12."}, {"title": "E EFFICIENCY ANALYSIS", "content": "We comprehensively compare the forecasting and imputation in performance, training speed, and memory footprint of the following models: TIMEMIXER++, iTransformer(Liu et al., 2023), PatchTST(Nie et al., 2023), TimeMixer(Wang et al., 2024), TIDE(Das et al., 2023a), Fedformer(Zhou et al., 2022b), TimesNet(Wu et al., 2023), MICN(Wang et al., 2023), and SCINet(Liu et al., 2022a).\nAs illustrated in Figure 13 for imputation in Weather and long-term forecasting in ETTm1, TIMEMIXER++ demonstrates a balance between low memory footprint and training time while achieving competitive MSE scores. In the Weather imputation task, TIMEMIXER++ has a smaller memory footprint (2.9GB) and moderate training time (395ms) compared to other models like FEDformer, which has higher memory usage (9.8GB) and longer training time (812ms). For long-term forecasting, TIMEMIXER++ also shows efficiency with the lowest memory footprint (2.1GB) and competitive training time (274ms), while maintaining a low MSE. To be further detailed, TIMEMIXER++ demonstrates efficiency by transforming 1D time series data into 2D images. This transformation allows the use of parameter-efficient inception block 2D convolutions, which not only enhance computational efficiency but also effectively capture long-range dependencies that are"}, {"title": "F HYPERPARAMTER SENSITIVITY", "content": "We conduct a hyperparameter sensitivity analysis focusing on the four important hyperparameters within TIMEMIXER++: namely, the number of scales M, the number of layers L, the time series input length T, and the selection of the top K periods with the highest amplitudes in the spectrogram. The related findings are presented in Figure 14. Based on our analysis, we have made the following observations: (1) As the number of scales increases, the MSE generally decreases. Increasing the number of scales benefits model performance across all prediction lengths, with noticeable improvements observed between 3 and 4 scales. Considering overall performance and efficiency, the marginal benefits of increasing M significantly diminish, so we set M to 3. (2) Adding more layers typically reduces MSE, particularly between 1 and 2 layers where the change is most pronounced."}, {"title": "G ERROR BARS", "content": "In this paper, we repeat all the experiments three times. Here we report the standard deviation of our model and the second best model, as well as the statistical significance test in Table 13, 14, 15."}]}