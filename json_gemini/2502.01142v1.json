{"title": "DeepRAG: Thinking to Retrieval Step by Step for Large Language Models", "authors": ["Xinyan Guan", "Jiali Zeng", "Fandong Meng", "Chunlei Xin", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Le Sun", "Jie Zhou"], "abstract": "Large Language Models (LLMs) have shown remarkable potential in reasoning while they still suffer from severe factual hallucinations due to timeliness, accuracy, and coverage of parametric knowledge. Meanwhile, integrating reasoning with retrieval-augmented generation (RAG) remains challenging due to ineffective task decomposition and redundant retrieval, which can introduce noise and degrade response quality. In this paper, we propose DeepRAG, a framework that models retrieval-augmented reasoning as a Markov Decision Process (MDP), enabling strategic and adaptive retrieval. By iteratively decomposing queries, DeepRAG dynamically determines whether to retrieve external knowledge or rely on parametric reasoning at each step. Experiments show that DeepRAG improves retrieval efficiency while improving answer accuracy by 21.99%, demonstrating its effectiveness in optimizing retrieval-augmented reasoning.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated significant potential in reasoning (Plaat et al., 2024). However, limited by the capacity and capabilities of LLM, it still suffers from severe factual hallucination problems due to the timeliness, accuracy, and coverage of parametric knowledge (Zhang et al., 2023; Huang et al., 2023). Retrieval-Augmented Generation (RAG) has been proposed as a promising paradigm to address this issue by integrating relevant information from knowledge bases or search engines, thereby improving the factuality of model response (Zhao et al., 2024). However, incorporating reasoning with retrieval-augmented generation still presents several challenges. One major issue is that complex queries often require multi-step decomposition to establish a coherent reasoning process (Radhakrishnan et al., 2023). Iterative retrieval has been proposed as a solution to continuously update retrieval results to address the dynamic information needs that arise during the generation process (Yue et al., 2024). However, LLMs often struggle to generate atomic and precise subqueries, which are critical for more effective retrieval (Wu et al., 2024). From the perspective of RAG, iterative retrieval should ideally generate the next atomic query based on the current question and the available information in an adaptive manner. Moreover, retrieval is not always necessary. Some queries require knowledge, while others rely solely on reasoning within the LLM. Furthermore, LLMs have demonstrated their capability to serve as knowledge bases themselves (Petroni et al., 2019). Unnecessary retrieval, in addition to being redundant, can introduce noise, degrade generation quality, and increase inference latency (Chen et al., 2023; Tan et al., 2024; Bian et al., 2024). To address this, inspired by the way humans search the Internet based on demand, we propose"}, {"title": "2 Related Work", "content": "Adaptive Retrieval-Augmented Generation Existing adaptive RAG approaches can be broadly categorized into three types: classifier-based methods (Cheng et al., 2024; Jeong et al., 2024) requiring additional linear head training for retrieval decisions, confidence-based methods (Jiang et al., 2023; Su et al., 2024; Dhole, 2025) relying heavily on threshold-dependent uncertainty metrics, and LLM-based methods (Asai et al., 2023; Zhang et al., 2024) generating retrieval decisions but often fail to accurately recognize their knowledge boundaries, making it unreliable to delegate retrieval timing decisions to the model. Our method leverages the inherent generative capabilities of LLMs to explore knowledge boundaries in RAG settings. This design maintains the model's native generation abilities while eliminating the need for additional parameters or unreliable uncertainty metrics.\nReasoning in Retrieval-Augmented Generation Recent advances in RAG have increasingly focused on incorporating reasoning capabilities. Self-RAG (Asai et al., 2023) and Auto-RAG (Yu et al., 2024) leverage automatic data synthesis to enhance reasoning within retrieval-augmented frameworks. Search-o1 (Li et al., 2025) incorporates retrieval into inference to construct an agentic system, though its applicability is limited to o1-like large reasoning models. AirRAG (Feng et al., 2025) combines Monte Carlo Tree Search and self-consistency. In contrast to these approaches that rely heavily on extensive retrieval operations or large reasoning models, DeepRAG provides an end-to-end method, enabling an arbitrary model to think to retrieval step by step on demand.\nKnowledge Boundary LLMs struggle to accurately distinguish between what they know and what they don't know (Yin et al., 2023; Kapoor et al., 2024a; Yin et al., 2024). Additional fine-tuning (Kapoor et al., 2024b) or precise probing (Cheng et al., 2024) is typically required to calibrate the model's cognition. Our approach explores knowledge boundaries in RAG settings."}, {"title": "3 Thinking to Retrieval Step by Step", "content": "In this section, we introduce our proposed method DeepRAG. At its core, DeepRAG treats the process of question decomposition, atomic decisions, and final answer generation as a Markov Decision Process (MDP). As shown in Figure 2, our framework comprises three key steps: 1) Binary Tree Search, which constructs a binary tree for each subquery related to the given question, exploring paths based on either parametric knowledge or external knowledge base; 2) Imitation Learning, which extracts the reasoning process that arrives at the correct final answer with minimum retrieval cost for imitation learning; 3) Chain of Calibration, which"}, {"title": "3.1 Overview of the MDP Modeling", "content": "We formalize the step by step reasoning process for retrieval-augmented generation as a Markov Decision Process (MDP) defined by the tuple (S, A, P, R), which comprises a set of states S, actions A, transition dynamics P, and a reward function R.\nStates. At each step t, the state $s_t \\in S$ represents the partial solution to the original question. We denote $s_t = [x, (q_{1}, r_{1}), \\dots, (q_{t}, r_{t})]$, where x is the input question, and $(q_i, r_i)$ captures the i-th subquery along with the intermediate answer (and any retrieved documents).\nActions. At state $s_t$, the model selects an action $a_{t+1} = (\\sigma_{t+1}, \\delta_{t+1}) \\in A$, which consists of two sub-decisions:\n1. Termination decision: Given the partial solution $s_t$, the model makes a binary decision $\\sigma_{t+1} \\in {continue, terminate}$ to determine whether to proceed with generating the next subquery $q_{t+1}$ or finalize the answer o.\n2. Atomic decision: For each subquery $q_{t+1}$,"}, {"title": "3.2 Binary Tree Search", "content": "In Section 3.1, we model the step-by-step reasoning process as a Markov decision process, where the LLM iteratively decomposes a given question into subqueries, each derived from previously acquired information. The detailed generation instruction is"}, {"title": "3.3 Imitation Learning", "content": "In this section, we present an algorithm that leverages binary trees to identify the optimal reasoning process that leads to the correct final answer while minimizing retrieval costs, corresponding to the highest reward as defined in Section 3.1. Based on the synthesized optimal reasoning data, we fine-tune the model to improve its termination and atomic decisions while enhancing its query decomposition capabilities and generating faithful intermediate answers, thereby enabling a more comprehensive and coherent retrieval narrative process."}, {"title": "Training Objective", "content": "Specifically, we implement a masked loss function for the retrieved documents to prevent the model from learning irrelevant or noisy text that could negatively impact its performance. In this way, we hope the model to enhance the ability to decompose subqueries and retrieve them based on demand. For each instance, the loss function is formulated as follows:\n$L = - \\sum_{1<i<n} log [Pr(q_i | S_{i-1}) + Pr(a_i | S_{i-1}, q_i, d_i)]$\n$d_i$ refers to null if there is no reieval for ith reasoning step, n refers to the total iteration."}, {"title": "3.4 Chain of Calibration", "content": "Building on the Markov process in Section 3.1, we identify four key optimization aspects for DeepRAG: termination and atomic decisions, query decomposition, and intermediate answer generation. Unlike the others, atomic decisions require the model to recognize its own knowledge boundaries to make precise judgments.\nWe propose a method that dynamically optimizes atomic decisions for each subquery, rather than training LLMs on complete reasoning paths. Our approach consists of two key components: (1) synthesizing preference data to determine when retrieval is necessary, and (2) fine-tuning the LLM with this data using Chain of Calibration training to enhance its ability to make informed atomic decisions based on its internal knowledge boundaries."}, {"title": "4 Experiment", "content": "We use five open-domain QA datasets for our experiments. We split the datasets used for training our models as the in-distribution dataset, while those not used for training are considered the out-of-distribution dataset. The in-distribution datasets include HotpotQA (Yang et al., 2018), and 2WikiMultihopQA (2WMQA) (Ho et al., 2020), and the"}, {"title": "4.2 Baselines", "content": "We use the following baselines to evaluate the performance: CoT (Wei et al., 2022) and CoT*, which employ 8-shot examples extracted from the training dataset. The asterisk (*) indicates that the model output was trained using the same data employed for training the DeepRAG. CoT-Retrieve and CoT-Retrieve* augment the eight examples in the context with retrieved relevant documents based on the query. IterDRAG (Yue et al., 2024) refers to decomposing question and answer step by step based on in-context learning. UAR (Cheng et al., 2024) employs a trained classifier to determine when retrieval is necessary. FLARE (Jiang et al., 2023) and DRAGIN (Su et al., 2024) are confidence-based method that decide the timing of retrieval based on token importance and uncertainty. TAARE (Zhang et al., 2024) allows the LLM itself to determine when retrieval is needed. AutoRAG (Yu et al., 2024) uses trained models to iteratively decompose questions and retrieve relevant documents for answering."}, {"title": "4.3 Implementation Details", "content": "We construct training datasets using the training subsets of 2 QA datasets: HotpotQA, and 2WMQA. For imitation learning, we randomly sampled 4,000 data from HotpotQA, and 2WMQA respectively. For chain of calibration, we individually sampled 1,000 data points from each of the two datasets. We evaluate our method on the corresponding test sets of these datasets with Exact Match (EM) and F1 score as evaluation metrics. Following Su et al. (2024), we adopt BM25 as our retrieval model. For the external knowledge corpus, we utilize Wikipedia\u00b9, with each article segmented into 100-token passages. We selected Llama-3-8B-Instruct (Dubey et al., 2024) and Qwen-2.5-7B (Yang et al., 2024) as our base model."}, {"title": "4.4 Overall Results", "content": "We evaluate DeepRAG on two in-distribution datasets and three out-of-distribution datasets. The results in Table 1 demonstrate DeepRAG's superior performance and robustness across different scenarios.\nOur method demonstrates superior performance across most datasets via thinking to retrieval step by step. Our method consistently outperforms existing approaches by enabling step-by-step retrieval reasoning. Compared to reasoning-based and adaptive RAG baselines, DeepRAG achieves improvements across all datasets, demonstrating the effectiveness of the structured retrieval narrative and its reliable, on-demand atomic decisions. Specifically, the poor performance of Iter-DRAG highlights the necessity of learning both query decomposition and faithful answering. In contrast, confidence-based methods like FLARE struggle to determine the optimal retrieval timing due to their reliance on unstable, predefined metrics. Moreover, we observe that such confidence-based methods suffer from instability, as their performance is highly sensitive to threshold selection. Meanwhile, iterative retrieval methods like Auto-RAG often fall into continuous retrieval loops when no highly relevant information is found. It is worth noting that the CoT-Retrieve method outperforms on CAG. We attribute this to the fact that CAG consists of straightforward, one-hop questions, where direct question-relevant retrieval proves more effective.\nOur DeepRAG approach exhibits remarkable generalization capabilities and robustness in time-sensitive and out-of-distribution settings. In the time-sensitive dataset CAG, DeepRAG performs well compared to other adaptive retrieval methods. Furthermore, DeepRAG achieves substantial F1 score improvements of 2.63 and 4.57 on PopQA and WebQuestions respectively, even in scenarios where relevant information may be sparse or missing from the knowledge base.\nBy learning from self-synthesized data, DeepRAG effectively explores knowledge boundaries while minimizing hallucination risks. We observe that TAARE often underperforms direct re-"}, {"title": "5 Analysis", "content": "5.1 Retrieval Efficiency\nTo demonstrate the efficiency of our method, we compare the average number of retrievals on 2Wiki-MultihopQA and WebQuestions. As shown in Table 2, We have following observations: 1) DeepRAG can achieve higher accuracy with relatively lower retrieval costs, attributed to its dynamic usage of internal knowledge. 2) Confidence-based approaches demonstrate limited robustness across datasets. For instance, both FLARE and DRAGIN methods doesn't trigger retrieval under the default confidence threshold in WQ. 3) Iterative retrieval-based approaches typically require numerous retrieval operations. Therefore, efficient adaptive retrieval methods like DeepRAG become crucial for optimizing resource utilization while maintaining performance.\n5.2 Relevance to Parametric Knowledge\nIn this section, we investigate the relationship between retrieval needs and internal knowledge to demonstrate how effectively atomic decisions explores the knowledge boundary. The detail setting are shown in Appendix B.2. We report four metrics. F1 score and Accuracy serve as basic performance measures, while balanced accuracy and Matthews Correlation Coefficient(MCC) (contributors, 2025) are employed to account for the class imbalance between retrieval-required and retrieval-not-required cases.\nAs shown in Table 3, we find that: 1) DeepRAG demonstrates superior relevance performance across F1, balanced accuracy, and MCC metrics. This suggests that DeepRAG successfully identifies retrieval necessity by exploring knowledge boundary; 2) While FLARE, DRAGIN, and TAARE exhibit high accuracy scores, their relatively low balanced accuracy and MCC scores suggest they mainly succeed in retrieval-required cases but struggle to properly avoid unnecessary retrievals."}, {"title": "5.3 Different Inference Strategy", "content": "To gain a deep insight into the effectiveness of retrieval narartive, we evaluate DeepRAG's performance under two extreme scenarios: relying solely on internal knowledge and using retrieval in each subquery. As shown in Figure 5, depending solely on internal knowledge yields poor performance, while relying entirely on external knowledge achieves relatively higher accuracy but incurs substantial retrieval costs. In contrast, DeepRAG achieves superior performance by adaptively selecting between internal and external knowledge sources. Specifically, DeepRAG outperforms the retrieve only approach. This may be attributed to the fact that in certain scenarios, retrieval can"}, {"title": "5.4 Question Decomposition Effectiveness", "content": "We systematically analyze the effectiveness of question decomposition in retrieval narrative. As shown in Figure 3, we present the distribution of subquery counts and retrieval attempts for different questions. Most questions require 3-5 decomposition steps, while retrieval attempts are primarily concentrated within 0-2 rounds. This demonstrates that DeepRAG effectively decomposes questions while minimizing redundant retrieval."}, {"title": "5.5 Ablation Study", "content": "In this section, we conducted experiments to validate the effectiveness of DeepRAG's data construction and training process.\nImitation Learning We compare our default strategy of selecting paths with minimal retrieval cost against two alternative approaches: maximum retrieval cost and random path selection. As shown"}, {"title": "Chain of Calibration", "content": "We compare our default approach of constructing preferences based on nodes from optimal paths against two alternatives: constructing pairs for all nodes and constructing sentence-level partial order pairs based on retrieval efficiency. As shown in Table 5, DeepRAG demonstrates significant advantages over both variants. Specifically, as illustrated in Figure 6(b), DeepRAG achieves lower retrieval costs while maintaining higher average performance. In contrast, the sentence-level partial order pairs learned incorrect preferences, resulting in over-reliance on internal knowledge and consequently leading to both low retrieval costs and poor performance."}, {"title": "5.6 Performance against Strong Baseline Models", "content": "In this section, we compare DeepRAG with recent strong baseline models. Specifically, we select two state-of-the-art open-source models: QwQ-32B-preview (Team, 2024) and gpt-4o-turbo (OpenAI). As shown in Table 6, by leveraging external knowledge bases through dynamic cognitive decision-making, DeepRAG achieves superior average per-"}, {"title": "5.7 Case Study", "content": "As illustrated in Figure 7, we conduct a case study comparing DeepRAG with Auto-RAG (Yu et al., 2024), a closely related method that utilizes iterative retrieval for retrieval-augmented generation. For each subquery, Auto-RAG retrieves relevant documents and generates a corresponding subanswer. This approach is not only time-consuming but also fails when no relevant documents are retrieved. Although Auto-RAG attempts to address"}, {"title": "6 Conclusion", "content": "In this paper, we present DeepRAG, a simple yet effective approach that enhances LLM's awareness of retrieval requirements through self-calibration. Our method decomposes queries into subqueries and uses binary tree search for data synthesis to help models better understand their knowledge boundaries. Experimental results across various QA tasks demonstrate that DeepRAG significantly improves the accuracy and efficiency of retrieval-augmented generation."}, {"title": "A Templates", "content": "A.1 DeepRAG Construct Instruction\nInstruction: You are a helpful Retrieve-Augmented Generation (RAG) model. Your task is to answer questions by logically decomposing them into clear sub-questions and iteratively addressing each one.\nUse \"Follow up:\" to introduce each sub-question and \"Intermediate answer:\" to provide answers.\nFor each sub-question, decide whether you can provide a direct answer or if additional information is required. If additional information is needed, state, \"Let's search the question in Wikipedia.\" and then use the retrieved information to respond comprehensively. If a direct answer is possible, provide it immediately without searching."}, {"title": "B Detailed Analysis", "content": "B.1 Retrieval Efficiency\nTo demonstrate the efficiency of our method, we compare the average number of retrievals on 2Wiki-MultihopQA and WebQuestions. As shown in Table 2, We have following observations:\n1) Compared to other adaptive retrieval methods, DeepRAG can achieve higher accuracy with relatively lower retrieval costs. This can be attributed to our dynamic usage of internal knowledge. Additionally, DeepRAG exhibits a positive trend in exploring relevant evidence when faced with insufficient retrieval results, as evidenced by the lower average retrieval numbers in both 2WMQA (0.92 compared to 1.25) and WQ"}, {"title": "B.2 Relevance to Parametric Knowledge", "content": "In this section, we investigate the relationship between retrieval needs and parametric knowledge to demonstrate how effectively our method explores the knowledge boundary.\nIdeally, models should initiate retrieval for queries beyond their parametric knowledge while utilizing their existing knowledge for familiar queries. We use CoT results as an indicator of whether the model can answer questions using its parametric knowledge. Subsequently, we analyze whether other adaptive retrieval methods align with this pattern of parametric knowledge utilization. We evaluate the relevance using four metrics. F1 score and Accuracy serve as basic performance measures, while balanced accuracy and Matthews Correlation Coefficient(MCC) are employed to account for the class imbalance between retrieval-required and retrieval-not-required cases. The MCC ranges from -1 to 1, where a value of 1 indicates perfect correlation, 0 represents no correlation (random chance), and -1 signifies an inverse correlation.\nAs shown in Table 3, we find that 1) DeepRAG demonstrates superior relevance performance across F1, balanced accuracy, and MCC metrics. This suggests that DeepRAG successfully identifies retrieval necessity by exploring knowledge boundary. 2) While FLARE, DRAGIN, and TAARE exhibit high accuracy scores, their relatively low balanced accuracy and MCC scores suggest they mainly succeed in retrieval-required cases but struggle to properly avoid unnecessary retrievals."}, {"title": "B.3 Ablation Study", "content": "Table 7 and Table 8 show the detailed results of the ablation study."}]}