{"title": "MMHMR: Generative Masked Modeling for Hand Mesh Recovery", "authors": ["Muhammad Usama Saleem", "Ekkasit Pinyoanuntapong", "Mayur Jagdishbhai Patel", "Hongfei Xue", "Ahmed Helmy", "Srijan Das", "Pu Wang"], "abstract": "Reconstructing a 3D hand mesh from a single RGB image is challenging due to complex articulations, self-occlusions, and depth ambiguities. Traditional discriminative methods, which learn a deterministic mapping from a 2D image to a single 3D mesh, often struggle with the inherent ambiguities in 2D-to-3D mapping. To address this challenge, we propose MMHMR, a novel generative masked model for hand mesh recovery that synthesizes plausible 3D hand meshes by learning and sampling from the probabilistic distribution of the ambiguous 2D-to-3D mapping process. MMHMR consists of two key components: (1) a VQ-MANO, which encodes 3D hand articulations as discrete pose tokens in a latent space, and (2) a Context-Guided Masked Transformer that randomly masks out pose tokens and learns their joint distribution, conditioned on corrupted token sequence, image context, and 2D pose cues. This learned distribution facilitates confidence-guided sampling during inference, producing mesh reconstructions with low uncertainty and high precision. Extensive evaluations on benchmark and real-world datasets demonstrate that MMHMR achieves state-of-the-art accuracy, robustness, and realism in 3D hand mesh reconstruction.", "sections": [{"title": "1. Introduction", "content": "Hand mesh recovery has gained significant interest in computer vision due to its broad applications in fields such as robotics, human-computer interaction [41, 53], animation, and AR/VR [10, 30]. While previous methods have explored markerless, image-based hand understanding, most depend on depth cameras [3, 22, 42, 47, 54] or multi-view images [5, 23, 51, 52]. Consequently, most of these methods are not feasible for real-world applications where only monocular RGB images are accessible. On the other hand, monocular hand mesh recovery from a single RGB image, especially without body context or explicit camera parameters, is highly challenging due to substantial variations in hand appearance in 3D space, frequent self-occlusions, and complex articulations.\nRecent advances, especially in transformer-based methods, have shown significant promise in monocular hand mesh recovery (HMR) by capturing intricate hand structures and spatial relationships. For instance, METRO [13] and MeshGraphormer [39] utilize multi-layer attention mechanisms to model both vertex-vertex and vertex-joint interactions, thereby enhancing mesh fidelity. Later, HaMeR [46] illustrated the scaling benefits of large vision transformers and extensive datasets for HMR, achieving improved reconstruction accuracy. However, these methods"}, {"title": "2. Related Work", "content": "Human hand recovery has been developed in recent years, with early approaches [21, 28, 55, 58, 68, 70] leveraging optimization techniques to estimate hand poses based on 2D skeleton detections. Later, MANO [49] introduced a differentiable parametric mesh model that capture hand shape and articulation, allowing the model to provide a plausible mesh with end-to-end estimation of model parameters directly from a single-view image. Boukhayma et al. [6] presented the first fully learnable framework to directly predict the MANO hand model parameters [49] from RGB images. Similarly, several subsequent methods have leveraged heatmaps [67] and iterative refinement techniques [4] to ensure 2D alignment. Kulon et al. [34, 35] proposed a different regression approach that predicts 3D vertices instead of MANO pose parameters, achieving notable improvements over prior methods. Recent methods, such as METRO [13], MeshGraphormer [39], HaMeR [46] achieve the SOTA performance by modeling both vertex-vertex and vertex-joint interactions. These existing methods, based on discriminative regression, learn a deterministic mapping from the input image to the output mesh. This deterministic approach struggles to capture the uncertainties and ambiguities caused by hand self-occlusions, interactions with objects, and extreme poses or camera angles, resulting in unrealistic hand mesh reconstructions."}, {"title": "2.2. Generative Methods", "content": "Our MMHMR employs a generative method that learns a probabilistic mapping from the input image to the output mesh. It utilizes this learned distribution to synthesize high-confidence, plausible 3D hand meshes based on 2D visual contexts. HHMR [37] is the only other generative hand mesh recovery method in the literature. Unlike HHMR [37] that utilizes diffusion models, MMHMR is inspired by the success of masked image and language models for image and text generation tasks [7, 8, 16, 17, 69]. This fundamental difference allows MMHMR to explicitly and quantitatively estimate confidence levels or prediction probabilities for all mesh reconstruction hypotheses, enabling confidence-guided hypothesis selection for accurate reconstruction. In contrast, HHMR's denoising diffusion process synthesizes multiple mesh hypotheses without associating a confidence level with each hypothesis. Thus, it only reports the theoretically best mesh reconstruction by finding the hypothesis with minimal reconstruction errors under the assumption that ground-truth meshes are available."}, {"title": "3. Proposed Method: MMHMR", "content": "Problem Formulation. Given a single-hand image I, we aim to learn a mapping function \\(f(I) = {\u03b8, \u03b2, \u03c0}\\) that re-"}, {"title": "3.1. Hand Model and VQ-MANO", "content": "Our approach utilizes the MANO hand model [49], which takes pose parameters \\(\u03b8\u2208 R^{48}\\) and shape parameters \\(\u03b2\u2208 R^{10}\\) as input. The function M (\u03b8, \u03b2) outputs a 3D hand mesh \\(M\u2208 R^{V\u00d73}\\) with V = 778 vertices and joint locations X \u2208 \\(R^{K\u00d73}\\) with K = 21 joints, enabling both surface and pose representation.\nThe VQ-MANO is a MANO hand tokenizer that learn a discrete latent space for 3D pose parameters \\(\u03b8\u2208 R^{48}\\) by quantizing the continuous pose embeddings into a learned codebook C with discrete code entries, as depicted in Figure 2(a). To this end, we employ a Vector Quantized Variational Autoencoder (VQ-VAE) [56] for pretraining the tokenizer. Specifically, we input the MANO pose parameters @ into a convolutional encoder E, which maps them to a latent embedding z. Each embedding zi is then quantized to its nearest codebook entry Ck \u2208 C based on Euclidean distance, defined as\n\\(2i = arg min || Zi - Ck ||^2.\\)\nCk EC\nThe total loss function of VQ-MANO is formulated as:\n\\(Lvq-mano = Are Lrecon + XE|| Sg[z] - C||^2 + da||z - sg[c]||^2,\\)"}, {"title": "3.2. Context-Guided Masked Transformer", "content": "The context-guided masked transformer comprises two main components: the multi-scale image encoder and the masked graph transformer decoder."}, {"title": "3.2.1. Multi-scale Image Encoder", "content": "Our encoder uses a vision transformer (ViT-H/16) to extract image features [46], processing 16x16 pixel patches into feature tokens. Following ViTDet [2], we adopt a multi-scale feature approach by upsampling the initial feature map to produce feature maps at varying resolutions. This multi-scale representation is critical for handling complex articulations and self-occlusions in hand poses. High-resolution maps provide fine-grained joint details, while low-resolution maps capture global hand structure, balancing precision in joint positioning with overall anatomical coherence. Moreover, we utilize cross-attention with low-resolution feature maps in the x-Attention head to regress stable shape parameters (8) and camera orientation (\u03c0), making the process computationally efficient. This approach decouples shape estimation from pose modeling, preserving morphological stability and spatial alignment, and enhancing robustness and anatomical accuracy in 3D hand reconstruction."}, {"title": "3.2.2. Masked Graph Transformer Decoder", "content": "The Masked Transformer Decoder is composed of two key components: Graph-Guided Pose Modeling (GGPM) and the Context-Infused Masked Synthesizer Module."}, {"title": "Graph-Guided Pose Modeling (GGPM)", "content": "Our decoder employs 2 blocks of lightweight graph transformer that processes pose tokens generated by VQ-MANO, enriched with 2D pose guidance, where hand pose tokens are represented as graph nodes linked by learnable adjacency matrices to capture joint relationships effectively. To enhance stability and anatomical accuracy, we integrate a transformer"}, {"title": "Context-Infused Masked Synthesizer", "content": "We leverage a multi-layer transformer whose inputs are refined pose tokens QGGPM and cross-attends them with multi-scale feature maps generated by the image encoder. To enhance computational efficiency with high-resolution feature maps, a deformable cross-attention mechanism is employed [72]. This allows each pose token to focus on a selected set of sampling points around a learnable reference point, rather than the entire feature map. By concentrating attention on relevant areas, the model achieves a balance between computational efficiency and spatial precision, preserving essential information for accurate 3D hand modeling. The deformable cross-attention is defined as:\nMCDA(QGGPM, Py, {x}) = \u2211 Alyk. Wx' (py + \u2206plyk)\nl,k\nwhere QGGPM are refined manopose token queries, py are learnable reference points, \u0394plyk are sampling offsets, {x} are multi-scale features, Alyk are attention weights, and W is a learnable weight matrix. With the inclusion of a [MASK] token in the masked transformer decoder, the module can predict masked pose tokens during training, while also facilitating token generation during inference. This approach allows the [MASK] token to serve as a placeholder for final pose token predictions, supporting robust synthesis of occluded or unobserved hand parts for a coherent 3D hand reconstruction."}, {"title": "3.3. Training: Differential Masked Modeling", "content": "Context-conditioned Masked Modeling. We employ masked modeling to train our model that learns the probabilistic distribution of 3D hand poses, conditioned on multiple contextual cues. Given a sequence of discrete pose tokens Y = [yi]=1 from the pose tokenizer, where L is the sequence length, we randomly mask out a subset of m tokens with m = [\u03b3(\u03c4) \u00b7 L], where \u03b3(\u03c4) is a cosine-based masking ratio function. Here, 7 is drawn from a uniform distribution U(0, 1), and we adopt the masking function y(t) = cos (\uc9e4), inspired by generative text-to-image"}, {"title": "3.4. Inference: Confidence-Guided Sampling", "content": "Our model leverages confidence-guided sampling to achieve precise and stable 3D hand pose predictions. This process begins with a fully masked sequence Y\u2081 of length L, with each token initialized as [MASK]. Over T decoding iterations, each iteration t applies stochastic sampling to predict masked tokens based on their distributions P(Yi YM, X2D, Ximg). Following each sampling step, tokens with the lowest prediction confidences are re-masked to be re-predicted in subsequent iterations. The number of tokens re-masked is determined by a masking schedule [\u300c(+) \u00b7 L], where y is a decaying function of t. This schedule dynamically adjusts masking intensity based on confidence, using a higher masking ratio in earlier iterations when prediction confidence is lower. Consequently, the model iteratively refines ambiguous regions, progressively improving prediction confidence as context builds. The masking ratio decreases with each step, stabilized by the cosine decay function \u03b3; alternative decay functions are discussed in the supplementary material."}, {"title": "4. Experiments", "content": "Datasets. To train the hand pose tokenizer, we employed a diverse set of datasets to capture a wide range of hand poses and interactions. This includes DexYCB [9], InterHand2.6M [43], MTC [61], and RHD [73]. For training MMHMR, we utilized a diverse dataset, following a similar setup as in [46] to ensure a fair comparison. Specifically, the training data was drawn from FreiHAND [74], HODv2 [25], MTC [61], RHD [73], InterHand2.6M [43], H2O3D [25], DexYCB [9], COCO-Wholebody [32], Halpe [20], and MPII NZSL [51]."}, {"title": "Evaluation Metrics", "content": "Following standard protocols [37, 46, 71], MMHMR evaluated on reconstructed 3D joints using PA-MPJPE and AUCJ, while 3D mesh vertices were evaluated with PA-MPVPE, AUCv, F@5mm, and F@15mm. Additionally, to examine MMHMR's generalization and accuracy in diverse real-world settings, we employed the Percentage of Correct Keypoints (PCK) [46] metric at multiple thresholds, ensuring a robust evaluation of its performance across varied conditions. To evaluate hand image generation with mesh-guided control, we compute FID-H and KID-H on cropped hand regions, following previous works [44] and use MediaPipe [65] as a hand detector to measure confidence."}, {"title": "3D Reconstruction Accuracy Evaluation", "content": "To comprehensively evaluate MMHMR's 3D joints and mesh reconstruction capabilities, we used the HO3Dv3 [26] and FreiHAND"}, {"title": "4.1. Comparison to State-of-the-art Approaches", "content": "We evaluate MMHMR against a range of state-of-the-art methods (SOTA) on the HO3Dv3 [26], FreiHAND [74], and HInt [46] benchmarks, as detailed in Table 1, Table 2, and Table 3. MMHMR consistently outperforms competing methods across key evaluation metrics, demonstrating robust accuracy in 3D hand reconstruction. Notably, we perform zero-shot evaluations on both the HO3Dv3 and HInt benchmarks to assess MMHMR's generalizability. A core contributor to MMHMR's success is its capability to model and refine uncertainty, making it highly effective in scenarios with complex hand poses and significant occlusions. On the HO3Dv3 dataset (Table 1), MMHMR achieves a PA-MPJPE reduction of approximately 19.5% and a PA-MPVPE reduction of 15.7% compared to the existing SOTA method. This improvement underscores MMHMR's precision in handling challenging hand poses and occlusions. Similarly, on the HInt benchmark (Table 3), MMHMR achieves notable improvements in occluded joint reconstruction at the PCK@0.05 threshold on the HInt benchmark. Specifically, MMHMR shows an 8.1% increase on HInt-NewDays, a 21.2% increase on HInt-VISOR, and a 27.8% increase on HInt-Ego4D compared to the closest"}, {"title": "4.2. Ablation Study", "content": "The effectiveness of MMHMR is grounded in its mask modeling and iterative decoding techniques. This ablation study examines how iterative refinement and mask-"}, {"title": "5. Conclusion", "content": "In this paper, we introduced MMHMR, a novel generative masked model designed for accurate and robust 3D hand mesh reconstruction from single RGB images. MMHMR addresses the longstanding challenges posed by complex hand articulations, self-occlusions, and depth ambiguities by leveraging a generative framework that captures and refines hand pose distributions. Central to our approach are two key components: VQ-MANO, which encodes 3D hand articulations as discrete pose tokens in a learned latent space, and the Context-Guided Masked Transformer, which models token dependencies conditioned on both image features and 2D pose cues. This framework uses confidence-guided iterative sampling to refine reconstructions, producing realistic hand meshes under challenging conditions. MMHMR outperforms state-of-the-art methods, setting a new benchmark for 3D hand modeling in human-computer interaction, AR, and VR."}]}