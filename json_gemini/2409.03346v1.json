{"title": "Sketch: A Toolkit for Streamlining LLM Operations", "authors": ["Xin Jiang", "Xiang Li", "Wenjia Ma", "Xuezhi Fang", "Yiqun Yao", "Naitong Yu", "Xuying Meng", "Peng Han", "Jing Li", "Aixin Sun", "Yequan Wang"], "abstract": "Large language models (LLMs) represented by GPT family have achieved remarkable success. The characteristics of LLMs lie in their ability to accommodate a wide range of tasks through a generative approach. However, the flexibility of their output format poses challenges in controlling and harnessing the model's outputs, thereby constraining the application of LLMs in various domains. In this work, we present Sketch, an innovative toolkit designed to streamline LLM operations across diverse fields. Sketch comprises the following components: (1) a suite of task description schemas and prompt templates encompassing various NLP tasks; (2) a user-friendly, interactive process for building structured output LLM services tailored to various NLP tasks; (3) an open-source dataset for output format control, along with tools for dataset construction; and (4) an open-source model based on LLaMA3-8B-Instruct that adeptly comprehends and adheres to output formatting instructions. We anticipate this initiative to bring considerable convenience to LLM users, achieving the goal of \u201cplug-and-play\u201d for various applications. The components of Sketch will be progressively open-sourced at https://github.com/cofe-ai/Sketch.", "sections": [{"title": "Introduction", "content": "Generative pre-trained large language models (LLMs) have achieved remarkable success, with notable examples including GPT [1, 17], LLaMA [24, 25, 5], and FLM [11, 12, 13] series. One of the key advantages of these models lies in their powerful generalization capabilities: a single model is capable of handling a diverse range of tasks. However, accurately generating formatted outputs, such as JSON, remains challenging for LLMs because they do not always strictly follow instructions. On the demand side, AI-driven applications urgently require the integration of structured outputs (e.g., JSON) from LLMs into their data streams. This has heightened the urgency for LLMs to produce controlled and structured outputs as demanded.\n\nThe requirement for structured outputs from LLMs can be resolved through a multitude of approaches. In-context learning is a typical approach. It not only enhances model performance but also offers a certain degree of format control without incurring additional computational costs for model fine-tuning. However, this approach faces challenges, such as an inability to determine when to end the generation. Besides, it needs long-text ability when meeting complex questions, as it relies on extensive input examples to ensure accurate decision-making. Moreover, tasks that require complex constraints on format and content, such as relation extraction and event extraction, pose significant difficulties for in-context learning.\n\nSupervised fine-tuning (SFT) refers to the process of training a pre-trained model on a labelled dataset specifically tailored for a particular task. Although SFT can enhance performance on specific tasks and has generalization capabilities, its ability to control the format of the output remains unsatisfactory. After all, the integration of LLM outputs into applications typically demands the output format that is entirely compliant with specified requirements, a feat that LLMs, proficient in \u201cnext token prediction\u201d, are unable to ensure. Another issue is that, to the best of our knowledge, there is a lack of open-source models and datasets specifically addressing the problem of formatted output control. This somewhat limits the application of LLMs across various fields.\n\nTo ensure that the outputs of LLMs conform to formatting requirements, numerous decoding control tools (guidance\u00b9, outlines[28], llama.cpp\u00b2, Im-format-enforcer\u00b3 ) based on regular expressions or context-free grammars (CFGs) have been developed. These tools first convert the user's requirements for output format into formal languages. Under the constraints of these formal languages, these models could decode responses that meet the formatting requirements. More importantly, as these tools are involved in the decoding process of the model, they could potentially impair the model's performance[22], especially if the model itself is not adept at generating structured outputs. To address those issues, an open-source model that excels in generating structured responses according to requirements, along with a framework for streamlining various LLM-based operations, holds significant value.\n\nIn this work, we introduce Sketch, a toolkit designed to assist users in effectively operating LLMs and generating results in their expected format. The core idea of Sketch is as follows: targeting on various NLP tasks, we establish a collection of task description schema, within which users can delineate their own tasks, including task objectives, labelling systems, and most critically, the specifications for the output format. An LLM can then be deployed out of the box to handle these unfamiliar tasks, ensuring the correctness and conformity of the output format. This approach not only streamlines the process for users but also enhances the reliability and precision of the model's outputs, making it a versatile and robust solution for a wide array of NLP applications.\n\nThe main contributions are as follows:\n\n\u2022 We propose Sketch, an innovative operating framework simplifying the process for LLM users, enabling \"plug-and-play\u201d functionality for task-specific applications with predefined schemas. The proposed Sketch makes it easier to instantiate and manage NLP tasks.\n\n\u2022 To optimize the performance within Sketch framework, we build a dataset and conduct model fine-tuning based on LLaMA3-8B-Instruct, ensuring superior task handling and output consistency. Both the dataset and fine-tuned model will soon be made available to the public.\n\n\u2022 By integrating constrained decoding frameworks, Sketch ensures precise control over the model's output format, enhancing the reliability and precision of outcomes, and facilitating direct application of large models in industry settings."}, {"title": "Sketch Architecture", "content": "Sketch is designed to enable controlled formatting and easy interaction with LLMs. In this section, we detail the architecture of Sketch and how to use it easily. Figure 1 illustrates the concepts and internal workflow of Sketch. The workflow consists of four steps: schema selection, task instantiation, prompt packaging, and generation. In practical applications, the complex aspects of this process are transparent to the user.\n\nFirst, users are guided to choose the appropriate schema from a predefined set that aligns with the specific NLP task requirements. A schema, in essence, is a class (or a JSON Schema 4 in practice) that standardizes the user's description of tasks. Second, in the task instantiation phase, users populate"}, {"title": "Schema Selection", "content": "Schema is the bridge between tasks and LLMs. It outlines a descriptive framework for each kind of task based on the task-specific characteristics. A schema can be represented by either a Pydantic model or a JSON Schema. When customizing a specific task, users are advised to select the most appropriate schema and instantiate the task within its constraints. This process can be achieved through a Python API, and we also provide a more intuitive interactive method in the form of filling out a form generated by Sketch based on the schema. To date, as the initial phase of Sketch's development, we have experimentally built a set of schemas for tasks, including over ten subcategories under the three main categories of text classification, text generation, and information extraction, as shown in Table1. A selection of the schemas we have crafted is showcased in Appendix A. For an extensive view of the task schemas available, please visit our project repository at https://github.com/cofe-ai/Sketch"}, {"title": "Task Instantiation", "content": "We define Task Instance as a standardized description of a particular task within the constraints of the schema it belongs to, and the process of creating it by the user is referred to as Task Instantiation. A task instance typically includes the following basic fields:\n\nTask specification fields delineates the task, which may encompass the \"taskDesc\" field detailing the task's purpose, along with the \u201clabelSet\u201d and \u201cchoiceType\u201d fields that respectively define the"}, {"title": "Prompt Packaging", "content": "The process of packaging an instantiated task and input is crucial for ensuring LLMs understand the task requirements and process the input correctly. This step involves combining the structured task description with the user's input data into a format that is optimized for interaction with the LLM.\n\nInput Integration. The user's input, whether it be a common text snippet or any other form of information relevant to the task, will be integrated into the prompt most intuitively. This integration is guided by a prompt template associated with the schema, ensuring that the input is presented in a manner that is coherent and comprehensible to LLMs.\n\nAs shown in Figure 1, for a NER task, the packaged prompt might include [Task Description], [Label Architecture], [Output Format (Json Schema)], and [Input Data] to be processed. This ensures that the LLM understands the task criteria and outputs the result in the desired format."}, {"title": "Generation", "content": "The final step in the workflow of Sketch involves the interaction with LLMs to generate the desired output. Sketch is able to generate the expected response directly with a good performance. Besides, there are some more precise control methods. Throughout this process, we ensure that the model's output conforms to the required format from two perspectives.\n\nConstrained Generation. Considering that even with meticulous fine-tuning, LLMs cannot guarantee 100% accuracy in output format, we integrate a mature decoding control framework, lm-format-enforcer. It employs CFG to ensure that the model's responses align perfectly with the predefined output format. Simultaneously, recognizing that any constraints to the decoding process may impact the model's performance, this strict output format control is made optional in Sketch.\n\nOutput Validation. Given that not all JSON Schema properties are supported by decoding control frameworks, the output produced by the LLM cannot be assured to adhere to the constraints of the specified output format. To ensure compliance with the expected format, we employ the jsonschema"}, {"title": "Code Example", "content": "Listing 1 demonstrates the basic usage of Sketch through a simple named entity recognition (NER) task. Sketch is still under development prior to its release, and the APIs may change at any time."}, {"title": "Sketch-8B Fine-tuning Approach", "content": "We fine-tune LLaMA3-8B-Instruct to enhance the model's capability to generate structured data that adheres to JSON schema constraints across a variety of tasks. Our training process emphasizes two key aspects: ensuring strict adherence to the specified JSON schema constraints in the model's outputs and fostering robust generalization across various tasks. To achieve these goals, we carefully design a specialized fine-tuning dataset."}, {"title": "Data Preparation", "content": "The capability of the model to adhere to formats and its ability to understand and tackle tasks are distinct attributes. To enhance these aspects, we have constructed two targeted datasets: NLP task data and schema following data. The primary objective of NLP task data is to enable models to learn how to tackle NLP tasks. However, considering the limitations in output format diversity of manually curated fine-tuning data for NLP tasks, we propose the automated construction of schema following data to enhance the model's adherence to the output format schema.\n\nNLP Task Data. We assemble a comprehensive collection of over 20 datasets, encompassing more than ten subcategories within three primary domains: text classification, text generation, and information extraction. Through the meticulous design of output formats for each dataset, we construct a task instance set of size 53. Among them, 37 task instances are dedicated to training, while the remainder are reserved for evaluation.\n\nSchema Following Data. To ensure the diversity of JSON schemas, we generated 10,000 JSON schemas with widths and depths within 5 with a random schema generation method. Then, we utilized LLaMA3-8B-Instruct, under the constraint of a decoding control tool, to generate JSON instances that conform to the schemas. Following the patterns of NLP task data, we designed a task that involves selecting values from a randomly generated list of given values to construct JSON objects that match specific schemas. Finally, we constructed 20,000 pieces of fine-tuning data for this task by modifying the values in the JSON instances generated by LLaMA3-8B-Instruct."}, {"title": "Fine-tuning Method", "content": "Reinforcement learning is one of the popular ways to tune the LLMs. LeCun holds the opinion, \"I do favor MPC over RL\u201d6. We have a similar opinion so we use the easy fine-tuning methods under data-driven. Indeed, it doesn't mean reinforcement learning is useless, but it could be used in the following steps such as resort. Generating valid outputs that conform to the JSON Schema is not simply a matter of mimicking formats, it necessitates a thorough comprehension of the schema's descriptions. Consequently, data adhering to the schema is essential for enhancing the model's ability.\n\nThe training objective of Sketch-8B considers two aspects: enhancing the model's adherence to format and improving its NLP task performance. To this end, we use the proposed mixed dataset comprising NLP task data and schema following data for fine-tuning. The inclusion of NLP task data markedly boosts the model's capabilities in handling NLP tasks while the schema following data is crucial for enhancing the model's adherence to various output format requirements.\n\nWe use fine-tuning method to optimize the proposed model, the objective L(0) could be formatted as:\n\n$L(\\theta) = \\sum_{t=1}^{m} logP_{\\theta}(\\hat{y}_t = Y_t|Y_{1:t-1}, X)$\n\nwhere $X = \\{x_1,x_2,...,x_n\\}$ represents an input sequence of length n, which is the constructed prompt. $Y = \\{Y_1, Y_2,...,Y_m\\}$ is the label of the generated sequence of length m, and $\\hat{Y} = \\{\\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_m \\}$ is the actual output of the model. Note that both Y and $\\hat{Y}$ exclude the prompt and consist only of the response. $\\theta$ denotes the model parameters, and $P_{\\theta}$ represents the conditional probability under the parameters \u03b8.\n\nEach sample consisting of X and Y is sampled from a carefully constructed mixed dataset. The optimal fine-tuning effect is achieved by appropriately balancing the ratio of NLP task data to schema-following data in the mixed dataset."}, {"title": "Experiments", "content": "In this section, we validate the model's generalization capabilities through experiments and discuss the effectiveness and optimal configuration of our fine-tuning data."}, {"title": "Experiment Settings", "content": "We use publicly available NLP task datasets (See Appendix C for details) for experiments. For each dataset, we carefully construct different task instances, expanding a single dataset into multiple experimental datasets with varying outputFormat and other task-related parameter configurations. To validate different hypotheses, we selectively exclude some data from the training set to create test datasets. These test datasets include three types: (1) the output format not seen in the training set while other output formats from the same dataset are included, (2) the entire dataset is not present in the training set, and (3) the entire tasks are not included in the training set.\n\nFine-tuning Settings. We experiment on LLaMA3-8B-Instruct since it has strong foundational capabilities. We fine-tune the model for 8 epochs with a global batch size of 128, setting the learning rate to le-6 and weight decay to 0.1. The learning rate is decayed to 0 using a linear schedule. We select the best checkpoint from the model at the end of every epoch.\n\nEvaluation Methods. To comprehensively evaluate the model's schema adherence and NLP task performance, we assess from two perspectives:\n\n1. We define a metric to assess the model's ability to produce outputs that conform to the outputFormat: Legal Output Ratio. First, we determine whether the model's output can be converted into a JSON object; if not, the output is considered invalid. Next, we check if the JSON object meets the outputFormat requirements; otherwise, it is considered invalid. The legal output ratio is calculated by dividing the number of valid outputs by the total number of test samples.\n\n2. To evaluate NLP task performance, we employ traditional metrics like F1-score or accuracy, tailored to the specific requirements of each task."}, {"title": "Comparison with Baselines", "content": "To evaluate generalization, we fine-tune Sketch-8B-w.o.-ner with a partially removed dataset and benchmark it against mainstream models, including GPT-40, DeepSeek, and ChatGLM. Using identical prompts across models, we gather results via API and assess performance. We also compare Sketch-8B-w.o.-ner with the original LLaMA3-8B-Instruct (local inference). Additionally, we evaluate DeepSeek's one-shot results and GPT-40's constrained decoding. Sketch-8B-w.o.-ner and LLaMA-8B-Instruct use FSM and CFG constraints for decoding. The comparison covers three dataset types: (1) unknown format, with output formats absent in training data, (2) unknown domain, with datasets from untrained domains, and (3) unknown task, focusing on task types not covered during training. NER is the test task for the Unknown Task category.\n\nSchema Adherence Comparison. Table 2 illustrates notable differences in schema adherence among baseline models under unconstrained output conditions. For simpler formats like S10T8 and HOTEL, LLaMA3-8B-Instruct achieves nearly 100% on legal output ratio but fails completely on 20NEWS. Across most datasets, its legal output ratio ranges from 50% to 75%, averaging 64.9%. In contrast, Sketch-8B-w.o.-ner achieves an average legal output ratio of 96.2% under unconstrained conditions, with its lowest performance on CNL03 still at 83.8%. This demonstrates Sketch-8B-w.o.-ner's strong generalization in format adherence.\n\nPerformance Comparison. We compare with LLaMA3-8B-Instruct to assess training effectiveness and with mainstream models to evaluate performance level:\n\n1. vs LLaMA3-8B-Instruct. Table 2 shows that Sketch-8B-w.o.-ner consistently outperforms LLaMA3-8B-Instruct under the same decoding strategy, both on individual subsets and in average scores. Furthermore, the unconstrained Sketch-8B-w.o.-ner surpasses LLaMA3-8B-Instruct across all"}, {"title": "Generalization Capability Analysis", "content": "Output Format Generalization Capability. We first evaluate Sketch-8B's generalization capability across different output formats within the same dataset. As shown in the \u201cUnknown Format\u201d column of Table 2, the output formats of the two datasets (S10T8 and 20NEWS) used for evaluation are not"}, {"title": "Data Configuration Experiment", "content": "Fine-tuning data is central to this work. We analyze how data proportion and scale affect model performance. The evaluation focuses on the model's results on a test set with seven tasks: three with unseen output formats, three from unseen domains, and one entirely new task.\n\nData Proportion. Different sampling proportion affects the performance of pretraining foundation models. Similar to this phenomenon, the sampling proportion of schema following data leads to a decline in task performance.\n\nTo assess the effectiveness and configuration of NLP task data and schema following data, we conduct experiments using a fixed 20k dataset with various proportions, including a setup without schema following data. Performance is evaluated on the test set, with results shown in Table 3."}, {"title": "Related Work", "content": "Significant advancements have been made in the realm of format-constrained generation for LLMs. We roughly divide these methods into three categories: pre-generation tuning, in-generation control, and post-generation parsing.\n\nPre-generation Tuning. Pre-generation tuning encompasses a suite of techniques designed to fine-tune the behaviour of LLMs before the actual text-generation process begins. This approach involves modifying the model's training data[32, 29] or prompts[2, 27] to align more closely with the specific format constraints required by the task at hand.\n\nIn-generation Control. There are numerous frameworks dedicated to intervening in the decoding process of LLMs to control the permissible range of the next token, ensuring that the output of the LLM meets the format requirements. The predominant control strategies employed include JSON Schema (i.e., Jsonformer7, Im-format-enforcer and outlines), regular expression (i.e., guidance, Im-format-enforcer and outlines) and context-free grammar (i.e., llama.cpp). Although these methods typically ensure high accuracy in response format, they often lead to a decrease in the usefulness of the responses [22], which is one of the starting points for the work presented in this paper.\n\nPost-generation Parsing. This category involves techniques that parse the output of LLMs after generation to ensure it conforms to specific formats. These methods often rely on post-processing algorithms to refine the raw output into a structured format. Guardrails is a framework of this kind, designed to enforce constraints on the output of LLMs by filtering or modifying the generated text to ensure it adheres to predefined guidelines or specifications."}, {"title": "Conclusions and Future Work", "content": "In this work, we propose Sketch to simplify and optimize the applications of LLMs. Using a schema-based approach, Sketch can tackle the challenges in structured output generation and model generalization. Key contributions include the schema architecture for task description, data and model fine-tuning for improved performance, and the integration of a constrained decoding framework for precise output management. Experimental results not only demonstrate the enhanced capability of the fine-tuned Sketch-8B in adhering to output formats but also validate the effectiveness of the fine-tuning data we build, particularly the schema following data.\n\nFuture work involves expanding task categories, optimizing model performance, lowering entry barri-ers, and exploring new applications in diverse domains. Sketch's innovative approach and ongoing development promise to drive advancements in LLM applications and unlock new possibilities for harnessing the power of LLMs."}]}