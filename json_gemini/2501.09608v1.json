{"title": "Metric Learning with Progressive Self-Distillation for Audio-Visual Embedding Learning", "authors": ["Donghuo Zeng", "Kazushi Ikeda"], "abstract": "Metric learning projects samples into an embedded space, where similarities and dissimilarities are quantified based on their learned representations. However, existing methods often rely on label-guided representation learning, where representations of different modalities, such as audio and visual data, are aligned based on annotated labels. This approach tends to underutilize latent complex features and potential relationships inherent in the distributions of audio and visual data that are not directly tied to the labels, resulting in suboptimal performance in audio-visual embedding learning. To address this issue, we propose a novel architecture that integrates cross-modal triplet loss with progressive self-distillation. Our method enhances representation learning by leveraging inherent distributions and dynamically refining soft audio-visual alignments-probabilistic aligns between audio and visual data that capture the inherent relationships beyond explicit labels. Specifically, the model distills audio-visual distribution-based knowledge from annotated labels in a subset of each batch. This self-distilled knowledge is used to automatically generate soft-alignment labels for the remaining audio-visual samples. These soft-alignment labels are used to construct soft cross-modal triplets, which in turn are employed to fine-tune the model's parameters. Experimental results on two audio-visual benchmark datasets demonstrate the effectiveness of our proposed method in the cross-modal retrieval task, achieving state-of-the-art performance with improvements of 2.13% and 1.82% on the AVE and VEGAS datasets, respectively, in terms of average MAP metrics.", "sections": [{"title": "I. INTRODUCTION", "content": "Metric learning [8], [10], [12], [13], [15], [19], [20], [22], [23] is a powerful approach for learning representations by comparing sample similarities and dissimilarities based on an-notated labels, such as triplet loss [6]. This technique projects samples into an embedded space, where the relationships between samples are quantified according to their learned representations, which can significantly improve tasks such as audio-visual embedding for cross-modal retrieval [26], [29], [30]. However, metric learning often has a heavy reliance on annotations, which limits the effectiveness and scalability of the approach, as obtaining high-quality annotations is often labor-intensive and costly. Furthermore, the simplicity of these annotations means that label-guided representation learning often cannot achieve acceptable performance on its own, as it may not capture the complex, latent features of the data.\nExisting methods [28], [31] attempt to mitigate these issues by focusing on selecting impactful data samples for the representation learning on embedding space. However, these approaches fail to fully explore the space due to the limited training data, resulting in an incomplete representation of samples. The state-of-the-art (SOTA) model, AADML [27], addresses this issue by discovering the potential correlations among similar samples to enhance the quality of representa-tions (see Fig. 1). Nonetheless, AADML does not effectively take into account the valuable latent features and potential relationships inherent in the data distributions. Specifically, when determining positive and negative samples using fixed annotations, this approach treats all positives and negatives with equal importance. Consequently, it neglects the distri-butional nuances of the data, which are essential for more accurately identifying and differentiating between positive and negative samples."}, {"title": "II. RELATED WORK", "content": "A. Audio-visual Embedding Learning\nAudio-visual embedding learning creates a shared feature space for audio and visual data, preserving semantic relation-ships and enabling effective cross-modal retrieval [28]. CCA [2] and its variants, including K-CCA [11], Cluster-CCA [16], DCCA [1] and C-DCCA [24]. These methods often struggle to capture complex non-linear relationships. Deep learning excels in audio-visual embedding by aligning matched samples using neural networks. Models like CCTL [28], and EICS [29] capture intricate audio-visual relationships to advance retrieval tasks. Inspired by AADML [27], we introduce a novel metric learning that leverages latent features and potential relation-ships beyond annotation through self-knowledge distillation.\nB. Self-knowledge Distillation\nKnowledge distillation (KD) [7] typically involves a large teacher model guiding a smaller student model. In cross-modal tasks, KD [4] transfers knowledge from a pre-trained model on one modality to a student model with a new modality, often using paired samples. Various methods [4] enhance per-formance using paired samples, contrastive loss, and modality hallucination. Despite its success in visual recognition, cross-modal knowledge transfer struggles with modality gaps and paired sample availability. We introduce progressive self-distillation to iteratively refine the model's knowledge, and soften constraints to capture transferable cross-modal repre-sentations."}, {"title": "III. APPROACH", "content": "To address the issue of overlooking inherent distribution beyond annotations, we propose a model that progressively distills its own knowledge to generate soft-alignment labels and enabling more transferable representations for cross-modal triplet learning, as seen in Fig. 2.\nA. Preliminaries\nConsider an audio-visual dataset comprising n videos, de-noted as D = {(ai, vi)}_{i=0}^{n}, where each video pair (ai, vi) is represented as (ai, vi). The ai \u2208 R^{128} denotes the audio fea-ture extracted using a VGGish model, and vi \u2208 R^{1024} denotes the visual features extracted using an Inception V3 model. Each pair is associated with a binary semantic label vector Y\u00bf \u2208 {0, 1}^{c}, where c is the number of categories, 1 indicates that the video pair (ai, vi) belongs to the corresponding class, while 0 indicates otherwise. The goal of our model is to train encoders fa for audio data and fu for visual data such that for a given semantically similar instance pairs (fa(ai), fv(vj)) with the same label get closer together while dissimilar instance pairs with different labels push farther apart.\nB. Soft Cross-modal Triplet\nA cross-modal triplet (e.g., (ai, vt, v\u00bd)) is composed of a sample (audio ar) as the anchor from one modality and two samples as positive (visual v\u2020) and negative (visual v\u2081) from another modality. The purpose of a cross-modal triplet is to ensure that the positive visual sample vit is closer to the anchor ai than the negative visual sample v\u2081 within a common feature space, thereby improving the model's ability to differentiate relevant from irrelevant content across modalities. In relatively large batch settings, relying solely on annotated labels to determine positive and negative samples will overlook the varying degrees of similarity and dissimilarity between the audio and visual samples. This is because such an approach fails to capture potential relationships beyond the labels, making it difficult to accurately define vt and v. To address this issue, our approach generates soft-alignment labels for audio-visual data by capturing latent features and potential relationships beyond explicit annotations. This process allows the teacher model to iteratively refine the classification of samples by adjusting the identification of soft positives t and soft negatives \u00fb based on the learned distributions. The soft-alignment labels guide the model in distinguishing subtle variations in semantic relationships, leading to improved matching and overall performance.\nSoft-alignment labels are generated by the teacher model trained on a subset of every batch. The teacher model consists of audio and visual encoders fa(\u00b7) and fr(\u00b7), the output logits \u00c2 = {\u00e2\u00bf} \u2208 R^{N\u00d7128} and \u2228 = {0} \u2208 R^{N\u00d71024} used to produce soft-alignment label distributions La and LV, which defined as:\nLa = \u03c3(V \u00b7 \u00c2T) and LV = \u03c3(\u00c2\u00b7 VT) \\ (1)"}, {"title": "C. Progress Self-distillation", "content": "where \u03c3(\u00b7) is the softmax function to transform raw logits into probability. The soft label distributions are used to construct soft cross-modal triplets by incorporating the adjacency matrix A and the adjacency-not matrix A into the loss calculation. They are defined as:\nAij =\\begin{cases}1 & \\text{if } \\operatorname{argmax}(L_i^a) = \\operatorname{argmax}(L_j^v) \\\\0 & \\text{otherwise}\\end{cases} \\\\ Aij =\\begin{cases}0 & \\text{if } \\operatorname{argmax}(L_i^a) \\neq \\operatorname{argmax}(L_j^v) \\\\1 & \\text{otherwise}\\end{cases} \\\\(2)\nwhere the L refers to the i-th column of La and L cor-responds to the j-th row of L. Incorporate these matrices into the triplet loss to identify the positive visual vt = 0; (Aij = 1) and the negative visual v\u2081 = \u00fbk (Aik = 1) for ai. By using the AA(\u00b7) proxy [27] for each representation to enhance the correlation between positive samples, our final cross-modal triplet loss can be formulated as:\nl_{cross} = \\sum_{i=0}^{N-1} \\max{0, d(AA(a_i), AA(v^+))\n-d(AA(a_i), AA(v_i)) + \u03b1}\\\\(3)\nwhere d() are the normalized Euclidean distance, N is the batch size.\nC. Progress Self-distillation\nSelf-knowledge distillation [3]-[5] allows a student net-work to act as its own teacher, cutting computational costs compared to traditional methods. Progressive self-knowledge distillation enhances this by gradually evolving the student into its own teacher during training, leading to a more dynamic and efficient learning process. Our progressive self-distillation process begins with a batch of N samples, which is randomly partitioned into two subsets: N1 = [rN] samples with ground-truth labels and N2 = N - [r. N] samples without labels. Initially, the subset N\u2081 is used to train the teacher network with ground truth pairings. The teacher network then generates soft-alignment labels for the subset N2, which are used to supervise its own student model.\nTo enhance the teacher's influence on learning, we progres-sively reduce r. This gradual decrease ensures that the student network increasingly relies on its own predictions over time. We use a step-wise schedule to adjust r from a specified start value (1.0) to an end value (0.2), as this method has proven more effective for our model than other strategies, such as linear or cosine-annealing schedule [14].\nBy incorporating this progressive self-distillation approach, our model dynamically evolves and improves its represen-tational capabilities. In addition, we reduce the cross-modal dependency of exact audio-visual pairs while preserving the original alignment, achieved by ldis loss. The final objective loss is defined as follows:\nLoss = llab + Icross + Idis \\\\ llab = \\frac{1}{n}||f_a(a_i) - Y(a_i)||_F + \\frac{1}{n}||f_v(v_i)-Y(v_i)||_F \\\\ (4) \\\\ ldis = \\frac{1}{n}||f_a(a_i) \u2013 f_v(V_i)||_F\nwhere || || F signifies the Frobenius norm, f(x) represents the projected feature in the shared label space and Y() denotes the label representations. The final objective loss is optimized using the stochastic gradient descent (SGD) [17]."}, {"title": "IV. EXPERIMENTS", "content": "A. Datasets and Metrics\nOur model effectively performs the AV-CMR task by leveraging the assumption that audio and visual modalities share identical semantic information. We use two audio-visual datasets, VEGAS [32] and AVE [18], where the VEGAS includes 28,103 videos with 10 labels, and the AVE com-prises 1,955 videos with 15 labels. We follow the same data partitioning and feature extraction methods as detailed in prior work [30]. We adopt the mean average precision (MAP) used in related works [28], [29] as the evaluation metric.\nB. Implementation Settings\nOur model incorporates three fully connected (FC) layers and a prediction layer for audio and visual inputs Each FC layer has 1024 hidden units with a dropout rate of 0.1 to prevent overfitting. The dimensionality of the projected features matches the number of labels: 15 for AVE and 10 for VEGAS, aligning audio and visual features in a common space. The model is trained with batch sizes of 400 for both VEGAS and AVE, over 1,000 epochs. The triplet loss function is optimized using margin values of 1.2. We utilized the Adam optimizer [9] with default parameters, setting the learning rate at 0.0001 for all training procedures. These settings are consistent with the baselines in Table I. Our experiments were conducted using the PyTorch and run on an Ubuntu Linux 22.04.2 system with an NVIDIA GeForce 3080 (10 GB) GPU.\nC. Results\nTo evaluate our method, we compared it with 9 algorithms: CCA-variants and deep learning (DL)-based methods. CCA-variants includes CCA [2], and TNN-C-CCA [30]. DL-based methods contain EICS [29], VideoadViser [21], CCTL [28], TLCA [25], and AADML [27]. We applied our approach to the AV-CMR task and compared it against the aforementioned methods on two audio-visual datasets shown in Table I. Our method outperforms the others in all MAPs on both datasets, achieving gains of 1.9%, 2.4%, and 2.1% on AVE, and 1.7%, 1.9%, and 1.8% on VEGAS in terms of A2V, V2A, and Aver-age, respectively. These results demonstrate the effectiveness of our proposed model."}, {"title": "V. CONCLUSION", "content": "Our approach overcomes the limitations of label-guided representation learning in existing metric learning methods by integrating cross-modal triplet loss with progressive self-distillation, enabling more robust audio-visual embedding learning. Our experiments demonstrate substantial improve-ments over existing methods in cross-modal retrieval tasks. Future work will extend this approach to other multimodal scenarios and explore real-world applications."}]}