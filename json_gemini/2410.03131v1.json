{"title": "AIME: AI System OPTIMIZATION VIA MULTIPLE LLM EVALUATORS", "authors": ["Bhrij Patel", "Souradip Chakraborty", "Wesley A. Suttle", "Mengdi Wang", "Amrit Singh Bedi", "Dinesh Manocha"], "abstract": "Text-based AI system optimization typically involves a feedback loop scheme where a single LLM generates an evaluation in natural language of the current output to improve the next iteration's output. However, in this work, we empirically demonstrate that for a practical and complex task (code generation) with multiple criteria to evaluate, utilizing only one LLM evaluator tends to let errors in generated code go undetected, thus leading to incorrect evaluations and ultimately suboptimal test case performance. Motivated by this failure case, we assume there exists an optimal evaluation policy that samples an evaluation between response and ground truth. We then theoretically prove that a linear combination of multiple evaluators can approximate this optimal policy. From this insight, we propose AI system optimization via Multiple LLM Evaluators (AIME). AIME is an evaluation protocol that utilizes multiple LLMs that each independently generate an evaluation on separate criteria and then combine them via concatenation. We provide an extensive empirical study showing AIME outperforming baseline methods in code generation tasks, with up to 62% higher error detection rate and up to 16% higher success rate than a single LLM evaluation protocol on LeetCodeHard and HumanEval datasets. We also show that the selection of the number of evaluators and which criteria to utilize is non-trivial as it can impact pact success rate by up to 12%.", "sections": [{"title": "1 INTRODUCTION", "content": "Pre-trained foundation models, such as Large Language Models (LLMs), have developed rapidly over the recent years (Achiam et al., 2023; Touvron et al., 2023). With these advancements, AI systems have grown in popularity for various tasks such as code generation (Chen et al., 2024; Gulwani, 2010), question-answering (Patel et al., 2024; Wang et al., 2024), mathematical reasoning (Trinh et al., 2024; Song et al., 2024), exploration (Dorbala et al., 2024; 2023; Ren et al., 2024), and information retrieval (Gao et al., 2023) etc. As the application complexity increases, the shift to AI systems containing multiple components such as LLM-based agents and web search (Xiong et al., 2024), will continue (Zaharia et al., 2024; Yuksekgonul et al., 2024). Thus, automatically optimizing these systems, Al system optimization (Yuksekgonul et al., 2024), becomes increasingly necessary.\nAn emerging paradigm is text-based optimization, also known as prompt optimization (Cheng et al., 2023; Wang et al., 2023; Zhou et al., 2022), whereby the natural language input prompt is tuned to generate an optimal output. This method requires no numerical gradient descent updates typical in optimization for machine learning models (Van Der Malsburg, 1986; Hassoun, 1995; Barto, 1992) and is thus appropriate for optimizing AI systems with fixed LLM components. Recently, there has been a growing class of iterative online methods for text-based optimization (Cheng et al., 2024;"}, {"title": "2 TEXT-BASED AI SYSTEM OPTIMIZATION", "content": "Objective Function. In this section, we now characterize mathematically text-based prompt opti-mization as a system of LLM-based policies. Let \u03c0(\u00b7|x) be the LLM-based AI system parameterized"}, {"title": "3 \u0391\u0399\u039c\u0395: AI SYSTEM OPTIMIZATION VIA MULTIPLE LLM EVALUATORS", "content": "Our key idea is to utilize multiple evaluations than single evaluators used in state-of-the-art. The thought that multiple evaluators would work better than one sounds intuitive but a naive introduc-tion of multiple evaluators does not work in practice. We theoretically prove the merit of multiple evaluators and then discuss how to introduce them into the pipeline described in Section 2."}, {"title": "3.1 INCREASING EVALUATIONS REDUCES THE EVALUATION SUBOPTIMALITY GAP", "content": "Let \nI = {\u03c0\u03ba(\u00b7|x, y)}Kk=1\nbe the set of diverse evaluators for x, y. We start our theoretical justifica-tion by defining the sub-optimality metric to measure the evaluation performance between \u3160 and II as\n\u2206Eva-sub-opt = Ee*~\u03c0*(\u00b7|x,y) [e] - E{ek~\u03c0k(\u00b7|x,y)}Kk=1 [g(e1,\u2026\u2026, \u0435\u043a)],\nwhich is nothing but the difference between the expected value of the evaluation under the optimal unknown evaluation distribution, and the expected function g(...) which maps the K different evaluations to one. In practice, g can be seen as an aggregation function such as concatenation. Note that if we had access to the optimal evaluator \u03c0*, we would have been able to get the ground-truth evaluation e* = l(y*, y) to perform the AI text optimization. However, in place of that, we have a diverse set of evaluators II = (\u03c01, \u03c02\u00b7\u00b7\u00b7 \u03c0\u03ba) and g(e1, e2\uff65\uff65\uff65ek) is the aggregation function to combine the losses from the diverse evaluators. We provide the following theorem to relate the number of evaluations to the Eva-sub-opt\nTheorem 1. Let dry denote the total variation distance between two distributions and let \n\u03a3Kk=1 ak = 1\n. Assuming all pairs \u03c01, \u03c02 \u2208 \u03a0 are independent of one another,\n\u2206Eva-sub-opt <le* dTV (\u03c0, \u03a3Kk=1 ak\u03c0k)."}, {"title": "3.2 OVERVIEW OF AIME: MULTIPLE ROLE-SPECIFIC EVALUATORS", "content": "Now that we have motivated utilizing multiple LLM-based evaluators, we now address the question on how to utilize multiple evaluators. To do so, we look at the ideas of roles. The LLM-based evaluation policy has an evaluation system prompt to specify what the evaluation should be based on. For tasks such as code generation, there may be multiple criteria or objectives to evaluate for such as correctness, clarity, and efficiency. Furthermore, aspects such correctness of code can rely on various aspects such as logic and syntax. Normally, with a single evaluator, all the criteria are specified together in the system prompt. However, we see from Figure 1 and later in Section 4 that this approach can fail significantly to reach the optimal performance. We thus propose splitting the evaluation instruction across multiple evaluators, each one getting a specific role. We then aggregate via string concatenation them into a final evaluation. We chose concatenation as the aggregation method as it is analogous to creating a linear combination of the outputs (Yuksekgonul et al., 2024). We call this approach AIME: AI System Optimization with Multiple Evaluators.\nOur AIME approach is a simple-to-implement approach that requires minimal changes to the already established methods (Yuksekgonul et al., 2024; Cheng et al., 2024) for system optimization. Our approach requires only modifying the evaluation step of the optimization pipeline from one evaluator to multiple. In Algorithm 1, given an output y, set of k roles R, and pre-trained LLM \u03c0\u03b8 we sample k evaluations, {ek}K_1. We obtain er by conditioning \u03c0\u03bf by x, y and Rk \u2208 R. Conditioning on k is to specify the role in the evaluation system prompt."}, {"title": "4 EXPERIMENTS AND RESULTS", "content": "We test the merits of our AIME approach via the code generation task because of its practicalness and its multiple plausible criteria (e.g., correctness, efficiency). Here, the AI system is an LLM generator that is given a code prompt and must produce a code snippet that passes the unit tests for that prompt. This code generation task is a form of instance optimization (Yuksekgonul et al., 2024), whereby the optimization variable, the input prompt, is defined as xt+1 := (yt, ft). Yo, fo are empty strings. We provide empirical results showing that AIME is superior to the single-evaluation (Single-Eval) approach in detecting code errors and that AIME-based optimization achieves higher success in test cases than Single-Eval-based optimization. Experiments were run on an Apple M1 Pro and macOS 14.5.\nAIME and Single-Eval Implementation Details: We use TextGrad from Yuksekgonul et al. (2024) to implement AIME and Single-Eval. We chose TextGrad because it separates the evaluation and"}, {"title": "4.1 \u0391\u0399\u039cE IS ROBUST TO INCORRECT EVALUATIONS", "content": "AIME has a higher chance to catch errors: Figure 1 displays portions of an evaluation generated by Single-Eval and AIME. In this scenario, the evaluations were generated for the same coding problem at the second iteration of optimization. For both Single-Eval and AIME, the code failed all test cases, thus meaning there exists some error in the code. The evaluation from Single-Eval for both correctness and logic states there is nothing wrong. For AIME, the correctness evaluator incorrectly states nothing is wrong with the generated code but the logic evaluator detects a logical error. In the next iteration of optimization, the code generated based on the Single-Eval evaluation still fails all cases but the code generated from AIME passes them all.\nError Detection Measurement: To quantitatively analyze the error detection of AIME, we develop a heuristic measurement, Error Detection Rate (EDR). For each optimization iteration that has at least one failed test case, if the given evaluation contains at least one phrase indicating failure, we consider that as an error was detected. For example if the phrase \u201chas a logical error\" appears in the evaluation, we count that as an error detected. We provide a complete list of phrases used for detection in Appendix A.2. Let Zfail be the set of iterations with at least one failed test case and let\nQ(z) = 1error detected\nbe the indicator value of whether an error was detected at iteration z \u2208 Zfail. We calculate the EDR as\n\u03a3z\u2208Zfail Q(z) /\nLeft of Figure 2 shows AIME has up to ~ 62% higher EDR than Single-Eval. Table 2 in Appendix A.3 summarizes the EDR for Single-Eval and AIME across various evaluation call temperatures. AIME achieves ~ 53 - 62% higher error detection rate than Single-Eval on LeetCodeHard and ~ 38 -57% higher rate on HumanEval. This demonstrates that"}, {"title": "4.2 \u0391\u0399\u039cE-BASED OPTIMIZATION ACHIEVES HIGHER TASK PERFORMANCE", "content": "Now that we have established the error detection capabilities of AIME over Single-Eval, we now fo-cus on the overall performance of system optimization with AIME on the code generation task. For these experiments, we provide results with two additional baselines: 1) Zero-Shot: Initial generated code with no iterative optimization process; 2) Refinement with No Separate Text-based Evalua-tion Step (Implicit Eval): The evaluation and feedback steps are within the same LLM \u201creflection\" call. The LLM reflection call is allowed 3600 max output tokens and is sampled once per iteration. We implement this baseline with Reflexion by Shinn et al. (2024).\nMetrics for Code Correctness: We report the following metrics to inspect the correctness of the code generated; for AIME, Single-Eval, and Implicit Eval, we report these metrics using the best-performing code generated in the optimization process after the initial zero-shot generation: 1) Success Rate (SR), the percentage of test cases passed across the entire dataset; 2) Completion Rate (CR), the percentage of coding problems with all passed test cases."}, {"title": "4.3 ABLATION STUDIES", "content": "We perform two experiments: 1) for AIME-based optimization, we ablate on the number of eval-uators from 1 \u2192 3 \u2192 6. However, each evaluator has the same role. Max output tokens in each experiment across all evaluators is 3600. When all the evaluators have the correctness role (left of Figure 5), the EDR for AIME increases. This result emphasizes that AIME-based evaluations, even without role-specific evaluators, can detect more errors than Single-Eval. This finding then begs the question of whether there is a need for different roles to optimize for passed test cases if increasing the number of same-role evaluators already helps. When comparing the SR, CR, and EDR of AIME with 6 correctness evaluators against AIME with 6 distinct roles (correctness, logic, syntax, read-"}, {"title": "4.3.2 \u0421\u043eMBINATION OF EVALUATION ROLES AFFECTS OPTIMIZATION PERFORMANCE", "content": "We now analyze the effect the different roles have on SR and CR on LeetCodeHard. We perform this study for two reasons: 1) to see the change in performance due to utilizing various evaluation roles and 2) to see how the relative performance between Singl-Eval and our AIME changes based on the roles given. The total max output tokens for evaluation is still 3600, and for AIME, it is distributed equally across the evaluators. Therefore, for experiments with 3 evaluators, each one has max output tokens of 1200."}, {"title": "5 RELATED WORKS", "content": "AI System Optimization: Many prior works have studied the optimization of complex AI systems.? was one of the first works to propose a text-based iterative feedback loop for refining LLMs, and Pryzant et al. (2023) established text-based gradients, or Textual Gradients, as feedback to an AI system. DSPy (Khattab et al., 2024; 2022; Singhvi et al., 2023), Trace Cheng et al. (2024), and TextGrad (Yuksekgonul et al., 2024) have formulated LLM and AI-based systems as a network of multiple layers and provided methods to optimize these system analogous to backpropagation and autodifferentiation. Chakraborty et al. (2024a); Ding et al. (2024) used a bi-level optimization formulation to align AI agents and systems. Text-based reinforcement learning has also been used to improve LLM-based systems (Shinn et al., 2024). Decoding and RLHF is an alternative method to optimize or align an LLM with gradient descent (Chakraborty et al., 2024b; Mudgal et al., 2023; Chakraborty et al., 2024c). While these works have shown tremendous results, there has been a gap in the literature we aim to address analyzing the effect of using multiple independent evaluations to optimize the AI system for a complex task, code generation (Chen et al., 2024; Zeng et al., 2024; Zhang et al., 2023; Jha et al., 2010; Shinn et al., 2024; Yuksekgonul et al., 2024; Zan et al., 2022; Jiang et al., 2024; Chen et al., 2021; Gulwani, 2010).\nLLM-based Evaluation: LLM-based evaluation, or LLM-as-a-Judge (Zheng et al., 2023), has been growing in interest due to the ability of LLMs to evaluate large outputs like text (Sellam et al., 2020; Kocmi & Federmann, 2023) quickly and to align with human preferences. Verga et al. (2024) showed a panel of smaller LLM judges can provide numeric scores correlating to human judgment than a single larger LLM model can. Prior work has also studied finetuning LLMs to be judges (Zhou et al., 2022). Ankner et al. (2024) used LLM-generated critiques to augment the scalar reward from a reward model. Li et al. (2023) used discussion between multiple LLMs to select a strong LLM-based evaluator for question-answering. Strong LLM judges have been shown to generalize across tasks (Huang et al., 2024). Weak LLM evaluators have been used to judge the debate between two stronger LLMs (Kenton et al., 2024). We are the first to use multiple LLM-based evaluators for iterative AI system optimization."}, {"title": "6 CONCLUSION, LIMITATIONS, AND FURTHER WORKS", "content": "In this work, we tackle AI system optimization by introducing AIME. AIME utilizes multiple LLM-based evaluators to provide natural language evaluation for the current system output, improving on prior methods that only use a single evaluator. Our key insight is to condition each evaluator with a specific role rather than giving all the roles to a single evaluator. We prove that increasing the number of evaluations reduces the suboptimality evaluation gap, and empirically demonstrate that AIME outperforms Single-Eval in code generation tasks, analyzing success, completion, and error detection rates. Furthermore, we study AIME's robustness to the adversarial evaluator that generate incorrect evaluations. We also provide ablations such as on the diversity of roles, role combinations, and evaluation temperature, consistently demonstrating AIME's superior performance and the need for multiple evaluators.\nLimitations and Further Work. We only empirically study our approach in code generation. Further work could extend this evaluation approach to other tasks that require multiple criteria like molecule optimization or text generation. In terms of system complexity, we only study multiple evaluators for AI systems comprising a single LLM-based agent, and using a compound system with multiple elements such as a web search agent (Agentic AI system) could be interesting. Another aspect of the work that can be explored further is weighting the different LLM-based evaluations. We gave uniform weighting to all evaluations by giving them the same max output tokens and concatenating them. Future research could investigate methods of weighting and aggregation, possibly using another LLM to summarize or perform best-of-N on the evaluations."}, {"title": "A APPENDIX", "content": "A.1 EVALUATION SYSTEM PROMPT\nWe provide the evaluation system prompt in Figure 6. For Single-Eval the system prompt is given to only one LLM call and all the roles utilized are listed together in [INSERT UTILIZED ROLE]. For AIME, each evaluator gets one role specified in [INSERT UTILIZED ROLE].\nRemark: It may seem conflicting that we specify conciseness in the evaluation system prompt and highlight that the evaluations from AIME are more descriptive in Figure 3. However, we would like to clarify that we do not believe that the evaluations are verbose, using more words without giving more information. The longer, thorough evaluations from AIME like in Figure 3 provide more information on their judgment, helping with the explainability of the evaluation model."}, {"title": "A.2 ERROR DETECTION PHRASES", "content": "Below is the list of phrases we used to analyze the error detection of evaluations,\n\u2022 has logical errors\n\u2022 contains logical errors\n\u2022 has a logical error\n\u2022 contains a logical error\n\u2022 is incorrect\n\u2022 to be incorrect\n\u2022 has a syntax error\n\u2022 contains a syntax error\n\u2022 contains syntax errors\n\u2022 has syntax errors\n\u2022 has several issues\n\u2022 does not correctly\n\u2022\nappears to be mostly correct\n\u2022 have several issues\n\u2022 has several issues\n\u2022 flaw\n\u2022 incorrect\n\u2022 not correct\n\u2022 some issue\n\u2022 there seems to be some issues\n\u2022 has issue\n\u2022 have issue"}]}