{"title": "DreamGarden: A Designer Assistant for Growing Games from a Single Prompt", "authors": ["SAM EARLE", "SAMYAK PARAJUILI", "ANDRZEJ BANBURSKI-FAHEY"], "abstract": "Coding assistants are increasingly leveraged in game design, both generating code and making high-level plans. To what degree can these tools align with developer workflows, and what new modes of human-computer interaction can emerge from their use? We present DreamGarden, an AI system capable of assisting with the development of diverse game environments in Unreal Engine. At the core of our method is an LLM-driven planner, capable of breaking down a single, high-level prompt-a dream, memory, or imagined scenario provided by a human user-into a hierarchical action plan, which is then distributed across specialized submodules facilitating concrete implementation. This system is presented to the user as a garden of plans and actions, both growing independently and", "sections": [{"title": "1 INTRODUCTION", "content": "A recent boom in generative Al has led to the widespread adoption of assistive AI tools by human creators both casual and professional in generating text, code, images and 3D assets. The AI models underpinning these tools are generally large neural networks, trained to predict or denoise next tokens in corpora of human-generated content scraped from the internet. Large language models are perhaps most ubiquitous, existing most commonly in the form of chatbots, predictive text models which have been fine-tuned via specialized datasets and human feedback to be generally \"helpful\" and excel in following user requests [26, 37, 54]. In the visual domain, diffusion models have brought on similarly impressive capabilities, allowing anyone to create visually convincing results from a single prompt [35, 42, 44]. Most of these assistants respond however only to direct user prompting, and do not work autonomously toward a high-level goal.\nIn game design, procedural content generation (PCG) has long been leveraged to various ends; in turn effectively allowing for compression of large environments into generative algorithms to be executed at runtime (particularly in the early days of digital games, when disk space was a significant bottleneck), affording designers a new means of expression and a higher level of abstraction when designing content, accelerating the design process, and producing novel and surprising artifacts leading to increased replayability from the perspective of the player. PCG via machine learning is an active area of research, though industry professionals tend to still rely on more symbolic algorithms that provide finer-grained control and a greater degree of expressivity and interpretability.\nIn this work, we introduce DreamGarden, an AI-driven assistant for game design, which works semi-autonomously toward a high-level goal by growing a \"garden\" of plans and actions, where intermediary output is human-interpretable and editable. In designing this system, we pursue a sliding scale between autonomy and reactivity. Our target for a game design assistive tool is in the early ideation stages and prototyping, with a goal of eventually enabling rapid prototyping of playable game snippets. As a first step towards this, we explore a tool which elaborates on an initial game design plan and then proposes a concrete implementation plan for building the experience in the Unreal game engine, which it then delegates to various agents in an orchestration that is decided on recursively by an LLM planner. DreamGarden is a first such attempt at an automated game prototyping tool to our knowledge.\nTo validate the system and explore its potential as a new means of human-computer interaction, we conduct a study with the following research questions in mind:\n(Q1) Is the system capable of transforming open-ended, potentially dream-like natural language prompts into func-tioning 3D simulation environments?\n(Q2) Is the system's hierarchical and iterative process intuitive and accessible to users?\n(Q3) Does DreamGarden's user interface provide ample means for users to intervene in this process, at various points in time and at various levels of abstraction?"}, {"title": "2 RELATED WORK", "content": "DreamGarden is conceived of both as a designer facing tool and a self-perpetuating ecosystem of design ideas and content. In terms of this latter autonomous nature, we take inspiration from the field of Artificial Life [2], which partially involves the design of simulated environments in which complex and life-like phenomena are able to emerge automatically. Dave Ackley's Moveable Feast Machine (MFM), for example, offers a compelling early example of a decentralized computational model designed for infinite scalability [1]. Drawing inspiration from its evolving, adaptable ecosystem, we conceptualize our system as a form of a \"garden.\" In this framework, we design agents focused on specialized tasks ranging from asset generation to code execution, including one that focuses on automated debugging akin to the self-healing wire within the MFM. Through the interaction of these agents, our system exhibits continuous growth fostering emergent behavior within game design.\nMore recently, the Artificial Life system Lenia [7], a continuous extension of John Conway's ubiquitous Game of Life [25] has been combined with open-ended search algorithms such as Quality Diversity evolutionary search [40] for the automatic discovery of novel and interesting \"life-forms\" [15].\nOther approaches to open-ended search involve varying degrees of human involvement. [8] develop an online Twitterbot which seeks to autonomously generate aesthetically pleasing gridworld game maps by soliciting human feedback via automated polls. Users are also able to design their own levels on the platform, which are pitted against output from a small generative model that learns to imitate highly-rated levels. PicBreeder [45], on the other hand, is entirely dependent on user involvement. But given its idiosyncratic representation of images as Compositional Pattern Producing Networks [47], the participation of users in interactively evolving generative designs results in the discovery of novel designs without users setting these discoveries as explicit objectives in advance. Though DreamGarden is framed as a system for translating a prompt to a corresponding end product, the plans and artifacts it sometimes generates (e.g. to generate 2,000 blades of grass individually, or the expressionistic and self-colliding meshes resulting from initial attempts to procedurally generate standard objects) could potentially be explored as art objects on their own merits, with the system's intermediary outputs repurposed for divergent, open-ended search [48]."}, {"title": "2.2 Generative design assistants", "content": "Large models have been increasingly studied for their potential in assisting in creative design tasks. The authors of [30] developed a web-based tool which uses text-to-image models to help users prototype their design concepts, rapidly translating between text and images with an eye toward bridging the gap that can emerge between artists and designers when communicating across these modalities. LLMs have been used to power an evolutionary engine for collaborative game design in [28]. Such LLM-driven tools build on prior work using more conventional methods. Sentient Sketchbook [29], for example, uses novelty search to provide alternatives to human designs, and automates playability checks; while [36] develop an authoring tool for machinima, automating aspects like character control and shot framing to help novice creators readily apply conventional cinematic forms.\nLLMR [53] is another example of a multi-agent system that translates input prompts to code. Their platform is focused on the Unity game engine whereas we target more high fidelity game design in Unreal. Fundamentally, LLMR is a tool for casual creators [10] enabling real-time scene and environment generation in VR for immediate use. We focus on game creation over an extended time horizon necessitating the use of hierarchical planning. Our tool offers more specialized usability for game designers, enabling them to begin with minimal high-level input followed by an automated and adaptable process. Whereas in LLMR, interaction with the system is designed to be seamless, with intermediary prompts and code largely concealed from the user, our user interface is designed to expose this process in an intuitive way, allowing ample opportunities for designer intervention."}, {"title": "2.3 World models", "content": "In reinforcement learning, where game-playing agents are trained via a reward corresponding to in-game score or success, world models-generative models of game mechanics and levels [21]-have been developed to alleviate the bottleneck of running expensive game simulations during agent training, often leading to highly performant game-playing agents [22]. Generally, these models predict the next frame of the simulation given the previous frame(s) and current agent action. Similar models have also been explored for their purely generative capability, with [6] training a model on a large internet corpus of videos of gameplay of diverse games to predict next frames and infer player actions, then prompting it at runtime to generate next frames of imagined games given out-of-distribution input such as human-drawn sketches. [55] showed that diffusion models can serve as game engines, with a demonstration of stable simulation of the classic game Doom running at 20 fps on a single TPU, though for relatively short times before losing stability.\nOne possible issue with such approaches, when thinking of assistive tools for game designers, is that they are trained to generate the game at the level of pixels. A design workflow involving such models, then, limits designer intervention to the provision of the initial prompt: the model does not manipulate any symbolic representation of the game world, and designers cannot tweak its output with any level of detail once the generation process is complete, nor do the models make use of existing methods for rendering assets or encoding game logic which might make their output more reliable and their generative task more tractable (as a result, these models often lose coherence quite rapidly after generating beyond the initial prompt).\nThe system developed in this paper, by contrast, develops an interface between generative models and an existing game engine, namely Unreal Engine [14], a robust, open-source editor for game levels, assets and logic, popular both with indie developers and triple-A studios."}, {"title": "2.4 Automatic game generation", "content": "Prior work has explored the use of language models to generate games from scratch. Word2World [34] generates a narrative from scratch-which tend to be of a Brothers Grimm fairy tale variety [20]-and subsequently uses this narrative to specify a set of characters, a tileset, and a sequence of grid-based level layouts. Individual tiles are then generated using the text-to-image model Stable Diffusion [44]. Here, game mechanics and objectives are essentially fixed in advance, and limited to player movement, the impassibility of certain tile types, and achievement of certain objectives by reaching special tiles.\nBy contrast, DreamGarden is restricted to generating 0-player simulations, though the mechanics-in terms of interactions between abstract Actor classes written in C++ -are unrestricted and can be determined from scratch by our code generation submodule.\nThe procedural generation of entire games has been an area of research prior to the emergence of large pretrained models, with [11] raising pressing questions around the valuation of creativity and the participation of such autonomous agents in game development communities. While our system could also be let loose as an autonomous agent, this angle is at present somewhat less viable because of the scope of our ambition in generating fully open-ended 3D simulations via arbitrary code. We also take for granted that-no matter how much the frontier of automatic game generation is pushed forward by the orchestration of increasingly sophisticated specialized models-the components of such a system will inevitably be repurposed as designer-facing tools, whether by experimental AI artists or seasoned developers. In this paper we therefore focus mainly on the possible modes of interaction afforded within such systems.\nRosebud AI [3] serves as a notable startup in this domain, providing an AI-powered platform for creating browser-based games in JavaScript, along with a collaborative community where game developers can share and build upon each other's creations. Our method offers a similar natural language-to-code tool but focuses on structured, plan-to-action workflow. This workflow guides users through the necessary sub-tasks to fully realize their creative vision. Furthermore, while Rosebud AI focuses on 2D browser games, we specialize in the 3D domain allowing us to support more complex asset creation and immersive environments."}, {"title": "2.5 Large models for code, asset and environment generation", "content": "Alongside the a nascent push to leverage large models for automatic game generation, LLMs have also been applied to game design sub-tasks, such as the generation of levels in Sokoban [52] and Super Mario Bros. [49]. Meanwhile, large text-to-image models have been leveraged for the generation of structures and environments in MineCraft [13].\nA large body of work has been focused on creation of assets that could be used in games. Much of this focus has been on the automated creation of 3D assets and textures, for example [9, 17, 31, 43, 51]. Text2AC [50] is a hybrid approach using LLMs and diffusion models to generate game-ready 2D characters. Automated animation of rigged assets has been considered for example in [24] using LLMs to generate structured outputs of joint rotations at key frames. Animation generation has also been explored in [23].\nOMNI-EPIC [16] uses language models to generate 3D environments and tasks for embodied agents. DreamGarden similarly leverages large models for feedback on generated environments, but also considers aesthetic and non-functional aspects of the environment. Instead of being geared toward RL agents, DreamGarden is at root a user-facing tool. In future work, however, it could be combined with open-ended player agent learning to result in more functionally complex games with sophisticated player mechanics."}, {"title": "3 METHODS", "content": "We have three main goals in the design of our system. First, we explore the capabilities of large models in generating interpretable, designer-editable representations of functional game simulations from a single, high-level prompt. Second, we develop a user interface for intervening with the system at various points in the generation process and at varying levels of abstraction. Finally, we seek to validate our approach, by conducting an in-person usability study in which participants with interest or experience in game design interact with and give feedback on the system.\nIn order to effectively generate entire simulations from a single prompt, we argue that three elements are crucial. First, just as most game design involves multiple specialists (or at least the donning of many caps by a generalist designer) multiple specialized AI agents are required to competently perform the plethora of semi-independent tasks involved in full-fledged game design. Second, these specialists must be orchestrated dynamically and automatically, as not all game design endeavors involve the execution of the same tasks in the same order. Finally, this orchestration must be carried out via hierarchical planning, with high level goals broken down recursively into sub-goals and implementation tasks, to ensure coherence between high-level objectives and actual implementation steps.\nIn terms of the user interface, we develop a node-based visual interface for editing steps in the hierarchical planning tree, pruning or expanding sub-plans, and providing feedback on the simulation resulting from generated code and assets.\nVia our usability study, we seek to answer our questions about the system's capabiltities (Q1), users' perception of its planning and implementation process (Q2), and their means of interaction with it (Q3). By recruiting a mix of professional and novice designers, along with more casual creators, we are particularly interested in the diversity of modes of interactivity that might emerge among users of the system, and how future development of the system can serve to accomodate them effectively."}, {"title": "3.2 System overview", "content": "In this section we give a detailed description of the DreamGarden pipeline."}, {"title": "3.2.1 Planning module", "content": "We begin with the planning module, whose input is the seed prompt, which is open-ended and may describe a dream, an imagined scenario, or a rough game sketch. This is fed into the broad planner, which is system-prompted to take this open-ended input and reformulate it as an outline for a video game to be implemented in Unreal Engine. This submodule is also asked to create a broad, high-level plan for implementing the game. This high level game description and broad plan is parsed, and converted to the root and child nodes-the game reformulation and broad plan steps, respectively-of a hierarchical plan tree.\nNext is the sub-planner. This submodule is shown the entire plan tree in its current state, and asked to re-articulate a particular node (a step or sub-step of the plan) in more detail, then further expand it into a list of sub-steps. The sub-planner is applied to the tree breadth-first, and at each step its output is parsed, and a node is added to the working tree for each new generated sub-step. Both planner sub-modules are given a description of the available implementation submodules-each specializing in a particular concrete code or asset generation task-and told that the tree they generate must ultimately terminate in leaf nodes that each correspond to an individual task for one of these submodules. The sub-planner is able to flag a generated node as a leaf node by appending special text to its output that indicates to which submodule the node is to be assigned. The sub-planner's tree generation process terminates when all plan nodes have children or are marked as leaf nodes.\nThe leaf nodes are then translated into prompts or other formatted inputs for the implementation submodules by the task generator, whose prompt is adapted to give general guidance on how best to prompt for the particular implementation submodule at hand. The corresponding implementation tasks are then executed in sequence-visiting sub-plan steps in order-with the generated assets from prior tasks fed as input to subsequent submodules. This process of recursively growing the plan tree then translating its leaf nodes into implementation task prompts is shown in Figure 3."}, {"title": "3.2.2 Implementation submodules", "content": "We now describe the specialized submodules designed to carry out individual implementation tasks.\nCode generator. The coding submodule is the most general and complex submodule in DreamGarden (see Figure 4). It involves the generation of C++ code for Unreal Engine \"Actors\"-essentially, general discrete entities within the simulation-and the generation of an initial layout for these actors in the level, which layout is then instantiated via a hard-coded python script inside the editor. In general, we observe that code generation models like [4, 32, 59] are trained on large volumes of C++ code which grant them capabilities in implementing a wide range of functions in Unreal"}, {"title": "3.2.3 Graphical user interface", "content": "We develop a Graphical User Interface (GUI) for DreamGarden, to expose its growing the user's seed prompt in a more intuitive and visually informative way. Because the action plan is effectively a tree, the basis of our GUI is a node-based editor, i.e. an interface in which users can manipulate nodes in a tree. Nodes are either (sub-)steps in the hierarchical plan, implementation tasks, or steps taken by implementation submodules, and edges correspond to the hierarchical relationship between planning steps and tasks, and the the chronological flow of implementation steps. The GUI is shown in Figure 7\nTo facilitate responsiveness between the system's backend and the GUI, we structure the backend call as a repeated function on a \"frontier\" of nodes to be expanded/implemented. The user's edits can effectively result in pruning this tree, modifying nodes, and adding/removing them from this frontier."}, {"title": "User edits", "content": "Each node in the action tree has an \"is leaf\" checkbox. When toggled by the user, this determines whether or not the given node is considered as a leaf in the tree (i.e. the description of a concrete implementation task to be handed off to a particular submodule). By turning a non-leaf into a leaf node (i.e. when the system has unnecessarily expanded a relatively atomic step into sub-steps), the user effectively prunes the tree. During the next call to the node-expansion function, any plan nodes that are (grand-)children of this node are removed. If any extant children are leaves that are attached to task nodes, these task nodes are additionally removed from the tree. Similarly, if these tasks have already resulted in implementation step nodes, then these implementation nodes are also removed (this data is backed up to a separate folder, should the user want to reverse their changes). Finally, because any code and assets resulting from these implementation steps would have been handed down to implementation tasks following it in the ordered sequence of leaf nodes, then any implementation steps belonging to these tasks are also backed up and deleted.\nBy turning a leaf node into a non-leaf node, the user effectively adds this node to the frontier, queuing it for expansion into a sub-plan. Again, if a task node was generated for this leaf, it is deleted, along with any implementation nodes belonging to this task, or any implementation nodes belonging to tasks following it in the ordered list of tasks.\nDuring code (and procedural mesh code) generation, coding attempts and code evaluations are displayed in nodes that are chained in a sequence, moving from left to right out from the parent task node (itself stepping from a leaf node of the plan tree). When the system produces code that compiles and runs successfully in Unreal, screenshots are automatically captured of the first moments of simulation. These screenshots, along with feedback produced by a vision language model based on them (in addition to the runtime logs and generated code and actor layout), are displayed in a special \"visual evaluation node\", which is chained to the \"code attempt\" node from which it stemmed. These nodes are augmented with a \"compile & run\u201d button, which allows the user to compile the code as it was at this state, and launch the editor with the corresponding actor layout. The user is then able to inspect the resultant simulation in Unreal Engine. They may, for example, make edits to generated code (testing them via recompilation from within the editor), which when saved would then be fed into subsequent iterations of code generation. They may also return to the GUI, and edit the feedback generated by automatic evaluation. This will then result in the pruning of all implementation steps having this node as parent (or, spatially speaking in the GUI, to the right of or below it), and the rerunning of"}, {"title": "3.3 Study design", "content": "To explore the interaction affordances of our system, validate it against user needs and behaviors, and explore directions for future development, we conducted a usability study with N = 10 participants. These participants were were recruited via emails and posts in messaging groups from within the researchers' organization. Users first filled out an intake survey, in which they specified their levels of experience with game design, generative AI, and Unreal Engine. The users were then given a brief overview of the system's intended purpose, and the means they had available to interact with it. Users were first invited to imagine a high-level seed prompt, with little restriction on form or content, which they then entered into the initial \"dream\u201d node in the GUI. The study was conducted in person, with the DreamGarden system running on a laptop with a top-tier GPU. Users were encouraged to observe the system's operation by interacting"}, {"title": "3.3.1 Data collection and analysis", "content": "In addition to the intake and exit surveys, the researcher present during the study took notes of the participants experience with and commentary on the system. Additionally, each participant consented to audio and screen recordings of their study session. Audio recordings captured participants' comments during their interaction with the system and discussion surrounding the design process. These recordings were transcribed using Microsoft Teams transcription software. Screen recordings captured interactions with the system (both the GUI as well as the file explorer and code editors used by some users to more closely track the system's output). Finally, DreamGarden automatically logged the work of the system in concert with the user-including any user edits, and backups in the case of any resultant tree-pruning-that is, the sequence of all prompts, responses, generated code and artifacts, and in-engine screenshots produced over the course of the study.\nFollowing guidelines for qualitative HCI practice [33], authors then collectively reviewed notes, transcripts, and system output, identifying recurring topics and behavioral themes to uncover emergent themes pertinent to our research questions [18]."}, {"title": "4 RESULTS", "content": "We experiment with various hyperparameters, most notably the branching factor and maximum depth of the tree generated by the planning module, and the maximum number of attempts allowed to the coding generator before forcibly moving on to the next task. Note that the branching factor and depth limits are insisted upon in the system prompts of the planning module, though they could also be manually enforced by throwing away supplementary children and forcing certain nodes to be leaves (and querying the language model separately to assign it an appropriate implementation submodule), respectively.\nWe note that the system will not always expand the tree to the fullest extent. Still, in simpler scenarios, like \"a sheep grazing on a grassy hillside\", it will often superfluously subdivide certain steps, e.g. breaking down \"generate sheep asset\" into separate tasks for the generation of individual body parts, or for the generation of mesh and texture. Note that this latter case speaks to a problem we notice occasionally with the system, in which it will invent tasks beyond the capability of the submodules to which it currently has access, or at least which are not explicitly mentioned to it. This was the case with lighting, atmosphere effects (such as fog) and skyboxes, for example, where the system would sometimes implement these features despite their conflicting with the default level's default lighting. In this case, we subsequently removed all environment assets from the default level, prompted the coding module to account for these explicitly, and gave an example of a sufficient 'EnvironmentManager' Actor class in its prompt. With animation, on the other hand, we adapted our prompts to insist that the system would not be capable of animating skeletal meshes.\nWhen we attempt to restrict the tree to particularly small depths and branching factors, it sometimes ignores these requests, for example adding too many children to the root node.\nWe note anecdotally that for more complex tasks, larger depths and branching factors allow for better subdivision of implementation tasks, making it easier for the system to progress toward high-level objectives without getting stuck on implementation tasks involving extending the code in very complex ways in one pass. Instead, it is easier for the system to add small but observable functional changes to the code step by step."}, {"title": "4.2 Usability study results", "content": "We will now discuss findings from the user study and group them into a few broad categories. Our survey indicated that all the users were at least partially familiar with generative AI, though their familiarity with game design tools was more varied, while Unreal engine itself was largely unfamiliar to most of the participants, see Figure 9. We present sample intermediary results from the user study generations in ??.\nUtility of planning. A common theme expressed among users was the utility or potential utility of DreamGarden in the planning and prototyping phases of design, or for creating first drafts of more complex projects. Indeed, several users expressed that the planning module may have some standalone utility, irrespective of the system's ability to execute these plans itself. Conversely, users expressed reservations regarding the potential use of the tool in cases demanding more complex game mechanics and environments. Though not prompted to directly consider the merits of DreamGarden as an educational tool, a natural split on this matter emerged among some users with P10 stating that, \"seeing a game design being broken down into steps may be useful to new developers trying to learn how to build their own game\", and P3, on the other hand, stating that they \"would be hesitant about using it for the purpose of really learning, since it takes some of the opportunity to problem solve away from the user\u201d. Both P3 and P10 were familiar with game design and generative AI, and very unfamiliar with Unreal Engine.\nWhen not to use such a tool. When asked to consider what circumstances were not befitting of the application of a tool like DreamGarden, several users imagined scenarios in which designers desired higher degrees of control, or more fine-grained control over the final product, with P4 stating \"I would be reluctant to use a tool like LucidDev\u00b9 for any kind of deliberately artistic endeavor, as I would prefer to have direct control over the outputs\". P9 raised the issue of the potential limitations of language, stating that the system would be unsuitable in situations \"where I don't"}, {"title": "Enjoyability", "content": "When asked to discuss elements of the system and experience which they enjoyed or did not enjoy, several users highlighted the pleasure of watching the system grow their seed prompt into a tree-structured plan action, with P8 stating that \"the tree-like visualization of progress and what is the system doing at every step is pretty remarkable\" and P2 stating that they \u201cfound it exciting to track how the system interpreted my simple prompt and how I could see it's plan\". The fact that this relatively fast pace is not maintained in the GUI (with, e.g., intermediary steps of the coding submodule being hidden from the front-end) seems to have caused some displeasure, with P2 stating that \"the slow process of generating the scene wasn't very enjoyable\". In this same vein, some users expressed interest in having more intermediate states visualized in the GUI, with P8 suggesting that \"the code output can be shown on the browser window too so you have an even more detailed description of what's going on\". Some of the intermediary output of the system generated over the course of the user study is shown in Figure 10.\nUsers expressed a range of emotions with respect to their experiences with the system Figure 11. In their explanations of these ratings, it is clear that expectations and investment play a large role. While P4 experienced high frustration (4) because the system \"doesn't really work\", they also experienced high enjoyment (4) because \"it was fun to try out and see what it was doing\". P3, meanwhile experienced low enjoyment (2) and frustration (2), explaining that they were \"not super invested in the outcome\"."}, {"title": "Transparency and Explainability", "content": "In one notable example, P8 requested a \"Super Mario Bros.-like\" platformer \"but with the art style of MineCraft\" in their seed prompt. However, the system lost the intended MineCraft influence as it sub-divided the plan, generating generic NPC assets instead of stylized ones. Thanks to the explicit nature of all steps in hierarchical plan generation via the GUI, the user was able to pinpoint the expansion step at which this request was ignored. This would have made it straightforward to remedy via human intervention. Cumulative errors in code, on the other hand, were not obvious to users, who generally wound up disoriented and lagging behind system progress when digging through generated code and evaluation-here, output is verbose, dense, and less accessible through the GUI. Along these lines, several users suggested providing summaries of code changes during generation, or errors during evaluation, with reference to relevant code snippets, making the system's functioning \"easier to digest\" to users, such that they might \"jump in and give it some correction so it doesn't have to flounder so much on its own\" (P7)."}, {"title": "Interactivity", "content": "While some users seemed to envision a more generally capable and fully autonomous system, capable of making functional games from scratch without intervention, others imagined ways in which the system might depend on human designers, and even explicitly ask them for help given particularly difficult subtasks or uncertain results. P8, for example, thinks it \"less important that I give it a simple prompt and come back the next day hoping it did the right thing, and more important that I can craft and sculpt with it directly\", for example \"I could be like \"make a big cube in the center\" and *poof* a big cube appears\". P2, meanwhile, thinks \u201cmore iteration on each step would make the end result better\", and that \"this could be human in the loop or not\".\nP4, who was slightly familiar with Unreal, spent time investigating generated source code, using file editors to examine the sequence of prompts and responses produced by the coding submodule.\nThough users were all reminded that in their consent forms, they were welcome to take breaks (check phones, grab drinks) during the study, due to the semi-autonomous and long-timescale nature of the system, only P3 actually left the\""}, {"title": "5 DISCUSSION & DESIGN IMPLICATIONS", "content": "In this section we discuss the findings from the usability study and the implications for design of future versions of assistive systems for game development.\nImproving overall performance. Most obviously, the fact that the system struggled with more complex scenarios, as many users pointed out (or suspected, given the limited time allotted to the interactive portion of the study itself-around 25 minutes-relative to the time actually required by the system to finish iterating on a seed prompt given the default hyperparameters-around 1 hour) points to a general need to make the system more performant overall. We concede that large models may not be able to produce genuinely novel artifacts relative to the corpus of human-generated content upon which they have been trained. Nonetheless, by integrating them in a hierarchical, iterative, and specialized pipeline such as DreamGarden, we argue that they can at least provide a powerful means of interpolating among this vast space of prior human content, a source of what Kate Compton has termed \"liquid art\" [27]. Perhaps the simplest way of making the system a better interpolator of existing artifacts would be to use more thorough few-shot prompting. Though there is a vast amount of documentation, tutorials and example code snippets pertaining to Unreal"}, {"title": "Enjoyability", "content": "When asked to discuss elements of the system and experience which they enjoyed or did not enjoy, several users highlighted the pleasure of watching the system grow their seed prompt into a tree-structured plan action, with P8 stating that \"the tree-like visualization of progress and what is the system doing at every step is pretty remarkable\" and P2 stating that they \u201cfound it exciting to track how the system interpreted my simple prompt and how I could see it's plan\". The fact that this relatively fast pace is not maintained in the GUI (with, e.g., intermediary steps of the coding submodule being hidden from the front-end) seems to have caused some displeasure, with P2 stating that \"the slow process of generating the scene wasn't very enjoyable\". In this same vein, some users expressed interest in having more intermediate states visualized in the GUI, with P8 suggesting that \"the code output can be shown on the browser window too so you have an even more detailed description of what's going on\". Some of the intermediary output of the system generated over the course of the user study is shown in Figure 10.\nUsers expressed a range of emotions with respect to their experiences with the system Figure 11. In their explanations of these ratings, it is clear that expectations and investment play a large role. While P4 experienced high frustration (4) because the system \"doesn't really work\", they also experienced high enjoyment (4) because \"it was fun to try out and see what it was doing\". P3, meanwhile experienced low enjoyment (2) and frustration (2), explaining that they were \"not super invested in the outcome\"."}, {"title": "Transparency and Explainability", "content": "In one notable example, P8 requested a \"Super Mario Bros.-like\" platformer \"but with the art style of MineCraft\" in their seed prompt. However, the system lost the intended MineCraft influence as it sub-divided the plan, generating generic NPC assets instead of stylized ones. Thanks to the explicit nature of all steps in hierarchical plan generation via the GUI, the user was able to pinpoint the expansion step at which this request was ignored. This would have made it straightforward to remedy via human intervention. Cumulative errors in code, on the other hand, were not obvious to users, who generally wound up disoriented and lagging behind system progress when digging through generated code and evaluation-here, output is verbose, dense, and less accessible through the GUI. Along these lines, several users suggested providing summaries of code changes during generation, or errors during evaluation, with reference to relevant code snippets, making the system's functioning \"easier to digest\" to users, such that they might \"jump in and give it some correction so it doesn't have to flounder so much on its own\" (P7)."}, {"title": "Interactivity", "content": "While some users seemed to envision a more generally capable and fully autonomous system, capable of making functional games from scratch without intervention, others imagined ways in which the system might depend on human designers, and even explicitly ask them for help given particularly difficult subtasks or uncertain results. P8, for example, thinks it \"less important that I give it a simple prompt and come back the next day hoping it did the right thing, and more important that I can craft and sculpt with it directly\", for example \"I could be like \"make a big cube in the center\" and *poof* a big cube appears\". P2, meanwhile, thinks \u201cmore iteration on each step would make the end result better\", and that \"this could be human in the loop or not\".\nP4, who was slightly familiar with Unreal, spent time investigating generated source code, using file editors to examine the sequence of prompts and responses produced by the coding submodule.\nThough users were all reminded that in their consent forms, they were welcome to take breaks (check phones, grab drinks) during the study, due to the semi-autonomous and long-timescale nature of the system, only P3 actually left the\""}, {"title": "5 DISCUSSION"}]}