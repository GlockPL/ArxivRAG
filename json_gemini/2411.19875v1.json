{"title": "ENHANCED ANOMALY DETECTION IN WELL LOG DATA THROUGH\nTHE APPLICATION OF ENSEMBLE GANS", "authors": ["Abdulrahman Al-Fakih", "A. Koeshidayatullah", "Tapan Mukerji", "SanLinn I. Kaka"], "abstract": "Although generative adversarial networks (GANs) have shown significant success in modeling data\ndistributions for image datasets, their application to structured or tabular data, such as well logs,\nremains relatively underexplored. This study extends the ensemble GANs (EGANs) framework to\ncapture the distribution of well log data and detect anomalies that fall outside of these distributions.\nThe proposed approach compares the performance of traditional methods, such as Gaussian mixture\nmodels (GMMs), with EGANs in detecting anomalies outside the expected data distributions. For the\ngamma ray (GR) dataset, EGANs achieved a precision of 0.62 and F1 score of 0.76, outperforming\nGMM's precision of 0.38 and F1 score of 0.54. Similarly, for travel time (DT), EGANs achieved a\nprecision of 0.70 and F1 score of 0.79, surpassing GMM's 0.56 and 0.71. In the neutron porosity\n(NPHI) dataset, EGANs recorded a precision of 0.53 and F1 score of 0.68, outshining GMM's 0.47\nand 0.61. For the bulk density (RHOB) dataset, EGANs achieved a precision of 0.52 and an F1 score\nof 0.67, slightly outperforming GMM, which yielded a precision of 0.50 and an F1 score of 0.65.\nThis work's novelty lies in applying EGANs for well log data analysis, showcasing their ability to\nlearn data patterns and identify anomalies that deviate from them. This approach offers more reliable\nanomaly detection compared to traditional methods like GMM. The findings highlight the potential\nof EGANs in enhancing anomaly detection for well log data, delivering significant implications for\noptimizing drilling strategies and reservoir management through more accurate, data-driven insights\ninto subsurface characterization.", "sections": [{"title": "1 Introduction", "content": "Well log data, such as gamma ray (GR), bulk density (RHOB), sonic travel time (DT), neutron porosity (NPHI), and\ndeep resistivity (ILD), are fundamental to understanding subsurface geological formations [10, 20, 23, 25, 13, 34].\nAccurate interpretation of these datasets is vital for efficient reservoir management, directly influencing drilling and\nproduction decisions [27]. Proper modeling of geophysical data and the ability to detect anomalies are significant"}, {"title": "2 Methodology", "content": ""}, {"title": "2.1 Data overview and exploratory data analysis", "content": "The datasets used in this study comprise comprehensive well log data from two wells in the North Sea Dutch region:\nF17-04 and F15-A-01. These wells provide detailed logging data for GR, DT, NPHI, and RHOB logs, totaling 6553\ndata points within a depth range of 1925-2605 meters. These wells were selected due to their comprehensive logging\ndata, which is pivotal for accurate machine learning model (MLM) training and evaluation.\nK-Means clustering was applied jointly across the dimensions of the dataset to leverage the multivariate relationships\nbetween features. Various cluster numbers (e.g., 2, 4, 6, 8, 10, and 12) were evaluated based on visual patterns and\nmetric consistency. Among these, 10 clusters were selected as they provided the optimal balance between capturing\ninherent patterns and avoiding over-segmentation or excessive generalization, as confirmed by exploratory analysis\nand visual assessments. The data was then standardized using StandardScaler before clustering. This standardization\nperformed a Z-score normalization, which transformed the data such that each feature had a mean of 0 and a standard\ndeviation of 1. This transformation ensures that the features are on the same scale, which is crucial for clustering\nperformance.\nAfter applying KMeans, the dataset was filtered for clusters 0 and 1, which exhibited relevant characteristics for the\nanomaly detection task. Specifically, data from clusters 0 and 1 were selected, and data from other clusters were\ndiscarded. This ensures that only the data points belonging to these two clusters are retained for further analysis.\nThe isolation forest (IF) algorithm was employed after the clustering process to detect anomalies within the filtered data\n(from clusters 0 and 1). The IF algorithm flags potential anomalies by labeling data points as either -1 (anomalies) or 1\n(normal). The model was trained on the training set and evaluated on the testing set, to identify outliers that can later be\nrefined using advanced models like GMM and EGAN.\nThe data was split into training and testing sets using an 80-20 split, ensuring the model was evaluated on unseen data.\nThe results highlight the areas where anomalies occur, offering a starting point for further refinement."}, {"title": "2.2 Model configuration and hyperparameter settings", "content": "Each anomaly detection model GMM and EGAN was configured with tailored hyperparameters to optimize performance.\nThe chosen hyperparameters are important as they directly impact the models' accuracy, efficiency, and effectiveness in\ndetecting anomalies. outlines the specific hyperparameter settings for each model, providing transparency and\nfacilitating reproducibility of the study's methodology."}, {"title": "2.3 Proposed models' workflow", "content": "The overall workflow for this study involved two key models: GMM and EGANs. illustrates the distinct\nworkflows used for each model. 2(a) represents the anomaly detection workflow for GMM, showcasing steps\nsuch as clustering, model training, evaluation, and result visualization. 2(b) presents the workflow for EGANS,\nhighlighting the processes of setting hyperparameters, defining network architectures, training the EGAN model, and\nevaluating its performance."}, {"title": "2.4 Technical description for each model", "content": ""}, {"title": "2.4.1 Gaussian mixture model", "content": "The GMM is a probabilistic model that posits data points originate from a blend of multiple Gaussian distributions,\neach characterized by unknown parameters[9]. Given a d-dimensional vector x, the probability density function of the\nGaussian mixture model can be formulated as in equation 1:\n$p(x) = \\sum_{i=1}^{C} Wipi(x)$\nIn this formulation, e denotes the number of mixture components, and the mixture weights wi adhere to the condition\n$\\sum_{i=1}^{C}Wi= 1$ and wi > 0 for each componenti.\nEach component density pi(x) represents the probability density function of a Gaussian distribution characterized by a\nd \u00d7 1 mean vector \u00b5i and a d \u00d7 d covariance matrix \u2211i as shown in equation 2:\n$p(x) = \\frac{1}{(2\u03c0)^{d/2}|\u03a3_i|^{1/2}} exp {\u2212\\frac{1}{2}(x \u2212 \u03bc_i)^T \u03a3_i^{\u22121} (x \u2212 \u03bc_i)}$"}, {"title": "2.4.2 Generative adversarial networks model", "content": "Suspendisse vel felis. Ut lorem lorem, interdum eu, tincidunt sit amet, laoreet vitae, arcu. Aenean faucibus pede eu\nante. Praesent enim elit, rutrum at, molestie non, nonummy vel, nisl. Ut lectus eros, malesuada sit amet, fermentum eu,\nsodales cursus, magna. Donec eu purus. Quisque vehicula, urna sed ultricies auctor, pede lorem egestas dui, et convallis\nelit erat sed nulla. Donec luctus. Curabitur et nunc. Aliquam dolor odio, commodo pretium, ultricies non, pharetra in,\nvelit. Integer arcu est, nonummy in, fermentum faucibus, egestas vel, odio.\nGANs, first proposed by Goodfellow et al. (2014), consist of two neural networks in competition: the generator (G)\nand the discriminator (D). Through an adversarial training process, the generator aims to produce synthetic data that\nresembles real data, while the discriminator aims to differentiate between real and synthetic data. This adversarial\ntraining framework progressively upgrades the generator's capacity to generate realistic data samples [14].\nThe GAN architecture involves two main components, the G and the D. Both are implemented as three-layer multi-layer\nperceptrons (MLPs), each with distinct roles and configurations. The G generates synthetic data samples, and the D\nevaluates the authenticity of these samples by distinguishing real data from the synthetic data generated by G.\nThe adversarial training process in GANs is formulated as a minimax game, where the generator and discriminator play\nagainst each other. The value function V(G, D) governing this game is defined as follows in equation 3:\n$\\min_{G}\\max_{D} V (G, D) = E_{x~Pdata} [log D(x)] + E_{z~P_{z}(z)} [log(1 \u2013 D(G(z)))]$\nWhere D(x) represents the discriminator's estimation of the probability that the input x is real data, G(z) is the\ngenerator's output given noise z, Pdata is the data distribution, and P\u2082 is the noise distribution."}, {"title": "2.5 Evaluation metrics for each model configuration", "content": "To comprehensively evaluate the effectiveness of the models employed in this study, several key metrics were employed.\nThese metrics offer insights into various facets of the model's predictive accuracy and reliability, particularly in the\ncontext of anomaly detection in well log data. The following metrics were used:\n\u2022 Precision (Prec): This metric measures the ratio of true positive predictions to the total predicted positives.\nIt is key for understanding the proportion of relevant instances among the retrieved instances as shown in"}, {"title": "3 Results and discussion", "content": "The comparative study evaluated two advanced MLMS, EGANs, and GMM, using well log data from two North Sea\nDutch wells, focusing on GR, DT, RHOB, and NPHI logs. The objective was to assess each model's ability to classify\nand detect anomalies in these datasets."}, {"title": "3.1 Gaussian mixture model analysis", "content": "The GMM analysis for the GR, DT, RHOB, and NPHI logs reveals distinct patterns and insights, as shown in with its four-column layout.\nThe scatter plots in the first column illustrate the anomaly detection results using the IF algorithm, as explained in\nthe methodology section. The IF method effectively identifies anomalies in the GR, DT, RHOB, and NPHI datasets,\nmarking outliers in red. The distribution of normal and anomalous points shows clear clustering, with most anomalies\nlocated at the edges of the data distribution. The data distributions identified in this stage will be used for the GMM in\nsubsequent steps, where the model will focus on modeling the structure of the data distributions and detecting anomalies\nthat deviate from these distributions.\nThe second column illustrates the training stage of the GMM, where the model fits Gaussian ellipses around the data\nclusters for each dataset (GR, DT, RHOB, NPHI). Since the dataset is unlabeled, the GMM performs unsupervised\nclustering, grouping the data points based on their inherent distributions. The model fits two Gaussian components\nto the data, as shown by the yellow and purple ellipses, which represent the high probability regions for each cluster.\nThese ellipses capture the underlying structure of the data, effectively distinguishing between the normal data points in"}, {"title": "3.2 Ensemble generative adversarial networks analysis", "content": "The EGAN analysis for the GR, RHOB, DT, and NPHI logs, as shown in , demonstrates the model's ability to\neffectively approximate the underlying data distributions and identify anomalies.\nThe first column provides scatter plots illustrating anomaly detection using the IF algorithm. In these plots, normal data\npoints are marked in blue, and anomalies are marked in red. The clustering of points shows a clear distinction between\nnormal and anomalous data, with anomalies generally located at the edges of the data distributions. This step establishes\nthe foundation for later stages by identifying potential outliers, which are critical for the GAN's learning process.\nIn the training stage (second column), the contour plots illustrate how the GAN learns the data distribution by modeling\nhigh and low-density regions. The GAN captures the underlying structure of the data through smooth gradients and"}, {"title": "3.3 Performance comparison of EGAN and GMM for anomaly classification using labeled data", "content": "The performance of EGANs and GMM was evaluated across several datasets using key metrics\u2014precision, recall,\nand F1 score. summarizes the results for both models, showing that EGANs generally outperformed GMM\nin precision and F1 score, demonstrating better ability to minimize false positives. For example, in the GR dataset,\nEGANS achieved a higher precision (0.62) and F1 score (0.76), while GMM had a lower precision (0.38) despite high\nrecall (0.95). Similarly, EGANs consistently showed better precision and F1 scores in other datasets such as RHOB,\nDT, and NPHI, where GMM generally produced more false positives.\nIt is important to clarify that the classification performed in this study is univariate-each variable (GR, DT, RHOB,\nNPHI) is analyzed independently for anomaly detection. Although the bivariate distributions and contour plots presented\nin the figures are useful for visualizing the learned data distributions and model behavior, they do not influence the\nclassification process. These visualizations are meant to provide insights into how the models separate the data in\ntwo-dimensional space but are not used in the actual anomaly classification, which is based on individual variables.\nThus, the classification task remains univariate, with the bivariate plots serving only as a tool for model evaluation and\nunderstanding.\nFor a more detailed exploration of the code, datasets, and result visualizations used in this study, please visit the\nfollowing GitHub repository: https://github.com/ARhaman/EGANs-vs.GMM. This repository contains Python scripts\nand Jupyter notebooks for model implementation and evaluation, well log data (GR, DT, RHOB, and NPHI) used in\nthis study, and visualizations and results from the anomaly detection analysis."}, {"title": "4 Future directions", "content": "This study highlights the effectiveness of EGANs in anomaly detection for well log data. However, there are several\navenues for future work that could enhance and expand the findings:\n\u2022 Multivariate anomaly detection: While this study focused on univariate anomaly detection, future research\nshould explore multivariate anomaly detection to leverage the relationships between different well log features"}, {"title": "5 Conclusion", "content": "This research demonstrates the robustness of EGANs in analyzing well log data, providing an effective solution\nfor anomaly detection outside the data distribution and forecasting in geophysics. Extensive testing across various\nlogs highlights the superior performance of EGANs, setting a new benchmark for anomaly detection in geosciences.\nWhile this study focuses on univariate anomaly detection, future research should extend to multivariate approaches,\nleveraging relationships between features like GR, DT, RHOB, and NPHI. EGANs consistently outperformed GMM in\nprecision and F1 scores, reducing false positives while maintaining a strong recall balance. In contrast, GMM, although\nachieving high recall, often suffered from lower precision, leading to more false positives. These findings emphasize\nthe importance of balancing precision and recall in selecting the appropriate model for anomaly detection tasks. A key\nadvantage of EGANs is their adaptability, allowing for modification of confidence intervals to suit various domains.\nThis flexibility enables users to tailor model sensitivity, making EGANs a robust tool for time series analysis in diverse\nindustrial applications. Additionally, EGANs are well-positioned for real-time anomaly detection, a promising area\nfor future exploration in reservoir management. This work's novelty lies in comparing GMM's predictive capabilities\nwith EGANs' advanced anomaly detection, setting a new standard for well log analysis in the oil and gas industry.\nFuture research should explore other advanced anomaly detection techniques, such as autoencoders and isolation\nforests, to further enhance model robustness. Additionally, hyperparameter optimization using techniques like Bayesian\noptimization could improve both computational efficiency and anomaly detection performance."}, {"title": "Author Contributions", "content": "Abdulrahman Al-Fakih: Formal analysis, Methodology, Software, Writing \u2013 original draft, Data preparation, Code\ncreation. A. Koeshidayatullah: Resources, Supervision, Review & editing. Tapan Mukerji: Conceptualization,\nReview & editing, Scientific additions. SanLinn I. Kaka: Supervision, reviewed & edited."}, {"title": "Declaration of Competing Interest and Use of Generative AI", "content": "The authors affirm that they have no known competing financial interests or personal relationships that may have\ninfluenced the work presented in this paper. During the preparation of this work, the author(s) used the ChatGPT\nlanguage model from OpenAI for refining grammar and enhancing text coherence in this article. After using this tool,\nthe author(s) reviewed and edited the content as needed and take(s) full responsibility for the content of the publication."}, {"title": "Data Availability", "content": "The codes and datasets used in this study are available on GitHub at https://github.com/ARhaman/EGANs-vs.GMM.\nThe repository includes Python scripts, Jupyter notebooks for model implementation and evaluation, well log data (GR,\nDT, RHOB, and NPHI), and visualizations and results from the anomaly detection analysis."}, {"title": "Acknowledgements", "content": "The authors would like to express their gratitude to the College of Petroleum Engineering at KFUPM for their invaluable\nsupport in presenting this work at international conferences. Special thanks are extended to the NLOG website and\nUtrecht University for providing the dataset. Additionally, the authors acknowledge EAGE for the opportunity to\npresent this work at the European Conference on the Mathematics of Geological Reservoirs (ECMOR 24), held in Oslo,\nNorway."}, {"title": "Abbreviations", "content": "\u2022 AI = Artificial Intelligence\n\u2022 API = Application Programming Interface\n\u2022 CGANs = Conditional Generative Adversarial Networks\n\u2022 D = Discriminator\n\u2022 DT = Sonic Travel Time\n\u2022 EM = Expectation-Maximization (the algorithm used in GMM)\n\u2022 EGAN = Ensemble Generative Adversarial Networks\n\u2022 F1 = F1 Score\n\u2022 FP = False Positives\n\u2022 FN = False Negatives\n\u2022 G = Generator\n\u2022 GAN = Generative Adversarial Networks\n\u2022 GMMs = Gaussian Mixture Models\n\u2022 GR = Gamma Ray\n\u2022 ILD = Deep Resistivity\n\u2022 IF = Isolation Forest\n\u2022 K-Means = Number of Clusters\n\u2022 LIME = Local Interpretable Model-Agnostic Explanations\n\u2022 MLM = Machine Learning Models\n\u2022 MLP = Multi-layer Perceptrons\n\u2022 NPHI = Neutron Porosity\n\u2022 NixtlaClient = A client library provided by Nixtla for interacting with their API\n\u2022 Prec = Precision\n\u2022 PDFs = Probability Density Functions\n\u2022 Rec = Recall"}]}