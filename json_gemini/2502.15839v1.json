{"title": "FedMobile: Enabling Knowledge Contribution-aware Multi-modal Federated Learning with Incomplete Modalities", "authors": ["Yi Liu", "Cong Wang", "Xingliang Yuan"], "abstract": "The Web of Things (WoT) enhances interoperability across web-based and ubiquitous computing platforms while complementing existing IoT standards. The multimodal Federated Learning (FL) paradigm has been introduced to enhance WoT by enabling the fusion of multi-source mobile sensing data while preserving privacy. However, a key challenge in mobile sensing systems using multimodal FL is modality incompleteness, where some modalities may be unavailable or only partially captured, potentially degrading the system's performance and reliability. Current multimodal FL frameworks typically train multiple unimodal FL subsystems or apply interpolation techniques on the node side to approximate missing modalities. However, these approaches overlook the shared latent feature space among incomplete modalities across different nodes and fail to discriminate against low-quality nodes. To address this gap, we present FedMobile, a new knowledge contribution-aware multimodal FL framework designed for robust learning despite missing modalities. FedMobile prioritizes local-to-global knowledge transfer, leveraging cross-node multimodal feature information to reconstruct missing features. It also enhances system performance and resilience to modality heterogeneity through rigorous node contribution assessments and knowledge contribution-aware aggregation rules. Empirical evaluations on five widely recognized multimodal benchmark datasets demonstrate that FedMobile maintains robust learning even when up to 90% of modality information is missing or when data from two modalities are randomly missing, outperforming state-of-the-art baselines. Our code and data are available at the link.", "sections": [{"title": "1 Introduction", "content": "In the Web of Things (WoT) [17, 18, 29], multimodal mobile sensing systems enhance the interoperability and usability of web-based mobile platforms by integrating data from multiple sources [6, 36, 42]. In this context, these systems boast a diverse array of real-world applications [26], frequently deployed to address complex tasks within domains such as autonomous driving [47], mobile health-care [8], and the Internet of Things [16]. In this context, these tasks often prove too intricate and dynamic to be effectively tackled solely through reliance on a single sensor modality [12, 25]. Consequently, a straightforward approach involves aggregating complementary modality data from multiple sensors to extract feature information across various sensing channels, thereby enhancing model performance [38]. However, this multimodal learning paradigm, reliant on centralized processing and aggregation of raw user data, introduces significant privacy concerns [5, 19, 34, 35, 44].\nTo mitigate the privacy concerns outlined above, Federated Learning (FL) [22] emerges as a solution. FL, characterized as an evolving privacy-preserving distributed machine learning paradigm, facilitates collaboration among mobile sensing devices across regions without compromising privacy [11]. By sharing model updates instead of raw data, FL fosters collective learning of global models among geographically spread devices. Although most FL methods deal with unimodal data for tasks like next-word prediction [9], some applications, e.g., Alzheimer's detection [27], necessitate combining data from diverse sources (multimodal data). This has led to the development of multimodal FL systems [7] tailored for efficient processing of data from various sensory channels.\nWhile the multimodal FL system addresses some challenges in multimodal data processing, it still suffers from incomplete sensing modalities [13, 27, 40], as shown in Fig. 1. For example, in mobile healthcare, sensor modalities often become unavailable due to sensor failures or malfunctions. This increases the variability of available sensor modalities across different nodes during runtime [27]. Thus, aggregating model updates in multimodal FL systems with incomplete sensing modalities becomes very challenging due to the varied distributions of modality types across different mobile nodes. This modality heterogeneity also intensifies model disparities between nodes, affecting the accuracy and convergence of FL [13, 40]. Existing multimodal FL methods use techniques like data interpolation [47] and modal fusion [7] to address these issues,\nbut there is still a significant gap in efficiently utilizing cross-node modal feature information and selecting high-quality data nodes.\nIn this paper, we introduce FedMobile, a novel knowledge contribution-aware multimodal FL system designed specifically for mobile sensing applications with missing modalities. Unlike existing multimodal FL methodologies [7, 27, 40], which typically focus on training multiple unimodal FL subsystems concurrently, FedMobile adopts a distinct approach. It aims to reconstruct the features of missing modalities by utilizing knowledge distillation while introducing a knowledge contribution-aware aggregation rule via Shapley value to discern and aggregate high-quality model updates. To fulfill these objectives, FedMobile is guided by two primary goals: 1) Effectively reconstructing features of missing modalities without exacerbating modality heterogeneity or compromising main task performance. 2) Streamlining the process of identifying mobile nodes with substantial contributions while minimizing computational overhead. Next, we focus on responding to the following two challenges:\n\u2022 (C1.) How to collaboratively interpolate missing sensing modal features for different nodes with cross-modal heterogeneity.\n(S1.) - The heterogeneous modality between different nodes has a common feature subspace. In mobile sensing scenarios, malfunctioning sensor modalities at various nodes give rise to missing modalities, causing modality heterogeneity [43]. In such circumstances, conventional solutions like zero-filling [10] and parallel training of unimodal models [27] often inadequately handle this inherent feature and modality diversity. Our goal, therefore, is to leverage knowledge distillation for constructing a shared feature subspace among node modalities to improve model performance. We implement a feature generator on both the server and node levels to tackle missing modality issues. This generator, trained in a coupled training manner, aims to align different modalities by capturing a common feature space.\n\u2022 (C2.) How to find relevant metrics for measuring the contribution of a specific node in a computationally cost-friendly manner.\n(S2.) - Knowledge and model updates shared between nodes and server generalize the contributions of nodes. In FedMobile, the contributory role of participating nodes is manifested across dual dimensions: knowledge shared by local generators and local model updates shared by local nodes. To incentivize local generators to yield high quality features for incomplete modalities, we devise a novel Clustered Shapley Value approach that quantifies the individual contributions of these generators. This subsequently allows for adaptive modulation of their respective weights, thus facilitating the aggregation of high-quality feature representations. Moreover, with the objective of discerning nodes that high-quality model updates, we introduce a contribution-aware aggregation mechanism designed to retain those elements that are conducive to the overall improvement of the global model. Conversely, it eliminates nodes that do not meet this criterion. By dynamically choosing nodes based on this principle, we effectively ensure the aggregation of high-quality model updates during the training.\nAdditionally, we evaluate FedMobile across five real-world multimodal sensing datasets: USC-HAD [46], MHAD [24], Alzheimer's Disease Monitoring (ADM) [27], C-MHAD [37], and FLASH [30], which encompass tasks related to autonomous driving, mobile healthcare, and Alzheimer's disease detection. The results indicate\nthat FedMobile effectively leverages various sensor types (such as GPS, gyroscopes, and radar) in scenarios with incomplete sensing modalities to accurately perform assigned tasks, even amid operational dynamics like sensor failures. Furthermore, we analyze the computational and communication overhead of FedMobile across different tasks. Extensive evaluations show that FedMobile outperforms existing multimodal FL systems (e.g., FedMM [7], AutoFed [47], and PmcmFL [1]), achieving higher model accuracy with reasonable additional computational and communication overheads, especially under dynamic modality missing conditions.\nThe contributions of our work can be summarized as follows:\n(1) We tailor FedMobile, a multimodal federated learning framework that is robust to incomplete modal data, for web-based mobile sensing systems in WoT.\n(2) We design a knowledge distillation-driven cross-node modality reconstruction network to efficiently reconstruct the missing modality data without introducing excessive overhead.\n(3) We design an efficient generator contribution evaluation module based on clustered Shapley value and contribution-aware aggregation mechanism to further improve system performance.\n(4) We implement our design and conduct extensive experiments on 5 datasets related to 3 mobile sensing downstream tasks to explore the performance, efficiency, generality, and parameter sensitivity of FedMobile. Compared to the baselines, our approach achieves state-of-the-art performance on all tasks while maintaining comparable computation and communication overhead."}, {"title": "2 Related Work", "content": "Multimodal Learning for Mobile Sensing Systems. Multimodal learning aims to extract complementary or independent knowledge from various modalities, enabling the representation of multimodal data [20, 45]. This empowers machine learning models to comprehend and process diverse modal information [39]. As a result, multimodal learning techniques have become prevalent in mobile sensing, facilitating the development of systems that can understand and process diverse sensor data. For instance, multimodal learning can enhance model performance in areas such as traffic trajectory prediction [31], disease diagnosis [27], human activity recognition [3], audio-visual speech recognition [23], and visual question answering [21]. However, solving the problem of missing modalities in such systems remains an open challenge.\nUnimodal and Multimodal FL systems. To address privacy concerns in mobile sensing systems, privacy-preserving distributed learning systems, notably FL [22, 27, 47], are emerging as a solution. FL systems can be categorized into unimodal and multimodal FL based on the number of data modalities involved. Unimodal FL focuses on constructing a global model from unimodal data while preserving privacy [28]. Similarly, multimodal FL integrates data\nfrom multiple modalities to develop an effective global model [7]. Multimodal FL systems are increasingly used in mobile sensing applications, particularly in tasks such as autonomous driving [47] and Alzheimer's disease detection [27], due to their robust multimodal data processing capabilities.\nMultimodal FL Systems with Missing Modality. Multimodal FL systems have emerged as a promising approach for training ML models across multiple modalities while preserving data privacy. However, in real-world scenarios, certain modalities may be missing from some nodes due to hardware limitations, data availability constraints, or privacy concerns [13, 27, 43]. To address this challenge, researchers have developed multimodal FL systems using various approaches, including modality filling [40], parallel training of unimodal models [27], and cross-model [43] techniques. For example, Xiong et al. [40] introduced a modality-filling technique using reconstruction networks, while Ouyang et al. [27] proposed Harmony, a heterogeneous multimodal FL system based on disentangled model training. However, these methods often overlook the common feature space and the evaluation of node marginal contributions, leading to issues with model accuracy. This paper aims to address these challenges by developing a knowledge contribution-aware multimodal FL system for mobile sensing."}, {"title": "3 Preliminary", "content": "3.1 Multimodal Federated Learning\nMultimodal FL is a cutting-edge approach in machine learning (ML) that addresses the challenges of training models across multiple modalities while preserving data privacy. Formally, in mobile sensing scenarios, let us denote \\(M = \\{m_0, m_1, ..., m_{M-1}\\}\\) as the set of modalities of the local multimodal dataset \\(D_k\\), \\(K\\) as the number of participating mobile nodes, \\(n_k\\) as the number of samples in the node \\(k\\), and \\(d\\) as the dimensionality of modality \\(m\\) in the node \\(k\\). The objective of Multimodal FL is to optimize a global model \\(F(w)\\) parameterized by \\(w\\) across all modalities while minimizing the following federated loss function:\n\\[\\min_w \\frac{1}{K} \\sum_{k=1}^K \\frac{n_k}{n} \\sum_{m \\in M} L(w; X_k^m, y_k),\\]\nwhere \\(L(w; X_k^m, y_k)\\) is the loss function for modality \\(m\\) at node \\(k\\), \\(X_k^m\\) represents the data samples for modality \\(m\\) at node \\(k\\), \\(y_k\\) is the target label associated with the samples at node \\(k\\), and \\(n = \\sum_{k=1}^K n_k\\) represents the total number of samples across all nodes. In multimodal FL, the global model \\(F(w)\\) is updated by aggregating local model updates from each node while respecting data privacy constraints. The update rule for the global model at iteration \\(t\\) can be formalized as:\n\\[w^{t+1} = w^t - \\frac{\\eta}{n} \\sum_{k=1}^K n_k \\nabla L(w^t; X_k, y_k),\\]\nwhere \\(\\eta\\) is the learning rate and \\(\\nabla L(w^t; X_k, y_k)\\) is the gradient of the loss function with respect to the global model parameters \\(w^t\\) at node \\(k\\). Clearly, when a modal sensor on a mobile node fails or ceases to function, resulting in a missing modality, the optimization of Eqs. (1) and (2) becomes challenging. This impediment\nimplies that multimodal FL may struggle to fulfill the designated task effectively under such circumstances."}, {"title": "3.2 Shapley Value in ML", "content": "Shapley Value [32] is a concept from cooperative game theory used to fairly distribute the value generated by a coalition of players. In the context of ML, it is often applied to understand the contribution of each feature to a model's prediction [2]. Let us denote a predictive model as \\(f\\), and \\(\\Phi_i(f)\\) represents the Shapley value of feature \\(i\\) in the model \\(f\\). The Shapley value of feature \\(i\\) can be computed as:\n\\[\\Phi_i(f) = \\sum_{S \\subset N\\{i\\}} \\frac{|S|!(|N| - |S| -1)!}{|N|!} [f(x_{S \\cup \\{i\\}}) - f(x_S)],\\]\nwhere \\(N\\) is the set of all features, \\(x_S\\) represents the instance with only features in set \\(S\\), \\(f(x_{S \\cup \\{i\\}})\\) is the prediction of the model when feature \\(i\\) is added to the set \\(S\\), \\(f(x_S)\\) is the prediction of the model when only features in set \\(S\\) are considered, \\(|S|\\) denotes the cardinality of set \\(S\\), and \\(|N|\\) is the total number of features.\nThe above formula computes the marginal contribution of feature \\(i\\) when added to different subsets \\(S\\) of features, weighted by the number of permutations of features in \\(S\\) to the total number of permutations of all features. In fact, calculating the Shapley value directly using the above formula might be computationally expensive [2, 32], especially for models with a large number of features."}, {"title": "4 Our Approach", "content": "4.1 Knowledge Distillation-driven Cross-node Modalitiy Reconstruction Network\nImpute Missing Modalities. Different from existing works such as AutoFed [47], which focus on reconstructing local missing modalities while ignoring cross-node feature information, FedMobile (as shown in Fig. 2) aims to collaboratively utilize the common feature subspace across nodes to iteratively reconstruct the feature information of missing modalities. Specifically, to gain insight into the data distribution of missing modalities across nodes and use this understanding to guide local model training with incomplete modalities, we employ conditional distributions to characterize the modal data distribution of each node. Let \\(Q_k: \\mathcal{Y}_k \\rightarrow \\mathcal{X}_k\\) denote the above conditional distribution, which is tailored for each node and aligns with the ground truth data distribution. This distribution encapsulates the necessary knowledge to guide multimodal FL training with incomplete modalities:\n\\[Q_k = \\underset{Q_k: \\mathcal{Y}_k \\rightarrow \\mathcal{X}_k}{\\arg \\max} \\mathbb{E}_{y \\sim p(y_k)} \\mathbb{E}_{x \\sim Q_k(X_k|y_k)} [\\log p(y|x; \\omega_k)],\\]\nwhere \\(p(y_k)\\) and \\(p(y | x)\\) denote the ground-truth prior and posterior distributions of the target labels, respectively. Given these conditions, we employ local models to infer \\(p(y | x)\\). Consequently, a straightforward approach involves the direct optimization of Eq. (4) in the input space \\(\\mathcal{X}\\) to approximate features for missing modalities. However, when \\(\\mathcal{X}_k\\) is of high dimensionality, this approach may lead to computational overload and could potentially disclose information about user data configuration files. Therefore, a more feasible alternative is to reconstruct an induced distribution \\(\\psi_k: \\mathcal{Y}_k \\rightarrow \\mathcal{Z}_k\\) over a latent space. This latent space, being more compact than the raw data space, can help mitigate certain\nprivacy-related concerns:\n\\[\\Psi_k = \\underset{\\psi_k:\\mathcal{Y}_k \\rightarrow \\mathcal{Z}_k}{\\arg \\max} \\mathbb{E}_{y \\sim p(y_k)} \\mathbb{E}_{z \\sim \\psi_k(Z_k|y_k)} [\\log p(y|z; \\omega_k)].\\]\nHence, nodes engage in knowledge extraction from missing modality data by acquiring insights from a parameterized condition generator \\(\\psi\\) by \\(\\omega\\). The optimization process is as follows:\n\\[\\min J(\\omega) = \\min \\mathbb{E}_{y \\sim p(y_r)} \\mathbb{E}_{z \\sim \\psi_k(z|y_r)} [\\mathcal{L} (\\sigma (g(z; \\omega_\\kappa)), y)],\\]\nwhere \\(y_r\\) represents a set of random labels generated from the training dataset \\(D_k\\), \\(g(\\cdot)\\) denotes the logits output of a predictor, and \\(\\sigma(\\cdot)\\) signifies the non-linear activation applied to these logits.\nAlign Missing Modalities. On the other hand, it is necessary to refine feature subspaces to more accurately encapsulate the local knowledge of nodes. For instance, considering a two-modality task, we can derive the generated latent space via the labels \\(y_k\\):\n\\[Z^{m_0}, Z^{m_1} = \\psi_k (y_k; \\omega_k),\\]\nwhere \\(Z^{m_0}\\) and \\(Z^{m_1}\\) represent the respective latent features of each modality. Assuming \\(m_1\\) denotes the missing modality, our objective is to further empower \\(\\psi_k\\) to assimilate knowledge from various modalities, thereby enhancing the completeness and generalization of the feature space. For modality \\(m_0\\), the learning process can be expressed as follows:\n\\[\\mathcal{L}_{m_0}^k (\\omega; \\omega_k) = \\min_{\\omega_k^{m_0}} \\frac{1}{B} \\sum_{i=1}^{B} \\mathbb{E}_{x \\sim D_k} [D_{KL} (f_{\\theta_0} (x^{m_0}; \\omega_0) || Z^{m_0})],\\]\nwhere \\(B\\) represents the number of samples in the local training batch. For modality \\(m_1\\), we only learn from the missing data of this modality, which is formally expressed as follows:\n\\[\\mathcal{L}_{m_1}^k (\\omega; \\omega_k) = \\min_{\\omega_k^{m_1}} \\frac{1}{I} \\sum_{i=1}^{I} [D_{KL} [(f_i (x^{m_1}; \\omega_{k}^{m_1}) ||Z^{m_1})]],\\]\nuse \\(Z^{m_1}\\) instead of the feature \\(f_i (x^{m_1}; \\omega_{k}^{m_1})\\) of modality \\(m_1\\) for multimodal feature fusion (e.g., concatenated fusion) to achieve feature alignment for missing modality. According to the above method, the overall optimization goal of every FL node is:\n\\[\\min \\mathcal{L}_{train} = J(\\omega) + \\mathcal{L}_{m_0}^k (\\omega; \\omega_k) + \\mathcal{L}_{m_1}^k (\\omega; \\omega_k) + L_{CE}(\\omega_k),\\]\nwhere \\(L_{CE}(\\omega_k)\\) represents the cross entropy loss of model training.\nTransfer Feature Space. In this context, we consider the global distribution generator, denoted as \\(\\tilde{\\psi}\\), and the set of local distribution generators, represented by \\(\\psi_k\\) for each node \\(k\\), as the source and target domains, respectively, in a framework of domain adaptation. This particular form of adaptation is referred to as global-to-local knowledge transfer. Conversely, the local-to-global knowledge transfer takes place at the server side. During the knowledge exchange process, the node \\(k\\) transmits its locally generated distribution model, \\(\\psi_k\\), to the server. The server then orchestrates a guided adjustment of \\(\\psi_k\\) with the aim of systematic reduction in the discrepancy between the local and global knowledge domains through the mechanism of FL aggregation. The above process can be formalized as follows:\n\\[\\psi = \\frac{1}{K} \\sum_{k=1}^K \\Psi_k.\\]"}, {"title": "4.2 Clustering Shaple Value-driven Generator Contribution Evaluation Module", "content": "Evaluate Generator Contribution. Considering the inherent heterogeneity of data across nodes and the varied modality missing scenarios that often arise on individual nodes, a naive aggregation of the local distribution models \\(\\psi_k\\) for knowledge transfer might inadvertently cause a general shift in the collective knowledge domain, leading to counterproductive outcomes. To mitigate this issue, we use the SV method to quantitatively evaluate the marginal contribution of each distinct \\(\\psi_k\\) to the overarching learning task. However, directly applying the SV to compute the marginal contributions of individual nodes is computationally burdensome, especially in FL scenarios involving hundreds of mobile devices. To address this challenge, we incorporate the K-means clustering algorithm to reduce the computational complexity of the SV computation. Specifically, we employ the K-means clustering algorithm to cluster the \\(Z\\) generated by \\(\\psi_k\\), resulting in multiple clusters containing \\(\\psi_k\\). We then perform average aggregation on the generator parameters in the cluster to obtain \\(\\tilde{\\psi}\\) as the node representative of the cluster. In this way, we can get \\(K\\) node representatives and use these node representatives as a set \\(N = \\{1, ..., \\chi\\}\\) in SV. Consequently, the final computation of SV can be expressed as:\n\\[\\Phi_i (F) = \\sum_{S \\subset N\\{i\\}} \\frac{|S|!(|N| - |S| -1)!}{|N|!} [P(F(\\tilde{\\psi}_S(y_r) \\cup\\{\\psi_i(y_r)\\}) - P(F(\\psi_S(y_r)))],\\]"}, {"title": "4.3 Contribution-aware Aggregation Rule", "content": "Node Contribution. To generalize node contribution in a fine-grained manner, we divide the contribution of each node in each round into local and global contributions. The local contribution represents the node's performance in that round of local training, while the global contribution represents the node's impact on model aggregation. We can calculate the local contributions as follows:\n\\[P_{local,k}^t = P(\\omega; D_{proxy}),\\]\nwhere \\(P\\) is a performance metric function, such as accuracy, F1-score, or loss, \\(D_{proxy}\\) represents the proxy dataset, and \\(\\omega_k\\) represents the local model parameters of node \\(k\\). It is important to note that the proxy dataset does not compromise the privacy of the training set and can be collected by the server, as is consistent with previous work [32, 41]. Furthermore, we need to traverse all nodes to calculate the above contribution. To assess how much a node's update would improve the global model, we can perform a hypothetical update by applying only node \\(k\\)'s update to the global model and measuring the global contribution:\n\\[\\omega_{temp,k}^{t+1} = \\omega^t + \\eta \\Delta \\omega_k^t\\]\n\\[\\Delta P_{global,k}^{temp} = P(\\omega_{temp,k}^{t+1}; D_{proxy}) - P(\\omega^t; D_{proxy}) = P_{global,k}^{t+1} - P_{global}^t\\]\nwhere \\(\\eta\\) is the learning rate. Due to computational constraints (since evaluating each node's update individually can be costly), we can approximate this by estimating the potential improvement based on surrogate metrics. Hence, we can approximate it using the node's local loss reduction metric as follows:\n\\[\\Delta L = L(\\omega^t; D_{proxy}) - L(\\omega^t; D_{proxy}).\\]\nWe use \\(\\Delta L\\) as a proxy for \\(\\Delta P_{global,k}^{temp}\\). The reason we do this is that larger differences have a larger impact on the global model.\nNode Contribution-aware Aggregation. Upon determining the global and local contributions, we strive to incorporate them adaptively into the quality assessment process of nodes participating in model aggregation, thereby mitigating the impact of updating nodes with lower quality. To achieve this goal, we can define the aggregation weight \\(\\alpha\\) for a node \\(k\\) as a function of \\(P_{local,k}^t\\) and \\(\\Delta P_{global,k}^t\\):\n\\[\\alpha_k = \\frac{p(P_{local,k}^t, \\Delta P_{global,k}^t)}{\\sum_{j=1}^K p(P_{local,j}^t, \\Delta P_{global,j}^t)}\\]\nwhere \\(p\\) is a function that combines the two performance metrics. Here, an intuitive choice for \\(p\\) is to multiply the normalized performance metrics. Thus, we normalize the Local Contribution Metrics:\n\\[P_{local,k}^t = \\frac{P_{local,k}^t}{\\sum_{j=1}^K P_{local,j}^t}\\]\nand we normalize the Global Contribution Improvements:\n\\[\\Delta P_{global,k}^t = \\frac{\\Delta P_{global,k}^{temp}}{\\sum_{i=1}^{K} \\Delta P_{global,j}^{temp}}.\\]\nCombining Eqs. (16) and (17), if we use local loss reduction, we have:\n\\[\\alpha_k = \\frac{n_k \\times P_{local,k}^t \\times \\Delta L}{\\sum_{j=1}^K n_j \\times P_{local,j}^t \\times \\Delta L},\\]\nwhere \\(\\sum_{k=1}^K \\alpha_k = 1\\). Therefore, we can use this weight to update the global model:\n\\[\\omega^{t+1} = \\omega^t + \\eta \\sum_{k=1}^K \\alpha_k \\Delta \\omega_k^t\\]"}, {"title": "5 Experiment", "content": "5.1 Experiment Setup\nTo evaluate the performance of our FedMobile system, we conduct extensive experiments on four benchmarking datasets. All experiments are developed using Python 3.9 and PyTorch 1.12 and evaluated on a server with an NVIDIA A100 GPU.\nDatasets. We adopt five multimodal datasets for evaluations, i.e., USC-HAD [46], MHAD [24], ADM [27], C-MHAD [37], and FLASH [30] datasets. The datasets cover different modalities, objects, domains, attributes, dimensions, and number of classes, as shown in Table 7, allowing us to explore the learning effectiveness of FedMobile. To simulate an environment characterized by incomplete sensing modalities, we adopt a random selection methodology to identify a target mode from the local dataset, which will represent the state of incompleteness. We then proceed to randomly eliminate a predetermined proportion of the modal data, thereby simulating the phenomenon of missing information. Note that we distinguish between the small-scale node and large-scale node scenarios according to the scale of users (i.e., nodes) involved in the dataset. In the dataset used, FLASH will be evaluated in the large-scale node scenario. More details can be found in Appendix C.1.\nModels. When processing ADM dataset, we harness the TDNN for audio feature extraction and combine it with CNN layers for radar and depth image feature extraction. For USC, MHAD, and FLASH datasets, a 2D-CNN model is utilized to process accelerometer data, whereas a 3D-CNN architecture is employed to analyze skeleton data. Finally, when working with the CMHAD dataset, we exploit a 2D-CNN architecture to derive video features, while 3D-CNN layers are used for extracting features from inertial sensors.\nParameters. For the ADM dataset, we set the learning rate at 1e-3, with a batch size of 64. Regarding the USC dataset, the learning rate is 1e-6, and the batch size is 16. For the MHAD and FLASH datasets, the learning rate is 1e-3, with a batch size of 16. When working with the CMHAD datasets, we maintain a learning rate of 1e-4, alongside a batch size of 16. Throughout this experiment, we utilize the SGD optimizer with a momentum of 0.9 and a weight decay of 1e-4. We set the total number of nodes K = 10, local epoch E = 5, global epoch T = 100, and node participation rate q = 100%. We set K = 5 in the K-means algorithm. We use a multilayer perceptron as our generator (see Appendix C.3)."}, {"title": "5.2 Numerical Results", "content": "Research Questions. In this section, we aim to answer the following research questions:\n\u2022 (RQ1) How effectively does FedMobile, along with its respective baseline methods, fare in handling diverse and complex scenarios characterized by incomplete modal environments?\n\u2022 (RQ2) How does FedMobile demonstrate computational and communicational efficiency in its running processes?\n\u2022 (RQ3) How does FedMobile perform in heterogeneous data scenarios, especially in dynamic modality missing scenarios?\n\u2022 (RQ4) How does FedMobile perform in scenarios with large-scale nodes and missing modalities?\n\u2022 (RQ5) What are the capabilities of FedMobile in terms of multimodal feature extraction, and how proficiently can it harness and integrate features from multiple modalities?\n\u2022 (RQ6) How do the individual components of the FedMobile framework contribute to its overall performance, and what specific impact do they have on its effectiveness?\nSystem Performance (RQ1). To address RQ1, we perform an extensive evaluation of FedMobile, along with its comparative baseline algorithms, using four benchmark multimodal datasets. To assess performance under diverse levels of modal data loss, we introduce a set of modality missing rates designated as \\(\\beta = \\{20\\%, 40\\%, 60\\%, 70\\%, 80\\%, 90\\%\\}\\). The experimental results demonstrate that FedMobile outperforms all other baseline algorithms consistently across all these varying degrees of missing modality data, as clearly depicted in Table 1. Notably, FedMobile showcases a 1.9% improvement relative to the current state-of-the-art baseline,\nAutoFed, specifically in the MHAD dataset with \\(\\beta = 80\\%\\). These enhanced results stem from FedMobile's innovative strategy, which entails reconstructing modal features across nodes and tactically selecting nodes with high-quality contributions. By discovering a shared feature subspace among distinct missing modalities, FedMobile efficiently reconstructs features and simultaneously excludes nodes with inferior-quality data, thereby boosting the performance. To further evaluate the performance of FedMobile, we tested it under a more challenging scenario involving the absence of two modalities (i.e., two-modal data missing). Specifically, we randomly omitted two modalities in a fixed ratio within the ADM dataset, which consists of three modal data, and maintained this missing configuration throughout the training process. The numerical results, recorded in Table 2, demonstrate that FedMobile continues to deliver excellent performance, outperforming other state-of-the-art baselines, including an average 4.3% improvement over AutoFed on the ADM dataset. Additionally, we observed that existing methods struggle with missing data across multiple modalities, as they heavily depend on sufficient modal information to reconstruct the missing data. FedMobile, on the other hand, does not require this, making it more robust in handling such scenarios. Additionally, we provide a privacy analysis in Appendix A.\nComputational & Communication Overhead (RQ2). To address RQ2, we systematically document and analyze the communication cost, local running time, and GPU usage of all examined methods on the USC dataset with \\(\\beta = 60\\%\\). Note that since AutoFed and Harmony also include hardware equipment, they are not included in the comparison. For the convenience of comparison, we record the communication overhead of 100 global training rounds and ignore factors such as the network environment. First, while the introduction of the generator does cause additional communication overhead, this overhead is acceptable. Specifically, the additional overhead caused by the generator is 1.65 MB for each training round. Furthermore, compared to baselines such as FedProx, which do not introduce much additional overhead, our method only adds an additional 9.02% communication overhead, as shown in Fig. 3. In performance-critical multimodal services, a small amount of additional communication overhead is acceptable because it improves the quality of service (i.e., accuracy), which is a performance-overhead trade-off. The results depicted in Figs. 5-6 and Table 3 show that the GPU utilization and local running time of FedMobile consistently remains lower than or close to that of the comparative baseline methods. This indicates that our approach does not appreciably increase local computational overhead. Given that servers typically operate as resource-rich cloud infrastructure, computations related to the server-side SV calculation do not impose any significant extra computational load.\nData Heterogeneity Scenarios (RQ3). To address RQ3, we evaluate the performance of the baselines and FedMobile in a dynamic modality-missing scenario on the ADM dataset. Unlike unimodal FL, where the Dirichlet function is commonly used to control the degree of heterogeneity (i.e., non-IID data), we dynamically adjust both the modality missing rate and the number of missing modality types to control heterogeneity in multimodal FL. The primary reason for this adjustment is that multimodal FL involves multiple"}, {"title": "A Privacy Analysis", "content": "The proposed method for imputing missing modalities in FedMobile introduces privacy-preserving strategies by utilizing latent space reconstruction and conditional distributions across nodes. This\nprivacy analysis explores how these strategies help mitigate privacy risks associated with multimodal FL under scenarios where data modalities are incomplete across nodes.\nPrivacy Issues in Multimodal FL. On the one hand", "48": ".", "7": ".", "techniques": "n\u2022 Conditional Distribution in Latent Space. Instead of directly imputing missing modalities in the raw input space \\(\\mathcal{X}_k\\), FedMobile reconstructs an induced distribution \\(\\psi_k\\) over a latent space \\(\\mathcal{Z}_k\\), as shown in Eq. (5). The latent space is more compact and lower-dimensional than the raw data space. This shift to a latent space significantly reduces the risk of privacy leakage because the latent representations contain abstracted information rather than raw, potentially sensitive features of the original data. Additionally, latent spaces typically obscure fine-grained details about individual data points, making it harder to reverse-engineer or infer private information from the shared model updates.\n\u2022 Conditional Distributions for Imputation. FedMobile uses conditional distributions \\(Q_k\\) (in Eq. (4)) and \\(\\psi_k\\) (in Eq. (5)) to model"}]}