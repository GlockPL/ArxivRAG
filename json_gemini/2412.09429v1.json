{"title": "FROM INTENTION TO IMPLEMENTATION: AUTOMATING BIOMEDICAL RESEARCH VIA LLMS", "authors": ["Yi Luo", "Linghang Shi", "Yihao Li", "Aobo Zhuang", "Yeyun Gong", "Ling Liu", "Lin Chen"], "abstract": "Conventional biomedical research is increasingly labor-intensive due to the exponential growth of scientific literature and datasets. Artificial intelligence (AI), particularly Large Language Models (LLMs), has the potential to revolutionize this process by automating various steps. Still, significant challenges remain, including the need for multidisciplinary expertise, logicality of experimental design, and performance measurements. This paper introduces BioResearcher, the first end-to-end automated system designed to streamline the entire biomedical research process involving dry lab experiments. BioResearcher employs a modular multi-agent architecture, integrating specialized agents for search, literature processing, experimental design, and programming. By decomposing complex tasks into logically related sub-tasks and utilizing a hierarchical learning approach, BioResearcher effectively addresses the challenges of multidisciplinary requirements and logical complexity. Furthermore, BioResearcher incorporates an LLM-based reviewer for in-process quality control and introduces novel evaluation metrics to assess the quality and automation of experimental protocols. BioResearcher successfully achieves an average execution success rate of 63.07% across eight previously unmet research objectives. The generated protocols averagely outperform typical agent systems by 22.0% on five quality metrics. The system demonstrates significant potential to reduce researchers' workloads and accelerate biomedical discoveries, paving the way for future innovations in automated research systems.", "sections": [{"title": "1 INTRODUCTION", "content": "Biomedical research is a fundamental driving force behind human development. By uncovering the under-lying mechanisms of diseases (Council et al., 2005), biomedical research improves global health, extends life expectancy, and enhances the quality of life. It also fuels economic growth, scientific advancements, and overall societal well-being.\nTraditional biomedical research relies heavily on labor-intensive processes like manual data collection, com-prehensive literature reviews, complex experimental designs, and extensive data analysis. Although the con-ventional approach has facilitated notable breakthroughs in disease prevention (Huang et al., 2023; Wang et al., 2023d; Agarwal et al., 2024), diagnosis (Therriault et al., 2024; Zheng et al., 2024; Sepahi et al., 2024), and treatment (Chang et al., 2024; Apperloo et al., 2024; Douvaras et al., 2024), it struggles to keep"}, {"title": "2 RELATED WORK", "content": "Artificial intelligence (AI) has improved biomedical applications about processing different textual data (Jin et al., 2024; Li et al., 2023b; Sudarshan et al., 2024). Conventionally, small-scale language models like BERT (Devlin, 2018) were used, but scaling laws indicate that increasing model parameters brings enhanced performance, leading to superior reasoning capability and higher answer accuracy. Therefore, we have seen large amounts of applications based on large language models (LLMs) (Singhal et al., 2023; Zhang et al., 2023; Chen et al., 2023; Bolton et al., 2024; Li et al., 2023c).\nLarge language models (LLMs) are generally pre-trained on vast open-domain corpora but lack domain-specific knowledge. To enhance domain-specific performance, several techniques are used: (1) Fine-tuning optimizesthe total or a part of parameters to improve the model performance on a small, specific dataset (Hu et al., 2021; Singhal et al., 2023; Chen et al., 2023; Bolton et al., 2024). (2) Reinforcement learning with human feedback (RLHF) or AI feedback (RLAIF) updates the model parameters to align LLM's responses with responses from humans or a teacher-model via a reinforcement learning framework (Ouyang et al., 2022; Zhang et al., 2023; Li et al., 2023c). (3) Prompt engineering(White et al., 2023) involves giving instructions or examples to LLM to enforce rules or enhance reasoning(Maharjan et al., 2024; Nachane et al.,"}, {"title": "2.1 BIOMEDICAL LLMS", "content": "Large language models (LLMs) are generally pre-trained on vast open-domain corpora but lack domain-specific knowledge. To enhance domain-specific performance, several techniques are used: (1) Fine-tuning optimizes the total or a part of parameters to improve the model performance on a small, specific dataset (Hu et al., 2021; Singhal et al., 2023; Chen et al., 2023; Bolton et al., 2024). (2) Reinforcement learning with human feedback (RLHF) or AI feedback (RLAIF) updates the model parameters to align LLM's responses with responses from humans or a teacher-model via a reinforcement learning framework (Ouyang et al., 2022; Zhang et al., 2023; Li et al., 2023c). (3) Prompt engineering(White et al., 2023) involves giving instructions or examples to LLM to enforce rules or enhance reasoning(Maharjan et al., 2024; Nachane et al.,"}, {"title": "2.2 LLM-BASED AGENTS FOR RESEARCH", "content": "LLM-based agents offer significant advantages over the direct use of LLMs, such as actively acquiring in-formation, interacting with environments, and stronger reasoning and planning(Gao et al., 2024). LLMs can function as a single agent by being assigned with specific roles through in-domain fine-tuning (Weng et al., 2024) or role-specific prompts(Boiko et al., 2023; Nori et al., 2023; Park et al., 2023; Wang et al., 2023a; Sudarshan et al., 2024; Fernando et al., 2023; Yang et al., 2023). In contrast, a Multi-Agent System (MAS) comprises multiple LLM-based agents, enabling task completion through various cooperative methods. One approach involves assigning distinct roles to agents, who then reach consensus through negotiation, like discussing clinical diagnosis via multi medical experts(Tang et al., 2024; Fan et al., 2024; Li et al., 2024). However, the consensus can be unreliable due to potential instability and hallucinations from LLMs. Al-ternatively, MAS can distribute complex tasks into sub-tasks among agents, such as dividing a scientific discovery process into creating idea, experimentation, and writing (Lu et al., 2024). MAS is promising for tasks with complex logical chains and highly detailed requirements. After decomposing tasks and as-signing them to agents, we can synthesize the results effectively. Furthermore, employing a professional and rigorous workflow framework helps constrain the agents' generative processes, ensuring highly reliable outcomes.\nDue to the robust performance of LLM-based multi-agent systems (MAS), AI for Research (AI4Research),with MAS at its core, has gained significant attention, particularly in automating scientific workflows. A survey categorizes AI agents into four automation levels based on their proficiency in experimental design and execution (Gao et al., 2024). Level 0 agents can only perform specific predefined tasks (Lim et al., 2023). Level 1 agents can design simple experimental protocols and use in-silico or lab tools (Boiko et al., 2023; Zhou et al., 2023). Level 2 agents, which include our work, are capable of developing rigorous experimental protocols and employing ex-silico tools and statistical methods for hypothesis evaluation (Lu et al., 2024; M. Bran et al., 2024). Level 3 agents, which remain undeveloped, are envisioned to discover new methods and employ diverse techniques to measure biological phenomena.\nCurrent AI4Research systems are infeasible for the biomedical field. Firstly, most systems are designed for computer science (CS) (Weng et al., 2024; Baek et al., 2024; Lu et al., 2024; Su et al., 2024; Yang et al., 2024), where public datasets like OpenReview are available, facilitating training and evaluation. Such exten-sive review data is lacking in the biomedical domain. Moreover, existing studies emphasize novelty based on literature summaries rather than reliability. Our goal, however, is to design experimental protocols with a focus on reliability and feasibility, necessitating fine-grained literature analysis. Additionally, the focus on experimental design differs across disciplines, further complicating the application of current systems to the biomedical field. For example, in AI, a recent study introduced an AI Scientist (Lu et al., 2024) to enhance and improve existing solutions based on a given task and initial experimental code. Conversely, BioRe-searcher often designs a series of customized experiments for a new research subject rather than improving existing solutions.\nRecently, some studies have proposed AI assistants for specific subfields of biomedical research. For in-stance, Genesis (Tiukova et al., 2024) focuses on systems biology but requires manual maintenance of a specialized database and the definition of specialized templates, thereby limiting its level of automation."}, {"title": "3 BIORESEARCHER", "content": "BioResearcher is designed to automate the research process for biomedical studies. Users provide a research objective and the conditions under which the experiments will be conducted. Users can also specify the research requirements, such as desired experimental steps or outcomes. All user input is in natural language."}, {"title": "3.1 FRAMEWORK OVERVIEW", "content": "BioResearcher is designed to automate the research process for biomedical studies. Users provide a research objective\u00b3 and the conditions under which the experiments will be conducted. Users can also specify the research requirements, such as desired experimental steps or outcomes. All user input is in natural language."}, {"title": "3.2 SEARCH", "content": "Scientific research initiates with a thorough review of related literature. Literature searching is especially important in BioResearcher because, unlike previous studies that use literature surveys to identify research gaps and propose novel ideas, BioResearcher ensures the process is built upon existing knowledge and preserves the reliability of the output. The Search module comprehensively explores pertinent literature and datasets throughout various research phases. Its internal procedure is as follows:\n(1) Query Generation: Searching directly with the user input can yield imprecise results for two reasons.Firstly, the user describes the research objective and conditions in natural language, while most databases use Boolean query logic, making it difficult to retrieve results that completely match the lengthy user input. Secondly, the user intention is provided with key high-level concepts, while the literature may use a more detailed description or synonyms, making it challenging to obtain all relevant materials. Thus, query rewrit-ing is crucial (Clark, 2013). However, manually crafting effective queries costs a lot of expertise, labor, and time (Scells et al., 2020). To address this, the Search module in BioResearcher employs an LLM-based query generator agent to create Boolean queries based on the user input. As displayed in Figure 3, the query generator extracts keywords to improve the retrieval accuracy. It also performs synonym expansion (e.g., expanding \"Single-cell sequencing\" with \"scRNA-seq\") to improve the retrieval recall. These struc-tured queries allow the module to interpret the research objective effectively and focus on the most relevant materials.\n(2) Retrieval: The module interfaces with databases through their APIs, enabling the retrieval of relevantliterature and datasets from established repositories such as PubMed Central (PMC), PubMed, and GEO. Additional databases can be integrated as needed to expand the system's capabilities.\n(3) Filtration: To retain only the most relevant and useful materials for further stages, our system employsan LLM-based filter agent to filter the returned research articles and datasets. For the research articles,we define a set of criteria detailed in Table 1. The filter examines the titles and abstracts of each article to determine their potential contribution to the research objectives. Each article receives a helpfulness score"}, {"title": "3.3 LITERATURE PROCESSING", "content": "Literature comprehension can be challenging for both researchers and LLMs due to the massive, lengthy, un-structured, and logically complex nature of research papers. To address this, we introduce the Literature Processing module, which streamlines comprehension and aids in experimental design. This module first standardizes research papers into structured experimental reports and systematically analyzes them. It then interacts with the Search module to assess the usability of datasets mentioned in these reports. The module operates in two main phases: report generation and report analysis. Figure 4 demonstrates the module in action.\nReport Generation Each biomedical research paper contains experiment-related contents scattered acrossvarious sections. We aim to extract and reorganize these contents into a condensed, highly structured exper-imental report. Doing so brings three advantages. Firstly, the experimental report is shorter than the original paper, enhancing the efficiency of LLMs. Secondly, uniform formatting across reports provides logical struc-ture coherence and format consistency, ensuring the LLMs grasp a big-picture idea of the most commonly acknowledged methods. Finally, the report is modularized with sections focusing on different aspects of the"}, {"title": "3.4 EXPERIMENTAL DESIGN", "content": "Scientific findings in biomedical research frequently face reproducibility issues, wasting resources and time while undermining the credibility of scientific outcomes (Begley & Ioannidis, 2015). A well-designed exper-imental protocol is crucial for obtaining reliable results and optimizing resource use (Larijani et al., 2020). Therefore, we develop the Experimental Design module, which uses an LLM-based experiment de-signer agent to create scientific and reproducible protocols. We propose a hierarchical learning approach to ensure the rationality of the logical structure of the generated experimental protocols. This method employs the Retrieval-Augmented Generation (RAG) technique aided by the analysis (i.e., using relevant reports' sections with high referability as reference materials), enabling the model to subsequently learn first-level headings independently, then outlines, and then experimental details from the reorganized reports. The design process is structured into three essential steps, as demonstrated in Figure 5.\n(1) First-Level Heading Design: The designer begins by reviewing the first-level headings and correspond-ing analyses of relevant experimental reports, paying particular attention to the reference and modification suggestions. Integrating this information with the research objective, conditions, and requirements, the de-signer crafts first-level headings for the new protocol. Additionally, for each section, the designer provides a rationale detailing the section's purpose, design reason, and reference source. This step is crucial as it lays the foundation for the experimental framework, ensuring alignment with the research objective.\n(2) Outline Generation: The designer then constructs a brief protocol outline, referencing the outlines of pertinent reports and analyses. The designer also includes the sources of reference following the outlines. The generated outline serves as a framework for organizing the protocol.\n(3) Generation of Implementation Details: Finally, the designer generates complete and specific imple-mentation details for each part of the experimental protocol. Relevant sections of experimental reports and corresponding analyses are extracted based on the reference sources provided in the previous step. This information, along with the useful datasets, the protocol outline, and the summaries from earlier sections, is"}, {"title": "3.5 PROGRAMMING", "content": "Programming in biomedical research presents a unique challenge to researchers, necessitating a combi-nation of programming proficiency and domain-specific knowledge. To address this, we propose the Programming module, which is crucial in enhancing the reproducibility of experimental designs and automating systems.\nTo reduce the complexity of coding and debugging, the Programming module employs an LLM-baseddry lab experiment extractor agent to derive a series of dry experiment tasks from the designed experimental protocol. Each task includes a task ID, a description of the task, and the types and descriptions of the input and output."}, {"title": "4 EVALUATION METRICS", "content": "Quantitative evaluation metrics are demanded to fully reflect a research assistant's ability to advance the automation of biomedical research. However, no existing study has proposed such metrics. In this context, we propose a comprehensive method for assessing the quality of the resulting experimental protocols and programs."}, {"title": "4.1 PROTOCOL EVALUATION", "content": "We evaluate experimental protocols from five dimensions: completeness, level of detail, correctness, logical soundness, and structural soundness. The definitions and formulas for these five metrics are as follows.\nCompleteness Completeness assesses how thoroughly each section of the protocol is described, consid-ering the necessary steps that should be added to achieve the design purpose of this part. The formula for completeness is given by:\n$Completeness = \\frac{\\sum_{i=1}^{m} \\frac{n_{es}}{n_{es} + n_{as}}}{\\sum_{i=1}^{m} \\frac{n_{ts}}{n_{ts} + n_{as}}}$\nwhere m represents the number of sections in a protocol. Here, $n_{es}$ refers to the number of existing steps in the i-th section, while $n_{as}$ denotes the total necessary steps for that section. Additionally, $n_{ts}$ indicates the total number of steps in a protocol, and $n_{as}$ represents the number of steps that need to be added.\nLevel of Detail The level of detail measures the degree to which a protocol provides sufficient information for each step, ranging from 0 (no detail) to 1 (fully detailed).\nCorrectness Correctness assesses the proportion of protocol steps that are free from factual errors. Ouranalysis reveals that protocols with shorter steps tend to exhibit higher correctness scores, as the probability of factual errors increases with longer and more detailed steps. Drawing inspiration from the BLEU (Pap-ineni et al., 2002) metric in machine translation, we also introduce a brevity penalty (BP) for shorter steps. The BP is constrained to a minimum of 0.5, and the formula is as follows:\n$BP = \\begin{cases}1, & \\text{if } l_{steps} > L \\\\\\max(e^{(1-\\frac{L}{l_{steps}})}, 0.5), & \\text{if } l_{steps} \\leq L\\end{cases}$\nWhere $l_{step}$ represents the length of steps, quantified by the number of sentences containing more than six words within each step (this criterion is employed to exclude steps' titles from the count). The parameter L denotes the average number of sentences within a step, calculated to be 4.42 from 315 protocols generated by different methods."}, {"title": "4.2 PROGRAM EVALUATION", "content": "As a critical component of research automation, the extent to which the Programming module can aug-ment research efficiency deserves attention. As detailed in Section 3.5, this module generates tailored code for each dry lab experiment task. To assess its effectiveness, we propose two scoring systems. The first computes the execution success rate, reflecting the percentage of successfully completed tasks per protocol. The second metric assigns error levels to the tasks that remain incomplete, reflecting the severity of errors encountered during code execution. The detailed grading criteria are outlined in Table 2."}, {"title": "5 EXPERIMENT", "content": "In this section, we design comprehensive experiments to answer the following research questions:\n1. How does BioResearcher perform in automating the entire biomedical research process?"}, {"title": "5.1 EXPERIMENT SETUP", "content": "In our experiments, we utilize the GPT-40 model as the foundational LLM for all the agents in BioRe-searcher. The temperature settings for various agents within the system are as follows: the query generatoris configured with a temperature of 0.7 to introduce a moderate level of variability in the generated queries. Conversely, the reviewer and the LLM used for evaluation are set to a lower temperature of 0.1 to ensure more deterministic and consistent evaluations. All other agents operate at a temperature of 0.5, balancing between randomness and determinism to maintain overall coherence and reliability in the agents' outputs. Additionally, the query generator generates five queries for each user input, with a maximum retrieval quan-tity of 10 per query for each database. The maximum number of interaction rounds for the LLM reviewer in a single session is 6.\nBaselines We evaluate BioResearcher by comparing it with three well-known agent systems. (1) Re-Act (Yao et al., 2023), which integrates reasoning and action within LLMs to effectively manage complex reasoning and decision-making tasks. (2) Plan-and-Execute (Wang et al., 2023c), which utilizes an iterativeframework to accomplish tasks through sequential planning and execution. (3) A naive RAG-based LLM system, which employs a naive RAG module to search for relevant content and generates the answer in a single step. Section A provides a detailed description of the implementation of these baseline systems."}, {"title": "5.2 PERFORMANCE OF END-TO-END AUTOMATION", "content": "We collect eight ongoing research objectives from a biomedical laboratory to ensure that no published work has addressed these research objectives, as detailed in Table 9. Each objective is respectively processed for three runs to minimize randomness. We evaluate three baseline systems and our system for designing and executing experiments for these objectives. We equip the React and Plan-and-Execute systems with four tools: (1) a search tool utilizing the NCBI API 4 to retrieve descriptions of relevant papers, (2) a downloadtool for acquiring these papers and storing them in a chunked and vectorized format, (3) a search tool using the NCBI API to obtain descriptions of relevant datasets and storing them in a chunked and vectorized format, and (4) a search tool that extracts pertinent content from the resulting vector database."}, {"title": "5.2.1 RESULTS", "content": "Table 3 presents the average scores for protocols generated by different systems, including Completeness,Level of Detail, Correctness, Logical Soundness, and Structural Soundness. We also calculate the overall"}, {"title": "5.2.2 THE QUALITY OF LLM EVALUATION", "content": "To testify whether the evaluation gained by the LLM judge is truthful, we engage both human experts andthe LLM to evaluate 18 protocols using consistent criteria focused on a single research objective. By con-"}, {"title": "5.3 PERFORMANCE OF SEARCH MODULE", "content": ""}, {"title": "5.3.1 THE EFFECT OF GENERATED QUERIES", "content": "We conduct a comparative experiment to assess the effectiveness of LLM-generated queries. An LLM andthree human participants independently generate five queries for each user input, which specifies a research objective, conditions, and requirements. The evaluation metric is the number of relevant papers or datasets retained after retrieval and filtering. The LLM generates queries three times, with the results averaged, and a similar average is calculated across the three human participants. This experiment, covering ten research objectives listed in Table 10, presents its results in Figure 8.\nLLM-generated queries generally outperform human-generated ones across most objectives. (1) In Fig-ure 8(a), which compares the number of useful papers retrieved, the LLM-generated queries show signifi-cantly stronger performance in objectives 3 and 7, but the increases are modest on objectives 2, 5, 6, and 9. Human-generated queries perform slightly better in three objectives, suggesting that human intuition can offer an edge in certain cases. (2) Figure 8(b) presents a similar pattern in dataset retrieval. The LLM outper-forms human participants in retrieving useful datasets for eight objectives, particularly Objective 3, where the LLM retrieved an average of six datasets compared to the human average of one-third. This indicates that only one human participant successfully generated queries that retrieved one useful dataset. Conversely, for objectives such as Objective 2 and Objective 10, human-generated queries show a slight advantage."}, {"title": "5.3.2 THE EFFECT OF LLM-BASED FILTER AGENT", "content": "To assess the precision of the ratings assigned by the LLM-based filter agent, we engage human reviewers to undertake a parallel assessment using the scoring criteria outlined in Table 1. Both the filter agent and human reviewers evaluate the same set of papers and datasets, with their ratings based solely on each paper's title and abstract or the dataset's description. We use Kendall's W to assess agreement for the ordinal ratings of 150 papers, as it is suitable for measuring concordance in ordinal data. For the binary ratings of 50 datasets, we apply Fleiss' kappa, which evaluates inter-rater agreement for categorical data among multiple raters. These papers and datasets are sampled from the search results of 10 topics listed in Table 10. Higher values in both metrics indicate stronger agreement, enhancing the reliability of the human ratings. A majority voting mechanism establishes the ground truth for human ratings, which serves as the benchmark for calculating the model's accuracy. For papers, those with scores of 4 or higher are classified as useful, while others are deemed not useful, and the accuracy is calculated based on this binary classification. The results of this study are represented in Table 5."}, {"title": "5.4 PERFORMANCE OF REPORT GENERATION", "content": "To test the impact of our hierarchical report generation method, we compare its reports against those pro-duced by ReAct, Plan-and-Execute, and a single-step LLM approach. Specifically, we standardized 20 papers into an experimental report format using each method. None of the methods are equipped with any additional tools. We prompt an LLM to evaluate the reports across four dimensions: logical soundness, level of detail, consistency with the original paper, and readability. The model assigns a score ranging from 1 to 5 for each dimension, with the scoring criteria detailed in Table 6. The final score is calculated as the average"}, {"title": "5.5 PERFORMANCE OF EXPERIMENTAL DESIGN", "content": "To validate the efficacy of the experimental design module, we construct a comparison experiment against the three baselines introduced in Section 5.1. We employ the search module and Literature Processing module of BioResearcher for 15 research objectives listed in Table 11, thereby obtaining reports and analyses that serve as a knowledge base. The three baselines can invoke search tools to retrieve relevant content from this knowledge base. All experiments are repeated three times, and the evaluation results are averaged and presented in Table 7."}, {"title": "6 LIMITATIONS AND FUTURE WORKS", "content": "Limitations In this paper, BioResearcher effectively automates the entire biomedical research process,from literature and dataset searches to experimental design and execution, given a research objective and specified conditions. However, the system does not achieve complete success in executing experiments without manual intervention. This limitation is partly due to the need for further enhancement of the code generator agent's performance. Additionally, certain anomalies, such as the unavailability of resources spec-ified in the experimental protocols, also hinder fully automated execution. It highlights the necessity of an-ticipating a broader range of exceptional scenarios and developing corresponding solutions in future work.Moreover, during our practice, we observed that excessively long inputs often result in LLMs producing shorter, more generalized outputs. To enable LLMs to generate detailed and comprehensive experimental protocols, we employed the multi-step, section-wise processing approach at various stages, including ex-perimental report generation and experimental design. However, this iterative method increases the overall cost."}, {"title": "7 CONCLUSION", "content": "In this study, we introduced BioResearcher, an intelligent research assistant that automates the biomedical research process. Utilizing a modular LLM-based multi-agent architecture, BioResearcher addresses the multidisciplinary demands, logical complexities, and performance evaluation challenges of biomedical re-search. It automates tasks such as literature review, experimental protocol design, and code implementation, significantly improving research efficiency and reducing manual workload. We developed novel evaluation metrics focusing on protocol quality and experimental automation, providing a robust framework for assess-ing performance. Our results show that BioResearcher designs executable experimental protocols with a high success rate, outperforming existing typical agent systems.\nThe practical significance of BioResearcher lies in its ability to automate the research pipeline, allowing researchers to focus on strategic decision-making and innovation. This advancement accelerates biomedical discoveries and future developments in automated research systems. By potentially extending its capabilities to wet lab experiments, BioResearcher promises broader applications. This study lays the groundwork for enhancing automated research technologies, contributing to global health and scientific progress."}]}