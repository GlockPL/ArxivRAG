{"title": "Sparks of Artificial General Intelligence(AGI) in Semiconductor Material Science: Early Explorations into the Next Frontier of Generative AI-Assisted Electron Micrograph Analysis", "authors": ["Sakhinana Sagar Srinivas", "Geethan Sannidhi", "Sreeja Gangasani", "Chidaksh Ravuru", "Venkataramana Runkana"], "abstract": "Characterizing materials with electron micrographs poses significant challenges for automated labeling due to the complex nature of nanomaterial structures. To address this, we introduce a fully automated, end-to-end pipeline that leverages recent advances in Generative AI. It is designed for analyzing and understanding the microstructures of semiconductor materials with effectiveness comparable to that of human experts, contributing to the pursuit of Artificial General Intelligence (AGI) in nanomaterial identification. Our approach utilizes Large MultiModal Models (LMMs) such as GPT-4V, alongside text-to-image models like DALL-E-3. We integrate a GPT-4 guided Visual Question Answering (VQA) method to analyze nanomaterial images, generate synthetic nanomaterial images via DALL-E-3, and employ in-context learning with few-shot prompting in GPT-4V for accurate nanomaterial identification. Our method surpasses traditional techniques by enhancing the precision of nanomaterial identification and optimizing the process for high-throughput screening.", "sections": [{"title": "Introduction", "content": "The multifaceted journey of semiconductor production involves several stakeholders. Fabless firms such as NVIDIA, Qualcomm, and AMD focus on designing and developing semiconductor chips, yet they do not own fabrication facilities. Instead, they utilize Electronic Design Automation (EDA) tools for designing, simulating circuits, and verifying semiconductor devices. Following this phase, specialized foundries like Taiwan Semiconductor Manufacturing Company (TSMC) and Samsung Electronics fabricate the designs provided by the fabless companies onto silicon wafers. These foundries employ advanced sub-14 nm technology to etch precise geometries essential for modern high-performance chips. After fabrication, companies like Advantest and Teradyne employ specialized semiconductor test equipment to subject the chips to a rigorous evaluation phase, ensuring they meet performance and reliability standards. Post-testing, packaging, and assembly companies such as ASE Technology Holding and Amkor Technology prepare the semiconductor devices for integration into larger electronic systems. In contrast to these individual stakeholders, Integrated Device Manufacturers (IDMs) like Intel and Texas Instruments oversee nearly all aspects of the semiconductor production process, from design to packaging. As the semiconductor industry continues to strive toward miniaturization, aiming for more powerful and energy-efficient chips, it faces challenges such as manufacturing errors and quantum tunneling. Addressing these challenges requires advanced imaging and analysis, as well as innovative engineering approaches, all of which are crucial for maintaining the rapid evolution of semiconductor technology in today's digital age. One of the key advancements in the industry, particularly in sub-7 nm technology, hinges on achieving micro and nanoscale precision. Tools like Scanning Electron Microscopy (SEM) and Transmission Electron Microscopy (TEM) are at the forefront of this effort. These electron beam tools provide detailed micrographs, or nano images, of semiconductor materials and structures. The advanced imaging techniques play a vital role in manufacturing analysis, enabling clear visualization and analysis of microstructures. This makes these tools indispensable for quality control, process monitoring, and failure analysis to ensure that semiconductors adhere to design parameters and identify defects. Materials characterization at the micro and nanoscale is imperative for continued technological advancement. However, automated labeling of electron micrographs faces challenges due to the high similarity between different nanomaterial categories (high inter-similarity), wide appearance variance within a single category (high intra-dissimilarity), and spatial heterogeneity of patterns of nanomaterials across different length scales in electron micrographs. The manifold complexities of automated nanomaterial identification tasks are illustrated in Figure 1. Advancements in machine learning and image recognition technologies are crucial for the accurate labeling and analysis of electron micrographs, thereby improving quality control and performance in the semiconductor industry and aiding its further progression. In the realm of AI, Large Language Models (LLMs) such as GPT-4 (language-only), which empower conversational agents like ChatGPT to generate human-like dialogue as responses to user inputs, have recently gained prominence and showcased unprecedented capabilities in human-AI interaction. These large-scale models leverage an autoregressive, decoder-only architecture and undergo pre-training in a self-supervised learning paradigm on vast amounts of unlabeled text corpora. At their core, they operate by predicting the next token in a sequence based on the context provided by preceding tokens a foundational principle of language modeling. Additionally, to refine their outputs and better align with human preferences, they are fine-tuned using reinforcement learning from human feedback (RLHF). The foundational LLMs have revolutionized natural language processing(NLP) with their advanced text comprehension and sophisticated logical reasoning, leading to remarkable performance across various NLP tasks. A key feature of these large-scale models is their \"prompt and predict\" paradigm, which allows users to instruct LLMs using natural language prompts to set the context and task-specific instructions to generate the text-based response. The term \u201cprompting\" refers to the method of conditioning the language model to respond to the instructions based solely on the patterns and knowledge acquired during the training phase. General-purpose language models, like GPT-4, can be steered to generate desired outputs using various prompt engineering strategies. One of these strategies is zero-shot learning, where the language model generates an output based solely on its pre-trained knowledge, without any task-specific demonstrations (input-output mappings). In contrast, few-shot learning provides the language model with a limited number of demonstrations to guide its output. In essence, prompts that include both explicit conditioning based on task-specific instructions and a few demonstrations are termed few-shot prompts, while those that rely solely on task-specific instructions are referred to as zero-shot prompts. Chain-of-thought (CoT) and tree-of-thought (ToT) prompting techniques assist LLMs in explaining their reasoning step-by-step and in exploring multiple possible thought paths simultaneously, thus enhancing performance on tasks involving reasoning, logic, and more. The choice between these strategies typically depends on the context and specific objectives of the request, with each designed to optimize the language model's performance. Proprietary LLMs, such as GPT-4(OpenAI 2023b), demonstrate advanced language comprehension. However, their 'black-box' nature can pose challenges to interpretability and explain-ability, especially given the lack of direct access to internal state representations like logits or token embeddings. Furthermore, while general-purpose LLMs are designed to handle a broad range of tasks, adapting them for niche tasks can be highly resource-intensive due to their high model complexity and size, and their performance might not always be optimized for specialized applications. In contrast, open-source small-scale models like BERT(Devlin et al. 2018), following a \"pre-training and fine-tuning\" approach, can be more cost-efficient for task-specific customization. These smaller language models also provide better interpretability because they allow access to internal state representations like logits or token embeddings, thanks to their open nature. However, they might not match the reasoning and generalization capabilities of proprietary LLMs, sometimes producing less coherent and contextually apt outputs. In recent times, the trend has shifted towards exploring and expanding the capabilities of foundational language-only LLMs in multimodal settings to enhance their performance and applicability across a wider range of tasks. GPT-4 with Vision (GPT-4V(OpenAI 2023c)) represents a significant advancement over the earlier, text-focused OpenAI GPT-4, which was limited to language processing. Large multimodal models (LMMs) such as GPT-4V are instruction-following, language-based human-AI interaction systems capable of analyzing image inputs by interpreting and responding to text prompts, which enables them to generate text-only outputs conditioned on the provided visual context. GPT-4V integrates visual capabilities with its existing language processing abilities, enabling it to perform tasks such as analyzing and describing images based on textual prompts, transcribing text from images, and deciphering data, among others, thereby broadening the horizons for real-world applications. Similarly, DALL-E-3(OpenAI 2023a,d), an advanced version of OpenAI's DALL-E(Ramesh et al. 2022), excels in text-to-image synthesis, designed to generate accurate images from textual prompts. A significant advancement over previous models in the realm of AI image generation, DALL\u00b7E-3 not only generates high-quality images that accurately reflect the intended textual descriptions but also has the new ability to modify (edit) and transform existing images based on textual inputs"}, {"title": "Problem Statment", "content": "Our study focuses on the classification of electron micrographs using few-shot learning in large multimodal models (LMMs), such as GPT-4V. This approach involves leveraging a small set of relevant demonstrations(image-label pairs) to make predictions on new data (query images) without further fine-tuning of the model parameters. A common scenario is where the model samples image-label pairs from a training dataset D as demonstrations, and then predicts the label of a query image from the test dataset based on these demonstrations. Consider a training dataset D consisting of image-label pairs {(Ii, yi)}i=1N. Additionally, let Iq denote a query image. The task is to predict the label yq of the query image Iq based on D, without model parameters update. In this scenario, using GPT-4V, the task can be framed as a probabilistic inference problem where the objective is to estimate the conditional probability distribution P(yq|Iq, D), representing the probability of the label yg given the query image Iq and the training dataset D. Through this formulation, the few-shot learning task aims to sample the relevant demonstrations in the dataset D to make an informed prediction for the label yg of the query image Iq, without requiring additional training of the model parameters."}, {"title": "Proposed Method", "content": "Electron Micrograph Encoder: Let's consider an input image, denoted as I expressed as a 3D tensor with dimensions H \u00d7 W \u00d7 C, where H represents the image's pixel height, W its pixel width, and C the number of channels associated with each pixel in the image. To process this input image, it is divided into smaller, non-overlapping patches, with each patch treated as a token having fixed-size spatial dimensions of P \u00d7 P \u00d7 C, where P represents the patch size. The tokenization of the image results in a total number of patches given by n = (HW)/P^2. These patches are linearly encoded into 1D vectors, forming a sequence of tokens represented as I' \u2208 Rnxd where d is the patch embedding dimension. To maintain the spatial information of patches from the original image, differentiable positional embeddings representing patch positions are added element-wise to the patch embeddings. This process allows the framework to effectively analyze and understand the complex visual and spatial context of image patches. We also append a classification token  to the token sequence. This token aggregates information from all patches, creating a global representation that helps the framework gain a coherent understanding of the holistic visual context of the image. We input this augmented token sequence into ViT(Dosovitskiy et al. 2020), which is composed of multiple stacked transformer encoder layers. Each encoder layer processes the patch embeddings hierarchically using a higher-order attention mechanism instead of the standard multi-head self-attention (MHSA), iteratively updating patch representations at different levels of abstraction. The hierarchical attention mechanism allows the framework to grasp visual information comprehensively at different levels of detail, from fine-grained features to high-level context. This process operates in two stages: local attention, which focuses on patch-level relationships to capture the interactions between patches and their immediate context within the image, and global attention, which aggregates global information by incorporating the classification token, aiding the framework in achieving an overarching understanding of the visual context throughout the entire image. After passing through the transformer layers, we consider only the output embedding hcls corresponding to the  token as the unified, holistic representation of the entire image, aggregating information from all patches by distilling the diverse and distributed information from the smaller, localized regions of the image. In summary, the framework processes input images by dividing them into patches, encoding them into tokens, incorporating a classification token , and using a hierarchical attention mechanism to create a holistic image representation, hcls, that embodies both local and global context. For few-shot prompting of LMMs such as GPT-4V, we provide a small number of demonstrations(image-label pairs as input-output mappings) for nanomaterial identification in the query image. This is accomplished using an electron micrograph encoder that selects relevant images from the training set resembling or matching the query image.\nZero-Shot Chain-of-Thought (CoT) GPT-4V Prompting: The GPT-4V API, accessible through Multimodal Modeling as a Service (MMaaS)\u2014a cloud-based platform that accepts both image and text inputs to generate output is not yet fully available to the public. While still in beta phase, GPT-4V can be accessed by ChatGPT Plus subscribers at chat.openai.com, but usage is subject to a cap. Our work on nanomaterial image interpretation begins with using GPT-4 to generate natural language questions that serve as task-specific instructions. These textual prompts, combined with visual (image) inputs, are employed to construct multimodal prompts that guide GPT-4V in Visual Question Answering (VQA) tasks for analyzing nanomaterial images. Consequently, GPT-4V provides contextually rich textual responses that encapsulate the information within the visual inputs. The task instructions created by GPT-4 (language-only) are crucial for directing GPT-4V's VQA performance on nanomaterial images. By utilizing a zero-shot CoT prompt template with these instructions and the query image, LMMs like GPT-4V can generate detailed descriptions of nanomaterial images. This approach takes advantage of the multimodal model's intrinsic domain-specific knowledge acquired during training to provide comprehensive insights into the images. Essentially, GPT-4 formulates general questions about nanomaterial images, which are then converted into structured CoT prompts guiding GPT-4V in its detailed visual analysis that explores the image's structure, patterns, imaging techniques, and context-be it experimental, real-world, or theoretical. In guiding GPT-4V's analysis of nanomaterial images, we focus on the following key areas: (a) Basics: Identify the type and scale of the nanomaterial. (b) Morphology and Structure: Describe the shape, layers, domains, and uniformity. (c) Size and Distribution: Determine size, distribution pattern, and signs of aggregation. (d) Surface Characteristics: Observe texture, defects, or impurities. (e) Composition and Elements: Identify compositional variations and specific elements. (f) Interactions and Boundaries: Examine nanostructure interactions and boundaries. (g) External Environment: Observe interactions with surroundings and identify non-nanomaterial structures. (h) Image Technique and Modifications: Identify the imaging technique and any post-processing. (i) Functional Features: Look for functional features and assess if dynamic processes are captured. (j) Context and Application: Understand the sample's intended use and its status as real, experimental, or theoretical. The CoT prompt format is as follows:"}, {"title": "Experiments And Results", "content": "The primary focus of our research was to automate the identification of nanomaterials using the SEM dataset (Aversa et al. 2018). This benchmark dataset, annotated by human experts, encompasses 10 unique categories that reflect a wide variety of nanomaterials, including particles, nanowires, and patterned surfaces. It contains around 21,283 electron micrographs in total. A visual depiction of the nanomaterial categories within the SEM dataset is provided in Figure 3. Although the first experimental findings (Modarres et al. 2017) explored a subset of this dataset, our work leveraged the entire dataset as the subset was not publicly available. The curators of the original dataset (Aversa et al. 2018) did not specify predefined splits for training, validation, and test datasets, prompting us to employ a custom approach to evaluate the performance of our framework. This approach enabled a balanced comparison with widely-accepted baseline models in a competitive benchmark scenario.\nData Prepration : Identifying Hard-to-Classify Micrographs: A Train/Test Approach\nThe SEM dataset(Aversa et al. 2018), comprises images with original dimensions of 1024 \u00d7 768 \u00d7 3 pixels, which were downscaled to 224 \u00d7 224 x 3 pixels to facilitate our analysis. We standardized the images using z-score normalization to ensure a mean of zero and a variance of one, and then flattened the images into 1D-vectors. Subsequently, we employed Principal component analysis (PCA) to reduce the dimensionality of the image data, which involved computing the eigenvectors and eigenvalues of the data covariance matrix. We selected the top-N eigenvectors, where N represents the desired reduced dimensionality, and projected the original data onto the lower-dimensional subspace spanned by these eigenvectors. After dimensionality reduction with PCA, we applied K-Means clustering to segment the images into distinct groups based on inherent patterns and similarities, enabling a more structured analysis of the electron micrographs to identify and understand underlying structures and variations. For our analysis, we set the initial number of clusters at K=10, in line with the predefined number of nanomaterial categories in the SEM dataset. K-Means clustering iteratively works by randomly initializing centroids, assigning each image to the nearest centroid, recalculating the centroids as the mean of the images in each cluster, and repeating this process until the centroids no longer change significantly. After clustering, the most difficult images to classify can be identified by calculating the distance of each image from its assigned centroid, where larger distances suggest greater classification difficulty. Images in smaller or high-variance clusters may also indicate a more challenging classification task. Additionally, calculating the silhouette score for each image, with lower scores indicating a possible better fit with neighboring clusters, further highlights classification challenges. Evaluating the clustering and pinpointing hard-to-classify images through comparison with available ground truth labels enables a thorough analysis and deeper understanding of the image data. We sampled 10% of the hard-to-classify images from the SEM dataset to create a fixed test dataset, and used the remaining images as the training dataset. We then evaluated our proposed framework and the baseline algorithms on these datasets. Incorporating hard-to-classify images into the test set is essential for a thorough evaluation of classification algorithms. This approach challenges the algorithms, providing a rigorous assessment that prevents overestimation of performance based on simpler examples. Moreover,"}, {"title": "Conclusion", "content": "In this work, we introduce an autonomous framework that innovatively applies advanced generative AI for identifying nanomaterials in electron micrographs. Our framework synergizes the sophisticated capabilities of large multimodal models like GPT-4V with the generative prowess of text-to-image models such as DALL\u00b7E 3 to substantially enhance nanomaterial classification accuracy. It employs GPT-4V's Visual Question Answering (VQA) for in-depth analysis of nanomaterial images, utilizes DALL-E 3 for creating synthetic images from question-and-answer pairs generated by GPT-4V, and leverages few-shot prompting of GPT-4V's for in-context learning, enabling more efficient classification. The method marks a significant advance over conventional techniques, offering a streamlined process for high-throughput screening within the semiconductor industry."}, {"title": "Technical Appendix", "content": "In our experiments, we specifically designed an electron micrograph encoder to process electron micrographs and generate a comprehensive image representation. The ultimate goal is to leverage this encoder for few-shot prompting of Large Multimodal Models (LMMs), such as GPT-4V, to identify the nanomaterial category for a given query image. In this few-shot prompting approach, the encoder computes image embeddings and then identifies a select number of analogous or identical images from the training set, relevant to the query image, through a similarity learning technique. By presenting these selected demonstrations (sampled image-label pairs) to the LMMs, they can effectively predict the nanomaterial category of the query image, even with minimal demonstrations. Unsupervised image representation learning is essential in this context for several reasons. First, it provides the foundation for few-shot prompting with LMMs like GPT-4V, enabling the electron micrograph encoder to capture comprehensive image representations that are critical for effectively identifying relevant demonstrations (input-output pairs). Unsupervised learning may lead to more generalized image representations since the encoder, not limited by predefined labels, can capture a wider range of features potentially relevant to the identification of nanomaterials-features that supervised training sets might not include. Moreover, the encoder's ability to identify similar images affords a nuanced understanding of the data, uncovering relationships and structures within the electron micrographs that could elude human observers or be too complex for supervised models to discern without extensive labeled data. In essence, this approach is a calculated strategy that utilizes the abundance of data to set the stage for proficient few-shot prompting with LMMs. We describe the training of the electron micrograph encoder in unsupervised learning settings as follows: We utilized the SEM dataset(Aversa et al. 2018), which is a compilation of electron micrographs of various nanomaterials with dimensions of 1024 x 768 \u00d7 3 pixels. For our analysis, we resized these images to 224 \u00d7 224 x 3 pixels and standardized them to maintain a constant mean and covariance of 0.5 across channels. This data preprocessing ensures that image values span between -1 and 1. Subsequently, we split the downsized images into distinct patches, representing the micrographs as patch sequences. We obtained patch sequences with a resolution of 32 pixels each. The patch dimension (dpos) and the position embedding dimension (d) were both set to 128. The encoder was trained for 50 epochs with an initial learning rate of 1 \u00d7 10-3 and a batch size of 48. Additionally, we configured a few hyperparameters for the attention layer: the number of attention heads (H) was set to 4, and the dimensionality of Key/Query/Value (dh) was set to 32. To enhance the performance of the electron micrograph encoder, we employed two key strategies: (a) early stopping on the validation set, which halts training when the encoder's performance on the validation data plateaus, thereby preventing overfitting; and (b) a learning rate scheduler that systematically reduces the learning rate by half if the validation loss does not improve for five consecutive epochs. This reduction in the learning rate aids the encoder in converging to a better solution and mitigates overfitting. Moreover, we utilized the Adam optimization algorithm (Kingma and Ba 2014) to update the encoder's trainable parameters. Training the electron micrograph encoder for unsupervised image representation learning involves optimizing a similarity measure between the representations of different views of the same image while minimizing similarity between views of different images. The Normalized Temperature-Scaled Cross Entropy Loss (NT-Xent Loss(Sohn 2016; Chen et al. 2020b)) is a commonly employed loss function for this task. Given a batch of images, we first generate two augmented views of each image. The micrograph encoder is then used to obtain representations hkls and hets of the two views of each image. The NT-Xent loss is defined as follows:\nL_{NT-Xent} = \\frac{1}{2N} \\sum_{k=1}^{2N} log \\frac{exp(sim(h_{kls}, h_{k+}))}{ \\sum_{l=1,l\\neq k,l\\neq k+} exp(sim(h_{kls}, h_{lets})/T)}\nwhere N is the number of images in the batch, sim(zk, z\u0131) = \\frac{ Zk \\cdot Z1}{|| Zk || || Z1 || } is the cosine similarity between representations zk and z\u0131, k+ is the index of the positive pair for zk, and T is the temperature parameter. The objective is to minimize LNT-Xent with respect to the parameters of the micrograph encoder, typically using gradient-based optimization algorithms to learn a representation space where similar images are mapped close together and dissimilar images are mapped far apart, thus maximizing similarity between like images. Once the micrograph encoder has been trained to represent images, it can be used to sample related images from the entire training dataset for few-shot prompting of GPT-4V. This is achieved by using the unsupervised image embeddings computed by the micrograph encoder to determine the similarity between different images. The images most similar to a given query image are then selected. The corresponding image-label pairs (demonstrations), along with the task-specific instruction to predict the nanomaterial category of the query image, are provided to GPT-4V, which then outputs the predicted nanomaterial category. The experiments were carefully designed to demonstrate the effectiveness of the proposed fusion framework, Generative Deep Learning for Nanomaterial Identification (GDL-NMID) leveraging the strengths of both GPT-4V and DALL\u00b7E 3, in comparison to the baselines. Note: API access for GPT-4V and DALL-E 3 has been restricted from public use but may become accessible starting in mid-November 2023. ChatGPT Plus subscribers can access GPT-4V and DALL-E 3 through the OpenAI ChatGPT web interface. To optimize computational resource usage, the system is trained on two V100 GPUs, each equipped with 8 GB of GPU memory, utilizing the PyTorch framework. This configuration ensures that the training process is completed within a reasonable timeframe. We conducted two individual experiments and reported the averaged results. Figure 6 illustrates the end-to-end pipeline of the framework. In our work, we explore Large Multimodal Models (LLMs) such as GPT-4V, which can process both input text and images to generate text responses, and text-to-image diffusion generative models like DALL-E 3. These"}]}