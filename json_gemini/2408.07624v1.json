{"title": "Battery GraphNets : Relational Learning for Lithium-ion Batteries(LiBs) Life Estimation", "authors": ["Sakhinana Sagar Srinivas", "Rajat Kumar Sarkar", "Venkataramana Runkana"], "abstract": "Battery life estimation is critical for optimizing battery performance and guaranteeing minimal degradation for better efficiency and reliability of battery-powered systems. The existing methods to predict the Remaining Useful Life(RUL) of Lithium-ion Batteries (LiBs) neglect the relational dependencies of the battery parameters to model the nonlinear degradation trajectories. We present the Battery GraphNets framework that jointly learns to incorporate a discrete dependency graph structure between battery parameters to capture the complex interactions and the graph-learning algorithm to model the intrinsic battery degradation for RUL prognosis. The proposed method outperforms several popular methods by a significant margin on publicly available battery datasets and achieves SOTA performance. We report the ablation studies to support the efficacy of our approach.", "sections": [{"title": "1 Introduction", "content": "High-energy density LiBs[13, 52, 89] are driving the global revolution of battery-powered solutions across the whole spectrum, from portable electronics to large-scale electric mobility and energy storage systems. Recent advances in battery materials and technologies[73, 22] have enabled low battery self-discharge, high power density, and lightweight, fast-charging LiBs. Despite the progress, battery capacity deteriorates[57, 40, 86, 53] due to repetitive charging and discharging, leading to low performance, low storage characteristics, and shorter life of the LiBs. The development of accurate battery RUL[81, 30, 80] estimation algorithms has gained attention in the advancement of battery management systems(BMS, [50, 67]) as the RUL translates to reliability, available range, and performance of battery-powered systems. The existing battery RUL prediction methods[62, 19, 56, 1] are limited in modeling the non-linear behavior of discharge characteristics of aging LiBs for accurate RUL prognosis. We begin with the hypothesis that LiBs are stochastic dynamical systems. The complex interactions between multiple degradation mechanisms, characterized by different physical and chemical processes, are responsible for battery degradation. The collective behavior of the interconnected and individual degradation mechanisms, as a whole, give rise to the non-linear dynamics of the complex system. The battery parameters capture the measurable effects of tractable battery degradation. There is a need and necessity to learn the system's elusive, relational structural-dynamic dependencies of the interdependent battery parameters while simultaneously learning the non-linear degradation dynamics of LiBs. The literature on relational learning or reasoning tasks is overwhelmingly focused on dynamical learning of interacting systems dynamics using the following approaches. (a) The implicit interaction models[68, 29, 63, 77, 34] utilize a fully-connected graph to model the relational structure. Redundant edges and learning noise-incorporated representations are the drawbacks of implicit interaction models. (b) The explicit interaction models are categorized into the following methods. (1) continuous relaxation methods[37, 76, 14, 83] that utilize graph similarity metric learning to obtain the real-valued weighted adjacency matrix, and (2) probabilistic sampling approaches [26, 85, 38] model the edge probability distribution over graphs. The relaxation methods"}, {"title": "2 Problem statement", "content": "Consider an arbitrary training set of graph-structured battery data consisting of graph-label pairs {(Gt, yt)}. The goal is to learn a model(f : Gt \u2192 yt) trained in inductive supervised learning to predict the label yt at any time step t associated with the input graph Gt and estimate uncertainties. In short, the input to the model is the multivariate battery parameters measurements viewed as the sequence of graphs Gt and predicts the RUL estimates yp."}, {"title": "3 Proposed Approach", "content": "The proposed framework consists of the following modules. (a) The dynamic graph inference module, for brevity, DGI. It is a probabilistic model that learns the time-evolving optimal graph structure for the structured spatio-temporal representation of the battery data. (b) The grapher module presents a node-level graph encoder that computes the fixed-size representation for each node by jointly modeling the latent inter-parameters and the long-range, intra-parameters dependencies in the graph-structured battery data. (c) A graph readout module for global pooling of the node-level representations to capture the global semantic structure in the fixed-length graph-level vector. We utilize a linear projection to transform the entire-graph fixed-size vector and apply a non-linear activation function to predict the high-quality RUL estimates. In summary, the proposed framework simultaneously learns the spatio-temporal dependencies among multiple battery parameters over time with a GNN backbone operating on a discrete latent graph in the node-level representations. It then performs inference over the node-level latent variables to predict RUL. Figure 1 illustrates the proposed method."}, {"title": "3.1 Dynamic Graph Inference(DGI)", "content": "The battery parameters do not have an explicit relational graph structure(unknown or partially known) underlying the data. We define a dynamic graph, Gt at a time point, t as a tuple(V, Et, {x}iev). V denotes a set of nodes. We represent each battery parameter as a node i \u2208 V. Every node is associated with a feature vector x \u2208 RC\u00b7T. Here, C denotes the number of charge-discharge cycles of the LiBs, and T is the number of time steps in each charge cycle. Et is the set of (directed) edges where each edge eji \u2208 Et models the pairwise relationship between the neighboring nodes j, i. The dynamic graphs Gt have a feature matrix, Xt that changes dynamically over time, an adjacency matrix, At, and an edge-set, Et that may vary over time, whereas the node-set V remains unchanged. The nodes of the dynamic graph are associated with trainable embeddings bi \u2208 Rd, 1 \u2264 i \u2264 n, which are d-dimensional continuous vector representations. DGI module learns the sparse regularized dynamic-graph structure from the explicit-battery data, x integrated with the implicit node embeddings, bi. We compute the pairwise parameters as given by,\n\n =(gfc(b\u1d62 + W\u1d62Wx\u209c||W\u1d62Wx\u2c7c)), \u2200i, j \u2208 {1, ..., n}, k \u2208 {0, 1}\n\nWhere Ok\u2208 Rn2x2 denotes probability matrix, Wdx(CT) is trainable weight matrix. gfc is a stack of fully-connected layers. o is a logistic sigmoid function. n, i.e., |V| denotes number of battery parameters. || denotes vector concatenation. O encodes relation between a pair of nodes (i, j) to a scalar \u2208 [0, 1]. 0 represents probability of a directed edge from node i to its potential neighbor j at a time point t, and 0 represents contrariwise probability. We sample the time-specific adjacency matrix[66], i.e., A; \u2208 Rn\u00d7n to determine the edges to preserve and drop the redundant edges via the element-wise, differentiable method, using the Gumbel-softmax trick[36, 51] as described by,\n\nA\u1d62'\u2c7c = exp((\u03c6\u1d62'\u2c7c + g\u1d62'\u2c7c)/\u03c4) / \u03a3\u2096 exp((\u03c6\u2096 + g\u2096)/\u03c4)\n     (1)\nwhere g; ~ Gumbel(0, 1) = log(- log(U(0, 1)) where U is uniform distribution. y, denotes the temperature parameter with 0.05. In summary, the DGI module learns the evolving irregular graph structure, which captures the time-dependent interdependence relations of the battery parameters. By viewing the battery data as node-attributed discrete-time dynamic graphs, we explore the GNN backbone to encode the complex discrete graph-structured data by summarizing the entire graph's structural characteristics to compute its optimal representations to facilitate the RUL prediction task."}, {"title": "3.2 Grapher", "content": "The grapher module consists of two blocks (1) the GNN block and the (2) the RNN block. The GNN block consists of a single-layer GNN \u2192 Batch Norm \u2192 Dropout sequence. We apply batchnorm[35] and dropout[5] for regularization. The GNN layer-wise operator involves two basic node-level computations at every layer. (1) AGGREGATE operation: aggregates neural messages(mji) from local-graph neighbors j \u2208 N\u2081, where mji denotes the neural message dispatched from source node j to sink node i via edge eji. It captures the lateral interaction. (2) UPDATE operation: transforms each node representation by combining its fixed-length representation obtained in the previous layer and the aggregated neural messages. We utilize the vanilla variant of GCN[43] to model the GNN layer, which couples the aggregate and update operations to refine the node-level representations. Let's consider an L-layer GCN, the mathematical formulation of the lth layer (1 < 1 < L) described below,\n\nh\u1d62^(l,t) = ReLU (W^(l) * \u03a3_{j\u2208N(i)\u222a{i}} 1/d\u1d62 * h\u1d62^(l-1,t))\n      (2)\nwhere hol,t) \u2208 Rd, i , i \u2208 V denotes node representation in the [th layer with h(0,t) = ho,t) = Wsx. Wg \u2208 Rdxd is weight matrix. di represents the degree of node i. We stack two GNN blocks and concatenate the output node-level representations from each block to compute h\u2081 \u2208 R2d. The RNN block consists of double-stacked GRU units[16, 17] that operate on hi \u2208 R2d for transforming the node-level representations to compute hi \u2208 Rd. It regulates the information flow through the gating mechanism to better capture the long-range spatio-temporal dependencies."}, {"title": "3.3 Graph Readout(GRo)", "content": "We utilize the global average pooling operation to aggregate node-level representations(h) computed by the grapher module and obtain a graph-level representation hg, as given by\n\nhg = READOUT ({(ho b\u2081) | i \u2208 V}\n        (3)"}, {"title": "3.4 Uncertainty Estimation", "content": "The BGN framework is trained in a supervised learning approach to minimize the mean squared error deviations between model RUL estimates y and ground-truth yt as described below,\n\n\u03b2\nLMSE = 1/\u03b2 \u03a3\u1d57 (y\u1d57 \u2212 y\u209a)\u00b2\n     (4)\nwhere \u1e9e = Ctrain Ttrain denotes total time steps in the training set. We present the BGN framework with predictive uncertainty estimation for short, BGN-UE that computes the mean(\u00b5\u03c6 (x)) and the variance(03 (x)) of the Gaussian distribution and regards RUL estimate as a sample from this distribution. We predict the outputs of the BGN-UE framework, \u03bc\u03c6 (x), \u03c3\u00b2 (x) as given by,\n\n\u03bc\u03c6 (x), \u03c3\u03c6\u00b2 (x) = f\u03b8 ({(ho b\u2081) | i \u2208 V}\n      (5)\nwhere & represents trainable parameters of the BGN-UE framework and fe is modeled by the stack of fully-connected layers. The RUL prediction, \u0177 ~ \u039d (\u03bc\u03c6 (x), 3(x)) at the tth time point is the maximum likelihood estimate(MLE) of the predicted Gaussian distribution, i.e., its mean at time point t, i.e., \u0177 = \u03bc\u03c6 (x). The BGN-UE framework optimizes the negative Gaussian log likelihood[55] of the battery parameters observations based on its predicted mean (\u00b5(x)) and variance ((x)) as described by,\n\nN(y\u1d57; \u03bc\u03c6 (x), \u03c3\u03c6(x)) = 1/(\u03c3\u03c6(x) \u221a2\u03c0) * e^(-(y\u1d57 - \u03bc\u03c6 (x))\u00b2/ 2\u03c3\u03c6(x)\u00b2)\n       (6)\nWe apply logarithm transformation on both sides of the equation,\n\nlog N(y\u1d57; \u03bc\u03c6 (x), \u03c3\u03c6(x)) = log 1/(\u03c3\u03c6(x) \u221a2\u03c0) + log e^(-(y\u1d57 - \u03bc\u03c6 (x))\u00b2/ 2\u03c3\u03c6(x)\u00b2)\n       (7)\n = \u2013 log(\u03c3\u03c6(x)) + log (1/\u221a2\u03c0) \u2013 (y\u1d57 \u2212 \u03bc\u03c6(x))/(2\u03c3\u03c6\u00b2(x))\n       (8)\nlog N(y\u1d57; \u03bc\u03c6 (x), \u03c3\u03c6(x)) = -log(\u03c3\u03c6) - (y\u1d57 \u2212 \u03bc\u03c6(x))/(2\u03c3\u03c6\u00b2(x)) + C\n        (9)\nWe drop the constant(C) and the Gaussian negative log likelihood loss(i.e., negative log gaussian probability density function(pdf)) for the full training dataset is described by,\n\nLGaussianNLLLoss = 1/\u03b2 \u03a3 [log(\u03c3\u03c6\u00b2(x)) + (y\u1d57 \u2212 \u03bc\u03c6(x))/(2\u03c3\u03c6\u00b2(x))]\n      (10)\nNote: The neural network(fo) predicts the mean(\u00b5\u00f8 (x)) and standard deviation(03 (x)) of a normal distribution modeling the target RUL, yt. In summary, the BGN framework minimizes LMSE(refer Equation 4) and the BGN-UE framework minimizes LGaussianNLLLoss(refer Equation 10) respectively."}, {"title": "4 Experiments and Results", "content": "We validate the proposed frameworks on two publicly available datasets: the NASA Randomized Battery Usage Dataset[61] and the UNIBO Powertools Dataset[78]. We utilize the predefined train/test datasets provided by the dataset curators for competitive benchmarking with the varied baseline methods. The battery datasets consist of the following parameters: voltage, current, charge"}, {"title": "4.2 Benchmarking algorithms", "content": "Table 1 reports the performance of the state-of-the-art methods compared to our proposed models on both datasets. The performance evaluation metric is RMSE. The list of baseline models includes several graph-learning techniques, Graph Neural Networks(GNNs, [25]), Graph Temporal Algo-rithms(GTA, [60]), and sequence models such as Recurrent Neural Networks(RNN, [33, 15, 17]), Transformer networks[74], etc. The results show that our proposed models attains significant gains w.r.t to the lower value of the evaluation metric on all datasets. The experimental results support the rationale of our framework to learn the time-varying dependencies and dynamics of the LiBs with a GNN backbone to provide better RUL estimates compared to the baseline methods. We brief the baseline algorithms and report additional results in the appendix."}, {"title": "4.3 Ablation Studies", "content": "We perform ablation studies to provide insights into the relative contribution of the Dynamic Graph Inference(DGI) module to the improved overall performance of the BGN model. We model the graph structure learning module with simple algorithmic approaches to design several variants of our proposed framework. We investigate the variant model's performance compared to the BGN model on all the datasets. We refer to the BGN model for which the DGI module is modeled with the different operators as follows,\n\n\u2022 w/ fcg: BGN model with fully-connected graph(fcg). We disable the DGI module.\n\u2022 w/o bi: BGN model with DGI module, for which the sensor embeddings(b\u2081) are not taken into account to infer the dynamic graph. We learn the dynamic graph by leveraging the temporal features(x) and without the sensor embeddings(b). We compute the pairwise parameter, i.e., as described below,\n\n\u03c6\u1d62'\u2c7c = \u03c3(gfc(Wax\u1d57||W\u2096\u02e3||Wax\u1d57||W\u2096\u02e3)), \u2200j \u2208 {1, ..., n}, k \u2208 {0, 1}\n\n\u2022 w/o x: BGN model with static graph. We deactivate the DGI module. We learn the static graph by leveraging the sensor embeddings(b) and without the temporal features(x). We compute the pairwise parameter, i.e., as described below,\n\nAt,k_ij=9fc(b\u1d62||b\u2c7c), \u2200j \u2208 {1, ..., n}, k\u2208 {0, 1}"}, {"title": "4.4 Hyperparameters Study", "content": "We conduct hyperparameter tuning using the grid search technique to determine the optimal hyperpa-rameter values. We perform hyperparameters optimization to minimize the RMSE on the validation dataset. The training hyperparameter configuration was explored from the following ranges: node embedding dimension(for zz) \u2208 [16, 128], the hidden dimension(for h\u2081)\u2208 [16,128], the learning rate(lr) \u2208 [0.1, 0.0001]. The selected hyperparameter ranges are the same across NASA Randomized"}, {"title": "5 Conclusion", "content": "Precise battery remaining useful life estimation is crucial for battery-powered systems. We present the Battery GraphNets(BGN) framework that jointly learns the discrete dependency structures of the battery parameters and exploits the relational dependencies through neural message-passing schemes to provide accurate RUL estimates. We achieve SOTA performance on the open-source benchmark battery datasets on the RUL prediction task. The experimental results support the effectiveness of our approach for achieving better performance compared to the state-of-art methods."}, {"title": "Broader Impact", "content": "The current techniques in literature for prognostics or the remaining service life estimation of batteries are classified as follows, (a) physics-based, and (b) data-driven modeling techniques. The traditional physics-based modeling methods rely on the first-principles approach-based techniques to develop LiBs equivalent models. The physics-aware mechanistic models have inherent drawbacks of the high computational complexity of numerical simulations. The high-fidelity Reduced-Order"}, {"title": "A Appendix", "content": "Furthermore, we compare the performance of the graph self-supervised techniques, i.e., graph-contrastive learning algorithms(GCL, [87]) on the RUL estimation task w.r.t, to the BGN model trained in supervised settings. The GCL algorithms are classified based on three main components, (a) graph augmentation techniques, (b) contrasting architectures and modes, and (c) contrastive objectives. The graph-input-based GCL algorithms utilize the DGI module to learn the dynamic graph structure. Here, we briefly discuss the baseline GCL techniques,\n\n\u2022 BGRL[71]: a) utilizes edge removing and feature masking augmentation techniques, (b) the contrastive objective is the Bootstrapping Latent(BL) loss. (c) The contrastive modes are cross-scale(G2L, i.e., Global-Local mode) and same-scale contrasting(L2L, i.e., Local-Local mode), and (c) utilizes bootstrapped contrasting technique.\n\nNote: L2L and G2G contrasts the embedding pairs of the same node/graph with different views as positive pairs. G2L contrast the cross-scale, i.e., the node-graph embedding pairs at differing granularity as positive pairs.\n\n\u2022 GBT[8]: (a) utilizes edge removing and feature masking augmentation techniques, (b) the contrastive objective is the Barlow Twins (BT) loss. (c) the contrastive modes are Local-Local (L2L), Global-Global (G2G), and leverage the within-embedding contrasting technique.\n\n\u2022 GRACE[88]: a) utilizes edge removing and feature masking augmentation techniques, (b) the contrastive objective is the InfoNCE loss. (c) the contrastive mode is Local-Local(L2L), and (c) utilizes a dual-branch contrasting technique.\n\n\u2022 GraphCL[82]: a) utilizes subgraphs induced by random walks (RWS), node dropping, edge removing, and feature masking augmentation techniques, (b) the contrastive objective is the InfoNCE loss, the contrastive mode is Global-Global (G2G) and (c) utilizes a dual-branch contrasting technique.\n\n\u2022 InfoGraph[69]: (a) the contrastive objective is the Jensen-Shannon Divergence (JSD) loss. (b) The contrastive mode is Global-Local(G2L), and (c) utilizes a single-branch contrasting technique.\n\n\u2022 MVGRL[31]: a) utilizes the Personalized PageRank(PPR), Markov Diffusion Kernel (MDK) augmentation techniques, (b) the contrastive objective is the Jensen-Shannon Diver-gence(JSD) loss, the contrastive mode is Global-Local(G2L), and (c) utilizes dual-branch contrasting technique."}, {"title": "A.2 Baseline Algorithms", "content": "The graph-input-based baseline algorithms utilize the DGI module to learn the dynamic graph structure. Here, we discuss the short description of the baseline GCN algorithms.\n\n\u2022 AGNN[72] : The algorithm presents a neural-message passing schema to obtain the whole graph representation. It computes the cosine similarity of the node embeddings to obtain the propagation matrix that accounts for the dynamic and adaptive summary of the local-graph neighborhood. We utilize the propagation matrix to refine the node embeddings.\n\n\u2022 GCN[43] : The algorithm is a vanilla variant of the convolutional neural network operator to encode the discrete graphs to low-dimensional embeddings. It aggregates the normalized neural messages from the local neighborhood to update each node's fixed-length embeddings.\n\n\u2022 GCN2[12] : The algorithm extends the vanilla GCN operator. It integrates the initial residual connection and identity mapping strategy to overcome the over-smoothing issue.\n\n\u2022 APPNP[44] : The algorithm presents an improved neural-message passing schema based on the personalized PageRank technique. The model trained end-to-end by the gradient-based optimization technique utilizes the initial-connection strategy to refine the node embeddings for learning on graphs.\n\n\u2022 GatedGraphConv[48]: The algorithm presents node-update equations. It utilizes the GRU unit to refine the node embeddings by regulating the information flow aggregated from the local-graph neighbors and outputs the transformed node embedding sequences.\n\n\u2022 GraphConv[54] : The algorithm presents the k-dimensional GNNs that account for the fine-and coarse-grained structures to compute the graph representations at multiple scales for better inference of the graph-structured data.\n\n\u2022 ARMA[7] : The algorithm substitutes the conventional polynomial spectral filters with the auto-regressive moving average (ARMA) filters to perform convolution on the graphs to better capture the global graph structure.\n\n\u2022 DNA[24] : The algorithm presents the dynamic neighborhood aggregation (DNA) scheme to perform a node-adaptive aggregation of neighboring embeddings. This algorithmic approach offers a highly-dynamic receptive field to preserve the higher-order features of the dominant subgraph structures.\n\n\u2022 GAT[75] : The algorithm presents the local-graph attention strategy to refine the node-level representations by weighing the local-graph neighbors of importance. Stacking multiple-layers aggregates high-order information to compute the expressive node embeddings.\n\n\u2022 GCChenConv[20] : The algorithm utilizes the fast localized spectral filtering from spectral graph theory to design low-pass convolutional filters on graphs and coupled with a graph-coarsening and pooling operation for higher filter resolution.\n\n\u2022 GraphUNet[28] : The algorithm is an encoder-decoder architecture to operate on arbitrary-sized graphs. It presents the differentiable local-graph pooling and unpooling operations for"}, {"title": "A.3 Additional Experiments", "content": "The objective of the data augmentation is two-fold. The first objective is to generate a synthetic-battery dataset while conserving probability distributions and spatio-temporal dynamics of the real-battery dataset. In this work context, the real-battery dataset is the battery parameters observations. The synthetic-battery dataset generation potentially serves as virtual simulations to capture the battery performance, an alternative research paradigm to first-principles-based non-linear-mechanistic models and numerical solutions. The second objective is to demonstrate the lower prediction error of the baseline prediction model trained jointly with the real-battery training dataset and the synthetic-battery dataset on the real-holdout set compared to the variant of the baseline prediction model trained with the real-battery training dataset to estimate RUL. We demonstrate the graph deep learning-based Variational Auto-Encoders (VAEs, [42]) efficacy on synthetic dataset generation and its utility in the"}, {"title": "Framework:", "content": "The VAE-based generative model consists of (a) inference neural network g(x) and (b) generative neural network go(z). The encoding distribution of the probabilistic encoder g(x) is given as p(z|x) and approximated as q(z|x). The probabilistic encoder g\u00f8(x) computes latent variables z given observed data x by approximating the true posterior distribution p(zx) as q(zx). The decoding distribution of the probabilistic decoder ge(z) is denoted by p(xz) and approximated as po(xz). The probabilistic decoder ge(z) maps latent variable z to x by approximating true likelihood p(xz) as po(xz). We sample the latent variables z from a prior distribution, i.e., a standard normal distribution. We optimize the parameters 4, 6 of the encoding network q$(z|x) and the decoding network po(x|z) by maximizing the variational lower bound of the probability of sampling real-battery dataset as described below,\n\nLVAE(0, 4) = - log p(x) + DKL(q4zx)||pz|x))\n\n\n= -Ez~q(x) log po (x2) + DKL(q\u2084(zx)||p(z))\n\n\n0*, * = arg min LVAE\n\n\n\nThe VAE model (a) maximizes the (log-)likelihood of generating a real-battery dataset, i.e., log p(x) when learning the true-probability distribution. (b) minimizes the difference between the estimated posterior, i.e., q$(z|x) and real p(z|x). (c) minimizes the reconstruction loss, i.e., po(x2), which incentivizes the decoder network to effectively transform the latent variables z to x. (d) minimizes the KL divergence between real prior p(z) and estimated posterior distribution, q(z|x)."}, {"title": "Encoder:", "content": "We model the probabilistic encoder with the BGN model. The input to the encoder is the real-battery data x. The DGI module learns the adjacency matrix At at time-point t. The grapher module computes the node-level representations h. We optimize the node embeddings bi along with the other training parameters of the VAE. The pooling operator performs the element-wise product of hand bi. It then performs mean average pooling to obtain graph-level representation. We linearly transform the entire graph representation to compute the mean and variance of the distribution, \u03bc\u03b5, and o, respectively. We obtain the latent variable z through the reparameterization trick as described below,\n\nz~qzx, At) = N(z; \u03bc, (\u03c3)2\u0399)\n          (11)\nz = \u03bc + \u03c3 \u0454, where \u0454 ~ N(0, I)\n (12)"}, {"title": "Decoder:", "content": "We model the probabilistic decoder with the BGN neural network and have different trainable parameters compared to the encoder. The input to the decoder is the latent variable z. The DGI module reconstructs the adjacency matrix, At at time-point t. The grapher module learns the reconstructed node-level representations h. The pooling operator computes the element-wise product of h, b, and outputs the reconstructed real-battery dataset, x). We additionally reduce the binary cross entropy loss between the real At and reconstructed \u0100t. After training the VAE model on x, the synthetic-battery data \u017e is determined independently by sampling sequences using the generative decoder."}, {"title": "RUL Prediction:", "content": "We demonstrate the benefits of the synthetic-battery dataset. We perform experiments on the NASA Randomized Battery Usage Dataset[61] and the UNIBO Powertools Dataset[78]. We train the VAE algorithm on the real-battery training dataset. We utilized the validation dataset for hyperparameter tuning and tuned model selection. We sampled the synthetic data from the VAE framework. We select the BGN framework as a baseline model trained by the real-battery training dataset for RUL prediction. The BGN* denotes the baseline model variant trained jointly with the real-battery training and the sampled synthetic-battery dataset. We evaluate the performance of both models on the real-battery holdout dataset. We report in Table 6 a 60.65% & 46.47% drop in prediction error(RMSE) of BGN* model performance on NASA Randomized Battery [61] and UNIBO [78] holdout datasets, respectively. The synthetic-battery dataset has learned key-dominant patterns across the real-battery training dataset and is generalized better. It leads to better performance of the BGN* model on the real-battery holdout set compared to the BGN baseline model."}, {"title": "A.4 Missing Battery Data Imputation for Battery Life Estimation in Battery Digital Twins", "content": "The objective of missing data imputation is two-fold. The first objective is to impute missing data of battery parameters. The second objective is to demonstrate the utility of the imputed data. The algorithmic framework is driven by the two-player, non-cooperative, zero-sum adversarial game based on game theory and a minimax optimization approach. The framework with an adversarially trained generator neural network presents an alternative paradigm to numerical modeling and simulations of first principles based on imputation techniques. We leverage artificial intelligence systems to demonstrate the random missing data imputation-utility efficacy tradeoff for the downstream task of RUL prediction on real-world open-source NASA Randomized Battery Usage Dataset[61] and the UNIBO Powertools Dataset[78]."}, {"title": "Framework:", "content": "The Wasserstein Generative Adversarial Network(WGAN, [2]) is trained to impute the unobserved data of x. Let us assume that denotes the imputed dataset. The generator network of the WGAN framework learns a density p(x) of that best approximates the probability density function, p(x) of x. The mathematical description is as follows,\n\nmin W(p(x)|| f(x))\n      (13)\nW denotes the Wasserstein distance of order-1. In general, x had obtained by determining the Nash equilibrium of the competing game-setting of two distinct machine players, Gn and Dn. Gn, Dn denotes the generative and the discriminator neural networks. The game minimizes the variance of the Earth-Mover distance(Wasserstein-1 metric, deduced from the Kantorovich-Rubinstein duality) between two multidimensional datasets(original & imputed) based on probability distributions through the minimax(bi-level) optimization schema described by,\n\nmin max (Exp(x) [x] - Ez~p(z) [Dn(Gn(z))]).\n      (14)\nGn Dn\nThe loss function for Dn is described below,\n\nExp(x) [x] - Ez~p(z) [Dn (Gn(z))]\n         (15)\n\n-Ezp(z) [Dn (Gn(z))]\n\n\nThe loss function for Gn as follows,\n\nEzp(z) [Dn (Gn(z))]\n(16)\nz\u2208 RTC is sampled from a uniform distribution. We model the Gn and Dn of the WGAN framework with the BGN model with different trainable parameters. The input to the Gn is described below,\n\n= mx + (1-m) z\n      (17)\n\nm\u2208 {0,1} denotes the random mask variable. The DGI module of Gn computes the sparse adjacency matrix \u00c2(t). The grapher module determines the node-level representations ht). We learn the node embeddings b\u2081 along with the other training parameters of the GAN. We compute the element-wise product of ht and bi to obtain the imputed data . The imputed data is obtained by,\n\n=(t)=mx+(1-m) z\n(t)\nThe discriminator net, Dn : \u00c2\u2192 m takes as input the imputed data and outputs an estimated mask matrix m. The cross-entropy loss in binary classification is described by,\n\n-(mlog m+ (1 \u2212 m) log(1 \u2013 m)) | m = 0\n\nThe generative neural network(Gn) minimizes the following loss function,\n\nm(x-x)2\nIt minimizes the imputation loss. We also minimize the Wasserstein distance between the estimates of two probability distributions p(x) and p(x). In summary, Gn and Dn are trained adversarially by deceptive input to impute the missing data."}]}