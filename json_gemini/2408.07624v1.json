{"title": "Battery GraphNets : Relational Learning for\nLithium-ion Batteries(LiBs) Life Estimation", "authors": ["Sakhinana Sagar Srinivas", "Rajat Kumar Sarkar", "Venkataramana Runkana"], "abstract": "Battery life estimation is critical for optimizing battery performance and guaran-\nteeing minimal degradation for better efficiency and reliability of battery-powered\nsystems. The existing methods to predict the Remaining Useful Life(RUL) of\nLithium-ion Batteries (LiBs) neglect the relational dependencies of the battery\nparameters to model the nonlinear degradation trajectories. We present the Battery\nGraphNets framework that jointly learns to incorporate a discrete dependency\ngraph structure between battery parameters to capture the complex interactions and\nthe graph-learning algorithm to model the intrinsic battery degradation for RUL\nprognosis. The proposed method outperforms several popular methods by a signifi-\ncant margin on publicly available battery datasets and achieves SOTA performance.\nWe report the ablation studies to support the efficacy of our approach.", "sections": [{"title": "Introduction", "content": "High-energy density LiBs[13, 52, 89] are driving the global revolution of battery-powered solutions\nacross the whole spectrum, from portable electronics to large-scale electric mobility and energy\nstorage systems. Recent advances in battery materials and technologies[73, 22] have enabled low\nbattery self-discharge, high power density, and lightweight, fast-charging LiBs. Despite the progress,\nbattery capacity deteriorates[57, 40, 86, 53] due to repetitive charging and discharging, leading to\nlow performance, low storage characteristics, and shorter life of the LiBs. The development of\naccurate battery RUL[81, 30, 80] estimation algorithms has gained attention in the advancement of\nbattery management systems(BMS, [50, 67]) as the RUL translates to reliability, available range, and\nperformance of battery-powered systems. The existing battery RUL prediction methods[62, 19, 56, 1]\nare limited in modeling the non-linear behavior of discharge characteristics of aging LiBs for accurate\nRUL prognosis. We begin with the hypothesis that LiBs are stochastic dynamical systems. The\ncomplex interactions between multiple degradation mechanisms, characterized by different physical\nand chemical processes, are responsible for battery degradation. The collective behavior of the\ninterconnected and individual degradation mechanisms, as a whole, give rise to the non-linear\ndynamics of the complex system. The battery parameters capture the measurable effects of tractable\nbattery degradation. There is a need and necessity to learn the system's elusive, relational structural-\ndynamic dependencies of the interdependent battery parameters while simultaneously learning the\nnon-linear degradation dynamics of LiBs. The literature on relational learning or reasoning tasks is\noverwhelmingly focused on dynamical learning of interacting systems dynamics using the following\napproaches. (a) The implicit interaction models[68, 29, 63, 77, 34] utilize a fully-connected graph to\nmodel the relational structure. Redundant edges and learning noise-incorporated representations are\nthe drawbacks of implicit interaction models. (b) The explicit interaction models are categorized into\nthe following methods. (1) continuous relaxation methods[37, 76, 14, 83] that utilize graph similarity\nmetric learning to obtain the real-valued weighted adjacency matrix, and (2) probabilistic sampling\napproaches [26, 85, 38] model the edge probability distribution over graphs. The relaxation methods"}, {"title": "Problem statement", "content": "Consider an arbitrary training set of graph-structured battery data consisting of graph-label pairs\n{(Gt, yt)}. The goal is to learn a model(f : Gt \u2192 yt) trained in inductive supervised learning to\npredict the label yt at any time step t associated with the input graph Gt and estimate uncertainties.\nIn short, the input to the model is the multivariate battery parameters measurements viewed as the\nsequence of graphs Gt and predicts the RUL estimates yp."}, {"title": "Proposed Approach", "content": "The proposed framework consists of the following modules. (a) The dynamic graph inference\nmodule, for brevity, DGI. It is a probabilistic model that learns the time-evolving optimal graph\nstructure for the structured spatio-temporal representation of the battery data. (b) The grapher module\npresents a node-level graph encoder that computes the fixed-size representation for each node by\njointly modeling the latent inter-parameters and the long-range, intra-parameters dependencies in\nthe graph-structured battery data. (c) A graph readout module for global pooling of the node-level\nrepresentations to capture the global semantic structure in the fixed-length graph-level vector. We\nutilize a linear projection to transform the entire-graph fixed-size vector and apply a non-linear\nactivation function to predict the high-quality RUL estimates. In summary, the proposed framework\nsimultaneously learns the spatio-temporal dependencies among multiple battery parameters over\ntime with a GNN backbone operating on a discrete latent graph in the node-level representations. It\nthen performs inference over the node-level latent variables to predict RUL."}, {"title": "Dynamic Graph Inference(DGI)", "content": "The battery parameters do not have an explicit relational graph structure(unknown or partially known)\nunderlying the data. We define a dynamic graph, Gt at a time point, t as a tuple(V, Et, {x}iev).\nV denotes a set of nodes. We represent each battery parameter as a node i \u2208 V. Every node is\nassociated with a feature vector x \u2208 RC\u00b7T. Here, C denotes the number of charge-discharge cycles\nof the LiBs, and T is the number of time steps in each charge cycle. Et is the set of (directed)\nedges where each edge eji \u2208 Et models the pairwise relationship between the neighboring nodes\nj, i. The dynamic graphs Gt have a feature matrix, Xt that changes dynamically over time, an\nadjacency matrix, At, and an edge-set, Et that may vary over time, whereas the node-set V remains\nunchanged. The nodes of the dynamic graph are associated with trainable embeddings bi \u2208 Rd,\n1 \u2264 i \u2264 n, which are d-dimensional continuous vector representations. DGI module learns the sparse\nregularized dynamic-graph structure from the explicit-battery data, x integrated with the implicit\nnode embeddings, bi. We compute the pairwise parameters as given by,\n=(gfc (b\u2082 || Wx,)||gf\u00a9 (bi || Wx,)));, Vi, j \u2208 {1, ..., n}, k \u2208 {0,1}\nWhere Ok\u2208 Rn2x2 denotes probability matrix, Wdx(CT) is trainable weight matrix. gfe is a stack\nof fully-connected layers. o is a logistic sigmoid function. n, i.e., |V| denotes number of battery\nparameters. || denotes vector concatenation. O encodes relation between a pair of nodes (i, j) to a\nscalar \u2208 [0, 1]. 0 represents probability of a directed edge from node i to its potential neighbor j at\na time point t, and 0 represents contrariwise probability. We sample the time-specific adjacency\nmatrix[66], i.e., A; \u2208 Rn\u00d7n to determine the edges to preserve and drop the redundant edges via\nthe element-wise, differentiable method, using the Gumbel-softmax trick[36, 51] as described by,\nAt,k = exp((+)/) / exp((+)/) (1)\nwhere g; ~ Gumbel(0, 1) = log(- log(U(0, 1)) where U is uniform distribution. y, denotes the\ntemperature parameter with 0.05. In summary, the DGI module learns the evolving irregular graph\nstructure, which captures the time-dependent interdependence relations of the battery parameters.\nBy viewing the battery data as node-attributed discrete-time dynamic graphs, we explore the GNN\nbackbone to encode the complex discrete graph-structured data by summarizing the entire graph's\nstructural characteristics to compute its optimal representations to facilitate the RUL prediction task."}, {"title": "Grapher", "content": "The grapher module consists of two blocks (1) the GNN block and the (2) the RNN block. The GNN\nblock consists of a single-layer GNN \u2192 Batch Norm \u2192 Dropout sequence. We apply batchnorm[35]\nand dropout[5] for regularization. The GNN layer-wise operator involves two basic node-level\ncomputations at every layer. (1) AGGREGATE operation: aggregates neural messages(mji) from\nlocal-graph neighbors j \u2208 N\u2081, where mji denotes the neural message dispatched from source node j\nto sink node i via edge eji. It captures the lateral interaction. (2) UPDATE operation: transforms\neach node representation by combining its fixed-length representation obtained in the previous layer\nand the aggregated neural messages. We utilize the vanilla variant of GCN[43] to model the GNN\nlayer, which couples the aggregate and update operations to refine the node-level representations.\nLet's consider an L-layer GCN, the mathematical formulation of the lth layer (1 < 1 < L) described\nbelow,\nh,t) = ReLU (W() (w\u03a3 -1,t)) (2)\nwhere hol,t) \u2208 Rd, i , i \u2208 V denotes node representation in the [th layer with h(0,t) ho,t) = Wsx. Wg \u2208\nRdxd is weight matrix. di represents the degree of node i. We stack two GNN blocks and concatenate\nthe output node-level representations from each block to compute h\u2081 \u2208 R2d. The RNN block consists\nof double-stacked GRU units[16, 17] that operate on hi \u2208 R2d for transforming the node-level\nrepresentations to compute hi \u2208 Rd. It regulates the information flow through the gating mechanism\nto better capture the long-range spatio-temporal dependencies."}, {"title": "Graph Readout(GRo)", "content": "We utilize the global average pooling operation to aggregate node-level representations(h) computed\nby the grapher module and obtain a graph-level representation hg, as given by\nh = READOUT ({(ho b\u2081) | i \u2208 V} (3)"}, {"title": "Uncertainty Estimation", "content": "where denotes element-wise multiplication. We utilize a linear layer and apply a sigmoid activation\nfunction to transform hand predict the RUL(y) at a time point, t.\nThe BGN framework is trained in a supervised learning approach to minimize the mean squared error\ndeviations between model RUL estimates y and ground-truth yt as described below,\nLMSE = \u03a3(y-y)2 (4)\nwhere \u1e9e = Ctrain Ttrain denotes total time steps in the training set. We present the BGN framework\nwith predictive uncertainty estimation for short, BGN-UE that computes the mean(\u00b5\u03c6 (x)) and\nthe variance(03 (x)) of the Gaussian distribution and regards RUL estimate as a sample from this\ndistribution. We predict the outputs of the BGN-UE framework, \u03bc\u03c6 (x), \u03c3\u00b2 (x) as given by,\n\u03bc\u03c6 (x), (x) = fo {(hb\u2081) | i \u2208 \u03bd V} (5)\nwhere & represents trainable parameters of the BGN-UE framework and fe is modeled by the stack of\nfully-connected layers. The RUL prediction, \u0177 ~ \u039d (\u03bc\u03c6 (x), 3(x)) at the tth time point is the\nmaximum likelihood estimate(MLE) of the predicted Gaussian distribution, i.e., its mean at time point\nt, i.e., \u0177 = \u03bc\u03c6 (x). The BGN-UE framework optimizes the negative Gaussian log likelihood[55] of\nthe battery parameters observations based on its predicted mean (\u00b5(x)) and variance ((x)) as\ndescribed by,\nN(y; \u03bc\u03c6 (x), \u03c3\u03c6(x)) = e (6)\nWe apply logarithm transformation on both sides of the equation,\nlog N(y; \u03bc\u03c6 (x), \u03c3\u03c6(x)) = log + log e\n= \u2013 log \u03c3\u03c6 \u03c3\u03c6(x) + C \u2212 (9)\nWe drop the constant(C) and the Gaussian negative log likelihood loss(i.e., negative log gaussian\nprobability density function(pdf)) for the full training dataset is described by,\nLGaussianNLLLoss\u03a3log \u03c3\u03c6(x)\u00b2 + (10)\nNote: The neural network(fo) predicts the mean(\u00b5\u00f8 (x)) and standard deviation(03 (x)) of a normal\ndistribution modeling the target RUL, yt. In summary, the BGN framework minimizes LMSE(refer\nEquation 4) and the BGN-UE framework minimizes LGaussianNLLLoss(refer Equation 10) respectively."}, {"title": "Experiments and Results", "content": "We validate the proposed frameworks on two publicly available datasets: the NASA Randomized\nBattery Usage Dataset[61] and the UNIBO Powertools Dataset[78]. We utilize the predefined\ntrain/test datasets provided by the dataset curators for competitive benchmarking with the varied\nbaseline methods. The battery datasets consist of the following parameters: voltage, current, charge"}, {"title": "Benchmarking algorithms", "content": "reports the performance of the state-of-the-art methods compared to our proposed models\non both datasets. The performance evaluation metric is RMSE. The list of baseline models includes\nseveral graph-learning techniques, Graph Neural Networks(GNNs, [25]), Graph Temporal Algo-\nrithms(GTA, [60]), and sequence models such as Recurrent Neural Networks(RNN, [33, 15, 17]),\nTransformer networks[74], etc. The results show that our proposed models attains significant gains\nw.r.t to the lower value of the evaluation metric on all datasets. The experimental results support the\nrationale of our framework to learn the time-varying dependencies and dynamics of the LiBs with\na GNN backbone to provide better RUL estimates compared to the baseline methods. We brief the\nbaseline algorithms and report additional results in the appendix."}, {"title": "Ablation Studies", "content": "We perform ablation studies to provide insights into the relative contribution of the Dynamic Graph\nInference(DGI) module to the improved overall performance of the BGN model. We model the\ngraph structure learning module with simple algorithmic approaches to design several variants of\nour proposed framework. We investigate the variant model's performance compared to the BGN\nmodel on all the datasets. We refer to the BGN model for which the DGI module is modeled with the\ndifferent operators as follows,\n\u2022 w/ fcg: BGN model with fully-connected graph(fcg). We disable the DGI module.\n\u2022 w/o bi: BGN model with DGI module, for which the sensor embeddings(b\u2081) are not taken\ninto account to infer the dynamic graph. We learn the dynamic graph by leveraging the\ntemporal features(x) and without the sensor embeddings(b). We compute the pairwise\nparameter, i.e., as described below,\nAt,k = gf(gfc (Wx(wx)) || Wx))), vj \u2208 {1,..., n}, k \u2208 {0,1}\n\u2022 w/o x: BGN model with static graph. We deactivate the DGI module. We learn the static\ngraph by leveraging the sensor embeddings(b) and without the temporal features(x). We\ncompute the pairwise parameter, i.e., as described below,\n=(gfc(billb\u2083)),,\u2200j \u2208 {1, ..., n}, k\u2208 {0,1}\nthe results of ablation studies."}, {"title": "Hyperparameters Study", "content": "We conduct hyperparameter tuning using the grid search technique to determine the optimal hyperpa-\nrameter values. We perform hyperparameters optimization to minimize the RMSE on the validation\ndataset. The training hyperparameter configuration was explored from the following ranges: node\nembedding dimension(for zz) \u2208 [16, 128], the hidden dimension(for h\u2081)\u2208 [16,128], the learning\nrate(lr) \u2208 [0.1, 0.0001]. The selected hyperparameter ranges are the same across NASA Randomized"}, {"title": "Conclusion", "content": "Precise battery remaining useful life estimation is crucial for battery-powered systems. We present\nthe Battery GraphNets(BGN) framework that jointly learns the discrete dependency structures of the\nbattery parameters and exploits the relational dependencies through neural message-passing schemes\nto provide accurate RUL estimates. We achieve SOTA performance on the open-source benchmark\nbattery datasets on the RUL prediction task. The experimental results support the effectiveness of our\napproach for achieving better performance compared to the state-of-art methods."}, {"title": "Broader Impact", "content": "The current techniques in literature for prognostics or the remaining service life estimation of\nbatteries are classified as follows, (a) physics-based, and (b) data-driven modeling techniques. The\ntraditional physics-based modeling methods rely on the first-principles approach-based techniques to\ndevelop LiBs equivalent models. The physics-aware mechanistic models have inherent drawbacks\nof the high computational complexity of numerical simulations. The high-fidelity Reduced-Order"}, {"title": "Appendix", "content": "Furthermore, we compare the performance of the graph self-supervised techniques, i.e., graph-\ncontrastive learning algorithms(GCL, [87]) on the RUL estimation task w.r.t, to the BGN model\ntrained in supervised settings. The GCL algorithms are classified based on three main components,\n(a) graph augmentation techniques, (b) contrasting architectures and modes, and (c) contrastive\nobjectives. The graph-input-based GCL algorithms utilize the DGI module to learn the dynamic\ngraph structure. Here, we briefly discuss the baseline GCL techniques,\n\u2022 BGRL[71]: a) utilizes edge removing and feature masking augmentation techniques, (b) the\ncontrastive objective is the Bootstrapping Latent(BL) loss. (c) The contrastive modes are\ncross-scale(G2L, i.e., Global-Local mode) and same-scale contrasting(L2L, i.e., Local-Local\nmode), and (c) utilizes bootstrapped contrasting technique.\nNote: L2L and G2G contrasts the embedding pairs of the same node/graph with different\nviews as positive pairs. G2L contrast the cross-scale, i.e., the node-graph embedding pairs\nat differing granularity as positive pairs.\n\u2022 GBT[8]: (a) utilizes edge removing and feature masking augmentation techniques, (b)\nthe contrastive objective is the Barlow Twins (BT) loss. (c) the contrastive modes are\nLocal-Local (L2L), Global-Global (G2G), and leverage the within-embedding contrasting\ntechnique.\n\u2022 GRACE[88]: a) utilizes edge removing and feature masking augmentation techniques, (b)\nthe contrastive objective is the InfoNCE loss. (c) the contrastive mode is Local-Local(L2L),\nand (c) utilizes a dual-branch contrasting technique.\n\u2022 GraphCL[82]: a) utilizes subgraphs induced by random walks (RWS), node dropping, edge\nremoving, and feature masking augmentation techniques, (b) the contrastive objective is the\nInfoNCE loss, the contrastive mode is Global-Global (G2G) and (c) utilizes a dual-branch\ncontrasting technique.\n\u2022 InfoGraph[69]: (a) the contrastive objective is the Jensen-Shannon Divergence (JSD) loss.\n(b) The contrastive mode is Global-Local(G2L), and (c) utilizes a single-branch contrasting\ntechnique.\n\u2022 MVGRL[31]: a) utilizes the Personalized PageRank(PPR), Markov Diffusion Kernel (MDK)\naugmentation techniques, (b) the contrastive objective is the Jensen-Shannon Diver-\ngence(JSD) loss, the contrastive mode is Global-Local(G2L), and (c) utilizes dual-branch\ncontrasting technique.\nWe demonstrate the graph deep learning-based\nVariational Auto-Encoders (VAEs, [42]) efficacy on synthetic dataset generation and its utility in the"}, {"title": "Data Augmentation for Battery Life Estimation in Battery Digital Twins", "content": "The objective of the data augmentation is two-fold. The first objective is to generate a synthetic-battery\ndataset while conserving probability distributions and spatio-temporal dynamics of the real-battery\ndataset. In this work context, the real-battery dataset is the battery parameters observations. The\nsynthetic-battery dataset generation potentially serves as virtual simulations to capture the battery\nperformance, an alternative research paradigm to first-principles-based non-linear-mechanistic models\nand numerical solutions. The second objective is to demonstrate the lower prediction error of the\nbaseline prediction model trained jointly with the real-battery training dataset and the synthetic-battery\ndataset on the real-holdout set compared to the variant of the baseline prediction model trained with"}, {"title": "Missing Battery Data Imputation for Battery Life Estimation in Battery Digital Twins", "content": "The objective of missing data imputation is two-fold. The first objective is to impute missing data\nof battery parameters. The second objective is to demonstrate the utility of the imputed data. The\nalgorithmic framework is driven by the two-player, non-cooperative, zero-sum adversarial game based\non game theory and a minimax optimization approach. The framework with an adversarially trained\ngenerator neural network presents an alternative paradigm to numerical modeling and simulations\nof first principles based on imputation techniques. We leverage artificial intelligence systems to\ndemonstrate the random missing data imputation-utility efficacy tradeoff for the downstream task of\nRUL prediction on real-world open-source NASA Randomized Battery Usage Dataset[61] and the\nUNIBO Powertools Dataset[78].\nFramework: The Wasserstein Generative Adversarial Network(WGAN, [2]) is trained to impute the\nunobserved data of x. Let us assume that denotes the imputed dataset. The generator network of\nthe WGAN framework learns a density p(x) of that best approximates the probability density\nfunction, p(x) of x. The mathematical description is as follows,\nmin W(p(x)|| f(x)) (13)\nW denotes the Wasserstein distance of order-1. In general, x had obtained by determining the Nash\nequilibrium of the competing game-setting of two distinct machine players, Gn and Dn. Gn, Dn\ndenotes the generative and the discriminator neural networks. The game minimizes the variance of\nthe Earth-Mover distance(Wasserstein-1 metric, deduced from the Kantorovich-Rubinstein duality)\nbetween two multidimensional datasets(original & imputed) based on probability distributions through\nthe minimax(bi-level) optimization schema described by,\nmax (Exp(x) [x] - Ez~p(z) [Dn(Gn(z))]) (14)\nThe loss function for Gn as follows,\n(15)\nmin max (Exp(x) [x] - Ez~p(z) [Dn(Gn(z))]).\n Gn Dn\nThe loss function for Gn as follows,\n(16)\nz\u2208 RTC is sampled from a uniform distribution. We model the Gn and Dn of the WGAN framework\nwith the BGN model with different trainable parameters. The input to the Gn is described below,\n). The imputed data is obtained by,"}]}