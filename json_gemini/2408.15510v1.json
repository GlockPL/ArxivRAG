{"title": "Measuring the Reliability of Causal Probing Methods:\nTradeoffs, Limitations, and the Plight of Nullifying Interventions", "authors": ["Marc Canby", "Adam Davies", "Chirag Rastogi", "Julia Hockenmaier"], "abstract": "Causal probing is an approach to interpreting\nfoundation models, such as large language mod-\nels, by training probes to recognize latent prop-\nerties of interest from embeddings, intervening\non probes to modify this representation, and\nanalyzing the resulting changes in the model's\nbehavior. While some recent works have cast\ndoubt on the theoretical basis of several lead-\ning causal probing intervention methods, it has\nbeen unclear how to systematically and empir-\nically evaluate their effectiveness in practice.\nTo address this problem, we propose a general\nempirical analysis framework to evaluate the\nreliability of causal probing interventions, for-\nmally defining and quantifying two key causal\nprobing desiderata: completeness (fully trans-\nforming the representation of the target prop-\nerty) and selectivity (minimally impacting other\nproperties). Our formalism allows us to make\nthe first direct comparisons between different\nfamilies of causal probing methods (e.g., linear\nvs. nonlinear or counterfactual vs. nullifying\ninterventions). We conduct extensive experi-\nments across several leading methods, finding\nthat (1) there is an inherent tradeoff between\nthese criteria, and no method is able to consis-\ntently satisfy both at once; and (2) across the\nboard, nullifying interventions are always far\nless complete than counterfactual interventions,\nindicating that nullifying methods may not be\nan effective approach to causal probing.", "sections": [{"title": "1 Introduction", "content": "What latent properties do large language models\n(LLMs) learn to represent, and how are these rep-\nresentations leveraged by models in performing ar-\nbitrary downstream tasks? Causal probing aims to\nanswer this question by training probing classifiers\nto predict some property of interest (e.g., parts-of-\nspeech) from the model's embedding representa-\ntions, using these probes to perform interventions\nthat modify the model's representation of the prop-\nerty, then feeding the resulting embeddings back\ninto the LLM to observe how such interventions\nimpacts its behavior (Geiger et al., 2020; Ravfogel\net al., 2020; Elazar et al., 2021; Tucker et al., 2021;\nLasri et al., 2022; Davies et al., 2023; Zou et al.,\n2023). If the model is indeed using the property,\nthen we expect its behavior to change; otherwise,\nthe intervention should have little to no effect. The\npotential is that, in principle, it should be possi-\nble to use causal probing to interpret how arbitrary\nproperties are used by LLMs (or any other neu-\nral network) in the context of specific tasks, thus\nexplaining their behavior in terms of interpretable\nlatent properties.\nIn order to confidently attribute any resulting\nchanges in an LLM's behavior to its use of the\ntarget property, such analyses require that interven-\ntions fully and precisely carry out the desired trans-\nformation (Davies and Khakzar, 2024). However,\nprior works have indicated that interventions may\noften have a greater impact on the representation\nof non-targeted properties than the target property\n(Kumar et al., 2022), or that information about the\ntarget property that should have been completely\nremoved from embeddings may still be recover-\nable by models (Elazar et al., 2021; Ravfogel et al.,\n2022b), making it impossible to draw strong con-\nclusions on the basis of causal probing analyses. So\nfar, there has been no generally accepted approach\nto empirically measuring such limitations or evalu-\nating the reliability of causal probing interventions.\nOur main goal in this work is to address such\nconcerns and work toward a systematic understand-\ning of the effectiveness and limitations of current\ncausal probing methodologies. Specifically, we pro-\npose an empirical analysis framework to evaluate\nthe reliability of leading causal probing methodolo-\ngies according to two key desiderata:\n1. Completeness: interventions should fully\ntransform the representation of targeted prop-\nerties.\n2. Selectivity: interventions should not impact"}, {"title": "2 Background and Related Work", "content": "Structural Probing The goal of structural prob-\ning (Hewitt and Manning, 2019; Maudslay et al.,\n2020; Belinkov et al., 2020) is to analyze which\nproperties (e.g., part-of-speech, sentiment labels,\netc.) are represented by a deep learning model\n(e.g., LLM) by training probing classifiers to pre-\ndict these properties from latent embedding repre-\nsentations (Belinkov, 2022). That is, for deep learn-\ning model (e.g., LLM) M, input token sequence\nx = (x1, ..., XN), and embeddings h' = M\u2081(x) of\ninput x at layer l of M, suppose Z is a latent prop-\nerty of interest that takes a discrete value Z=z\nfor input x. The goal of structural probing is to\ntrain a classifier gz : M\u2081(x) \u2192 z to predict the\nvalue of Z from the embeddings h\u00b9. On the most\nstraightforward interpretation, if gz achieves high\naccuracy on the probe task, then the model is said to\nbe \"representing\" Z (Belinkov, 2022). One leading\ncriticism of such methodologies is that correlation\ndoes not imply causation \u2013 i.e., that simply because\na given property can be predicted from embedding\nrepresentations does not mean that the model is\nusing the property in any way (Hewitt and Liang,\n2019; Elazar et al., 2021; Belinkov, 2022; Davies\net al., 2023).\nCausal Probing A prominent response to this\nconcern has been causal probing, which uses\ntrained structural probes in order to intervene on\nmodels' representation of the probed property, ob-\nserving the impact of interventions on models' pre-\ndictions to analyze how the property is being used\n(Elazar et al., 2021; Tucker et al., 2021; Lasri et al.,\n2022; Davies et al., 2023). Specifically, causal\nprobing performs interventions do(Z) that mod-\nify M's representation of Z in the embeddings\nh\u00b9, producing h\u00b9, where interventions can either\nencode a counterfactual value Z = z' (denoted\ndo(Z = z') where z \u2260 z'), or remove the rep-\nresentation of Z entirely (denoted do(Z = 0)).\nFollowing the intervention, modified embeddings\nh\u00b9 are fed back into M beginning at layer l + 1 to\ncomplete the forward pass, yielding intervened pre-\ndictions PrM(x, do(Z)). Comparison with the\noriginal predictions Pr\u33a2(\u00b7|x) allows one to mea-\nsure the extent to which M uses its representation\nof Z in computing PrM(x).\nCausal Probing: Limitations However, prior\nworks have indicated that information about the\ntarget property that should have been completely\nremoved may still be recoverable by the model\n(Elazar et al., 2021; Ravfogel et al., 2022b, 2023),\nin which case interventions are not complete; or\nthat most of the impact of interventions may actu-\nally be the result of collateral damage to correlated,\nnon-targeted properties (Kumar et al., 2022), in\nwhich case interventions are not selective. How se-\nriously should we take such critiques? We observe\nseveral important shortcomings in each of these\nprior studies on the limitations of causal probing\ninterventions:\n1. These limitations have only been empirically\ndemonstrated for the specific task of removing\nall information about a target property from\nembeddings such that the model cannot be\nfine-tuned to use the property for downstream\ntasks (Kumar et al., 2022; Ravfogel et al.,\n2022b, 2023). But considering that the goal of\ncausal probing is to interpret the behavior of\nan existing pre-trained model, the question is\nnot whether models can be fine-tuned to use\nthe property; it is whether models already use"}, {"title": "3 Evaluating Causal Probing Reliability", "content": "Recall that our main goal in this work is to evaluate\nintervention reliability in terms of completeness\n(completely transforming M's representation of\nsome target property Zi) and selectivity (minimally\nimpacting M's representation of other properties\nZj \u2260 Zi). Given that we cannot directly inspect\nwhat value M encodes for any given property Zi,\nhow can we determine the extent to which inter-\nventions have fulfilled either criterion? In order\nto formally define these criteria, we introduce the\nnotion of oracle probes.\nOracle Probes We define an oracle probe o as\na structural probe that can perfectly determine\nthe distribution Pro(Z|h') over the possible val-\nues taken by property Z as encoded by represen-\ntations h\u00b9 = M\u2081(x). For example, if represen-\ntation h\u00b9 does not encode property Z at all, we\nexpect Pro(Z|h\u00b2) = U(Z), where U(Z) is the uni-\nform distribution over values of Z; or if h' en-\ncodes value Z = 2 with complete confidence, then\nPro(Z|h') = 1(Z = 2), where 1(\u00b7) is the indi-\ncator function. Thus, a sufficiently high-quality\noracle probe enables us to measure how well vari-\nous intervention methodologies perform the desired\nintervention. Naturally, a perfect oracle does not\nexist in practice, so any practical implementation\nmust approximate it (see Section 4.4).\nCompleteness If a counterfactual intervention\ndo(Z = z') is perfectly complete, then it should\nproduce hz that fully transforms h\u00b9 from en-\ncoding value Z = z to encoding counterfac-\ntual value Z = z' \u2260 z. Thus, after perform-\ning the intervention, oracle o should emit Pz =\nPro(Z = zh_z) = 1. For nullifying interven-\ntions do(Z = 0), a perfectly nullified represen-\ntation hz=0 should not encode Z at all: Pz =\nPro(Zhz=0) = U(Z).\nWe can use any distributional distance metric\n\u03b4(\u00b7,\u00b7) bounded by [0, 1] to determine how far the\nobserved distribution Pz = Pro (Zhz) is from the\n\"goal\" distribution P. Throughout this work, we\nuse total variation (TV) distance, but any other such\nmetric can be substituted in its place. Importantly,\ndefining completeness C(\u0125z) in this way allows\nus to directly compare counterfactual and nullify-\ning distributions: in both cases, 0 \u2264 C(2) \u2264 1,\nwhere attaining 1 means the intervention had its\nintended effect in transforming the encoding of Z.\nCounterfactual Interventions: For a counterfactual\nintervention, we measure completeness as:\nC(hz) = 1 \u2013 \u03b4(P, P) (1)"}, {"title": "4 Experimental Setting", "content": "We test our framework by carrying out an exten-\nsive range of experiments to analyze current causal\nprobing interventions in the context of BERT (De-\nvlin et al., 2019). We opt for BERT because it is\nvery well-studied in the context of language model\ninterpretability (Rogers et al., 2021), particularly in\ncausal probing (Ravfogel et al., 2020; Elazar et al.,\n2021; Ravfogel et al., 2021; Lasri et al., 2022; Rav-\nfogel et al., 2022b, 2023; Davies et al., 2023). We\nfurther motivate this decision in Section 7.\n4.2 Task: Subject-Verb Agreement\nTo test our evaluation framework experimentally,\nwe need a task with a clear causal structure. We\nstart with the simplest possible case, where labels\nare fully determined by the value of a single binary\ncausal variable Zc, with a second binary environ-\nmental variable Ze that influences inputs and may\nbe spuriously correlated with the label (meaning\nthat LLMs might still leverage it in performing\nthe task). By intervening on each of these vari-\nables, we can calculate how well the target variable\nis damaged (completeness) and how little the op-\nposite variable is damaged (selectivity). We also\nwant a task that BERT performs well, allowing us\nto assess the extent to which its performance is at-\ntributable to its representation of the causal variable\nZe versus the spuriously-correlated environmental\nvariable Ze.\nTo fulfill both criteria, we select the cloze\nprompting task of subject-verb agreement, which\nhas also been the subject of analysis in multiple\nprior causal probing works (Lasri et al., 2022; Rav-\nfogel et al., 2021). Each instance in this task takes\nthe form (xi, Yi) where x\u012b is a sentence such as \u201cthe\ngirl with the keys [MASK] the door,\u201d and the task\nof the LLM is to predict PrM(yi|x) > Pr\u043c(y|x)"}, {"title": "5 Experimental Results", "content": "We first verify that our MLP probes are able to pick\nup on the properties of interest. Table 2 reports the\naccuracy of each probe on the test set, showing that\nthey all have reasonably high accuracy.\n5.1 Summary\nTable 3 shows completeness, selectivity, and relia-\nbility scores for each intervention under the hyper-\nparameters that yield the highest reliability when\nthe intervention is performed against Ze (as visu-\nalized in Figures 1 to 3). AlterRep (the only inter-\nvention we consider that is both linear and counter-\nfactual) achieves the highest reliability score, with\nperfect completeness and reasonably high selectiv-\nity. The nonlinear counterfactual methods (FGSM,\nPGD, and AutoAttack) all perform very similarly\nto each other, but are significantly less reliable than\nAlterRep. Finally, the linear nullifying methods\nINLP and RLACE are least reliable, largely due to\ntheir low completeness."}, {"title": "6 Discussion", "content": "Tradeoff: Completeness vs. Selectivity In Fig-\nure 1, Figure 2, and Figure 3, we observe that in-\ncreasing the degree of control that interventions\nhave over the representation of the target property\n(either via increasing e, a, or rank) generally leads\nto both improved completeness and decreased se-\nlectivity.\nCounterfactual methods are able to achieve near\nperfect completeness, for both linear (AlterRep)\nand nonlinear (GBI) interventions; but AlterRep\nhas much higher selectivity while maintaining per-\nfect completeness. These findings suggest that lin-\near methods may indeed be preferred for causal\nprobing because their limited expressivity prevents\nthem from memorizing spurious associations in\nprobe training data (Hewitt and Liang, 2019), po-\ntentially making them less likely to damage spu-\nrious (environmental) features, meaning that they\nwould be more selective. Another possibility is\nthat linear methods can only achieve such high\ncompleteness in these experiments because we are\nconsidering embeddings from the final layer, which\nhave been argued to exhibit a greater degree of lin-\nearity (Alain and Bengio, 2017). However, despite\nthe impressive results of AlterRep, its reliability\nscore still peaks at 0.8346 for the optimal value\nof hyperparamteter a, indicating that there is still\nroom for improvement. In particular, while GBIs\ncan be precisely calibrated to manage the complete-\nness/selectivity tradeoff and can reach full com-\npleteness for high e, they never achieve an overall\nreliability above 0.5518, meaning that improving\nselectivity for nonlinear interventions is very much\nan open research question.\nIn contrast to counterfactual methods, nullify-\ning interventions never achieve high completeness\n(peaking at 0.3308 for INLP at the high water-\nmark for reliability, 0.4644), but tend to have much\nhigher selectivity (at least relative to GBIs). This is\nlikely because both INLP and RLACE are explic-\nitly optimized to minimize collateral damage while\nremoving the target representation (Ravfogel et al.,\n2020, 2022a), whereas GBIs are not (Tucker et al.,\n2021; Davies et al., 2023).\nCounterfactual vs. Nullifying Interventions\nAcross the board, all counterfactual methods (the 3\nGBI methods and AlterRep) achieve substantially\nhigher completeness and overall reliability scores\nthan all nullifying methods (INLP and RLACE).\nOur empirical evaluation framework is equally ap-"}, {"title": "7 Conclusion", "content": "In this work, we propose a general empirical eval-\nuation framework for causal probing, defining the\nreliability of interventions in terms of complete-\nness and selectivity. Our framework makes it possi-\nble to directly compare different kinds of interven-\ntions, such as linear vs. nonlinear or nullifying vs.\ncounterfactual methods. We study leading causal\nprobing techniques, finding that they all exhibit\na tradeoff between completeness and selectivity.\nCounterfactual interventions tend to be more com-\nplete and reliable, and nullifying techniques are\ngenerally more selective. In particular, we find that\na linear counterfactual intervention has a very favor-\nable tradeoff between completeness and selectivity,\nwhile a closely-related nullifying intervention us-\ning the exact same probes has very low reliability.\nThis suggests that underlying differences between\nthe types of interventions, rather than differences\nbetween specific methods of each type, may ex-\nplain why counterfactual interventions appear more\nreliable than nullifying methods for causal probing."}, {"title": "Limitations", "content": "Experimental Task and LLM In this work, we\nfocus primarily on developing an empirical evalua-\ntion framework to evaluate causal probing reliabil-\nity, and deploy this framework in the context of a\nrelatively simple model (BERT) and task (subject-\nverb agreement). As we discuss in Section 4, we\nintentionally select a more straightforward, well-\nstudied experimental setting in order to focus on\nour framework and the distinctions it reveals be-\ntween several types of causal probing interventions.\nThat is, despite the proliferation of much larger\nand more powerful LLMs than BERT, there is still\nno commonly agreed-upon method for interpret-\ning BERT's learned representations and explaining\nits behavior, even for simple zero-shot prompting\ntasks on which it achieves high accuracy. As our\nfocus is to rigorously evaluate existing causal prob-\ning methodologies (many of which have been de-\nsigned specifically with masked language models\nlike BERT in mind), we believe it is more useful to\nevaluate causal probing methods in the simpler and\nbetter-understood context of BERT than it would\nbe to scale our empirical analysis to larger models.\nHowever, it is important to note that our evalu-\nation framework makes no assumptions regarding\nLLM scale or architecture, nor the complexity of\ncausal task structures. Thus, now that we have\ndemonstrated the effectiveness of our framework\nin discovering new insights regarding leading in-\ntervention methods and providing the first hard\nevidence to empirically inform long-standing de-\nbates regarding causal probing, such as the concep-\ntual issues with nullifying interventions (Abraham\net al., 2022; Kumar et al., 2022), we hope that our\nframework will be deployed and improved upon\nby future work to study larger and more complex\ntasks and models, such as autoregressive decoder-\nonly (GPT-style) models and tasks with many more\nlatent properties of interest.\nMultiple Layers Another potential limitation of\nour work is that our experiments only explore in-\nterventions in the context of a single layer. As we\nexplain in Section 4, we only examine the final\nlayer in order to prevent information from non-\nintervened embeddings to be recovered by subse-\nquent attention layers (as observed by Elazar et al.\n2021). However, our framework makes no assump-\ntions about the specific layer to analyze \u2013 indeed,\nit is even possible to study the completeness, se-\nlectivity, and reliability of interventions performed\nin earlier layers l with respect to their impact on\noracle probes in downstream layers l' > l. We rec-\nommend such study as an interesting direction for\nfuture work, particularly in developing approaches\nfor understanding how information is distributed\nacross different contextualized token embeddings\nand accessed by downstream attention heads (e.g.,\nas studied in circuit discovery; Wang et al., 2023;\nConmy et al., 2023).\nOracle Probe Approximation Finally, in our\nmain paper, we report only results obtained us-\ning MLP oracle probes (a decision which we jus-\ntify at length in Section 4.4). However, we also\nperformed experiments with linear oracle probes,\nwhich we report in Appendix B.1. In general, these\nresults to be similar: the ordering of methods and\ngeneral trends remain the same, and the main dif-\nferences are that INLP and RLACE have slightly\nbetter completeness and lower selectivity (yield-\ning a marginally higher reliability score) and Al-\nterRep also has lower selectivity (with a similar\noverall trend). This is not surprising, as linear or-\nacle probes are expected to be more vulnerable to\nlinear interventions than MLPs. Given that our\ngoal in approximating oracle probes is to find the\nstrongest possible probe that is best able to recog-\nnize the model's representation, we believe that\nthe results from the MLP oracle probes are more\naccurate, which is why we focus on them in the\nmain paper."}, {"title": "A Experimental Details", "content": "A.1 MLP Probes\nWe do a hyperparameter sweep with grid search for\nthe MLP probes (both oracle and interventional)\nthat we train. The hyperparameters we consider\nare:\n\u2022 Num. layers: [1, 2, 3]\n\u2022 Layer size: [64, 256, 512, 1024]\n\u2022 Learning rate: [0.0001, 0.001, 0.01]\nSince the MLPs are performing classification, they\nare trained with standard cross-entropy loss. The\nprobes are trained for 8 epochs, and the best probe\nis selected based on validation accuracy.\nA.2 Interventions\nGradient Based Interventions: For all gradient-\nbased intervention methods (Davies et al., 2023),\nwe define the maximum perturbation magnitude of\neach intervention as e (i.e., ||\u0125'z \u2013 h'||\u221e < \u20ac), and\nexperiment over a range of e values between 0.005\nto 5.0 - specifically, \u0454 \u2208 [0.005, 0.006, 0.007,\n0.009, 0.011, 0.013, 0.016, 0.019, 0.024, 0.029,\n0.035, 0.042, 0.051, 0.062, 0.076, 0.092, 0.112,\n0.136, 0.165, 0.2, 0.286, 0.409, 0.585, 0.836,\n1.196, 1.71, 2.445, 3.497, 5.0]. (These are the\npoints along the x-axis for the results visualized in\nFigures 1 and 4. Figures 1 and 4.) We consider the\nfollowing gradient attack methods for GBIs:\n1. FGSM We implement Fast Gradient Sign\nMethod (FGSM; Goodfellow et al., 2015) in-\nterventions as:\nh' = h + e sgn (\u2207hL (fcls, x, y))\n2. PGD We implement Projected Gradient De-\nscent (PGD; Madry et al., 2017) interventions\nas h' = ht where\nht+1 = IN(h) (ht + a \u00b7 sgn (\u2207hL(fcls, x, y)))\nfor iterations t = 0, 1, . . ., T, projection oper-\nator II, and Lx-neighborhood N(h) = {h' :\n||h - h'|| \u2264 \u20ac}. For PGD, we use 2 additional\nhyperparameters: iterations T and step size a,\nwhile fixing T = 40, as suggested by (Davies\net al., 2023).\n3. AutoAttack AutoAttack (Croce and Hein,\n2020) is an ensemble of adversarial attacks\nthat includes FAB, Square, and APGD attacks.\nAuto-PGD (APGD) is a variant of PGD that\nautomatically adjusts the step size to ensure\neffective convergence. The parameters used\nwere set as norm = L\u221e and for Square attack,\nthe n_queries=5000.\nNullifying Interventions: For nullifying interven-\ntions, we project embeddings into the nullspaces\nof classifiers. Here, the the rank r corresponds\nto the dimensionality of the subspace identified\nand erased by the intervention, meaning that the\nnumber of dimensions removed is equal to the\nrank.4 We experiment over the range of values\nr\u2208 [0,1, ..., 40]. (These are the points along the\nx-axis for the results visualized in Figures 3 and 6.)\nWe consider the following nullifying interventions:\n1. INLP We implement Iterative Nullspace Pro-\njection (INLP; Ravfogel et al., 2020) as\nfollows: we train a series of classifiers\nW1,..., wn, where in each iteration, embed-\ndings are projected into the nullspace of the\npreceding classifiers Pr(wo)\u2229\u2229PN(Wn).\nWe then apply the combined projection ma-\ntrix to calculate the final projection where\nP := PN(w\u2081)\u2229\u2229N(wi), X is the full set\nof embeddings, and Xprojected \u2190 P(X).\n2. RLACE We implement Relaxed Linear Ad-\nversarial Concept Erasure (R-LACE; (Ravfo-\ngel et al., 2022a)) which defines a linear min-\nimax game to adversarially identify and re-\nmove a linear bias subspace. In this approach,\nPk is defined as the set of all D \u00d7 D orthogo-\nnal projection matrices that neutralize a rank\nr subspace:\nP \u2208 Pk \u2194 P = ID \u2013 W\u00aeW\nThe minimax equation is then solved to ob-\ntain the projection matrix P which is used\nto calculate the final intervened embedding\nXprojected, similar to INLP\nmingmax PEP \u2211l (Yn, 9-1 (0 PXn))\nn=1\nThe parameters used for P and @ included a\nlearning rate of 0.005 and weight decay of\nle-5.\nAlterRep We implement AlterRep (Ravfogel\net al., 2021) by first running INLP, saving all clas-\nsifiers, and using these to compute rowspace pro-\njections that push all embeddings to the positive"}]}