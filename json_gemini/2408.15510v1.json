{"title": "Measuring the Reliability of Causal Probing Methods: Tradeoffs, Limitations, and the Plight of Nullifying Interventions", "authors": ["Marc Canby", "Adam Davies", "Chirag Rastogi", "Julia Hockenmaier"], "abstract": "Causal probing is an approach to interpreting foundation models, such as large language models, by training probes to recognize latent properties of interest from embeddings, intervening on probes to modify this representation, and analyzing the resulting changes in the model's behavior. While some recent works have cast doubt on the theoretical basis of several leading causal probing intervention methods, it has been unclear how to systematically and empirically evaluate their effectiveness in practice. To address this problem, we propose a general empirical analysis framework to evaluate the reliability of causal probing interventions, formally defining and quantifying two key causal probing desiderata: completeness (fully transforming the representation of the target property) and selectivity (minimally impacting other properties). Our formalism allows us to make the first direct comparisons between different families of causal probing methods (e.g., linear vs. nonlinear or counterfactual vs. nullifying interventions). We conduct extensive experiments across several leading methods, finding that (1) there is an inherent tradeoff between these criteria, and no method is able to consistently satisfy both at once; and (2) across the board, nullifying interventions are always far less complete than counterfactual interventions, indicating that nullifying methods may not be an effective approach to causal probing.", "sections": [{"title": "Introduction", "content": "What latent properties do large language models (LLMs) learn to represent, and how are these representations leveraged by models in performing arbitrary downstream tasks? Causal probing aims to answer this question by training probing classifiers to predict some property of interest (e.g., parts-of-speech) from the model's embedding representations, using these probes to perform interventions that modify the model's representation of the property, then feeding the resulting embeddings back into the LLM to observe how such interventions impacts its behavior (Geiger et al., 2020; Ravfogel et al., 2020; Elazar et al., 2021; Tucker et al., 2021; Lasri et al., 2022; Davies et al., 2023; Zou et al., 2023). If the model is indeed using the property, then we expect its behavior to change; otherwise, the intervention should have little to no effect. The potential is that, in principle, it should be possible to use causal probing to interpret how arbitrary properties are used by LLMs (or any other neural network) in the context of specific tasks, thus explaining their behavior in terms of interpretable latent properties.\nIn order to confidently attribute any resulting changes in an LLM's behavior to its use of the target property, such analyses require that interventions fully and precisely carry out the desired transformation (Davies and Khakzar, 2024). However, prior works have indicated that interventions may often have a greater impact on the representation of non-targeted properties than the target property (Kumar et al., 2022), or that information about the target property that should have been completely removed from embeddings may still be recoverable by models (Elazar et al., 2021; Ravfogel et al., 2022b), making it impossible to draw strong conclusions on the basis of causal probing analyses. So far, there has been no generally accepted approach to empirically measuring such limitations or evaluating the reliability of causal probing interventions.\nOur main goal in this work is to address such concerns and work toward a systematic understanding of the effectiveness and limitations of current causal probing methodologies. Specifically, we propose an empirical analysis framework to evaluate the reliability of leading causal probing methodologies according to two key desiderata:\n1. Completeness: interventions should fully transform the representation of targeted properties.\n2. Selectivity: interventions should not impact"}, {"title": "Background and Related Work", "content": "Structural Probing The goal of structural probing (Hewitt and Manning, 2019; Maudslay et al., 2020; Belinkov et al., 2020) is to analyze which properties (e.g., part-of-speech, sentiment labels, etc.) are represented by a deep learning model (e.g., LLM) by training probing classifiers to predict these properties from latent embedding representations (Belinkov, 2022). That is, for deep learning model (e.g., LLM) M, input token sequence x = (x1, ..., XN), and embeddings h' = Ml(x) of input x at layer l of M, suppose Z is a latent property of interest that takes a discrete value Z=z for input x. The goal of structural probing is to train a classifier gz : Ml(x) \u2192 z to predict the value of Z from the embeddings h\u00b9. On the most straightforward interpretation, if gz achieves high accuracy on the probe task, then the model is said to be \"representing\" Z (Belinkov, 2022). One leading criticism of such methodologies is that correlation does not imply causation \u2013 i.e., that simply because"}, {"content": "Causal Probing A prominent response to this concern has been causal probing, which uses trained structural probes in order to intervene on models' representation of the probed property, observing the impact of interventions on models' predictions to analyze how the property is being used (Elazar et al., 2021; Tucker et al., 2021; Lasri et al., 2022; Davies et al., 2023). Specifically, causal probing performs interventions do(Z) that modify M's representation of Z in the embeddings h\u00b9, producing h\u00b9, where interventions can either encode a counterfactual value Z = z' (denoted do(Z = z') where z \u2260 z'), or remove the representation of Z entirely (denoted do(Z = 0)). Following the intervention, modified embeddings h\u00b9 are fed back into M beginning at layer l + 1 to complete the forward pass, yielding intervened predictions PrM(x, do(Z)). Comparison with the original predictions PrM(\u00b7|x) allows one to measure the extent to which M uses its representation of Z in computing PrM(x).\nCausal Probing: Limitations However, prior works have indicated that information about the target property that should have been completely removed may still be recoverable by the model (Elazar et al., 2021; Ravfogel et al., 2022b, 2023), in which case interventions are not complete; or that most of the impact of interventions may actually be the result of collateral damage to correlated, non-targeted properties (Kumar et al., 2022), in which case interventions are not selective. How seriously should we take such critiques? We observe several important shortcomings in each of these prior studies on the limitations of causal probing interventions:\n1. These limitations have only been empirically demonstrated for the specific task of removing all information about a target property from embeddings such that the model cannot be fine-tuned to use the property for downstream tasks (Kumar et al., 2022; Ravfogel et al., 2022b, 2023). But considering that the goal of causal probing is to interpret the behavior of an existing pre-trained model, the question is not whether models can be fine-tuned to use the property; it is whether models already use"}, {"title": "Evaluating Causal Probing Reliability", "content": "Recall that our main goal in this work is to evaluate intervention reliability in terms of completeness (completely transforming M's representation of some target property Zi) and selectivity (minimally impacting M's representation of other properties Zj \u2260 Zi). Given that we cannot directly inspect what value M encodes for any given property Zi, how can we determine the extent to which interventions have fulfilled either criterion? In order to formally define these criteria, we introduce the notion of oracle probes.\nOracle Probes We define an oracle probe o as a structural probe that can perfectly determine the distribution Pro(Z|h') over the possible values taken by property Z as encoded by representations h\u00b9 = Ml(x). For example, if representation h\u00b9 does not encode property Z at all, we expect Pro(Z|h') = U(Z), where U(Z) is the uniform distribution over values of Z; or if h' encodes value Z = z with complete confidence, then Pro(Z|h') = 1(Z = z), where 1(\u00b7) is the indicator function. Thus, a sufficiently high-quality oracle probe enables us to measure how well various intervention methodologies perform the desired intervention. Naturally, a perfect oracle does not exist in practice, so any practical implementation must approximate it (see Section 4.4).\nCompleteness If a counterfactual intervention do(Z = z') is perfectly complete, then it should produce hz that fully transforms h\u00b9 from encoding value Z = z to encoding counterfactual value Z = z' \u2260 z. Thus, after performing the intervention, oracle o should emit P = Pro(Z = z'|\u0125_z1) = 1. For nullifying interventions do(Z = 0), a perfectly nullified representation hz=0 should not encode Z at all: P = Pro(Z|hz=0) = U(Z).\nWe can use any distributional distance metric \u03b4(\u00b7,\u00b7) bounded by [0, 1] to determine how far the observed distribution Pz = Pro (Z|hz) is from the \"goal\" distribution P. Throughout this work, we use total variation (TV) distance, but any other such metric can be substituted in its place. Importantly, defining completeness C(\u0125z) in this way allows us to directly compare counterfactual and nullifying distributions: in both cases, 0 \u2264 C(2) \u2264 1, where attaining 1 means the intervention had its intended effect in transforming the encoding of Z.\nCounterfactual Interventions: For a counterfactual intervention, we measure completeness as:\n$C(\\hat{h}_Z) = 1 \u2013 \\delta(P_Z, P)$ (1)"}, {"title": "Experimental Setting", "content": "We test our framework by carrying out an extensive range of experiments to analyze current causal probing interventions in the context of BERT (Devlin et al., 2019). We opt for BERT because it is very well-studied in the context of language model interpretability (Rogers et al., 2021), particularly in causal probing (Ravfogel et al., 2020; Elazar et al., 2021; Ravfogel et al., 2021; Lasri et al., 2022; Ravfogel et al., 2022b, 2023; Davies et al., 2023). We further motivate this decision in Section 7.\nTo test our evaluation framework experimentally, we need a task with a clear causal structure. We start with the simplest possible case, where labels are fully determined by the value of a single binary causal variable Zc, with a second binary environmental variable Ze that influences inputs and may be spuriously correlated with the label (meaning that LLMs might still leverage it in performing the task). By intervening on each of these variables, we can calculate how well the target variable is damaged (completeness) and how little the opposite variable is damaged (selectivity). We also want a task that BERT performs well, allowing us to assess the extent to which its performance is attributable to its representation of the causal variable Ze versus the spuriously-correlated environmental variable Ze.\nTo fulfill both criteria, we select the cloze prompting task of subject-verb agreement, which has also been the subject of analysis in multiple prior causal probing works (Lasri et al., 2022; Ravfogel et al., 2021). Each instance in this task takes the form (xi, yi) where x\u012b is a sentence such as \u201cthe girl with the keys [MASK] the door,\u201d and the task of the LLM is to predict PrM(yi|x) > Pr\u043c(y|x)"}, {"title": "Interventions", "content": "In this section we describe each intervention that we study. In all cases, probes used for interventions are trained on a fully disjoint train set from that used to train the oracle probes.\nNullifying Interventions We experiment with two nullifying interventions:"}, {"title": "Discussion", "content": "Tradeoff: Completeness vs. Selectivity In Figure 1, Figure 2, and Figure 3, we observe that increasing the degree of control that interventions have over the representation of the target property"}, {"title": "Limitations", "content": "Experimental Task and LLM In this work, we focus primarily on developing an empirical evaluation framework to evaluate causal probing reliability, and deploy this framework in the context of a relatively simple model (BERT) and task (subject-verb agreement). As we discuss in Section 4, we intentionally select a more straightforward, well-studied experimental setting in order to focus on our framework and the distinctions it reveals between several types of causal probing interventions. That is, despite the proliferation of much larger and more powerful LLMs than BERT, there is still no commonly agreed-upon method for interpreting BERT's learned representations and explaining its behavior, even for simple zero-shot prompting tasks on which it achieves high accuracy. As our focus is to rigorously evaluate existing causal probing methodologies (many of which have been designed specifically with masked language models like BERT in mind), we believe it is more useful to evaluate causal probing methods in the simpler and better-understood context of BERT than it would be to scale our empirical analysis to larger models.\nHowever, it is important to note that our evaluation framework makes no assumptions regarding LLM scale or architecture, nor the complexity of causal task structures. Thus, now that we have demonstrated the effectiveness of our framework in discovering new insights regarding leading intervention methods and providing the first hard evidence to empirically inform long-standing debates regarding causal probing, such as the conceptual issues with nullifying interventions (Abraham et al., 2022; Kumar et al., 2022), we hope that our framework will be deployed and improved upon by future work to study larger and more complex tasks and models, such as autoregressive decoder-only (GPT-style) models and tasks with many more latent properties of interest.\nMultiple Layers Another potential limitation of our work is that our experiments only explore interventions in the context of a single layer. As we explain in Section 4, we only examine the final layer in order to prevent information from non-intervened embeddings to be recovered by subsequent attention layers (as observed by Elazar et al. 2021). However, our framework makes no assumptions about the specific layer to analyze \u2013 indeed, it is even possible to study the completeness, selectivity, and reliability of interventions performed"}, {"title": "MLP Probes", "content": "We do a hyperparameter sweep with grid search for the MLP probes (both oracle and interventional) that we train. The hyperparameters we consider are:\n\u2022 Num. layers: [1, 2, 3]\n\u2022 Layer size: [64, 256, 512, 1024]\n\u2022 Learning rate: [0.0001, 0.001, 0.01]\nSince the MLPs are performing classification, they are trained with standard cross-entropy loss. The probes are trained for 8 epochs, and the best probe is selected based on validation accuracy."}, {"title": "Interventions", "content": "For all gradient-based intervention methods (Davies et al., 2023), we define the maximum perturbation magnitude of each intervention as e (i.e., ||\u0125'z \u2013 h'||\u221e < \u20ac), and experiment over a range of e values between 0.005 to 5.0 - specifically, \u0454 \u2208 [0.005, 0.006, 0.007, 0.009, 0.011, 0.013, 0.016, 0.019, 0.024, 0.029, 0.035, 0.042, 0.051, 0.062, 0.076, 0.092, 0.112, 0.136, 0.165, 0.2, 0.286, 0.409, 0.585, 0.836, 1.196, 1.71, 2.445, 3.497, 5.0]. (These are the points along the x-axis for the results visualized in Figures 1 and 4. Figures 1 and 4.) We consider the following gradient attack methods for GBIs:\n1. FGSM We implement Fast Gradient Sign Method (FGSM; Goodfellow et al., 2015) interventions as:\nh' = h + e sgn (\u2207hL (fcls, x, y))\n2. PGD We implement Projected Gradient Descent (PGD; Madry et al., 2017) interventions as h' = ht where\nht+1 = IN(h) (ht + a \u00b7 sgn (\u2207hL(fcls, x, y)))\nfor iterations t = 0, 1, . . ., T, projection operator II, and Lx-neighborhood N(h) = {h' : ||h - h'|| \u2264 \u20ac}. For PGD, we use 2 additional hyperparameters: iterations T and step size a, while fixing T = 40, as suggested by (Davies et al., 2023).\n3. AutoAttack AutoAttack (Croce and Hein, 2020) is an ensemble of adversarial attacks that includes FAB, Square, and APGD attacks. Auto-PGD (APGD) is a variant of PGD that automatically adjusts the step size to ensure"}]}