{"title": "AI-Spectra: A Visual Dashboard for Model Multiplicity to Enhance Informed and Transparent Decision-Making", "authors": ["Gilles Eerlings", "Sebe Vanbrabant", "Jori Liesenborgs", "Gustavo Rovelo Ruiz", "Davy Vanacken", "Kris Luyten"], "abstract": "We present an approach, AI-Spectra, to leverage model multiplicity for interactive systems. Model multiplicity means using slightly different AI models yielding equally valid outcomes or predictions for the same task, thus relying on many simultaneous \"expert advisors\" that can have different opinions. Dealing with multiple AI models that generate potentially divergent results for the same task is challenging for users to deal with. It helps users understand and identify AI models are not always correct and might differ, but it can also result in an information overload when being confronted with multiple results instead of one. AI-Spectra leverages model multiplicity by using a visual dashboard designed for conveying what AI models generate which results while minimizing the cognitive effort to detect consensus among models and what type of models might have different opinions. We use a custom adaptation of Chernoff faces for AI-Spectra; Chernoff Bots. This visualization technique lets users quickly interpret complex, multivariate model configurations and compare predictions across multiple models. Our design is informed by building on established Human-AI Interaction guidelines and well know practices in information visualization. We validated our approach through a series of experiments training a wide variation of models with the MNIST dataset to perform number recognition. Our work contributes to the growing discourse on making Al systems more transparent, trustworthy, and effective through the strategic use of multiple models.", "sections": [{"title": "1 Introduction", "content": "Integrating artificial intelligence (AI) models into interactive software systems is becoming increasingly common across sectors such as healthcare and customer service. Typically, a single AI model is embedded within an interactive system, requiring careful design to ensure accessibility and usability for end users. Relying on a single model might quickly lead to a miscalibration in trust, as there is"}, {"title": "2 Related Work", "content": "When training an AI model for a task, multiple decisions are made about how the model should be constructed (e.g., hyperparameters, training data), resulting in several possible variations of models trained for the same purpose. Traditionally, the model that achieves the highest accuracy for that task is chosen. However, this process falls short when multiple models trained for the same task achieve similar accuracies in different ways due to their distinct decision boundaries [20], which are the surfaces formed by a model's learning process to separate classes within the feature space. This phenomenon is known as model multiplicity (MM) or the Rashomon effect [8], and the set of models that conform to model multiplicity for a certain task is referred to as the Rashomon set having multiplicitous models [17]. Model multiplicity was previously leveraged to prioritize other desirable properties beyond accuracy during the model selection process, such as interpretability or fairness, by choosing the model from the Rashomon set that prioritized this desired property [7]. While this prioritization focuses on the individual strengths of these equivalent models, equivalent models may make different choices, potentially confusing users. This can lead to issues of both over- and undertrust in the AI system. Users may lose trust when they cannot understand why one model's predictions are favored over another [4, 28], or conversely, they may place too much trust in the system, even when it is not performing as expected [39]. Both scenarios can negatively impact the user's interaction with the system.\nRelated work has also addressed procedural multiplicity, which occurs when models within a Rashomon set produce consistent predictions for a given input despite having different internal logic [9]. Because of these internal differences, local explanations such as those generated by SHAP [27] or LIME [31] will also vary between models, even in the case of procedural multiplicity, a situation"}, {"title": "2.2 Visualizing and Comparing Multiplicitous Models", "content": "Exploring the outputs of multiple AI models presents a challenge. Many tools have been developed to help users assess machine learning models through in- teractive visualizations [34, 36, 38]. However, one lesser-researched area is model comparison, as opposed to model interpretation. Most machine learning tools support comparing different models' parameterizations by summarizing perfor- mance statistics, but they do not show the models themselves as they evolve during (interactive) training [2, 21]. Only a few visualization tools, such as Tim- berTrek [37], facilitate the exploratory comparison of multiple models in a way that is accessible to end users. These tools provide intuitive dashboards for ex- ploring multiple model results, and their rich visualizations are primarily used as stand-alone systems designed for deep exploration through visual explana- tions. However, TimberTrek is limited to decision trees, making it unsuitable for systems that do not use this specific model architecture.\nAn early approach to comparing multiple models was implemented using CueFlik, a system that allows end users to train visual concepts for re-ranking web image search results based on their visual characteristics [2, 19]. In this system, users are provided with a historical overview of how different labels for objects influence the system's behavior in relation to their goals. By incorpo- rating history and revision mechanisms, CueFlik enabled users to achieve better final models within the same timeframe. These mechanisms allowed users to com- pare the current model with any previous version and backtrack if the model appeared to deviate from the desired outcome.\nAggregated measures provide a quick summary of model performance but often lack the detail needed to examine performance on different subsets of data. To address this limitation, the Boxer system was developed [21]. Boxer allows for a detailed examination and comparison of decision tree classification models"}, {"title": "3 Training, Preparing and Executing Model Multiplicity", "content": "Building upon the frameworks provided by Amershi et al. in Software Engineer- ing for Machine Learning [1], we designed an initial process for AI-Spectra that integrates these principles with a model multiplicity approach. This process is il- lustrated in Fig. 1. It consists of two stages for implementing model multiplicity: the preparation stage and the usage stage.\nModel Preparation The preparation stage begins with defining the model re- quirements and aligning the problem at hand with the capabilities of the machine learning model by determining which models are most suitable for the given task. Afterward, data must be collected, cleaned, and labeled to ensure usability. If necessary, feature engineering is applied to improve data quality. Following these steps, the 'many expert' candidate models are trained. To distinguish between models trained for the same purpose, we consider variations in 1) the input data for the models, 2) the architecture of the models, and 3) the training process. Specifically, models are generated by applying different configurations to the same dataset, trained with varying parameter settings (such as the number of layers and neurons per layer in a neural network), and the training processes are modified (e.g., varying the number of epochs, batch sizes, or optimizers). This approach resembles a grid search; however, instead of seeking a single optimal model, we aim to retain all models with their respective configurations, stored as the 'model metadata'. This metadata is used to keep track of which model has which configurations applied to them. Next, we search for Rashomon sets with the goal of grouping models that correctly predict the label for a given set"}, {"title": "Model Usage", "content": "Running multiple models simultaneously produces a range of outcomes that need to be aggregated. Since we prioritize insights from 'many experts' rather than relying on a single prediction, it is essential to present this variety of predictions without reducing them to a single value, such as an aver- age. However, obtaining a visual overview of each model's prediction for different classes can be challenging, especially as the number of possible outcomes grows. For n-class classification models like neural networks, where outputs span numer- ous possible classes, the visualization will be more complex than visualizing the solution space of models performing binary classification. That said, explaining how each model arrives at its specific conclusion can complicate visualizations even further, especially depending on the type of model used (e.g., neural net-"}, {"title": "4 Engineering A Visual Dashboard for Comparing Multiplicitous Model Sets", "content": null}, {"title": "4.1 Challenges in Visualizing Model Multiplicity in Interactive Systems", "content": "When integrating multiple model outputs into a user interface, presenting these outcomes in a meaningful and interpretable way poses significant challenges, particularly for complex models like neural networks. This complexity arises because different models may produce varying results for the same input, making it difficult for users to understand the differences or trust the predictions. To address these issues, we turn to the Guidelines for Human-AI Interaction [3] as a reference framework. These guidelines provide valuable insights into designing interfaces that support effective human-AI collaboration, ensuring that users can interpret, trust, and act on the aggregated predictions from multiple models."}, {"title": "4.2 Applying Human-AI Interaction Guidelines to Model Multiplicity", "content": "We selected six guidelines on which model multiplicity has the most significant impact with respect to when only a single AI model is used:\n1. (G3) Time services based on context. Model multiplicity introduces additional computational demands, which can lead to delays in system respon- siveness. An interactive system must strategically manage when and how to engage users to prevent interruptions at inopportune moments. By optimizing the timing of prediction requests and displays according to the user's current task, the system can effectively balance the computational load of multi- ple models while preserving a positive user experience.\n2. (G6) Mitigate social biases. Model multiplicity aims to deliver more bal- anced results and assist in detecting bias across different models. An interactive system that incorporates model multiplicity should facilitate the identifica- tion of biased models. By providing visual comparisons of model outcomes, the system can help users recognize biases more effectively, ultimately guiding better decision-making.\n3. (G11) Make clear why the system did what it did. Explaining the behavior of a system using multiple models can be challenging, especially when employing black-box models such as deep neural networks. The interface should enable users to explore the reasoning behind each model's output to enhance transparency. Interactive dashboards that allow users to investigate the spe- cific outcomes produced by various models can facilitate a better under- standing of the reasoning behind each model's decisions.\n4. (G14) Update and adapt cautiously. Updating model multiplicitous systems can be challenging, as each model may respond differently to changes. Such updates might introduce inconsistencies or affect performance in ways that eliminate model multiplicity when the accuracy of models becomes drastically different, unlike in single-model systems. Therefore, an interactive system should enable users to compare similar models that behave differently to assess the potential impact of updates on overall performance.\n5. (G15) Encourage granular feedback. Collecting feedback in a model multiplicity system is more complex, as different models may respond differ- ently to the same input. The system should facilitate users in reflecting on individual model outputs rather than providing generalized feedback. This approach enables more targeted improvements, allowing for the refinement of model selection based on user observations.\n6. (G16) Convey the consequences of user actions. In systems with multiple models, user actions can impact each model differently, making it a complex and time-consuming process. As a result, immediately conveying the consequences of user actions may not be feasible. Instead, the system should give users insights into how their actions influence each model over time, allowing for a more informed decision-making process.\nOur focus is on leveraging model multiplicity to enhance informed and trans- parent decision-making, for which guidelines (G6) \u201cfacilitate the identification of biased models\u201d, (G11) \u201cinvestigate the specific outcomes produced by various models", "enable users to compare similar models that behave differently": "nd (G15) \"facilitate users in reflecting on individual model outputs\" are par- ticularly important. Guideline (G3) \u201cbalance the computational load of multiple models while preserving a positive user experience"}, {"title": "4.3 Chernoff Bots for Multidimensional Data Representation", "content": "In this section we introduce a technique to visualize the variations of mul- tiple models at once, enabling quick comparisons between models and their behavior. We use Chernoff faces [11, 18] to graphically represent points in k- dimensional space. Chernoff faces leverage our innate ability to recognize dif- ferences in human-like faces, even when these are rendered in a cartoon style. They serve as mnemonic devices to reveal complex relationships that are not immediately apparent. Moreover, Chernoff faces guarantee the presence of positive hedonic aspects for human users by leveraging this cognitive famil- iarity of facial recognition, fostering intuitive pattern recognition, memorability, and enjoyable, exploratory interaction with data. This approach offers a pow- erful tool for enhancing human-in-the-loop systems by using facial features to represent multidimensional data. This approach takes advantage of human intu- ition and our natural ability to recognize facial patterns, making it particularly effective for identifying differences in complex models, such as machine learning models. By mapping various model parameters to distinct facial characteristics, users can quickly and easily spot variations between models, facilitating faster and more intuitive comparisons.\nWe developed a custom approach based on Chernoff faces: our visual rep- resentation is tailored to the type of (meta-)data of machine learning models. We use depictions of robot faces, dubbed a \"Chernoff Bot\", instead of a human face to allow for greater creative freedom in introducing unconventional features, such as varying the number of eyes. This choice lets us explore designs that do not need to adhere to realistic human proportions. We aimed to map variations in the data directly to various parts of the robot's face wherever applicable. The Chernoff Bots capture two variations of the preparation process outlined in Fig. 1: variations in training data and differing model configurations. Since the dataset is unique to each problem to be solved, we categorize the aspects into independent and dependent factors. Independent factors are not specific to any particular problem and are instead tied to the model architecture, in this case, neural networks. Dependent factors, on the other hand, are data-specific factors and cannot always be transferred to other problems. These dependent factors allow model developers to create custom representations of problem-specific data within a Chernoff Bot. We will discuss how we transferred our MNIST dataset- dependent variations into Chernoff representations in Section 5.1. We decided to omit the optimizer from this representation, as its impact on the classifier was negligible."}, {"title": "4.4 Integrating Chernoff Bots into an Interactive Dashboard", "content": "As we work with a model multiplicity system, we integrate multiple Chernoff Bots into a visualization so that users can explore the model multiplicity output space (depicted in Fig. 1). Concretely, we create a bar chart out of Chernoff Bots for each predicted sample as shown in Fig. 3. The x-axis of this bar chart displays the possible labels for the task (in the case of the MNIST dataset, the ten possible digits). Whenever a model predicts a particular label, its Chernoff representation is stacked on that label, forming a bar in the y-axis. The height of the resulting bar corresponds to the number of models that predicted the same label for that input sample. This grouping of similar 'thinking' experts allows the user to get an immediate impression of what output is most agreed upon and, therefore, assumed most likely to be correct. Furthermore, users can form their own judgments by observing the input data and comparing the differences between the Chernoff Bots."}, {"title": "5 Application to the MNIST Dataset", "content": "As a use case for our process (introduced in Fig. 1), we focus on neural networks trained on the MNIST dataset, which consists of 70,000 28x28 grayscale images of handwritten digits from 0 to 9 [23]. We selected the MNIST dataset because it has become a benchmark for classification tasks in machine learning and com- puter vision applications [12]. Neural networks were chosen due to their inherent lack of interpretability and their modularity, which allows them to be easily customized by adding layers, changing activation functions, or adjusting other parameters. This flexibility simplifies the process of creating and comparing var- ied models. Additionally, we apply our novel method for visualizing and, more importantly, comparing multiple models using Chernoff Bots corresponding to these neural networks visualized in a bar chart as introduced in Section 4."}, {"title": "5.1 Preparation: Training a Multitude of Models", "content": "The first stage of the model multiplicity framework is the preparation stage, dur- ing which many similar models are trained differently for the same purpose. This approach aims to identify a subset of neural networks that achieve similar accu- racy levels but arrive at their decisions through different internal mechanisms, thus belonging to the same Rashomon set. We introduce variations across several dimensions, including model hyperparameters and training data, to induce these differences in the models' internal logic. To facilitate this process, we employ a method that automatically generates models based on random combinations of"}, {"title": "Training dataset variations", "content": "Since we work with images, we apply data aug- mentation to enhance the training data. We primarily use position augmenta- tions (translations and rotations) and color augmentations (contrast manipula- tion and image color inversion) to create variations in the dataset. Translations are limited to a maximum of two pixels in any direction to ensure digits remain within the frame, and rotations are restricted between -20\u00b0 and 20\u00b0 in 5\u00b0 in- crements to avoid misinterpreting numbers. This variation enables some models to train with mostly original images, while others are exposed to more inverted images, enhancing their ability to recognize digits across different color schemes.\nImage color inversion is achieved by subtracting each pixel's value from 255, effectively transforming light colors to dark and vice versa. A contrast factor of one maintains the original contrast, while values below one reduce the contrast (with a minimum threshold of 0.2 to prevent the image from becoming uniformly black). Values above one increase the contrast up to a maximum of 1.8 to avoid excessive whitening that could obscure important image features. This variation allows some models to train with primarily increased or decreased contrast, leading to different model behaviors. The proportion of these variations in the dataset varies from 0% to 100% in increments of 20%.\nAdditionally, we explore the effects of varying the proportion of outliers and typicals in the training data, ranging from 0% to 100%. This approach allows us to observe how models perform under different conditions: some models are trained exclusively with outliers, some with no outliers, and others with a mix of typical data points and outliers. As a result, we obtained a diverse set of models with varying performance, with some excelling in handling outliers and others performing better with typical data. Outlier detection was performed using isolation forests for each digit label [26]. We chose to identify anomalies within each group of digits rather than across the entire dataset, ensuring that the detection process remains sensitive to the unique characteristics of each digit class and avoids masking subtle outliers specific to individual groups."}, {"title": "Model parameter variations", "content": "The model parameters we selected include the number of hidden layers, the dropout rate, and the activation function. We use between one and three hidden layers, each with 128 neurons, to keep the training duration manageable. Introducing dropout layers helps create model multiplicity by inducing variations in neuron participation, which effectively generates dis- tinct models during each training iteration, even if all other parameters remain the same. For activation functions, we included a range of linear, non-linear, and specialized functions to examine different learning dynamics and performance levels. This variety is particularly relevant for studying model multiplicity, as diverse activation functions can lead to different model behaviors and gener- alization abilities. It is important to note that only the hidden layers employ variable activation functions, while the output layer consistently uses the soft- max function to ensure comparability in predictions. Additionally, dropout and activation functions are consistent across all hidden layers to constrain the vari- ation."}, {"title": "Training variations", "content": "Training variations in our experiment include batch size, optimizer choice, and the use of a validation set. Mini-batch gradient descent is chosen for its advantages in accelerating training and allowing flexibility in batch size adjustment. According to the literature, the optimal batch size typ- ically ranges between 32 and 512 [6, 22], so we allowed models to train with batch sizes of 32, 64, 128, 256, and 512 to cover a broad spectrum and enhance model diversity. We considered four categories of optimizers: basic gradient de- scent variants, adaptive learning rate optimizers, adaptive moment estimation optimizers, and hybrid optimizers. Additionally, we allowed models to either use a validation set or train without one. While a validation set is typically used to assess model performance on unseen data, its use reserves a portion of the training data for validation, which could reduce the amount of data available for training. Given the importance of training data volume in the performance of machine learning models [35], we also opted to include models trained on the full dataset. We set a hard limit of 100 epochs to manage training time across numerous models. Training stops when this limit is reached, even if the model's validation loss continues to improve. This approach ensures models are adequately trained while keeping the overall process more time-efficient."}, {"title": "6 Limitations and Future Opportunities", "content": "In our current solution, we omitted guideline (G3) \u201cbalance the computational load of multiple models while preserving a positive user experience\u201d since this is out of scope for this particular work. However, the scalability trade-off between the number of models employed, their respective size and complexity and the importance of the tasks these models support needs to be explored. Increasing the number of models by an order of magnitude must not necessarily correlate with a proportional increase in computational power requirements. For instance, if (a subset of) models are allowed to be less complex as the number of models increases, the overall computational burden could be partially mitigated. For a sustainable and responsible usage of these models, we need to define metrics (e.g., Pareto optimality [33, Section 3.3]) to map the relationship between using more models and the increase in accuracy and trustworthiness.\nOur work is focused on black box models, namely classifiers based on neural networks, and the visual dashboard displays the metadata from the models rather than exposing (parts of) the internal behavior of the model. One of the next steps is allowing for a deeper integration of multiple black-box and white-box models in AI-Spectra. Since white box models are more interpretable and can provide more detailed explanations, the transparency, auditability and fairness of the overarching system's decision-making will increase by combining these with our current approach. This will require careful balancing of the additional information that is provided to the user with the actual need for more profound insights into model behavior. Furthermore, when the AI models become more context-aware, the need to design solutions that apply guideline (G16) \u201cgive users insights into how their actions influence each model over time\u201d will become more important.\nIntegrating model multiplicity into existing interactive systems necessitates re-evaluating user interface (UI) design; existing UI paradigms must be adapted to effectively communicate the insights of multiple expert models and mech- anisms must be developed to aggregate predictions behind the scenes. How- ever, such aggregations may compromise interpretability for end users, raising concerns about transparency in decision-making processes. Consequently, these modifications to established systems would be a costly endeavor. Deploying mul- tiple models simultaneously can lead to inefficiencies in system performance, in-"}, {"title": "7 Conclusion", "content": "Model multiplicity presents unique challenges in integrating AI into interactive systems, particularly concerning trust and transparency. To address these issues, we proposed AI-Spectra that supports a shift toward a \u201cmany simultaneous ex- pert advisors\" approach. This approach enhances the accuracy and reliability of AI systems by leveraging multiple models instead of relying on a single model.\nTo address the challenge of integrating model multiplicity into interactive systems, AI-Spectra includes a process and visual dashboard that facilitates informed and transparent decision-making. The process is designed to prepare, train and identify sets of multiplicitous models for a given dataset. Specifically, it provides a way to train models with various configurations and demonstrates how these models can be used together within an interactive system to counteract the non-deterministic nature of AI.\nTo effectively present these models to enhance informed, transparent decision- making, we designed a compact and easy-to-recognize visual dashboard by in- tegrating Chernoff Bots, our customized and expandable approach to Chernoff faces, into a bar chart, following the Guidelines for Human-AI Interaction. This visualization approach allows users to quickly understand the hyperparameters, properties and background of an AI model. This transparency ensures that hu- mans remain in the loop so that they can endorse or disregard specific models based on informed judgment selectively. We demonstrate AI-Spectra on multi- plicitous neural networks for digit classification trained on the MNIST dataset.\nFinally, we discussed the limitations of our work and identified future re- search opportunities, particularly regarding the scalability of our approach to more complex use cases. We believe that investigating the trade-offs between the number of models used, their size and complexity, and the significance of the tasks they address will be crucial for responsibly scaling the use of multiple models. Additionally, we plan to integrate white-box models alongside black-box models in AI-Spectra, as we believe this integration would improve the system's overall interpretability. Lastly, we aim to apply AI-Spectra within established interactive systems to enhance the transparency of AI systems and facilitate the decision-making of end users. In summary, AI-Spectra contributes to the ongoing discourse on enhancing AI systems by promoting greater transparency, trustworthiness, and effectiveness, through the use of multiplicitous models."}]}