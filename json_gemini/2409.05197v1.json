{"title": "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?", "authors": ["Neeladri Bhuiya", "Viktor Schlegel", "Stefan Winkler"], "abstract": "State-of-the-art Large Language Models (LLMs) are accredited with an increasing number of different capabilities, ranging from reading comprehension, over advanced mathematical and reasoning skills to possessing scientific knowledge. In this paper we focus on their multi-hop reasoning capability-the ability to identify and integrate information from multiple textual sources.\nGiven the concerns with the presence of simplifying cues in existing multi-hop reasoning benchmarks, which allow models to circumvent the reasoning requirement, we set out to investigate, whether LLMS are prone to exploiting such simplifying cues. We find evidence that they indeed circumvent the requirement to perform multi-hop reasoning, but they do so in more subtle ways than what was reported about their fine-tuned pre-trained language model (PLM) predecessors. Motivated by this finding, we propose a challenging multi-hop reasoning benchmark, by generating seemingly plausible multi-hop reasoning chains, which ultimately lead to incorrect answers. We evaluate multiple open and proprietary state-of-the-art LLMs, and find that their performance to perform multi-hop reasoning is affected, as indicated by up to 45% relative decrease in F1 score when presented with such seemingly plausible alternatives. We conduct a deeper analysis and find evidence that while LLMs tend to ignore misleading lexical cues, misleading reasoning paths indeed present a significant challenge.", "sections": [{"title": "1 Introduction", "content": "Recent developments in the field of language modelling, and the introduction of open (Touvron et al., 2023) and proprietary (OpenAI et al., 2023) Large Language Models (LLMs), have undeniably advanced the state of the art in Natural Language Processing (NLP). LLMs have been credited with various understanding and reasoning capa-\n\nbilities, ranging from the ability to perform arithmetic (Cobbe et al., 2021), deductive (Saparov et al., 2023) and formal (Schlegel et al., 2022b; Madusanka et al., 2023) reasoning and possessing general (AlKhamissi et al., 2022), and domain-specific (He et al., 2023) knowledge. Due to their size and generalisation capabilities (Brown et al., 2020), their evaluation on benchmarks requiring such types of reasoning is typically performed in zero- or few-shot settings on many NLP tasks, without the need of fine-tuning datasets.\nThese zero- and few-shot capabilities seem to alleviate one of the weaknesses identified with the previous generation of fine-tuning based NLP architectures such as transformer-based (Vaswani et al., 2017), and pre-trained language models (Devlin et al., 2019)\u2014the reliance on data-set specific \u201carte-facts\" (Gururangan et al., 2018; Schlegel et al., 2022a) and, as a consequence, lack of generalisation beyond specific datasets. For example, in one of the popular reading comprehension and reasoning benchmarks (Dua et al., 2019), the majority of questions starting with \u201cHow many\" can be an-"}, {"title": "2 Related Work", "content": "It has been shown that basic pattern matching (Schlegel et al., 2020) and one-hop (Min et al., 2019a) models can solve a large proportion of questions in multi-hop question answering datasets, presumably because the answer sentence often contains keywords common with the question, thus negating the need to follow a reasoning path and attend to multiple documents. Particularly HotpotQA (Yang et al., 2018b), due to its multi-hop question design, was the target of multiple studies. Studies have shown that approaches architecturally incapable of multi-hop reasoning still achieved close to state-of-the-art performance (Min et al., 2019a; Trivedi et al., 2020), suggesting questions answerable in such a way do not necessitate multi-hop reasoning.\nIn light of these results, several adversarial attacks have been proposed to ensure that the dataset evaluates multi-hop reasoning without exhibiting \"shortcuts\", by ensuring that the correct answer can only be procured if the evaluated model can retrieve and combine information from distinct reasoning hops. Jiang and Bansal (2019) elicited distracting paragraphs by using the titles of the gold paragraphs and the answer, which are subjected to"}, {"title": "3 Methodology", "content": "In this section, we describe our approach to evaluating the multi-hop reasoning capabilities of LLMs. We do so by creating \"distractor\" paragraphs that present seemingly plausible yet incorrect alternative paths in the reasoning chain while ensuring that this process doesn't affect the final solution. First, the question is treated as a two-hop question and converted into two sub-questions. This is done to be able to branch out alternative reasoning paths from each of the sub-questions. The sub-questions are analyzed to identify modifiable portions, which are then manipulated to create \"distractor\" sub-questions that lead to a different answer and thus a different reasoning chain, which is ultimately wrong, as the models are presented with the original, unmodified question. The \u201cdistractor sub-questions\" are finally used to generate \u201cdistractor paragraphs\" containing \u201cdistractor answers\", using an LLM. The method comprises three main steps: I. Acquiring the main entity, II. Extracting its modifiable details, and III. Creating the distractor paragraphs.\nI. Acquiring the main entity We use the human-annotated sub-questions from Tang et al. (2021), as exemplified in Figure 2. We define main entities"}, {"title": "4 Experimental Setup", "content": "First, we investigate LLM's capabilities and limitations compared to previous PLM-based state of\nthe art. Then, we evaluate the multi-hop reasoning capabilities of LLMs using our proposed methodology. Finally, we conduct an in-depth analysis of what makes reasoning hard for LLMs on our benchmark and conclude by evaluating state-of-the-art LLMs and prompting techniques. Unless mentioned otherwise, we use the chat models for Llama-2.\nDo LLMs suffer from the same flaws as fine-tuned models? Llama-2-13B (Touvron et al., 2023) is used as the baseline LLM. We evaluate using few-shot prompts, as these allow the model to stick to the expected output format better than zero-shot. This setting is used throughout the paper unless mentioned otherwise. Two styles of prompts were used, normal and chain of thought, as per the strategies discussed in Wei et al. (2023). All reported metrics are measured at token level and averaged across all the instances, following standard evaluation practice (Yang et al., 2018b).\nFollowing this, we test the LLMs' performance when attacked with AddDoc (Jiang and Bansal, 2019), an adversarial attack on HotpotQA for BERT-based models. This is intended to test an LLM's ability to handle \"distracting\" paragraphs. SubQA was used to determine if the models could answer the individual questions before answering the entire question. It is a sample of 1000 questions and their sub-questions from the dev set of HotpotQA, with the sub-questions being human-verified. This allows us to evaluate model consistency in answering both the multi-hop question as well as the individual sub-questions correctly. It also allows us to investigate the opposite: When the (more complex) composite question is answered correctly, but either of the (simpler) decomposed questions is answered wrongly, the model might rely on some reasoning shortcuts, discarding sub-question information. Finally, we evaluate if LLMs can retrieve the correct answer when necessary information from one of the gold paragraphs is missing, using the DiRe test set (Trivedi et al., 2020).\nDo LLMs get distracted by seemingly plausible alternate reasoning paths? As described in Section 3, the attack aims to create paragraphs that provide irrelevant information that is closely related to the property/entity being questioned about. Here, we evaluate a representative sample of open-source and proprietary LLMs, specifically, Llama-2-13B, Llama-2-70B, Mixtral-8x7B-Instruct-v0.1, GPT-3.5 and"}, {"title": "5 Experimental Results", "content": "In this section, we present the results of our experiments, compare them against prior work, and discuss deeper insights. Unless otherwise stated, all reported results of the adversarial attack are sta-tistically significant at p < 0.05, determined by conducting a one-sided Student's t-test.\n5.1 Do LLMs suffer from the same flaws as\nfine-tuned models?\nI. Setting up the baseline Llama-2-13B chat\nmodel is used as the baseline for the performance\nof an LLM in a zero/few-shot setting; results are\nshown in Table 1. The F1-score indicates that the\nfew-shot setting without chain-of-thought prompt-ing performs best. This is because in the chain\nof thought setting the model often gives a lengthy\nexplanation, thus reducing the precision and F1\nscore.\nII. Reasoning shortcuts using SubQA Table 2\nshows the result of running few-shot Llama-2-13B\nin the controlled setting on the SubQA dataset.\nIII. Reasoning shortcuts in DiRe DiRe consists\nof removing the bridging gold paragraph from the\ncontext, with the claim that a model should not be\nable to answer them under these conditions, and if"}, {"title": "5.2 Do LLMs get distracted when faced with\nseemingly plausible alternatives?", "content": "Table 4 shows the results of various open- and\nclosed-source LLMs using our proposed bench-marking method. All models show a significant\ndrop in their F1 scores and their Exact-Match (EM)\nscores. Importantly, this seems to be a model prop-erty rather than an artefact of the prompting tech-nique, as the behaviour persists across different\nprompting methods (see Appendix H). Further-more, even GPT-4 exhibits a drop of 14 points\nin F1 under the strongest adversarial attack setting\ni.e., when adding four adversarial paragraphs (see\nAppendix G). This is remarkable, as the bench-mark was partially generated with GPT-4 in the\nloop. This highlights the feasibility of our method\nto evaluate a model using an equally strong model\nas an adversary, a property which other benchmarks\ntend to lack (Zellers et al., 2018, 2019)."}, {"title": "5.3 Analysing the effects of different\nparameters", "content": "Next", "Para-graph count\\\" columns, shows the results of the\nvarious models in the chain of thought few-shot set-ting when facing two or four distractor paragraphs,\nrespectively. Indeed, the higher the number of ad-versarial paragraphs, the more the model struggles,\nwith an additional decrease of about 10 F1 points\nfor every fake reasoning chain on average.\nAre the paragraphs related? As our method\ncreates fake sub-questions that are used to generate\ndistractor paragraphs, we can modify if the para-graphs to be used in the attack belong to the same\nfake question pair or not. If not, the attack will use\nparagraphs from different pairs but will ensure that\nif k adversarial paragraphs are being added, k/2\nare generated from the first sub-question and the\nother from the second sub-question. This is useful\nto check if models struggle because of the pres-ence of alternate multi-hop reasoning chains, or if\nthe difference in performance is attributed to dis-tractor paragraphs containing similar but otherwise\nunrelated information.\nTable 4, columns \\\"Paragraph Related\\\" shows\nthe performance of the models in this setting. For\nLlama-2-13B, Mixtral-8x7B-Instruct-v0.1,and Llama-2-70b, related paragraphs, and there-fore complete alternate reasoning chains, cause a\nlarger drop than unrelated distractor paragraphs.\nInterestingly, GPT-3.5 exhibits the opposite\nbehaviour, performing slightly worse when the\ndisctractor paragraphs are not connected by an\nalternate reasoning chain.\nModified type Because the main entity of the\nquestion can be either part of a Named Entity or\nnot, we can distinguish model performance be-tween these settings. Table 4, columns \\\"Modified\nType\\\", shows the results of this test. Aside from1lama-2-13b, which performs significantly worse\non Named Entities, the differences are not statis-tically significant, indicating that both distractor\ntypes seem to be equally difficult.\nAre the paragraphs unrelated and only belong\nto the 2nd subquestion? We have shown that(with the exception of GPT-3.5) examples contain-ing fake paragraphs related by a seemingly alter-nate reasoning chain are harder for LLMs to pro-cess correctly. Similarly, we can investigate if fake\nparagraphs that are generated purely from the sec-ond sub-question add further complexity. Since the\nparagraph generated from the second sub-question\nis the only paragraph that contains an entity of the\nsame type as the actual answer, the rationale is to\ninvestigate what contributes more to hard multi-hop\nreasoning": "producing seemingly alternate reason-ing chains or just adding adversarial paragraphs\nsimilar to the paragraph answering the second sub-question. We ensure that the number of adversar-ial paragraphs, generated using our method, is thesame in both settings.\nAs can be seen in the last column of Table 4,\"Second Sub-Q only\u201d, all LLMs perform worse\nwhen the paragraphs are not generated from thesecond sub-question only, thus adding further evi-dence to the hypothesis that examples with seem-ingly plausible alternate reasoning chains are in-deed harder for LLMs to process correctly. Ad-ditionally, only the fine-tuned longformer model\nexhibits the opposite behaviour, suggesting thatPLM-based fine-tuned models indeed tend to learnmore simple word-matching type heuristics, as gen-erating multiple paragraphs from the second sub-question results in more fake paragraphs that arelexically similar to the question and answer sen-tence. This adds further evidence that there is aneed to reevaluate the weaknesses of LLMs, as in-sights derived from PLMs do not necessarily carryover.\nThe second sub-question-only setting is most\nsimilar to AddDoc (Jiang and Bansal, 2019) andother existing attacks on HotpotQA. However, un-like for AddDoc, all LLMs still show a drop inperformance. This demonstrates the effectivenessof generating adversarial paragraphs by changing\nminute details extracted from the question, surpass-ing the impact of existing attacks. The paragraph"}]}