{"title": "Beyond Labels: A Self-Supervised Framework with Masked Autoencoders and Random Cropping for Breast Cancer Subtype Classification", "authors": ["Annalisa Chiocchetti", "Marco Dossena", "Christopher Irwin", "Luigi Portinale"], "abstract": "This work contributes to breast cancer sub-type classification using histopathological images. We utilize masked autoencoders (MAEs) to learn a self-supervised embedding tailored for computer vision tasks in this domain. This embedding captures informative representations of histopathological data, facilitating feature learning without extensive labeled datasets. During pre-training, we investigate employing a random crop technique to generate a large dataset from WSIs automatically. Additionally, we assess the performance of linear probes for multi-class classification tasks of cancer sub-types using the representations learnt by the MAE. Our approach aims to achieve strong performance on downstream tasks by leveraging the complementary strengths of ViTs and autoencoders. We evaluate our model's performance on the BRACS dataset and compare it with existing benchmarks.", "sections": [{"title": "1. Introduction", "content": "Histopathological image analysis plays a critical role in disease diagnosis, particularly in cancer. Whole-slide images (WSIs) offer high-resolution views of entire tissue sections, enabling comprehensive evaluation by pathologists. However, manual analysis of WSIs is time-consuming and prone to inter-observer variability. Deep learning models have emerged as powerful tools to automate histopathological image analysis, offering the potential for faster, more consistent, and potentially more accurate diagnoses. Given the high-resolution nature of WSIs, it is interesting to adopt a localized analysis approach. This involved extracting image patches and applying tasks like classification at the level of bag of tissue regions, employing a Multiple Instance Learning (MIL) framework [1]. Furthermore, the dimensions of the extracted patches facilitate the application of deep learning models for computer vision tasks, including Convolutional Neural Networks (CNN) and Vision Transformer (ViT) architectures, for various objectives such as tissue classification and cell segmentation.\nCNNs have become the most used approach in this domain due to their ability to capture spatial relationships within images [2, 3]. Architectures such as VGG, ResNet, and Inception excel at learning hierarchical features directly from raw image data, making them ideal for tasks like tissue classification, tumor segmentation, and cell detection. However, CNNs struggle to capture long-range dependencies within complex tissues.\nThis is where Graph Neural Networks (GNNs) offer a promising alternative. GNNs represent tissue structures as graphs [4, 5], where nodes represent cells and edges depict their relationships. This allows GNNs to effectively model complex interactions between cells, making them particularly useful for analyzing cell-to-cell communication or studying the spatial distribution of different cell types.\nAlternatively, for tasks aiming to classify multiple bag of regions under a single label, the graph representation can be constructed at the patch level. This approach treats each region as a node within the graph [6]. ViTs represent another interesting approach. Unlike CNNs, ViTs process image patches directly, leveraging transformer techniques to learn global dependencies across the entire image [7, 8]. This approach proves advantageous for tasks requiring analysis of intricate tissue patterns or overcoming limitations imposed by pre-defined filter sizes in CNNs.\nExisting methods try to achieve an optimal latent representation that facilitates accurate classification of tissue regions or whole slide images. We propose a reconstruction framework called Histopathological Masked AutoEncoder (HMAE) followed by a simple classifier. The construction of the latent space of this model is achieved through the self-supervised objective of reconstructing the original image. By masking a significant portion of image patches during input, the encoder is forced to identify increasingly intricate patterns in the remaining data to reconstruct the complete image. Finally, a simple MLP with an output linear layer is applied after the training of the transformer, in order to perform classification, so that the model is never exposed to the image class labels during the learning process.\nThe datasets we have considered in the present work are the following.\nBRACS. The BReAst Carcinoma Subtyping (BRACS) dataset [9] is a collection of digital images used to study breast lesions. It includes 547 WSIs, which are high-resolution scans of entire tissue samples. Additionally, 4539 smaller, more specific areas called regions of interest (ROIs) are extracted from these whole-slide images. Each WSI and its corresponding ROIs are carefully examined and labeled by three pathologists. These categories encompass three main lesion types: benign (healthy tissue), malignant (cancerous tissue), and atypical. Further details are provided by seven subcategories within these main types, allowing for a more precise understanding of the specific lesion.\nBACH. The Breast Cancer Histology (BACH) dataset [10] is a significant resource for researchers developing computer algorithms to automatically diagnose breast cancer. It consists of a collection of digitized images from breast biopsies (WSIs). The ROIs extracted from each WSI are labeled according to four classifications: normal tissue, benign tumors, in situ carcinoma (precancerous cells), and invasive carcinoma (cancerous cells). There are 100 ROIs for each class."}, {"title": "2. Methodologies", "content": "This section investigates the synergy between ViTs and masking techniques for image reconstruction using a Masked Autoencoding framework (MAE). We will begin by investigating the theoretical foundations of ViT and masked image encoding, exploring their architectural details and functionalities. Next, we will describe the process of acquiring and pre-processing a dataset suitable for MAE training. This dataset will be extracted from whole slide images (WSI) obtained from BRACS patients. Finally, we will elucidate the methodology employed to leverage the feature representations (embeddings) learnt the ViT model for the task of cancer classification."}, {"title": "2.1. Vision Transformers", "content": "Vision Transformers (ViTs) [11] represent a paradigm shift in computer vision, achieving state-of-the-art results on image classification tasks while departing from the traditional dominance of Convolutional Neural Networks (CNNs). Unlike CNNs that rely on hand-crafted filters for feature extraction, ViTs leverage the Transformer architecture, originally successful in Natural Language Processing (NLP). The key idea is to split an image into fixed-size patches. These patches are then embedded into a vector representation and fed into a Transformer encoder. The Transformer utilizes self-attention [12] mechanisms to learn long-range dependencies between different parts of the image, allowing the model to capture global context crucial for classification."}, {"title": "2.2. Masked Encoder", "content": "As first, we divide the image into equally sized non-overlapping patches. Then, instead of using all the patches, we pick a certain number and hide the rest (masking) [13]. These patches are chosen randomly and without replacement. We also pick patches randomly across the whole image to avoid favoring the center (center bias).\nUsing random sampling with a lot of masking makes it harder for the model to guess what's missing by just looking at the nearby patches. Finally, having very few patches allows us to design a more efficient system for processing the image, which we'll explain next. Finally, unmasked patches (25% of the original image) will be used as input for a ViT encoder."}, {"title": "2.3. Masked Decoder", "content": "After the encoding phase, we have two sets of information: encoded data for the visible patches and special \"mask tokens\" [14]. Such mask tokens represent missing patches that the model needs to predict. Positional encoding is then added to both the encoded patches and mask tokens.\nThis information is passed through another series of Transformer blocks, acting as a decoder. The decoder is only used during training to learn how to fill in the missing patches. It does this by predicting the actual pixel values for each masked area. The decoder's output is a vector of pixel values representing a patch, and the final step is to put all these patches back together to form a complete reconstructed image. To measure the goodness of the reconstruction process we compare the reconstructed image with the original one pixel by pixel. We use the mean squared error (MSE) to calculate this difference."}, {"title": "2.4. Dataset Generation", "content": "Our ultimate goal is to distinguish tumor tissue from healthy tissue and further classify tumor subtypes. This requires the model to learn informative representations of the tissues. To achieve this, we randomly extract a large number of unlabeled image regions from each WSI. This process aims to capture a diverse pool of tissue regions encompassing both tumor and non-tumor areas.\nThe steps for extracting an image from WSI are listed below:\nRegion Selection. A square-shaped image patch is chosen from the whole slide image (WSI). The side length of the region is determined by sampling from a normal distribution. The mean and standard deviation of this distribution is calculated based on the size statistics of previously annotated ROIs in the BRACS dataset.\nRegion Quality Control. The average variation is computed for the pixel intensities within the extracted patch. This is a measure of dispersion relative to the mean. If the average variation is greater than a predefined threshold, the region is considered informative and included in the dataset. This step aims to exclude regions from uninformative zones, such as borders, which often exhibit low variability with predominantly white or black pixels."}, {"title": "2.5. Annotated ROIs classification", "content": "This task investigates the ability of a pre-trained model to classify annotated regions of interest (ROIs) within whole slide images (WSIs) for tissue type classification. The approach is divided in two-steps: first, ROIs are fed into the pre-trained model, which utilizes MAE to generate informative feature vectors for the patches. Subsequently, a mean aggregation of these patch-level embeddings is performed in order to obtain a single vector for every ROI. These embeddings capture the essential characteristics learnt by the model from the data. Secondly, in order to carry out the classification task, we resort in a MLP with one hidden layer. The MLP utilizes the ROI embeddings as input features, where each class corresponds to a specific tissue type present within the ROIs. This approach serves as an evaluation of the representativeness and discriminative power of the unsupervised embeddings generated by the pre-trained model. By successfully classifying different tissue types solely based on the embeddings, we can demonstrate that the learnt features effectively capture relevant biological information within the ROIs."}, {"title": "3. Experiments", "content": "During the experimental phase we applied our method to breast cancer subtype classification using histopathological hematoxylin and eosin (H&E) tissue images. Following the BRACS group's categorization (see Figure 3), we designed two multi-class classification experiments. The first experiment differentiates cancerous from non-cancerous tissues, additionally incorporating an intermediate class for atypical cases.\nThe second experiment extends the classification by attempting to categorize the tissue into seven distinct subtypes. Furthermore, we explore the utilization of a MAE network. Trained to reconstruct the image without prior labels, the model generates embeddings that will be analyzed.\nWe will evaluate the clustering behavior of these embeddings in latent space and utilize attention maps to qualitatively assess the image regions on which the model focuses. The results will be discussed in section 4.\nGeneralization experiment. We investigated the generalization capabilities of MAE embeddings for breast cancer histopathology image classification. The BACH dataset [10], containing 400 breast cancer histopathology images classified into four categories, was used for this evaluation. The pre-trained MAE model's encoder was used to extract feature representations (embeddings) from the BACH dataset images. Subsequently, a simple linear classifier was trained on the four classification labels specific to the BACH dataset.\nExperimental setup. The datasets were split into training, validation, and test sets using an 70/10/20 split. To account for potential variability, each classification experiment was run 100 times, and the average value for each chosen metric was reported. The MAE training was performed on a single Nvidia A40 GPU and took 32 hours."}, {"title": "3.1. Cancer classification", "content": "This experiment evaluated our model's ability to classify tissue samples as cancerous or non-cancerous. Additionally, the model assigned ambiguous cases to an 'atypical tissue' class. For a standardized comparison, all models employed a ViT-S/16 encoder architecture. This facilitated a comprehensive evaluation against various baseline models, leveraging the groundwork established in [15]. These models use either an attention mechanism or a ViT to correlate different region of a tissue image. They then classify the tissue by considering the relationships between these regions (the first two in the table perform the maximum and the average between the image patches respectively). The model's performance, measured by F1-score and AUC, is presented in Table 1."}, {"title": "3.2. Sub-type Cancer classification", "content": "This experiment seeks to develop a model capable of classifying diverse cancer subtypes within the regions of interest. This task presents a heightened challenge compared to prior endeavors due to the often subtle morphological distinctions between certain cancer types. Additionally, the inherent heterogeneity of the training data, encompassing a significant proportion of non-tumor tissue secondary to the random extraction process, introduces an inherent class imbalance within the latent representation space. We have evaluated performance of various models based on the weighted F1 metric for a 7-class classification task (Table 2).\nPrevious exposure to a larger proportion of non-cancerous tissue during the training phase appears to have influenced the model's prediction distribution. This is reflected in the superior F1-score achieved for the \"normal\" class compared to both the benchmark model and other classes within this investigation (see Table 3). However, performance on the remaining classes, particularly the \"ic\" class with the highest F1-score, remains comparable to previously tested models. These observations suggest that despite a potential bias towards non-cancerous tissue introduced by the training data, the model retains the ability to discriminate effectively between different cancer sub-types."}, {"title": "3.3. Generalization capabilities", "content": "To evaluate the representational capacity of our model, we have investigated its ability to generalize to data coming from completly unseen WSIs. This has been achieved by assessing its performance on a classification task using a dataset (BACH) not included in the training phase. The BACH dataset contains ROIs from tumor and non-tumor tissues, further categorized into four distinct classes. For consistent evaluation and to isolate the classification performance, the embeddings are fed into a linear classifier. Results are shown on Table 4."}, {"title": "3.4. Qualitative Evaluation", "content": "This section dives deeper into the quality of the learnt representations in the MAE model. We have employed t-SNE dimensionality reduction to visualize the embeddings in a 2D latent space, as shown in Figure 3. Each embedding is plotted and colored according to its corresponding class label. Interestingly, even though the model was trained unsupervised, some degree of class separation is already evident.\nNext, we have analyzed the attention maps, a crucial component of the Transformer architecture. These representations reveal which parts of the input image the model focuses on the most. By visualizing them, we can understand which image regions are most relevant for the model's predictions. Figure 4 showcases four original images alongside the attention maps generated by the final layer of the MAE. Notably, even during unsupervised training, the model appears to differentiate between connective and glandular tissue. This distinction likely arises because glandular tissue is structurally more complex, requiring the model to retain more information for reconstruction."}, {"title": "4. Discussion and future works", "content": "In this work we presented an autoencoder architecture using a Vision Transformer (ViT) as the embedding module. This model effectively generates informative representations of input images by masking random regions and reconstructing the masked areas in a self-supervised learning setting. Applied to histopathological breast cancer images, the model successfully captures relevant features from both tumor and non-tumor regions. These learnt representations were then utilized as input to a classification model, achieving accurate cancer subtype identification.\nOur model exhibits performance on par with current state-of-the-art techniques, consistently achieving second or third place in the conducted benchmarks. Notably, the latent space of the MAE is optimized solely for image reconstruction, and not specifically for classification tasks unlike the benchmark models. This observation strengthens the positive outcomes and underscores the efficacy of employing random input masking. Furthermore, the model's generalization capabilities suggest the potential for being applicable across diverse breast cancer datasets.\nDue to the high spatial resolution of whole slide images, data augmentation can be easily reachable. By extracting a larger number of random regions from each WSI, the dataset size can be significantly expanded while minimizing redundancy within the generated images. This step aims to achieve a more balanced representation of cancerous versus non-cancerous tissue within the training data without the need for large labeled datasets.\nIn future work, we will explore the impact of expanding the training dataset to enhance the model's ability to differentiate cancerous and non-cancerous regions. Additionally, to assess the model's generalizability, we propose training it on a collection of diverse datasets. Finally, we also want to explore more in depth the dataset augmentation given by the random cropping of the WSIs by experimenting with more sophisticated methods that could potentially improve the model performance even more."}]}