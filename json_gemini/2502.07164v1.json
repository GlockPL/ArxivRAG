{"title": "Does Training on Synthetic Data Make Models Less Robust?", "authors": ["Lingze Zhang", "Ellie Pavlick"], "abstract": "An increasingly common practice is to train large language models (LLMs) using synthetic data. Often this synthetic data is produced by the same or similar LLMs as those it is being used to train. This raises the question of whether the synthetic data might in fact exacerbate certain \"blindspots\" by reinforcing heuristics that the LLM already encodes. In this paper, we conduct simulated experiments on the natural language inference (NLI) task with Llama-2-7B-hf models. We use MultiNLI as the general task and HANS, a targeted evaluation set designed to measure the presence of specific heuristic strategies for NLI, as our \"blindspot\" task. Our goal is to determine whether performance disparities between the general and blind spot tasks emerge. Our results indicate that synthetic data does not reinforce blindspots in the way we expected. Specifically, we see that, while fine-tuning with synthetic data doesn't necessarily reduce the use of the heuristic, it also does not make it worse as we hypothesized.", "sections": [{"title": "Introduction and Related Work", "content": "Constructing a dataset for a specific task in natural language processing can be costly in terms of time and labor. An increasingly common approach to solve this problem is to take advantage of large language models (LLMs) to generate training data. It's simple to fine-tune an LLM or just use in-context learning to generate huge amounts of training data with a relatively small number of demonstrations. However, how effective the model-written datasets are for different tasks is still an open question.\nModel-generated training data is widely used in different domains like image classification (Besnier et al., 2020; Gowal et al., 2021), visual language concepts understanding (Cascante-Bonilla et al., 2023) and medical image understanding (Fernandez et al., 2022). In many NLP tasks, such as commonsense reasoning (Yang et al., 2020), question-answering (Bartolo et al., 2021; Paranjape et al., 2022), sycophancy reduction (Wei et al., 2024), cultural debiasing (Li et al., 2024a,b) and general instruction alignment (Wang et al., 2023), synthetic data created with generative models are utilized in model training. In cases where there are limited sources for model training, synthetic data would greatly benefit the performance of finetuned model. High-quality model-written datasets may also be used for evaluations. Perez et al. (2023) created 154 evaluation datasets and discovered inverse scaling of language models in some scenarios.\nHowever, synthetic data may also be harmful. Shumailov et al. (2024) found that language models may collapse if recursively finetuned with generated text. Such degradation has also been discovered in image generation tasks (Alemohammad et al., 2024). The use of synthetic data is also criticized from the perspective of ethics and social impact (Susser and Seeman, 2024). There's a series of research about what bias is manifested in synthetic data and how the performance in specific tasks is affected. For example, gender stereotype is a common kind of bias amplified in data generated by language models (Kirk et al., 2021; Kotek et al., 2023a). Li et al. (2023) investigated the text classification task and showed that subjectivity is a matter affecting the performance of models trained with synthetic data. Bisbee et al. (2024) found less variation in ChatGPT responses than in the real ANES survey. Similarly, a study from Chen et al. (2024) indicates that the uniform format of synthetic data can lead to pattern overfitting and thus harm the instruction-following capabilities of the model trained with it. Seddik et al. (2024) reveals that the recursive training loop makes the tails of the original distribution disappear and makes the model forget the real distribution from a statistical perspective.\nOne particular way in which synthetic data might be harmful is if it reinforces ungeneralizable heuristics. It is well know that LLMs often rely on features that perform well on the training set but do not necessarily generalize as we would like, for example, relying on gender bias (Kotek et al., 2023b), word-overlap bias in NLI (Rajaee et al., 2022), or exhibiting a preference toward longer responses in text generation (Singhal et al., 2024). We refer to these types of heuristics as blindspots.\nIn this work, we hypothesize that, because synthetic data less diverse than the original training data (Whitney and Norman, 2024), it is more likely to have blindspots and thus that fine-tuning on model-generated data will exacerbate these blindspots in the tuned model. In particular, we hypothesize that the synthetic data will encode the heuristic to a larger extent that would naturally occurring data, and thus that fine-tuning on synthetic data will lead the model to more strongly favor the heuristic. This weakness would be revealed in data that specifically is designed to test whether models are using the heuristic, as models trained on synthetic data might still show improved performance on generic test sets on which the heuristic performs well.\nAs a case study, we focus on the natural language inference (NLI) task evaluated with the MultiNLI dataset (Williams et al., 2018). The MultiNLI dataset covers general examples collected from various sources, but models trained on MultiNLI may tend to make judgments based on superficial syntactic properties and perform badly on HANS, an adversarial dataset created with syntactic heuristics (McCoy et al., 2019). The HANS task can be regarded as a measure of the model's \"blindspot\".\nOur expected result is that finetuning an NLI model with synthetic MultiNLI-like data will reduce its performance on HANS while improving its performance on the MultiNLI test set. However, we observed that this is not a consistent pattern under various settings of starting point model and size of synthetic dataset, though some biases do exist in the synthetic dataset. Our hypothesis is thus not fully supported by the experimental results. We have nonetheless discovered different patterns of performance change on both test sets in different scenarios. We hope the discovered insights will foster novel research ideas in understanding model degradation with synthetic training data and advancing fairness and robustness of language models."}, {"title": "Methods", "content": "We assume a task model T and a generator model G. T can be a model for any kind of NLP tasks, and G is a language model used to generate training examples for T. Let $X_T$ denote the set of all possible input of model T. The existence of a blindspot means that there's a non-random subset $X_T \\subseteq X_T$ on which the model T performs worse than on $X_T$ in general. Let $D_G$ denote the synthetic dataset generated by G, and $T_{D_G}$ denote the model fine-tuned on $D_G$. Our hypothesis is that $T_{D_G}$ will perform worse than T on $X_T$, but better than T on $X_T$."}, {"title": "Tasks, Models and Datasets", "content": "In this study, we focus on the natural language inference (NLI) task. An input example of this task contains a premise sentence, a hypothesis sentence, and a label indicating the relationship between the two sentences. The label can be one of {entailment, neutral, contradictory}."}, {"title": "Models and Input", "content": "Our experiments are based on the Llama-2-7B-hf model (Touvron et al., 2023). We fine-tuned a Llama-2 model with a classification head on top with MultiNLI as the task model T. The input sequence is constructed with the template\nPlease indicate the relationship between the premise and the hypothesis with entailment, neutral or contradiction. Premise: <premise> Hypothesis: <hypothesis> The relationship between premise and hypothesis is\nand the classification is based on the embedding of the last token in the input sequence. Our generator G is also a Llama-2-7B-hf model fine-tuned with MultiNLI. It's tuned to generate examples in the form of\nThis is an example where the relationship between the premise and the hypothesis is <label>. Premise: <premise> Hypothesis: <hypothesis> \u2013 This is the end of the example.\nThe label is put before the premise and the hypothesis for more flexible control of generated labels."}, {"title": "Datasets", "content": "We use MultiNLI (Williams et al., 2018) as a measure of the models performance on NLI in general. The original task model T and generator G are both Llama-2-7B-hf models finetuned on MultiNLI. To measure the presence of the \"blindspot\", we use HANS (McCoy et al., 2019). HANS is an NLI dataset created adversarially with three heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. Poor performance on HANS indicates that the model is likely using these heuristics to solve the NLI task.\nWhen training the task model T, we used the training set of MultiNLI as the training data, with 750 examples (250 for each label) excluded as the dev set. HANS is not used in training at all, but the results on its test set are reported. The maximum training set size for T is 391,722.\nNote that there are only two labels in HANS (because of how the dataset is constructed): entailment and non-entailment. In our experiments, the base task model and generator are fine-tuned with three labels of MultiNLI. When testing on HANS, predicted labels neutral and contradictory are both regarded as non-entailment."}, {"title": "Experiments", "content": "In our experiment pipeline, we first fine-tuned a classifier model T with the MultiNLI training set from the pretrained Llama-2-7B-hf model with a classification head. Then we fine-tuned another Llama-2-7B-hf model as the generator G, also with the MultiNLI training set. After training G, we generated a dataset $D_G$ with it and used $D_G$ to further fine-tune T to obtain the further tuned model $T_{D_G}$. We varied T (by changing the number of MultiNLI examples used for the initial fine-tuning) and $D_G$ for different settings."}, {"title": "Starting Models", "content": "The initial task model T is fine-tuned with data from the original MultiNLI dataset. In order to simulate task models in different stages, we trained 6 starting models with training set sizes of 0 (meaning the official pretrained model with a random classification head), 5000, 10000, 20000, 100000, and 391722."}, {"title": "Synthetic Datasets", "content": "The synthetic data examples are all generated by a Llama-2-7B-hf model G fine-tuned with the MultiNLI training set for 1 epoch. The generator model is fine-tuned to generate text in the specific format aforementioned with the following prompt:\nThis is an example where the relationship between the premise and the hypothesis is <label>\nWe kept the generated examples in which the premise and the hypothesis can be extracted with a regular expression without further filtering.\nWe generated 1,819,813 examples, which is more than necessary for the training. We sampled two kinds of synthetic datasets: uniformly random sampled datasets and showcasing datasets with a stronger bias. We took the lexical overlap (LO) heuristic addressed in the HANS dataset as an example. Lexical overlap means all the words in the hypothesis appear in the premise.\nBased on the availability of synthetic data, we constructed synthetic training sets of three sizes: 73080, 36040, and 18020. In each synthetic set, there are equal numbers of examples with each label. The random synthetic dataset (marked as Synthetic) is uniformly sampled for each label, and the more strongly biased dataset (Biased Synthetic) is sampled to make sure all entailment examples follow the lexical overlap heuristic and all other examples do not. We also included baseline datasets sampled from the original MultiNLI training set of the same sizes, marked as Original. The datasets used in the experiment can be represented as {73080, 36040, 18020} \u00d7 {Original, Synthetic, Biased Synthetic}."}, {"title": "Test Sets", "content": "We report our results on three test sets: the MultiNLI Matched test set, the HANS test set, and the subset of the HANS test set with lexical overlap and a non-entailment label, which reflects the model's performance specifically on the blind spot. In addition to the augmented model with different training sets, we also report the classification performance of each starting model."}, {"title": "Results", "content": "Our main results are reported in Figure 1. Each subplot corresponds to a different starting model T. When starting with undertrained task models, further fine-tuning with synthetic data will improve the performance on the MultiNLI Matched test set. The amount of improvement is on par with the model fine-tuned with original MultiNLI training data if the training set size is large enough. For relatively well-established starting models, neither fine-tuning with original nor synthetic data would significantly improve the performance of MultiNLI.\nThe performance on the HANS test set is trickier. The hypothesized trend, in which the performance of HANS goes down while the performance of MultiNLI goes up, only happens in for the 20K starting point. We also see a fairly sizable drop in HANS performance for the 392K starting point, but the curve is not monotonic and thus it is inconclusive. Overall, under most settings, further fine-tuning with original MultiNLI data would always benefit more or harm less on HANS performance than synthetic data. The gap does exist, but may not be as serious as expected.\nAs a sanity check, we also trained the model with the biased synthetic dataset in which all examples with lexical overlap are labeled entailment, and no example with neural or contradiction label satisfies the lexical overlap heuristic. As expected, such models perform worst in almost all tests, with the accuracy on the HANS subset of lexical overlap heuristic and non-entailment label dropping significantly towards zero over training. This indicates that a very biased synthetic dataset could exacerbate blindspots as expected, and thus implies that true synthetic data does not overrepresent the heuristic as much as hypothesized."}, {"title": "Conclusion", "content": "From the simulated experiments, we observed that while training the task model with synthetic data contributes to the performance on the general tasks almost equally compared with training with the original data, the contribution gap on the \u201cmore difficult\" blindspot task does exist. Under some settings, there's a dispersion where the accuracy on the blindspot task goes down while the general task accuracy goes up, but this is not a consistent tendency. Reinforcement of bias while training may happen, but this would probably not cause significant issues if we just use the unfiltered synthetic data for training."}, {"title": "Limitations", "content": "We need to note that the study with MultiNLI and HANS is a case study addressing the issue of bias reinforcement when training models with synthetic data. It's still an open question whether the results about the biases we are studying are generalizable to other cases. According to Table 1, lexical overlap is more common in the synthetic dataset than in MulitNLI for all labels, which may indicate that synthetic data is less diverse. However, the correlation between lexical overlap and entailment label is just slightly stronger. Different kinds of bias can emerge in very different ways in synthetic data, and this makes it challenging to evaluate the effect of training models with synthetic data holistically.\nOur design choices about the experiments may also be arbitrary. The task model we choose is the Llama-2-7B-hf model with a classification head. The pretrained Llama model is a relatively strong model, while the initialization of the classification head is random. Whether jointly training these parts is a reasonable choice is still arguable. Moreover, the two-step approach of model training is also not the only choice. It's also common to mix the original and synthetic data in different ratios and train the model with the mixed dataset in one run. Varying the experiment design is also necessary for further validations about the findings in this study.\nAnother notable point is that pretrained large language models, such as Llama, encode a wealth of world knowledge. Many potential biases may have been addressed in the training process. On the other hand, human created or audited data are not inherently free from implicit biases. The more concentrated distribution and reduced diversity of synthetic data might reinforce biases in certain blindspot scenarios. However, the rich world knowledge embedded in the generator model can also address some biases, potentially outperforming humans in certain cases and contributing positively to bias mitigation. A critical direction for future research is to disentangle these two effects and assess the significance of each, thereby enhancing our understanding of the impact of training models with synthetic data."}]}