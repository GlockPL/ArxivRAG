{"title": "Tazza: Shuffling Neural Network Parameters for Secure and Private Federated Learning", "authors": ["Kichang Lee", "Jaeho Jin", "JaeYeon Park", "JeongGil Ko"], "abstract": "Federated learning enables decentralized model training without sharing raw data, preserving data privacy. However, its vulnerability towards critical security threats, such as gradient inversion and model poisoning by malicious clients, remain unresolved. Existing solutions often address these issues separately, sacrificing either system robustness or model accuracy. This work introduces Tazza, a secure and efficient federated learning framework that simultaneously addresses both challenges. By leveraging the permutation equivariance and invariance properties of neural networks via weight shuffling and shuffled model validation, Tazza enhances resilience against diverse poisoning attacks, while ensuring data confidentiality and high model accuracy. Comprehensive evaluations on various datasets and embedded platforms show that Tazza achieves robust defense with up to 6.7\u00d7 improved computational efficiency compared to alternative schemes, without compromising performance.", "sections": [{"title": "1 INTRODUCTION", "content": "Federated learning is a transformative paradigm that facilitates training machine learning models in decentralized environments while preserving the privacy of data stored on distributed devices [31]. In federated learning, clients perform local model training over several epochs and share model updates, rather than raw data, with a central server. The server then aggregates these updates across clients over multiple rounds, collaboratively building a robust global model. This decentralized approach is particularly promising for mobile and embedded computing systems [2, 46], enabling them to leverage vast amounts of locally collected data while delivering personalized models tailored to individual users [45].\nDespite its decentralized and privacy-preserving design, federated learning still faces significant privacy and security threats. Unlike centralized training, where data resides on a single server, federated learning relies on distributed, heterogeneous client data, leaving it vulnerable to malicious model updates that compromise system integrity [18]. Furthermore, honest-but-curious servers can analyze model updates to infer private client data, breaching confidentiality. These concerns are particularly critical in mobile and embedded systems, where sensitive data is abundant, and the software environment is vulnerable due to the ease of installing third-party apps. These threats are categorized into two primary attack vectors: integrity attacks, in which malicious clients undermine model robustness, and confidentiality attacks, where curious servers endanger data privacy.\nSpecifically, integrity attacks involve malicious clients disrupting federated learning by sharing tampered updates, such as injecting noise [18], training with incorrect labels [6, 20], or embedding adversarial patterns [4]. Without effective defense, these integrity attacks hinder global model convergence and performance. Existing solutions include filtering tampered updates using statistical priors [54] or employing robust aggregation [7, 9]. Furthermore, confidentiality attacks, where the server aims to infer client data from model updates, are typically countered by techniques like differential privacy [1, 16] or gradient pruning [57]. However, such methods often compromise global model convergence due to altered updates [23]. This paper aims to comprehensively address these challenges.\nWe emphasize that ensuring both integrity and confidentiality is critical in federated learning, particularly for mobile and embedded systems. With large-scale participation from distributed and often anonymous clients, it is unrealistic to assume that all participants act in good faith. Robust mechanisms are needed to defend against malicious clients that compromise the integrity of the system. Equally important is safeguarding sensitive data collected from mobile and embedded devices, such as health information [39], daily activities [19, 37, 38], or environmental metrics [48, 52] to maintain user trust and meet privacy regulations.\nHowever, addressing integrity and confidentiality simultaneously poses unique challenges. Confidentiality defenses, such as noise addition, obscure model updates to protect privacy but can inadvertently hinder the detection of malicious patterns for backdoor attacks or label poisoning. At the same time, integrity defenses identify and mitigate such threats but may conflict with confidentiality measures, complicating the design of robust systems that target both objectives.\nThis challenge is amplified by the non-i.i.d. nature of real-world data. The diversity of benign model updates makes it difficult to differentiate legitimate updates from malicious ones, especially when compounded by the inaccuracies introduced by confidentiality defenses. These trade-offs create significant barriers to ensuring security and privacy without degrading global model performance.\nDespite the critical importance of these issues, few approaches simultaneously address integrity and confidentiality threats while preserving model performance. Tackling this intertwined challenge is imperative for real-world federated learning applications, where system reliability and user trust depend on effective, balanced defense mechanisms.\nIn this paper, we address the challenges of ensuring both integrity and confidentiality in federated learning by leveraging the permutation equivariant and invariant properties of neural networks (c.f., Sec 2.2). Permutation equivariance ensures that a function's output changes consistently with the order of its inputs, while permutation invariance guarantees the output remains unchanged regardless of input order. These properties are fundamental to widely used neural network architectures, including multilayer perceptrons (MLPs), recurrent neural networks (RNNs), and Transformers.\nLeveraging these properties, we propose Tazza, a novel approach that simultaneously ensures confidentiality and integrity in federated learning. Specifically, by shuffling neural network model parameters, Tazza utilizes the permutation equivariance property to obscure sensitive data, enhancing confidentiality. Meanwhile, permutation invariance ensures that the shuffled parameters produce consistent computational outcomes, thereby maintaining integrity. This approach effectively demonstrates how permutation-based techniques can address fundamental security issues in federated learning without sacrificing model performance.\nNevertheless, properly shuffling model parameters while preserving desired properties is challenging, as it requires a deep understanding of neural network operations. While many architectures exhibit permutation equivariant and invariant properties, the mechanisms enabling these properties vary across models. To address this, we present tailored algorithms for representative architectures, offering a systematic approach to harness these properties. Our methods demonstrate the practical potential of permutation-based techniques in federated learning.\nBuilding on the aforementioned properties, Tazza introduces a novel weight shuffling module, designed to obscure the original model by rearranging its weights while maintaining permutation equivariant and invariant properties. This ensures that the shuffled model remains valid, enabling us to propose a shuffled validation process for integrity checks, confirming that model outputs remain accurate despite weight shuffling. Finally, we leverage the similarity metrics obtained from the shuffled validation process and cluster the models to distinguish and isolate malicious updates from impacting benign models.\nBy integrating these components, Tazza emerges as a secure, privacy-preserving, and accurate federated learning framework that effectively addresses both integrity and confidentiality challenges. Along with a strong mathematical foundation leveraging permutation properties, we demonstrate its practical efficacy through extensive evaluations on diverse datasets and embedded platforms. Specifically, Tazza maintains high model accuracy while mitigating both integrity and confidentiality attacks, with a computational speedup of up to 6.7\u00d7 compared to alternative defense schemes.\nOur key contributions are summarized as follows:\n\u2022 Building on mathematical evidence, we propose an effective and efficient model-securing mechanism tailored for federated learning scenarios. This mechanism leverages the permutation equivariance and invariance properties of neural networks through two key components: Weight Shuffling and Shuffled Model Validation.\n\u2022 By seamlessly integrating Weight Shuffling and Shuffled Validation, we present Tazza, a privacy-preserving and secure federated learning framework specifically designed for mobile and embedded scenarios, achieving robust security without compromising model performance.\n\u2022 We conduct an extensive evaluation of Tazza across diverse datasets, model architectures, and attack scenarios. Our results demonstrate that Tazza effectively isolates malicious model updates from benign ones to counter integrity attacks, while its model parameter shuffling mechanism robustly mitigates confidentiality attacks."}, {"title": "2 BACKGROUND AND RELATED WORKS", "content": "We begin by discussing background information on the two core security threats in federated learning: integrity and confidentiality attacks."}, {"title": "2.1 Security Threats in Federated Learning", "content": "Integrity Attacks. Malicious clients can undermine the integrity of the federated learning process by compromising the reliability of model weights. Figure 1 (a) illustrates how such attacks operate. For instance, in data poisoning attacks, adversaries manipulate local models with altered data-label pairs (e.g., label flipping) to degrade global performance. In model poisoning attacks, they tamper with model weights by injecting noise (noise-injection) or scaling specific parameters (scaling attack) to distort outcomes [18]. Additionally, backdoor attacks involve training models to produce undesired outputs when triggered by specific input patterns [4]. Collectively, these adversarial activities are referred to as integrity attacks in this paper.\nTo mitigate integrity attacks, prior research has proposed secure model aggregation techniques leveraging statistical priors or similarity-based approaches. Methods like Median [51] and Trimmed-Mean [54] aggregation exclude malicious updates by focusing on statistical measures rather than naive averaging, assuming that malicious updates are statistical outliers. Alternatively, similarity-based techniques such as Krum [7] use Euclidean distance to select updates with minimal deviation across models, while FLTrust [9] calculates trust scores based on cosine similarity with a verified, clean model stored at the server. Despite their strengths, these methods often discard valuable information in the updates, leading to suboptimal global model performance.\nConfidentiality Attacks. Another significant security threat in federated learning systems is confidentiality attacks, which aim to infer and reveal local training data at the federated learning server. Figure 1 (b) illustrates the workflow of these attacks. Zhu et al. introduced DLG, demonstrating that training data can be reconstructed by analyzing the differences"}, {"title": "2.2 Permutation Equivariance/Invariance", "content": "We now explore the concepts of permutation equivariance and permutation invariance, which are fundamental to the approach presented in this work.\nPermutation Equivariance refers to the property where the order of a function's output aligns with the order of its input. Figure 2 (a) illustrates this concept using matrix multiplication, where R represents a column-reversed matrix. Specifically, the figure demonstrates that when the column order of the input matrix X is reversed (X \u2192 XR), the resulting matrix multiplication outputs, WX (top) and WXR (bottom), differ only in the column order, which corresponds to the input's order.\nFormally, if a function f(\u00b7) satisfies permutation equivariance, for a given input X = {X1, X2, ..., XN } and a permutation function \u03c0(i) : {1, 2, . . ., N} \u2192 {1, 2, . . ., N}, where \u03c0(i) is a bijective mapping, the lemma \u03c0(f(X)) = f(\u03c0(X)) holds. Applying this to the case of matrix multiplication, the function f(X) = WX satisfies permutation equivariance with respect to the column order. Consequently, WXR is identical to (WX)R.\nPermutation Invariance describes a function that produces identical results regardless of the order of its input. For example, a function that computes the average of the elements in a given array is permutation invariant because the arrangement of elements does not influence the outcome. Figure 2 (b) illustrates this concept using the average operation in the context of matrix multiplication, where n represents the number of rows and R denotes a row-permuted version of the input. As shown, despite reordering the rows, the final result remains unchanged (i.e., E[X] = E[XR]). Without loss of generality, if a function f(.) satisfies permutation invariance, then for a given input X and a permutation function \u03c0(\u03af), the lemma \u03c0(f(X)) = f(\u03c0(X)) holds.\nIt is important to note that matrix multiplication, in general, is not a permutation invariant operation. Specifically, if the weights used in the multiplication are not uniform (e.g., $\\frac{1}{n}$ for all elements, as in the averaging operation shown in Figure 2 (b)), the result will differ depending on the order of the rows. This is because such weights can be interpreted as a weighted sum, being sensitive to the input arrangement.\nNeural Networks and Shuffling. Modern neural networks exhibit an intriguing property that achieves nearly identical training results even when the arrangement of elements in the input data is shuffled, demonstrating similar behavior to permutation invariance [34, 49]. Shuffling, here, refers to rearranging the elements of a data sample, such as altering the position of pixels in an image. Similarly, reordering ground truth labels has minimal impact on training outcomes, reflecting characteristics similar to permutation equivariance. However, it is important to note that neural networks exhibit a semi-permutation equivariance and invariance property, as results are not perfectly identical but still closely adhere to these principles. This property arises primarily from the reliance of neural networks on matrix multiplication for most of their operations [25, 55].\nLeveraging this property, neural networks can achieve similar training results with spatially shuffled data as with unshuffled data. Since shuffled data is semantically uninterpretable to humans, prior research has highlighted its potential to enhance system privacy. For instance, Amin et al. used permutation equivariance to secure medical image processing [3], while Xu et al. explored its applications in transformers for model encryption and privacy-aware split learning [53]."}, {"title": "3 FEASIBILITY STUDY AND CORE IDEA", "content": "We now present our preliminary studies, which demonstrate the feasibility of permuting data during neural network training from the perspectives of data privacy and model performance preservation. Building on the insights gained from these experiments, we introduce the core concept underlying our proposed framework Tazza.\nFeasibility Study. To evaluate the feasibility of leveraging shuffling methods for preserving data privacy, we measured the Peak Signal-to-Noise Ratio (PSNR) between original and shuffled samples in the MNIST dataset. For shuffling, we randomly rearranged the pixels in the input data following a consistent and fixed order applied uniformly across all samples. Figure 3 presents sample original/shuffled images and the distribution of measured PSNR values between 60,000 original and shuffled images. A high PSNR indicates structural similarity between two images, whereas a low PSNR suggests significant differences. As Figure 3 (a) shows, the shuffled samples do not retain the semantic information of the original images. Moreover, Figure 3 (b) shows that most samples exhibit low PSNR in the range of 6-8 dB. Even for the top-4 PSNR samples, the shuffled images remain semantically uninterpretable to human observers, confirming the effectiveness of shuffling in preserving data privacy.\nNext, we trained a 3-layer MLP model for 20 epochs using two dataset configurations: one with original data and the other with shuffled data, as exemplified in Figure 3 (a). For the shuffling process, the test data and training data were shuffled using the same order. Figure 4 presents the average validation accuracy for 10 different random seeds. As the plots show, the model achieves (nearly) identical accuracy regardless of whether the input data were shuffled,"}, {"title": "4 THREAT MODEL", "content": "This work considers two key stakeholders in federated learning scenarios: clients and the server. Both clients and the server can potentially be victims or act as adversaries. Thus, we address two different types of attackers namely integrity attacker, and confidentiality attacker.\nGoals and Capability First, an integrity attacker refers to a malicious client that aims to compromise the performance of the global model by injecting harmful information"}, {"title": "5 FRAMEWORK DESIGN", "content": "We now detail the design of Tazza, a secure and efficient federated learning framework that simultaneously addresses integrity and confidentiality attacks. Specifically, Tazza features two core modules, weight shuffling and shuffled model validation and exploits the permutation equivariance and invariance properties of neural networks."}, {"title": "5.1 Overview of Tazza", "content": "Figure 5 illustrates the overall workflow of Tazza: 1 The federated learning server distributes a global model to each client. \u2461 Clients selected to participate in training share a shuffling rule via a safeguarded peer-to-peer communication. As a communication-efficient approach, Tazza simply shares the random seed for generating the shuffling order (Sec 5.2). \u2462 Participants of the federated learning round train the neural network model with their locally collected data. \u2463 Upon completing local training, each participant shuffles its local model weights with respect to the shared rule to ensure the confidentiality of the training operations (Weight Shuffling-Sec. 5.3). 5 Participating clients transmit the shuffled model parameters and a small number of input samples (as small as one) to the server. We note that the input samples are also shuffled to prevent the confidentiality of the system from being compromised. \u2465 With the updated/shuffled models and data samples, the server performs an integrity check via shuffled model validation on the data samples using each client's model by measuring the similarity of resulting output vectors (Sec. 5.4). Since models and data samples are shuffled to satisfy the permutation invariance of the neural network, the resulting output vectors are still legitimate. \u2466 Exploiting the similarity between the clients calculated in the previous step, Tazza employs a clustering algorithm to cluster the clients. As malicious and benign models exhibit different computation results, grouping the clients based on their similarity effectively assures that benign clients are not affected by malicious model updates.\nTo summarize, in Step 4, the weight shuffling process ensures that by shuffling the weights, the server is not able to infer information about the client's local data through model analysis. Nevertheless, since weights are shuffled while preserving permutation invariance, the vectors calculated in Step 6 still remain valid. This enables the shuffled validation process to effectively identify malicious clients attempting poisoning attacks by clustering them in Step 7."}, {"title": "5.2 Shuffling Rule Sharing", "content": "Before delving into the detailed design of Tazza, we first address a potential issue regarding clients independently shuffling their model weights in differing orders, which would void the validation process and lead to degraded aggregated model performance. Therefore, it is important for clients to agree upon a unified weight-shuffling order. However, the challenge here is that if the server gains access to this unified shuffling rule, it could potentially revert the shuffled models to their original state, posing a security risk. To mitigate this, the exchange of shuffling rules must be conducted exclusively among the clients without server involvement.\nSecure peer-to-peer communication protocols, such as zero-knowledge proof [8] and Byzantine fault-tolerant methods [11], provide practical and well-established mechanisms for achieving this client-only data exchange. As noted earlier, we assume that clients leverage these secure communication methods to share shuffling rules amongst themselves, ensuring that the server remains uninformed of the shuffling specifics. This assumption supports the integrity and confidentiality of the proposed system.\nSpecifically, Tazza employs a Practical Byzantine Fault Tolerant (PBFT) mechanism [11] for exchanging its shuffling rules. The server selects one participating client as the head, which distributes a shuffling rule to others. Participants that receive the rule then broadcast the rule to non-participating clients to reach consensus. Even if a malicious client is chosen as the leader, PBFT ensures that consensus is achieved and the shuffling rule remains secure and consistent across all clients, despite potential attacks."}, {"title": "5.3 Weight Shuffling", "content": "The weight shuffling module in Tazza aims to prevent data leakage while preserving computational correctness by leveraging permutation equivariance and invariance. As the name suggests, this operation shuffles the neural network weights of a model updated at the client before sharing them with the server for aggregation. For clarity, we explain the weight shuffling process in the context of matrix multiplication, given its predominant role in neural network computations. It is worth noting that implementation details of this module can vary depending on the baseline federated learning model architecture due to differences in design and computational complexity, as discussed in Section 6.\nIn essence, weight shuffling in Tazza consists of two core operations: row shuffling and column shuffling. Consider a matrix multiplication between the input matrix $X \\in \\mathbb{R}^{N \\times d}$ and the weight matrix $W \\in \\mathbb{R}^{d \\times h}$, where N, d, and h denote the number of vectors, input dimension, and embedding dimension, respectively. To shuffle the weight rows, a row-permutation matrix R can be applied to W (i.e., R \u00d7 W). This operation is computationally equivalent to directly shuffling the input, thereby ensuring data confidentiality. Furthermore,"}, {"title": "5.4 Integrity Check", "content": "Integrity check in Tazza is designed to identify and isolate malicious models from benign ones through two key steps: shuffled model validation and cluster-aware aggregation."}, {"title": "5.4.1 Shuffled Model Validation", "content": "In the Shuffled Model Validation process, the server utilizes client-provided samples as inputs to the uploaded models. As previously mentioned, these data samples are also shuffled to obscure their semantics, ensuring data confidentiality. Despite this obfuscation, the corresponding weight shuffling maintains computational validity, allowing the server to trust the outputs based on the assumed correctness of the shuffle rules. If an attacker uploads models with mismatched data and weight shuffle orders, inconsistent results will emerge, distinguishing these models from legitimate ones. This forces attackers to comply with the correct shuffle rules to avoid exclusion during subsequent stages.\nMalicious models produce outputs that deviate from those of benign models, revealing distinctive patterns. Tazza identifies such deviations by computing pairwise cosine similarity among the output vectors of all models [40]. Malicious models typically exhibit lower cosine similarity with benign ones, providing a basis for their classification and isolation.\nClients are then clustered based on their pairwise cosine similarities, with the similarity matrix (S) transformed into a distance matrix (1 \u2013 S) to interpret similarity as a distance metric, which is used as input to the DBSCAN algorithm [17]. Tazza leverages DBSCAN for its ability to identify outliers and its independence from predefined cluster numbers, making it particularly well-suited for federated learning scenarios where the presence of attackers is unknown."}, {"title": "5.4.2 Cluster-aware Aggregation", "content": "Finally, we implement cluster-aware aggregation to fully segregate malicious models from benign ones. In each federated learning round, computed cluster labels serve as pseudo-labels, reflecting the similarities among clients within that round only. These labels are independent across rounds (e.g., labels from round t have no relation to those from round t + 1). To ensure consistency, Tazza assigns global cluster labels by leveraging clustering histories, enabling effective cross-round clustering. Clients are categorized into two groups: (1) those participating for the first time, who lack a global cluster label, and (2) those with prior participation and an assigned global cluster.\nUpon performing shuffled model validation, if all clients in a cluster are first-time participants without any global cluster assignment, Tazza creates and assigns new global clusters. Conversely, if a pseudo-cluster contains clients with pre-existing global cluster labels, all clients in the cluster are assigned to the global cluster label of those clients.\nFurthermore, if clients who were previously part of different global clusters are grouped in the same pseudo-cluster during the current round, Tazza merges their global clusters. However, instead of merging all clients from both clusters indiscriminately, Tazza adopts a conservative approach by reassigning only the overlapping clients to the new cluster. This precautionary strategy ensures that a single attacker infiltrating a benign cluster cannot lead to all attackers being included in the same benign cluster.\nWhen global clusters are assigned for participating clients, Tazza performs aggregation within each cluster based on the clustering results, producing an updated model for each cluster. We note that Tazza employs averaging as the aggregation algorithm while alternative methods such as median or trimmed mean can also be applied, offering flexibility in system design. By aggregating models only within the same cluster, Tazza prevents compromised models from contaminating the global model shared across other clusters.\nThis cluster-specific aggregation ensures that the integrity of the federated learning system is maintained, as malicious models cannot propagate their impact to benign clusters [30]. Furthermore, by leveraging shuffled model validation, Tazza achieves this robust defense without exposing or interpreting the semantics of clients' data, thereby preserving both system security and data confidentiality. Overall, this dual protection reinforces Tazza's effectiveness in safeguarding federated learning processes against security threats."}, {"title": "6 IMPLEMENTATION", "content": "As previously noted, details of the weight shuffling operation (and implementations) can vary for different model architectures. In this section, we provide a detailed explanation of the weight shuffling process for representative architectures."}, {"title": "Multi-layer Perceptron (MLP)", "content": "As a simple architecture, we demonstrate the weight shuffling operation in an MLP. In an MLP with L layers, the output y is computed as follows:\n$h^{(0)} = x$,\n$h^{(l)} = \\sigma(W^{(l)}h^{(l-1)} + b^{(l)}), l = 1, . . ., L - 1,$\n$y = W^{(L)}h^{(L-1)} + b^{(L)},$\n(1)\nHere, $h^{(l)}$ denotes the activations of the l-th layer, $W^{(l)}$ represents the weight matrix associated with the l-th layer, $b^{(l)}$ is the bias vector for the l-th layer, and $o(\u00b7)$ is a non-linear activation function such as ReLU, sigmoid, or tanh.\nA critical observation is that commonly used activation functions are parameter-free and permutation-equivariant, meaning their intrinsic behavior remains unaffected by input shuffling. As a result, activation functions can be excluded from considerations in weight shuffling operations. This leaves the linear transformations in each layer, represented as matrix multiplications $(W^{(l)}h^{(l\u22121)})$, as the primary focus. The weight matrices and bias vectors in these transformations can be shuffled according to the previously outlined strategies, ensuring that computational outcomes remain consistent despite weight shuffling. By applying this shuffling mechanism iteratively across layers, the entire MLP architecture retains compatibility with weight shuffling operations, preserving computational integrity while safeguarding model confidentiality."}, {"title": "Sequential Models (RNN and LSTM)", "content": "For recurrent sequential models such as RNNs and LSTMs, the weight shuffling process is similar to that of MLPs, with one key distinction: the inclusion of hidden state operations. These operations enable sequential models to capture and process temporal dependencies in input data. Figure 6 illustrates the fundamental operation of a recurrent model (e.g., RNN), where the next output is computed by integrating the current"}, {"title": "Transformers", "content": "Transformers, a widely used modern neural network architecture, operate in three main steps, as depicted in Figure 7. First, the input is segmented into a sequence of tokens, which are passed through a linear embedding layer to generate token embeddings. These embeddings are then processed by multiple encoder layers that analyze the input. Each encoder employs a multi-head attention mechanism comprising three learnable matrices: $W_Q$ (query), $W_K$ (key), and $W_V$ (value). Finally, the encoded feature representation, Z, is fed into the classification head (i.e., CLS Head) to produce the final output. Importantly, the encoded feature Z preserves the order of the input tokens, ensuring the model's output aligns with the input sequence.\nFrom a data perspective, shuffling can be applied at two levels: intra-token and inter-token. At the intra-token level, individual elements within a token are permuted. At the inter-token level, the sequence order of tokens is rearranged. Importantly, since all tokens pass through a linear embedding layer-essentially a matrix multiplication operation-the permutation invariance property of matrix multiplication ensures that identical computational results can be achieved by shuffling the embedding weights accordingly.\nFor inter-token shuffling, the sequential order of tokens is initially preserved during the attention mechanism (e.g., row order e\u2081, e2, . . ., en), where spatial or sequential positions are learned and encoded. However, in the final CLS Head, these positional features determine the model's predictions. Consequently, inter-token shuffling can be achieved by appropriately shuffling the weights of the classification head, ensuring consistent results while altering the token order.\nWe note that operations beyond the matrix multiplications discussed above (e.g., layer normalization, softmax) exhibit permutation equivariance (e.g., the softmax function) or permutation invariance (e.g., mean and variance computations in layer normalization). Consequently, these operations are unaffected by weight shuffling, ensuring that the computational results remain consistent. This allows Tazza to uphold not only the confidentiality of client data but also the integrity of the system."}, {"title": "7 EVALUATION", "content": "Now we evaluate Tazza with thorough and extensive experiments with four datasets and various comparison baselines."}, {"title": "7.1 Experiment Setup", "content": "Dataset and Model. We evaluate Tazza using four datasets with corresponding model architectures suitable for mobile federated learning scenarios as we detail below.\n\u2022 MNIST [14] dataset consists of 60,000 training and 10,000 test grayscale handwritten digit images with 28\u00d728 pixel resolution. We use a 3-layered MLP [43] for evaluating MNIST.\n\u2022 CIFAR10 [27] is a benchmark dataset consisting of 60,000 32\u00d732 pixel color images, categorized into 10 distinct classes. For this dataset, we use the MLP-Mixer [49] as the model, a design similar to the Vision Transformer but substituting self-attention blocks with MLP-based layers.\n\u2022 STL10 [13] consists of 13,000 natural images across 10 classes, with a resolution of 96\u00d796 pixels. Its higher resolution makes it ideal for assessing Tazza's scalability with image resolution. For this dataset, we employ Vision Transformer [15] to manage the increased complexity.\n\u2022 MIT-BIH Arrhythmia Database [32, 33] is a widely used electrocardiogram (ECG) benchmark dataset comprising approximately 70K five-second ECG signals. We implement an RNN [44] to classify three arrhythmia types along with normal cardiac patterns. This dataset is utilized to explore the potential extension of Tazza to time-series inputs.\nBaselines. For confidentiality attacks, we evaluate three baseline defense mechanisms: no defense, gradient pruning [57], and differential privacy [1, 16]. Gradient pruning zeros out small-amplitude gradients to reduce data leakage but requires a carefully balanced pruning ratio; higher ratios mitigate leakage but risk discarding useful knowledge, impacting both performance and convergence. Differential privacy, implemented via DPSGD [1], adds perturbation to gradients to obscure sensitive information, with perturbation levels divided into small, medium, and large, as in prior work [1]. Similar to gradient pruning, differential privacy also requires careful tuning of perturbation levels to balance between accuracy and privacy.\nFor integrity attacks, we evaluate two baseline configurations: FedAvg [31] and Oracle, representing naive and ideal model aggregation scenarios, respectively. Specifically, FedAvg does not employ security mechanisms, leaving it vulnerable to malicious updates, while Oracle assumes perfect knowledge of attackers, with the capability of fully excluding malicious models' contributions on the global model.\nWe also evaluate robust aggregation methods leveraging statistical priors, including Median and Trimmed Mean [54]. Median replaces averaging with the median to mitigate extreme outlier influence, while Trimmed Mean excludes outliers before averaging, reducing the impact of malicious updates. Additionally, we compare Tazza with MultiKrum [7] and FLTrust [9]. MultiKrum selects updates with the smallest pairwise Euclidean distances after discarding the most distant ones. FLTrust assigns trust scores based on cosine similarity with a server-trained reference model, adjusting aggregation weights to suppress malicious updates.\nAttack Methods. For confidentiality attacks, we use the inverting gradient attack [21] for 100 test samples as a baseline. This method reconstructs training data by optimizing inputs to produce gradients similar to those sent by the client, achieving state-of-the-art performance with minimal additional assumptions [23].\nFor integrity attacks, we evaluate three poisoning attack types: label flipping, noise injection, and backdoor attacks [18]. In label flipping, the labels of a subset of training data are altered to mislead the locally trained model. Noise injection introduces random noise into the model parameters before they are sent to the server, aiming to degrade the global model's performance. Backdoor attacks embed specific triggers into the training data, causing the model to behave maliciously when exposed to those triggers.\nMiscellaneous Configurations. Throughout the evaluation, unless otherwise specified, datasets are distributed across 100 clients, consisting of 25 attackers, with data shared via a Dirichlet distribution with \u03b1=0.5 [22]. In each federated learning round, 25 participants are selected, and their models are trained locally for 5 epochs using the Adam optimizer [26] with a learning rate of 1e-3 and a batch size of 64. Besides the evaluations performed on real embedded computing platforms in Section 7.5, all experiments were conducted on a server with an NVIDIA RTX 2080 GPU, an Intel Xeon Silver 4210 CPU 2.20GHz, and 64GB RAM."}, {"title": "7.2 Overall Performance", "content": "To evaluate the overall performance of Tazza, we conducted experiments using the MNIST-MLP and CIFAR10-Mixer configurations with 75 benign clients and 25 label-flipping attackers (i.e., integrity attack), totaling 100 clients. To understand Tazza's confidentiality-preserving capabilities, the server executed an inverting gradient attack. The effectiveness of the attack was measured using Learned Perceptual Image Patch Similarity (LPIPS) [56], which quantifies semantic distance in image data. Lower LPIPS values indicate greater similarity between the original training samples and those reconstructed by the attack, signifying higher attack success.\nFigure 8 presents the global model accuracy under labelflipping attacks and the LPIPS across various integrity and confidentiality defense schemes. The legend indicates different integrity defense schemes, with triangle (\u25b2) and circle (\u2022) markers representing confidentiality defense schemes using gradient pruning and DPSGD, respectively. Gradient pruning is applied with pruning ratios of 70%, 90%, and 99%, while DPSGD uses three perturbation levels: small, medium, and large, as outlined in prior work [1]. Each scheme includes three plots: the leftmost plot corresponds to the weakest confidentiality defense configuration (70% pruning for gradient pruning and small perturbation for DPSGD), while the rightmost plot represents the strongest configuration (99% pruning for gradient pruning and large perturbation for DPSGD).\nExisting confidentiality defense mechanisms improve data privacy, achieving higher LPIPS values, but often result in reduced accuracy. In contrast, Tazza achieves both high LPIPS and high accuracy, demonstrating its robustness against integrity and confidentiality attacks. Notably, even under an idealized (Oracle) scenario where the server perfectly excludes malicious updates during aggregation, the model accuracy remains lower than that of Tazza, due to inaccuracies"}, {"title": "7.3 Confidentiality Attack Mitigation", "content": "To evaluate the efficacy of Tazza in mitigating confidentiality attacks, we measured PSNR and LPIPS between the original training samples and those inferred from the attack. PSNR assesses structural similarity, while LPIPS evaluates semantic distance for image data. Note that, as LPIPS is specific to images, only PSNR is reported for the MIT-BIH time-series dataset. Table 1 presents the average and best PSNR and LPIPS for various dataset and defense scheme configurations. The best values represent the most similar samples among the 100 used in the experiments. Higher PSNR and lower LPIPS indicate greater similarity between inferred and original samples.\nAs shown in Table 1, FedAvg without defense mechanisms results in high PSNR and low LPIPS, indicating substantial local data leakage. In contrast, defenses such as gradient pruning with 90% pruning (GP 0.9), DPSGD with medium perturbation (DP-M), and Tazza demonstrate competitive reductions in PSNR and increases in LPIPS, effectively mitigating confidentiality attacks. Notably, while gradient pruning and differential privacy face challenges in balancing model accuracy with confidentiality, Tazza successfully defends against attacks without compromising accuracy.\nFigure 9 presents the inferred training samples generated by a gradient inversion attack at the server. In the absence of a defense mechanism (FedAvg), the local training data are completely exposed, significantly compromising the confidentiality of federated learning systems. Methods such as DPSGD with large perturbation (DP-L), pruning 99% of the local gradient effectively mitigate the gradient inversion attack, preserving the privacy of the training data while sacrificing the model training performance as noted earlier. On the other hand, when employing relatively weak defense mechanisms such as gradient pruning with pruning ratios 70% and 90% (GP 0.7, 0.9) or DPSGD with smaller perturbations (DP-S, M), the inferred samples still reveal interpretable semantic information, though with reduced visual clarity, requesting the need for stronger privacy-preserving techniques. As Tazza's samples in Figure 9 show, our proposed approach effectively conceals semantic information embedded for both STL10-ViT and MIT-BIH-RNN configurations, suggesting its potential usage in high-resolution image and time-series data-based applications."}, {"title": "7.4 Integrity Attack Mitigation", "content": "In this section, we evaluate Tazza's robustness against various integrity attacks, specifically focusing on noise addition and backdoor attack scenarios. The experiments exploit the CIFAR10-Mixer configuration with medium-level DPSGD as the confidentiality defense mechanism for all baselines. For the backdoor attack, attackers assign specific labels to samples by injecting a 4\u00d74 red square patch into the upper-left corner of 50% of their training data as the backdoor trigger [28"}]}