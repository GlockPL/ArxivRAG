{"title": "CARBONCHAT: LARGE LANGUAGE MODEL-BASED CORPORATE CARBON EMISSION ANALYSIS AND CLIMATE KNOWLEDGE Q&A SYSTEM", "authors": ["Zhixuan Cao", "Ming Han", "Jingtao Wang", "Meng Jia"], "abstract": "As the impact of global climate change intensifies, corporate carbon emissions have become a focal point of global attention. In response to issues such as the lag in climate change knowledge updates within large language models, the lack of specialization and accuracy in traditional augmented generation architectures for complex problems, and the high cost and time consumption of sustainability report analysis, this paper proposes CarbonChat: Large Language Model-based corporate carbon emission analysis and climate knowledge Q&A system, aimed at achieving precise carbon emission analysis and policy understanding.First, a diversified index module construction method is proposed to handle the segmentation of rule-based and long-text documents, as well as the extraction of structured data, thereby optimizing the parsing of key information.Second, an enhanced self-prompt retrieval-augmented generation architecture is designed, integrating intent recognition, structured reasoning chains, hybrid retrieval, and Text2SQL, improving the efficiency of semantic understanding and query conversion.Next, based on the greenhouse gas accounting framework, 14 dimensions are established for carbon emission analysis, enabling report summarization, relevance evaluation, and customized responses.Finally, through a multi-layer chunking mechanism, timestamps, and hallucination detection features, the accuracy and verifiability of the analysis results are ensured, reducing hallucination rates and enhancing the precision of the responses.", "sections": [{"title": "Introduction", "content": "Global climate change is a widespread and profound systemic transformation of the economic and social systems, involving various sectors such as energy structure, industrial processes, transportation, as well as ecosystem carbon sinks, engineered carbon sinks, and more. In the context of current global climate change, sustainability issues have gradually evolved into a critical topic. Enterprises are facing management challenges in measuring, reporting, and verifying greenhouse gas emissions during their operations. The Greenhouse Gas Protocol(GHG Protocol) provides methods and standards for greenhouse gas accounting, enabling an accurate and fair reflection of an enterprise's carbon emissions. However, the lengthy corporate carbon emission reports and government policies related to carbon emissions (often exceeding 30 pages) pose challenges for enterprises, investors, and the general public in terms of understanding and analysis. At the same time, reliance on third-party rating agencies is not always effective, as the services provided by these agencies are often costly, lack transparency, and use differing standards when evaluating corporate carbon emission reports. This further increases the complexity and uncertainty of the evaluation process.\nWith the development of large language model (LLM) technology, deep learning models based on massive text data have demonstrated powerful language processing and generation capabilities. In 2022, OpenAI's ChatGPT[1], based on the GPT-3.5 architecture, made groundbreaking progress in general knowledge question answering. Subsequently, GPT-4 and its multimodal version, GPT-4V, further enhanced the model's capabilities. Meanwhile, Alibaba's Qwen[2]has also excelled in Chinese tasks, achieving performance comparable to the internationally leading GPT-4 and GPT-3 models. Despite significant advancements in the research field, large language models often struggle to provide precise answers when addressing domain-specific issues, due to a lack of domain knowledge or up-to-date data. Therefore, building an intelligent system based on large language models can more efficiently conduct data analysis and professional question answering, bridging the gaps in traditional question-answering systems."}, {"title": "Results", "content": "This paper proposes Large Language Model-based enterprise carbon emission analysis system and global climate change knowledge Q&A system, as shown in Figures 1 and 2. To address the issues of insufficient data referencing and integration of policies and regulations in carbon emission-related knowledge and regulatory searches, this paper proposes a diversified indexing module construction method. Through this method, a high-quality carbon emission knowledge and regulatory search dataset is built, significantly improving the accuracy and reliability of knowledge retrieval, and providing users with more comprehensive and accurate policy and regulatory references.\nAdditionally, this paper proposes a Self-Prompting Retrieval-Enhanced Generation (Self-Prompting RAG) architecture to improve the quality and authenticity of responses from large language models through retrieval and self-prompting. The architecture includes four modules: intent recognition (question classification), Structured chain-of-thought (COT) prompting, vector database hybrid retrieval, and Text2SQL system. By incorporating prompt engineering techniques, high-quality prompts are constructed to ensure seamless coordination among modules, enabling efficient semantic understanding and query transformation. The system can rapidly handle multi-dimensional complex issues while effectively enhancing the traceability and relevance of answers to the questions, reducing hallucination rates during the generation process, and further improving the overall performance of the system.\nThe enterprise carbon emission analysis system is based on the widely used international Greenhouse Gas Protocol (GHG Protocol) [3], and integrates modules for report summarization, relevance assessment, and customized question answering. The system conducts an in-depth analysis of enterprise sustainability reports across fourteen dimensions, including energy usage analysis, accurately quantifying their greenhouse gas emissions and generating structured and traceable analysis reports. It also combines carbon emission target setting and risk management to provide carbon emission assessments and predictions. The system automatically calculates the alignment of reports with the GHG Protocol and generates a compliance evaluation report, offering key references for enterprises to understand their report content and improve standardization. Through diversified indexing module construction, modality tagging, and hallucination detection features, the system significantly reduces the risk of hallucination generation, ensuring the accuracy and verifiability of results. Additionally, the system supports personalized question parsing and semantic search, allowing it to flexibly address complex scenarios and provide comprehensive support for enterprises to optimize their carbon emission analysis strategies."}, {"title": "In summary, the main contributions of this paper include:", "content": "\u2022 The paper proposes a diversified index module construction method and establishes a search dataset for carbon emissions-related knowledge and regulations. This approach addresses the issue of insufficient data in terms of referencing and integrating policy and regulatory knowledge, thereby enhancing the retrieval quality of carbon emissions-related knowledge.y enhancing the retrieval quality of carbon emission-related information.\n\u2022 The paper proposes a Self-Prompting Retrieval-Augmented Generation architecture, which integrates com-ponents such as intent recognition (question classification), question parsing, hybrid retrieval from vector databases, structured Chain-of-Thought (COT), and a Text2SQL system. This architecture achieves efficient se-mantic understanding and query transformation, enhancing the ability to handle complex queries and reducing hallucination rates in large models.\n\u2022 This paper proposes a Text2SQL system that utilizes Large Language Model to parse user queries. It generates SQL through step-by-step reasoning guided by Chain-of-Thought (COT) examples retrieved via Retrieval-Augmented Generation (RAG). The system combines semantic information with the database schema for pattern matching and semantic mapping, thereby generating structured SQL queries. Security validation is introduced to restrict high-risk operations, while automatic SQL repair and structural rewriting are employed to optimize the queries, ensuring both their safety and accuracy.\n\u2022 This paper develops a structured prompt engineering method based on the most widely used greenhouse gas accounting framework internationally, the GHG Protocol. The proposed method can automatically analyze corporate carbon emission reports across various dimensions, quantify their compliance with the international greenhouse gas accounting system, and implement benchmark testing and automatic measurement standards for disclosure quality."}, {"title": "Methods", "content": "Global Climate Change Knowledge Q&A System"}, {"title": "Diversified Index Module Construction", "content": "During the document segmentation process, larger chunks provide more context, enhancing comprehension but increasing processing time. Smaller chunks improve retrieval recall and reduce time, but may lack sufficient context.\nThis system adopts different segmentation strategies for different types of documents, with the overall process illustrated in Figure 3."}, {"title": "1.Document Tree-Based Chunking", "content": "For well-structured documents (e.g., corporate reports, academic papers), the document is hierarchically divided according to its structure, such as by sentences, paragraphs, and headings, forming multiple levels. Key information from articles and paragraphs is extracted, and retrieval is performed sequentially at each level. An explicit stack iteration is used to replace recursion, allowing the system to search for node titles level by level. This addresses potential stack overflow issues caused by deep recursion and optimizes path compression. By updating the parent node information along the entire path in a single computation, the system shortens the path length and reduces redundant computations, as illustrated in Algorithm 1."}, {"title": "2.Rule-Based Chunking and Semantic-Based Chunking", "content": "For documents with fixed formats, such as regulatory policy documents, financial reports, and bidding documents, rule-based chunking is more suitable. These documents often contain fixed clauses or section numbers (e.g., \"Contract Clauses\", \"Appendix\") and itemized numbers (e.g., \"Article 1\", \"Article 2\"), which can be precisely located using predefined rules.\nHowever, for documents that are more complex in content and lack a fixed structure, such as global climate change policy analysis reports and news sentiment reports, which are primarily in natural language and lack clear section divisions, semantic chunking is a more appropriate approach. In this paper, semantic chunking is based on the SeqModel proposes by Zhang et al. [19]. The model first uses BERT to jointly encode multiple consecutive sentences, directly modeling the dependencies between sentences to capture contextual semantic representations. Then, it predicts the segmentation boundaries of each sentence to accurately identify topic shifts. Finally, the model integrates adaptive sliding window techniques, dynamically adjusting the window size based on the content of the text, thereby improving inference efficiency while ensuring chunking accuracy. Semantic chunking effectively uncovers implicit semantics and integrates cross-paragraph information, making the chunking results better aligned with the logical structure of the topic content."}, {"title": "3.Paragraph-Based Chunking and Sliding Chunking for Long Text Blocks", "content": "Paragraph-based chunking is a document-structure-based segmentation approach, which divides the text by identifying natural paragraph boundaries within the document. This method is suitable for long texts that have clear paragraph divisions, such as white papers or technical documentation. If a paragraph is too lengthy to be directly used for subsequent processing, sliding window chunking is employed for further refinement. Sliding window chunking ensures that each chunk contains a certain amount of overlapping content, thereby preserving the contextual continuity between paragraphs. Meanwhile, Large Language Model is utilized to generate high-quality summaries for each chunk, extracting the core keywords from the long text. The generated summaries are then converted into vector representations for semantic search. Through multi-level chunking and semantic processing mechanisms, this approach addresses the issue of information omission in long texts, ensuring both the completeness and accuracy of the content in subsequent retrieval and generation tasks."}, {"title": "4.Processing of Structured Data", "content": "In document recognition and extraction, for the processing of structured data (such as images, formulas, and tables), accurate data extraction is achieved through classification methods. For images, key content is extracted using a multimodal annotation approach [20]. The extracted text is then transformed into semantic vectors using embedding models and stored in a vector database. For formulas, the Mathpix tool is used to parse them into \\LaTeX expressions [21], and the semantic information of the formulas is then extracted using large language models, converting them into vectors for storage, facilitating subsequent semantic retrieval and analysis. For table data processing, the main challenges include page-break disconnections, lack of headers, and merged cells. To address the issue of tables spanning multiple pages, table boundaries and their context (e.g., footer and header text) are detected to determine table continuity, and document layout analysis (such as PDF layout or docx XML attributes) is used to track table numbering and titles to establish page-to-page connections. For tables without headers, a content-based classification model is used to predict the header position, and contextual information is employed to automatically complete the header fields. The extraction of merged cells depends on the parsing of PDF XML attributes, particularly by reading the gridSpan and vMerge attributes to determine cell merging in the horizontal and vertical directions, ensuring accurate data reclassification. To ensure the accuracy of table extraction, the system combines XML attributes and text extraction information for verification and fusion. The precise row and column data of the table are obtained by analyzing the XML structure of docx files. In case of data conflicts, content similarity matching and field validation logic are used to select high-confidence results. The processed table content is finally stored in the MySQL database, while the semantic information of images and formulas is stored in the vector database to support efficient semantic retrieval."}, {"title": "Self-Prompting Enhanced Retrieval-Generation Architecture", "content": "Traditional retrieval methods still face challenges in accuracy and authority when addressing knowledge-based questions in the field of carbon emissions. To address this issue, this paper proposes a Self-Prompting Enhanced Retrieval-Generation Architecture, which includes intent recognition, structured chain-of-thought (COT) reasoning (including query rewriting, diverse expressions, few-shot COT prompts + pre-answering, key sentence extraction, and reordering), hybrid retrieval, and the Text2SQL system. Combined with high-quality prompt engineering, this architecture achieves efficient semantic understanding and query transformation. The text-based retrieval process is illustrated in Figure 4.The prompt for this module can be found in Appendix 1.\nBy rewriting user queries, we first decompose them into simpler, more answerable subqueries. Then, we identify and eliminate ambiguities in complex queries, recognize and extract the basic intent and core conceptual elements of the query, and create a high-level representation that captures the essential meaning while removing specific details. The rewritten queries expand the semantic space, helping to uncover more potentially relevant information, especially when user queries are vague or unclear. This method compensates for gaps in user input.\nBy generating multiple expressions, the system handles different wordings and phrasings in user queries, thereby enhancing the recall and precision of the retrieval system.\nFew-shot COT prompts assist the model in better simulating human reasoning processes, generating more accurate and contextually coherent preliminary answers. By simulating the reasoning chain, the model relies not only on semantic matching but also on knowledge and logic for inference, guiding it to gradually deduce conclusions. This approach reduces the likelihood of generating incorrect answers while helping the model better understand complex problems.\nBy extracting key sentences from the preliminary answers, more precise core information can be distilled. This enables the system to match each of the user's key points, improving the relevance of the retrieval. Key sentences preserve the complete semantics of the original text, whereas keywords, typically individual terms, may capture part of the core information but risk losing context, leading to partial or inaccurate understanding of the retrieved information.\nBM25 is a classical retrieval model[22] based on Term Frequency-Inverse Document Frequency (TF-IDF), which effectively handles word matching between documents and queries, and is particularly efficient in processing short-text queries, providing highly accurate retrieval results. Embedding, on the other hand, captures deeper semantic similarities, compensating for BM25's limitation in semantic matching. Through similarity calculations based on word or sentence embeddings, the Embedding model can identify documents that are semantically closer. By combining BM25 and Embedding in a hybrid retrieval approach, the system integrates the advantages of both term frequency matching and semantic matching. This hybrid model ensures high retrieval accuracy while enhancing semantic coverage.\nFinally, the system employs a re-ranking model, BGE-reranker-large (BAAI General Embeddings) [23], to concatenate and reorder the retrieval results, ensuring that the most relevant results are returned to the user. The overall process is shown in Figure 4.\nUse BM25 and BGE M3-Embedding model [24]to perform hybrid retrieval, and select Top5.\nScore(q, d) = \\lambda\u00b7 \\frac{1}{BM25(q, d) + c} + (1 \u2212\u03bb)\u00b7\\frac{1}{Embedding(q, d) + c} \nHere: \u03bb\u2208 [0, 1] is the weight of the BM25 model, and 1 A is the weight of the BGE M3-Embedding model. BM25(q,d) and Embedding(q,d) are the rankings of document d in the BM25 and BGE M3-Embedding models. The lower the ranking (i.e. the larger the number), the less relevant the document is. c is used to prevent the denominator from being zero when the ranking is 0, and to control the smoothness of the ranking. The usual value is 60. The larger the c value, the smaller the impact of low-ranking documents.\nAfter the intelligent retrieval results are spliced, the bge-reranker-large rearrangement model is used to rearrange, and the Top5 are selected.\nRfinal = BGEreranker(TopN(\bigcup_{qi \u2208rewrite(Q)}score (qi, d),\bigcup_{kj\u2208keysentences(COT(Q))} score(kj, d)))\nHere: rewrite(Q): This refers to rewriting the original query Q to generate multiple query variants qi, thereby expanding the search scope and increasing the diversity of the retrieval. score(qi, d): This refers to the score between each query variant qi and the document d. The score can be based on some similarity measure, such as BM25 or embedding-based matching.key_sentences(COT(Q)): This refers to the key sentences kj within the reasoning chain (COT) generated based on the query Q. The reasoning chain may consist of a series of logical steps, and extracting key sentences from this chain enhances the accuracy and relevance of the reasoning. score(kj, d): This computes the score between each key sentence kj and the document d. Similar to the query variant scores, it measures the degree of match between the sentence and the document."}, {"title": "Text2SQL System", "content": "The corporate carbon emissions knowledge query system requires real-time querying and analysis of large volumes of data, efficiently integrating with enterprise databases. Text2SQL, as a significant direction in natural language processing (NLP), is a technique that converts user queries in natural language into Structured Query Language (SQL). The core of this technology lies in the fusion of natural language understanding with database syntax structure, which helps lower the technical barrier for users interacting with databases. However, traditional Text2SQL methods still face the following challenges in practical applications:1) Complex queries, such as multi-table queries and queries across multiple time periods, are difficult to parse;2) The completeness and accuracy of the generated query results are not always high;3) The lack of syntax validation and security checks often leads to execution errors or security risks. To address these issues, this paper reconstructs an optimized Text2SQL system, combining the strong semantic understanding capabilities of large language model with schema awareness of databases to overcome the limitations of traditional methods.\nThe system architecture is shown in Figure 5, consisting of the following key steps:\nFirst, the large language model is used to parse the user query, performing concurrent tasks of time extraction and data table identification. The system then retrieves similar question examples using RAG to guide the model in step-by-step reasoning to generate the SQL query. Each example demonstrates the reasoning chain from the question to the SQL query, helping the model learn how to generate accurate SQL statements without a large amount of annotated data.\nThe system combines the extracted semantic information with the database schema structure to generate a preliminary SQL query. In this step, schema matching and semantic mapping are performed to ensure that the generated SQL query is consistent with the logical structure of the database. By analyzing the relationships between database tables and field constraints, the system can select appropriate tables and fields based on the query semantics, avoiding mismatches between the semantics and the table structure.\nAfter the preliminary SQL query is generated, the system introduces two important validation steps:\nSecurity Validation: By setting up whitelists or rule-based filters, high-risk operations (such as DELETE, INSERT, UPDATE) are restricted to minimize their impact on the database, thereby enhancing the system's security. This step uses static analysis to parse and check the SQL query's syntax tree.\nSyntax Optimization: The system performs syntax validation on the preliminary SQL query to detect potential errors, such as missing join conditions or incorrect logical operators. Syntax optimization utilizes SQL correction and structured rewriting methods [25] to improve the completeness and accuracy of the generated query.\nAfter the optimized SQL query is validated, the system performs a final syntax check, executes the query, and returns the result. The system integrates the semantic information parsed by the large language model, the database schema, and the execution logic of the SQL query at multiple levels to ensure both the efficiency and accuracy of the query."}, {"title": "Global Climate Change Knowledge Q&A System", "content": "Analysis strategy\nTo ensure the scientific, accurate, and practical nature of the enterprise carbon emissions analysis system, the system's analysis dimensions are carefully designed to incorporate international standards from the GHG Protocol, the operational characteristics of the enterprise, data traceability, complexity management, and personalized needs.\nThe analysis dimensions are based on the three emission scopes defined by the GHG Protocol (Scope 1, Scope 2, and Scope 3), comprehensively covering the enterprise's direct emissions (such as emissions from fixed facilities and mobile equipment), indirect emissions (such as the use of purchased electricity), and supply chain emissions (such as upstream and downstream logistics and procurement). By clearly defining emission activities, the system ensures the comprehensiveness and compliance of the accounting process. Additionally, the system incorporates industry-specific design, such as focusing on energy consumption and supply chain activities in manufacturing, and emphasizing employee commuting and waste management in the service industry.\nThe system also analyzes the boundaries of the enterprise's carbon emissions inventory (such as organizational structure, operational boundaries, and business scope), verifying whether all relevant emission sources are included in the report, and reviewing any excluded emission activities and their disclosure reasons. This verification ensures the completeness of the emissions inventory and helps assess whether the enterprise meets the relevance requirements for providing decision-making support to both internal and external users. Moreover, the system evaluates whether the enterprise publicly discloses long-term emission tracking information and whether the summarized reports maintain consistency over time, thereby enhancing the timeliness and logical coherence of the reports.\nBuilding on this, the system performs a comprehensive analysis by integrating future greenhouse gas target setting, carbon emission risk identification, and management, providing the enterprise with both carbon emissions assessments and future carbon emissions forecasts and controls. The active involvement of climate experts further improves the accuracy of assessments and the relevance of strategies, making the analysis more tailored to the enterprise's actual situation, and enhancing the system's ability to recognize complex emission data. This multi-layered, multi-dimensional design not only meets global carbon emission accounting requirements but also flexibly adapts to the specific circumstances of each enterprise, providing high-quality analysis results for decision-makers."}, {"title": "Hallucination analysis", "content": "The large language model has demonstrated great potential in carbon emission analysis and knowledge question-answering systems, but still faces several key challenges in practical applications, particularly the phenomenon of hallucination. Specifically, hallucination can manifest in the following forms: 1) factual inconsistency, where a certain carbon emission standard may be incorrectly stated as inconsistent with the actual regulations; 2) query inconsistency, where other aspects of carbon emissions may be discussed but the model fails to address the specific question posed by the user; 3) topic deviation, where certain carbon emission-related concepts may be discussed, but the model does not delve into the specific details directly related to the question. To address these issues, the generated responses of the large model were analyzed and optimized in depth, focusing on improving factual recall, understanding context, and enhancing reasoning ability, thereby ensuring the authenticity, consistency, and verifiability of the generated content. The following work was carried out:\n\u2022 Constructing document trees and multimodal annotations: Each record in the knowledge base retains the full path and specific location of the source document (e.g., page number, paragraph number). By combining multimodal annotation methods for text, tables, and visual content, the system is able to handle structured data and trace the sources of unstructured text and visual content, as detailed in Diversified Index Module Construction module.\n\u2022 Self-prompt retrieval architecture: Using a self-prompt retrieval architecture to guide the large model through gradual reasoning, as detailed in Self-Prompting Enhanced Retrieval-Generation Architecture module.\n\u2022 Timestamp and version control: To handle the potential updates to corporate reports over time, each record in the knowledge base is assigned a timestamp and version information to ensure that users are aware of the timeliness of the data. The system also allows for differential comparisons between different versions of the report, tracking historical content changes.\n\u2022 Hallucination tagging function: In the process of generating answers, the carbon emission analysis module specifically designed a \"hallucination tagging\" function. This function verifies whether the generated response is contained in the evidence sentences from the report, screening and tagging any information that deviates from or exceeds the original content to ensure that all responses are based on authentic and verifiable content."}, {"title": "Pipeline", "content": "This paper constructs a scientifically robust and highly practical carbon emission report analysis system through the integration of three modules: report summary, report evaluation, and personalized question answering. Specific examples are provided in Appendices 3 and 4. The overall process flow is shown in Figure 1.\nReport Summary Module:The report summary module aims to assist users in efficiently understanding and analyzing corporate carbon emission reports, particularly focusing on systematically summarizing the disclosure content of companies according to the GHG Protocol guidelines. By integrating multi-source information and extracting relevant report content, along with background data (from the vector database) and company information (from the relational database), the system generates structured and traceable responses. Through summary and key extraction, users can quickly grasp the core information without reading the report word by word, significantly improving report reading efficiency. By using the information from the target report as analytical prompts, this approach effectively reduces hallucinations, ensuring answer accuracy and a clear correspondence with the source data.\nThe report summary module also promotes report compliance. By systematically analyzing the recommendations in the GHG Protocol, the system helps companies identify potential deficiencies in their environmental reports and provides references for improving the content of these reports. It enhances the normative and comprehensive nature of future disclosures, supporting companies in formulating more effective carbon emission strategies. The module achieves end-to-end optimization from data extraction to information summarization through intelligent retrieval, contextual integration, and closed-loop answer generation. The design of the prompt templates ensures user-friendliness while enhancing multi-scenario adaptability, fully leveraging the role of report summaries in decision support. The prompt templates for this module are provided in Appendix 2.The analysis results are shown in Appendix 3.\nReport Evaluation Module:The GHG Protocol provides detailed disclosure guidelines for each recommendation, specifying the types and granularity of information that companies need to disclose in their reports. To assess whether a corporate carbon emission report complies with the GHG Protocol, a report evaluation module has been designed to analyze the extent to which the report follows the GHG Protocol. For each GHG guideline content, the report evaluation module receives relevant context from the vector retrieval module. It then compares and evaluates this content against the corresponding GHG guidelines, generating an analytical paragraph and a GHG Protocol compliance score ranging from 0 to 100. The prompt template for this module is provided in Appendix 2.The analysis results are shown in Appendix 4.\nCustomized Question Module:This module provides users with the ability to conduct personalized analysis beyond the GHG Protocol framework. It can receive user-defined questions and retrieve relevant contextual information based on the self-prompting retrieval-augmented generation (RAG) architecture to address personalized analysis needs. The module features adaptive prompt templates, specifically optimized for noisy scenarios, reducing redundant information and enhancing semantic parsing capability, thereby ensuring the scientific accuracy and precision of personalized analyses. This flexibility enables the system to better handle complex issues, offering reliable customized analysis support to users. The prompt template for this module is provided in Appendix 2."}, {"title": "Experiments", "content": "The system uses Qwen-max as the foundational large language model for conducting the experiments and analyses presented in this paper. Langchain is employed to manage Qwen API calls and vector database retrievals. The BGE-reranker-large reranking model is utilized. The text embedding BGE-M3 from Zhiyuan AI is used to embed text blocks, and the Chroma vector database is employed for storing the vectorized text blocks. If the prompt becomes too lengthy (e.g., exceeding 3000 tokens) after inserting retrieved background information, the background information is summarized by progressively removing the least relevant blocks until the prompt fits within the context window. The temperature for all LLM calls is set to 0, and a static vector database is reused for each report."}, {"title": "Data construction", "content": "This paper systematically collects and organizes 1,000 policy and regulatory documents related to corporate carbon emissions management issued by the Chinese government (including both central and local governments) between 2018 and 2024, along with 1180 QA pairs annotated as a test set. Additionally, 100 environmental reports from publicly listed companies are collected from various public resources and databases. These reports typically include information on greenhouse gas emissions, The goal of reducing carbon emissions, implementation measures, and progress in emission reductions during company operations. The data sources include annual and sustainability reports published by companies such as Apple and Google, which provide detailed accounts of greenhouse gas emissions and environmental measures. Another important data source is carbon disclosure platforms, such as CDP (Carbon Disclosure Project) [26] , which provide extensive carbon emissions data from publicly listed companies. Other sources include government and regulatory publications, such as those from the China Securities Regulatory Commission and the EU's Sustainable Finance Disclosure Regulation (SFDR) standards.\nTo evaluate the performance of content-aware questions, this paper constructs a test dataset, manually annotating 2,133 high-quality QA pairs based on two real business databases. The dataset covers types such as filtering, aggregation, multi-table joins, and nested queries, emphasizing the context dependence of tabular content. It requires the model to generate accurate SQL queries by combining table structures and instance data. All QA pairs are annotated and reviewed by domain experts to ensure high consistency between questions and answers, providing a reliable evaluation tool for Text2SQL technology."}, {"title": "Evaluation indicators", "content": "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) [27] is an automated method used to evaluate the quality of generated text, widely applied in tasks such as text summarization, translation, and generation. It primarily measures the quality of text generation by calculating the overlap of vocabulary between the generated text and reference text.\nROUGE-N is used to calculate the overlap of N-grams, with the following formula:\nROUGE -N = \\frac{\\sum_{i = 1}^{N} count(ref \u2229 sys)}{\\sum_{i=1}^{N} count(ref)}\nHere, \"ref\" represents the reference text, \"sys\" represents the generated text, \"n\" denotes the intersection between the two, and \"count\" refers to the number of N-grams in the intersection. The core idea of ROUGE-N is to measure the matching ratio of N-grams between the two texts, which reflects the coverage of the reference text by the generated text.\nAdditionally, ROUGE-L is based on the Longest Common Subsequence (LCS) measure, focusing on the matching of word order and considering both semantic and structural similarity. These metrics provide comprehensive automated evaluation tools for text generation tasks.\nROUGE-L = \\frac{2x LCS(ref, sys)}{Lref + Lsys}\nHere, LCS(ref, sys) represents the length of the Longest Common Subsequence (LCS) between the reference text and the generated text, while Lref and Lsys represent the lengths of the reference and generated texts, respectively.\nBERTScore is a semantic-based text evaluation method [28] that leverages the pre-trained contextual embeddings of the BERT model to assess the semantic similarity between the generated text and the reference text using cosine similarity. Specifically, BERTScore first generates embedding vectors for both the generated text and the reference text using the BERT model, and then calculates the maximum matching similarity of the embedding vectors for each word in the two texts. BERTScore mainly includes the following three metrics:"}, {"title": "Results and analysis", "content": "Through ablation experiments, the paper of the proposes method and its individual modules in improving model performance is further validated. A total of five configurations were compared: standard RAG, the introduction of structured chain-of-thought, diversified indexing, hybrid retrieval, and the final model incorporating all components of our work. The standard RAG, as the baseline method, directly relies on the retrieval-augmented generation framework. Based on the performance of several large models in evaluations on Hugging Face, the following models were selected for experimental comparison: ChatGPT-40-2024-05-13, Qwen-Max, GLM-4[31], Spark 4.0 Ultra [32], Baidu ERNIE-4.0-Turbo [33], and Llama-3.1-70B-Instruct [34].\nAs shown in Table 2, the experimental results indicate that the ROUGE-1, ROUGE-2, and ROUGE-L scores for the standard RAG are 0.529, 0.392, and 0.468, respectively, and the BERTScore Precision, Recall, and F1 scores are 0.826, 0.837, and 0.831. Although the standard RAG exhibits basic answering capabilities, its performance in handling complex questions is more limited. After integrating all the proposes modules, the model performance reaches its optimal level, with ROUGE-1, ROUGE-2, and ROUGE-L scores of 0.592, 0.450, and 0.540, respectively, and BERTScore Precision, Recall, and F1 scores of 0.887, 0.924, and 0.906, which significantly outperform the performance of individual module configurations.\nIn conclusion, the method achieves the best performance under the collaborative effect of all modules. In particular, the structured chain-of-thought module excels in providing logical answers to complex questions, the diversified indexing module effectively optimizes retrieval quality, and the hybrid retrieval module further enhances the system's comprehensive query capability."}, {"title": "Analysis report correlation analysis", "content": "The question-answer relevance refers to the semantic association between the question and the generated answer, evaluated through Large Language Model. In our paper, Qwen-Max is used as the evaluator to assess the semantic correlation between the question and the generated answer. The specific approach involves constructing prompts (see Appendix I), then combining the question and generated answer with the prompt and invoking the large language model to analyze the degree of semantic association between the question and the generated answer, thus determining the accuracy of the generated answer.\nAdditionally, expert scoring is employed to evaluate the responses based on their analysis results, with the understanding of the question being incorporated into the scoring criteria. The experts involved in the evaluation include 10 specialists in carbon emission policy, computer information engineering technology, and energy subfields. We randomly selected 10 reports, each of which was evaluated by 10 experts across 14 analytical dimensions. After statistical analysis of the expert scores, the overall score results are shown in Figure 7. (LLM relevance score (rqa): indicates that the carbon emission analysis system does not use the climate change intelligent question-answering system.)"}, {"title": "Conclusions and future work", "content": "This paper proposes a LLM-based enterprise carbon emission analysis and knowledge question-answering system, aiming to enhance the analysis capabilities of carbon emission data and improve the efficiency of knowledge acquisi-tion. The system combines the Self-Prompted Retrieval-Augmented Generation architecture and structured prompt engineering methods, enabling automatic analysis of corporate carbon emission reports and answering knowledge-based questions on global climate change. This helps enterprises provide real-time, accurate information support for policymakers, businesses, and the public, offering comprehensive carbon emission analysis, emission reduction recommendations, and future predictions. It assists businesses in making more scientific and precise decisions in complex carbon emission regulatory environments. Through the intelligent question-answering system, users can easily access climate change-related data and obtain guidance based on the latest research and policies.\nIn future work, the following areas will be further optimized:\nLocalization of LLM Development: Transitioning from the existing Qwen model to a localized large language model to enhance the controllability and reliability of outputs, making it more suitable for carbon emission analysis tasks across various domains. This will involve fine-tuning the model using high-quality datasets evaluated by experts to improve its performance in climate-related tasks.\nOptimizing Natural Language Processing Capabilities: Further enhancing the natural language processing module, especially in handling complex problems, to reduce hallucination issues that may arise during the parsing and generation process."}]}