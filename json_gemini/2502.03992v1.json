{"title": "Ontology-Guided, Hybrid Prompt Learning for Generalization in Knowledge Graph Question Answering", "authors": ["Longquan Jiang", "Junbo Huang", "Cedric M\u00f6ller", "Ricardo Usbeck"], "abstract": "Most existing Knowledge Graph Question Answering (KGQA) approaches are designed for a specific KG, such as Wikidata, DBpedia or Freebase. Due to the heterogeneity of the underlying graph schema, topology and assertions, most KGQA systems cannot be transferred to unseen Knowledge Graphs (KGs) without resource-intensive training data. We present OntoSCPrompt, a novel Large Language Model (LLM)-based KGQA approach with a two-stage architecture that separates semantic parsing from KG-dependent interactions. OntoSCPrompt first generates a SPARQL query structure (including SPARQL keywords such as SELECT, ASK, WHERE and placeholders for missing tokens) and then fills them with KG-specific information. To enhance the understanding of the underlying KG, we present an ontology-guided, hybrid prompt learning strategy that integrates KG ontology into the learning process of hybrid prompts (e.g., discrete and continuous vectors). We also present several task-specific decoding strategies to ensure the correctness and executability of generated SPARQL queries in both stages. Experimental results demonstrate that OntoSCPrompt performs as well as SOTA approaches without retraining on a number of KGQA datasets such as CWQ, WebQSP and LC-QUAD 1.0 in a resource-efficient manner and can generalize well to unseen domain-specific KGs like DBLP-QuAD and CoyPu KG 1.", "sections": [{"title": "I. INTRODUCTION", "content": "KGQA systems enable non-expert users to pose natural language queries and retrieve precise and relevant answers from the underlying KG based on the facts available in the KG. There's a significant need for a KGQA system that can generalize across diverse KGs. This is a challenging task due to the heterogeneity of the underlying KG. As shown in Figure 1, Freebase, DBpedia and Wikidata, the most popular three general KGs, have their unique way of representing the same world facts regarding Apple Inc., Steve Jobs and Steve Wozniak. Heterogeneity can be found as 1) schema heterogeneity\u00b2: differences in the concepts\u00b3 and the relations between them across different KGs. For instance, the three KGs in Figure 1 have their own naming convention for the same concept of a Person, namely dbo:Person in DB\u0440\u0435-dia, ns:people.person in Freebase, and human in Wikidata; 2) topology heterogeneity: differences in how a fact is represented and accessed within a KG. For instance, Wikidata utilizes a special type of connection known as a \u201cqualifier\" (depicted as nodes in orange in Figure 1) to furnish the triple (wd:Q19837, wdt:P169, wd:Q312) with additional details like the start date; 3) assertions heterogeneity: differences in the assertion about entities and their relations across different KGs. For instance, the assertion \"Joe Biden is the President of the United States\u201d is represented as (Joe Biden, office, President of the United States)in DBpedia, whereas in Wikidata, it is (Joe Biden, position held, President of the United States). \nThe majority of current KGQA systems lack generalization because they are typically tailored for a particular KG [1]\u2013[3], or focus only on within-a-KG generalization [4]\u2013[6]. Although some approaches [7]\u2013[10] showed their ability to generalize across KGs in certain respects, e.g., regarding the assertion heterogeneity which lies between dataset identifiers such as WebQSP (Freebase) [11] and MetaQA (Wikimovies) [12], they fail to generalize to other aspects such as different schemas or topologies.\nLarge language models (LLMs) have demonstrated remark-able reasoning capabilities. However, several studies reveal that LLMs perform inadequately in knowledge-intensive task KGQA [13]\u2013[15]. LLMs suffer from issues such as halluci-nation [16] and factual inaccuracy when answering questions, mainly because they lack domain-specific knowledge, stem-ming from insufficient training data or missing interactions with an unseen or heterogeneous KG [17].\nWe present OntoSCPrompt, a two-stage ontology-guided hybrid prompt learning KGQA method. To be agnostic of the underlying KG, we use a two-stage process [18], [19] that separates semantic parsing from intensive KG-dependent interactions. First, we forecast a SPARQL Query Structure in-dependent of any specific KG. Second, we fill the placeholders with missing KG identifiers, such as entities and relations. To enhance the understanding of the semantics of the underlying KG, we integrate ontology knowledge into the learning process"}, {"title": "II. METHODOLOGY", "content": "In this section, we explain the two-stage approach On-toSCPrompt and how it facilitates generalization across di-verse KGs."}, {"title": "A. Two-Stage Framework", "content": "As discussed earlier, a KGQA system that can generalize to unseen KGs is expected to comprehend the similarities and differences between KGs. That is, for the same natural language question and a different KG on which it is trained, the system should be able to generate a matching SPARQL query without extensive retraining. To this end, we utilize a two-stage framework: 1) Query Structure Prediction, wherein questions are translated into generic SPARQL query structures independent of any particular KG, called the structure stage (Stage-S). 2) KG Content Population, where the SPARQL structures are populated with schema elements such as con-cepts, relations and entities specific to the given KG, called the content stage (Stage-C). Finally, the SPARQL query for a given question is generated by combining the predictions of these two sub-tasks."}, {"title": "B. Structured SPARQL Query", "content": "The inherent heterogeneity of the schema of different KGs, including entity identifiers, concepts, and relations, challenge KGQA systems to adapt their understanding and reasoning processes across different KGs. To tackle this challenge, we design a generic query structure representation, which con-sists of the following elements of standard SPARQL queries: 1) Reserved Keywords, such as SELECT, ASK, FILTER, COUNT, etc.; 2) Literals, such as numbers, strings, dates, and fixed textual values; 3) KG-specific Identifiers, such as entities, relations, concepts, and variables. Many linguistically similar questions share similar SPARQL skeletons, even across different KGs, if not identical, due to ontological modelling based on humans' natural language usage.\nWe extend previous work [22] by adding a new placeholder for concepts and supporting more complex SPARQL clauses like having, group by, order by, etc. We also prove that this approach with our extensions can generalize well to other KGs without retraining. We use 6 special tokens to serve as placeholders for to-be-filled components within SPARQL queries. These tokens are: 1) [ent] for entities mentioned in a given question, 2) [cct] for concepts, 3) [rel] for relations specified in a KG ontology, 4) [var] for variables, 5) [val] for literals appearing in basic graph patterns, value clauses or solution modifiers of SPARQL queries, and 6) [con] to signify a constraint or condition in such SPARQL keywords as FILTER, ORDER BY, GROUP BY or HAVING. Such a placeholder enhances cross-KG alignment and broadens the coverage of complex questions with constraint(s).\nIn terms of reasoning difficulty, our structure representation can cover structures of questions that require single-hop or multi-hop reasoning (with or without aggregates, conditions or both)."}, {"title": "C. Ontology-Guided Hybrid Prompt Learning", "content": "Our prompt construction offers LLMs with task-specific information for accurate predictions. There are two prompt construction methods: (1) Textual Prompts, a textual template like \"Answer the question: [input], [output]\" to guide LLMs to generate the desired output; (2) Learnable Vectors, a series of continuous vectors prepended to the input which can be optimized during training. Here, we have two main considerations in prompt construction: (1) to mitigate the effect caused by the heterogeneity of the underlying KGs and (2) to ensure the adaptability to new KGs. Thus, we propose a novel ontology-guided, hybrid prompt learning method.\na) Task-specific, Ontology-Guided Textual Prompts.: Ontology knowledge is explicitly prepended to enhance the understanding of KG-specific semantics in two stages. The structure prompt $P_s$ in Stage-S and content prompt $P_c$ in Stage-C are designed as follows.\n\u2022 $P_s$ = \"[prefix] [question] - [ontology]\"\n\u2022 $P_c$ = \"[prefix] [structure] [question] - [ontology]\"\nWe use \"translate the question into sparql according to the ontology:\" as [prefix]. We use a verbalization method [23] to convert the ontology to text format. For example, the DBpedia ontology in Figure 1 is verbalized as \"ontology: concepts: Company, Person; relations: foundedBy, birthDate, deathDate, type; entities: Steve_Jobs, Steve_Wozniak, Apple_Inc.\"\nb) Aspect-aware Continuous Prompts.: We introduce four learnable vectors [19] with random initialization for understanding different aspects of the input, i.e., $v^Q$ for learning question Q specific features, $v^G$ for learning ontology G specific features, $v^B$ and $v^E$ for learning task-specific features at the beginning and end of the input respectively (see Equation 1 and 2). It holds that $v^Q, v^Gv^E, v^B \u2208 R^d$ where d is the dimensionality of the LLM input representations. Therefore, hybrid prompts (i.e., prompts containing textual and continuous parts) for stage-S and stage-C, i.e., $I_s$ and $I_c$ are constructed as follows:\n$I_s = v^B \u2295 v^Q \u2295 v^G \u2295 e^s \u2295 e^Q \u2295 e^G \u2295 v^E$ (1)\n$I_c = v^B \u2295 v^Q \u2295 v^G \u2295 v^E \u2295 e^c \u2295 e^Q \u2295 e^G$ (2)\nwhere, $e^s, e^c, e^Q$ and $e^G$ represent the embedding of structure prefix in $P_s$, content prefix in $P_c$, question Q and ontology G respectively. Here, $e^s, e^c, e^Q, e^G \u2208 R^d$ and $\u2295$ is the operator for vector concatenation.\nc) Hybrid Prompt Learning.: At both stages, the train-able parameters \u0398 correspond to the base model parameters $O_m$ and the learnable vectors $\u0398_\u03b9$. The desired output is generated through auto-regressive decoding [19], [24]. The pa-rameter \u0398 is learnt by minimizing the negative log-likelihood loss.\n$L(\u0398) = -\\frac{1}{n}\\sum_{i=1}^n log P(O_{gold}^{(i)} | I^{(i)}; \u0398)$ (3)\n$= -\\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^{m_i} log P(OI_j^{(i)} | OI_{gold}^{(i)}... O_{j-1}^{(i)}; \u0398)$\nwhere I and O are the hybrid prompts as the input and the ground truth output at each stage.\nFor example, given a question, \"Who founded Apple?\", the ground truth output Os is its SPARQL structure \"select [var] where { [ent] [rel] [var] }\" at the structure stage while the ground truth output Oc is \"[var] var0 [ent] dbr:Apple_Inc. [rel] dbo:foundedBy [var] var0\" at the content stage."}, {"title": "D. Constrained Decoding Strategies", "content": "Constrained decoding approaches facilitate the generation of text sequences in a controllable and expected fashion. This technique is widely used in neural machine translation [25], text summarization [26], and neural semantic parsing [27]. To ensure the validity of the generated SPARQL queries, thus, we devise different task-specific decoding strategies: 1) Grammar-constrained Decoding at stage-S, and 2) Structure and/or Subgraph Guided Decoding at stage-C.\nGrammar-constrained Decoding. Integrating grammar constraints at the decoding stage guarantees grammatical correctness, particularly in low-resource situations [28]. For example, the model must have the ability to discard outputs like \"select [var] where { [var] [ent] [ent] }\", as they do not conform to our simplified SPARQL grammar rules where we expect a [rel] instead of [ent]. Referring to the standard SPARQL grammar definition5.\nStructure-guided Decoding. To ensure the validity of the resulting SPARQL query, the content predicted in Stage-C must be consistent with the placeholders predicted in Stage-S. In the beam search process, we adjust the score of the candidate placeholder tokens - which do not align with their respective counterparts in the structure - to -\u221e. For example, if the predicted structure is \"select [var] where { [ent] [rel] [var]}\", and the predicted content is \"[var] var0 [var] var0 [rel] dbo:founders [ent] dbr:Microsoft\", the model fails to merge them into the final query due to structure inconsistency.\nSubgraph Constrained Decoding. Subgraphs of question entities provide contextual information. Understanding the surrounding context helps to disambiguate the meaning of entities or relations [29], [30]. Thus, we introduce subgraph constraints in stage-C to assign higher priority to relations in a subgraph relevant to the given question. Consider the question \"Who plays Ray Barone?\" in the WebQSP dataset. It is challenging for the model to differentiate between \"film.performance.actor\" and \"tv.regular_tv_appearance.actor\". Subgraph constraints prefer to choose \"tv.regular_tv_appearance.actor\" as \"film.performance.actor\" does not exist in the extracted subgraph of the entity \"m.05h7f2 (Ray Barone)\"."}, {"title": "III. EXPERIMENTAL SETUP", "content": "We use datasets across a wide range of KGs, such as Freebase, DBpedia and DBLP, to show the generalization abilities of OntoSCPrompt: 1) WebQSP (Freebase): A popular dataset with 4,937 questions extracted from Google Search logs. Those questions involve up to 2-hop reasoning and constraints. 2) LC-QuAD 1.0 (DBpedia): The dataset con-sists of question and SPARQL query pairs generated using predefined question templates and crowdsourcing. It contains diverse types of complex questions, such as simple, multi-hop, and aggregation. 3) CWQ (Freebase): A KGQA benchmark modified from WebQSP dataset, having a higher percentage of complex questions with multi-hops and constraints. 4) Sim-pleDBpediaQA (DBpedia): A mapping of the SimpleQues-tions dataset from Freebase to DBpedia. 5) DBLP-QUAD (DBLP): A newly released complex question benchmark over the scholarly KG (i.e., DBLP) with 10,000 pairs of question and SPARQL queries. 6) CoyPuKGQA (CoyPu KG): a newly created KGQA benchmark over an industrial, global economy KG, namely the CoyPu KG, with 939 questions6."}, {"title": "A. Datasets", "content": "We use datasets across a wide range of KGs, such as Freebase, DBpedia and DBLP, to show the generalization abilities of OntoSCPrompt: 1) WebQSP (Freebase): A popular dataset with 4,937 questions extracted from Google Search logs. Those questions involve up to 2-hop reasoning and constraints. 2) LC-QuAD 1.0 (DBpedia): The dataset con-sists of question and SPARQL query pairs generated using predefined question templates and crowdsourcing. It contains diverse types of complex questions, such as simple, multi-hop, and aggregation. 3) CWQ (Freebase): A KGQA benchmark modified from WebQSP dataset, having a higher percentage of complex questions with multi-hops and constraints. 4) Sim-pleDBpediaQA (DBpedia): A mapping of the SimpleQues-tions dataset from Freebase to DBpedia. 5) DBLP-QUAD (DBLP): A newly released complex question benchmark over the scholarly KG (i.e., DBLP) with 10,000 pairs of question and SPARQL queries. 6) CoyPuKGQA (CoyPu KG): a newly created KGQA benchmark over an industrial, global economy KG, namely the CoyPu KG, with 939 questions6."}, {"title": "B. Baselines", "content": "We compare OntoScPrompt to several existing KGQA sys-tems, which were themselves evaluated over different KG sets: 1) STaG-QA [18] is a KGQA system for evaluating generalizability on WebQSP, LC-QuAD 1.0, MetaQA and Simple QuestionWikidata, separating the cross-KG reasoning process into two stages, i.e., softly-tied query sketch gener-ation and KG alignment. 2) GraphNet [7] is a method that integrates information from both knowledge bases and text corpora at an early stage of processing. 3) PullNet [31] use an iterative process to build a question-specific subgraph. In each iteration, a graph-CNN is used to pinpoint subgraph nodes that should be expanded. 4) EmbedKGQA [8] leverages KG embeddings to perform multi-hop KGQA on WebQSP and MetaQA datasets. 5) HGNet [32] is an end-to-end method for query graph generation, using hierarchical autoregressive decoding and a unified graph grammar AQG to delineate the structure of query graphs. 6) TERP [9] integrates explicit textual information and implicit KG structural features based on a novel entity link prediction framework."}, {"title": "C. Evaluation Metrics", "content": "For comparison, we use Precision, Recall and F1 as standard evaluation metrics for LC-QuAD 1.0, SimpleDBpediaQA and DBLP-QuAD, and Hits@1 for CWQ and WebQSP. Note that the Exact Match (EM) score metric is used while training, as the target in each stage, is either the structure or content of a SPARQL query instead of full SPARQL queries."}, {"title": "D. Implementation Details", "content": "KG Endpoints. We use DBpedia 2016-107 for LC-QuAD 1.0 and SimpleDBpediaQA, the latest Freebase dumps for We-bQSP and CWQ. We host local SPARQL Virtuoso endpoints for DBpedia and Freebase. For querying the DBLP KG, we use the official live SPARQL query endpoint for DBLP-QuAD [21].\nData preprocessing. We preprocess [22] the SPARQL queries in each benchmarking dataset, which involves prefix removal, variable name standardization, lowercasing, redun-dant whitespace removal, prefixing IRIs and so on. Note that the preprocessing procedure does not change the seman-tics or executability of the SPARQL queries. For example,"}, {"title": "IV. EVALUATION", "content": "In this section, we examine our experimental findings and assess how effective OntoSCPrompt is for KGQA generaliza-tion. First, we gauge OntoSCPrompt's performance on indi-vidual KGQA datasets. Second, we assess OntoSCPrompt's capacity to generalize across KGQA datasets within the same KG. Third, we evaluate its capability to generalize across different KGs."}, {"title": "A. Evaluation on Generalization", "content": "Table IV and Table V show the overall results of On-toSCPrompt on WebQSP, CWQ, LC-QuAD 1.0 and Sim-pleDBpediaQA in comparison to baselines for KGQA gener-alization. We train and evaluate each individual dataset based on its respective KG.\nOur proposed method, OntoSCPrompt, demonstrates com-petitive or state-of-the-art accuracy on both LC-QuAD 1.0 and CWQ compared to existing KGQA generalization baselines. From Table IV, we observe that (1) OntoSCPrompt achieves an F1 score of 79.1% on LC-QuAD 1.0, surpassing STaG-QA and HGNet by significant margins, namely 35.0% and 5.1% respectively. (2) OntoSCPrompt performs better than HGNet and STaG-QA on WebQSP by 4.5% and 13%. However, it underperforms TERP by 4.1%. The main reason is that TERP exploited relation paths' hybrid semantics (explicit text infor-mation and implicit KG structural features). (3) Constrained decoding contributes significantly to OntoSCPrompt's overall performance improvement on WebQSP and LC-QuAD 1.0, respectively. (4) The methods addressing schema or topol-ogy heterogeneity demonstrate relatively lower performance compared to those targeting assertion heterogeneity, such as TERN, GrapftNet, PullNet and EmbedKGQA, since schema or topology heterogeneity is more complicated than assertion heterogeneity.\n1) Generalization Within the Same KG: KGQA generaliza-tion within the same KG refers to the ability of a QA system to provide accurate answers to questions across different subsets or versions of the same KGQA dataset. Essentially, it involves the transfer of learned knowledge and reasoning capabilities within a single KG. To assess the ability of OntoSCPrompt, without any fine-tuning, we directly evaluate the model trained on the source KGQA dataset on the target KGQA dataset. We assume the source and target KGQA datasets are heteroge-neous, as they handle different subsets of the same KG despite overlapping schema elements.\nTable V shows the performance on CWQ using the WebQSP-trained model and the performance on SimpleDB-pediaQA using the LC-QuAD 1.0-trained model. Following previous works, we report Hits@1 for CWQ and F1 score for SimpleDBpedia. DA and GA indicate the data split of the dataset x on which the model is trained and test, with the ontology of the dataset x integrated respectively. A is the target dataset, i.e., CWQ or SimpleDBpedia, B is the source dataset, i.e., WebQSP for CWQ or LC-QuAD 1.0 for SimpleDBpedia. OntoSCPrompt achieves Hits@1 of 48.8% and F1 score of 34.0% respectively, without any fine-tuning, only with their ontology provided, which performs below those trained and evaluated on the source KGQA dataset, such as TERP and HGNet. However, OntoSCPrompt achieves Hits@1 of 70.4% and performs above TERP by 43%, HGNet by 21.2%, PullNet by 53.4%, and EmbedKGQA by 57.5% respectively, if fine-tuned on the target KGQA dataset with its ontology, i.e., trained on DA, GA. For SimpleDBpediaQA, we can also see the performance gain brought by KG ontology and fine-tuning. Thus, we demonstrate OntoSCPrompt's potential to generalize across different KGQA datasets within the same KG, even without any fine-tuning. This highlights the importance of the ontology on understanding the semantics of the underlying KG. Remember, the model has never seen any of these datasets (see trained on DB, GB and tested on DA, GA). From earlier papers, we know that other models do not achieve any hits [35].\n2) Generalization Across Different KGs: KGQA general-ization across different KGs refers to the ability of a KGQA system to provide correct answers to questions across vari-ous KGs without extensive retraining. It involves transferring knowledge and reasoning capabilities from one KG to another. To assess the ability of OntoSCPrompt, we use the model pre-trained with a source KGQA dataset based on one KG and adapt to a target KGQA dataset based on another KG.\nWe find that the pre-trained variant improves the accuracy by +6.4 on DBLP QuAD and +3.1 on CoyPuKGQA respec-tively, showing that pre-training could bring significant perfor-mance gains. In particular on a domain-specific or even low-resource dataset, and facilitate generalization across multiple KGs."}, {"title": "B. Evaluation on Ontology-Guided Hybrid Prompts", "content": "As discussed earlier, we construct KG ontology-guided hybrid prompts and introduce four continuous vectors to learn different aspects of the input. Thus, the trainable parameters correspond to the parameters of the base LLM $O_m$ and the learnable vectors $\u0398_\u03b9$. However, the extent of the potential contribution of such a hybrid prompt to KGQA generalization remains uncertain. Thus, we conduct an ablation study to investigate its effect on the overall performance. First, we only fine-tune $O_\u03b9$ with $O_m$ fixed, i.e., prompt tuning (PT). Second, we fine-tune both $O_m$ and $\u0398_\u03b9$ (i.e. PT+FT). The results are reported in Table VII. It is evident that such a hybrid prompt contributes to the overall performance as only optimizing ontology-guided hybrid prompt results in 70.3% on LC-QuAD 1.0 and 62.1% on WebQSP. This highlights its importance in understanding the semantics of the underlying KG and adapting to unseen KG without extensive re-training."}, {"title": "C. Impact of Constrained Decoding", "content": "We perform an ablation analysis to examine the impact of each task-specific decoding technique on the performance. Note that regarding to the choice of WebQSP over LC-QUAD and CWQ for this evaluation, we have the following reasons: (1) in LC-QuAD 1.0, all test set structures are seen in the training set, leading to high accuracy in structure inference. (2) LC-QuAD 1.0 lacks constraint-based structures and is mostly multiple hops, while WebQSP and CWQ have more complex SPARQL structures. (3) WebQSP has 34% unseen structures in the test set, compared to 12.4% in CWQ, making it more challenging despite structural differences. (4) Both CWQ and WebQSP are based on Freebase, making it unnecessary to evaluate both. The results are reported in Figure 2, indicat-ing a rise in performance when employing each constrained decoding strategy as the beam size increases. OntoSCPrompt, equipped with all constrained decoding strategies, achieves su-perior performance, showing the effectiveness of our proposed decoding strategies for KGQA generalization."}, {"title": "V. RELATED WORK", "content": "Our approach is in line with methodologies employing a two-stage architecture for semantic parsing tasks, akin to works such as Coarse2Fine [36], STaG-QA [18], and HGNet [32], where questions are first mapped to an initial outline and then filled in details later. However, they over-look condition expressions or constraints in SPARQL queries, whereas OntoSCPrompt's SPARQL structure representation is more comprehensive, enhancing KGQA generalization. Most existing KGQA systems lack generalization as they are either typically tailored to a specific KG or focus only on within-a-KG generalization. While some methods have demonstrated limited ability to generalize across KGs, par-ticularly in handling assertion heterogeneity between datasets such as WebQSP (Freebase) and MetaQA (Wikimovies), they fail to generalize across different schemas or topologies. LLMs also suffer from issues like hallucinations and factual inaccuracy when answering questions [16], specifically in the knowledge-intensive task KGQA [13], [14]. Some studies [15], [37] resort to KG-augmented prompt, i.e., injecting question-related factual information (e.g., KG triples) into predefined templates. Hallucinations still remain in the context of KGQA generalization as they adapt to heterogeneous KGs. In this work, we integrate the ontology verbalized in a unified way into the prompt and guide LLMs to fulfil our task, facilitating reasoning over KG ontology."}, {"title": "VI. CONCLUSION", "content": "We propose OntoSCPrompt, a novel KGQA model that can generalize across multiple KGs. Regarding similarities and differences between heterogeneous KGs, we employ a two-stage approach that separates semantic parsing from KG-dependent interactions. In the structure stage, the model predicts a SPARQL query sketch. In the content stage, the model fills the structure with KG-specific information. We also employ an ontology-guided hybrid prompt learning strat-egy where the KG ontology is integrated into the learning process of hybrid prompts, which we prove is effective in mitigating KG heterogeneity and facilitating KGQA general-ization. Meanwhile, we propose several decoding strategies tailored to different stages to further improve the performance. We evaluate OntoSCPrompt on diverse KGQA datasets from different KGs for KGQA generalization. Experimental results demonstrated its effectiveness."}, {"title": "LIMITATIONS", "content": "Despite achieving state-of-the-art or competitive accuracy on KGQA benchmarks for generalization across multiple KGs, OntoSCPrompt still exhibits several limitations: 1) Direc-tionality of Relations: The directionality of relations poses a challenge. For instance, in Freebase, the relation \"capi-tal\" connects a country entity to a city entity as \"(Ger-many),(capital),(Berlin)\u201d while in other KGs it might be rep-resented as \"(Berlin),(capital),(Germany)\u201d. These variations in relation to directionality can result in errors. 2) Differences in SPARQL Query Writing Style: Different human anno-tators may annotate the same question using varying writing styles or SPARQL grammar instantiations. This diversity in annotation can restrict OntoSCPrompt's ability to general-ize across multiple KGs. 3) Verbose Naming Convention: Compared to the more concise conventions in DBpedia and Wikidata, in Freebase, the naming convention for schema elements tends to be verbose. This verbose approach leads to an explosive increase in the LLM's context length, posing challenges during training."}]}