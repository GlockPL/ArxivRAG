{"title": "MUSICLIME: EXPLAINABLE MULTIMODAL MUSIC UNDERSTANDING", "authors": ["Theodoros Sotirou", "Vassilis Lyberatos", "Orfeas Menis Mastromichalakis", "Giorgos Stamou"], "abstract": "Multimodal models are critical for music understanding tasks, as they capture the complex interplay between audio and lyrics. However, as these models become more prevalent, the need for explainability grows understanding how these systems make decisions is vital for ensuring fairness, reducing bias, and fostering trust. In this paper, we introduce MUSICLIME, a model-agnostic feature importance explanation method designed for multimodal music models. Unlike traditional unimodal methods, which analyze each modality separately without considering the interaction between them, often leading to incomplete or misleading explanations, MUSICLIME reveals how audio and lyrical features interact and contribute to predictions, providing a holistic view of the model's decision-making. Additionally, we enhance local explanations by aggregating them into global explanations, giving users a broader perspective of model behavior. Through this work, we contribute to improving the interpretability of multimodal music models, empowering users to make informed choices, and fostering more equitable, fair, and transparent music understanding systems.", "sections": [{"title": "1. INTRODUCTION", "content": "As artificial intelligence (AI) continues to evolve, researchers are increasingly focusing on multimodal approaches to harness the strengths of deep learning (DL) across diverse types of data. These multimodal models integrate various data sources, such as text, audio, and images, to enhance accuracy and make better use of the available data [1]. In the Music Information Retrieval (MIR) domain, multimodal approaches are becoming increasingly prominent as they combine audio and lyrical data to achieve more precise music analysis [2]. This includes tasks such as mood classification [3], emotion recognition [4], and music auto-tagging [5]. However, the complexity of multimodal models amplifies transparency challenges. The interaction between modalities makes understanding their decisions harder, adding to the transparency issues already present in unimodal systems. This lack of interpretability can obscure the decision-making process, impacting the reliability and fairness of the models.\nExplainable AI (XAI) has emerged as a crucial area of research focused on enhancing the interpretability and transparency of machine learning models. XAI methods are essential for understanding how models make decisions, thereby improving user trust and facilitating responsible AI deployment [6]. Among these methods, Local Interpretable Model-agnostic Explanations (LIME) stands out as a seminal and widely accepted approach in the XAI field [7]. It provides local explanations by systematically perturbing input features and observing how predictions change, offering a valuable tool for examining model behavior at the instance level. Recent advances in the area include AUDIOLIME, a variant of LIME adapted specifically for the audio domain, which applies the same principle to audio-specific features [8]. In the music domain, XAI methods have been applied to interpret models through attention mechanisms [9], perceptual musical features [10], genre and spectral prototypes [11, 12], and concept-based explanations [13].\nWhile existing XAI methods have advanced explainability in the music domain, there is a notable gap in approaches tailored to multimodal models, particularly in music, which combines both audio and lyrical data. Multimodal explainability offers a significant advantage over unimodal methods by providing a more comprehensive understanding of how different modalities interact within a model's decision-making process. Unlike unimodal approaches, which analyze each modality in isolation and can lead to incomplete or misleading insights, multimodal explanations enable a holistic overview of the model's behavior by revealing the contributions and interactions between features from different modalities. This allows users to better understand the intricate dynamics between, for example, lyrical content and audio features in music. Several studies have explored XAI methodologies for multimodal settings [14], including the development of a multimodal LIME approach for image and text sentiment classification [15]. However, these methodologies have yet to be fully applied to the MIR domain, leaving a gap in explainability for multimodal music models.\nIn this paper, we introduce MusicLIME, a model-agnostic feature importance explanation method specifically designed for multimodal music understanding systems. As part of our methodology, we curated two datasets tailoring them for multimodal music emotion and genre recognition and developed a transformer-based multimodal model to tackle these challenges. MusicLIME addresses the challenge of explaining the interactions between audio and lyrical features, providing a more comprehensive view of how these modalities contribute to predictions. Additionally, we provide global explanations by aggregating local explanations, offering a broader understanding of the model's overall behavior. All code, implementation details, and instructions for reproducing the results are available in our GitHub repository 1."}, {"title": "2. METHODOLOGY", "content": "2.1. Model Architecture\nWe experimented with two modalities: text (lyrics) and audio, utilizing language model for text and a audio model respectively. These two transformer-based models were combined into a single multi-"}, {"title": "2.2. Unimodal and Multimodal Explainability", "content": "In this study, we selected LIME as the foundation for our explainability approach due to its simplicity, widespread adoption, and proven effectiveness in providing intuitive model explanations. LIME has been successfully adapted to various domains and modalities, including images, audio, and text. In the music domain, the two primary modalities of interest are text and audio. For text-based models, LIME assigns importance scores to individual words, indicating their contribution to the final prediction. In the audio domain, while spectrograms can be treated as images to highlight important parts using LIME, such explanations are often difficult to understand or interpret. A more suitable approach is AUDIOLIME, a specialized version that segments audio into meaningful time intervals and isolates components like vocals or instruments, resulting in more comprehensible and intuitive explanations.\nWhile the aforementioned approaches provide useful explanations for unimodal models, the multimodal nature of music requires an adaptation that can capture the intricate interplay between its different modalities. To address this limitation, we created MUSI-"}, {"title": "2.3. Global Aggregations of Local Explanations", "content": "To gain a comprehensive understanding of the model's behavior beyond individual instances, generating class-wide explanations, we implemented Global Aggregations of Local Explanations as described in [20]. In our work we apply two methods: (1) Global Average Importance, and (2) Global Homogeneity-Weighted Importance.\nThe Global Average Class Importance is calculated as:\n$I_{c}^{AVG} = \\frac{\\sum_{i \\in S_c} \\sum_j |W_{ij}|}{\\sum_{i \\in S_c} (\\sum_j |W_{ij}| > 0)}$\nwhere $S_c$ is the set of all instances classified as class c, and $W_{ij}$ is the weight of feature j for instance i provided by LIME.\nThe second method involves calculating a normalization vector\nfor each feature j across all classes as $p_{cj} = \\frac{\\sum_{i \\in S_c} W_{ij}}{\\sum_{c \\in C} \\sum_{i \\in S_c} |W_{ij}|}$.\nThe normalized LIME importance $p_{cj}$ represents the distribution of feature j's importance across classes. The Shannon entropy of this distribution is calculated as $H_j = - \\sum_{c \\in C} p_{cj} log(p_{cj})$, measuring the homogeneity of feature importance across multiple classes."}, {"title": "Finally, the Homogeneity-Weighted Importance is:", "content": "$I_{cj}^{HW} = (\\frac{H_{max} - H_j}{H_{max} - H_{min}}) (\\frac{\\sum_{i \\in S_c} W_{ij}}{\\sum_j | \\sum_{i \\in S_c} W_{ij}|})$\nwhere $H_{min}$ and $H_{max}$ are the minimum and maximum entropy values across all features. Intuitively this method penalizes features that influence multiple classes, whereas higher weights are assigned to features that are important for specific classes.\nImplementing (1) and (2) we note that for the multimodal models, homogeneity-weighted importance does not accurately capture the influence of multimodal features. This is due to the different nature of the features. While words are distinct, audio features encapsulate different sounds. For example, a vocal feature can contain various styles ranging from soothing singing to screams and shouts. As a result, the same audio features can impact many classes for different reasons. Since Homogeneity-weighted importance punishes features that impact multiple classes, lower weights are assigned to audio features compared to the text ones, which is inaccurate. Therefore, global average class importance is more suited for multimodal analysis."}, {"title": "3. EXPERIMENTS", "content": "3.1. Datasets\nAlthough the Music Information Retrieval (MIR) community has created various multimodal datasets [21], many of which can be found on ISMIR's resource page, finding a dataset that includes both audio and lyrics remains challenging due to copyright restrictions. For this study, we curated two datasets: Music4All[22] (M4A), a multimodal dataset with both audio and lyrics, and a manually constructed multimodal subset of AUDIOSET[23], where we combined audio from AUDIOSET with lyrics sourced from external databases.\nM4A provides 30-second audio clips and lyrics for each instance, along with metadata including genre labels and valence-energy values. Using these metadata, we categorized the songs into nine distinct genres (heavy music, punk, rock, alternative rock, folk, pop, rhythm music, hip hop and electronic) based on Musicmap 5 and nine distinct emotion categories derived from Russel's circumplex model [24]. To ensure label accuracy, we cross-referenced the genre labels with Spotify's artist genre classifications, refining the dataset to include around 60,000 songs, with 50,000 reserved for training. We maintained a data split where no artist from the training and validation sets appeared in the test set, ensuring that the model was evaluated on truly unseen data for generalizability.\nTo further validate our methodology and ensure that our results are not dependent on a single dataset, we created a small multimodal dataset based on a subset of music-related recordings from AUDIOSET [23]. AUDIOSET contains descriptive labels (e.g., fireworks, harmonica) of YouTube videos' audio segments. We focused on music samples and matched the song titles with lyrics from two openly available sources 67. This process involved fetching video titles for all entries, filtering out non-song instances (such as compilations, remixes, or series episodes), extracting artist and song names"}, {"title": "3.2. Experimental Setup", "content": "Our configurations utilized NVIDIA's V100 and P100 GPUs, with 16 GB of RAM each. All models were implemented using the Py-Torch framework, with additional utility libraries provided by Hugging Face. A preprocessing step was necessary for our data. For the textual data, this involved standard data-cleaning procedures, such as converting characters to lowercase and removing punctuation. After cleaning, the text was tokenized into sequences of up to 256 tokens. For the audio data, we extracted mel-spectrograms with 128 mel bands, utilizing a window and FFT size of 512, with a sampling rate of 44100 Hz. This procedure resulted in an input MFCC $K \\in R^{S \\times P}$, where S is the number of segments and P is the number of MFCCs.\nTo generate the global aggregates, we combined the weights produced by multiple instances, each generated with a different number of perturbations. Specifically, for the M4A dataset, we aggregated the results from 640 instances for the lyrical model, 240 instances for the audio model, and 128 instances for the multimodal model. For the AudioSet dataset, we combined the results of all the instances for the language model and 232 for the audio and multimodal models. The number of perturbations per instance for the language, audio and multimodal models were 2500, 2000 and 5000 respectively. Finally, to visualize the aggregate weights of the words for each class and facilitate comparisons, we employed GloVe embeddings combined with T-SNE for dimensionality reduction."}, {"title": "4. RESULTS & DISCUSSION", "content": "Overall, the results emphasize the complementary strengths of each modality and highlight the importance of using multimodal explanations to better understand model behavior."}, {"title": "5. CONCLUSIONS & FUTURE WORK", "content": "In this study, we explored deep-learning multimodal models and introduced MUSICLIME, a novel model-agnostic multimodal explanation methodology tailored for music understanding. Our findings highlight that multimodal approaches outperform unimodal ones by leveraging the complementary information embedded in different modalities and their interactions. We also developed a global aggregation methodology that enhances the interpretation of the relationships between genres or emotions and their associated audio and lyrical features, providing a comprehensive view of the most representative characteristics of each class. We assessed the robustness of MUSICLIME through its application to two distinct datasets and tasks, demonstrating its effectiveness in various contexts.\nFuture research will focus on improving MUSICLIME by refining various modules of the pipeline, including data preprocessing, encoding techniques, as well as sample selection and perturbation strategies within the core LIME algorithm. One limitation of our current approach is that the lyrical modality is analyzed at the word level, which may overlook broader contextual meaning. To address this, we plan to explore ways to make MUSICLIME more context-aware, enabling it to capture more general ideas beyond individual words. Additionally, we will investigate alternative explanation methods, such as counterfactual explanations, and assess their applicability in a multimodal framework for music understanding."}]}