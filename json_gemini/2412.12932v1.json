{"title": "COMT: A Novel Benchmark for Chain of Multi-modal Thought on Large Vision-Language Models", "authors": ["Zihui Cheng", "Qiguang Chen", "Jin Zhang", "Hao Fei", "Xiaocheng Feng", "Wanxiang Che", "Min Li", "Libo Qin"], "abstract": "Large Vision-Language Models (LVLMs) have recently demonstrated amazing success in multi-modal tasks, including advancements in Multi-modal Chain-of-Thought (MCOT) reasoning. Despite these successes, current benchmarks still follow a traditional paradigm with multi-modal input and text-modal output, which leads to significant drawbacks such as missing visual operations and vague expressions. Motivated by this, we introduce a novel Chain of Multi-modal Thought (COMT) benchmark to address these limitations. Different from the traditional MCOT benchmark, COMT requires both multi-modal input and multi-modal reasoning output, aiming to mimic human-like reasoning that inherently integrates visual operations. Specifically, COMT consists of four categories: (1) Visual Creation, (2) Visual Deletion, (3) Visual Update, and (4) Visual Selection to comprehensively explore complex visual operations and concise expression in real scenarios. We evaluate various LVLMs and strategies on COMT, revealing some key insights into the capabilities and limitations of the current approaches. We hope that COMT can inspire more breakthroughs on introducing multi-modal generation into the reasoning process.", "sections": [{"title": "1 Introduction", "content": "Recently, large vision-language models (LVLMs) have achieved remarkable success across various multi-modal tasks (Liu et al. 2024b; Zhu et al. 2023; Qin et al. 2024b; Zhang et al. 2024b; Fei et al. 2024b). In addition, LVLMs have also emerged with amazing capabilities, especially the capability of chain-of-thought (CoT) reasoning, which can perform step-by-step reasoning (Lu et al. 2022; Chen et al. 2024b; Xu et al. 2024; Fei et al. 2023). Specifically, Zhang et al. (2023) first formally introduce the concept of Multimodal-CoT (MCOT) and extend it into a rationalizing-answering stages paradigm. Wang et al. (2024a) propose T-SciQ to distill the advanced large language models (LLMs) to smaller models for better MCoT reasoning. Building on this foundation, Zheng et al. (2024) propose DDCoT, utilizing advanced LLMs to split questions into a series of sub-questions and then answer them by LVLMs. Mondal et al. (2024) further inject the knowledge graph information into the MCOT reasoning process, reducing the hallucinations of LLMs. He et al. (2024) devise a novel latent space learning approach to acquire image features through diffusion processes, achieving more complex CoT reasoning capabilities.\nWhile remarkable success has been witnessed in MCOT, current MCOT benchmarks still follow a traditional paradigm that reads multi-modal input but can only produce single-modal reasoning output. Such a paradigm lacks integrated multi-modal reasoning output, leading to the following issues:\n(1) Missing Visual Operations. Effective multi-modal reasoning often requires visual operations. However, traditional MCOT paradigms produce only textual reasoning outputs, which greatly hinders the multi-modal reasoning. As shown in Figure 1 (a), traditional methods can express operations in language, such as \"label the angles\", but they fail to execute visual operations, omitting the actual image-processing procedure.\n(2) Vague Expressions. The adage \"a picture is worth a thousand words\" highlights the limitations of text in conveying visual reasoning conditions. As shown in Figure 1 (a), phrases like \u201c\u22201=40\u00b0\u201d are imprecise in the absence of actual annotations, failing to accurately reflect the mapping relationship between angles and measures, thus leading to ambiguity and loss of visual information.\nActually, when humans perform reasoning, they naturally integrate images into the process: using visual thought for concrete, detailed reasoning while using textual thought for abstract, logical reasoning (Lehmann et al. 2010; Lin et al. 2024; Wu et al. 2024b). Take Figure 1 (b) as an example, LVLMs can accurately locate the specific angle by generating an annotated image. By labeling the angles and drawing auxiliary lines, LVLMs can perform clearer expressions and better multi-modal reasoning. Inspired by this, in this paper, we aim to explore a new MCOT paradigm that requires generating multi-modal reasoning outputs.\nTo fill this gap, we introduce a novel Chain of Multi-modal Thought benchmark (COMT). Unlike the traditional MCOT benchmarks, COMT requires both multi-modal input and multi-modal reasoning output, aiming to enhance LVLMs' performance in concise expression and complex visual operations in real-world scenarios. Specifically, COMT contains four categories to comprehensively assess the ability of LVLMs to use multi-modal thought processes: (1) Visual Creation assesses the ability to generate images from scratch, thereby visualizing abstract problems; (2) Visual Deletion evaluates the removal of irrelevant information from given images; (3) Visual Update examines the integration of updated images while retaining prior information; (4) Visual Selection tests the selection of specific visual features for improved image comparison. The detailed compar-"}, {"title": "2 Benchmark Construction", "content": "We introduce COMT2, which aims to assess the ability of multi-modal thought, consisting of four types: Visual Creation (\u00a72.1), Visual Deletion (\u00a72.2), Visual Update (\u00a72.3), and Visual Selection (\u00a72.4). Specially, we design a specified question-answering template, which involves question, options, image, rationale, and answer, to standardize the format for all tasks within COMT. More annotation details are shown in Technical Appendix C."}, {"title": "2.1 Visual Creation", "content": "An image is worth a thousand words. As shown in Figure 2 (a), visual creation tasks emphasize generating images from textual descriptions to improve multi-modal reasoning."}, {"title": "2.2 Visual Deletion", "content": "In logical reasoning, it is crucial to eliminate redundant information and clarify the logical chain. By progressively removing visual features, LVLMs experience reduced confusion, enabling step-by-step reasoning for the final answer, as illustrated in Figure 2 (b).\n\u2022 Original Dataset: We utilize the crowd-counting task from the JHU-CROWD++ dataset (Sindagi, Yasarla, and Patel 2020), which includes images with numerous faces and corresponding boxing.\n\u2022 Step-by-Step Boxing: The most crucial aspect of crowd-counting is identifying human individuals, where faces serve as a significant visual feature. To demonstrate the marking and removal of redundant visual features, we batch-mask faces based on the boxing provided, preparing for the next operation.\n\u2022 Template-based Modification: We construct the complete sample by following the COMT template, involving inquiries about the people count in the image (question) and clarifications of the identified count (rationale), etc. The prepared images serve as the visual thought within the rationale."}, {"title": "2.3 Visual Update", "content": "Marking can help sort out the logic. LVLMs often make mistakes in reasoning due to forgetting visual features, while humans mitigate this by annotating images. Inspired by this, as illustrated in Figure 2 (c), we propose the Visual Update task to annotate the images step-by-step.\n\u2022 Original Dataset: We leverage the KILOGRAM (Ji et al. 2022) dataset to implement tangram recognition, including the tangram image and labels of both individual pieces and the whole shape.\n\u2022 Tangram Annotation: For accurate assessments, we enhance the original tangram by applying different colors to each label category which consists of multiple individual pieces. After coloring, we explicitly annotate each category with label texts.\n\u2022 Template-based Modification: Finally, we follow the COMT template to construct the whole sample and combine the enhanced images with the textual rationales to represent the multi-modal thoughts."}, {"title": "2.4 Visual Selection", "content": "Text cannot indicate the location intuitively. Accurately selecting among similar objects using text alone is challenging due to the inherent difficulty in precise location and difference descriptions. Following this intuition, we construct the Visual Selection task, as shown in Figure 2 (d).\n\u2022 Original Dataset: We construct the task from the spot-diff dataset. This dataset provides pairs of similar images and corresponding difference annotations, requiring precise identification of differences between two images.\n\u2022 Step-by-Step Annotation: According to the annotations, we extract the distinct areas of image pairs in batches, keeping the same position and size as the original images.\n\u2022 Template-based Modification: We then supplement the textual section within the template and integrate corresponding images to construct a multi-modal rationale."}, {"title": "3 Benchmark Analysis", "content": "Basic statistics As shown in Table 2, COMT comprises 3,853 samples and 14,801 images. COMT encompasses two primary domains within M\u00b3COT (Chen et al. 2024b) and four visual operations (illustrated in Figure 3 (a)) for comprehensive evaluation. Additionally, COMT requires more intricate reasoning, with an average length of 104.7 words and 7.7 steps per sample, significantly higher than ScienceQA's 48 words and 2.5 steps.\nMulti-modal diversity COMT includes a diverse array of multi-modal tasks (visual creation, visual deletion, visual update and visual selection), ranging from mathematical problems to commonsense challenges, such as geometry and recognition. Furthermore, as depicted in Figure 3 (b), COMT features a wide range of image types encompassing \u201cCulture & Art\", and \"Abstract Graph\", etc, classified by CLIP (Radford et al. 2021).\nRationale diversity As illustrated in Figure 3 (c), COMT exhibits a broad range in the number of reasoning steps. Additionally, the multi-modal thought steps also show both diversity and sufficient volume. This allows for a comprehensive evaluation across different steps within COMT."}, {"title": "4 Experiments", "content": "In our experiments, we select a range of LVLMs as backbones, including those trained on image generation tasks as well as those that are not, including Gemini-Pro (Team et al. 2023), Qwen-VL (Bai et al. 2023), LLaVA-NeXT (Liu"}, {"title": "4.2 Main Results", "content": "Table 3 presents the main results, from which we derive the following key findings:\nAll LVLMs perform poorly on the COMT. Despite Gemini achieving a 28.67% F1 score across four tasks, this performance is marginally better than the random baseline by 3.3%, indicating significant room for improvement. Additionally, except for Gemini, most models perform at or below random levels. We attribute these to the lack of multimodal reasoning in current LVLMs.\nTraditional Multimodal CoT almost completely fails on COMT. We observe that pure text-modal CoT does not attain improvement in addressing the COMT problem and even degrades the performance of most models to near-random levels. We attribute it to the fact that the inability of the model to execute specific visual logic expressions and operations results in poor performance.\nAll models fail to visualize thought in textual words. As demonstrated in Table 3, all LVLMs fail to utilize VoT effectively to improve performance. Specifically, VoT prompts LVLMs to visualize states through textual representation and results in an average accuracy decrease of 12.28%. This finding suggests that although textual representation can convey visual features, the inherent differences between modalities still constrain the expression of multi-modal thought."}, {"title": "4.3 Analysis", "content": "This section will conduct a further analysis on COMT. See Technical Appendix E for more implementation details.\nImproving the quality of rationale is essential for COMT. As illustrated in Figure 4, the quality of CoT rationale significantly impacts the COMT performance. Poor rationale quality constrains the logical coherence of LVLMs, limiting their reasoning capacities, which aligns with Chen et al. (2024b). Consequently, enhancing reasoning quality in LVLMs is a crucial area for further exploration.\nCOMT benefits from improved multi-modal thought. To assess the impact of multi-modal thought on performance within COMT, we calculate the CLIPScore (Hessel et al. 2021) to reflect the similarity between model output and each image within the ideal rationale pre-defined. Averaging these scores yields a multi-modal alignment score for each reasoning chain generated. As shown in Figure 5, there is a significant positive correlation between performance and multi-modal alignment scores across four tasks, which indicates that COMT benefits from more multi-modal thought.\nThe performance relies more on the quality of multi-modal alignment than on parameter size. As shown in Table 4 in Technical Appendix F, the IDEFICS2-8B, with fine-grained multi-modal alignment, surpasses the 13B mod-"}, {"title": "4.4 In-context Learning Explorations", "content": "In-context Learning with multi-modal input and output can effectively promote the performance in COMT. As shown in Figure 6, using in-context learning (ICL) (Li et al. 2023a; Qin et al. 2024a) with multi-modal input and multimodal output demonstrations significantly improves performance. It not only surpasses zero-shot prompting but also outperforms ICL with text-modal output. This approach can be successful due to the fact that LVLMs can learn to effectively facilitate multi-modal thought through such demonstrations, even though Gemini is limited to producing rationales in the textual modality alone.\nNot more demonstrations means better performance in COMT. As shown in Figure 6, the model exhibits a significant downward trend in performance when the number of demonstrations exceeds four. It shows that more demonstrations are not necessarily better, as multimodal demonstrations often require the consumption of a substantial number of tokens, which can also lead to more complex challenges associated with longer contexts."}, {"title": "4.5 Error Analysis", "content": "Insufficient Multi-modal Thought. When dealing with multi-modal problems, models struggle to integrate multimodal thought most of the time. As illustrated in Figure 7, we observe that despite certain models (e.g., GILL, NEXT-GPT, AnyGPT) being trained on image generation tasks, at least 48% of their reasoning processes do not incorporate image generation. This occurs even when image generation is crucial for accurate outcomes, indicating a disjunction between image generation and text processing.\nInaccurate Textual Reasoning. When logical errors occur in textual reasoning, they hinder the advancement towards the correct answer. For example, Figure 10 in Technical Appendix reveals that the model demonstrates poor rea-"}, {"title": "4.6 Future Directions", "content": "Based on the above analysis, we summarize the future directions for current LVLMs tackling COMT.\nHow can we effectively integrate multi-modal thought reasoning? The absence of visual thought significantly increases the difficulty when addressing certain multi-modal tasks, such as COMT. How to enable models to integrate multi-modal reasoning is an intriguing research topic. Furthermore, given the inherent differences between textual and visual modalities, exploring how to align these two modalities during reasoning presents another valuable challenge.\nHow can we enhance logical reasoning capabilities for textual reasoning? The inadequacies in textual reasoning logic lead to inaccurate conclusions during inference, such as calculation mistakes. Therefore, how to enable models with better textual logic to perform effective text reasoning is a critical topic to explore.\nHow can we achieve effective vision logic for visual reasoning? Since some generated images fail to perform effective visual logic or even be irrelevant, not all visual thoughts generated have a positive influence on the reasoning. How to enable models to develop better visual logic to produce images that are relevant and consistent with the progression of rationale is a topic worth exploring."}, {"title": "5 Related Work", "content": "The emergence of Multi-modal Chain-of-Thought (MCOT) techniques elicits the step-by-step zero-shot and few-shot multi-modal reasoning capabilities of Large Vision-Language Models (LVLMs) (Wang et al. 2024c,b; Chen"}, {"title": "6 Conclusion", "content": "In this work, we introduce a Chain of Multi-modal Thought (COMT) benchmark to evaluate and improve the multi-modal reasoning capabilities of Large Vision-Language Models (LVLMs). Through extensive experiments, our findings reveal a significant performance gap between LVLMs and human, with models generally not outperforming random chance in zero-shot scenarios. In-context Learning with multi-modal rationale emerges as a promising approach to better integrate visual and textual reasoning in LVLMs. We hope this research lays the groundwork for future enhancements in multi-modal reasoning technologies."}, {"title": "A Background", "content": "As shown in Figure 9 (a), the traditional multi-modal Chain-of-Thought (MCoT) involves generating a text-modal rationale, based on a multi-modal input \\(X_{inp} = [X_{txt}; X_{vis}]\\). The LVLM generates a textual rationale step \\(R_{txt}^{i}\\) based on the rationales from the previous \\(i - 1\\) steps rationales \\(R_{txt}^{<i}\\). This process can be mathematically represented as:\n\\[R_{txt}^{i} = \\underset{R_{txt}}{argmax}\\ \\pi(R_{txt}^{i} | X_{inp}, R_{txt}^{<i}),\\]\nwhere \\(\\pi(\u00b7)\\) denotes the probability of the model generating the rationale \\(R_{txt}^{i}\\) from the vocabulary of textual tokens."}, {"title": "A.2 Chain of Multi-modal Thought", "content": "Unlike the traditional MCoT, Chain of Multi-modal Thought (COMT) incorporates visual thought into rationale generation. Formally, as shown in Figure 9 (b), given an multimodal input \\(X_{inp}\\), the model generates a multi-modal rationale step \\(R^{i}\\), which can be defined as:\n\\[R^{i} \\underset{R_{vis}}{argmax}\\ \\pi(R_{vis}^{i} | X_{inp}, R^{<i}), \\ if\\  \\pi^{v} > \\pi^{t}\\\\ \\underset{R_{txt}}{argmax}\\ \\pi(R_{txt}^{i} | X_{inp}, R^{<i}), \\ if\\  \\pi^{v} < \\pi^{t}\\]\nwhere \\(\\pi(\u00b7)\\) represents the probability that the model generates a rationale step with visual information, such as images or detailed descriptions of visual concepts."}, {"title": "B Quality Assurance", "content": "We adopt Onboarding Test and Human Recheck method to ensure the quality of annotated data.\nOnboarding Test All annotators must complete a preliminary test involving the annotation of 100 samples. Their annotations are assessed by three experts, and only those who achieve an accuracy of at least 85% are allowed to continue to subsequent annotation tasks.\nHuman Recheck Following the onboarding test, annotators are required to recheck all data twice. This step ensures that each sample meets the multi-modal thought criteria and possesses coherent logical rationale. Only samples in COMT for which at least two annotators agree are accepted. The kappa coefficient among annotators reaches 0.93, indicating perfect agreement (Landis and Koch 1977)."}, {"title": "C Annotation Details", "content": "To standardize the format of all data in the COMT dataset, we design a multiple-choice question-answering template for all 4 tasks. The template includes five keys: question, option, image, rationale, and answer. The specific content of the template is as follows:"}, {"title": "G Irrelevant Image and Text Logic", "content": "Effective vision logic is crucial for visual reasoning. However, we observe that LVLMs sometimes generate irrelevant text and image logic across COMT tasks, which hinders the reasoning process. This highlights the challenges current LVLMs face in integrating effective visual logic for visual reasoning. Specific examples are as follows."}, {"title": "G.1 Accurate Text with Inaccurate Image", "content": "During the experiments, we observe cases where LVLMs generate accurate textual reasoning but produce images irrelevant to the problem. As shown in Figure 11(a), NExT-GPT (?) are expected to generate an image that contains geometric shapes consistent with the rationale. However, due to the lack of effective visual logic, LVLMs produce images that only contain textual content, which does not aid in promoting visual reasoning."}, {"title": "G.2 Accurate Image with Inaccurate Text", "content": "There are cases where LVLMs generate accurate images according to task requirements but produce text descriptions inconsistent with these images. As shown in Figure 11(b), AnyGPT (Zhan et al. 2024) generate an image of a fox consistent with the correct answer; however, the rationale determines the answer to be a beagle. This reflects the current LVLMs struggle to perform further reasoning based on the generated images, indicating a lack of effective visual logic."}, {"title": "H Ethical Considerations", "content": "Data Access We collect data from GeoQA+ (Cao and Xiao 2022), JHU-CROWD++ dataset (Sindagi, Yasarla, and Patel 2020), KILOGRAM (Ji et al. 2022) and online websites. These datasets are all open-source and permitted for academic research, complying with ethical commitments for data usage.\nParticipant Recruitment We recruit participants from multiple universities and require each participant to meet a language proficiency requirement of either passing the CET-6 exam or scoring 6 or above on the IELTS. Additionally,"}]}