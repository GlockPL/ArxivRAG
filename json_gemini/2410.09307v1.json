{"title": "Graph Neural Alchemist: An innovative fully modular architecture for time series-to-graph classification", "authors": ["Paulo Coelho", "Raul Araju", "Lu\u00eds Ramos", "Samir Saliba", "Renato Vimieiro"], "abstract": "This paper introduces a novel Graph Neural Network (GNN) architecture for time series classification, based on visibility graph representations. Traditional time series classification methods often struggle with high computational complexity and inadequate capture of spatio-temporal dynamics. By representing time series as visibility graphs, it is possible to encode both spatial and temporal dependencies inherent to time series data, while being computationally efficient. Our architecture is fully modular, enabling flexible experimentation with different models and representations. We employ directed visibility graphs encoded with in-degree and PageRank features to improve the representation of time series, ensuring efficient computation while enhancing the model's ability to capture long-range dependencies in the data. We show the robustness and generalization capability of the proposed architecture across a diverse set of classification tasks and against a traditional model. Our work represents a significant advancement in the application of GNNS for time series analysis, offering a powerful and flexible framework for future research and practical implementations.", "sections": [{"title": "1 Introduction", "content": "Time Series are all around us: from financial markets and weather forecast-ing to physiological signals such as electrocardiograms (ECGs), the ubiquitous nature of time series has sparked increasing interest in applying Deep Neural Net-work (DNN) models for analysis. Those models, however, rely heavily on feature extraction techniques and often fail to capture the intrinsic and complex spatio-temporal dynamics inherent in time series data, besides being computationally expensive and lacking interpretability. To address that, time series-to-graph rep-resentation has emerged as a powerful tool to represent spatial relationships"}, {"title": "2 Related Works", "content": "In the Great Time Series Classification Bakeoff [1], the authors conducted an extensive review of time series classification algorithms based on similarity, interval, and shapelet representations, ranking the best ones in terms of mean ac-curacy on the UCR Archive datasets. One such traditional approach, ROCKET (RandOm Convolutional KErnel Transform) [7], has shown remarkable perfor-mance across a wide range of time series classification tasks. ROCKET utilizes a large number of random convolutional kernels to transform the time series data, followed by a simple linear classifier. This approach has been noted for its efficiency and effectiveness, particularly in balanced datasets.\nDespite their effectiveness, these models exhibit high time complexity and rely heavily on feature extraction to represent and learn from data, failing to"}, {"title": "3 Theoretical Background", "content": "In this section we present the theoretical background needed to understand our work. We start by discussing Visibility Graph, which is used to time series-to-graph representation, followed by a brief explanation on the GNNs model and the Neural Message Passing Framework as defined by Hamilton [8]."}, {"title": "3.1 Visibility Graph", "content": "Proposed by Lacasa et al. [11], Natural Visibility Graph (NVG) is a method of time series-to-graph representation that preserves the structure and dynamics of the original series. Formally, the NVG defines a graph G = (V,E), where vertices represent values of the time series and edges are defined based on a visibility criterion between those values.\nInformally, we can put the general idea of NVG as follows: given a time series T of dimension m, its values {t1, t2,...tm} are projected onto a Cartesian plane, where the temporal dimension of the series corresponds to the x-axis and its absolute value to the y-axis. More formally, two arbitrary data values A(ta, Ya) and B(tb, yb) are mutually visible if there is no point C(tc, yc) between A and B that is above the line connecting A and B, as defined by the equation:\nIt is worth mentioning that resulting NVG is:\nUndirected: The visibility between two points is symmetric;\nConnected: Adjacent points are always visible to each other;\nInvariant under affine transformations: The Visibility criterion is in-variant under rescaling of both x and y's axis and under horizontal and vertical translations.\nLan et al. [12] proposed an efficient methodology that computes VGs in time O(nlogn), where n = |V|, making it more practical and efficient for large-scale time series data."}, {"title": "3.2 Graph Neural Networks", "content": "GNNs work directly on graph structure and compute vertex representations by aggregating neighboring node embeddings. The input for a GNN is a set of node feature vectors, which are real-valued characteristics of each node in the graph. GNN models aim to learn low-dimensional representations of the nodes that summarize their graph position and the structure of their local graph neighborhood into a latent space z \u2208 Rd, where d is the dimension of the feature vector [9].\nGNNs can be understood by an Encoder-Decoder perspective, where the Encoder aggregates and updates information for every node in the graph based on its local neighborhood; learning a hidden embedding representation of the node features that is used by the Decoder to reconstruct information about each node's neighborhood in the original graph to perform downstream tasks such as node or graph classification [8].\nThe Neural Message Passing Framework is fundamental to understand how the GNN model learns hidden node embeddings, and can be expressed as: given a graph G(V,E) and the node feature embeddings {hu \u2208 Rd} where d is the feature dimension, for each node u \u2208 V of G, information about the current node u and its neighbourhood N(u) are iteratively exchanged and updated across the edges that connects them, producing an hidden embedding h(k) that summarizes the information of the node and its neighbourhood at the k-th iteration. The message passing framework can be formalized as:\nwhere UPDATE and AGGREGATE are differentiable arbitrary functions such as Neural Networks, my(u) is the \"message\" aggreagated from the neigh-bourhood graph N(u) of u and k is the iterate step. The initial node embeddings h(0) are the input features of the graph, and superscripts are used to distinguish embeddings and functions at different steps of the message passing interations.\nAmong the numerous GNN models, GraphSAGE (SAmple and AggreGate), is a successful variant of the vanila GCN that performs well in large graphs and, learn the embeddings in a inductive manner, generalizing for nodes that were not present during training. Based on a sampling and aggregation strategy of node neighbors that updates node embeddings through random subsets of sampled neighbors, GraphSAGE effectively reduces the dimension of the graph during training and improves the learning capacity of the model. The modified message passing equation for GraphSAGE is:"}, {"title": "4 The Graph Neural Alchemist architecture", "content": "Designed to classify time series data represented as graphs, the Graph Neural Alchemist (GNA) architecture is fully modular and is comprised of four primary modules: the Graph Representation Module, the Graph Neural Network (GNN) Module, the Readout Layer, and the Multilayer Pooling Perceptron (MLPP) Module. Each module is highly customizable, allowing for easy substitution or even supression depending on the data domain and specific task requirements. The architecture is depicted in Figure 1."}, {"title": "4.1 Graph Representation Module", "content": "We define a Directed Acyclic Graph (DAG) G(V,E) where the nodes corre-spond to all the values from the time series, and the edges are established by the visibility criterion of equation 1. Using left-to-right directed edges and NVG to represent time series enables the model to better capture the temporal dynamics of the series as they progress over time and space.\nFor the initial node features, the representation encodes two key metrics: In-degree and PageRank. The in-degree feature quantifies the number of edges incident on a node, reflecting its local importance and connectivity. Nodes with higher in-degree are central within their local neighborhoods and the informa-tion from this feature is crucial in understanding local patterns and anomalies in the data.\nConversely, the PageRank feature measures the global importance of each node within the entire graph, based on the number and significance of incoming con-nections. This dual encoding ensures that each node captures both local and global importance, effectively representing the intrinsic relationships within the time series, and the relationships between past and future data points."}, {"title": "4.2 Graph Neural Network (GNN) Module", "content": "The GNA architecture employs a 4-layer GraphSAGE network, which ag-gregate features from sampled neighbors to learn the hidden embeddings of the graph via the neural message passing framework (Equation 3). We used the Leaky ReLU activation function between each layer to introduce non-linearity and enhance learning capabilities, defined as:\nwhere a controls the angle of the negative slope and is set to the default value of le - 2.\nBy employing GraphSAGE, we ensure that every possible neighbor, carrying local (in-degree) and global (PageRank) information, contributes to the node's updated representation, allowing the model to capture the evolution of the time series and learn its spatio-temporal dynamics. The choice for 4 layers was to avoid oversmothing, a common issue in deep GNNs, where successive message passing iterations cause the model to converge into indistinguishable vectors for all nodes, losing the local information."}, {"title": "4.3 Readout Layer", "content": "Following the GNN module, a readout layer aggregates the hidden features from all nodes to produce a single vector representation of the graph. This step is essential for graph classification tasks as it summarizes the learned node em-beddings, encoding the information of the entire graph into a form suitable for"}, {"title": "4.4 Multilayer Pooling Perceptron (MLPP) Module", "content": "The Multilayer Pooling Perceptron (MLPP) module in our architecture per-forms the classification step as well as the pooling operation, a strategy borrowed from Convolutional Neural Networks (CNNs) to reduce the graph's dimension-ality while preserving its essential information. While some GNN models incor-porate pooling between their layers to coarsen the graph, we apply it afterwards in the MLPP module to keep the original graph structure and to enhance mod-ularity.\nEach layer in the MLPP module halves the dimension of the hidden features during the feedforward step and the final layer outputs the predicted probabilities for each class that is then passed through a log softmax activation function to obtain the logit values. To introduce non-linearity, a Leaky ReLU activation function is used between each layer. The log softmax function is defined as:\nwhere h is the learned embedding vector of the graph and i is the index of the class.\nThe weights of the networks are learned using the Adam stochastic gradient descent optimizer, which optimizes the cross-entropy loss function, defined as:\nwhere yi is the true label and \u011di is the predicted probability for class i.\nConsidering the VG representation, GraphSAGE and MLPP used here, the overall time complexity of the architecture is O(M \u00d7 (E + N)), where M is the number of time series, E is the number of edges, and N is the number of nodes in the graph, which is far more efficient than traditional time series classification models."}, {"title": "5 Experimental Results", "content": "We implement our GNA architecture using PyTorch \u00b9 and the Deep Graph Library (DGL) 2 for deep learning and graph-based computations, and the ts2vg"}, {"title": "6 Conclusion and future works", "content": "Here we proposed the Graph Neural Alchemist (GNA) architecture, a mod-ular framework for time series classification based on visibility graph represen-tations and GNNs. Our architecture is designed to capture the spatio-temporal dynamics of time series data by encoding them as directed visibility graphs and learning their hidden features using a GraphSAGE network. We evaluated the GNA architecture on a variety of time series datasets from the UCR Time Series Classification Archive, comparing its performance against the ROCKET model. Our results show that the GNA architecture outperforms ROCKET in some specific cases, demonstrating its robustness in handling noisy data, large time series, and scenarios with limited training data. However, the GNA architecture struggles with imbalanced datasets and subtle spatial variations, indicating areas for improvement.\nFor future work, we aim to demonstrate the representation-agnostic capabil-ity of our architecture by testing it with other time series-to-graph representa-tions, such as the Ordinal Pattern Transition Graph (OPTG), and a different classificator, such as Xgboost, to further enhance its performance. This would allow us to verify the generalization and robustness of the proposed architecture, potentially leading to a more versatile framework. Additionally, we are currently evaluating its performance on a larger real-world health dataset, to assess its applicability in clinical settings and robustness capabilities.\nFurthermore, extending the classification module to evaluate the architecture on a variety of tasks, including node classification, community detection, and link prediction, would be a promising line of work. These experiments could help assess the generalization capability of the proposed architecture across different types of tasks, broadening its applicability and demonstrating its versatility."}]}