{"title": "Interpreting Neural Networks through Mahalanobis Distance", "authors": ["Alan Oursland"], "abstract": "This paper introduces a theoretical framework that connects neural network linear layers with the Mahalanobis distance, offering a new perspective on neural network interpretabil-ity. While previous studies have explored activation functions primarily for performance optimization, our work interprets these functions through statistical distance measures, a less explored area in neural network research. By establishing this connection, we provide a foundation for developing more interpretable neural network models, which is crucial for applications requiring transparency. Although this work is theoretical and does not include empirical data, the proposed distance-based interpretation has the potential to enhance model robustness, improve generalization, and provide more intuitive explanations of neural network decisions.", "sections": [{"title": "1 Introduction", "content": "Neural networks have revolutionized machine learning, achieving remarkable success across diverse applications. Central to their efficacy is the use of activation functions, which introduce non-linearity and enable the modeling of complex relationships within data. While Rectified Linear Units (ReLU) have gained prominence due to their simplicity and effectiveness [Nair and Hinton, 2010], the exploration of alternative activation functions remains an open and valuable area of research [Ramachandran et al., 2018].\nNeural network units are often viewed as linear separators that define decision boundaries between classes [Minsky and Papert, 1969] with larger activation values suggesting stronger contributions of features to those decisions. Our work challenges this perspective, exploring how individual neurons can be understood through the lens of statistical distance measures. Clustering techniques use distance measures. They aim to minimize the distance between data points and feature prototypes, with smaller values indicating stronger membership to the feature or cluster [MacQueen, 1967a]. We explore the intersection between these perspectives on activation interpretations, leveraging the distance-minimization approach of clustering techniques to lay the groundwork for novel neural network designs based on statistical distance measures.\nThis paper establishes a novel connection between neural network architectures and the Mahalanobis distance, a statistical measure that accounts for the covariance structure of data [Mahalanobis, 1936]. We present a robust mathematical framework that bridges neural networks with this statistical distance measure and lays the groundwork for future research into neural network interpretability and design. Our key contributions are:\n1. We establish a mathematical connection between neural network linear layers and the Mahalanobis distance, demonstrating how Absolute Value (Abs) activations facilitate distance-based interpretations."}, {"title": "2 Background and Related Work", "content": "2. We analyze the solution space that neural networks are likely to learn when approximating\nMahalanobis distance, exploring the effects of non-uniqueness in whitening transformations\nand the role of Abs-activated linear nodes.\n3. We discuss the broader implications of this framework for neural network design and\ninterpretability, laying the groundwork for more interpretable models."}, {"title": "2.1 Activation Functions", "content": "Activation functions introduce non-linearity in neural networks, enabling them to model complex data relationships. The field has evolved from early sigmoid and hyperbolic tangent functions [Rosenblatt, 1958] to the widely adopted Rectified Linear Unit (ReLU) [Nair and Hinton, 2010], which mitigates the vanishing gradient problem in deep networks [Glorot and Bengio, 2010, Krizhevsky et al., 2012].\nReLU variants that address its shortcomings include Leaky ReLU [Maas et al., 2013], Parametric ReLU (PReLU) [He et al., 2015], and Exponential Linear Unit (ELU) [Clevert et al., 2016]. Additionally, newer activation functions like Swish [Ramachandran et al., 2018] and GELU [Hendrycks and Gimpel, 2016] have been proposed to further enhance network performance and training dynamics.\nTanh and Sigmoid activations are still useful in many architectures such as recurrent neural networks (RNNs) and Long Short-Term Memory (LSTM) networks [Hochreiter and Schmidhuber, 1997].\nThe variety of activation functions used in modern networks reflects the diverse needs of different architectures. The exploration of activation functions remains an active area of research, with ongoing investigations into their impact on neural network performance, generalization, and interpretability [Ramachandran et al., 2018]. Despite extensive research, the interpretation of activation functions in terms of statistical measures remains an open area of investigation."}, {"title": "2.2 Overview of Distance Metrics in Clustering", "content": "Distance metrics are fundamental in clustering algorithms, determining how similarity between data points is measured. Various clustering methods employ different distance measures:\n\u2022 K-Means typically uses Euclidean distance (l2 norm), assuming spherical clusters and equal feature importance [MacQueen, 1967b].\n\u2022 Gaussian Mixture Models (GMMs) employ Mahalanobis distance, accounting for data covariance and modeling elliptical clusters [Reynolds, 2009].\n\u2022 Radial Basis Function (RBF) networks use Gaussian-like activations based on Euclidean distance, creating spherical clusters around learned centers [Broomhead and Lowe, 1988].\n\u2022 Hierarchical and Agglomerative Clustering can use various metrics (Euclidean, Manhattan, correlation-based), affecting dendrogram shape [Murtagh, 1983].\n\u2022 DBSCAN, while often using Euclidean distance, can employ any metric for density-based clustering [Ester et al., 1996].\n\u2022 Spectral Clustering incorporates similarity measures like Gaussian kernel functions [Von Luxburg, 2007]."}, {"title": "2.3 Neural Network Interpretability and Statistical Models", "content": "The interpretability of neural networks remains a critical challenge, often referred to as the \"black-box\" nature of these models [Lipton, 2016]. In applications requiring transparency, such as healthcare and finance, understanding the decision-making processes of neural networks is paramount [Rudin, 2019]. Various approaches have been developed to enhance interpretability, including feature visualization, saliency maps, and prototype-based methods [Erhan et al., 2009, Simonyan and Zisserman, 2013, Kim et al., 2018a].\nRecent advancements in explainable AI (XAI) have introduced tools like SHAP (SHapley Additive exPlanations) [Lundberg and Lee, 2017] and LIME (Local Interpretable Model-Agnostic Explanations) [Ribeiro et al., 2016], which provide both local and global insights into model predictions. SHAP offers a consistent approach to feature attribution grounded in cooperative game theory, providing robust explanations even for complex models like deep neural networks [Lundberg and Lee, 2017, Markov, 2020]. LIME, by contrast, offers localized interpretability by approximating the model's behavior with simpler, interpretable models, making it particularly useful for individual predictions [Ribeiro et al., 2016, Ali et al., 2024].\nConnections between neural networks and statistical models, such as Bayesian neural networks [Neal, 1996, Blundell et al., 2015], continue to provide a probabilistic framework for understanding model uncertainty. Moreover, concept-based interpretability methods like TCAV (Testing with Concept Activation Vectors) [Kim et al., 2018b] have emerged, allowing for a more granular analysis of what neural networks learn, which can be crucial in high-stakes domains like healthcare [Hanif et al., 2024].\nWhile significant strides have been made in explaining model behavior, there remains a gap in establishing direct mathematical connections between neural network components, specifically activation functions, and statistical distance measures like the Mahalanobis distance. Addressing this gap can provide deeper insights into feature learning and decision-making processes, enhancing both interpretability and robustness of neural network models."}, {"title": "3 Mathematical Framework", "content": "Gaussians fall out of second-order Taylor series approximations [Bishop, 2006, Section 4.4], making them effective for modeling data, even when the data is not explicitly Gaussian. Gaussian mixtures can serve as piecewise linear approximations of complex distributions and surfaces. They are well-suited for modeling point clouds such as the ones neural networks are trained on. In this section, we develop the mathematical foundation that connects neural networks to the Mahalanobis distance, thereby providing a framework for interpreting neural network operations through the lens of statistical distance metrics. We begin by revisiting key concepts related to Gaussian distributions and the Mahalanobis distance, followed by a detailed exploration of how neural network components, particularly linear layers and activation functions, can approximate these distance metrics. This framework not only enhances our understanding of neural network behavior but also lays the groundwork for leveraging statistical principles to improve network interpretability and training dynamics."}, {"title": "3.1 Mahalanobis Distance for a Multivariate Gaussian Distribution", "content": "A multivariate Gaussian (Normal) distribution is a fundamental concept in statistics, describing a $d$-dimensional random vector $x \\in \\mathbb{R}^d$ with a mean vector $\\mu \\in \\mathbb{R}^d$ and a covariance matrix $\\Sigma \\in \\mathbb{R}^{d\\times d}$ [Bishop, 2006]. We denote this distribution as $x \\sim \\mathcal{N}(\\mu, \\Sigma)$.\nThe Mahalanobis distance quantifies the distance between a point $x$ and the mean $\\mu$ of a distribution, while considering the covariance structure of the data [Mahalanobis, 1936, De Maesschalck et al., 2000]. It is defined as:\n$D_M(x) = \\sqrt{(x - \\mu)^T\\Sigma^{-1}(x - \\mu)}.$ \nThis metric adjusts for variance across dimensions by effectively whitening the data, resulting in a spherical distance measure."}, {"title": "3.2 Principal Component Analysis (PCA)", "content": "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms data into a new coordinate system, emphasizing directions (principal components) that capture the most variance [Jolliffe, 2002]. When performing PCA on the covariance matrix $\\Sigma$, it is decomposed using eigenvalue decomposition:\n$\\Sigma = V\\Lambda V^T,$ \nwhere:\n\u2022 $V = [v_1, v_2, ..., v_d]$ is a matrix whose columns are the orthogonal unit eigenvectors of $\\Sigma$.\n\u2022 $\\Lambda = diag(\\lambda_1, \\lambda_2, ..., \\lambda_d)$ is a diagonal matrix of the corresponding eigenvalues $\\lambda_i$, representing the variance along each principal component.\nSubstituting $V\\Lambda V^T$ for $\\Sigma$ in the Mahalanobis distance equation (1), we obtain:\n$D_M(x) = \\sqrt{(x - \\mu)^TV\\Lambda^{-1}V^T(x - \\mu)}.$ \nSimplify to express the Mahalanobis distance in terms of individual component contributions:\n$D_M(x) = \\sqrt{(x - \\mu)^TV\\Lambda^{-1}V^T (x - \\mu)}$\n$= \\sqrt{(V^T(x - \\mu))^T \\Lambda^{-1} (V^T (x - \\mu))}$\n$= \\sqrt{\\sum_{i=1}^d \\lambda_i^{-1} (v_i^T(x - \\mu))^2}$\n$= |\\Lambda^{-1/2} V^T (x - \\mu) |_2,$ \nwhere $|\\cdot|_2$ denotes the Euclidean ($l_2$) norm. This shows that the Mahalanobis distance can also be expressed as the $l_2$ norm of the number of standard deviations of $x$ along each principal component."}, {"title": "3.3 Connecting Neural Networks to Mahalanobis Distance", "content": "We consider the Mahalanobis distance along a single principal component.\n$D_{M,i}(x) = |\\lambda_i^{-1/2}v_i^T (x - \\mu)|,$"}, {"title": "3.4 Non-Uniqueness of Whitening", "content": "The principal components of a Gaussian distribution, as used in the Mahalanobis distance, form an orthonormal set of axes. Projecting Gaussian data onto these axes transforms the distribution from an oriented ellipsoid into a spherical Gaussian, effectively converting the data from $\\mathcal{N}(\\mu, \\Sigma)$ to $\\mathcal{N}(0, I)$. This process is known as whitening [Bishop, 2006, Section 12.1.3].\nHowever, the transformation to whitened data is not unique. Specifically, any rotation applied in the whitened space results in another valid whitening transformation. Mathematically, if $x_w$ is the whitened data, then for any orthogonal rotation matrix $R \\in SO(d)$, the rotated data $x'_w = Rx_w$ is also whitened.\nThis non-uniqueness implies that multiple sets of axes, possibly non-orthogonal in the original space, can serve as a whitening basis. When we transform the rotated basis back to the original space, we obtain a new set of basis vectors $W$ that still whiten the data but may not correspond to the original principal components and may not even be orthogonal.\nIn the context of neural networks, this means that although linear nodes can represent directions that effectively whiten the data, they are unlikely to precisely learn the actual principal components when estimating Mahalanobis distances. Instead, they may learn any basis that achieves whitening. Nevertheless, the learned hyperplanes (decision boundaries) should still pass through the data mean $\\mu$, allowing for prototype interpretation.\nTo encourage the network to learn the actual principal components, one could apply an orthogonality constraint or regularization on the weight matrices. This regularization promotes learning orthogonal directions, aligning the learned basis with the true principal components of the data clusters and providing statistically independent features."}, {"title": "4 Implications and Discussion", "content": "We discuss implications, potential impact and future work of this reframing of linear layers in neural networks. While this paper provides a robust theoretical foundation for interpreting neural networks through Mahalanobis distance and Abs activation functions, it does not include empirical results. Future work will involve validating these theoretical insights with empirical data to further assess their applicability and performance in real-world scenarios."}, {"title": "4.1 Expected Value Interpretation", "content": "The expected value, or mean, is a central concept in statistics, representing the average tendency of a distribution. In neural networks, finding the expected value for each neuron would reveal the features it recognizes. Interpreting linear nodes as approximations of Gaussian principal components provides a path towards recovering the neuron mean value. The estimated mean serves as a prototype for the feature that the neuron has learned to recognize [Li et al., 2018], representing the 'ideal' input for that neuron. This interpretation enhances the transparency of the feature extraction process, potentially leading to more interpretable models and improved architectures."}, {"title": "4.2 Equivalence between Abs and ReLU Activations", "content": "While our analysis utilizes linear layers with Abs activation functions to model deviations along principal component directions, ReLU activations can provide comparable information within the same framework.\nFor the Abs activation, each linear node computes:\n$y_{Abs} = |w^T x + b|$ \nwhere the weights $w$ and bias $b$ are set such that $w^T\\mu + b = 0$. This centers the decision boundary at the cluster mean $\\mu$, and within a confidence interval $\\delta$, the pre-activation output ranges from $-\\delta$ to $+\\delta$.\nFor the ReLU activation, we adjust the bias to shift the decision boundary just outside the cluster:\n$y_{ReLU} = max \\left(0, -w^T x - b + \\delta \\right).$ \nHere, the pre-activation output ranges from 0 to $2\\delta$ within the cluster. Although ReLU zeros out negative inputs, by negating the pre-activation and adjusting the bias, it effectively captures the magnitude of deviations similar to the Abs activation.\nThe hyperplanes defined by $w$ maintain the same orientation in both cases, providing equivalent views of the cluster. Subsequent layers can adapt to either activation's output range, making Abs and ReLU functionally comparable in capturing essential features.\nThis suggests that techniques developed for networks with Abs activations may be adaptable to ReLU activations, bridging theoretical insights with practical neural architectures commonly utilizing ReLU."}, {"title": "4.3 Activations as Distance Metrics", "content": "Traditional neural networks typically employ an \"intensity metric model,\" where larger activation values indicate stronger feature presence. In contrast, a \u201cdistance metric model\" interprets smaller activation values as indicating closer proximity to a learned feature or prototype. The following observations suggest directions for future work:\n\u2022 Most error functions (e.g., Cross Entropy Loss, Hinge Loss) are designed for intensity metrics. Output layers using Abs activation may require modification of their output values.\n\u2022 While some architectures, like Radial Basis Function networks [Broomhead and Lowe, 1988], utilize distance metrics, they are not widely adopted in modern deep learning.\n\u2022 Distance metrics conflict with the goal of sparse output layers. In a distance metric model, zero is the strongest signal, making it illogical for most outputs to have the strongest signal."}, {"title": "4.4 Model Initialization and Pretraining", "content": "Interpreting neurons as learning distances from cluster means suggests novel approaches to model initialization and pretraining. This perspective offers an alternative to standard random initialization techniques [Kamilov et al., 2017] by incorporating data-driven insights into the model's starting configuration.\nRather than initializing with random weights, an approach could involve clustering the input data (e.g., using k-means) and calculating the covariance of each cluster. Applying Principal Component Analysis (PCA) to these covariance matrices can provide a basis for directly initializing network parameters. This strategy leverages the structure of the data to guide the network's early learning stages. This approach and its approximations may offer several advantages:\n\u2022 Faster convergence by starting with parameters informed by the data distribution\n\u2022 Enhanced interpretability, as network weights are aligned with meaningful features from the outset\n\u2022 Improved generalization by incorporating information about cluster structures"}, {"title": "4.5 Model Translation and Componentization", "content": "The interpretation of neurons as principal components of Gaussians suggests a potential mapping between neural networks and hierarchical Gaussian Mixture Models (GMMs) [Jacobs et al., 1991]. By performing PCA on the clusters in a GMM, we can extract principal components, converting them directly into neurons. Conversion from neurons to Gaussian representations may also be possible. The process of directly translating between neural networks and GMMs offers several potential advantages:\n\u2022 Enhanced Interpretability: Neural networks can be better understood through their GMM equivalents, providing insights into the data distribution and feature representations.\n\u2022 Application of Statistical Techniques: Established statistical methods used in GMM analysis can be applied to neural networks, potentially improving training and evaluation.\n\u2022 Hybrid Models: Combining neural networks and GMMs can leverage the strengths of both, enhancing performance in tasks like clustering and classification.\n\u2022 Model Decomposition: Large networks might be decomposable into smaller, context-specific subnetworks, facilitating easier analysis and maintenance.\n\u2022 Efficient Storage and Computation: Subnetworks can be stored offline and dynamically loaded based on data context, improving memory efficiency and reducing computational overhead.\n\u2022 Scalability in Large-Scale Applications: This approach can lead to faster inference and more efficient resource utilization in applications dealing with massive datasets."}, {"title": "4.6 Direct use of Mahalanobis equation", "content": "Equation 5 explicitly incorporates the variance eigenvalue $\\lambda$, the unit eigenvector $v$, and the mean $\\mu$. Batch Normalization already makes use of $\\lambda$ and $\\mu$ [Ioffe and Szegedy, 2015], while the nGPT model employs unit weight vectors, which are analogous to $v$ [Loshchilov et al., 2024]. The success of these techniques suggests there might be further opportunities to decompose the standard linear layer equation $y = Wx + b$ towards the Mahalanobis equation in a way that leads to improvements in training speed and representation quality."}, {"title": "5 Conclusion", "content": "This paper establishes a novel connection between neural network architectures and the Mahalanobis distance, providing a fresh perspective on neural network interpretability. By demonstrating how linear layers with Abs activations can approximate Mahalanobis distances, we bridge the gap between statistical distance measures and neural network operations. This framework offers several key insights:\n\u2022 It provides a probabilistic interpretation of neural network nodes as learning principal components of Gaussian distributions.\n\u2022 It suggests new approaches for model initialization, pretraining, and componentization.\n\u2022 It establishes a potential homomorphism between neural networks and hierarchical Gaussian Mixture Models.\nThese findings lay the groundwork for future research into more interpretable and robust neural network architectures. By leveraging statistical principles in neural network design, we open new avenues for enhancing model transparency, improving generalization, and developing more efficient training techniques. As the field of AI continues to evolve, such interpretable frameworks will be crucial in building trustworthy and explainable AI systems."}]}