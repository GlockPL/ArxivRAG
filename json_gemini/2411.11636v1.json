{"title": "SP3: Superpixel-propagated pseudo-label learning for weakly semi-supervised medical image segmentation", "authors": ["Shiman Li", "Jiayue Zhao", "Shaolei Liu", "Xiaokun Dai", "Chenxi Zhang", "Zhijian Song"], "abstract": "Deep learning-based medical image segmentation helps assist diagnosis and accelerate the treatment process while the model training usually requires large-scale dense annotation datasets. Weakly semi-supervised medical image segmentation is an essential application because it only requires a small amount of scribbles and a large number of unlabeled data to train the model, which greatly reduces the clinician's effort to fully annotate images. To handle the inadequate supervisory information challenge in weakly semi-supervised segmentation (WSSS), a SuperPixel-Propagated Pseudo-label (SP3) learning method is proposed, using the structural information contained in superpixel for supplemental information. Specifically, the annotation of scribbles is propagated to superpixels and thus obtains a dense annotation for supervised training. Since the quality of pseudo-labels is limited by the low-quality annotation, the beneficial superpixels selected by dynamic thresholding are used to refine pseudo-labels. Furthermore, aiming to alleviate the negative impact of noise in pseudo-label, superpixel-level uncertainty is incorporated to guide the pseudo-label supervision for stable learning. Our method achieves state-of-the-art performance on both tumor and organ segmentation datasets under the WSSS setting, using only 3% of the annotation workload compared to fully supervised methods and attaining approximately 80% Dice score. Additionally, our method outperforms eight weakly and semi-supervised methods under both weakly supervised and semi-supervised settings. Results of extensive experiments validate the effectiveness and annotation efficiency of our weakly semi-supervised segmentation, which can assist clinicians in achieving automated segmentation for organs or tumors quickly and ultimately benefit patients.", "sections": [{"title": "I. INTRODUCTION", "content": "Medical Image Segmentation (MIS) is a crucial task in medical image analysis, and holds significant importance for clinical assessment, diagnosis, and treatment [1], [2]. Recently, with the rapid advancement of deep learning, deep learning-based methods have gradually become mainstream in MIS. U-Net [3] and nn-Unet [4] exhibit impressive learning abilities in MIS, demanding extensive datasets with fine-grained annotations for training.\nNevertheless, acquiring high-quality full annotations for large-scale medical images is often challenging and costly [5]\u2013[7]. In recent years, approaches have been explored to reduce annotation costs, primarily following two strategies: 1) using weakly annotated data (e.g., image-level label, bounding boxes, scribbles, etc.), such as weakly supervised segmentation. Scribble-based learning has gained popularity due to the user-friendly manual annotation, allowing annotators to quickly and accurately label a small pixel set [8]\u2013[10]. 2) utilizing unlabeled data, such as semi-supervised segmentation. Semi-supervised segmentation enhances segmentation performance by mining information in unlabeled data to address limited annotation challenges [11]\u2013[13].\nTo cut annotation costs further, there's a focus on merging different learning methods for mixed supervision. For instance,"}, {"title": "II. RELATED WORKS", "content": "Weakly Semi-Supervised Segmentation (WSSS) trains networks with limited weakly annotated data and abundant unlabeled data to achieve notable performance [14], [15]. Gao et al. [16] utilized scribbles as the weak annotation and employs multi-task learning for distinct supervision of different data. However, due to the large quantity of unlabeled data compared to weak annotation data, the inconsistent supervision results in over-guidance from unlabeled data, causing suboptimal representations during the training process [17]. Therefore, we employ pseudo-label learning to impose unified supervision on both types of data, mitigating inconsistent supervision issues.\nHowever, in WSSS, establishing an efficient pseudo-label learning method faces two main challenges. Firstly, insufficient supervisory information leads to ambiguous boundaries and the defective appearance of pseudo-labels. Secondly, noisy pseudo-labels brought by weakly labeled data and vast unlabeled data may accumulate errors and lead to unstable segmentation. To tackle these issues, we propose a superpixel-propagated pseudo-label learning approach to promote weakly semi-supervised medical image segmentation.\nAs illustrated in Figure 1(a), sparse annotations like scribbles lack complete target regions and precise boundaries compared to dense annotations. Conversely, superpixels and ground truth exhibit regional and boundary-wise consistency. Pixels within a superpixel share textual similarity usually corresponding to the same semantic category and superpixels' boundaries often exhibit pronounced grayscale gradients similar to ground truth. Therefore, we aim to utilize the image's prior information (texture and boundary) contained within superpixels to boost the pseudo-label supervision, complementing insufficient supervision. However, the superpixels may contain some noise that is inconsistent with ground truth, so it's important to assess such bad superpixels during training. As shown in Figure 1(b), the superpixel quality can be distinguished by the class proportion, for poor superpixel with lower class proportion may introduce boundary noise. Given this, we would like to filter high-quality superpixels for pseudo-label refinement, propagating beneficial boundary information in superpixels.\nMoreover, the high noise in pseudo-labels produced by WSSS models remains non-negligible even after high-quality superpixel refinement. A way to discriminate the potential noise in prediction is based on the consistency between multiple outputs, as shown in Figure 1(c). A consistent region often corresponds to a simple region with a lower probability of bad prediction, and bias tends to occur in difficult regions with inconsistent predictions. To further mitigate the effect of noisy pseudo-labels, we propose superpixel-level uncertainty based on the consistency of predictions to guide and stabilize pseudo-label supervision.\nIn this paper, to address the above issues, a comprehensive superpixel-propagated pseudo-label learning framework are proposed. Our superpixel-propagated pseudo-label learning method consists of three components: superpixel-based scribble expansion; dynamic threshold filtering superpixels for pseudo-label refinement; and superpixel-level uncertainty guidance. Specifically, to fully utilize limited label information of scribble, we employ a scribble expansion approach that expands scribble labels to similar adjacent pixels bounded by superpixels to obtain superpixel-level dense annotations. Subsequently, to improve pseudo-label quality, we use high-resolution superpixels to refine pseudo-labels by compensating image details like boundaries. The dynamic threshold adaptively selects simple and consistent superpixels for relabeling during the training process. Additionally, we utilize the difference between predictions from two decoders within a superpixel to gauge superpixel-level uncertainty. Then, introducing superpixel-level uncertainty as weights for pseudo-label supervision directs the network's attention towards more dependable pseudo-label regions, thus augmenting model stability. To recap, the main contributions are as follows:\n\u2022 To alleviate the insufficient and inconsistent supervision in WSSS, a superpixel-propagated pseudo-label learning is proposed to use the superpixel for supplementary information and union pesudo label learning for both weakly annotated and unlabeled data.\n\u2022 A dynamic threshold filtering strategy is employed to ensure the propagation of beneficial superpixel information during training.\n\u2022 The superpixel-level uncertainty is applied to guide pseudo-label learning and alleviates the bias caused by imprecise pseudo-label predictions.\n\u2022 Our WSSS method achieves superior performance in two public datasets and can be applied to other annotation-efficient learning."}, {"title": "A. Weakly Supervised Learning in MIS", "content": "Weakly supervised learning simplifies the manual annotation task to image-level labels [18], bounding boxes [19], points [20], and scribbles [21], [22]. In this paper, we focus on scribble-based learning. Some approaches generated initial dense annotations by relabeling adjacent pixels through methods like region growing [8] and graph-based techniques [23]. These steps, however, are time-intensive and introduce label noise. So, conditional random fields (CRF) were integrated either as post-processing [8] or trainable layers [23], enhancing segmentation results but necessitating multi-stage training and updates.\nLee et al. [24] proposed a self-training-based pseudo-label learning and selected reliable pseudo-labels by confidence thresholding for network training. Luo et al. [25] dynamically mixed two predictions from dual branches to generate pseudo-labels for supervision. Zhang et al. [26] incorporated a mixup strategy and cycle consistency for effective scribble-based learning. Liu et al. [27] presented an uncertainty-aware self-ensemble approach to produce reliable predictions for supervision and impose regularization through transformation consistency. Additionally, other studies [28], [29] assessed segmentation quality and encouraged the generated segmentation to be closer to the ground truth segmentation through adversarial learning, but required additional dense annotations."}, {"title": "B. Semi-Supervised Learning in MIS", "content": "The dominant semi-supervised learning for MIS can be categorized into pseudo label-based and consistent regularization-"}, {"title": "D. Superpixels in MIS", "content": "Superpixel segmentation is a way of over-segmenting an image by composing pixels that are adjacent and similar to small regions as superpixels [46]\u2013[48]. Superpixels are traditionally generated by graph-based [49] and clustering-based [50], [51] unsupervised algorithms. The generated superpixels preserve low-level image representations that are useful for image segmentation and help to extract boundary information of image objects. Hence, some current works incorporate superpixels to further improve segmentation performance in deep learning. Ouyang et al. [52] use superpixels for self-supervised learning to help improve the performance of network few-shot segmentation. Wang et al. [53] introduce superpixels for dense contrast learning to improve multi-organ representation. Thompson et al. [54] merge the pseudo labels of all similar superpixels in semi-supervised learning. Li et al. [55] use the internal consistency of superpixels to design regularisation terms to achieve learning from noisy labels. However, none of the above methods adequately consider the mis-segmentation in superpixels and no methods have yet been developed to evaluate the quality of different superpixels and handle bad superpixels to avoid the effects of their noise. In contrast, our method filters high-quality superpixels through dynamic thresholding and assigns different weights to different superpixel regions through superpixel-level uncertainty, which helps to ensure the utilization of beneficial superpixels and avoid the propagation of erroneous superpixel information."}, {"title": "III. METHOD", "content": "In the weakly semi-supervised segmentation task, a small number of weakly labeled data Di and a large number of unlabeled data Du are used for model training, and the obtained model f (0) can segment the test data. In this paper, the scribbles are used as the weak annotations, and the labeled dataset $D_{l} = \\{(X_{i}, S_{i})\\}_{i=1}^{N_{l}}$ contains $N_{l}$ samples. X denotes the medical image and S = {$s_{k}, C_{k}$} denotes its corresponding scribble annotation, where $s_{k}$ denotes the pixel set of scribble k, and $0 \\leq C_{k} < C$ is the class of the scribble. The unlabeled dataset $D_{u} = \\{X_{b}\\}_{b=1}^{N_{u}}$ has $N_{u}$ images without annotations.\nAn encoder-decoder network architecture is employed as the backbone, as shown in Figure 2. The framework comprises an encoder and two slightly different decoders (one with dropout and one without). The framework works in conjunction with three operations: scribble expansion, pseudo-label refinement, and superpixel-level uncertainty guidance, whose details are shown in the subfigure in Figure 2 respectively. Here, the algorithm of our method is exhibited in Algorithm 1. First of all, the superpixels of all images are generated offline by Simple Linear Iterative Clustering (SLIC). During training, the network samples both weakly labeled data $D_{l}$ and unlabeled data $D_{u}$ as input simultaneously. For each input image X, the network outputs two predictions $P^{1}$ and $P^{2}$, and applies the average ensemble strategy to obtain pseudo label $\\tilde{Y}$. For the labeled data, the scribble annotations are expanded by superpixels to generate expanded scribbles $\\bar{Y}$, which is used to calculate the partial supervised loss $L_{sup}$. Additionally, for both weakly labeled and unlabeled data, their pseudo labels $\\tilde{Y}$ are refined by superpixels selected by the dynamic threshold to obtain refined pseudo labels $\\hat{Y}$. Besides, two predictions from the network can be used to calculate superpixel-level uncertainty, which is used to guide the pseudo-label supervision loss $L_{pseu}$ by reweighting the superpixel region. Finally, the model parameters are optimized by the total loss of supervised loss and pseudo-label loss in each iteration. More methodological details can be found in the subsequent subsections."}, {"title": "B. Superpixel-based Scribble Expansion", "content": "We employ the conventional SLIC algorithm for superpixel generation. For a single image X, SLIC partitions image into n superpixels $SP = \\{sp_{j}\\}_{j=1}^{n}$. Each superpixel $sp_{j} = \\{x_{i}\\}_{i=1}^{M_{j}}$ contains $M_{j}$ similar adjacent pixels $x_{i}$. Here, we set n to a relatively large value to ensure each superpixel is smaller than the size of the target and exclude the case of superpixel with multi-class scribbles. Since the scribbles only annotate limited pixels, we leverage pixel similarity within superpixel to expand the scribbles. We assign superpixel with the class of scribble it contains, and obtain the expanded scribble $\\bar{Y} = \\varphi_{1} (Y_{ij} | S, SP)$. The corresponding mapping relationship is formulated as:\n$\\varphi_{1} (Y_{ij}) = \\begin{cases} C_{k} & \\text{if } sp_{j} \\cap s_{k} \\neq 0 \\\\ \\text{none, } & \\text{else} \\end{cases}$ (1)"}, {"title": "C. Pseudo-label refinement with superpixel filtered by dynamic threshold", "content": "To get full supervision for the whole image, we apply pseudo-label learning with superpixel-based refinement. The pseudo label $\\tilde{Y}$ is generated by averaging two predictions of network:\n$\\tilde{Y} = argmax ((P^{1} + P^{2}) * 0.5)$ (4)\nTo address blurred boundaries of pseudo-labels, we leverage superpixels to help capture edges for pseudo-label refinement. We filter superpixels by the proportion of the dominant class to relabel high-quality superpixels, thereby obtaining the refined pseudo-labels $\\hat{Y} = \\varphi_{2} (\\hat{y}_{ij} | \\tilde{Y}, SP)$. We relabel superpixels with their maximum class when the proportion exceeds the threshold, and otherwise keep the original prediction. This process is visually explained in Figure 4. The mapping function $\\varphi_{2}$ is defined as:\n$\\varphi_{2} (Y_{ij}) = \\begin{cases} argmax (\\hat{y}_{j}), & \\text{if } \\psi (Y_{ij}) > T \\\\ \\hat{Y}_{ij}, & \\text{else} \\end{cases}$ (5)\n$\\psi (y_{j}) = \\frac{\\sum_{i \\in sp_{j}} \\mathbb{1} (\\tilde{Y}_{ij} == argmax (\\tilde{y}_{j}))}{M_{j}}$ (6)\nwhere $\\psi (y_{j})$ denotes the proportion of pixels with dominant class in superpixel $sp_{j}$, and T is the threshold. Considering the influence of the threshold on the learning status, we design a dynamic threshold for filtering inspired by the adaptive thresholding in [56]. To better exploit superpixel information, the dynamic threshold uses an exponentially moving average (EMA) strategy performing an increasing trend during training, which ensures reliable superpixels are relabeled and unreliable ones are gradually ignored as training progresses. Furthermore, given the class imbalance, we define class-specific thresholds as follows:\n$T_{t}(c) = \\begin{cases} T_{0}, & \\text{if } t = 0 \\\\ \\lambda T_{t-1}(c) + (1 - \\lambda) max(\\psi(c)), & \\text{else} \\end{cases}$ (7)\nwhere $T_{t}(c)$ denotes the threshold for class c at the t-th time step (iteration), $T_{0}$ indicates the initial threshold and $\\lambda$ is the momentum decay of EMA to dynamically update the thresholds. $\\psi(c)$ is the class proportion of class c."}, {"title": "D. Superpixel-level Uncertainty Guidance", "content": "In order to address the influence of noisy pseudo-label, we utilize superpixel-level uncertainty to guide pseudo-label supervision. Considering that the uncertainty is associated with consistency of two predictions, we compute the ratio of the number of pixels with different predictions within superpixel to assess the superpixel-level uncertainty of the pseudo label:\n$\\varphi_{3} (U_{j}) = \\frac{\\sum_{i \\in sp_{j}} \\mathbb{1} (argmax (p^{1}_{ij}) \\neq argmax (p^{2}_{ij}))}{M_{j}}$ (8)\n$L_{pseu} = L_{wdice} (P^{1},\\hat{Y} | U) + L_{wdice} (P^{2}, \\hat{Y} | U)$ (9)\n$L_{wdice} (P,Y) = 1 - \\frac{2\\sum P * W + \\sum Y * W}{\\sum P * W + \\sum Y * W}$ (10)\n$W_{ij} = e^{-U_{j}}$ (11)\nwhere $L_{wdice}$ indicates weighted dice loss, W is the weight map based on superpixel-level uncertainty, composed of a union of pixel weight $W_{ij}$. Guided by the superpixel-level uncertainty, the network can focus more on learning from higher quality pseudo-labels. Finally, the complete training loss function is defined as:\n$L_{total} = L_{sup} + L_{pseu}$ (12)"}, {"title": "IV. EXPERIMENT", "content": "Dataset: We evaluated our method using two publicly available datasets. The ACDC (Automated Cardiac Diagnosis Challenge) [57] dataset consists of cine-MR images from 100 patients. Each image is manually annotated with scribbles for left ventricle (LV), right ventricle (RV), and myocardium (MYO) structures, following the annotations provided in previous work [58]. In ACDC experiments, we performed 5-fold cross-validation with 80 samples for training and 20 samples for testing in each fold. The BraTS2019 dataset [59] consists of 335 multimodal MR images of brain tumors, and we used the preprocessed FLAIR images along with corresponding whole tumor labels from [41] for our segmentation task.\nTo generate corresponding scribble annotations, we used the method introduced in previous work [60] to simulate scribble annotations. In BraTS2019 experiments, we utilized 250, 25, and 60 samples as the training, validation, and test sets, respectively. In WSSS and semi-supervised segmentation, the labeled samples are randomly selected from the training set.\nTraining Details: Our method was implemented using the PyTorch framework [61] and all experiments were executed on an NVIDIA 3090 GPU. We evaluated our approach on both 2D and 3D image segmentation. For ACDC and BraTS2019 datasets, we adopted U-Net and 3D U-Net as the backbone for encoding and decoding, respectively. During network training, we used stochastic gradient descent (SGD) optimizer with a momentum of 0.9 and weight decay of 0.0001. Learning rates are set to 0.01 for 2D segmentation and 0.1 for 3D segmentation, respectively."}, {"title": "B. Comparison with the state-of-the-art", "content": "1) Main result: To validate the effectiveness of our proposed SP3 method, we conducted a comparative evaluation against a series of state-of-the-art methods. All experiments were performed with the same settings to ensure a fair comparison. Specifically, we constructed two baseline methods that do not utilize unlabeled data as references: Sparse Anno. and Dense Anno. indicate training with labeled data with scribble and full annotations respectively. We compared our method with the leading WSSS methods SOUSA. Due to the lack of WSSS methods for comparison, we adapted advanced weakly supervised methods (S2L, USTM) and semi-supervised (ICT, URPC) methods to the WSSS setting for comparison. For the pseudo-label-based S2L method, we assign pseudo-labels to unlabeled data and impose pseudo-label supervision, the same as scribble-labeled data. As for USTM, we extend its consistency concept to the utilization of unlabeled data, calculating transformation consistency loss for the predictions of unlabeled data from both branches. In the case of semi-supervised methods, we replace the fully dense annotation with weakly annotation to compute partial cross-entropy loss. Simultaneously, we also apply their consistency supervision for unlabeled data to the supervision of scribble-annotated data, adapting the ICT and URPC to the WSSS setting.\nBraTS2019 dataset Segmenting whole brain tumors is a more challenging task due to the inherent ambiguity and diverse shapes of tumors.\n2) Results of semi-supervised experiments: Our SP3 learning can be generalized to pure weakly supervised and semi-supervised segmentation. In this study, we use our superpixel"}, {"title": "3) Results of weakly-supervised experiments", "content": "Our proposed method can also serve as an effective approach for pure weakly supervised learning. Our method is compared with advanced weakly supervised approaches based on regularization and pseudo-labeling and achieves the best experimental results with 0.8751 Dice. Compared to other pseudo-labeling methods like SS and S2L, our pseudo-labeling method based on superpixel propagation exhibits performance superiority. Furthermore, our method achieves comparable performance to 0.9013 Dice of fully supervised learning using only scribble annotations (FS)."}, {"title": "C. Ablation experiments", "content": "To concretely validate the practicality of each module, we conducted ablation experiments on two datasets in two scenarios: using only scribble data Di and incorporating unlabeled data (Di+Du) for training. Compared to using only scribble data, we observe that the employment of expanded scribbles for partial supervision led to an improvement over LB, particularly for tumor segmentation (improved by 0.0838 Dice). The pseudo-label supervision refined by superpixels further enhanced segmentation performance, especially the boundary accuracy on the ACDC dataset (reduced by 25.14 ASD). When incorporating unlabeled data using refined pseudo-labels, we can observe significant performance improvement (0.0684 Dice on the ACDC dataset and 0.0664 Dice on the BraTS dataset) brought by the rich sample information"}, {"title": "V. FURTHER ANALYSIS", "content": "To analyze why and how our method works, both quantitative and qualitative analyses are presented as follows:"}, {"title": "A. Qualitative analysis in iteration", "content": "Convergence: In Figure 5(a), we explored the effects of different modules with only D\u2081 for training. We can observe that supervision with expanded scribbles and refined pseudo-labels both improves the convergence speed. This is primarily due to regional information provided by expanded scribble, and the supervision of the entire image region by the complete pseudo-label facilitating the learning of complete structural appearance. The supervision guided by superpixel-level uncertainty also contributes to more stable early-stage learning and aids in improving performance by focusing on high-quality regions during training. In Figure 5(b), our method converges"}, {"title": "B. Quantitative analysis", "content": "Dynamic thresholding: To further demonstrate the importance of dynamic thresholding, we conduct a comparative study on thresholding strategies, as shown in Table VI. Compared with using pseudo label without superpixel-based refinement (pseu w/o SP), directly assigning maximum class (w/o 7) degrades the performance, as it introduces superpixel noise and it's hard to correct biases during learning. Using a fixed threshold 0.8 (T=0.8) to filter potentially high-quality superpixels for refinement can attain performance improvement over directly using pseudo label without refinement, and adjusting threshold with EMA (Tt) can get higher performance. These results prove that screening superpixels can reduce the influence of bad superpixels and improve the segmentation performance. Our class-specific dynamic threshold (Tt(c)) achieves the best performance, facilitating better propagation of beneficial superpixel information with the consideration of category difference.\nRefinement effect: The refinement with superpixels filtered by the dynamic threshold can help the model generate better pseudo labels during iterations, as depicted in Figure 6. As the iteration proceeds, the generated pseudo-labels get closer to the ground truth. Utilizing superpixels helps to eliminate false positive predictions and fills holes in pseudo labels. By employing dynamic thresholds for superpixel selection, we filter out bad superpixels and only use higher-quality superpixels for refinement, facilitating the propagation of favorable boundary information throughout the network. When the accuracy of pseudo-labels increases, the dynamic threshold also increases continuously causing fewer relabeling operations.\nSuperpixel-level uncertainty To deeply elucidate the motivation behind our uncertainty strategy, we also evaluate different strategies to generate uncertainty maps and show the results in Table VII. Firstly, we investigate the level of uncertainty map. Compared with the pixel-level uncertainty map [33] generated by our same prediction consistency (Pixel-level), the superpixel-level uncertainty map helps gain better results. This improvement is attributed to the fact that the superpixel-level evaluation aggregating information from pixels within superpixels ensures consistency between the evaluation level and refinement level, leading to a more reliable assessment of refined pseudo-labels. Secondly, we explore the measurement of uncertainty. We calculate the uncertainty by averaging the commonly used KL divergence [40] between two soft outputs within the superpixel to obtain a KL-based superpixel-level uncertainty map (KL_SP). Our method achieves more comprehensive results by utilizing the proportion of inconsistent predictions of each superpixel as superpixel-level uncertainty. Additionally, we reverse the magnitude relation between the uncertainty and the weight (Ours_re), assigning higher weights to the regions with higher uncertainty (with the weight denoted as $W_{ij} = e^{-u_{j}+1}$), which significantly declines the performance. This decline is primarily due to the impact of substantial noise related to high-uncertainty regions. Our superpixel-level uncertainty guidance helps the network focus on more reliable regions and achieve the best result, leading to robust network learning.\nHyperparameters of superpixel generation In this study, we employ the conventional superpixel generation algorithm, SLIC [64], for image data mining, a technique often used in domain adaptation-related research [65]\u2013[67]. In experiments, we utilize the scikit-image package to perform 2D and 3D SLIC segmentation on the ACDC and BraTS2019 datasets. In our superpixel propagation method, the size of superpixels"}, {"title": "VI. CONCLUSION", "content": "This work mainly focuses on automated medical image segmentation under annotation scarcity. Weakly semi-supervised segmentation (WSSS) is an effective way to alleviate this dilemma, which trains a model with only a small number of scribbles and a large amount of unlabeled data. The proposed superpixel-propagated pseudo-label (SP3) learning is a novel WSSS method that achieves promising results with high label efficiency. Specifically, the structural information contained in superpixels complements insufficient supervision from limited scribbles by expanding the scribbles to provide denser labels. The pseudo-label learning handles the inconsistent supervision caused by mixed supervision, and the superpixel refinement and the superpixel-level uncertainty are utilized to ensure stable pseudo-label supervision. Experimental results show that our method outperforms the current state-of-the-art methods significantly on two public datasets in the WSSS setting, demonstrating its remarkable annotation efficiency. Besides, our method shows promising results in pure weak and semi-supervised settings. These results indicate its substantial potential to serve as a general solution for other annotation-efficient medical segmentation tasks and may inspire research on other learning paradigms, such as partially supervised learning and weakly supervised learning with points [68]."}]}