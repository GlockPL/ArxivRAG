{"title": "PoseBench: Benchmarking the Robustness of Pose Estimation Models under Corruptions", "authors": ["Sihan Ma", "Jing Zhang", "Qiong Cao", "Dacheng Tao"], "abstract": "Pose estimation aims to accurately identify anatomical keypoints in humans and animals using monocular images, which is crucial for various applications such as human-machine interaction, embodied AI, and autonomous driving. While current models show promising results, they are typically trained and tested on clean data, potentially overlooking the corruption during real-world deployment and thus posing safety risks in practical scenarios. To address this issue, we introduce PoseBench, a comprehensive benchmark designed to evaluate the robustness of pose estimation models against real-world corruption. We evaluated 60 representative models, including top-down, bottom-up, heatmap-based, regression-based, and classification-based methods, across three datasets for human and animal pose estimation. Our evaluation involves 10 types of corruption in four categories: 1) blur and noise, 2) compression and color loss, 3) severe lighting, and 4) masks. Our findings reveal that state-of-the-art models are vulnerable to common real-world corruptions and exhibit distinct behaviors when tackling human and animal pose estimation tasks. To improve model robustness, we delve into various design considerations, including input resolution, pre-training datasets, backbone capacity, post-processing, and data augmentations. We hope that our benchmark will serve as a foundation for advancing research in robust pose estimation. The benchmark and source code will be released at PoseBench.", "sections": [{"title": "1 Introduction", "content": "Pose estimation aims to locate anatomical keypoints of the human or animal body from a single image [58, 32]. This fundamental ability to accurately interpret human movements drives advancements across various fields, enabling innovative applications in healthcare, entertainment, safety, autonomous driving, and wildlife conservation [53, 28, 18, 17, 11]. Existing pose estimation models achieve remarkable performance on various benchmarks [23, 1, 56, 19, 45] through two typical paradigms: top-down [27, 38, 33, 52, 46, 39, 40] and bottom-up [26, 4, 10, 40]. These paradigms differ in whether they detect individuals at the beginning or end of the process. Based on the keypoint prediction heads, top-down methods can further be categorized into approaches using 2D Gaussian heatmaps centered at keypoints [33, 21, 2, 42, 39], regression of keypoint locations [20, 25], or classification of keypoints [22]. All the paradigms have their unique advantages, e.g., in terms of accuracy or running speed, contributing to the versatility and effectiveness of pose estimation in diverse applications.\nDespite significant advancements in pose estimation, existing models often overlook the challenges of real-world deployment. Different from the \u201cclean\u201d images used during training, images from"}, {"title": "2 Related Work", "content": "cameras deployed in real-world scenarios can be corrupted at various stages - during sensing, signal transmission, or data storage - introducing natural corruptions such as motion blur, noise, low lighting, and compression artifacts [36, 48]. These issues extend beyond the scope of current pose datasets, making pose models trained on them vulnerable to such corruptions and raising safety concerns in practical applications.\nTo address this issue, it is necessary to establish a comprehensive benchmark to assess the robustness of current pose estimation methods and to devise an effective strategy against real-world corruptions. Recent studies have explored robustness benchmarks for 3D perception tasks in autonomous driving, such as depth estimation [16], 3D detection and segmentation [15]. However, the robustness of pose estimation, a crucial location-sensitive task, remains under-explored. In 2021, [36] tested several pose estimation methods on common corruptions. This study, however, included only four convolutional neural networks (CNN)-based methods and did not adequately address broader pose estimation tasks including those for animals, which is insufficient to fully validate the robustness of current techniques, especially considering the many recent advancements in vision transformers (ViT) [8, 24, 41, 54, 55, 39, 40, 22, 46, 42].\nTo fill this gap, we propose a more comprehensive robustness benchmark to evaluate existing representative CNN-based and ViT-based pose estimation models under a range of natural corruptions. Our benchmark assesses 60 models across various pose estimation methods, including top-down, bottom-up, heatmap-based, regression-based, and classification-based approaches, on both human and animal pose estimation tasks. We introduce four sets of corruptions encompassing 10 typical natural corruptions observed in real-world scenarios. To ensure a thorough evaluation, we consider five levels of severity for each type of corruption and report the average performance across all severity levels. Furthermore, we delve deeper into the impact of various configurations on robustness, including input resolution, pre-training datasets, backbone capacity, post-processing, and data augmentations. This comprehensive evaluation enables a clear understanding of how different factors influence the robustness of pose estimation models under real-world conditions.\nExtensive experiments were conducted to evaluate robustness, yielding several valuable findings: 1) Existing pose estimation models demonstrate a notable vulnerability to corruptions, yet their robustness is strongly correlated with performance on images without corruptions. 2) Among all the corruptions, motion blur and contrast have the most significant impact on model robustness, while brightness has the least impact. 3) Regression-based methods demonstrate the highest resistance to mask corruption, even though their performance on clean images is not the best. 4) The robustness against corruptions varies across existing datasets due to the different characteristics of the data. Human pose models are particularly vulnerable to compression and blur, whereas animal pose models experience the largest performance drop with contrast changes. 5) Among the various design factors, pre-training, post-processing, and large transformer-based backbone designs significantly enhance robustness, whereas input resolution does not have a noticeable effect.\nThe key contributions of this work are summarized as follows:\n\u2022 We introduce PoseBench, a comprehensive benchmark for evaluating the robustness of pose estimation models against various natural corruptions.\n\u2022 We extensively evaluate 60 model variants from 15 state-of-the-art CNN-based and ViT-based pose estimation methods, covering top-down, bottom-up, heatmap-based, regression-based, and classification-based approaches. This includes testing their robustness against four sets of corruptions, featuring 10 types of common real-world corruptions.\n\u2022 We investigate the impact of key design and training factors, such as backbone capacity, pre-training, input resolution, post-processing, and data augmentation. Our findings suggest effective strategies for enhancing both performance and robustness against corruptions."}, {"title": "2.1 Pose estimation", "content": "Pose estimation aims to localize body joint positions from a single image of a human or animal and can be classified into two main approaches: top-down and bottom-up methods. Top-down methods [38, 27, 38, 33, 52, 46, 42, 2, 21, 39, 40, 22] first detect individuals and then localize the"}, {"title": "2.2 Robustness against corruptions", "content": "Recent studies have explored robustness in various fields such as detection [15], segmentation [15], depth estimation [16], pose estimation [36], and action detection [48], emphasizing models' generalization under corruptions due to safety concerns. Pose estimation robustness is particularly challenging because it involves both the classification of different human and animal species and pixel-level localization of keypoints. A previous seminal study [36] evaluated the performance of four CNN-based models under corruptions like blur, noise, and lighting changes, providing valuable insights into robust pose estimation. Nevertheless, this study, conducted three years ago, did not include recent state-of-the-art models such as vision transformer-based methods (e.g., ViTPose [39], TransPose [42], HRFormer [46]) and classification-based methods (e.g., SimCC [22]). Moreover, it did not address animal pose estimation, which significantly differs from human pose estimation. To fill these gaps, we develop a more comprehensive benchmark to assess the robustness of 60 pose estimation models, covering 15 methods, against natural corruptions for both human and animal poses.\nWe also delve into various factors for improving robustness, such as backbone capacity, pre-training, input resolution, post-processing, and data augmentation. Our findings offer interesting observations and valuable insights, which could inform the design of robust pose estimation models in the future."}, {"title": "2.3 Data augmentation", "content": "Data augmentation [6, 7, 30, 47, 29, 3, 34, 59, 9] is a widely used and effective technique in computer vision tasks to enhance model generalization and robustness. By applying various transformations to the training data, it increases the dataset's size and diversity without the need to collect new data, making it easy to implement and highly effective. This process helps models learn more general features by exposing them to diverse data variations, thereby improving their ability to generalize to new, unseen data [48]. Common techniques include color changes, geometric transformations, blur and noise addition, and dropout. For instance, adding Gaussian or speckle noise [30] has been proven effective in improving generalization against various image corruptions. Research [34] has also highlighted the impact of blur on recognition tasks. Composite techniques that combine different transformations often yield better performance. AutoAugment [6], for example, searches automatically for the optimal combination of augmentation policies. Additionally, methods like Cutmix [47] and Mixup [51] enhance single images by merging them through overlapping, cutting, and pasting. In this study, we group different augmentations into four sets to assess their effectiveness"}, {"title": "3 Benchmark", "content": "In this paper, we investigate the robustness of 60 pose estimation models, categorized into top-down or bottom-up methods using heatmap-based, regression-based, or classification-based representations. We examine four groups of natural corruptions, covering 10 common types that could affect the robustness of pose estimation models in real-world scenarios. These corruptions include blur and noise, compression and color loss, lighting, and masks. To ensure comprehensive and fair evaluation, we evaluate human pose estimation models using the COCO [23] and OCHuman [56] datasets, and animal pose estimation models using the AP10K [45] dataset, which are representative datasets in the field. Each corruption type is applied at five levels of severity to the validation sets, maintaining consistency across all methods. The results averaged over these five severity levels form a thorough comparison. This section details the types of corruptions (Section 3.1) and the evaluation metrics used to measure robustness (Section 3.3)."}, {"title": "3.1 Corruptions", "content": "When deploying pose estimation models in real-world scenar- ios, it is crucial that they reliably handle issues arising during signal capture. Ad- dressing these challenges significantly en- hances system reliability and robustness, while reducing repair and maintenance costs. These issues include motion blur from object or sensor movement, noise from sensor hardware malfunctions, and noise from unstable signal transmission. To simulate image quality degradation un- der these conditions, we use three com- mon types of corruption: Motion Blur, Gaussian Noise, and Impulse Noise. Compression and Color Loss. Image compression and color loss pose significant challenges for pose estimation models deployed in practical applications due to storage, transmission, and device limitations. These issues result in reduced color detail and fidelity, along with color banding and artifacts that impair pose estimation performance. To assess the impact, we simulate three types of compression and color-related corruptions: Pixelate, JPEG Compression, and Color Quant."}, {"title": "Lightning", "content": "Lightning changes, such as extremely dark environments at night, excessive brightness from sunlight, and sensor malfunctions causing overexposure, are common issues in real-world scenarios. These conditions impair visibility and obscure essential features needed for accurate pose estimation. Current datasets lack representation of diverse lighting conditions, resulting in pose estimation models that are not robust under these circumstances. To address this, we simulate various lighting conditions using three types of corruptions: Brightness, Darkness, and Contrast."}, {"title": "Mask", "content": "While occlusion has been extensively studied in pose estimation, current datasets mainly address occlusions caused by environments or other human bodies, which provide ample context for inferring hidden parts. However, in real-world scenarios, occlusions can also result from data loss during sensing, signal transmission, data processing, or privacy protection, where parts of the image are missing without any contextual clues. To simulate this, we apply random-size masks to keypoints to evaluate model robustness under mask corruption. This method enables us to assess model performance when dealing with occlusions that lack contextual information."}, {"title": "3.2 Benchmark Datasets", "content": "Inspired by [15, 16, 36], our robustness benchmark for pose estimation comprises two tasks, utilizing three datasets termed COCO-C and OCHuman-C for human body pose estimation and AP10K-C for animal pose estimation. These datasets are created by applying four corruption sets introduced in Section 3.1, covering 10 types of corruptions, to their original validation sets. Each corruption type includes five levels of severity."}, {"title": "COCO-C Dataset", "content": "The COCO-C dataset is constructed from the validation set of the COCO Keypoints 2017 dataset [23], which includes over 118K labeled images for training and 5000 images for validation. The COCO [23] dataset supports a 17-keypoint annotation for human bodies, facilitating comprehensive human body pose estimation under various scenarios."}, {"title": "OCHuman-C Dataset", "content": "The OCHuman-C dataset is based on the OCHuman [56] dataset, which contains 13,360 meticulously annotated human instances within 5,081 images. The OCHuman dataset is considered the most complex and challenging for human pose estimation due to the heavy occlusion of humans in this dataset."}, {"title": "AP10K-C Dataset", "content": "The AP10K-C dataset is derived from the AP10K [45] dataset, which consists of 10,015 images representing 23 animal families and 54 species, along with their high-quality keypoint annotations. Following the COCO [23] style, the AP10K dataset supports a 17-keypoint annotation for animal bodies."}, {"title": "3.3 Evaluation Metrics", "content": "In this section, we present two standard evaluation metrics for pose estimation: mean average precision (mAP) and mean average recall (mAR). These metrics evaluate model accuracy on both clean and corrupted test images. Additionally, following [48], we introduce the concept of relative robustness to assess a model's robustness."}, {"title": "mAP and mAR", "content": "are two widely adopted metrics for assessing pose estimation performance. Following [38, 27], we report mAP as the average of AP values at thresholds ranging from 0.5 to 0.95, with a step size of 0.05. We also provide \"AP .5\" and \"AP.75\", representing the AP at thresholds of 0.5 and 0.75, respectively. \u201cAP (M)\" and \"AP (L)\" measure the AP for medium-sized and large-sized objects, respectively. Similarly, we report mAR, AR .5, AR .75, AR (M), and AR (L)."}, {"title": "Mean Relative Robustness (mRR)", "content": "Following [48], we introduce the robustness metric, mean Relative Robustness (mRR), to evaluate how much a model's performance drops under certain corruptions compared to clean images. To calculate this metric, we first evaluate the model on clean images and obtain the mean Average Precision (mAP), denoted as $mAP_{clean}$. For any corruption c, we then calculate the mAP at each severity level s, denoted as $mAP_{c,s}$. The relative robustness for"}, {"title": "corruption c and the mean Relative Robustness (mRR) are defined as follows:", "content": "$RR_{c} = \\frac{1}{5} \\sum_{s=1}^{5}(1 - \\frac{mAP_{c,s}}{mAP_{clean}}),  MRR = \\frac{1}{C} \\sum_{c=1}^{C} RR_{C},$"}, {"title": "where c indexes the C types of corruption.", "content": "where c indexes the C types of corruption."}, {"title": "4 Experiment", "content": "We conduct a comprehensive benchmark of 15 pose estima- tion methods and their 60 variants, representing the most significant models in the pose estimation domain. Our evaluation encompasses a variety of approaches, including top-down [27, 33, 46, 39], bottom-up [26, 4, 10], heatmap-based [42, 2, 21], regression-based [20, 25], and classification- based [22] methods."}, {"title": "4.1 Benchmark Settings", "content": "We conduct a comprehensive evaluation using three representative datasets: the human body datasets COCO-C and OCHuman-C, and the animal pose estimation dataset AP10K-C (Sec- tion 3.2). Without further description, all models are trained on the original clean training sets and evaluated on the corrupted validation sets. Notably, for the OCHuman-C dataset, models trained on COCO [23] training set are directly evaluated without additional fine-tuning."}, {"title": "4.2 Robustness Analysis", "content": "Table 1 presents the results of 15 methods and their 60 variants evaluated on the COCO-C dataset under four different corruption scenarios. The findings reveal that all models, regardless of being top-down or bottom-up, heatmap-based, regression-based, or classification-based, experience varying degrees of performance degradation across different corruption types. Figure 3 shows the corruption robustness of 8 representative models on the COCO dataset in terms of mRR (%).\nThe overall relative robustness of models against all corruptions is strongly correlated with their performance on original clean images, as shown in Figure 4a. In Table 1, models that perform better on clean images, such as ViTPose-L and ViTPose-H [39], tend to exhibit the best robustness under corrupted conditions. A deeper analysis of per-severity error rates, illustrated in Figure 5, supports this conclusion. For instance, ViTPose-H [39], with a mAP of 78.84, demonstrates higher corruption robustness than DEKR HRNet-W32 [10], which has a mAP of 68.64, across all severity levels. As the severity level increases, all models show a greater degradation in corruption robustness, as indicated by the mRR(%) values."}, {"title": "Comparison of Corruption Types", "content": "Table 1 shows the performance decline of various methods across different corruption types. Robustness against each corruption type is closely correlated to the clean AP, except in the mask category. Notably, for regression methods like PRTR [20] and Poseur [25], despite their lower clean mAP compared to ViTPose [39], they outperform ViTPose and show higher robustness in the presence of mask corruption. Table 2 provides a detailed comparison of two heatmap-based models (i.e., ViTPose with the ViT-L and ViT-H backbones) and two regression- based models (i.e., Poseur with the HRFormer-B and ViT-B backbones) under mask corruption. Generally, regression-based models excel over heatmap-based ones under mask corruption across most metrics, even though the reverse is true for clean images. This is possible because regression- based methods, by directly predicting coordinates, can better integrate global context information, aiding in the resolution of ambiguities and occlusions.\nIn Figure 6a, we show the averaged mRR values of all models on the COCO-C dataset for each corruption type. Motion blur, Gaussian noise, and Impulse Noise cause the most significant perfor- mance degradation, while Brightness and Mask corruptions have a milder impact. This difference is likely because images with abundant global context support pose estimation models more effectively. Local corruptions like masks, which maintain most of the contextual information, lead to smaller performance drops compared to global corruptions like motion blur and Gaussian noise. Although"}, {"title": "Comparison on Different Datasets", "content": "Both COCO-C and OCHuman-C are human pose datasets, but the robustness of pose estimation models to corruption on them differs significantly. As shown in Table 3 and Figure 6b, models exhibit higher overall robustness on OCHuman-C. This is likely because models were tested directly on OCHuman-C without prior training, resulting in a naturally lower clean mAP. While models on both datasets show similar insensitivity to changes in brightness, OCHuman-C models demonstrate notable improvements in robustness against mask and motion blur corruptions. This is probably due to OCHuman-C's high level of occlusion from multi-person interactions, making additional mask and blur corruptions less impactful.\nIn animal pose estimation, as validated in Table 4, models show less sensitivity to image compression corruptions like pixelate and JPEG compression on the AP10K-C dataset. However, they suffer significant performance drops under contrast changes. This likely occurs because the monochromatic skin and fur of animals make these pixel-level losses less impactful. Furthermore, the similarity between animal coloration and natural environments results in poor performance under low-light conditions, such as dark and contrast corruptions."}, {"title": "4.3 Robustness Enhancement", "content": "In this section, we examine various enhancement factors to improve the corruption robustness of pose estimation models, aiming to offer valuable insights for training robust pose estimation models."}, {"title": "Impact of Backbones", "content": "To assess the robustness of various architectural designs, we conducted a thorough evaluation across multiple backbones, as detailed in Table 1. Our findings reveal that models utilizing vision transformer (ViT) backbones, including HRFormer [46] and ViTPose [39], consistently outperform CNN-based counterparts. As shown in Tables 1, 3, 4, the best and second best models on clean images and overall corruption robustness are all ViT-based. Specifically, ViTPose [39] with the ViT-H backbone achieves an impressive 78.84 mAP on clean images and 65.02 mAP on corrupted images, significantly outperforming other models. Notably, when comparing methods available with both CNN and vision transformer backbones, such as Poseur [25], the transformer backbone (ViT-B) exhibits superior performance over the CNN ones in both mAP and mRR under corruptions, underscoring its significant advantage in robustness. In summary, ViT backbones are more promising for achieving top performance in pose estimation. The advantage stems from the scalability of vision transformer models, which can be easily expanded in terms of layers, hidden units, and attention heads [49, 35, 8]. This scalability enhances the representation capability of large models, enabling them to effectively learn from extensive and intricate datasets, thereby achieving superior performance. In contrast, CNN-based methods often face performance limitations with increasing layer depth. For example, in Table 1, MSPN [21] and RSN [2] use"}, {"title": "Impact of Input Resolutions", "content": "It has been shown that training on high-resolution images leads to the acquisition of more generalized and robust features, thereby improving performance under corruptions [16]. However, our investigation, detailed in Table 5a, examining two input sizes (256x192 and 384\u00d7288) across four models (i.e., SimpleBaseline [38] with the Res50 and Res152 backbones and HRNet [33] with the HRNet-W32 and HRNet-W48 backbones), reveals subtle distinctions. While larger input resolutions indeed enhance mean average precision (mAP) on both clean and corrupted images, the relative improvement under corruptions is not as pronounced as on clean images, leading to a lower mean relative rank (mRR) value. Consequently, increasing input resolution enhances model performance on clean images but also increases susceptibility to corruptions."}, {"title": "Impact of Post-processing", "content": "We investigate the impact of two commonly used post-processing techniques for top-down methods, UDP [13] and DARK [50]. The results are presented in Table 5b. We evaluate two models, HRNet W32 and HRNet W48, trained on COCO [23] and tested on the COCO-C dataset, both with and without post-processing. The findings indicate that while post- processing techniques effectively improve the mAP on both clean and corrupted images, there is a slight degradation in corruption robustness as measured by mRR values."}, {"title": "Impact of Training on Multiple Datasets", "content": "Training on diverse datasets has long been acknowl- edged as an effective strategy for enhancing model generalization and robustness. Exposure to a wider range of data during training enables models to learn more generalized features, thereby enhancing performance on unseen data. To investigate the impact of pre-training datasets on corruption robust- ness, we trained ViTPose (ViT-B) [39] on various dataset combinations, including COCO [23], AI Challenger (AIC) [37], MPII [1], CrowdPose [19], AP10K [45], \u0391\u03a1\u03a436K [43], and WholeBody [14]."}, {"title": "Impact of Data Augmentation", "content": "We investigated the impact of data augmentation during training to enhance robustness against diverse natural corruptions. Utilizing the state-of-the-art ViTPose model (ViT-B) as our base, we employed four augmentation sets: blur and noise (A), compression and color"}, {"title": "5 Conclusion", "content": "In this study, we performed a thorough robustness benchmark for pose estimation models across diverse real-world corruptions. We assessed 60 models derived from 15 leading pose estimation meth- ods, covering human and animal bodies across three representative datasets: COCO-C, OCHuman-C, and AP10K-C datasets. Our findings offer crucial insights into model reliability under corruptions and highlight key factors that can improve robustness and generalization. We hope this study could facilitate the development of more resilient and effective pose models and training methodologies."}, {"title": "Augmentation Definition", "content": "Table 9 provides details of the augmentation types included in each set used in Section 4.3."}, {"title": "C Experiment Results", "content": "We present the detailed results for all models across each corruption type on the three datasets. Results for the COCO-C dataset are shown in Tables 10, 11, and 12. Tables 13, 14, and 15 show results for the OCHuman-C dataset. Results for the AP10K-C dataset are listed in Tables 16, 17, and 18. These results further support the conclusions drawn in Section 4."}, {"title": "D Limitations and Future Work", "content": "While PoseBench offers a comprehensive benchmark for evaluating the robustness of pose estimation methods, it has several limitations:\n\u2022 Score of Research Fields. Although we assess 60 pose models, all methods focus solely on 2D pose estimation from monocular images. Evaluations on 3D skeleton-based pose estimation models, multi-view 2D images, and videos remain unexplored.\n\u2022 Limited Exploration on Mask Corruptions. We propose evaluating robustness against occlusions and data loss by randomly occluding a fixed-size rectangle with zero values. Future assessments should consider the occlusion's position, such as covering specific joints.\n\u2022 Real-World Scenarios. The applied corruptions are simulated, potentially creating a domain gap between simulated images and real-world corruptions from sensor failures. Collecting and benchmarking real-world corrupted data is essential for a more comprehensive study of robustness."}]}