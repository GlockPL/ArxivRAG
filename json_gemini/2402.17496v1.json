{"title": "Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages", "authors": ["Luc\u00eda G\u00f3mez-Zaragoz\u00e1", "Roc\u00edo del Amor", "Elena Parra Vargas", "Valery Naranjo", "Mariano Alca\u00f1iz Raya", "Javier Mar\u00edn-Morales"], "abstract": "Emotional Voice Messages (EMOVOME) is a spontaneous speech dataset containing 999 audio messages from real conversations on a messaging app from 100 Spanish speakers, gender balanced. Voice messages were produced in-the-wild conditions before participants were recruited, avoiding any conscious bias due to laboratory environment. Audios were labeled in valence and arousal dimensions by three non-experts and two experts, which were then combined to obtain a final label per dimension. The experts also provided an extra label corresponding to seven emotion categories. To set a baseline for future investigations using EMOVOME, we implemented emotion recognition models using both speech and audio transcriptions. For speech, we used the standard eGeMAPS feature set and support vector machines, obtaining 49.27% and 44.71% unweighted accuracy for valence and arousal respectively. For text, we fine-tuned a multilingual BERT model and achieved 61.15% and 47.43% unweighted accuracy for valence and arousal respectively. This database will significantly contribute to research on emotion recognition in the wild, while also providing a unique natural and freely accessible resource for Spanish.", "sections": [{"title": "Background & Summary", "content": "Communication is a fundamental aspect of human existence that allows individuals to articulate complex thoughts, express emotions, and foster connections with others. Among the different human communication methods, speech remains as the most natural and effective way through which individuals interact with their environment. It conveys not only linguistic information, but also the individual's emotions, which are used to module social interactions. Speech Emotion Recognition (SER) is a field of research that aims to automatically recognise an individual's emotional state by analysing their voice. SER has potential applications for the study of human-to-human communications, e.g., fear detection in emergency call centres\u00b9 or depression detection2. Recognising human emotions is also relevant to human-computer interactions, e.g., creating affect-adaptive games3.\nSER is still an open-ended problem due to its complexity. A key challenge lies in precisely defining the meaning of emotions. Many different versions have been proposed to define emotions throughout the 20th century4, and there is still no consensus on the matter. The variety of emotional speech databases in the literature, with different methodologies (acted, induced and natural) and emotional models for labelling (dimensional and categorical), is a clear reflection of the lack of agreement. Moreover, they have several limitations for their application in the recognition of real-life emotions. The most popular databases contain acted emotions, which are easier to collect, but they contain stereotypical emotions that differ from real emotions. Natural databases are more suitable for real-life emotion recognition, but are difficult to acquire due to ethical and legal restrictions. In addition, they contain audio recorded in uncontrolled environments (so-called \"in the wild\" conditions), which may include background noise and overlapping voices, making emotion recognition more difficult.\nDatabase selection is crucial when developing a SER system. Existing emotional corpus are mainly in English due to its global prevalence, thus having abundant resources. However, the landscape becomes considerably limited when it comes to languages other than English, with Spanish being one such example. Only twelve Spanish databases have been found in the literature, six of them private6\u201311, one commercially available12, and five free to research use13\u201317 (16 does not provide the original clips). Most of them are acted databases6\u201312,15 where professional or non-professional actors portray different emotions while reading texts. One of the databases is elicited13 and three are natural14,16,17. Within natural databases, two include clips sourced from YouTube videos14,16. In these cases, speaker awareness during recording may result in controlled or unnatural emotions18, and samples might lack the spontaneity seen in natural conversations19. The other natural database used dubbed films17, where acting is inherent and particularly emphasized in the context of dubbed films. In addition, existing databases have a limited number of speakers (\u226415), so samples from the same speaker are used for training and testing, reducing the model's generalizability20. Exceptions are two cases with \u224850 speakers13,17, one with 10514 and another with 341 speakers16)."}, {"title": "Methods", "content": "This section is divided into the following three parts: 1) Participants, including details of the speakers included in the database; 2) Data collection, where the experimental setup used to obtain the audios is explained; and 3) Data labelling, describing the procedure to assign emotional labels to the collected audios. Figure 1 presents a diagram illustrating the steps followed to create the EMOVOME database."}, {"title": "Participants", "content": "The database includes 100 Spanish speakers (50% females, 50% males) between the ages of 18 and 55 years old, with no self-reported speech disorder, and used to sending audio messages to their contacts. Details of the demographic information can be found in Table 1, where age ranges are utilised to safeguard individuals' anonymity. Participants were sourced via a recruitment agency, which identified suitable individuals from their database based on the specified criteria and invited them to participate in the study for a monetary compensation of 25\u20ac. All methods and experimental protocols were performed in accordance with the guidelines and regulations of the local ethics committee of the Universitat Polit\u00e8cnica de Val\u00e8ncia (reference number P02_04_06_20)."}, {"title": "Data collection", "content": "The sample collection was carried out through a web-based application designed for the study. Participants completed the study with their computer, following illustrated step-by-step instructions given on the platform. First, they were required to read the study protocol, ask any questions if needed and provide informed consent. Then, they answered a sociodemographic questionnaire with the information detailed in Table 1. They were required to record an audio reading the short text in Table 2, indicated in the platform. They also completed the NEO Five Factor Inventory (NEO-FFI)21, a 60-item questionnaire designed to evaluate the individual's Big Five personality traits: neuroticism, extraversion, openness to experience, agreeableness, and conscientiousness. Then, they were asked to upload 12 voice messages according to the following criteria: the audios should have been sent to other contacts prior to the study and one-third of them should have positive, neutral and negative valence, respectively. The latter was requested to obtain a balanced sample in terms of valence. The instructions given in the platform provided an explanation of the concepts of positive, neutral and negative valence, with examples of each class."}, {"title": "Quality check", "content": "Voice messages were produced in-the-wild conditions before participants were recruited, avoiding any conscious bias due to the laboratory environment. They were recorded in the participants' natural environments, which could include a quiet room at home but also the street on the way to work. Therefore, to obtain naturalistic data and good quality, a manual screening of the voice messages was performed on the audios containing emotions. As the audios were uploaded, those recorded in critical background noise conditions (such as microphone failure, background music or TV) were manually identified and rejected. Participants were asked to upload audios with low noise levels, so if an audio was not considered suitable for the study, they were required to send a new audio. As a result, we received a total of 1605 audio recordings, but 574 audios were not included in the EMOVOME database for this reason, resulting in 1031 audios. Finally, 32 audios exceeding 60 seconds were rejected in order to obtain a homogeneous sample concerning duration. Therefore, a total of 999 audios were considered in the final speech database, apart from the 100 audios of the text reading."}, {"title": "Data labelling", "content": "The emotional content of the audios that successfully passed the manual evaluation was labelled along two dimensions: valence, i.e., the degree to which an emotion is perceived as positive or negative; and arousal, i.e., how strongly the emotion is felt. For"}, {"title": "Technical Validation", "content": "The voice messages included in the EMOVOME database were produced in-the-wild conditions before participants were recruited, avoiding any conscious bias due to the laboratory environment. As a result, different devices were used for recording, most likely different smartphone microphones, leading to different sampling rates (94% at 48 kHz and 6% at 44.1 kHz). Audios are provided with the original sampling frequencies to let the users choose between applying upsampling at 48kHz to 6% of the audios, applying downsampling at 44.1Hz to 94% of the audios, or eliminating this 6% at 44.1 kHz to have a homogeneous sample in terms of sampling frequency. The audio clips were shared via the WhatsApp application, so they were encoded using the Ogg Vorbis format. Moreover, as indicated in section Quality check, a maximum duration of 60 seconds was established to obtain a homogeneous sample with respect to audio length. Audios vary in length from 1 to 59 seconds and contain from 2 to 220 words, as shown in Figure 6. They have a total length of 293 minutes (~5 hours). Additionally, all audio files were manually checked to remove those recorded in critical background noise conditions."}, {"title": "Usage Notes", "content": "All the data included in the EMOVOME database is publicly available under the Creative Commons Attribution 4.0 International license. The only exception is the original raw audio files, for which an additional step is required as a security measure to safeguard the speakers' privacy. To request access, interested authors should first complete and sign the agreement file EMOVOME_agreement.pdf available in the Zenodo repository https://zenodo.org/records/10694370 and send it to the corresponding author (jamarmo@htech.upv.es). The data included in the EMOVOME database is expected to be used for research purposes only. Therefore, the agreement file states that the authors are not allowed to share the data with profit-making companies or organisations. They are also not expected to distribute the data to other research institutions; instead, they are suggested to kindly refer interested colleagues to the corresponding author of this article. By agreeing to the terms of the agreement, the authors also commit to refraining from publishing the audio content on the media (such as television and radio), in scientific journals (or any other publications), as well as on other platforms on the internet. The agreement must bear the signature of the legally authorised representative of the research institution (e.g., head of laboratory/department). Once"}, {"title": "Code availability", "content": "The code and data partitions for implementing the baseline emotion recognition models are available as part of the EMOVOME database in Zenodo https://zenodo.org/records/10694370. We developed the code using Python programming language. The speech models rely on the Scikit-learn library, while the text models require the TensorFlow library."}]}