{"title": "Seeing Through the Fog\nA Cost-Effectiveness Analysis of Hallucination Detection Systems", "authors": ["Alexander Thomas", "Seth Rosen", "Vishnu Vettrivel"], "abstract": "This paper presents a comparative analysis of hallucination detection systems for Al, focusing on\nautomatic summarization and question answering tasks for Large Language Models (LLMs). We\nevaluate different hallucination detection systems using the diagnostic odds ratio (DOR) and\ncost-effectiveness metrics. Our results indicate that although advanced models can perform better\nthey come at a much higher cost. We also demonstrate how an ideal hallucination detection system\nneeds to maintain performance across different model sizes. Our findings highlight the importance\nof choosing a detection system aligned with specific application needs and resource constraints.\nFuture research will explore hybrid systems and automated identification of underperforming\ncomponents to enhance Al reliability and efficiency in detecting and mitigating hallucinations.", "sections": [{"title": "1. Introduction", "content": "We will explore the realm of Large Language Model (LLM) hallucinations, exploring their nature\nand characteristics. We will shed light on the systems employed to detect these hallucinations,\ndelving into their inner workings and examining how they can be compared to each other.\nFurthermore, we will investigate the multifaceted landscape of metrics available for evaluating\nthe efficacy of such detection systems.\n\nNext, we will embark on a journey through the evaluation methodology, scrutinizing the various\ndatasets utilized to conduct a comprehensive comparison among hallucination detection\nsystems. This exploration will provide invaluable insights into the practical implementation and\nassessment of these systems.\n\nSubsequently, we will unravel the results yielded by the diverse hallucination detection systems,\nuncovering their strengths and weaknesses. This analysis will enable us to discern the most\neffective approaches for identifying and mitigating LLM hallucinations.\n\nTo conclude, we will engage in a discussion of the trade-offs associated with different detection\nsystems. This will include an examination of their respective advantages and disadvantages,\nguiding future research and development efforts. Additionally, we will propose potential avenues\nfor further improvements, aiming to refine and enhance the capabilities of hallucination detection\nsystems. Through this comprehensive exploration, we aspire to deepen our understanding of\nLLM hallucinations and contribute to the advancement of detection systems capable of\nsafeguarding the integrity and reliability of language models."}, {"title": "2. Background and Related Work", "content": "In Large Language Models (LLMs), hallucinations are \"nonsensical or unfaithful\" text generated\nby the model, deviating from reality or context. They fall into two categories: [Ji, Z. et al., 2023]\n1. Intrinsic Hallucinations: These directly contradict the provided source material.\nExamples include generating factually incorrect statements or illogical conclusions.\n2. Extrinsic Hallucinations: These hallucinations don't directly contradict the source but\nintroduce unverifiable elements. This could involve speculation or claims that cannot be\nconfirmed against the source."}, {"title": "2.1. The Cost of Hallucinations", "content": "Hallucinations in Al technology can cause significant financial losses and safety issues. For\ninstance, Google lost $100 billion due to a hallucination in a promotional video.[Coulter, M., &\nBensinger, G., 2023]. Companies can be liable for chatbot claims. Air Canada's chatbot offered\na discount, but the airline denied it. The court ruled in favor of the customer. [Moffatt v. Air\nCanada, SC-2023-005609, 2024]\n27. Air Canada argues it cannot be held liable for information provided by one of its\nagents, servants, or representatives \u2013 including a chatbot. It does not explain why it\nbelieves that is the case. In effect, Air Canada suggests the chatbot is a separate legal\nentity that is responsible for its own actions. This is a remarkable submission. While a\nchatbot has an interactive component, it is still just a part of Air Canada's website. It\nshould be obvious to Air Canada that it is responsible for all the information on its\nwebsite. It makes no difference whether the information comes from a static page or a\nchatbot.\n28. I find Air Canada did not take reasonable care to ensure its chatbot was accurate.\nWhile Air Canada argues Mr. Moffatt could find the correct information on another part of\nits website, it does not explain why the webpage titled \u201cBereavement travel\u201d was\ninherently more trustworthy than its chatbot. It also does not explain why customers\nshould have to double-check information found in one part of its website on another part\nof its website."}, {"title": "2.2. Related Work", "content": "The most reliable form of hallucination detection is still human labeling [Ji, Z. et al., 2023].\nHowever this is not tenable for measuring at scale, let alone monitoring. Classic metrics like\nROUGE, BLEU, and METEOR are ineffective for detecting errors in generated text as they\npenalize different language styles.t [Ji, Z. et al., 2023, Zha, Y. et al., 2023]. There are also some\napproaches that attempt to use non-LLM models (e.g. BERT) to detect hallucinations [Zhang, T.\net al., 2020, Zha, Y. et al., 2023]. LLMs are more generalizable than these models for this task.\nSome systems directly \u201cask\u201d the model if the given example is a hallucination, this is known as"}, {"title": "2.3. Metrics", "content": "Currently, there is no single metric that is considered standard. The novelty of this issue means\nthat there has not been enough time to coalesce. Different systems and authors adopt different\nmetrics depending on what data they have available. Some use accuracy [Ravi, S. S. et al.,\n2024], some use Spearman correlation [Zha, Y. et al., 2023], etc. We considered three\napproaches to metrics in our work. First, we can use the metric already associated with the\ndataset being used. This has the benefit of us not needing to re-interpret the labels into some\nother form, for example, binarizing them, as well as allowing us to compare our hallucination\ndetection system to other kinds of systems evaluated on the dataset. The downsides are that\nthere is no ability to generalize the results into a score to compare different systems, and some\nof the dataset-associated metrics are not amenable to the hallucination detection task. The\nsecond approach is to binarize the labels and use accuracy, [Ravi, S. S. et al., 2024]. This has\nthe benefit of being easy to calculate, and it requires effectively no explanation for someone to\nunderstand the metric. The downside is that accuracy is not a robust metric, it is very sensitive\nto imbalanced data. The third option is to binarize the labels but choose a more sophisticated\nmetric, such as diagnostic odds ratio. This has the benefit of being independent of prevalence.\nThe downside is that the metric does require some explanation. Also, this metric is not bound by\n0 and 1, instead it is bound on the lower end by 0, and is unbound on the higher end.\n\nThe diagnostic odds ratio (DOR) measures the effectiveness of a diagnostic test by combining\nsensitivity and specificity into a single value. A DOR of 1 indicates no discrimination, while\nhigher values indicate better diagnostic accuracy. It's used in medical research to compare and\nevaluate diagnostic tests.\n[Glas, A. S. et al., 2003]. In our case, we are \u201cdiagnosing\" the hallucinatory text."}, {"title": null, "content": "A.\nDiagnostic odds ratio, $DOR = \\frac{TP/FN}{FP/TN} = \\frac{TP/FP}{FN/TN} = \\frac{\u03a4\u03a1.\u03a4\u039d}{FP-FN}$\nB.\nSE (In DOR) = $\\sqrt{\\frac{1}{TP} + \\frac{1}{FN} + \\frac{1}{FP} + \\frac{1}{TN}}$\nC.\nIn DOR \u00b1 1.96 \u00d7 SE (In DOR)"}, {"title": "3. Methodology", "content": "Cost is crucial for hallucination detection systems as the task can be complex. Latency may not\nbe directly significant, but it can define the cost if the application owner hosts the model. For\ninterventions, latency becomes separately important."}, {"title": "3.1 Data", "content": "The two tasks selected for this experiment were selected due to them being common LLM\napplication tasks, and there being many viable datasets. The automatic summarization task\nconsists of 2174 examples from three different data sources. Two of these data sources use the\nsame CNN / Daily Mail data set as references, but the summaries were not generated in the\nsame manner.\n\nThe data sets making up the automatic summarization data set are: QAGS-CNNDM,\nQAGS-XSUM and SummEval. These data sets were selected from the work done on\nAlignScore [Zha, Y. et al., 2023]. The QAGS-CNNDM data set contains news articles from CNN\nand Daily Mail. The summaries are generated from a QA summarization process. The labels are\nfrom crowdsourced human labelers [Wang, A. et al., 2020]. The QAGS-XSUM is made from\nnews articles from the BBC, but otherwise is similar to QAGS-CNNDM [Wang, A. et al., 2020].\nThe SummEval data set is also made from news articles from CNN and Daily Mail, but the"}, {"title": null, "content": "summaries are generated from various models. The labels for SummEval come from both\nexperts and crowdsourced human labels with these labels marked separately in the source data\n[Fabbri, A. R. et al., 2021]. Since there is substantial disagreement between the expert labels\nand the crowdsourced labels, the expert labels are used. These data sets have the advantage\nof being classic NLP data sets. This means that the data is well understood by the NLP\ncommunity. The data sets making up the RAG-QA data set are: DROP, FinanceBench,\nRAGTruth, and TruthfulQA. The first three of these data sets were selected since they are part\nof HaluBench used by LynxQA [Ravi, S. S. et al., 2024].\n\nThe other three data sets from HaluBench were excluded. PubmedQA and covidQA were\nexcluded due to some data quality issues experienced by the team in this and previous projects.\nHaluEval was excluded since its hallucinatory answers were produced by asking an LLM to\nhallucinate [Li, J. et al., 2023]. In the team's opinion, this produces low quality examples. The\nfourth data set in the RAG-QA data set is TruthfulQA. The TruthfulQA data set tests language\nmodels' truthfulness with 817 questions across 38 categories, focusing on common\nmisconceptions. It evaluates how well models avoid generating misleading or incorrect answers,\nemphasizing factual accuracy. This data set is structured such that each row has a question, an\nideal answer, good answers, bad answers, and a source reference. Most of the source\nreferences are Wikipedia and other URLs. Where these URLs could be resolved they were used\nto create the context. This left 688 rows. The data set was then exploded along the answers\nyielding 6735 examples. The data sets from HaluBench were selected since we were testing\nLynxQA. TruthfulQA was added since it allowed us to test LynxQA on new data.\n\nEach system is evaluated against data sets, and DOR and cost metrics are calculated. We\ncompare these metrics to baselines for performance and cost."}, {"title": "3.2 Overview of Strategies Tested in Hallucination Detection", "content": "In the pursuit of improving the reliability of Al systems, particularly in natural language\ngeneration tasks, various strategies for hallucination detection have been developed and tested.\nThis section introduces the key strategies evaluated in our comparative analysis, highlighting\ntheir mechanisms and intended applications. Each strategy leverages distinct approaches to\nmitigate the impact of hallucinations, enabling Al developers to select the most suitable method\nbased on their specific use cases.\n\nWhen we discuss systems in this paper we are referring to a combination of a strategy and a\nmodel. The strategy can be a singular prompt, a chain, or a more complex program for\nevaluation. The model is generally a LLM model, but could also be an ensemble of LLMs, or\neven a completely different kind of model. In comparing Pythia and LynxQA we are primarily\ncomparing their strategies. We will also look at their performance on other models as well."}, {"title": "1. LynxQA Strategy", "content": "LynxQA uses large language models (LLMs) to generate and verify answers to questions. While\neffective, it relies on the model's accuracy and requires continuous fine-tuning, leading to\npotentially high costs."}, {"title": "2. Pythia Strategy", "content": "Pythia is a proprietary strategy based on the concept of claim extraction. The idea is that\ncorrectness or faithfulness of an application's output is approximately equivalent to how well\nsupported the constituent claims of the output are supported by the context.\nWe separate use cases into different kinds of context and different kinds of outputs. Automatic\nsummarization has large context (references) and large outputs, though the outputs should be\nsmaller than the provided references. RAG-QA has large context and variable outputs.\nWhenever a piece of input or output is large, it is viable for claim extraction. Otherwise, it will be\ntreated as a claim itself. Pythia uses the classic natural language inference classes of\nentailment, contradiction, neutral to classify the claims in the output [Silva, V.. et al., 2018]. From\nthis, a factual accuracy metric is calculated. This factual accuracy score is then thresholded to\nmake a prediction (diagnosis)."}, {"title": null, "content": "Accuracy = $\\frac{W_e + W_c + W_r}{w_e(Entailment + \\epsilon)^{-1} + w_c(1 - Contradiction + \\epsilon)^{-1} + w_r(Reliability + \\epsilon)^{-1}}$"}, {"title": "3. Baseline Strategy - Grading", "content": "The Grading strategy is a simple one-prompt approach intended to work with minimal fine\ntuning. It instructs the LLM to grade the response using the A-F US grading system. This works\ndue to the many examples of text qualitatively judged with such a grade. Although the\ncommonness of this kind of evaluation helps the LLM assign a grade, the examples the model\nmight have seen likely are not purely about factual correctness. So there is potential for a\nnon-hallucinatory response to be given a low grade to some quality problem other than\naccuracy (e.g. writing style). The opposite problem can also occur if a hallucinatory response\nhas otherwise good qualities.\n\nEach strategy operates under unique principles that define their approach to detecting\nhallucinations:\n\nLynxQA relies on the strength of the LLM alone, emphasizing prompt engineering, but at the\ncost of increased resource consumption for model improvements.\n\nPythia enhances detection capabilities by integrating LLM outputs with external logic, allowing\nfor a distributed approach to processing and error mitigation."}, {"title": null, "content": "Baseline Strategy capitalize on the diversity of models to foster more reliable outputs through\nvalidation, yet they require sophisticated orchestration.\n\nBy comparing these strategies, our study aims to identify their strengths and weaknesses in\nreal-world applications, providing valuable insights for developers looking to enhance the\nreliability of their Al systems in the face of inherent challenges like hallucinations."}, {"title": "3.1. Case Study", "content": "The primary goal of this Pythia case study was to enhance the trustworthiness of generative Al\noutputs for a major pharmaceutical partner. The focus was on reducing the occurrence of\nhallucinations to ensure precision in patient care and safety. The pharmaceutical industry faces\nsignificant challenges related to the accuracy of Al-generated content. With some LLMs\nexhibiting error rates exceeding 25% in their outputs, the implications of these inaccuracies can\nbe severe, including operational disruptions, reputational damage, and life-threatening risks in\npatient care. Their existing methods of addressing these hallucinations relied on human\nannotation, which proved insufficient for scalability and timely intervention.\n\nPythia's automated evaluation of patient summaries generated by large language models\n(LLMs) showed a strong correlation with human-annotated results, validating the system's\neffectiveness in identifying inaccuracies. However, despite these positive findings, the\noccasional detection of discrepancies underscores the need for continuous monitoring to\nmaintain reliability over time."}, {"title": "4. Results and Analysis", "content": "Firstly, we will look over the automatic summarization results. To understand the performance,\nfirst let's look at the distribution of PASS/FAIL by component data set. All three source data sets\nare highly imbalanced. The labels were binarized by looking for perfect grades, and complete\nlabel agreement."}, {"title": null, "content": "Since LynxQA is only defined for the RAG-QA task, it does not have a score here. We see that\nThe Grading strategy performs best on SummEval, but Pythia does better on the QAGs data.\nNote also that the size of the 95% confidence interval for Grading is quite large on the QAG\nCNNDM data set. The interpretation of the score 11.44 (Pythia QAGS-CNNDM) is the ratio of\nthe odds that a summary is assigned \u201cFAIL\u201d given that it is labeled \u201cFAIL, with respect to the\nodds that a summary is assigned \u201cFAIL\u201d given that it is labeled \u201cPASS\u201d.\n\nNow let us look at the RAG-QA data. With the exception of RAGTruth these data sets are nearly\nbalanced. The difference between these data sets and the automatic summarization datasets\nillustrates why it is important to have a metric that is independent of prevalence."}, {"title": null, "content": "With the RAG-QA data, LynxQA performs competitively with the datasets on which it was\nengineered. Pythia performs best on the TruthfulQA dataset. The Grading strategy performs\nsimilarly to LynxQA on all datasets except RAG-QA."}, {"title": null, "content": "The three strategies compared above were all run with the GPT-40-mini model. The LynxQA\nstrategy was also run with the GPT-40 model, since it performs similarly to the fine tuned model\n[Ravi, S. S. et al., 2024]. This run performed well in terms of its DOR metric, but it incurs 16.85\ntimes the cost.\n\nFinally, let's compare these strategies using the Grading strategy as a baseline."}, {"title": "4.1 Analysis", "content": "In the analysis of the results, we can see that the difference between Pythia and LynxQA is less,\nrelatively speaking, on the TruthfulQA dataset. This dataset is the fairest comparison between\nthe two, since it is the task that LynxQA supports, but it was not prompt engineered on. Here we\nsee that the difference between Pythia and LynxQA (GPT-40), for DOR over Grading, is actually\nslightly worse, 50% higher ratio for overall data vs 30% higher for TruthfulQA in isolation."}, {"title": "5.Conclusion & Future Direction", "content": "The findings of this paper illustrate that the LynxQA strategy demonstrates improved\nperformance when paired with a more advanced model, resulting in a higher diagnostic odds\nratio (DOR). However, this improvement comes at a significantly increased cost, reflecting the\ninherent design differences between strategies. LynxQA utilizes a single prompt that relies\nentirely on the language model (LLM) to address the problem's complexity. In contrast, Pythia\nintegrates the LLM with external logic, demonstrating only a minor performance decrease when\ntransitioning from GPT-40 to GPT-40-mini in previous research.\n\nIn any LLM-based application, developers face a critical decision regarding the extent to which\nthey depend on the LLM to address the problem. This includes determining whether to use the\nLLM for the entire solution or to employ it for specific components while leveraging other\nmodels, algorithms, or heuristics for different parts. The trade-off lies in resource allocation\nversus potential improvements. Strategies that heavily rely on LLMs can achieve straightforward\nenhancements through fine-tuning specialized models. However, this reliance also poses risks;"}, {"title": null, "content": "if the task complexity demands a larger model, costs can escalate dramatically. Conversely,\nemploying an LLM for subtasks results in a more robust system across various model families\nand sizes, although improvements may become more complicated as it requires pinpointing and\naddressing underperforming components within the system.\n\nWhen choosing an effective hallucination detection system, it is essential to consider the\nspecific applications being evaluated. For instance, if an application encompasses various tasks\nlike automatic summarization and retrieval-augmented question answering (RAG-QA), the\ndetection system must be versatile enough to identify hallucinations across both functions. This\nconsideration of versatility is crucial in making informed trade-offs between strategy and model\nselection."}, {"title": "Future Research Directions", "content": "To build upon the findings of this paper, future research could focus on developing hybrid\nsystems that optimize the balance between LLM reliance and external logic integration,\nenhancing versatility and cost-effectiveness. Additionally, investigating methods to automate the\nidentification of underperforming components within complex systems may lead to more efficient\nstrategies for improvement. Further studies could also explore the long-term impacts of different\ndetection mechanisms on diverse applications, providing a comprehensive understanding of\nhow to best deploy and refine hallucination detection systems in real-world scenarios."}, {"title": "Appendix", "content": "Definition of Terms"}]}