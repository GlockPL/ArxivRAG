{"title": "Adaptive Width Neural Networks", "authors": ["Federico Errica", "Henrik Christiansen", "Viktor Zaverkin", "Mathias Niepert", "Francesco Alesiani"], "abstract": "For almost 70 years, researchers have mostly relied on hyper-parameter tuning to pick the width of neural networks' layers out of many possible choices. This paper challenges the status quo by introducing an easy-to-use technique to learn an unbounded width of a neural network's layer during training. The technique does not rely on alternate optimization nor hand-crafted gradient heuristics; rather, it jointly optimizes the width and the parameters of each layer via simple back-propagation. We apply the technique to a broad range of data domains such as tables, images, texts, and graphs, showing how the width adapts to the task's difficulty. By imposing a soft ordering of importance among neurons, it is possible to truncate the trained network at virtually zero cost, achieving a smooth trade-off between performance and compute resources in a structured way. Alternatively, one can dynamically compress the network with no performance degradation. In light of recent foundation models trained on large datasets, believed to require billions of parameters and where hyper-parameter tuning is unfeasible due to huge training costs, our approach stands as a viable alternative for width learning.", "sections": [{"title": "1. Introduction", "content": "Since the construction of the Mark I Perceptron machine the effective training of neural networks has remained an open research problem of great academic and practical value. The Mark I solved image recognition tasks by exploiting a layer of 512 fixed \u201cassociation units\" that in modern language correspond to the hidden units of a (possibly) Multi-Layer Perceptron (MLP). MLPs possess universal approximation capabilities when assuming arbitrary width and sigmoidal activations, and their convergence to good solutions was studied, for instance, in where the backpropagation algorithm was described as \"simple, easy to implement on parallel hardware\", and improvable by other techniques such as momentum that preserve locality of weight updates.\nYet, after almost 70 years of progress , the vast majority of neural networks, be they shallow or deep, still rely on a fixed choice of the number of neurons in their hidden layers. This property is typically treated as an architectural design choice, one of the many hyper-parameters that have to be carefully tuned whenever we approach a new task . The tuning process has many names, such as model selection, hyper-parameter tuning, and cross-validation, and it is associated with non-negligible costs: Different architectural configurations are to be trained until one that performs best on a validation set is selected . The configurations' space grows exponentially in the number of layers, so practitioners often resort to shortcuts such as picking a specific number of hidden units for all layers, which greatly reduces the search space together with the chances of selecting a better architecture for the task. Other techniques to tune hyper-parameters include constructive approaches  which alternate parameter optimization and creation of new neurons, natural gradient-based heuristics that dynamically modify the network , bi-level optimization , and neural architecture search , which often requires separate training runs for each configuration.\nThe hyper-parameters' space exploration problem is exacerbated by the steep increase in size of recent neural architectures for language  and vision , for example, where parameters are in the order of billions to (supposedly) accommodate for a huge dataset. Training these models requires an amount of time, compute power, and energy that currently makes it unfeasible for most institutions to perform a thorough model selection and find good width parameters; the commonly accepted compromise is to stick to previously successful hyper-parameter choices. This may also explain why network pruning , distillation  and quantization  techniques have recently been in the spotlight, as they trade-off hardware requirements and performance."}, {"title": "2. Related Work", "content": "Constructive methods dynamically learn the width of neural networks and are related in spirit to this work. The cascade correlation algorithm alternates standard training with the creation of a new hidden unit minimizing the neural network's residual error. Similarly, the firefly network descent grows the width and depth of a network every N training epochs via gradient descent on a dedicated loss. propose an ad-hoc algorithm for lifelong learning that grows the network by splitting and duplicating units to learn new tasks. alternate training the network and then splitting existing neurons into offspring with equal weights. These works mostly focus on growing the neural network; propose natural gradient-based heuristics to grow/shrink layers and hidden units of MLPs and CNNs. The main difference from our work is that we grow and shrink the network by simply computing the gradient of the loss, without relying on human-defined heuristics. The unbounded depth network of , from which we draw inspiration, learns the number of layers of neural networks. Compared to that work, we focus our attention to the number of neurons, modifying the internals of the architecture rather than instantiating a multi-output one. Finally, we mention Bayesian nonparametrics approaches that learn a potentially infinite number of clusters in an unsupervised fashion.\nOrthogonal Methods Neural Architecture Search (NAS) is an automated process that designs neural networks for a given task and has been applied to different contexts . Typically, neural network elements are added, removed, or modified based on validation performance , by means of reinforcement learning , evolutionary algorithms , and gradient-based approaches . Typical NAS methods require enormous computational resources, sometimes reaching thousands of GPU days , due to the retraining of each new configuration. While recent advances on one-shot NAS models have drastically reduced the computational costs, they mostly focus on CNNs, assume a bounded search space, and do not learn the width. As such, NAS methods are complementary to our approach. Bi-level optimization algorithms have also been used for hyper-parameter tuning , where hyper-parameters are the variables of the outer objective and the model parameters those of the inner objective. The solution sets of the inner problem are usually not available in closed form, which has been partly addressed by repeated application of (stochastic) gradient descent . These methods are restricted to continuous hyper-parameters' optimization, and cannot therefore be applied to width optimization. Finally, pruning and distillation  are two methods that (only) reduce the size of neural networks by trading-off performances; the former deletes neural connections  or entire neurons , the latter trains a smaller network (student) to mimic a larger one (teacher) . Compared to most pruning approaches, our work can delete connections and reduce the model's memory; compared to distillation, we do not necessarily need a new training. These techniques, however, can be easily combined with our approach."}, {"title": "3. Adaptive Width Learning", "content": "We introduce a general probabilistic framework, called Adaptive Width Neural Networks (AWNN) for convenience, showing how (ultimately simple) design choices arise from a variational inference treatment of a graphical model. We are given a dataset of N i.i.d. samples (xi, Yi), with input xi \u2208 RF, F \u2208 N+ and target yi whose domain depends on whether the task is regression or classification. The learning objective is to maximize\n$\\log [P(y_i|x_i) = \\sum_{i=1}^{N}\\logp(y_i|x_i)$ (1)\nwith respect to the learnable parameters of p(y|x). To formalize learning of an MLP\u00b9 that maximizes Equation (1) and learns an unbounded width for each hidden layer l, we assume the existence of an infinite sequence of i.i.d. latent variables 0\u2113 = {0ln}\u221en=1, where 0ln is a multivariate variable over the learnable weights of neuron n at layer l. Since this implies modeling an infinite-width layer, we introduce a latent variable Ae that decides how many neurons to use at each layer l. That is, it \"truncates\u201d an infinite width to a finite value so that we can perform inference. For a network of L layers, we define 0 = {0\u2113}L\u2113=1 and X = {\u03bb\u2113}L\u2113=1, assuming independence across layers. Therefore, one can write p(yi|xi) = \u222bp(yi, \u03bb, \u03b8|xi)d\u03bbd\u03b8. Figure 1 (left) describes our independence assumptions of the generative model, which decompose the joint distribution as:\n$p(Y_i, \\lambda, \\theta|x_i) = p(y_i|\\lambda, \\theta, x_i)p(\\lambda)p(\\theta)$ (2)\n$p(\\lambda) = \\prod_{l=1}^{L}p(\\lambda_l) = \\prod_{l=1}^{L}N(\\lambda_l; \\mu_l, \\sigma^2_l)$ (3)\n$p(\\theta) = \\prod_{l=1}^{L} \\prod_{n=1}^{\\infty} p(\\theta_{ln})=\\prod_{l=1}^{L}\\prod_{n=1}^{\\infty} N (\\theta_{ln}; 0, diag(\\sigma^2_{l}))$ (4)\n$P(Y_i \\lambda, \\theta, x_i) =$ Neural Network of Section 3.1. (5)\nHere, \u03c3\u2113, \u03bc\u2113, \u03c3\u2113 are hyper-parameters. The MLP is parametrized by realizations \u03bb, \u03b8, so it relies on a finite number of neurons and outputs either class probabilities (classification) or the mean of a Gaussian distribution (regression) to parametrize p(yi|\u03bb, \u03b8, xi) depending on the task. Maximizing Equation (1), however, requires computing the above integral, which is intractable. Therefore, we turn to mean-field variational inference  to maximize an expected lower bound (ELBO) instead. This requires to define a distribution over the latent variables q(\u03bb, \u03b8) and re-phrase the objective as:\n$\\sum_{i=1}^{N}\\log p(y_i|x_i) \\ge \\sum_{i=1}^{N}E_{q(\\lambda,\\theta)}\\Big[\\log \\frac{P(Y_i,\\lambda,\\theta|x_i)}{q(\\lambda,\\theta)}\\Big],$ (6)\nwhere q(\u03bb, \u03b8) is parametrized by learnable variational parameters. We factorize the variational distribution into:\nq(\u03bb, \u03b8) = q(\u03bb)q(\u03b8|\u03bb) (7)\n$q(\\lambda) = \\prod_{l=1}^{L}q(\\lambda_l) = \\prod_{l=1}^{L}N(\\lambda_l; \\nu_l, 1)$ (8)\n$\\prod_{l=1}^{L} \\prod_{n=1}^{D_l}q(\\theta_{ln})$\nq(\u03b8|\u03bb) =  (9)\n$q(\\theta_{ln}) = N(\\theta_{ln}; diag(p_{ln}), I).$ (10)\n$D_l =$ quantile function of $f_l(\\cdot;\\lambda_l)$ evaluated at k (11)\nThe value k is a hyper-parameter, \u03bd\u2113, p\u2113n are variational parameters and, as before, we define p\u2113 = {p\u2113n}Dn=1 , p = {p\u2113}L\u2113=1 and v = {\u03bd\u2113}L\u2113=1. Note that the set of variational parameters is finite. The truncated width Dl, that is the finite number of neurons at layer l, is computed as the quantile function evaluated at k of a distribution\u00b2 fl with infinite support over N+, parametrized by \u03bb\u2113. W.l.o.g., we implement fl as a discretized exponential distribution, following the discretization strategy of Roy (2003): For every natural x, the discretized distribution relies on the cumulative distribution function (c.d.f.) of the exponential:\n$f_l(x; \\lambda_l) = (1 - e^{\\lambda_l(x+1)}) - (1 - e^{\\lambda_l(x)}).$ (12)\nWe choose the exponential because it is a monotonically decreasing function and allows us to impose an ordering of importance among neurons, as detailed in Section 3.1. By expanding Equation 6 using the above definitions and approximating the expectations at the first order, i.e., Eq(x) [f(x)]=f(v) and Eq(\u03b8|\u03bb)[f(\u03b8)] = f(p) as in Nazaret & Blei (2022), we obtain the final form of the objective:\n$\\sum_{l=1}^{L}\\frac{p(\\nu_l; \\mu_l, \\sigma^2_l)}{q(\\nu_l; \\nu_l)} + \\sum_{l=1}^{L} \\sum_{n=1}^{D_l} \\log \\frac{p(\\rho_{ln}; \\sigma_l)}{q(\\rho_{ln}; \\rho_{ln})} + \\sum_{i=1}^{N}logp(y_i|\\lambda=\\nu, \\theta=\\rho, x_i)$ (13)\nwhere distributions' parameters are made explicit to distinguish them. The first two terms in the loss regularize the width of the layers and the magnitude of the parameters, respectively, whereas the third is is the predictive loss. In practice, the finite variational parameters v, p are those used by the neural network in place of \u03bb, \u03b8, which enables easy optimization via backpropagation. Maximizing Equation (13) will change each variational parameter v\u2113, which in turn will change the value of Dl during training. If Dl increases we initialize new neurons and draw their weights from a standard normal distribution, otherwise we remove the excess ones. When implementing mini-batch training, the predictive loss needs to be rescaled by N/M, where M is the mini-batch size. From a Bayesian perspective, this means regularizers weigh less if we have more data.\nCompared to a fixed-width network with weight decay, we need to choose the values of \u03bc\u2113, \u03c3\u2113, as well as initialize v\u2113 (shared across layers as there is no particular reason to do otherwise). Therefore, we have two more hyper-parameters compared to the fixed-width network, but we make some considerations: i) it is always possible to use an uninformative prior over \u03bb\u2113, removing the extra hyper-parameters letting the model freely adapt the width of each layer (as is typical of frequentist approaches); ii) the choice of higher level of hyper-parameters is known to be less stringent than that of hyper-parameters themselves , so we do not need to explore many values of \u03bc\u2113 and \u03c3\u2113; iii) our experiments suggest that AWNN converges to similar widths regardless of the starting point v\u2113, so that we may just need to perform model selection over one/two sensible initial values."}, {"title": "3.1. Imposing a Soft Ordering on Neurons' Importance", "content": "Now that the learning objective has been formalized, the missing ingredient is the definition of the neural network p(yi|\u03bb=\u03bd, \u03b8=\u03c1, xi). Compared to a standard MLP, we need to make use of the variational parameters \u03bd that affect the truncation width at each hidden layer, whereas p corresponds to the the usual MLP weights. We choose a monotonically decreasing function fl for the simple reason that when a new neuron is added, its relative importance is low and will not drastically impact the network. As a result, we are imposing a soft ordering of importance among neurons.\nWe modify the classical activation hl of a hidden neuron j at layer l as\n$h_j^l = \\sigma(\\sum_{k=1}^{D_{l-1}} W_{jk}h_k^{l-1}) \\cdot f_l (j; \\nu_l),$ (14)\nwhere Dl\u22121 is the truncated width of the previous layer, \u03c3 is a non-linear activation function and $W_{jk}^l \\in \\rho_{ej}$. That is, we rescale the activation of each neuron k by its \"importance\" $f_l (j; \\nu_l)$. Note that the bias parameter is taken into account by concatenating a dummy value 1 to $h_k^{l-1}$.\nIt is easy to see that, in theory, the optimization algorithm could rescale the weights of the next layer by a factor $1/f_l(j; \\nu_l)$ to compensate for the term $f_l(j; \\nu_l)$. This may lead to a degenerate situation, which is actually quite common in classical MLPs, where the activations of the first neurons are small relative to the others, thus breaking the soft-ordering and wasting neurons. There are two strategies to address this undesirable effect. The first is to regularize the magnitude of the weights thanks to the prior $p(\\theta_{l+1n})$, so that it may be difficult to compensate for the least important neurons that have a high $1/f_l(j)$. The second and less obvious strategy is to prevent the units' activations of the current layer to compensate for high values by bounding their range. We apply both strategies to our experiments."}, {"title": "3.2. Rescaled Weight Initialization for Deep AWNN", "content": "Rescaling the activations of hidden units using Equation 14 causes activations of deeper layers to quickly decay to zero, as shown in Figure 2 for a 5-layer AWNN MLP with ReLU nonlinearity initialized using the well known kaiming scheme . This affects convergence since gradients get close to zero and it becomes impractical to train deep AWNN MLPs. We therefore derive a rescaled kaiming weight that guarantees that the variance of activation across layers is constant at initialization.\nTheorem 3.1. Let us consider an MLP with activations as in Equation 14 and ReLU nonlinearity. At initialization, given\n$\\sigma( \\sum_{j=1}^{D_{e-1}} W_{jk}h_k^{l-1})$,\nVar[$W_{jk}^l$] = $\\frac{2}{\\sum_{j=1}^{D_{e-1}} (\\p_{j}^l)^2} \\Rightarrow$ Var[$a$] \u2248 Var[$h_k^{l-1}$] (15)\nAlgorithm 1 summarizes the main changes to the training procedure, namely the new initialization and the update of the model's truncated width at each training step."}, {"title": "3.3. Future Directions and Limitations", "content": "MLPs are ubiquitous in modern deep architectures. They are used at the very end of CNNs, in each encoder and decoder layer of Transformer, and they process messages coming from neighbors in DGNs. Our experiments focus on MLPs to showcase AWNN's broad applicability, but there are many other scenarios where one can apply AWNN's principles. For instance, one could impose a soft ordering of importance on CNNs' filters at each layer, therefore learning the number of filters during training. Alternatively, one could investigate the application of AWNN to timeseries, since recurrent networks can be seen as a special case of an MLP with weight sharing. From a more theoretical perspective, we believe one could draw connections between our technique and the Information Bottleneck principle , which seeks maximally representative (i.e., performance) and compact representations (e.g., width). Finally, the analysis of different functions f to soft-order neurons, for instance a power-law distribution or a sigmoid-like function, may have a different impact on convergence and performance and could be empirically investigated.\nThe current limitation of AWNN is the relative overhead to change the network at training time. A minimal and na\u00efve implementation has up to 10x cost compared to a fixed network (this relative overhead may reduce for more compute-bound tasks). Besides optimizing our code, other optimizations are conceivable: i) by updating the width every M minibatches rather than at every step; ii) by allocating a larger network and dynamically grow it only if needed; iii) by creating ad-hoc support of AWNN on deep learning libraries like Pytorch. In any case, this overhead is relatively small compared to the exponential size of the hyper-parameter space when cross-validating the number of neurons of a multi-layered architecture."}, {"title": "4. Experiments and Setup", "content": "The purpose of the empirical analysis is not to claim AWNN is generally better than the fixed-width baseline. Rather, we demonstrate how AWNN overcomes the problem of fixing the number of neurons by learning it, thus reducing the amount of configurations to test. As such, due to the nature of this work and similarly to , we use the remaining space to thoroughly study the behavior of AWNN, so that it becomes clear how to use it in practice. We first quantitatively verify that AWNN does not harm the performance compared to baseline models and compare the chosen width by means of grid-search model selection with the learned width of AWNN. Secondly, we check that AWNN chooses a higher width for harder tasks, which can be seen as increasing the hypotheses space until the neural network finds a good path to convergence. Third, we verify that convergence speed is not significantly altered by AWNN, so that the main limitation lies in the extra overhead for adapting the network at each training step. As a sanity check, we study conditions under which AWNN's learned width does not seem to depend on starting hyper-parameters, so that their choice does not matter much. Finally, we analyze other practical advantages of training a neural network under the AWNN framework: the ability to compress information during training or post training, and the resulting trade-offs. Further analyses are in the Appendix.\nWe now provide the details to reproduce our experiments on 9 datasets. We compare a baseline that undergoes proper hyper-parameter tuning against its AWNN version, where we replace any fixed MLP with an adaptive one. First, we train an MLP on 3 synthetic tabular tasks of increasing binary classification difficulty, namely a double moon, a spiral, and a double spiral that we call SpiralHard. A stratified hold-out split of 70% training/10% validation/20% test for risk assessment is chosen at random for these datasets. Similarly, we consider a ResNet-20  trained on 3 image classification tasks, namely MNIST , CIFAR10, and CIFAR100 , where data splits and preprocessing are taken from the original paper and AWNN is applied to the downstream classifier. In the graph domain, we train a Graph Isomorphism Network  on the NCI1 and REDDIT-B classification tasks using the same split and evaluation setup of . These tasks are chosen as the structure was found to have a proper influence on the final performance. Here, the first 1 hidden layer MLP as well as the one used in each graph convolutional layer are replaced by adaptive AWNN versions. On all these tasks, the metric of interest is the accuracy. Finally, for the textual domain we train a Transformer architecture  on the Multi30k English-German translation task , using a pretrained GPT-2 Tokenizer, and we evaluate the cross-entropy loss over the translated words. On tabular, image, and text-based tasks, an internal validation set (10%) for model selection is extracted from the union of outer training and validation sets, and the best configuration chosen according to the internal validation set is retrained 10 times on the outer train/validation/test splits, averaging test performances. Due to space reasons, we report datasets statistics and the hyper-parameter tried for the fixed and AWNN versions in Appendix B and C, respectively."}, {"title": "5. Results", "content": "We begin by discussing the quantitative results of our experiments: Table 1 reports means and standard deviations across the 10 final training runs. In terms of performance, we observe that AWNN is more stable or accurate than a fixed MLP on DoubleMoon, Spiral, and SpiralHard; all other things being equal, it seems that using more neurons and their soft ordering are the main contributing factors to these improvements. On the image datasets, performances of AWNN are comparable to those of the fixed baseline but for CIFAR100, due to an unlucky run that did not converge. In this case, AWNN learns a smaller total width compared to grid search.\nResults on graph datasets are interesting in two respects: First, the performance on REDDIT-B is significantly improved by AWNN both in terms of average performance and stability of results; second, the total learned width is significantly higher than those tried in ; meaning that a biased choice of a good range of width has had a profound influence on the estimation of the risk for a specific family of DGN models (i.e., GIN). This result makes it evident that it is important to let the network decide how many neurons are necessary to solve the task. To check that the learned width is indeed conducive to good performances, Appendix D shows what happens when we retrain some fixed baselines using the total width as the width of each layer.\nFinally, the results on the Multi30k show that the AWNN Transformer learns to use 200x parameters less than the fixed Transformer for the feed-forward networks, achieving a statistically comparable test loss. This result is appealing when read through the lenses of modern deep learning, as the power required by some neural networks such as Large Language Models is so high that cannot be afforded by most institutions, and it demands future investigations."}, {"title": "5.1. Adaptation to Task Difficulty and Convergence", "content": "Intuitively, one would expect that AWNN learned larger widths for more difficult tasks. This is indeed what happens on the tabular datasets (and image datasets, see Appendix E) where some tasks are clearly harder than others. Figure 3 (left) shows that, given the same starting width per layer, the learned number of neurons grows according to the task's difficulty. It is also interesting to observe that the total width for a multi-layer MLP on SpiralHard is significantly lower than that achieved by a single-layer MLP, which is consistent with the circuit complexity theory arguments put forward in . It also appears that convergence is not affected by the introduction of AWNN, as investigated in Figure 3 (right), which was not obvious considering the parametrization constraints encouraged by the rescaling of neurons' activations."}, {"title": "5.2. Training Stability Analysis", "content": "To support our argument that AWNN can reduce the time spent performing hyper-parameter selection, it is instructive to check whether AWNN learns a consistent amount of neurons across different training runs and hyper-parameter choices. Figure 4 reports the impact of the batch size and starting width averaged across the different configurations tried during model selection. Smaller batch sizes cause more instability, but in the long run we observe convergence to a similar width. Convergence with respect to different rates holds, instead, for the bounded ReLU6 activation; Appendix F shows that unbounded activations may cause the network to converge more slowly to the same width, which is in accord with the considerations about counterbalancing the rescaling effect of Section 3.1. Therefore, whenever possible, we recommend using bounded activations."}, {"title": "5.3. Online Network Compression via Regularization", "content": "So far, we have used an uninformative prior p(\u03bb) over the neural networks' width. We demonstrate the effect of an informative prior by performing an annealing experiment on the SpiralHard dataset. We set an uninformative p(\u03b8) and ReLU6 nonlinearity. At epoch 1000, we introduce p(\u03bb\u03b5) = N(\u03bb\u03b5; 0.05, 1), and gradually anneal the standard deviation up to 0.1 at epoch 2500. Figure 5 shows that the width of the network reduces from approximately 800 neurons to 300 without any test performance de gradation. We hypothesize that the least important neurons mostly carry negligible information, and therefore they can be safely removed without drastic changes in the output of the model."}, {"title": "5.4. Post-hoc Trucation achieves a trade-off between performance and compute resources", "content": "To further investigate the advantages of imposing a soft ordering of importance among neurons, we show that it is possible to perform hard network's truncation after training while still controlling the trade-off between performance and network size. Figure 6 shows an example for an MLP on the Spiral dataset, where the range of activation values (Equation 14) computed for all samples follows an exponential curve (right). Therefore, it is expected that removing the last neurons may have a negligible performance impact at the beginning and a drastic one as few neurons remain. This is what happens in the left plot, where we are able to cut an MLP with hidden width 83 by 30% without loss of accuracy, after which a smooth degradation happens. If one accepts such a trade-off, this technique may be used to distill a trained neural network at virtually zero additional cost while reducing the memory requirements."}, {"title": "6. Conclusions", "content": "We introduced a new methodology to learn an unbounded width of neural network layers within a single training, by imposing a soft ordering of importance among neurons. Our approach requires very few changes to the architecture, adapts the width to the task's difficulty, and does not impact negatively convergence, although it introduces some overhead during training. We showed stability convergence to similar widths under bounded activations for different hyper-parameters configurations, advocating for a practical reduction of the width's search space. A by-product of neurons' ordering is the ability to easily compress the network during or after training, which is relevant in the context of foundational models trained on large data, which are believed to require billions of parameters. Finally, we have tested AWNN on different models and data domains to prove its broad scope of applicability: a Transformer architecture achieved the same loss with 200x less parameters."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."}, {"title": "A. Full derivation of the rescaled weight initialization", "content": "This Section derives the formulas for the rescaled weight initialization both in the case of ReLU and activations like tanh.\nBackground We can rewrite Equation 14 as\n$h_j^l = \\alpha_j p_j$ (16)\n$\\alpha_j = \\sigma( \\sum_{i=1}^{D_{e-1}} W_{ji}h_i^{l-1})$ (17)\nwhere $p_j = f_e(i)$.\nAs a refresher, the chain rule of calculus states that, given two differentiable functions g : RD \u2192 R and f = (f1, ..., fD) :\nR \u2192 RD, their composition g\u25e6 f : R \u2192 R is differentiable and\n$(g\\circ f)' (t) = \\nabla g(f(t)) \\cdot f'(t)$\n$\\nabla g(f(t)) = (\\frac{dg(f_1(t))}{df_1(t)},..., \\frac{dg(f_D(t))}{df_D(t)}) \\in R^{1xD}$\n$f'(t) = (f_1'(t), ..., f_D'(t)) \\in R^{D\\times 1}$\nFor reasons that will become clear later, we may want to compute the gradient of the loss function with respect to the intermediate activations a\u2113 at a given layer, that is \u2207L(a\u2113) = ($\\frac{\\partial L(a^e)}{\\partial a_i^e}, ..., \\frac{\\partial L(a^e)}{\\partial a_{Ne}^e}$). We focus on the i-th partial derivative$\\frac{\\partial L(a^e)}{\\partial a_i^e}$, where the only variable is ai\u2113. Then, we view the computation of the loss function starting from a\u2113 as a composition of a function G\u2113+1 : R \u2192 RN\u2113+1 = $(a_1^{l+1}(a^l), ..., a_{N_{l+1}}^{l+1}(a^l))$ and another function (abusing the notation) L : RN\u2113+1 \u2192 R that computes the loss value starting from a\u2113+1. By the chain rule:\n$\\frac{\\partial L(a^e)}{\\partial a_i^e} = (\\frac{\\partial L(a^{l+1})}{\\partial a_1^{l+1}}, ...,\\frac{\\partial L(a^{l+1})}{\\partial a_{N_{l+1}}^{l+1}}) \\cdot (\\frac{\\partial a_1^{l+1}(a^e)}{\\partial a_i^e}, ..., \\frac{\\partial a_{N_{l+1}}^{l+1}(a^e)}{\\partial a_i^e})$ (18)\nTheorem 3.1 Let us consider an MLP with activations as in Equation 17. Let us also assume that the inputs and the parameters have been sampled independently from a Gaussian distribution with zero mean and variance \u03c32 = Var[wij\u2113] \u2200i, j, l. At initialization, the variance of the responses a\u2113 across layers is constant if, \u2200l \u2208 {1, ..., L}\nVar[$w^e$] = $\\frac{1}{D_{e-1} (p_j^{e-1})^2}$ for activation \u03c3 such that \u03c3\u2032(0) \u2248 1 (19)\nVar[$w^e$] = $\\frac{2}{D_{e-1} (p_j^{e-1})^2}$ for the ReLU activation. (20)\nIn addition, we provide closed form formulas to to preserve the variance of the gradient across layers.\nProof. Let us start from the first case of \u03c3\u2032(0) \u2248 1. Using the Taylor expansions for the moments of functions of random variables as in Glorot & Bengio (2010)\nVar[$a^l$] = Var[$\\sigma( \\sum_{i=1}^{D_{e-1}} W_{ji}h_i^{l-1})p_j^e$] (21)\n\u2248 Var[$\\sigma'( \\sum_{i=1}^{D_{e-1}} W_{ji}h_i^{l-1})p_j^e$] (22)"}, {"title": "Adaptive Width Neural Networks", "content": "Using the fact that pj is a constant and that w and a are independent from each other\n$E[\\sum_{j=1}^{D_{e-1}} W_{ji}h_i^{l-1}", "E[W_{ji}": "E[h_i^{l-1}", "W_{ji}h_i^{l-1}": "sigma'(0) = 1$ (24)\nAs a result, we arrive at\nVar[$a^l$", "Var[$W_{ji}W_{ji}^{l-1}p_j^e$": 25, "Var[aX": "na2Var[X", "W_{ji}W_{ji}^{l-1}p_j^e$": "sum_{j=1}^{D_{e-1}} Var[W_{ji}", "Var[W_{ji}^{l-1}": "p_j^{e-1})^2$ (26)\nFinally, because the mean of the independent variables involved is zero by assumption, it holds that Var[$h_i^{l-1}$", "nVar[$W_{ji}^l$": "Var[W_{ji}^{l-1}", "Var[ai\u2113": "Var[aj\u2113", "obtaining\u00b3\nVar[$a^l$": "Var[w2", "p_j^{l-1})^2$": 27, "Var[a\u2113": "Var[a\u2113-1", "whenever\nVar[$w^e$": "frac{1}{\\sum_{i=1}^{D_{e-1}} (p_j^{e-1})^2}$ (28)\nCondition on the gradients From a backpropagation perspective, a similar desideratum would be to ensure that\nVar[$\\frac{\\partial L(a^e)}{\\partial a_i^e}$", "a_i^{l+1}}$": ".", "1,\nVar[$a^l$": "Var[$W_{ij}^{l+1} \\frac{\\partial h_{l+1}}{\\partial a_i^e}$"}]}