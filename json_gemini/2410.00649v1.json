{"title": "LASMP: Language Aided Subset Sampling Based Motion Planner", "authors": ["Saswati Bhattacharjee", "Anirban Sinha", "Chinwe Ekenna"], "abstract": "This paper presents the Language Aided Subset Sampling Based Motion Planner (LASMP), a system that helps mobile robots plan their movements by using natural language instructions. LASMP uses a modified version of the Rapidly Exploring Random Tree (RRT) method, which is guided by user-provided commands processed through a language model (ROBERTa). The system improves efficiency by focusing on specific areas of the robot's workspace based on these instructions, making it faster and less resource-intensive. Compared to traditional RRT methods, LASMP reduces the number of nodes needed by 55% and cuts random sample queries by 80%, while still generating safe, collision-free paths. Tested in both simulated and real-world environments, LASMP has shown better performance in handling complex indoor scenarios. The results highlight the potential of combining language processing with motion planning to make robot navigation more efficient.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous robot navigation has expanded into numerous areas, including indoor and outdoor exploration, collab-oration among multiple agents, localization and mapping (SLAM) [1], interaction with humans, self-driving technolo-gies, search and rescue missions, warehouse automation, and space exploration. Advances in Visual Large Language Mod-els (VLLMs) [2]-[6] have enabled autonomous navigation to interface with humans effectively. The fundamental idea behind VLLM-based robot navigation is to map the language and visual semantics into the robot's navigation controls in an end-to-end fashion. The mapping is done by training one or multiple neural networks in supervised [7], [8], imitation [4], or reinforcement learning frameworks [9]. To train these models, several works have developed datasets [2], [3], [10]-[12]. A recent advancement in this field is to reason about the decisions made by the trained networks [13], [14] and interact with the user to make navigation decisions for the next steps with the help of Generative Pretrained Trans-former models [15]\u2013[17]. Despite the success of VLLMs for indoor [2], [18], [19] and outdoor [5], [11], [13] autonomous navigation, a major limitation has been the requirement of exponentially large datasets [20] for training.\nIn many of the end-to-end robot navigation research, [18], [22]-[24], RRT [25], [26] or its variants [27], [28] are used to find global or local plans. Although RRT plan-ners are probabilistically complete, they are not sample-efficient. Specifically, random samples are often discarded because they cannot be added to the search tree, leading to unnecessary computational load on autonomous vehicles with limited energy resources. Although the work in [29] addresses this issue by using a prolated ellipsoidal subset of the workspace, it requires a precomputed path; otherwise, the algorithm behaves like standard RRT until a path is found. Fig. 1 illustrates our proposed planner. When the robot receives the command Go to the living room, it interprets the necessary navigation command (NC) as \"left\", \"right\", \"right\" to reach the destination. We introduce the Language Aided Subset Sampling Based Motion Planner (LASMP) for mobile robot navigation, enhanced with language assistance. The overview of LASMP is shown in Fig. 2. Upon receiving user instructions via text or speech, a trained LLM identifies the destination and optionally navigation entities like left or right on a metric map [30]. If no navigation entities are provided, the robot's current and destination positions are fed into a pre-trained network, which identifies a sequence of navigation instructions. Using these instructions, a mod-ified, sample-efficient RRT planner, one of the paper's key contributions, computes a collision-free path.\nOur contributions are\n\u2022 LASMP provides a structured system that converts natural language instructions (both text and speech) into low-level motion commands. It is tested on different robots across various environments.\n\u2022 We introduce an improved RRT planner that leverages language-based inputs, significantly improving sample efficiency. The planner dynamically adjusts the sam-pling area based on the robot's current pose and the user's commands, leading to faster and more efficient path planning compared to traditional RRT approaches.\n\u2022 We developed a dataset that includes destination names or navigation-related instructions. This dataset was used to train a transformer-based model (ROBERTa) to im-prove the system's ability to recognize navigation enti-ties and plan paths accordingly.\n\u2022 LASMP combines natural language processing and path planning by interpreting abstract high-level commands into precise turn-by-turn directions. This not only en-"}, {"title": "II. RELATED WORK", "content": "Language commands have shown great potential in guid-ing autonomous vehicles (AVs) for human-robot interaction. Several approaches have been developed to ground natural language commands into controls for AVs. A popular method is end-to-end navigation, where language, vision, or other sensor data are fused to train AI models for generating motion commands. In [3], [12], language instructions are combined with local vision data to train networks for se-quential control inputs, while [31] trains a LSTM-RNN to translate language commands into motion sequences. A modular approach is proposed in [11], where a natural language encoder and semantic image input suggest potential waypoints, and an RRT planner generates a collision-free path. Room-to-Room (R2R) navigation using vision and language has been explored in [2], [19]. While end-to-end methods have shown success, they often require large labeled datasets, limiting their use.\nRecent advancements in large language models (LLMs) integrate multiple modalities, such as audio [32], [33], video [34], [35], and point cloud data [36], [37]. Building on these works, we employed a speech recognition approach to interpret spoken language and enhance model diversity.\nAnother line of research uses natural language to define constraints in robot navigation. In [18], a local cost map is updated based on language instructions mapped to envi-ronmental objects. Generalized Grounded Graphs (G\u00b3) [38] and Dynamic Grounding Graphs (DGG) [39] dynamically parse language to identify motion constraints. Similarly, [40] presents motion planning as a constrained optimization problem, where constraints activate or deactivate based on language input. However, extracting complex constraints from language remains challenging.\nRecent work like GPT-Driver [15] treats AV motion plan-ning as a language modeling problem, mapping trajectory data into words for safe navigation. In [41], LLMs fuse environmental data to generate AV controls with reasoning, while DriveGPT4 [34] interprets video sequences for control signals. Further, LLMs can convert scene and guidance data into numerical instructions for controllers, as seen in [16], and LaMPilot [17] autonomously generates code to enhance AV functionality. However, most of these approaches are still in the simulation stage with limited real-world testing.\nLanguage models have also been applied to aid plan for manipulation tasks. In [42], a framework learns a collision function based on robot state, scene information, and lan-guage prompts, using it for motion planning. DeepRRT [23], [24] combines vision data, language lexicons, and a proposal layer to guide RRT planners in exploring robot configura-tions. However, none of these methods integrate language input to improve the sample efficiency of RRT planners.\nOur method uses NCs to make RRT planners sample efficient for generating low-level controls. We introduce a novel modified RRT algorithm that leverages language instructions to dynamically select a subset of the robot's workspace for random sampling to grow the search tree. Unlike Informed RRT* [29], which requires a precomputed path and adjusts only the volume of the subset, our method intelligently adjusts both the orientation and size of the subset based on language input, without needing a precom-puted path. Additionally, while [24] focuses on optimizing node extension direction using vision data, which requires significant computational resources, our approach improves sample efficiency using a ray-casting technique to sense the environment, making it an energy-efficient solution."}, {"title": "III. METHODOLOGY", "content": "Fig. 2(a) shows the workflow of computing a collision-free navigation path from textual or verbal commands, while Fig. 2(b) extends it by illustrating the transcription of verbal commands using the Whisper architecture [43].\nThe proposed robot navigation framework consists of two main modules. As described in section III-A, the first module converts high-level natural language instructions into low-level NCs. For example, the instruction \"go to kitchen\" is transformed into the sequence [left \u2192 right \u2192 left] with identified destination kitchen. The second module, LASMP, is a sample-efficient planner that uses these low-level com-mands to guide the path search, as detailed in section III-B.\nA. Grounding Natural Language Instructions\nThe proposed framework handles speech-based instruc-tions with a transformer-based Automatic Speech Recogni-"}, {"title": "B. Language Assisted Sampling-Based Motion Planner", "content": "Let's define the planning domain as $D \\subset R^n$ and $D_{obs} \\subset D$ is the set of states in collision. Then the set of collision-free states is $D_{free} = D \\backslash D_{obs}$. An element $x(t) \\in D$ denotes the state of the robot at time instant t. Let $\\Phi = [\\Phi_1,..., \\Phi_n]$ denote an ordered list of the low-level NC inferred from the input textual (or speech) instruction with its elements as $\\phi_i, i \\in {1,...,n}$. Let's also define the augmented state of the robot as $\\tilde{x(t)} = (x(t), \\Phi_i)$. The operator (.) when applied to the augmented state returns the actual state, i.e., $\\tilde{x(t)} \\rightarrow x(t)$. Given the start and goal state of the robot as $x_s$ and $x_g$ and NC list $\\Phi$, the LASMP finds a collision-free path\n$P = {x(t)|x(0) = x_s, x(t_f) = x_f, \\forall x(t) \\in D_{free}}$   (1)\nFor brevity, we will drop t to express the robot's state for the remaining of the paper or specify it as needed.\nLASMP Solution Methodology: The LASMP is a sample-efficient RRT-type motion planner that can find a collision-free path by sampling from a smaller subset of the planning domain D. Since the planner is informed with an ordered turn list $\\Phi$, the planner employs a local search strategy to find a collision-free path to the intersections to complete the turns in order one by one. This structure of the problem allows us to decompose the planning problem into several smaller subproblems. Given a planning problem P and an associated with n commands, we need to solve (n+1) planning sub-problems. Mathematically we can write,\n$P = {P_0,... P_{n+1}} = {P_j}$ where $j \\in {0,..., n + 1}$    (2)\nThe goal state of the path for $P_j$ becomes the initial state of the sub-problem $P_{j+1}$ with the NC updated to $\\phi_{j+1}$ i.e.,\n$P_j = {(\\tilde{x(t_k)},..., (\\tilde{x(t_{k+m})}, \\phi_j)}$   (3)\n$P_{j+1} = {((\\tilde{x(t_{k+m})}, \\phi_{j+1}),...,\\tilde(t_{k+m+r})}$  (4)\nwhere m and r are the number of states in the paths for $P_j$ and $P_{j+1}$. Next solution method for $P_j$ is discussed.\nSolution methodology of $P_i$: The planning sub-problems, $P_i$s, only have the starting states defined whereas their goal states are not known uniquely before the problems are solved, except for $P_{n+1}$ which has its goal state as $x_f$. However, since $\\phi_j$ is known, the goal state $x_{jf}$ of the planning problem $P_j$ could be any element of the set $D_{i_s} \\subset D$, i.e., $x_{jf} \\in D_{i_s}$ where $D_{i_s}$ is the set associated with the states at an intersection region in the planning environment where the motion command $\\phi_j$ needs to be executed. However, the region of the $D_{i_s}$ is not known during the planning time either. Therefore the planning problem $P_j$ reduces to finding a collision-free path while simultaneously identifying a goal state $x_{jf}$ as described next.\n1) Collision-free Path for planning sub-problem $P_i$: From now on, we will consider the planning domain $D \\subset R^2$ for the ease of explanation of the method. Then the states and augmented states are now defined as $x = (x_r, y_r)$ and $\\tilde{x} = (x_r, y_r, \\phi_j)$ respectively where $x_r, y_r$ defines the robot's position. Also for mobile robots, $\\phi_j$ can be mapped to a unit direction vector, $v_j = [v_x, v_y] \\in R^2$ towards which the robot needs to take the turn from the state $x_{jf}$.\nHere we assume $x_{jf}$ is known and in the next subsection, we provide method to find it. Known $x_{jf}$ turns $P_j$ into a regular planning problem but grounded with $\\phi_j$. Thus instead of sampling from the whole planning domain, it will be efficient to sample from a subset of the planning domain. For user-defined parameters h and w, we propose a rectangular subset to draw random samples to grow the search tree. The rectangular subset is a function of robot state x corresponding to the node that is being expanded to grow the search tree. The vertices of the rectangular subset are defined as\n$\\chi_{v_1}, \\chi_{v_3} = x \\pm [w/2,h/2]^T$  \n$\\chi_{v_2}, \\chi_{v_4} = x \\pm [w/2, -h/2]^T$  (5)\nThen the sampling subset is defined as,\n$D_{smp} = {(x,y)|x_{min} \\leq \\chi_{v_i} \\leq x_{max}}, i \\in {1,...,4}$   (6)\nwhere $x_{min} = (X_{min}, Y_{min})$, $x_{max} = (X_{max}, Y_{max})$. Note that, in sampling-based path planning configuration space is generally preferred [47]-[49] but for LASMP task space sampling [50] is adopted since subset is defined in there."}, {"title": "Remark 1.", "content": "Generating random samples from a local subset as compared to the complete planning domain increases the probability of sampling from the desired region by a factor of $\\frac{\\zeta(D)}{\\zeta(D_{smp})}$ where $\\zeta$ is a set measure."}, {"title": "Remark 2.", "content": "Sampling from a subset instead of the whole planning domain helps the planner to focus on the informed regions where openings to take turns would exist. This allows more samples to be generated in the area of interest and po-tentially get added to the tree. LASMP uses subset sampling to find collision-free paths and detect narrow openings where commanded turns can be executed.\n2) Detecting intersection: In section III-B.1 we assumed $x_{jf}$ is known, but $x_{jf}$ could be any state in the set $D_{i_s}$. Note that $x_{jf}$ is the terminal state for the planning sub-problem $P_j$ at which the navigation instruction $\\phi_j$ is executed. This implies that at the state $x_{jf}$ if a ray is cast along the direction $v_j$ then it will not hit any obstacle at least for a threshold distance d. d is a user defined parameter.\nIn order to detect whether any state in $D_{i_s}$ is reached while expanding a node N with state $x_{near}$ towards a randomly sampled state $x_{rand}$, a ray is cast along the $v_j$ from each of the discretized states between $x_{near}$ and $x_{rand}$. If the discretization is done with a step size d, then the kth intermediate state $x_{im}$ between $x_{near}$ and $x_{rand}$ will be\n$x_{im}^k = x_{near} + kd\\frac{(x_{rand} - x_{near})}{||X_{rand} \u2013 X_{near} ||}$  (7)\nIf for consecutive ncons points, i.e., $x_{im}^k,...,x_{n}^k,...,x_{ncons}^k$, there are no obstacles found upto a distance d along the direction $v_j$, then the state $x_{ncons}^k$ is marked as the terminal state i.e., $x_{jf}$ of the planning sub-problem $P_j$.\nIn Algorithm1, the steps of the LASMP are shown.\nThe GetSubset function is an overloaded function that uses Eq.(5), while the function Intersection implements the method in sectionIII-B.2. The variable $F_{turn}$ is a flag which raised to true if $x_{ncons}^k$ is $x_{jf}$."}, {"title": "IV. IMPLEMENTATION DETAILS", "content": "LASMP is tested in Coppeliasim [51] environment using the MATLAB remote interface on an Ubuntu machine with an Intel i7 processor, 16GB RAM, and an NVIDIA GeForce RTX GPU. We used four planning scenes-three simulated (Fig. 3) and one real-world (Fig. 9)\u2014with three robots, Turtlebot3 [52] and Pioneer3DX [53] for simulation, and Turtlebot2 [52] for real-world experiments. The Spa-CyV3 [44] framework was used to train the language models.\nA. Planning Scenarios\nThe three planning scenes considered in the paper are the domestic environment (DE), office space (OS), and random obstacle (RO) scenes. The point cloud representations of these scenes are converted into three-dimensional occupancy grid maps for planning purposes as shown in Fig. 3. The paths computed by LASMP are smoothened before executing on the robots using a simple pure pursuit-type controller. For safe obstacle avoidance, inflated occupancy grid maps are used. The test parameters for LASMP are detailed in [54]. For all the planning scenarios, the performance of LASMP is evaluated against classical RRT with the following metrics (i) the number of nodes added to the search tree and (ii) the number of queries to the random sample generator."}, {"title": "Algorithm 1", "content": "LASMP"}, {"title": "V. RESULTS", "content": "In section V-A we first describe the performance of the transformer-based models in predicting navigation-related entities from the speech and textual commands. In section V-B we evaluate the performance of LASMP in finding paths as compared to classical RRT, followed by experimental results.\nA. Evaluation of speech and language models\nThis section presents the performance analysis of the ASR and NER tasks. The Whisper's performance for ASR task was evaluated using several metrics: Number of Correct Words (CW), Number of Deleted Words (DEL), Number of Substituted Words (SUB), Number of Inserted Words (INS), Word Error Rate (WER), and Character Error Rate (CER). Fig. 4(a) illustrates the performance of the Whisper model which is obtained by utilizing a subset of our dataset. The results in Fig. 4(a) show that the Whisper model correctly identified 90% of the words. We achieved an average error rate for WER of 0.08728 and for CER of 0.0487, indicating good transcription accuracy in both word and character-level metrics. To evaluate the performance of NER task, we trained language models using our dataset, and the resulting perfor-mance metrics were presented in TableII and an illustration was presented in Fig.4(b). The results demonstrate the strong performance of RoBERTa in identifying the custom entities, with high precision (0.883), recall (0.903), and F1 score (0.893) which ensures transformer-based models are more suitable for our NER task than deep learning-based models."}, {"title": "B. Performance of the LASMP", "content": "1) Evaluation Metrics and Performance Comparison: We evaluated LASMP's performance using two key metrics: (a) the number of queries made to the random function generator, and (b) the total number of nodes added to the final search tree before finding a collision-free path. Unlike optimal RRT variants, which require a precomputed feasible path, LASMP operates without such requirements, making it more flexible in real-time applications. For the evaluation, we considered several start and goal states in each of the three planning scenes (Fig. 3). These states were selected to involve 2, 3, or 4 turns, increasing the complexity of the planning problem. Table IV summarizes the results across different scenes, with visual representations shown in Fig. 6.\nIn the DE scene with two turning commands, RRT re-quired 42 nodes, about 5 times more than LASMP, which only required 8 nodes. RRT also queried the random state generator 79 times, while LASMP made only 8 queries. In the more complex scenario with three turns, RRT required 43 nodes and 75 queries, compared to LASMP's 10 nodes and 10 queries. Similarly, in the four-turn DE environment, RRT needed 33 nodes and 66 queries, while LASMP required only 16 nodes and 17 queries. These trends are consistent across other environments, as LASMP consistently outperformed RRT in both node generation and query count. On average, LASMP reduced node generation by 55% and query genera-tion by 80%, demonstrating its high sample efficiency. These results, averaged over 10 independent runs, are summarized in Table IV and Table V and visualized in Fig. 7, show LASMP's superior performance across all cases.\n2) Path Length and Real-World Execution: Table III reports the path lengths (PL) and elapsed times (ET) for the planning problems on different environments. In most cases, LASMP computed shorter paths than RRT. In the RO environment, LASMP took ~ 89% less time to compute path compared to RRT, underscoring LASMP's time efficiency. The search trees generated by LASMP and RRT for three planning problems in three different scenes are shown in Fig. 6. The green vertices represent the nodes, and yellow edges represent the connections, with the computed paths highlighted in purple. The top row shows the search trees for LASMP, while the bottom row shows those for RRT. As evident from these visuals, LASMP required fewer nodes to find a feasible path, resulting in more efficient planning. The start and goal states, along with the retrieved navigation commands, are listed in Table VI. To verify whether the computed paths could be executed on physical robots, we deployed a trajectory-following controller on Pioneer3DX and Turtlebot3 robots. These robots were chosen for their variability in wheel separation distances and wheel radii (Ta-ble VII), two key kinematic parameters influencing control velocities in path-following tasks. Both robots successfully followed the smoothed paths generated by LASMP using spline curve fitting, as shown in Fig. 8. We also conducted tests using TurtleBot2 in a university building corridor envi-ronment, (see Fig. 9). These experiments validated LASMP's practicality to handle real-world planning problems."}, {"title": "Discussions about LASMP", "content": "The convergence of LASMP to a feasible user-instructed path efficiently depends on $\\frac{\\zeta(D)}{\\zeta(D_{smp})}$ which is determined by the choice of h and w. We have considered h > w in all the examples in this work based on the fact that h determines the space along the heading direction of the robot. Larger h will allow the robot to find a state in $D_{i_s}$ faster. In the future we would like to determine the optimal values for h and w. It can be formally proven that the LASMP holds the probabilistic completeness property like the RRT planner, because of the space limitation, this proof is omitted. The parameter d, associated with the ray-casting step, is chosen heuristically based on the average lengths of the aisles of a given environment. In our method, ray-casting is done only in the informed direction of the upcoming turn, reducing computational load."}, {"title": "VI. CONCLUSIONS", "content": "LASMP is a hybrid planning method that combines a large language model with sampling-based planning to efficiently generate collision-free paths for mobile robots. By leveraging language-based cues, LASMP focuses sampling on rele-vant areas, significantly improving efficiency over traditional methods like RRT. Currently designed for environments with static obstacles, LASMP operates as a global planner. To handle dynamic obstacles, a future extension will incorporate a local planning module. Extensive simulations demonstrated that LASMP consistently outperforms RRT in terms of node generation, query count, path length, and computation time. LASMP was also validated in real-world tests, proving its practical applicability for smooth and efficient path execu-tion. Future work will focus on dynamic obstacle avoidance and scaling the system for larger environments."}]}