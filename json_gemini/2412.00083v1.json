{"title": "VISUAL ERROR PATTERNS IN MULTI-MODAL AI: A STATISTICAL APPROACH", "authors": ["Ching-Yi Wang"], "abstract": "Artificial Intelligence (AI) has achieved transformative success across a wide range of domains, revolutionizing fields such as healthcare, education, and human-computer interaction [1]. However, the mechanisms driving AI's performance often remain opaque, particularly in the context of large language models (LLMs), which have advanced at an unprecedented pace in recent years. Multi-modal large language models (MLLMs) like GPT-40 exemplify this evolution, integrating text, audio, and visual inputs to enable interaction across diverse domains. Despite their remarkable capabilities, these models remain largely \"black boxes,\" offering limited insight into how they process multi-modal information internally. This lack of transparency poses significant risks, including systematic biases, flawed associations, and unintended behaviors, which demand urgent attention [7]. Understanding the decision-making processes of MLLMs is not only beneficial but essential for mitigating these risks and ensuring their safe deployment in critical applications.\nGPT-40 was chosen as the focus of this study for its advanced multi-modal capabilities, which allow simultaneous processing of textual and visual information [4]. These capabilities make it an ideal model for investigating the parallels and distinctions between machine-driven and human-driven visual perception. While GPT-40 excels in tasks involving structured and complete data, its reliance on bottom-up processing\u2014a feature-by-feature analysis of sensory inputs\u2014renders it less effective at interpreting complex or ambiguous stimuli. This limitation contrasts sharply with human vision, which is remarkably adept at resolving ambiguity and reconstructing incomplete information through high-level cognitive processes [11, 13].", "sections": [{"title": "Introduction", "content": "Artificial Intelligence (AI) has achieved transformative success across a wide range of domains, revolutionizing\nfields such as healthcare, education, and human-computer interaction [1]. However, the mechanisms driving Al's\nperformance often remain opaque, particularly in the context of large language models (LLMs), which have advanced\nat an unprecedented pace in recent years. Multi-modal large language models (MLLMs) like GPT-4o exemplify\nthis evolution, integrating text, audio, and visual inputs to enable interaction across diverse domains. Despite their\nremarkable capabilities, these models remain largely \"black boxes,\" offering limited insight into how they process\nmulti-modal information internally. This lack of transparency poses significant risks, including systematic biases,\nflawed associations, and unintended behaviors, which demand urgent attention [7]. Understanding the decision-making\nprocesses of MLLMs is not only beneficial but essential for mitigating these risks and ensuring their safe deployment in\ncritical applications.\nGPT-40 was chosen as the focus of this study for its advanced multi-modal capabilities, which allow simultaneous\nprocessing of textual and visual information [4]. These capabilities make it an ideal model for investigating the\nparallels and distinctions between machine-driven and human-driven visual perception. While GPT-40 excels in tasks\ninvolving structured and complete data, its reliance on bottom-up processing\u2014a feature-by-feature analysis of sensory\ninputs-renders it less effective at interpreting complex or ambiguous stimuli. This limitation contrasts sharply with\nhuman vision, which is remarkably adept at resolving ambiguity and reconstructing incomplete information through\nhigh-level cognitive processes [11, 13].\nHuman vision uses top-down processing, a mechanism that integrates prior knowledge, experiences, and expectations\nto interpret sensory inputs, enabling rapid and accurate perception even in the face of uncertainty. Gestalt principles,\nsuch as the Law of Closure and Amodal Completion, exemplify this ability, allowing humans to infer missing elements\nof objects. In contrast, bottom-up processing, the primary mechanism used by GPT-40, relies solely on raw sensory\ndata, lacking the contextual inference and mental frameworks that characterize human cognition [8]. This fundamental\ndistinction highlights the challenges faced by MLLMs in handling visual classification tasks and underscores the\nimportance of investigating cognitive-inspired approaches to improve these models.\n1 Bottom-Up Limitation Hypothesis: Errors in MLLMs arise primarily from their dependence on raw sensory\ndata, which limits their ability to integrate contextual inference or prior knowledge.\n2 Feature-Specific Error Hypothesis: Errors are driven more by the inherent complexity of specific visual\nfeatures, such as 3D structures, rotations, or missing faces, rather than the absence of contextual reasoning [2].\nThis study aims to explore whether MLLMs like GPT-4o encounter specific challenges in visual classification due to\ntheir reliance on bottom-up processing. Two hypotheses guide our investigation:\nThe Bottom-Up Limitation Hypothesis: Errors in MLLMs arise primarily from their dependence on raw sensory data,\nwhich limits their ability to integrate contextual inference or prior knowledge. The Feature-Specific Error Hypothesis:\nErrors are driven more by the inherent complexity of specific visual features, such as 3D structures, rotations, or missing\nfaces, rather than the absence of contextual reasoning [5, 6].\nTo evaluate these hypotheses, we designed a dataset of 54 3D and 21 2D geometric stimuli, featuring shapes like cubes,\nprisms, pentagons, and hexagons. These stimuli were characterized by attributes such as missing faces, rotations, and\nstacked elements, designed to test the model's ability to interpret visual complexity. We assessed GPT-40's performance\nusing multi-regression analysis to quantify how individual visual features contributed to error rates. Our analysis\nfocused on the model's capacity to distinguish between geometrically similar shapes, reconstruct incomplete structures,\nand resolve ambiguities in orientation and arrangement. The findings offer critical insights into the limitations of\nMLLMs in visual classification and potential improvement inspired by human cognitive processes [3]."}, {"title": "METHODS", "content": "The dataset used in this study comprised 75 carefully curated visual stimuli designed to evaluate the performance of\nmulti-modal large language models (MLLMs) in visual classification tasks. Each stimulus was characterized by a\ncombination of geometric features, including '3D,' 'Circle,' 'Pentagon,' 'Hexagon,' 'Cube,' and 'Triangle,' alongside\nspatial attributes such as arrangement patterns, repetition, combination, rotation, and missing faces. These features were\nselected to represent diverse structural and spatial challenges, emphasizing complex arrangements and incomplete\nstructures to simulate real-world scenarios where stimuli are often ambiguous or incomplete. For example, shapes were\npresented with varying orientations, stacked or overlapped in systematic patterns, or featured missing surfaces to test\nthe model's ability to interpret depth cues, recognize patterns, and reconstruct partial information. This dataset was\nengineered to address two objectives: (1) to introduce visual complexity by incorporating ambiguous and challenging\nstimuli, and (2) to ensure feature diversity by including a wide spectrum of geometric and spatial attributes. By focusing\non these aspects, the dataset provides a unique testing ground for probing the strengths and limitations of MLLMs, such\nas GPT-40, offering insights into their capacity to process complex visual inputs and handle edge cases that are often\noverlooked in traditional datasets."}, {"title": "Feature Classification:", "content": "GPT-40's outputs were evaluated to classify errors based on visual features. Errors were categorized by:"}, {"title": "Prompt Design:", "content": "The following prompt was used to evaluate GPT-40's ability to analyze visual stimuli:\n\"Please analyze the uploaded image and provide a detailed, text-based description of the features you can extract.\nSpecifically, identify key attributes such as shapes, orientation, depth cues, symmetry, arrangement, rotation, and any\nother noticeable elements in the image. If the image contains two different shapes, describe it as a 'combination.' If the\nsame shape is repeated multiple times, describe it as 'repetition.' For any other configurations, provide a description of\nthe observed structure.\""}, {"title": "Statistical Models", "content": "After classifying the errors based on the all input images, we aimed to investigate the factors contributing to visual\nclassification errors and perform feature importance analysis using supervised learning techniques. To achieve these\nobjectives, we applied four statistical and machine learning models based on their unique strengths in capturing both\nlinear and non-linear relationships [12].\n1 Logistic Regression: Baseline model to predict classification errors and interpret linear relationships between\nfeatures and outcomes.\n2 Ridge Logistic Regression: Using L2regularization to reduce overfitting, stabilizing predictions in the presence\nof multicollinearity.\n3 Random Forest: Implemented with hyperparameter tuning (e.g., number of trees, maximum depth, minimum\nsamples split) to optimize performance and prevent overfitting while leveraging its robust handling of non-linear\nrelationships and feature interactions.\n4 Gradient Boosting (XGBoost): Includes hyperparameter tuning (e.g., learning rate, maximum depth, number\nof estimators, subsample ratio) to achieve optimal performance and ensure efficient model convergence, while\nidentifying influential features. The hyperparameters were explored across the following ranges: learning\nrate (0.01-0.3) to control weight updates and prevent overfitting, maximum depth (3\u201310) to balance model\ncomplexity and generalizability, number of estimators (50\u2013200) to control the number of boosting iterations,\nand subsample ratio (0.5-1.0) to reduce overfitting by using random subsets of training data."}, {"title": "Evaluation Metrics", "content": "To evaluate the performance and robustness of the models, the following metrics were employed:\n\u2022 Area Under the Curve (AUC): AUC measures the model's ability to discriminate between different classes, pro-viding a comprehensive evaluation of its predictive performance. A higher AUC indicates better performance\nin correctly distinguishing between error and non-error cases.\n\u2022 Receiver Operating Characteristic (ROC) Curve: The ROC curve illustrates the trade-off between true\npositive rate (TPR) and false positive rate (FPR) across various classification thresholds. It provides a\nvisual representation of model performance, with the curve's proximity to the top-left corner indicating high\ndiscriminative ability.\n\u2022 Cross-Validation (5-fold): A 5-fold cross-validation strategy was used to assess the robustness and generaliz-ability of the models. The dataset was partitioned into five subsets, with the model trained on four subsets and\nvalidated on the fifth, repeated for all combinations. This ensured consistent evaluation across different splits\nand minimized overfitting."}, {"title": "RESULTS:", "content": "We evaluated GPT-40's performance on a dataset comprising 75 geometric stimuli, including 54 3D and 21 2D shapes.\nThese stimuli were systematically designed to include attributes such as missing faces, rotations, and stacked elements,\nchallenging the model's ability to process structural complexities and ambiguities. Each stimulus was binary-coded\nto indicate whether the model classified it correctly, enabling quantitative analysis of error rates. A multi-regression\nanalysis was conducted to examine the contributions of specific visual features\u2014such as symmetry, orientation, and\ndepth-to classification errors, integrating both qualitative insights and quantitative metrics. This approach provided a\ncomprehensive evaluation of GPT-40's strengths and limitations in visual feature classification."}, {"title": "Error Evaluation", "content": "For 3D stimuli, GPT-40 demonstrated substantial difficulty in distinguishing geometrically similar shapes. For\nexample, pentagonal and hexagonal prisms were frequently misclassified, resulting in a 32.7% error rate for shape type.\nFurthermore, the model struggled significantly with incomplete structures, exhibiting a 63% error rate when identifying\nshapes with missing faces. These findings underscore the model's limitations in processing partial or ambiguous inputs.\nIn contrast, GPT-40 performed exceptionally well in tasks requiring depth perception, achieving a low overall error rate\nof 3.7% for 3D structure identification. This suggests that the model excels in recognizing spatial relationships and\ndepth cues, particularly in well-defined and complete structures.\nFor 2D stimuli, the model's most frequent classification error involved orientation, with a 14.3% error rate. This\nlimitation likely stems from the absence of depth cues, which are critical for resolving ambiguities in object rotation\nand alignment. While GPT-4o achieved high accuracy for simpler features, such as symmetry and shape type, its\nperformance diminished as the visual complexity of stimuli increased. Overlapping shapes and rotated configurations,\nin particular, exacerbated classification errors.\nAn in-depth analysis of error rates revealed that GPT-40 encountered the most significant challenges with 3D stimuli,\nparticularly with Missing Faces and Shape Type. The model exhibited a high error rate of 63.0% for shapes with\nmissing faces, reflecting its difficulty in reconstructing incomplete structures. Similarly, a 32.7% error rate for Shape\nType indicates substantial confusion in distinguishing geometrically similar shapes, such as pentagonal and hexagonal\nprisms. For 2D stimuli, the model's performance was most affected by Orientation, with an error rate of 14.3%,\nsuggesting challenges in interpreting rotated or misaligned shapes in the absence of depth cues. Additionally, a 25%\nerror rate for Missing Faces in 2D stimuli further underscores GPT-40's limitations in processing incomplete and\nambiguous visual features."}, {"title": "MODEL RESULTS", "content": "We evaluated the predictive performance of four statistical models\u2014Logistic Regression, Ridge Logistic Regression,\nRandom Forest, and Gradient Boosting (XGBoost)\u2014to predict the occurrence of missing face errors. This task chal-lenges the ability to model top-down inference. Logistic Regression served as a baseline, modeling linear relationships\nbetween input features and the occurrence of errors, while Ridge Logistic Regression incorporated L2-regularization to\ncontrol multicollinearity and stabilize predictions. Random Forest, an ensemble method utilizing bagging, captured\nnon-linear relationships and feature interactions through randomized decision trees, offering robustness to noise and\noverfitting. XGBoost, a gradient boosting algorithm, iteratively optimized a differentiable loss function to minimize\nprediction errors, applying its ability to capture complex, non-linear feature interactions.\nEach model was evaluated using a 5-fold cross-validation strategy, splitting the dataset into training and validation\nsubsets to assess generalizability. The Area Under the Curve (AUC) was used as the primary evaluation metric to\nmeasure the model's discriminative ability in predicting whether missing face errors occurred. Missing face errors are\nparticularly challenging because they require simulating top-down inference akin to human cognition, where prior\nknowledge and contextual cues are integrated to infer missing structures. Unlike simple classification tasks, accurately\npredicting missing faces relies on a model's ability to combine geometric and contextual information, extending beyond\ntraditional bottom-up processing. This study aimed to assess the ability of these statistical models to meet this challenge\nand uncover key insights into their strengths and limitations in handling such inferential tasks."}, {"title": "Feature Importance Analysis", "content": "The feature importance analysis reveals the relative contribution of each input feature to the model's decision-making\nprocess, offering insights into the predictors that most significantly influence performance. Using XGBoost, we\nquantified the impact of features in predicting classification outcomes for the 'Missing Face' error. This analysis\nhighlights the model's reliance on structural and geometric features, such as '3D' (33.24%) and 'Pentagon' (33.61%),\nwhich are critical in addressing the challenges of handling missing faces and complex arrangements.\nNotably, features like '3D' highlights the model's difficulty with depth-related tasks, such as distinguishing between\nsimilar 3D shapes or interpreting incomplete structures. Missing faces in 3D objects disrupt spatial relationships, often\nleading to incorrect classifications. Humans, in contrast, resolve such ambiguities using top-down cognitive processes\nlike Amodal Completion, which enables mental reconstruction of incomplete forms."}, {"title": "Discussion", "content": "The purpose of the present study was to examine the performance of multi-modal large language models (MLLMs),\nspecifically GPT-40, in visual classification tasks involving geometric stimuli. Our primary goal was to understand\nthe factors contributing to classification errors, particularly for features like \u20183D' structures and \u2018missing faces,' and to\nexplore whether the dependence on bottom-up processing limits the model's ability to handle complex visual stimuli.\nWe hypothesized that (a) the model would struggle with tasks requiring top-down inference, such as reconstructing\nmissing elements or resolving ambiguities, or (b) the model would perform similarly to human perception when\nhandling simpler geometric features by using its advanced processing capabilities.\nOur results showed that statistical models, particularly XGBoost, effectively predicted classification errors, achieving the\nhighest average AUC (0.85, SD = 0.02) compared to both linear models (Logistic Regression, Ridge Logistic Regression)\nand another non-linear model (Random Forest). The analysis of the feature importance identified \u20183D,' 'Circle,' and\n'Pentagon' as the most influential predictors of top-down errors, highlighting the challenges caused by geometric and\nstructural complexity. These findings support our hypothesis that MLLMs such as GPT-4o rely heavily on bottom-up\nprocessing, which limits their ability to infer missing elements or resolve ambiguous visual stimuli. For instance, errors\ninvolving '3D' features often came from the model's difficulty in interpreting depth cues or incomplete structures,\nsuch as missing faces, where humans tend to perform better due to top-down processing mechanisms like Amodal\ncompletion [8]. Our findings highlight the limitations of GPT-40's feature-by-feature processing approach, particularly\nin tasks requiring reconstruction of incomplete structures, which aligns with broader challenges in multi-modal LLM\ndevelopment [10].\nFrom the analysis, it is evident that GPT-40 struggles with visual ambiguity and incomplete information, failing to infer\nmissing faces or reconstruct objects with disrupted symmetry. This limitation is particularly concerning for features like\n'3D' and 'rotation,' where the absence of contextual reasoning leads to misclassifications. While the model excels with\nsimpler features like circles or cubes, its errors with complex stimuli emphasize a significant gap compared to human\ntop-down processing.\nOur observations reveal that GPT-40 is dependent on raw data which limits its robustness in complex scenarios. Errors\nwith features like 'rotation' and 'repetition' often occurred in unusual orientations or overlapping patterns. This\nemphasizes the need for human-like strategies, such as top-down processing into MLLMs to enhance their ability to\nprocess incomplete or ambiguous visual data.\nOur findings are limited by the scope of the dataset, which consisted of 75 geometric stimuli. While the stimuli were\ndesigned to capture a diverse range of structural and spatial attributes, a larger and more varied dataset could provide a\ndeeper understanding of the model's performance across different types of visual complexity. In addition, the study\nfocused primarily on geometric features, such as shape type and depth cues, without exploring other potentially relevant"}, {"title": "Conclusion", "content": "This study used statistical modeling to evaluate the performance of GPT-40 in visual classification tasks involving\ngeometric stimuli, focusing on misclassification predictors like '3D' structures and 'missing faces.' XGBoost achieved\nthe highest predictive accuracy with an AUC of 0.85 (SD = 0.02) which outperforming both linear models (Logistic\nRegression, Ridge Logistic Regression) and Random Forest. Feature importance analysis identified '3D,' 'Circle,' and\n'Pentagon' as key contributors to errors, demonstrating a significant relationship between geometric complexity and\nclassification performance. These results highlight GPT-40's reliance on bottom-up processing, which limits its ability\nto handle ambiguous or incomplete visual data, such as depth cues and disrupted symmetry.\nOur findings show that statistical techniques, such as feature importance metrics are essential for understanding the\nlimitations of MLLMs. While GPT-40 handles simpler features effectively, its difficulty with complex stimuli highlights\nthe need for incorporating human-like top-down inference mechanisms to improve robustness. Expanding this work\nwith larger datasets and features like texture or shading can enhance our understanding of multi-modal AI systems.\nStatistical models can assist by identifying key limitations and guiding improvements for more robust performance."}]}