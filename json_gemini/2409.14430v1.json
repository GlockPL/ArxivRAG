{"title": "Pomo3D: 3D-Aware Portrait Accessorizing and More", "authors": ["Tzu-Chieh Liu", "Chih-Ting Liu", "Shao-Yi Chien"], "abstract": "We propose Pomo3D, a 3D portrait manipulation framework that allows free accessorizing by decomposing and recomposing portraits and accessories. It enables the avatars to attain out-of-distribution (OOD) appearances of simultaneously wearing multiple accessories. Existing methods still struggle to offer such explicit and fine-grained editing; they either fail to generate additional objects on given portraits or cause alterations to portraits (e.g., identity shift) when generating accessories. This restriction presents a noteworthy obstacle as people typically seek to create charming appearances with diverse and fashionable accessories in the virtual universe. Our approach provides an effective solution to this less-addressed issue. We further introduce a \"Scribble2Accessories\" module, enabling Pomo3D to create 3D accessories from user-drawn accessory scribble maps. Moreover, we design a bias-conscious mapper to mitigate biased associations present in real-world datasets. In addition to object-level manipulation above, Pomo3D also offers extensive editing options on portraits, including global or local editing of geometry and texture and avatar stylization, elevating 3D editing of neural portraits to a more comprehensive level. Project page: https://tzuj6.github.io/Pomo3D.", "sections": [{"title": "1 Introduction", "content": "Recently, with the swift development of virtual reality (VR) and augmented reality (AR), portrait generation has found various promising applications, such as avatar-based telepresence or teleconference, providing users with immersive experiences. Thus, producing 3D portrait images that are highly realistic and editable has been a surge of interest in recent years.\n2D GANs, such as the prevalent StyleGAN-based approaches [1, 13, 19, 57, 66,82], can achieve impressive portrait generation and manipulation. However, they inherently disregard fundamental principles of projective geometry, thus leading to incoherent editing when the viewpoint shifts. To address this issue, several prior work [7, 10, 26, 50, 51, 56] have employed 3D GANs to yield view- consistent results based on the 3D-structure-aware inductive bias introduced by the neural rendering pipeline. Additionally, given the promising results of diffusion models [61] on 2D images, some studies [25, 32, 38, 39, 70] integrate"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Neural Scene Representation", "content": "A scene representation incorporating a neural network to approximate surface or volumetric representation functions is commonly referred to as neural scene representation within the field of neural rendering [44, 68]. These scenes can be"}, {"title": "2.2 3D-Aware Generative Models", "content": "Equipped with the neural scene representation and a differentiable rendering algorithm, 3D-aware GANs can produce multi-view consistent images [10,56,67]. Several approaches [9, 26,51,75,80] adopt a two-stage rendering process, which leverages convolution neural networks (CNNs) to increase the resolution of the image or neural rendering features, to generate 3D-aware images at higher resolu- tion efficiently. In addition, GIRAFFE [50], gCoRF [5], and CNeRF [45] employ a compositional neural radiance field that supports object-level controls. However, GIRAFFE falls short in explicitly separating objects of interest, while gCoRF and CNeRF neglect handling uncommon objects (e.g., earrings, headwear, etc.)."}, {"title": "2.3 Controllable Portrait Synthesis", "content": "With the advent of generative models, researchers are also intrigued by the capac- ity for portrait manipulation. However, in style-based generators, the meanings of latent codes still need to be clarified, making them difficult to manipulate. SemanticStyleGAN [59] utilizes compositional synthesis to encourage spatial dis- entanglement, attaining photo-realistic images and finer-grained control. Never- theless, akin to other 2D mask-based editing methods [40, 81, 82], it lacks 3D knowledge, yielding incoherent editing when the viewpoint shifts. SofGAN [12] produces perspective-consistent semantic maps with a semantic occupancy field, achieving independent control over portrait shape and texture. Nonetheless, it only ensures semantic-level 3D consistency and requires additional labeled multi-view data. By learning the joint distribution of images and semantics, FeNeRF [64] utilizes the semantic branch as an intermediary to edit color im- ages via GAN inversion and achieves multi-view consistent and regional editing. However, owing to the extensive computational demand, the image quality is sub-optimal. IDE-3D [62] utilizes a pair of tri-planes to disentangle shape and appearance attributes, resulting in portrait editing capabilities that are both flexible and 3D consistent. Nonetheless, it restricts editing only on a fixed view and may produce artifacts at the semantic boundary when locally editing the texture. pix2pix3D [18] maps semantic graphs onto the generator's latent space, facilitating controllable graph-to-image generation. However, its geometry and texture spaces are highly entangled, which means that altering the semantic maps easily leads to variations in texture."}, {"title": "3 Methodology", "content": "Overview As shown in Fig. 2, we first utilize a StyleGAN2 generator Ggeo to construct the geometry tri-plane of portraits ftriplane with the portrait geometry code wpor,g, and a feature adapter maps the features from the portrait space to the accessory space with the accessory geometry code Wacc,g, thereby establish- ing another geometry tri-plane for the accessory ftriplane. Two projected feature maps (fpor, facc) and corresponding semantic maps (Spor, Sacc) can be obtained via volume rendering and a per-pixel classifier FtoSEG. Next, the structure en- coder Estr fuses two geometry feature maps (fpor, facc), forming a structural"}, {"title": "3.1 Bias-Conscious Mapper", "content": "Following StyleGAN2 [36], to handle the non-linearity of data distribution, a noise vector z from the spherical Gaussian space Z is first to be mapped into an intermediate style space W with Multi-Layer Perceptron (MLP) layers and be extended into a W+ space. However, unlike most methods, we attempt to decompose the W+ space into four subspaces: portrait geometry, accessory ge- ometry, portrait texture, and accessory texture. The factorized W+ space can be formalized as:\n\\(W^+ = W_{geometry} \\times W_{texture} = W_{por,g} \\times W_{acc,g} \\times W_{por,t} \\times W_{acc,t}\\) (1)\nwhere the subscripts por, acc, g and t, refer to portrait, accessory, geometry and texture, respectively. As depicted in Fig. 2(c), due to the decomposition of our latent space into four distinct subspaces, each managed by a unique code, we greatly increase flexibility in manipulating the avatar.\nNevertheless, most real-world datasets contain biases. Unsophisticated han- dling of these biases leads to unfavorable results and entangled attributes. In this work, we mainly address two significant biases. Firstly, for the task that learns 3D consistent views from a set of arbitrary single-view images, the model is prone to capture spurious correlations between poses and appearances (e.g., expression, gaze direction) due to ambiguities between viewpoints. Secondly, for the task of producing a diverse combination of accessories and wearers, due to the model's limited observation of real-world data, it tends to incorrectly asso- ciate accessories with portraits of specific attributes. For instance, the propor- tion of males wearing earrings is much lower than that of females, demonstrating an accessory-gender association. Faithfully modeling these attribute correlations during training and decoupling these intertwined attributes during inference is necessary to achieve both multi-view consistent and diverse accessories wearing."}, {"title": "3.2 Generation of Dual Geometry Tri-planes", "content": "As depicted in Fig. 2(a), we first learn a StyleGAN-based generator Ggeo to construct a portrait geometry tri-plane ftriplane \u2208 [R3\u00d732\u00d7256\u00d7256 with a por- trait geometry code wpor,g, following the setting in EG3D [9]. In general, given the structure of a face, people can easily imagine where accessories can be placed (e.g., earlobes, top of the head). Hence, our key insight is that, for an already established portrait geometry representation, a lightweight model can be introduced to infer the accessory geometry representation based on the knowledge of the portrait's structure. We implement this model with a com- pact feature adapter \u03c6, which consists of three branches (Ya,b,c), where Li: Spor fplane \u2208 R32\u00d7256\u00d7256, mapping the features of each 32-channel plane from portrait to the accessory space, conditioned on the acces- sory geometry code wacc,g. As such, dual geometry tri-planes (ftriplane, ftriplane) can be constructed. The features f and density o of a 3D point can be queried by projecting the point onto three orthogonal planes, aggregating the features from three planes, and processing by a decoder. Then we can project the 3D feature volume onto 2D feature images fpor, facc \u2208 R64\u00d7128\u00d7128 via volume rendering:\n\\(f_{por} = \\S (G_{geo} (w_{por,g}), \\pi)  f_{acc} = \\S (\\varphi (G_{geo} (w_{por,g}), w_{acc,g}), \\pi)\\) (2)\nHere, \\S denotes the neural volume rendering process, given a camera pose \u03c0.\nWe further extract the semantic maps Spor \u2208 R20\u00d7128\u00d7128 and Sacc \u2208 R5\u00d7128\u00d7128, by predicting the semantics of each pixel on the feature images via a MLP clas- sifier FtosEG, which describes the probabilistic distribution over N semantic classes (such as background, skin, cloth, etc.). As the structure of the feature images is so well-defined, the classifier FtosEG can be designed to be extremely lightweight. As shown in Fig. 4, the accessories inferred from the portrait ge- ometry will implicitly and correctly align with the portraits without imposing additional constraints."}, {"title": "3.3 Structure-Guided Texture Renderer", "content": "This section consists of two parts: the structure encoder Estr and the texture ren- derer Rtex. The structure encoder is proposed to capture the essential structural"}, {"title": "3.4 Data Preparation and Training Scheme", "content": "Data Preparation We curate PAC-Mask from the widely accessible datasets, CelebAMask-HQ [40], FFHQ [35] and FaceSynthetics [71]. The main preprocess- ing involves emphasizing the accessory region of segmaps (segmentation maps), splitting the nose semantics into two parts for clearer geometry, and conducting pose detection. Next, we categorize the segmaps into two groups based on the presence of accessories and extract the accessory semantics. Consequently, the PAC-Mask dataset comprises three non-overlapping groups: accessory segmaps, unadorned portrait segmaps, and RGB images. Further details and statistics are included in the supplementary materials.\nTraining Scheme As illustrated in Fig. 2(d), we novelly employ a three- pronged strategy to train Pomo3D, specifically by using three discriminators to supervise three corresponding branches separately. The three real data groups from PAC-Mask and the generated results by three branches, Sacc, Spor and IRGB, are used for adversarial learning independently. The architecture of the three discriminators is identical to those of EG3D [9], with the only difference being the input channel (for example, 5 for the discriminator Do that discrim- inates accessory segmaps). Note that two branches for segmaps Sacc and Spor inject two shorter gradient backward paths, which makes the training more sta- ble. This strategy explicitly aligns the distribution of each branch's output with that of the corresponding data group."}, {"title": "3.5 Scribble2Accessories Inversion", "content": "What sets us apart from most existing approaches is our capability to conduct GAN inversion on specified objects while keeping the remaining part unchanged, thanks to the design of the dissociated branch tailored for accessories. We intro- duce a Scribble2Accessories module to synthesize photo-realistic 3D accessory assets from inaccurate hand-drawn scribbles. In practice, we first design a 2D encoder Eacc that maps scribbles to the accessory geometry code Wacc,g, provid- ing the encoder with projected portrait features fpor as the pose and structural information of the wearer, namely, Wacc,g = Eacc (Sacc, fpor). During training, Sace is an actual accessory segmap (either sampled from the training data group"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Evaluation Metrics", "content": "To evaluate the image quality of synthesized images, we use FID (Fr\u00e9chet In- ception Distance) [27] and KID (Kernel Inception Distance) [4] to measure the distribution distance between generated images and real images. To evaluate the alignment between output RGB images and output segmaps, we first predict the semantics of RGB images with an off-the-shelf network [76] and then calculate mIoU (mean Intersection-over-Union) and Acc (pixel accuracy) between output segmaps and the predicted semantics following [53]. For view consistency eval- uation, we assess the preservation of facial identity across different viewpoints with FV-ID (free-view facial identity consistency), which calculates the mean cosine similarity of Arcface [17] features."}, {"title": "4.2 Comparison", "content": "Quantitative evaluations We compare Pomo3D with other SOTA mask- based 3D portrait editing models using their official codes. As shown in Fig. 6, in addition to providing more flexible training data setups, our proposed method outperforms FeNeRF [64], IDE-3D [62], and pix2pix3D [18] in editing flexibil- ity, image quality, and alignment while being competitive with IDE-3D and pix2pix3D in view consistency.\nQualitative evaluations In Fig. 7, we compare the editing capability of acces- sories and the impact region of editing with SOTAs. It is observed that IDE-3D still struggles with tasks beyond eyeglass editing; for instance, in task(a), their result more closely resembles braids rather than earrings, and in task(c), it mis- takenly generates hair instead of headwear. pix2pix3D can edit earrings; however, the result is less photo-realistic and not aligned with the desired segmap. FeN- eRF struggles to provide such fine-grained editing. In terms of the impact region, it is clear that all models, except for Pomo3D, induce global alterations, notably"}, {"title": "4.3 Ablation Study", "content": "Pose Conditioning We first evaluate the effect of pose conditioning in the bias-conscious mapper. As illustrated in Fig. 8, removing pose condition- ing weakens the generator's sensitivity to camera poses, leading to changes in the geometry of the mouth as the viewpoint shifts. Instead, with pose conditioning, models can better decouple expres- sions and poses, enhancing geometry consistency.\nIdentity Conditioning In Fig. 9, We further study the effect of identity conditioning in the bias-conscious mapper. During training, we in- troduce FMD (Fr\u00e9chet Mask Inception Distance) to measure the distribution distance between real and generated accessory segmaps and FID for RGB images. During inference, SIG (Single Iden- tity Generation) diversity is utilized to evaluate the diversity of accessory segmaps produced from a single identity by calculat- ing the average LPIPS [77] score. A larger hyperparameter p indicates a greater"}, {"title": "5 Conclusion", "content": "This paper provides an effective solution to a less-addressed issue with Pomo3D, which offers the most extensive control over decorative objects of 3D portraits. It utilizes a bias-conscious mapper to create diverse accessories-portrait combina- tions and introduces a method to transform user's scribbles into 3D accessories. The training dataset used in this method is publicly available for future studies. Pomo3D has various applications, such as 3D accessory virtual try-ons, opening new possibilities for further research. More experimental details, applications, limitations, and discussions can be found in the supplementary material."}, {"title": "6 Overview", "content": "We present additional results as a supplement to the main paper. Firstly, in Sec. 7, we present further details of the main paper's experiments and PAC-Mask. Sec- ondly, additional experiments and comparisons are provided in Sec. 8. Thirdly, we introduce applications in Sec. 9. Then we discuss the model's limitations and ethical considerations in Secs. 10 and 11. Lastly, more visual results are displayed in Sec. 12."}, {"title": "7 Implementation Details", "content": "Dual Geometry Tri-planes The portrait geometry tri-plane fpriplane is largely based on EG3D [9] and utilizes the same hyperparameters as stated in the orig- inal paper. Each branch of the feature adapter i consists of two blocks, each containing two modulated convolution layers, modulated by the accessory geom- etry code Wacc,g. The per-pixel classifier FtosEG is implemented by two multi- layer perceptrons (MLPs) with no output activation and about 6k trainable parameters.\nStructure Encoder Within the structure encoder Estr, portrait features fpor and accessory features face are first combined into a single feature map based on the binary mask macc. Namely, fcombined = macc \u2299 facc + (1 \u2013 macc) fpor\u00b7 Then, we use two residual blocks to learn to form the structural prior ffused from combined features fcombined. Besides, since not all samples are accessorized, to fit the training data distribution better, we adaptively set the probability of (Accs == True) based on the ratio of accessories in datasets during training (for instance, about 0.37 for CelebAMask). When (Accs == False), macc is an all-zeros mask; otherwise, macc is a binary mask indicating the accessory region.\nTexture Renderer In the texture renderer Rtex, features in each block are modulated by two styles and then re-combined based on binary masks macc and (1 - macc), then followed by a spatially-adaptive normalization. To further edit textures other than accessories, we switch among three schemes of compositional synthesis during training. When (Accs == True), macc is set as the binary mask of accessories. When (Accs == False), we set macc either as an all-zeros mask (same as the structure encoder) or as a binary mask for other decorative ob- jects, such as clothing, hair, etc. (different from the structure encoder). In other words, even without adding accessories, we randomly render two texture styles on different semantic regions of the portrait, thereby enhancing the disentangle- ment of these semantic regions. These decorative objects share the same texture style with accessories, i.e., Wacc,t. As such, the accessory texture code wacc,t is extended to encompass the texture style for other decorative objects, not just accessories."}, {"title": "Multiple Accessory Wearing", "content": "In cases where N accessories are worn simul- taneously, we forward the accessory branch (i.e., Gacc) N times with N differ- ent accessory geometry codes Wacc,g, obtaining N sets of (facc, Sacc, Macc) while keeping the same portrait geometry (i.e., the same wpor,g). Let mace be the union of these N accessory binary mask, macc = macc Umacc Umacc... Umace. Inside the structure encoder, the combined feature map fcombined is now composed of a single fpor and N pairs of (facc, Macc).\n\\(f_{combined} = (1 - m_{acc}^{U}) f_{por} + m_{acc}^{U} \\sum_{n=0}^{N} face\\) (5)\nHere, face denotes the projected accessory features generated by the nth accessory geometry code. In the texture renderer, the original macc is replaced with mace, with all else remaining unchanged.\nU U U U U N U N N N N N n n n n n"}, {"title": "Bias-Conscious Mapper", "content": "In order to make the accessory generation identity- aware, we introduce a cross-attention module. The original accessory code wacc,g serves as the query, while the identity embedding acts as both key and value. Let WA and W\u2081 denote the original code and the identity embedding, respectively. WQ, WK and Wy are projection matrices. The cross-attention process can be formalized as follows: softmax((WQWA)(WKW1)T)WvW1"}, {"title": "Dataset Details: PAC-Mask", "content": "We curate PAC-Mask from the widely accessible dataset, CelebAMask-HQ [40], FFHQ [35] and FaceSynthetics [71]. The main pre-processing involves three steps as in Fig. 12. First, we notice that many existing mask-based portrait synthesis methods directly train with single-channel semantic maps, obtained by stacking binary masks of each semantic region (e.g., hair, brow, lip) from the raw annotations (such as CelebAMask-HQ) along the channel axis and returning the indices of the maximum values of each pixel along the axis. This process makes each pixel represent one semantic type, compressing multiple regional semantic masks into a single semantic map, and facilitating easier data processing. Yet, there may be some potential issues. In areas of overlap, a single pixel may contain more than one semantic type. This leads to the overlap area being dominated by a certain semantic (based on the order of channel index) and the loss of information from other semantics. As illustrated in"}, {"title": "8 Additional Experiments and Visualization", "content": "Feature Adapter To better visualize the function of the feature adapter, as illustrated in Fig. 15, we modify the architecture from (a) to (b). We em- ploy one of the feature adapter's branches, keep the architecture unchanged, and re-train it. The feature adapter's three branches originally map each plane's features in the portrait geometry tri- plane ftriplane to the accessory geometry tri-plane ftriplane. Now, the single-branch feature adapter di- rectly maps the projected portrait features fpor to the accessory space on the image plane. As such, we can observe the influence of the feature adapter on the portrait features fpor.\nAs shown in Fig. 16, the feature adapter shifts the spatial focus based on different accessory geom-"}, {"title": "Identity Conditioning", "content": "We present the visualization results of identity condi- tioning in Fig. 17. Next to each identity, there are two rows of accessory samples produced by the top ten seeds during inference. The top row shows results with- out identity conditioning, while the bottom row indicates results with identity conditioning. It can be observed that without identity conditioning, the variety of accessories is usually limited and often related to the external attribute of the identity. For example, for a more masculine appearance (a), earrings are rarely produced in the top row; or for a more feminine appearance (b), there are no beard-related items in the top row. Conversely, with identity condition- ing, as shown in the bottom row, a wider variety of accessories can be obtained. Therefore, identity conditioning greatly enhances the possibility of creating new combinations of accessories and wearers even beyond the representation of the dataset."}, {"title": "Ablation Study of SPADE", "content": "We no- tice that even with the generated se- mantics of accessories, relatively small accessories may still be overlooked during texture generation due to the holistic nature of the global discrim- inator. Therefore, we further intro- duce spatially-adaptive normalization (SPADE) [53] into our texture render-"}, {"title": "9 Applications", "content": "3D Accessory Virtual Try-on Virtual try-on technology allows consumers to virtually try on clothes or accessories, finding out how these products appear on them without any physical interaction [16,29]. Through our dedicated accessory branch, we extend 3D portrait synthesis to broader applications such as 3D portrait accessorizing or 3D accessory virtual try-ons, opening new possibilities for further research. As shown in Fig. 20, Pomo3D is capable of producing diverse accessories on specified portraits. With Scribble2Accessories, users can first draft a rough design of the accessory, and then choose a preferred texture.\nInteractive Avatar Customization We provide a GUI interface for real-time portrait generation featuring comprehensive controllability, integrating explicit"}, {"title": "10 Limitations", "content": "While Pomo3D is capable of producing more diverse accessories, there are still some potential issues that need to be addressed. For instance, the quality of certain accessories, particularly necklaces with their significant variation and limited training data, is not entirely satisfactory. Moreover, Pomo3D also strug- gles to generate complex patterns on headwear. With larger and more varied training datasets of accessories, we believe these problems can be greatly alle- viated. Additionally, this generation pipeline makes the inversion of the entire image more difficult. In practice, we first invert the segmap and fix its geometry code, then search for the optimal texture code. A more effective solution may be to train a separate encoder that maps entire images back to the generator's latent space. We leave it for future work."}, {"title": "11 Ethical Concerns", "content": "Pomo3D possesses the capability to generate and manipulate 3D portraits. Therefore, there are risks of misuse, such as identity fraud or causing misrecog- nition in facial recognition systems. This capability should be used carefully and must be regulated. There are several ongoing research on deepfakes detec- tion [46,49], which aim to distinguish between synthesized faces and real faces. The training data we used and our generated results can also aid in training for deepfakes detection."}, {"title": "12 Additional Results", "content": "In this section, we provide additional visual results as a supplement to the main paper. We demonstrate additional sequential edits on stylized portraits (Fig. 24), accessory wearing in a nine-grid layout from different viewpoints (Figs. 25, 26 and 29), accessory wearing on stylized portraits (Fig. 27), diverse shapes and textures of accessories (Fig. 28), view-consistent texture editing (Fig. 30), and a demonstration of accessories implicitly aligned with portraits (Fig. 31)."}]}