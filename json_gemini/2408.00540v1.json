{"title": "The Energy Cost of Artificial Intelligence of Things Lifecycle", "authors": ["Shih-Kai Chou", "Jernej Hribar", "Mihael Mohor\u010di\u010d", "Carolina Fortuna"], "abstract": "Artificial Intelligence (AI) coupled with existing In-\nternet of Things (IoT) enables more streamlined and autonomous\noperations across various economic sectors. Consequently, the\nparadigm of Artificial Intelligence of Things (AIoT) having AI\ntechniques at its core implies additional energy and carbon costs\nthat may become significant with more complex neural architec-\ntures. To better understand the energy and Carbon Footprint\n(CF) of some AIoT components, very recent studies employ\nconventional metrics. However, these metrics are not designed to\ncapture energy efficiency aspects of inference. In this paper, we\npropose a new metric, the Energy Cost of AIoT Lifecycle (eCAL)\nto capture the overall energy cost of inference over the lifecycle of\nan AIoT system. We devise a new methodology for determining\neCAL of an AloT system by analyzing the complexity of data\nmanipulation in individual components involved in the AIoT\nlifecycle and derive the overall and per bit energy consumption.\nWith eCAL we show that the better a model is and the more it\nis used, the more energy efficient an inference is. For an example\nAIoT configuration, eCAL for making 100 inferences is 1.43 times\nhigher than for 1000 inferences. We also evaluate the CF of the\nAIoT system by calculating the equivalent CO2 emissions based\non the energy consumption and the Carbon Intensity (CI) across\ndifferent countries. Using 2023 renewable data, our analysis\nreveals that deploying an AIoT system in Germany results in\nemitting 4.62 times higher CO2 than in Finland, due to latter\nusing more low-CI energy sources.", "sections": [{"title": "I. INTRODUCTION", "content": "Next-generation networks aim to deliver unprecedented\nlevels of connectivity and intelligence, facilitating seamless\ninteractions between a massive number of devices, services,\nand applications. The trend is especially noticeable in Internet\nof Things (IoT) where devices are becoming increasingly\nmore intelligent [1], from smart home appliances [2] to\nindustrial machinery [3], Smart City [4], vehicle-to-everything\n(V2X) [5], and beyond [6], [7]. With the help of Artificial\nIntelligence (AI), we are moving away from conventional IoT\napplications towards Artificial Intelligence of Things (AIoT)\nsystems [8] designed to achieve more streamlined and au-\ntonomous operations. For example, in future smart factories,\nthe integration of AI and IoT will enable real-time monitoring\nand optimization of production processes, predictive mainte-\nnance, and enhanced decision-making capabilities.\nWhile AIoT systems enable unprecedented automation and\nagility, their reliance on Al implies additional energy and\ncarbon costs [9]. Especially in the cases when models with\nmany parameters are employed, such as Large language model\n(LLM)s, such costs may be significant [10]. In light of climate\nchallenges, the general AI research community's effort to\nbetter estimate the costs of training [11], estimate the cost\nof AI [12], the Carbon Footprint (CF)s of LLMs [13] and\nfind ways to scale models efficiently [14] are intensifying.\nFurthermore, the networking and IoT communities increas-\ningly relying on AI techniques are also investigating pathways\ntowards [15] net-zero carbon emissions, analyzing the CF of\nvarious learning techniques [16] and data modalities while also\ndeveloping various approaches to optimizing energy efficiency\nthrough hardware-software optimization [17], scheduling [18],\nmore efficient model design [19] and energy and carbon\nconsumption testing [20].\nDepending on the scope of the study, various metrics are\nemployed to understand energy and CF. For instance, [17]\nconsiders normalized energy as well as energy for buffering in\n[J], energy [Wh] and [CO2eq] for the various phases involved\nin federated learning (FL) edge systems [15], [16], [J] and\nGFLOPS of the neural architectures [19] and energy [Wh]\nfor the scheduled loads in [18]. However, these metrics are\nnot specifically designed to capture the energy efficiency of a\nsystem or part of. As well noted in [15], the traditional ITU\nstandardized Energy-per-Bit [J/b] metric 'will no longer be\nable to reflect the environmental impact of the modern mobile\nservices, especially network Al-enabled smart services'.\nRecently, a number of environmentally friendly metrics,\nsuch as accuracy per consumption (APC), that capture the\nperformance of a AI model together with its environmental\nimpact [21]. Nevertheless we observe the following. First,\nwhile the metrics proposed in [21] are powerful in terms of\nassessing the energy/performance tradeoffs of models, these\nmetrics are less suitable in assessing the energy efficiency\nof a system such as AIoT. Second, that a metric somewhat\nsimilar to the Energy-per-Bit [J/b] would be more suitable\nfor measuring energy efficiency. Third, other efficiency metrics\nhave been proposed in wireless in particular and engineering\nand economy in general [22]. For instance, spectral efficiency,\nmeasured in [b/s/Hz] [23] provides a means of calculating\nthe amount of data bandwidth available in a given amount\nof spectrum, while lifecycle emissions for vehicles, measured\nin [g/km] [24], enables computing the CF in grams per\nkilometer.\nFollowing these observations, in this paper, we aim to find a\nsuitable energy efficiency metric that would be simple, general\nand holistically quantify the environmental cost of inference"}, {"title": "II. RELATED WORKS", "content": "While AIoT systems enable unprecedented automation and\nagility, their reliance on AI implies additional energy and car-\nbon costs [9] that in some cases that employ models with many\nparameters, such as LLMs, may be significant [10]. In light\nof climate challenges and the dazzling footprints of the latest\ngeneration of LLMs [13], the general AI research community's\neffort to better understand and then formalize methods to asses\nthe environmental impact of the AI revolution is rising. Due to\nthe complexity of state of the art neural architectures, in many\ncases the energy cost of training is computed after the training\nby measuring the performed computation through interfaces\nsuch as the performance application programming interface\n[25]. Furthermore, [11] shows that the energy footprint of in-\nference is more studied than the one for training and highlights\nbelow 70% accuracy between predicted and measured energy\nconsumption.\nHowever, a number of approaches and tools capable of\nproactively estimating the energy and environmental cost of\ntraining have also emerged. LLMCarbon [13] is a very recent\nend-to-end CF projection model designed for both dense\nand mixture of experts LLMs. It incorporates critical LLM,\nhardware, and data center parameters, such as LLM parameter\ncount, hardware type, system power, chip area, and data center\nefficiency, to model both operational and embodied CFs of\na LLM. Furthermore, [14] aims at providing guidelines for\nscaling AI in a sustainable way. They analyze the energy effi-\nciency of new processing units, the CF of the most prominent\nLLM models since GPT-3 and analyze their lifecycle carbon\nimpact showing that inference and training are comparable.\nThey conclude that, to enable sustainability as a computer\nsystem design principle, better tools for carbon telemetry are\nrequired, large scale carbon datasets, carbon impact disclosure\nand more suitable carbon metrics.\nThe networking and IoT communities, increasingly relying\non AI techniques, are also investigating pathways towards [15]\nnet-zero carbon emissions. The authors of [15] notice that\nin spite of improvements in hardware and software energy\nefficiency, the overall energy consumption of mobile networks\ncontinues to rise, exacerbated by the growing use of resource-\nintensive AI algorithms. They introduce a novel evaluation\nframework to analyze the lifecycle of network AI implemen-\ntations, identifying major emission sources. They propose the\nDynamic Energy Trading and Task Allocation framework, de-\nsigned to optimize carbon emissions by reallocating renewable\nenergy sources and distributing tasks more efficiently across\nthe network. Similar as in [14], they highlight the development\nof new metrics to quantify the environmental impact of new\nnetwork services enabled by AI.\nThe authors of [16] introduce a novel framework to quantify\nenergy consumption and carbon emissions for vanilla FL meth-\nods and consensus-based decentralized approaches, identifying\noptimal operational points for sustainable FL designs. Two\ncase studies are analyzed within 5G industry verticals: contin-\nual learning and reinforcement learning scenarios. Together\nwith the authors of [15], they consider energy [Wh] and\n[CO2eq] in their evaluations. The solution proposed in [17] is\na hardware/software co-design that introduces modality gating\n(throttling) to adaptively manage sensing and computing tasks.\nAMG features a novel decoupled modality sensor architec-\nture that supports partial throttling of sensors, significantly\nreducing energy consumption while maintaining data flow\nacross multimodal data: text, speech, images and video. More\nenergy efficient task scheduling has been considered in [18],\nwhile more efficient neural architecture design and subsequent"}, {"title": "III. ECAL DEFINITION AND METHODOLOGY", "content": "In our work, we consider an AIoT communication and com-\nputing system whose lifecycle is depicted in Fig. 1, consisting\nof the following data manipulating components:\na) Data Collection: This data-manipulating component,\ndepicted in the lower part of Fig. 1, the collected data Ns,\nincludes receiving telemetry data from sensors to the terminal\ncomputing infrastructure via wired or wireless technologies.\nThe collected information can be turned into indicators such\nas signal strength, and for example, be interpreted by the AI\nmodels to predict the location of a user. To move this data\nfrom the sensor to the access point (AP), \u0415\u0442 [J] is needed.\nb) Storage and Data Preprocessing: Once the data from\nIoT devices arrives at the computing infrastructure, it is first\nstored on the hard drive, and then fetched for preprocessing.\nTherefore, the energy consumption, Estorage, of this compo-\nnent arises from reading and writing data Ns into the storage\nunits, which can vary depending on data volume and modality.\nTo ensure accuracy and reliability during the training process,\nthe data, Ns, must go through several preprocessing steps\nsuch as cleaning, feature engineering and transformation. The\nenergy consumption for preprocessing Epre, depends on the\nintegrity of the ingested dataset. For example, datasets with a\nlot of invalid or missing data, require more extensive cleaning\nand error correction.\nc) Training and Evaluation: The training component\ncomprises the model development of the AI/Machine Learn-\ning (ML) using selected AI/ML techniques, such as neural\narchitectures, and data. In this step of the model development\nprocess, the processed data, NS,T is fetched and utilized\nto learn weights and biases in order to approximate the\nunderlying distribution. For most neural architectures, the\nlearning processes rely on Graphics Processing Unit (GPU)\nand Tensor Processing Unit (TPU) performing complex tensor\nprocessing operations and consuming Etrain. Once the neural\narchitecture weights are learned using the data in view of\nminimizing a loss function, the model is considered ready\nfor evaluation and deployment. Subsequently, the quality of\nthe learned model is evaluated on the evaluation datasets,\nNS,E, a process that consumes Eeval energy. The data storage,\npreprocessing, training, and evaluation form the end-to-end\ntraining of the AI/ML model and cumulatively require ED\nenergy to complete.\nd) Inference: Once the model is trained, it can be used by\napplications in inference mode. Various applications can send\nsamples of data NI,P and receive model outputs in the form\nof forecasts for regression problems or discrete (categorical)\nfor classification problems. The energy consumption Einf of a\nsingle inference is relatively small, however for high volumes\nof requests it can become significant.\nBased on this lifecycle, we proposed eCAL as the ratio\nbetween all the energy consumed by all data manipulation\ncomponents and all the manipulated application-level bits:"}, {"title": "IV. ENERGY COST OF DATA COLLECTION", "content": "In this section, we derive the energy cost of data collection\nin an AIoT system. To calculate the cost, we first determine\nthe number of bits that need to be transmitted from IoT devices\nto the server via wireless technology. Then we calculate the\nenergy consumption based on the given transmitting power and\ncorresponding transmission rate. In the last step, we derive the\ntotal energy consumption for collecting data from IoT devices.\nThe energy consumption of wired communication compared\nto the wireless part is negligible [29]. Therefore, in this work,\nwe only consider the communication from the IoT device to\nthe AP via wireless technologies.\nEach wireless technology relies on different multi-layer pro-\ntocol stack adding overhead bits, e.g., header, error correction,\netc., to ensure collected data integrity and efficient trans-\nmission thus increasing energy consumption. Therefore, each\npacket is composed of payload, e.g., measurements collected\nby an IoT device, and overhead, with overhead size depending\non the selected technology. The size of the transmitted data in\nbits (BT) can be expressed as defined in [30]:\n\u0412\u0442 [6] = as + \nwhere a represents bit precision, i.e., the number of bits re-\nquired to store value of a sample in floating number. Typically\na equals 32 bits for single precision (float) or 64 bits for\ndouble precision. Moreover, we denote samples as Ns which\nrepresents the telemetry data that the application requests from\nthe device in floating-point form, i.e., when an application\nrequests 256 samples, the payload size can be calculated as\n8192 and 16384 bits for single precision and double precision,\nrespectively. The second part of the equation represents the\noverhead. First, the number of packets is determined, by\nrounding up the payload divided with the maximal number\nof bits in one packet (Fu) for the selected access technology.\nTo obtain the total overhead the number of packets is then\nmultiplied with the number of overhead bits per packet, that\nalso depends on the selected access technology.\nTo illustrate the behavior of Eq. 2, we consider a scenario\nof a single device transmitting Ns = 256 of double-precision\nsamples to the application and alter the overhead percentages\nin one packet. More specifically, we consider a fixed packet\nsize of 2000 bits. We consider overhead of 1%, 30%, 50%,\nand 70%. Additionally, we consider overhead percentages of\nthree mainstream wireless technologies [31] for IoT systems,\nnamely Bluetooth Low Energy (BLE) 5.0 [32], ZigBee [33],\nand Long Range Wide Area Network (LoRaWAN) [34]. In\nTable I, we summarize the parameters of these three tech-\nnologies based on IoT protocol stack, in which the uplink data\ngoes through the physical layer to the application layer. Fig. 2\nillustrates the relationship between the number of samples and\nthe size of the transmitted data for different percentages of\noverhead. As seen in the figure, higher overhead percentages\nresult in significantly more data being sent than when overhead\nis low. For example, when transferring 256 samples, the data\ntransfer requirements for the scenarios \u03a9\u03c5 = 1%Fu and\n\u03a9\u03c5 = 70% Fu are 16564 and 28984 bits, respectively, which\nmeans the latter has 1.7 times higher data transfer requirements\nthan the former.\nThus, we can conclude that when the size of samples (Ns) is\nsmall, leveraging wireless technologies with lower overhead,\nsuch as BLE, is advantageous. These technologies efficiently\ntransmit fewer total bits. However, as the size of samples\nincreases, the impact of overhead becomes less significant. In\nthese scenarios, technologies with higher overhead but larger\nmaximum packet sizes can become more beneficial. This shift\nemphasizes the importance of the trade-off between overhead,\nsample size, and maximum packet size, and highlights the\nimportance of selecting the appropriate technology to achieve\nadequate energy efficiency in AIoT deployments.\nNext, we determine the energy consumption for a given"}, {"title": "V. ENERGY COST OF STORAGE AND DATA\nPREPROCESSING", "content": "Typically, data is first stored on a hard drive, subsequently\npreprocessed, and then used for the ML model development.\nIn the following, we consider the dataset with Ns samples\ncollected via wireless transmissions as depicted in Figure 1\nand discussed in Section IV.\nA. Storage\nThe energy consumption of storage primarily depends on\nthe type of hard drive. According to [36], a typical hard drive\nconsumes 0.65 Watt-Hour per Terabyte stored with a hard\ndisk drive (HDD)-based technology. On the other hand, solid\nstate drive (SSD)-based hard drive consumes 1.2 Watt-Hour\nper Terabyte. For example, the energy consumption (Estorage\n[J]) of a dataset with Ns = 256 samples in the form of\ndouble-precision floating number consumes 4.79 \u00d7 10-6 and\n8.85 \u00d7 10-6 Joules for HDD and SSD-based technology,\nrespectively, whereas the corresponding energy consumption\nper bit (Estorage,b [J/b]) are 2.92 \u00d7 10-10 and 5.4 \u00d7 10-10.\nB. Data preprocessing\nWe consider a data preprocessing approach consisting of\ndata cleaning and data standardization steps, while we leave\nother more complex approaches such as feature engineering\nand transformations to future work. We assume that the data-\ncleaning process is performed in two steps. The first step is\nidentifying and removing invalid samples from a list or array\nwhich incurs 0 FLOP since no floating points additions and\nmultiplications are performed, only comparisons and memory\noperations. The second step is to determine the complexity\nof data standardization (MDS). We provide two examples, i.e.,\nmin-max scaling and normalization process, both representing\na standard approach for data standardization."}, {"title": "VI. ENERGY COST OF TRAINING", "content": "While there are many AI/ML models available, in this work\nwe focus on a fully connected Multilayer Perceptron (MLP), as\nthey form the foundation of all modern AI methods and point\nthe reader to works such as [11] for a more comprehensive\nview on estimating energy consumption in machine learning.\nA filly connected MLP architecture is depicted in Fig. 7 with\nthe blue and red arrows illustrating the forward and backwards\npropagation taking place during the training process while\nthe training is summarized in Algorithm 3. The architecture\ncomprises of one input layer with N\u2081 neurons, K hidden\nlayers, in each layer it has M neurons and No neurons in\nthe output layer. As shown in the Fig. 1, the training process\nincludes two steps, namely, training and evaluation. Typically,\ninput samples are split into certain ratios for training and\nevaluation. For example, with a 70/30 split, a dataset with 256\nsamples will have 179 samples for training, and 77 samples\nfor evaluation. Therefore, we introduce \u03b2 for such ratio. More\nspecifically, the input samples can be expressed as:\nNs = \u03b2Ns + (1 \u2212 \u03b2)Ns,\nNS,T\nNS,E\nwhere NS,T and NS,E represent the number of samples for\ntraining and evaluation, respectively.\nModel training of MLP involves two main phases as de-\npicted in Fig. 7 and Algorithm 3. First, the training sam-\nples undergo forward propagation where neurons compute\nlinear combinations of weights and biases, apply activation\nfunctions, and calculate the loss between predicted and actual\nvalues. Subsequently, the model performs backward propaga-\ntion where it calculates gradients of the loss with respect to\neach parameter, updates these gradients in reverse through the\nlayers, and adjusts the weights and biases accordingly, which is\ndescribed in Algorithm 3 from line 11 to line 17. This process\ncompletes one epoch. Consequently, we can calculate the en-\nergy costs associated with forward and backward propagation,\nleading to an understanding of the overall energy consumption"}, {"title": "VII. ENERGY COST OF INFERENCE", "content": "The complexity of making an inference depends on the\nnumber of FLOPs required for forward propagating with input\nsample size NI,P, i.e., Ninf = MFP NI,P. For example, when\nwe adopt the aforementioned MLP model with N\u2081,P = 77, the\ncomplexity is calculated as Ninf = 22677 = 17402 FLOPs."}, {"title": "VIII. ECAL: THE ENERGY COST OF AIOT LIFECYCLE", "content": "In this section, we describe the end-to-end energy con-\nsumption of an AIoT system for making inferences with a\nMLP model. First, we consider the energy consumption of\ndeveloping the model illustrated on the left hand side of Fig. 1\nas follows:\nED = ET + Estorage + Epre + Etrain + Eeval,\nand the energy consumption per bit that can be expressed as:\nED\nED,b =\n\u0412\u0442 + \u03b1(2Ns + NS,T + NS,E)\nTo compare the energy consumption of all the data manip-\nulation components required to develop and deploy an AloT\nsystem, we continue with the guiding example from the paper\nwhere the AI model relies on an an MLP with 3 hidden\nlayers (5 neurons each), whereas on the input layer and output\nlayer there are 6, 3 neurons, respectively, and it requests 256\nsamples from a IoT device via BLE to train and evaluate"}, {"title": "IX. CARBON FOOTPRINT OF AIOT INFERENCE", "content": "The CF is defined as the product of energy consumption and\nCI, where CI indicates the amount of carbon dioxide equiva-\nlents (CO2eq) emissions associated with generating electrical\nenergy, accounting for all greenhouse gases by weighting each\ngas by its Global Warming Potential (GWP). This relationship\nis expressed as:\nCF[gCO2eq] = E[kWh] \u00b7CI[gCO2eq/kWh]."}, {"title": "X. CONCLUSION AND FUTURE WORK", "content": "In this work, we proposed a novel metric, namely, eCAL,\nunlike traditional metrics that focus only on the energy re-\nquired for transmitting or AI models. It can capture the overall\nenergy cost of generating an inference in an AIoT system\nduring the whole lifecylce of a trained model. We propose\na detailed methodology to determine the eCAL of an AIoT\nsystem by breaking it down into various data manipulation\ncomponents, such as data collection, storage, preprocessing,\ntraining, evaluation, and inference, and analyzing the complex-\nity and energy consumption of each component. Our proposed\nmetric demonstrates that the more a model is utilized, the more\nenergy-efficient each inference becomes. For example, con-\nsidering a simple MLP architecture, the energy consumption\nper bit for 100 inferences is 1.43 times higher than for 1000\ninferences. Moreover, we assess the CF of the AIoT system\nby calculating the equivalent CO2 emissions based on energy\nconsumption and CI across different countries. Our analysis,\nusing 2023 renewable data, shows that CF varies deploying an\nAIoT system in different geographic locations, indicating the\nimportance of considering regional energy profiles and their\nassociated CIs in the deployment of AIoT systems.\nThrough the proposed eCAL metric, this study provides\nfoundations for understanding the energy consumption of\nAIoT systems. The assumptions are that the model is based\non MLPs which are a type of feed-forward neural architec-\ntures. Future work may focus on removing that assumption\nand extend the study to more complex architectures such as\nrecurrent, convolutional, recursive, attention based, etc. and\nprovide a relative complexity comparison of these categories.\nFurthermore, as virtualization permeates 6G networks, the\nimpact of network slices on the data collection part could\nbe derived. Finally, the development of dynamic energy man-"}], "equations": ["eCAL = \\frac{\\text{Total energy of data manipulation components [J]}}{\\text{Total manipulated application level data [b]}}", "\u0412\u0442 [6] = as + \\lceil \\frac{aNs}{F_U} \\rceil \\cdot \\Omega_U", "E_T[J] = \\frac{P_T[W]}{R_T[b/s]} \\cdot B_T[b].", "E_{T,b} [J/b] = \\frac{P_T[W] \\cdot T_T[s]}{B_T [b]} = \\frac{P_T[W]}{R_T[b/s]}", "M_{minmax} = 2 \\cdot (N_S - N_{NaN}) - 1,", "M_{normalization} = 6 \\cdot (N_S - N_{NaN} - 1) + 3.", "M_{pre} = \\begin{cases} 2 \\cdot (N_{DS} - N_{NaN}) - 1, & \\text{if min-max scaling,} \\\\ 6 \\cdot (N_{DS} - N_{NaN}) - 3, & \\text{if normalization.} \\end{cases}", "E_{pre}[J] = P_{pre} [W] \\cdot T_{pre}[s],", "T_{pre}[s] = \\frac{M_{pre} [FLOPS]}{M_{PU}[FLOPS/S]}", "E_{pre,b}[J/b] = \\frac{E_{pre}[J]}{\\alpha N_S [b]}", "N_S = \\beta N_S + (1 - \\beta) N_S,\\\\ N_{S,T} \\quad N_{S,E}", "M_{FP} = \\sum_{l=1}^{L-1} (2 \\cdot (M_{l-1} \\cdot M_l) + 2 \\cdot M_l)", "M_{MLP,FP} = N_{epochs} \\cdot N_{batch} \\cdot batchsize \\cdot M_{FP} \\\\ = N_{epochs} \\cdot N_{S,T} \\cdot M_{FP}.", "M_{MLP} \\approx 3 M_{MLP,FP}.", "E_{train} = \\frac{3M_{MLP,FP}[FLOPs]}{PU_{performance}[FLOPs/s/W]},", "E_{train, b} = \\frac{3M_{FP}}{aPU_{performance}}", "E_{eval} = \\frac{M_{FP} N_{S,E}}{PU_{performance}}", "E_{eval,b} = \\frac{M_{FP}}{aPU_{performance}}", "E_{inf} = \\frac{M_{FP} N_{I,P}}{PU_{performance}}", "E_D = E_T + E_{storage} + E_{pre} + E_{train} + E_{eval},", "E_{D,b} = \\frac{E_D}{B_T + \\alpha (2N_S + N_{S,T} + N_{S,E})}", "E_{inf,p} = E_T + E_{storage} + E_{pre} + E_{inf},", "E_{inf,p,b} = \\frac{E_{inf,p}}{B_T + 3 \\alpha N_{I,P}}", "eCAL_{abs} = E_D + \\gamma E_{inf,p}.", "eCAL = \\frac{eCAL_{abs}}{\\gamma}", "eCAL = \\frac{eCAL_{abs}}{B_T + a(2N_S + N_{S,T} + N_{S,E}) + \\gamma (B_T + 3aN_{I,P})}", "CF[gCO_2eq] = E[kWh] \\cdot CI[gCO_2eq/kWh]."]}