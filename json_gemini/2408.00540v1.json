{"title": "The Energy Cost of Artificial Intelligence of Things Lifecycle", "authors": ["Shih-Kai Chou", "Jernej Hribar", "Mihael Mohor\u010di\u010d", "Carolina Fortuna"], "abstract": "Artificial Intelligence (AI) coupled with existing In- ternet of Things (IoT) enables more streamlined and autonomous operations across various economic sectors. Consequently, the paradigm of Artificial Intelligence of Things (AIoT) having AI techniques at its core implies additional energy and carbon costs that may become significant with more complex neural architec- tures. To better understand the energy and Carbon Footprint (CF) of some AIoT components, very recent studies employ conventional metrics. However, these metrics are not designed to capture energy efficiency aspects of inference. In this paper, we propose a new metric, the Energy Cost of AIoT Lifecycle (eCAL) to capture the overall energy cost of inference over the lifecycle of an AIoT system. We devise a new methodology for determining eCAL of an AloT system by analyzing the complexity of data manipulation in individual components involved in the AIoT lifecycle and derive the overall and per bit energy consumption. With eCAL we show that the better a model is and the more it is used, the more energy efficient an inference is. For an example AIoT configuration, eCAL for making 100 inferences is 1.43 times higher than for 1000 inferences. We also evaluate the CF of the AIoT system by calculating the equivalent CO2 emissions based on the energy consumption and the Carbon Intensity (CI) across different countries. Using 2023 renewable data, our analysis reveals that deploying an AIoT system in Germany results in emitting 4.62 times higher CO2 than in Finland, due to latter using more low-CI energy sources.", "sections": [{"title": "I. INTRODUCTION", "content": "Next-generation networks aim to deliver unprecedented levels of connectivity and intelligence, facilitating seamless interactions between a massive number of devices, services, and applications. The trend is especially noticeable in Internet of Things (IoT) where devices are becoming increasingly more intelligent [1], from smart home appliances [2] to industrial machinery [3], Smart City [4], vehicle-to-everything (V2X) [5], and beyond [6], [7]. With the help of Artificial Intelligence (AI), we are moving away from conventional IoT applications towards Artificial Intelligence of Things (AIoT) systems [8] designed to achieve more streamlined and au- tonomous operations. For example, in future smart factories, the integration of AI and IoT will enable real-time monitoring and optimization of production processes, predictive mainte- nance, and enhanced decision-making capabilities.\nWhile AIoT systems enable unprecedented automation and agility, their reliance on Al implies additional energy and carbon costs [9]. Especially in the cases when models with many parameters are employed, such as Large language model (LLM)s, such costs may be significant [10]. In light of climate challenges, the general AI research community's effort to better estimate the costs of training [11], estimate the cost of AI [12], the Carbon Footprint (CF)s of LLMs [13] and find ways to scale models efficiently [14] are intensifying. Furthermore, the networking and IoT communities increas- ingly relying on AI techniques are also investigating pathways towards [15] net-zero carbon emissions, analyzing the CF of various learning techniques [16] and data modalities while also developing various approaches to optimizing energy efficiency through hardware-software optimization [17], scheduling [18], more efficient model design [19] and energy and carbon consumption testing [20].\nDepending on the scope of the study, various metrics are employed to understand energy and CF. For instance, [17] considers normalized energy as well as energy for buffering in [J], energy [Wh] and [CO2eq] for the various phases involved in federated learning (FL) edge systems [15], [16], [J] and GFLOPS of the neural architectures [19] and energy [Wh] for the scheduled loads in [18]. However, these metrics are not specifically designed to capture the energy efficiency of a system or part of. As well noted in [15], the traditional ITU standardized Energy-per-Bit [J/b] metric 'will no longer be able to reflect the environmental impact of the modern mobile services, especially network Al-enabled smart services'.\nRecently, a number of environmentally friendly metrics, such as accuracy per consumption (APC), that capture the performance of a AI model together with its environmental impact [21]. Nevertheless we observe the following. First, while the metrics proposed in [21] are powerful in terms of assessing the energy/performance tradeoffs of models, these metrics are less suitable in assessing the energy efficiency of a system such as AIoT. Second, that a metric somewhat similar to the Energy-per-Bit [J/b] would be more suitable for measuring energy efficiency. Third, other efficiency metrics have been proposed in wireless in particular and engineering and economy in general [22]. For instance, spectral efficiency, measured in [b/s/Hz] [23] provides a means of calculating the amount of data bandwidth available in a given amount of spectrum, while lifecycle emissions for vehicles, measured in [g/km] [24], enables computing the CF in grams per kilometer.\nFollowing these observations, in this paper, we aim to find a suitable energy efficiency metric that would be simple, general and holistically quantify the environmental cost of inference carried out in AIoT systems. The contributions of this paper are as follows:\n\u2022 We propose a new metric, the Energy Cost of AIoT Lifecycle (eCAL), also measured in [J/b], that captures the overall energy cost of generating an inference in an AIoT system. Unlike capturing the energy required for transmitting bits that is done by the Energy-per-Bit [J/b] metric, or [J] for AI model complexity, the proposed eCAL captures the energy consumed by all the data manipulation components in an AIoT system during the lifecycle of a trained model enabling inference.\n\u2022 We devise a methodology for determining the eCAL of an AIoT system. The methodology breaks down the AIoT system into different data manipulation components, i.e. data collection, storage, and data preprocessing, training along with evaluation, and inference, and, for each com- ponent, it analyzes the complexity of data manipulation and derives the overall and per bit energy consumption.\n\u2022 Using the proposed metric, we show that the better a model is and the more it is used, the more energy efficient an inference is. For an example AIoT configuration, the energy consumption per bit for making 100 inferences is 1.33 times higher than for 1000 inferences. This can be explained by the fact that the energy cost of data col- lection, storage, training and evaluation, all necessary to develop an AI model are spread across more inferences.\n\u2022 We evaluate the CF of the AIoT system by calculating the equivalent CO2 emissions based on the energy con- sumption and the Carbon Intensity (CI) across different countries. Using 2023 renewable data, our analysis re- veals that deploying an AIoT system in Germany results in emitting 4.62 times higher CO2 than in Finland, due to the latter using more low-CI energy sources.\nThe paper is structured as follows. We first review the relevant literature in Section II, we then describe the lifecycle of an AIoT system, define eCAL and the methodology for deriving it in Section III. Subsequently, the derivation of the energy consumption formulas for different data manipulation components involved in the AIoT lifecycle are provided in Sections IV to VII. In Section VIII, the proposed metric, eCAL, is derived and analyzed, demonstrating its utility in capturing the overall energy cost of AIoT systems over its lifecycle. whereas in Section IX we discuss the CF aspect of such a system, comparing emissions across different European countries. Finally, Section X concludes the paper and outlines future research directions."}, {"title": "II. RELATED WORKS", "content": "While AIoT systems enable unprecedented automation and agility, their reliance on AI implies additional energy and car- bon costs [9] that in some cases that employ models with many parameters, such as LLMs, may be significant [10]. In light of climate challenges and the dazzling footprints of the latest generation of LLMs [13], the general AI research community's effort to better understand and then formalize methods to asses the environmental impact of the AI revolution is rising. Due to the complexity of state of the art neural architectures, in many cases the energy cost of training is computed after the training by measuring the performed computation through interfaces such as the performance application programming interface [25]. Furthermore, [11] shows that the energy footprint of in- ference is more studied than the one for training and highlights below 70% accuracy between predicted and measured energy consumption.\nHowever, a number of approaches and tools capable of proactively estimating the energy and environmental cost of training have also emerged. LLMCarbon [13] is a very recent end-to-end CF projection model designed for both dense and mixture of experts LLMs. It incorporates critical LLM, hardware, and data center parameters, such as LLM parameter count, hardware type, system power, chip area, and data center efficiency, to model both operational and embodied CFs of a LLM. Furthermore, [14] aims at providing guidelines for scaling AI in a sustainable way. They analyze the energy effi- ciency of new processing units, the CF of the most prominent LLM models since GPT-3 and analyze their lifecycle carbon impact showing that inference and training are comparable. They conclude that, to enable sustainability as a computer system design principle, better tools for carbon telemetry are required, large scale carbon datasets, carbon impact disclosure and more suitable carbon metrics.\nThe networking and IoT communities, increasingly relying on AI techniques, are also investigating pathways towards [15] net-zero carbon emissions. The authors of [15] notice that in spite of improvements in hardware and software energy efficiency, the overall energy consumption of mobile networks continues to rise, exacerbated by the growing use of resource- intensive AI algorithms. They introduce a novel evaluation framework to analyze the lifecycle of network AI implemen- tations, identifying major emission sources. They propose the Dynamic Energy Trading and Task Allocation framework, de- signed to optimize carbon emissions by reallocating renewable energy sources and distributing tasks more efficiently across the network. Similar as in [14], they highlight the development of new metrics to quantify the environmental impact of new network services enabled by AI.\nThe authors of [16] introduce a novel framework to quantify energy consumption and carbon emissions for vanilla FL meth- ods and consensus-based decentralized approaches, identifying optimal operational points for sustainable FL designs. Two case studies are analyzed within 5G industry verticals: contin- ual learning and reinforcement learning scenarios. Together with the authors of [15], they consider energy [Wh] and [CO2eq] in their evaluations. The solution proposed in [17] is a hardware/software co-design that introduces modality gating (throttling) to adaptively manage sensing and computing tasks. AMG features a novel decoupled modality sensor architec- ture that supports partial throttling of sensors, significantly reducing energy consumption while maintaining data flow across multimodal data: text, speech, images and video. More energy efficient task scheduling has been considered in [18], while more efficient neural architecture design and subsequent model development in [19]. The energy efficiency of various programming languages is studied in [26] while energy and carbon consumption testing for AI-driven IoT services is developed in [20].\nNovel metrics to evaluate Deep Learning (DL) models, considering not only their accuracy and speed but also their energy consumption and cost have been proposed in [21]. The four metrics are: Accuracy Per Consumption (APC) and Accuracy Per Energy Cost (APEC) for inference, and Time To Closest APC (TTCAPC) and Time To Closest APEC (TTCAPEC) for training. These metrics aim to promote the use of energy-efficient and green energy-powered DL systems by integrating energy considerations into the performance evaluation. Nevertheless, while powerful in terms of assessing the energy/performance tradeoffs of models, these metrics are less suitable in assessing the energy efficiency of a system such as AIoT. The Energy-per-Bit [J/b] has been defined for assessing the efficiency of transmissions and therefore does not capture the AI aspects as also noted in [15]. Looking at the electric vehicle literature, we notice that, when comparing to traditional gas vehicles they employ lifecycle emissions as a metric, measured in [g/km] [24].\nInspired by the simplicity and effectiveness of metrics such as transmission efficiency through energy-per-bit and the lifecycle emissions, and responding to calls for new metrics [14], [15] that capture the energy cost in the era of AI enabled systems, this work proposes the novel Energy Cost of AIOT Lifecycle (eCAL) metric and methodology to derive it."}, {"title": "III. ECAL DEFINITION AND METHODOLOGY", "content": "In our work, we consider an AIoT communication and com- puting system whose lifecycle is depicted in Fig. 1, consisting of the following data manipulating components:\na) Data Collection: This data-manipulating component, depicted in the lower part of Fig. 1, the collected data Ns, includes receiving telemetry data from sensors to the terminal computing infrastructure via wired or wireless technologies. The collected information can be turned into indicators such as signal strength, and for example, be interpreted by the AI models to predict the location of a user. To move this data from the sensor to the access point (AP), \u0415\u0442 [J] is needed.\nb) Storage and Data Preprocessing: Once the data from IoT devices arrives at the computing infrastructure, it is first stored on the hard drive, and then fetched for preprocessing. Therefore, the energy consumption, Estorage, of this compo- nent arises from reading and writing data Ns into the storage units, which can vary depending on data volume and modality. To ensure accuracy and reliability during the training process, the data, Ns, must go through several preprocessing steps such as cleaning, feature engineering and transformation. The energy consumption for preprocessing Epre, depends on the integrity of the ingested dataset. For example, datasets with a lot of invalid or missing data, require more extensive cleaning and error correction.\nc) Training and Evaluation: The training component comprises the model development of the AI/Machine Learn- ing (ML) using selected AI/ML techniques, such as neural architectures, and data. In this step of the model development process, the processed data, NS,T is fetched and utilized to learn weights and biases in order to approximate the underlying distribution. For most neural architectures, the learning processes rely on Graphics Processing Unit (GPU) and Tensor Processing Unit (TPU) performing complex tensor processing operations and consuming Etrain. Once the neural architecture weights are learned using the data in view of minimizing a loss function, the model is considered ready for evaluation and deployment. Subsequently, the quality of the learned model is evaluated on the evaluation datasets, NS,E, a process that consumes Eeval energy. The data storage, preprocessing, training, and evaluation form the end-to-end training of the AI/ML model and cumulatively require ED energy to complete.\nd) Inference: Once the model is trained, it can be used by applications in inference mode. Various applications can send samples of data NI,P and receive model outputs in the form of forecasts for regression problems or discrete (categorical) for classification problems. The energy consumption Einf of a single inference is relatively small, however for high volumes of requests it can become significant.\nBased on this lifecycle, we proposed eCAL as the ratio between all the energy consumed by all data manipulation components and all the manipulated application-level bits:\n$$eCAL = \\frac{\\text{Total energy of data manipulation components [J]}}{\\text{Total manipulated application level data [b]}}$$"}, {"title": "IV. ENERGY COST OF DATA COLLECTION", "content": "In this section, we derive the energy cost of data collection in an AIoT system. To calculate the cost, we first determine the number of bits that need to be transmitted from IoT devices to the server via wireless technology. Then we calculate the energy consumption based on the given transmitting power and corresponding transmission rate. In the last step, we derive the total energy consumption for collecting data from IoT devices.\nThe energy consumption of wired communication compared to the wireless part is negligible [29]. Therefore, in this work, we only consider the communication from the IoT device to the AP via wireless technologies.\nEach wireless technology relies on different multi-layer pro- tocol stack adding overhead bits, e.g., header, error correction, etc., to ensure collected data integrity and efficient trans- mission thus increasing energy consumption. Therefore, each packet is composed of payload, e.g., measurements collected by an IoT device, and overhead, with overhead size depending on the selected technology. The size of the transmitted data in bits (BT) can be expressed as defined in [30]:\n$$B_T [b] = aN_S + \\lceil \\frac{aN_S}{F_U} \\rceil \\cdot \\Omega_U$$\nwhere a represents bit precision, i.e., the number of bits re- quired to store value of a sample in floating number. Typically a equals 32 bits for single precision (float) or 64 bits for double precision. Moreover, we denote samples as Ns which represents the telemetry data that the application requests from the device in floating-point form, i.e., when an application requests 256 samples, the payload size can be calculated as 8192 and 16384 bits for single precision and double precision, respectively. The second part of the equation represents the overhead. First, the number of packets is determined, by rounding up the payload divided with the maximal number of bits in one packet (Fu) for the selected access technology. To obtain the total overhead the number of packets is then multiplied with the number of overhead bits per packet, that also depends on the selected access technology.\nTo illustrate the behavior of Eq. 2, we consider a scenario of a single device transmitting Ns = 256 of double-precision samples to the application and alter the overhead percentages in one packet. More specifically, we consider a fixed packet size of 2000 bits. We consider overhead of 1%, 30%, 50%, and 70%. Additionally, we consider overhead percentages of three mainstream wireless technologies [31] for IoT systems, namely Bluetooth Low Energy (BLE) 5.0 [32], ZigBee [33], and Long Range Wide Area Network (LoRaWAN) [34].\nNext, we determine the energy consumption for a given technology, which we define as follows:\n$$E_T[J] = \\frac{P_T[W]}{R_T[b/s]} \\cdot B_T[b]$$\nwhere PT represents the transmitting power. As a result, we can obtain energy consumption per bit for transmitting part of AIoT system as follows:\n$$E_{T,b} [J/b] = \\frac{P_T[W] \\cdot T_T[s]}{B_T [b]} = \\frac{P_T[W]}{R_T[b/s]}$$\nwhere Tr is the corresponding transmitting time. Tr can be derived from the corresponding transmitting rate (RT) and \u0412\u0442, and is expressed as Tr = BTRT.\nWe can observe from the above equations that we are able to determine that when considering energy consumption per bit (\u0415\u0442\u044c), this metric is exclusively related to Pr and RT, which indicates that the energy required to transmit one bit of data depends on the power used for transmission and the speed of the transmission. Meaning, that energy consumption per bit is the inverse of energy efficiency [35].\nWe consider the wireless technologies listed in Table I to illustrate per-bit energy consumption with the corresponding Pr and Ry and calculated \u0415\u0442\u044c in Table II. The comparison between E\u0442\u044c of different technologies is shown in Fig. 3. It can be seen that BLE exhibits the best performance due to its low power consumption and high transmission rate, while LoRaWAN exhibits the highest energy consumption per bit despite its low overhead. This emphasizes the crucial role of transmission power and rate in determining the overall energy efficiency in AIoT systems. Our results underscore the need to evaluate both power consumption and transmission rate, not just overhead, when selecting the communication technology for AIoT deployments. Prioritizing lower power consumption and higher transmission rates can significantly improve energy consumption per bit.\nFinally, the total energy consumption of the data transmis- sion in an AIoT system is illustrated in Fig. 4. We assume that the signals are transmitted at one-minute intervals and uniformly. These results showcase the total energy consump- tion over a period of 24 hours of continuous operation. BLE 5.0 benefits from low power consumption, a high data rate, and low overhead, resulting in the lowest energy consumption per bit and very low overall energy consumption. Despite lower energy consumption per bit of ZigBee wireless tech- nology, overall energy consumption rises with larger data sizes due to higher overhead percentage, whereas, LoRaWAN has the highest energy consumption owing to its \u0415\u0442,\u044c is more dominate factor of the energy consumption. Moreover, these technologies excel in different aspects. For example, LoRaWAN is optimal for long-range communication, whereas ZigBee is suitable for mesh networking. Consequently, both the total number of bits that devices transmit (BT) and energy consumption per bit (\u0415\u0442,\u044c) play a role in determining energy cost of data transmission. Once the data reaches the AP is passed to the server for storage and preprocessing."}, {"title": "V. ENERGY COST OF STORAGE AND DATA PREPROCESSING", "content": "Typically, data is first stored on a hard drive, subsequently preprocessed, and then used for the ML model development. In the following, we consider the dataset with Ns samples collected via wireless transmissions as depicted in Figure 1 and discussed in Section IV.\nA. Storage\nThe energy consumption of storage primarily depends on the type of hard drive. According to [36], a typical hard drive consumes 0.65 Watt-Hour per Terabyte stored with a hard disk drive (HDD)-based technology. On the other hand, solid state drive (SSD)-based hard drive consumes 1.2 Watt-Hour per Terabyte. For example, the energy consumption (Estorage [J]) of a dataset with Ns = 256 samples in the form of double-precision floating number consumes 4.79 \u00d7 10-6 and 8.85 \u00d7 10-6 Joules for HDD and SSD-based technology, respectively, whereas the corresponding energy consumption per bit (Estorage,b [J/b]) are 2.92 \u00d7 10-10 and 5.4 \u00d7 10-10.\nB. Data preprocessing\nWe consider a data preprocessing approach consisting of data cleaning and data standardization steps, while we leave other more complex approaches such as feature engineering and transformations to future work. We assume that the data- cleaning process is performed in two steps. The first step is identifying and removing invalid samples from a list or array which incurs 0 FLOP since no floating points additions and multiplications are performed, only comparisons and memory operations. The second step is to determine the complexity of data standardization (MDS). We provide two examples, i.e., min-max scaling and normalization process, both representing a standard approach for data standardization.\na) Min-max scaling: In this method, which we detail in Algorithm 1, we first initialize two values for the minimum and maximum. Then, we iteratively search for minimum and maximum values by going through all the valid sam- ples to update these values (lines 2 to 9), which involve comparisons but no FLOP. After finding the minimum and maximum values, we calculate the range (line 10), which requires one FLOP. Finally, we scale the dataset (lines 11 to 13) by performing a subtraction and division for each valid sample, costing 2(Ns - NNaN - 1) floating-point operations (FLOPs). Therefore, the corresponding total number of FLOPs (Mminmax) of min-max scaling can be expressed as:\n$$M_{minmax} = 2 \\cdot (N_S \u2013 N_{NaN}) \u2013 1,$$\nwhere NNan is the number of invalid samples.\nb) Normalization: Normalization is performed in three steps, as described in Algorithm 2. The algorithm starts by calculating the mean value (lines 2 to 6), followed by the calculation of the standard deviation (lines 8 to 12). Finally, the algorithm iteratively normalizes each sample (lines 14 to 16). Thus, the total operation (Norm) can be expressed as:\n$$M_{normalization} = 6 \\cdot (N_S \u2013 N_{NaN} \u2013 1) + 3.$$\nAs a result, the total number of FLOPs for preprocessing can be expressed as:\n$$M_{pre} = \\begin{cases} 2 \\cdot (N_{DS} - N_{NaN}) - 1, & \\text{if min-max scaling}, \\\\ 6 \\cdot (N_{DS} - N_{NaN}) \u2013 3, & \\text{if normalization}. \\end{cases}$$\nWe can observe that the computing complexity of both stan- dardization methods decreases linearly as the number of invalid samples increases. This indicates that a higher number of invalid samples lowers the computing complexity, leading to reduced energy consumption. However, this also means that the application has to request additional transmissions from the device and perform extra operations to preprocess the data on the server.\nWith the computing complexity in FLOPs formalized, we can obtain total energy consumption (Epre) of the prepro- cessing (Epre) by utilizing the power consumption of the processing unit and the executing time (Tpre). Thus, it is expressed as follows:\n$$E_{pre} [J] = P_{pre} [W] \\cdot T_{pre}[s]$$\nwhere Tpre can be calculated as the ratio between FLOPs needed for preprocessing and the computational power of a processor (NPU).\n$$T_{pre}[s] = \\frac{M_{pre} [FLOPS]}{M_{PU} [FLOPS/S]}$$\nConsequently, the energy consumption per bit for preprocess- ing can then be determined as:\n$$E_{pre,b} [J/b] = \\frac{E_{pre} [J]}{aN_S [b]}$$\nFor example, the Epre and the corresponding Epre,b for performing data preprocessing on a CPU with a performance of MPU = 10 GFLOPs/s and consume power of Ppre = 140 W can be seen in the Fig. 5 and Fig. 6, respectively. More specifically, Fig. 5 shows that the energy consumption for both standardization methods decreases as the number of invalid samples increases, confirming that higher invalid sample rates lead to lower computational requirements. In addition, Fig. 6 illustrates the energy consumption per bit for normalization and min-max scaling with Ns = 256, where normalization consumes three times more energy per bit than min-max scaling. These results highlight the importance of selecting an appropriate preprocessing method based on both the energy efficiency requirements and AI/ML technique requirements. After processing the samples, the training process described in the next section can be carried out."}, {"title": "VI. ENERGY COST OF TRAINING", "content": "While there are many AI/ML models available, in this work we focus on a fully connected Multilayer Perceptron (MLP), as they form the foundation of all modern AI methods and point the reader to works such as [11] for a more comprehensive view on estimating energy consumption in machine learning. A filly connected MLP architecture is depicted in Fig. 7 with the blue and red arrows illustrating the forward and backwards propagation taking place during the training process while the training is summarized in Algorithm 3. The architecture comprises of one input layer with N\u2081 neurons, K hidden layers, in each layer it has M neurons and No neurons in the output layer. As shown in the Fig. 1, the training process includes two steps, namely, training and evaluation. Typically, input samples are split into certain ratios for training and evaluation. For example, with a 70/30 split, a dataset with 256 samples will have 179 samples for training, and 77 samples for evaluation. Therefore, we introduce \u03b2 for such ratio. More specifically, the input samples can be expressed as:\n$$N_S = \\beta N_S + (1 \u2013 \\beta)N_S,$$\nwhere NS,T and NS,E represent the number of samples for training and evaluation, respectively.\nModel training of MLP involves two main phases as de- picted in Fig. 7 and Algorithm 3. First, the training sam- ples undergo forward propagation where neurons compute linear combinations of weights and biases, apply activation functions, and calculate the loss between predicted and actual values. Subsequently, the model performs backward propaga- tion where it calculates gradients of the loss with respect to each parameter, updates these gradients in reverse through the layers, and adjusts the weights and biases accordingly, which is described in Algorithm 3 from line 11 to line 17. This process completes one epoch. Consequently, we can calculate the en- ergy costs associated with forward and backward propagation, leading to an understanding of the overall energy consumption for training, subsequently model evaluation, and inference.\nA. Computational Complexity of Forward and Backward Propagation\nAssuming L layers where L = K + 2, the total FLOPs for the forward propagation of a single input sample for an MLP can be expressed as:\n$$M_{FP} = \\sum_{l=1}^{L-1} (2\\cdot (\u039c_{l-1} \\cdot \u039c_l) + 2\\cdot \u039c_l)$$\nwhere L represents the overall number of layers in the MLP, the first term in the sum represents one multiplication and one addition (= 2 FLOPs) related to the weight and bias corresponding to an the edges times the number of edges on that layer (= M1-1 M\u2081) while the second term represents the summation of all the values of the incoming edges and the application of the activation function (= 2 FLOPs) times the number of nodes in that layer (M\u2081). As in our guiding example we assume all hidden layers having the same number of neurons, Eq. 12 becomes MFP = (2\u00b7N\u2081\u00b7M+2\u00b7M)+(2\u00b7(K- 1)\u00b7 \u041c\u00b2+2\u00b7M)+(2\u00b7MNo+2\u00b7No) using the notations from Fig. 7. Considering an example MLP with 3 hidden layers with 5 neurons on each hidden layer, 6 neurons on the input layer. Moreover, we aim to train the MLP model and utilize it to make predictions of three-dimensional coordinates based on the input data, thus, there will be 3 neurons on the output layer. Therefore, the total number of FLOPs can be calculated as MFP = 2(6.5+5) + 2(2.25+5) + 2(5.3 + 3) = 226.\nTraining consists of multiple epochs and batches. Therefore, the total complexity of forward propagation is:\n$$M_{MLP,FP} = N_{epochs} \\cdot N_{batch} \\cdot batchsize \\cdot M_{FP} = N_{epochs} \\cdot N_{S,T} \\cdot M_{FP}.$$\nContinuing with the example with 256 samples that goes through the aforementioned MLP model across 10 epochs, the computational complexity for forward propagation can be calculated MMLP,FP = 10256226 = 578560 FLOPs as also depicted in Fig. 8a. When all hidden layers have the same number of neurons and their number M is increasing, it can be seen from Fig. 8a that the FLOPs also increase quadratically. Adding more hidden layers K results in increases more than linearly due to the multiplicative nature of the connections between layers. As per Eq. 13 and Fig. 8b, the relationship between the increase in complexity and the augmentation of the number of samples and epochs is linear. These observa- tions highlight the importance of considering the impact of architectural parameters on the computational complexity of MLP models.\nAs we can see from Algorithm 3 (line 11 to line 17), the backward propagation consists of approximately twice the computational complexity of forward propagation, which is also confirmed in [37]. Thus, we can approximate the computing complexity of training a model as:\n$$M_{MLP} \u2248 3M_{MLP,FP}.$$\nAs a result, we can compute the energy consumption for training, model evaluation, and the corresponding energy con- sumption per bit.\nB. Energy Cost of Training\nWe can obtain the energy consumption of the entire training process as:\n$$E_{train} = \\frac{3M_{MLP,FP} [FLOPS]}{PU_{performance} [FLOPS/s/W]},$$\nwhere PUperformance represents the theoretical peak perfor- mance of a processing unit. Moreover, the corresponding energy consumption per bit can be obtained from Eq. 15 and the number of input sample, thus, we can show it as:\n$$E_{train, b} = \\frac{3MFP}{aPU_{performance}}$$\nC. Energy Cost of Model Evaluation\nThe model evaluation starts once training is complete. This functionality tests the performance of the model on a separate dataset. During the evaluation, the model processes the data using forward propagation without any adjustments to its parameters, which can be seen from lines 5 to 9 in Algorithm 3. Therefore, the energy consumption and energy consumption of model evaluation is expressed as:\n$$E_{eval} = \\frac{MFP \\cdot N_{S,E}}{PU_{performance}}$$\nBy utilizing the same technique as in Eq. 16, the energy consumption per bit of evaluating the model can be expressed as:\n$$E_{eval,b} = \\frac{MFP}{aPU_{performance}}$$"}, {"title": "VII. ENERGY COST OF INFERENCE", "content": "The complexity of making an inference depends on the number of FLOPs required for forward propagating with input sample size NI,P, i.e., Ninf = MFP NI,P. For example, when we adopt the aforementioned MLP model with N\u2081,P = 77, the complexity is calculated as Ninf = 22677 = 17402 FLOPs.\nTherefore, the energy consumption of the forward propagation and the corresponding inference can be calculated as:\n$$E_{inf} = \\frac{MFP \\cdot N_{I,P}}{PU_{performance}}$$\nWe can then observe from Eq. 17 and 18, the cost per bit of inference is the same as in evaluation. Thus, we can conclude that the computational complexity between finishing one train- ing and inference is on the magnitude of 3Nepochs NS,T/NI,P. This indicates that the training process involves significantly more computational operations, particularly when the number of epochs or Ns.T >> NI.P. On the other hand, the energy consumption per bit depends on the model complexity and hardware capability. Moreover, the factor of three increases in the energy consumption per bit during training versus inference highlights the additional computational load inher- ent to training."}, {"title": "VIII. ECAL: THE ENERGY COST OF AIOT LIFECYCLE", "content": "In this section", "follows": "n$$E_D = E_T + E_{storage"}, "E_{pre} + E_{train} + E_{eval},$$\nand the energy consumption per bit that can be expressed as:\n$$E_{D,b} = \\frac{E_D}{B_T + \u03b1(2N_S + N_{S,T} + N_{S,E})}$$\nTo compare the energy consumption of all the data manip- ulation components required to develop and deploy an AloT system, we continue with the guiding example from the paper where the AI model relies on an an MLP with 3 hidden layers (5 neurons each), whereas on the input layer and output layer there are 6, 3 neurons, respectively, and it requests 256 samples from a IoT device via BLE to train and evaluate the model with a 70/30 split as discussed in Section VI. In addition, we utilize HDD for storage and normalization as data standardization as discussed in Section V. The energy consumption for this example is shown in Fig. 9 and reveals that the total energy consumed in this scenario is 0.0264J. Moreover, the energy consumption of a training session is equal to 141 times collecting the same amount of samples from the IoT device, or 71 times of inference. However, these numbers can be significantly higher if the complexity of the model increases or the size of input samples for training is significantly larger than that used for inference as the respective analyses in Sections VI and V showed.\nOnce the model is developed, it is packaged and deployed in the AIoT system and it can produce inference in the form of continuous or discrete outputs. During its operational lifecycle, the model will be presented some input data"]}