{"title": "CORDIAL: Can Multimodal Large Language Models Effectively Understand Coherence Relationships?", "authors": ["Aashish Anantha Ramakrishnan", "Aadarsh Anantha Ramakrishnan", "Dongwon Lee"], "abstract": "Multimodal Large Language Models (MLLMs) are renowned for their superior instruction- following and reasoning capabilities across diverse problem domains. However, existing benchmarks primarily focus on assessing factual and logical correctness in downstream tasks, with limited emphasis on evaluating MLLMs' ability to interpret pragmatic cues and intermodal relationships. To address this gap, we assess the competency of MLLMS in performing Multimodal Discourse Analysis (MDA) using Coherence Relations. Our benchmark, CORDIAL, encompasses a broad spectrum of Coherence Relations across 3 different discourse domains at varying levels of granularity. Through our experiments on 10+ MLLMS employing different prompting strategies, we show that even top models like Gemini 1.5 Pro and GPT-40 fail to match the performance of simple classifier-based baselines. This study emphasizes the need to move beyond similarity- based metrics and adopt a discourse-driven framework for evaluating MLLMs, providing a more nuanced assessment of their capabilities. The benchmark and code are available at: ht tps://github.com/aashish2000/CORDIAL.", "sections": [{"title": "1 Introduction", "content": "The recent advancements in Multimodal Large Language Models (MLLMs) enable them to effectively capture diverse representations of problem domains (Alayrac et al., 2022; Chen et al., 2024c; Pichai, 2024; Liu et al., 2024a). These MLLMs are capable of adapting to various downstream tasks with limited data through Parameter-Efficient Fine-Tuning (PEFT) (Hu et al., 2021) and In-Context Learning (ICL) (Brown et al., 2020) approaches. Existing Vision-based MLLM benchmarks assess different aspects of model performance such as Perception, Cognition, and Reasoning (Li et al., 2024) through various downstream tasks.\nCurrent benchmark design strategies often focus on evaluating the ability of MLLMs to utilize the intersection of input sources to solve a common problem (Kruk et al., 2019). Although this helps assess the model's ability to interpret its inputs factually and logically, it does not fully capture the model's understanding of the relationships be- tween these modalities. Similarly, benchmarks that evaluate the alignment between images and text (Thrush et al., 2022), utilize curated or syntheti- cally generated image-text pairs. These methods focus solely on literal relations that measure the level of overlap between the image and text. On the other hand, pragmatic cues provide information on non-literal relations where the true intent/message of an example may not be directly referenced in both modalities as shown in Figure 1. These cues are leveraged routinely in real-world multimodal discourses, which are characterized by the use of multiple modes of communication to convey dif- ferent components of a message. Multimodal Dis- course Analysis (MDA) studies how the interaction between these different modes can create semiotic meaning (Kress, 2009).\nTo operationalize the assessment of these inter- modal relationships, we turn to theories of Dis- course Coherence (Hobbs, 1978), which offer a way to quantify the organization and flow of ideas across information sources. From these theories, we focus on the concept of Coherence Relations (Alikhani and Stone, 2019), which provides a finite structure to link different parts of a discourse. Re- cent studies have extended these traditionally text- only theories to multimodal discourses, showing that Coherence Relations can be effectively applied to image-text pairs (Alikhani et al., 2020). With Coherence Relations being a fundamental aspect of human communication, we evaluate whether MLLMs can effectively predict and verify these relations.\nIn this work, we propose the CORDIAL (COherence Relations in Discourse for Images"}, {"title": "2 Related Work", "content": "Multimodal Large Language Models MLLMs are fundamentally generative models that com- bine Large Language Models (LLM) (Brown et al., 2020) with multimodal encoders (Dosovitskiy et al., 2021). In recent years, several new MLLMs have been released, based on various proprietary (Ope- nAI et al., 2024; Anthropic; Pichai, 2024) and open-source LLM backbones (Liu et al., 2023; Wu et al., 2024; Bai et al., 2023). These models have shown impressive performance on a variety of downstream reasoning tasks, including Visual Question Answer- ing (Wu and Xie, 2024), Document Analysis (Lv et al., 2023), Embodied AI agents (Shek et al., 2024), etc.\nMLLM Reasoning Benchmarks Recent works that have proposed benchmarks evaluating vision language reasoning, focus on assessing different facets of their input modalities. Visual Reason- ing benchmarks measure the capability of these models to understand spatial and object-level re- lations among image components (Kamath et al., 2023; Rajabi and Kosecka, 2024; Nie et al., 2024; Thrush et al., 2022; Kamoi et al., 2024). Contextual Reasoning benchmarks demonstrate how MLLMS interpret in-context examples and compositional language prompts (Zong et al., 2024; Wu and Xie,"}, {"title": "3 The CORDIAL Benchmark", "content": "3.1 Motivation\nWith Coherence Relations providing a finite repre- sentation of image-text linkages, we aim to mea- sure MLLM performance through relation classifi- cation and verification tasks. Traditional alignment benchmarks often evaluate models using similar- ity scores. But multiple states of alignment be- tween image-text pairs can exist, at the object-level,"}, {"title": "3.2 Coherence Relations", "content": "Each dataset we include in CORDIAL assesses a unique set of Coherence Relations. To under- stand how communication in a discourse can be quantified by Coherence Relations, we turn to the Theory of Coherence (Hobbs, 1978). We define communication as the transfer of information and ideas from a speaker to a listener. For success- ful communication, a discourse needs to satisfy 4 conditions: (1) The message contents should be present in the discourse (2) The message must be relevant to the overall context of the discourse (3) Any new/unpredictable attributes of the message must build on the listener's existing world knowl- edge (4) The speaker must provide cues to guide the listener to graph their intended meaning. The goal of defining Coherence Relations is to serve any of the above-mentioned communicative func-"}, {"title": "3.3 Data Sources", "content": "To construct our benchmark, we leverage existing datasets that provide image-text pairs along with human-annotated Coherence Relations across dif- ferent discourse domains. We select three datasets that offer a diverse set of Coherence Relations: Dis-Rel (Disaster Management), Tweet Subtitles (So- cial Media), and CLUE (Online Articles).\nDisRel This dataset (Sosea et al., 2021) explores the relationship of image-text pairs from disaster-related tweets, with labels collected through crowd-sourcing on Amazon MTurk. The dataset contains 4600 multimodal tweets with a test set size of 500 examples with a 50% split between the two classes:\n\u2022 Similar: The image and text share the same fo- cus and attempt to convey the same message. There exists a significant overlap in the informa- tion conveyed between modalities.\n\u2022 Complementary: The image and text do not share the same focus, but one modality helps understand the other better. Both modalities pro- vide independent information which when com- bined, provide a more complete picture of the"}, {"title": "4 Experiments", "content": "To answer our research questions, we conduct ex- periments on the CORDIAL benchmark with top open-source and proprietary MLLMs. For (RQ1), we evaluate the performance of 12 MLLMs from 9 different model families across our benchmark along with a classifier baseline. The 4 settings in our benchmark are structured with increasing dif- ficulty, with DisRel and Tweet Subtitles being the simpler settings while CLUE Single-Label (SL) and CLUE Multi-Label (ML) are more complex. To answer (RQ2), we pick a selection of MLLMs"}, {"title": "4.1 Models Evaluated", "content": "We evaluate 4 proprietary MLLMs: GPT-40 (OpenAI et al., 2024), Gemini 1.5 Flash (Pichai, 2024), Gemini 1.5 Pro (Pichai, 2024), and Claude 3.5 Sonnet v2 (Anthropic) and 8 open-source MLLMs: LLaVA 1.6 (7B, 13B, 34B) (Liu et al., 2024b), LLaVA OneVision 7B (Li et al., 2025), Qwen2-VL-7B (Wang et al., 2024), Llama 3.2 11B Instruct (Meta AI), Phi3.5 Vision Instruct (Abdin et al., 2024), and InternVL 2.5 26B (Chen et al., 2024b). We selected these model families as they demonstrated acceptable prompt adherence as de- scribed in Appendix Sections B, C. We also include a pre-trained classifier fine-tuned for the task of co- herence relation prediction. We selected GPT-40, Gemini 1.5 Pro, and Claude 3.5 Sonnet v2 as they were among the better-performing MLLMs on our benchmark for verification, with more details pro- vided in Appendix Section D."}, {"title": "4.2 Evaluation Metrics", "content": "On the task of coherence relation prediction, we report the per-class F1 score and overall F1 score"}, {"title": "4.3 Prompting Strategies and Fine-tuning", "content": "In addition to zero-shot evaluation, we also inves- tigate the contribution of few-shot and Chain-of- Thought (CoT) prompting strategies in enabling MLLMs to learn coherence relations better. For few-shot, we include one example per coherence relation in each prompt as examples in the 3 single- label classification settings. For multi-label clas- sification on CLUE ML, we include 6 different examples covering different combinations of rela- tions in our prompt. To perform CoT, we include a reasoning step in our prompt that asks the model to generate a rationale before predicting the coherence relation. More details about the prompt templates used for each of the tasks are present in Sections C.1 and D.1 of our appendix. We fine-tune the Llama 3.2 11B Instruct model on our benchmark to measure the impact of task-specifc fine-tuning in open-source MLLMs with hyperparameter se- lection described in Appendix Section E."}, {"title": "4.4 Main Results", "content": "MLLMs Struggle with Coherence Relations\nFrom our results in Tables 2, 4, 5, 6 we observe that no MLLM shows improvements over our baseline"}, {"title": "5 Conclusions", "content": "We propose CORDIAL, a novel benchmark to evaluate how MLLMs perform MDA using Coher- ence Relations. Our experiments show existing state-of-the-art MLLMs struggle to match simple baseline classifiers in predicting Coherence Rela- tions across different discourse domains. We also show the impact of evaluating different prompt strategies and the importance of using diverse datasets to probe intermodal reasoning capabili- ties of MLLMs. Finally, we show that fine-tuning MLLMs on coherence relations can help allevi- ate model biases and improve their performance on these tasks. This work highlights the need for MLLM benchmarks to evolve beyond factual & perceptual assessment tasks and focus on under- standing both literal and pragmatic relationships between multimodal components of real-world dis- courses. We hope that CORDIAL will serve as a stepping stone for future research in MDA and encourage the community to explore new methods to improve MLLMs on these tasks."}, {"title": "Limitations", "content": "While our proposed benchmark provides a com- prehensive assessment of intermodal reasoning in current MLLMs, several limitations must be ac- knowledged. Firstly, the benchmark is currently limited to analyzing coherence relations in single- turn discourses. This is due to a lack of publically available datasets that provide multi-turn image- text pairs with annotated coherence relations. We plan to extend our benchmark to include multi- turn discourse relations as future work. Secondly, although we analyze different discourse domains in our benchmark, we lack a unified set of coher- ence relations that can be applied across all do- mains. The difficulty in defining a universal set of coherence relations is due to the varying nature of discourse in different domains. This limits our ability to analyze the inter-domain performance of MLLMs on the same set of relations. Finally, our benchmark is currently limited to the English language and must be extended to multi-lingual discourses as well."}, {"title": "A Data Preparation", "content": "This section sheds light on the methods used while preparing all the datasets mentioned in this paper for model evaluation. We verify all three datasets used to construct this benchmark have a permis- sive license that allows usage for research purposes without restrictions (DisRel - MIT License, Tweet Subtitles - MIT License, CLUE - Sourced from Conceptual Captions and free for research use)."}, {"title": "A.1 DisREL", "content": "Due to limited number of samples in the Unre- lated category, these image-text pairs were dis- carded from our train and test set. All placeholder instances of <URL> were removed from the text as a part of our data cleaning."}, {"title": "A.2 Tweet Subtitles", "content": "This dataset contains two types of captions for tweets: actual and text generated by an image cap- tioning model. We use only the actual caption as part of our evaluation."}, {"title": "A.3 CLUE", "content": "The labels other than the ones mentioned in Section 3.3 were disregarded from our train and test set for both settings, due to the lack of examples. We construct the CLUE Single-Label dataset with the same heuristic used by Alikhani et al. (2020):\nStep 1: If the set contains a Meta relation, assign it to the image-text pair. Else, proceed to the next step.\nStep 2: If the set contains a Visible relation and doesn't contain either a Meta or Subjective relation, assign it to the image-text pair. Else, proceed to the next step.\nStep 3: If none of the above rules are met, ran- domly sample one relation from the 5 avail- able, and assign it to the pair."}, {"title": "B Model Availability", "content": "This section focuses on the details of model avail- ability and parameters, that we use in Section 4.1. For all models, we set temperature to 0 or do_sample=False, maximum output tokens to 512 and the random seed set to 42, wherever possible to ensure reproducibility. The model responses in this paper were collected between January 12, 2025 and February 12, 2025."}, {"title": "B.1 Proprietary Models", "content": "OpenAI GPT: We access the GPT-40 model via the official OpenAI API. We evaluate gpt-40-2024-08-06."}, {"title": "C MLLM Evaluation Details", "content": "This section provides details about the evaluation task (RQ1) mentioned in Section 4.1."}, {"title": "C.1 Prompt Templates", "content": "As mentioned in Section 4.3, we make use of Zero- Shot, Few-Shot and Chain of Thought prompting for evaluation. Every prompting strategy utilizes three different messages:\n\u2022 System Message: We explain the task and the definitions of each Coherence Relation present in the dataset being evaluated.\n\u2022 User Message: This message is used to reit- erate the task again, along with the required output format. The image and text that need to be evaluated, is also added here."}, {"title": "C.2 Few Shot Prompting", "content": "In this prompting strategy, we utilize user-assistant message pairs that are inserted right after the user message which specifies output format. For the Tweet Subtitles and CLUE Single-Label datasets, we utilize 5-shot examples to include all possible coherence relations. In the case of CLUE Multi-Label and DisREL, we utilize 6-shot examples and 2-shot examples respectively.\nWe do not evaluate LLaVA 1.6 7B and 13B using this prompting technique, as our prompt (text + multimodal tokens) does not fit into the context length (4096) of these models."}, {"title": "C.3 Chain-of-Thought Prompting", "content": "We instruct the model to analyze the image-text pair, before assigning a Coherence Relation in this prompting strategy. We incorporate the instruc- tion \"Let's think step by step\", to make the model respond with concise sentences that detail its rea- soning process."}, {"title": "C.4 Preprocessing Images for Claude", "content": "We noticed that some images were above the 5 MB per file size limit imposed by Anthropic for their API. As per their recommendations, we evaluate Claude on images that are resized to 1.3 megapix- els, while preserving the aspect ratio."}, {"title": "C.5 Postprocessing MLLM Responses", "content": "In the case of single-label datasets, we remove instances of the phrase \"Coherence Relation:\" along with other punctuation and whitespace. If there exists only one occurrence of a particular coherence relation, we use that as the prediction result for the image-text pair.\nWhile working with CLUE Multi-Label responses, we remove instances of the phrase \"Coherence Relations:\". All valid JSON in the response is parsed using regular expressions. If the output format is comma-separated values, those responses are parsed appropriately."}, {"title": "D MLLM Verification Details", "content": "This section provides details about the verification task (RQ2) mentioned in Section 4.1."}, {"title": "D.1 Prompt Templates", "content": "For this task, we utilize a Chain-of-Thought prompting strategy. Each model is given the same system message as before, but along with the image-text pair, we also give the ground truth Co- herence Relation. The model is then asked to re- spond with a True/False answer, along with its ra- tionale for its response."}, {"title": "D.2 Preprocessing Images for Claude", "content": "We use the same strategy as mentioned in Section C.4, only for the images that don't come under the file size limit."}, {"title": "D.3 Postprocessing MLLM Responses", "content": "We parse boolean values from each MLLM re- sponse, and assign False to an image-text pair, only if there is any occurrence of the same. For CLUE ML, we provide only overall verification accuracies since it is a multi-label verification problem."}, {"title": "E Fine-tuning Details", "content": "We fine-tune LLaMA 3.2 Vision 11B Instruct (unsloth/Llama-3.2-11B-Vision-Instruct in Huggingface) using the Unsloth framework. We opted for this framework due to its memory effi- ciency and rapid fine-tuning. We perform Parame- ter Efficient Fine-Tuning (PEFT) of all layers (Vi- sion & Language) and modules (Attention & MLP) present. We use the hyperparameters mentioned in Section E.1 on each dataset for fine-tuning. Other parameters have been initialized to their default values."}, {"title": "E.1 Hyperparameters", "content": "Common Parameters\n\u2022 LORA Parameters: r=16\n\u2022 num_train_epochs = 3"}, {"title": "E.2 Train Set Preparation for CLUE", "content": "During experimentation, we noticed that models fine-tuned on CLUE Single-Label and Multi-Label, tend to skew their responses towards the majority classes (Visible, Story and Meta) in the dataset. In order to curb this behavior, we decided to randomly sample 200 examples from the CLUE Single-Label train set for these coherence relations alone. The same image-text pairs were used for the multi-label setting as well."}, {"title": "F Baseline Classifier Details", "content": "As mentioned in Section 3.4, we em- ploy CLIP Text and Image Encoders (openai/clip-vit-large-patch14 in Hug- gingface) in a zero-shot manner to extract multi-modal embeddings. These embeddings are then concatenated together, to form a tensor of size 1536. This multi-modal tensor is then passed through a Multi-Layer Perceptron with two hidden layers of size 512 and 256, along with an output layer equal to the number of Coherence Relations in each dataset. The MLP uses RELU in between each layer for introducing non-linearity, and a Dropout of 0.2 between the first two layers.\nA validation split of 10% was created from the train sets. The DisREL, Tweet Subtitles and CLUE Single-Label classifiers were trained using the Cross Entropy Loss, whereas the CLUE Multi- Label classifier used the Binary Cross Entropy Loss along with a Sigmoid Layer. Due to the large"}, {"title": "G Computational Resources", "content": "To evaluate and fine-tune open-source models, we use 2 NVIDIA H100 80GB HBM3 and 2 NVIDIA A100 SXM4 GPUs for around two days worth of computation."}, {"title": "System Message for DisREL", "content": "You are an expert linguist and your task is to predict the Coherence Relations of a given image-text pair. A coherence relation captures the structural, logical, and purposeful relationships between an image and its text, capturing the author's intent.\nThese are the possible coherence relations you can assign to an image-text pair:\nSimilar: The image and text provide the same information and share the same focus. There exists significant overlap in information conveyed between modalities.\nComplementary: The image and text do not provide the same information or share the same focus but one modality helps understand the other better."}, {"title": "System Message for Tweet Subtitles", "content": "You are an expert linguist and your task is to predict the Coherence Relations of a given image-text pair. A coherence relation captures the structural, logical, and purposeful relationships between an image and its text, capturing the author's intent.\nThese are the possible coherence relations you can assign to an image-text pair:\nInsertion: The salient object described in the image is not explicitly mentioned in the text.\nConcretization: Both the text and image contain a mention of the main visual entity.\nProjection: The main entity mentioned in the text is implicitly related to the visual objects present in the image.\nRestatement: The text directly describes the image contents.\nExtension: The image expands upon the story or idea in the text, presenting new elements or elaborations, effectively filling in narrative gaps left by the text."}, {"title": "System Message for CLUE Single-Label and Multi-Label", "content": "You are an expert linguist and your task is to predict the Coherence Relations of a given image-text pair. A coherence relation captures the structural, logical, and purposeful relationships between an image and its text, capturing the author's intent.\nThese are the possible coherence relations you can assign to an image-text pair:\nVisible: The text presents information that is intended to recognizably characterize what is depicted in the image.\nAction: The text describes an extended, dynamic process of which the moment captured in the image is a representative snapshot.\nMeta: The text allows the reader to draw inferences not just about the scene depicted in the image but about the production and presentation of the image itself.\nSubjective: The text provides information about the speaker's reaction to, or evaluation of, what is depicted in the image.\nStory: The text provides a free-standing description of the circumstances depicted in the image, analogous to including instructional, explanatory and other background relations."}, {"title": "Zero/Few Shot Prompt for DisREL, Tweet Subtitles and CLUE Single-Label", "content": "System\n<insert-system-message>\nUser\nBased on provided information, predict the most applicable Coherence Relation for the next image-text pair. Output only one relation (<insert-coherence-relations>) and do not include any other information in your response.\nUse the format \"Coherence Relation: <insert-coherence-relation>\" for your response.\n(Added to finetuned LLaMA 3.2 Vision's prompt in CLUE Single-Label, to enhance output format adherence.)\n<add-few-shot-examples>\n<insert-image-text-pair>\nAssistant\nCoherence Relation:"}, {"title": "CoT Prompt for DisREL, Tweet Subtitles and CLUE Single-Label", "content": "System\n<insert-system-message>\nUser\nBefore assigning a coherence relation, let's think step by step and analyze the image-text pair in depth.\n<insert-image-text-pair>\nAssistant\nAnalysis: <add-analysis-from-model>\nUser\nBased on provided information, predict the most applicable Coherence Relation for the next image-text pair. Output only one relation (<insert-coherence-relations>) and do not include any other information in your response.\nAssistant\nCoherence Relation:"}, {"title": "Zero/Few Shot Prompt for CLUE Multi-Label", "content": "System\n<insert-system-message>\nUser\nBased on provided information, predict the correct Coherence Relations for the next image-text pair. Output them as a JSON value to the key labels\" and do not include any other information in your response. (Default output format for all models)\nGive your predicted labels as comma separated values. Do not include any other information in your response.\n(Alternate output format for LLaMA 3.2, Phi 3.5, Qwen2-VL and LLaVA-OneVision)\nUse the format \"Coherence Relation: <insert-coherence-relation>\" for your response.\n(Added to LLaVA 1.6 13B prompt to enhance output format adherence.)\n<add-few-shot-examples>\n<insert-image-text-pair>\nAssistant\nCoherence Relations:"}, {"title": "CoT Prompt for CLUE Multi-Label", "content": "System\n<insert-system-message>\nUser\nBefore assigning a coherence relation, let's think step by step and analyze the image-text pair in depth.\n<insert-image-text-pair>\nAssistant\nAnalysis: <add-analysis-from-model>\nUser\nNow, using your analysis, predict the correct Coherence Relations for the image-text pair. Output them as a JSON value to the key labels\" and do not include any other information in your response. (Default output format for all models)\nGive your predicted labels as comma separated values. Do not include any other information in your response.\n(Alternate output format for LLaMA 3.2, Phi 3.5, Qwen2-VL and LLaVA OneVision)\nUse the format \"Coherence Relation: <insert-coherence-relation>\" for your response.\n(Added to LLaVA 1.6 13B prompt to enhance output format adherence.)\nAssistant\nCoherence Relations:"}, {"title": "Verification Prompt Template", "content": "System\n<insert-system-message>\nUser\nBased on provided information, reply True (if appropriate) or False (if not appropriate) for the following image-text pair. Give your rationale behind it.\n<insert-image-text-pair>\n<insert-coherence-relation>\nSample Assistant Response\n<True/False>\nRationale: <model-response>"}, {"title": "B.2 Open Source Models", "content": "We evaluate models published on Huggingface Hub. LLaVA 1.6 34B and Llama 3.2 11B Vision were evaluated using the LMDeploy 1 framework. We evaluate Qwen2-VL using code released by the authors. All other models, were evaluated using the VLLM 2 framework. Refer to Table 7 for the models we evaluate."}]}