{"title": "More Questions than Answers? Lessons from Integrating Explainable AI into a Cyber-AI Tool", "authors": ["Ashley Suh", "Harry Li", "Caitlin Kenney", "Kenneth Alperin", "Steven R. Gomez"], "abstract": "We share observations and challenges from an ongoing effort to implement Explainable AI (XAI) in a domain-specific workflow for cybersecurity analysts. Specifically, we briefly describe a preliminary case study on the use of XAI for source code classification, where accurate assessment and timeliness are paramount. We find that the outputs of state-of-the-art saliency explanation techniques (e.g., SHAP or LIME) are lost in translation when interpreted by people with little Al expertise, despite these techniques being marketed for non-technical users. Moreover, we find that popular XAI techniques offer fewer insights for real-time human-Al workflows when they are post hoc and too localized in their explanations. Instead, we observe that cyber analysts need higher-level, easy-to-digest explanations that can offer as little disruption as possible to their workflows. We outline unaddressed gaps in practical and effective XAI, then touch on how emerging technologies like Large Language Models (LLMs) could mitigate these existing obstacles.", "sections": [{"title": "Motivation", "content": "While Al has been applied successfully in several tasks related to cybersecurity (e.g., flagging emails as potential spam [2]), its use in tasks that are primarily driven by human operators requires a sufficient degree of interpretability before it can be adopted for real-world use [7]. Srivastava et al. provide an overview of some of these challenges for Al adoption in the cyber domain [9].\nFor cyber operations specifically, the use of Explainable Al (XAI) presents a unique challenge, as the end users of Al decision-support tools are not necessarily Al experts; they tend to be highly skeptical about Al altogether [10]; and the system behaviors they analyze (e.g., network or software behavior) are highly context dependent.\nAs a result, a motivating question for the Human-Centered XAI community is: Do current XAI techniques effectively support users in cyber-related analysis tasks, and are there significant remaining challenges that need to be addressed?\nWe reflect on these questions following a preliminary study into improving interpretability for a source code classifier using model-agnostic, local explainers."}, {"title": "Case Study: XAI for a Source Code Classifier", "content": "We are broadly interested in understanding how to design effective Al-driven decision support for cyber analysts whose jobs are to test software systems, identify issues, and decide on appropriate courses of action. Here, our goal is to understand whether off-the-shelf, widely-used XAI techniques provide useful transparency to a neural network classifier (the \"model\"). While we did not create this model, we were tasked to illuminate how XAI can (or cannot) support explaining this model's classification of source code artifacts. The model was trained to read a collection of Python source code files and classify them as either \"Yes, this Python file has code that implements ML\" or \"No, this Python file does not have code that implements ML.\" The purpose of the model was to assist analysts in identifying structures in the source files that may need additional testing. It also predicts the presence of ML sub-types in the implementation. \nWe integrated SHAP [5] and LIME [8] into a decision-support web application to explain the model behavior on instances of source code. In particular, we used SHAP and LIME to compute values for highlighting within a Python file, indicating whether a particular word or set of words from the file contributed to or contradicted the model's prediction.\nSHAP and LIME are widely marketed as interpretable solutions to explain black box models [3], in particular to discover comparable local feature-importance values (e.g., saliency) for a black-box model. We used both explainers to provide some redundancy, and to investigate how agreement and conflict between the methods are understood by users. Moreover, we hoped that by situating the saliency values into the domain itself (i.e. visually highlighting discriminatory text in source code files) then end-users would have a better understanding of the explanations. We found overall that end-users struggled to comprehend the explanations provided by SHAP and LIME, we discuss this challenge later. Consequently, integrating additional explainers (other gradient methods [6], counterfactual explanations [11], etc.) that have shown promise for related applications is an essential next step."}, {"title": "Positive Observations", "content": "Explainability is necessary for cyber. We find that both stakeholders and users consider it helpful to have any kind of explanation provided for a model's outputs, increasing confidence in the use of Al altogether.\nExplanations help analysts understand a model's behavior. We find that users were capable of pointing out interesting behaviors of the classifier based on the outputs of SHAP and LIME. For example, one user asked us, \"should the model really be using words like def or print for its prediction?\"\nXAI bridges gaps in Al expertise, creating a dialogue. As SHAP and LIME provide users some transparency in what keywords the classifier is learning for its decision making (e.g., the use of numpy for classifying a file as implementing ML), added explainability can open up a dialogue for non-Al experts (e.g., cyber analysts) and the developers of these models during collaborative workflows."}, {"title": "Top Challenges", "content": "Explainers contradict each other, promoting distrust and confusion by users. Indisputably, the biggest challenge we ran into was the disagreements between explainers. In some cases, SHAP would highlight particular tokens as strongly supportive, while LIME would highlight those same tokens as highly contradictory.\nXAI, on a conceptual level, is still confusing to users and experts alike. Analysts and developers expressed confusion between the distinction of the model itself and the explanation outputs. After clarifying this to cyber analysts, they tended to expect that the outputs of SHAP and LIME would be responsible for 'retraining' the model and correcting the classifier's mistakes, without human intervention.\nOff-the-shelf XAI options are insufficient for cyber. We find that the post-hoc and localized nature of XAI techniques for black-box models leave users dissatisfied. They stressed the need to understand a model's outputs in-situ, and on data the model had not been seen before. Moreover, users expressed the need for higher-level interpretations of the explanations, as to not interrupt their workflows.\nHigher-level visual abstractions are needed for better end-user interpretability. We found that manually inspecting the highlighted tokens provided by SHAP and LIME for each source code file is a burdensome task for analysts. Importantly, the typical visualizations provided by explainers require a high level of visual literacy, insufficient for our user group. Future work must address how higher-level visual abstractions - perhaps an ensemble-based approach to illustrate multiple factors or features - can be used to reduce time spent interpreting explanations.\nBroader Reflections for Human-Centered X\u0391\u0399. We ought to be more transparent in the actual expertise required for interpreting explainers. Common XAI techniques like SHAP and LIME are consistently cited as methods that \"promote trust and understanding\" for stakeholders, decision-makers, and \u201cnon-technical\" end-users (e.g., [3]). However, we observed (as have others in the HCXAI community) that these techniques are insufficient for actual \"non-technical\" end-users.\nOur community should work towards understanding why this misconception continues to be perpetuated, and how we can mitigate it. Is it because techniques like SHAP and LIME are the most widely available to model developers? Is it because they are actually the best we can do to ex-"}, {"title": "When explanations leave users with more questions than answers, can we leverage dialogue systems?", "content": "The current state of commonly-used explainers like SHAP and LIME will likely leave end-users with more questions than answers. This could present an opportunity for us to incorporate additional tools, like conversational agents, that help users interpret the outputs of these XAI methods. Emerging generative Large Language Models and Vision-Language Models (LLMs and VLMs) could soon facilitate question-answering about models and explanations.\nImportantly, future research into XAI-focused conversational agents should consider the essential human factors needs of the particular domain the model is deployed for. For example, in a high-stakes, high-risk environment like cybersecurity, \"the simple classification result is not the essential information that the [operator] requires, instead, they need to understand more about the threat and the reason for it to be treated as [such].\" [9]\nBeyond interpreting explanations, it is also possible that an LLM could participate in a back-and-forth dialogue with an end-user to understand model requirements that will lead to acceptance. For example, an LLM could facilitate questions like \"how much risk are you willing to take on with this model?\" or \"how much time are you willing to spend interpreting the model's outputs?\" Illuminating the answers to these questions can inform downstream modeling and development, and potentially alleviate the burden of this back-and-forth for data scientists.\nAn essential unknown that will need to be addressed before the incorporation of these tools is how an LLM might output false causalities, correlations, or explanations when answering a user's questions (often referred to as 'hallucinations')."}]}