{"title": "Automatic Feature Learning for Essence: a Case Study on Car Sequencing", "authors": ["Alessio Pellegrino", "\u00d6zg\u00fcr Akg\u00fcn", "Nguyen Dang", "Zeynep Kiziltan", "Ian Miguel"], "abstract": "Constraint modelling languages such as ESSENCE offer a means to describe combinatorial problems at a high-level, i.e., without committing to detailed modelling decisions for a particular solver or solving paradigm. Given a problem description written in ESSENCE, there are multiple ways to translate it to a low-level constraint model. Choosing the right combination of a low-level constraint model and a target constraint solver can have significant impact on the effectiveness of the solving process. Furthermore, the choice of the best combination of constraint model and solver can be instance-dependent, i.e., there may not exist a single combination that works best for all instances of the same problem. In this paper, we consider the task of building machine learning models to automatically select the best combination for a problem instance. A critical part of the learning process is to define instance features, which serve as input to the selection model. Our contribution is automatic learning of instance features directly from the high-level representation of a problem instance using a language model. We evaluate the performance of our approach using the ESSENCE modelling language with a case study involving the car sequencing problem.", "sections": [{"title": "1\nIntroduction", "content": "In many domains, it has long been observed that there is no single algorithm that performs best on all problems or even on all instances of the same problem [39, 29, 27]. To solve difficult computational problems effectively, it is often beneficial to utilise a portfolio of algorithms with complementary strengths. This gives rise to the field of Automated Algorithm Selection (AAS), where the aim is to automatically select the best algorithm(s) from an algorithm portfolio for a given problem instance. Over the last few decades, AAS has been shown to be very successful in various applications across a wide range of domains, including Boolean Satisfiability (SAT) [47], Constraint Programming (CP) [37, 33], AI planning [45], and combinatorial optimisation [30].\nIn the CP domain, an algorithm can be seen as a constraint solver (or a specific parameter configuration of a solver). Several studies have demonstrated complementary strengths of constraint solvers [17, 16] and the advantage of using them in combination in a portfolio setting [37, 8, 9]. However, the concept of a CP algorithm can be extended beyond the scope of a constraint solver, which often works on a low-level representation of a problem."}, {"title": "2\nBackground and Related Work", "content": "Constraint Modelling Tools To facilitate the modelling phase of combinatorial problems in CP, several domain-specific languages have been developed. Notable among these are MiniZinc [35] and ESSENCE [21]. ESSENCE is a high-level language designed to abstract problem modelling using a blend of natural language and discrete mathematics. This abstraction addresses the challenging nature of problem modelling, which demands expertise and domain-specific knowledge. CONJURE [2], a tool designed for ESSENCE, incrementally refines an initial ESSENCE model into ESSENCE PRIME, a lower level solver-independent constraint modelling language [36], through a series of transformations. Non-trivial transformations may yield multiple effective refinements, resulting in a portfolio of models with varying performances depending on the specific instance and solver used. This creates a complex landscape for selecting the optimal algorithm (ESSENCE PRIME model and solver combination).\nML for Algorithm Configuration and Selection Algorithm configuration is a field focused on optimizing the hyperparameters of an algorithm to enhance its performance based on criteria such as speed, memory usage, or accuracy. This process is essentially a search problem within the hyperparameter space, evaluated against a set of training instances [38]. Complementary to this is the field of algorithm selection, which involves choosing the best-performing algorithm from a portfolio of pre-tuned options to solve a specific problem instance [32]. Both algorithm configuration and selection often leverage ML techniques to inform their decision-making processes.\nML algorithms like random forests [12] and support vector machines [43] are particularly effective at identifying patterns in input features to predict optimal output, making them well-suited for these tasks. An ML algorithm takes as input a set of data points represented by a set of input features and their corresponding desired output (dataset). The initial dataset is analyzed by the algorithm that produces an ML model designed to address the desired task with a certain degree of correctness in the output. Essentially, an ML model is a function approximation from the feature input space to the desired output space. The efficiency of ML models in algorithm selection has been demonstrated in numerous applications [32, 47].\nNeural Networks and Language Models Neural Networks (NNs) represent a powerful paradigm within ML, renowned for their ability to learn complex patterns from large datasets. They are particularly adept at generating features from textual input data [20], which simplifies the creation of ML models. Since the introduction of AlexNet in 2012 [31], NNs have been successfully applied to a wide array of tasks, such as image classification [41], text classification [46], robotics [14], and environmental science [34].\nRelated Work. Many AAS tools have been proposed to tackle CSPs. Most notably, SUNNY [33] and CPHydra [13] use a k-NN approach to compute a schedule of solvers which maximizes the chances of solving an instance within a given timeout, while Proteus [23] is a hierarchical portfolio-based approach to CSP solving that does not rely purely on CP solvers: it may choose a SAT solver along with an accommodating CSP-to-SAT translation to solve an instance. Moreover, AAS tools designed for SAT problems can be easily adapted to tackle CSPs (and vice-versa). An empirical evaluation of different AAS approaches for solving CSPs (including SAT portfolios) can be found in [5] and [7], which show empirical comparisons between SUNNY and AAS approaches originally proposed for SAT scenarios, such as 3S [26] and SATzilla [47].\nLanguage models have previously been applied in CP to generate models from natural language problem descriptions [44, 4]. NNs have been used to learn features from the raw trajectories of search algorithms for selecting heuristic algorithms in bin packing problems [3]."}, {"title": "3\nMethodology", "content": "Recall that given a problem class instance written in ESSENCE and a set of constraint solvers, we can generate a portfolio of algorithms for the instance, where each algorithm is a combination of an ESSENCE PRIME model and a solver. The aim of our AAS task is to build a prediction model to select from the portfolio the best algorithm (with shortest runtime) for the instance. This task involves two key steps: (i) learning features representing a given ESSENCE instance from its raw text content; and (ii) using the learnt features to predict the best algorithm.\nTo address the first step, we propose to employ a Neural Network (NN) that encapsulates a language model to deal with text input. This approach has many advantages. First, language models like Bert have been proven effective in capturing high-level language features [20], eliminating the need to run a solver to extract the necessary features. Second, N\u039d models can automatically generate the necessary features by starting from the raw input. This eliminates the need for handcrafting an effective feature set.\nFor the second step, we consider different options. A possibility is to combine the two steps and address the entire AAS task using a single NN. In this case, the probability associated to an algorithm by the NN indicates its likelihood of being the best and thus the one with the highest probability is deemed as the best. Another possibility is to detach the second step from the first and adopt an ML-based algorithm selector. This gives flexibility in the algorithm selection method, allowing us to leverage state-of-the-art tools as well as to experiment with others. In this case, the probability associated with an algorithm indicates its likelihood to be competitive (that exhibits good performance on the given instance). The algorithms along with the produced features are then given as candidates to the algorithm selector which then decides the best one. Both approaches are depicted in Figure 1, the details of which are explained in the following subsections."}, {"title": "3.1 Feature Learning Using a Language Model", "content": "We adopt a language model, a particular NN architecture, to learn a set of features that will be later used to select an algorithm in both options mentioned previously. The input of such a model is the raw text of the ESSENCE instance in tokenized form (where each input word and symbol are transformed into a number), and the output is a feature vector that describes the semantic meaning of the input. In particular, we use an 8-bit-quantized [48] version of Longformer[10]. This is a Bert-like [20] architecture whose main advantage is the larger input size (2048 tokens instead of 512) and it has been proven competitive for a fine-tuning task [28]. In addition to the language model, the NN encapsulates a linear layer to process the features and produce an output. The linear layer comprises a neuron for each of the possible algorithms to choose from. Each neuron receives as input the feature vector produced by the language model. Then it computes the dot product with the learnt weights and adds a bias. The final result is a floating point value for each neuron.\nThe main difference between the network of the first method (entirely NN-based) and the second one (hybrid of NN and ML-based algorithm selector) lies in the activation function that can transform the output of the linear layer from floating-point values into probabilities to better interpret the NN output. In the entirely NN-based approach, we want to learn a probabilistic distribution which has, as the most probable value, the best algorithm to choose. To achieve this output, we use the softMax activation function that transforms the input sequence into a probability distribution. In the hybrid case instead, we train the NN on a multi-label classification task, where the output comprises probabilities for each algorithm, indicating their competitiveness fraction. A higher probability suggests that the algorithm is less likely to be competitive. We consider an algorithm to be competitive if it solves an instance in less than ten seconds or in less than double the time taken by the best-performing algorithm for that instance. For example, if the best algorithm takes 15 seconds, any algorithm that completes the task in under 30 seconds is deemed competitive. To obtain such output, we use the sigmoid activation function which transforms each input value to a proper fraction, depending on its magnitude."}, {"title": "3.2 Algorithm Selection Using the Learnt Features", "content": "Once the NN is trained, the best algorithm for a given ESSENCE instance is chosen based on the probabilistic NN output. In the entirely NN-based approach, it is the one with the highest probability. In the hybrid approach, the probabilistic NN output is fed as input to an ML-based algorithm selector.\nAs an algorithm selector, we can rely on well-known methods such as Autofolio [32] and K-means clustering [1]. The first is a state-of-the-art tool that tunes the underlying model and its hyperparameters to optimize the performance. It can be used both for classification and regression tasks. The second is a clustering algorithm that assigns a cluster to a new instance. As features, these methods can exploit both the language model output and the probabilistic NN output. The features derived from the language model would be useful because they are trained on a similar task, capturing the general semantic structure of the instance. Whereas, the linear layer output indicates which algorithms are most likely to perform competitively. By combining the two, the features can encapsulate both a broad semantic representation of the instance and a specific prediction of the algorithms most likely"}, {"title": "4\nA Case Study with the Car Sequencing Problem", "content": "We evaluate the performance of our approach to AAS using the ESSENCE modelling language with a case study involving the car sequencing problem. In this section, we describe the case study. We start with the problem description in ESSENCE and the instance set employed in the evaluation. We then present the combinations of (low-level) ESSENCE PRIME models produced by CONJURE and constraint solvers, giving rise to a portfolio of algorithms to choose from. Finally, we describe how we obtain a dataset starting from the instance set and the algorithms, and discuss its suitability for an AAS task."}, {"title": "4.1 Problem Description and Instance Set", "content": "A series of cars are scheduled for production, each varying due to the availability of different optional features. The assembly line consists of various stations that install these options, such as air conditioning and sunroofs. Each station is designed to handle only a specific percentage of the cars passing through. To ensure that the workload at each station remains manageable, cars requiring the same option must be distributed evenly along the assembly line; clustering of these cars must be avoided to prevent overwhelming any single station. Therefore, cars must be sequenced so that the capacity of each station is not exceeded. For example, if a particular station can only manage a maximum of 50% of the cars passing through, the sequence must ensure that at most one car in every two requires that option. This sequencing problem is known to be NP-complete [22]. An ESSENCE model for this problem is shown in Figure 2.\nThe ESSENCE model defines three integer parameters n_cars, n_classes, and n_options representing the number of cars, classes of cars, and options available, respectively. Using these, three integer domains are defined: Slots, Class, and Option. These domains are used when defining further parameters and decision variables in the model as well as in constraint expressions. Three parameters with function domains are defined to represent the quantity of each class of car required, a maximum number of cars (maxcars) that can appear in any block of cars, and block size (blksize) for each option. The usage parameter is a relation that indicates which classes use which options.\nThe only decision variable (car) in the model is a mapping from car production slots to classes. The problem constraints are captured in two top-level constraints (denoted by the keywords such that). The first set of constraints ensures that the number of cars in each class matches the required quantity. The second set of constraints ensures that for each option, in"}, {"title": "4.2 Combinations of Models and Solvers", "content": "Our algorithm portfolio contains three alternative ESSENCE PRIME models and four state- of-the-art solvers. The solvers are Kissat, Chuffed, CPLEX, and OR-Tools CP-SAT, each chosen for their potential complementary characteristics in combinatorial optimization. Kissat [11] is a modern clause-learning Satisfiability (SAT) solver. Chuffed [15] is a Constraint Programming (CP) solver enhanced with clause learning. CPLEX [25] is a commercial Mixed- Integer Programming (MIP) solver that excels in solving problems that heavily use arithmetic constraints. OR-Tools CP-SAT 3 is a hybrid solver developed by Google that integrates clause learning, CP-style constraint propagation, and MIP solving methods.\nWe use SAVILE ROW [36] to target these solvers. SAVILE Row is a modelling tool that converts problem models written in ESSENCE PRIME into the input format required by these solvers and optimises the models based on the characteristics of the specific instance being solved. The ESSENCE PRIME models are obtained using CONJURE [2] in its portfolio mode, with variations arising from different representations for the car decision variable and the usage parameter, as well as the way problem constraints are formulated.\nThe car decision variable has two possible representations. The first is a one-dimensional array indexed by cars, containing decision variables with integer domains, where each entry represents the class selected for that car. The other is a two-dimensional Boolean array, indexed by both cars and classes, where a true value indicates the assignment of a car to a class. The usage parameter also has two possible representations: a two-dimensional Boolean"}, {"title": "4.3 Dataset and Algorithm Complementarity", "content": "The combination of three ESSENCE PRIME models and four constraint solvers results in a total of 12 algorithms. To perform the AAS task, we create a dataset by running the algorithms on the 10,214 car sequencing instances and record their runtime. The runtimes are measured on a computer with an AMD EPYC 7763 CPU, where each algorithm is given one CPU core and one hour of cut-off time per instance. We define the overall performance of an algorithm on a given instance set as the average runtime required to solve all the instances. To account for cases where an algorithm does not produce an answer within the given cut-off time, we adopt the Penalised Average Runtime (PAR10) metric from the AAS literature [32], where unsolved instances are penalised as 10 times the cut-off time. AAS techniques aim at minimising the PAR10 score.\nTo establish the potential of AAS in this case study, we analyze the performance of each algorithm on the instance set. Figure 3 shows the PAR10 score of the algorithms as well as the Virtual Best Algorithm (VBS), defined as the (hypothetical) algorithm selector that always correctly chooses the best algorithm for each instance. We see that, there is no model (resp. solver) that alone is always the best or worst independently of the coupled solver (resp. model). While M2 is fastest with Chuffed, for M\u2081 it is OR-Tools, and these combinations are the two best algorithms. Even though M3 has a much worse score with all the solvers, it does not take part of the worst algorithm, which is M2-CPLEX. Except for the four algorithms involving M3, they all exhibit different performances. Another observation is the big gap between the VBS and the best overall algorithm (M2-Chuffed). We can therefore conclude"}, {"title": "5\nExperimental Evaluation", "content": "Having established the potential gain of AAS in the car sequencing case study, in this section, we experimentally evaluate the effectiveness of our approach.\nThe research questions (RQs) that we aim to answer in the evaluation are:\nRQ1: Can we learn an effective AAS model when combining feature learning and algorithm selection in a single NN model, or do we need to split the learning into two phases (as depicted in Figure 1)?\nRQ2: How do the learnt features perform on the AAS task compared to the existing fzn2feat features?\nRQ3: What is the feature extraction cost of the learnt features compared to the existing fzn2feat features?\nWe first describe in Section 5.1 how we trained the NN models and then present our study on each RQ in the subsequent sections."}, {"title": "5.1 Neural Network Training", "content": "All NN models are trained on a GPU with Nvidia A5000 accelerator. We trained each NN using a 10-fold cross-validation technique. At each fold, 10% of the dataset was used as a test set while the remaining 90% was split into training (90%) and validation (10%). For the approaches where the feature learning and algorithm selection are conducted separately, the same data split is used for the ML-based algorithm selector, therefore, if an instance was in the test set of the NN, it was also in the test set of the ML model that used the extracted features. Each network is trained for 10 epochs. For each fold, it took 57,328 seconds, which is around 15.9 hours, to complete the training of each network.\nFor the entirely NN-based approach where feature learning and algorithm selection are in a single NN model, the training is done using the typical cross entropy loss function for multi-class classification tasks. For the hybrid approach where the NN output is based on algorithm competitiveness, for the first 3 epochs, we used a learning rate of le-4 and, as a loss function, a weighted version of the Binary Cross-Entropy (BCE) loss that prioritised recall over precision. The formula of the weighted BCE loss function on each sample is shown in Equation (1), where $n$ is the number of algorithms and $y_i$ and $\\hat{y_i}$ are the true and the predicted binary labels, indicating whether algorithm $i$ is competitive or not. The first term in this formula represents the recall metric and is weighted twice over the second term.\n$L_{BCE}(y,\\hat{y}) = - \\frac{1}{n} \\sum_{i=1}^{n} [2y_i log(\\hat{y_i}) + (1 - y_i) log(1 - \\hat{y_i})]$  (1)\nFor the next 6 epochs, we dropped the custom weights to use the normal BCE loss. The only notable change between epochs 3 to 6 and 6 to 10 was the change of learning rate that was le-4 for epochs 3 to 6 and le-5 for the final 4 epochs. For the whole training process, we used stochastic gradient descent as an optimizer for the model.\nWe leave as future work a more systematic study of which training schedules and hyper- parameter configurations are best suited to our task. The current decision is based on a small manual tuning study. The intuition behind splitting the training into different phases is as follows. At the first stage of the training process (the first 6 epochs), we prioritise recall over precision. If an algorithm is not competitive but is predicted as so, it may be incorrectly chosen by the algorithm selector and could potentially result in a larger performance loss (in PAR10 score), therefore, the first term in Equation (1) is weighted higher to emphasise it."}, {"title": "5.2 Feature Learning and Algorithm Selection: Combining vs Splitting", "content": "In this section, we investigate RQ1: Can we learn an effective AAS model when combining both feature learning and algorithm selection in a single NN model, or do we need to split the learning into two phases (as depicted in Figure 1)?"}, {"title": "5.3 Learnt Features vs fzn2feat", "content": "In this section, we investigate RQ2: How do the learnt features perform on the AAS task compared to the existing fzn2feat features?\nOur learnt features are the concatenation of the language model output and the prob- abilistic output of the NN, as illustrated in the bottom part of Figure 1. As an ML-based algorithm selector, we adopt AutoFolio [32] and K-means clustering [1], as mentioned in Section 3.2. With these selectors, we can use either our NN-based or the fzn2feat features. We refer to the four possible combinations as NN-Autofolio, fzn2feat-Autofolio, NN-Kmeans, and fzn2feat-Kmeans.\nIn addition to the algorithm selectors named above, our feature learning method offers other possibilities for algorithm selection. As a by-product of the feature learning process, we have a prediction model that tells us which algorithms are less competitive (with probability less than 0.5) for a given instance. As described in Section 3.2, this information can be used to filter out the less-promising algorithms for that particular instance. Among the remaining ones, we can select the best algorithm based on a specific criterion (measured on the training set), such as the PAR10 score or the number of instances where the algorithm wins. We refer to these simple selection approaches as NN-based Single Best Selection (NN-SBS) and Winner Selection (NN-WS), respectively."}, {"title": "5.4 Feature Extraction Cost", "content": "In this section, we investigate RQ3: What is the feature extraction cost of the learnt features compared to the existing fzn2feat features?\nAs indicated in Table 1, a significant advantage of the NN-based approach is the time"}, {"title": "6\nConclusions", "content": "In this paper, we explored the use of automatic feature learning for algorithm selection in the context of the car sequencing problem, leveraging the high-level constraint modelling language Essence. Our approach employed a language model to learn instance features directly from the problem descriptions, which were then used to predict the best algorithm for solving each instance.\nOur experiments demonstrated that the learnt features could effectively be utilized within two different algorithm selection strategies (AutoFolio and K-means clustering). Both strategies showed promise, but each had its own strengths and weaknesses. The tuning experiments with AutoFolio highlighted the importance of careful feature set selection and tuning, especially given the high dimensionality of the learned features.\nDespite these challenges, our results indicate that NN-based feature extraction offers a viable and efficient alternative to traditional methods, with significantly lower computational costs for feature extraction. However, the instability observed in the performance of tuned AutoFolio with NN-based features suggests further refinements are necessary. Future work could involve developing an NN-based algorithm selection approach tailored to handle high-"}]}