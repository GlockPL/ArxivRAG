{"title": "Scaling Parameter-Constrained Language Models with Quality Data", "authors": ["Ernie Chang", "Matteo Paltenghi", "Yang Li", "Pin-Jie Lin", "Changsheng Zhao", "Patrick Huber", "Zechun Liu", "Rastislav Rabatin", "Yangyang Shi", "Vikas Chandra", "AI at Meta", "Iowa State University", "Virginia Tech"], "abstract": "Scaling laws in language modeling traditionally quantify training loss as a function of dataset size and model parameters, providing compute-optimal estimates but often neglecting the impact of data quality on model generalization. In this paper, we extend the conventional understanding of scaling law by offering a microscopic view of data quality within the original formulation - effective training tokens \u2013 which we posit to be a critical determinant of performance for parameter-constrained language models. Specifically, we formulate the proposed term of effective training tokens to be a combination of two readily-computed indicators of text: (i) text diversity and (ii) syntheticity as measured by a teacher model. We pretrained over 200 models of 25M to 1.5B parameters on a diverse set of sampled, synthetic data, and estimated the constants that relate text quality, model size, training tokens, and eight reasoning task accuracy scores. We demonstrated the estimated constants yield +0.83 Pearson correlation with true accuracies, and analyzed it in scenarios involving widely-used data techniques such as data sampling and synthesis which aim to improve data quality.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in language model (LM) development have been significantly influenced by the exploration of scaling laws, which articulate the relationship between training loss, dataset size, and the number of model parameters (Hestness et al., 2017; Kaplan et al., 2020; Aghajanyan et al., 2023). These scaling laws have been instrumental in predicting the computational resources necessary for training increasingly large models and have provided a framework for understanding how model performance scales with data and parameters (Hoffmann et al., 2022; Kaplan et al., 2020). However, these laws primarily focus on the quantity of data and model size, often underestimating the critical role of data quality in model generalization.\nIn this work, we challenge the prevailing focus\u00b9 on merely increasing data volume and model size by emphasizing the importance of data quality, particularly in scenarios constrained by the number of model parameters. We argue that for sub-billion parameter models, the quality of data \u2013 or what we term as effective training tokens \u2013 plays a more decisive role in model performance than previously recognized. This perspective shifts the paradigm from a quantity-centric view to a quality-centric approach in the development of language models.\nFurther, we provide qualitative measures of standard data refinement techniques including data sampling (Penedo et al., 2023; Wang et al., 2024; Albalak et al., 2024) and text synthesis (Liu et al., 2024), applied to a pretraining corpus such as RefinedWeb (Penedo et al., 2023). This helps to formulate the relationship between the diversity and syntheticity of pretraining data in order to compute the number of effective training tokens, which evaluate the impact of data quality in terms of model size and the token number. Further, we conduct extensive experiments across eight different benchmarks to evaluate the impact of data refinement techniques which allow us to significantly outperform models trained on randomly selected data samples, across a spectrum of model sizes ranging from 25 million to 1.5 billion parameters.\nBy integrating the notion of effective token size into the scaling law formulation, we extend the existing scaling law formulation to better capture the nuances of data quality. Our results underscore the pivotal role of high-quality data in training efficient and powerful language models, particularly in"}, {"title": "2 Background", "content": "Chinchilla scaling law (Hoffmann et al., 2022) provides a predictive framework for estimating model training loss, considering the number of training tokens and model parameters. Initially designed to identify optimal compute settings for extensive pretraining\u2014a costly and time-consuming endeavor these laws are crucial for optimizing computational resources. Recent studies by Abbas et al. (2023); Liu et al. (2024); Goyal et al. (2024) emphasize the pivotal role of data quality in model pretraining, underscoring the need for revising scaling law formulations.\nOn the other hand, data refinement can be categorized into non-transformative and transformative types (Zhao et al., 2023). Non-transformative refinements involve selective curation of data samples without altering their core characteristics. In contrast, transformative refinements generate new text data, rearranging and introducing new tokens, thus impacting training token distributions and data quality. This significantly affects the effective number of training tokens used in model training.\nIn non-transformative refinements, data deduplication is essential for preventing model generalization issues by removing duplicate documents (Lee et al., 2022; Penedo et al., 2023; Tirumala et al., 2024). This process not only reduces the number of training tokens but also enhances the quality and effectiveness of the remaining tokens, improving model performance (Muennighoff et al., 2024; Lee et al., 2022). Data selection, another non-transformative method, involves choosing an optimal data subset from a larger corpus for model training. Both approaches aim to enhance model performance, reduce computational costs, and maintain evaluation metric integrity (John and Draper, 1975; Murphy, 2012).\nTransformative refinements, such as synthetic data generation through instructional prompts, are becoming popular (Long et al., 2024; Chung et al., 2023; Ding et al., 2024). This approach creates new data to fill existing dataset gaps or introduce new learning scenarios. Integrating synthetic data into large-scale pretraining has significantly improved model robustness and generalization (Li et al., 2023; Maini et al., 2024; Liu et al., 2024). Synthetic data generation allows for controlled training dataset expansion, ensuring exposure to diverse inputs and scenarios (Adler et al., 2024).\nGenerally, data refinements are crucial in shaping the training landscapes of modern machine learning models, directly influencing training token distribution and quality, thereby enhancing training efficiency and effectiveness in line with scaling laws (Adler et al., 2024)."}, {"title": "3 Formulating Data Quality", "content": "Here we adopt two popular metrics to measuring text quality that are easy to compute on large-scale pretraining data, which is an important considerations when measuring data quality of pretraining sets.\nDiversity: Following Shaib et al. (2024), we utilize the compression ratio, which has been demonstrated to be effective for large-scale pretraining datasets and correlates well with other diversity metrics (Figure 4). Past metrics generally quantify the number of repeated substrings across outputs. Among these, the token-type ratio is calculated by dividing the count of unique tokens by the total number of tokens in a text. To capture the lexical dynamics across varying text lengths, the moving average token type ratios (MATTRS) were introduced, providing a robust measure that is insensitive to text length (Covington and McFall, 2010). This metric focuses on the frequency of individual word repetition within text segments and does not account for longer repeated sequences.\nTo address longer sequences, the concept of token-type ratio has been expanded through the introduction of n-gram diversity, as explored in"}, {"title": "4 Scaling Law with Data Quality", "content": "We propose to modify the third approach of the Chinchilla scaling law (Hoffmann et al., 2022) which originally models the losses in training large language models with the functional form $E+\\frac{A}{N^{\\alpha}} + \\frac{B}{D^{\\beta}}$ with the constants: (E = 1.89, A = 463.3, \u03b1 = 0.345, B = 12530, \u03b2 = 0.452). In this formulation, (E) represents the baseline loss, akin to the entropy of natural text under an ideal generative process, setting the theoretical minimum loss achievable with data D and model parameter N.\nIn this work, we model the zero-shot accuracy on common sense reasoning as we postulate that the score provides an indication on how much reasoning ability a given data D could possibly instill. To incorporate data quality into this framework, we propose to use a quality term Q to provide a quality-adjusted number of training tokens (Dq), combining Eq. 1 and Eq. 2:\n$Dq = D \\cdot exp(c\u2081 \\cdot diversity + c2 \\cdot syntheticity)$\n$= D \\cdot exp(c\u2081\\cdot Dr(D) + c2 \\cdot S(D))$\n(3)\nScaling factor Q\nwhere (c1) and (c2) are scaling factors that adjust (Dq) to account for the syntheticity and diversity of"}, {"title": "5 Data Refinement: A Case Study", "content": "We explore two prevalent data refinement techniques aimed at enhancing data quality: data selection and data synthesis. These methods have become standard practices in the preparation of pretraining datasets, significantly influencing text diversity and syntheticity and downstream performance as shown in various studies (Abdin et al., 2024; Albalak et al., 2024).\nTo put them in context, we present a comparative analysis in Figure 2, which displays the relationship between effective token counts Dq and the total number of tokens D. It clearly demonstrates that data synthesis has a more substantial impact on increasing the effective token count compared to data selection and the use of original datasets. This underscores the value of synthesis in optimizing data quality for model training."}, {"title": "5.1 Data Selection", "content": "Coreset Selection. One way to create a higher quality dataset is via importance sampling (Xie et al., 2023; Wang et al., 2018), which transformed input data into n-gram based feature vectors and compares the feature distributions between the raw and target datasets and assigning importance weights to each example.\nThis selectively enhance the dataset's syntheticity and directly influenced the Dq term in the revised scaling law, increasing the syntheticity factor without compromising on diversity. While this approach assumes the knowledge of target applications, but it also allows us to easily explore the impact of having more in-domain data on the data quality and losses.\nText Deduplication. An orthogonal approach is text deduplication (Sorscher et al., 2022; Penedo et al., 2023, 2024) which removes redundant data, ensuring a balanced dataset that does not favor frequently occurring examples. This method modulates the diversity and quality of the dataset, which is crucial for robust model training. The deduplication process effectively controlled the Dq term by filtering out excessive redundancy, which could lead to overfitting if left unchecked."}, {"title": "5.2 Synthetic Data", "content": "In transformative data refinement, one popular approach is to utilize a teacher model trained on a diverse and comprehensive dataset to generate synthetic data (Narayan et al., 2024; Abdin et al., 2024). We provided the instruction prompts in the appendix, which aim to paraphrased pretraining documents. In general, the synthetic data broadened the diversity of the dataset and introduced more complex token patterns, which can lead to improved model performance, particularly in providing complex scenarios that were not well-represented in the original dataset."}, {"title": "6 Experimental Setup", "content": "Network and Training Details. For all experiments, we pretrain the decoder-only transformer using causal language modeling objectives on selected datasets, where model weights were randomly initialized. We evaluated with the language models of sizes {25, 50, 75, 125, 350, 500}M and 1.5B parameters which allowed us to explore how"}, {"title": "7 Discussions", "content": "By over 200 training runs, we re-estimate all the constants which we show in Table 1. Here we first discuss the estimation of constants that relate to accuracy and the rest of the scaling parameters in Eq. 4. In particular, we discuss the scaling factor Q and how it can be applied to pretraining scenarios.\nCorrelation Strength of Estimated Constants. In Table 1, we show the estimated constants for the scaling law Eq.4 and the proposed scaling factor term Eq.3. The constants were estimated with the nonlinear least-squares method with the Scipy optimizer, where the initial guesses were the original Chinchilla scaling law constants in Hoffmann et al. (2022), and the maximum number of function calls was set as 2000. To validate our estimated constants, we provide a predicted vs. true accuracy plot and the Pearson correlation in Figure 5. This gives us ideas on how strongly these constants are correlated to the training set used to estimate our revised scaling formulation. Strikingly, this amounts to the correlation strength of +0.83 across all model sizes and data samples. We attribute the robustness of the formulation to the use of data-agnostic compression ratio and a reasonably-capable language model as teacher.\nHow to Improve Data Quality for Better Models? In the left plot of Figure 3, we first explore the impact of effective tokens on model accuracy. It is evident that an increase in effective tokens correlates with higher accuracy. However, the influence of the scaling factor Q varies across different models. Notably, the impact of data quality is more pronounced in smaller model sizes ranging from 25M to 500M, and it gradually levels off as the value of scaling factor Q increases, eventually reaching a point where effective tokens Dq are predominantly determined by the sheer number of tokens. Additionally, we examine the interplay between the scaling factor Q, diversity, and syntheticity in the right plot of Figure 3. Several key observations emerge:\n1. There is an inverse relationship between diversity and syntheticity, which is expected as"}, {"title": "8 Conclusion and Future Works", "content": "In this paper, we revisited traditional scaling laws in language modeling that often overlook the critical impact of data quality on model generalization. We introduced the concept of effective training tokens, emphasizing its significance in enhancing model performance, particularly for models with constrained parameters, in order to offer a more precise understanding of data quality's role in model scaling. Our findings highlight the pivotal role of data quality and pave the way for developing more efficient and compact language models suitable for on-device applications."}, {"title": "Limitations", "content": "While our revised scaling law incorporating effective training tokens offers a nuanced understanding of data quality, a significant limitation arises from the number of sample points required to accurately estimate the constants within the law. The precision of these constants is crucial as they directly influence the model's performance predictions and generalizations. However, obtaining a sufficient number of diverse and representative sample points to robustly estimate these constants is challenging. This limitation is particularly pronounced in scenarios involving rare or complex data characteristics, where the availability of adequate and varied training examples is limited. Consequently, the reliability of our scaling law under these conditions may be compromised, necessitating further research and potentially more sophisticated sampling techniques to enhance the robustness of our estimates."}, {"title": "Ethics Statement", "content": "In this study, we explore the impact of data quality on language model performance by introducing the concept of effective training tokens. Our experiments, conducted on a diverse set of sampled and synthetic data, adhere to rigorous standards to ensure the reproducibility and reliability of our findings. While our research utilizes datasets that are well-established within the academic community, the application of our findings to sensitive or private datasets must be approached with strict ethical considerations and robust privacy safeguards. Additionally, the methodologies proposed for enhancing data quality, such as text diversity and fidelity assessments, should be applied judiciously to avoid unintended biases or ethical dilemmas. As we push the boundaries of model efficiency and performance, it is imperative to balance these advancements with careful consideration of their broader implications, including the potential increase in computational demands and its associated environmental impact."}, {"title": "A Details of Data Synthesis", "content": "Here we provide the instruction prompt that is used for data synthesis, which is used to rewrite with a Llama-3-70B-instruct (https://ai.meta.com/blog/meta-llama-3/) model to rewrite provided documents from the pretraining data. The data for synthesis was sourced from a directory with JSONL files organized by group numbers and shards, and the model was configured to process sequences up to 8196 tokens in length. Computational precision was optimized for specific hardware by enabling BF16 and disabling FP16, with a batch size of 8 per device to ensure efficient processing and resource utilization. We provide the instruction prompt here:\nCreate a common sense reasoning problem-answer pair based on the following text. However, if it's impossible to create a problem, rewrite the text to be a textbook style language that is clear and concise. Only provide the relevant response and do not say anything else. Do not assume the reader to know anything about the text, so make sure to provide the context for the reasoning problem.\nText:\n{Pretraining Document}\nResponse:"}, {"title": "B Details of Data Selection", "content": "We employ data selection as described in Xie et al. (2023). Here we provide additional details into the feature extraction process from documents. Due to memory limitations on our computational resources, we divided the RefinedWeb dataset into 16 distinct shards. From each shard, we selectively sampled a subset of data tailored to our target specifications. The entire sampling process typically requires approximately 1.5 days to complete across all methodologies. It is important to note that variations in the tokenizer's vocabulary do not significantly affect the sampling speed. This observation suggests that the vocabulary size primarily influences the sentence compression ratio rather than the processing time."}, {"title": "C Computing Text Syntheticity", "content": "To accurately assess syntheticity, it is essential to compute the perplexity for each document. This involves deploying a language model with a context length of 1024 tokens to process all documents. The average perplexity score across these documents serves as the metric for syntheticity.\nGiven the computationally demanding nature of calculating perplexity with language models, we strategically sampled 25% of complete documents from each dataset. This sampling strategy results in a substantial volume of data, ranging from approximately 100 million to several billion subword tokens, ensuring a robust and efficient analysis."}, {"title": "D Scaling Law Constant Estimation", "content": "In this work, we introduce a scaling law for language modeling systems, defined as $\\hat{G}(N, D) = E + \\frac{A}{N^{\\alpha}} + \\frac{B}{D^{\\beta}}$. Here, $\\hat{G}(N, D)$ estimates accuracy, with N as model size and D as dataset size. Constants E, A, \u03b1, B, \u03b2, c1, and c2 are parameters to be determined.\nThe estimation of this scaling law constants involved analyzing a dataset of 210 data points, each representing different model and dataset sizes with corresponding training losses and accuracy scores. These estimation accounted for the refinement of the training data that incorporate additional factors such as diversity and syntheticity into the dataset size. Further, different transformations of the dataset size were included to determine how these factors could be integrated effectively. The accuracy of the model was then obtained for each of these refinements. This comprehensive dataset allowed for robust parameter estimation. Parameter estimation was achieved through nonlinear curve fitting, aiming to align the scaling law's predictions with observed training losses. The process included:\n1. Model Definition: Formulating the scaling law as a function with parameters to estimate. Overall, we have experimented with four equations for Dq:"}, {"title": "E Deriving Effective Token Dq Equation", "content": "We derive the formula to obtain the number of effective tokens as a function of the loss.\nOriginal formula:\n$\\hat{I}(N, D) \\equiv E + \\frac{A}{N^{\\alpha}} + \\frac{B}{D_q^{\\beta}}$\n(5)\nWe consider shorten the loss $\\hat{I}(N, D)$ as L.\n$L \\equiv E + \\frac{A}{N^{\\alpha}} + \\frac{B}{D_q^{\\beta}}$\n(6)\nMove the E to the left:\n$L - E = \\frac{A}{N^{\\alpha}} + \\frac{B}{D_q^{\\beta}}$\n(7)\nMake same denominator:\n$L\\frac{N^{\\alpha}}{N^{\\alpha}} - E\\frac{N^{\\alpha}}{N^{\\alpha}} = \\frac{A}{N^{\\alpha}} + \\frac{B}{D_q^{\\beta}}$\n(8)\nGroup the $N^{\\alpha}$:\n$\\frac{(L - E)N^{\\alpha}}{N^{\\alpha}} = \\frac{A}{N^{\\alpha}} + \\frac{B}{D_q^{\\beta}}$\n(9)\nFlip Both:\n$\\frac{N^{\\alpha}}{(L-E)N^{\\alpha} - A} = \\frac{D_q^{\\beta}}{B}$\n(10)\nIsolate D to the beta on the right:\n$\\frac{B}{D_q^{\\beta}} = \\frac{(L-E)N^{\\alpha} - A}{N^{\\alpha}}$\n(11)\nApply root of beta to get D effective tokens\n$D_q \\approx { \\frac{B N^{\\alpha}}{(L - E)N^{\\alpha} - A}}^{\\frac{1}{\\beta}}$\n(12)\nHere we provide additional details regarding the process of feature extraction from documents. Due to the memory constraints on the machines, we split the RefinedWeb data into 16 shards, and sampled a subset from each shard based on the target data. This process takes around 1.5 days on average for all approaches, meaning that the change in tokenizer's vocabulary does not result in noticeable differences in sampling speed, since vocabulary also defines sentence compression ratio."}, {"title": "F Diversity and syntheticity Result Table", "content": null}, {"title": "G Scaling Law Result Table", "content": null}, {"title": "H Ablation Plots", "content": null}]}