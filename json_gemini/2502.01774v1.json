{"title": "Grokking Explained: A Statistical Phenomenon", "authors": ["Breno W. Carvalho", "Artur S. d'Avila Garcez", "Lu\u00eds C. Lamb", "Em\u00edlio Vital Brazil"], "abstract": "Grokking, or delayed generalization, is an intriguing learning phenomenon where test set loss decreases sharply only after a model's training set loss has converged. This challenges conventional understanding of the training dynamics in deep learning networks. In this paper, we formalize and investigate grokking, highlighting that a key factor in its emergence is a distribution shift between training and test data. We introduce two synthetic datasets specifically designed to analyze grokking. One dataset examines the impact of limited sampling, and the other investigates transfer learning's role in grokking. By inducing distribution shifts through controlled imbalanced sampling of sub-categories, we systematically reproduce the phenomenon, demonstrating that while small-sampling is strongly associated with grokking, it is not its cause. Instead, small-sampling serves as a convenient mechanism for achieving the necessary distribution shift. We also show that when classes form an equivariant map, grokking can be explained by the model's ability to learn from similar classes or sub-categories. Unlike earlier work suggesting that grokking primarily arises from high regularization and sparse data, we demonstrate that it can also occur with dense data and minimal hyper-parameter tuning. Our findings deepen the understanding of grokking and pave the way for developing better stopping criteria in future training processes.", "sections": [{"title": "1. Introduction", "content": "The grokking phenomenon \u2014 a sudden improvement in model performance on unseen data after a period of seemingly stagnant learning, also known as late or delayed gen-\neralization \u2014 was first detected in 2021, in the work of (Power et al., 2022). The subsequent literature attributed the phenomenon mostly to three main factors: data sparsity, large initialization values, and high regularization rates. Although the combination of these factors may indeed cause grokking in some situations, they cannot be said to be the cause or to provide a complete explanation of grokking.\nWe posit that data sparsity induces grokking by causing a distribution shift during training. To systematically investigate this hypothesis, we construct two synthetic datasets where distribution shifts between training and testing data can be precisely controlled to reproduce grokking, even in the presence of dense data. In the first dataset, classes are subdivided into equidistant subclasses. We show that sub-sampling a subclass can induce grokking with minimal hyperparameter tuning, whereas removing a subclass entirely prevents grokking. This suggests that a weak training signal in data organized into classes and subclasses can compensate for sparsity, enabling late generalization by leveraging relationships among subclasses. These findings highlight the importance of organizing data based on relational structures (Seng et al., 2024), such as hierarchical class-subclass relationships, and may provide an explanation for the success of geometric and graph-based methods (Bronstein et al., 2021) and the broader adoption of neuro-symbolic approaches (d'Avila Garcez et al., 2009).\nIn the second dataset, classes form an equivariant map, where transitions between subclasses and classes exhibit a consistent relational structure. Here, we demonstrate that the intensity of grokking depends directly on the distances between classes as defined by subclass relationships. In other words, the relational structure within the data plays a critical role in facilitating late generalization beyond the training data.\nIn addition to the synthetic datasets, we conduct experiments on the MNIST dataset to explore grokking under a real-world scenario of induced distribution shifts. To create these shifts, we apply a clustering algorithm to each digit based on its representation in a learned feature space. Specifically, we train a ResNet classifier on MNIST and use its latent representations as input for clustering. This approach leverages the ResNet's ability to capture high-level"}, {"title": "2. Background and Related Work", "content": "Grokking was detected in a few different datasets and different kinds of model architecture. To the best of our knowledge, no definition for grokking exists in the literature, only descriptions and examples of the phenomenon. Thus, we compiled those descriptions into Definition 3.1 below, as also illustrated in Figure 2. In the paper that first reported the phenomenon (Power et al., 2022), the grokking signature has a large As (see Figure 2) as evaluated predominantly on algorithmic datasets, that is, where the labels are produced by a known algorithm, such as a mathematical operation. How large As gets depends on how much the task relies on learning representations, as illustrated by the authors when comparing the latent space of the model before and after grokking takes place.\nIn (Liu et al., 2022), a rich study is presented on how different parameter choices may influence the occurrence of grokking. This is done by training a multilayer perceptron (MLP) on MNIST and reducing the size of the training set to 1,000 examples; (Liu et al., 2022) do not study what characteristics of the training data might contribute to or avoid grokking. By contrast, our main focus in this paper is to study the training process conditions that create grokking using arbitrary datasets. Nonetheless, (Liu et al., 2022) discuss interesting notions of knowledge representation which align with our idea that relational knowledge and class hierarchy is important for grokking.\nIn Nanda et al. (2023), the focus is on understanding the grokking phenomenon by reverse-engineering neural networks, specifically one-layer transformers trained on modular arithmetic tasks. The authors provide interesting insight"}, {"title": "3. A Definition for Late Generalization (Grokking)", "content": "As illustrated in Figure 2, grokking can be characterized by the transitioning from a low-performing region to a high-performing region as measured on the test set (curve TE in Fig.2) after the training set performance has converged (TR in Fig.2). The interval between the inflection points \u03b1\u2080 (end of memorization phase) and \u03b1\u2081 (start of generalization phase) signifies the transition phase, during which the test\nset performance gradually improves and aligns with the performance on the training set.\nDefinition 3.1 (Grokking). Let \u0394\u1d63 denote an interval with lower bound \u03c3\u2091 containing the limit value of a converging training set performance curve at time \u03b1\u2080. Let \u03c3\u209b \u226a \u03c3\u2091 denote an upper bound on the test set performance measured up to time point \u03b1\u2080. We say that grokking took place if at time point \u03b1\u2081 > \u03b1\u2080, the test set performance is within \u0394\u1d63 and remains within \u0394\u1d63 for an interval S \u226b (\u03b1\u2081 \u2013 \u03b1\u2080).\nWhen training a model, we typically observe training loss and evaluation (test set) loss. Understanding the relationship between these two metrics is essential for interpreting model behavior. If the training loss approaches zero, it suggests that the model has memorized the training data rather than learned to generalize from it. In such cases, one might expect performance on unseen data, represented by the evaluation loss, to degrade due to over-fitting. However, if the evaluation loss decreases sharply at a later time, even after the training loss has approached zero, this indicates a capability to generalize, rather than memorization, given a very weak training signal that needs to be understood."}, {"title": "4. New Synthetic Datasets for Grokking", "content": "The two synthetic datasets introduced in this paper enable the induction of grokking with minimal tuning of model parameters, avoiding an extensive search of the parameter space. These datasets enable the creation of a shift between training and test set distributions, allowing for user control over how much information each subclass carries, as detailed below. The datasets are available in the supplementary material accompanying the paper.\nThe primary goal of the datasets is to be linearly separable when considering all sample dimensions d, while also being non-trivial to solve even with the addition of noise or extreme down-sampling. Data can be created with an arbitrary number of dimensions d. The datasets are defined by projecting classes onto this d-dimensional space using multivariate normal distributions, ensuring control over both class definitions and their distances in the d-dimensional space.\nWe define classes C\u2081, ...,C\u2099, where each class C\u1d62, 1 < i < n, is the union of the samples in the disjoint subclasses C\u1d62,\u2081, ..., C\u1d62,\u2098. We say that C\u1d62 = \u222a\u2c7c\u208c\u2081\u1d50C\u1d62,\u2c7c, and C\u1d62,\u209a \u2229 C\u1d62,\u2096 = \u00d8 for all k \u2260 p.\nThe above structure of classes and subclasses mimics the organization of some real-world datasets, such as COCO (Lin et al., 2014), which is used in image segmentation where objects can be part of a larger scene or other objects. More importantly, it provides a straightforward and\neffective method to induce distribution shifts on demand\nby down-sampling specific subclasses while preserving the\noriginal proportions of the other classes. We also define the\ndimensionality, d, of the dataset.\nIn the creation of the datasets, we ensure that each subclass\nC\u1d62,\u2c7c samples a normal multivariate distribution projected\nonto a d-dimensional space for a given d. By doing this,\nwe ensure that the dataset is linearly separable in the d-\ndimensional space. Thus, the datasets require a simple\nclassification task solvable with a few optimization steps,\nbut are not so trivial that noise or missing samples would\nnot change model performance.\nWe cause arbitrary distribution shifts in the datasets by se-\nlecting a specific set of subclasses to contain fewer samples\nthan the other subclasses. Without loss of generality, we\ncreate all subclasses with the same number of samples in\nevery dataset that we study, i.e. |C\u1d62,\u2c7c| = s, for all i, j.\nFor example, to create a dataset D with 4,000 samples\nand four classes, C\u2081, C\u2082, C\u2083, C\u2084, each with two subclasses,\nC\u1d62,\u2081, C\u1d62,\u2082, 1 \u2264 i \u2264 4, each subclass will have 500 samples.\nTo downsample any given subclass, we define f\u2208 [0,1]\nsuch that if f = 0 then every sample of that subclass is\nremoved; if f = 1 no change is made to the number of\nsamples in the subclass. Given a choice of value for f, let\n\u03b3\u209b denote the number of subclasses to subsample and \u03b3\u1d63\nthe number of remaining subclasses. Then, we sample s\u209b\nexamples from each subclass to subsample, and we sample\ns\u1d63 examples from the remaining subclasses, as defined in\nEquation 1, where \u03b3\u1d30 < |D| is the total number of samples\nwe want to keep.\ns\u209b = \\frac{f\u03b3\u1d30}{f\u03b3\u209b + \u03b3\u1d63} and s\u1d63 = \\frac{\u03b3\u1d30}{f\u03b3\u209b + \u03b3\u1d63} (1)\nReturning to our example of four classes with two subclasses\neach, if \u03b3\u1d30 = 2000, f = 0.2 and we want to subsample\none subclass per class then s\u209b = \\frac{0.2 \u00b7 2000}{0.2 \u00b7 4 + 4} = 84 examples\nand s\u1d63 = \\frac{2000}{0.2 \u00b7 4 + 4} = 416 examples. In what follows,\nwe will subsample the subclasses using different values for\nf up to the point of removing them from the training set\naltogether. We will study equidistant subclasses (dataset 1)\nand equivariant subclasses (dataset 2), as described next.\nDataset 1 (equidistant subclasses): to investigate the im-\npact of shifting the training distribution away from the\ntest set distribution in a systematic way, we created a\ndataset with 4 classes, C\u2081, ..., C\u2084, such that each class\nC\u1d62 is composed of two disjoint subclasses C\u1d62,\u2081 and C\u1d62,\u2082,\nC\u1d62 = C\u1d62,\u2081 \u222a C\u1d62,\u2082, C\u1d62,\u2081 \u2229 C\u1d62,\u2082 = \u00d8. All the subclasses\n(no matter which class they belong to) have equidistant\ncentroids. This means that each subclass is a multivariate"}, {"title": "5. Experimental Results", "content": "All our experiments were designed to run on a single GPU, although we ran multiple experiments in parallel to expedite the data analysis process. Each compute node used in the experiments was equipped with an NVIDIA V100 GPU and 16 GB of memory. We conducted experiments in batches, varying the sample size, fraction f, and model parameters such as the learning rate and weight decay. Each batch took less than 24 hours to run under these settings. For robustness sake, the results presented in this paper are aggregated from 10 independent runs."}, {"title": "5.1. Network Architectures Evaluated", "content": "We conducted our experiments using two different architectures: a Multi-layer Perceptron (MLP) and a 2-layer Transformer. Throughout this paper, we primarily present results for the MLP architecture since the results obtained with both architectures exhibited very similar patterns. For completeness, the results obtained with the 2-layer Transformer can be found in Appendix A. We also provide the code for replicating the experiments in the"}, {"title": "5.2. Distribution-shift impact on equidistant subclasses", "content": "Distribution-shift caused by data sparsity: When performing a balanced down-sampling of our training set, i.e. without forcing any subclass to be sampled more than any other, we find that grokking diminishes as the number of samples increases, only existing in considerably small sample sizes, as depicted in Figure 3a. This is consistent with the literature, as discussed in section 3.\nDistribution-shift caused by unbalanced equidistant subclass sampling: In order to evaluate the impact of distribution shifts in the occurrence of grokking, we evaluated different data unbalancing created by the use of different values for f as described in Section 4. It is clear from Figure 3b that removing completely a subclass has a detrimental effect on generalization, with the models being stuck on a sub-optimal plateau. Nevertheless, adding even a very small number of examples in that subclass (as defined by the fraction e.g. f = 0.01 of the missing subclasses) not only allowed the model to generalize well, but allowed the model to generalize late (i.e. grokking occurred). Since this very small fraction of the examples was not visibly relevant for training (was not misclassified) their impact on the test set performance was seen only after training converged and weight decay led the network to a more sparse internal representation.\nWhen we increase the training data size, as mentioned, we see a diminishing effect on grokking. In order to keep analysing this, we then evaluated more aggressive imbalance factors, such as f = 0.01 and f = 0.05. In a nutshell, Figure 3c shows that the data distribution shift alone is able to induce late generalization."}, {"title": "5.3. Distribution-shift impact on equivariant subclasses", "content": "We investigate the effects of sampling strategies on the grokking phenomenon by examining unbalanced sampling of classes. Firstly, we analyze balanced sampling of classes, as illustrated in Figure 4. Next, we explore the impact of unbalanced sampling, depicted in Figure 4b. Our findings reveal some form of information leakage among subclasses due to their proximity, which is evidenced by the comparison between the similar graphs with and without samples (Figures 4a and 4b).\nDistribution-shift caused by unbalanced equivariant subclass sampling: The other setting evaluated in our experiments consisted of inducing different imbalances, using different values of f as described in Section 4a. The first noteworthy result here is that we were able to detect grokking even in the cases where samples were removed completely from specific subclasses, as depicted in Figure 4b. This surprising behavior contrasts with the results obtained for the equidistant dataset. It is attributed to the fact that in the equivariant case, subclasses of a given class are closer to each other than to subclasses of other classes, thus carrying information that may result in grokking."}, {"title": "6. Final remarks", "content": "This paper has contributed a different perspective to the understanding of the grokking (or late generalization) phenomenon in machine learning. By providing a definition for grokking along with two synthetic datasets that were created to serve as a benchmark for the study of grokking, we have been able to investigate the phenomenon from a new angle, that of data distribution shifts taking place in the context of relations among classes and subclasses of data. We claim that this new perspective on late generalization should offer a better understanding of the phenomenon in connection with relational learning and neuro-symbolic Artificial Intelligence using data and knowledge. Moreover, our observations of emergent behavior when training with very limited examples of a pattern that is nevertheless structured into classes and subclasses underscore the complexity and adaptability of neural networks at learning, the possibility of more efficient learning in future from fewer data and relational knowledge, and the need for further study of learning dynamics in neural networks with implications to training and generalization strategies such as early stopping.\nBroader impact: Firstly, by providing a deeper understanding of grokking and late generalization, our findings can help researchers and practitioners develop models that generalize better to unseen data. Improved training strategies, informed by insights from how distribution shifts and sampling may affect the probability of grokking, may be created in future. The detailed analysis of grokking available as a Python notebook along with the benchmark datasets may serve as an useful resource for the research community and students new to the field to help them understand complex training dynamics and generalization phenomena. Finally, the insights gained should be applicable across various domains of AI application.\nLimitations: Despite the better characterization of the grokking phenomenon provided in this paper showing that it may not be simply an obscure and rare phenomenon confined to the case of very limited training samples, we have"}, {"title": "A. Dataset creation procedure", "content": "We propose a method for constructing datasets where the distance between different classes can be controlled, enabling systematic analysis of grokking dynamics. Each class C\u1d62is characterized by a multivariate distribution D\u1d62. I.e., a data point x \u2208 \u211d\u207f is assigned to class C\u1d62 if it is drawn from the distribution D\u1d62, allowing precise control over the structure and relationships within the data. To construct the set of distributions {D\u2080, ..., D\u2096}, where k = 2\u1d56 \u2013 1, we follow a well-defined procedure.\nTo build the set of distributions {D\u2080,\u2026\u2026,D\u2096}, where k = 2\u1d56 \u2013 1, we create a set of p pairs of sub-distributions (d\u1d62\u2070, d\u1d62\u00b9). Each sub-distribution d\u1d62\u2070 and d\u1d62\u00b9 has the same dimension r, so x \u2208 \u211d\u1d56\u02e3\u02b3.\nTo define D\u2c7c, we transform the index j into its binary representation with p fixed positions, denoted as B\u209a(j). For example, B\u2084(5) = 0101. We then associate the digit in the i\u1d57\u02b0 position with the sub-distribution indexed by the digit. For instance, when p = 4, we can create 15 distributions, with D\u2085 = {d\u2070, d\u00b9, d\u00b2, d\u00b3}.\nTo ensure that the distance between distributions is preserved in the Euclidean space, we generate d\u1d62\u2070 = \ud835\udca9(\u03bc\u2070, \u03a3\u1d62), where \u03bc\u2070 \u2208 \u211d is the location and \u03a3\u1d62 \u2208 \u211d\u02b3\u02e3\u02b3 is the covariance matrix. The difference between d\u2070 and d\u00b9 lies in their locations, since we use the same covariance matrix. For this work, we set ||\u03bc\u2070 \u2013 \u03bc\u00b9|| = 1 for all i.\nNow, we can define the distance between two classes using their indices in binary representation. For instance, when p = 4, the distance between C\u2081 and C\u2085 is calculated as follows: B\u2084(1) = 0001 and B\u2084(5) = 0101, so D(C\u2081, C\u2085) = 1. Similarly, D(C\u2081, C\u2082) = 2.\nIn this work, we constructed a base set of classes with p = 9, resulting in 512 distinct classes, and r = 13, which implies that each data point x \u2208 \u211d\u00b9\u00b9\u2077. To simulate real-world scenarios, noise and imbalance are systematically introduced. Imbalance is achieved by varying the number of samples drawn from each class, mimicking uneven class distributions often encountered in practice."}, {"title": "A.1. The Equidistant Dataset", "content": "To construct the equidistant dataset, we first create a distance graph G, where each node represents a class and each edge is weighted by the distance between the corresponding classes. We then extract the sub-graph consisting of all edges with a weight of 2. Within this sub-graph, we identify the largest clique, which corresponds to the largest subset of classes with a pairwise distance of 2."}, {"title": "A.2. The Equivariant Dataset", "content": "To construct the equivariant dataset, we reuse the same graph G, but this time extract a clique from the sub-graph of edges with a weight of 6. This yields a subset of classes {63, 240, 323, 396}. For each of these classes, we identify all classes that are at a distance of 1 from them. This results in 4 super-classes, each comprising 10 classes. Where we have that:\nC\u1d62 \u2208 SC\u2096 and C\u2c7c \u2208 SC\u2097 \u21d2 {D(C\u1d62, C\u2c7c) \u2265 4 if k \u2260 l, D(C\u1d62, C\u2c7c) \u2264 2 if k = l.\nThe datasets are designed to support controlled experiments focusing on distribution shifts, noise tolerance, and sampling strategies. They simulate challenges often observed in real-world tasks, including imbalanced datasets and noisy measurements, and serve as benchmarks for evaluating model robustness and generalization across various conditions. By making these datasets publicly available, we aim to facilitate further research into grokking and provide a versatile tool for understanding generalization phenomena in deep learning."}]}