{"title": "Reasoning about unpredicted change and explicit time", "authors": ["Florence Dupin de Saint-Cyr", "J\u00e9r\u00f4me Lang"], "abstract": "Reasoning about unpredicted change consists in explaining observations by events; we propose here an approach for explaining time-stamped observations by surprises, which are simple events consisting in the change of the truth value of a fluent. A framework for dealing with surprises is defined. Minimal sets of surprises are provided together with time intervals where each surprise has occurred, and they are characterized from a model-based diagnosis point of view. Then, a probabilistic approach of surprise minimisation is proposed.", "sections": [{"title": "1 Introduction", "content": "Reasoning about time, action and change in full generality is a very complex task, and much research has been done in order to find solutions suited to specific subclasses of problems, obtained by making some simplificaying assumptions. Sandewall [15] provides a taxonomy for positioning the various subclasses, based on a lot of ontological and epistemological assumptions that may be present or absent in the specification of the subclass. In this paper we consider an integer representation of time and as in [15] we define fluents as propositions whose truth value evolves over time, observations as pieces of knowledge about the value of some fluents at some time points, changes as pairs made of a fluent f and a time point t such that the value of the fluent f at t differs from that at t \u2212 1. Changes may be either caused by actions, which are initiated by the agent, or by events, which are initiated by the world. This latter form of change will be called unpredicted changes; as they are unrelated to the performance of any action by the agent, they are totally out of the control of the agent. While many approaches in the literature focus on reasoning about change caused by actions\u00b9, there has been significantly less attention on reasoning about unpredicted change.\nIn this article we propose a framework for detecting unpredicted changes from observations at different time points: the task consists thus in explaining obser-vations by a set of elementary unpredicted changes. Note that it is analogous in many aspects to temporal diagnosis (an elementary change corresponding to a faulty component). We do not explicitly consider actions, since this is not our purpose; however, as we will show, our framework can handle actions as well,"}, {"title": "2 Minimizing unpredicted change in K-IS", "content": "From now on, the class of problems we consider coincides with the class K \u2013 IS in Sandewall's taxonomy. The symbol K means that all observations reported to the agent are correct, i.e., true in the real world. I is for inertia with integer time and means that fluent values generally tend to persist. S is for surprises;\na surprise, or unpredicted change, is a change of the value of a fluent during a given interval, without any reason known by the agent; S means that surprises may occur (and are thus considered when computing completions of scenarios). Inertia implies that surprises occur with a low frequency, and should thus be minimized in order to be in accordance with observations. At this point we should say something about actions. In this paper we are not concerned with actions, and the general assumption we make throughout the paper is that the agent is totally passive (no action is performed)2.\nLet L be a propositional language built on a set of propositional variables V = {v1, ..., vn}. A fluent is a variable or its negation, i.e., f = vi or f = \u00acvi. The set of all fluents is denoted by F; thus F = {v1, \u00acv1, ..., vn, \u00acvn}. Time is assumed to be discrete: the time scale T is a sequence of integers {0, 1, ..., tmax} (it is assumed that between two consecutive time-points the system is completely inert).\nIf \u03c6 \u2208 L and t \u2208 T then [t]\u03c6 is an elementary timed formula (meaning that \u03c6 holds at t). (Complex) timed formulas are built from elementary timed formulas and usual connectives.\nA timed model M is a mapping from V \u00d7 T to {0, 1} : M assigns a truth value to each fluent at each time point. M |= [t]vi iff M(vi, t) = 1. Satisfaction is extended to elementary timed formulas in the usual way, i.e., M |= [t]\u03c6 \u2227 \u03c6' iff M |= [t]\u03c6 and M |= [t]\u03c6', etc., and then to complex timed formulas: M |= [t]\u03c6 \u2227 [t']\u03c6' iff M |= [t]\u03c6 and M |= [t']\u03c6', etc. Note that [t]\u00ac\u03c6 is equivalent to \u00ac[t]\u03c6, [t]\u03c6 \u2228 \u03c6' is equivalent to [t]\u03c6 \u2228 [t]\u03c6', etc.\nLastly, we abbreviate [t]\u03c6 \u2227 [t + 1]\u03c6 \u2227\u00b7\u00b7\u00b7\u2227 [t']\u03c6 by [t, t']\u03c6."}, {"title": "2.2 Minimal explanations", "content": "The problem that we address is: given a scenario \u03a3, find the preferred timed-models satisfying \u03a3, i.e., find the timed-models in which the changes are mini-mal: {M, M |= \u03a3 and there is no M' such that C(M') \u2286 C(M)}.\nDefinition 4. A pointwise explanation PE for \u03a3 is a set of changes {(fi, ti), i = 1... p} such that \u03a3 \u222a {[t \u2212 1]f \u2192 [t]f | f \u2208 F, t \u2208 T, t \u2260 0, (f, t) \u2209 PE} is consistent.\nIntuitively, PE consists in specifying which fluents change their value and when so as to be in accordance with \u03a3.\nNote that {(a, 2), (b, 3)} is also a pointwise explanation, but, since it is not necessary to assume that b changed in order to explain \u03a31, this pointwise expla-nation is not minimal:\nDefinition 5. A pointwise explanation PE is minimal iff there is no pointwise explanation PE' strictly included in PE\nDefinition 6. An explanation for \u03a3 is a set of surprises {< fi, ti, t'i >, i = 1... p} such that \u2200(t''1, ..., t''p) \u2208 [t1 + 1, t'1] \u00d7\u00b7\u00b7\u00b7\u00d7 [tp + 1, t'p], PE = {(fi, t''i), i = 1\u00b7\u00b7\u00b7p} is a pointwise explanation for \u03a3. We say that each of these PE is covered by E.\nFor any two explanations E and E', we say that E covers E' iff any pointwise explanation covered by E' is also covered by E.\nNote that an explanation has to be understood as a disjunction of point-wise explanations. In the example above, {(a, 2, 5)} is an explanation for \u03a31; corresponding to the pointwise explanations {(a, 2), (a, 3), ..., (a, 5)}.\nDefinition 7. An explanation E is minimal if any pointwise explanation which is covered by E is minimal.\nThe previous explanation {(a, 2, 5)} is minimal, but for instance {(a, 2, 5), (b, 1, 3)} or {(a, 2, 5), (a, 0, 5)} are explanations but they are not minimal."}, {"title": "2.3 Computing minimal explanations", "content": "The method consists in completing the set of observations \u03a3 by a set of per-sistence assumptions in order to express that, by default, fluents persist during the interval between two consecutive observations. In order to add only \"rele-vant\" persistence assumptions, we identify, for each variable, the \"relevant time points\" where we have some observation regarding it.\nDefinition 10. The set of relevant variables V(\u03a3) with respect to the set of observations \u03a3 is defined by: V(\u03a3) = {v \u2208 V | v appears in \u03a3}.\nThe set of relevant time points RT\u03a3(v) to a variable v \u2208 V(\u03a3) w.r.t. \u03a3 is defined by: RT\u03a3(v) = {t \u2208 T | [t]\u03c6 \u2208 \u03a3 and v appears in \u03c6}\nLet RT(v) be the set of relevant time points to a variable v except the last one: RT(v) = RT\u03a3(v) \\ {max{t \u2208 T, t \u2208 RT\u03a3(v)}}\nDefinition 11. The set of persistence axioms of \u03a3 is defined by:\n$\\PERS(\\Sigma) = \\bigcup\\limits_{v\\in V(\\Sigma), t \\in RT(v)} {[t]v \\rightarrow [next(v, t)]v, [t]\\neg v \\rightarrow [next(v, t)]\\neg v}$ where next(v, t) = min{t' \u2208 RT\u03a3(v), t' > t} is the next relevant time point for v after t.\nDefinition 12.\nLet MaxCons(\u03a3) be the set of all maximal subsets of PERS(\u03a3) consistent with \u03a3\nlet Cmc(\u03a3) = \u222aX\u2208MaxCons(\u03a3) (PERS(\u03a3) \\ X)\nlet Candidates(\u03a3) = {{{f, t, t') | [t]f \u2192 [t']f \u2208 X}, X \u2208 Cmc(\u03a3)}.\nProposition 6.\nAny set of surprises in Candidates(\u03a3) is a minimal explanation for \u03a3. Every pointwise explanation for \u03a3 is covered by a unique set of surprise in Candidates(\u03a3)\nThis principle of finding minimal explanations corresponds exactly to finding minimal sets of faulty components in model-based diagnosis (or candidates in ATMS [4] terminology). Hence, the computation of minimal explanation can be done by well-known algorithms from these fields."}, {"title": "3 Probabilities and unpredicted change", "content": "We assume now that probabilistic information about fluents truth values and persistence is available. This will enable us to compute the probability of each explanation, and thus will help us ranking them. We will take account of 2 im-portant factors which can influence quantitatively the persistence: time duration and the intrinsic tendency of each fluent to persist (some fluents persist longer than other, e.g., sleeping persists usually longer than eating an apple).\nFor the sake of simplicity, throughout this Section we assume fluents are mutually independent and Markovian4. We will consider stationary fluents in some cases but we will not impose this restriction to the whole section.\nDefinition 13."}, {"title": "3.1 Markovian fluents", "content": "- fluents are mutually independent iff \u2200t, t', \u2200f \u2208 F, \u2200f' \u2208 F \\ {f, \u00acf},\n$Pr([t]f \\wedge [t']f') = Pr([t]f).Pr([t']f')$\n- f is stationary iff \u2200t, t', \u2200f \u2208 F, $Pr([t]f) = Pr([t']f) = pf$.\n- f is Markovian iff \u2200t, $Pr([t + 1]f | [t]f \\rightarrow H_{0\\rightarrow t-1}) = Pr([t + 1]f | [t]f) = \\epsilon_f$.\nWhere $H_{0\\rightarrow t-1}$ is the history of the system from 0 to t \u2212 1.\nThe independence and Markovian assumption imply that the only necessary data are, for each propositional variable v and each time-point t, a prior probability of v, pt, v and the elementary change probabilities \u03f5v and \u03f5\u00acv. We will show later that in many cases prior probabilities are not necessary. Generally \u03f5v \u2260 \u03f5\u00acv, i.e., the persistence of a fluent may be different from the persistence of its negation (think of f = alive or f = bell-ringing).\nIf fluents are stationary then for each variable it is enough to consider a prior probability pr which is independent of the time-point. Moreover, in this case, there is a relationship between the persistence of a fluent and its negation:\nProposition 7. If f is stationary then \u03f5f.pf = \u03f5\u00acf.p\u00acf\nProof. Pr([t]f [t + 1]\u00acf) = Pr([t + 1]\u00acf | [t]f).Pr([t]f) = \u03f5f.pf. And Pr([t]\u00acf\u2228 [t + 1]f) = Pr([t]\u00acf) + Pr([t + 1]f) \u2212 Pr([t]\u00acf^[t + 1]f) = p\u00acf + pf \u2212 \u03f5\u00acf.p\u00acf = 1 \u2212 \u03f5\u00acf.p\u00acf.\nObviously, some fluents are not Markovian:\neither because they do not tend to persist independently of t: consider a clock which always rings around 7 a.m., then the fluent ringing is not Markovian (nor stationary). However, the fact that the clock is on the bedside table is usually a Markovian fluent (unless you throw it away each time it rings ).\nor because their tendency to persist depend on their history: consider a bus which period is 15 minutes, the probability that the bus will come depends on the length of the interval since it last came. If you know that no bus has came for 10 minutes, you have more chance than the bus will arrive immediately than if you know that no bus has come for 5 minutes. Clearly, Pr([t]\u00acbus_coming | [t \u2212 1]\u00acbus_coming) = 1, while Pr([t]bus_coming | [t-k, t \u2212 1]\u00acbus_coming) = $\\frac{5}{15}$. And thus bus_coming is not Markovian as all periodic fluents (however it may reasonably be assumed stationary without considering the rush hour).\nAmong all Markovian fluents, some can be distinguished:\npersistent fluents, such that \u03f5f = 0 (e.g. f =dead). A stationary and persis-tent fluent is a degenerate case (either it never change, or the probability of its opposite is null).\nchaotic fluents, such that: Pr([t + 1]f | [t]f) = Pr([t + 1]f) (these fluents have no tendency to persist, since knowing this fluent true at t does not make it true at t + 1 more probable). It means that \u03f5f = 1 \u2212 Pt+1, f, hence, chaotic fluents must be stationary."}, {"title": "3.2 Highly persistent fluents and probabilities of explanations", "content": "From proposition 8 and the independence assumption it is also possible to com-pute the prior probability of an explanation (minimal or not) :\n$Pr(\\{(f_i, t_i, t'_i), i = 1 ... n\\}) = \\prod\\limits_{i=1}^{n}(1 - (1 - \\epsilon_{f_i})^{t'_i - t_i}). p_{f_i}$\nNow, these prior probabilities have to be conditioned on the observations (\u03a3) in order to get posterior probabilities of all explanations: if E is an explanation, then Pr(E | \u03a3) = $\\frac{P(E \\wedge \\Sigma)}{P(\\Sigma)}$.\nIn the general case these probabilities are not straightforward to compute, mainly because Pr([t]f \u2227 [t']\u00acf) may be much lower than Pr((f, t, t'))5. More-over, non-minimal explanations may have a significant probability, even in the case where {0} is an explanation, which contradicts the principle of minimal change. This is because elementary probabilities of change (the \u03f5f's) may be"}, {"title": "4 Discussion", "content": "The original aspects of our work are that (i) minimal changes (explanations) are provided together with the interval when change may have occurred, which makes the representation concise, and (ii) knowing probabilities of change of fluents from one point to the subsequent one, and fluent prior probabilities, we compute the probability of each explanation and thus we rank them accordingly. We showed that minimizing change is coherent with a probabilistic handling of"}]}