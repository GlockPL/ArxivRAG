{"title": "Reasoning about unpredicted change and explicit time", "authors": ["Florence Dupin de Saint-Cyr", "J\u00e9r\u00f4me Lang"], "abstract": "Reasoning about unpredicted change consists in explaining observations by events; we propose here an approach for explaining time-stamped observations by surprises, which are simple events consisting in the change of the truth value of a fluent. A framework for dealing with surprises is defined. Minimal sets of surprises are provided together with time intervals where each surprise has occurred, and they are characterized from a model-based diagnosis point of view. Then, a probabilistic approach of surprise minimisation is proposed.", "sections": [{"title": "Introduction", "content": "Reasoning about time, action and change in full generality is a very complex task, and much research has been done in order to find solutions suited to specific subclasses of problems, obtained by making some simplificaying assumptions. Sandewall [15] provides a taxonomy for positioning the various subclasses, based on a lot of ontological and epistemological assumptions that may be present or absent in the specification of the subclass. In this paper we consider an integer representation of time and as in [15] we define fluents as propositions whose truth value evolves over time, observations as pieces of knowledge about the value of some fluents at some time points, changes as pairs made of a fluent f and a time point t such that the value of the fluent f at t differs from that at t \u2212 1. Changes may be either caused by actions, which are initiated by the agent, or by events, which are initiated by the world. This latter form of change will be called unpredicted changes; as they are unrelated to the performance of any action by the agent, they are totally out of the control of the agent. While many approaches in the literature focus on reasoning about change caused by actions\u00b9, there has been significantly less attention on reasoning about unpredicted change.\nIn this article we propose a framework for detecting unpredicted changes from observations at different time points: the task consists thus in explaining obser-vations by a set of elementary unpredicted changes. Note that it is analogous in many aspects to temporal diagnosis (an elementary change corresponding to a faulty component). We do not explicitly consider actions, since this is not our purpose; however, as we will show, our framework can handle actions as well,"}, {"title": "Minimizing unpredicted change in K-IS", "content": "From now on, the class of problems we consider coincides with the class K \u2013 IS in Sandewall's taxonomy. The symbol K means that all observations reported to the agent are correct, i.e., true in the real world. I is for inertia with integer time and means that fluent values generally tend to persist. S is for surprises;"}, {"title": "Background", "content": "a surprise, or unpredicted change, is a change of the value of a fluent during a given interval, without any reason known by the agent; S means that surprises may occur (and are thus considered when computing completions of scenarios). Inertia implies that surprises occur with a low frequency, and should thus be minimized in order to be in accordance with observations. At this point we should say something about actions. In this paper we are not concerned with actions, and the general assumption we make throughout the paper is that the agent is totally passive (no action is performed)\u00b2.\nLet L be a propositional language built on a set of propositional variables V = {v\u2081,..., vn}. A fluent is a variable or its negation, i.e., f = vi or f = \u00acvi. The set of all fluents is denoted by F; thus F = {v\u2081, \u00acv\u2081,..., vn, \u00acvn}. Time is assumed to be discrete: the time scale T is a sequence of integers {0,1,..., tmax} (it is assumed that between two consecutive time-points the system is completely inert).\nIf \u03c6 \u2208 L and t \u2208 T then [t]\u03c6 is an elementary timed formula (meaning that \u03c6 holds at t). (Complex) timed formulas are built from elementary timed formulas and usual connectives.\nA timed model M is a mapping from V \u00d7 T to {0,1} : M assigns a truth value to each fluent at each time point. M |= [t]vi iff M(vi,t) = 1. Satisfaction is extended to elementary timed formulas in the usual way, i.e., M |= [t]\u03c6 \u2227 \u03c6' iff M |= [t]\u03c6 and M |= [t]\u03c6', etc., and then to complex timed formulas: M |= [t]\u03c6 \u2227 [t']\u03c6' iff M |= [t]\u03c6 and M |= [t']\u03c6', etc. Note that [t]\u00ac\u03c6 is equivalent to \u00ac[t]\u03c6, [t]\u03c6 \u2228 \u03c6' is equivalent to [t]\u03c6 \u2228 [t']\u03c6', etc.\nLastly, we abbreviate [t]\u03c6 \u2227 [t+1]\u03c6 \u2227\u00b7\u00b7\u00b7\u2227 [t']\u03c6 by [t, t']\u03c6."}, {"title": "Definition 1.", "content": "A scenario \u03a3 in K-IS consists in a set of timed-formulas"}, {"title": "Definition 2.", "content": "Let f \u2208 F, t \u2208 T, t\u2260 0.\n1. (f,t) is a change in M (notation: CM(f,t)), iff M |= [t-1]f and M |= [t]\u00acf.\n2. The set of all changes in M is C(M) = {(f,t) | CM(f,t)}.\nWhen the exact time-point where the change occurred is not known, we affect an interval to a change occurrence and call it a surprise."}, {"title": "Definition 3.", "content": "Let f \u2208 F, t,t' \u2208 T, t < t'. (f,t,t') is a surprise with respect to M (notation: SM(f,t,t')), iff M |= [t]f and M |= [t, t']f.\nIntuitively, SM(f,t,t') means that the truth value of f changed at least once between t and t' (note that it does not necessarily imply that M |= [t']\u00acf, since f may have changed its truth value several times within [t, t'])."}, {"title": "Minimal explanations", "content": "The problem that we address is: given a scenario \u03a3, find the preferred timed-models satisfying \u03a3, i.e., find the timed-models in which the changes are mini-mal: {M, M |= \u03a3 and there is no M' such that C(M') \u2286 C(M)}."}, {"title": "Definition 4.", "content": "A pointwise explanation PE for \u03a3 is a set of changes {(fi,ti), i = 1...p} such that \u03a3 \u222a {[t\u22121]f\u2192[t]f | f\u2208F,t\u2208T,t\u22600, (f,t) \u2209 PE} is consistent.\nIntuitively, PE consists in specifying which fluents change their value and when so as to be in accordance with \u03a3."}, {"title": "Example 1.", "content": "Let us consider a set of pointwise observations \u03a3\u2081 such that:\n\u03a3\u2081: { [0] a \u2227 b\n [5] \u00aca\n{(a, 2)} is a pointwise explanation for \u03a3\u2081: in order to explain the non-inert behaviour of the system, we must assume that the value of the fluent a changed between 0 and 5, (for instance at time point 2).\nNote that {(a, 2), (b, 3)} is also a pointwise explanation, but, since it is not necessary to assume that b changed in order to explain \u03a3\u2081, this pointwise expla-nation is not minimal:"}, {"title": "Definition 5.", "content": "A pointwise explanation PE is minimal iff there is no pointwise explanation PE' strictly included in PE"}, {"title": "Definition 6.", "content": "An explanation for \u03a3 is a set of surprises {< fi,ti,t'i >, i = 1\u2026p} such that \u2200(t''\u2081,..., t''p) \u2208 [t\u2081 + 1, t'\u2081] \u00d7\u00b7\u00b7\u00b7\u00d7 [tp +1, t'p], PE = {(fi,t''i),i= 1\u00b7\u00b7\u00b7p} is a pointwise explanation for \u03a3. We say that each of these PE is covered by E.\nFor any two explanations E and E', we say that E covers E' iff any pointwise explanation covered by E' is also covered by E.\nNote that an explanation has to be understood as a disjunction of point-wise explanations. In the example above, {(a, 2,5)} is an explanation for \u03a3\u2081; corresponding to the pointwise explanations {(a, 2), (a, 3), ..., (a, 5)}."}, {"title": "Definition 7.", "content": "An explanation E is minimal if any pointwise explanation which is covered by E is minimal.\nThe previous explanation {(a, 2,5)} is minimal, but for instance {(a, 2,5), (b, 1,3)} or {(a, 2, 5), (a, 0,5)} are explanations but they are not minimal."}, {"title": "Proposition 2.", "content": "E = {(fi, ti, t'i), i = 1...p} is an explanation (resp. a minimal explanation) for \u03a3 iff\n\u03a3 \u222a {[t]f\u2192[t+1]f | f\u2208F,t\u2208T,t\u2260tmax, and \u00ac\u2203(fi, ti,t'i) \u2208 E s.t. t \u2208 [ti, t'i \u2212 1]}\nis consistent.\nIt is a minimal explanation iff the above set of formulas is maximally con-sistent in \u03a3 \u222a {[t]f\u2192[t+1]f | f\u2208F,t\u2208T,t\u2260tmax}"}, {"title": "Definition 8.", "content": "A minimal explanation E is compact iff there is no explanation E' which strictly covers E."}, {"title": "Proposition 3.", "content": "If there is no disjunction in \u03a3 (i.e., each pointwise observation [t]\u03c6 is a conjunction of fluents) there is only one compact minimal explanation for \u03a3."}, {"title": "Example 2.", "content": "{\n [0] a\n [5] a \u2228 c\n\u03a3\u2082: [10] b\n [15] \u00aca \u2228 \u00acb\n [20] \u00acc\nThere are 3 minimal compact explanations:\nE\u2081 = {(a,5,15)}, E\u2082 = {(b, 10, 15)} and E\u2083 = {(a,0,5), (c, 5, 20)} covering 10+5+ (5 \u00d7 15) pointwise explanations.\n{(a, 10, 15)} is an explanation for E\u2082 but it is not compact since {(a, 5, 15)}\nis an explanation for \u03a3\u2082.\n{(a, 0, 15)} is not an explanation for \u03a3\u2082 since, for instance, {(a, 1)} is not a\npointwise explanation for \u03a3\u2082.\n{(a, 0, 15), (c, 5, 20)} is an explanation for \u03a3\u2082 but it is not minimal since\n{(a, 6), (c, 7)} is covered by it and is not a minimal pointwise explanation\n(since {(a, 6)} alone is a pointwise explanation)."}, {"title": "Definition 9.", "content": "Let Cme(\u03a3) be the set of all compact minimal explanations for \u03a3."}, {"title": "Proposition 4.", "content": "For each minimal pointwise explanation PE for \u03a3, there is a unique compact minimal explanation E which covers it.\nThus, Cme(\u03a3) is the most concise expression covering all minimal pointwise explanations."}, {"title": "Proposition 5.", "content": "Cme(\u03a3) = {\u2205} \u21d4 \u03a3 \u222a {[t \u2212 1]f \u2192 [t]f | f \u2208 F,t \u2208 T,t \u2260 0}\nis consistent."}, {"title": "Computing minimal explanations", "content": "The method consists in completing the set of observations \u03a3 by a set of per-sistence assumptions in order to express that, by default, fluents persist during the interval between two consecutive observations. In order to add only \"rele-vant\" persistence assumptions, we identify, for each variable, the \"relevant time points\" where we have some observation regarding it."}, {"title": "Definition 10.", "content": "The set of relevant variables V(\u03a3) with respect to the set of observations \u03a3 is defined by: V(\u03a3) = {v \u2208 V |v appears in \u03a3}.\nThe set of relevant time points RT\u03a3(v) to a variable v \u2208 V(\u03a3) w.r.t. \u03a3 is defined by: RT\u03a3(v) = {t \u2208 T | [t]\u03c6\u2208\u03a3 and v appears in \u03c6}\nLet RT\u03a3(v) be the set of relevant time points to a variable v except the last one: RT\u03a3(v) = RT\u03a3(v) \\ {max{t \u2208 T, t \u2208 RT\u03a3(v)}}"}, {"title": "Definition 11.", "content": "The set of persistence axioms of \u03a3 is defined by:\nPERS(\u03a3) = \u222av\u2208V(\u03a3),t\u2208RT(v) {[t]v \u2192 [next(v,t)]v, [t]\u00acv \u2192 [next(v,t)]\u00acv}\nwhere next(v,t) = min{t' \u2208 RT\u03a3(v), t' > t} is the next relevant time point\nfor v after t."}, {"title": "Definition 12.", "content": "Let MaxCons(\u03a3) be the set of all maximal subsets of PERS(\u03a3) consistent\nwith \u03a3\nlet Cmc(\u03a3) = \u222aX\u2208MaxCons(\u03a3) (PERS(\u03a3) \\ X)\nlet Candidates(\u03a3) = {{< f,t,t' > | [t]f \u2192 [t']f \u2208 X}, X \u2208 Cmc(\u03a3)}."}, {"title": "Proposition 6.", "content": "Any set of surprises in Candidates(\u03a3) is a minimal explanation for \u03a3.\nEvery pointwise explanation for \u03a3 is covered by a unique set of surprise in Candidates(\u03a3)\nThis principle of finding minimal explanations corresponds exactly to finding minimal sets of faulty components in model-based diagnosis (or candidates in ATMS [4] terminology). Hence, the computation of minimal explanation can be done by well-known algorithms from these fields."}, {"title": "Example 3.", "content": "(Example 2 continued) The relevant time-points for \u03a3\u2082 are :\nRT\u03a3\u2082(\u03b1) = {0,5,15}, RT\u03a3\u2082(b) = {10,15} and RT\u03a3\u2082(c) = {5, 20}."}, {"title": "Probabilities and unpredicted change", "content": "We assume now that probabilistic information about fluents truth values and persistence is available. This will enable us to compute the probability of each explanation, and thus will help us ranking them. We will take account of 2 im-portant factors which can influence quantitatively the persistence: time duration and the intrinsic tendency of each fluent to persist (some fluents persist longer than other, e.g., sleeping persists usually longer than eating an apple)."}, {"title": "Markovian fluents", "content": "For the sake of simplicity, throughout this Section we assume fluents are mutually independent and Markovian4. We will consider stationary fluents in some cases but we will not impose this restriction to the whole section."}, {"title": "Definition 13.", "content": "fluents are mutually independent iff \u2200t,t',\u2200f \u2208 F,\u2200f' \u2208 F \\ {f,\u00acf},\nPr([t]f \u2227 [t']f') = Pr([t]f).Pr([t']f').\nf is stationary iff \u2200t,t',\u2200f \u2208 F, Pr([t]f) = Pr([t']f) = pf.\nf is Markovian iff \u2200t, Pr([t + 1]f | [t]f \u2227 H\u2080\u2192t-1) = Pr([t + 1]f | [t]f) = \u03b5\u03c6.\nWhere H\u2080\u2192t-1 is the history of the system from 0 to t \u2212 1.\nThe independence and Markovian assumption imply that the only necessary data are, for each propositional variable v and each time-point t, a prior probability of v, pt,v and the elementary change probabilities \u03b5\u03c6 and \u03b5\u00acv. We will show later that in many cases prior probabilities are not necessary. Generally \u03b5\u03c6 \u2260 \u03b5\u00acv, i.e., the persistence of a fluent may be different from the persistence of its negation (think of f =alive or f =bell-ringing).\nIf fluents are stationary then for each variable it is enough to consider a prior probability pr which is independent of the time-point. Moreover, in this case, there is a relationship between the persistence of a fluent and its negation:"}, {"title": "Proposition 7.", "content": "If f is stationary then \u03b5f.pf = \u03b5\u00acf.p\u00acf\nProof. Pr([t]f[t+1]\u00acf) = Pr([t+1]\u00acf | [t]f).Pr([t]f) = \u03b5\u03c6.pf. And Pr([t]\u00acf \u2228 [t+1]f) = Pr([t]\u00acf) + Pr([t+1]f) \u2013 Pr([t]\u00acf \u2227 [t+1]f) = p\u00acf+pf-\u03b5\u00acf.p\u00acf =\n1- \u03b5f.pf.\nObviously, some fluents are not Markovian:\neither because they do not tend to persist independently of t: consider a clock which always rings around 7 a.m., then the fluent ringing is not Markovian\n(nor stationary). However, the fact that the clock is on the bedside table is\nusually a Markovian fluent (unless you throw it away each time it rings ).\nor because their tendency to persist depend on their history: consider a bus\nwhich period is 15 minutes, the probability that the bus will come depends\non the length of the interval since it last came. If you know that no bus\nhas came for 10 minutes, you have more chance than the bus will arrive\nimmediately than if you know that no bus has come for 5 minutes. Clearly,\nPr([t]\u00acbus_coming | [t\u22121]\u00acbus_coming) = 1, while Pr([t]bus_coming | [t-\nk, t \u2212 1]\u00acbus_coming) = 5. And thus bus_coming is not Markovian as all\nperiodic fluents (however it may reasonably be assumed stationary without\nconsidering the rush hour).\nAmong all Markovian fluents, some can be distinguished:\npersistent fluents, such that \u03b5\u03c6 = 0 (e.g. f =dead). A stationary and persis-tent fluent is a degenerate case (either it never change, or the probability of its opposite is null).\nchaotic fluents, such that: Pr([t+1]f | [t]f) = Pr([t+1]f) (these fluents have no tendency to persist, since knowing this fluent true at t does not make it true at t + 1 more probable). It means that \u03b5\u03c6 = 1 \u2212 pt+1,f, hence, chaotic fluents must be stationary."}, {"title": "switching fluents", "content": "(provided that the time unit is well chosen) such that \u03b5\u03c6 = 1\nNow, given the prior probability distribution pf and the switch probability\nPr([t+1]\u00acf | [t] f) = \u03b5\u03c6 (which are constants, independent of t), it is possible to\ncompute the prior probability of surprise for each stationary Markovian fluent\nbetween any time points."}, {"title": "Definition 14.", "content": "Notation: Pr((f,t,t')) = Pr([t]f \u2227 \u00ac[t,t']f) is the probability of\nthe surprise (f,t,t')."}, {"title": "Proposition 8.", "content": "If f is stationary and Markovian, then\nPr((f,t,t + n)) = (1 \u2212 (1 \u2212 \u03b5\u03c6)\u207f).pf\nProof.\nPr((f,t,t + n)) = Pr((f,t,t + n) | [t]f).Pr([t]f)\n+ Pr((f,t,t + n) | [t]\u00acf).Pr([t]\u00acf)\n= (1 - Pr([t]\u00acf \u2228 [t + 1,t + n]f | [t]f)).pf + 0\n= (1 \u2013 Pr([t + 2, t + n] f | [t + 1]f).Pr([t + 1]f | [t]f)).pf\n= (1 \u2212 (1 \u2212 \u03b5\u03c6)\u207f).pf"}, {"title": "Highly persistent fluents and probabilities of explanations", "content": "From proposition 8 and the independence assumption it is also possible to com-pute the prior probability of an explanation (minimal or not) :\nPr({{fi, ti, t\u2081), i = 1 ... n}) = \u220f\u1d62=\u2081(1 \u2212 (1 \u2013 \u03b5\u03c6i)ti'-ti). pfi\nNow, these prior probabilities have to be conditioned on the observations (\u03a3) in order to get posterior probabilities of all explanations: if E is an explanation, then Pr(E | \u03a3) = Pr(E \u2227 \u03a3)/Pr(\u03a3).\nIn the general case these probabilities are not straightforward to compute, mainly because Pr([t]f \u2227 [t']\u00acf) may be much lower than Pr((f,t,t'))5. More-over, non-minimal explanations may have a significant probability, even in the case where {0} is an explanation, which contradicts the principle of minimal change. This is because elementary probabilities of change (the \u03b5\u03c6's) may be"}, {"title": "Proposition 9.", "content": "Assume that all fluents are highly persistent w.r.t. [0, tmax], then:\n1. Pr((f,t,t')) \u2248 Pr([t]f \u2227 [t']\u00acf) = (t' - t)\u03b5f.pf + O(\u03b5f).\n2. \u2211E\u2208Cme(\u03a3) Pr(E | \u03a3) \u2248\u03b5f\u21920,\u2200f 1.\nItem 9.2 means that non-minimal explanations for \u03a3 have a very low pos-terior probability and thus they can be neglected. In particular, if {\u2205} is an explanation for \u03a3 then Pr({0} | \u03a3) \u2248 1. Proposition 9.1 intuitively means that if f changed at least once its truth value within [t, t'] ((f, t, t')) then it changed exactly once (and thus \u00acf holds at t'). Now probabilities of minimal explana-tions are easy to compute (provided that all fluents are highly persistent). We start by giving two detailed examples."}, {"title": "Discussion", "content": "The original aspects of our work are that (i) minimal changes (explanations) are provided together with the interval when change may have occurred, which makes the representation concise, and (ii) knowing probabilities of change of fluents from one point to the subsequent one, and fluent prior probabilities, we compute the probability of each explanation and thus we rank them accordingly. We showed that minimizing change is coherent with a probabilistic handling of change only if probabilities of change are very small (with this assumption, only minimal explanations may have significant probabilities). This probabilis-tic handling of change needs explicit and metric time, and is coherent with the intuitive idea that the longer the time interval during which a change may have occurred, the more likely it has actually occurred within this interval. Many related works share some features with ours. The first approach dealing with unpredicted change (\"fluents that may change by themselves\") is Lifschitz and Rabinov's [11]; using the situation calculus and circumscription, they minimize the set of unpredicted changes from one state to another, giving, thus, some-thing analogous to our pointwise explanations (without the explicit temporal dimension). Our notion of \"surprise\" is borrowed from Sandewall's ontology [15] where surprises are defined as unpredicted changes with low frequency; in an earlier version of his book [14], he proposed to select among competing surprise sets by associating to each fluent a penalty (the higher the penalty, the more unlikely the fluent may change unpredictedly), preferred surprises sets minimiz-ing the sum of the penalties of changing fluents. Dealing with penalties is very analogous to dealing with probabilities, especially in the case of infinitely small probabilities [7], thus our approach extends his by taking interval durations into account in defining the penalties.\nA probabilistic handling of unpredicted change using explicit, metric time has been proposed for temporal projection by Dean and Kanazawa [6] and later on by Hanks and McDermott [10] in a more general setting. In [6], unpredicted change is modelled with exponential survivor functions, asserting that Pr([t +\n\u25b3t]f | [t]f) = exp(t1\u25b3t). They do not assume that probabilities of change are small, and thus do not minimize change. We recall that the reason why we want change to be minimized is that it leads to a concise list of explanations, since only minimal explanations may have significant probabilities. This has to be related to the fact that, in [6], probabilities of change are only used for temporal projection, namely, computing the probability of given fluents at the latest time point, but they are not used to deal with explaining scenarios and the reasoning is only forward (it has for instance no postdiction capabilities). Lastly, [6] framework assumes that observations are atomic (no disjunctive observations are allowed while our approach allows the observation of any propositional formula). The same remarks apply to [10].\nOther related approaches include work in the domains of probabilistic ab-duction and model-based diagnosis, temporal diagnosis, and also belief update. Probabilistic (and cost-based) abduction ([12], [8], [5]) attach probabilities to explanations from prior probabilities of hypotheses (faults in model-based diag-nosis) and generally an independence assumption amongst them. In the domain of temporal model-based diagnosis, Friedrich and Lackinger [9] attach time in-tervals to fault configurations, meaning that a given component is in a failure mode during the whole interval - contrarily to our surprises which are instan-taneous (recall that the interval in a surprise is understood disjunctively, not conjunctively); probabilities are attached to temporal diagnoses, the evolution of the system is modelled by Markov chains (with transition probabilities for the"}]}