{"title": "Personalized Item Embeddings in Federated Multimodal Recommendation", "authors": ["Zhiwei Li", "Guodong Long", "Jing Jiang", "Chengqi Zhang"], "abstract": "Federated recommendation systems play a crucial role in protecting user privacy. However, existing methods primarily rely on ID-based item embeddings, overlooking the rich multimodal information of items. To address this limitation, we propose a novel Federated Multimodal Recommendation System called FedMR. FedMR leverages a foundation model on the server side to encode multimodal data, such as images and text, associated with items. To tackle the challenge of data heterogeneity caused by varying user preferences, FedMR introduces a Mixing Feature Fusion Module on the client. This module dynamically adjusts the weights of different fusion strategies based on user interaction history, generating personalized item embeddings that capture fine-grained user preferences. FedMR is compatible with existing ID-based federated recommendation systems, improving their performances without modifying the original framework. Our experiments on four real-world multimodal recommendation datasets demonstrate the effectiveness of FedMR.", "sections": [{"title": "1 Introduction", "content": "Under privacy protection regulations, such as GDPR [60], that emphasize user data privacy, Federated Recommendation Systems (FedRec) have become an essential technology in modern life [56]. However, in these systems, since each client typically only has access to a single user's interaction history, data sparsity is a common issue that degrades recommendation performance [30]. Modeling item features as shared information across clients can effectively mitigate the impact of data sparsity. Many studies [31, 32, 79] have shown that personalized item feature representations can significantly improve recommendation results. Despite items often containing rich multimodal information [89], such as such as text and images, most existing FedRec methods [80, 82, 83] still rely mainly on ID embeddings, underutilizing the potential of multimodal data. Thus, generating personalized item embeddings using multimodal data while preserving user privacy remains a significant challenge.\nAs a potential solution, Multimodal Recommendation Systems (MMRec) integrate data from different modalities, significantly enhancing recommendation performance [18, 66, 92]. Unlike single-modal systems, MMRec can capture hidden relationships and complementary information between various modalities, leading to a better understanding of user preferences and item characteristics [89]. For example, MMRec methods [57, 64, 66] combine video frames, audio tracks, and item descriptions when users interact with online platforms, providing a more accurate representation of user preferences. To effectively handle multimodal data, MMRec often uses Foundation Models (FM) pretrained on large-scale datasets, which possess powerful feature extraction and representation capabilities [6]. By applying FMs, MMRec can efficiently extract and integrate information from multiple modalities, capturing commonalities and complementary relationships between them [8, 15, 16, 51, 69]. However, these centralized MMRec methods rely on servers to aggregate user data, digital activities, and preferences to train accurate models [40], which raises privacy concerns [7, 31].\nTo address these challenges, Federated Multimodal Recommendation Systems (FedMMRec) have emerged, which integrates MMRec and introduces the capability of processing multimodal information into FedRec. By utilizing multimodal data and user-specific information, such as profiles and interaction history, locally on each client to generate personalized recommendations and aggregating"}, {"title": "2 Related Work", "content": "model parameters at the server, FedMMRec enhances recommendation performance while preserving user privacy [11]. FedMMRec is still in the early stages of research, and related studies are limited. FedMMR [69] adopts a model-splitting strategy where the computationally intensive components are handled by the server, while clients perform lightweight collaborative filtering. AMMFRS [11] introduces an attention mechanism on the client side to perform weighted fusion of multimodal data, effectively distinguishing and utilizing the contributions of different modalities. These methods not only protect user privacy but also effectively integrate multimodal features to improve recommendation accuracy and personalization. However, they rely on fixed multimodal fusion strategies, neglecting the personalized representation of item features on clients, which limits their ability to adapt to diverse user needs. Although AMMFRS uses an attention mechanism to distinguish the importance of different modalities, it lacks adaptability to the diversity of user data, resulting in limited flexibility and accuracy of fusion strategies [46].\nMain Contribution. To integrate multimodal item information into FedRec while ensuring user privacy, we propose a Federated Multimodal Recommendation system (FedMR) based on foundation models. FedMR uses a foundation model on the server side to encode multimodal item data (e.g., images, text) into feature embeddings, and constructs a global item ID embedding to capture structural features among items. On each client, FedMR introduces a Mixing Feature Fusion Module that dynamically adjusts fusion strategies based on user interaction history, combining multimodal and ID embeddings to generate personalized item embeddings. This approach provides more personalized recommendations. Thus, FedMR seamlessly integrates multimodal information with existing ID-based FedRec methods, improving the flexibility and accuracy of recommendations without altering the existing framework, and effectively handles data heterogeneity across clients.\nThe main contributions of this work are summarized as follows:\n\u2022 We propose FedMR, a federated multimodal recommendation system using foundation models, which efficiently leverages multimodal data while preserving user privacy;\n\u2022 A Mixing Feature Fusion Module is designed to dynamically adjusts fusion strategies based on user interaction histories to generate personalized item embeddings, enhancing personalization and flexibility in recommendations;\n\u2022 FedMR is compatible with existing ID-based federated recommendation methods, improving recommendation performance without modifying the existing framework;\n\u2022 Extensive experiments are conducted on four real-world multimodal recommendation datasets to validate the effectiveness of our proposed method FedMR;\n\u2022 We provide the source code to support the reproducibility."}, {"title": "2.1 Federated Recommendation Systems", "content": "Federated recommendation systems (FedRec) aim to provide personalized recommendations while ensuring user privacy and data security [71]. In recent years, FedRec has made significant progress in areas such as rating prediction [29, 33, 35-37, 82], top-k recommendation [3, 9, 27, 43, 49, 50, 78], and addressing the cold-start"}, {"title": "2.2 Multimodal Recommendation Systems", "content": "Unlike traditional Recommendation Systems (RecSys) that rely primarily on user-item interaction data (such as clicks or ratings) to capture user behavior [73], Multimodal Recommendation systems (MMRec) aim to improve the accuracy of recommendations and enhance user experience by utilizing multiple modalities of data, such as text, images, audio, and video [89]. As unique identifiers for items, item IDs not only contain semantic information about the items but also capture the characteristics of neighboring items [42, 76]. Therefore, in MMRec, item IDs are often used as auxiliary information and combined with multimodal data to enhance item representations [62, 86]. This integration helps capture a more comprehensive understanding of users' interests and preferences, ultimately improving the quality of recommendations. A typical MMRec training process generally involves feature extraction and modality fusion [18, 57, 64, 66]. Feature extraction [17] aims to obtain representative features from multiple modalities to better capture the attributes and preferences of users and items. Modality fusion [14] focuses on effectively integrating data from different modalities to enhance the overall performance of the MMRec and the relevance of its results. The recommendation models [58, 68, 91] combine multimodal data with user-item interaction data to predict user preferences and provide personalized recommendations."}, {"title": "2.3 Foundation Models", "content": "Foundation Models (FM) are often pre-trained on large-scale unlabeled datasets for general purposes, that can be adapted to a variety of downstream tasks through appropriate fine-tuning, such as task-specific training [6]. This makes them highly generalizable and adaptable across different applications. They are then adapted to specific recommendation tasks through methods such as representation transfer [10, 38, 39, 65], model fine-tuning [25, 26, 55, 63, 85], adapter tuning [5, 13, 16, 20], and prompt tuning [28, 54, 70, 88, 90]."}, {"title": "3 Problem Formulation", "content": "Let $U = \\{u\\}$ and $I = \\{i\\}$ be the set of users and items correspondingly, we assume that the users' interaction history data is implicit feedback, represented by $R = [r_{1}, r_{2}, ...,r_{n}]^{T} \\in \\{0,1\\}^{|U|\\times|I|}$, where $r_{u,i} = 1$ if user $u$ has interacted with item $i$, and $r_{u,i} = 0$ otherwise. For user $u$, we use $r_{u} \\in \\{0, 1\\}^{|I|}$ to denote their interaction history and $I_{u}$ to represent the set of items they have interacted with. Moreover, we define $O = \\{(u, i) \\text{ user } u \\text{ has interacted with item } i\\}$ to index the items each user has interacted with. Each item in $I$ has a set of modality features $M$. The m-th modality feature is represented as $E_{m} = [e_{1,m}, e_{2,m}, ..., e_{|I|,m}]$. To capture user diversity, FedMR also introduces a set of modality fusion strategies $G = \\{g_{1}, g_{2},\u2026,g_{|G|}\\}$. In this work, we focus on the visual and textual modalities, denoted as $M = \\{v, c\\}$, respectively. It is important to note that FedMR is not limited to these two modalities and the existing fusion strategies, it can include additional modalities or modify the fusion strategies as needed."}, {"title": "3.1 Objective of FedRec", "content": "In classic FedRec with implicit feedback, each client only has the user's historical interaction data $r_u$. The goal is to train a personalized recommendation model for each user while protecting user privacy, providing personalized recommendations by predicting possible scores $\\hat{r}_{u,i}$ for items the user has not yet interacted with, based on their observed historical data $r_u$. To measure the difference between the prediction $\\hat{r}_u$ and the truth $r_u$, we have the following:\n$\\min \\sum_{u \\in U} \\alpha_u \\sum_{(u,i) \\in O} L_{recon} (\\hat{r}_{u,i}, r_{u,i}).$ (1)\nHere, the function $L_{recon}$ measures the reconstruction error between $\\hat{r}_{u,i}$ and $r_{u,i}$, and $\\alpha_u$ represents the loss weight for the client $u$ ($u \\in U$), which is used to balance the client's contribution to the overall training, and satisfies the condition $\\sum_{u \\in U} \\alpha_u = 1$. By optimizing Eq. (1), FedRec aims to make the predicted item scores $\\hat{r}_{u,i}$ for each user $u$ as close as possible to their true interactions $r_{u,i}$."}, {"title": "3.2 ID-based FedRec", "content": "Currently, most FedRec methods [31-33, 35, 36, 49, 79, 82] construct a global item embedding $D$ to capture the items' features and their relationships with neighboring items, forming $\\hat{r}_u$ as follows:\n$\\hat{r}_u = f(D; \\theta_u),$ (2)\nwhere the function $f$, driven by the user-specific parameters $\\theta_u$, generates personalized predictions $\\hat{r}_u$ upon receiving $D$. However, these methods overlook the rich item multimodal information. Therefore, we consider such methods as ID-based FedRec in this work."}, {"title": "4 Methodology", "content": "The aim of our proposed method FedMR is to integrate the multimodal features of items with ID embeddings within a federated environment, and incorporates these features into existing ID-based FedRec methods. By doing so, we transform current FedRec methods into FedMMR methods, allowing them to fully utilize the multimodal features of items to predict items that users have not yet viewed but may find interesting, while protecting user privacy."}, {"title": "4.1 Encoding Multimodal Information with FM", "content": "In RecSys, users typically interact with only a small subset of all available items, which are usually stored on the server. Therefore, in multimodal scenarios, we assume the server holds the complete modal information of the item set. FedMR begins by introducing a foundation model $h$ on the server side to encode the items' modal information, producing feature embeddings for each modality:\n$V = h(E_v), C = h(E_c).$ (3)\nEq. 3 has two main advantages: 1) It transforms the visual or textual modality $E_m$ into a format that the recommendation model can process; 2) Compared to transmitting raw modality, the encoded feature embeddings require less communication overhead, thus improving communication efficiency. Moreover, FedMR also builds an id embedding $D$ to capture the structure information of items."}, {"title": "4.2 Mixing Feature Fusion Module", "content": "Considering the diverse interests and behavior patterns of users, FedMR incorporates a Mixing Feature Fusion Module (MFFM) that adopts multiple modality fusion strategies $\\{g_j\\}$ from $G$ to fuse the multimodal feature embeddings $V, C$, and the id embedding $D$:\n$F_j = g_j(V, C, D; y_j),$ (4)\nwhere $y_j$ is parameters of the fusion strategy $g_j$, and $F_j$ is the feature embedding obtained after applying the j-th fusion strategy. To capture user diversity by enabling each strategy to focus on specific user groups based on their interaction histories $r_u$, FedMR introduces a user-specific router $\\varphi_u$, driven by the parameters $\\phi_u$:\n$w_u = \\varphi_u(\\{F_j\\}_{i \\in I_u}; \\phi_u).$ (5)\nThe router $r_u$ dynamically generates a weight vector $w_u \\in \\mathbb{R}^{|G|}$ based on the feature embeddings of items the user has interacted with, obtained through different fusion strategies, and assigns weights to each fusion strategy, i.e., the weight of the strategy"}, {"title": "Algorithm", "content": "$g_j$ is $w_{u,j}$. Then, we have the final fused features $F$ as follows:\n$F = \\sum_{j=1}^{|G|} w_{u,j}F_j.$ (6)\nAccording to Eq. (6), FedMR dynamically integrates and weights each user's $u$ multimodal information and ID features to generate the personalized item embeddings $F$, enhancing the system's ability to capture complex user interaction patterns."}, {"title": "4.3 Objective of FedMR", "content": "By replacing $D$ in Eq. (2) with $F$ obtained from Eq. (6) and then substituting into Eq. (1), we have the objective of FedMR as follows:\n$\\min_{ \\{\\theta_u\\}, \\{\\phi_u\\}, \\{y_j\\}, D} \\sum_{u \\in U} \\alpha_u \\sum_{(u,i) \\in O} L_{recon} (f(F; \\theta_u), r_{u,i}).$ (7)\nEq. (7) defines the objective of FedMR and is compatible with most existing ID-based FedRec methods. By replacing the original reconstruction error calculation function and the values of $\\{\\alpha_u\\}$ in these methods with Eq. (7), they can integrate the multimodal information of items into personalized recommendations while protecting user privacy. This leads to a more comprehensive understanding of item features and enhances the model's grasp of user preferences."}, {"title": "4.4 Framework", "content": "Fig. 1 illustrates the overall framework of FedMR, which leverages a FM to process multimodal information for enhancing personalized recommendations. On the server side, FedMR uses item cover images as visual modality data and titles as textual modality data. These inputs are processed by the FM to generate fixed visual embeddings $V$ and text embeddings $C$. Since both the FM and the input data are fixed, these item feature embeddings do not need to be updated during training. Additionally, the server maintains an ID embedding $D$ to capture the structural information of the items.\nUpon receiving the feature data $V, C$ and $D$ from the server, the client filters the embeddings of items that the user has interacted with and performs fusion using the MFFM. Within this module, a learnable user-specific router $\\phi_u$ dynamically assigns weights to the fused embeddings for further weighted fusion. The fusion"}, {"title": "4.5 Algorithm", "content": "strategies include a Sum strategy, a MLP [59] strategy, and a Gate strategy. The fused features $F$ are then fed into the prediction function $f$ to generate predicted scores $\\hat{r}_{u,i}$ for items, and the local model parameters are optimized by minimizing reconstruction loss.\nDuring the entire process, only the ID embedding $D$ and the gradients of fusion strategies in the MFFM are sent back to the server for aggregation and updates, hereby protecting user privacy while integrating shared information across clients. The core of FedMR lies in dynamically adjusting weights in the MFFM based on the user's local interactions $r_u$, thereby enhancing the diversity and personalization of the RecSys while ensuring privacy protection.\nAs a result, FedMR is compatible with existing ID-based FedRec methods, allowing multimodal item information to be integrated into these methods. Furthermore, FedMR can be extended by incorporating more modalities or adjusting fusion strategies as needed.\nTo optimize the FedMR objective function defined in Eq. (7), we propose an alternating optimization algorithm for training FedMR. The detailed steps of the algorithm are presented in Alg. 1.\nThe FedMR framework consists of two main components: the GlobalProcedure and the ClientUpdate. In the GlobalProcedure, the server first uses a FM $h(\\cdot)$ to encode the multimodal data, generating the corresponding embeddings $V$ and $C$. The server then randomly selects a subset of clients to participate in each training round to reduce the risk of privacy leakage caused by clients participating in consecutive rounds [31]. For each selected client $u \\in S_t$, the client update process is executed to obtain the local prediction result $\\hat{r}_{u,i}$, the local updated id embeddings $D^{(u)}$, and the local accumulated gradient $\\{\\nabla_{\\gamma_j}^{(u)}\\}$. The server aggregates the results returned by the clients in a weighted manner, and updates the global shared embedding $D$ and fusion strategy parameters $\\{y_j\\}$.\nDuring the ClientUpdate, each client calculates the gradients of the model parameters, such as $\\nabla_{\\theta_u}$, $\\nabla_{\\phi_u}$, $\\nabla_{D}$, and $\\{\\nabla_{\\gamma_j}^{(u)}\\}$, based on the FedMR objective function in Eq. (7) and updates these parameters using gradient descent [2]. Additionally, each client accumulates the local gradient $\\nabla_{\\gamma_j}^{(u)}$ into $\\nabla_{\\gamma_j}$ for the global update of the fusion"}, {"title": "4.6 Complexity Analysis", "content": "strategy parameters. By incorporating a FM $h(\\cdot)$ to handle multimodal data $M$, FedMR achieves efficient personalized multimodal recommendation while preserving user privacy.\nSince FedMR is compatible with various FedRec methods, we focus on the time and space complexity introduced by incorporating multimodal data of items in this work. Assuming n users, m items and q experts, FedMR, where the multimodal information of the items is encoded on the server side into feature embeddings $V \\in \\mathbb{R}^{m\\times d}$ and $C \\in \\mathbb{R}^{m\\times d}$ using a foundation model, with $d \\ll \\{n, m\\}$. Since these embeddings remain fixed during training, they contribute a space complexity of $O(md)$ while having negligible impact on time complexity. FedMR also maintains an ID embedding $D \\in \\mathbb{R}^{m\\times d}$ to capture structural information of the items, with a space complexity of $O(md)$. During each communication round, the server aggregates and updates local data from the clients, requiring a time complexity of $O(aumd(q + 1))$, while each client performs the Mixing Feature Fusion Module to fuse features, resulting in a total client-side space complexity of $O(md + K)$ and a time complexity of $O(P)$, where $K$ and $P$ represents the size and cost time of the component parameters, such as Router, MLP, Gate, etc, respectively. Therefore, FedMR requires a total space complexity of $O(n(md+K) +3md)$ and a total time complexity of $O(nmd(q + 1) + nP)$."}, {"title": "4.7 Privacy Preservation", "content": "FedMR uses a distributed architecture based on FL to effectively reduce the risk of user privacy leakage. Except for the Mixing Feature Fusion Module, all other components of FedMR have the same privacy risk level as existing ID-based federated recommendation methods. In the Mixing Feature Fusion Module, only the locally updated ID embeddings $D^{(u)}$ and the local accumulated gradients $\\nabla_{\\gamma_j}^{(u)}$ are sent to the server, without revealing any raw user data, also maintaining the same communication privacy protection strategy as current methods [11, 31, 32, 69, 79]. To protect user privacy, randomly selecting participants in each communication round and avoiding the same participants in consecutive rounds can effectively reduce the risk of privacy leakage [7, 31]. Additionally, FedMR can incorporate techniques such as differential privacy [1] and stochastic gradient clipping to further enhance privacy protection. We also conduct an ablation study to evaluate the effectiveness of FedMR when additional privacy-preserving measures are applied."}, {"title": "5 Experiments", "content": "In this study, we conducte comprehensive experiments on four downstream datasets from Ninerec\u00b9 [84] to evaluate the performance and effectiveness of the proposed method FedMR: KU, Bili_Food (Food), Bili_Dance (Dance), and Bili_Movie (Movie). Table 1 provides the detailed statistics of these datasets. The KU dataset is derived from KuaiShou, a well-known short video platform in China, where each entry represents a short video. The Food, Dance, and Movie datasets are collected from the BiliBili platform, focusing on food and cooking videos, dance-related videos, and movie-related videos, respectively. Each dataset presents unique content types and user interaction patterns, but all share a common challenge of high sparsity, with sparsity levels exceeding 95%. For our experiments, we include only users who had interacted with at least 5 items in each dataset to avoid the cold-start problem. Additionally, for missing images in the datasets, we fill them with the average values of visual features; for missing titles, we set \"The title is missing.\""}, {"title": "5.2 Baselines", "content": "To comprehensively evaluate the performance of FedMR, we select 3 cutting-edge centralized MMRec methods, and 5 ID-based federated methods commonly used in FedRec tasks with implicit feedback to serve as baselines in our study:"}, {"title": "5.3 Experimental Setting", "content": "Following previous studies [31, 79], we perform negative sampling for each positive sample in the training set and use the leave-one-out strategy for validation. We conduct hyperparameter tuning for all baseline methods on each dataset used, selecting the learning rate $\\eta$ and adjusting the hyperparameters in FedRAP's objective function from $\\{10^i | i = -4, . . ., -1\\}$. To ensure fairness, we use the CLIP [51] implemented by the Open CLIP [22, 52, 53] as the foundation model in our experiments, loading the ViT-B-32 parameters trained by OpenAI to process multimodal information. It should be noted that the foundation model in FedMR can be replaced with other"}, {"title": "5.4 Evaluation", "content": "models as needed. Additionally, we add a feature mapping layer before the input fusion layer of the multimodal embeddings and ID embeddings, mapping the feature dimensions output by the foundation model to a fixed latent embedding dimension, i.e. 64, to improve storage efficiency. We fix the training batch size at 2048 for all methods, and for methods with hidden layers, the number of layers is set to 3. For each federated baseline, we adopt the same aggregation strategy to update the global part of FedMR. In the main experiments, all clients participated in the training of both the original versions and the FedMR versions of all federated baselines, and no additional privacy protection measures are applied. Additionally, for these federated baselines, the number of local training epochs was set to 10. Except for loading the foundation model, we do not use any pre-training strategies for any methods.\nIn our experiments, we treat the observed interactions as positive samples and the rest as negative samples. We sort the predicted interaction scores between users and all candidate items in descending order, masking the observed positive samples from the training set. Then, we evaluate the recommendation performance using two widely-used metrics: Hit Rate (HR@K) and Normalized Discounted Cumulative Gain (NDCG@K), which are formally defined in [19]. In the experiments, we set K = 50."}, {"title": "5.5 Comparison Analysis", "content": "Table 2 shows the performance of both centralized and federated methods across the four datasets. The results highlight that FedMR significantly boosts the performance of the used FedRec methods, particularly in the KU dataset, where HR@50 for FedAvg, FCF, PFedRec, and FedRAP exhibits notable improvements. This indicates that FedMR effectively enhances traditional ID-based FedRec methods by integrating multimodal item information. The performance improvements in other datasets further demonstrate FedMR's ability to alleviate data sparsity by better leveraging users' interaction histories alongside multimodal data for more personalized recommendations, while protecting user privacy. Compared to centralized MMRec methods, FedRec approaches using FedMR achieve similar or even superior performance while maintaining privacy, e.g., in the KU dataset, FedAvg with FedMR surpasses all centralized methods in terms of HR@50. Federated methods with FedMR also perform"}, {"title": "5.6 Ablation Study", "content": "strongly across other datasets, reinforcing FedMR's capability to improve recommendation quality without sacrificing user privacy.\nTable 2 further shows that by incorporating multimodal information, FedMR provides a more comprehensive representation of item features. Compared to ID-based FedRec, the use of multimodal data enables more accurate predictions of user preferences. Additionally, the MFFM of FedMR dynamically adjusts the weights of visual, textual, and ID features based on each user's dependence on these modalities. This flexible approach to feature fusion avoids the drawbacks of single fusion strategy methods, allowing the model to better capture complex relationships between modalities and further enhance personalized recommendation accuracy.\nTo validate the effectiveness of each component in FedMR, we conducted experiments using several variants of the system. Specifically, we independently applied the Sum, MLP, and Gate strategies for the fusion of multimodal features and ID features, to assess the impact of dynamic weighted fusion based on users' interaction history $r_u, \\forall u \\in U$. Additionally, we introduce random Gaussian noise into the gradients uploaded to the server to evaluate the potential performance impact of this privacy-preserving measure. We also examine the distribution of multimodal and ID features within the same feature space. These ablation studies demonstrate the effectiveness of each component of FedMR."}, {"title": "5.6.1 Ablation Study on Modality Distributions", "content": "To investigate the distribution relationship between multimodal features and ID features, we select FedRAP [31] as the backbone for FedMR in this study. FedRAP maintains both globally shared and user-specific ID features, which helps demonstrate how different ID features affect the distribution within the feature space.\nFig. 2 visualizes the distribution of modality features and ID features after training FedRAP with FedMR, on the KU dataset. The first subfigure in Fig. 2 shows that when the multimodal features of items and globally shared ID features are mapped into a shared representation space, they remain in distinct subregions. This phenomenon, known as the \"Modality Gap\" [34, 87], helps the model distinguish between different modalities, reducing noise interference and improving the robustness of multimodal data processing. The following two subfigures incorporate the personalized ID embeddings of the users 0 and 42, respectively, highlighting the significant impact of personalized ID features on the distribution in the shared space. By comparing the first subfigure with the subsequent two, we observe that the introduction of personalized ID features causes the ID features to gradually expand towards the regions of visual and textual features, shrinking the modality gap. This facilitates the fusion of multimodal information and enhances the accuracy of personalized recommendations. Furthermore, by comparing the differences in modality gap changes for each user, it becomes evident that the model can adjust the feature distribution based on user-specific behaviors and preferences, thus better satisfying individual user needs.\nThis study underscores the importance of personalized ID features in federated recommendation systems. Due to privacy constraints, the model cannot access raw user data directly, but through"}, {"title": "5.6.2 Ablation Study on Modality Efficiency", "content": "the use of personalized ID features, the model can provide personalized recommendation services while maintaining user privacy.\nTable 3 presents a performance comparison of the five used ID-based FedRec methods with the inclusion of multimodal information. It evaluates these methods under the full model (FedMR) and with the removal of visual modality (w/o V), textual modality (w/o C), and ID features (w/o D), focusing on HR@50 and NDCG@50 metrics.\nFirstly, removing the visual modality results in a noticeable performance drop across all methods, particularly for FCF (HR@50 decreased by 71.67%) and FedRAP (HR@50 decreased by 30.78%). This highlights the critical role of visual information in RecSys, especially for FCF. Visual features provide rich, intuitive image-based information that helps models better capture user preferences regarding the appearance of items. Without this visual context, models struggle to accurately infer user preferences, leading to a significant decline in recommendation performance. Secondly, removing the ID feature also causes a substantial decline in performance. The ID feature is essential for personalized recommendations, as it captures users' historical interactions and reflects their past behaviors and interests. When the ID feature is removed, the model loses its primary mechanism for personalization, resulting in decreased accuracy and personalization of recommendations. This impact is especially pronounced in methods like PFedRec (HR@50 decreased by 82.82%) and FedRAP (HR@50 decreased by 51.92%), which heavily rely on ID features. In contrast, removing the text modality has a relatively smaller impact, though it still affects some methods. For instance, in FedAvg and PFedRec, removing the text modality led to HR@50 decreases of 27.63% and 43.21%, respectively. This indicates that text information plays an important role in capturing semantic relationships between user behaviors and item descriptions, helping the model better understand user needs and item characteristics, thereby enhancing recommendation performance.\nIn summary, all methods experienced performance degradation to varying degrees when any modality was removed. Considering Fig. 2, visual, text, and ID features are often complementary. Removing any one of these disrupts the balance, making it difficult for the model to fully represent user preferences and item characteristics. Additionally, different methods depend on different modalities to varying extents. For instance, FCF and PFedRec are more dependent on visual information, whereas FedAvg is less reliant on ID features. These differences in dependency lead to varying levels of"}, {"title": "5.6.3 Ablation Study on Fusion Strategy", "content": "performance decline when specific modalities are removed. Further experimental results are provided in the appendix.\nFig. 3 presents the HR@50 and NDCG@50 evaluation results for five used FedRec methods using different feature fusion strategies with FedMR on the KU dataset. The results indicate that different fusion strategies significantly impact performance. Notably, the Mix strategy, which dynamically combines multiple fusion methods based on user interaction history, outperforms others, while the Sum strategy shows poor performance. This trend is consistent across all metrics.\nThe Sum strategy simply adds different modality features without considering their complex interactions and inter-relationships. Since each modality's importance varies, simple addition can lead to information loss or imbalanced weighting, making it difficult to capture relationships effectively. Additionally, RecSys often face a long-tail distribution, where a few popular items dominate user interactions [47, 72]. The simplicity of the Sum strategy may lead to overfitting on popular items, resulting in poor performance on less interacted items and affecting overall model performance.\nIn contrast, the MLP strategy uses fully connected networks to learn nonlinear modality combinations, which better captures complex interactions between item modalities and ID features. However, its effectiveness is limited by network depth and the number of parameters. On the other hand, the Gate strategy learns adaptive weights to dynamically filter the contribution of each modality, optimizing the importance of different features."}, {"title": "5.6.4 Privacy Protection", "content": "The Mix strategy integrates all fusion methods and adjusts their weights dynamically based on user interaction history, allowing the model to capture more fine-grained personalized item representations. This approach enables more accurate matching of user preferences and effectively captures complex relationships between multimodal features, leading to superior performance.\nThis study highlights the importance of personalized item embeddings in FedRec. In FedMR, each client dynamically adjusts fusion strategy weights through the Mixing Feature Fusion Module, based on user interaction history, to generate personalized item embeddings, enhancing personalization and accuracy while maintaining user privacy, thus improving overall system performance.\nTable 4 presents the experimental results of the used FedRec methods combined with FedMR, comparing the performance with and without adding Gaussian noise (mean=0, variance=0.1) to the gradients of the global components uploaded to the server, to protect user privacy. The experiments are conducted on the KU dataset during feature fusion. The results indicate that all methods experienced some performance degradation after adding noise, particularly on the NDCG metric, which is more sensitive to ranking accuracy. This is because noise disturbs the gradient precision, which affects the model's ability to update effectively.\nAmong the methods, FedNCF is the most sensitive to noise, with HR@50 dropping by 14.33% and NDCG@50 by 31.03%. As a complex neural network-based model, FedNCF depends on numerous"}, {"title": "6 Conclusion", "content": "parameters to capture nonlinear features. The introduction of noise disrupts these feature interactions, resulting in a significant decline in performance. In contrast, FedAvg and FedRAP show greater robustness to noise, with only minimal performance degradation. This suggests that these methods are more effective at maintaining recommendation accuracy while ensuring privacy protection.\nIn conclusion, although adding noise helps protect user privacy, its impact varies based on model complexity and feature interaction mechanisms. Simpler models like FedAvg and adaptive models like FedRAP are more resilient to noise, while complex neural network models like FedNCF experience more significant performance drops due to the noise interference.\nThis paper presents FedMR, a federated multimodal recommendation system that incorporates a mixing feature fusion module on each client. This module dynamically adjusts fusion weights based on user interaction history, generating personalized item embeddings that capture fine-grained user preferences, thereby enhancing personalized recommendations. FedMR allows flexible modification of fusion strategies and supports different modalities of item data as needed. Experimental results show that FedMR performs well on four multimodal recommendation datasets and is compatible with existing ID-based methods without requiring changes to the underlying framework. This study offers new directions for the development of federated recommendation systems, particularly in the application of multimodal data and foundation models."}, {"title": "C.1 Ablation Study on Client Sampling", "content": "To investigate the impact of introducing FedMR for multimodal data"}]}