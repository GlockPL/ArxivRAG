{"title": "Gradient Weight-normalized Low-rank Projection for Efficient LLM Training", "authors": ["Jia-Hong Huang", "Yixian Shen", "Hongyi Zhu", "Stevan Rudinac", "Evangelos Kanoulas"], "abstract": "Large Language Models (LLMs) have shown remarkable per- formance across various tasks, but the escalating demands on computational resources pose significant challenges, par- ticularly in the extensive utilization of full fine-tuning for downstream tasks. To address this, parameter-efficient fine- tuning (PEFT) methods have been developed, but they of- ten underperform compared to full fine-tuning and struggle with memory efficiency. In this work, we introduce Gradient Weight-Normalized Low-Rank Projection (GradNormLoRP), a novel approach that enhances both parameter and memory efficiency while maintaining comparable performance to full fine-tuning. GradNormLoRP normalizes the weight matrix to improve gradient conditioning, facilitating better conver- gence during optimization. Additionally, it applies low-rank approximations to the weight and gradient matrices, signif- icantly reducing memory usage during training. Extensive experiments demonstrate that our 8-bit GradNormLoRP re- duces optimizer memory usage by up to 89.5% and enables the pre-training of large LLMs, such as LLaMA 7B, on consumer- level GPUs like the NVIDIA RTX 4090, without additional inference costs. Moreover, GradNormLoRP outperforms exist- ing low-rank methods in fine-tuning tasks. For instance, when fine-tuning the RoBERTa model on all GLUE tasks with a rank of 8, GradNormLoRP achieves an average score of 80.65, surpassing LoRA's score of 79.23. These results underscore GradNormLoRP as a promising alternative for efficient LLM pre-training and fine-tuning.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) pre-trained on extensive datasets have demonstrated exceptional effectiveness across various domains (Devlin et al. 2019; Liu et al. 2019a; \u041d\u0435 et al. 2022; Xie et al. 2022; Baevski et al. 2020; Lu et al. 2019; Tan and Bansal 2019). As time progresses, open-source LLMs have consistently improved in their capabilities, ac- companied by a striking increase in the scale of pre-trained models (Raffel et al. 2020a; Zhang et al. 2022; Le Scao et al. 2023; Touvron et al. 2023; Tay et al. 2023). Consequently, employing full fine-tuning, where all learnable parameters of a pre-trained model are updated for performing down- stream tasks, poses unparalleled challenges despite its track record of delivering numerous state-of-the-art results. These challenges primarily stem from the escalating demands on computational resources.\nTo tackle the aforementioned challenge, researchers have developed parameter-efficient fine-tuning (PEFT) techniques (Houlsby et al. 2019; Hu et al. 2022; Lialin et al. 2023; Liu et al. 2024; Kopiczko, Blankevoort, and Asano 2024). These methods are tailored to update only a small amount of task- specific parameters while leaving the majority of the model's parameters unchanged. Among these techniques, low-rank approximation-based approaches utilize low-rank matrices to approximate weight changes during training, achieving both parameter and memory efficiency without requiring additional trainable subnetworks to be added to the original model architecture.\nDespite their advantages, low-rank-based methods often underperform compared to full-rank fine-tuning (Hu et al. 2022; Lialin et al. 2023; Liu et al. 2024). This performance gap is typically attributed to the reduced number of train- able parameters, but other underlying factors, such as altered gradient dynamics due to reparameterization, also play a sig- nificant role. In Figure 2 of our Appendix\u00b9, we observe that the gradient descent process can become neither smooth nor stable when fine-tuning LLMs in an unnormalized subspace. This instability arises from conducting gradient descent on an incomparable scale, where some values are excessively large or small. Such numerical instability can lead to overflow or underflow during computations, negatively impacting the op- timization process and resulting in suboptimal performance. To mitigate this problem, we propose our method, Gradi-"}, {"title": "Related Work", "content": "Parameter-Efficient Fine-Tuning. Numerous PEFT meth- ods have emerged to address the computational challenges of fully fine-tuning LLMs. These methods can be grouped into those that increase model complexity and those that maintain or minimally modify the initial architecture. The first group, including methods like (Liao, Tan, and Monz 2023; Zhao et al. 2024a; Houlsby et al. 2019; Rebuffi, Bilen, and Vedaldi 2017; Gomez et al. 2017a; Pfeiffer et al. 2020; R\u00fcckl\u00e9 et al. 2020; Li and Liang 2021; Lester, Al-Rfou, and Constant 2021; Hambardzumyan, Khachatrian, and May 2021; Liu et al. 2023), often incorporate trainable adapter layers or optimize input layer activations, which can add inference latency and pose challenges in large-scale, latency-sensitive environments. The second group of methods, including (Liu et al. 2024; Hu et al. 2022; Lialin et al. 2023), utilizes low- rank matrices to approximate weight changes during training. These low-rank matrices are designed to integrate seamlessly with pre-trained weights before inference, ensuring that no additional inference overhead is introduced. Our proposed GradNormLoRP belongs to this second category, leveraging the advantages of low-rank approximation methods without introducing extra inference latency.\nGradient Projection. Gradient projection is used for rapid low-rank estimation (Chen and Wainwright 2015; Chen, Raskutti, and Yuan 2019; Zhao et al. 2024b). The work in (Chen and Wainwright 2015; Chen, Raskutti, and Yuan 2019) treats the objective function as a general non-linear function, analyzing gradients in vector space. GaLore (Zhao et al. 2024b), however, considers the specific structures of"}, {"title": "Methodology", "content": "In this section, we detail the key components of our Grad- NormLoRP and establish a theorem that theoretically demon- strates the effectiveness of GradNormLoRP in preserving the integrity of training dynamics. Please consult Algorithm 1 for a more comprehensive grasp of our GradNormLoRP."}, {"title": "Background", "content": "Weight Vector Normalization. Weight vector normalization is a technique that can be employed to expedite the conver- gence of the stochastic gradient descent optimization process (Srebro and Shraibman 2005; Salimans and Kingma 2016). We consider standard neural networks in which each neuron's computation involves calculating a weighted sum of input features, followed by a component-wise non-linearity:\n$y = \\theta((\\mathbf{w}^T\\mathbf{a}) + b) = \\theta((\\mathbf{w}, \\mathbf{a}) + b),$ (1)\nwhere $\\mathbf{w} \\in \\mathbb{R}^{k\\times 1}$ represents a weight vector, $\\mathbf{a} \\in \\mathbb{R}^{k\\times 1}$ signifies an input feature vector, $b \\in \\mathbb{R}$ indicates a bias term, $(\\cdot, \\cdot)$ denotes the inner product, $\\theta(\\cdot)$ is an component-wise non-linearity, e.g., the logistic activation $\\frac{\\exp(\\cdot)}{1+\\exp(\\cdot)}$, and $y$ indicates the scalar output of the neuron.\nAfter a loss function is associated with one or more neuron outputs, the parameters $\\mathbf{w}$ and $b$ for each neuron are typically optimized using stochastic gradient descent during the train- ing of such a neural network. To enhance the convergence of the optimization process, a reparameterization operation is introduced to express each weight vector $\\mathbf{w}$ in terms of a parameter vector $\\mathbf{v}$ and a scalar parameter $\\delta$:\n$\\mathbf{\\omega} = \\delta \\frac{\\mathbf{v}}{|\\mathbf{v}||},$ (2)\nwhere $\\delta \\in \\mathbb{R}$ denotes a scalar, $\\mathbf{v} \\in \\mathbb{R}^{k\\times 1}$, and $|\\cdot||$ indicates the Euclidean norm.\nThis reparameterization, which decouples the weight vector's norm $(\\delta)$ from the direction of the weight vector $(\\frac{\\mathbf{v}}{|\\mathbf{v}||})$, fixes the Euclidean norm of the weight vector, yielding $|\\mathbf{w}|| = \\delta$, which remains independent of the parameter vector $\\mathbf{v}$. After employing the reparameterization weight normalization process, we obtain:\ny = \\theta((\\mathbf{w}, \\mathbf{a}) + b) = \\theta((\\delta \\frac{\\mathbf{v}}{|\\mathbf{v}||}, \\mathbf{a}) + b) = \\theta((\\delta \\frac{\\mathbf{v}}{|\\mathbf{v}||},\\frac{\\mathbf{a}}{|\\mathbf{a}||}) + b). (3)\nSubsequently, the optimization process of stochastic gradient descent is conducted to the new parameters $\\mathbf{v}$ and $\\delta$ instead. In our proposed GradNormLoRP approach, we conduct the operation of reparameterization weight normalization on each column weight vector of a given weight matrix, resulting in a normalized weight matrix.\nChallenges in Memory Efficiency for PEFT. As discussed in (Raffel et al. 2020a; Zhao et al. 2024b; Liao, Tan, and Monz 2023; Touvron et al. 2023), the primary memory consumption during neural network training is attributed to activations, trainable parameters, and gradients of these parameters, along with optimizer states such as gradient momentum and variance in Adam (Kingma and Ba 2017). In this subsection, we employ a $T$-layer multilayer per- ceptron to illustrate the main origin of the memory ef- ficiency issue inherent in low-rank approximation-based PEFT methods. Consider a $T$-layer multilayer perceptron:\n$h_\\tau = \\xi_\\tau(\\xi_{\\tau-1}(. . . (\\xi_2 (\\xi_1 (h_0))) ...))$ with $h_0$ as the initial input, where the $t$th layer $h_t = \\pounds_t(h_{t-1}) = \\Phi_t(W_th_{t-1})$ comprises a nonlinear function $\\Phi_t$ and a weight matrix $W_t$, neglecting the bias term for simplicity. Let $\\psi_t = W_th_{t-1}$. During the process of backpropagation with a loss $L$, the gradient of $W_t$ is computed using the chain rule as:\n$\\frac{\\partial L}{\\partial W_t} = \\frac{\\partial h_\\tau}{\\partial W_t} = \\left(\\prod_{i=t+1}^\\tau \\frac{\\partial h_i}{\\partial h_{i-1}}\\right)^T \\frac{\\partial h_t}{\\partial \\psi_t} \\frac{\\partial \\psi_t}{\\partial W_t} = \\left(\\prod_{i=t+1}^\\tau \\frac{\\partial \\Phi_i}{\\partial h_{i-1}} W_i\\right)^T \\Phi'_t h_{t-1} \\frac{\\partial h_\\tau}{\\partial \\psi_\\tau} \\frac{\\partial \\psi_\\tau}{\\partial W_t}$ (4)\nwhere $\\psi_i = W_i h_{i-1}$, and $\\frac{\\partial \\Phi}{\\partial h_{i-1}} = \\Phi'_i$,\\\n$h_{t-1}, \\frac{\\partial h_t}{\\partial \\psi_t} = \\Phi'_t , ht-1$.\nSince $\\Phi'_t$ represents the derivative of $\\Phi_t$ and the compu- tation of $\\psi_t$ relies on $\\Phi'_t$, caching the sequence of activations {$\\psi_i$}$_{i=1}^t$ during the forward pass is essential to compute the gradient of $W_t$, even though {$W_i$}$_{i>t}$ remain frozen. In con- trast to full fine-tuning, existing low-rank approximation- based PEFT methods adjust only a limited number of pa- rameters, resulting in a negligible size of the optimizer state (Liu et al. 2024; Hu et al. 2022; Lialin et al. 2023; Kopiczko, Blankevoort, and Asano 2024). Nevertheless, there is no sig- nificant reduction in the memory consumption required for activations. Take BERTbase fine-tuned on the RTE benchmark"}, {"title": "Our Proposed GradNormLoRP", "content": "Gradient Projection. The efficacy of existing low-rank approximation-based PEFT approaches, such as LoRA (Hu et al. 2022), often falls short in comparison to full fine-tuning, primarily due to their limited number of trainable param- eters and the potential change of gradient training dynam- ics resulting from the low-rank reparameterization process (Xia, Qin, and Hazan 2024; Zhao et al. 2024b; Kopiczko, Blankevoort, and Asano 2024). A promising avenue to miti- gate this challenge is through gradient projection techniques (Chen and Wainwright 2015; Chen, Raskutti, and Yuan 2019; Zhao et al. 2024b). The core concept behind gradient pro- jection is to leverage the gradual evolution of the low-rank structure within the gradient of a weight matrix, instead of di- rectly approximating the weight matrix as done in the LoRA method. This principle is grounded on the claim that the gradient tends to exhibit low-rank characteristics as train- ing progresses. In this subsection, we substantiate this claim through rigorous proof.\nWeight Matrix Updates in Conventional Full-Rank Train- ing. Given $D_\\tau = -\\nabla_wL(W_\\tau) \\in \\mathbb{R}^{k\\times m}$ as the representa- tion of the backpropagated negative gradient matrix at time step $t$, the traditional pre-training weight update with a learn- ing rate $a$ can be expressed as follows:\n$W_T = W_0 + \\alpha \\sum_{\\tau=0}^{T-1} D_\\tau = W_0 + \\alpha \\sum_{\\tau=0}^{T-1} \\pi_\\tau (D_\\tau),$ (5)\nwhere $D_t$ represents the final processed gradient added to the weight matrix, and $n_t$ denotes a component-wise stateful gradient regularizer, such as Adam.\nWeight Matrix Updates in Low-Rank Approximation- Based Methods. For a linear layer with a weight matrix $W\\in \\mathbb{R}^{k\\times m}$, approaches, such as LoRA, which are based on low-rank approximation, leverage the low-rank structure of the update matrix by introducing a low-rank adaptor IJ.\n$W_T = W_0 + I_\\tau J_\\tau,$ (6)\nwhere $I \\in \\mathbb{R}^{k\\times r}, J\\in \\mathbb{R}^{r\\times m}$, and $r \\ll \\min(k, m)$. I and J denote the trainable low-rank adaptors, while $W_0$ stands as a fixed weight matrix, such as a pre-trained weight matrix.\nWhile low-rank updates are suggested to alleviate memory consumption, there is ongoing debate regarding whether the weight matrix should inherently adopt a low-rank parameteri- zation. This assumption may not hold in various scenarios, such as linear regression. However, the gradient often exhibits low-rank characteristics during training, especially with spe- cific gradient forms and associated network architectures (Zhao et al. 2024b). The proof for Lemma 1 is available in our Appendix.\nLemma 1 (Gradient Becoming Low-rank during Train- ing). Given $W_t \\in \\mathbb{R}^{k\\times m}$, where we assume $k < m$ with- out loss of generality. Consider the gradient matrix Dt =\nA - $BW^T+C$, where A denotes a constant matrix, B and C both are positive semidefinite (PSD) matrices, and Wo is ran- domly initialized. Then, the gradient in the update of weight matrix $W_t = W_{t-1} + aD_{t-1}$ results in low-rank gradient with high probability:\n$\\text{stable-rank}(D_t) \\leq 1 + \\sum_{i=2}^k \\mathcal{O}\\left(\\left(\\frac{1 - \\alpha \\lambda_i \\nu_1}{1 - \\alpha \\lambda_1 \\nu_1}\\right)^{2t}\\right),$ (7)\nwhere $v_1 = \\min(C)$ is the smallest eigenvalue of C and $\\lambda_1 \\leq ... \\leq \\lambda_m$ are eigenvalues of B. Moreover, if C is positive definite, i.e., $v_1 > 0$, and $12 > 1, Dt$ converges exponentially to rank-1.\nNormalization of Weight Matrix. The initial phase of our proposed GradNormLoRP involves normalizing a provided weight matrix W. This normalization entails reparameteriz- ing each column vector of the weight matrix using the oper- ation introduced in section \"Weight Vector Normalization\". The normalization process of the weight matrix $W \\in \\mathbb{R}^{k\\times m}$ can be expressed as follows:\nW = \\frac{W}{\\|W\\\\_c} \\frac{W}{\\|W\\\\_c} = M \\frac{W}{\\|W\\\\_c} (8)\nwhere $M \\in \\mathbb{R}^{1\\times m}$ indicates the reparameterized, i.e., train- able, length vector, $W/\\|W\\|_c \\in \\mathbb{R}^{k\\times m}$ represents the direc- tional matrix, and $|\\cdot||$ denotes the vector-wise matrix norm operated across each column.\nAfter performing the reparameterization weight normaliza- tion operation column-wise on the weight matrix, we have dis- entangled the magnitude of the weight vectors from their di- rection. This process ensures that each column of $W/\\|W\\|_c$ becomes a unit vector with an associated scalar. Each scalar element in vector M represents the length of a corresponding vector in weight matrix W.\nLow-rank Approximation. The proposed GradNormLoRP is initialized with pre-trained weight Wo as shown in Equa- tion (8), where M = $||W_0||_c$ and W = $W_0$ after initializa- tion. Subsequently, we freeze W while making M serve as a trainable vector. The directional matrix is then updated using low-rank approximation techniques, such as LoRA. Grad- NormLoRP can be formulated similarly to Equation (6) as follows:\nW = M \\frac{W_0 + IJ}{\\|W_0 + IJ\\\\_c}, (9)\nwhere M represents a vector comprising trainable parame- ters, while the weight matrices $I \\in \\mathbb{R}^{k\\times r}$ and $J \\in \\mathbb{R}^{r\\times m}$ are initialized following LoRA's approach to guarantee that W equals Wo before fine-tuning.\nAs the introduced low-rank approximation in our GradNorm- LoRP can be merged with the pre-trained weight before in- ference, it does not introduce any additional latency in the inference phase.\nGradient Projection Process. To enhance the convergence of the optimization process while simultaneously reducing memory usage during training, we integrate the gradient pro- jection technique introduced in section \"Gradient Projection\" into our proposed GradNormLoRP.\nSingular Value Decomposition (SVD) and Projection Ma- trices. In this study, we utilize SVD to obtain projection"}, {"title": "Experiments", "content": "In this section, we evaluate the efficacy of our proposed Grad- NormLoRP through a series of experiments. We assess its performance in fine-tuning and pre-training scenarios and conduct a thorough throughput analysis to confirm that Grad- NormLoRP integrates seamlessly without adding inference latency. Additionally, we perform comprehensive ablation studies to highlight GradNormLoRP's characteristics, includ- ing convergence speed, parameter efficiency, and GPU mem- ory utilization.\nExperimental Setup\nDatasets, Evaluation Metrics, Model Architectures, and Baselines. For fine-tuning, we use the GLUE benchmark (Wang et al. 2019), which includes single-sentence tasks (COLA, SST-2), similarity and paraphrase tasks (MRPC, QQP, STS-B), and inference tasks (MNLI, QNLI, RTE, WNLI). Evaluation metrics are accuracy for MNLI, QQP, QNLI, SST-2, MRPC, and RTE, Pearson and Spearman cor- relation for STS-B, and Matthews correlation for CoLA. For pre-training, we use the C4 dataset (Raffel et al. 2020b), a cleaned version of Common Crawl's web corpus, with per- plexity as the performance metric.\nIn our fine-tuning experiments, we use BERTbase (Devlin et al. 2018), ROBERTabase, ROBERTalarge (Liu et al. 2019a), and BARTbase (Lewis et al. 2019) for all GLUE tasks. For pre-training, we adopt the LLaMA model architecture, train- ing on the C4 dataset with no data repetition, scaling up to 7 billion parameters.\nOur primary baseline for fine-tuning is full parameter updat- ing. For PEFT experiments, we compare GradNormLoRP against LORA (Hu et al. 2022), DoRA (Liu et al. 2024), and GaLore (Zhao et al. 2024b), which are also used as baselines for our pre-training experiments due to their relevance in low-rank approximation methods.\nImplementation. For fine-tuning, we evaluated our model on the GLUE benchmark, exploring learning rates in the range of {1e-4, 2e-4, 3e-4, 4e-4, 5e-4}, batch sizes of 16 and 32, and a fixed number of 30 epochs. Specifically, we used a batch size of 16 for all tasks except for CoLA, which used a batch size of 32. The maximum sequence length for all tasks was set to 512 for BERTbase, ROBERTabase, ROBERTalarge, and BARTbase models. For pretraining, we applied GradNormLoRP across various model sizes ranging from 60M to 1B parameters. The hyperparameters for GradNormLoRP were consistent across all models, with a learning rate of 0.01 and a scale factor (as) of 0.25. The learning rate was fine-tuned from the set {1e-2, 1e-3,5e-4,1e-4}, selecting the best rate based on validation perplexity. Each model was pre-trained for 10,000 steps. For models scaled up to 7B parameters, we set the batch size to 16 and varied the training steps accordingly.\nResults and Analysis\nQuantitative Results for Fine-tuning. We fine-tune pre- trained ROBERTa models on GLUE tasks using GradNorm- LoRP and compare its performance with a full fine-tuning 'baseline, LoRA, DORA, and GaLore. We use hyperparame- ters from (Hu et al. 2022) for LoRA and (Liu et al. 2024) for"}, {"title": "Conclusion", "content": "GradNormLoRP addresses the growing computational de- mands of full fine-tuning for LLMs by enhancing parameter and memory efficiency while maintaining comparable perfor- mance. By normalizing weight matrices, applying low-rank approximation, and utilizing gradient low-rank projection, GradNormLoRP significantly reduces memory overhead dur- ing training without adding inference burden. We validate its effectiveness both mathematically and empirically. Our experiments demonstrate its efficacy in LLM pre-training and fine-tuning, achieving comparable or superior results to existing PEFT methods."}, {"title": "Appendix", "content": "Proof of Our Proposed Theorem 1\nTheorem 1 (Low-Rank Evolution of Gradient in Grad- NormLoRP during Training). Let r < m without loss of generality. The gradient update rules of GradNormLoRP:\n$Z_t = A-BI_t+C, I_t = I_{t-1}+\\gamma Z_{t-1} ; H_t = E-FJ_t+G, J_t = J_{t-1}+\\beta H_{t-1}$ (16)\nwith constant matrices (A and E), PSD matrices (B, C, F, and G), and randomly initialized Io and Jo leads to low-rank gradient with high probability:\n$\\text{stable-rank} (Z_t, H_t) \\leq 1+\\sum_{i=2}^k \\mathcal{O}\\left(\\left(1-\\gamma w_1 \\nu_1\\right)\\right)+\\sum_{j=2}^m \\mathcal{O}\\left(\\left(\\frac{1 - \\beta \\pi_1 \\mu_1}{1 - \\beta \\pi_1 \\mu_1}\\right)^{2t}\\right)$ (17)\n<Proof> By Lemma 1, V\u2081 = @min(C) denotes the smallest eigenvalue of C, with w\u2081 < < w representing the eigen- values of B. Additionally, if w2 > W\u2081 and v\u2081 > 0, then Zt converges exponentially to rank-1. Similarly, by Lemma 1, \u03bc\u2081 = min(G) signifies the smallest eigenvalue of G, while \u03c01 < ... < \u03c0m are the eigenvalues of F. Moreover, if \u03c02 > \u03c0\u2081 and \u03bc\u2081 > 0, Ht converges exponentially to rank-1. Con- sequently, the stable rank of stable-rank(Zt)stable-rank(Ht), denoted as stable-rank(Zt, Ht), converges exponentially to rank-1.\nProof of Lemma 1\nLemma 1 (Gradient Becoming Low-rank during Train- ing). Given Wt \u2208 Rk\u00d7m, where we assume k < m with- out loss of generality. Consider the gradient matrix D\u2081 =\nA - $BW^T+C$, where A denotes a constant matrix, B and C both are positive semidefinite (PSD) matrices, and Wo is ran- domly initialized. Then, the gradient in the update of weight matrix $W_t = W_{t-1} + aD_{t-1}$ results in low-rank gradient with high probability:\n$\\text{stable-rank}(D_t) \\leq 1 +\\sum_{i=2}^k \\mathcal{O}\\left(\\left(\\frac{1 - \\alpha \\lambda_i \\nu_1}{1 - \\alpha \\lambda_1 \\nu_1}\\right)^{2t}\\right),$ (18)\nwhere v\u2081 = min(C) is the smallest eigenvalue of C and 1 < ... < m are eigenvalues of B. Moreover, if C is pos- itive definite, i.e., v\u2081 > 0, and 12 > 1, then Dt converges exponentially to rank-1.\n<Proof> We have\n$D_t = A \u2212 BW_t+C = A \u2212 B(W_{t\u22121} + nD_{t-1})C = D_{t-1} - nBD_{t-1}C.$ (19)\nLet B = UDBUT and C = VDCVT be the eigen de- composition of B and C. DB = diag(x1,...,Am) and Dc = diag(v1,...,Vn) are their eigenvalues sorted in as- cending orders (i.e., \u03bb\u2081 < ... < Am and v\u2081 << Vn). Define X\u2081 := U\u00ae D\u2081V. It is clear that rank(X\u2081) = rank(Dt) and we have:\n$X_t := U^\\dagger D_tV = X_{t\u22121} \u2013 n\u010e_BX_{t-1}\u010e_c.$ (20)\nSuppose $X_{t,ij}$ is the ij component of X\u2081, then from the equa- tion above we have:\n$X_{t,ij} = X_{t\u22121,ij} - \\eta\\lambda_i\\nu_j x_{t\u22121,ij} = (1 - \\eta\\lambda_i\\nu_j) x_{t\u22121,ij} = (1 - \\eta\\lambda_i\\nu_j)^t x_{0, ij}$ (21)\nThen for the first few rows i and columns j that correspond to large eigenvalues, Xt,ij \u2192 0 quickly and rank(X\u2081) becomes small.\nTo make it more precise, consider the stable rank:\n$\\text{stable-rank}(D_t) = \\text{stable-rank}(X) = \\frac{\\|X_t^+\\|}{\\|X_t^+\\|_2}$ (22)\nBy the definition of the Frobenius norm, we then have:\n$\\left\\| X_{t}^{+}\\right\\|^{2} = \\sum_{i=1}^m \\sum_{j=1}^n (1 \u2013 \\eta \\lambda_{i} v_{j})^{2 t} x_{0,ij}^2$ (23)\nand\n$\\left\\| X_{t}^{+}\\right\\|_{*}^{2} \\geq \\sum_{j=1}^n x_{t,1 j}^{2} = \\sum_{j=1}^n (1 \u2013 \\eta \\lambda_{1} v_{j})^{2t} x_{0,1 j}^{2}.$ (24)\nWith high probability, $x_{0,1j}^2 \\geq c_0, since |x| < co is bounded, we have:\n$\\text{stable-rank}(D_t) \\leq 1 + \\frac{\\sum_{i=2}^m \\sum_{j=1}^n(1 \u2013 \\eta \\lambda_{i} v_{j})^{2 t}}{\\sum_{j=1}^n(1 \u2013 \\eta \\lambda_{1} v_{j})^{2 t}}.$ (25)\nUsing Mediant inequality, $\\frac{a}{b} \\leq \\frac{c}{d} \\leq \\frac{a+c}{b+d}$ for a, b, c, d > 0, therefore, we know that for i-th row (i > 2), since \u5165\u2081 \u2265 11:\n$\\frac{\\sum_{i=2}^m \\sum_{j=1}^n(1 \u2013 \\eta \\lambda_{i} v_{j})^{2 t}}{\\sum_{j=1}^n(1 \u2013 \\eta \\lambda_{1} v_{j})^{2 t}} < \\max_{j} (\\frac{1 - \\eta \\lambda_{i} v_{j}}{1 - \\eta \\lambda_{1} v_{j}})^2 = (\\frac{1 - \\eta \\lambda_{1} v_{1}}{1 - \\eta \\lambda_{1} v_{1}})^2 = \\frac{1 \u2013 \u03b7\u03bb\u03b9 \u03bdi}{1 \u2013 \u03b7\u03bb\u03b9 v1}$ (26)\nand the conclusion follows.\nNormalized Subspace\nA normalized subspace in linear algebra refers to a subspace of a vector space that has been scaled or normalized such that its vectors have a unit length. In other words, all vec- tors within the subspace have been adjusted such that their Euclidean norms are equal to 1. Mathematically, let V be a vector space and W be a subspace of V. W is considered a normalized subspace if for every vector v in W, the norm of v is 1, i.e., |||| = 1. Normalized subspaces are commonly encountered in various mathematical and computational con- texts, such as in optimization algorithms, signal processing, and machine learning, where ensuring consistency or stability in vector magnitudes is important (Shen et al. 2024; Zhang et al. 2024d,b,c,a; Huang 2024a,c; Huang et al. 2024c, 2025, 2024a; Huang 2024b; Zhu et al. 2024; Wang et al. 2024; Hu et al. 2019; Huang et al. 2024b, 2023b,c, 2022b, 2021a; Huang and Worring 2020; Di Sipio et al. 2022; Yang et al. 2018; Liu et al. 2019b; Huck Yang et al. 2019; Wu et al. 2023; Huang et al. 2021c,b; Huang, Wu, and Worring 2021; Huang et al. 2022a, 2021d, 2019a, 2023a; Huang 2017; Huang et al. 2018; Huang, Alfadly, and Ghanem 2017; Huang et al. 2019b; Guo et al. 2024; Aghapour et al. 2024; Xiao, Shen, and Pi- mentel 2022; Shen, Xiao, and Pimentel 2022; Niknam et al. 2023; Shen et al. 2023).\nAsymptotic Analysis for Existing PEFT Methods with Big-O Notation\nAsymptotic analysis is a mathematical method used to ana- lyze the behavior of functions as their input values approach"}]}