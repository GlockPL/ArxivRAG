{"title": "TaCIE: Enhancing Instruction Comprehension in Large Language Models through Task-Centred Instruction Evolution", "authors": ["Jiuding Yang", "Shengyao Lu", "Weidong Guo", "Xiangyang Li", "Kaitong Yang", "Yu Xu", "Di Niu"], "abstract": "Large Language Models (LLMs) require precise alignment with complex instructions to optimize their performance in real-world applications. As the demand for refined instruction tuning data increases, traditional methods that evolve simple seed instructions often struggle to effectively enhance complexity or manage difficulty scaling across various domains. Our innovative approach, Task-Centered Instruction Evolution (TaCIE), addresses these shortcomings by redefining instruction evolution from merely evolving seed instructions to a more dynamic and comprehensive combination of elements. TaCIE starts by deconstructing complex instructions into their fundamental components. It then generates and integrates new elements with the original ones, reassembling them into more sophisticated instructions that progressively increase in difficulty, diversity, and complexity. Applied across multiple domains, LLMs fine-tuned with these evolved instructions have substantially outperformed those tuned with conventional methods, marking a significant advancement in instruction-based model fine-tuning.", "sections": [{"title": "1 Introduction", "content": "The rapid development of Large Language Models (LLMs) and their expanding real-world applications necessitate closer alignment with complex human instructions to enhance performance across diverse tasks. This alignment demands high-quality instruction tuning data. However, manually crafting such instructions is impractical due to the time-intensive nature of the process and the tendency for these human-written instructions to remain simplistic (Xu et al., 2024), offering minimal benefits for tuning effectiveness(Kung et al., 2023).\nTo mitigate the high costs and challenges of manual instruction creation, researchers have developed automated synthesis methods using powerful LLMs to generate more sophisticated instructions from simpler ones. Notable among these are SELF-INSTRUCT by Wang et al. (2023a), which expands the range of instructions from a set of seed inputs, and EVOL-INSTRUCT by Xu et al. (2024), which refines instructions by enhancing either diversity or difficulty. Guo et al. (2024) also introduced Instruction Fusion, combining two distinct instructions to increase task complexity.\nDespite their success in enhancing instruction quality and LLM performance, existing methods like EVOL-INSTRUCT and Instruction Fusion exhibit significant limitations. Firstly, EVOL-INSTRUCT struggles with managing difficulty increments effectively; prompts such as \u201cadd one more constrain\u201d often lead to vague enhancements that do not genuinely increase the task's difficulty. For example, a depth evolving experiment with GPT-40* showed that only one out of three attempts successfully intensified the instruction's complexity. Other attempts either replicated existing requirements or merely substituted terms for more complex equivalents, illustrating the challenges of uncontrolled difficulty scaling. Furthermore, Luo et al. (2024)'s application in code generation tasks excessively escalated difficulty, adding seven constraints in just four rounds.\nSecondly, these methods fail to adequately address cross-domain tasks. Although EVOL-INSTRUCT was implemented in both math and code generation tasks, it largely focused on increasing difficulty within the specific domain of the initial instruction, leading to a lack of diversity in task complexity. To mitigate this, (Guo et al., 2024) developed Instruction Fusion to combine elements from two seed instructions. However, this approach, limited to a single round of fusion in code generation, falls short of fully exploiting the potential for enhanced complexity."}, {"title": "2 Approach", "content": "In this section, we propose TaCIE, a novel and efficient task-centred solution for instruction evolution to overcome the limitations of existing methods. Before introducing TaCIE, we first discuss the existing instruction evolution method."}, {"title": "2.1 Background", "content": "SELF-INSTRUCT enhanced LLMs by fine-tuning with diverse self-generated instructions, leading to the development of EVOL-INSTRUCT by Xu et al. (2024), which uses ChatGPT\u2020 to create more challenging and varied instructions from simple seed instructions. These seeds are evolved using five human-designed methods, significantly boosting LLM performance across various tasks. Recognizing limitations in task complexity, Guo et al. (2024) introduced Instruction Fusion, merging two seeds to enhance task complexity and performance, effectively complementing EVOL-INSTRUCT. Most recently, Zeng et al. (2024) advanced this by developing an active instruction evolution method that uses GPT to select the optimal evolution strategy for seed instructions, further enhancing the instruction evolution's impact on LLM performance.\nHowever, the methods discussed above share two significant limitations: inadequate management of difficulty increments and insufficient consideration of cross-domain tasks.\nDifficulty Increment Management is essential for effective instruction evolution. Current methods struggle with this aspect due to vague prompts provided to LLMs, which lack specific guidance for evolving instructions. This results in uncontrollable and unpredictable outcomes. For instance, Figure 1 demonstrates the evolution of a simple instruction using EVOL-INSTRUCT and GPT-40. Of three attempts, only the last one successfully added a new constraint to the seed instruction. The"}, {"title": "2.2 Instruction Decomposition", "content": "The most effective way to control incremental difficulty is to ensure that each new prompt introduces additional constraints or logical reasoning steps. To achieve this precise control over the evolution process, we draw inspiration from established decomposition methods in instructional design, as outlined in (Qin et al., 2024; Yang et al., 2024b). We employ GPT-40 to dissect seed instructions into three fundamental components: Background, Objectives, and Constraints. This allows for direct modifications in constraint and logic reasoning applied to these elements.\nFigure 1 illustrates an example of this decomposition approach. The Background component encapsulates all relevant information necessary for the instruction, such as facts, motivations, and provided texts for tasks like summarization. The Objectives segment outlines the primary tasks derived from the seed instruction, for instance, composing an SMS as depicted in the example. Lastly, the Constraints section details specific requirements and limitations related to the tasks, including word count and formatting requirements.\nLet C = {ci}1<i<N be the set of seed instructions, where ci is the i-th instruction and N is the number of seeds. We define:\nE = {ei}1<i<N = {Decompose(Ci)}0<i<N. (1)\nHere, ei represents the decomposed elements of seed ci, Decompose denotes the decomposition process, and E is the set of decomposed elements from the seed pool C. A detailed prompt template for decomposition is introduced in the Appendix."}, {"title": "2.3 Task-Centred Instruction Evolution", "content": "We break down the original seeds and then iteratively apply two types of evolution: depth evolution and task fusion, as illustrated in Figure 2.\nFor depth evolution, we aims to increase the difficult of newly generated instructions. To manage the increment in difficulty, we have developed a prompt template. This template guides the evolver to add precisely one additional constraint or one extra background setting to the elements of the seed instruction, thereby enhancing either the difficulty or the logical reasoning required. For example, as shown in Figure 2, the depth evolution successfully added an extra constraint to the original instruction by requiring the mention of the expected return time in an SMS requesting leave.\nFor task fusion, our objective is to enhance the complexity of tasks in fused instructions, making them more informative. We instruct the evolver to merge all elements from each pair of seed instructions, as illustrated in Figure 2. The different colors in the orange box represent elements from the two seed instructions.\nDespite the benefits these evolutionary methods offer to LLMs, selecting the right candidate instruction is crucial. A well-chosen seed can lead to evolved instructions of higher quality. To maximize the effectiveness of the proposed evolution methods, we have also developed a new candidate sampling technique. The entire process is segmented into the following stages:\nSeed Collection. We commence by collating seed instructions from a diverse assortment of open-source datasets tailored to various specializations: Alpaca (Taori et al., 2023) and ShareGPT (Chiang et al., 2023) for general instruction comprehension and execution; GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) for mathematical problem-solving; and CodeAlpaca (Luo et al., 2024) for coding tasks. Initially, we employ Sentence Transformers to compute embeddings for all instructions within these datasets. Using the Elbow method, we identify the optimal clustering configuration for each source. After clustering, we randomly select one seed instruction from each cluster."}, {"title": "Sample Scoring.", "content": "The effectiveness of instruction tuning for LLMs relies on the quality of instruction-response pairs, rather than their quantity. As Zhou et al. (2024) have noted, instruction sets of higher quality confer more significant benefits to LLMs than those merely larger in volume. To curate such beneficial instructions, following Kung et al. (2023), we employ uncertainty to filter out less beneficial instructions.\nAccording to Kung et al. (2023), it has been established that LLMs benefit more from fine-tuning with informative and novel instructions. Such instructions exhibit a significant alteration in the probability of their original responses when subjected to minor perturbations, like the random omission of a certain percentage of words. This alteration is considerably less pronounced in instructions that are less informative (low uncertainty), which are either \"easy\" (high response probability) or \u201cdifficult\" (low response probability) to the LLMs.\nIn the TaCIE framework, candidate sampling for the evolution process is guided by calculating uncertainty scores, which help identify instructions with potential for greater informativeness. The depth evolution strategy primarily targets seeds with high uncertainty, aiming to evolve these into more difficult instructions by incorporating additional logical reasoning steps or constraints. In contrast, task fusion merges information from two less informative seeds (low uncertainty) to create a single, more comprehensive instruction. This approach ensures that newly generated instructions are enriched with useful content without becoming overly complex, thereby enhancing their practical applicability in training LLMs. Further details on this process are elaborated in the Appendix.\nSpecifically, we define uncertainty ui as the score of the i-th instruction and use the following Score function to calculate it:\nui = Score(ci, ri) = \\frac{1}{N_u} \\sum_{j=1}^{N_u} (q_i - q_{i,j})), (2)\nwhere Nu is the number of perturbation for each instruction, and\nqi,j = P(r_i | c_i, W). (3)\nHere, ri represents the response to the instruction ci, and qi,j is the probability of response ri given instruction ci and the model weights W. The index j denotes the j-th perturbation of ci."}, {"title": "Candidate Sampling.", "content": "As mentioned by Guo et al. (2024), difficulty gradient is also an important key for better fine-tuning performance. To balance informative, \"easy\", and \"difficult\u201d instructions, we designed a sampler which samples the candidates for the depth evolution and task fusion according to different weighting approach. Denote the target evolution amount to be Me and Mt, for depth evolution, we directly use the uncertainty to weight each seed, and defined the sample probability of each instruction as:\np^{depth}_i = \\frac{u_i}{\\sum_{k=1}^{N} u_k}, (4)\nand we sample Me instructions:\nC^{depth} \\sim Multinomial (M_e, \\{p^{depth}_i\\}_1^N), (5)\nFor task fusion, we use the following weighting function:\np^{fuse}_i = \\frac{s_i}{\\sum_{k=1}^{N} s_k}, (6)"}, {"title": "where", "content": "where\ns_i = \\frac{1}{(n_{c_i} + 1) \\times N_{obj_i} \\times N_{root_i} \\times u_i}. (7)\nHere si is the punished uncertainty of the seed instruction ci. For each instruction, we punish the uncertainty with three factors. ne represent the frequency of instruction ci being used as the seed for task fusion; nobj; is the number of objectives of the instruction, which will increase if it is fused instruction last round; nroot; is the frequency of the root domain that instruction lies in, such as coding, math, etc. The sampling process is shown in the Algorithm 1."}, {"title": "2.4 Data Statistic", "content": "Figure 3 presents detailed statistics from our evolved instruction pool. We sampled a total of 12,000 seeds from multiple sources: 3,000 each from ShareGPT, Alpaca, and Code Alpaca, along with 1,500 from both the MATH and GSM8K training sets. Using these seeds, we prompted GPT-40 to generate an additional 36,000 variants, aiming to diversify the seed pool with varied objectives. Over six evolutionary rounds-excluding samples that failed in evolution or were unrecognized by our scripts-we applied two distinct evolutionary methods, ultimately producing 143,917 viable samples out of 144,000 attempts. This yields a success rate of over 99.94%, significantly surpassing the performance of EVOL-INSTRUCT. During the process, we utilized Llama-3-8B-Instructs to evaluate the uncertainty of each instruction.\nAdditionally, the figure illustrates frequency statistics across different domains. The scoring language model identified mathematics and coding problems as particularly \"informative\"\u2014a finding consistent with expectations, given that these categories often require robust logical reasoning and exhibit considerable variability with minor instructional modifications. This shift in domain distribution has effectively improved the balance of our data mix, enhancing instruction tuning. Subsequent experiments confirm that this rebalancing has notably improved performance across all evaluated metrics.\nFigure 4 depicts the distribution of rounds for each evolutionary method. From this visualization, it is clear that TaCIE, in contrast to full-size evolution-which processes all candidates in each round and reaches 144,000 instructions in just three rounds-selects more informative seeds for evolution. This approach enables the generation of informative instructions over a higher number of evolutionary rounds within the same total number of generated instructions."}, {"title": "3 Experiments", "content": "To effectively showcase the capabilities of TaCIE, we selected a diverse range of baselines, encompassing both general-purpose chatting LLMs and domain-specific LLMs. Our primary comparisons are with EVOL-INSTRUCT, Auto Evol-Instruct, and"}, {"title": "3.2 Result Analysis", "content": "Table 1 shows TaCIE's impact on LLaMA-3 across four benchmarks. We compared the performance of LLMs fine-tuned on 48,000 instructions from both evolved and seed datasets. The results demonstrate substantial improvements in instruction following, math, and coding tasks with the evolved instructions, while maintaining competitive performance on MT-bench. Using LLaMA-3-Instruct as the scorer, TaCIE effectively identifies and samples informative candidates, significantly enhancing task performance in these domains. Despite only a minor performance increment on MT-bench due to its multi-round chatting requirements\u2014a feature not covered in our datasets\u2014the overall gains from the evolved instructions outweigh this limitation. The domain shift focuses on complex tasks over multi-round chatting (Section 2.4), leading to less performance boost but considerable benefits across other domains, validating the evolution approach.\nTo justify the transferability of the evolved instructions (candidates sampled with the LLaMA-3-based scorer can also benefits other base LLMs), we use them to further fine-tune Mistral-7B-v0.1 and Qwen2-7B, which also show significant improvements all most domains. Here the beneftis for Qwen2 model is less than that on the other two base LLMs, because Qwen2-7B have better performance than LLaMA-3-8B, so what LLaMA-3-8B find informative may not be true for Qwen2-7B. However, it still outperforms the baseline fine-tuned with seeds.\nBesides the general purpose LLMs, we also conduct experiment fine-tuning base models using domain-specific instructions. We mainly compare our performance with AEI proposed by Zeng et al. (2024). For fair comparison, we use the same base LLMs and sample the same amount of evolved instructions that only contains single-domain information among all of its evolution history. For ShareGPT, we mix 7,000 evolved instructions with 3,000 original samples to cover multi-round requirements of MT-Bench.\nAs shown in the domain-specific part in Table 1, we outperform AEI on all three domains, especially on math and coding, which requires better logic for answering. For Instruction Fusion, we sampled 20,000 samples from their 110,000 evolved coding instructions for baseline fine-tuning (IF-20k). Although they has evolved more complex instructions with higher diversity due to the large amount of instructions, we still achieved 67.1% on HumanEval, which is comparable to their 67.7%.\nIn conclusion, TaCIE's evolved instructions significantly boost base LLM performance across tasks, particularly in complex areas like instruction comprehension, math, and coding. These enhancements persist even when applied to LLMs with varying architectures, such as Mistral-7B-v0.1 and Qwen2-7B, demonstrating their broad applicability and transferability. These findings confirm the effectiveness of the TaCIE evolution approach in enhancing instruction sets for diverse LLM applications."}, {"title": "3.3 Ablation Study", "content": "To further demonstrate task fusion's effectiveness, we fine-tuned LLaMA-3-8B with 10,000 single-domain and 10,000 cross-domain-only evolved instructions (excluding seeds). The results, shown as TaCIE-LLaMA-3-id and TaCIE-LLaMA-3-cd in Table 1, reveal that models fine-tuned with cross-domain instructions outperform those fine-tuned with in-domain instructions. This supports the notion that fusing objectives from different domains introduces greater complexity and information, enhancing LLM learning.\nAdditionally, to evaluate the scalability of TaCIE,"}, {"title": "4 Related Work", "content": ""}, {"title": "4.1 Instruction Tuning", "content": "Instruction Tuning (Wei et al., 2022) is a pivotal method for aligning Large Language Models (LLMs) with human instructions, enhancing their applicability to real-world scenarios (Zhang et al., 2023). This approach aims to improve the zero-shot capabilities of well-trained LLMs, enabling them to perform tasks guided solely by natural human instructions. It allows LLMs to cater to a diverse array of general requests (Wang et al., 2023b). However, the effectiveness of Instruction Tuning heavily depends on the availability of high-quality data to optimize both performance and interaction quality (Zhou et al., 2024; Zhao et al., 2024).\nFor enhancing and proofing high-quality instructions, Zhou et al. (2024) introduced LIMA, an LLM trained on just 1,000 high-quality instances. This model demonstrated that smaller datasets of superior quality can be more beneficial than larger, less curated ones. Additionally, Zhao et al. (2024) found that detailed, lengthy instructional responses also enhance LLM performance, underscoring the significance of data quality.\nConversely, Kung et al. (2023) explored the concept of task uncertainty by examining how minor perturbations in instructions affect response probabilities. They proposed that instructions whose perturbations can cause significant shifts in response probabilities are particularly informative and novel for LLMs, thus aiding in better alignment. This approach has inspired our methodology for the design of seed sampling.\nThese studies collectively highlight the critical role of high-quality instructions and responses in the efficacy of LLMs. Nonetheless, the reliance on manually crafted instructions or templates constrains the diversity, quantity, and creativity of the data available."}, {"title": "4.2 Instruction Evolution", "content": "In response to the growing demand for high-quality data, Wang et al. (2023a) developed SELF-INSTRUCT, a strategy that utilizes LLMs for both generating data and tuning instructions. This method produces enhanced synthetic instructions and responses. Building on this concept, Xu et al. (2024) introduced EVOL-INSTRUCT, which evolves initial simple instructions into more challenging or varied forms by leveraging sophisticated LLMs (e.g., ChatGPT) and a designed evolution methodology. Further expanding on this, Zeng et al. (2024) proposed an advanced framework that allows LLMs to autonomously determine the most effective evolution strategy for a given set of seed instructions.\nGuo et al. (2024) focused on task complexity by developing Instruction Fusion, which integrates two simple tasks into a single, more complex challenge, thereby enhancing performance. This method has inspired further exploration into cross-domain task fusion within our proposed approach."}, {"title": "5 Conclusion", "content": "In this paper, we introduce TaCIE, a novel method for evolving instruction that enhances difficulty management and promotes cross-domain complexity. TaCIE employs a decomposition approach to break down seed instructions into three fundamental elements. This transformation shifts the evolution process to the elemental level, allowing for targeted modifications that culminate in the regeneration of advanced instructions. Experimental results underscore TaCIE's efficacy, demonstrating significant performance improvements across a variety of base LLMs compared to previous methods."}, {"title": "Limitations", "content": "Our work focus on task-centered evolution, making them more difficult and complex in controllable manner, thus the multi-turn chatting is not considered in our evolving procedure. Future work could easily built evolution method for multi-turn conversation based on our proposed method.\nThe total cost of our experimental process was approximately 2,000 USD, largely driven by our reliance on GPT-40 for augmentation and evaluation\u2014a common challenge in LLM-related research. However, the expenses associated with using such APIs are decreasing due to rapid advancements in LLM technologies, making these tools more affordable and accessible."}, {"title": "Ethics Statement", "content": "Our data collection relies on publicly released datasets. The augmented data were generated using GPT-40, whose outputs are already carefully monitored, ensuring that no privacy-sensitive or confidential information was included."}, {"title": "A Data Analysis", "content": ""}, {"title": "A.1 Diversification", "content": "As detailed in Section 2, we utilize the Elbow method to optimally cluster each data source and randomly select one instruction from these clusters as the initial seeds. These seeds are then used to prompt GPT-4o to generate a diverse set of 48,000 instructions, forming our initial instruction pool (Round 0). Figure 6 shows a 2D projection of both the randomly sampled instructions from the original data sources and our diversified seed pool. It is evident that the diversified seeds cover more of the previously blank regions compared to the original data.\nTo quantify this increase in diversity, we employ the variance calculation method used in Instruction Fusion by Guo et al. (2024):\nU = \\frac{1}{N} \\sum_{i=1}^{N} (d_i - \\mu)^2, (8)\nwhere\ndi = ||(e_i, e_{NN})|| (9)\nand\n\\mu = \\frac{1}{N} \\sum_{i=1}^{N} d_i. (10)\nIn this formula, U represents the uniformity of the distribution, di denotes the Euclidean distance between the semantic embedding ei of a seed and the embedding eNN of its nearest neighbor, and \u03bc is the average Euclidean distance across all seeds. This calculation of variance in nearest neighbor distances provides a measure of the instruction pool's diversity. A lower variance signifies a more uniform distribution of data points, indicative of greater diversity. As depicted in the accompanying figure, the variance for the diversified seed pool stands at 0.0431, which is approximately 12% lower than that of the original data source, confirming an enhancement in diversity."}, {"title": "A.2 Uncertainty Shift", "content": "To examine the impact of depth evolution and task fusion on instruction uncertainty, we generated 24,000 depth-evolved instructions and 12,000 task-fused instructions, both in-domain and cross-domain. Figures 7-9 show 2D projections of the original and evolved instructions. From these figures, it is apparent that depth evolution does not significantly alter the uncertainty of instructions. In contrast, both in-domain and cross-domain task fusion lead to an increase in uncertainty, from 0.07 to 0.09. This indicates that task fusion, through the merging of two seed instructions, produces more informative content, thereby enhancing the overall uncertainty within the dataset. However, merging highly informative instructions could potentially overload the learning process of LLMs due to increased complexity. To mitigate this, during the sampling process, we prioritize less informative seeds, balancing the generation of enriched instructions while ensuring they remain tractable for learning enhancements."}, {"title": "B Prompt Templates", "content": "In this section, we introduce all prompt templates we designed and use in TaCIE."}, {"title": "B.1 Instruction Decomposition", "content": "To better decompose each instruction, we provided GPT-40 with two examples during the decomposition process.\nGiven a prompt, your task is to:\n1.**Extract Backgrounds Settings:** Identify and list the backgrounds of the prompt, such as facts, and motivations. If the prompt provides extra information such as code to debug, passage to polish or summarize, directly include them in this section, do not summarize them. Do not include Objectives or Requirements. If no Backgrounds Settings is given in the prompt, output 'N/A' in the **Extract Background Settings:** section.\n2.**Extract Objective:** List the core task of the prompt.\n3.**Extract Constraints:** List all specific requirements or constraints for the objectives. If no Constraints is given in the prompt, output 'N/A' in the **Extract Constraints: ** section.\n**Given Prompt:**\nI have to pick up my son. Write a short SMS to my supervisor asking for leaving. In 20 words. Be polite."}, {"title": "B.2 Diversification", "content": "During the diversification of the original seed, we use the following prompt to let GPT-40 generate another three different instructions for a given seed. **For the provided prompt, we detail the following elements:**\n1.**Background Settings:** This section presents the background information relevant to the prompt, including pertinent facts and motivations. If the prompt lacks background settings, this section will be labeled as 'N/A'.\n2.**Objectives:** This section outlines the main tasks associated with the prompt.\n3.**Constraints:** This lists any specific requirements or limitations tied to the objectives. If there are no constraints, this section will be labeled as 'N/A'.\n**Given Prompt:**\n{{ prompt }}\n{{ extracted }}\nBased on the information provided, your task is to:\n1.Develop ten new objectives to replace the original **Objectives** of the prompt. Each new objective should be diverse and maintain the same level of difficulty as the original.\n2.For each new objective, craft a corresponding prompt that mimics the tone and style of the original prompt. Ensure to vary the **Background Settings** and **Constraints** while making sure each prompt is reasonable and answerable.\nFormat each new objective and prompt as follows,do no provide corresponding background, objectives, and constraints:\n**New Objective 1:**\n[Describe the new diverse objective.]"}, {"title": "B.3 Depth Evolution", "content": "Based on a prompt's existing background, objectives, and constraints, increase its difficulty using ONLY one of the following methods: 1.If the prompt primarily involves reasoning, such as solving a mathematical problem, enhance its complexity by introducing an additional background element. Also, modify the existing background elements to ensure the task remains logical and solvable.\n2.Otherwise, introduce one additional reasonable constraint to ONLY one of the objectives of the given prompt to increase its difficulty.\nPlease respond using the format provided in the examples below. You can only change either the **Background Settings:** or the **Constraints:**. Do not change both.\nEnsure your response contains the following four sections even if they are empty: **Prompt:**, **Background Settings:**, **Objective:** and **Constraints:**"}, {"title": "B.4 Task Fusion", "content": "Based on the prompt's existing background, objectives, and constraints, your task is to act as a Prompt Fusion Specialist. Your target is to fuse **Given Prompt A** and **Given Prompt B** into a single, cohesive **Fused Prompt**, following the two steps below:\n1.Merge the elements in the background, objectives, and constraints of both **Given_Prompt_A** and **Given Prompt B** respectively, make sure the objectives are dependent to each other and solvable. Do not compress multiple elements into a single one.\n2.Based on the integrated background, objectives, and constraints, fuse the **Given Prompt A** and **Given Prompt B** into a new prompt. Mimic the tone and style of the original prompts. Make sure the new prompt is coherent and solvable"}]}