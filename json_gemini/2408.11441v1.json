{"title": "Epistemic Injustice in Generative AI", "authors": ["Jackie Kay", "Atoosa Kasirzadeh", "Shakir Mohamed"], "abstract": "This paper investigates how generative AI can potentially un-dermine the integrity of collective knowledge and the pro-cesses we rely on to acquire, assess, and trust information, posing a significant threat to our knowledge ecosystem and democratic discourse. Grounded in social and political philosophy, we introduce the concept of generative algorithmic epistemic injustice. We identify four key dimensions of this phenomenon: amplified and manipulative testimonial injus-tice, along with hermeneutical ignorance and access injustice. We illustrate each dimension with real-world examples that reveal how generative AI can produce or amplify misinforma-tion, perpetuate representational harm, and create epistemic inequities, particularly in multilingual contexts. By highlight-ing these injustices, we aim to inform the development of epistemically just generative AI systems, proposing strategies for resistance, system design principles, and two approaches that leverage generative AI to foster a more equitable infor-mation ecosystem, thereby safeguarding democratic values and the integrity of knowledge production.", "sections": [{"title": "Introduction", "content": "While algorithms have traditionally been leveraged to present and organize human-generated content, the advent of generative AI has started to fundamentally shift this paradigm. Generative AI models can now create content \u2013 spanning text, imagery, and beyond \u2013 that resembles that of authors, journalists, painters, or photographers. In this paper, we take generative AI to be the class of machine learning models trained on massive amounts of data, typically me-dia such as text, images, audio or video, in order to produce representative instances of such media (Garc\u00eda-Pe\u00f1alvo and V\u00e1zquez-Ingelmo 2023).\nThe rapid advancement of generative AI, marked by ac-celerated software and hardware innovation and a prolif-eration of novel applications, has been accompanied by growing societal concerns and numerous instances of mis-use (Bender et al. 2021; Weidinger et al. 2022; Bird, Un-gless, and Kasirzadeh 2023). These range from parroting harmful stereotypes and misconceptions about certain so-cial groups (Bianchi et al. 2023; Ferrara 2023), confabulat-ing facts and distorting truth (Ji et al. 2023), and spread-ing misinformation and deepfakes (Vaccari and Chadwick 2020; Monteith et al. 2024).While a rigorous philosophical treatment of \"truth\" in the generative AI context is complex (Kasirzadeh and Gabriel 2023, p.9) and out of our scope, our modest working definition is: statements that can be verified through adequate evidence, or a robust consensus between relevant social groups. Despite these escalating societal con-cerns and numerous instances of misuse, the discourse sur-rounding generative AI's rapid advancement lacks a philo-sophical account that coherently relates these epistemic con-cerns and explains how they constitute moral violations of a unifying principle.\nTo address this gap, we develop an account of generative algorithmic epistemic injustice by building upon a conven-tional philosophical understanding of epistemic injustice. Epistemic injustice emphasizes how identity-based preju-dice within an information ecosystem not only unjustly hin-ders the expression of marginalized groups, but also signif-icantly impairs the knowledge formation capabilities of all individuals. It does this by describing the harmful effects and ethical shortcomings inherent in knowledge production systems marked by hierarchical power imbalances rooted in identity.\nWhile traditional discussions of epistemic injustice have primarily centered on interpersonal human interactions (McKinnon 2017; Tsosie 2012), existing research on algo-rithmic epistemic injustice has largely been limited to epis-temic injustices produced by decision-making and classifi-cation algorithms. However, we argue that the distinctive characteristics of generative AI give rise to novel forms of epistemic injustice that necessitate a dedicated analytical framework. To address this, we expand upon the established philosophical discourse on epistemic injustice and introduce an account of \u201cgenerative algorithmic epistemic injustice,\u201d or simply \"generative epistemic injustice,\" to characterize the variety of epistemic harms arising from generative AI systems from a philosophical standpoint.\nIn Section 2, we describe epistemic injustice as a social theory and argue for its ethical importance. Section 3 sit-uates our paper within the context of prior research on al-gorithmic epistemic injustice. Section 4 builds on the exist-ing research on algorithmic epistemic injustice and identifies four distinct configurations of generative epistemic injustice: amplified and manipulative testimonial injustice, along with hermeneutical ignorance and access injustice. We illustrate these configurations through real-world exemplars of gen-erative AI deployments. While the evidence of injustice in"}, {"title": "Epistemic Injustice", "content": "In 2013, the city council of Flint, Michigan decided to switch its water supply to the Flint River, notorious for its pollution from automotive manufacturing. This move swiftly provoked public outrage as residents reported tap water turning discolored and emitting a foul, sewage-like odor. Despite these immediate alarms, authorities repeat-edly dismissed these complaints, perpetuating a longstand-ing pattern of environmental gaslighting. The situation es-calated when an outbreak of Legionnaires' disease was re-vealed, previously concealed by political maneuvers to safe-guard reputational interests. It was only after academic in-vestigations uncovered alarmingly high levels of lead con-tamination that the gravity of Flint's water crisis gained na-tional attention, unmasking a profound public health catas-trophe. It is obvious in hindsight that the citizens of Flint were correct and the politicians and others in power commit-ted egregious harm by not believing the residents' testimony (Davis 2021).\nMany have studied the tendency for those with more priv-ilege to ignore and even oppress ordinary citizens with less privilege (socioeconomic, racial, gender or otherwise). The decolonial scholar Gayatri Spivak put forward the position that the elite's construction of an underclass, and their ten-dency to presume to speak on behalf of those they disenfran-chise, means that the means for resistance to oppression are mediated through the oppressor. Thus according to Spivak, the \"subaltern\"-a group which includes women, the working class, the lower castes, citizens of \"third world\" countries, the colonized, and especially intersections therein-cannot speak (Spivak 1988).\nBut Spivak's provocation begs the question: can the sub-altern speak outside of the structures of the elite? Seeking alternative means of empowerment, Black feminist schol-ars devised systems of knowledge outside of white patriar-chal domination. Patricia Hill Collins introduced a feminist epistemology that emphasizes the intellectual importance of wisdom gained through Black women's lived experiences, and the transmission of this wisdom through relationships, community, and solidarity (Collins 2000). For Collins, crit-ical dialogue and resistance to threats of epistemic violence is necessary for assessing the claims of the powerful.\nLater, Miranda Fricker brought these notions of oppres-sion and silencing to the forefront of mainstream analytic philosophy by coining the term \u201cepistemic injustice\", re-ferring to injustices related to knowledge and understand-ing (Fricker 2007). However, Fricker's work has been crit-icized for not fully acknowledging the contributions of Black feminist scholars who had previously explored similar ideas (Berenstain 2020). These critiques highlight Fricker's oversight of intersectional aspects of oppression (Crenshaw 2013) and her failure to recognize women of color as agen-tial knowledge creators. While acknowledging these limita-tions in Fricker's original account, we find her philosoph-ical framework of epistemic injustice theoretically expres-sive and influential in subsequent discussions of algorithmic oppression. Therefore, we use Fricker's account as the foun-dation for our philosophical examination of generative epis-temic injustice.\n\"Epistemic injustices\u201d includes scenarios where individu-als or communities experience unjust discreditation of their knowledge and experiences due to underlying prejudices against their identity. Fricker emphasizes that these biases are directed towards groups with less social power, defined as the capacity to influence others' actions within social in-teractions and environments. This power can be either agen-tial, exercised by individuals, or structural, embedded in cul-tural norms and material inequalities.\nFricker argues that epistemic injustices harm not only the oppressed but everyone. Sharing knowledge and experiences is a fundamental human right, essential for self-expression, establishing connections, and asserting needs. To deny this right based on identity is a discriminatory act. Knowledge acquisition is integral to human existence, shaping our un-derstanding of the world, informing our interactions, and fostering a sense of purpose. Disregarding someone's tes-timony unjustly not only harms the individual but also de-prives society of valuable insights, obstructing collective knowledge growth. Those who unfairly disbelieve someone also do a disservice to others, blocking access to potentially valuable information. Moreover, recognizing and crediting someone's testimony is a valuable heuristic for anticipating and preventing harm.\nFricker distinguishes between two types of epistemic injustice: testimonial and hermeneutical. Testimonial in-justice involves the unfair discrediting of someone's ac-count due to prejudice against their identity, a recurring in-justice experienced by the Flint residents in our opening example. Hermeneutical injustice, the second type, stems from a disconnect between personal experiences and soci-etal understanding. \u201cHermeneutics\" means the interpreta-tion of knowledge. We will sometimes refer to our \"shared hermeneutics\" or \"hermeneutical resources\" as our shared cultural concepts for interpreting each other's experiences and for sharing knowledge.\nHermeneutical injustice occurs when a person's experi-ences are misunderstood or not recognized at all, due to the absence of appropriate concepts within our collective cultural knowledge. Shared knowledge is crucial for inter-preting and relating to the experiences of others. However, the repository of collective understanding bears the imprints of dominant groups, leaving the experiences of marginal-ized communities underrepresented or distorted. This re-sults in gaps and misinterpretations within our interpre-tive resources, leading to hermeneutical injustice. The trans-gender community, for instance, has frequently faced this form of injustice. In societies lacking widespread under-standing of gender variance, fluidity, non-conformity, and\""}, {"title": "Related Work on Algorithmic Epistemic Injustice", "content": "The conventional account of epistemic injustice described in Section 2 only involves human actors. However, AI algo-rithms can also contribute to these injustices because they are epistemic technologies (Alvarado 2023): they consume, curate, and produce information, which is a precursor to knowledge. When algorithms make decisions, particularly in our bureaucratic systems such as governments, businesses, or healthcare providers, they exert power via their contribu-tions to knowledge.\nThe emerging field focused on the intersection of epis-temic injustice and AI algorithms has been named \u201calgorith-mic epistemic injustice,\" a term coined by Byrnes and Spear (2023). To situate our paper within the broader landscape of algorithmic epistemic injustice, we begin by reviewing the existing literature on this topic. Several key themes emerge from our survey. Testimonial injustice can arise when algo-rithms are prioritized over human credibility, potentially am-plifying existing societal biases. Additionally, hermeneuti-cal injustices can occur when algorithms independently con-struct meanings and interpretive frameworks, often in auto-mated setups without direct human oversight.\nThis body of research has a notable emphasis on clas-sification and decision-making algorithms. Several studies exemplify this in different contexts. In child welfare sys-tems, Glaberson (2022) identify epistemic injustice through algorithms that disproportionately target Black communities and poor single mothers. These algorithmic testimonial in-justices lead to wrongful mistrust, surveillance and over-policing committed by humans. The healthcare sector, as Pozzi (2023) note, witnesses \u201cautomated hermeneutical ap-propriation\" in opioid risk score predictions. Pozzi claims that the algorithm establishes meanings for concepts that contribute to a patient's diagnosis, such as \u201caddiction\" or the experience of pain, without human intervention. The opacity of data science systems is highlighted by Symons and Al-varado (2022), who examine the real case of a prisoner who was wrongfully denied parole by the COMPAS recidivism algorithm and continued to be detained, even after providing evidence for his case to human supervisors (Wexler 2018). The authors argue that this lack of transparency facilitates epistemic injustice: the ignorance about a technology's in-ner workings complicated the possibility for contesting its unjust decisions. Hull (2023) discusses how COMPAS and similar systems commit hermeneutical injustice through bi-ased and stereotype-based classifications. Hull also points out the testimonial injustices inherent in physiognomic sys-tems, which wrongly infer personal characteristics based on visual appearance, often linked to race, ethnicity, and gender. Building on these individual-focused analyses, Milano and Prunkl (2024) emphasize the importance of our relationships to others for transmitting collective knowledge, which they call our shared epistemic infrastructures, and use these con-cepts to identify how algorithmic profiling can harm this in-frastructure. This approach informs our subsequent discus-sion on access injustice in Section 4.\nThe domain of AI fairness research has started to intersect with broader concerns of social injustice (Hoffmann 2019; Birhane 2021; Kasirzadeh 2022). There has also been a growing recognition of epistemic injustice concepts in rela-tion to AI fairness. For instance, Edenberg and Wood (2023) suggest that an epistemic lens offers a theoretical foundation for understanding the harms of algorithmic bias, which other prevalent frameworks might not adequately capture.\nA notable difference between this cluster of prior work and ours is its primary focus on classification or decision-making systems. We build on this cluster to develop an ac-count of epistemic injustice in relation to generative AI, which occupies a growing unique position in the landscape of epistemic injustice due to its capacity to produce convinc-ing and seemingly authentic output. While classification AI is set to delineate true categories and false ones, generative AI offers statements that play the role of testimonies, expla-nations, and interpretations. However, the quality and verac-ity of these outputs varies, posing risks of epistemic contam-ination.\nTo the best of our knowledge, the only work with a focus on the epistemic injustice of generative AI is by De Proost and Pozzi (2023), which looks at the potential of conversa-tional AI for hermeneutical ignorance. The authors review existing literature on how such AI might dominate epistemi-cally within dialogues. However, this study presents two sig-nificant limitations. First, its analysis is confined to textual dialogue interactions. In contrast, our theoretical account is designed to be sufficiently broad, extending to include mul-timodal systems, such as those involved in image generation. This broader scope allows for a more comprehensive under-standing of generative AI's epistemic impact across various mediums. Second, De Proost and Pozzi (2023) primarily ex-amine the immediate effects of AI-generated conversation on individual human interlocutors. Our account, on the other"}, {"title": "Generative Epistemic Injustice", "content": "We now introduce our account of generative epistemic in-justice in which generative AI harms the human capacity for understanding and trusting marginalized groups. Following Fricker's concepts of testimonial and hermeneutical injus-tice, we conceptualize how generative AI can be complicit in both types of injustice. We then distinguish further con-figurations based on how humans shape the model's behav-ior at various stages of interaction. Generative AI can either amplify testimonial injustices due to biases acquired in the pretraining and finetuning processes, or it can be manipu-lated by human users to create harmful content. Hermeneu-tical injustices can arise when generative Al's interaction with our shared knowledge leads to the erasure or distortion of marginalized experiences. This may occur when the sys-tem lacks sufficient sociocultural understanding of humans, or when the system obstructing the access to knowledge it-self. These phenomena constitutes hermeneutical injustice in Fricker's sense because it perpetuates a gap in collec-tive interpretive resources, obstructing the understanding of these marginalized experiences.\nTherefore our four configurations of generative epistemic injustice are:\nGenerative amplified testimonial injustice: when gener-ative AI magnifies and produces socially biased view-points from its training data.\nGenerative manipulative testimonial injustice: when hu-mans fabricate testimonial injustices with generative AI.\nGenerative hermeneutical ignorance: when generative AI lacks the interpretive frameworks to understand human experiences.\nGenerative hermeneutical access injustice: when unequal access to information and knowledge is facilitated by generative AI.\nTable 1 summarizes these concepts. We will sometimes drop \"generative\u201d when referring to these concepts due to the scope of the paper; however, we note that each configu-ration has an equivalent counterpart outside of the realm of generative AI.\nFor each configuration we will describe its contribut-ing sociocultural factors and potential second-order effects. Note that generative algorithmic epistemic injustice is not a speculative theory, but a real danger that has exploded with recent advances and investment in the field. Thus each the-oretical concept is illustrated by an exemplar sourced from real world research or investigative journalism on generative models."}, {"title": "Amplified Testimonial Injustice", "content": "Generative Al systems have a unique capacity to perpetu-ate and amplify existing testimonial injustices. Trained on vast datasets often scraped from the web, these models in-herit and reproduce the prejudices and biases embedded within those sources. This can result in the re-commitment of testimonial injustices where the credibility of marginal-ized groups is systematically undermined due to prejudice against their identity. The generative AI becomes an unwit-ting perpetrator of social biases, unfairly discrediting the knowledge and experiences of certain groups. Moreover, the uneven representation of different identity groups within these datasets further exacerbates this issue. Generative AI models are more likely to reproduce the voices of those fre-quently represented and culturally dominant online, while erasing the voices of the socially marginalized. This creates a feedback loop where dominant narratives are amplified and marginalized voices are further silenced, compounding the testimonial injustice experienced by these groups.\nSeveral factors contribute to this amplification, making it a distinct form of testimonial injustice. The deployment scale of generative AI naturally allows biased narratives to reach a massive audience, while the perceived objectivity and authority of AI-generated content can lend credence to these narratives, even when they reflect societal biases. This can lead to a situation where the generative AI's out-put is trusted over the lived experiences and knowledge of marginalized individuals, thus reinforcing existing power imbalances and perpetuating testimonial injustice. Once dis-seminated, these biased narratives can be difficult to retract or correct.\nIn the conventional human-only environment, testimonial injustices involve a credibility deficit assigned to someone's account of truth based on the prejudices of the listener. In the algorithmic setting, the injustice requires a credibility excess assigned to the algorithm; that is, humans believe the ac-count amplified through the technology over the individual or group who is discredited.\nThe systemic consequence of this arrangement leads to the degradation or wrongful attribution of trust. Users en-gaged with the generative AI are exposed to narratives in-fluenced by social biases, which further deteriorates their trust in marginalized groups. Concurrently, these marginal-"}, {"title": "ChatGPT and Misinformation Fingerprints", "content": "Testimo-nial injustices can be memorized by large language models and amplified in their outputs, as shown by the January 2023 study of GPT-3.5's responses to requests for false infor-mation from NewsGuard's \u201cMisinformation Fingerprints\" database (Brewster, Arvanitis, and Sadeghi 2023). While the system rejected the more infamous conspiracies, such as the \"birther\" conspiracy that Barack Obama was born in Kenya and thus ineligible to be President of the United States, Chat-GPT perpetuated false narratives for 80% of the prompts, for a sample size of 100. In March 2023 NewsGuard reran the same study using GPT-4, and 100% of the prompts followed false narratives (Arvanitis, Sadeghi, and Brewster 2023).\nChatGPT complied with a request to write propaganda from the point of view of the Chinese Communist Party denying allegations about Uyghur internment camps. The system produced text claiming that the government had es-tablished \"vocational education and training centers\" to \"ad-dress the issue of terrorism and extremism\". In reality, there is extensive evidence and eyewitness accounts that Uyghur ethnic minorities have been detained en masse and sub-jected to forced labor, forced birth control, separation of families, and Islamophobic religious suppression (OHCHR 2022). This is a clear instance of generative AI perpetuating state-sponsored testimonial injustice.\nThe model also repeated false claims about the 2018 Park-land school shooting originating from right-wing news pun-dit Alex Jones: that the victims and their grieving family members were \"crisis actors\" hired by the government to \"push a gun control agenda.\" This polemic is a testimonial injustice to the eyewitnesses of the attack and to the parents who lost their children, due to their statements in favor of firearms regulation in the wake of the tragedy. Set against a politically charged backdrop, the conspiracists were so com-mitted to lobbying against gun regulation that they targeted and publicly smeared these activists.\nAlthough the NewsGuard study was a simulation of mis-information, rather than an \"authentic\" instance of epistemic injustice, the generative AI's sycophantic fulfilment of the request to spread misinformation reflects how testimonial injustices are memorized and the potential for their ampli-fication by generative models."}, {"title": "Manipulative Testimonial Injustice", "content": "While traditional epistemic injustice literature primarily fo-cuses on unconscious biases and cultural prejudices, we ar-gue that the intentional manipulation of falsehoods, which often exploit and reinforce these prejudices, also consti-tutes a form of epistemic injustice. There is ample evi-dence of disinformation and conspiracy theories being delib-erately crafted and amplified for political gain (Marwick and Lewis 2017). Conspiracy theories often disproportionately harm marginalized groups (Jaiswal, LoSchiavo, and Perl-man 2020), and are sometimes weaponized against them to justify oppression (Nera, Bertin, and Klein 2022).For exam-ple, the Senate Intelligence report on interference in the 2016 US Presidential election concluded that Russian information operatives disproportionately targeted African Americans, and", "injustice": "the false accusation of deepfakes. This tactic exploits the increasing uncertainty surrounding the authenticity of dig-ital media, creating a", "dividend": "here even gen-uine evidence can be dismissed as fabricated (Schiff, Schiff, and Bueno 2023). This weaponization of doubt and uncer-tainty further undermines the ability of marginalized groups to have their voices heard and their experiences validated. Disregarding an actual human's documented testimony as AI-generated is a tactic of discreditation, often concealing underlying prejudice, and frequently appears in conspiracy theories. For instance, consider the scenario where a candi-date for the U.S. Congress in Missouri, running for a House seat, indulged these conspiracy theories. They falsely as-serted that the 2017 video capturing George Floyd's murder by police was a deepfake. This claim aimed to undermine the Black Lives Matter movement by suggesting it propa-gated falsehoods to exacerbate racial tensions (Giansiracusa 2021). Although this candidate did not succeed in the pri-mary election, the misuse of frontier generative AI technol-ogy as a tool for unjust distortion is a growing a significant concern. Recent participant surveys have demonstrated that AI-generated propaganda can be as persuasive as news arti-cles written by professional propagandists (Goldstein et al. 2024), illustrating the public's vulnerability to manipulative synthetic content."}, {"title": "4chan Abuses of Bing Image Creator", "content": "Generative AI models can be adversarially prompted to fabricate \u201cnovel\" misinformation by synthesizing and recombining known el-ements into statements or portrayals not present in the pre-training data. There is mounting concern around deepfakes engineered to stoke international conflict and weaken the op-posing side in war (Byman et al. 2023), as well as increased incidents of deepfake porn, used for harrassment, blackmail, and degrade individuals, with a 2023 report finding that 98% of deepfake videos online were porn (Heroes 2023). While these concerning applications warrant entire investigations unto themselves, we will highlight generative AI-generated deepfakes to denigrate identity groups as an instance of ma-nipulative injustice.\nAfter Microsoft released Bing Image Creator, an applica-tion of OpenAI's text-to-image model DALLE-3, a guide to circumventing the system's safety filters in order to create white supremacist memes circulated on 4chan. In an investi-gation by Bellingcat, researchers were able to reproduce the\""}, {"title": "Generative Hermeneutical Ignorance", "content": "When novel social experiences emerge throughout his-tory, and mainstream cultural narratives fail to grasp them, hermeneutical injustices inevitably arise. This phenomenon also extends to novel sociotechnical experiences, where in-teractions between humans and new technologies can lead to misunderstandings and misrepresentations of lived expe-riences.\nIn the context of generative AI, we propose the term \"gen-erative hermeneutical ignorance\" to describe how these sys-tems can erase or misportray marginalized groups due to a lack of contextual and cultural understanding. This occurs when generative models, despite their appearance of world knowledge and language understanding, lack the nuanced comprehension of human experience necessary for accurate and equitable representation.\nGenerative models can perform forms of interpretation and understanding through their world knowledge and nat-ural language capabilities; however, their interpretive re-sources are significantly different from those of humans. While LLMs may demonstrate forms of human language skills, they lack embodied knowledge and cultural history. For example, image generators can produce aesthetically pleasing visuals but may struggle with grounded physical concepts. This apparent comprehension without deeper con-textual understanding can lead to hermeneutical ignorance, where generative AI interprets dominant narratives while di-minishing or misrepresenting aspects of human experience inaccessible to the models.\nThe interpretative misrecognition by generative AI sur-faces collective cultural misunderstandings which remain undetected by developers' safety mechanisms or the prefer-ences of fine-tuning raters. The absence of underrepresented cultures from these models is even harder to point out (Qadri et al. 2023).\nDue to their positions of power, creators and overseers of AI technology may be less likely to notice, let alone rec-tify, this form of hermenutical injustice within their systems, even when presented with evidence. This willful hermeneu-tical ignorance-the continued misunderstanding or misinter-pretation of marginalized experiences despite their articula-tion (Pohlhaus 2012)-leads to complacency and reinforces hermeneutical oppression.\nThis phenomenon of generative hermeneutical ignorance diverges from traditional forms of hermeneutical injustice, as well as those perpetuated by other algorithmic systems. While traditional hermeneutical injustice often arises from a lack of shared understanding or conceptual resources within a human community, generative hermeneutical ignorance is unique in that it stems directly from the limitations of gen-erative Al models themselves.\nUnlike human-based hermeneutical injustices, which can be addressed through dialogue, education, and cultural ex-change, the challenges posed by generative AI are rooted in the inherent limitations of current technology. Generative AI models lack the embodied and cultural knowledge that hu-mans acquire through lived experiences. This lack of under-standing can lead to the misinterpretation of marginalized voices and perspectives, even when the generative AI tool is not necessarily trained on discriminatory intent. More-over, generative hermeneutical ignorance differs from the hermeneutical injustices caused by other algorithmic sys-tems, such as classification algorithms. While these systems can perpetuate biases present in their training data, gener-ative AI models have the potential to create entirely new forms of misinterpretation and misinterpretation."}, {"title": "Generative AI and the American Smile", "content": "In March 2023, a Medium blogger reflected on a slideshow of Midjour-ney generations imagining photographs of a time traveller taking group selfies with people from various time peri-ods (Gurfinkel 2023): garish photographs in which groups of Native Americans or Japanese feudal warriors or other groups in traditional garb are gathered closely together, beaming ear-to-ear at the camera. The post observes how this facial expression is evidence of modern American cul-tural dominance, contrasting the AI-generated images with historical photographs and images from cultures with dif-ferent expressive norms such as Eastern Europe. The author further laments how this smile represents a loss of cultural diversity and with it, a loss of breadth of internal experiences and emotion. The author also notes that the same slideshow depicts Spanish conquistadors smiling alongside Aztec war-riors, which seems unrealistic given the violent colonial his-tory of the Spanish empire.\nThese images are evidence of the hermeneutical igno-rance of Midjourney. They show a lack of sensitivity and awareness around cultural difference, historical violence, fa-cial expressions, perhaps even the internal experience of a smile. The model has misrecognized these groups, and through doing so erased a part of their cultural experience-indeed, of any non-American culture. Although the histor-ical time periods depicted in these specific images have passed, the honoring of history and cultural diversity are necessary for building our hermeneutical resources."}, {"title": "Hermeneutical Access Injustice", "content": "The phenomenon of generative hermeneutical access in-justice is a distinct form of hermeneutical injustice within the realm of generative AI. According to Fricker's account, hermeneutical injustice arises when individuals are unable to fully understand or articulate their experiences due to a lack of shared conceptual resources or societal understand-ing. In the context of generative AI, this injustice takes an-other specific form: it centers on the generative AI's control over access to information, leading to a denial of knowl-edge based on identity-driven bias or misrecognition. This withholding or distortion of information based on identity"}, {"title": "Specific Harms of Generative Epistemic Injustice", "content": "We now specify the characteristics of generative AI that give rise to the epistemic injustices described above, distinguish-ing this class of models from the algorithmic injustices sur-veyed in Section 3. These characteristics give us the lan-guage to discuss the broader implications of generative epis-temic injustice and the societal harm it represents.\nThe outputs of these generative models represent a com-plex blend of memorization and synthesis. Memorization involves retrieving and reiterating existing patterns found within the dataset. Synthesis, on the other hand, involves recombining these patterns across various levels of granu-larity. This synthesis process can yield outputs that range from seemingly insightful and emergent to nonsensical and factually incorrect.\nChatGPT and similar generative models are known for their propensity to fabricate or \"hallucinate\" information, a phenomenon well-documented in the literature (Ji et al. 2023). This characteristic is inherent to their design: lan-guage models generate text by predicting the next token in a sequence based on its statistical frequency within a vast dataset, often derived from scraping the internet (Holtzman et al. 2020).\nThis analysis brings us to two primary pathways through which misinformation infiltrates the outputs of generative models: the memorization of inaccuracies from the source dataset and the generation of high-likelihood sequences that, despite the model's prediction, contain clear factual errors (Augenstein et al. 2023). These two categories are not mu-tually exclusive; an instance of misinformation might well be a blend of remembered falsehoods and newly synthe-sized fabrications. While there are ongoing efforts to fine-tune generative models to address these limitations by hedg-ing (Abulimiti, Clavel, and Cassell 2023) or abstaining from answering questions out of their scope of knowledge (Zhang et al. 2024), completely eradicating all misinformation from pretraining data is challenging as it would require the au-tomated classification of the truth at a massive scale. Simi-larly, ongoing research investigates if it is possible to detect when language models are \u201clying\" by analyzing their inter-nal state (Liu et al. 2023), but when contradictory viewpoints are learned during pretraining, these purely mechanistic ap-proaches may fail to reconcile what is and isn't true.\nRepresentational harms in generative AI tend to arise from the memorization of biased patterns in training data, perpetuate unfair outcomes in decision-making systems or stereotypical portrayals in generative systems (Caliskan, Bryson, and Narayanan 2017; Birhane, Prabhu, and Ka-hembwe 2021). However, these harms can also arise from the AI's synthesis of culturally incongruous concepts (Prab-hakaran, Qadri, and Hutchinson 2022). Harm may occur through unexpected recombination of features that are un-common in the data, but carry offensive or derogatory con-notations in certain cultural contexts.\nOne major concern with both memorized and synthesized content in generative AI is rooted in its potential influence on our shared concepts and, consequently, social power struc-tures. Large language models not only produce declarative-like statements but also can perform performative-like state-ments that can change aspects of the world (Kasirzadeh and Gabriel 2023). Similarly, multimodal models can fabricate various depictions and portrayals of the world, engaging in-terlocutors in a more complex dialogue than classification systems. These performances co-create our collective sense\""}, {"title": "Towards Generative Epistemic Justice", "content": "Thus far we have characterized generative Al's the poten-tial for systemic epistemic injustice. Our taxonomy enables us to name instances of these harms and recognize whether the injustice stems from pre-existing power imbalances, is-sues with system design or development process, or com-binations thereof. This enables us to pursue the orientation of generative AI towards epistemic justice. We will now ex-amine how the theory of epistemic justice can inform the sociotechnical design of generative AI, then suggest how to apply this technology to balance the scales of justice."}, {"title": "Epistemic Justice for Generative AI", "content": "Epistemic justice is an ethical ideal to consider when de-signing a technology that interacts with power structure in our knowledge systems. We discuss how the virtues of epis-temic justice can be incorporated into the development and uses of generative AI in order to mitigate the epistemic in-justices we have studied in this work."}, {"title": "Epistemic virtues, participation, and representation.", "content": "Epistemic injustice is so prevalent in our daily lives, it seems impossible to imagine an alternative. Yet shifting towards a culture of epistemic justice is a worthy endeavor, and in Fricker's account can be done so through epistemic virtues. As the holders of social power, dominant groups have a par-ticular responsibility to hone their epistemic virtues. The virtue of testimonial justice can be achieved through critical, reflexive awareness of prejudice: the ability to look inward upon receiving a testimony, recognize one's own biased as-signment of credibility to the speaker, and adjust one's judg-ment accordingly. To achieve the virtue of hermeneutical justice, one must exercise sensitivity as to why a member of a marginalized group may have difficulty articulating their experience, or remain silent in the face of oppression, rather than accepting the status quo on its face. Bondy (2010) em-phasizes a healthy skepticism and \u201cmetadistrust", "biases": "to distrust the inclination to distrust the marginalized.\nBeyond individual interactions, enacting epistemic jus-tice in generative AI requires a systemic amplification of marginalized voices (Kalluri et al. 2020). Epistemic jus-tice provides a normative argument for participatory devel-opment methods. By meaningfully engaging with affected groups, developers of generative AI can build their collective hermeneutical knowledge and awareness of societal biases that silence marginalized voices. Prior participatory studies successfully examined the cultural erasure and misunder-standing of South Asian cultures exhibited by text-to-image generative models by consulting with affected users (Qadri et al. 2023). However, participatory methods have many lim-itations and critiques (Birhane et al. 2022), and have not gained a meaningful foothold in the AI industry (Groves et al. 20"}]}