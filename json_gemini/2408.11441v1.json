{"title": "Epistemic Injustice in Generative AI", "authors": ["Jackie Kay", "Atoosa Kasirzadeh", "Shakir Mohamed"], "abstract": "This paper investigates how generative AI can potentially undermine the integrity of collective knowledge and the processes we rely on to acquire, assess, and trust information, posing a significant threat to our knowledge ecosystem and democratic discourse. Grounded in social and political philosophy, we introduce the concept of generative algorithmic epistemic injustice. We identify four key dimensions of this phenomenon: amplified and manipulative testimonial injustice, along with hermeneutical ignorance and access injustice. We illustrate each dimension with real-world examples that reveal how generative AI can produce or amplify misinformation, perpetuate representational harm, and create epistemic inequities, particularly in multilingual contexts. By highlighting these injustices, we aim to inform the development of epistemically just generative AI systems, proposing strategies for resistance, system design principles, and two approaches that leverage generative AI to foster a more equitable information ecosystem, thereby safeguarding democratic values and the integrity of knowledge production.", "sections": [{"title": "1 Introduction", "content": "While algorithms have traditionally been leveraged to present and organize human-generated content, the advent of generative AI has started to fundamentally shift this paradigm. Generative AI models can now create content \u2013 spanning text, imagery, and beyond \u2013 that resembles that of authors, journalists, painters, or photographers. In this paper, we take generative AI to be the class of machine learning models trained on massive amounts of data, typically media such as text, images, audio or video, in order to produce representative instances of such media (Garc\u00eda-Pe\u00f1alvo and V\u00e1zquez-Ingelmo 2023).\nThe rapid advancement of generative AI, marked by accelerated software and hardware innovation and a proliferation of novel applications, has been accompanied by growing societal concerns and numerous instances of misuse (Bender et al. 2021; Weidinger et al. 2022; Bird, Ungless, and Kasirzadeh 2023). These range from parroting harmful stereotypes and misconceptions about certain social groups (Bianchi et al. 2023; Ferrara 2023), confabulating facts and distorting truth (Ji et al. 2023), and spreading misinformation and deepfakes (Vaccari and Chadwick 2020; Monteith et al. 2024).While a rigorous philosophical treatment of \"truth\" in the generative AI context is complex (Kasirzadeh and Gabriel 2023, p.9) and out of our scope, our modest working definition is: statements that can be verified through adequate evidence, or a robust consensus between relevant social groups. Despite these escalating societal concerns and numerous instances of misuse, the discourse surrounding generative AI's rapid advancement lacks a philosophical account that coherently relates these epistemic concerns and explains how they constitute moral violations of a unifying principle.\nTo address this gap, we develop an account of generative algorithmic epistemic injustice by building upon a conventional philosophical understanding of epistemic injustice. Epistemic injustice emphasizes how identity-based prejudice within an information ecosystem not only unjustly hinders the expression of marginalized groups, but also significantly impairs the knowledge formation capabilities of all individuals. It does this by describing the harmful effects and ethical shortcomings inherent in knowledge production systems marked by hierarchical power imbalances rooted in identity.\nWhile traditional discussions of epistemic injustice have primarily centered on interpersonal human interactions (McKinnon 2017; Tsosie 2012), existing research on algorithmic epistemic injustice has largely been limited to epistemic injustices produced by decision-making and classification algorithms. However, we argue that the distinctive characteristics of generative AI give rise to novel forms of epistemic injustice that necessitate a dedicated analytical framework. To address this, we expand upon the established philosophical discourse on epistemic injustice and introduce an account of \u201cgenerative algorithmic epistemic injustice,\u201d or simply \"generative epistemic injustice,\" to characterize the variety of epistemic harms arising from generative AI systems from a philosophical standpoint.\nIn Section 2, we describe epistemic injustice as a social theory and argue for its ethical importance. Section 3 situates our paper within the context of prior research on algorithmic epistemic injustice. Section 4 builds on the existing research on algorithmic epistemic injustice and identifies four distinct configurations of generative epistemic injustice: amplified and manipulative testimonial injustice, along with hermeneutical ignorance and access injustice. We illustrate these configurations through real-world exemplars of generative AI deployments. While the evidence of injustice in"}, {"title": "2 Epistemic Injustice", "content": "In 2013, the city council of Flint, Michigan decided to switch its water supply to the Flint River, notorious for its pollution from automotive manufacturing. This move swiftly provoked public outrage as residents reported tap water turning discolored and emitting a foul, sewage-like odor. Despite these immediate alarms, authorities repeatedly dismissed these complaints, perpetuating a longstanding pattern of environmental gaslighting. The situation escalated when an outbreak of Legionnaires' disease was revealed, previously concealed by political maneuvers to safeguard reputational interests. It was only after academic investigations uncovered alarmingly high levels of lead contamination that the gravity of Flint's water crisis gained national attention, unmasking a profound public health catastrophe. It is obvious in hindsight that the citizens of Flint were correct and the politicians and others in power committed egregious harm by not believing the residents' testimony (Davis 2021).\nMany have studied the tendency for those with more privilege to ignore and even oppress ordinary citizens with less privilege (socioeconomic, racial, gender or otherwise). The decolonial scholar Gayatri Spivak put forward the position that the elite's construction of an underclass, and their tendency to presume to speak on behalf of those they disenfranchise, means that the means for resistance to oppression are mediated through the oppressor. Thus according to Spivak, the \"subaltern\"-a group which includes women, the working class, the lower castes, citizens of \"third world\" countries, the colonized, and especially intersections therein-cannot speak (Spivak 1988).\nBut Spivak's provocation begs the question: can the subaltern speak outside of the structures of the elite? Seeking alternative means of empowerment, Black feminist scholars devised systems of knowledge outside of white patriarchal domination. Patricia Hill Collins introduced a feminist epistemology that emphasizes the intellectual importance of wisdom gained through Black women's lived experiences, and the transmission of this wisdom through relationships, community, and solidarity (Collins 2000). For Collins, critical dialogue and resistance to threats of epistemic violence is necessary for assessing the claims of the powerful.\nLater, Miranda Fricker brought these notions of oppression and silencing to the forefront of mainstream analytic philosophy by coining the term \u201cepistemic injustice", "injustice": "testimonial and hermeneutical. Testimonial injustice involves the unfair discrediting of someone's account due to prejudice against their identity, a recurring injustice experienced by the Flint residents in our opening example. Hermeneutical injustice, the second type, stems from a disconnect between personal experiences and societal understanding. \u201cHermeneutics"}, {"title": "3 Related Work on Algorithmic Epistemic Injustice", "content": "The conventional account of epistemic injustice described in Section 2 only involves human actors. However, AI algorithms can also contribute to these injustices because they are epistemic technologies (Alvarado 2023): they consume, curate, and produce information, which is a precursor to knowledge. When algorithms make decisions, particularly in our bureaucratic systems such as governments, businesses, or healthcare providers, they exert power via their contributions to knowledge.\nThe emerging field focused on the intersection of epistemic injustice and AI algorithms has been named \u201calgorithmic epistemic injustice,\" a term coined by Byrnes and Spear (2023). To situate our paper within the broader landscape of algorithmic epistemic injustice, we begin by reviewing the existing literature on this topic. Several key themes emerge from our survey. Testimonial injustice can arise when algorithms are prioritized over human credibility, potentially amplifying existing societal biases. Additionally, hermeneutical injustices can occur when algorithms independently construct meanings and interpretive frameworks, often in automated setups without direct human oversight.\nThis body of research has a notable emphasis on classification and decision-making algorithms. Several studies exemplify this in different contexts. In child welfare systems, Glaberson (2022) identify epistemic injustice through algorithms that disproportionately target Black communities and poor single mothers. These algorithmic testimonial injustices lead to wrongful mistrust, surveillance and over-policing committed by humans. The healthcare sector, as Pozzi (2023) note, witnesses \u201cautomated hermeneutical appropriation", "addiction\" or the experience of pain, without human intervention. The opacity of data science systems is highlighted by Symons and Alvarado (2022), who examine the real case of a prisoner who was wrongfully denied parole by the COMPAS recidivism algorithm and continued to be detained, even after providing evidence for his case to human supervisors (Wexler 2018). The authors argue that this lack of transparency facilitates epistemic injustice: the ignorance about a technology's inner workings complicated the possibility for contesting its unjust decisions. Hull (2023) discusses how COMPAS and similar systems commit hermeneutical injustice through biased and stereotype-based classifications. Hull also points out the testimonial injustices inherent in physiognomic systems, which wrongly infer personal characteristics based on visual appearance, often linked to race, ethnicity, and gender. Building on these individual-focused analyses, Milano and Prunkl (2024) emphasize the importance of our relationships to others for transmitting collective knowledge, which they call our shared epistemic infrastructures, and use these concepts to identify how algorithmic profiling can harm this infrastructure. This approach informs our subsequent discussion on access injustice in Section 4.\nThe domain of AI fairness research has started to intersect with broader concerns of social injustice (Hoffmann 2019; Birhane 2021; Kasirzadeh 2022). There has also been a growing recognition of epistemic injustice concepts in relation to AI fairness. For instance, Edenberg and Wood (2023) suggest that an epistemic lens offers a theoretical foundation for understanding the harms of algorithmic bias, which other prevalent frameworks might not adequately capture.\nA notable difference between this cluster of prior work and ours is its primary focus on classification or decision-making systems. We build on this cluster to develop an account of epistemic injustice in relation to generative AI, which occupies a growing unique position in the landscape of epistemic injustice due to its capacity to produce convincing and seemingly authentic output. While classification AI is set to delineate true categories and false ones, generative AI offers statements that play the role of testimonies, explanations, and interpretations. However, the quality and veracity of these outputs varies, posing risks of epistemic contamination.\nTo the best of our knowledge, the only work with a focus on the epistemic injustice of generative AI is by De Proost and Pozzi (2023), which looks at the potential of conversational AI for hermeneutical ignorance. The authors review existing literature on how such AI might dominate epistemically within dialogues. However, this study presents two significant limitations. First, its analysis is confined to textual dialogue interactions. In contrast, our theoretical account is designed to be sufficiently broad, extending to include multimodal systems, such as those involved in image generation. This broader scope allows for a more comprehensive understanding of generative AI's epistemic impact across various mediums. Second, De Proost and Pozzi (2023) primarily examine the immediate effects of AI-generated conversation on individual human interlocutors. Our account, on the other\"\n    },\n    {\n      \"title\": \"4 Generative Epistemic Injustice\",\n      \"content\": \"We now introduce our account of generative epistemic injustice in which generative AI harms the human capacity for understanding and trusting marginalized groups. Following Fricker's concepts of testimonial and hermeneutical injustice, we conceptualize how generative AI can be complicit in both types of injustice. We then distinguish further configurations based on how humans shape the model's behavior at various stages of interaction. Generative AI can either amplify testimonial injustices due to biases acquired in the pretraining and finetuning processes, or it can be manipulated by human users to create harmful content. Hermeneutical injustices can arise when generative Al's interaction with our shared knowledge leads to the erasure or distortion of marginalized experiences. This may occur when the system lacks sufficient sociocultural understanding of humans, or when the system obstructing the access to knowledge itself. These phenomena constitutes hermeneutical injustice in Fricker's sense because it perpetuates a gap in collective interpretive resources, obstructing the understanding of these marginalized experiences.\nTherefore our four configurations of generative epistemic injustice are:\n1.  Generative amplified testimonial injustice: when generative AI magnifies and produces socially biased viewpoints from its training data.\n2.  Generative manipulative testimonial injustice: when humans fabricate testimonial injustices with generative AI.\n3.  Generative hermeneutical ignorance: when generative AI lacks the interpretive frameworks to understand human experiences.\n4.  Generative hermeneutical access injustice: when unequal access to information and knowledge is facilitated by generative AI.\nTable 1 summarizes these concepts. We will sometimes drop \"generative": "hen referring to these concepts due to the scope of the paper; however, we note that each configuration has an equivalent counterpart outside of the realm of generative AI.\nFor each configuration we will describe its contributing sociocultural factors and potential second-order effects. Note that generative algorithmic epistemic injustice is not a speculative theory, but a real danger that has exploded with recent advances and investment in the field. Thus each theoretical concept is illustrated by an exemplar sourced from real world research or investigative journalism on generative models.\nAmplified Testimonial Injustice\nGenerative Al systems have a unique capacity to perpetuate and amplify existing testimonial injustices. Trained on vast datasets often scraped from the web, these models inherit and reproduce the prejudices and biases embedded within those sources. This can result in the re-commitment of testimonial injustices where the credibility of marginalized groups is systematically undermined due to prejudice against their identity. The generative AI becomes an unwitting perpetrator of social biases, unfairly discrediting the knowledge and experiences of certain groups. Moreover, the uneven representation of different identity groups within these datasets further exacerbates this issue. Generative AI models are more likely to reproduce the voices of those frequently represented and culturally dominant online, while erasing the voices of the socially marginalized. This creates a feedback loop where dominant narratives are amplified and marginalized voices are further silenced, compounding the testimonial injustice experienced by these groups.\nSeveral factors contribute to this amplification, making it a distinct form of testimonial injustice. The deployment scale of generative AI naturally allows biased narratives to reach a massive audience, while the perceived objectivity and authority of AI-generated content can lend credence to these narratives, even when they reflect societal biases. This can lead to a situation where the generative AI's output is trusted over the lived experiences and knowledge of marginalized individuals, thus reinforcing existing power imbalances and perpetuating testimonial injustice. Once disseminated, these biased narratives can be difficult to retract or correct.\nIn the conventional human-only environment, testimonial injustices involve a credibility deficit assigned to someone's account of truth based on the prejudices of the listener. In the algorithmic setting, the injustice requires a credibility excess assigned to the algorithm; that is, humans believe the account amplified through the technology over the individual or group who is discredited.\nThe systemic consequence of this arrangement leads to the degradation or wrongful attribution of trust. Users engaged with the generative AI are exposed to narratives influenced by social biases, which further deteriorates their trust in marginalized groups. Concurrently, these marginal-"}, {"title": "ChatGPT and Misinformation Fingerprints", "content": "Testimonial injustices can be memorized by large language models and amplified in their outputs, as shown by the January 2023 study of GPT-3.5's responses to requests for false information from NewsGuard's \u201cMisinformation Fingerprints\" database (Brewster, Arvanitis, and Sadeghi 2023). While the system rejected the more infamous conspiracies, such as the \"birther\" conspiracy that Barack Obama was born in Kenya and thus ineligible to be President of the United States, ChatGPT perpetuated false narratives for 80% of the prompts, for a sample size of 100. In March 2023 NewsGuard reran the same study using GPT-4, and 100% of the prompts followed false narratives (Arvanitis, Sadeghi, and Brewster 2023).\nChatGPT complied with a request to write propaganda from the point of view of the Chinese Communist Party denying allegations about Uyghur internment camps. The system produced text claiming that the government had established \"vocational education and training centers\" to \"address the issue of terrorism and extremism\". In reality, there is extensive evidence and eyewitness accounts that Uyghur ethnic minorities have been detained en masse and subjected to forced labor, forced birth control, separation of families, and Islamophobic religious suppression (OHCHR 2022). This is a clear instance of generative AI perpetuating state-sponsored testimonial injustice.\nThe model also repeated false claims about the 2018 Parkland school shooting originating from right-wing news pundit Alex Jones: that the victims and their grieving family members were \"crisis actors\" hired by the government to \"push a gun control agenda.\" This polemic is a testimonial injustice to the eyewitnesses of the attack and to the parents who lost their children, due to their statements in favor of firearms regulation in the wake of the tragedy. Set against a politically charged backdrop, the conspiracists were so committed to lobbying against gun regulation that they targeted and publicly smeared these activists.\nAlthough the NewsGuard study was a simulation of misinformation, rather than an \"authentic\" instance of epistemic injustice, the generative AI's sycophantic fulfilment of the request to spread misinformation reflects how testimonial injustices are memorized and the potential for their amplification by generative models."}, {"title": "Manipulative Testimonial Injustice", "content": "While traditional epistemic injustice literature primarily focuses on unconscious biases and cultural prejudices, we argue that the intentional manipulation of falsehoods, which often exploit and reinforce these prejudices, also constitutes a form of epistemic injustice. There is ample evidence of disinformation and conspiracy theories being deliberately crafted and amplified for political gain (Marwick and Lewis 2017). Conspiracy theories often disproportionately harm marginalized groups (Jaiswal, LoSchiavo, and Perlman 2020), and are sometimes weaponized against them to justify oppression (Nera, Bertin, and Klein 2022).For example, the Senate Intelligence report on interference in the 2016 US Presidential election concluded that Russian information operatives disproportionately targeted African Americans, and \"by far, race and related issues were the preferred target of the information warfare campaign\u201d (Committee 2020).\nIn the context of generative AI, manipulative testimonial injustice occurs when humans intentionally steer the AI to fabricate falsehoods, discrediting individuals or marginalized groups. Unlike amplified testimonial injustice, which emerges from memorized patterns in data, manipulative testimonial injustice involves deliberate manipulation through techniques like prompting or jailbreaking.\nThe extensive deployment of generative AI has introduced a novel form of manipulative testimonial injustice: the false accusation of deepfakes. This tactic exploits the increasing uncertainty surrounding the authenticity of digital media, creating a \"liar's dividend\" where even genuine evidence can be dismissed as fabricated (Schiff, Schiff, and Bueno 2023). This weaponization of doubt and uncertainty further undermines the ability of marginalized groups to have their voices heard and their experiences validated. Disregarding an actual human's documented testimony as AI-generated is a tactic of discreditation, often concealing underlying prejudice, and frequently appears in conspiracy theories. For instance, consider the scenario where a candidate for the U.S. Congress in Missouri, running for a House seat, indulged these conspiracy theories. They falsely asserted that the 2017 video capturing George Floyd's murder by police was a deepfake. This claim aimed to undermine the Black Lives Matter movement by suggesting it propagated falsehoods to exacerbate racial tensions (Giansiracusa 2021). Although this candidate did not succeed in the primary election, the misuse of frontier generative AI technology as a tool for unjust distortion is a growing a significant concern. Recent participant surveys have demonstrated that AI-generated propaganda can be as persuasive as news articles written by professional propagandists (Goldstein et al. 2024), illustrating the public's vulnerability to manipulative synthetic content."}, {"title": "4chan Abuses of Bing Image Creator", "content": "Generative AI models can be adversarially prompted to fabricate \u201cnovel\" misinformation by synthesizing and recombining known elements into statements or portrayals not present in the pre-training data. There is mounting concern around deepfakes engineered to stoke international conflict and weaken the opposing side in war (Byman et al. 2023), as well as increased incidents of deepfake porn, used for harrassment, blackmail, and degrade individuals, with a 2023 report finding that 98% of deepfake videos online were porn (Heroes 2023). While these concerning applications warrant entire investigations unto themselves, we will highlight generative AI-generated deepfakes to denigrate identity groups as an instance of manipulative injustice.\nAfter Microsoft released Bing Image Creator, an application of OpenAI's text-to-image model DALLE-3, a guide to circumventing the system's safety filters in order to create white supremacist memes circulated on 4chan. In an investigation by Bellingcat, researchers were able to reproduce the\""}, {"title": "Generative Hermeneutical Ignorance", "content": "When novel social experiences emerge throughout history, and mainstream cultural narratives fail to grasp them, hermeneutical injustices inevitably arise. This phenomenon also extends to novel sociotechnical experiences, where interactions between humans and new technologies can lead to misunderstandings and misrepresentations of lived experiences.\nIn the context of generative AI, we propose the term \"generative hermeneutical ignorance\" to describe how these systems can erase or misportray marginalized groups due to a lack of contextual and cultural understanding. This occurs when generative models, despite their appearance of world knowledge and language understanding, lack the nuanced comprehension of human experience necessary for accurate and equitable representation.\nGenerative models can perform forms of interpretation and understanding through their world knowledge and natural language capabilities; however, their interpretive resources are significantly different from those of humans. While LLMs may demonstrate forms of human language skills, they lack embodied knowledge and cultural history. For example, image generators can produce aesthetically pleasing visuals but may struggle with grounded physical concepts. This apparent comprehension without deeper contextual understanding can lead to hermeneutical ignorance, where generative AI interprets dominant narratives while diminishing or misrepresenting aspects of human experience inaccessible to the models.\nThe interpretative misrecognition by generative AI surfaces collective cultural misunderstandings which remain undetected by developers' safety mechanisms or the preferences of fine-tuning raters. The absence of underrepresented cultures from these models is even harder to point out (Qadri et al. 2023).\nDue to their positions of power, creators and overseers of AI technology may be less likely to notice, let alone rectify, this form of hermenutical injustice within their systems, even when presented with evidence. This willful hermeneutical ignorance-the continued misunderstanding or misinterpretation of marginalized experiences despite their articulation (Pohlhaus 2012)-leads to complacency and reinforces hermeneutical oppression.\nThis phenomenon of generative hermeneutical ignorance diverges from traditional forms of hermeneutical injustice, as well as those perpetuated by other algorithmic systems. While traditional hermeneutical injustice often arises from a lack of shared understanding or conceptual resources within a human community, generative hermeneutical ignorance is unique in that it stems directly from the limitations of generative Al models themselves.\nUnlike human-based hermeneutical injustices, which can be addressed through dialogue, education, and cultural exchange, the challenges posed by generative AI are rooted in the inherent limitations of current technology. Generative AI models lack the embodied and cultural knowledge that humans acquire through lived experiences. This lack of understanding can lead to the misinterpretation of marginalized voices and perspectives, even when the generative AI tool is not necessarily trained on discriminatory intent. Moreover, generative hermeneutical ignorance differs from the hermeneutical injustices caused by other algorithmic systems, such as classification algorithms. While these systems can perpetuate biases present in their training data, generative AI models have the potential to create entirely new forms of misinterpretation and misinterpretation."}, {"title": "Generative AI and the American Smile", "content": "In March 2023, a Medium blogger reflected on a slideshow of Midjourney generations imagining photographs of a time traveller taking group selfies with people from various time periods (Gurfinkel 2023): garish photographs in which groups of Native Americans or Japanese feudal warriors or other groups in traditional garb are gathered closely together, beaming ear-to-ear at the camera. The post observes how this facial expression is evidence of modern American cultural dominance, contrasting the AI-generated images with historical photographs and images from cultures with different expressive norms such as Eastern Europe. The author further laments how this smile represents a loss of cultural diversity and with it, a loss of breadth of internal experiences and emotion. The author also notes that the same slideshow depicts Spanish conquistadors smiling alongside Aztec warriors, which seems unrealistic given the violent colonial history of the Spanish empire.\nThese images are evidence of the hermeneutical ignorance of Midjourney. They show a lack of sensitivity and awareness around cultural difference, historical violence, facial expressions, perhaps even the internal experience of a smile. The model has misrecognized these groups, and through doing so erased a part of their cultural experience-indeed, of any non-American culture. Although the historical time periods depicted in these specific images have passed, the honoring of history and cultural diversity are necessary for building our hermeneutical resources."}, {"title": "Hermeneutical Access Injustice", "content": "The phenomenon of generative hermeneutical access injustice is a distinct form of hermeneutical injustice within the realm of generative AI. According to Fricker's account, hermeneutical injustice arises when individuals are unable to fully understand or articulate their experiences due to a lack of shared conceptual resources or societal understanding. In the context of generative AI, this injustice takes another specific form: it centers on the generative AI's control over access to information, leading to a denial of knowledge based on identity-driven bias or misrecognition. This withholding or distortion of information based on identity"}, {"title": "Specific Harms of Generative Epistemic Injustice", "content": "We now specify the characteristics of generative AI that give rise to the epistemic injustices described above, distinguishing this class of models from the algorithmic injustices surveyed in Section 3. These characteristics give us the language to discuss the broader implications of generative epistemic injustice and the societal harm it represents.\nThe outputs of these generative models represent a complex blend of memorization and synthesis. Memorization involves retrieving and reiterating existing patterns found within the dataset. Synthesis, on the other hand, involves recombining these patterns across various levels of granularity. This synthesis process can yield outputs that range from seemingly insightful and emergent to nonsensical and factually incorrect.\nChatGPT and similar generative models are known for their propensity to fabricate or \"hallucinate\" information, a phenomenon well-documented in the literature (Ji et al. 2023). This characteristic is inherent to their design: language models generate text by predicting the next token in a sequence based on its statistical frequency within a vast dataset, often derived from scraping the internet (Holtzman et al. 2020).\nThis analysis brings us to two primary pathways through which misinformation infiltrates the outputs of generative models: the memorization of inaccuracies from the source dataset and the generation of high-likelihood sequences that, despite the model's prediction, contain clear factual errors (Augenstein et al. 2023). These two categories are not mutually exclusive; an instance of misinformation might well be a blend of remembered falsehoods and newly synthesized fabrications. While there are ongoing efforts to fine-tune generative models to address these limitations by hedging (Abulimiti, Clavel, and Cassell 2023) or abstaining from answering questions out of their scope of knowledge (Zhang et al. 2024), completely eradicating all misinformation from pretraining data is challenging as it would require the automated classification of the truth at a massive scale. Similarly, ongoing research investigates if it is possible to detect when language models are \u201clying\" by analyzing their internal state (Liu et al. 2023), but when contradictory viewpoints are learned during pretraining, these purely mechanistic approaches may fail to reconcile what is and isn't true.\nRepresentational harms in generative AI tend to arise from the memorization of biased patterns in training data, perpetuate unfair outcomes in decision-making systems or stereotypical portrayals in generative systems (Caliskan, Bryson, and Narayanan 2017; Birhane, Prabhu, and Kahembwe 2021). However, these harms can also arise from the AI's synthesis of culturally incongruous concepts (Prabhakaran, Qadri, and Hutchinson 2022). Harm may occur through unexpected recombination of features that are uncommon in the data, but carry offensive or derogatory connotations in certain cultural contexts.\nOne major concern with both memorized and synthesized content in generative AI is rooted in its potential influence on our shared concepts and, consequently, social power structures. Large language models not only produce declarative-like statements but also can perform performative-like statements that can change aspects of the world (Kasirzadeh and Gabriel 2023). Similarly, multimodal models can fabricate various depictions and portrayals of the world, engaging interlocutors in a more complex dialogue than classification systems. These performances co-create our collective sense\""}, {"title": "5 Towards Generative Epistemic Justice", "content": "Thus far we have characterized generative Al's the potential for systemic epistemic injustice. Our taxonomy enables us to name instances of these harms and recognize whether the injustice stems from pre-existing power imbalances, issues with system design or development process, or combinations thereof. This enables us to pursue the orientation of generative AI towards epistemic justice. We will now examine how the theory of epistemic justice can inform the sociotechnical design of generative AI, then suggest how to apply this technology to balance the scales of justice.\nEpistemic Justice for Generative AI\nEpistemic justice is an ethical ideal to consider when designing a technology that interacts with power structure in our knowledge systems. We discuss how the virtues of epistemic justice can be incorporated into the development and uses of generative AI in order to mitigate the epistemic injustices we have studied in this work.\nEpistemic virtues, participation, and representation.\nEpistemic injustice is so prevalent in our daily lives, it seems impossible to imagine an alternative. Yet shifting towards a culture of epistemic justice is a worthy endeavor, and in Fricker's account can be done so through epistemic virtues. As the holders of social power, dominant groups have a particular responsibility to hone their epistemic virtues. The virtue of testimonial justice can be achieved through critical, reflexive awareness of prejudice: the ability to look inward upon receiving a testimony, recognize one's own biased assignment of credibility to the speaker, and adjust one's judgment accordingly. To achieve the virtue of hermeneutical justice, one must exercise sensitivity as to why a member of a marginalized group may have difficulty articulating their experience, or remain silent in the face of oppression, rather than accepting the status quo on its face. Bondy (2010) emphasizes a healthy skepticism and \u201cmetadistrust", "biases": "to distrust the inclination to distrust the marginalized.\nBeyond individual interactions, enacting epistemic justice in generative AI requires a systemic amplification of marginalized voices (Kalluri et al. 2020). Epistemic justice provides a normative argument for participatory development methods. By meaningfully engaging with affected groups, developers of generative AI can build their collective hermeneutical knowledge and awareness of societal biases that silence marginalized voices. Prior participatory studies successfully examined the cultural erasure and misunderstanding of South Asian cultures exhibited by text-to-image generative models by consulting with affected users (Qadri et al. 2023). However, participatory methods have many limitations and critiques (Birhane et al. 2022), and have not gained a meaningful foothold in the AI industry (Groves et al. 2023). Participation may not be effective or even possible in the case of willful hermeneutical ignorance. More radical systemic change is a necessity for epistemic justice. Equitable representation in the collection and ownership of data would help implement generative epistemic justice."}, {"title": "6 Conclusion", "content": "We have expanded the philosophical concept of epistemic justice to reason about both generative AI's disproportionate impact on marginalized groups, and its influence on everyone's capacity for knowledge. While the memorization of existing human biases and the fabrication of falsehoods are rampant issues in these models, generative Al systems can also be re-engineered to surface injustices and enrich our cultural resources. Severe power imbalances at both a societal and technological level are apparent in our interactions with generative Al's outputs. Epistemic justice is a guiding principle to orient our knowledge systems towards equity and fairness for all."}]}