{"title": "EMBODIEDCITY: A BENCHMARK PLATFORM FOR EMBODIED AGENT IN REAL-WORLD CITY ENVIRONMENT", "authors": ["Chen Gao", "Baining Zhao", "Weichen Zhang", "Jinzhu Mao", "Jun Zhang", "Zhiheng Zheng", "Fanhang Man", "Jianjie Fang", "Zile Zhou", "Jinqiang Cui", "Xinlei Chen", "Yong Li"], "abstract": "Embodied artificial intelligence (EmbodiedAI) emphasizes the role of an agent's body in generating human-like behaviors. The recent efforts on EmbodiedAI pay a lot of attention to building up machine learning models to possess perceiving, planning, and acting abilities, thereby enabling real-time interaction with the world. However, most works focus on bounded indoor environments, such as navigation in a room or manipulating a device, with limited exploration of embodying the agents in open-world scenarios. That is, embodied intelligence in the open and outdoor environment is less explored, for which one potential reason is the lack of high-quality simulators, benchmarks, and datasets. To address it, in this paper, we construct a benchmark platform for embodied intelligence evaluation in real-world city environments. Specifically, we first construct a highly realistic 3D simulation environment based on the real buildings, roads, and other elements in a real city. In this environment, we combine historically collected data and simulation algorithms to conduct simulations of pedestrian and vehicle flows with high fidelity. Further, we designed a set of evaluation tasks covering different EmbodiedAI abilities. Moreover, we provide a complete set of input and output interfaces for access, enabling embodied agents to easily take task requirements and current environmental observations as input and then make decisions and obtain performance evaluations. On the one hand, it expands the capability of existing embodied intelligence to higher levels. On the other hand, it has a higher practical value in the real world and can support more potential applications for artificial general intelligence. Based on this platform, we evaluate some popular large language models for embodied intelligence capabilities of different dimensions and difficulties. The executable program of this platform is available for download, and we have also released an easy-to-use Python library and detailed tutorial documents. All of the software, Python library, codes, datasets, tutorials, and real-time online service are available on this website: https://embodied-city.fiblab.net.", "sections": [{"title": "INTRODUCTION", "content": "Embodied AI (Duan et al., 2022) serves as the recent advance of artificial intelligence, presenting an emerging paradigm shift from the traditional artificial intelligence, which learns from static datasets (e.g., ImageNet, which contains 2D images). Specifically, embodied artificial intelligence is expected to behave like a real human, which is able to learn from the environment and dynamically interact with the world, considered an essential approach to Artificial General Intelligence (AGI) (Du\u00e9\u00f1ez-Guzm\u00e1n et al., 2023). Various tasks for embodied intelligence have been established in different domains, including robotics (He et al., 2023; Barreiros et al., 2022; Driess et al., 2023), game AI (Fan et al., 2022a; Nottingham et al., 2023), unmanned vehicles/aerial drones (Zhou et al., 2022), etc. Generally speaking, EmbodiedAI requires the agent to accurately understand the environment, perform high-level reasoning, and effectively choose appropriate actions to execute tasks. Therefore, the training and testing of embodied intelligence models are closely related to the environment. To accelerate training efficiency and test embodied agents more conveniently, researchers generally choose to"}, {"title": "RELATED WORK", "content": "Autonomous Driving. One of the related research directions is autonomous driving, in which the simulation platform can be used to train the driving and controlling algorithms for autonomous vehicles. CARLA (Dosovitskiy et al., 2017) constructs an environment for autonomous driving, which features urban streets and vehicle models equipped with sensing and control modules. However, it focuses on modeling road surfaces and traffic in small-town settings, with less emphasis on the realism of street layouts, urban planning, and building structures. MetaDrive Li et al. (2022) balances visual quality and efficiency by using Panda3D and Bullet to offer a lightweight driving simulator that supports research on generalizable reinforcement learning algorithms for vehicles. NuScenes (Caesar et al., 2020) provides a perception dataset captured on real roads using cameras, radars, and lidar. However, these works are designed as simulators and datasets for autonomous driving and are less suitable for broader urban embodied task research.\nEmbodied-AI Platforms based on Real City. CityNav (Lee et al., 2024) has developed a WebGL-based simulator grounded in real cities, but the tasks supported by this platform are quite limited, and actions in the navigation task are discrete, with a large gap to the real-world navigation. Both V-IRL (Yang et al., 2024) and TOUCHDOWN (Chen et al., 2019) are built on Google Maps, offering the highest visual realism, yet they only support discrete movements. AVDN (Fan et al., 2022b) provides agents with RGB sensing inputs derived from satellite images, which are of low precision. That is, these platforms based on real cities struggle to meet the high-quality, high-precision, and continuous sensing and movement requirements of embodied agents.\nEmbodied-AI Platforms based on Fictional City. MetaUrban (Wu et al., 2024) has developed a streetscape generation simulator that facilitates top-down design for road layout, object placement, and dynamic urban traffic generation, providing convenience for navigation simulation of ground-based robots. However, it focuses solely on the neighborhood level, neglecting city-scale design, thereby limiting the activity space of agents. Additionally, object modeling is relatively coarse, at a sticker level, which may pose challenges for applications like image recognition algorithms. GRUtopia (Wang et al., 2024) offers a variety of navigation, dialogue, and manipulation tasks for different types of ground robots within multiple indoor scenarios such as hospitals, restaurants, and libraries (it is still an indoor environment). AerialVLN (Liu et al., 2023) primarily addresses the vision-and-language navigation problem for drones in urban environments. However, these"}, {"title": "THE BENCHMARK PLATFORM", "content": "The core of this benchmark platform is an environment (as shown in Figure 1). Based on this environment, we have established interfaces for agents to be deployed in the environment, read input with first-view observations, and make decisions. The overall workflow is shown in Figure 2. In this section, we will elaborate on the detailed information, the 3D environment, the interface for embodied agents, SDK, and online access."}, {"title": "3D ENVIRONMENT", "content": "The basic environment of the simulator includes a 2.8km \u00d7 2.4km district in Beijing, one of the biggest cities in China, where we meticulously build 3D models for buildings, streets, and other outdoor elements, ensuring high-fidelity urban simulations, all hosted by Unreal Engine 5.32. In addition to this business district, we have also developed a nearby residential area that features detailed interior modeling, allowing the simulator to support both indoor and outdoor tasks. Below are the key components that make up the environment:\n\u2022 Buildings. We first manually use Blender\u00b3 to create the 3D assets of the buildings, for which we use the streetview services of Baidu Map\u2074 and Amap5. The city-level detail includes approximately 200 buildings, encompassing a variety of types such as office towers, shopping malls, residential complexes, and public facilities. These models are textured and detailed to closely resemble their real-world counterparts to enhance realism in the simulation.\n\u2022 Streets. The city contains a total of 100 streets, with a combined length of approximately 50 km. The streets are modeled to include all necessary components such as lanes, intersections, traffic signals, and road markings. We also incorporate pedestrian pathways, cycling lanes, and parking areas. Data from traffic monitoring systems and mapping services help ensure that the street layout and traffic flow patterns are accurate and realistic.\n\u2022 Vehicles and Pedestrians. Dynamic elements such as vehicles and pedestrians are simulated to move realistically within the environment. The simulation algorithms for these elements are based on the Mirage Simulation System (Zhang et al., 2022), providing realistic interactions and behaviors that mimic real-world traffic and pedestrian dynamics.\n\u2022 Other Elements. Besides streets and buildings, other elements include street furniture (benches, streetlights, signs), vegetation (trees, shrubs, lawns), and urban amenities (bus stops, metro en-"}, {"title": "INTERFACE OF EMBODIED AGENTS", "content": "With the environment of Unreal Engine, we further build the interface of embodied agents to ensure the agents can indeed embody themselves in the system. To implement it, we develop the input/output interfaces based on AirSim6, based on which the observations can be conducted in a first-view manner, and the control actions include motion, velocity, accelerated velocity, etc. This provides a robust framework for simulating realistic interactions and behaviors for both drones and vehicles.\n\u2022 Observations. The observations for the embodied agents are designed to replicate the sensory inputs available to real-world agents. For drones, these include image data such as RGB images (color images from the camera), depth images (showing the distance of each pixel from the camera), and segmentation images (semantic segmentation for different objects in the scene). Sensor data like IMU data (accelerometer and gyroscope data for measuring acceleration and angular velocity), GPS data (providing global positioning coordinates), and LiDAR data (3D point cloud information of the environment) are also included. State information such as position (current coordinates x, y, z), speed (current velocity vx, vy, vz), and attitude (current orientation roll, pitch, yaw) is vital. For vehicles, the observations include similar image data and sensor data. Additionally, state information for vehicles includes the position, speed, attitude, and wheel angle (current wheel steering angle).\n\u2022 Actions. The actions of the embodied agents mimic realistic controls similar to those used by air drones and vehicles. For drones, motion control involves setting target positions (x, y, z), target velocities (vx, vy, vz), and target orientations (roll, pitch, yaw). Camera control allows for viewpoint adjustments (changing camera direction pan, tilt), and other controls include starting or stopping the drone's flight. For vehicles, driving control includes steering angle (setting the steering wheel angle), acceleration (setting throttle), braking (setting brake force), and gear shifting (switching between gears: forward, reverse, neutral). Camera control for vehicles also allows for viewpoint adjustments, and other controls include starting or stopping the vehicle's movement."}, {"title": "SDK AND ONLINE ACCESS", "content": "To make the simulator less difficult to use, we developed a Python client software development kit (SDK) and a Python proxy server based on the HTTP protocol on top of the AirSim interface. The Python proxy server is used to convert client requests into AirSim interface calls and return AirSim responses. With this Python proxy server, we shield client development from the outdated, non-standard event loop asynchronous module employed by AirSim, and allow for easier remote access using a variety of HTTP-based infrastructure. The HTTP protocol they use mainly adopts JSON format for data transfer and sends images, for which the details are available in our open-source code repository. This Python client SDK implements synchronous and asynchronous methods based on the standard async mechanism to support users writing highly concurrent programs, such as concurrent requests for large models with the simulator.\nBased on the above open transport protocol and Python client SDK, we build an online platform for users to try out. The online platform supports the simultaneous simulation and control of up to 8 agents. Users can acquire control of one or more idle agents and manipulate their movements via keyboard keys, the web GUI, or even an online Python code editor. Users can also watch a first-person view of all the agents via live video streaming. The platform is open for registration and use since we hope that the open online platform will inspire more ideas and explorations, such as collaborations on embodied agents in urban environments."}, {"title": "BENCHMARK TASKS ON EMBODIED AI IN OPEN CITY ENVIRONMENTS", "content": "Using the constructed simulator platform, we have built a dataset comprising 87.1k cases, as shown in Table 2. This dataset includes five essential embodied tasks, which cover various aspects of embodied intelligence abilities. Specifically, the intelligent agents in the open world are expected to have three kinds of human-like abilities: perception, reasoning, and decision-making. For perception, we consider the task of embodied first-view scene understanding; for reasoning, we consider the task of embodied question answering and dialogue; for decision-making, we consider the task of embodied action (visual-language navigation) and embodied task planning. For a better understanding, we present five tasks in Figure 5.\n\u2022 Embodied first-view scene understanding. The first-view scene understanding requires the agent to comprehend its surrounding environment and give an accurate description, which could consid-ered a basic ability for further tasks. In our benchmark, we observe from different perspectives at the same location, generating a set of RGB images, i.e., the input of scene understanding, and the output is the textual description for the given scene images.\n\u2022 Embodied question answering. The embodied agent can be further queried in natural language about the environment. We have designed three types of embodied question-answering tasks: distance, position, and counting, as shown in Fig. 6. Distance questions involve determining the relative distance between the agent and surrounding city elements, such as \u201cWhich is closer to me, the blue building or the red building?\u201d and \u201cApproximately how many meters away is the building ahead of me?\u201d. Position questions assess the understanding of spatial relationships in the environment, such as \u201cIs object A to the left or right of object B?\u201d and \u201cWhich street is in front, Street A or Street B?\u201d. Counting questions evaluate the agent's accurate perception of the environment, such as \u201cHow many crosswalks can be seen in a full circle?\". Therefore, the input includes both the first-view RGB images and a query about the environment. The output should be the direct textual responses to the question.\n\u2022 Embodied dialogue. Despite the task of embodied question answering, a more complex embodied task close to the reasoning ability is embodied dialogue. Specifically, embodied dialogue involves ongoing interactions where the agent engages in a back-and-forth conversation with the user. This requires maintaining context and understanding the flow of dialogue. Therefore, the input includes embodied observations and multi-round queries, and the output is the multi-round responses.\n\u2022 Embodied action (navigation). Embodied Action, often referred to as Vision-and-Language Navigation (VLN), focuses on enabling an agent to navigate an environment based on natural language instructions. The input combines visual perception and natural language instructions to guide the agent through complex environments, and the output consists of the action sequences following the language instructions.\n\u2022 Embodied task planning. Most of the time, decision-making in the real world does not have explicit instructions; otherwise, there is only an unclear task goal. Thus, it is significant for the embodied agents to be able to compose the complex and long-term task goals into several sub-tasks, which we refer to as embodied task planning. The input is the first-view observations and a given natural language described task goal, and the output should be a series of sub-tasks that the agent plans to execute.\""}, {"title": "DISCUSSIONS AND LIMITATIONS OF THE BENCHMARK", "content": "Application and promotion on EmbodiedAI. The benchmark not only serves as the pure evaluation of the large language model or LLM agents but also could be a sim2real tool that supports the pre-training or pre-testing before being deployed to the real-world city environment. For the types of agents, the benchmark does not set constraints. That is, the agent deployed could be a robot or air drone. For example, the input of a robot may only include the RGB images, and for air drones, the input can also contain the radar signals. The degree of freedom of different agents could also be different. This platform expands the boundaries of embodied functions, promotes the category of embodied intelligence, and can effectively support the further development of this field.\nHuman refinement. When constructing the benchmark, we put a lot of effort into using human refinement steps to filter out low-quality responses or revise incorrect answers provided by LLMs. It is worth noticing that the paradigm of combining large language models and human crafts has recently become widely used since large language models accurately and skillfully generate various responses (but may be totally wrong). The key challenge here is accuracy rather than diversity, and thus, the human efforts to refine the answers are quite essential and useful. On the other hand, the cost of collecting all the responses with pure human labor is not affordable. Therefore, using the large language models does not introduce a large bias.\nExtensions of benchmark. In our constructed benchmark, we consider five types of embodied tasks, scene description, embodied question answering, embodied dialogue, visual-language navigation, and embodied task planning. From a perspective of human-like critical abilities, these tasks well cover the three most significant aspects: perception, reasoning, and decision-making. The follow-up work, based on the simulation environment, promises to extend to more tasks, of which the potential tasks could be as follows. (1) Multi-agent Collaboration: Introducing tasks that require coordination and communication between multiple agents to achieve common goals. (2) Human-Agent Interaction: Creating scenarios where human users interact with agents necessitates a more sophisticated understanding of human behavior and natural language. (3) Adaptability and Learning: Implementing tasks that test an agent's ability to learn from its environment and adapt to new and unforeseen scenarios."}, {"title": "CONCLUSION AND FUTURE WORK", "content": "In this work, we take a pioneering step by building a systematic benchmark for embodied intelligence in an open city environment. The benchmark contains a 3D city simulator, easy-to-use interfaces for embodied agents, and five kinds of embodied tasks. We further evaluate the intelligence capacities of some popular multi-modal large language models, which verify the rationality of the constructed benchmark. We also plan to evaluate large language model agents' performance and embodied intelligence level in the real city environment via a Sim2Real paradigm, which can further validate the application value of the benchmark. We believe this work can help narrow the gap between existing embodied intelligence research and the ultimate goal of artificial general intelligence."}, {"title": "SUPPLEMENTARY MATERIALS", "content": "A.1 SIMULATOR\nIn our city simulator, AirSim serves as a powerful plugin to facilitate realistic simulations of drones and unmanned vehicles. These autonomous systems leverage AirSim's robust observation and action mechanisms to navigate and interact with the urban environment.\nFor drones, the observation process involves capturing high-resolution images and sensor data from multiple perspectives, including RGB, depth, and segmentation views. These observations enable the drone to perceive its surroundings accurately, identify obstacles, and navigate complex urban landscapes. The action space for drones includes vertical movements (up and down), horizontal movements (forward, backward, left, and right), and rotational adjustments (yaw, pitch, and roll). This comprehensive action space allows drones to maneuver precisely and efficiently in three-dimensional urban environments.\nSimilarly, for unmanned vehicles, observation is achieved through an array of sensors that provide comprehensive environmental data, including visual feeds and depth information. This allows the vehicle to detect road features, other vehicles, pedestrians, and potential hazards. The action space for unmanned vehicles includes steering (left and right), acceleration (forward movement), and braking (deceleration and stopping). These actions ensure that the vehicle can navigate urban streets safely and efficiently by making real-time adjustments based on its observations.\nBy integrating AirSim into our city simulator, we provide a detailed and realistic platform for testing and optimizing the performance of autonomous drones and vehicles in urban settings.\nA.2 OPEN INTERFACE\nOur city simulator feature an open API interface. This API will provide users with the ability to programmatically access and manipulate various aspects of the simulator. Through this interface, users can control camera perspectives, navigate virtual characters, retrieve environmental data, and perform other interactive tasks. The API will be designed with robust measures to ensure safe and authorized access, thereby making our simulator a versatile tool for both research and practical applications.\nA.3 BENCHMARK AND DATASET\n\u2022 Embodied first-view scene understanding. We randomly walk around the city and record the surrounding RGB observations upon reaching a location. For each case, the prompts are fixed and can therefore be designed manually. For the ground truth, we first generate embodied descriptions using the VLM. Then we manually review and correct each response, as shown in Table 8. The refinement process involves five categories of raw responses:\n1. Object Counting: The question involves counting a specified object.\n2. Object Existence: The response asserts the presence of objects, which may or may not actually exist."}]}