{"title": "CHEMAGENT: SELF-UPDATING LIBRARY IN LARGE\nLANGUAGE MODELS IMPROVES CHEMICAL REASON-\nING", "authors": ["Xiangru Tang", "Tianyu Hu", "Muyang Ye", "Yanjun Shao", "Xunjian Yin", "Siru Ouyang", "Wangchunshu Zhou", "Pan Lu", "Zhuosheng Zhang", "Yilun Zhao", "Arman Cohan", "Mark Gerstein"], "abstract": "Chemical reasoning usually involves complex, multi-step processes that demand\nprecise calculations, where even minor errors can lead to cascading failures. Fur-\nthermore, large language models (LLMs) encounter difficulties handling domain-\nspecific formulas, executing reasoning steps accurately, and integrating code ef-\nfectively when tackling chemical reasoning tasks. To address these challenges,\nwe present ChemAgent, a novel framework designed to improve the performance\nof LLMs through a dynamic, self-updating library. This library is developed by\ndecomposing chemical tasks into sub-tasks and compiling these sub-tasks into\na structured collection that can be referenced for future queries. Then, when\npresented with a new problem, ChemAgent retrieves and refines pertinent infor-\nmation from the library, which we call memory, facilitating effective task de-\ncomposition and the generation of solutions. Our method designs three types\nof memory and a library-enhanced reasoning component, enabling LLMs to im-\nprove over time through experience. Experimental results on four chemical rea-\nsoning datasets from SciBench demonstrate that ChemAgent achieves perfor-\nmance gains of up to 46% (GPT-4), significantly outperforming existing meth-\nods. Our findings suggest substantial potential for future applications, including\ntasks such as drug discovery and materials science. Our code can be found at\nhttps://github.com/gersteinlab/chemagent.", "sections": [{"title": "1 INTRODUCTION", "content": "Chemical reasoning presents unique challenges in the realm of artificial intelligence, demanding\nsophisticated reasoning and precise calculations beyond typical reasoning tasks (McQuarrie, 2008;\nAtkins et al., 2014; Talanquer, 2022; Guo et al., 2023b; Cao et al., 2024). For example, the GPT\nsolution with CoT prompting in Figure 1 contains numerous errors in both the calculation process\nand the chemical constants used. Even in short reasoning chains, a single error can cascade, reduc-\ning answer quality and escalating the probability of additional errors (Liao et al., 2024; Sun et al.,\n2024b).\nRecent advancements in large language models (LLMs) have demonstrated capabilities in simpler\nscientific tasks or chemical scenarios that do not require complex reasoning (Boiko et al., 2023;\nAtkins et al., 2023; Wang et al., 2024a; Hu & Shu, 2023; Xiao et al., 2024; Darvish et al., 2024;\nSkreta et al., 2024). However, their application to complex chemical reasoning reveals significant\nlimitations Pei et al. (2024); Li et al. (2024a). LLMs often (1) struggle to effectively utilize domain-\nspecific formulas, (2) exhibit incorrect reasoning steps, and (3) produce errors when combining\ntextual reasoning with Python code for calculations (Zhong et al., 2024b); here, syntax errors may\narise, causing the code to fail to compile. As shown in Figure 1 (StructChem output), errors often\narise from determining a constant's incorrect form or unit."}, {"title": "2 METHOD", "content": "Analogous to how students organize and reference their problem-solving approaches for exams,\nour motivation for developing a library system in ChemAgent is to enhance LLMs' ability to tackle\ncomplex problems by providing structured access to a repository of previous sub-tasks and solutions.\nThe overall reasoning framework is shown in Figure 2 (a). Formally, in a simplified form, given\na complex and open-ended chemical problem $P_s$ as input, our method aims to generate a solution\n$O_s$. The problem $P_s$, which comprises problem descriptions $T_s$ and initial conditions $C_s$, is firstly\ndecomposed into a series of sub-tasks $P_s$. The solution $O_s$ is then synthesized from the sub-\nsolutions $O_s$ of the sub-tasks. Each $O_s$ includes intermediate steps, such as formulae, reasoning\nsteps, code, and calculations. Then, a final answer $A_s$ is derived from the overall solution $O_s$. We\nevaluate the performance via the accuracy of $A_s$ against a ground truth $A_{sg}."}, {"title": "2.1 PRELIMINARIES", "content": "Analogous to how students organize and reference their problem-solving approaches for exams,\nour motivation for developing a library system in ChemAgent is to enhance LLMs' ability to tackle\ncomplex problems by providing structured access to a repository of previous sub-tasks and solutions.\nThe overall reasoning framework is shown in Figure 2 (a). Formally, in a simplified form, given\na complex and open-ended chemical problem $P_s$ as input, our method aims to generate a solution\n$O_s$. The problem $P_s$, which comprises problem descriptions $T_s$ and initial conditions $C_s$, is firstly\ndecomposed into a series of sub-tasks $P_s$. The solution $O_s$ is then synthesized from the sub-\nsolutions $O_s$ of the sub-tasks. Each $O_s$ includes intermediate steps, such as formulae, reasoning\nsteps, code, and calculations. Then, a final answer $A_s$ is derived from the overall solution $O_s$. We\nevaluate the performance via the accuracy of $A_s$ against a ground truth $A_{sg}."}, {"title": "2.2 COMPOSITION OF THE LIBRARY", "content": "We divide library into three main memory components: planning memory, execution memory, and\nknowledge memory. Figure 3 provides detailed examples. The exact context of memory and corre-\nsponding prompts are in Appendix G.\n\u2022 Planning Memory ($M_p$): This component stores high-level strategies and methodologies for\napproaching complex problems.\n\u2022 Execution Memory ($M_e$): This contains highly structured descriptions of specific problem con-\ntexts and their corresponding solutions, serving as detailed execution plans.\n\u2022 Knowledge Memory ($M_k$): This houses fundamental chemistry principles and formulas, acting\nas a ready reference. This component is generated temporarily during the solution of a specific\ntask and is not intended for permanent retention.\nThus, ChemAgent enables LLMs to tackle problems proactively, form new memories from these\nattempts, utilize existing memories to solve complex problems, and continuously refine their solu-\ntions. Built upon the library of three types of memories, ChemAgent operates in two main stages:"}, {"title": "2.3 DECOMPOSITION AS ATOMIC BLOCKS", "content": "Human problem-solvers naturally break down complex chemistry problems into smaller, manage-\nable sub-tasks (Zhou et al., 2023). This decomposition enhances the understanding of individual\ncomponents and their interactions, facilitating the resolution of the original problem in a structured\nmanner (Johnson et al., 2017). These sub-tasks not only improve the reasoning process but also\nfunction as atomic building blocks within the execution memory, each paired with a corresponding\nsub-solution. Key characteristics of this decomposition include:"}, {"title": "2.4 LIBRARY CONSTRUCTION", "content": "The Library is constructed using the development\nset. As outlined in \u00a72.3, we leverage sub-tasks de-\ncomposed from each problem as the execution mem-\nory units. Each execution memory unit $U_i$ is defined\nas follows, where $C$ represents the conditions of a\ngiven problem $P$, and $T_i$ and $O_i$ denote the sub-task\nand its corresponding sub-solution, respectively:\n$U_i = (C, T_i, O_i)$ for $i = 1,2,...,k$\nGiven a problem $P$ and its corresponding solution $S\nin the development set, our method begins by iden-\ntifying and extracting the conditions from $P$. We\nthen verify these conditions for accuracy to ensure\nthat the subsequent steps operate with precise and\ncorrectly parsed data. Based on the identified con-\nditions, we instruct the LLMs to generate detailed\nsub-tasks. For each identified sub-task $T_i$, the corresponding sub-solutions $O_i$ are parsed from $S$\nand assigned accordingly. Inspired by curriculum learning (Bengio et al., 2009), we then rank the\nmemory units $U$ based on their difficulty. In addition to ranking, we discard any memory units that\ndo not meet a predefined confidence threshold, as evaluated by the LLMs. This ensures that the\nmemory utilized for future problem-solving is both relevant and reliable, enhancing the LLM's abil-\nity to tackle increasingly complex chemistry problems. The detailed memory construction process\nis further described in Algorithm 1."}, {"title": "2.5 LIBRARY-ENHANCED REASONING", "content": "During testing, we first decompose a given problem into several sub-tasks. For each sub-task, we\nretrieve related memory units $U$ to aid in solving it. Specifically, we compute the similarity between\nthe given sub-task and units stored in the memory. Memory units with similarity above a predefined\nthreshold $\\theta$ are used to assist the model in determining the answer for the sub-task.\nFormally, let $(C_j, T_j)$ represent a sub-task decomposed from a new problem. We retrieve memory\nunits $U_t$ that satisfy\nSimilarity $(T_j, T_{u_i}) \\geq \\theta$ for $U_i \\in M_e$."}, {"title": "2.6 EVALUATE & REFINE MODULE", "content": "To enhance the flexibility and relia-\nbility of ChemAgent, we propose an\nevaluation & refinement mod-\nule, to correct the planning trajectory and\nverify the response to each sub-task.\nAs illustrated in Figure 4, after address-\ning a specific sub-task $P_i$, ChemAgent ex-\namines the sub-solution $O_i$. The system\nevaluates whether the sub-solution con-\nflicts with fundamental knowledge in $M_k$\nor contains common errors, such as incor-\nrect units. If discrepancies are identified,\na new sub-solution $O'_i$ is generated by re-\nfining the original $O_i$ based on the relevant\nknowledge in $M_k$ and the identified mis-\ntakes.\nFurthermore, if a sub-task fails due to insufficient conditions or if the evaluation determines that the\nsub-task's question does not align with the main task (e.g., calculating energy using wavelength in-\nstead of calculating wavelength using given energy), the sub-tasks from $P_i$ to $P_n$ will be restructured\nas $P'_i$ to $P'_m$, taking into account the main task and all preceding sub-tasks.\nThe evaluation & refinement module can use a different LLM than the one used for the\nbase reasoning process. For instance, if GPT-3.5 is the base model, GPT-4 can handle evaluation\n& refinement. The evaluation component judges solutions without modifying them, while the\nrefinement component adjusts solutions based on these evaluations. This separation clarifies error\nidentification, helping humans understand where and why mistakes occur."}, {"title": "2.7 SETUP", "content": "We use four chemistry datasets from SciBench (Wang et al., 2024a), and the detailed distribution of\nthe specific fields covered by each problem in the four datasets is shown in Figure 10. Each dataset\nis divided into a development set ($D_d$) and a test set ($D_t$), with exact sizes provided in Table 6."}, {"title": "2.8 RESULTS", "content": "We report the performance of all methods regarding accuracy score for each sub-dataset and an\naverage score across the entire dataset. The results are summarized in Tables 1, 4 and 5. Additional\nresults and analysis of experiments conducted on other LLMs can be found in Appendix B.\nFirstly, ChemAgent consistently outperforms the other baselines across various datasets and settings.\nSpecifically, in terms of the average score, ChemAgent improves by 9.50% (47.66 vs. 57.16) over\nStructChem, which is a 2.93 times increase and by 37% (19.48 vs. 57.16) over direct reasoning,\nwhich is a 2.93 times increase. Notably, the performance gain varies across different datasets. In\nthe CHEMMC dataset, our method exhibits the largest improvement, with a 46% increase (28.21 vs.\n74.36) compared to the direct reasoning setup. Secondly, the results also highlight the crucial role\nof memory in our library. The version of our framework equipped with memory consistently outper-\nforms the version without memory across all cases. Specifically, there is an absolute improvement"}, {"title": "2.9 SELF-EVOLUTION DURING RUNTIME", "content": "Moreover, we aim to show that library systems with\nevolving memory perform better when exposed to an\nincreasing number of problems. Much like humans\nimprove their skills through practice, these systems\nbenefit from continuous exposure to new tasks.\nWe allow ChemAgent to dynamically update and en-\nrich its library during the test stage to analyze this\nself-evolution process. Specifically, in iteration $I_i$,\nChemAgent uses all accumulated long-term mem-\nory from iterations $I_1$ to $I_{i-1}$ as its library. When\nsolving a new problem $P$, all related responses and\nknowledge from that process are added to the library\nif the solution is correct. This means that in subse-\nquent iterations, the system can leverage the newly\nacquired information (updated $M_p$ and $M_e$) to im-\nprove its performance. We employ 2-shot $M_p$ and\n4-shot $M_e$ during reasoning but simplify the setup by removing the evaluation and refinement mod-\nules. To ensure accuracy and prevent target leakage, the memory derived from problem $P_i$ in itera-\ntion $I_j$ is excluded when solving $P_i$ again in $I_k$ for any $j < k$.\nThis iterative process demonstrates that as the memory pool grows with each new example, ChemA-\ngent's problem-solving performance improves. Figure 5 shows that as the number of iterations in-\ncreases, the agent's performance gradually improves and converges to a score higher than the base-\nline (44.89%). This improvement indicates that ChemAgent can enhance its performance through a\nsimple correct-or-not evaluation of past solutions instead of human-written high-quality trajectories."}, {"title": "2.10 COST ANALYSIS", "content": "On average, each problem requires 0.012\nmillion tokens without the Evaluation &\nRefinement module, equivalent to around\n$0.09 per example. When this module is ap-\nplied, the average token consumption per prob-\nlem increases to approximately 0.023 million,\ncosting about $0.1725 per example. Note that\nthe initial library construction is not included\nin these calculations as it is a one-time setup.\nBased on the results in Figure 6, the computa-\ntional time required by our method is reason-\nable. While the cost and resource consump-\ntion are slightly higher than StructChem, the\nimprovements justify the additional expense."}, {"title": "2.11 ERROR ANALYSIS", "content": "We conduct an analysis of the trajectories for the failed examples and find three types of errors,\nshown in Figure 7."}, {"title": "3 ABLATION STUDY", "content": "To understand why our method performs particularly well and which memory component con-\ntributes the most, we conduct an ablation study by independently removing each memory com-\nponent. We test these different settings on all four sub-datasets using GPT-4 as the base LLM. The\nresults are shown in Table 2. As mentioned in \u00a72.3, the execution memory also serves as a few-shot\nprompting strategy. Therefore, when we remove $M_e$, we add two fixed human-written few-shot ex-\namples (provided with our code) into each query of each sub-task. These examples are selected from"}, {"title": "3.1 MEMORY COMPONENT ANALYSIS", "content": "To understand why our method performs particularly well and which memory component con-\ntributes the most, we conduct an ablation study by independently removing each memory com-\nponent. We test these different settings on all four sub-datasets using GPT-4 as the base LLM. The\nresults are shown in Table 2. As mentioned in \u00a72.3, the execution memory also serves as a few-shot\nprompting strategy. Therefore, when we remove $M_e$, we add two fixed human-written few-shot ex-\namples (provided with our code) into each query of each sub-task. These examples are selected from"}, {"title": "3.2 MEMORY QUALITY INFLUENCE", "content": "We investigate the impact of memory quality\nthrough a series of experiments. Specifically,\nwe compare the performance of memory gener-\nated by GPT-4 against that generated by GPT-\n3.5 on the MATTER dataset. Additionally, we\ncreate a \"hybrid memory\" by mixing memo-\nries generated by both GPT-3.5 and GPT-4 to\nobserve its performance. These experiments\nwere conducted without including the relevant\nknowledge within the LLM itself ($M_k$). As\nmentioned in \u00a72.1, unlike $M_p$ and $M_e$, $M_k$\nis not preserved in the memory pool shared with other LLMs. Including this type of memory in the\nablation study would result in the pollution of provided memory by the $M_k$ generated during use."}, {"title": "4 CONCLUSION", "content": "Our research presents a novel approach to enhancing large language models for solving complex\nchemical problems through self-exploration and memory formation. This method enables mod-\nels to construct and utilize a library, significantly improving response accuracy. Experiments us-\ning datasets and models like GPT-3.5, GPT-4, and Llama3 demonstrate substantial performance\ngains, with the ChemAgent architecture achieving up to a 36% improvement. The structured library,\nbuilt through memory decomposition into planning, execution, and knowledge, enhances problem-\nsolving capabilities, which holds promise for generalization to other domains."}, {"title": "A RELATED WORKS", "content": "Recent research has explored the self-evolution and optimization of LLMs, which is particularly\nrelevant for tackling the complexities of chemical reasoning. Yang et al. (2023) explore meth-\nods for enhancing LLM performance through self-improvement techniques, while Fernando et al.\n(2023) investigate self-referential self-improvement via prompt evolution. Additionally, Zhou et al.\n(2024), Jiang et al. (2023), Hu et al. (2024) and Qian et al. (2024) present frameworks for agent\nself-evolution, aligning with our approach of enabling self-exploration and continuous learning in\ncomplex chemical problem-solving. While some of these frameworks also incorporate a memory\nsystem, they predominantly emphasize the reuse of past workflows in daily tasks, as demonstrated\nby Wang et al. (2024b) and Qian et al. (2024)."}, {"title": "A.1 CHEMICAL REASONING", "content": "Recent advances in LLMs have shown promise in scientific reasoning, yet chemical reasoning re-\nmains particularly challenging. Benchmarks like SciBench (Wang et al., 2024a) have revealed that\ncurrent LLMs struggle significantly with complex chemical calculations and multi-step reasoning\ntasks. SciBench includes 869 college-level problems across mathematics, chemistry, and physics,\nproviding a rigorous evaluation of LLM capabilities in these domains. Other datasets (Wadden et al.,\n2024; Li et al., 2024c; Sun et al., 2024a; Feng et al., 2024; Huang et al., 2024b) have also contributed\nto advancing the evaluation of LLMs in scientific problem-solving.\nIn response to these challenges, several approaches have been proposed. StructChem (Ouyang et al.,\n2024) provides structured guidance by decomposing chemical reasoning into phases such as formula\ngeneration, detailed step-by-step reasoning, and confidence-based review. While showing improve-\nments, it still faces limitations with highly complex problems. Other researchers have explored\nenhancing LLM performance through various prompting strategies (Yang et al., 2024; Yao et al.,\n2024; Besta et al., 2024)."}, {"title": "A.2 PROBLEM DECOMPOSITION IN SCIENTIFIC REASONING", "content": "Decomposing complex problems into smaller sub-tasks has shown to enhance model understanding\nand accuracy across various domains. In the context of chemical reasoning, this approach is particu-\nlarly relevant due to the multi-step nature of many chemical problems. Patel et al. (2022) highlights\nthe efficacy of question decomposition by breaking down complex queries into manageable sub-\ntasks. Similarly, Khot et al. (2022) demonstrates the benefits of modular task decomposition. Other\nstudies (Lu et al., 2022; Wei et al., 2023) further underscore the advantages of decomposition in\ncomplex question answering and reading comprehension. Our work builds on these insights, specif-\nically focusing on how breaking down complex chemical problems can improve the performance of\nself-evolving agents in this domain."}, {"title": "A.3 SELF-EVOLUTION AND SELF-CORRECTION IN AGENT REASONING", "content": "Recent research has explored the self-evolution and optimization of LLMs, which is particularly\nrelevant for tackling the complexities of chemical reasoning. Yang et al. (2023) explore meth-\nods for enhancing LLM performance through self-improvement techniques, while Fernando et al.\n(2023) investigate self-referential self-improvement via prompt evolution. Additionally, Zhou et al.\n(2024), Jiang et al. (2023), Hu et al. (2024) and Qian et al. (2024) present frameworks for agent\nself-evolution, aligning with our approach of enabling self-exploration and continuous learning in\ncomplex chemical problem-solving. While some of these frameworks also incorporate a memory\nsystem, they predominantly emphasize the reuse of past workflows in daily tasks, as demonstrated\nby Wang et al. (2024b) and Qian et al. (2024)."}, {"title": "B EXPERIMENTAL RESULTS OF MODELS OTHER THAN GPT-4", "content": "We also evaluated ChemAgent thoroughly on earlier and less powerful models, such as GPT-3.5\n(gpt-3.5-turbo-16k). Specifically, ChemAgent achieves an absolute improvement of +0.17%\nin terms of the average score compared with the previous SOTA. Also, our framework significantly\nperforms better when equipped with a memory system (+7.13% absolute improvement), which un-\nderscores the importance of memory.\nHowever, when it comes to the evaluation and refinement module, an interesting phenomenon is\nobserved: even when outputs generated by GPT-3.5 are evaluated by more capable models like\nGPT-4, the agent often fails to correct its mistakes. This indicates that GPT-3.5 has a relatively\nweaker self-correction ability than GPT-4, explaining why evaluation and refinement provide little\nbenefit when GPT-3.5 is used. This finding aligns with previous research (Zhang et al., 2024a).\nFor open-source models, we choose Llama3-7b-Instruct, Llama3-70b-Instruct, and Qwen2.5-72b-\nInstruct as the experimental model here. The baseline direct reasoning is to directly query the\nmodel without adding other instructions. The evaluation and refinement modules are removed from\nthe ChemAgent configuration, and only the Execution Memory (Me) is added due to the model's\nrelatively lower ability on instruction following. Each experiment is repeated at least 3 times, and\nthe results are averaged. On llama3-7b, the average increase across four datasets is 8.34%. On\nllama3-70b, the average increase across four datasets is 13.04%. The experiment demonstrates that\nthe stronger the self-capabilities of large models, the more pronounced the performance gains using\nour framework."}, {"title": "C PERFORMANCE BOOST: WHERE AND WHY OUR METHOD IMPROVES", "content": "We analyze the impact of the number of memory instances used on our agent's performance. In this\nanalysis, we preserve the evaluation and refinement module since they also utilize memory. Given"}, {"title": "C.1 CALCULATION AND UNIT CONVERSION ARE MORE PRECISE", "content": "ChemAgent achieves notably higher accuracy in calculations and unit conversions. Two key fac-\ntors contribute to this: (1) Python code is demonstrated alongside each corresponding sub-task in\nmemory; (2) During development, ChemAgent adopted a strategy to save unit conversion steps in\nthe long-term plan memory pool. This allows the agent to reference correct conversion steps when\nnecessary, ensuring accurate unit conversion."}, {"title": "C.2 HIGHER MEMORY'S SIMILARITY HELPS THE SOLUTION", "content": "When solving a given problem $P$, a series of memories $[U_1, ..., U_n]$ may be invoked during the\nprocess. Let their similarity to the problem be represented as $[S_1, ..., S_n]$, and the mean similarity\nvalue is denoted as $S_{mean,p}$. In Figure 8, we visualize the distribution of $S_{mean, P}$, categorized by\nwhether $P$ is correctly solved. It is evident that problems with higher $S_{mean,p}$ are more likely to be\nsolved correctly.\nWe also conducted a Chi-Square Test of Independence to assess the relationship between a similar-\nity threshold (i.e., whether the similarity is greater than 0.805) and the correctness of the solution.\nThe Chi-Square statistic is 8.77 with a p-value of 0.003, indicating a statistically significant rela-\ntionship. Thus, future work could focus on improving the similarity between invoked memories and\nthe problem at hand to further enhance problem-solving performance."}, {"title": "D THE INFLUENCE OF THE NUMBER OF MEMORY INVOKED", "content": "We analyze the impact of the number of memory instances used on our agent's performance. In this\nanalysis, we preserve the evaluation and refinement module since they also utilize memory. Given"}, {"title": "E LIMITATION", "content": "Despite our advances, limitations include the computational intensity and time required for self-\nexploration, as well as the need for further refinement of memory mechanisms for tackling more\ncomplex problems. Future research should focus on understanding the specific mechanisms by\nwhich memory formation benefits reasoning, exploring how different types of memory contribute\nto problem-solving, and identifying optimal strategies for memory utilization. Additionally, due\nto limitations in budget and computational resources, we demonstrate our approach solely in the\ncontext of chemistry and have not conducted comprehensive research across the entire scientific\ndomain, such as considering mathematics or physics.\nHowever, we believe our proposed method has strong potential for generalization across scientific\nfields. We have made our code open-source and encourage future researchers to apply it to their\nown datasets. Specifically, adapting our method to a new domain (e.g., the CLASS sub-dataset in\nSciBench, related to Physics) involves modifying a small portion of the prompts (such as replacing\nchemistry-related sentences, e.g., \"You are a Chemistry expert\u201d) and then re-starting the Library\nConstruction phase using a development set from the new domain."}, {"title": "F TASK DOMAINS OF THE DATASETS", "content": "The Chemistry domain of SciBench has four datasets, each hand-collected from four college chem-\nistry textbooks. Quantum chemistry (quan) (Hair Jr et al., 2010) provides an exploration of equilib-\nrium, structure, and reactions. Chemistry kinetics (matter) (Atkins et al., 2014) combines physics\nand mathematics, spanning through quantum mechanics and atomic structure. Quantum mechanics\n(chemmc) (McQuarrie, 2008) covers quantum mechanics and the applications in chemical bonding.\nPhysical chemistry (atkins) (Atkins et al., 2023) provides explorations of equilibrium, structure,\nand reactions. We leverage GPT-4 to annotate each data sample in these datasets for the specific\nsubfields. Statistically, the four datasets cover 15 chemical subfields, and the division of chemical\nsubfields also helps us make associations in related fields to enrich the memory pool.\nThe problems in this dataset are challenging, with an average of 2 formulas and 5 steps of reasoning\nrequired to solve the problems in the experiment (Ouyang et al., 2024). Meanwhile, each dataset\nprovides detailed step-by-step solutions for example problems, which fit well in our framework for\nthe initial construction of memory pools, shown in Table 6."}, {"title": "G PROMPTS", "content": "Here is the list of prompts we used in our study.\nG.1 INSTRUCTIONS FOR BUILDING MEMORY POOLS.\n(1) Split prompts direct ChatGPT to decompose a given task into condition and problem parts.\nReflect prompts are used to double-check the results of the initial decomposition to confirm that the\ndecomposition is complete and correct and to return corrected results in the event of errors. See\nFigure 11."}, {"title": "G.2 INSTRUCTIONS FOR SOLVING PROBLEMS.", "content": "(1) Decomposition prompts (Figure 13) are used to decompose a given chemistry problem into 1-3\nsubtasks, each with specific goals, criticism, and milestones."}, {"title": "H SPECIFIC CASE TRAJECTORIES", "content": "To concretely demonstrate the problem-solving process of our framework, we provide a complete\nexecution trajectory of a successfully answered question to illustrate the procedural workflow of our\nframework."}]}