{"title": "MixSumm: Topic-based Data Augmentation using LLMs for Low-resource Extractive Text Summarization", "authors": ["Gaurav Sahu", "Issam H. Laradji"], "abstract": "Low-resource extractive text summarization is a vital but heavily underexplored area of research. Prior literature either focuses on abstractive text summarization or prompts a large language model (LLM) like GPT-3 directly to generate summaries. In this work, we propose MixSumm for low-resource extractive text summarization. Specifically, MixSumm prompts an open-source LLM, LLaMA-3-70b, to generate documents that mix information from multiple topics as opposed to generating documents without mixup, and then trains a summarization model on the generated dataset. We use ROUGE scores and L-Eval, a reference-free LLAMA-3-based evaluation method to measure the quality of generated summaries. We conduct extensive experiments on a challenging text summarization benchmark comprising the TweetSumm, WikiHow, and ArXiv/PubMed datasets and show that our LLM-based data augmentation framework outperforms recent prompt-based approaches for low-resource extractive summarization. Additionally, our results also demonstrate effective knowledge distillation from LLaMA-3-70b to a small BERT-based extractive summarizer.", "sections": [{"title": "Introduction", "content": "Text summarization is a crucial task in today's data-driven era, with applications ranging from news summarization to scientific paper summarization (Cohan and Goharian, 2017; Zhong et al., 2020; Goyal et al., 2022). While both extractive and abstractive methods have shown significant promise, extractive summarization remains a popular choice due to its simplicity and reliability in preserving factual accuracy (Wong et al., 2008; Kry\u015bci\u0144ski et al., 2019). However, the performance of extractive summarization systems is often constrained by the availability and diversity of training data.\nData augmentation (DA) has been widely adopted in various natural language processing (NLP) tasks like text classification, summarization, and grammatical error correction to mitigate data annotation costs, address data scarcity, and enhance model robustness (Wei and Zou, 2019; Fabbri et al., 2020; Feng et al., 2021). Traditional augmentation methods, such as synonym replacement, sentence shuffling, and back-translation are effective to some extent, but they saturate very quickly as they cannot fully capture the semantic nuances of the text. However, the recent surge in the development of large language models (LLMs) like GPT-4 (Achiam et al., 2023), LLaMA-3 (Touvron et al., 2023), and Claude-3 (Anthropic, 2024), has given birth to a new paradigm of LLM-based data augmentation (Dai et al., 2023; Ding et al., 2024). Prompt-guided LLMs can generate diverse and contextually rich textual augmentations and recent works demonstrate the potential of LLM-based DA for various data-scarce NLP tasks such as dialog modelling (Chintagunta et al., 2021; Wan et al., 2022) and text classification (Yoo et al., 2021; Sahu et al., 2022). However, LLM-based DA methods have been underexplored for text summarization, especially for low-resource summarization setups.\nIn this work, we explore a challenging few-shot extractive text summarization setup. Specifically, we explore the 50-shot setting, where we can access only 50 document-summary pairs. We propose MixSumm, a prompt-based data augmentation strategy that first instructs an LLM to synthesize diverse documents that cover topical information from multiple documents, and then generates extractive summaries for the augmentations. To evaluate the effectiveness of our proposed framework, we conduct extensive experiments on the TweetSumm (Feigenblat et al., 2021), the WikiHow (Koupaee and Wang, 2018), and the ArXiv/PubMed (Cohan et al., 2018) text summarization datasets. We use the open-source LLaMA-3-70b-Instruct LLM for our tasks instead of a closed-source LLM like the GPT family of LLMs. For evaluation, we use the standard ROUGE scores (Lin, 2004) as well as L-Eval, an open-source version of the promising LLM-based evaluator for text summarization, G-Eval (Liu et al., 2023b).\nOur experiments demonstrate that MixSumm outperforms strong data augmentation and semi-supervised baselines for low-resource summarization setups and we show a knowledge distillation effect, where the knowledge of LLaMA-3-70b model is distilled into the downstream summarization model using BERTbase backend.\nWe summarize the contributions of our work as follows: 1) we propose MixSumm, a novel prompt-based data augmentation framework for the challenging low-resource setup of 50-shot extractive text summarization, 2) we show how we can effectively use an open-source LLM, LLaMA-3-70b for summarization instead of having to resort to expensive closed-source LLMs like GPT-4, 3) we conduct extensive experiments to show that our DA-based framework outperforms strong LLM-based DA and SSL existing methods and baselines, and 4) we utilize an effective knowledge distillation method from LLaMA-3 with 70B parameters to a BERT-based summarization model with 110M parameters where we retain much of the performance with much less memory requirements."}, {"title": "Related Work", "content": "LLM-based Text Summarization. Fabbri et al. (2020) use round-trip back-translation to improve BART's abstractive summarization performance. On the other hand, Dou et al. (2021) propose GSum, a fully supervised transformer-based architecture that can use a guidance signal from an external source for improved abstractive text summarization. Goyal et al. (2022) employ zero-shot prompting on GPT-3 for open-ended news summarization and show that humans overwhelmingly prefer GPT-3 summaries over human summaries. Pu and Demberg (2023) use prompting on GPT-3 for controllable text summarization and show that while GPT-3 can follow simple constraints in the prompt like length, it shows a noticeably lower degree of change in styles compared to human-written summaries. Liu et al. (2024a) and Zhang et al. (2024) benchmark the zero-shot performance of LLMs on instruction-controlled summarization and news summarization. Chintagunta et al. (2021) use GPT-3 as a data annotator for 210-shot medical dialog summarization and show significant gains equivalent to using 6400 human-written labels. More recently, Liu et al. (2024b) fine-tune BART on LLM-generated summaries instead of human-generated summaries and show that LLMs are excellent references. Notably, these works focus on abstractive text summarization and prompt GPT-3 directly for summarization in their experiments. Except the last two works, none of them use LLMs as data generators in low-resource setups. Additionally, they all use a closed-source model for their experiments.\nLLM-based Distillation and Data Augmentation in NLP. A large body of recent work uses LLMs as data generators for distilling a large teacher model's knowledge into smaller models for training instruction-tuned models and chain-of-thought reasoning, while reducing human annotation load (Ho et al., 2023; Shum et al., 2023; Meng et al., 2023; Liu et al., 2023a; Peng et al., 2023). Bonifacio et al. (2022) use few-shot prompting to construct training datasets with query-document pairs for information retrieval. In the landscape of few-shot text classification, Yoo et al. (2021) propose GPT3Mix and Sahu et al. (2022, 2023) propose PromptMix, where both the methods use LLMs as data generators and data labelers. We are inspired by the success of LLM-based DA for these diverse NLP tasks and adopt the best prompting practices based on these works. For instance, we generate diverse examples by mixing examples from different classes or groups as in GPT3Mix and PromptMix; and specify concrete criteria when using LLMs for generation and evaluation as in Pu and Demberg (2023) and Liu et al. (2024b). Furthermore, we conduct extensive experiments to test the capabilities of an open-source LLM, LLaMA-3-70b, for low-resource extractive text summarization, instead of using closed-source LLM like GPT-3 and GPT-4 for open-ended abstrative summarization. Finally, we also test our LLM-based DA strategy on datasets with extremely long documents."}, {"title": "Notations", "content": "We denote an annotated, many-shot extractive summarization dataset with N datapoints as $D = \\{(d_i, s_i)\\}_{i=1}^N$, where $(d_i, s_i)$ denotes the i-th datapoint with input text document $d_i$ and its ground truth extractive summary $s_i$. We refer to the training, validation, and testing parts of the dataset as $D_{\\text{train}}$, $D_{\\text{val}}$, and $D_{\\text{test}}$, respectively. Given the many-shot training set $D_{\\text{train}}$, we construct a few-shot version of the dataset with k examples $D_{F,\\text{train}}$ as follows:\nStep 1. Given $D_{F,\\text{train}}$, we group the training articles by topics. We do not define the topics explicitly and identify T groups by applying the k-means algorithm on the document embeddings (where $k = T$). We use the SBERT encodings (Reimers and Gurevych, 2019) of the input documents as document embeddings. If an input document exceeds SBERT's context window length of 512 tokens (roughly 300-400 English words), we chunk the document into smaller pieces and then average the chunk embeddings.\nStep 2. We construct our k-shot dataset $D_{F,\\text{train}}$ by randomly sampling an equal number of datapoints from each of the T clusters so that $D_{F,\\text{train}}$ has k examples in total.\nIn Section 6, we empirically show that our principled approach for constructing few-shot datasets is better than randomly sampling k examples from $D_{\\text{train}}$ as it provides better topical coverage.\nProblem Formulation. Given a text summarization dataset $D$: 1) use data augmentation on $D_{F,\\text{train}}$ to synthesize a labeled dataset $D_{A,\\text{train}}$, and 2) train an extractive text summarization model on the combined dataset $D_{F+A,\\text{train}}$."}, {"title": "Methodology", "content": "This section describes the proposed two-step approach, MixSumm, for synthesizing labeled summarization examples. We first instruct an LLM to generate documents such that it covers multiple topics and then generate extractive summaries for those documents. The following sections describe our two-step procedure in detail."}, {"title": "Step 1: Generating New Documents for Summarization", "content": "First, for every dataset, we manually write a short description that describes the type and approximate size of articles in the dataset. These descriptions enable the usage of our approach in even zero-shot settings. Next, we construct T pairs of clusters $(C_i, C_j) \\forall i,j \\in \\{1,...,T\\}, i \\neq j$, such that $c_j$ is the most distant cluster from $c_i$. We use the centroids of the clusters obtained during k-means clustering in Section 3 for our computation. We also ensure that all cluster pairs are unique as $(c_i, C_j) = (C_j, C_i)$. Finally, we combine the dataset description with k examples from each cluster and instruct the LLM to generate new examples that cover topics from both clusters. Specifically, we instruct the LLM to generate examples that contain $\\alpha\\%$ topics from the first cluster $c_i$ and $(1 - \\alpha)\\%$ topics from the second cluster $C_j$, where $\\alpha$ is sampled from a uniform distribution between 1-100. This is similar to applying the mixup algorithm (Zhang et al., 2018) in a natural language space and has proven highly effective for data augmentation in low-resource text classification setups (Yoo et al., 2021; Sahu et al., 2023). Prompt 1 in Appendix C shows the complete template for this step."}, {"title": "Step 2: Generating Extractive Summaries for the Documents", "content": "We now ask the LLM to generate extractive summaries for documents generated in the previous step. Specifically, we provide a generated document to the LLM and then instruct it to output a probability score for each sentence indicating whether that sentence should be included in the summary or not. We then rank the lines by the scores and choose the top-p lines, where p is the summary size and depends on the dataset. We truncate the input document if it exceeds the LLM's context window length. This approach ensures the extractiveness of the generated summary labels as it mimics PreSumm (Liu and Lapata, 2019), a strong baseline for extractive text summarization.\nWe separate the summarization step from the generation step due to two main reasons: 1) simplify the task of the LLM in one prompt, and 2) avoid exceeding the context window length of the generative LLM used."}, {"title": "Experimental Setup", "content": "We use three popular datasets in this work for extractive text summarization.\n1) TweetSumm (Feigenblat et al., 2021) is a real-world customer service dataset that has 1100 conversations between a customer and an agent, and each conversation has three human-annotated extractive summaries. The training set has 858 dialogs, and the validation and test sets have 100 examples each.\n2) WikiHow (Koupaee and Wang, 2018) contains WikiHow articles with their headlines as abstractive summaries. The dataset has over 180k articles, with around 168k training articles and 6000 test and validation articles.\n3) ArXiv/PubMed (Cohan et al., 2018) is a collection of scientific articles from PubMed and ArXiv with their abstracts as summaries. The dataset has ~325k articles, with nearly 300k training articles and 12.5k test and validation articles."}, {"title": "Implementation Details", "content": "Data Augmentation. We set the number of groups $T = 10$ for all datasets and randomly sample 5 examples from each group to get a 50-shot $D_{F,\\text{train}}$. Then, we obtain $D_{A,\\text{train}}$ by generating 1000 examples using the procedure described in Section 4. In the data generation prompt, we include five examples for each group for TweetSumm and WikiHow, but for ArXiv/PubMed, we could only fit two documents at a time in LLaMA-3's context window after applying the following truncation heuristic to the text. We include $l$ lines before and after each sentence in the ground truth summary such that we are able to fit two examples in the prompt. The average value of $l$ was 5.21 (so approximately ~90 for an average summary size of 8 sentences for the ArXiv/PubMed dataset sentences were selected for example). Here, we set the summary size $p$ to 4 sentences for TweetSumm and WikiHow datasets, and 8 sentences for the Arxiv/PubMed dataset. We determine these summary sizes based on the average summary size in the few-shot training data $D_{F,\\text{train}}$. We host LLAMA-3-70b-Instruct on 4\\times A100 GPUs with 80G VRAM each and use it as the backbone LLM for all our experiments. Generating $D_{A,\\text{train}}$ took ~4.2 hrs for TweetSumm, ~11.3 hrs for WikiHow, and ~1.4 days for ArXiv/PubMed dataset.\nTraining. We train a PreSumm extractive summarization model on the combined MixSumm-generated and seed few-shot dataset $D_{F+A,\\text{train}}$. We use the TransformerSum repository to implement our training pipeline. To handle long documents that cannot be fed to the PreSumm at once, we introduce a subroutine that iteratively chunks and summarizes the document until we obtain a summary of size $p$. The iterative subroutine is crucial to train PreSumm models on the WikiHow and ArXiv/PubMed datasets with long input documents.\nWe initialize the training process with a learning rate of 2\\times 10-5 and use a cyclic learning rate scheduler (Smith, 2015). We train all our models for 100 epochs with an early stopping criterion, where we stop the training process if the validation ROUGE-2 score does not improve for more than 10 epochs. We use the AdamW optimizer (Loshchilov and Hutter, 2017) with $\\epsilon = 1 \\times 10^{-8}$, $\\beta_1 = 0.9$, $\\beta_2 = 0.99$ and train all our models on one V100 GPU with 12G VRAM. We experiment with two backbones for the PreSumm model: DistilBERTbase and BERTbase. Training a model on MixSumm-generated data took ~2.5 hrs for TweetSumm, ~13.4 hrs, for WikiHow, and ~2.7 days for ArXiv/PubMed. Crucially, we repeat each experiment (data augmentation+model training) for 5 random seeds and report the mean and standard deviations for all models unless otherwise stated.\nEvaluation. We evaluate the summary quality of the models using the following metrics:\nROUGE Scores. We use ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-L (R-L) F1 scores (Lin, 2004) for evaluation, where R-1 and R-2 measure the unigram and bigram overlap between the predicted and the ground truth summaries, respectively, while R-L also considers the order of n-grams. We use the pyrouge Python package to compute ROUGE scores in our setup and report them in Table 2.\nL-Eval Scores. In addition to ROUGE, we use an LLM-based evaluation metric for our task. Specifically, we use LLaMA-Eval (L-Eval), an open-source variant of the G-Eval metric (Liu et al., 2023b), where we prompt LLAMA-3-70b-Instruct instead of a GPT model. We use L-Eval as it better aligns with human preferences for text summarization, compared to ROUGE scores and other model-based evaluation metrics, such as BERTScore and BARTScore (Zhang et al., 2019; Yuan et al., 2021). It is also not biased towards LLM-generated content; however, since LLM-inference speed is low for long documents, we did not compute L-Eval scores during training and only computed them during final testing. When computing L-Eval scores, we provide the LLM with the input article and a (generated) summary and instruct it to score the summary on a scale of 1-10 (see Prompt 2 in Appendix C for the full L-Eval prompt template). Given a test article A and a summary s, we compute the L-Eval score as follows:\n$L-\\text{Eval}(A, s) = \\sum_{r=1}^{10} p_r \\cdot r,$\nwhere $p_r$ is the probability of generating the rating $r$."}, {"title": "Baselines", "content": "We run the following baseline experiments: 1) MixSumm (Ours). We synthesize the augmented training dataset using the proposed MixSumm approach described in Section 4 then train an extractive summarization model on $D_{F+A,\\text{train}}$. To determine the effect of mixup and augmentation, we also run two variants of this baseline: MixSumm w/o Mixup where we remove the mixup-specific instructions from the data generation prompt and MixSumm w/o Aug. where we do not perform any data augmentation and train the summarization model only on $D_{F,\\text{train}}$. 2) Easy Data Augmentation (EDA). We use an edit-based data augmentation technique (Wei and Zou, 2019) to construct $D_{A,\\text{train}}$ instead of using MixSumm. Specifically, we apply the EDA technique to each sentence in an article to construct a new example. 3) MixSumm (rand.). The few-shot dataset is constructed by randomly selecting k examples from the full training set, instead of selecting examples from T clusters. We also run a variant of this model: MixSumm (rand.) w/o Aug. where we do not perform any data augmentation. 4) Teacher Student Learning (TSL). A semi-supervised setup proposed by Zhuang et al. (2023), where we have access to a few-shot labeled set $D_{F,\\text{train}}$ and many unlabeled examples. TSL employs a teacher-student learning framework where a PreSumm model is trained on just $D_{F,\\text{train}}$, which is then used to pseudolabel the unlabeled set and select the top pseudolabels based on PreSumm's confidence. We report the performance of the TSL (50:500) model, which uses 50 labeled and 500 unlabeled examples; and the TSL (500:500) model, which uses 500 labeled and 500 unlabeled examples. 5) Prompt-based Pseudolabeling for Semi-supervised Learning (PPSL). A semi-supervised approach using teacher confidence and prompt-based pseudolabel scoring for extractive text summarization (Sahu et al., 2024). We report results for the PPSL (50:250) setting that uses LLAMA-3-70b-Instruct. 6) LLaMA-3-70b (k-shot.) An in-context learning-based approach where we prompt LLAMA-3-70b-Instruct with k examples randomly selected from $D_{F,\\text{train}}$ and then instruct it to summarize a test article. We use the same prompt as the one we use for summarizing articles (Prompt 3 in Appendix C), except we remove the group information and directly populate it with k examples. 7) Oracle. We train a fully supervised PreSumm model on the complete training set $D_{\\text{train}}$ to gauge the upper-bound performance for this task."}, {"title": "Results", "content": "We now discuss the quantitative results in Table 2. First, removing data augmentation significantly drops ROUGE and L-Eval scores for all datasets (\"w/o Aug.\" rows v/s EDA v/s MixSumm). This suggests that data augmentation is a crucial component in the pipeline.\nSubsequent sections present a detailed discussion of our findings, but we summarize the ranking of models based on our overall findings as follows: LLaMA-3-70b \\approx MixSumm > PPSL > MixSumm w/o mixup > TSL >> EDA > MixSumm w/o aug."}, {"title": "Effect of Clustering Documents (MixSumm v/s MixSumm (rand.))", "content": "We perform a student's T-Test comparing results from MixSumm and MixSumm (rand.). The test revealed that while ROUGE scores for MixSumm are generally higher than MixSumm (rand.), the differences are not significant. The only exception was R-2 scores on TweetSumm when using BERTbase, where MixSumm outperforms MixSumm (rand.) by 2.1 points (R-2 of 52.7 v/s 50.6). On the other hand, the difference in L-Eval scores for the two methods was found significant by the T-test for all the datasets. Notably, MixSumm and MixSumm (rand.) methods achieve L-Eval scores of 65.3 v/s 60.3 on TweetSumm, 81.1 v/s 72.5 on WikiHow and 53.1 v/s 48.4 on ArXiv/PubMed. This suggests that ROUGE scores might not be able to capture the semantic correctness of the generated summaries and highlights the importance of an LLM-based evaluator that can discern between nuanced semantics in natural language text. We observe a similar trend after removing the augmentation component from both methods (MixSumm w/o Aug. v/s MixSumm (rand.) w/o Aug.).\nOverall, we conclude that MixSumm is better than MixSumm (rand.) and we should include diverse examples if possible in the prompt as it leads to direct improvements in LLM's generation quality. Even when we cannot obtain a diverse seed set, MixSumm (rand.) is a highly-performant method and is highly competitive with MixSumm, often achieving the second-best performance amongst all the baselines."}, {"title": "Comparison with Other DA Techniques", "content": "First, MixSumm approach significantly outperforms EDA on all the datasets (for both BERT and DistilBERT backbones). Notably, when using BERT as PreSumm's backbone, MixSumm and EDA achieve R-2 scores of 52.7 and 39.2 on TweetSumm, 27.3 and 23.4 on WikiHow, and 10.7 and 7.9 on ArXiv/PubMed. This demonstrates the generation capability of LLMs compared to a simple edit-based DA technique like EDA.\nNext, we compare the proposed MixSumm approach with MixSumm w/o Mixup, a strong LLM-based data augmentation baseline that does not use Mixup. Removing the mixup component significantly lowers ROUGE and L-Eval scores across the board. Specifically, MixSumm and MixSumm w/o Mixup achieve R-2 scores of 52.7 and 47.3 on TweetSumm, 7.8 and 6.2 on WikiHow, and 18.3 and 16.8 on ArXiv/PubMed. There is a significant difference in L-Eval scores as well, with MixSumm outperforming MixSumm w/o Mixup by 8.0 points on TweetSumm (65.3 v/s 57.3), 13.8 points on WikiHow (81.1 v/s 67.3), and 0.9 points on ArXiv/PubMed (53.1 v/s 52.3). All differences were determined to be significant based on a T-Test. The marginal gain in L-Eval on the ArXiv/PubMed dataset might hint towards a limitation of the LLM in understanding contexts in very long documents.\nWe also show qualitative examples generated by EDA, MixSumm w/o mixup and MixSumm in Table 3. In the context of Table 3, we note that w/o mixup, MixSumm generates decent quality documents but it only covers a single topic (phone/electronic device-related sentences.) MixSumm, on the other hand, generated an example that contains mention of terms from two topics (flight as well as a device-related issue.) EDA generates the lowest-quality documents with grammatical errors and other artifacts. However, we note that regardless of the quality of the original document, LLaMA-3-70b generates a high-quality summary in all cases."}, {"title": "Comparison with Semi-Supervised Methods (MixSumm v/s TSL v/s PPSL)", "content": "We compare our data-augmentation-based method with a classical semi-supervised method (TSL) and recently emerging prompt-based semi-supervised methods (PPSL) for extractive text summarization.\nFirst, we note that our 50-shot MixSumm and MixSumm (rand.) methods outperform TSL (50:500), which uses 50 labeled examples. Next, our two methods outperform TSL (500:500) on all the metrics except the R-1 score. MixSumm, MixSumm (rand.), and TSL (500:500) achieve a mean R-1 score of 59.1, 58.6, and 59.0, respectively; however, we should also note that the difference in R-1 scores for the two MixSumm models on TweetSumm is not significant, as determined through the T-Test earlier. Overall, MixSumm is better than TSL for extractive summarization in extreme data-scarce settings.\nMixSumm achieves slightly higher ROUGE scores and significantly higher L-Eval scores than PPSL (50:250), a strong prompt-based method employing 50 labeled and 250 unlabeled examples. Specifically, MixSumm and PPSL (50:250) achieve 59.1 v/s 58.4 R-1 scores, 52.7 v/s 50.1 R-2 scores, 60.5 v/s 59.1 R-L scores, and 65.3 v/s 56.3 L-Eval scores on the TweetSumm dataset (when using BERT). Other datasets follow a similar trend. These results show the augmented data leads to better generalization on unseen data. Overall, we conclude that prompt-based data augmentation might be better for data-scarce setups as it achieves better performance while being a more straightforward approach to adopt in practice than a semi-supervised method."}, {"title": "Comparison with LLaMA-3 (BERTbase v/s LLaMA-3-70b Summarizer)", "content": "We include the 0-shot, 1-shot, and 5-shot LLaMA-3 results in Table 2. We note that 0-shot LLaMA-3 outperforms 50-shot MixSumm w/o Aug baseline on the TweetSumm dataset in terms of ROUGE scores and L-Eval scores, and it achieves competitive results on ArXiv/PubMed (R-L score of 15.4 v/s 12.7 and R-1 score of 23.6 v/s 24.1 for 0-shot LLaMA-3 v/s MixSumm w/o Aug.).\nIncreasing the number of examples leads to an expected improvement in performance except L-Eval scores on the ArXiv/PubMed dataset, where 0-shot and 1-shot LLaMA-3 models achieve 38.4 and 38.3 L-Eval scores. This further suggests that LLaMA-3 may struggle with understanding very long documents.\nLastly, we compare the best performing LLaMA-3 summarizer with MixSumm. While LLaMA-3 outperforms MixSumm with DistilBERT, MixSumm with BERT achieves competitive performance against LLaMA-3 as a summarizer. We perform a T-Test to determine the significance of the performance difference and find that the difference in the R-1, R-2, and L-Eval scores is significant for TweetSumm; differences in the R-1, R-L, and L-Eval scores are significant for WikiHow; and differences in the R-2, R-L, and L-Eval scores are significant for ArXiv/PubMed. Based on the results of the T-test, and the scores in Table 2, we conclude that while LLaMA-3 models are slightly better than MixSumm with BERT in terms of ROUGE scores, MixSumm with BERT generally achieves better L-Eval scores.\nOverall, we conclude tgat MixSumm with BERT is highly competitive against the LLaMA-3-70b model, demonstrating a distillation effect from LLaMA-3-70b to a BERT-based PreSumm model."}, {"title": "Conclusion", "content": "In this work, we propose MixSumm, a novel prompt-based data augmentation framework for the challenging low-resource setup of 50-shot extractive text summarization. Our experiments show MixSumm is better than recent prompt-based approaches for low-resource extractive summarization. We also show that we can transfer the knowledge of a large teacher model like MixSumm into much smaller BERT-based models by generating documents covering diverse topics using the teacher model and then training the smaller model on that data. LLM-based data augmentation is underexplored for extractive summarization, and, through this work, we hope to spark an interest in the research community to address various challenges of this task."}, {"title": "Limitations", "content": "We use LLaMA-3-70b-Instruct for our experiments, which has a context window size of 8192 tokens, so it is not possible to fit many long documents in the model's context (like articles in the ArXiv/PubMed dataset). We can explore using position interpolation (PI) to increase the context window length of LLaMA (Chen et al., 2023).\nCurrently, we only consider text summarization for the English language. Moving forward, we can expand our method to multiple languages. More research on efficiently handling long documents during the training process is also needed, as we currently rely on a chunk-and-summarize subroutine to train PreSumm, which results in significant delays in document processing. We can consider using alternative transformer architectures such as LongFormer (Beltagy et al., 2020) as PreSumm's backend."}, {"title": "Ethics Statement", "content": "We generate large textual datasets using LLMs, and even though we use an instruction-tuned model, we need to be careful about any bias it might exhibit, or any potentially harmful content that it might generate. Language model debiasing is a common potential solution to address this issue (Meade et al., 2021; Guo et al., 2022). Additionally, we suggest involving a human moderator if these systems are to be made public-facing."}]}