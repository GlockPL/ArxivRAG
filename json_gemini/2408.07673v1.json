{"title": "Deep Learning: a Heuristic Three-stage Mechanism for Grid Searches to Optimize the Future Risk Prediction of Breast Cancer Metastasis Using EHR-based Clinical Data", "authors": ["Xia Jiang", "Yijun Zhou", "Chuhan Xu", "Adam Brufsky", "Alan Wells"], "abstract": "A grid search, at the cost of training and testing a large number of models, is an effective way to optimize the prediction performance of deep learning models. A challenging task concerning grid search is the time management. Without a good time management scheme, a grid search can easily be set off as a \u201cmission\u201d that will not finish in our lifetime. In this study, we introduce a heuristic three-stage mechanism for managing the running time of low-budget grid searches with deep learning, and the sweet-spot grid search (SSGS) and randomized grid search (RGS) strategies for improving model prediction performance, in an application of predicting the 5-year, 10-year, and 15-year risk of breast cancer metastasis.\nWe develop deep feedforward neural network (DFNN) models and optimize the prediction performance of these models through grid searches. We conduct eight cycles of grid searches in three stages, focusing on learning a reasonable range of values for each of the adjustable hyperparameters in Stage 1, learning the sweet-spot values of the set of hyperparameters and estimating the unit grid search time in Stage 2, and conducting multiple cycles of timed grid searches to refine model prediction performance with SSGS and RGS in Stage 3. We conduct various SHAP analyses to explain the prediction, including a unique type of SHAP analyses to interpret the contributions of the DFNN-model hyperparameters.\nThe grid searches we conducted improved the risk prediction of 5-year, 10-year, and 15-year breast cancer metastasis by 18.6%, 16.3%, and 17.3% respectively, over the average performance of all corresponding models we trained.\nGrid search can greatly improve model prediction. Our result analyses not only demonstrate best model performance but also characterize grid searches from various aspects such as their capabilities of discovering decent models and the unit grid search time. The three-stage mechanism worked effectively. It not only made our low-budget grid searches feasible and manageable, but also helped improve the model prediction performance of the DFNN models. Our SHAP analyses not only identified clinical risk factors important for the prediction of future risk of breast cancer metastasis, but also DFNN-model hyperparameters important to the prediction of performance scores.", "sections": [{"title": "INTRODUCTION", "content": "Electronic health records (EHR) systems have been used in clinical settings for many years. With the help of these systems, researchers or medical practitioners who work in a clinical environment often have opportunities to access and curate a clinical dataset concerning a group of patients, and sometimes they may consider using such a dataset to build a patient outcome prediction model. For instance, one can use the EHR data collected from a breast cancer patient care center to develop deep learning models that can be used to predict for a patient the risk of future occurrences of breast cancer metastasis.\nBreast cancer is a major cancer related cause of death for women worldwide. Based on the report updated on March 13, 2024 by the World Health Organization (WHO), \u201cBreast cancer was also the most common cancer in women in 157 countries out of 185 in 2022\", and it is also one of the main causes of cancer related death in women worldwide, and \u201ccaused 670,000 deaths globally in 2022\u201d. Again according to the WHO, as of the end of 2020, \u201cthere were 7.8 million women alive who were diagnosed with breast cancer in the past 5 years, making it the world's most prevalent cancer\u201d. Women do not die of breast cancer, rather, they die mainly due to breast cancer metastasis, which can occur years after the initial treatment of breast cancer [1,2]. Predicting a late metastatic occurrence of breast cancer for a patient is important, because the prediction can help making more suitable treatment plan for the patient, which may help prevent breast cancer metastasis. Improving our capability of predicting breast cancer metastasis is an important task in breast cancer patient care. Even just a small percentage of improvement can help greatly improve patient quality of life, and save lives and care related costs.\nThe field of machine learning (ML) and deep learning [3\u20138] has provided us with various AI-based computational methods for conducting predictions. Using these ML methods we can learn a prediction model automatically from a dataset. However, a prediction model that is developed in such a manner does not always predict well [9]. There are often multiple factors that can affect model performance. For example, the model performance is usually dataset dependent, that is, the same machine learning method can perform totally differently when it is applied to different datasets [4,9,10]. This phenomenon is perhaps partly because different datasets contain different levels of \u201csignals\u201d that are critical in making correct predictions. When a dataset contains very weak signals, even an advanced method can fail learning a good prediction model. By \u201csignals\" we mean the information usable for making predictions, contained in data. A good example of a signal is what so-called a correlation between two variables. If two variables are correlated, then one of them can be used to predict the other, and in that case, the former is often called the predictor and the latter is often called the outcome. Sometimes, to curate a dataset that contains sufficient information for learning a good prediction model, we need to collect a lot of data. Generally speaking, the more datapoints (cases) a dataset contains, the more likely it provides sufficient information for learning a good prediction model. That perhaps explains in an aspect why the applications of ML methods are often associated with the term \u201cbig data\u201d and a field called data science.\nOther than the dataset itself, another important factor for model performance is the value used for an adjustable hyperparameter of a prediction model. All machine learning methods that we used so far have adjustable hyperparameters, but a difference is some methods have more and some have less [4,11-14]. During the early years of using machine learning methods to carry on real-life prediction tasks, we paid little attention to the selection of a value for an adjustable hyperparameter that is built into a machine learning method [9,10,15]. A normal practice is to use the default value recommended by the developers of the machine learning method or by a machine learning textbook, or at most try a few values that are close to the default value. This may have contributed significantly to the fact that some of the ML method such as the first generation of the Neural Network were reportedly having poor prediction performance [9,10,16]."}, {"title": null, "content": "Deep learning is a machine learning method that has quite some adjustable hyperparameters [17\u201319]. For example, we identified 13 adjustable hyperparameters (see Table 1 and a more detailed description of these hyperparameters in Table 3) for the Deep Feedforward Neural Network (DFNN) models that we developed for predicting later occurrences of breast cancer metastasis [4,19]. In a previous study, by conducting machine learning experiments, we found that different value assignments of the adjustable hyperparameters can lead to models with significantly different prediction performance [4]. We used a method called grid search [4,20,21]to systematically train and test different DFNN models by changing the values of the set of adjustable hyperparameters [4]. Therefore, in order to do a grid search we normally preselect a range of ranges for each of the hyperparameters as an input to the grid search.\nWe now use an example to explain what a a grid grid search does and in the meantime introduce a major challenge of conducting a grid search. In this example, we use the number of values given to each of the 13 hyperparameters as shown in the second row of Table 1 below. The number of hidden layers and the number of hidden nodes (neurons) in a hidden layer are the two structural hyperparameters that together determines the structure of a DFNN model. In this example grid search, the number of hidden layers has four different values, that is, the model can contain up to four hidden layers. The number of hidden nodes per hidden layer has 22 different values. Therefore, there could be 22, 484, 10648, or 234,256 different model structures when the number of hidden layer is configured to be 1, 2, 3, or 4. Thus, we can make in total 245,410 different models by considering the two structural hyperparameters alone. The total number of possible unique value assignments to the set of the 11 remaining non-structural hyperparameters are the product of the number of values given to each of these hyperparameters, which arrives at 1.7424e9. Therefore, considering all 13 hyperparameters, there are in total 4.276e14 unique value assignments. What the example grid search does is to train and test 4.276e14 DFNN models determined by the 4.276e14 different value assignments, one at a time. We call a unique value assignment to the set of hyperparameters of a grid search a hyperparameter setting. Note that under each of the hyperparameter settings, there would be k different models trained and tested if the k-fold cross validation (CV) procedure (see the Methods section) is applied. Since we use a 5-fold CV procedure in our grid searches, the number of models trained and tested is five times the number of hyperparameters settings used in these grid searches.\nA grid search can be very costly! Based on the grade search experiments we did in our previous study [4], the average running time per hyperparameter setting is 117 seconds for a particular dataset we used. By using this average unit running time, the estimated total running time for the example grid search, as described above, would be 1,586,424,369 years. Apparently this grid search is not feasible for us unless we use billions of computers to run it parallelly. A grid search is in general very time consuming, but computation time can sometimes be resolved by using high-speed computing, which can be bought by money. Therefore, the feasibility issue of a grid search essentially boils down to a financial budget issue. Going back to the scenario that we mentioned in the beginning of this introduction, in which medical researchers or practitioners want to learn a prediction model using their own EHR-based datasets, a normal situation is that the time and funds available are both very limited for conducting a grid search to optimize prediction. We"}, {"title": null, "content": "call a grid search for which the money allocation is very limited a low-budget grid search. It is not uncommon to encounter a low-budget grid search in the real-life applications of machine learning methods.\nMoreover, the example grid search we described above can only be called a small-scale grid search, in which only a small number of values is given to each of the hyperparameters. But the estimated running time is already hard to manage. Based on this example, we see that the number of values allowed for each of the hyperparameters can only be very small for a low-budget grid search to finish in a foreseeable time. But note that some of the hyperparameters of our DFNN models can take a very large number of different values (see Table 3). For example, based on our previous studies [4,19], the range of values we consider for a hyperparameter called epochs is from 5 to 2000, which means for each grid search we need to select a very small set of values from the 1996 different values for epochs. As a matter of fact, most of the 13 hyperparameters can take a very large number of values, and some can even take an infinite number of different values within a normal range. It is often a challenging task to select a very small set of values from a large number of values available for each of the hyperparameters like epochs, and in the meantime to ensure the feasibility of a low-budget grid and meet the goal of digging out a better prediction model through the grid search. To our knowledge, there is no standard and good way of doing this! In this study, we have limited time to run grid searches for optimizing our DFNN models that predict the risk of breast cancer metastasis. We therefore introduce what we call a three-stage heuristic grid-search mechanism that we use to manage this challenging task. We describe this mechanism and the experiments we conduct, in which we apply this mechanism, in the Methods section below, and present and analyze the results of the experiments in the Results and Discussion sections."}, {"title": "METHODS", "content": "The DFNN Models\nDeep learning and deep feedforward neural network (DFNN): An Artificial Neural Network (ANN) is a machine learning framework, which is designed to recognize patterns using a model loosely resembling the human brain [22,23]. ANNs can be used for clustering (unsupervised) on unlabeled data or classification (supervised) on labeled data [4]. Deep Neural Networks (DNNs), called deep learning, refers to the use of neural networks composed of more than one hidden layers [3,5-7,24]. The DNN has obtained significant success in commercial applications such as voice and pattern recognition, computer vision, and image-based processing [25-35]. However, its power has not been fully explored or demonstrated in applications that are not image-based, such as the prediction of breast cancer metastasis using non-image clinical data. This is due in part to the sheer magnitude of the number of variables involved in these problems, which presents formidable computational and modeling challenges [17,18,36]. We developed the deep feedforward neural network (DFNN) models that predict the risk of a future"}, {"title": null, "content": "occurrence of breast cancer metastasis for a patient [4,19] and conduct grid searches to hand these challenges. The DFNN models we developed are fully connected neural networks that do not contain cycles. Figure 1 illustrates, as an example, the structure and the inner connections of the DFNNs that we have developed. The example showed in Figure 1 is a six-layer neural network that contains one input layer, four hidden layers, and one output layer. The 31 input nodes to this neural network represent the 31 clinical features contained in the patient data that we use, and the output layer contains two nodes representing the binary status of 5-year, 10-year, or 15-year breast cancer metastasis. Each node in this model has an activation function (see Table 3), represented by $f(x)$, which decides the node's individual output value established by the current value of the node. In such a DFNN model, each hidden layer has a certain number of hidden nodes that can be different from the other layers. Both the number of hidden layers and the number of hidden nodes in a hidden layer are two of the set of hyperparameters whose values are subjected to changes during grid searches. These two hyperparameters together with other adjustable hyperparameters for our grid searchers are described in Table 3 below.\nDatasets\nThe MBIL method is a Bayesian Network-based method for identifying risk factors (RFs) for an outcome feature, which was applied to three EHR-based clinical datasets concerning breast cancer metastasis, that is, the LSM-5Year, 10Year, and 15year datasets [37]. In this study, we use MBIL to retrieve all RFs concerning breast cancer metastasis from the LSM datasets and develop three new datasets according to the RFs: the LSM_RF-5Year, 10Year, and 15Year datasets. Using the LSM_RF-5Year dataset as an example, the 2-step procedure for developing this dataset is as follows: Step 1: Applying the MBIL method to the LSM-5Year dataset to retrieve the RFs of 5- year breast cancer metastasis. The original LSM-5Year dataset contains 32 features including a feature called metastasis, which represents the state of having or not having breast cancer metastasis by the 5th year post the initial treatment [37]. In this study metastasis is the outcome feature, which we also call the target feature, because we are interested in predicting the value of this features using the other features; The remaining 31 features are called predictors. Step 2: Removing all predictors that don't belong to the set of RFs found in Step 1 from the LSM-5Year dataset, and all data points of the remaining features form the LSM_RF-5Year dataset. We also follow this 2-step procedure to obtain the LSM_RF-10Year dataset from the original LSM-10Year dataset and the LSM_RF-15Year dataset from the LSM-15Year dataset. All experiments conducted in this study are based on the RF datasets. Table 2 below shows the counts of the cases and predictors included in the three RF datasets. A detailed description of the predictors is included in the Tables S1-S3 of the supplement."}, {"title": "The three-stage grid search mechanism and the Experiments", "content": "In this study, we tune 13 hyperparameters (Table 3) following a heuristic three-stage grid search mechanism for learning the DFNN models. In Stage 1, we focus on learning performance trend and a set of proper values for each of the hyperparameters that can take a very large or an infinite number of different values, such as the epochs and learning rate. The extreme values that can result in performance outliers, that is, the models that perform very poorly, are usually removed from the set of proper values. Stage 2 and stage 3 grid searches are guided by the set of proper values. In Stage 2, we attempt to estimate the average running time per hyperparameter setting (RTPS) and identify performance sweet spots by conducting a grid search that is guided by the results of the Stage 1 grid searches. A main difference between the Stage 1 and the Stage 2 grid searches is that in a Stage 1 grid search we only change the values of a single hyperparameter, and this allows the Stage 1 grid search to train and test models using a large number of different values of the hyperparameter and still finish in an acceptable timeframe. But in a Stage 2 grid search or a Stage 3 grid search, we allow all hyperparameters to take multiple different values. The Stage 2 experiments are important because based on the results of such a grid search, we can do better in managing the rur running time of the su subsequent Stage 3 grid searches. Specifically, we can estimate the total running time based on the RTPS if we know the input number of hyperparameter settings, and on the other hand, we can computer the input number of hyperparameter settings based the total running time allowed. This ensures that a grid search finishes within an acceptable timeframe, and therefore is critical to a low-budget and time-sensitive grid search. In Stage 3, we conduct multiple cycles of grid searches to further refining mode prediction performance. A cycle means the entire process of running a grid search from preselecting a set of values for each hyperparameter till finishing all experiments concerning model training and testing scheduled for the grid search. In this study, we conduct 6-cycles of Stage 3 grid searches for each of the three datasets, labeled as Stage3-c1, c2, and up to c6, each respectively. The hyperparameter settings of the stage3-c1 are determined based on the results of the Stage 2 grid searches, that is, when conducting hyperparameter value selection, we focus on a small number of values that are close to the value used by the best model obtained from the Stage 2 grid searches. We call such a grid search a sweet spot grid search (SSGS), in which the sweet spot is determined by a previous grid search. Stage 3 cycle 1 through 5 are all SSGSs. In Stage 3, we also use our out of the local optimal (OLO) strategy for preselecting the input hyperparameter values. Specifically, in Stage3-c4 and Stage3-c5, we identify a new sweet spot that is outside the \u201cneighborhood\u201d of the previous sweet spot to get out of the potential local optimal formed during the SSGSs. Finally, in Stage3-c6, we apply both OLO and a strategy called the randomized grid search (RGS) that we created. Once the proper set of values for each of the hyperparameters is determined via Stage 1, we will be able to determine all possible unique hyperparameter settings, which we call the pool of hyperparameter settings (PHS). A RGS is a grid search that trains and tests models at a set of hyperparameters settings that are randomly picked from a PHS. Table 3 below contains more detailed information about the13 hyperparameters of our DFNN models. The specific range of values used in each cycle of our three-stage grid searches are shown in Table S4 to S6 of the supplement."}, {"title": "Prediction performance metrics and the 5-fold cross validation process", "content": "Our grid searches follow the 5-fold cross validation (CV) mechanism to train and test models at each hyperparameter setting, and use an AUC score to measure the prediction performance of a model. AUC stands for the area under the curve of a receiver operator characteristic (ROC) curve that plots the true positive rate against the false positive rate for all possible cutoff values [38] . An AUC score measures the discrimination performance of the model.\nTo conduct a 5-fold CV, we need to split a dataset prior to grid searches. We use the following procedure to split our datasets: 1) split the entire dataset into a train-test set that contains 80% of the cases and a validation set that contains 20% of the cases. The train-test set will be given to a grid search as the input dataset, and the validation set will be kept aside for a later validation test. 2) divide a train-test set evenly into 5 portions for conducting the 5-fold CV. The division is mostly done randomly except that each portion should have approximately 20% of the positive cases and 20% of the negative cases to ensure that it is a representative fraction of the dataset. We conduct a 5-fold CV at each hyperparameter setting of a grid search. During a 5-fold CV, 5 different models are generated and tested, each is trained using a unique combination of 4 portions and tested with the remaining portion. 5 AUC scores are produced based on the tests, and the average of these scores is called the mean_test_AUC. The mean_test_AUC metric is used by grid searches to measure the model discrimination performance. A top hyperparameter setting selection at the end of a grid search is also based on this metric. A top model is developed by refitting the entire train- test set using the top hyperparameter setting selected by a grid search."}, {"title": "The SHAP values and plots", "content": "We use the SHAP (Shapley Additive Explanations) values to explain the prediction results of our DFNN models and identify the important features for the predicted future risk of breast cancer metastasis. The SHAP values are established based on the Shapley values, which distribute the payoff by measuring the marginal contribution of individual team members to the outcome of a cooperation game [39,40].\nWe use the Kernel Explainer provided by the SHAP package [41] to compute the SHAP values for the DFNN models. In order to manage computation time while preserving the integrity of the information contained in data, we process the training dataset using the k-means clustering method. We therefore identify k representative cluster centroids, where k is equal to the number of features in the dataset. These centroids, which epitomize the typical characteristics of the training dataset, are then employed as the background data for the SHAP Kernel Explainer. The background value of a feature is the mean of the corresponding feature values of the k centroids, identified using the k-means clustering.\nThe formula used to compute SHAP values is as follows:\n$\\Phi_{i}(p)=\\sum_{S \\subseteq F\\backslash{i}} \\frac{|S|!(|F|-|S|-1)!}{|F|!}[p(S \\cup {i}) - p(S)]$\nIn this formula, F denotes the complete set of features, i represents the ith feature, and S represents a subset of F not including the ith feature, that is, S \u2286 F\\{i}. |S| is the size of a subset S, and |F| is the size of the set F. Let p(x) represents a model's prediction outcome, then p(S \u222a {i}) represents the model's prediction output when using the features in a subset S together with the ith feature as the predictors, while p(S) denotes the model's prediction output when using only the features in S as the predictors. The term p(S\u222a{i}) \u2013 p(S) therefore reflects the contribution to the model's prediction output made by the ith feature with respect to a subset S.\nInspired by LIME [42], the Kernel Explainer generates synthetic samples, which are used as the test cases for a model of interest to compute the SHAP value of a feature. Let's call the feature the ith feature. Each of the synthetic samples contains the real values of a subset S, taken from the 20% set-aside validation dataset (see Methods), and the background values for the remaining features in F, excluding the ith feature. The ith feature takes its real value for the synthetic sample that is created for obtaining the p(S \u222a {i}), and takes its corresponding background value in the synthetic sample that is created to obtain the p(S). Recall that the background values for all features in F are generated using the k-means method as described above.\nWe also use the SHAP package to generate a SHAP bar plot which ranks the feature importance values of all predictors from high to low. A SHAP feature importance value is the mean absolute SHAP value of all the test cases for the feature of interest. Another type of SHAP plots we show are the so called SHAP summary plots, which not only rank the features by their feature importance values, but also show the SHAP value of each individual case that is tested. The SHAP heatmap plots show patterns of groups of instances or features, and the dependence plots explore potential interactive features which jointly affect a SHAP value. ."}, {"title": "RESULTS", "content": "We followed the three-stage mechanism, described in the Methods section, and ran grid search experiments in eight cycles: 1 cycle for Stage 1, 1 cycle for stage 2, and 6 cycles for stage 3, named as Stage 3-c1, Stage 3-c2, and so on and so forth. For each cycle, we identified groups of top performing models, that is, the top 1, top 5, top 10, top 50, and top 100 models, out of all models that were trained. We then computed the average mean_test_AUC for each group as a measurement of group prediction performance. We also obtained the average mean_test_AUC for all models trained during a cycle of grid searches as the largest group. We compare the group prediction performance side by side of the eight cycles of grid searches for predicting 5-year breast cancer metastasis in Table 4, 10-year in Table 5, and 15-year in Table 6 below."}, {"title": "DISCUSSION", "content": "Based on Table 4, for predicting 5-year breast cancer metastasis, the best performing model comes from Stage3-c6, with a mean_test_AUC that is higher than the average mean_test_AUC of all models trained in this cycle by 18.6%. Since we applied the RGS strategy in the Stage3-c6 grid searches, the average mean_test_AUC of all models trained in this cycle reflects the expected model performance when a hyperparameter value is randomly selected from a set of proper values. Table 5 shows that for predicting 10-year breast cancer metastasis, the best performing model comes also from stage 3 but is in cycle 3, with a mean_test_AUC that is higher than the average mean_test_AUC of all models trained in Stage3-c6 by 16.3%. For predicting 15-year breast cancer metastasis, the best performing model is found in stage3-c1, with a mean_test_AUC that is higher than the average mean_test_AUC of all models trained in Stage3-c6 by 17.3%."}, {"title": null, "content": "Table 4-6 also show that the three-stage grid search mechanism overall works as expected due o the following reasons: Firstly, all best models are found in the Stage 3 grid searches. This is consistent with our goal of further refining grid searches in Stage 3 by building upon the preparation work from the Stage 1 and Stage 2 grid searches; Secondly, we conducted SSGSs in Stage3-c1 through Stage3-c3 grid searches, focusing on refining prediction perform based on the \u201csweet spot\u201d derived from the results of its previous cycle. As expected, both Table 4 and 5 show a steady increase in terms of both the best and group-average model performance across these three cycles of grid searches; And thirdly, we used the RGS strategy in Stage3-c6, in which we allowed a hyperparameter to have a chance of taking any value in its proper set of values, determined based on Stage 1 grid searches. We reckon that such a grid search may accidently identify a best model because it has more \u201cfreedom\u201d in choosing its hyperparameter value combinations, but its group model performance may not be as good because the pure randomness in selecting hyperparameter values can also result in very bad models. This indeed is reflected in our results as shown in Table 4 through Table 6.\nBy Table 4-6 we not only compare the prediction performance of the best models, but also compare group model performance among the five groups including top 5, 10, 50, 100, and \u201cAll\u201d. The group performance comparisons reflect somewhat the characteristics of the grid searches of the different stages, in which different strategies were applied. As seen in Table 4 through Table 6, Stage 1 grid searches tend to have a low \u201cAll\u201d model performance. This is perhaps because in Stage 1 we ran grid searches that focused on one hyperparameter at a time, and therefore we were able to test a broad range of hyperparameter values. The extreme values used for a hyperparameter could lead to very bad models, which help drag down the average score of all models. Stage 2 overall does better than Stage 1 in group model performance, this is perhaps because the hyperparameter values used in stage 2 were selected from the set of proper values resulted from Stage 1. Table 4-6 also show that Stage 3 Cycle 1 through Cycle 5 tend to do better than both Stage 1 and Stage 2, and the explanation mostly lies in the fact that we conducted SSGSs in these cycles, in which the hyperparameter values used are \u201cin the neighborhood\u201d of some best values found in previous grid searches. The results seem to suggest that, in terms of model performance, if one model does well, then it's neighbors tend to do well also. In addition, Cycle 4 and 5 tend to do worse than Cycle 1 through 3 perhaps due to the OLO strategy. Our results also show that Stage 3 Cycle 6 grid searches are competitive in terms of group model performance except for the \u201cAll\u201d model group. This indicates that the RGS strategy is competitive in terms of identifying top performing models, but the average performance of all models is brought down by performance outliers resulted from the pure randomness of hyperparameter value selection.\nWe trained hundreds of thousands of models at each of the eight cycles (see Table 8, and recall that 5 models were trained at each hyperparameter setting due to the 5-fold CV). Out of such a large number of models, how many of them are decent prediction models? An answer to this question may help further characterize the performance of grid searches. Table 7 shows the summary data that we derived from the output of our grid searches to answer this question. We described what we meant by a decent model in the Result section. Based on the CHS/TNS ratios in Table 7, Stage 3 Cycle 1 through Cycle 5 grid searches, that is, the so called SSGSs, did the best in terms of searching for a decent prediction model. The models trained by each of these five cycles are 100% or close to 100% decent, and this is true regardless which dataset was used. We also notice that the Stage 3 Cycle 6 grid searches produced the worst results in terms of identifying"}, {"title": null, "content": "decent models, perhaps also due to the randomness of the hyperparameter value selection, required by the RGS strategy.\nAs described in the Introduction, time management has always been a critical issue in grid searches, especially in low-budget ones. Based on Table 8, Stage 1 and the SSGSs in Stage 3, especially the Cycle 1 through Cycle 4 tend to link to a relatively low RTPS, while the Stage 3 Cycle 6 for which we used the RGS strategy and Stage 2 tend to give a relatively high RTPS. By consulting Table S4 through S6, we found that the values of a hyperparameter called the number of hidden nodes in a hidden layer seem to positively correlate with the RTPS, because the value changes of this hyperparameter can well explain the changes of RTPS. For example, the highest RTPS that we see comes from Stage 3 Cycle 6, which can be explained by the range of high values (550-800) of this hyperparameter used in this cycle; Another example, we notice that Stage 1 allows this hyperparameter to take the highest value 1005, but only has the second highest RTPS. This can be explained by the broadest range of values (1 to 1005) that this hyperparameter takes in Stage 1, which should give an average value that is lower than the values used in Stage 3 Cycle 6. From Table 8, we also notice that within a same cycle of grid searches, where a same search strategy was used, the RTPSs for the three different datasets are also significantly different. The RTPS is the highest for the 5-year dataset, and lowest for the 15-year dataset. Based on Table 2, a significant difference among the three datasets is the size of a dataset, that is, the number of cases (also called data points in machine learning) contained in a dataset. The 5-year dataset contains the largest number of cases, while the 15-year contains the lowest number. Table 8 and Table 2 together reveal a positive correlation between the number of data points contained in a dataset and the RTPS of a grid search.\nBased on Figure 2-4, the important features in the best models are quite consistent among the eight cycles. A variable called LYP (lymph_nodes_positive) is found to be the most import feature for predicting 5-year breast cancer metastasis in all eight best models shown in Figure 2. Besides, Figure 2 also demonstrates that LYP is far more important than the other features, and this is consistently demonstrated by the SHAP importance plots of almost all the best models. According to our knowledge about cancer, it seems to make sense that the number of positive lymph nodes is a good indicator for breast cancer metastasis at diagnosis, and therefore a good indicator for the risk of cancer recurrence in the near future. Based on Figure 2, the top three important features for predicting 5-year breast cancer metastasis also include ER (estrogen_receptor_expression) and STA (stage), with ER to be the second most important feature voted by five out of the eight best models, and STA to be the third most important feature voted by six out of the eight best models.\nAccording to Figure 3, the top three most important features for predicting the risk of 10- year breast cancer include STA, identified as the most important in five out of the eight best models, GRA (grade), identified as the most important by three out of the eight best models, and AGE (age_at_diagnosis), identified as the number three important by five out of the eight best models. Unlike the situation where one feature mostly dominates as seen in the 5-year DFNN models, we see that more than one feature dominate in most of the importance plots. Based on Figure 4, the top three important features for predicting the risk of 15-year breast cancer metastasis are AGE, LYS (lymph_nodes_status) and MEN (menopausal_status). We also notice that in seven out of the"}, {"title": null, "content": "eight best models, these three top features together dominate the overall predicted risk of breast cancer metastasis.\nAlthough the feature importance figures (Figure 2-4) identify the top feature(s) that dominate(s) the overall importance of the predicted results, they don't show the relationships among the features in terms of their impact. The SHAP heatmaps as shown in Figure 5 perhaps do better in this regard. For example, Figure 5(e), a SHAP heatmap plot, reveals color patterns in terms of both features and cases, which indicate similar influences of the grouped members on the model's prediction. However, the heatmap plots don't show whether the features interact to have a joint effect on prediction.\nAn interactive"}]}