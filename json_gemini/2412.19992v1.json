{"title": "An Ordinary Differential Equation Sampler with Stochastic Start for Diffusion Bridge Models", "authors": ["Yuang Wang", "Pengfei Jin", "Li Zhang", "Quanzheng Li", "Zhiqiang Chen", "Dufan Wu"], "abstract": "Diffusion bridge models have demonstrated promising performance in conditional image generation tasks, such as image restoration and translation, by initializing the generative process from corrupted images instead of pure Gaussian noise. However, existing diffusion bridge models often rely on Stochastic Differential Equation (SDE) samplers, which result in slower inference speed compared to diffusion models that employ high-order Ordinary Differential Equation (ODE) solvers for acceleration. To mitigate this gap, we propose a high-order ODE sampler with a stochastic start for diffusion bridge models. To overcome the singular behavior of the probability flow ODE (PF-ODE) at the beginning of the reverse process, a posterior sampling approach was introduced at the first reverse step. The sampling was designed to ensure a smooth transition from corrupted images to the generative trajectory while reducing discretization errors. Following this stochastic start, Heun's second-order solver is applied to solve the PF-ODE, achieving high perceptual quality with significantly reduced neural function evaluations (NFEs). Our method is fully compatible with pretrained diffusion bridge models and requires no additional training. Extensive experiments on image restoration and translation tasks, including super-resolution, JPEG restoration, Edges-to-Handbags, and DIODE-Outdoor, demonstrated that our sampler outperforms state-of-the-art methods in both visual quality and Frechet Inception Distance (FID).", "sections": [{"title": "I. INTRODUCTION", "content": "CORE-based diffusion models [1]\u2013[4], grounded in stochastic theory, map Gaussian noise to the data distribution via learned score functions, and have achieved the state-of-the-art performance in image generation tasks. Compared to Generative Adversarial Networks (GANs) [5], [6], diffusion models provide superior perceptual quality and greater stability when sampling from complex data distributions. However, initializing the generative process from pure Gaussian noise can be suboptimal for conditional generation tasks like image restoration and translation, as corrupted images contain significantly more structural information than random noise. Diffusion bridge models address this limitation by starting the generative process from corrupted images, leveraging their structural similarity to clean images. Recent advances, such as the Image-to-Image Schr\u00f6dinger Bridge (I2SB) [7] and the Denoising Diffusion Bridge Model (DDBM) [8], have demonstrated substantial performance improvements over traditional diffusion models in these applications.\nDespite these advancements, existing diffusion bridge models, including Inversion by Direct Iteration (InDI) [9], I2SB, and Consistent Direct Diffusion Bridge (CDDB) [10], predominantly rely on Stochastic Differential Equation (SDE) samplers. This reliance results in slower inference speeds compared to diffusion models that adopt high-order Probability Flow Ordinary Differential Equation (PF-ODE) solvers for acceleration [3], [11], [12]. Notably, while DDBM highlights that pure ODE samplers often produce blurry images in diffusion bridge models, it did not identify the underlying cause. To address this issue, DDBM employs a high-order hybrid strategy alternating between ODE and SDE samplers. It still requires over 100 neural function evaluations (NFEs) to achieve satisfactory results.\nIn this work, we identify that the limited performance of pure ODE samplers in diffusion bridge models arises from the singular behavior of the PF-ODE at the start of the generative process. Leveraging this insight, we propose an ODE Sampler with a Stochastic Start (ODES3) for diffusion bridge models. Our approach employs posterior sampling to transition corrupted images into intermediate representations where the PF-ODE becomes well-defined. Subsequently, we apply the second-order Heun solver [13] to the PF-ODE, achieving high perceptual quality with reduced NFEs. Our sampler is fully compatible with pretrained diffusion bridge models and requires no additional training. Its effectiveness is validated on image restoration and translation tasks using pretrained models from I2SB and DDBM. The proposed sampler outperforms the original samplers used in I2SB and DDBM, as well as other state-of-the-art methods, in terms of both Frechet Inception Distance (FID) [14] and visual quality."}, {"title": "II. RELATED WORK", "content": "In this section, we review key acceleration strategies in diffusion models and recent advancements in diffusion bridge models."}, {"title": "A. Acceleration for Diffusion Models", "content": "Accelerating the inference process of diffusion models has become a critical area of research. DDIM [15] was the first to address this challenge by transforming the generative process of DDPM [16] from a Markovian to a non-Markovian framework, establishing its connection to the PF-ODE. ScoreSDE [1] further bridged DDPM and score-based models, showing that the reverse process can be described using either the reverse SDE or the PF-ODE. Acceleration strategies based on the PF-ODE can be broadly classified into two categories."}, {"title": "B. Diffusion Bridge Models", "content": "Diffusion bridge models have emerged as a compelling alternative to conditional diffusion models [2], [20], [21] for conditional image generation. Despite their diverse origins, models such as InDI [9], I2SB [7], and DDBM [8] can be unified under a shared framework [10], [22]. These models employ Doob's h-transform to adjust the forward process, ensuring it terminates at corrupted images. This adjustment allows the reverse process to initialize from the structurally informative corrupted images instead of Gaussian noise.\nSDE-based samplers are prevalent in existing diffusion bridge models, including InDI, I2SB, CDDB [10], and IR-SDE [23]. To accelerate the generative process, several PF-ODE-based techniques adapted from standard diffusion models have been applied. For instance, DDBM proposed a hybrid approach alternating between SDE and ODE samplers, utilizing the second-order Heun solver for the ODE step. I\u00b3SB [24] and DBIM [25] adopted strategies from DDIM, transitioning the generative process from a Markovian to a non-Markovian framework. Additionally, Consistency Diffusion Bridge Model (CDBM) [22] applied consistency distillation from Consistency Models, achieving competitive results with only two generative steps.\nDespite these advancements, the application of high-order ODE solvers to diffusion bridge models remains largely unexplored. To bridge this gap, we propose a novel method that integrates posterior sampling as a stochastic start and leverages the second-order Heun solver to solve the PF-ODE in diffusion bridge models. This approach demonstrates notable effectiveness and efficiency in image restoration and translation tasks."}, {"title": "III. METHOD", "content": ""}, {"title": "A. Preliminaries on Diffusion Bridge Model", "content": "Diffusion models establish a mapping between the data distribution $q_{data}$ and Gaussian noise by defining a continuous diffusion process $X_t \\in \\mathbb{R}^d$ indexed by a continuous time variable $t \\in [0, T]$. This process can be modeled as the solution to the following forward SDE [1]:\n$$dX_t = f(t)X_tdt + g(t)dw,$$\nwhere $X_0 \\sim q_{data}(X_0)$, $f(t)X_t$ represents the linear drift term, $g(t)$ controls the diffusion rate and $w$ is the Wiener process.\nDiffusion bridge models are designed for image restoration and translation tasks, where a corrupted image $y$ is provided, and the goal is to sample from the conditional data distribution $q_{data}(X_0|y)$. CDBM [22] offers a unified framework that encompasses prominent diffusion bridge models, such as DDBM and I2SB, despite originating from different perspectives. These models employ Doob's h-transform to adjust the dynamics of the diffusion process, resulting in the following forward SDE:\n$$dX_t = [f(t)X_t + g^2(t)\\nabla_{X_t}log p_{T|t}(y|X_t)]dt + g(t)dw,$$\nwhere $X_0 \\sim q_{data}(X_0|y)$, and $p_{T|t}$ denotes the transition kernel from time $t$ to $T$ as defined by the original SDE (1). The term $\\nabla_{X_t} log p_{T|t}$ is expressed as:\n$$\\nabla_{X_t}log p_{T|t}(y|X_t) = \\frac{(\\frac{a_t}{a_T})y - X_t}{\\alpha_{\\xi}(\\rho_{\\xi} - \\rho_{\\xi})},$$\nwith parameters defined as:\n$$\\alpha_t = exp(\\int_0^t f(\\tau) d\\tau), \\rho_{\\xi} = \\int_0^t g^2(\\tau) d\\tau,$$\nWe use $q_{t|0,y}(X_t|X_0, y)$ to denote the transition kernel from time 0 to t, and $q_{t|y}(X_t|y)$ to represent the marginal distribution of $X_t$, both determined by the forward SDE (2) and conditioned on the corrupted image $y$. The forward SDE (2) ensures that the diffusion process almost surely converges to the fixed endpoint y, i.e.,\n$$q_{T|y}(X_T|y) = \\delta(X_T - y),$$\nwhere $\\delta$ represents the Dirac function. Additionally, the transition kernel $q_{t|0,y}(X_t|X_0, y)$ has a closed form expression, allowing efficient sampling in forward process:\n$$q_{t|0,y}(X_t|X_0, y) = \\mathcal{N}(X_t; a_ty + b_tX_0, c_t^2I),$$\nwhere the parameters are given by:\n$$a_t = \\frac{\\rho_{\\xi}a_T}{\\rho_{\\xi}a_T},$$ \n$$b_t = (1 - \\frac{\\rho_{\\xi}}{\\rho_{\\xi}}),$$\n$$c_t^2 = \\alpha_{\\xi}\\rho_{\\xi}(1 - \\frac{\\rho_{\\xi}}{\\rho_{\\xi}}).$$"}, {"title": "B. ODE Sampler with Stochastic Start", "content": "High-order ODE samplers cannot be directly applied at the start of the reverse process in diffusion bridge models. While the reverse SDE (8) remains well-defined at time T, the PF-ODE (9) exhibits singular behavior at T due to the singularity in its non-linear drift term, as detailed in Theorem 1 and 2.\nTheorem 1. At t = T, the non-linear drift term in the reverse SDE (8) is well defined. Specifically,\n$$lim_{t\\rightarrow T} [\\nabla_{X_t}log q_{t|y}(X_t|y) - \\nabla_{X_t} log p_{T|t}(y|X_t)] = \\frac{1}{\\alpha_{\\xi} \\rho_{\\xi}} (\\mu - a_Ty),$$\nwhere the expected mean $X_0^{(T)}$ is defined as:\n$$X_0^{(T)} = \\int X_0 q_{data}(X_0|y) dX_0.$$\nTheorem 2. At t \u2192 T, the non-linear drift term in the PF-ODE (9) becomes singular. Specifically, $lim_{t\\rightarrow T} [\\nabla_{X_t}log q_{t|y}(X_t|y) - \\nabla_{X_t} log p_{T|t}(y|X_t)]$ does not exist.\nBased on Theorem 2, transitioning from time T to $\\tau$ ($\\tau < T$), where the PF-ODE (9) becomes well defined, is essential for utilizing high order ODE solvers in diffusion bridge models. While the reverse SDE (8) is well-defined at time T, discretizing it in a single Euler-Maruyama step for this transition can lead to significant discretization errors, particularly when the step size $T - \\tau$ is large. To address this issue, we employ a posterior sampling approach, generating $X_\\tau$ from the distribution $q_{post}(X_\\tau|y)$ defined as:\n$$q_{post}(X_\\tau|y) = q_{\\tau|0,y}(X_\\tau|X_0^{(T)}, y),$$\nwhere the expected mean $X_0^{(T)}$ is obtained from the trained data predictor $D_{\\theta^*}(X_T, y, T)$ as $X_0^{(T)} = D_{\\theta^*}(X_T, y, T)$, with $X_T = y$. The distribution $q_{\\tau|0,y}(X_\\tau|X_0, y)$ is defined by equation (6), with t substituted by $\\tau$ and $X_0$ approximated by $X_0^{(T)}$. Compared to the distribution $q_{EM}(X_\\tau|y)$ obtained by directly discretizing reverse SDE (8) in a single Euler-Maruyama step, $q_{post}(X_\\tau|y)$ aligns more closely with the true distribution $q_{\\tau|0,y}(X_\\tau|X_0, y)$ in terms of KL-Divergence."}, {"title": "C. Implementation Details", "content": "We validated our proposed sampler on both image restoration and translation tasks. For image restoration, we utilized the pretrained models from I2SB [7] and adhered to its time schedule during the generative process. Experiments were conducted on two types of degradations: 4\u00d7 super-resolution with bicubic interpolation (sr4x-bicubic) and JPEG restoration with a quality factor of 10 (JPEG-10). Evaluations were performed on 10,000 randomly selected images from the validation dataset of ImageNet 256\u00d7256 [26].\nFor image translation, we employed the pretrained Variance Preserving (VP) diffusion bridge models from DDBM [8], following its time schedule during the generative process. We tested our method on two translation tasks: Edges Handbags [6] at a resolution of 64\u00d764 and DIODE-Outdoor [27] at a resolution of 256x256. Evaluations were conducted on the entire training set for both Edges Handbags and DIODE-Outdoor tasks, consistent with previous works [8], [25].\nThe number of generative steps N was set to 20 (NFE=38) for image restoration tasks and 15 (NFE=28) for image translation tasks."}, {"title": "IV. RESULTS", "content": ""}, {"title": "A. Image Restoration Tasks", "content": "In image restoration tasks, we compared our proposed sampler with the SDE-based sampler from I2SB [7], using the same pretrained I2SB models. We also included comparisons with the conditional diffusion model from ADM [2], and several diffusion-based plug-and-play models, including DDNM [28], DDRM [29], [32], IGDM [30], and DPS [31].\nFor quantitative evaluation, we calculated FID [14] to assess perceptual quality and included Classifier Accuracy (CA) using a pretrained ResNet-50 [36], following previous work [7]. The results are presented in TABLE I for the sr4x-bicubic task and in TABLE II for the JPEG-10 task. Baseline values were computed using the official implementations of these methods with default hyperparameters. All experiments were conducted on a single A100 GPU, and the computation time for all tested methods is included in the tables. Representative results are visualized in Fig. 2 for the sr4x-bicubic task and Fig. 3 for the JPEG-10 task.\nIn the sr4x-bicubic task, our method, ODES3, achieved the best FID among all tested approaches. Compared to the 100-step I2SB, our sampler reduced FID by 9% and offered a 2.7\u00d7 acceleration in computation time while maintaining comparable CA. When compared to ADM, DDNM, DDRM, and DPS, our method demonstrated superior results, with a 6 to 16-point decrease in FID and a 0.04 to 0.08 increase in CA. Although IGDM achieved the highest CA in this task, it required a known forward operator to incorporate data consistency during inference, exhibited a worse FID, and was 9.5x slower than our method. The superior performance of our sampler is further confirmed by the visualization results in Fig. 2, where our method shows enhanced detail restoration compared to all comparison methods, particularly in regions such as the eyes of birds and humans, as well as in text characters.\nIn the JPEG-10 task, our method achieved the best FID among all tested approaches. Compared to the 100-step I2SB, our sampler reduced FID by 18% and achieved a 2.7x acceleration in computation time while maintaining comparable CA. When compared to DDRM and IGDM, our method demonstrated superior performance, with a 3 to 17-point reduction in FID and a 0.01 to 0.09 improvement in CA. The exceptional performance of our sampler is further supported by the visualization results in Fig. 3, where our method exhibits enhanced detail restoration compared to all comparison methods, particularly in regions such as the eyes and tails of birds, as well as the numbers on the clock."}, {"title": "B. Image Translation Tasks", "content": "In image translation tasks, we compared our sampler with the hybrid high-order sampler in DDBM [8] and the non-Markovian sampler in DBIM [25], using the same pretrained DDBM (VP) models. Following previous work [8], we also included comparisons with Pix2Pix [6], DDIB [33], SDEdit [34], Rectified Flow [35], and I2SB [7]. We used FID [14] to evaluate perceptual quality and also reported Inception Scores (IS) [37], Learned Perceptual Image Patch Similarity (LPIPS) [38], and Mean Squared Error (MSE) for all tested methods. The results are summarized in TABLE III, with baseline results directly taken from DDBM [8] and DBIM [25]. Representative results for DDBM and our method are visualized in Fig. 4 for the Edges Handbags (64\u00d764) task and in Fig. 5 for the DIODE-Outdoor (256x256) task.\nOur method achieved the best FID among all tested approaches in both image translation tasks. Compared to DDBM, our method exhibited slightly lower IS but delivered superior performance in terms of FID, LPIPS, and MSE. Notably, our sampler reduced FID by 70% in the Edges Handbags task and 48% in the DIODE-Outdoor task, with a 4.2x acceleration in NFE. Visualization results in Fig. 4 and 5 reveal that DDBM tends to generate artificial artifacts, such as checkerboard patterns on bags and tree branches floating in the sky. In contrast, our method effectively mitigates these issues, achieving better detail restoration, particularly in regions such as bag zippers, gaps in walls, and window frames.\nWhen compared to DBIM, our method achieved similar IS, LPIPS, and MSE but reduced FID by 39% in the Edges Handbags task and 11% in the DIODE-Outdoor task, with a 3.6\u00d7 acceleration in NFE. Against Pix2Pix, our method achieved substantial FID reductions of 99% in the Edges Handbags task and 97% in the DIODE-Outdoor task, although with a slightly lower IS in the Edges Handbags task. Additionally, our approach consistently outperformed other methods across all four metrics in both tasks."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "In conclusion, we presented a high-order ODE sampler with a stochastic start for diffusion bridge models. Recognizing that the PF-ODE exhibits singular behavior at the start of the generative process, while the reverse SDE remains well-defined, we introduced a stochastic start and employed posterior sampling to mitigate discretization errors. Following this, we utilized Heun's second-order solver to solve the PF-ODE, enabling our sampler to achieve high perceptual quality with reduced NFEs.\nOur sampler is fully compatible with pretrained diffusion bridge models, requiring no additional training, and was validated on both image restoration and translation tasks using pretrained models from I2SB and DDBM. Compared to the original samplers used in I2SB and DDBM, our sampler achieved better FID with fewer NFEs and provided superior detail restoration in visualizations. Additionally, our method demonstrated significant improvements over other state-of-the-art methods, including ADM, DDNM, DDRM, IGDM, and DPS in image restoration tasks, and Pix2Pix, DDIB, SDEdit, Rectified Flow, I2SB, and DBIM in image translation tasks.\nThis work focused on the starting strategy for the generative process of diffusion bridge models, leaving the exploration of alternative high-order ODE solvers as a future direction. While we employed Heun's second-order solver and demonstrated its strong performance, replacing it with other high-order ODE solvers, known for their effectiveness in diffusion models, holds potential for further improving the performance of our sampler. We plan to investigate these possibilities in future research."}, {"title": "A. Proofs", "content": "1) Proof for Theorem 1 and 2: For any t \u2208 (0,T), we proceed the score function $\\nabla_{X_t} log q_{t|y}(X_t|y)$ as follows:\n$$\\nabla_{X_t} log q_{t|y}(X_t|y) = \\frac{1}{q_{t|y}(X_t|y)} \\nabla_{X_t}q_{t|y}(X_t|y),$$\n$$= \\frac{1}{q_{t|y}(X_t|y)} \\nabla_{X_t} \\int q_{t|0,y}(X_t|X_0, y) q_{data}(X_0|y) dX_0,$$\n$$= \\frac{1}{q_{t|y}(X_t|y)} \\int (\\nabla_{X_t}q_{t|0,y}(X_t|X_0, y)) q_{data}(X_0|y) dX_0,$$\n$$= \\frac{\\int (\\frac{a_ty+b_tX_0 - X_t}{c_t^2} q_{t|0,y}(X_t|X_0, y)) q_{data}(X_0|y) dX_0}{q_{t|y}(X_t|y)},$$\n$$= \\frac{1}{c_t^2} (\\frac{\\int (a_ty + b_tX_0) q_{t|0,y}(X_t|X_0, y) q_{data}(X_0|y) dX_0}{q_{t|y}(X_t|y)} - X_t)$$\n$$=-\\frac{1}{c_t^2} (X_t - (a_ty + b_tX_0^{(t)})),$$\nwhere the expected mean $X_0^{(t)}$ is defined as:\n$$X_0^{(t)} = \\int X_0 q_{0|t,y}(X_0|X_t, y) dX_0,$$\nand $q_{0|t,y}(X_0|X_t, y)$ is given by Bayesian rule:\n$$q_{0|t,y}(X_0|X_t, y) = \\frac{q_{data}(X_0|y) q_{t|0,y}(X_t|X_0, y)}{q_{t|y}(X_t|y)}$$\nUsing the expression for $\\nabla_{X_t} log p_{T|t}$ in equation (3), the non-linear drift term in the reverse SDE (8) is expressed as:\n$$\\nabla_{X_t} log q_{t|y}(X_t|y) - \\nabla_{X_t} log p_{T|t}(y|X_t)$$\n$$= -\\frac{1}{c_t^2}(X_t - (a_ty + b_tX_0^{(t)})) - \\frac{(a_ty - X_t)}{\\alpha_{\\xi} \\rho_{\\xi}}.$$\nLetting t\u2192 T with $X_T = y$, we establish equation (12).\nFurthermore, the non-linear drift term in PF-ODE (9) can be expressed in terms of the corresponding term in the reverse SDE (8) and the score function as follow:\n$$\\frac{1}{2} \\nabla_{X_t} log q_{t|y}(X_t|y) - \\nabla_{X_t} log p_{T|t}(y|X_t)$$\n$$=-\\frac{1}{2c_t^2} \\nabla_{X_t} log q_{t|y}(X_t|y) - \\frac{(a_ty - X_t)}{\\alpha_{\\xi} \\rho_{\\xi}}.$$\nAs t \u2192 T, the term $\\frac{1}{2} \\nabla_{X_t} log q_{t|y}(X_t|y) - \\nabla_{X_t} log p_{T|t}(y|X_t)$ does not converge, due to the singularity of the score function at time T.\n2) Proof for Theorem 3: To proof Theorem 3, we start by formulating the following optimization problem:\n$$q^*(X_\\tau|y) = arg\\underset{q(X_\\tau|y)}{min} E_{X_0\\sim q_{data}(X_0|y)} [D_{KL}(q (X_\\tau|y) ||q_{\\tau|0,y} (X_\\tau|X_0,y))],$$\ns.t. $q (X_\\tau|y) = \\mathcal{N} (X_\\tau|\\mu_{\\tau} (y), \\sigma_{\\tau}^2 (y) I),$\nwhere $q_{\\tau|0,y} (X_\\tau|X_0,y)$ is defined in equation (6) with t substituted by $\\tau$, and $\\mu_{\\tau} (y)$ and $\\sigma_{\\tau}^2 (y)$ are the parameters to be optimized. Since both $q(X_\\tau|y)$ and $q_{\\tau|0,y} (X_\\tau|X_0,y)$ are Gaussian distributions, the KL-divergence can be explicitly computed. The objective function becomes:\n$$E_{X_0\\sim q_{data}(X_0|y)} [D_{KL}(q (X_\\tau|y) ||q_{\\tau|0,y} (X_\\tau|X_0,y))]$$\n$$= \\frac{1}{2} (d \\frac{\\sigma_{\\tau}^2 (y)}{c_{\\tau}^2} - ln \\frac{\\sigma_{\\tau}^2 (y)}{c_{\\tau}^2} + \\frac{1}{c_{\\tau}^2}E_{X_0\\sim q_{data}(X_0|y)}||a_{\\tau}y + b_{\\tau}X_0 - \\mu_{\\tau} (y) ||^2),$$,\nwhere d denotes the data dimension. The optimal parameters $\\mu^* (y)$ and $\\sigma^* (y)$ that minimize the objective function are:\n$$\\mu^* (y) = a_{\\tau}y + b_{\\tau}X_0^{(T)},$$\n$$\\sigma^* (y) = c_{\\tau},$$\nwhere the expected mean $X_0^{(T)}$ is defined in equation (13). Since $\\mu^* (y)$ and $\\sigma^{*2} (y) I$ correspond to the mean and covariance matrix of the Gaussian distribution $q_{post}(X_\\tau|y)$, we conclude:\n$$q^* (X_\\tau|y) = q_{post}(X_\\tau|y),$$\ndemonstrating that $q_{post}(X_\\tau|y)$ is the optimal Gaussian approximation to the true distribution $q_{\\tau|0,y} (X_\\tau|X_0,y)$ in terms of KL-divergence."}, {"title": "", "content": "Next, discretizing the reverse SDE (8) from time T to $\\tau$ using a single Euler-Maruyama step results in:\n$$X_\\tau - y = \\frac{f (T)}{2}(T)y + g^2 (T) \\frac{y - \\frac{a_T}{a_\\tau}X_\\tau}{\\alpha_{\\xi} (\\rho_{\\xi} - \\rho_{\\xi})}] (\\tau - T)$$\n$$+ g (T) \\sqrt{T - \\tau} \\epsilon,$$\nwhere $\\epsilon \\sim \\mathcal{N} (0, I)$. Thus, $q_{EM} (X_\\tau|y)$ is also a Gaussian distribution. By combining this result with equations (21) and (24), we conclude that the inequity (15) is satisfied, completing the proof of Theorem 3."}]}