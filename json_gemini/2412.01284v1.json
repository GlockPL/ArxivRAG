{"title": "MFTF: Mask-free Training-free Object Level Layout Control Diffusion Model", "authors": ["Shan Yang"], "abstract": "Text-to-image generation models have become transformative tools. However, diffusion-based vision language models still lack the ability to precisely control the shape, appearance, and positional placement of objects in generated images using text guidance alone. Global image editing models typically achieve global layout control by relying on additional masks or images as guidance, which often require model training. While local object editing models enable modification of object shapes, they do not provide control over the positional placement of these objects. To address these limitations, we propose the MFTF model, which enables precise control over object positioning without requiring additional masks or images. The MFTF model supports both single-object and multi-object positional control (such as translation, rotation, etc.) and allows for concurrent layout control and object semantic editing. This is achieved by controlling the denoising process of the diffusion model through parallel denoising. Attention masks are dynamically generated from the cross-attention layers of the source diffusion model and applied to queries from the self-attention layers to isolate objects. These queries are then modified according to layout control parameters and injected back into the self-attention layers of the target diffusion model to enable precise positional control.", "sections": [{"title": "1. Introduction", "content": "Text-to-image generation models have emerged as transformative tools, enabling the production of high-quality, photorealistic, and artistically sophisticated visuals from natural language inputs. Advances in vision-language generative modeling primarily fall into two categories: transformer-based autoregressive approaches [3, 25] and diffusion-based methods [7, 11, 18, 19]. Among these, diffusion models have shown outstanding promise, delivering state-of-the-art results across various benchmarks [1, 10, 14, 15, 17].\nDiffusion based vision language models, however, lack the capability to precisely control the generated images using text guidance alone [1]. Although images generated from the same text prompt with different initial noise maintain semantic consistency, they often exhibit considerable variations in shape, appearance, and object positioning [12].\nCurrent layout control methods encompass global layout control and local object editing. Global layout control typically utilizes additional masks or images as guidance and may require model training, depending on whether auxiliary networks are employed [20, 26]. While existing local object editing techniques provide control over semantics and structure, they still lack the ability to achieve precise control over the positional placement of objects [2, 6, 8, 12].\nTo complement existing spatial control techniques, we propose MFTF model, which enables precise positional control of individual objects without requiring additional masks as guidance or model training, shown in Fig. 1. The model achieves this by controlling the denoising process of the diffusion model through parallel denoising for both the source and target prompts. Attention masks are dynamically generated from the cross-attention layers of the source diffusion model and applied to queries from the self-attention layers to isolate objects. These queries are then modified according to layout control parameters and injected back into the self-attention layers of the target diffusion model, enabling precise positional control throughout the denoising process. Additionally, the cross-attention masks generated for tokens can also be used for text-guided semantic segmentation. The architecture of the proposed model is illustrated in Fig. 2.\nThe challenge lies in performing object-level layout control without relying on additional image guidance, and object background seperation. Drawing inspiration from previous work [2, 6, 20] that modifies either the cross-attention layer or the self-attention layer, as well as employing multi-self-attention techniques, our model generates attention masks dynamically. These masks are used in the cross-attention layers to effectively separate objects from the background within the self-attention layers. Ultimately, layout control of the objects is achieved through a modified multi-self-attention operation, allowing for precise control over the positioning and arrangement of the objects in the generated images.\nThe challenge lies in performing object-level layout control without relying on additional image guidance and object-background separation. Inspired by previous work [2, 6, 20] that modifies either the cross-attention layer or the self-attention layer, and utilizes multi-self-attention techniques, our model dynamically generates attention masks, applied in the self-attention layers to effectively separate objects from the background. Ultimately, object layout control is achieved through a modified multi-self-attention operation.\nThe contributions of this model are as follows: (1) The MFTF model enables single-object and multi-object level positional control, as well as concurrent object-level layout control and semantic editing. (2) MFTF supports concurrent layout control and semantic editing without requiring images or masks as guidance without model training or fine-tuning. (3) The proposed method can also be applied to text-controlled semantic segmentation."}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Text-to-Image Generation", "content": "Text-to-image generation primarily consists of two families of models: auto-regressive models and diffusion models. Among the auto-regressive models, the Parti [24] model integrates both text and image modules using a transformer architecture, enabling text-controlled, high-fidelity, photorealistic image generation. The Parti model is built upon VQVAE [21] and VQGAN [5, 23], which facilitate the tokenization and detokenization of images, which enables the fusion of text tokens and image tokens within the transformer architecture [22].\nThe diffusion-based family of text-to-image generation models has gained significant attention in recent years. Notable models in this category include Imagen [17], which utilizes pretrained large language models for text encoding, achieving strong text-image alignment and photorealistic image generation. DALL-E 2,3 [1, 14] from OpenAI achieve text and image alignment through the CLIP [13] model and generate images using a diffusion model. The Latent Diffusion Model (LDM) adopts a different architecture from DALL-E 2,3 by conditioning on text for image generation, achieving text-image alignment within the latent space [15]. Our model builds upon the open-source Stable Diffusion model, which is an open-source implementation of the Latent Diffusion Model."}, {"title": "2.2. Text-to-Image Generation Layout Control and Semantic Editing", "content": "While local object editing models enable shape modification of objects, they lack the ability to control the positional placement of objects. The Prompt-to-Prompt method [6] enables object editing while preserving the original object positions by modifying the text prompt. This method identifies the corresponding cross-attention layers for text to-"}, {"title": "3. Preliminaries", "content": null}, {"title": "3.1. Latent Diffusion Model", "content": "The diffusion model is inspired by principles of non-equilibrium thermodynamics [7, 11, 18, 19] and consists of two complementary processes: the forward process and the reverse process. In the forward process_q(xt|xt\u22121), Gaussian noise e is incrementally added to the original image xo, progressively transforming it into random noise xT ~ N(0, I). In contrast, the reverse process po(Xt-1|Xt) systematically removes the noise e from the noisy image xt, ultimately reconstructing the original image.\nOur model is based on the latent diffusion model [15], which enhances computational efficiency by performing diffusion in the latent space. This approach reduces the time complexity of both the forward and reverse processes while preserving synthesis quality [15]. Latent diffusion model employs either a variational autoencoder (VAE) [4, 9] or a VQGAN [5] for encoding and decoding images. Specifically, the encoder & maps the input image x to a latent representation z, while the decoder D reconstructs z back to the image domain. The diffusion processes, relying on a U-Net architecture [16], are executed entirely in the latent space z. To incorporate conditional guidance, a spatial transformer is integrated into the U-Net, enabling enhanced flexibility. Additionally, conditions are encoded using the encoder Te and incorporated into the diffusion model via cross-attention layers within the spatial transformer architecture. This design ensures robust and effective guidance throughout the diffusion process."}, {"title": "3.2. Cross-attention and Self-attention in Diffusion Models", "content": "In the stable diffusion model, the modified U-Net module E incorporates a spatial transformer block, which consists of a self-attention layer followed by a cross-attention layer. During the denoising process, the encoded image z is combined with the time-step embedding t and passed through the self-attention layer. The output from the self-attention layer is then processed by the cross-attention layer, where it interacts with text-guidance embeddings. The formulations of self-attention and cross-attention are presented in Eq. (1) and Eq. (2), respectively. Here, Qz, Kz and V\u2082 are derived from spatial features, K\u2081, V, are derived from text embeddings, and d represents the embedding dimension.\nself-attention(Qz, Kz, Vz) = softmax($\\frac{QzKz^T}{\\sqrt{d}}$) * V (1)\ncross-attention(Qz, K+, V+) = softmax($\\frac{QK^T}{\\sqrt{d}}$) * V (2)\nThe self-attention and cross-attention layers encode rich semantic and structural information, which can be leveraged to control style and semantic concepts, as demonstrated in Prompt-to-Prompt [6] and Plug-and-Play [20]. Additionally, the cross-attention mechanism can be utilized to generate masks for separating objects from the background, as implemented in Masactrl [2]. Specifically, objects from token i can be isolated from the background by applying the cross-attention mask M. This enables layout control through the application of the layout parameter L, which governs structural transformations such as rotation, position translation, and scaling, thereby facilitating precise control over the generated layout."}, {"title": "4. Simultaneous Object Level Layout Control and Semantic Editing", "content": "The primary objective of this model is to facilitate simultaneous object-level layout control and semantic editing without the need for additional image or mask guidance, and without requiring model training or fine-tuning. As demonstrated in Fig. 1, the model effectively achieves positional control of individual objects, enabling layout transformations such as translation, scaling, and rotation to be applied concurrently. Moreover, it illustrates that both object-level layout control and semantic editing can be performed simultaneously. Additionally, the model supports the control of multiple object positions concurrently. To realize this objective, we propose the MFTF model, which generates attention masks and utilizes cross-attention mechanisms to separate objects from the background within the self-attention layers. Object layout control is then achieved through multi-self-attention [2].\nThe architecture of the model is illustrated in Fig. 2. The model utilizes a prompt Ps for generating the source image Xs, and a separate prompt Pt for generating the target image Xt. The target prompt Pt can either remain the same as Ps or differ, depending on whether semantic editing is applied to the image. The object indices P from Ps and the object layout control parameters L\u00b2 are provided to the model to regulate the layout of objects in the generated target image Xt. For example, as shown in Fig. 1, the images are generated with for instance the position of the \"castle\" is shifted to the left according to L, the \u201csteam\" in Ps is replaced with \"desert\" in Pt.\nDive into details of the MFTF model. It governs the denoising process of the diffusion model by facilitating parallel denoising for both the source and target prompts. It dynamically generates attention masks from the cross-attention layers of the source diffusion model. These attention masks are then applied to the queries Qs derived from the self-attention layers to identify the objects. Subsequently, the modified queries Qms are generated in accordance with the layout control parameters. Finally, Qms is injected into the self-attention layer of the target diffusion model, enabling precise positional control during the denoising process.\nThe MFTF algorithm is summarized in Algorithm 1. The parameter t* represents the cutoff time step for applying layout control, while T denotes the total number of denoising time steps. For time steps t < t*, the mask M is obtained from the cross-attention layer, with Qs derived from the self-attention layer. Details of the cross-mask generation are provided in Sec. 4.1. The mask M is then applied to the query Qs in the self-attention mechanism to separate the object from the background. The object's position is adjusted according to the given layout control parameters L. Subsequently, the modified query Qms is passed to the target diffusion model to generate the target latent space Zt. Details of the self-attention query Qms generation are further explained in Sec. 4.1.\nThe MFTF model enables simultaneous object-level layout control and semantic Editing and through three key mechanisms: (1) the cross-attention mask, which encodes semantic and structural information of objects for to-"}, {"title": "4.1. Dynamic Cross-attention Mask Generation", "content": "The cross-attention mask is constructed utilizing the cross-attention layer, where the object mask is derived following the formula described in Eq. (3), with a specified threshold eta. The generated masks, containing both the object and the background, are depicted in Fig. 3.\nThe cross-attention map, defined as A = QKT, is represented by the dimensions b, n, m, where b corresponds to the batch size, n denotes the feature sequence length, and m represents the text sequence length. The model is capable of dynamically generating cross-attention masks M for token i at each sampling step t, which are applied to the self-attention layer l. This mechanism allows for controlled image generation using dynamically updated cross-attention masks.\nM\u00b2 =\n{\n1, if A\u00b2 >= \u03b7\n10, otherwise\nS\n(3)\nThe threshold \u03b7 has a significant impact on the generated images through the attention mask. When \u03b7 = 0, no"}, {"title": "4.2. Self-Attention-Based Layout Control", "content": "The self-attention layer Q encodes both detailed object information and global layout characteristics of the generated images. The semantic and structural information represented by Q varies across different self-attention layers. Specifically, Qs, corresponds to the self-attention representation at layer l as illustrated in Fig. 5. Lower layers (l = 0 to 5) lack significant semantic information but begin to establish the foundational structure. In the intermediate layers (l = 6 to 10), structural features become more prominent and start to take shape. The upper layers (l = 11 to 15) contain detailed object-specific structural and semantic information. For example, as shown in Fig. 6, layer l = 10 encapsulates information connecting the object to its background, layer l = 13 focuses on the object's overall profile, and layer l = 15 captures fine details of the generated image.\nThe cross-attention mask effectively selects objects from the background following Eq. (4), and is shown in the second column of Fig. 5 and Fig. 6.\nQms = Qs * M\nS\n(4)"}, {"title": "4.3. Applications", "content": null}, {"title": "4.3.1 Layout Control and Semantic Editing", "content": "The model successfully demonstrates concurrent single-object positional control and semantic editing, as illustrated in Fig. 8. The first column presents images generated from the source prompts: \"a small cat sitting on a desk\" \"a cat sitting on a chair, no background\" and \"a painting on the wall in a cozy house\". Following the application of layout controls L, the resulting images are shown in the second, third, and fourth columns. These results highlight the ability to apply positional control, including translation, rotation, and scaling, both independently and simultaneously for target image generation. Object-level layout control and semantic editing are applied together, with modified target prompts such as \"a small cat sitting on grass\" \"a small cat sitting on grass\" and \"a painting on a chair\". The images generated from these prompts are displayed in the last column of Fig. 8."}, {"title": "4.3.2 Multiple Object Layout Control", "content": "The model is capable of controlling the positions of multiple objects simultaneously, as demonstrated in Fig. 9. The first image in the first row, generated from the source prompt Ps = \"a small car and a cabin in the style of Disney, artstation\", shows the initial configuration. In this case, the \"car\" on the left-hand side is translated to the left, while the \"cabin\" is translated to the right. Subsequently, the \"car\" is rotated by 30\u00b0 and 330\u00b0 clockwise. The second to fourth images in the first row show the target images after the application of layout control L. The corresponding Qs and Qms values are presented in the second row. The third row illustrates another example of concurrent object position control for multiple objects."}, {"title": "4.3.3 Text-Controlled Semantic Segmentation", "content": "The attention map Ms and the masked queries, Qs or Qbackground, can also be utilized for text-guided image segmentation, as illustrated in Fig. 10. Here, M, serves as the mask for the token \"panda\", which emphasizes the object's profile, while Qs or Qbackground is used to generate detailed feature segmentation.\nThe procedure for obtaining the semantic segmentation mask is outlined as follows: First, the image is encoded into latent space z, with no noise added to z in order to prevent introducing randomness that could distort the original object structures. The latent space z is then denoised in conjunction with the prompt. Cross-attention or self-attention layers are used to extract segmentation masks for the specified tokens. While the generated images may not fully recover the original image, they effectively preserve the structural integrity of the input images for segmentation."}, {"title": "4.4. Ablation Study", "content": "The procedure for applying layout control includes the initial steps up to step t*, after which these steps are omitted until the final step. As shown in Fig. 11, the initial steps determine the positioning and arrangement of objects within the structure, while the later denoising steps focus on generating structural details. Layout control is effective when the initial denoising steps are included. However, if these initial steps are omitted, layout control fails to operate as intended.\nThe initial layers do not contain meaningful structural information; rather, detailed structural information gradually emerges in the higher layers of the U-Net architecture, as"}, {"title": "4.5. Limitations", "content": "Despite the effectiveness of the proposed model in achieving single-object and multi-object layout control, as well as simultaneous layout control and semantic editing, several limitations remain. A primary challenge arises from the mixed structural information present in the cross-attention layer, which makes it difficult to fully encapsulate complete structural information within a single attention mask. This limitation hinders the model's ability to fully separate objects from the background. Additionally, in multi-object position control, the model struggles with separate objects with complex interrelationships, particularly in scenarios involving tightly coupled or overlapping objects."}, {"title": "5. Conclusion", "content": "In this paper, we have presented a mask-free, fine-tuning-free object-level layout control diffusion model. The model utilizes self-generated attention masks and applies cross-attention masks to effectively separate objects from the background within the self-attention layers. By leveraging multi-cross attention, the model controls the layout of objects, enabling both single-object and multi-object layout control. MFTF supports concurrent layout control and semantic editing, as well as text-guided image segmentation, demonstrating its promising potential for further research and practical applications in this domain. ( to be modified) Future applications of MFTF models could potentially aid humans in creating compelling custom images with unprecedented speed and ease."}]}