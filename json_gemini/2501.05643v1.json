{"title": "Iconicity in Large Language Models", "authors": ["Anna Marklov\u00e1", "Ji\u0159\u00ed Mili\u010dka", "Leonid Ryvkin", "L'udmila Lackov\u00e1 Bennet", "Libu\u0161e Korman\u00edkov\u00e1"], "abstract": "Lexical iconicity, a direct relation between a word's meaning and its form, is an important aspect of every natural language, most commonly manifesting through sound-meaning associations. Since Large language models' (LLMs') access to both meaning and sound of text is only mediated (meaning through textual context, sound through written representation, further complicated by tokenization), we might expect that the encoding of iconicity in LLMs would be either insufficient or significantly different from human processing. This study addresses this hypothesis by having GPT-4 generate highly iconic pseudowords in artificial languages. To verify that these words actually carry iconicity, we had their meanings guessed by Czech and German participants (n=672) and subsequently by LLM-based participants (generated by GPT-4 and Claude 3.5 Sonnet). The results revealed that humans can guess the meanings of pseudowords in the generated iconic language more accurately than words in distant natural languages and that LLM-based participants are even more successful than humans in this task. This core finding is accompanied by several additional analyses concerning the universality of the generated language and the cues that both human and LLM-based participants utilize.", "sections": [{"title": "1. Introduction", "content": "Lexical iconicity is (in simplified terms) a direct relation between word's meaning and its form, and although it was marginalized by many 20th-century linguists starting with de Saussure, it remains an important aspect of language. It plays a role in acquisition (Perry et al., 2018) and is a consequence of our embodiment (Vigliocco et al., 2014). It will therefore be interesting to examine how large language models, which by default lack embodiment and acquire language in a technically different way than humans (Huebner et al., 2021), process iconicity. Surprisingly, there are very few studies investigating iconicity in large language models (with exceptions such as Trott (2024b) and Loakman et al. (2024)). This paper aims to fill this gap.\nThe very definition of iconicity suggests that large language models should struggle with it, as their processing of both semantic and phonetic aspects of language is indirect:\n1.  Regarding lexical semantics, the vector represen-\ntation of semantics in language models based on\nembeddings processed by transformers (Bahdanau\net al., 2016; Vaswani, 2017) does not describe\na direct relationship between the signifier\nand the signified, as de Saussure conceived\nit, but rather encodes relationships between"}, {"title": "2. Motivation and aims", "content": "A common definition of iconicity is that it is a property which holds when signs resemble their referents (Vinson et al., 2021). Iconicity can also be characterized by its role in human cognition and communication. Motamedi et al. (2019) offer a functional definition that states that iconicity is a feature of a signal that enables its meaning to be predicted from its form. Iconicity, together with its complementary force, arbitrariness (De Saussure, 1983), are widely considered to be present across all spoken and signed languages (Dingemanse et al., 2016; Perniss and Vigliocco, 2014). The question is, can iconicity be created artificially?\nThere were attempts to create artificial languages that would be universally well learnt and understood."}, {"title": "3. Methodology", "content": "Our experimental design follows a classic approach used in other empirical studies on iconicity (e.g., Kunihira (1971)). Each participant is presented with four stimuli: two of which are meaningful (e.g., words in a familiar language or images) and two that are primarily formal for the participant (e.g., pseudo- words or words in an unfamiliar language, either written or spoken). Participants are asked to match the two meaningful stimuli with the two formal stimuli.\nThe protocol about the exact run of the experiment, allowing replication, together with the full dataset, detailed demographic information about participants, and scripts of the experiment, can be found in Supplementary information."}, {"title": "3.1. Materials", "content": "To maintain comparability with previous studies, we used the same dataset of words as in the article Sources of Intelligibility of Distant Languages: An Empirical Study. Originally, we randomly selected 68 words from a frequency wordlist derived from the Hindi corpus HindEnCorp Diatka and Mili\u010dka (2017); Bojar et al. (2014). The selection adhered to specific criteria:\n\u2022\t22 words from the lower-frequency level (with 1-13 occurrences in the corpus);\n\u2022\t23 words from the middle-frequency level (with 14-200 occurrences in the corpus);\n\u2022\t23 words from the upper-frequency level (with over 200 occurrences in the corpus).\nThis stratified sampling approach ensures that the word set represents various frequency levels"}, {"title": "3.1.1. Generating pseudowords", "content": "The 68 words were translated into an artificial language created by GPT-4 by OpenAI. The exact scripts of the prompts and the answers given by the model can be found in Supplementary information. We generated two datasets of pseudowords: one using Czech as the language of the prompts, leading to the list of pseudowords we will call here Czech-prompted pseudowords; second using German as the prompting language, leading to the list of German-prompted pseudowords. The German and Czech prompt with English translations and an example of an answer can be seen in Table 1.\nThe prompt was refined after testing several versions, including one that did not specify the language features (i.e., asking to generate words from an imaginary language) and one that requested a fantasy language. The final prompt worked best for the experiment's purposes: Including the detail that the language is made-up ensured that the generated lexemes were not translations from a foreign language. Additionally, specifying a geographic area prevented the lexemes from closely resembling words"}, {"title": "3.1.2. Creating the word lists", "content": "The two lists of the 68 pseudowords German- prompted pseudowords and Czech-prompted pseudo- words were used to create four versions of the experiment, since we wanted to present both pseudoword lists to both Czech and German speakers. Therefore, we had:\n\u2022\tCzech-prompted pseudowords with Czech trans- lations for Czech speakers\n\u2022\tCzech-prompted pseudowords with German trans- lations for German speakers\n\u2022\tGerman-prompted pseudowords with German translations for German speakers\n\u2022\tGerman-prompted pseudowords with Czech trans- lations for Czech speakers\nThe lists were presented in an online experiment, either on android tablets and distributed in person among participants, or distributed by a link."}, {"title": "3.2. Participants", "content": null}, {"title": "3.2.1. Human participants", "content": "Participants were recruited either in person, distributing android tabled with BlackSquare appli- cation version of the experiment, or by distributing the link to the experiment through online platforms, such as Facebook and university webpages. In total the data of 672 participants who finished the experiment was collected across all conditions. Demographical details of the participants can be seen in Table 2."}, {"title": "3.2.2. LLM participants", "content": "To fully examine the pseudowords GPT-4 created, we wanted to see if the LLMs themselves will have a better chance to correctly assign the pseudowords to the meanings. We simulated both Czech participants guessing Czech pseudowords in Czech transcription and German participants guessing German words in German transcription.\nWe tested all combinations of words (i.e.,\n$68(68-1)/2 = 2278$\ncombinations for each language). To simulate participants, we selected gpt-4-0613, the same model used to generate the words. For comparison, we also tried another state-of-the-art model from a competing company, Anthropic's Claude 3.5 Sonnet (version claude-3-5- sonnet-20241022). Since the prompting was zero- shot, we ended up with:\n\u2022\t2278 GPT-4 Czech participants guessing Czech- prompted pseudowords\n\u2022\t2278 claude-3-5-sonnet Czech participants gues- sing Czech-prompted pseudowords\n\u2022\t2278 GPT-4 German participants guessing German-prompted pseudowords"}, {"title": "3.3. Procedure", "content": null}, {"title": "3.3.1. Human participants", "content": "Before the beginning of the experiment, the following instruction for Czech speakers was displayed (in Czech) on the screen: 'You are currently participating in a linguistic experiment. Your responses will be processed for scientific purposes. On each screen, you will see four words: two Czech words and their translations from an unknown language. Your task is to correctly match the words (using finger gestures or mouse draws), even though you do not know the foreign language. There are correct answers. Use your linguistic intuition. You may withdraw from the experiment at any time, in which case your responses will be deleted.' The same instruction for German speakers was displayed in German (with \"two German words\" instead of \"two Czech words\"). Participants were required to tap the OK button to proceed.\nEach participant received 34 assignments, each consisting of a set of four words: the upper pair in Czech/German and the lower pair were the two matching pseudowords. The order of the pairs as well as the pairing of the words was randomized for each participant. Participants were tasked with matching the corresponding words using gestures (see Figure 1). After completing the experiment, participants were informed about their results and asked to provide information about their age and gender."}, {"title": "3.3.2. LLM-based participants", "content": "A modified version of the experiment was presented to the LLMs (Claude 3.5 Sonnet and GPT- 4). All categories were prompted in a way that closely resembled the circumstances of the human experiment. the only exception was that the prompt"}, {"title": "4. Results and discussion", "content": "Our project generated an extensive dataset that enables testing numerous hypotheses. However, within the scope constraints of this paper, we focus exclusively on our initial research questions. The complete raw dataset, including all scripts used for pseudoword generation, experiment execution, and analysis, is available at https://osf.io/ywjrk/. This repository also includes a comprehensive 115-page Laboratory Protocol documenting the experimental procedures and analyses in detail, along with additional statistical analyses beyond those presented here. We encourage researchers interested in exploring other hypotheses to utilize this publicly available dataset and accompanying materials."}, {"title": "4.1. Correctness overview", "content": "First, let us examine the basic correctness statistics. We present these results alongside findings from Mili\u010dka et al. (2025) to compare correctness rates between natural languages and generated pseudowords. The proportion of correct responses"}, {"title": "4.2. Logistic regression", "content": "We are interested not only in participants' success rates but also in understanding the factors that influenced their performance. What independent variables played a role in determining the meanings of unknown words, what features participants relayed on? We can draw on results from previous comparable research conducted with natural languages (Mili\u010dka et al., 2025), where we found that people relied primarily on vector phonological similarity, edit phonological similarity, and length agreement. In the previous study, we also included variables named part of speech mismatch, semantic distance, and trial order."}, {"title": "4.3. LLMs' justification of decisions", "content": "As mentioned in the previous chapter, logistic regression revealed a large intercepts, that means, there might be some hidden factors influencing the correctness of the guesses. These factors are not related to the phonetic resemblances or length agreements. To uncover some of the possible factors, we explored the answers of LLM-based participants. As described in the Methodology chapter, these participants were asked to explain their decision when pairing the words with their translations, and while we do not have space in this paper to quantify the explanations they provided, we can describe them qualitatively. We will use the translated examples from Czech prompts and outputs, however, the models behaved comparably in German.\nFirstly, we will describe the reasoning of GPT- 4 while guessing the pseudowords. Unsurprisingly, it frequently reasoned based on word length, e.g., \"op\u011bt 'again' is short and simple, so it could correspond to the foreign word \u00e9r\u00f3, which is also short and simple. On the other hand, pokr\u00fdvka 'cover' is a longer word with more syllables, which could correspond to the foreign word mufutu, which is also longer and has more syllables.\" Simplicity/complexity of the words was another feature often mentioned (e.g., \"\u0161usmin sounds like a more complex word, which could correspond to a more complex concept, such as prodava\u010d 'shop assistant\")."}, {"title": "4.4. Comparison at the word level", "content": "Given that the models differ in how they explain their word-matching strategies, we might expect differences in which words they correctly or incorrectly assign. As shown in Figure 6, there are indeed words that GPT-4-based participants guessed correctly while Claude Sonnet 4.5 based participants guessed incorrectly (e.g., pikur\u2018which\u2019 in the German dataset, Figure 6, bottom charts) and vice versa (e.g., loooombu 'a lot' in the German dataset). However, we also find words where they agreed in their judgments: for instance, both models systematically guessed gron 'he was' correctly, while both consistently failed to correctly identify luliya 'contact'.\nSimilar differences and similarities can be observed between Czech and German participants, e.g., orutu 'direction' was quite easy for Germans but impenetrable for Czechs in the German dataset (Figure 7, bottom), while Czech prompted pseudoword \u0161\u00edja 'immaterial' was hard to guess for both Czechs and Germans (Figure 7, top).\nSuch patterns of agreement and disagreement are also found between human and LLM-based participants, e.g. German prompted pseudoword olam 'digestion' was hard to guess for Germans, Czech and also for Claude 3.5 Sonnet (Figure 8, bottom), but Czech prompted psoudoword l\u00f3malu 'ultimate good' was much easier for the LLM-based participants than for Czechs (Figure 8, top).\nTo quantify these differences and similarities, we employed Pearson's correlation coefficient, calculated for all relevant pairs and visualized in a heatmap (Figure 9). The analysis reveals no correlation between the two artificial languages"}, {"title": "5. Conclusions", "content": "The most remarkable finding of this study is that large language models encode not only arbitrary relations between symbols but also iconicity: despite both semantic and phonological qualities being only mediated and indirect for LLMs, they successfully connect them together.\nLLM training extracts meta-linguistic information about iconicity and non-arbitrariness from their training data, making this information accessible both for generating highly iconic pseudowords and for guessing their meanings. When prompted to create iconicity-rich pseudowords in an artificial language, the LLM-based entity does not tailor the output according to the prompt language, but instead creates a language that is partially intelligible to human speakers of various languages.\nThe meanings of words in these LLM-generated artificial languages can be guessed not only by humans but also by LLM-based participants, even when based on a different model (Claude 3.5 Sonnet) than the one that generated the pseudowords (GPT- 4). They are, in fact, much better than human participants in this task. Their results correlate with each other and also correlate with results of humans (slightly less, though).\nThe information about iconicity likely is not drawn solely from linguistic literature (which is"}, {"title": "6. Competing interests", "content": "No competing interest is declared."}, {"title": "7. Author Contributions Statement", "content": "A.M. and J.M. planned the research, conceived and conducted the experiments, and wrote and reviewed the manuscript. J.M. also performed the statistical analyses and created the visualizations. L.R. programmed the online version of the experiment and collected the majority of data from German speakers. L.L.B. and L.K. conducted the experiments, contributed to the research planning, and revised the manuscript."}, {"title": "8. Funding", "content": "Ji\u0159\u00ed Mili\u010dka was supported by Czech Science Foundation Grant No. 24-11725S, gacr.cz.\nAnna Marklov\u00e1 was supported by Primus Grant PRIMUS/25/SSH/010.\n\u013dudmila Lackov\u00e1 Bennet has been supported by the project JG_2024-020 implemented within the Palack\u00fd University Young Researcher Grant."}]}