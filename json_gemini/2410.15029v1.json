{"title": "Enhancing Multimodal Sentiment Analysis for Missing Modality through Self-Distillation and Unified Modality Cross-Attention", "authors": ["Yuzhe Weng", "Haotian Wang", "Tian Gao", "Kewei Li", "Shutong Niu", "Jun Du"], "abstract": "In multimodal sentiment analysis, collecting text data is often more challenging than video or audio due to higher annotation costs and inconsistent automatic speech recognition (ASR) quality. To address this challenge, our study has developed a robust model that effectively integrates multimodal sentiment information, even in the absence of text modality. Specifically, we have developed a Double-Flow Self-Distillation Framework, including Unified Modality Cross-Attention (UMCA) and Modality Imagination Autoencoder (MIA), which excels at processing both scenarios with complete modalities and those with missing text modality. In detail, when the text modality is missing, our framework uses the LLM-based model to simulate the text representation from the audio modality, while the MIA module supplements information from the other two modalities to make the simulated text representation similar to the real text representation. To further align the simulated and real representations, and to enable the model to capture the continuous nature of sample orders in sentiment valence regression tasks, we have also introduced the Rank-N Contrast (RNC) loss function. When testing on the CMU-MOSEI, our model achieved outstanding performance on MAE and significantly outperformed other models when text modality is missing. The code is available at: https://github.com/WarmCongee/SDUMC.\nIndex Terms-Multimodal Fusion, Sentiment Analysis, Self Distillation, Missing Modality", "sections": [{"title": "I. INTRODUCTION", "content": "Multimodal sentiment analysis has garnered significant at- tention [1]\u2013[4], with numerous studies focusing on the repre- sentation [5]\u2013[8], alignment [9], [10], and fusion [11]\u2013[14] of sentiment information from various modalities. The absence of certain modalities during training and inference significantly influences the alignment and fusion of multimodal informa- tion, prompting increased research efforts [15]\u2013[19] aimed at enhancing robustness under these conditions. Traditionally, re- search [15] has classified the absence of text, audio, and visual modalities as either complete or frame-level missing, often em- ploying partial word masks to mimic text frame-level absence. However, this simulated absence differs markedly from typical real-world scenarios where text is either completely missing or substituted with low-quality ASR transcriptions. Given the text modality\u2019s superior performance [13] in certain tasks, previous studies [20], [21] have relied on manually annotated transcriptions or the development of ASR/AVSR systems. We contend that it is unnecessary to reconstruct the final text when the text modality is missing. Instead, the model can simply use the hidden states derived from the audio or visual modalities to approximate the semantic space of the missing text modality.\nTheoretically, textual information can be highly abstracted from audio and visual modalities. With advancements in Large Language Models (LLM), some studies [22], [23] have em- ployed LLMs as decoders to address ASR tasks. For instance, the SLAM-ASR [22] projects WavLM\u2019s [24] audio represen- tations into the text space, and inputs them with prompt into an LLM to generate ASR results directly. Consequently, it is natural to infer that simulated text representations can be derived from the projected audio representations via LLMs, facilitating unified multimodal sentiment analysis.\nOur research proposes a Double-Flow Self-Distillation Framework, which includes the Modality Imagination Autoen- coder (MIA) and Unified Modality Cross-Attention (UMCA). This unified model can address both complete and text- missing multimodal sentiment analysis scenarios by simply activating or deactivating the MIA during inference. The main contributions of our work are: (1) We design UMCA for comprehensive multimodal fusion to extract sentiment- related information. (2) Our method uses the LLM-based model Vicuna [25] to generate simulated text representations from WavLM\u2019s hidden state projection when text modality is missing, with MIA complementing the missing information. (3) We train a unified multimodal sentiment analysis network using self-distillation combined with MIA activation and deac- tivation, while achieving distance-space alignment of complete and missing modality representations through MKD Loss and RNC Loss [26]. (4) Our method achieves optimal performance on the CMU-MOSEI [27] across multiple metrics for complete modality inference, and significantly improving performance in text-missing scenarios compared to other methods."}, {"title": "II. METHOD", "content": "In this section, we discuss our proposed Double-Flow Self- Distillation Framework for the multimodal sentiment analysis task. The overall architecture of our system is depicted in Fig. 1 and will be detailed in the subsections."}, {"title": "A. Unified Modality Cross-Attention", "content": "As shown in the bottom right corner of the Fig. 1, the UMCA model is divided into two main stages. And we set up the cross-attention module, which is applied in both stages.\nThe formulas are as follows:\n$K_m = Tanh(WV_m + b)$ (1)\n$R_m = Softmax( \\frac{Q K_m^T}{\\sqrt{D}} )V_m$ (2)\nm\u2208 {a, v, t}, where a, v, and t refer to the three modalities of audio, vision, and text. \\( \\tau \\) denotes the scaling parameter that ensures the stability of the gradient of softmax. \\( K_m \\) and \\( V_m \\) are the key and value sequences from the m modality, \\( Q \\) is the query involved in the cross-attention, and \\( R_m \\) is the representation of the m modality in the same dimension as Q.\nThe first stage is used to obtain the query embedding of the multi-view modalities combinations. First, each modality representation is mapped to the same dimension \\( E_m \\in \\mathbb{R}^{S \\times D} \\) by MLP, where S represents the sequence length and D denotes the dimension determined by a hyperparameter. We initialize the Gaussian-distributed learnable query (\\( Q_m \\)) for each modality during model initialization, where \\( Q_m \\in \\mathbb{R}^{S_1 \\times D} \\), with S1 set to 1. For each modality, the original modal representations are mapped to the key (\\( K_m \\)) and value (\\( V_m \\)). According to the equation, we use the cross-attention module to obtain the representations \\( R_m \\) for each modality.\nThe Attention-guided Feature Gathering (AFG) module [28] concatenates the three modal representations and feeds them into an MLP to obtain attention weights. These weights are used to compute the weighted sums of unimodal, bimodal, and trimodal representations, producing a multi-view query (\\( Q_{multv} \\)) with seven combinations. For instance, in a bimodal scenario, we set the unused modality\u2019s weight to zero while maintaining the weights of the other two modalities. This results in a weighted sum that forms the bimodal query.\nIn the second stage, the multi-view query obtained in the previous stage is used as the query (\\( Q_{multv} \\)). The original modal representations are mapped as key (\\( K_m \\)) and value (\\( V_m \\)). The modal representations \\( R_{seq} \\) are obtained by cross- attention, and the final representation \\( r \\) is obtained by attention weighted summing \\( R_{seq} \\) by AFG module, and regression is performed to obtain the sentiment valence."}, {"title": "B. Missing Modality Imagination", "content": "1) LLM-based Text Representation Simulation: SLAM- ASR [22] uses WavLM as an audio encoder and LLM-based model Vicuna [25] as a decoder. Then the method trained the projector between them and generated ASR results from the audio encoder representations directly using the generative power of the LLM. Based on this, we work on unifying the representation of text modality presence and missing. Definitions are as follows:\n$E_t = LLM_f(tokenizer(T))$ (3)\n$\\hat{E_t} = LLM_g(cat(proj(E_a), tokenizer(P)))$ (4)\nIn the definitions, T denotes the text transcribed from video content. The symbol \\( E_t \\) refers to the real text representation obtained from the text modality. The function \\( LLM_f(\\cdot) \\) is employed for the direct forward pass to obtain the hidden state. Additionally, the symbol \\( \\hat{E_t} \\) denotes the simulated text repre- sentation generated by the LLM in scenarios where the text modality is missing. \\( E_a \\) refers to the representation obtained from the audio modality, and P refers to the input prompt for LLM. The notation \\( proj(\\cdot) \\) is used to refer to the pre- trained projector network within the SLAM-ASR framework. As shown in the middle of the left side of Fig. 1, \\( LLM_g(\\cdot) \\) refers to the hidden state derived from the generated results.\n2) Modality Imagination Autoencoder: We use the pre- trained projector and Vicuna to obtain real text representations when text modality is present and generate simulated ones when text modality is missing. However, the gap between real and simulated representations can negatively impact perfor- mance if used directly for training. To address this, we use the Residual Autoencoder [29] to construct the missing modality imagination autoencoder, which activates only when the text modality is missing. The structure of MIA is depicted in the bottom left corner of Fig. 1. The definition is as follows:\n$H_t = f(W^{(1)} cat(R_v, R_a, R_t) + b^{(1)})$ (5)\n$R_t = R_t + f(W^{(2)} H_t + b^{(2)})$ (6)\n\\( R_v, R_a, R_t \\in \\mathbb{R}^{S \\times D} \\), where S is the sequence length and D is the unified dimension of modality representations. \\( H_t \\in \\mathbb{R}^{S \\times D'} \\), where D\u2019 is the dimension of the intermediate hidden state. We replace the original \\( R_t \\) with the reconstructed \\( R_t \\). As shown in Fig. 1, since there are two MIA modules, the input is \\( R_m \\) in the MIA-1 module and \\( R_{seq} \\) in the MIA-2 module."}, {"title": "C. Modality Missing Self-Distillation", "content": "We design a self-distillation framework to handle cases where all modalities are present and cases where the text modality is missing. The network has two data flows: one with full modalities input (visual, audio, text) and one with only visual and audio modalities input.\nDuring training, we distill text modality representation knowledge from the complete multimodal flow to the scenario where the text modality is missing. To achieve this, we have designed the Modality Knowledge Distillation Loss (LMKD) and Representation Similarity Loss (LRS). To capture the continuous nature of sample orders in the sentiment valence re- gression task, we introduced the Regression Rank-N Contrast Loss (LRNC). Additionally, the Mean Squared Error loss is used as the sentiment valence regression task loss (LTask) for sentiment analysis. To ensure the model acquires these various capabilities, we use the weighted sum of these losses as the final loss function. The overall loss is defined as follows:\n\\( \\mathcal{L} = \\alpha \\mathcal{L}_{Task} + \\beta \\mathcal{L}_{LMKD1} + \\gamma \\mathcal{L}_{LMKD2} + \\theta \\mathcal{L}_{RS} + \\delta \\mathcal{L}_{RNC} \\) (7)\nThe \\( \\alpha \\) to \\( \\delta \\) here defines the weights of the various losses, with specific values displayed in our open-sourced code.\n1) Sentiment Valence Regression Loss: This is a regression task for predicting sentiment valence. Thus, the most important loss is the MSE loss between the true and predicted valence, defined as follows:\n$\\mathcal{L}_{Task} = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2 $ (8)\nHere, \\( y_i \\) is the valence label, and \\( \\hat{y_i} \\) is the predicted valence."}, {"title": "2) Modality Knowledge Distillation Loss", "content": "The objective of LMKD is to align the simulated text modality representations, produced by the MIA, as similar as possible with the real text modality representations to enhance the model\u2019s robustness when text modality is missing. Specifically, we detach the real text modality representation and then compute the RMSE loss between it and the simulated text modality representation from MIA. Consequently, it should be highlighted that the gradient of \\( \\mathcal{L}_{LMKD} \\) will only back-propagate through text missing data flow. Considering the presence of two MIA modules, we calculate two MKD losses, which are defined as follows:\n$\\mathcal{L}_{LMKD} = \\frac{1}{N} \\sum_{i=1}^{N}\\frac{(R_{i(detached)} - \\hat{R_i})^2}{N}$ (9)\nDuring the process, we need to calculate \\( \\mathcal{L}_{LMKD1} \\) and \\( \\mathcal{L}_{LMKD2} \\). First, we detach \\( R_t \\) and use it along with \\( \\hat{R_t} \\) in the formula to calculate \\( \\mathcal{L}_{LMKD1} \\). Then, we detach \\( R_{seq} \\) and use it along with \\( \\hat{R_{seq}} \\) in the formula to calculate \\( \\mathcal{L}_{LMKD2} \\).\n3) Representation Similarity Loss: In order to make the representations entering the final regression head as similar as possible when the modality is complete and the text modality is missing, we design a representation similarity loss \\( \\mathcal{L}_{RS} \\).\n$\\mathcal{L}_{RS} = \\frac{1}{N} \\sum_{i=1}^{N}\\frac{(r_i - \\hat{r_i})^2}{N}$ (10)"}, {"title": "4) Regression Rank-N Contrast Loss", "content": "For the sentiment valence regression task, the RNC loss has two goals. First, it brings the representations of two data flows of the same data similar. Second, it allows the model to learn and align the representation distance spaces correctly for both the missing text modality and the complete modalities. The RNC loss is defined as follows:\n$\\mathcal{L}_{RNC}^+ = \\frac{1}{2N} \\sum_{i=1}^{N} \\sum_{\\forall j \\in S_{i,j}} log \\frac{exp(sim(r_i,r_j)/\\tau)}{\\sum_{k=1, k\\neq i}^{N} exp(sim(r_i,r_k)/\\tau)}$ (11)\n$\\mathcal{L}_{RNC} = \\frac{1}{2N} [ \\sum_{i=1}^{N} \\mathcal{L}_{RNC,i}^+ + \\frac{1}{2N - 1} \\sum_{i=1}^{N} \\sum_{j=1,j \\neq i}^{N} - \\mathcal{L}_{RNC,i}^+ ]$ (12)\nHere, N is the batch size, and r refers to the final representa- tion. The \\( sim() \\) function computes the L2 distance. \\( S_{i,j} \\) refers to the set of final representations of all other data in the batch whose distance from the valence label of anchor i is greater than or equal to the distance between the labels of i and j."}, {"title": "III. EXPERIMENTS", "content": "We test our model on the CMU-MOSEI [27] dataset, which includes 22,856 videos: 16,326 for training, 1,871 for validation, and 4,659 for testing. Each sample is labeled with a sentiment valence from -3 (strongly negative) to +3 (strongly positive). As is common in studies using this dataset, we primarily use Mean Absolute Error (MAE) and prediction Accuracy (ACC) as metrics to evaluate model performance."}, {"title": "B. Implementation Details", "content": "In our experiments, we use MANet [30] as the visual encoder, WavLM-Large [24] as the audio encoder, and Vicuna- 7B [25] as the text encoder. The dimension of the visual representation obtained from the visual encoder is 1024 frame- level features. And we use the output of the 20th hidden layer of WavLM-Large as the audio representation and the output of the last hidden layer to obtain the simulated text representation. When text is present, the sum of the last four hidden layers of Vicuna is used as the text representation. When the text modality is missing, the output of the last hidden layer of the audio encoder is fed into the pre-trained Projector and Vicuna to generate the hidden state, and the penultimate fourth layer of this hidden state is taken as the simulated text representation."}, {"title": "C. Ablation Study", "content": "To evaluate the effectiveness of the missing modality imag- ination module, the text modality missing self-distillation, and the loss functions, we design a series of ablation experiments to illustrate the role of each component in the architecture. The results are shown in Table I, where LLM, indicates that the LLM-based model generated text representations when the text modality is missing. When LLMg is not used, the model does not use the text modality representations simulated by LLM, but only uses the audio and visual representations for modality imagination and inference."}, {"title": "D. Overall Comparison", "content": "We compare our method with other high-performing ap- proaches that have achieved excellent performance or studied modality missing. In detail, the models listed in Table II explore robustness against missing modalities: GMC uses contrastive learning to align representations, GCN employs graph neural networks, GMD reduces modality dependence via Gradient-guided Decoupling, MMIN reconstructs missing modalities, and MInD separates modality-invariant informa- tion and modality-specific information. We referenced the experimental results of these models obtained from articles on the GMD method. The results are shown in Table II.\nFirst, compared to studies conducted under complete modal- ities such as UniMSE [31], AMB [32], and ALMT [13], our model achieves significantly better MAE and ACC perfor- mance. Second, when the text modality is missing, our model\u2019s MAE increases by only 0.044, and the accuracy decreases by just 3.4%, outperforming other studies on modality-missing robustness. Additionally, the performance decay of our model is an order of magnitude less than that of other models when the text modality is missing, demonstrating its superior design."}, {"title": "IV. CONCLUSIONS", "content": "Our work addresses the challenge of the high cost of acquiring text modality and the strong reliance of multi- modal sentiment analysis on it. Specifically, we designed the Double-Flow Self-Distillation Framework with Unified Modal- ity Cross-Attention as the main network structure, combined with Modality Imagination Autoencoder to simulate missing text modality. Additionally, we developed a series of loss functions to enhance the model\u2019s performance in multimodal sentiment analysis, ensuring robustness both when all modal- ities are present and when the text modality is missing."}]}