{"title": "LEARN: A Unified Framework for Multi-Task Domain Adapt Few-Shot Learning", "authors": ["Bharadwaj Ravichandran", "Alexander Lynch", "Sarah Brockman", "Brandon RichardWebster", "Dawei Du", "Anthony Hoogs", "Christopher Funk"], "abstract": "Both few-shot learning and domain adaptation sub-fields in Computer Vision have seen significant recent progress in terms of the availability of state-of-the-art algorithms and datasets. Frameworks have been developed for each sub-field; however, building a common system or frame-work that combines both is something that has not been explored. As part of our research, we present the first unified framework that combines domain adaptation for the few-shot learning setting across 3 different tasks - image clas-sification, object detection and video classification. Our framework is highly modular with the capability to support few-shot learning with/without the inclusion of domain adaptation depending on the algorithm. Furthermore, the most important configurable feature of our framework is the on-the-fly setup for incremental n-shot tasks with the op-tional capability to configure the system to scale to a tradi-tional many-shot task. With more focus on Self-Supervised Learning (SSL) for current few-shot learning approaches, our system also supports multiple SSL pre-training configu-rations. To test our framework's capabilities, we provide benchmarks on a wide range of algorithms and datasets across different task and problem settings. The code is open source has been made publicly available here: https://gitlab.kitware.com/darpa_learn/learn", "sections": [{"title": "1. Introduction", "content": "Up until 2018, with the advent of metric learning and meta-learning, the field of few-shot learning had little innova-tion with most methods revolving around pre-training and fine-tuning [55]. However, after 2018, there has been an exponential increase in the number of few-shot learning publications with the majority of them coming within the field of computer vision. There have been a number of datasets adapted specifically for the task of few-shot learn-ing such as mini-Imagenet [13], CIFAR-10 [30], and Meta-Dataset [59] which either subsample a larger dataset to re-strict it to a few-shot setting or combine smaller datasets to increase the number of available classes. There has al-ready been remarkable progress on these datasets with the accuracy on mini-Imagenet being pushed from 46.6% in 2016 with Matching Nets [63] to 95.3% in 2022 with the P>M>F algorithm [20]. While image classification was the first computer vision task to see real attention using the few-shot paradigm, both object detection and video classi-fication have also seen progress as both meta-learning and transfer learning have been applied with success [4, 7, 84].\nThe domain shift problem in computer vision is cru-cial when considering building systems around multiple datasets with varying image domains and class labels. Back in 2017, Tzeng et al. [60] proposed a simple-yet-efficient adversarial adaption baseline for domain transfer between the digit images of MNIST [35] and USPS [22]. Since then, the number of algorithms and datasets introduced as part of domain adaptation has seen a huge increase. For instance, in adversarial domain adaption, the MADA method [44] introduced a fine-grained alignment using mul-tiple domain discriminators (one for each class). Follow-ing this, many more methods like DADA [57], IDDA [33], RADA [70] were introduced to push state-of-the-art per-formance across different datasets such as Office-31 [51] and Office-Home [62]. Unsupervised and Semi-Supervised Domain Adaption paved the way for a whole set of meth-ods that uses Knowledge Distillation [41, 58, 72, 76, 80], Statistical Domain Alignment [14, 16, 25, 81], Contrastive Learning [1, 21, 67, 71]. In addition to the methods dis-cussed above, a large number of public datasets have been made available for the Domain Adaptation Image Classi-fication task [22, 35, 46, 51, 62]. Out of these, since the VisDA2019 [65] challenge, the DomainNet [46] dataset is considered as a standard to advance state-of-the-art Domain Adaptation algorithms for fine-grained classification.\nFor object detection and video classification, there are a number of public datasets used in combination for do-main adaptation. Cityscape [12], FoggyCityscape [53], Sim10k [26], and KITTI [39] datasets are a group of over-head imagery datasets that are commonly used to train do-main adaptation models using any two of the above men-tioned datasets as the source and target. [43, 78]. PASCAL VOC is also commonly used as a source dataset which is then adapted to ClipArt1K [24], Watercolor2K [24], and BDD100K [79] for object detection domain adaptation [43]. For video classification, UCF101 [56], HMBD [31], Kinet-ics [27], and ARID [73] are used interchangeably for adapt-ing from one dataset to another. These datasets are some of the oldest and most commonly used datasets for the video classification domain adaptation problem [74, 75, 77].\nTherefore, with the increasing interest in few-shot learn-ing and domain adaptation, there is a greater need for tools that can facilitate running few-shot and domain shift experiments. Existing few-shot learning frameworks like the LibFewShot [37] and learn2learn [3] provide a vari-ety of APIs across different components of the framework pipeline, but are focused on only the image classification task. In this paper, we showcase a new framework which is easy to use across multiple computer vision tasks and pro-vides APIs for algorithms to be applied in a domain-adapt few-shot setting.\nFurthermore, to provide a single cohesive workflow for training models in a variety of n-shot settings, we devel-oped the iterative active learning workflow with an increas-ing number of label instances budgeted either on a per class basis or in totality as described in Figure 2. This modular training loop includes domain adaptation steps for at least one algorithm per task giving users the option to perform domain adaptation regardless of the task being executed. In order to balance the different algorithm and general frame-work parameters, we use a customizable configuration pro-tocol in which the user specifies parameters to be used in their experiments either in default or through custom con-figuration files or as optional command line arguments for every possible parameter.\nBased on our knowledge about existing domain-adapt few-shot learning frameworks, we believe our proposed sys-tem is the first unified framework to\n\u2022 support multi-stage domain-adapt incremental n-shot learning, where n is the number of labels per class;\n\u2022 support different computer vision tasks - image clas-sification, object detection and video classification (with/without self-supervised pre-training based on the algorithm) - for different types of domain-adapt scenar-ios;\n\u2022 provide a modular and scalable way to extend to a many-shot task without having to restart a few-shot experiment from scratch.\nIn order to support these claims, we conducted extensive experiments on the 3 tasks mentioned above across differ-ent domain-adapt few-shot settings and have included the corresponding results in Section 6."}, {"title": "2. Related Works", "content": "Over the last few years, there has been significant develop-ment with respect to few-shot learning frameworks. Chada et al. [10] created a NLP framework named \"FewshotQA\" which leverages pre-trained text-to-text models for creating benchmarks for a Q&A chatbot. Wang et al. [69] introduced a few-shot framework for 1D data, specifically Signal Mod-ulation classification.\nIn regard to vision-based few-shot learning frameworks, Lin et al. [40] introduced a unified framework for image classification and object detection that supports episodic learning methods, meta-learning and fine-tuning across multiple source/target datasets and pre-trained models. In addition to this, the authors propose a meta-dropout to im-prove the generalization capability during the meta-training stage. The benchmarks reported from a wide range of al-gorithms are primarily on 1, 5-shot tasks on the CUB [66] and mini-ImageNet [64] datasets and 1,3,10-shot tasks on the VOC2007 [15] dataset. LibFewShot is a similar framework from Li et al. [37] which contains benchmarks across the different modes of few-shot learning, but focuses only on the image classification task. Furthermore, the framework supports K-way, n-shot task setups with bench-marks from different algorithms reported on the following datasets - mini-ImageNet [64], tieredImageNet [49], Stan-ford Dogs [28], Stanford Cars [29] and CUB [66]. Arnold et al. [3] introduced the learn2learn library which primar-ily focuses on Meta-learning few-shot benchmarks. The library consists of APIs to interact with PyTorch models and datasets and also contains high-level wrapper code for a bunch of existing meta-learning algorithms - MAML [17], Meta-SGD [38], MetaOptNet [36] and ProtoNets [83]. In contrast, our proposed framework is expanded to support"}, {"title": "3. The LEARN Framework", "content": "The Label-Efficient Active Resilient Networks framework (LEARN) is a unified framework for multi-task, domain-adapt few-shot learning. In the following subsections, we discuss the overall design and the experiment protocol con-figurations for the tasks and algorithms supported by the framework."}, {"title": "3.1. Experiment Protocol", "content": "Our framework is capable of running a variety of ex-periments for different tasks using different algorithms with a plethora of both general and algorithm specific hyperparameters. In order to give users access to the full breadth of experimental possibilities, the entire set of hyperparameters is exposed to the users to maximize ease of use. The experiment running protocol for the framework is configured with the help of Hydra [23] which provides a customizable way to configure experiment parameters. A sample Hydra command looks for the following parameters to start an experiment problem.tasks_to_run, image_classifier (or) video_classifier (or) object_detector (based on the algorithm).\nThese parameters point to individual JSON files con-taining the configuration details for the chosen task and algorithm respectively. Furthermore, selecting the source method pre-trained backbone weights is specified by domain_network_selector.params.set_source_data_network. One of the most important parameters is the problem.add_dataset_to_whitelist. This provides a list of source datasets to consider during the protocol setup and skips all other datasets to save on processing time."}, {"title": "3.1.1 Task Configuration", "content": "There are many parameters to select between the task, al-gorithm, and experiment specific parameters. In order to make it easier for users to specify each of these parameters, the LEARN framework has a built-in interface which fo-cuses on task JSON files that are defined locally. Each file contains the following task parameters:\n1. name: A unique string identifier for the task.\n2. problem_type: String value to denote 1 out of the 3 tasks supported by the framework\n\"image_classification\u201d, \"object_detection\" and \"video_classification\".\n3. stages: A list of JSON objects (where each object corre-sponds to one stage) containing the following fields:\n(a) name: Name of the domain-adapt stage - \"base\" or \"adapt\".\n(b) dataset: Name of the target dataset for the stage.\n(c) seed_budgets: A list containing the cumulative n-shot budgets (n labels per class), where each n-shot problem corresponds to the (n - 1)th checkpoint during training and inference.\n(d) label_budget: A list containing the cumulative la-bel budget across all classes (not n-shot). These additional labels are used during training after completing the n-shot training based off of the seed_budgets.\n4. whitelist: A list containing the names of the source datasets that may be used for task. The dataset names not mentioned in the whitelist are skipped by the system during source dataset and method setup.\n5. results file: Name of the results files under the default \"outputs/<current_date>/<experiment_start_time>\" path that contains the predictions for each checkpoint of each stage."}, {"title": "3.1.2 Algorithm Setup", "content": "As shown in Figure 2, each task has N stages and a \"source\" & \"target\" dataset for each stage. Each stage consists of the following four algorithm-related sub-stages below that oc-cur in an iterative manner based on the available label bud-get for each checkpoint. Furthermore, each of these sub-stages have their own dedicated sub-folders and are config-ured with default experiment parameters using YAML con-figuration files:\n1. Domain/Network Selection: Determine the source dataset and source pre-trained backbone based on the closest similarity to the chosen target dataset.\n2. Algorithm Selection: Choose the algorithm to run based on a specified task and given source & target dataset.\n3. Active Learning Query Strategy: Iteratively queries the unlabeled data that is most impactful for labeling based on the available label budget level (determined by label budget). By default, the querying algorithm does a random shuffle of the unlabeled target dataset and updates the training set based on the additional available label budget.\n4. Few-Shot Domain Adaptation: Perform the algorithm that creates a classification for the target task. After a particular budget level is reached, the adaptation stage performs an evaluation on the test data."}, {"title": "4. Algorithms", "content": "In this section, we briefly describe the representative al-gorithms on three tasks used in our framework. The al-gorithms discussed below are chosen based on their avail-ability for domain adaption scenarios (Image Classification) and self-supervised pre-training capability (for Object De-tection and Video Classification) and thus supported by the LEARN framework."}, {"title": "4.1. Image Classification", "content": "MetaBaseline [11] is a 2-stage architecture for few-shot im-age classification. In the first classification stage, a classi-fication model is trained using all the samples from base classes set and the last FC layer is removed to obtain the encoder. Following this, the average feature of per-class support set samples is computed and a given sample from a query set is classified using nearest-centroid with co-sine similarity as the distance metric. In the second meta stage, the cosine similarity range is scaled using an ad-ditional learnable scalar before applying softmax during Meta-Baseline training. The goal of Meta-Baseline is to verify the efficacy of the meta-learning objective over a whole-classification model.\nThe benchmarking for Meta-Baseline includes a ResNet-12 [18] backbone trained and evaluated on the miniIm-ageNet [64] and tieredImageNet [49] few-shot classifica-tion datasets. In addition to these benchmarks, ResNet-18 and ResNet-50 backbones are trained and evaluated on the ImageNet-800 [11] dataset.\nMME (MiniMax Entropy) [52] was proposed to effectively align feature distributions of the source and target domains under a semi-supervised domain adaptation setting. The al-gorithm uses an adversarial optimization approach by up-dating the classifier's estimated class prototypes to maxi-mize entropy on the unlabeled target domain and cluster features based on the estimated prototypes by minimizing entropy with respect to the feature extractor. As part of the maximization step, the entropy measured on unlabeled target samples shows the similarity between the estimated prototypes and target features. So, the weight vectors are modified to shift towards the target data by maximizing the entropy of the unlabeled target samples. Following this, as part of the minimization step, the feature extractor is up-dated to minimize entropy of the unlabeled target samples to enable better clustering around the estimated prototypes.\nThe proposed model consists of a feature extractor cre-ated by removing the last linear layer of an existing pre-trained model and adding a K-way linear classification layer with random initial weights. The feature extrac-tors experimented by the authors include AlexNet, VGG16 and ResNet34 loaded with ImageNet pretrained weights and adapted for cross domain scenarios using the Domain-Net [46], Office-Home [62] and Office-31 [51] datasets.\nPACMAC (Probing Attention-Conditioned Masking Con-sistency) [47] is a selection algorithm proposed for domain adaptation of self-supervised Vision Transformers (ViTs) to unseen domains. First, it performs in-domain self-supervised learning on a combination of source and target data to exploit task-discriminative features. Then, the algo-rithm selects reliable samples for self-training based on the predictive consistency across a set of attention-conditioned masks applied on a set of the target data.\nDuring the attention-conditioned masking stage, a greedy assignment strategy is employed to select the high attention patches followed by generating its corresponding set of disjoint masks. After this, a consistency-based relia-bility metric is computed based on the original and masked images for selecting target samples for self-training.\nBenchmark comparisons for the PACMAC system re-port target test set accuracy for three few-shot domain-adapt classification datasets - DomainNet [46], Office-Home [62] and VisDA2017 [45]. The model architecture consists of a ViT-Base model with 16x16 images patches, starting from two different pretrained backbones initialized with ima-genetlk [50] weights - MAE [19] and DINO [9]."}, {"title": "4.2. Video Classification", "content": "X-Clip [42] is an expansion of CLIP [48] specifically de-signed for the task of video recognition. The key contribu-tion is a cross-frame attention mechanism for encoding long range temporal dependencies across frames. This trans-former takes in raw frame data and generates frame level embeddings while allowing information to be shared across frames during the embedding process. A second \"multi-frame integration transformer\" [42] is then used to fuse those frame-level embeddings and output a single embed-ding for the whole video.\nThe X-Clip algorithm then uses a learned text encoder to augment the raw label text embeddings. This text en-coder first uses a multi-head attention layer to create a joint embedding of the encoded raw label text and the cohesive video embedding. This text-video embedding is then fed through a FFN (feed forward network) and outputs a text embedding based on the raw label that is specific to the con-text of the provided video.\nFinally a cosine similarity is calculated between the aug-mented label text embedding and the full video embedding. The goal of this training is to maximize the cosine similarity between these embeddings.\nTimeSformer [5] implements a transformer architecture that enables feature learning across the temporal and spatial dimensions. The algorithm takes as input a set of frames and then splits each frame into a set of N non-overlapping square patches. The patches are then mapped to an embed-ding vector through a learned embedding matrix, and these embeddings are then fed as inputs into the transformer.\nThe authors experiment with four different variations of self-attention blocks, but the one that achieves the high-est success and the one we focus on is referred to as the \"Divided Space-Time Attention (T +S)\"[5] block. In this block, temporal attention is calculated first by comparing each patch with the patch at the same location across all the given frames. The encoding from the temporal attention block is then fed back into the spatial attention block which compares all of the patches across a single frame. The en-coding given from the spatial attention block is then passed through a final multi-layer perceptron (MLP) module to get the final encoding for a given patch.\nCoMix (Contrast and Mix) [2] uses contrastive learning by maximizing the similarity between both the same video played at different playback speeds as well as different videos played back at varying playback speeds. Sets of fast (f) and slow (s) clips are used as inputs to a feature encoder which maps them to a set of feature encodings. A temporal graph encoder is then used to create a fully connected graph on top of the clip-level feature embeddings. The graph rep-resentation is then fed into a graph convolutional network which produces video classification output with per-class confidences.\nAnother unique contribution of CoMix is the mixing of background frames in a video. Synthetic videos created by cross-domain background frame mixing has the action semantics of the original video but also consists of frames from a different domain. This allows additional positive ex-amples to be created for any given video to be utilized in the contrastive learning process."}, {"title": "4.3. Self-Supervised Object Detection Pre-training", "content": "DETReg [4] While most object detection pre-training al-gorithms focus on just training the backbone in the pre-training step, DETReg [4] pre-trains both the embedding backbone as well as the localization head by carrying out an object localization task as well as an object embedding task. DETReg's object localization pre-training \u201cuses simple re-gion proposal methods for class-agnostic bounding-box su-pervision\" by using the Selective Search [61] method. Se-lective search uses various visual cues to generate a set of bounding boxes around the predicted objects in the image excluding any class predictions. The object localization task takes a set of boxes output by the selective search unsuper-vised region proposal network (RPN) and optimizes a loss that minimizes the difference between the boxes output by the detector and the boxes generated from the RPN.\nWhen pre-training the object embedding task, the en-coders learn transformation-invariant embeddings so that the detectors trained are robust to variation in image trans-formations such as cropping or translation. To accomplish this, a pre-trained SwAV [8] is used to generate a ground truth feature embedding to train the object detection classi-fication head.\nCutLER (Cut and Learn) [68] is agnostic to the underly-ing detector being used. It is comprised of two sections that are repeated for multiple rounds. First CutLER uses a novel expansion of the NCut [54] algorithm, referred to as MaskCut, that iteratively runs through the NCut algorithm detecting x objects where x is 3 by default. The MaskCut algorithm produces a series of binary masks for each object detected. After generating the set of binary masks, a loss function called DropLoss is applied. DropLoss is specifi-cally designed to encourage the discovery of the \u201cground-truth\u201d objects that were not discovered by the initial Mask-Cut algorithm. It accomplishes this by dropping the loss for each predicted region that has an overlap greater than or equal to a set threshold.\nBy using this coarse mask generation from MaskCut and the DropLoss function that encourages new objects to be detected, multiple rounds of this self-training process is ap-plied to steadily increase the number of \u201cground-truth sam-ples\u201d obtained from a given image."}, {"title": "5. Datasets", "content": "The datasets discussed below are commonly used public datasets that vary in terms of the granularity of the classes and also provide the domain adaptation scenarios (for Im-age Classification) that could be used to evaluate our frame-work's capabilities.\nDomainNet [46] consists of about 600k images with 345 overlapping categories across 6 different image domains: Real, Sketch, ClipArt, Quickdraw, Painting, and Infograph. Figure 3(A) shows image samples for the \"Aircraft Carrier\" class across the different image domains. For our experi-ments, we consider 3 out of the 6 domains (Real, Sketch, ClipArt) with all classes included.\nOffice-Home [62] consists of about 15.5k images with 65 overlapping categories across 4 domains. Figure 3 (B) con-tains examples for the \"Alarm Clock\" class from the differ-ent image domains. For our benchmark comparisons, we will be using the image samples from the Real, Art and Cli-pArt domains.\nOffice-31 [51] consists of totally 4.1k images with 31 over-lapping categories across 3 domains. Figure 3 (C) contains examples for the \u201cBackpack\u201d class from 3 domains - Ama-zon, DSLR, and Webcam.\nPoolCar [6] is made up of overhead imagery with 2 classes labeled (Pool and Car). The dataset is split into a training set with 2, 998 images and a test set with 750 images. Due to the presence of only two classes, the regression task of predicting the bounding boxes is much more challenging than the classification task of identifying the category of an object.\nxView [34] contains over 1 million individual object in-stances labeled as one of 60 classes. The sizes of these labeled object vary highly ranging from objects 10 pixels wide to 10,000 pixels wide. Additionally xView provides a granularity of classes as 80% of the classes are specific sub-classes of a given parent class, e.g., \u201cPickup Truck\u201d, \"Utility Truck\".\nUCF101 [56] contains over 27 hours of video with over 13k clips annotated as one of 101 classes. These 101 classes can be broken down into 5 major categories that are shown in Figure 4. Released in 2012, it is one of the most bench marked activity classification datasets and contains all pub-lic data."}, {"title": "6. Evaluation", "content": "6.1. Framework and Experiment Setup\nThe LEARN framework is structured as specified in Sec-tion 3. Before that, to install the framework, there is a README that goes through a series of python conda and pip installations that sets up the dependencies based on a \"requirements.txt\" file and installs the general dependencies of the framework. In addition to this, each algorithm folder has specific dependencies installed as part of the setup pro-cess.\nWith the help of the configurations discussed in Sec-tion 3, we have a straightforward way of setting up the dif-ferent task and algorithm parameters needed for a specific experiment. Furthermore, using the Hydra command, we can override a wide range of experiment parameters during runtime."}, {"title": "6.2. Result Analysis", "content": "In our experiments we are able to train successive models in as many few-shot domains as we like, scaling up from train-ing on a single instance of each class in a dataset to training on the full dataset in a single experiment. The ability to run a multitude of n-shot settings in a single experiment as opposed to having a user run each experiment individually is both task and algorithm agnostic. This functionality pro-vides a benefit in both efficiency and scalability to the user as it allows them to have a single run that encapsulates the full breadth of few-shot experiments that they would want to run.\nTable 2 provides an overview of the results across the different tasks, algorithms, and datasets for different incre-mental label budgets ranging from 1-shot to the full size of the training set. Using our framework we show that it is pos-sible to train effectively in the few-shot setting on a number of different datasets across multiple domains and tasks. In total, we train 8 networks on 6 different datasets. In each of our few-shot experiments we show that the model scores at least 79% of the accuracy that is achieved on the full dataset with certain domains showing even more progress in the few-shot setting. For the Video Classification task, for example, the models trained on 10 samples from each class scores 98% of the accuracy that is achieved on the full dataset. This can be attested to the strength of the algo-rithms and simplicity of the datasets, but it showcases the high level of accuracy that can be achieved even through training on a small number of examples. Figure 7 contains some sample results for the 1-shot and 4-shot settings using the TimeSformer [5] method on the UCF101 [56] dataset. This example shows that our framework can also handle a custom n-shot (4-shot) setting apart from the 1,2,5 and 10-shot results discussed in Table 2.\nOur object detection models perform quite well in the few-shot setting. DETReg [4] and CutLER [68] are able to achieve a mAP of nearly 0.5 on the PoolCar [6] dataset with only 2 labels per class as shown in Table 2. Qualitative re-sults for the CutLER pre-training algorithm on PoolCar and xView [34] in the 1-shot and 5-shot settings can be seen in Figure 6. Our model is able to detect most of the objects on both datasets with very few training labels. The model does particularly well with the \u201cSmall Car\u201d and \u201cBuilding\u201d detections, which is expected as these are the two most com-mon classes in xView. The model has some false positives on what appear to be shipping containers, which may in fact be missing ground truth (i.e. not actually false positives).\nWe also showcase our use of domain adaptation for the Image Classification problem. In Table 2, we show bench-marks on MetaBaseline [11], MME [52] and PACMAC [47] methods for the Domain-Net [46] Clipart \u2192 Sketch, Office-Home [62] ClipArt \u2192 Art and the Office-31 [51] Webcam \u2192 DSLR domain-adapt scenarios. Figure 5 shows sam-ple images and predictions for the 1-shot and 5-shot image classification tasks using the PACMAC [47] algorithm. Fig-ure 5(A) shows the top-1 predictions comparison between the 1-shot and 5-shot tasks for the same image sample from the DomainNet Sketch dataset. Figure 5(B) shows a similar comparison for the Office-Home Art dataset.\nFor the DomainNet [46] dataset, PACMAC [47] has the best accuracy with the accuracy score ranging between 48.8% to 56.6% for 1-shot through 10-shot tasks. PACMAC also achieves perfect accuracy on the Office31 [51] dataset. However, for the Office-Home [62] dataset, MME [52] out-performs the other algorithms on the different n-shot tasks. Figure 5(A) shows some interesting misclassified results in 1-shot that are corrected in the 5-shot task. Out of those examples, the \"Beach\" prediction in the 1-shot task is rea-sonable since the model would have focused on the water and land elements in the image. Another interesting exam-ple in Figure 5(B) is the one that is misclassified in both 1 and 5-shot tasks - \"Candles\" - where both tasks focus on the illumination in the silhouette image."}, {"title": "7. Conclusions and Lessons Learned", "content": "Customizability. The Hydra config protocol provided the basis for customizing the framework across different tasks, datasets and algorithms with the help of fine-grained pa-rameters. For example, for the zero-shot image classifica-tion task, we were able to do a straightforward customiza-tion of the original image classification task to add a flag to identify if the task is a zero-shot task or not. The frame-work is thus built with a level of modularity that is scalable to adding new computer vision tasks. When we consider datasets, our framework has the capability to include mul-tiple domain adaptation stages with each stage having an on-the-fly specification of the k-shot task, where k is spec-ified as part of the seed budgets for a particular stage. In addition, there is flexibility in terms of the label budgets to extend the framework from a few-shot system to a many-shot classification/detection system.\nUsability. An important aspect to any system is that it behaves the same way for each user. In the context of a training system, this means that all of the packages and tools that the system utilizes must be the same across all devices it is running on. The LEARN system accomplishes this by including a general requirements file specifying pack-age and tool versions as well as specific package files for each of the algorithms that require additional dependencies. This delineation allows an end user to create either a sin-gle reproducible environment to run multiple algorithms or to create algorithm specific environments. Either way this guarantees the consistent training and use of the LEARN system through consistent dependency specifications."}, {"title": "8. Future Work", "content": "The proposed LEARN framework has numerous ways of extending capabilities with respect to the domain-adapt few-shot learning problem. One future feature priority would be the capability to setup thousands of randomly ini-tialized episodic N-way tasks where N is the number of classes. In terms of experiment reliability, it is hard to keep track of all the customizable parameters while setting up the Hydra command and can lead to setting up wrong parameter values or sometimes overriding the same parameter twice. To help with this, building a simple GUI for setting param-eters would be very a useful extension for the framework."}]}