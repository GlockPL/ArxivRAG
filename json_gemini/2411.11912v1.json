{"title": "FOCUS - Federated Finetuning of Vision-Language Foundation Models with Optimal Client Layer Updating Strategy via Multi-objective Meta-Heuristics", "authors": ["Pramit Saha", "Felix Wagner", "Divyanshu Mishra", "Can Peng", "Anshul Thakur", "David Clifton", "Konstantinos Kamnitsas", "J. Alison Noble"], "abstract": "Effective training of large Vision-Language Models (VLMs) on resource-constrained client devices in Federated Learning (FL) requires the usage of parameter-efficient fine-tuning (PEFT) strategies. To this end, we demonstrate the impact of two factors viz., client-specific layer importance score that selects the most important VLM layers for fine-tuning and inter-client layer diversity score that encourages diverse layer selection across clients for optimal VLM layer selection. We first theoretically motivate and leverage the principal eigenvalue magnitude of layerwise Neural Tangent Kernels and show its effectiveness as client-specific layer importance score. Next, we propose a novel layer updating strategy dubbed F3OCUS that jointly optimizes the layer importance and diversity factors by employing a data-free, multi-objective, meta-heuristic optimization on the server. We explore 5 different meta-heuristic algorithms and compare their effectiveness for selecting model layers and adapter layers towards PEFT-FL. Furthermore, we release a new MedVQA-FL dataset involving overall 707,962 VQA triplets and 9 modality-specific clients and utilize it to train and evaluate our method. Overall, we conduct more than 10,000 client-level experiments on 6 Vision-Language FL task settings involving 58 medical image datasets and 4 different VLM architectures of varying sizes to demonstrate the effectiveness of the proposed method.", "sections": [{"title": "1. Introduction", "content": "Large Vision-Language Models (VLMs) have made significant advancements in multi-modal learning, excelling in tasks like Visual Question Answering (VQA) [9, 35, 45, 46, 51]. Their effectiveness stems from their extensive parameters often reaching millions or billions, allowing them to learn complex representations of image and text data. Fine-tuning these models with task-specific data is crucial for adapting them to specialized applications. However, gathering diverse training data centrally is challenging, especially in fields like healthcare, where strict privacy regulations prevent data aggregation across different centers. To address the privacy concerns, Federated Learning (FL) [2, 34, 47, 55] allows models to be trained directly on local devices, such as in healthcare clinics, without sharing sensitive data. Yet, fine-tuning large models locally is difficult due to limited computational power and smaller datasets, which hinders VLM adaptation.\nBalancing privacy with these resource limitations requires innovative solutions like Parameter-Efficient Fine-Tuning (PEFT) that fine-tunes either selected model parameters or added parameters while keeping the original model fixed [8, 24, 30, 32, 43, 48, 49, 60, 70]. Combined with FL, these offer a privacy-preserving and resource-efficient strategy for training large models collaboratively across multiple clients, particularly in computationally constrained settings.\nPrevious research [16, 65, 77, 79, 82, 87] has mostly focused on naive combination of centralized finetuning methods with FedAvg [55]. However, these works are primarily confined to single-modalities, addressing either visual or textual inputs independently. Besides, they do not consider the diverse characteristics and capacities of individual clients and typically assume homogeneous computational resources across all clients which is not applicable in most real-world collaborative settings. Hence, some clients either under-utilize the available resources or are unable to participate due to lack of compute. Our flexible layer selection PEFT-FL framework effectively addresses these issues.\nThe naive combinations of centralized selective PEFT methods [40, 42, 54, 61, 66, 72] and FL only consider local client data and task for selecting parameter subsets without considering other client requirements. This is especially problematic for FL clients facing challenges like heterogeneous modalities and computes, domain shifts, and statistical heterogeneity. In such cases, a poorly chosen client-specific configuration can not only slow down the overall convergence (see Fig. 2), but may also perform worse than training each client independently. Achieving the best performance requires a tailored approach that jointly considers client-specific as well as global optimization requirements.\nTo this end, we present a novel framework called FOCUS (Federated Finetuning of Foundation Models with Optimal Client-specific Layer Updating Strategy) to improve layer selection by considering both local and global FL characteristics while respecting the client-specific resource constraints. We propose a two-step \"define and refine\" procedure at the beginning of every round: (a) client-level strategy, that defines layer importance scores based on the principal eigenvalue of layerwise Neural Tangent Kernel (LNTK) and (b) server-level strategy, that refines client-specific layer selection by maximizing the overall client-specific importance scores while simultaneously minimizing the variance of the histogram of layer selections across clients, thereby promoting a more uniform distribution of layer participation. Our method (see Figs. 1 & 4) provides the clients with a flexible and dynamic solution for selecting layers where each client can specify their computational budgets, while ensuring faster convergence (see Fig. 2). In order to showcase the effectiveness of F3OCUS, we conduct over 10,000 client-level experiments under 6 Vision-language FL task settings using 4 VLMs and 58 medical image datasets that involve 4 types of heterogeneities based on data, modality, device, and task. Our primary contributions can be summed up as follows:\n\u2022 Dataset contribution: We release Ultra-MedVQA, the largest medical VQA dataset to date, consisting of 707,962 VQA triplets including 9 different modalities and"}, {"title": "2. Background and Related works", "content": "Federated Learning (FL) FL enables various clients to collaboratively train models in a decentralized manner without sharing local data. The classical FL framework, FedAvg [55], offers a practical method for model aggregation. Several modifications have emerged to address the adverse impact of data heterogeneity in FL [2, 34, 47].\nCentralized selective fine-tuning: Various methods have been explored for selecting subsets of parameters for fine-tuning foundation models in centralized training. These include optimizing non-structured mask matrices [36, 38, 39, 63, 76, 80, 83, 84], employing layer-wise selection [33, 36, 39, 42] and pruning methods [40, 44, 58, 72, 86].\nFederated selective fine-tuning: Recent research has adapted these selective PEFT methods for FL [16, 28, 56, 84]. Specifically, studies by [22, 41] explore layer-wise network decomposition to facilitate selective model fine-tuning on client devices. Partial model personalization algorithms [13, 57] aim to train tailored subnetworks on clients to improve local models. However, these studies do not provide adaptive or dynamic layer selection strategies that consider the diverse characteristics of clients. Unlike prior works, we account for client-specific differences in resources and data distributions while also considering the global optimization requirements to perform selective layer fine-tuning."}, {"title": "3. Problem Formulation", "content": "Consider an FL system with a central server and N clients, represented by N = {1,...,N}. Each client has its own private dataset Di, containing di = |Di| data points. The server contains a pre-trained foundation model parameterized by \\theta \\in R^p, comprising L layers, indexed by L = {1,2,...,L}. The server's objective is to fine-tune this model based on the clients' data {Di}ien without directly accessing these datasets. The learning goal is formalized as:\n$\\min_\\theta F(\\theta) = \\sum_{i=1}^{N} a_i F_i(\\theta),$ (1)\nwhere $a_i = \\frac{d_i}{\\sum_{j=1}^{N} d_j}$ denotes relative sample size, and $F_i(\\theta) = \\sum_{B_i \\in D_i} F_i(\\theta; B_i)$ represents local training objective for client i, with $F_i(\\theta; B_i)$ being the (potentially non-convex) loss function of model \\theta on data batch Bi. The FL training process proceeds over T rounds. In each round t\u2208 [T], the server selects a subset of clients St and distributes the updated global model \u03b8t to them for training.\nDue to resource constraints, instead of the entire model, clients update only a subset of layers during local training. Each client-specific masking vector can be denoted as $m_{i,t} \\in {0,1}^L$, where $m_{i,t}^l$ = 1 if layer l is selected for training in round t, and $m_{i,t}^l$ = 0 otherwise. Thus, the set of selected layers for client i at round t is denoted as $L_{i,t} = {l \\in L | m_{i,t}^l = 1}$, and the union of all selected layers across clients in round t is $L_t = \\cup_{i \\in S_t} L_{i,t}$.\nClients initialize their local models using global model from the server, \\theta_i^t, and perform \\tau steps of local training using mini-batch SGD. For each local step k \u2208 [\\tau], client i samples a batch of data Bi,t and calculates gradients for the selected layers as:\n$G_{i,t}^l(\\theta_{i,t}^k; B_{i,t}) = \\sum_{l \\in L_{i,t}} \\nabla_l F_i(\\theta_{i,t}^k; B_{i,t}),$ (2)\nwhere $\\nabla_l F(\\theta)$ denotes the gradient of F(\\theta) with respect to the parameters of layer l. After computing the gradients, the local model is updated with learning rate \\eta as:\n$\\theta_{i,t}^{k+1} = \\theta_{i,t}^k - \\eta \\sum_{l \\in L_{i,t}} G_{i,t}^l(\\theta_{i,t}^k; B_{i,t}), \\forall k \\in {1,2,...,\\tau},$ (3)\nThe accumulated weight update in one local round is:\n$\\delta_{i,t} = (\\theta_{i,t}^\\tau - \\theta_i^t) = \\eta \\sum_{k=0}^{\\tau-1} \\sum_{l \\in L_{i,t}} G_{i,t}^l(\\theta_{i,t}^k; B_{i,t})$ (4)\nAccumulated update after federated aggregation on server:\n$\\delta_t = \\sum_{i \\in S_t} \\sum_{k=0}^{\\tau-1} \\sum_{l \\in L_{i,t}} \\alpha_i G_{i,t}^l(\\theta_{i,t}^k; B_{i,t})$ (5)"}, {"title": "4. Client-level layer importance", "content": "In this section, we first motivate the usage of LNTK towards prioritizing selected layers for client-specific fine-tuning. To capture the model training dynamics, consider the evolution of \\theta_t and training loss L over input instances X \u2282 Di:\n$\\dot{\\theta_t} = -\\eta \\nabla_\\theta F(X;\\theta_t) \\nabla_\\theta F(X;\\theta_t)^L$\n$\\dot{L} = \\nabla_\\theta F(X; \\theta_t)^L \\nabla_\\theta F(X;\\theta_t)$\n$\\dot{\\theta_t} = -\\eta \\nabla_\\theta F(X; \\theta_t)^L \\Theta_t(X, X) \\nabla_\\theta F(X;\\theta_t)^L$ (6)\nwhere the NTK matrix \\Theta_t(X, X) at time t is defined as:\n$\\Theta_t(X, X) \\equiv \\nabla_\\theta F(X;\\theta_t) \\nabla_\\theta F(X;\\theta_t)^T \\in R^{n \\times n}$. (7)\nThe integral NTK of a model [31] can be computed as the sum of layerwise NTK (LNTK), where we define LNTK as:\n$\\Theta^l(X, X) = \\nabla_{\\theta_l} F(X;\\theta') \\nabla_{\\theta_l} F(X;\\theta')^T,$ (8)\nwhere $\\nabla_{\\theta_l} F(X; \\theta)$ denotes the Jacobian of F at input points X with respect to the l-th layer parameters \\theta. The integral NTK can be expressed as: $\\Theta(X,X)$\n$\\sum_{l=1}^{L} \\Theta^l(X,X) = \\sum_{l=1}^{L} (\\nabla_{\\theta_l} F(X;\\theta) )^T \\nabla_{\\theta_l} F(X;\\theta)$\n$\\sum_{l=1}^{L} (\\sum_{\\theta_l \\in \\Theta_l} \\frac{\\partial}{\\partial \\theta_l} F(X))^T (\\sum_{\\theta_l \\in \\Theta_l} \\frac{\\partial}{\\partial \\theta_l} F(X)) (ii)$\n$\\sum_{l=1}^{L} \\nabla_{\\theta_l} F(X)^T \\nabla_{\\theta_l} F(X)$ (9)\nwhere (i) decomposes the matrix multiplication into the sum of vector multiplications; (ii) gathers addends by each module; and (iii) follows the definition of the LNTK. Since $\\Theta^l(X, X)$ is a positive semi-definite real symmetric matrix, we perform an eigen-decomposition of LNTK as:\n$\\Theta^l(X,X) = U^l \\Lambda^l (U^l)^T = \\sum_{j=1}^{n_k} \\lambda_j^l (u_j^l)(u_j^l)^T$ (10)\nwhere $\\Lambda^l = diag(\\lambda_1^l, \\lambda_2^l, ..., \\lambda_{n_k}^l)$ contains eigenvalues $\\lambda_j^l$ of $\\Theta^l(X, X)$, and each $\\lambda_j^l \\ge 0$. The mean output of the l-th layer in the eigenbasis of the LNTK can be described by:\n$[U^l \\mathbb{E}[f(x)]]_j = (1 - e^{-\\eta t}) [U^l y]_j,$ (11)\nwhere f(X) and y are actual and target l-th layer outputs. This formulation demonstrates that the convergence behavior of a layer is largely influenced by the eigenvalues of LNTK. Let $\\lambda_1^l \\ge \\lambda_2^l \\ge ... > \\lambda_k^l$ denote the eigenvalues of $\\Theta^l(X, X)$, where $\\lambda_1^l$ is the principal eigenvalue. In particular, $\\lambda_1^l$ plays a dominant role in the convergence dynamics [10]. When using the largest possible layer-specific learning rate, $\\eta \\sim \\frac{1}{\\lambda_1^l}$, the training process aligns most strongly with the direction associated with the principal eigenvalue as the NTK eigenvectors corresponding to the principal eigenvalue are learned quicker due to spectral bias [10, 12, 59, 64]. This motivates the use of the principal eigenvalue $\\lambda_1^l$ in our client-specific layer importance score, as it represents the maximum alignment of each layer's parameter space with the client's data distribution. This provides a principled basis for prioritizing these layers.\nFor a step of gradient descent, the loss reduction can be characterized by the directional derivative of the loss:\n$\\Delta L \\approx \\lim_{\\epsilon \\to 0} \\frac{L(\\theta + \\epsilon \\nabla_\\theta L(\\theta)) - L(\\theta)}{\\epsilon} \\approx (\\frac{\\partial}{\\partial \\theta})^T \\nabla L(\\theta) \\nabla_\\theta L(\\theta)$\n$\\approx \\sum_{l=1}^{L} \\nabla_\\theta L(\\theta)^T (\\nabla_\\theta F(\\theta) \\nabla_\\theta F(\\theta)^T ) \\nabla_\\theta L(\\theta)$\n$\\approx \\sum_{l=1}^{L} \\nabla_\\theta L(\\theta)^T (\\sum_{j=1}^{link} \\lambda_j^l (u_j^l)(u_j^l)^T ) \\nabla_\\theta L(\\theta)$\n$\\approx \\sum_{l=1}^{L} (\\sum_{j=1}^{link} ((\\nabla L(\\theta))^T (u_j^l))^2 ) \\lambda_j^l$, (12)\nwhere (i) follows the definition of the directional derivative; (ii) follows the first-order Taylor expansion; (iii) applies the chain rule of derivatives; (iv) follows from Eq. (8); and (v) follows the eigen-decomposition of the layerwise NTK under the assumption of squared error loss. Assuming that the true labels align well with the top eigenvectors as\ndiscussed earlier, i.e., $((\\nabla L(\\theta))^T (u_j^l))^2$ is large for large $\\lambda_j^l$, directional derivative of the loss function can be regarded as closely related to the eigenspectrum of the layerwise NTKs. Specifically, (vi) suggests that layers with higher principal eigenvalues contribute more significantly to loss reduction during training. Overall, Eq. 12 suggests that the loss decreases more rapidly along the eigenspaces corresponding to larger LNTK eigenvalues. Given that the principal eigenvalue $\\lambda_1^l$ vary across layers as seen in Fig. 3, we propose to selectively fine-tune layers with larger $\\lambda_1^l$ to achieve efficient learning with limited computational resources.\nTherefore, we define the client-specific layer importance score $S^l = \\frac{\\lambda_{i,1}^l}{\\sum_{k=1}^{L} \\lambda_{i,1}^k}$, where the sum in the denominator normalizes the principal eigenvalue across all layers, ensuring that $S^l$ captures the relative importance of each layer for client i in terms of its contribution to the model's predictive capacity for that client's data distribution. This formulation prioritizes layers whose NTK principal eigenvalues dominate, indicating strong client-specific parameter alignment (see Figs. 3 & 5). See Algorithm in Suppl. \u00a7A."}, {"title": "4.2. Convergence Analysis of LNTK", "content": "We begin with some necessary assumptions following previous works [34, 73] and then introduce a set of assumptions to analyze the impact of the layers selected using LNTK:\nAssumption 1: (\u03b3-Smoothness) There exists a constant \u03b3 > 0 such that for any \u03b8, \u03c6 \u2208RP:\n$||\\nabla F_i(\\theta) - \\nabla F_i(\\phi)||_2 \\le \\gamma||\\theta \u2013 \\phi||_2, \\forall i \\in N$. (13)\nAssumption 2: (Unbiased and variance-bounded stochastic gradient) The layerwise stochastic gradient $G_{i,l}(\\theta_t; B_{i,t})$ computed on a randomly sampled data batch $B_{i,t}$ serves as an unbiased estimate of the layerwise full-batch gradient:\n$\\mathbb{E}_{B_{i,t}} [G_{i,l}(\\theta_t; B_{i,t})] = \\nabla F_{i,l}(\\theta_t)$. (14)"}, {"title": "5. Server-level Overall Layer Importance", "content": "To explicitly examine the impact of potential noise introduced by LNTK-based layer selection as well the variation of selection count across clients, we reformulate the convergence in Theorem 2 leveraging two additional assumptions:\nAssumption 4: (Bounded stochastic gradient) The expected squared norm of stochastic gradients is bounded uniformly, i.e., for constant $\\sigma_g > 0$ and any i, t:\n$\\mathbb{E}_{B_{i,t}} [||G_i(\\theta_t; B_{i,t})||^2] < \\sigma_g^2$. (18)\nAssumption 5: (Normalized Layer Selection Noise Bound) There exist some $\\xi^2 \\in [0,1)$ and any t, i, the normalized layer selection noise due to LNTK is bounded by:\n$\\frac{||\\tilde{\\theta}_t - \\theta_t^i ||^2}{||\\theta_t||^2} < \\xi^2$ (19)\nwhere $\\tilde{\\theta}_t^i$ denotes LNTK-based client-specific layers.\nTheorem 2 (Impact of layer selection-based noise \\xi and variance of selection count $s_d^2$ on LNTK convergence):\n$\\min_{t \\in [T]} \\mathbb{E} [||\\nabla F(\\theta_{t+1})||^2] \\le \\frac{2(\\eta - 3 \\gamma \\eta^2)T}{[\\frac{2}{N}(1 + 3 \\eta \\gamma) \\mathbb{E} \\sum_{t=1}^{T} F(\\theta_t) ]} \\frac{F(\\theta^0) - F(\\theta^*)}{T}$\n$+ (\\eta \\gamma \\sigma (1 + 3 \\gamma) + 3 \\eta \\gamma^2 \\sigma_g^2 + \\eta \\xi^2 \\sigma^2 NT)$ (20)\nRemark 2: With $\\eta \\le min{\\frac{1}{\\gamma}, \\frac{1}{27}}$, model converges to a neighborhood of a stationary point of FL with a small gap due to layer selection-based noise \\xi and variance of selection count $s_d^2$ over clients. This motivates us to jointly minimize the influence of $\\xi$ and $s_d$ for better convergence."}, {"title": "5.2. Multi-objective Optimization", "content": "Motivated by this, we refine the selected layers on server (see Fig. 6) to achieve two primary objectives (see Eq. 21): maximizing the cumulative client-specific importance scores based on LNTK and minimizing the variance of layer selection histogram (see Fig. 7) to encourage more balanced usage of each layer over clients. Let $n_l$ represent the number of clients that select layer l, with $\\bar{n} = \\frac{1}{L} \\sum_{l=1}^{NL} n_l$ as the mean count of layer usage. (See Suppl. \u00a7A for more details.) The joint optimization problem is formulated as:\n$\\max S = \\sum_{i=1}^{I} \\sum_{l=1}^{L} S_i^l$ \n$\\min s_d^2 = \\frac{1}{L} \\sum_{l=1}^{L} (n_l - \\bar{n})^2$ (21)\nThe latter objective prevents over-reliance on specific layers even if they have high importance scores thereby increasing diversity in layer selection across clients. The constraint incorporates client-specific computational budget Li,max.\nTraditional optimization methods struggle to optimize these two conflicting objectives. Neural networks cannot be employed to optimize due to absence of data on server. Besides, the number of possible configurations is particularly high due to the large number of layers in foundation models. Since this problem involves multiple objectives, there is no \"best\" solution but rather a set of optimal trade-offs (the Pareto front). Meta-heuristic algorithms are well-suited to explore such complex, high-dimensional solution spaces in absence of training data by balancing exploration and exploitation. To this end, we carefully select and investigate 5 meta-heuristic algorithms spanning all 3 algorithm categories: evolutionary, physics-based, and swarm-based. See Suppl. \u00a7A for detailed description and pseudo-code."}, {"title": "5.3. Meta-heuristic algorithms", "content": "Each of the following algorithms aim to maximize client-specific importance while ensuring balanced layer utilization across clients based on their underlying principles:\n(1) Non-Dominated Sorting Genetic Algorithm (NSGA): We use NSGA [20] to iteratively evolve a population of layer selections. We initialize population based on client-specific importance scores while incorporating probability-based sampling for broader search space coverage. This guided randomness helps balance exploration (diversifying choices) and exploitation (prioritizing high-importance layers). Genetic operations like crossover and mutation generate new solutions, while non-dominated sorting and crowding distance ensure diversity on the Pareto Front.\n(2) Artificial Bee Colony (ABC) [4]: Each bee represents a potential layer selection for the clients. The optimization proceeds in three phases: Employed bees exploit local solutions, adjusting layer assignments based on importance scores and diversity, occasionally accepting worse solutions to escape local optima. Onlooker bees then select solutions based on a probability weighted by importance and diversity, while Scout bees abandon unproductive solutions, replacing them with randomly generated ones to promote exploration. Non-dominated solutions are stored in a Pareto archive, which guides refinement over iterations.\n(3) Ant Colony Optimization (ACO) [21]: Each ant represents a candidate layer selection, constructing paths influenced by pheromone trails (which reinforce successful layer choices from previous iterations) and importance scores (which guide ants toward layers that are likely beneficial based on client-specific requirements). The Pareto archive preserves non-dominated solutions. Pheromone trails are then updated, with evaporation to prevent stale paths and additional pheromone deposits on layers selected, thereby encouraging their selection in subsequent iterations.\n(4) Simulated Annealing (SA): SA [71] starts with a high-temperature, randomly initialized solution, gradually exploring optimal layer selections by cooling over iterations. It accepts new configurations if it offers higher importance or lower variance based on Eq. 21. If the new configuration is less optimal, it may still be accepted based on a probability that decreases with the temperature allowing SA to avoid being trapped in local optima. As the temperature cools, SA gradually refines the search, focusing on fine-tuning layer assignments that respect both client-specific importance and a balanced distribution of layer usage.\n(5) Multi-Objective Particle Swarm Optimization (MOPSO) [18]: Each particle in the swarm represents a candidate layer assignment, initialized with client-specific"}, {"title": "6. Experiments and Results", "content": "We evaluate our performance for fine-tuning selected (i) layers, and (ii) adapters [29] with 4 VLMs of varying size and architecture, viz., ViLT [35], ALBEF [45], LIAVA-1.5 [51], and BLIP-2 [46], for 3 FL task settings: (a) Visual Question Answering, (b) Image and Text-based Disease Classification, (c) Heterogeneous tasks combining (a), (b).\n(a) Visual Question Answering: We consider 3 scenarios with data of varying sizes, class counts, and complexity:\n(i) Task 1 (with Domain gap): Five-client setting with SLAKE [50], VQA-RAD [37], VQA-Med 2019 [6], VQA-Med 2020 [1], and VQA-Med 2021 [7].\n(ii) Task 2 (with Modality gap): Modality specific 8-client setting with CT, Ultrasound, Dermatoscopy, Fundus, Histology, Microscopy, OCT, and X-Ray clients.\n(iii) Task 3 (with Modality gap): Modality specific 9-client setting with our Ultra-MedVQA dataset (see Fig. 8).\n(b) Image and text-based multi-label disease classification: We consider 2 FL settings [62] with Dirichlet coefficient y = 0.5 for Chest X-Ray and Radiology report-based multi-label disease detection (with 15 classes).\n(i) Task 4 (with label shift): 4 client-scenario with Open-I\n(ii) Task 5 (w/ label shift): 10 client scenario with MIMIC.\n(c) Heterogeneous tasks: We consider Task 6 (with task heterogeneity) combining three Visual Question answering clients, viz., SLAKE, VQA-RAD, VQA-Med 2019, and two disease-classification clients, viz., Open-I and MIMIC.\nDevice Heterogeneity: In Tab. 2 and 5, to simulate varying resource constraints among clients, we adjust the number of trainable layers across different tasks. For Tasks 1 and 6, 6 layers are finetuned for 2 clients, 4 layers for another 2 clients, and 2 layers for the remaining client. For Task 2, 6 layers are finetuned for 3 clients, 4 layers for 3 clients, and 2 layers for the last 2 clients. For Task 3, 2 layers are finetuned for 3 clients, 4 layers for another 3 clients and 6 layers for the last 3 clients. For Task 4, 6 layers are finetuned for 1 client, 4 layers for another client, and 2 layers for the remaining 2 clients. For Task 5, 6 layers are finetuned for 3 clients, 4 layers for 1 client, and 2 layers for the last client. See Suppl. \u00a7C for dataset and implementation details"}, {"title": "6.2. Performance comparison with State-of-the-arts", "content": "We compare F\u00b3OCUS with 28 SOTA methods:\n(i) Tab. 3 shows comparison with 5 SOTA PEFT baselines, viz., LayerNorm Tuning (LN) [5], LoRA [30], Bias Tuning [11], Prompt Tuning [25], and FedDAT [14] in terms of communication (MBits), Computation (GFLOPs), total number of trainable parameters (Millions) and accuracy in each client. FOCUS is observed to outperform all PEFTs except adapters [29] and FedDAT which finetune all adapters whereas F\u00b3OCUS finetunes only selected 4 adapter layers in each client leading to reduced communication (9.7 MBits) and computational needs (80.6 GFLOPs).\n(ii) In Tab. 6, F\u00b3OCUS is seen to consistently outperform 12 SOTA Personalized FL baselines viz. perFedavg [23], MetaFed [17], FedPAC [75], FedAS [78], FLUTE [52], FedALA [81], FedProto [68], FedRod [15], FedAP [53], FedFomo [85], FedRep [19], and Fedper [3] for 5 tasks.\n(iii) We adapt 7 SOTA Pruning baselines viz. Federated Dropout [74], Magnitude [26], FishMask [66], GradFlow [54], GraSP [72], SNIP [40], and Synflow [69] in our context and compare with proposed method in Tabs. 2 and 5. LNTK surpasses the performance of the closest SOTA pruning method by 2.11% and 2.30% while F\u00b3OCUS outperforms it by 5.35% and 5.33% respectively for homogeneous and heterogeneous device settings over all architectures.\n(iv) We also compare with 4 SOTA Layer Selection baselines viz. Adapter-drop (denoted as 'last') [61], RGN [42], Fedselect [67], and SPT [27] (see Tabs. 2, 5). From Table 2, we observe that LNTK outperforms the closest SOTA method by 2.08% and 2.17% for homogeneous and heterogeneous resource settings respectively. FOCUS further improves performance over LNTK by 3.24% and 3.03%."}, {"title": "6.3. Other Experimental Results", "content": "We plot the loss curves of LNTK and F3OCUS and compare them with several pruning methods in Fig. 2, which demonstrates the impact of our server-level optimization on faster convergence. We also visualize the principal eigenvalue magnitudes of all 32 layers of LLaVA across different clients at the beginning of a round for Task 1 in Fig. 3 and show the layer ranks over all rounds in Fig. 5. Consequently, the effect of server-level optimization on overall layer selections is shown in Fig. 6 using relative layer ranks and in Fig. 7 using the layer selection histogram. In Fig. 9, we motivate the current work by revealing that, contrary to common belief, fine-tuning the last 'K' layers is only as effective as fine-tuning random 'K' layers. Table 4 shows the comparable performance of different meta-heuristic algorithms for all clients in Task 2. For the 'microscopy' client in the same task, we present t-SNE feature visualizations in Fig. 10 for (a) Federated Dropout (labeled random), (b) LNTK, and (c) F\u00b3OCUS, highlighting the striking improvement in generating distinctive feature representations."}, {"title": "7. Conclusion", "content": "We presented F3OCUS, a novel and theoretically grounded approach for federated fine-tuning of Vision-Language foundation models in client-specific resource-constrained settings. F\u00b3OCUS effectively balances individual client requirements with collective layer diversity thereby improving model convergence and performance accuracy. Our server-level multi-objective meta-heuristic optimization scheme does not require any data on server and can be easily combined with any existing layer selection or pruning algorithms. Additionally, we released Ultra-MedVQA, the largest medical VQA dataset, covering 12 anatomies, for supporting further VLM research. Experimental results demonstrate that F\u00b3OCUS achieves superior performance across multiple vision-language tasks and models, providing a practical solution for fine-tuning large foundation models in decentralized environments. Our code and dataset will be made available upon acceptance."}]}