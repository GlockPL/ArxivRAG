{"title": "SECURE: Semantics-aware Embodied Conversation\nunder Unawareness for Lifelong Robot Learning", "authors": ["Rimvydas Rubavicius", "Peter David Fagan", "Alex Lascarides", "Subramanian Ramamoorthy"], "abstract": "This paper addresses a challenging interactive task learning\nscenario we call rearrangement under unawareness: to manip-\nulate a rigid-body environment in a context where the robot is\nunaware of a concept that's key to solving the instructed task.\nWe propose SECURE, an interactive task learning framework\ndesigned to solve such problems by fixing a deficient domain\nmodel using embodied conversation. Through dialogue, the\nrobot discovers and then learns to exploit unforeseen possi-\nbilities. Using SECURE, the robot not only learns from the\nuser's corrective feedback when it makes a mistake, but it\nalso learns to make strategic dialogue decisions for reveal-\ning useful evidence about novel concepts for solving the in-\nstructed task. Together, these abilities allow the robot to gen-\neralise to subsequent tasks using newly acquired knowledge.\nWe demonstrate that a robot that is semantics-aware\u2014that\nis, it exploits the logical consequences of both sentence and\ndiscourse semantics in the learning and inference process\u2014\nlearns to solve rearrangement under unawareness more effec-\ntively than a robot that lacks such capabilities.", "sections": [{"title": "Introduction", "content": "A central theme in human-robot interaction (Bartneck et al.\n2020) is to use embodied conversation (Cassell 2001) to in-\nstruct and teach robots to perform a variety of tasks (Shrid-\nhar, Manuelli, and Fox 2021; Ahn et al. 2022; Brohan\net al. 2023). In this pursuit, models for vision, language,\nand action have been developed to design generalist robotic\nagents (Shridhar, Manuelli, and Fox 2021; Ahn et al. 2022;\nZitkovich et al. 2023; Octo Model Team et al. 2024). One\nchallenge of using these agents is the interactive task learn-\ning (ITL) setting (Laird et al. 2017), in which the instruction\nmay feature a neologism (i.e., a newly coined expression)\nthat denotes a concept the robot is unaware of, yet must\nunderstand to perform the task. For example, consider the\nscenario in Fig. 1: a robot is instructed to \"Place the two\ngranny smith apples inside a basket\", but before deployment\nthe robot was not trained on and cannot correctly ground\nthe concept \"granny smith\", let alone distinguish granny\nsmith apples from other objects, including other kinds of\napples. Doing such fine-grained categorisation (Wei et al.\n2022) with limited experience (zero-shot or few-shot learn-\ning) might not be sufficiently captured in advance (Udan-\ndarao et al. 2024).\nMore generally, the task in Fig. 1 is an instance of rear-\nrangement (Batra et al. 2020) in which the rigid-body envi-\nronment is manipulated to a state that satisfies the specified\ngoal using sensory observations. In this work, we consider\na special case of rearrangement, in which the goal speci-\nfication is given as a natural language instruction, and the\nrobot is not aware of at least one concept required to un-\nderstand it. We refer to this class of tasks as rearrangement\nunder unawareness. To solve such tasks, the robot can en-\ngage in the embodied conversation with the instructor to\nlearn online how to recognise unforeseen concepts by in-\nteractively grounding new symbols. Compared to traditional\nsymbol grounding (Harnad 1999), this introduces two chal-\nlenges. First, learning must be incremental and should occur\nas and when the instructor says something. Second, learning\nmust cope with an expanding hypothesis space of concepts\nas information exchange in the embodied conversation may\nreveal unforeseen concepts to the robot. Consequently, the\nagent: (a) cannot assume mutual exclusivity between con-\ncepts; (b) it must detect false beliefs about the hypothesis\nspace; and (c) on discovering such false beliefs, it must adapt\nits prior beliefs to the newly expanded hypothesis space.\nA principal way to process embodied conversation is to\nparse it into a logical form in a symbolic logic that sup-\nports automated inference (Blackburn and Bos 2005). In\nthis work, we study how such logical reasoning can en-\nhance solving rearrangements under unawareness through\ninteraction. Our goal is to make the robot semantics-aware,\nby which we mean it can exploit the logical consequences\nof the logical forms of natural language to aid learning\nand inference. Previous work showed the value of sentence-\nlevel (Rubavicius and Lascarides 2022) and discourse-level\n(Appelgren and Lascarides 2020; Park, Lascarides, and Ra-\nmamoorthy 2023) logical semantics in ITL, but focused on\nscenarios with limited perception and dialogue moves.\nTo this end, we develop the ITL framework SECURE-\nSemantics-aware Embodied Conversation under\nUnawareness for Lifelong Robot Learning. It com-\nbines symbolic reasoning and neural grounding to process\nembodied conversation, so that it continually revises its\ndomain understanding and uses it to plan: it not only main-\ntains and revises beliefs, but also adapts to its discovery\nof unforeseen possibilities. It also learns how to engage in\nconversation, making decisions that resolve the dilemma\nbetween asking about the domain (which comes at a cost)\nor risking executing plans using its current beliefs, for\nwhich failure is costly. Furthermore, SECURE can process\ncorrections from the instructor during execution, use it to\nupdate its (false) beliefs and subsequently engage in further\nconversation. The framework is evaluated in a simulated\nblocksworld and real robot fruit manipulation on several\ninstances of rearrangement under unawareness, to test the\nvalue of semantics-awareness. The results show that using\nthe logical consequences of dialogue utterances aids ITL."}, {"title": "Related Work", "content": "Lifelong Learning Lifelong learning focuses on scenar-\nios in which an agent trained for tasks in one domain faces\nnew tasks in a new domain (Thrun and Mitchell 1995; Chen\nand Liu 2018). The aim is to use what it has learned so far\nto efficiently learn from subsequent evidence, even when\nit involves novel or out of distribution observations (Sil-\nver, Yang, and Li 2013). Reusability of previous experi-\nence in learning new tasks has motivated research in transfer\nlearning (Pan and Yang 2010), multitask learning (Caruana\n1997), and meta-learning (Vilalta and Drissi 2002). We are\nparticularly interested in the ITL case, where there is natural\ninteraction (embodied conversation in our case) between the\ntask executor and instructor (Laird et al. 2017). By design,\nsuch cases are not at odds with large multimodal models\n(Tan and Bansal 2019; Li et al. 2023; Driess et al. 2023), but\nrather provide the means of structure-level domain adapta-\ntion when agents are deployed in settings in which new con-\ncepts are frequently introduced or changed, requiring learn-\ning to adapt rapidly by using all possible incidental supervi-\nsion signals. (Roth 2017).\nSymbol Grounding Symbol grounding links objects to\nsymbols, given observable evidence like visual features\n(Harnad 1999). It has been extensively studied (Hu et al.\n2016b,a; Du et al. 2021), including in robotics (Matuszek\n2018). Many contemporary grounding models are not suffi-\ncient for handling unawareness in embodied conversations,\nbecause such scenarios impose additional constraints: learn-\ning must be active (Thomason et al. 2017), incremental\n(Spranger 2015; Yi, Kim, and Chernova 2022), able to han-\ndle a non-stationary domain (expanding vocabulary) (Fagin\nand Halpern 1987; Sugiyama and Kawanabe 2012), and be\nrevisable in case of false beliefs (Hansson 2022).\nLanguage-conditioned Manipulation Language can be\nused to specify goals for manipulation (Stepputtis et al.\n2020; Shridhar, Manuelli, and Fox 2021; Ichter et al. 2022)\nwith grounded representations (Jiang et al. 2023; Gkanat-\nsios et al. 2023) offering highly modular and generalizable\nsolutions. We extend such work by offering a way for such\nsystems to overcome unawareness and to adapt to the dis-\ncovery of unforeseen possibilities. SECURE is most simi-\nlar to Ren et al. (2023), which explores how planners can\nbe interactively repaired when predictions are confident but\nwrong. SECURE goes beyond this by: a) considering a com-\nplete embodied conversation that includes corrective feed-\nback; and b) explicitly learning a dialogue policy rather than\nperforming statistical tests (Angelopoulos and Bates 2021).\nLearning from Human Feedback Learning from human\nfeedback is designed to align system behaviour with human\npreferences (Christiano et al. 2017; Ziegler et al. 2019; Bai\net al. 2022) by integrating learning and acting during the\nfine-tuning phase or continuously through online, human-\nin-the-loop interactions (Mosqueira-Rey et al. 2023). Tradi-\ntional models primarily rely on sparse binary rewards, but\nSECURE leverages symbolic inference from semantically"}, {"title": "Framework", "content": "This section describes our ITL framework for solving rear-\nrangement under unawareness by processing embodied con-\nversation in a semantics-aware manner to interactively esti-\nmate the domain model.\nInteractive Domain Estimation\nPreliminaries The domain model M consists of a set of\nobjects U and an interpretation function $I: V \\rightarrow U^*$ that\nmaps symbols from the finite vocabulary V to their deno-\ntations (sets of objects or tuples of objects). In rearrange-\nment, the objects U are detected from localization, but the\ninterpretation function I is latent and has to be estimated\nusing visual observations and embodied conversation. Un-\nder unawareness V is also not fixed and expands over time:\nelements of V are incrementally discovered from the neolo-\ngisms in the embodied conversation. To estimate the domain\nmodel M, SECURE maintains a belief state b\u2208 B consist-\ning of: (a) a domain theory \u2206 that is a set of valid formula\n$\u03c6 \u2208 L$ in a first-order logic on a finite domain (Russell and\nNorvig 2020), a canonical logical form for natural language\n(Blackburn and Bos 2005) (the set L of well-formed formu-\nlae is recursively defined from V in the usual way); and (b)\nweights $w \u2208 [0,1]^H$, where H is the Herbrand base (Chang\nand Lee 1973) of all atomic propositions (here, an atom a is\na predicate symbol followed by a relevant number of terms),\nwith a weight for each a \u2208 H from interactive grounding\n(see the following section for details).\nReasoning SECURE reasons about the domain by com-\nputing probabilistic queries (Getoor and Taskar 2007) us-\ning weighted model counting WMC (Chavira and Darwiche\n2008), which treats the atoms a \u2208 H as Bernoulli random\nvariables Ber and uses the weights w to compute the prob-\nability that an arbitrarily complex formula $\u03a6 \u2208 L$ is true:\n$WMC(\u03a6, w) = Pr (\u03a6) = \\sum_{M: M \\models \u03a6}W(M, W)$\n$W(M, W) = \\prod_{i=1}^{|H|} [w_i1_{a_i\u2208H_M} + (1 \u2212 w_i)(1 \u2013 1_{a_i\u2208H_M})]$ (1)\nwhere $H_M$ is the set of atoms that are true in M (i.\u0435., $H_M$ =\n${a \u2208 H | M \\models a}$), and $1_{a_i\u2208H_M}$ is an indicator function\nwhich is 1 if $a_i \u2208 H_M$ and 0 otherwise.\nWMC is used to to compute complete evidence EVI\u2014\nthe probability that formula $\u03c6$ is true\u2014 and maximum a-\nposteriori MAP\u2014the most probable domain model M,\ngiven the belief state $b \u2208 B$:\n$EVI(\u03a6,b) = Pr (\u03a6 | \u2206) = \\frac{WMC(\u03a6 \\land \u2206, w)}{WMC(\u2206, w)}$ (2)\n$MAP(b) = M = arg \\max_M EVI (\u03a6(\u041c), b)$ (3)\nwhere $\u03a6(M)$ is the conjunction of atom assignments based\non the model M, namely $\u03a6(M) = \\bigwedge_{a \u2208H_M} a \\land \\bigwedge_{a \\notin H_M} \\neg a$.\nGrounding The weights $w \u2208 [0,1]^H$ of the belief state\n$b \u2208 B$ comes from the visual observations X \u2208 Rd, esti-\nmated using a grounding model g : Rd \u2192 $[0, 1]^H$. In ground-\ning, we first localize a patch x \u2208 Rd from X for each object\n$u \u2208 U$ in the domain and use an extractor f: Rd \u2192 R\u00b9 to\nextract an embedding that is used to predict a semantic vec-\ntor $\u0177 \u2208 [0,1]^{|V|}$ in which $\u0177_i$ estimates the probability that\nsymbol i \u2208 V is true for the corresponding object u. The es-\ntimation of the semantic vector is performed using a multil-\nabel prototype network\u00b9 h: R\u00b9 \u2192 $[0, 1]^{|V|}$(Yang et al. 2019;\nCano Sant\u00edn, Dobnik, and Ghanimifard 2020) that given a\nquery q \u2208 Rd and the support S = {(xi, yi)}=1 (previ-\nous interaction experience) computes positive/negative pro-\ntotypes $z_i^{+/-}$ for symbol i from positive/negative support\n$S_i^{+/-}$ used for logistic regression:\n$\u0177_i = h(q) = \\frac{1}{1+e^{-(z_i^{+}-z_i^{-})f(q)}}$\n$z_i^{+}= \\frac{1}{|S_i^{+}|}\\sum_{(x,y) \u2208S_i^{+}} y_if(x)$\n$S_i^{+}= {(x, y) \u2208 S | y_i > H_{Ber}(y_i) \u2264\u03c4}$ (4)\n$z_i^{-}= \\frac{1}{|S_i^{-}|}\\sum_{(x,y) \u2208S_i^{-}} (1-y_i) f(x)$\n$S_i^{-}= {(x, y) \u2208 S | y_i <= H_{Ber}(y_i) \u2264\u03c4}$\nwhere $H_{Ber}(y_i)$ is an entropy of Bernoulli random variable\n$y_i$, and \u03c4 is a threshold for adding exemplars to $S_i^{+/-}$. If for\nthe lack of evidence $S_i^{+/-}$\nare empty, $z_i^{+/-}$ defaults to ex-\nemplars with largest/smallest Bernoulli entropy. Semantic\nvector predictions for all patches extracted from the visual\nobservations specifies the weights w = [\u01771, \u01772,\u2026, \u0177|U|].\nHandling Unawareness In case a neologism is observed,\nSECURE handles becoming aware of it by updating its vo-\ncabulary and support. The vocabulary V increases by one:\n$V \u2190 V \u222a {v+1}$, which in turn increases the cardinality of H by\nU. For support S, it expands by adding an element to each\ny and assigning it an initial value $v \u2208 [0,1]$. Critically, the\ninitial value signifies the initial belief about symbol denota-\ntions. In case of genuine unawareness, it is 0.5, while in case\nof some prior beliefs before interaction, it might be different.\nInitial values for all weights are denoted as $v \u2208 [0, 1]^{|H|}$.\nBelief Update The beliefs are updated when a new valid\nformula $\u03c6 \u2208 L$ is observed, which gets integrated into the\ndomain theory and weights:\n$Update(b, \u03a6) = (\u2206 \u222a \u03c6, \u03b6(\u2206 \u222a \u03c6, v))$ (5)\nHere, $\u03b6$ denotes weight update, utilizing prior weights v and the\nupdated domain theory. It is done by dynamically updating"}, {"title": "Processing Embodied Conversation", "content": "the support S and in particular the elements of the semantic\nvectors y using the following update rule:\n$y_i \u2190 EVI (a_i, (\u2206\u222a\u03c6, v)) \\quad a_i \u2208 H$ (6)\nThis update computes a probability of a truth-value from\neach atom, stemming from the symbolic reasoning as well\nas prior beliefs v.\nNon-logical and Semantic Analysis SECURE updates\nits beliefs by constructing valid logic formulae $\u03c6 \u2208 L$\nfrom embodied conversation. E.g., these formulae are con-\nstructed whenever the instructor utters a referential ex-\npression r and points to its referent. \u03a6\u03b7\u03b9 captures the\nnon-logical semantics: e.g., in a domain with two objects\n{U1, U2}, saying \u201cevery granny smith\u201d, \u201ca granny smith\" or\n\"the one granny smith\u201d and pointing to u\u2081 yields \u03c6\u03b7\u03b9 =\ngrannysmith(41). Osem then captures the logical conse-\nquences (Russell 1917): for \u201cevery granny smith\" (universal\nquantifier) and \"the one granny smith\u201d (uniqueness quan-\ntifier), $\u03c6_{sem} = \\neg grannysmith(42)$, because they entail\ngrannysmith does not denote any object that wasn't des-\nignated.\nConversation before Execution When solving rearrange-\nments under unawareness, each task is given an instruction\nby an instructor (domain expert). Before trying to solve the\ntask using its (current) belief b, this option being the ac-\ntion act \u2208 A, SECURE can choose instead to utter a query\n$q \u2208 Q \u2286 A$, as a means to reduce uncertainty about the do-\nmain via embodied conversation. The queries are of the form\n\"show me r\" where r is a referential expression. The instruc-\ntor responds to such queries by pointing to a referent, which\nresults in a logical formula \u03c6 being added to \u2206 as described\nearlier: $\u03a6 = Result(q)$, where q \u2208 Q is the query. To main-\ntain coherence, the set of possible queries Q depends on the\ninstruction: specifically, r in the query can feature only non-\nlogical words used in the instruction, and its quantifier can\nbe replaced with an existential or universal. E.g., for instruc-\ntion \"move every cylinder to the left of the one cube\", SE-\nCURE can use the following referential expressions: \u201cevery\ncylinder\", \"the one cube\u201d, \u201ca cylinder\u201d, \u201ca cube\u201d and \"all\ncubes\".\nQueries have an inherent cost $C: Q \\rightarrow R^+$, which ap-\nproximates the instructor's effort to answer. It depends on\ntwo quantities: Obj(r) is the number of objects in the ref-\nerent of r and this approximates the pointing gesture effort;\nand Sym(r) is the number of symbols in the logical form\nof r and this approximates reference resolution effort. These\nquantities are weighted by unit pointing cost Cpoint and unit\nreference cost Cref, resulting in the overall cost function:\n$C(a) = C_{point}Obj(r) + C_{ref}Sym(r) \\quad r \u2208 a$ (7)\nFor example, r =\u201cthe one red cube\u201d cost is $C_{point} + 2C_{ref}$ as\nit features two symbols and the answer involves pointing to\none object. For the universal quantifier the number of objects\nin a referent is not known in advance as it ranges from 1 to\n|U|, thus is approximated by the average i.e., $|U|/2$.\nFor action act \u2208 A and successful task execution, the\nreward of 1 is observed and -1 otherwise, resulting to the\nfollowing reward function R: A \u00d7 B \u2192 [-1,1]:\n$R(a) =\\begin{cases}\n1 & if a = act and task success\\\\\n-C(a) & if a \u2208Q\\\\\n-1 & otherwise\n\\end{cases}$ (8)\nActions (queries and act) result in a valid formula (conjunc-\ntion of \u03a6\u03b7\u03b9 and $\u03c6_{sem}$) observed $\u03a6 \u223c Result(a)$ updating\nthe belief state Update(b, \u03a6). This improves the belief states\nas more information is acquired about the domain, result-\ning in a reduction of belief entropy $H(b) = H((\u2206, w)) =$\n$\\sum_{i=1}^{W} H_{Ber}(W_i)$. Using entropy, SECURE can measure ex-\npected information gain $I: B \u00d7 A \u2192 R^+$ (Lindley 1956;\nThrun, Burgard, and Fox 2005) for possible actions:\n$I(b,a) = H(b) \u2013 E_{\u03a6 \u223c Result(a)}[H(Update(b, \u03a6))]$ (9)\nwhere the expectation in Eq. 9 is computed by marginalising\nover answers \u03c6 consistent with b, with the likelihood that \u03c6\nis the actual answer computed using WMC (Eq. 1).\nSECURE compares expected information gain to the ex-\npected reward of the current belief state:\n$E_b[R(a)] =\\begin{cases}\n-C(a) & if a \u2208 Q\\\\\n2Pr (\u03a6(\u041c) | b) \u2013 1 & otherwise\n\\end{cases}$ (10)\nleading to the overall action-value function $Q: B \u00d7 A \u2192 R$:\n$Q(b,a) = \u03bb_1I(b,a) + \u03bb_2E_b[R(a)]$ (11)\nwhere {1, 2} are parameters signifying preferences in ex-\nploration (engaging in embodied conversation) and exploita-\ntion (solving rearrangement under unawareness).\nCorrections in Execution Choosing act \u2208 A triggers task\nplanning with M, and pick-and-place moves for rearrange-\nment are executed in the domain while the instructor moni-\ntors them. They stay silent if the move is part of an optimal\nplan and correct it if not, exposing the agent to false be-\nliefs or unidentified risks (unknown unknowns). In embod-\nied conversation, each correction consists of the corrective\ncue \"No.\", followed by a referential expression and desig-\nnation that highlights the source of error, e.g. \u201cNo. This is a\ncylinder (points to u\u2081)\".\nCorrections in execution cover two cases. First, a correc-\ntion is triggered by a wrong move that is not part of the op-\ntimal plan. To illustrate, consider the instruction \"Move a\ncube in front of a cylinder\" and a correction \u201cNo. This is\na sphere (points to u\u2081)\". If this corrects a pick move, SE-\nCURE infers not only \u03a6\u03b7\u03b9 = sphere(u\u2081) due to designa-\ntion, but also $\u03c6_{sem} = \\neg cube(u\u2081)$ due to the truth conditions\nof corrections. If such an utterance corrects a place move, in\nwhich case u\u2081 is not the picked object but rather an object\nto which the placed object is not in the desired \"in front\nof\" spatial relationship, then not only does it follow that\n\u03a6\u03b7\u03b9 = sphere(u\u2081), but also that $\u03c6_{sem} = \\neg cylinder(u\u2081)$\nby the semantics of corrections.\nThe second corrective case is one where the SECURE\nagent declares that the goal is reached, but it is in premature"}, {"title": "Experiments", "content": "completion\u2014i.e., an object that has to be picked and moved\nisn't picked (yet). In this case, the instructor designates an\nobject that still needs to be moved: e.g. \"No. this is a cylinder\n(points to u\u2082)\" yielding \u03a6\u03b7\u03b9 = cylinder(u2). These (addi-\ntional) valid formulae $\u03c6$ that are inferred via symbolic in-\nference from corrections\u2014both from the non-logical mean-\ning and semantic analysis of corrections\u2014trigger belief re-\nvision, a return to re-estimation of the domain model and\nthe possibility of engaging in embodied conversation before\nattempting to execute the rearrangement again.\nOur framework is evaluated in simulation and in real robot\nexperiments on a Franka Emika Panda robot. We com-\npare SECURE's policy secure against two baseline policies\nsimple and correct:\n\u2022\nsecure: the robot can engage in conversation before ex-\necution and makes decisions using both non-logical and\nsemantic analysis as well as processing corrections in ex-\necution.\n\u2022\nsimple: the robot can engage in conversation before exe-\ncution and makes decisions using only non-logical anal-\nysis on as well as processing corrections in execution.\n\u2022\ncorrect: the robot lacks all querying capabilities but pro-\ncesses corrections.\nITL policy performance is measured using cumulative re-\nward (CR) as an extrinsic metric of task success, and mean\nmicro F1 score (F1) between ground truth and estimated de-\nnotations at the beginning of each task for symbols in the in-\nstruction as an intrinsic metric for grounding. Hyperparam-\neters are given in Table 1. For secure and simple, {1, 2}\n(Eq. 11) are optimized using semi-gradient SARSA (Rum-\nmery and Niranjan 1994) (See Fig. 2 for details).\nSimulation Experiments\nSimulation Setup Simulation experiments are conducted\nin a blocksworld domain (see Fig. 2). Each instruction fol-\nlows the template \u201cmove r\u2081 rel r2\" where rel is a spatial\nrelationship and r1,r2 are referential expressions which take\nthe form of a quantifier, followed by one to three symbols\n(e.g. \"every red cube\"). CodeLlama7B (Rozi\u00e8re et al. 2023)\nis used to parse referential expressions to their logical forms\nand DINOv2 (Oquab et al. 2023) as the feature extractor f.\nIn the blocksworld domain, the robot interacts with an oracle\ndomain expert: it answers queries and provides corrections\nin execution when appropriate. When the robot decides to\nact \u2208 A, pick-and-place moves are executed using an op-\nerational space controller (Khatib 1987) and a rudimentary\nlogic for setting control targets based on the pose of the ob-\nject of interest. The robot can attempt to execute the task up\nto 5 times. If it does not succeed, the task ends and a new\ntask is presented. Our three ITL policies are evaluated over\nthe same sequence of 60 tasks, averaged over 5 runs.\nITL Experiments We conduct experiments to compare\ndifferent ITL policies. The results of these experiments are\ngiven in Fig. 3a. CR taking negative values indicates both\nrobot querying and corrections in execution occurring dur-\ning interaction before succeeding (or not) to execute the task.\nITL policies that engage in conversation before execution\n(i.e., Tsecure and\nsimple) receive a higher and statistically\ndifferent CR than correct. This signifies the value of engag-\ning in conversation and using it as a means for more sam-\nple efficient learning than corrective feedback alone, which\nis qualitatively similar to reinforcement learning from the\nsparse reward signal. Comparing CR for secure and simple,\nwe observe that for the majority of the tasks secure per-\nforms better than simple except for a brief task period 33-\n37 in which there's no statistically significant difference.\nThis period indicates that both policies have similar embod-\nied conversations, taking into account the uncertainty about\nthe domain and individual policy risk tolerance, encoded\nin {1, 2}. When analyzing F1, we observe that on aver-\nage secure performs 5% better than simple and 10% than\ncorrect. This signifies that its beneficial to use an ITL policy\nwhose strategic decisions factor in the additional knowledge\none gains from semantic analysis: e.g., designating \u201cevery\ncube\u201d entails the non-designated objects aren't cubes.\nBias Belief Experiments We test whether our framework\ncan cope with (initial) biased beliefs that might be false,\nthrough its reasoning about observed evidence. To evaluate\nthis, we consider a modified version of the above ITL exper-\niments, in which for all textures (plain, dotted, starry) we as-\nsigned optimistic (vopt = 0.7) and pessimistic (vpes = 0.3)\ninitial values: these respectively impose an initial bias that\nall objects are (and respectively are not) plain, dotted and\nstarry. Thus they encourage (respectively discourage) sym-\nbol prediction. Figs. 3b and 3c show the results. When com-\nparing CR values for both pessimistic and optimistic ITL\npolicies show that correct performs worse than ITL poli-\ncies that engage in embodied conversation. For simple and\nsecure, for the period of the first 20 tasks, secure performs\nbetter for both optimistic and pessimistic cases while on later\ntrials the performance difference is not conclusive, fluctuat-\ning between simple or secure. Their F1 scores show that\nhaving optimistic beliefs hurts \u03c0simple, making its ground-\ning performance similar to correct, while secure has sim-\nilar grounding performance as with unbiased initial values.\nOn the other hand, pessimistic initial values do not yield any\nstatistically significant difference between the policies. This\nis not surprising: the value coming from the semantic anal-\nysis diminishes because it predicts atoms are false, which\nmatches the pessimistic bias.\""}, {"title": "Real Robot Experiment", "content": "Experimental Setup We conduct real robot experiments\nin a fruit domain (see Fig. 4), in which the robot is asked\nto \"put the two granny smith apples inside the basket.\" in 3\ndifferent workspaces. To detect fruits of interest and assign\ninitial values for grannysmith, we use grounding DINO\n(Liu et al. 2023): an open-vocabulary object detection and\ngrounding model, which when given the prompt \"granny\nsmith apple\"2 localizes all the apples in each workspace and\noutputs the similarity score between the prompt and local-\nized area in the workspace. This score is used as an initial\nbelief value for grannysmith, and in all workspaces this\nranges between 0.4 to 0.6 before interaction. To evaluate\nITL policies, we conduct runs with all possible sequences\nof task permutations (6 in total) in which the robot starts un-\naware of grannysmith and through embodied conversation\nwith the human teacher (both queries and corrections) gath-\ners evidence. As in the simulation experiments, the robot can\nattempt to solve the task 5 times before termination and ad-\ndressing the next task. For real robot experiments, we have\nmodified unit pointing Cpoint and reference costs Cref (see\nTab. 1) so as to make querying more expensive and encour-\nage the robot to query only on high uncertainty situations\u2014\nthese arise at the beginning of interaction when a neologism\nis encountered. When the robot decides to execute the plan,\nit performs pick-and-place moves using the MoveIt (Chitta\n2016) to plan and execute collision free trajectories with\npick-and-place locations determined using Transporter Net-\nworks (Zeng et al. 2020) from ZED 2i camera feed. The hu-\nman teacher monitors the execution and after each pick-and-\nplace provides correction if necessary or stays silent.\nResults Tab. 2 shows the summary results of the real robot\nexperiments. The policy secure has both the highest and sta-\ntistically significant CR and F1 values. We observe the fol-\nlowing behaviours of the ITL policies on learning to ground\ngrannysmith:\n\u2022\nsecure: is highly uncertain about the denotations of\ngrannysmith and keeps asking for evidence using the\nquery \u201cshow me the two granny smiths\u201d, after which zero\nreward is obtained for the task (-1 for the query and +1 for\ntask success). After the two tasks, on the 3rd workspace,\nthe policy is able to correctly identify denotations and is\nconfident enough for task execution without querying.\n\u2022\nsimple: is highly uncertain about the domain and always\nfirst opts to ask a query \u201cshow me a granny smith\u201d, which\nalways results in several corrections on falsely picked\nfruits. Importantly, from the evidence gathered, simple\nis not able to significantly change its grounding from the\nevidence gathered from its previous tasks.\n\u2022\ncorrect: is attempting to solve the task from corrective\nfeedback, leading to a statistically significantly lowest\nCR, on average requiring 3 attempts to solve the task. At\nthe same time, this gathers a lot of negative evidence, so\nafter attempting to solve the task twice, this policy suc-\nceeds on the third task."}, {"title": "Conclusion", "content": "We have presented a framework for processing embod-\nied conversation to solve rearrangement under unawareness\nwhich broadly falls into the type of task expected to be han-\ndled in the ITL scenario. We have shown the benefits of mak-\ning the agent semantics-aware in interactive symbol ground-\ning, task learning as well as having a mechanism to deal with\nfalse beliefs. There are several directions of future work to\nconsider, including handling and exploiting other semantics\nof conversation (e.g. generics or contrast) as well as using\nadditional knowledge sources to boostrap ITL."}]}