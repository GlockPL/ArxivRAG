{"title": "MISALIGNMENTS IN AI PERCEPTION: QUANTITATIVE FINDINGS\nAND VISUAL MAPPING OF HOW EXPERTS AND THE PUBLIC\nDIFFER IN EXPECTATIONS AND RISKS, BENEFITS, AND VALUE\nJUDGMENTS", "authors": ["Philipp Brauner", "Felix Glawe", "Gian Luca Liehner", "Luisa Vervier", "Martina Ziefle"], "abstract": "Artificial Intelligence (AI) is transforming diverse societal domains, raising critical questions about\nits risks and benefits and the misalignments between public expectations and academic visions.\nThis study examines how the general public (N=1110)\u2014people using or being affected by AI-and\nacademic AI experts (N=119)\u2014people shaping AI development-perceive AI's capabilities and\nimpact across 71 scenarios, including sustainability, healthcare, job performance, societal divides,\nart, and warfare. Participants evaluated each scenario on four dimensions: expected probability,\nperceived risk and benefit, and overall sentiment (or value). The findings reveal significant quantitative\ndifferences: experts anticipate higher probabilities, perceive lower risks, report greater utility, and\nexpress more favorable sentiment toward AI compared to the non-experts. Notably, risk-benefit\ntradeoffs differ: the public assigns risk half the weight of benefits, while experts assign it only a third.\nVisual maps of these evaluations highlight areas of convergence and divergence, identifying potential\nsources of public concern. These insights offer actionable guidance for researchers and policymakers\nto align AI development with societal values, fostering public trust and informed governance.", "sections": [{"title": "Introduction", "content": "Although the origins of Artificial Intelligence (AI) date back decades (McCarthy et al. 2006; Hopfield 1982; Rumelhart,\nHinton, and Williams 1986), recent developments, driven by advancements in algorithms, computing power, and\navailability of training data (Deng et al. 2009), yielded transformative breakthroughs and rise in funding (Lecun, Bengio,\nand Hinton 2015; Statista 2022). AI becomes increasingly integrated in many sectors such as education (Chen, Chen,\nand Lin 2020), healthcare (Amunts et al. 2024), journalism (Diakopoulos 2019), forestry abnd farming (Holzinger et\nal. 2024), as well as production and manufacturing (Brauner et al. 2022) and offers benefits in terms of convenience,\nefficiency, and innovation (Bouschery, Blazevic, and Piller 2023).\nHowever, AI also raises concerns\u2014such as privacy infringements, job displacement (Acemoglu and Restrepo 2017),\nalgorithmic bias (Brauner et al. 2019), and ethical challenges (Awad et al. 2018)\u2014that affect individuals, organizations,\nand society as a whole. Consequently, the expectations regarding AI seem divided: Some view AI as a revolutionary\ntool that will improve our lives (Brynjolfsson and McAfee 2014; Makridakis 2017), others express concerns about\nethical implications and potential risks (Cath 2018; Bostrom 2003).\nFor decades, it has been known that computers and algorithms are not value-neutral but inherently reflect embedded\nvalues and potential biases (Friedman and Nissenbaum 1996; Nissenbaum 2001; Sadek, Calvo, and Mougenot 2024).\nThese embedded values can influence decisions and outcomes, potentially perpetuating inequality or reinforcing existing\nbiases. Furhter, Crawford (2021) argues that artificial intelligence is neither artificial nor intelligent. It is not artificial,\nas it relies on vast, hidden cloud infrastructure, consumes enormous amounts of energy for training and inference, and\ndepends heavily on invisible human labor-primarily from the Global South-for labeling training data (Hern 2024).\nNor is it intelligent because, though it mimics intelligent behavior, it lacks independent thought, genuine understanding,\nand self-awareness. Instead, it is a product of human design, shaped by the developers goals, assumptions, and biases.\nThis underscores the importance of examining both the similarities and differences in AI perception between experts\nand developers, who shape these technologies, and the general public, who ultimately use or are affected by them.\nIn this article, we therefore queried the opinions of both academic experts in AI and the general public regarding\nAl's future using 71 micro scenarios. As individual risk and benefit perceptions shape attitudes, usage intentions, and\nactual usage across various domains Huang, Dai, and Xu (2020), we measured the expected likelihood of occurrence,\nperceived risks and perceived benefits, as well as the overall valence (value, or sentiment) towards these topics. We\nanalysed the responses for similarities, differences, and patterns and provide visual maps of the findings. The study\naims to inform about similarities and differences in AI perception and evaluation between experts and the public to\nsupport the development of a research agenda for AI's future, identify which AI fields may require stronger regulation,\nand highlight areas where accessible education on AI and its implications is needed to enhance public understanding."}, {"title": "Related Work", "content": "The structure of the related work is as follows. First, we outline related work on the perception of AI. Second, we\npresent work on the similarities and differences in AI and technology perception between experts and the general public.\nThird, we introduce the psychometric model of risk perception and works on risk perception, as this approach serves as\nthe basis of our work."}, {"title": "Public Perception of AI", "content": "While AI has been around for decades, the public release and rapid adoption of ChatGPT at the end of 2022 (Hu 2023)\nsparked unprecedented academic interest regarding broader implications and public perceptions of AI across various\ntasks and domains. As research on this topic is still evolving, many findings remain preliminary, and knowledge about\nAI perception is yet to be fully consolidated. Consequently, the following section provides a brief and necessarily\nselective overview of this rapidly developing field.\nPublic perception of artificial intelligence (AI) is multifaceted and influenced by various factors, including media\nrepresentation, personal experiences, and societal context.\nMedia plays a significant role in shaping public opinion about AI. Fast and Horvitz (2017) conducted an extensive\nanalysis of three decades of AI coverage in the New York Times, observing a marked rise in public interest after\n2009. Their findings indicate that coverage has generally been more positive than negative, balancing optimism\nwith growing concerns over control and ethical issues, while recent years reflect heightened enthusiasm for AI's\npotential in fields like healthcare. News coverage often emphasizes the benefits of AI while downplaying potential\nrisks, leading to a perception of AI as superior to human capabilities and fostering anthropomorphization of technology\n(Puzanova, Tertyshnikova, and Pavlova, n.d.). Cave, Coughlan, and Dihal (2019) explored common AI narratives in\nthe UK, identifying four optimistic and four pessimistic themes. These narratives often involve anxiety, with only two\nemphasizing benefits over risks, such as making life easier. Sentiment analysis of WIRED articles shows a trend towards\npolarized views, with both positive and negative sentiments increasing over time (Moriniello et al. 2024). Sanguinetti\nand Palomo (2024) investigated how news outlets portray \"AI anxiety\", often depicting AI as an autonomous and\nopaque entity beyond human control. Using an AI anxiety index, the study analyzed newspaper headlines before and\nafter ChatGPT's launch, finding increased coverage and heightened negative sentiment.\nPerceived Risks and Benefits influence the perception of AI. Surveys indicate that the public views AI as both a risk\nand an opportunity. Concerns include privacy invasion and cybersecurity threats (Brauner et al. 2023), while benefits\nare seen in areas like urban services and disaster management Yigitcanlar, Degirmenci, and Inkinen (2022). Neri\nand Cozman (2019) argue that experts play a significant role in shaping public perception of AI risks. Their public\npositioning can amplify the perception of certain risks by the public, such as existential threats, which may not be\nbased on actual disasters but rather on expert discourse. Lee et al. (2024) found that individuals with higher education\nlevels, interest in politics, and knowledge about ChatGPT tend to perceive greater risks associated with AI. This finding\nchallenges the conventional \u201cknowledge deficit\" model and indicates that negative perceptions can stem from a critical\nmindset approaching AI technology with caution.\nTrust in AI varies by context and demographic factors. People generally trust AI in personal lifestyle applications but are\nmore sceptical about its use by companies and governments (Yigitcanlar, Degirmenci, and Inkinen 2022). Willingness\nto use AI is influenced by the perceived balance of risks and opportunities, which varies across different application\ndomains such as medicine, transport, and media (Schwesig et al. 2023).\nThere is a significant gap in public understanding of AI, often leading to irrational fears and control beliefs. Promoting\nAl literacy is essential for informed decision-making and responsible innovation (Brauner et al. 2023).\nFurther, public perception of AI can vary significantly based on local context, political ideology, and exposure to science\nnews. For instance, the people from the US generally expects more benefits than harms from AI, with a significant\nportion supporting regulation to mitigate potential risks (Elsey and Moss 2023). However, existential risks are not a\nprominent concern for most people, who tend to worry more about concrete issues like job loss. Sindermann et al.\n(2022) explored cross-cultural differences in AI attitudes among Chinese and German participants, linking fear of AI to\nneuroticism in both groups and highlighting cultural variations in AI acceptance and fear. Kelley et al. (2021) surveyed\nover 10,000 participants across eight countries to examine public attitudes toward AI (Australia, Canada, USA, South\nKorea, France, Brazil, India and Nigeria), finding that developed nations predominantly expressed worry and futuristic\nexpectations, while developing nations showed excitement about AI's potential, with South Korea emphasizing Al's\nusefulness and future applications, amid widespread uncertainty about its societal impact. In Taiwan, science news\nconsumption and respect for scientific authority positively influence AI perceptions (Wen and Chen 2024).\nThe public discourse on AI frequently features either fears and inflated expectations, especially concerning artificial\ngeneral intelligence (AGI), which remains largely speculative and fictional as of today (Jungherr 2023). Ipsos (-@\nIpsos 2022) conducted a survey revealing that the general population frequently lacks a nuanced understanding of\nAl's technical capabilities and limitations. Similarly, Pew Research (2023) found that only a small percentage of\nAmericans could accurately identify AI in everyday scenarios, underscoring widespread confusion about Al's scope and\ncapabilities. The Alan Turing Institute (2023) similarly noted that public understanding of AI varies significantly by\neducation level and context, with common concerns centered on automation and robotics, particularly in employment\nand security applications. This limited awareness fosters misconceptions and simplistic views of AI's impact, hindering\ninformed public discourses on societal implications.\nIn summary, public perception of AI is complex and shaped by a combination of media influence, perceived risks and\nbenefits, trust levels, and cultural context.\nIn a comment, four AI researchers (Russell et al. 2015) Stuart Russell warns against lethal autonomous weapons,\narguing for an international ban to prevent an arms race. He stresses ethical concerns and argues tahat the AI community\nmust take a clear stance, as in past cases with nuclear and chemical weapons. Sabine Hauert advocates for researchers\nto engage the public and counter misconceptions about AI. She highlights the importance of balanced communication\nto shape perceptions and policies. Coordinated efforts could unify communication strategies across AI stakeholders.\nRuss Altman emphasizes AI's transformative potential in healthcare but warns of inequitable access. Without careful\nimplementation, AI could widen disparities, creating unjust systems. Clinicians also need AI systems they can\nunderstand and trust. Manuela Veloso: Envisions a collaborative future where robots complement humans and how\nrobots and humans can mutually assist one another. Practical challenges remain, including improving communication\nand robot capabilities in real-world settings. She sees robots enhancing, not replacing, human productivity."}, {"title": "Similarities and Differences in Risk Perception between Experts and the Public", "content": "The perception of risk varies significantly between experts and the general public or novices across various domains.\nWe outline findings outside outside the technoogy context to findings on AI in particular.\nHealth experts and the public differ in their risk assessments of health hazards (Krewski et al. 2012). Experts tend\nto perceive behavioral health risks (e.g. smoking, obesity) as more significant, while the public may have different\npriorities. This discrepancy highlights the need for effective risk communication strategies to align public perception\nwith expert assessments. Public perception of risks of production facilities is often more subjective and emotional\ncompared to the objective evaluations by safety professionals (Botheju and Abeysinghe 2015). This misalignment\nnecessitates two-way communication to prevent public concerns from escalating. In different context, Siegrist et al.\n(2007) contrasted the public perception of nanotechnology between 375 laypeople and 46 experts using the psychometric\nmodel. In the context of environmental hazards, such as nuclear waste, experts and laypeople have different perceived\nrisks and risk perception is influenced more by attitudes and moral values than by cognitive factors (Sj\u00f6berg 1998).\nLaypeople, who generally have less technical knowledge, tend to assess risks based on intuitive and emotional factors.\nIn contrast, experts rely more on evidence-based, technical assessments. This difference often leads to divergent views\non policy and regulation, with members of the public perceiving higher levels of risk than experts. The research also\nhighlighted that the public prioritize ethical and societal implications, while experts focus primarily on scientific and\ntechnical risks (Siegrist et al. 2007).\nIn contrast, in the context of natural hazards like hurricanes and cyclones, there is often a good deal of consistency\nbetween expert risk assessments and public perceptions, especially in high-risk areas (Peacock, Brody, and Highfield\n2005; Sattar and Cheung 2019). However, public risk perception can be influenced by factors such as trust in authorities\nand previous disaster experience\nIn the realm of autonomous vehicles (AVs), public risk perception is heavily influenced by trust in technology and\nauthorities. Knowledge about AVs can improve trust, which in turn reduces perceived risk, emphasizing the importance\nof trust-building initiatives (Robinson-Tay and Peng 2024). In aviation, experts generally have a more accurate\nperception of relative risks compared to novices, whose perceptions can be influenced by overconfidence and lack of\nexperience (Thomson et al. 2004).\nElena and Johnson (2015) examined expert and public perceptions of cloud computing services. The findings indicate\nthat experts generally have a more nuanced understanding of risks, especially concerning data security and integrity,\nwhile members of the public are more likely to experience a generalized \u201cdread risk\u201d related to unfamiliar or abstract\ntechnological threats. Perceptions were also shaped by factors like trust in regulatory bodies and the perceived benefits\nof the technology.\nAbout a decade ago, M\u00fcller and Bostrom (2016) surveyed AI experts on their expectations for AI's future capabilities,\nfinding that most anticipated the development of \u201csuperintelligence\u201d between 2040 and 2050. Notably, a third of these\nexperts viewed this development as \u201cbad\u201d or \u201cextremely bad\u201d highlighting significant concerns even within the expert\ncommunity.\nCrockett et al. (2020) compared trust and risk perceptions of AI between the general public and computer science\nstudents, people with above average expertise in AI. The findings suggest differences in risk and trust perceptions with\neducation playing a crucial role in building trust and mitigating perceived risks. Novozhilova et al. (2024) found that\nhigher technological competence and familiarity with AI increases trust in AI.\nComprehensive expert and novice comparisons are comparatively rare. Recently, Jensen et al. (2024) interviewed 25\nindividuals from the general public and 20 AI experts in the United States to assess their perceptions of AI. Experts\nand the public emphasized that AI reflects its creators, with inherent biases and limitations. Ethical worries include\nAl's lack of transparency, profit-driven development, and potential to exacerbate inequalities. Both groups advocate for\nhuman oversight, particularly in high-stakes scenarios like healthcare. Despite its efficiency, Al's inability to replicate\nhuman empathy raises trust issues. In both groups, ideas about humanness and ethics were central.\nThese studies highlight the importance of integrating both expert and public perspectives when shaping policies for\nAI as an emerging technology, as differing perceptions of risk can significantly influence technology design, public\nacceptance, and regulatory decisions.\nDespite the growing body of literature on the perception and use of AI, the similarities and differences in expert and\npublic perception of AI and its potential implantations are still not sufficiently understood. On that account, the present\nstudy addresses these gaps and is guided by the following research questions:\n1. What are the similarities and differences in the perception of AI and AI's likely capabilities and impacts\nbetween Al experts, as creators of AI-based systems, compared to the general public or potential users of these\nsystems?\n2. In which fields are these expectations similar or different?\n3. Which value do both groups attribute to AI's? In which areas is AI perceived as positive or negative and which\nareas do these attributions differ between both groups?"}, {"title": "Method", "content": "The studies' goal is to identify similarities and differences in how the general public and academic AI experts perceive\nAI, focusing on the underlying trade-offs between perceived risks, benefits, and value."}, {"title": "Risk Perception and the psychometric model", "content": "In this work, we build on the psychometric model that focusses on an individuals' subjective perception of risk by, for\nexample, quantifying risk through subjective rating scales Slovic, Fischhoff, and Lichtenstein (1986). This model is\nsuitable to study risk perception of emerging technologies and offers a framework to describe how individuals perceive\nand balance their risk and benefits. It has been applied an a variety of contexts, such as gene technology (Connor and\nSiegrist 2010), genetically modified food (Verdurme and Viaene 2003), nuclear energy (Slovic et al. 2000), climate\nchange (Pidgeon and Fischhoff 2011), and Carbon Capture and Utilization technology (Arning et al. 2020). A common\npattern in technology perception is the inverse relationship between perceived risk and perceived benefits: technologies\nviewed as highly useful are typically seen as safer, while those perceived as risky are often regarded as less beneficial\n(Alhakami and Slovic 1994).\nTo achieve this, we designed two closely related surveys: one targeting the general public and the other directed at\nacademic AI experts. Both surveys centered on a core where participants evaluated a randomized subset of 15 out of\n71 micro-scenarios (to mitigate fatigue), describing potential AI developments across five assessment items (Brauner\n2024). This approach provides two distinct perspectives for interpreting the data collected: First, responses can serve as\na reflexive measure of underlying dispositions or individual differences among participants. Second, responses can be\ninterpreted as topic or technology attributions, allowing for the analysis of common patterns or visual mapping of the\ndata.\nTo develop the list of scenarios, we first identified potential capabilities and impacts that AI might have in the near\nfuture. Drawing on expert workshops and prior research (Brauner et al. 2023), we compiled an initial set of potential\ntopics for the survey. Through several iterations, we refined the selection by removing redundancies and optimizing\nthe labels for clarity and conciseness. The final list of topics encompasses both straightforward and more speculative\nstatements, such as AI \u201ccreates many jobs", "promotes innovation\u201d, \u201cacts according to moral concepts": "or", "threat": "A complete list of items is provided in Table 4 in the Appendix.\nWe chose five dependent variables to assess each of the topics, measuring each on a single 6-point semantic differential\nscale.\n\u2022 Expectation: How likely is the projection to occur in the next 10 years (\u201cwill not happen-will happen", "risk": "How do you evaluate the risk of this development/aspect for you personally (\u201clow-risk\nhigh-risk"}, {"risk": "How do you evaluate the risk of this development/aspect for the society as a whole \u201csocially\nharmful-socially harmless", "Benefit": "How do you evaluate the benefits or utility of the respective development/aspect (\u201cuseful-useless", "Valence": "How do you consider this development, should it come true, to be positive or negative (\u201cpositive-\nnegative"}, {"title": "Sample Acquisition", "content": "The sample of public was gathered in collaboration with a panel provider Consumerfieldworks. The sample of 1354\nparticipants was selected to be representative of the population in terms of age, gender, socio-economic status, and\ndistribution across the federal states in Germany. Participants received a monetary compensation of approximately\n1 Euro for their participation. For the expert sample, we utilized convenience sampling through our personal social\nnetworks from joint projects and project proposals. We also gathered publicly available email addresses from prominent\njournals and conferences in the field and asked for further dissemination of the survey (snowball sampling). In total, we\nreached out to approximately 450 experts to participate in the survey of which 139 passed beyond the first page."}, {"title": "Data Analysis and Cleaning", "content": "We analyzed the data using R version 4.3.2 (2023-10-31) using both parametric and non-parametric methods, including\nthe Bravais-Pearson (r) and Kendall's Tau ($\\tau$) correlation coefficients, Chi-square ($\\chi^2$) tests, multiple linear regressions,\nand multivariate analysis of variances (MANOVA). We used Pillai's trace (V) as the test statistic for the MANOVA\nomnibus tests. We evaluated the assumptions underlying each test and report any violations. Missing responses were\nhandled on a test-wise basis. Consistent with social science standards, we set the Type I error rate at 5% ($\\alpha$ = .05) to\ndetermine statistical significance (Field 2009). All data, calculations, and this reproducible manuscript are publicly\navailable in the open data repository."}, {"title": "Sample 1: General Public", "content": "After cleaning, the sample of the general public consists of 1100 people, 524 reporting to be male and 570 reporting to\nbe female. The participants' age ranged from 18-85 years with a median age of 51 years (SD 14.2) years. There is no\nsignificant correlation between age and gender in the sample (r = -0.031, p = 0.210 > .05)."}, {"title": "Sample 2: Experts on Artificial Intelligence", "content": "After cleaning, the expert sample has 119 people with 93 reporting being male and 25 reporting being female. The age\nranged from 23\u201475 years with a median age of 36.5 years (SD 13.4 years). Again, there is no significant association\nbetween age and gender (r = -0.113, p = 0.143 > .05). The majority of the experts were from Germany (N = 77),\nfollowed by the Netherlands (N = 10). All other countries received less than 10 mentions. The years of AI experience\nranged from 1 to 40 years with an arithmetic mean of 10.3 and a median of 5 years. In total, the participants wrote\n2713 academic publications with an arithmetic mean of 22.8 and a median of 5 publications per expert. As indicated by\nthe high Gini coefficient (G = 0.771), the number of publications is unevenly distributed among the experts\u2014many\nhave few, few have many-which suggests that both visible figures from the field as well as junior researchers have\nparticipated in the survey. When asked for their expertise, 11 participants reported having basic knowledge and 51\nparticipants reported being well-informed, 48 reported being experts, and 9 said they are a \"recognized authority in the\nfield of AI\"."}, {"title": "Results", "content": "In this section, we first analyze differences in the overall attributed risk, benefit, and valence, as well as the average\nexpectancy between the general public and academic AI experts. Next, we interpret the individuals' responses as\nreflexive measurements of latent constructs of expectancy, risk, benefit, and overall valence towards AI and analyse the\ndifferent risk and benefit trade-offs between both AI experts and the public. Following, we switch the perspective and\ninterpret how the many AI-related statements are evaluated and how experts and the public differ in their evaluation in\nterms of expectancy and valuation, as well as the different risk and benefit attributions towards the queried statements"}, {"title": "Differences in the Grand Mean of the Evaluations", "content": "First, we analyse how both samples rate the topics in regard to the queried dependent variables risk, benefit, valence,\nand expectancy. As the boxplots in Figure 2 illustrate, there are differences in the evaluation between both samples. A\none-way MANOVA with group as independent variable showed a small but statistically significant difference between\nboth samples on the combined dependent variables (V=0.057, F(4, 1214) =18.332, p<0.001). Further analysis revealed\nthat each of the dependent variables-expected likelihood of occurance, risk, benefit, as well as overall valence-showed\nsignificant differences based on the group membership (expectancy: F(1, 1217) = 18.572, p<0.001; risk: F(1, 1217)\n= 26.716, p<0.001; benefit: F(1, 1217) = 43.443, p<0.001 valence: F(1, 1217) = 21.929, p<0.001). These findings\nsuggest that academic AI expertise significantly impacts the overall multivariate response as well as on each individual\ndependent variable.\nIn particular, the AI experts, on average, report the queried projections to be more likely to come true within the next\ndecade (25.2%), less risky (19.3%), more useful (15.3%), and more positive (-4.0%) compared to the members of the\npublic (expectancy: 12.7%, risk: 34.7%, benefit: -5.2%, valence: -19.7%)."}, {"title": "Perspective 1: Different Risk-Benefit Tradeoffs between Academic AI Experts and the Public", "content": "In this section, we analyse how individuals of both samples perceive AI in terms of expectancy, risk, benefit, and\nvalence and interpret the subjects' responses as reflexive measurements of latent constructs.\nTable 2 presents the correlation patterns between individuals' AI expectancy, AI risk and benefit dispositions, and their\noverall valence towards AI for the two samples: academic AI experts and the general public.\nAmong experts, the expected likelihood of occurance is positively associated with both the perceived benefit of AI and\nthe overall valence. However, there is no significant relationship between perceived risk and expectancy regarding\nwhether projections will come true within the next decade. Risk and benefit are moderately negatively correlated, while\nrisk is also negatively associated with valence. Conversely, perceived benefit exhibits a strong positive relationship with\noverall valence towards AI.\nFor the general public, AI expectancy is positively correlated with both an individual's perceived AI risks and benefits\nbut shows no significant association with perceived valence: Individuals who view AI as either riskier or more useful\nare more likely to believe that projections will materialize within the next decade, and vice versa. Overall valence\ntowards AI is strongly and positively correlated with perceived benefits, while it is strongly and negatively associated\nwith perceived risks."}, {"title": "Differences in Expectancy between Experts and the Public", "content": "As the previous analysis indicates, experts and the public differ in their expectations regarding what Al will likely\nachieve in the next 10 years (referred to as expectancy hereafter). To illustrate these differences at the topic level, Figure\n3 presents the average expectancy evaluations for each topic as a scatter plot. The figure can be read as a visual map\nand interpreted as follows (Brauner 2024): Each point represents a question item, with its coordinates corresponding to\nthe experts' (x-axis) and the public's assessment (y-axis). Points farther to the left or right indicate lower or higher\nexpert expectations for a given statement. Similarly, points lower or higher on the plot reflect lower or higher public\nexpectations. Points on or near the dashed diagonal represent topics where experts and the public have congruent\nassessments, meaning both groups assign similar expectancy evaluations to the statements. In contrast, points far from\nthe diagonal highlight differences between the groups: topics above the diagonal are considered more likely by the\npublic, while those below the diagonal are deemed more likely by experts. Finally, the blue line represents the maximum\nlikelihood estimation of the regression line, while the gray area shows the 95%-confidence interval, indicating the range\nwithin which the true regression line is expected to fall.\nThe figure displays several similarities and distinct differences in the expectancy assessment between the public and\nexperts in AI. First, overall there is a strong and positive correlation between the expectations regarding AI of experts\nand the public (r = 0.734, p < 0.001). This suggests that, at least for many topics, there is an agreement on the\nperceived expectancy on AI's capabilities and impact. Second, however, the distribution of the expectancy of the\npublic is much narrower than the expectancy distribution of the experts. This suggests that the experts may have a\nmore nuanced and differentiated view on the potential of AI than the public. This is also reflected by the flatter slope\nof the regression line. Third, despite the general agreement on many topics, there are various aspects with a strong\ndiscrepancy between experts and the public. Topics where the experts had much lower expectations than the public\nwere the statement that AI will destroy humantity, AI leads to personal lonliness, and that AI can no longer be controlled\nby humans. Statements where the experts had much higher expectations where that AI will improve our health, AI will\nprefer certain groups of people, and that AI will become humorous."}, {"title": "Differences in Attributed Valence between Experts and the Public", "content": "Next, we analyse the similarities and differences between academic AI experts and the public in terms of attributed\nvalence or Al sentiment. Again, we created a scatter plot in Figure 4 with the average valence attributed to a topic by\nexperts on the x-axis (horizontally) and by the public on the y-axis (vertically). Apparently, the average attributed\nvalence between boith groups is tighter correlated (r = 0.855, p < 0.001) compared to the expectancy, which suggests\nthat the agreement of both groups is higher.\nOverall, experts exhibit a more positive outlook on AI compared to public. However, the analysis reveals notable\ndifferences in the valence evaluations of AI projections between the two groups, with sentiments diverging significantly\non specific topics. Specifically, the public expresses greater negativity toward projections where AI is perceived as\ndividing society, destroying humanity, or AI viewing humans as a threat. Conversely, experts are more positive about\nscenarios where AI acts according to moral principles, contributes to a more sustainable society, or makes decisions\nabout medical treatments. These differences underscore varying levels of optimism and concern, reflecting contrasting\nperceptions of AI's potential risks and benefits across the two groups."}, {"title": "Illustration of risks and benefits attributed to the statements for experts and the general public", "content": "As the overall sentiment towards AI-related statements appears to differ between academic AI experts and the general\npublic, we analyze the attributed risk and benefit for each group independently. The left panel of Figure 5 illustrates the\nrisk-benefit attributions for AI experts, while the right panel represents these attributions for the general public.\nThe x-axis (horizontal) represents the perceived risk attributed to each topic, and the y-axis (vertical) represents the\nperceived benefit. Note that these plots do not depict the relationship between risk, benefit, and the overall valence of\nthe topics. A regression analysis examining the relationship between attributed risk, benefit, and the overall valence is\nprovided in Section A.2 in the Appendix.\nData points in the plots illustrate where the perceived risks and utilities of evaluated statements fall. Items below the\nhorizontal axis are considered less useful, while items above are seen as more useful. Similarly, items to the left of the\nvertical axis are perceived as more risky, whereas items to the right are viewed as safer. Items in the top-right quadrant\nare perceived as both risky and useful, while those in the bottom-left quadrant are considered neither risky nor useful.\nItems in the top-left quadrant are seen as risky but not useful, and items in the bottom-right quadrant are evaluated as\nnot risky but useful.\nThe evaluations provided by AI experts display greater distribution compared to those of the general public, suggesting\nmore nuanced assessments. Experts' risk and benefit attributions span a broader range, with a substantial proportion of\ntopics deemed useful and a smaller proportion assessed as useless. Experts perceive topics as both risky and safe across\nthe spectrum. In contrast, the general public's evaluations are concentrated, with most topics viewed as relatively risky."}, {"title": "Discussion", "content": "As AI is reshaping our world, we explored the similarities and differences in the perceptions of those who use or\nare affected by AI technology and those who design or advance AI. Drawing on the psychometric paradigm (Slovic,\nFischhoff, and Lichtenstein 1986), we conducted a survey based study involving both a representative sample of the\nGerman general public and academic AI experts from Germany and the Netherlands. Both groups evaluated a diverse\nset of AI-related"}]}