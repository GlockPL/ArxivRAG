{"title": "ET-PLAN-BENCH: EMBODIED TASK-LEVEL PLANNING BENCHMARK TOWARDS SPATIAL-TEMPORAL COGNITION WITH FOUNDATION MODELS", "authors": ["Lingfeng Zhang", "Yuening Wang", "Hongjian Gu", "Atia Hamidizadeh", "Zhanguang Zhang", "Yuecheng Liu", "Yutong Wang", "David Gamaliel Arcos Bravo", "Junyi Dong", "Shunbo Zhou", "Tongtong Cao", "Yuzheng Zhuang", "Yingxue Zhang", "Jianye Hao"], "abstract": "Recent advancements in Large Language Models (LLMs) have spurred numerous attempts to apply these technologies to embodied tasks, particularly focusing on high-level task planning and task decomposition. To further explore this area, we introduce a new embodied task planning benchmark, ET-Plan-Bench \u00b9, which specifically targets embodied task planning using LLMs. It features a controllable and diverse set of embodied tasks varying in different levels of difficulties and complexities, and is designed to evaluate two critical dimensions of LLMs' application in embodied task understanding: spatial (relation constraint, occlusion for target objects) and temporal & causal understanding of the sequence of actions in the environment. By using multi-source simulators as the backend simulator, it can provide immediate environment feedback to LLMs, which enables LLMs to interact dynamically with the environment and re-plan as necessary. We evaluated the state-of-the-art open source and closed source foundation models, including GPT-4, LLAMA and Mistral on our proposed benchmark. While they perform adequately well on simple navigation tasks, their performance can significantly deteriorate when faced with tasks that require a deeper understanding of spatial, temporal, and causal relationships. Thus, our benchmark distinguishes itself as a large-scale, quantifiable, highly automated, and fine-grained diagnostic framework that presents a significant challenge to the latest foundation models. We hope it can spark and drive further research in embodied task planning using foundation models.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in Large Language Models (LLMs) and Vision Language Models (VLMs) have spurred numerous attempts to apply these technologies to embodied tasks, particularly focusing on high-level task planning and task decomposition. To further explore this area, we introduce a new embodied task planning benchmark, ET-Plan-Bench, which specifically targets high-level embodied task planning using foundation models. It features an automatic embodied task generation and evaluation pipeline that is designed to evaluate tasks with spatial and temporal understanding of the environment, which is known to be challenging for LLM and VLM application in embodied task planning (Jia et al., 2022; Chen et al., 2024; Jain et al., 2023). By using multiple simulators (including Virtual Home (Puig et al., 2018) and Habitat (Khanna et al., 2024)) as the backend engine, it can provide immediate environmental feedback to LLMs, which enables LLMs to interact dynamically with the environment and replan as necessary.\nFor spatial understanding, our benchmark aims to assess the LLMs' ability to complete tasks with relational, size, and occlusion constraints between objects, which are pivotal for effective interaction\nThe benchmark and source code will be publicly released soon."}, {"title": "2 RELATED WORK", "content": "Evaluation of foundation models for household embodied planning LLMs are used in task planning for their generalization capabilities across various tasks(Brown et al., 2020; Ahn et al., 2022; Huang et al., 2022). Recent work shows significant advances in using LLMs for long-horizon planning (Zhao et al., 2024), but challenges like hallucination and poor spatial reasoning remain (Valmeekam et al., 2024; Dziri et al., 2024). An automatic evaluation of LLM-based planners is crucial, yet performance comparison is complicated by factors such as prompt construction and model selection. Despite advances in task planning with LLMs, related benchmarks and automatic evaluations remain limited."}, {"title": "3 BENCHMARK DESCRIPTION", "content": "Since we propose a task generation pipeline, we can generate infinite task data given the preferred constraints and task difficulties. We include a subset of those task data as the evaluation task data. \nFigure 1 and Table 2 summarize our embodied planning evaluation benchmark, which includes navigation and manipulation tasks, along with their variants under spatial and temporal constraints."}, {"title": "3.1 BENCHMARK TASK DEFINITION", "content": "In our benchmark, we mainly provide three task categories: navigation tasks with or without spatial constraints, navigation and manipulation tasks with or without spatial constraints, and navigation and manipulation tasks with temporal constraints. For each task category, we divide it into several sub-task categories based on different aspects that require spatial and temporal understanding for the embodied agents. We introduce higher levels of task difficulty by combining the basic tasks. We further discuss the 5 key aspects that will impact the task difficulties in Appendix A.5.\nNavigation Navigation tasks are common in everyday household scenarios, such as locating items. We define navigation tasks more specifically as object navigation, where the agent is required to navigate to an instance of a specified object category in unseen environments.\nNavigation with layout map In real-world scenarios, robots often possess knowledge about certain environmental priors, such as a layout map which contains the locations of large pieces of furniture (e.g., sofas and refrigerators), which are less frequently moved compared to smaller objects. This information enables the robot to perform its navigation tasks more efficiently.\nNavigation with spatial relation constraint This task requires the robot to identify a specific target object that meets given criteria, such as locating a box on the wall self. This spatial relation constraint could assess the LLM agent's perception ability of relative positions between objects. In general, these tasks are more challenging than navigation tasks without constraints.\nNavigation with occlusion due to small object size 'Occlusion' refers to situations where large objects or barriers obstruct the view of target items, challenging an agent to provide strategies to uncover or reach these hidden target objects. This scenario is common in small object navigation, where the object will not be directly visible for the robot and complicating the robot's visual field and increase the difficulty of the tasks.\nNavigation with occlusion due to distant initial position We design tasks targeting objects located far from the robot's initial position. This setup poses greater challenges for the agent as it navigates through the room, with objects more likely to be obscured by large pieces of furniture.\nNavigation and manipulation The most common embodied tasks, typically involve moving and then organizing items. For example, one might pick up one object and relocate it to a designated area, which could be a container or a furniture surface. Other task categories, such as navigation and manipulation with spatial constraints or occlusions, are similarly described in the earlier description.\nNavigation and manipulation with temporal and causal constraints Compared to simply combin-ing tasks that are order-invariant, temporal constraints involve actions that must be performed in a strict sequence. For example, an agent might need to prepare a meal where each step is executed in a specific order to ensure the dish is correctly completed. Alternatively, tasks may require the agent to optimize the order of actions to minimize the number of steps needed to complete a task. Additionally, some tasks feature Dependency Chains, where certain actions can only start once previous steps have been completed, such as needing to unlock and open a door before entering a room. Our designed navigation and manipulation tasks include both temporal and causal constraints. For example, the agent must sequentially place object A into container B, and then place object A and container B together into container C, in the correct order. Alternatively, the task requires the agent to sequentially find the target objects, grasp them, locate the recipient area, and place the object in the designated spot in the correct order. These tasks are crucial for assessing the agent's ability to understand the temporal and causal effects of environmental states and the actions it performs.\nNavigation and manipulation multiple objects in an optimal path with two arms In navigation and manipulation tasks, a typical scenario involves preparing various items to achieve specific goals. This requires the robot to handle multiple objects simultaneously, such as gathering a pen and a notebook for writing. Virtual Home supports this by equipping the robot with two arms, allowing it to use either one or both arms to complete tasks. We design embodied tasks that specifically exploit these unique features. Using both arms can often be more efficient and optimal, for example, by reducing the moving distance and the number of exploration plan steps. We offer two versions for navigating and manipulating multiple objects: one-arm and two-arm configurations, enabling a comparison of their performance."}, {"title": "3.2 TASK GENERATION USING LLMS", "content": "To efficiently generate a large and diverse set of tasks with various spatial and temporal constraints, we have incorporated large language models into the task generation pipeline. To enhance the generalizability of our task generation pipeline, each component can be adapted by a human to the specific simulator in use, incorporating a human-in-the-loop approach. Specifically, the pipeline includes the following steps, as illustrated in Figure 2:\nGenerating task template. The first step is to generate a task template for a specific type of task. Each template includes a task description and completion criteria. The description accounts for the number of objects involved, their properties, and any additional spatial or temporal constraints. This information is integrated as placeholders into various phrasings of the task. The completion criteria comprise the necessary state of relations between the elements involved at specific points during task execution. For example, for a task involving finding an apple and placing it in a fridge, the final success criteria would be (CLOSE, robot, fridge) & (INSIDE, apple, fridge).\nGathering environment information from the simulator. Given the specific task template, we gather related information about the simulation environments through its scene graph. We select all possible object candidates that satisfy the required constraint from the task template. For example, if"}, {"title": "4 BENCHMARK EVALUATION", "content": "While some existing methods rely on labor-intensive human evaluation (Huang et al., 2022), we propose an LLM agent baseline for automatic quantitative evaluation. As illustrated in the Figure 3, our LLM agent processes tasks through the following steps:\nTask classification Firstly, the LLM agent determines the type of task, as listed in Section 3, based on the task description. Subsequently, tools and prompt examples specific to the identified task type are retrieved from the library and utilized in subsequent processes.\nNavigation-room The object search algorithm employs a top-down hierarchical approach, beginning with the room where the target object is most likely to be found. The sequence of rooms to be searched is determined by the LLM agent, based on the task description and the list of rooms in the environment. For tasks involving spatial relation constraints, the room containing both the target object and the anchor object will be prioritized. If the anchor object is limited to a single room, only that room needs to be explored.\nNavigation-object After arriving at a designated room, a 360-degree scan is conducted, and the objects within the field of view are identified by the perception module. If the target object is not located, the LLM agent will actively select the most relevant visible object for further exploration. The observed objects and their corresponding locations are stored in the agent's memory, and this information will be utilized to assist in locating subsequent objects if the task involves multiple items.\nManipulation For tasks involving the manipulation of target objects, the specific manipulation action will be executed as required by the LLM agent. Depending on the task requirements, the robot can employ one of several manipulation skills, including OPEN, CLOSE, GRAB, PUT, and PUTIN, to interact with the target object within the virtual environment.\nCompletion check and iteration The completion criteria will be evaluated after each navigation or manipulation step, as needed. If the criteria are not met, an additional round of navigation and manipulation will be carried out. The task will be deemed unsuccessful if the maximum number of action steps is reached before the task is completed."}, {"title": "4.1 OVERALL EVALUATION PIPELINE", "content": "4% to 8% of the tasks were deemed unreasonable. The primary cause of the LLM's failure to correctly classify these tasks was ambiguity or misinterpretation in the descriptions of the objects involved. For instance, a task such as \"Find the face cream tube and put it into the folder\" might pass the LLM's post-checking process because the term \"folder\" could sometimes refer to a larger container. Overall, LLMs perform well in identifying unreasonable tasks."}, {"title": "4.2 EVALUATION RESULTS", "content": "Experiments were conducted in 8 NVIDIA Tesla V100 32G GPUs with Intel(R) Xeon(R) Gold 6140 CPU @ 2.30GHz. We use five evaluation metrics to evaluate the performance on our proposed embodied tasks, including success rate, action sequence length, longest common subsequence (LCS) length, LCS ratio with the ground truth action sequence, and moving distance for executing the tasks. A detailed explanation of these metrics is discussed in the Appendix A.9.\nWe present comprehensive experimental results to explore the performance of our proposed LLM agent across various tasks. We analyze the performance on tasks of varying difficulty within the Virtual Home environment. In Appendix A.10, we examine whether Chain of Thought (CoT) prompting and few-shot in-context learning aid in task planning.\nMain results in Virtual Home  In navigation tasks, the addition of spatial constraint dramatically impact the success rate. For occlusion cased by size or distance, we selected two subsets of tasks: The tasks of which the target object's size is the 20% smallest; And the tasks of which the distance between the robot's initial position and the target object position is top 20% largest. We observe a drop of 7% to 6% in success rate for both tasks, indicating that the occlusion caused by small object size or long distance increases the difficulty of the tasks. Consequently, the average sequence length and moving distance increase. Similar trend can be observed for tasks with spatial relation constraints. Combining relation and occlusion constraint is even more challenging for the agent to succeed.\nThe layout map significantly enhanced the success rate for both navigation and navigation & manipulation tasks. This improvement is attributed to the agent's prior global knowledge of large objects,"}, {"title": "4.3 CASE STUDY", "content": "Figure 4 illustrates one example of successful cases where the robot explores different rooms before finding the target box with relation constraint. Some failed cases are shown in Appendix A.2."}, {"title": "4.4 Results with additional LLMS", "content": "We have additionally evaluated the benchmark with more LLM models as shown in the Table 3. While other powerful open-source LLMs like Llama3-70B and Mixtral-8x7B show similar performance as GPT-4, smaller model Llama3-8B struggles with navigation task, with only 27.74% success rate for navigation tasks with spatial constraint."}, {"title": "4.5 Supervised finetuning", "content": "To evaluate whether our generated data can enhance the ability of smaller LLMs to do embodied reasoning and follow instructions better, we introduce additional data from other layout environments to conduct supervised fine-tuning (Tasks from 34 room layout for training, 5 for validation and 10 for evaluation, please refer to Appendix A.7 for details). This setup allows us to assess generalization across different environments. Using the data generation pipeline described in Section 4.1, with the assistance of GPT-4 (Achiam et al., 2023), we generated question-and-answer (QA) pairs and executable action plans from the training data. These QA pairs were then used for supervised fine-tuning (SFT) a smaller open-source LLM, LLAMA2-7B (Touvron et al., 2023), and its performance was compared with other LLMs, such as GPT-4. Furthermore, this generated data has the potential to enhance foundational models' capabilities in both real-world understanding and task decomposition.\nThe performance of SFT LLAMA closely matches that of GPT-4. In most cases, SFT LLAMA slightly outperforms GPT-4, primarily because SFT LLAMA tends to identify at least one large object that allows the robot to explore, whereas GPT-4 may indicate that no visible objects need exploration in these tasks. The training data teaches the LLMs to actively explore the environment. This results in SFT LLAMA having a longer moving distance than GPT-4 in navigation tasks, thereby increasing the likelihood of finding the target object through more comprehensive exploration.\nResults in Habitat In addition to the Virtual Home environment, we conducted experiments on navigation tasks using another widely used household task simulator, Habitat 2.0 (Figure 5). The results presented in Table 4 demonstrate that our fine-tuning strategy significantly enhances the performance of small-scale LLMs, enabling them to achieve a level comparable to state-of-the-art models like GPT-4. Moreover, the fine-tuned LLMs performs especially well on instruction following."}, {"title": "5 LIMITATIONS", "content": "Our study has some limitations worth noting. The evaluation was conducted in two virtual envi-ronments. While we can benefit from controlled experimentation, it might not fully capture the complexities of real-world settings, potentially leading to a sim-to-real gap. Furthermore, there are areas within the perception module that could benefit from further exploration and improvement."}, {"title": "6 CONCLUSION", "content": "In this work, we present an automatic embodied planning task generation and an LLM agent baseline for benchmark evaluation. We introduce spatial and temporal constraints in navigation and manipulation tasks, which are barely touched by existing embodied planning benchmarks. The method is scalable and can generate infinite number of diverse embodied planning data. The experiment results demonstrate that addition of spatial and temporal constraints, which are common in real world embodied tasks, poses significant challenge to the agent with the most advanced LLM GPT-4, because the success rate dramatically reduces for the tasks involving spatial or temporal constraints. Through supervised finetuning, the much smaller model like LLAMA-7B can match or even slightly outperform GPT-4. Given that we merely introduced an LLM agent as a baseline for benchmark evaluation, there remains considerable potential for baseline improvement in future research."}]}