{"title": "Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization", "authors": ["Mucong Ding", "Chenghao Deng", "Aakriti Agrawal", "Avi Schwarzschild", "Jocelyn Choo", "Tianyi Zhou", "Zichu Wu", "Tom Goldstein", "John Langford", "Anima Anandkumar", "Furong Huang"], "abstract": "While generalization over tasks from easy to hard is crucial to profile language models (LLMs), the datasets with fine-grained difficulty annotations for each problem across a broad range of complexity are still blank. Aiming to address this limitation, we present Easy2Hard-Bench, a consistently formatted collection of 6 benchmark datasets spanning various domains, such as mathematics and programming problems, chess puzzles, and reasoning questions. Each problem within these datasets is annotated with numerical difficulty scores. To systematically estimate problem difficulties, we collect abundant performance data on attempts to each problem by humans in the real world or LLMs on the prominent leaderboard. Leveraging the rich performance data, we apply well-established difficulty ranking systems, such as Item Response Theory (IRT) and Glicko-2 models, to uniformly assign numerical difficulty scores to problems. Moreover, datasets in Easy2Hard-Bench distinguish themselves from previous collections by a higher proportion of challenging problems. Through extensive experiments with six state-of-the-art LLMs, we provide a comprehensive analysis of their performance and generalization capabilities across varying levels of difficulty, with the aim of inspiring future research in LLM generalization. The datasets are available at https://huggingface.co/datasets/furonghuang-lab/Easy2Hard-Bench.", "sections": [{"title": "Introduction", "content": "The development and evaluation of Large Language Models (LLMs) depend crucially on their ability to generalize across a broad spectrum of tasks, ranging from basic to complex problem-solving scenarios. However, among the current prevalent benchmarks, only a select few include problems with annotated difficulty levels. These annotations are typically presented as categorical values (Rein et al., 2023; Shoham & Rappoport, 2024; Bean et al., 2024; Huang et al., 2024) or through pairwise comparisons (Yang et al., 2024b), neither of which provide a detailed portrayal of the difficulty distribution within the dataset. Such granularity is essential for effectively benchmarking and enhancing the adaptability and training approaches of LLMs. To address this gap, there is a pressing need for a benchmark that provides numerical difficulty estimations for problems across various domains."}, {"title": "Related Works", "content": "Our work intersects with several established datasets and benchmarks across different domains, each challenging and assessing specific areas of capabilities of large language models. First, we provide an overview of some notable prior works on related datasets and benchmarks, categorized by domain.\nMath LLM Benchmarks: MATH (Hendrycks et al., 2021b) offers a variety of high-school level math problems, with a small proportion overlapping with our E2H-AMC dataset in the Easy2Hard-Bench, since MATH also collects problems from math competitions like AMC8 and AMC10 (see fig. 2). However, MATH also includes a large portion of easier math problems. MATH offers a coarse five-level difficulty rating for each problem. Agieval (Zhong et al., 2023), SciBench (Wang et al., 2023), MiniF2F (Zheng et al., 2022), and OlympiadBench (He et al., 2024) generally aim towards challenging, math competition-style problems. However, to address the lack of a sufficient number of problems in math competitions, they mix in other sources like the US's SAT and Chinese GaoKao questions, or problems from other science subjects like physics and chemistry into the dataset. As a result, these datasets do not maintain a continuous and uniform span of difficulty nor a clear and recognizable concept of difficulty. On the other hand, apart from focusing on math problem solving, GHOSTS (Frieder et al., 2023) proposes a mixture of five types of abstract mathematical challenges. However, it only has 709 questions and requires professional expert evaluation. Recently, (Yang et al., 2024a) proposes LeanDojo, a toolkit and playground for LLM theorem proving.\nCoding LLM Benchmarks: APPS (Hendrycks et al., 2021a) is one of the earliest benchmarks for evaluating machine learning models on code generation, featuring 10,000 problems with performance assessed against multiple test cases. HumanEval (Chen et al., 2021) is a dataset for code synthesis from docstrings, revealing insights and achieving notable problem-solving rates through repeated sampling strategies, popularizing the pass@k metric. LiveCodeBench (Jain et al., 2024) proposes continuously incorporating new problems from coding competitions, aiming to provide a more holistic assessment. Only a fraction of problems in LiveCodeBench have difficulty ratings provided by specific code platforms; however, some difficulty ratings are categorical, and ratings from different sources are not properly unified and standardized. TACO (Li et al., 2023) introduces more fine-grained problem tagging, yet only part of the problems has a coarse five-level difficulty rating.\nCommon-Sense Reasoning LLM Benchmarks: HellaSwag (Zellers et al., 2019) employs adversarial filtering to challenge models with commonsense inference, where machines lag significantly behind humans. OpenBookQA (Mihaylov et al., 2018) tests multi-hop reasoning on elementary science facts, uncovering significant gaps between human and AI capabilities. WinoGrande (Sakaguchi et al., 2020) enhances the Winograd Schema with a larger, bias-reduced dataset, critically evaluating commonsense reasoning in AI. ARC (Clark et al., 2018) challenges AI with complex science questions beyond current model capacities. BoolQ (Clark et al., 2019) and PIQA (Bisk et al., 2020) expose the limitations of pretrained models in answering natural yes/no questions and physical commonsense queries, respectively, highlighting the discrepancies in real-world reasoning abilities. None of these datasets have fine-grained or continuous difficulty ratings.\nTo the best of our knowledge, there are very few publicly available established LLM datasets and benchmarks on puzzles (e.g., chess, go, maze, sudoku, etc.). Therefore, instead of reviewing datasets, we focus on reviewing the methodological works on LLMs for puzzles, which may or may not have publicized the dataset used for training.\nLLMs for Puzzles: Move-by-move (Jhamtani et al., 2018) introduces a novel large-scale dataset comprising over 298K chess move-commentary pairs from 11K games, focusing on generating natural language descriptions that capture diverse commentary styles and the pragmatic context of each"}, {"title": null, "content": "move. Meanwhile, Chess Transformer (Noever et al., 2020) leverages a massive training corpus of 2.8 million games in Portable Game Notation, fine-tuning OpenAI's GPT-2 to generate strategic chess moves and recognize classic game formations, demonstrating the model's capacity to engage in strategic thinking beyond mere move generation. In a more integrated approach, ChessGPT (Feng et al., 2023) bridges policy learning and language modeling by incorporating a large-scale game and language dataset, enhancing decision-making in chess with combined insights from historical games and analytical strategies. Google Brain's Grandmaster (Ruoss et al., 2024) significantly scales up, training a 270M parameter transformer on a dataset annotated with 15 billion data points from 10 million games, evaluated by the Stockfish (The Stockfish developers, 2024) engine, achieving high-level performance that challenges conventional chess engines and even surpasses AlphaZero's networks in certain aspects without domain-specific adaptations.\nLLM Benchmarks with Difficulty Annotations: Besides the previously mentioned datasets, several other LLM benchmarks annotate each problem with difficulty, though these annotations are typically categorical or presented pairwise. GPQA (Rein et al., 2023), a dataset consisting of graduate-level multiple-choice questions, utilizes the average of a 4-point difficulty rating provided by two expert validators. The medical concepts question answering benchmark, MedConceptsQA (Shoham & Rappoport, 2024), assigns difficulty levels to questions based on the distances among four options in the medical code vocabulary hierarchy, represented as an undirected graph. In this setup, options in harder questions are closely related due to smaller distances. The linguistic reasoning benchmark LingOly (Bean et al., 2024) categorizes question difficulty into five levels based on semantic similarity to English and reasoning complexity. OlympicArena (Huang et al., 2024), a comprehensive cognitive reasoning benchmark, categorizes difficulty into three levels, evaluated by LLMs based on the required abilities for problem-solving, ranging from direct recall of facts to logical or visual reasoning. All difficulty annotations in these benchmarks are categorical. Meanwhile, ConsisEval (Yang et al., 2024b) comprises pairs of questions ordered strictly by difficulty; in each pair, the easy problem is sourced from existing datasets, while the hard problem is derived from the easy one through either human annotation or automatic generation.\nAs an LLM benchmarking and evaluation suite, we share similarities with other LLM evaluation suites in aspects such as the design of evaluation pipelines and methods. We review some prior work on LLM evaluation methods as follows.\nLLM Evaluation Suites and Methods: The Open LLM Leaderboard (Beeching et al., 2023) utilizes EleutherAI's Evaluation Tool to benchmark LLMs across diverse tasks, emphasizing realistic performance assessments. AlpacaEval 2.0 (Dubois et al., 2023) introduces regression analysis to mitigate biases in LLM auto-evaluations, enhancing alignment with human judgments. MT-Bench (Zheng et al., 2023) employs LLMs as judges for multi-turn evaluations on crowdsourced platforms, effectively approximating human preferences. Nevertheless, they generally lack a domain-specific approach with progressively scaled difficulty, which is critical for detailed assessments of LLMs' learning curves and adaptability.\nFinally, our work also explores the generalization behaviors of LLMs, especially under the easy-to-hard setup. We review the prior work on easy-to-hard generalization below.\nEasy2Hard Generalization: (Schwarzschild et al., 2021b) explores how recurrent neural networks generalize from simple to complex tasks by increasing computational steps. (Schwarzschild et al., 2021a) introduces datasets spanning various difficulties to study this generalization capability in tasks from prefix sums to chess puzzles, which are also sourced from Lichess (team, 2024b). On LLMs, (Hase et al., 2024) studies show that pretrained language models can generalize well from easy to"}, {"title": "Easy2Hard-Benchmarking Suite", "content": "Dataset Overview and Statistics. The Easy2Hard-Bench (see Table 1) offers a diverse combination of challenges designed to rigorously assess the capabilities of large language models (LLMs) across varying complexity levels. This benchmark suite comprises six diverse datasets, meticulously curated to test problem-solving skills in various domains. Three of these datasets E2H-AMC, E2H-Codeforces, and E2H-Lichess are newly curated, with difficulties estimated based on human performance statistics. The remaining datasets E2H-GSM8K, E2H-ARC, and E2H-Winogrande are widely used, and we provide additional sample-wise difficulty estimations derived from the performance of thousands of LLMs tracked on the Open LLM Leaderboard (Beeching et al., 2023).\nAccurate difficulty estimation is crucial for evaluating LLM performance across a spectrum of problems and for profiling its capabilities in easy-to-hard generalization. In the E2H-AMC dataset, item difficulty is represented by the percentage of students who answered each question correctly. For the E2H-GSM8K, E2H-ARC, and E2H-Winogrande datasets, the correctness of evaluator LLMs in answering the questions is recorded as binary values. In the E2H-Codeforces and E2H-Lichess datasets, the records indicate the success of users with specific capabilities in solving problems or items and include timestamps to reflect the variation in contestants' capabilities over time. However, deriving the item difficulty level from the raw statistics in these datasets is nontrivial. In this paper, we estimate the difficulty levels for each dataset sample using either IRT or Glicko-2 (with time consideration), an advanced version of the Elo rating system. Figure 1 shows example problems from the three newly curated datasets. The difficulty rating distributions of problems are plotted in Figure 2. The difficulty distributions of existing E2H-GSM8K, E2H-ARC, and E2H-Winogrande datasets are presented in Appendix B.\nDesign Choices and Data Sources. Easy2Hard-Bench integrates a variety of problems"}, {"title": "Standardized Difficulty Estimation", "content": "Method I: IRT without Time Consideration. IRT (Lord & Novick, 2008) is utilized to estimate the difficulty of individual problems by analyzing the response patterns of participants. This model is particularly adept at handling datasets where the assumption of consistent participant ability over time is reasonable, such as in academic competitions like \u0410\u041c\u0421.\nFollowing prior work like (Rodriguez et al., 2021), we applied different variations of IRT models including one-to-four parameter logistic models (denoted as 1PL-4PL models (Rodriguez et al., 2021)"}, {"title": "Verification of Difficulty Estimations", "content": "The quality and reliability of the Easy2Hard-Bench dataset hinge critically on the accuracy of our difficulty estimations compared to human perception. Verification of these estimations is thus essential. Further verification is deemed unnecessary for E2H-AMC, E2H-Codeforces, and E2H-Lichess datasets, where difficulty estimations are derived directly from well-established and highly publicized human performance metrics. The difficulty estimation of all these three datasets is based on the metrics of human participants in real evaluation and the ratings of human participants by well-accepted rating systems acknowledged by large professional communities (for example, in Appendix E we compare the estimated difficulty with professional guides, e.g., contest difficulty rating on AoPS and justify the natural well-alignment; see Appendix E for E2H-Codeforces and E2H-Lichess). However, further verification is imperative for E2H-GSM8K, E2H-ARC, and E2H-Winogrande datasets, which are primarily based on model performance statistics, to ensure that these metrics accurately reflect human performance potential.\nHuman Verification of E2H-GSM8K, E2H-ARC, E2H-Winogrande Difficulties. To validate our model-based difficulty estimations, we conduct surveys where participants are asked to rank problem difficulties. We show participants pairs of problems and ask them to determine which of the two is more difficult. In our survey, we show the participants 10 pairs of problems from each of the 3 datasets and request they rank the difficulty of two questions in each pair. The majority vote from these surveys is then compared to our model-derived difficulty rankings to assess alignment. For each pair of problems, we use the majority vote from 5 participants' responses as the"}, {"title": "Benchmarking SoTA LLMs via Easy2Hard-Bench", "content": "Model Selections and Details. In light of the novel and challenging problems presented in our Easy2Hard-Bench, we have selected the most advanced generations and versions from both proprietary and open-source large language model (LLM) families for evaluation. Our lineup includes GPT4-Turbo (Achiam et al., 2023), Claude3-Opus (Anthropic, 2024), and Gemini 1.5-Pro (Reid et al., 2024) from the proprietary series, alongside Llama3-70B (AI@Meta, 2024), Mixtral-8x22B (Jiang et al., 2024), and Qwen1.5-110B (Bai et al., 2023) from the open-source series. By leveraging these state-of-the-art LLMs, we aim to gain a deeper understanding of AI capabilities to solve problems of increasing difficulty across different domains. The selection of these models ensures that even on the most challenging problems, their performance provides valuable insights, allowing us to assess the capabilities of these LLMs across a spectrum of difficulties comprehensively.\nEvaluation Setups and Metrics. Our evaluation setups predominantly adopt metrics from existing evaluation pipelines, as the evaluation design for assessing math, coding, and reasoning tasks is thoroughly established. An exception is made for chess puzzles, a relatively unexplored challenge for LLMs, necessitating a specifically tailored evaluation setup and prompt template (see appendix F). While techniques like chain-of-thought prompting (Wei et al., 2022) and majority voting (Wang et al., 2022) can enhance LLM performance, our focus remains on benchmarking datasets and difficulty ratings with naive zero- or few-shot prompting setups. We defer some more complex setups to Appendix F and future work.\n\u2022 E2H-AMC: Similar to MATH (Hendrycks et al., 2021b), problems and answers are encoded as \\LaTeX strings, with the final answer required to be enclosed in \u201c\u53e3\u201d.Accuracy of matching the answer within ``''s is reported.\n\u2022 E2H-Codeforces: Following the evaluation frameworks of HumanEval (Chen et al., 2021) and APPS (Hendrycks et al., 2021a), our evaluation package supports metrics like \"pass@k\". However,"}, {"title": null, "content": "due to resource constraints, in the paper we mainly focus on the test case average accuracy, a standard from APPS.\n\u2022 E2H-Lichess: Chess puzzles are converted into QA format. Prompts are designed to describe the puzzle using multiple chess notations including FEN, PGN, and UCI. Moreover, evaluations and annotations from the Stockfish Chess Engine (The Stockfish developers, 2024) are provided. LLMs are asked to format their answers, the next chess moves, in both PGN or UCI notation. The answer matching criterion detailed in Appendix F ensure that correct answers in either notation can be recognized.\n\u2022 E2H-GSM8K, E2H-ARC, and E2H-Winogrande: For these existing datasets, we adhere to the standard evaluation protocols used by Open LLM Leaderboard (Beeching et al., 2023), which applies 5-, 25-, and 5-shot prompting for E2H-GSM8K, E2H-ARC, and E2H-Winogrande, respectively. As E2H-ARC and E2H-Winogrande are multiple-choice QAs requiring log-probabilities from LLMs, proprietary models cannot be evaluated on them."}, {"title": "Profiling Easy2Hard Generalizations", "content": "Contrary to other LLM benchmarking suites like (Suzgun et al., 2022), the Easy2Hard-Bench provides sample-wise continuous difficulty ratings with uncertainty for all six datasets, enabling a big step forward in benchmarking LLM capabilities and profiling their behaviors. Instead of only assessing the static behavior of specific checkpoints, our approach allows for fine-grained profiling of LLMs as they generalize across various training and evaluation difficulties. This also caters to the need to simulate challenging problems like weak-to-strong generalization (Burns et al., 2023). To our best knowledge, Easy2Hard-Bench is the first to deliver detailed easy-to-hard generalization results across continuous, wide-range of difficulties on LLMs.\nMethod to Profile Easy2Hard Generalizations over Ranges of Training and Evaluation Difficulties. To capture the \"two-dimensional\" generalization behavior, we divide the training data into a bins based on difficulty ratings and undertake training a + b times: a times on each difficulty bin and b times on randomly chosen subsets of the same size. During evaluation, we assess all a + b trained LLMs across the complete range of evaluation difficulties. We further interpolate the evaluation performances of the a LLMs trained at different difficulty levels, by employing an RBF kernel. We also subtract the \u201cbackground performance\" of the b LLMs trained on random difficulties and thus highlight the generalization gain. The results are visually represented through contour plots in Figure 6.\nExperimental Setups. In our preliminary experimental exploration, we focus on Super-vised Finetuning (SFT) with relatively smaller LLMs, while deferring more specialized finetuning frameworks for future studies. We deploy three setups on the E2H-AMC, E2H-GSM8K, and E2H-Lichess datasets, setting a = 7 and b = 1 unless specified otherwise.\n\u2022 E2H-AMC and E2H-GSM8K: We utilize GPT3.5-Turbo (Achiam et al., 2023), still a leading proprietary LLM with accessible fine-tuning APIs. On E2H-AMC, where the training split is relatively small (1,000 training and 2,975 evaluation samples), we invert the roles of train and eval splits.\n\u2022 E2H-Lichess: We employ a novel approach by utilizing GPT2 models, which have been retrained on a vast corpus of real-world chess games sourced from E2H-Lichess (not overlapping with the puzzles). To better capture the nuances of chess move notations, we replace the standard tokenizer with a specialized one designed specifically for chess moves, similar to the strategy employed in (Ruoss et al., 2024). The optimal pretrained GPT2 checkpoint is sourced from LeonLLM (LLM, 2024), and used to profile the easy-to-hard generalization on the Lichess puzzles in Easy2Hard-Bench.\nObservations on Easy2Hard Generalization Behaviors. In Figure 6, we present the easy-to-hard generalization margins through contour plots. Across different datasets and models, a common pattern emerges in the generalization behavior: a \u201cgeneralization ridge\" typically aligns"}, {"title": "Limitations", "content": "This section summarizes the major limitations of this work. This work primarily focuses on dataset and benchmark creation, which brings about several limitations.\n\u2022 In our evaluations, we only consider primary setups and metrics because we are mainly a dataset and benchmark work. We did not explore many state-of-the-art setups or methods, such as chain of thought for evaluation. This focus on fundamental approaches might limit the usefulness and comprehensiveness of our findings regarding the latest evaluation techniques.\n\u2022 We conducted a human evaluation to verify the estimated difficulty on three datasets in Easy2Hard-Bench: E2H-GSM8K, E2H-ARC, and E2H-Winogrande. We involved 50 participants to rank 100 pairs of problems per dataset. Due to time and resource constraints, we could only secure a limited number of participants for the human evaluation. This limited scale affects the robustness of our difficulty estimation verification. To mitigate this limitation, we also considered the GPT4-Turbo ranking as a proxy, but found that GPT4-Turbo's ranking did not closely match human difficulty rankings for problem pairs, indicating it is not a very reliable proxy for human judgment in this context.\n\u2022 Although we considered a collection of six datasets covering four domains math, coding, puzzles, and reasoning this collection might not be exhaustive. This selection does not fully elaborate on all possible domains and tasks that language models could encounter. Other domains and datasets could further enrich and diversify the Easy2Hard-Bench suite."}, {"title": "Difficulty Estimation Details", "content": "E2H-AMC Item difficulties IRT Logistic Regression\nE2H-GSM8K Sample-wise evaluation Filtering IRT Variational Bayes Continuous Difficulties & \nE2H-ARC results of LLMs Uncertainties\nE2H-Winogrande \nE2H-Codeforces Submission status & Glicko-2 Calculation\n Contestant ratings\nPuzzle ratings & Glicko-2 Calculation\nE2H-Lichess\nPlayer ratings"}, {"title": "Preprocessing to IRT/Glicko-2 Inputs", "content": "E2H-AMC. We collect the item difficulty directly or indirectly from the official reports. Item difficulty refers to the percentage of participants answering an item correctly. MAA provides item difficulties of AMC and AIME directly. HMMT presents the score of each individual or team on each problem, with which we can compute the corresponding item difficulty.\nE2H-Codeforces. We collect the submission records and contestant rating history via the official API. The submission record shows that whether the specific submission is accepted or not. The rating history illustrates the variation of a contestant's performance. Moreover, we scrape the official rating for problems, and we use it as an alignment of our estimation.\nE2H-Lichess. We gather the puzzle rating and the player ratings in each puzzle. Puzzle rating shows the difficulty of the puzzle approximately while player ratings indicates the fluctuation of strength.\nE2H-GSM8K, E2H-ARC, E2H-Winogrande. We gather the evaluation results of LLMs on each problem from these datasets reported in open LLM leaderboard. For each dataset, we use a greedy search algorithm to find a subset of LLMs so that the difficulty ranking results based on the average accuracy of these models is as near as possible to human verification results."}, {"title": "IRT", "content": "IRT (Lord & Novick, 2008) is utilized to estimate the difficulty of individual problems by analyzing the response patterns of participants. It is based on the idea that the probability of a correct response to an item (problem) is a logistic function of some person and item parameters. Since item difficulty is one of item parameters in IRT method, we aim to estimate the difficulty of problems by fitting the IRT model to the performance metrics we collect previously.\nThe logistic model we used in IRT, specifically the 1PL-with-guessing model, is expressed as follows:\n$P(X_{ui} = 1 | \\theta_u, b_i, c_i) = c_i + \\frac{1-c_i}{1 + e^{-(\\theta_u - b_i)}}$ where $P(X_{ui} = 1 | \\theta_u, b_i, c_i)$ is the probability that user u correctly solves problem i. $\\theta_u$ represents the latent ability of user u. $b_i$ is the difficulty parameter of problem i.\nTo illustrate how we choose the parameters of IRT model for difficulty estimation, we compare"}, {"title": null, "content": "four variations of IRT models (denoted as 1PL-4PL models) with different number of logistic model parameters.\n\u2022 1PL: The model assumes that guessing is included in the ability and all items fitting the model sharing the same discrimination. So the only parameter to describe the items is $b_i$, i.e.,\n$P(X_{ui} = 1 | \\theta_u, b_i) = \\frac{1}{1 + e^{-(\\theta_u - b_i)}}$\n\u2022 2PL: The model assumes that guessing is included in the ability but the item i fitting the model has the discrimination $a_i$. So the parameters to describe the items are $a_i$ and $b_i$, i.e.,\n$P(X_{ui} = 1 | \\theta_u, a_i, b_i) = \\frac{1}{1 + e^{-a_i(\\theta_u - b_i)}}$\n\u2022 3PL: The model assumes that guessing is excluded in the ability and formulated as an asymptotic minimum of $c_i$ for each item. So the parameters to describe the items are $a_i$, $b_i$ and $c_i$, i.e.,\n$P(X_{ui} = 1 | \\theta_u, a_i, b_i, c_i) = c_i + \\frac{1 - c_i}{1 + e^{-a_i(\\theta_u - b_i)}}$\n\u2022 4PL: Besides the guessing formulated as $c_i$, the model assumes that the item is intrinsically unsolvable with probability and the corresponding asymptotic maximum is formulated as $d_i$. So the parameters to describe the items are $a_i$, $b_i$, $c_i$ and $d_i$, i.e.,\n$P(X_{ui} = 1 | \\theta_u, a_i, b_i, c_i, d_i) = c_i + \\frac{d_i - c_i}{1 + e^{-a_i(\\theta_u - b_i)}}$\nFor our case, the problems from both math competitions (E2H-AMC) and prevalent reasoning task dataset (E2H-GSM8K, E2H-ARC, E2H-Winogrande) are well-defined and principally solvable, suggesting that d\u2081 = 1, and there is no discrimination among different problems within the same dataset, suggesting that b\u2081 = 1. Therefore, we use only two parameters, difficulty $b_i$ and guessing $c_i$, in our difficulty estimation and propose 1PL-with-guessing (1gPL) as following\n$P(X_{ui} = 1 | \\theta_u, b_i, c_i) = c_i + \\frac{1 - c_i}{1 + e^{-(\\theta_u - b_i)}}$ \nFor E2H-AMC, the input examples for difficulty estimation are exactly item difficulties of problems released in the official reports, thus we do not need any further preprocessing. Moreover, IRT implicitly assumes that users consistently solve problems. Although a student usually participants in the contest at the specific level only once, we assume that the ability of students taking contests at the same level in different years is constant."}, {"title": "Glicko-2", "content": "The update mechanism in the Glicko-2 system incorporates the outcome of games, the reliability of the rating, and the time between games as follows:\n$r' = r + \\frac{q}{c^2} \\sum_{j=1}^{n} g((rd)_j) (s_j - E(s_j | r, r_j)), rd' = \\sqrt{\\frac{1}{rd^2} + \\frac{1}{c^2}}$ where $r'$ and $rd'$ are the updated rating and rating deviation, respectively. $q = \\log \\frac{10}{400}$ is a scaling factor. $g(RD) = \\frac{1}{\\sqrt{1 + 3q^2 (RD^2) / \\pi^2}}$ is a function that reduces the impact of matches with"}, {"title": null, "content": "opponents having high rating deviations. $s_j$ represents the outcome of game $j$ (1 for a win, 0.5 for a draw, 0 for a loss). $E(s_j | r, r_j)$ is the expected score against opponent $j$, who has rating $r_j$ and deviation $RD_j$. $d^2$ is the variance of the rating changes, $d^2 = 1 / (q^2 \\sum_{j=1}^{n} g(RD_j)^2 E(s_j | r, r_j) (1 - E(s_j | r, r_j)))$.\n\u2022 Step 1: Ancillary quantities. During each rating period (such as the interval between contests), consider a player with current rating $\\mu$ and rating deviation $\\phi$. Assuming that this player plays against $m$ opponents with ratings $\\mu_1, \\dots, \\mu_m$ and rating deviations $\\phi_1, \\dots, \\phi_m$ and these games result in scores $s_1, \\dots, s_m$, we compute two ancillary quantities $v$ and $\\Delta$:\n$v = \\sum_{j=1}^{m} g(\\phi_j)^2 E[s | \\mu, \\mu_j, \\phi_j] \\{1 - E[s | \\mu, \\mu_j, \\phi_j] \\}$ ,  $\\Delta = v \\sum_{j=1}^{m} g(\\phi_j) \\{s_j - E[s | \\mu, \\mu_j, \\phi_j] \\}$ where\n$g(\\phi_j) = \\frac{1}{\\sqrt{1 + 3 \\phi_j^2 / \\pi^2}}$ , $E[s | \\mu, \\mu_j, \\phi_j] = \\frac{1}{1 + exp\\{-g(\\phi_j) (\\mu - \\mu_j) \\}}$\n\u2022 Step 2: Rating volatility. The second step is to update rating volatility $\\sigma$. This parameter measures the expected fluctuation of rating over time. A larger $\\sigma$ means that the player behaves more inconsistently across the past rating periods. With a small constant $\\tau$ constraining the volatility over time, we use the iterative procedure to find the solution $x_0$ for $f(x) = 0$ where $f$ is given by\n$f(x) = \\frac{e^x (\\Delta^2 - \\sigma'^2 - v - e^x)}{2 (\\phi^2 + v + e^x)^2} - \\frac{x - 2 \\ln \\sigma}{\\tau^2}$ and set the new rating volatility as $\\sigma' = exp(x_0 / 2)$.\n\u2022 Step 3: Rating and rating deviation. With the new rating volatility $\\sigma'$, we calculate the updated rating deviation $\\phi'$ and the updated $\\mu'$ as follows\n$\\frac{1}{\\phi'^2} = \\sqrt{\\frac{1}{\\phi^2} + \\frac{1}{v}}$ , $\\mu' = \\mu + \\phi'^2 \\sum_{j=1}^{m} g(\\phi_j) \\{ s_j - E[s | \\mu, \\mu_j, \\phi_j] \\}$ .\nFor the problems in E2H \u2013 Codeforces, For the problems in E2H \u2013 Lichess, since Lichess uses the Glicko-2 system to rate the players and the puzzles, so we inherit the puzzle ratings for the original problems and convert them into [0, 1] scale."}, {"title": "Difficulty Verification Details", "content": "Natural Verification of E2H-AMC, E2H-Codeforces, E2H-Lichess\nFor the problems in E2H-AMC, E2H-Codeforces and E2H-Lichess, the statistics used for difficulty estimation are human performance metrics in real-world competitions, contests and games. These sources are either authoritative (competition organization committee) or rigorously examined by the professional community. Moreover, both IRT and Glicko-2 are prevalent rating systems used in various scenarios. Thus, we are confident with the difficulty estimation results on these three datasets. Instead of having a large scale of human verification, we randomly sample some problems and check their contents and estimated difficulty. Generally, the estimated results are well aligned with the human understanding to the problems via human's perspective. We use the problems presented in fig. 1 to briefly illustrate the alignment."}, {"title": "Human Difficulty Ranking", "content": "For the problems in the datasets E2H-GSM8K", "question": "are these model performance metrics a good surrogate of human? To answer this", "parts": 1, "Introduction": "We briefly introduce the participants' task, determining which question in each pair is more difficult, and the content of three datasets"}]}