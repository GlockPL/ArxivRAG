{"title": "CELI: Controller-Embedded Language Model Interactions", "authors": ["Jan-Samuel Wagner", "Dave DeCaprio", "Abishek Chiffon Muthu Raja", "Jonathan M. Holman", "Lauren K. Brady", "Sky C. Cheung", "Hosein Barzekar", "Eric Yang", "Mark Anthony Martinez II", "David Soong", "Sriram Sridhar", "Han Si", "Brandon W. Higgs", "Hisham Hamadeh", "Scott Ogden"], "abstract": "We introduce Controller-Embedded Language Model Interactions (CELI), a framework that integrates control logic directly within language model (LM) prompts, facilitating complex, multi-stage task execution. CELI addresses limitations of existing prompt engineering and workflow optimization techniques by embedding control logic directly within the operational context of language models, enabling dynamic adaptation to evolving task requirements. Our framework transfers control from the traditional programming execution environment to the LMs, allowing them to autonomously manage computational workflows while maintaining seamless interaction with external systems and functions. CELI supports arbitrary function calls with variable arguments, bridging the gap between LMS' adaptive reasoning capabilities and conventional software paradigms' structured control mechanisms. To evaluate CELI's versatility and effectiveness, we conducted case studies in two distinct domains: code generation (HumanEval benchmark) and multi-stage content generation (Wikipedia-style articles). The results demonstrate notable performance improvements across a range of domains. CELI achieved a 4.9 percentage point improvement over the best reported score of the baseline GPT-4 model on the HumanEval code generation benchmark. In multi-stage content generation, 94.4% of CELI-produced Wikipedia-style articles met or exceeded first draft quality when optimally configured, with 44.4% achieving high quality. These outcomes underscore CELI's potential for optimizing AI-driven workflows across diverse computational domains.", "sections": [{"title": "Background", "content": "The rapid evolution of language models (LMs) has expanded their capabilities from basic question-answering to complex problem-solving tasks. This progression has driven research towards increasingly sophisticated approaches for leveraging LM capabilities through advanced prompt engineering techniques and orchestration layers. Early applications of LMs primarily relied on manually crafted single prompts, where performance was highly dependent on prompt precision and contextual relevance (Brown, 2020). Subsequent research introduced more advanced strategies such as Chain-of-Thought (Wei et al., 2022), Tree of Thoughts (Yao et al., 2023a), ReAct (Yao et al., 2023b), and Reflexion (Shinn et al., 2023), each enhancing LMs' problem-solving and decision-making abilities. As the field matured, structured approaches for prompt management and workflow optimization emerged. Frameworks like DSPy (Khattab et al., 2023) and Trace (Cheng et al., 2024) offered significant improvements in systematization and optimization.\nHowever, existing frameworks frequently encounter challenges in real-time adaptation, especially when managing complex, multi-stage tasks. These limitations include the inability to handle unforeseen scenarios, lack of dynamic task prioritization, and reduced context awareness across multi-stage processes. To address these challenges, we introduce the Controller-Embedded Language Model Interactions (CELI) framework. CELI embeds control logic directly within LM prompts, enabling dynamic execution of complex tasks and seamless interfacing with object-oriented programming environments. This approach allows CELI to overcome the limitations of static prompt structures, offering a flexible, adaptive, yet powerful solution for managing intricate, multi-stage tasks that require both nuanced natural language processing and precise programmatic execution."}, {"title": "Related Work", "content": "Recent work has explored discrete optimization and reinforcement learning for single LM calls. The concept of foundation model programming (Khattab et al., 2023) has emerged, treating LMs as integral components of larger computational systems rather than standalone question-answering tools. Existing frameworks like LangChain (Chase, 2022) and Semantic Kernel (Microsoft, 2023) have made significant progress in connecting LMs with external tools. These frameworks have improved the efficiency and reliability of LM-driven tasks, expanding their potential applications. The versatility of these systems is often demonstrated through support for various learning paradigms, including zero-shot, one-shot, and multi-shot approaches, building upon the capabilities of in-context learning (McCann et al., 2018; Radford et al., 2018; Brown, 2020). Recent work has also explored discrete optimization and reinforcement learning for single LM calls (Guo et al., 2023; Pryzant et al., 2023), as well as enhancing LMs' interaction with external knowledge sources (Lewis et al., 2023).\nWhile these approaches aim to enhance LM performance in specific scenarios, they often fall short in addressing the broader limitations of existing frameworks. These limitations include difficulties in maintaining coherence in complex, expansive processes, inability to autonomously refine strategies based on intermediate results, and challenges in error detection and recovery during task execution. CELI addresses these gaps by introducing a comprehensive approach to embedding control logic within LM workflows, enabling more effective handling of complex, multi-stage tasks requiring real-time adaptations."}, {"title": "Foundational Elements of the CELI Framework", "content": "CELI addresses several key limitations of existing frameworks through its novel approach to embedding control logic within LM prompts. For instance, while frameworks like LangChain struggle with maintaining coherence across extended multi-stage processes, CELI's cumulative context mechanism ensures consistent state management throughout task execution. This allows CELI to handle complex tasks like multi-section document generation without losing context or coherence. Additionally, where traditional frameworks often rely on predefined workflows, CELI's dynamic task prioritization enables real-time adaptation to unforeseen scenarios. For example, in code generation tasks, CELI can autonomously reprioritize subtasks based on intermediate results, such as shifting focus to error handling when unexpected inputs are encountered. Furthermore, CELI's ability to seamlessly interface with external tools and functions allows it to overcome the limited scope of single LM calls. This is particularly evident in tasks requiring specialized knowledge or computations, where CELI can dynamically invoke relevant tools or APIs as needed, maintaining a fluid workflow that adapts to the task's evolving requirements."}, {"title": "Embedded-Controller Logic", "content": "CELI framework's embedded controller logic (which \u201ccalls the shots\u201d) is implemented through a Job Description module, which serves as the blueprint for task execution. This Job Description is compiled into two key components: a system message and an initial user message. The system message encapsulates the LM's role, objectives, task list, and operational rules, while the initial user message initiates the workflow. Together, these elements form the foundation of CELI's control mechanism, guiding the LM's behavior throughout the execution process. For a detailed explanation of these components and their roles in the CELI framework, see Appendix A.2.1 and A.2.2."}, {"title": "Workflow Architecture", "content": "Figure 1 illustrates the CELI framework's architecture and workflow, starting with the initialization of both the Job Description and Tool Implementations, which define the tasks and available tools for the system. This feeds into the Processor Loop, where the core task execution takes place. During this loop, the LM may invoke specific tools to complete its tasks, interacting with the Tool Implementations Object that stores these functions. Responses are generated based on tool outputs or direct LM reasoning and are continuously fed into the cumulative context, which maintains the state across multiple iterations of the task execution. Errors during tool execution are handled within this context, ensuring smooth progression through the workflow.\nThe Monitor component oversees the workflow's progress, checking for termination conditions. It monitors the process to ensure tasks are fully completed, popping context when all tasks are complete, preventing context window limits from being reached. Before termination, the system saves relevant assets, such as logs or results. Once all tasks are finalized, the process terminates. This architecture highlights CELI's ability to dynamically handle complex tasks by combining LM-based processing with tool execution, while maintaining context and ensuring robust monitoring for task completion.\nA comprehensive overview of the CELI framework architecture and its core components is provided in Appendix A.1 and A.2."}, {"title": "Iterative Task Execution", "content": "Figure 2 illustrates the detailed iterative task execution process within the CELI framework. Each task begins with an initial input, which is processed either directly or through a function call, as needed. The function returns, represented with a dotted outline, indicate that the function execution is optional and may not always occur, depending on the task requirements. The output is generated from this process and is either passed forward to the next iteration or further refined. Throughout the execution, CELI performs a continuous check for termination conditions to determine if the task has reached completion. If the termination criteria are met, the process concludes, and the results are stored. Otherwise, the task continues to iterate with new inputs and optional function executions until it reaches the termination point. This figure highlights the flexibility of CELI in dynamically adapting its execution based on task needs, ensuring efficient task completion while maintaining adaptability for complex workflows. For an in-depth explanation of CELI's workflow execution process, refer to Appendix A.3."}, {"title": "Addressing Limitations and Fostering Emergent Capabilities", "content": "This architecture and workflow directly address many of the limitations observed in traditional LM frameworks. By embedding control logic within prompts and employing an iterative execution process, CELI enables real-time adaptations to unforeseen scenarios and evolving task requirements. The Processor Loop and Monitor component work in tandem to maintain context awareness across multi-stage tasks, facilitating coherent execution of complex, long-running processes. The framework's ability to dynamically invoke tools and refine strategies based on intermediate results supports autonomous learning and improvement during task execution. Furthermore, CELI's flexible architecture fosters emergent properties such as context-awareness, self-guided learning, and adaptability. These features collectively enable CELI to handle intricate, multi-stage tasks with a level of sophistication and efficiency that surpasses traditional prompt-based approaches, paving the way for more advanced AI-driven problem-solving across diverse domains. The advanced features of CELI, including monitoring, embedding, and mapping capabilities, are further detailed in Appendix A.4."}, {"title": "Case Studies", "content": ""}, {"title": "Native Integration with Traditional Programming Execution Environment (HumanEval)", "content": ""}, {"title": "Background and Objectives", "content": "The HumanEval dataset (Chen et al., 2021), introduced by OpenAI, provides a benchmark for evaluating the coding abilities of LMs through 164 Python coding challenges. These challenges span a wide range of programming concepts and algorithms, offering a comprehensive test of code generation capabilities. While primarily designed for code assessment, HumanEval presents an opportunity to showcase CELI's broader potential in integrating LMs with diverse programming tools and paradigms.\nOur objectives for this case study were to demonstrate CELI's effectiveness in tackling coding challenges through its native integration with execution environments, evaluate its performance against traditional LM approaches and state-of-the-art frameworks, and assess its ability to implement an iterative process of solution generation, testing, and refinement."}, {"title": "Implementation", "content": "The implementation is built around a JobDescription module defining key tasks and HumanEvalTools class providing essential functionality. The framework retrieves coding prompts, develops test cases, iteratively writes and tests code, and saves final outputs. It employs sandboxed execution for safety and supports both user-defined and official tests. This implementation showcases CELI's ability to integrate effectively with LMs and programming environments while maintaining security and flexibility in automated code generation and testing. JobDescription tasks and HumanEvalTools functions are highlighted in Appendix B.1 and B.2."}, {"title": "Evaluation Process", "content": "CELI's performance on the HumanEval benchmark was evaluated through two main approaches. First, we performed a qualitative analysis of the logs generated during CELI's problem-solving process. These logs offered insights into how CELI interpreted problems, generated test cases, implemented solutions, and refined code based on test results, illustrating its reasoning and adaptation capabilities (Log Analysis). Second, we conducted a quantitative assessment by running CELI's solutions against the official HumanEval test cases and calculating the number of correct solutions. This provided a clear measure of CELI's ability to generate functionally correct code across diverse programming challenges (Quantitative Analysis).\nTo contextualize CELI's performance, we compared it against baseline GPT-4 models and other state-of-the-art frameworks. We used both GPT-4-Turbo (1106) and (0125) as baselines, with scores obtained from a comprehensive benchmark study (OpenAI, 2024). This dual-baseline approach allowed for fair comparisons, as existing frameworks primarily used the 1106 version, while we found the 0125 version more suitable for our specific implementation (further discussed in Appendix B.4). By examining CELI's results in relation to these baselines and other frameworks, we gained a comprehensive understanding of its performance in the context of current LM capabilities."}, {"title": "Results", "content": "Log Analysis Analysis of the logs revealed insights into CELI's problem-solving process across various HumanEval tasks. The logs demonstrated CELI's ability to interpret problems, generate test cases, implement solutions, and refine code based on test results. This qualitative analysis showcased CELI's reasoning and adaptation capabilities, as illustrated in\nA representative example from the logs for task HumanEval/2 is paraphrased below (see Appendix B.3 for the full log):\n1. CELI analyzed the problem and developed test cases.\n2. Identified key scenarios to test various number types.\n3. Created check(candidate) function with appropriate test cases.\n4. Implemented function and attempted to run tests; encountered error.\n5. Corrected error, ran tests again; identified one test case failure.\n6. Analyzed failure, attributed to floating-point issues; updated function accordingly.\n7. Updated function passed all tests; CELI saved final implementation.\nThis log analysis reveals several key strengths of CELI's approach to coding challenges. Firstly, CELI demonstrates proactive error prevention by developing comprehensive test cases (steps 1-3) that cover various number types, showcasing its ability to anticipate edge cases. Secondly, CELI's error handling and debugging capabilities are evident in its response to the initial test failure (steps 4-5). Rather than simply regenerating code, CELI analyzed the failure, identified the specific issue with floating-point numbers, and made targeted corrections. This targeted refinement process likely contributes to CELI's high success rate on the HumanEval benchmark, as it allows for efficient and effective code improvement without unnecessary rewrites. Furthermore, CELI's ability to maintain context throughout the problem-solving process (steps 1-7) enables it to build upon its initial approach rather than starting from scratch with each iteration. This contextual awareness and iterative refinement capability align with CELI's strong performance relative to specialized coding frameworks, suggesting that these problem-solving strategies are key factors in its effectiveness across diverse programming challenges.\nQuantitative Analysis CELI's performance on the HumanEval benchmark was evaluated against baseline GPT-4 models and state-of-the-art coding frameworks."}, {"title": "Document Generation with Multi-Stage Processing (Wikipedia Articles)", "content": ""}, {"title": "Background and Objectives", "content": "Wikipedia articles present unique challenges for language models due to their demand for factual accuracy, consistent structure, and comprehensive coverage across diverse topics. These articles require synthesizing information from multiple sources, maintaining a neutral point of view, and adhering to specific formatting guidelines. Wikipedia's breadth of subjects also tests a system's adaptability to various domains. We chose Wikipedia as an exemplar for evaluating CELI's capabilities in multi-stage document processing because it represents a gold standard for structured, factual content creation. Our objective was to assess CELI's performance in generating high-quality articles across diverse topics (Bands, Drugs, and Countries), using existing articles as templates. This study aimed to demonstrate CELI's effectiveness in maintaining coherence, accuracy, and appropriate structure in complex, multi-domain content generation tasks."}, {"title": "Implementation", "content": "The implementation is built around a JobDescription defining key tasks and WikipediaTools class providing essential functionality. The framework retrieves example and target Wikipedia pages, analyzes section structures, develops content, and compiles final articles. It employs content retrieval, question-asking, and drafting mechanisms for comprehensive article generation. This implementation showcases CELI's ability to integrate effectively with LMs for complex document generation while maintaining flexibility in automated content creation and structuring. The use of two advanced LMS, GPT-4 (OpenAI et al., 2024) and Claude 3.5 Sonnet (Anthropic, 2024), allows CELI to benefit from their complementary strengths in natural language understanding and generation. Detailed descriptions of JobDescription tasks and WikipediaTools functions are provided in Appendix C.1 and C.2."}, {"title": "Evaluation Process", "content": "CELI's performance in generating Wikipedia-style articles was evaluated through two complementary approaches. First, we conducted a qualitative analysis by examining the logs generated during CELI's article creation process, providing insights into its content structuring, information gathering, and adaptation strategies (Log Analysis). Second, we performed a quantitative assessment of 18 articles across three categories (Bands, Drugs, and Countries) using GPT-4 and Claude 3.5 Sonnet on a 0-6 scale (Quantitative Analysis). Scores of 4 and above were considered to meet or exceed first draft quality, with 4 defined as \"Satisfactory: Acceptable as a first draft but requires refinement.\" Scores of 5 and above were classified as achieving high quality, indicating comparable or superior quality to the ground truth. We analyzed performance across categories and models, including a best-case scenario selecting the highest-performing model for each article. This dual approach allowed us to assess both CELI's problem-solving process and the quality of its output. Full evaluation criteria, including limitations and potential for hallucination, are detailed in Appendix C.3."}, {"title": "Results", "content": "Our evaluation of CELI's performance in generating Wikipedia-style articles combined qualitative log analysis with quantitative assessment.\nLog Analysis Analysis of the logs revealed CELI's sophisticated approach to multi-stage document generation, particularly in handling complex, long-form content like Wikipedia articles. The logs demonstrated CELI's ability to structure content, identify information gaps, ask relevant questions, and adapt to challenges. A representative example from the logs for the Jonas Brothers article generation (with Led Zeppelin as example article) is paraphrased below (see Appendix C.4 for the full log):\n1. CELI analyzed the example document structure and content.\n2. Identified key sections to include in the target article.\n3. Generated relevant questions (to query vector database) in order to fill information gaps.\n4. Encountered errors in retrieving additional information.\n5. Adapted approach to use available information from references and example document.\n6. Defined subsections for the article based on available information.\n7. Drafted and saved the article section, demonstrating ability to complete task despite challenges.\nAnalysis of the logs revealed CELI's sophisticated approach to multi-stage document generation, particularly in handling complex, long-form content like Wikipedia articles. The representative example from the Jonas Brothers article generation process illustrates several key strengths: CELI's ability to analyze and adapt to complex document structures, proactively gather information by generating relevant questions, and resiliently handle information retrieval challenges. Notably, CELI demonstrated context-aware content structuring by defining subsections based on available information, and showed persistence in completing the task despite obstacles. These capabilities suggest CELI's potential for producing well-organized, comprehensive articles across diverse topics. The framework's approach of combining structural understanding, proactive information gathering, and adaptive refinement not only showcases its effectiveness in Wikipedia article generation but also indicates its versatility for tackling a wide range of complex document creation tasks. This qualitative analysis provides valuable insights into CELI's problem-solving strategies and adaptability, setting the stage for a deeper understanding of its performance in generating Wikipedia-style articles."}, {"title": "Discussion", "content": "The CELI framework represents a significant advancement in integrating LMs into complex computational systems. By embedding control logic directly within LM prompts, CELI addresses key limitations identified in existing frameworks such as LangChain (Chase, 2022), Semantic Kernel (Microsoft, 2023), DSPy (Khattab et al., 2023), and Trace (Cheng et al., 2024), particularly in handling complex, multi-stage tasks with real-time adaptations.\nThe application of CELI in our case studies illustrates its effectiveness across three distinct areas: integrating control mechanisms within language model prompts, facilitating iterative problem-solving through interactive recursion, and managing the execution of complex, multi-phase tasks. In the HumanEval benchmark, CELI demonstrated the ability to autonomously navigate through the steps of code generation, testing, and correction, achieving a notable improvement over the baseline GPT-4 model. The embedded control logic enables CELI to adjust its behavior in real-time, handling exceptions and reordering tasks based on intermediate outcomes. In the task of multi-stage document generation, CELI exhibited strong performance by coordinating content development in a structured manner, producing high-quality outputs in the majority of cases. These findings suggest that CELI is well-suited for optimizing workflows that require both adaptive reasoning and precise task management across varied computational domains.\nSeveral emergent properties of CELI were observed across the case studies. CELI demonstrated strong context-awareness in the HumanEval benchmark, maintaining contextual understanding throughout problem analysis, test case creation, function implementation, and result-based refinement. Self-guided in-context learning was evident in the Wikipedia article generation task, where CELI autonomously adapted its approach when faced with information retrieval challenges, leveraging available references and examples. Furthermore, CELI exhibited adaptability and dynamism in both case studies, showcasing its ability to adjust strategies and logic in response to new patterns or challenges encountered during task execution.\nThese properties underscore a fundamental shift in how we conceptualize and utilize LMs. The transition from traditional \"prompts\" to \"Language Model Interactions\" in CELI represents a new paradigm that allows for more sophisticated, context-aware, and goal-oriented engagements with LMs. By enabling LMs to autonomously manage complex workflows while maintaining seamless interaction with external systems and functions, CELI opens up new possibilities for AI-driven task automation and problem-solving across diverse computational domains.\nBuilding on these advancements, future work should focus on further enhancing CELI's capabilities. This includes improving its robustness in handling highly dynamic task environments, refining its error detection and recovery mechanisms, and exploring its potential in multi-modal tasks. Additionally, investigating CELI's applicability in collaborative human-AI settings could provide valuable insights into its real-world utility and potentially expand its impact across various fields and industries."}, {"title": "Limitations", "content": "While CELI shows promising results, several areas warrant further investigation. First, although our case studies span diverse domains (coding and content generation), the framework's performance across an even broader range of task types requires evaluation to fully assess its generalizability. Ongoing work is focused on extending CELI's application to additional domains, such as decision-making tasks and large-scale data processing, with preliminary results indicating promising adaptability to varied task structures.\nSecond, embedding control logic within prompts may introduce computational overhead, particularly in large-scale applications. This trade-off between enhanced functionality and increased computational demands is being addressed through continuous optimization efforts. These include refining prompt engineering techniques, exploring task decomposition methods, and improving language model efficiency to minimize overhead while maintaining CELI's advanced capabilities.\nThird, the potential for error propagation in complex, multi-stage tasks requires further analysis. While CELI has demonstrated the ability to autonomously handle and adapt to errors, minor errors accumulating across multiple stages could lead to inaccuracies in final outputs. To mitigate this, we are developing more robust error detection and correction mechanisms, such as real-time feedback loops and enhanced monitoring, to ensure reliable outputs in complex workflows.\nFourth, CELI's current implementation is constrained by the context window limits of the underlying LMs, which may hinder its performance in tasks requiring extensive cumulative context. To address this, we are actively investigating techniques such as recursive summarization and hierarchical task decomposition, which will allow CELI to handle larger volumes of data and extended task durations more effectively.\nLastly, CELI's performance is closely linked to the capabilities of the underlying LMs. As demonstrated in our experiments, the choice of model can significantly impact outcomes. However, CELI's modular architecture allows for rapid integration of new models, and regular benchmarking is conducted to ensure optimal performance as new LMs are released. In addition, CELI's ability to dynamically generate and execute code raises important ethical and safety considerations. To mitigate these risks, we have implemented sandboxed execution environments, and utilize human oversight, to ensure the safe and accurate execution of tasks.\nAdditionally, while not unique to CELI, the framework's ability to generate and execute code dynamically raises important ethical and safety considerations. The potential for generating harmful or malicious code, albeit unintentionally, necessitates robust safety measures and careful monitoring. In applications involving content generation, such as the Wikipedia article creation case study, ensuring factual accuracy and avoiding biases becomes crucial.\nAddressing these limitations is essential to unlocking the full potential of CELI and similar frameworks in AI-driven task automation and problem-solving.\nIt should be noted that, in the preparation of this paper, we utilized GPT-based AI tools to assist with language refinement. These tools provided suggestions for improving clarity, grammar, and phrasing, as well as offering edits for better readability. All AI-generated suggestions were carefully reviewed by the authors to ensure they aligned with the content and research objectives. No AI-generated content was accepted without thorough validation by the authors."}]}