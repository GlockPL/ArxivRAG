{"title": "Counter-Current Learning: A Biologically Plausible Dual Network Approach for Deep Learning", "authors": ["Chia-Hsiang Kao", "Bharath Hariharan"], "abstract": "Despite its widespread use in neural networks, error backpropagation has faced criticism for its lack of biological plausibility, suffering from issues such as the backward locking problem and the weight transport problem. These limitations have motivated researchers to explore more biologically plausible learning algorithms that could potentially shed light on how biological neural systems adapt and learn. Inspired by the counter-current exchange mechanisms observed in biological systems, we propose counter-current learning (CCL), a biologically plausible framework for credit assignment in neural networks. This framework employs a feedforward network to process input data and a feedback network to process targets, with each network enhancing the other through anti-parallel signal propagation. By leveraging the more informative signals from the bottom layer of the feedback network to guide the updates of the top layer of the feedforward network and vice versa, CCL enables the simultaneous transformation of source inputs to target outputs and the dynamic mutual influence of these transformations. Experimental results on MNIST, FashionMNIST, CIFAR10, and CIFAR100 datasets using multi-layer perceptrons and convolutional neural networks demonstrate that CCL achieves comparable performance to other biologically plausible algorithms while offering a more biologically realistic learning mechanism. Furthermore, we showcase the applicability of our approach to an autoencoder task, underscoring its potential for unsupervised representation learning. Our work presents a direction for biologically inspired and plausible learning algorithms, offering an alternative mechanisms of learning and adaptation in neural networks.", "sections": [{"title": "Introduction", "content": "In deep learning, biological plausibility refers to the properties that deep learning algorithms could respect to avoid inconsistency with current understandings of neural circuitry or violation of fundamental physical constraints, such as the localized nature of synaptic plasticity [Grossberg, 1987, Crick, 1989]. Consequently, error backpropagation (BP), despite its wide application, has been frequently criticized for its lack of biological plausibility, particularly for the following three challenges: (a) The weight transport problem, which arises because BP requires the feedback pathway to use the same set of weights as the feedforward process, a mechanism not observed in biological systems [Burbank and Kreiman, 2012, Bengio et al., 2015, Lillicrap et al., 2016]. (b) The non-local credit assignment problem arises because backpropagation relies on the global error signal to update the synaptic weights throughout the network, instead of depending on local errors derived from local loss\n38th Conference on Neural Information Processing Systems (NeurIPS 2024)."}, {"title": "Literature Review", "content": "Target Propagation: Addressing Biological Plausibility. The target propagation (TP) family [Bengio, 2014] and its variants (e.g., local target representations) [Ororbia et al., 2018, 2023], first explored in the late 1980s [Le Cun, 1986, Le Cun and Fogelman-Souli\u00e9, 1987], have been developed to optimize the neural networks by using locally generated error signals. TP explicitly constructs local targets for each layer using a separate feedback network. Take difference target propagation (DTP) Lee et al. [2015] for example, an idealized global target signal is computed based on the labels and the prediction error at the output layer. Then, the local idealized targets are generated by (1) propagating the idealized global targets through the feedback network and (2) computing a linear correction using the activations from the forward network. Subsequently, local losses are computed by comparing the layer activations with their corresponding local targets. The weights of both the forward and feedback networks are updated based on these local losses. Notable variants such as Direct Difference Target Propagation (DDTP) [Meulemans et al., 2020], Local-Difference Reconstruction Loss (L-DRL) [Ernoult et al., 2022], and Fixed-Weight Target Propagation Shibuya et al. [2023] further refine this approach by introducing mechanisms to improve feedback weight training and enhance the accuracy of local error signals. Despite these advancements, TP methods still encounter the backward locking issue, since TP methods depend on the forward network's outputs and intermediate activations to compute targets. Moreover, recent iterations of TP algorithms, such as DDTP and L-DRL, can be computationally expensive. They require additional feedback\nIn biological systems, synaptic plasticity is believed to be governed by local learning rules, such as Hebbian learning, where synaptic changes depend on the correlated activity of the pre-and post-synaptic neurons [Dan and Poo, 2004, Bartunov et al., 2018, Whittington and Bogacz, 2019]\n2"}, {"title": "Counter-Current Learning Framework", "content": "In this section, we present the counter-current learning (CCL) framework, as shown in Figure 1, focusing on its formulation and key components.\nSetup and Feedforward Network. Consider input space X and output space Y, each with dimensions do and d\u2081, respectively. The objective is to learn a mapping F : X \u2192 Y that minimizes the discrepancy between the predicted output and the target. We adopt an L-layered feed-forward neural network with activation function \u03c3. Let g\u2081(\u00b7) denote the operation at layer l, and define Ffw = GL GL\u22121 \u00b0 . . . \u00b0 91. Each gi is parameterized by weights U\u2081. The output of layer l is \u03b1l = g\u03b9(\u03b1l\u22121) = \u03c3(U\u03b9\u03b1l\u22121), where \u03b1o = x.\nFeedback Network. The proposed learning scheme introduces a complementary backward function Ffw that mirrors Ffw in an anti-parallel manner. Fbw comprises layers [h\u2081, . . ., h1], with each hi parameterized by weights V\u012b. We define Fbw = h\u2081 0 . . . 0h\u2081. The output of layer l is bl\u22121 = hl(bl) = \u03c3(Vlbl), with b\u2081 = y. The dimensions of hidden layers align between Ffw and Fbw.\n3"}, {"title": "Stop Gradient Operation", "content": "To address the backward locking problem and ensure local synaptic learning, we use the SG() operation to decouple activations from weights in previous layers, disrupting the long error-backpropagation chain into local update segments. The SG() can be implemented using PyTorch gradient detach operation easily. In CCL, each layer's input is processed with the SG() operation. To avoid confusion, we use the hat symbol to denote the exact activations in the CCL paradigm. Specifically, for 1 < l < L:\n$\\tilde{\\alpha}_l = g_l(\\tilde{\\alpha}_{l-1}) = \\sigma(U_l SG(\\tilde{\\alpha}_{l-1})),$$\n$\\delta_{l-1} = h_l(\\delta_l) = \\sigma(V_l SG(\\delta_l)),$$\nwhere ao = x and b\u2081 = y."}, {"title": "Loss Objective Function", "content": "The objective of the counter-current learning algorithm is to minimize the difference between activations of Ffw and Fbw across all layers but the output layer:\n$\\min_{\\Theta} \\sum_{l=0}^{L-1} ||\\hat{a}_l - b_l||,$$\nwhere 0 = {U1, . . ., UL\u22121, V1, . . ., VL } are learnable parameters. For example, let us consider the local loss function at the first layer (i.e., l = 1), where the loss function is mine||\u00e2\u2081 \u2013 81||. Plugging Eq. 1 gives us min{U1,V2} ||\u03c3(U\u2081SG(ao)) \u2013 \u03c3(V2SG(62))||, where SG(20) and SG(62) are treated as constants due to the stop gradient operation. For the last layer (e.g., the output layer), the weight UL is trained to minimize the cross-entropy loss between the outputs \u00e2\u2081 and the one-hot labels b\u2081 = y.\nBiological Plausibility. We examine the biological plausibility of the proposed counter current learning scheme. This framework mitigates the weight transport problem by using a different weight parameterization for the feedback network. For the non-local credit assignment problem, the update of the parameters is driven by local loss, instead of the back-propagated global error signals. Finally, we partially address the backward update problem by removing the dependency of the backward network and the forward network with careful gradient detachment.\nImplementation. In Figure 2, we present a code snippet for the CCL algorithm in PyTorch, tailored for an MNIST classification. It shows the independence of the forward process and the feedback (backward) process from each other. The main C2Model module comprises three C2Linear layers,\n4"}, {"title": "Experiments", "content": "4.1 Counter Current Learning Facilitates Dual Network Feature Alignment\nTo investigate the alignment of latent features within the counter-current learning framework, we visualized embeddings from the penultimate layer of the forward network and the corresponding second layer of the feedback network at various stages of training. We employ a six-layer neural net trained on MNIST and analyze embeddings from both networks. t-SNE was applied independently to these embeddings at each training iteration to effectively visualize the evolution of feature spaces.\nAs illustrated in Figure 3, the embeddings from the forward model trained on MNIST data are represented by colored dots, while the embeddings from the backward model related to one-hot encoded labels are denoted by outlined squares of the same color. Throughout the training process, embeddings from the same class progressively align between the forward and backward models, suggesting that the forward and backward models mutually guide each other's feature representations toward a coherent and discriminative structure. Please refer to Appendix 6.3 for visualization of feature alignment of more layers and Appendix 6.4 for experiments on weight alignement between forward and backward layers.\n4.2 Classification Performance\nTask Setup. We evaluate the performance of our proposed method against several biologically plau-sible algorithms, including direct target propagation (DTP), DTP with difference reconstruction loss (DRL), local difference reconstruction loss (L-DRL), and fixed-weight difference target propagation (FW-DTP). The evaluation is conducted on MNIST, FashionMNIST, CIFAR-10, and CIFAR-100. All experiments are performed using stochastic gradient descent optimization with 100 epochs. We use cross-validation over 5 different random seeds and report the testing performance. The models are implemented using the PyTorch deep learning framework, and the code is available in the Supple-mentary Material. For the experiments on multi-layer perceptrons, we apply image normalization as a preprocessing step. For the convolutional neural network experiments, we use additional data augmentation techniques, including cropping and horizontal flipping. For the counter-current learning (CCL) algorithm, we search across different learning rates and gradient norm clipping values to find the optimal hyperparameters following cross-validation, as detailed in Appendix 6.1.\n5"}, {"title": "Convolutional Neural Network", "content": "We also evaluate CCL on convolutional neural networks (CNN) consisting of five convolutional layers (each with a kernel size of 3 and a max-pooling operation with a kernel size of 2) followed by a linear classifier, tested on CIFAR-10 and CIFAR-100. As shown in Table 3, our CCL-based model performs comparably to, or slightly inferiorly than, the BP-based model. Additionally, we visualize the kernels in the first convolutional layer to inspect the learned representations in Figure 4, demonstrating that CCL enables the model to learn meaningful convolutional kernels without using error backpropagation. Furthermore, we compare our CNN results on CIFAR-10 with those of L-DRL Ernoult et al. [2022] in Appendix 6.2, while FW-DTP Shibuya et al. [2023] does not include CNN implementations.\n4.3 Auto-Encoder Performance\nWe explore the applicability of the counter-current learning (CCL) algorithm to autoencoders.\nAuto-Encoder on STL-10 Dataset. A convolutional autoencoder with a four-layer encoder and a four-layer decoder is used. Different from the classification tasks, this architecture replaces the 2x2 kernel max-pooling with convolution layers with a stride of 2. Batch normalization is applied following each linear projection and before activation functions to ensure both stability and optimal performance. The hidden layers of the network are structured with dimensions of [128, 256, 1024, 2048]. Orthogonal weight initialization is used for training stability. Data augmentation techniques such as random cropping and horizontal flipping are incorporated. Hyperparameters including gradient clipping, learning rate, momentum, and weight decay are subjected to grid search, while cross-validation across five different seeds is employed to assess the reconstruction L2 loss.\nResults. The test set's reconstruction metric-mean square error-is quantified as 0.0059 \u00b1 0.0001 for BP and 0.0086 \u00b1 0.0001 for CCL. The outcomes, illustrated in Figure 5, underscore the models'\n7"}, {"title": "Empirical Analysis of Learning Dynamics for Counter-Current Learning", "content": "In this section, we provide insights into the functioning of the proposed Counter-Current Learning (CCL) algorithm by examining the representation similarity between the forward and feedback networks. We start by analyzing the feature similarity between randomly initialized feedforward and feedback models. Following this, we focus on the feature alignment in the high-level feature regime. Our investigation reveals the emergence of a reciprocal learning structure, where the top layers of both networks benefit from the bottom layers of each other during training.\nWe trained a model on the MNIST classification task using a configuration of five convolutional layers topped with a linear classification head. The training was conducted over 160 steps with a batch size of 32, achieving an average testing accuracy of 88.88%. To measure the cross-network representational similarity, we utilized Centered Kernel Alignment (CKA) Kornblith et al. [2019], a metric known for its robustness to invertible linear transformations. This was applied to evaluate layer and architecture similarity. The results, obtained using five different seeds, are presented as averaged CKA values on the test set. Figure 6 displays the horizontal axis marking the activations of forward layers, ranging from the input MNIST images to the logits (i.e., a6), whereas the vertical axis denotes the activations of the feedback network starting from one-hot labels (i.e., target).\nObservation 1: Initialization Shows Noisy Top Layers and Misalignment Between Networks. At initialization, our premise that the bottom layers contain more relevant information is validated. The initial CKA (t = 0, top-left subplot in Figure 6) reveals low similarity between the a6 column (i.e., the top layer of the forward network) and the feedback network. Similarly, the bo row (i.e., the top layer of the feedback network) shows low CKA values with the forward layers, confirming our hypothesis of feature misalignment at random initialization.\nObservation 2: Alignment of High-Level Features During Training. As training progresses, high-level features (i.e., the top layers of the forward network and the bottom layers of the feedback network) begin to show increasing CKA values (Figure 6, t = 20 to t = 160, top row). This trend suggests that the forward and feedback networks gradually align their high-level representations.\nObservation 3: Emergence of a Counter-Current Reciprocal Structure. The dynamics of the counter-current learning algorithm reveal significant changes in CKA, particularly at higher layers (illustrated in the bottom subplots of Figure 6). Notably, the increases are concentrated in the a4, a5, a6 columns of the forward network and the bo, b1, b2 rows of the feedback network. This pattern supports the counter-current intuition that top layers benefit from the more informative bottom layers, fostering a reciprocal learning structure that guides each network's optimization process using the most informative features available.\nThis empirical analysis underscores the hypothesis that the counter-current learning scheme effectively leverages complementary and reciprocal alignment of representations between the forward and feedback networks. By exploiting the informative features from the bottom layers of one network to refine the noisy features in the top layers of the other, the counter-current signal propagation algorithm achieves a biologically plausible and efficient learning mechanism."}, {"title": "Ablation on Asymmetric Learning Rates for Dual Network Optimization", "content": "We delve deeper into the effects of varying learning rates between forward and feedback networks in CCL. As illustrated in Figure 7, our experiments confirm that asymmetric learning rates in forward and feedback networks facilitate effective and robust learning. Particularly noteworthy is our observation that robust learning outcomes are achievable even when the feedback network operates under a fixed random setting\u2014specifically, with a zero learning rate (refer to the bottom rows of the figure). This suggests that the random projection of target labels by the feedback network conveys meaningful target domain information, echoing findings from the DRTP [Frenkel et al., 2021]. Moreover, our experiments indicate superior performance compared to the DRTP approach (refer to Table 1), possibly hinting that a random, non-linear neural network projection (i.e., CCL with a feedback\n8"}, {"title": "Conclusion", "content": "In this paper, we introduced the counter-current learning framework, a novel approach addressing the critical limitations of traditional error backpropagation. Our dual network architecture enables a\n9"}, {"title": "Experimental Setup", "content": "Hyperparameter Search. For experiments with MLP architectures, we conduct hyperparameter searches for each algorithm. We run all the combinations of the hyperparameters with 5 different ran-dom seeds and then select the hyperparameter set with the highest accuracies (or lowest loss for auto-encoder task) evaluated on the validation set and test on the testing set. For DTP, DRL, L-DRL, and FWDTP, we search across forward learning rates [0.3, 1, 3], step sizes [0.001, 0.003, 0.01, 0.03, 0.1], and backward learning rates [0.0001, 0.0003, 0.001, 0.003, 0.01]. Note that FWDTP-BN does not have the backward learning rate hyperparameter. For DRTP, the search space includes learning rates [0.010.030.10.3] and mean ([0.0.05]) and standard deviation ([0.010.030.10.3]) for random project matrix. For BP and FA, we use learning rates [0.4, 0.2, 0.1, 0.05, 0.02, 0.01] for hyperparameter search. and gradient clip values of [0.5,1]. For CCL, the search space includes learning rates [0.2, 0.5, 1, 1.5, 2] and gradient clip values of [0.5, 1].\nImplementation Details. Training MLP and CNN models with CCL can be unstable initially since both the forward and feedback networks are randomly initialized. We use learning rate warm-up for the initial 200 steps for CCL, which is adopted in Ernoult et al. [2022]. Moreover, unlike algorithms in the target propagation family [Meulemans et al., 2020, Ernoult et al., 2022], where the feedback network weights are trained using additional for-loops to tune the weights for each data batch, we introduce some techniques to stabilize the training course. We found that normalizing the activations during loss computation helps stabilize the training process. Additionally, as counter-current learning can also suffer from feature collapse, where a trivial solution to all pairwise losses is to produce constant activations, we introduce a remedy. Inspired by layer-wise training, for each latent activation X \u2208 Rbxd, where b stands for batch size and d stands for feature dimension, we added an additional L2 loss to minimize the difference between norm(X)norm(X) and the identity matrix. Finally, in contrast to error backpropagation, where the error signals can be zero and the weight updates can be small at the end of training, the layer-wise losses in CCL are seldom zero, thus the weights can keep changing during the training. This can lead to worse minima. To accommodate this, we use gradient centralization [Yong et al., 2020] to centralize the gradient for parameters for both BP and CCL and flooding method [Ishida et al., 2020] to prevent some weights from updating if the sample-wise difference between output and target is smaller than 0.2 in CCL on CNN."}, {"title": "Comparison of Implementation", "content": "Comparison with Ernoult et al. [2022]. We compare the results on CIFAR-10 using a VGG-like network architecture. As shown in Table 4, the results indicate that L-DRL [Ernoult et al., 2022] achieved similar testing accuracies to BP, reaching 89%. While these results are promising, it's worth noting some aspects of their methodology that may affect direct comparisons: (1) Their models were trained on the full training set, which differs from our approach; (2) The authors do not provide details on the hyperparameter search space or the implementation of cross-validation.\n13"}, {"title": "Visualization of Representations in Other Layers", "content": "To show that the feature alignment can present in more than one layer, we show the results of a five-layered CNN model trained on CIFAR10. The model is trained for 3000 steps using CCL, and the t-SNE for layers 1, 3, and 5 with different time steps are shown in Figure 8."}, {"title": "Experiments on Weight Alignment", "content": "To better understand how CCL learns: Does CCL learn by aligning the forward and backward network? We compute the cosine similarity between the forward layer weights and the corresponding feedback layer weights for the model trained on MNIST using a five-layered MLP architecture.\nAs shown in Figure 9, our findings reveal two distinct phases during training:\n\u2022 Phase one: Layers 1 and 5 show rapid alignment increases.\n\u2022 Phase two: Alignments in layers 1 and 5 saturate. Alignments in the intermediate layers gradually increase until saturation.\nInterestingly, intermediate layers showed a bottom-up convergence pattern, with layer 2 achieving highest similarity first, followed by layers 3 and 4. We found that high alignment isn't always achieved or necessary for effective learning. This may be due to (1) Dimensional reduction: Information propagation through the network affects alignment, and (2) Non-linearity: Activation functions impact the relationship between forward and backward weights. These factors may contribute to observed alignment patterns without requiring perfect symmetry between forward and backward weights."}, {"title": "Literature Reviews on More Biologically Plausible Algorithms", "content": "Algorithms with only forward passes. Forward-forward algorithm (FFA) [Hinton, 2022] only uses forward passes to update the neural network. Specifically, in the supervised learning scheme,\n14"}]}