{"title": "FEDGCA: GLOBAL CONSISTENT AUGMENTATION BASED SINGLE-SOURCE\nFEDERATED DOMAIN GENERALIZATION", "authors": ["Yuan Liu", "Shu Wang", "Zhe Qu", "Xingyu Li", "Shichao Kan", "Jianxin Wang"], "abstract": "Federated Domain Generalization (FedDG) aims to train the\nglobal model for generalization ability to unseen domains\nwith multi-domain training samples. However, clients in fed-\nerated learning networks are often confined to a single, non-\nIID domain due to inherent sampling and temporal limita-\ntions. The lack of cross-domain interaction and the in-domain\ndivergence impede the learning of domain-common features\nand limit the effectiveness of existing FedDG, referred to as\nthe single-source FedDG (sFedDG) problem. To address this,\nwe introduce the Federated Global Consistent Augmentation\n(FedGCA) method, which incorporates a style-complement\nmodule to augment data samples with diverse domain styles.\nTo ensure the effective integration of augmented samples,\nFedGCA employs both global guided semantic consistency\nand class consistency, mitigating inconsistencies from local\nsemantics within individual clients and classes across mul-\ntiple clients. The conducted extensive experiments demon-\nstrate the superiority of FedGCA.", "sections": [{"title": "1. INTRODUCTION", "content": "In recent years, Federated Learning (FL) has emerged as a po-\ntent paradigm for effectively training machine learning mod-\nels across a network of decentralized clients [1-4]. Numerous\nFL applications are deployed in scenarios where data decen-\ntralization is imperative due to privacy concerns, communica-\ntion limitations, or regulatory constraints. In the earlier stages\nof FL research [1-4], there was a common assumption that the\ntraining and testing data of clients followed the same distribu-\ntion, ensuring the generalizability of the global model to test\ndata. However, guaranteeing that a trained model will always\nbe applied to the distribution it was trained on is challeng-\ning. This inherent uncertainty necessitates a critical extension\nof FL known as Federated Domain Generalization (FedDG)\n[5-7], which seeks to empower the global model to gener-\nalize to out-of-distribution (OOD) [8, 9] domains, where the\nboundaries between different domains are unavailable, mak-\ning the task challenging to detect and address.\nMany existing FedDG studies [5-7] predominantly fo-\ncus on scenarios where the clients in the FL network orig-\ninate from multiple domains. These studies assume a one-\nto-one correspondence between clients and domains, leading\nto cross-domain concept drift among clients. Typically, they\nbenefit from the rich domain information, facilitating straight-\nforward generalization to unseen target domains. However, a\nmore realistic scenario is where client samples are confined to\nspecific times or regions. For instance, in seasonal flu predic-\ntion, samples exhibit distinct temporal feature distributions.\nSimilarly, in image classification tasks within the desert envi-\nronment, the images are prone to exhibit common background\nfeatures associated with desert characteristics.\nIn such cases, clients are confined to a single domain, gen-\nerating a new problem called single-source Federated Do-\nmain Generalization (sFedDG). As illustrated in Fig. 1, all\nclients belong to one domain, but they aim to train a global\nmodel that can work for other unseen target domains. This\nissue entails two prominent challenges: 1) limited accessible\ndomain styles. Unlike FedDG setups [5\u20137, 10], where clients\nengage with multiple domains, the constraint of sFedDG re-\nstricts clients' ability to interact with and learn from other\ndomains. This limitation hinders the exploration of com-\nmon features to achieve effective domain generalization. Fur-\nthermore, sFedDG can be considered as an advanced exten-\nsion of the OOD testing problem, focusing on generality en-\nhancement [8]. 2) Semantic inconsistency on both local and\nglobal objectives. Unlike the single-source Domain General-\nization (sDG) problem [11-13], the data distribution on each\nclient exhibits non-IID nature in sFedDG due to the nature\nof FL. Despite the application of data augmentation tech-\nniques [14,15] to diversify domain styles, it remains challeng-\ning due to the absence of samples for certain classes, resulting\nin an inconsistent decision boundary.\nTo address the introduced challenges, we present a novel\nand versatile algorithm: Federated Global Consistent Aug-\nmentation (FedGCA), crafted to elevate generalization per-\nformance against sFedDG. First, FedGCA employs a style-\ncomplement module, leveraging cutting-edge data augmen-\ntation techniques to effectively enrich data samples across a\nlimited set of accessible domain styles, drawing inspiration\nfrom [12, 14, 15]. Subsequently, FedGCA addresses the in-\ndomain divergence caused by the non-IID nature of FL and\nensures semantic information consistency across individual\nclients. It achieves this by incorporating both global guided\nsemantic consistency and class consistency loss into the client\ntraining process. These losses utilize dynamic regularization\nstrategies within the federated framework, effectively reduc-\ning features that are semantically irrelevant to the specific\nclass distributions of clients.\nThe contributions of this paper are: 1) To the best of\nour knowledge, this is the first study to introduce and ad-\ndress the practical problem sFedDG, which poses two sig-\nnificant challenges that existing studies fail to address. 2)\nInnovatively addressing the unique challenges of sFedDG,\nwe propose the corresponding FedGCA method: The style-\ncomplement module generates diverse and informative sam-\nples from the source domain with limited styles, and the\ntwo innovative strategies mitigate inconsistencies, thereby en-\nhancing the model's ability to generalize effectively from one\nsingle domain to various domains. 3) Experimental results\nunequivocally show that FedGCA outperforms benchmarks\non several datasets, underscoring its superior effectiveness."}, {"title": "2. PRELIMINARIES AND METHODOLOGY", "content": "2.1. Preliminaries\nIn FL, the primary objective is to collaboratively train a con-\nsensus global model. Typically, consider K clients, denoted\nas i \u2208 [K], with non-IID datasets. Let D\u2081 = {(x, y))}(i)(i) Ni\nXjYjj=1\nbe the dataset on client i, where x) is the j-th input sam-\n(i)\nple and y; is its corresponding label. The size of datasets\non client i is Ni, and the total number of data samples across\nall clients is N = \u22111 N. Let w be the global model,\nand Li(w) be the local empirical risk function on client i.\nThe objective of FL can be formulated as minw L(w) =\nK\n1 Li(w). Each client calculates Li(w) locally and\nthen sends the updated model parameter w\u2081 to the server. The\nserver then performs aggregation to update the global model\nw. The data distribution pi(x, y) of client i are requested to\nbe relevant (sampled from a family & of distributions).\nIn FedDG, the data distribution pi(x, y), Vi is set to be\ndifferent source domains, aiming to minimize the loss on un-\nseen target domains Ptest(x, y) ~ E. The average case of\nthis loss can be defined as Eptest~& [Eptest(x,y)l(w; x, y)], where\nl(w; x, y) denotes the loss function of a data point (x, y).\nNote that in the DG/FedDG problem, different domains are\ntypically considered to have a significant shift in style, tex-\nture, or appearance [5, 7], while clients are set in the same\nlabel distribution across domains [6]."}, {"title": "2.2. sFedDG", "content": "In this paper, we consider a new problem called sin-\ngle-source Federated Domain Generalization (sFedDG).\nMore specifically, given a source domain S, data distribu-\ntions pi(x, y) across the clients are sampled from the same\ndomain S, i.e., pi(x,y) ~ S,\u2200i \u2208 [K] with pi(x,y) \u2260\nPi (x,y), Vi, i' \u2208 [K]. The goal of sFedDG is to collabo-\nratively learn a global model that can generalize to unseen\ntarget domains T. The source domain S and target domains\nT belong to the family & of distributions. For the test dataset\ndistribution Ptest(x, y) ~ T, Ptest(x, y) \u2260 Pi (x, y) and T can-\nnot be accessed in training. The expectation on target domains\ncan be defined as Eptest~T[Eptest(x,y)l(w; x, y)].\nWith no prior knowledge of the target domains T, we can\njust estimate the objective using finite clients and finite sam-\nples from the source domain S. As a result, the sFedDG\nobjective function can be accessed as minw LDG(w) =\nminw Epi~s [Epi (x,y)l(w; x, y)]."}, {"title": "2.3. FedGCA", "content": "The sFedDG problem primarily faces two challenges: (1)\nlimited accessible source domain styles and (2) semantic in-\nconsistency on both local and global objectives. To address\nthese challenges, we propose a method called Federated\nGlobal Consistent Augmentation (FedGCA), illustrated in\nFig. 2. Specifically, to tackle the first challenge, our proposed\nFedGCA method augments the source data samples through\nvarious transformations or generates pseudo-novel samples\nfor the source domain. However, generated data introduces\nsemantic noise, and clients can only generate enhanced data\nfrom their own distributions, leading to drift between clients\nand incurring the second challenge. Therefore, we first de-\nsign semantic consistency loss from both coarse-grained and\nfine-grained perspectives, incorporating global information\nfor constraint. Additionally, we employ dynamic regulariza-\ntion to guide augmented clients' training towards consistent\nobjectives."}, {"title": "Style-complement module.", "content": "The style-complement mod-\nule functions as either a data augmentation filter or a genera-\ntion model [12, 14-16], aiming to enhance the data samples x\nfrom the source domain S by synthesizing x' with the same\nsemantic information but different styles. Let X+ = {x, x'},\nwith corresponding labels Y+ = {y,y}. Following the ac-\nquisition of synthetic data samples, the objective of sFedDG\ncan be formulated as follows:\n$\\min_w L_{DG}(w) = \\min_w E_{p_i\\sim S} [E_{p_i(x^+,y^+)}l(w; X^+,Y^+)]$.", "eq": "\\min_w L_{DG}(w) = \\min_w E_{p_i\\sim S} [E_{p_i(x^+,y^+)}l(w; X^+,Y^+)]"}, {"title": "Global Guided Semantic Consistency.", "content": "To ensure se-\nmantic consistency within client i, we employ a loss func-\ntion $L^{CP}_i$ that encourages consistent predictions for the same\nimage but with different filters. Additionally, we leverage\nglobal predictions to guide consistent predictions, aiding dif-\nferent client models in learning semantically consistent fea-\ntures for the same class. Specifically, P\u2081 = {$p_{i,j}$}$^{J+1}_{j=0}$ denotes\nthe softmax predictions of the training model wi on X+, and\nPG = {$p_{i,j}$} represents the global model predictions on\nX+. In contrast to contrastive-based FL methods [2], moving\nthe current local model away from the old local model may be\ntoo drastic. Instead, we utilize the Kullback-Leibler (KL) di-\nvergence to minimize the global-local semantic consistency.\nThe calculation of $L^{CP}_i$ is as follows:\n$L^{CP}_i=\\sum_{p_{ij}\\in P_i} p_{i,j} \\log(p_i) - (1 - p_{i,j}) \\log(1 - p_i)$", "eq": "L^{CP}_i=\\sum_{p_{ij}\\in P_i} p_{i,j} \\log(p_i) - (1 - p_{i,j}) \\log(1 - p_i)"}, {"title": "Class Consistency.", "content": "The semantic consistency in (4) aims\nto learn similar features from existing classes within one\nclient. However, due to the non-IID nature of FL, if a client\nlacks data samples from some specific classes, it cannot aug-\nment enough data samples to approach the objective in (1) and\nmay incur a blurred decision boundary. To mitigate the per-\nformance degradation caused by class imbalance, we employ\na dynamic regularizer [3] on each client via a similar pattern\nas ADMM, to further improve the consistency from a global\nperspective. Specifically, we denote our class consistent reg-\nularizer on client i as $L^{OC}_i$, which is defined as follows:\n$L^{OC}_i = \\frac{1}{2\\alpha} ||\\triangledown_{w_i}L^{t}_{w_i} - w_i - \\bar{w}^t||^2 + \\frac{\\eta}{2\\alpha} ||w_i - w^t||^2$,", "eq": "L^{OC}_i = \\frac{1}{2\\alpha} ||\\triangledown_{w_i}L^{t}_{w_i} - w_i - \\bar{w}^t||^2 + \\frac{\\eta}{2\\alpha} ||w_i - w^t||^2"}, {"title": "3. EXPERIMENT", "content": "3.1. Experimental Setups\nDatasets: We conducted experiments on the following three\ndatasets. (1) Digits consists of five distinct datasets: MNIST,\nSVHN, USPS, Synth, and MNIST-M [7]. (2) The PACS\n[20] dataset encompasses four domains: photo, art, cartoon,\nsketch. Each domain contains 224 \u00d7 224 images belonging\nto seven categories, posing challenges due to substantial style\nshifts among domains.\nCompared methods: The FL setting includes 10 clients on\neach dataset, where the non-IID is followed by Dirichlet dis-\ntribution with parameter 0.3. To validate the efficiency of\nFedGCA, we compare the performance of FedGCA against\nseveral state-of-the-art FL and FedDG methods, including Fe-\ndAVG [1], FedSAM [4], FedDyn [3], Moon [2], FedADG [6],\nand FedSR [7]. Additionally, we introduce augmented vari-\nants of each benchmark denoted by \"+RC,\" which represents\nthe inclusion of the RandConv variant. Due to the page lim-\nitation, we defer the detailed experimental setups and part of\nresults in the supplementary materials."}, {"title": "3.2. Performance Evaluation", "content": "Accuracy Evaluation. The comparison results on Digits be-\ntween FedGCA and other methods are presented in Table 1.\nIt is evident that the performance of existing methods signif-\nicantly improves upon the addition of RandConv (RC), un-\nderscoring the significance of data augmentation in address-\ning the sFedDG problem. Notably, FedGCA demonstrates\nan average performance gain of at least 5.91% compared to\nall other methods. Specifically, FedGCA outperforms the\nsecond-best method, FedSAM+RC, by 7.55%, 3.28%, 7.21%,\nand 5.60% on test domains, respectively, highlighting its su-\nperiority in overall generalizability. Similarly, the comparison\nresults on PACS are presented in Table 2, which can improve\nup to 21.74%. While FedADG and FedSR have demonstrated\neffectiveness in the FedDG problem [6, 7], they show perfor-\nmance degradation in sFedDG due to the absence of multiple\nsource domain styles. Our method consistently achieves the\nhighest performance.\nSensitivity for a and \u03b2. We show sensitivity analyses for"}, {"title": "4. CONCLUSION", "content": "In this paper, we have addressed the emerging challenges\nof the sFedDG problem, a practical scenario where clients\nare confined to a single domain, by designing the FedGCA\nmethod. Unlike FedDG settings, clients lack interactions\nwith other domains, hindering the identification of common\nfeatures crucial for effective generalization. To tackle the\nsFedDG challenge, we introduced a style-complement mod-\nule that enriches the semantic information on each client.\nBy employing the global guided semantic consistency strat-\negy with dynamic regularization, we effectively addressed the\nlimitations posed by the single-source and heterogeneous na-\nture of sFedDG. Extensive experiments on different datasets\nsuccessfully demonstrate the efficiency of FedGCA."}]}