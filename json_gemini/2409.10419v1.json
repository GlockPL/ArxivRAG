{"title": "HiFi-CS: Towards Open Vocabulary Visual Grounding For Robotic Grasping Using Vision-Language Models", "authors": ["Vineet Bhat", "Prashanth Krishnamurthy", "Ramesh Karri", "Farshad Khorrami"], "abstract": "Robots interacting with humans through natural language can unlock numerous applications such as Referring Grasp Synthesis (RGS). Given a text query, RGS determines a stable grasp pose to manipulate the referred object in the robot's workspace. RGS comprises two steps: visual grounding and grasp pose estimation. Recent studies leverage powerful Vision-Language Models (VLMs) for visually grounding free-flowing natural language in real-world robotic execution. However, comparisons in complex, cluttered environments with multiple instances of the same object are lacking. This paper introduces HiFi-CS, featuring hierarchical application of Featurewise Linear Modulation (FiLM) to fuse image and text embeddings, enhancing visual grounding for complex attribute rich text queries encountered in robotic grasping. Visual grounding associates an object in 2D/3D space with natural language input and is studied in two scenarios: Closed and Open Vocabulary. HiFi-CS features a lightweight decoder combined with a frozen VLM and outperforms competitive baselines in closed vocabulary settings while being 100x smaller in size. Our model can effectively guide open-set object detectors like GroundedSAM to enhance open-vocabulary performance. We validate our approach through real-world RGS experiments using a 7-DOF robotic arm, achieving 90.33% visual grounding accuracy in 15 tabletop scenes. We include our codebase in the supplementary material.", "sections": [{"title": "1. Introduction", "content": "Language-guided robotic manipulation is crucial for the development of human-robot interactive systems. A key component of this is Referring Grasp Synthesis (RGS), which enables autonomous robots to execute pick-and-place tasks based on text commands. Given a request to grasp a specific object within its workspace, RGS identifies a stable grasp pose for execution using a robotic arm [71]. This process connects abstract natural language instructions with physical manipulation policies, forming a critical component of modern robotic visual perception [46]. For instance, when given a command such as \"grasp the blue bottle,\" the RGS visual grounding module locates the referred \"blue bottle\" in the robot's surroundings, either through 2D images [38, 44] or through a reconstructed 3D representation [1,7,83]. These visual representations are used to construct object point clouds, which are then fed into downstream grasping models to determine and execute the grasp pose [15, 16, 19,43]."}, {"title": "2. Background and Related Work", "content": "Foundational Models in Robotics: Large Language Models (LLMs) can generate high-level robotic execution plans based on task inputs and environmental context [25, 66]. However, a recurring challenge with LLMs is their tendency to hallucinate, generating plans that are not physically feasible [59]. To enhance robustness, LLMs require real world grounding, which can be achieved through feedback from the environment [4, 26, 65], integration with visual perception systems [18,36,85], or human-in-the-loop interventions like question-answering [50, 82]. Vision-Language Models (VLMs), trained on vast image-text datasets, excel at visual reasoning tasks [78] and have been applied to diverse robotics problems such as encoding 3D semantic memory [20, 56], guiding object manipulation based on language instructions [64,68], and enabling robotic navigation [22, 24]. Recent work has focused on training VLMs using multimodal robotic demonstrations, where vision and language are directly mapped to actions [2, 5, 6, 12]. These methods show strong performance in familiar environments but require substantial data and GPU resources for deployment in novel settings [73]. Consequently, modular systems that integrate planning, grounding, control, and feedback appear more promising for robust robot automation [40, 47].\nReferring Grasp Synthesis (RGS): Earlier approaches for RGS often used LSTM networks. INGRESS [63] employed two LSTMs for grounding, one generating visual object descriptions and another assessing pairwise relations between candidates. [57] introduced a learning-based approach incorporating grasp type and dimension selection models for predicting grasp poses from natural language object descriptions. However, these methods struggled with natural language complexities, hindering precise visual grounding. Recent studies show the effectiveness of VLMs in associating language with images [34, 60]. [49] used GPT-4 and Owl-VIT [48] to identify objects for grasping from text queries. [75] employed CLIP as a vision-text encoder with cross-modal attention for sampling and scoring grasp poses. [44] introduced the RoboRefIt corpus to train a transformer-based network to predict 2D object masks from referred text queries. Neural Radiance Fields (NeRFs) can also used for grounding natural language to 3D directly, followed by grasp pose estimation [58, 62]. However, computing NeRFs is time-consuming and thus difficult for real-world deployment. End-to-end RGS directly maps natural language queries to grasp parameters. [8] trained a ResNet50-LSTM network for merging multi-modal features for GPE. [30] fine-tuned a multi-modal VLM for reasoning expression segmentation along with GPE. [70] used CLIP multi-modal features to train a fusion network with self and cross-attention for task-oriented grasping. [71] released the OCID-VLG dataset for RGS, fine-tuning a CLIP-based model with a transformer decoder for pixel-level object segmentation and GPE. Robust end-to-end RGS requires diverse annotated datasets with images, text queries, and grasp poses, but such datasets are either limited or focus on a small, fixed object set. Recently, [72] created a large-scale dataset using foundational models for end-to-end RGS. However, it relies on 2D grasp poses, which are less robust than 6D grasp poses in real-world cluttered scenes. Recent work in GPE has focused on training models with large and diverse pose datasets, such as GraspNet-1Billion [16], and learning robust grasp poses for unseen objects. Our two-stage RGS approach uses pre-trained VLMs to generate accurate pixel-level segmentation of referred objects, which can then be used by state-of-the-art GPEs to generate stable 6-DOF grasp poses.\nVisual Grounding: Visual Grounding (VG) in robotics identifies an object or region in 2D/3D space related to a given query, making it critical for connecting natural language to the real world [21, 35, 84]. This process involves segmenting the referred part and projecting it across camera views to construct a 3D object point cloud. Downstream grasping modules can then use this point cloud to determine grasp poses [13]. Our work focuses on 2D Visual Grounding, which is often studied as Referring Image Segmentation (RIS) in computer vision. Traditional RIS models utilize Convolutional Neural Networks or Long Short-Term Memory Networks [23, 51, 79]. The field has advanced significantly with transformer-based architectures enhancing language grounding in visual contexts [10, 17, 41, 77]. State-of-the-art RIS models employ large transformer architectures with cross-attention and fine-tune for generating object bounding boxes or pixel-wise segmentation [74, 76]. Such models are often compute intensive, requiring multiple A-100 GPUs for finetuning and deployment making it challenging for usage in real-time processing for robotic visual grounding. PolyFormer [39] uses a transformer-based architecture with separate visual and textual feature extractors and a multimodal fusion strategy for polygon regression of the segmentation mask in the image. Annotating accurate polygon regression coordinates for segmentation masks by human experts is time consuming, with each sample on average requiring 79s [52]. Weakly supervised methods alleviate some of the costs associated with segmentation annotations by employing innovative strategies, such as combining positive and negative queries during training or using negative anchor features [29, 37]. Although these models report high performance in diverse testing, their application in robotics face challenges due to a lack of robotics related data representation in popular datasets like Flickr30K-entities [54], RefCOCO [80] and ReferIt [31]. Robotic setups often contain (i) cluttered environments with overlapping objects and occlusions, and (ii) complex referring queries describing object attributes, such as color, shape, or relative position, to uniquely identify the object to grasp in the presence of distractors. For example the query: \"Grab the blue rectangular box on the right side\" can resolve ambiguity if the workspace contains multiple boxes. Recent work highlights the challenges in directly using RIS methods in robotics, where failure to predict accurate masks for smaller scaled objects and in cluttered scenes causes downstream problems in manipulation [27, 71]. Thus recent autonomous robots like MOKA [36] and OK-Robot [40] use open-set detectors like GroundedSAM [60] and OwlVIT [48] for visual grounding, as they are more robust to language variations and complexities. These models are trained on millions of images and use transformer-based architectures for generating probabilistic bounding box predictions after sampling text queries for a large set of objects. We identify four critical characteristics of an ideal VG model: (i) ability to leverage referring attributes in the input text to distinguish target object among distractors, (ii) robustness to occlusions and partial visibility of the target object, (iii) ease of fine-tuning on custom annotated datasets of RGB-Text-Mask tuples to improve in-domain performance, and (iv) generalizability to open-vocabulary settings with unseen object categories.\nGrasp Synthesis: Robotic grasping has been explored using both 4 and 6 Degree of Freedom (DOF) grasp poses. The 4-DOF grasp representation involves 3D positioning and top-hand orientation about the robot gripper axis [3, 11,28]. In contrast, the 6-DOF method, which includes three kinematic variables for both position and orientation, provides greater robustness, allowing object manipulation in cluttered environments with an arbitrary direction of grasping [14,43,67]. [16] introduced the GraspNet-1Billion dataset, which has been used to train DNN-based models on RGB-D or point cloud data [19, 43]. Recently, [15] achieved a 93.3% grasping accuracy by training GSNet [32] on GraspNet-1Billion, utilizing 3D convolutional layers to process point cloud data, followed by stacked MLP layers to predict grasp parameters. We focus on training a VG model to produce segmented object masks, which combine with depth maps to generate object-level point clouds compatible with downstream grasping modules."}, {"title": "3. Proposed Method: Hierarchical F\u0130LM ClipSeg (HiFi-CS)", "content": "We study VG in two scenarios: Closed and Open Vocabulary. In Closed Vocabulary, models are tested on datasets with pre-known object categories. Open Vocabulary evaluations assess methods on unseen environments and objects.\nReferring queries in robotic visual grounding often contain multiple object attributes that need to be accurately remembered and utilized for segmentation mask prediction. An effective visual grounding model must therefore employ a robust multi-modality fusion and learning strategy to correctly identify the target object. Our model (Fig. 2) utilizes a frozen CLIP VLM as a feature extractor for both image and text modalities, leveraging its joint embedding space. We hypothesize that a hierarchical and repeated fusion of image and text modalities can provide the segmentation decoder with sufficient clues to learn accurate segmentation masks especially when text queries are longer and more complex. Featurewise Linear Modulation (FiLM) [53] layers have been shown to effectively merge multi-modal features [9]. Building on this concept, we integrate FiLM layers into our trainable segmentation decoder.\nIn our approach, the referring text query is processed through the CLIP text encoder to produce conditional text embeddings, while the RGB image of the workspace passes through the CLIP visual encoder, with projections extracted from a selected set of K transformer blocks. We introduce FiLM layers to fuse extracted visual projections with conditional text embeddings before each decoder block, enhancing semantic retention for disambiguated segmentation. Our lightweight segmentation decoder, which contains K transformer blocks, receives language-conditioned visual inputs from FiLM. The final output, resized to match the input image dimensions, is then processed through a softmax layer. This layer assigns a binary label to each pixel, predicting whether it belongs to the referred object or the surrounding background. Our work differs from previous methods which typically merge multi-modal features at the first step of the decoder [45]. We show that continually merging these features within the segmentation decoder effectively improves semantic retention without parameter-heavy cross-modal attention. As a result, our model only contains around 6M trainable parameters, a 100x reduction over previous methods in Robotic VG and RIS.\nMathematical Formulation: Given input RGB image \\(I \\in \\mathbb{R}^{H\\times W\\times 3}\\) and referring query Q, VG models predict a binary mask \\(M \\in [0,1]^{H\\times W}\\) with H and W denoting the height and width of the image, respectively. The region with pixel values equal to 1 corresponds to the object referred to by Q, while pixel values equal to 0 correspond to the background. CLIP intermediate projections \\((P_1, P_2, . . . P_K)\\) are combined with query embedding QE using FiLM layers to generate decoder inputs \\((D_1, D_2, . . . D_K)\\) -\n\\[D_i = \\alpha(Q_E) (P_i + T_{i-1}(D_{i-1})) + \\beta(Q_E) \\quad (1)\\]\nwhere \\(\\alpha\\) and \\(\\beta\\) are feed-forward networks, \\(T_{i-1}\\) denotes the (i - 1)th transformer block and \\(D_0 = 0\\). Decoder progressively learn representations to segment the correct object. The final decoder output \\(D_K\\) is upsampled to the original image resolution H \u00d7 W with the predicted mask given by:\n\\[M_{pred} = Softmax(TransConv2D(D_K)). \\quad(2)\\]\nBoosting Zero-Shot Performance: We freeze the pre-trained CLIP VLM and only train the decoder. CLIP, pre-trained with contrastive image-language learning on large internet scale dataset, generates high quality visual and textual embeddings [55]. By leveraging these pre-trained capabilities, we hypothesize improved performance on unseen objects compared to full fine-tuning-based methods."}, {"title": "4. Experimental Results", "content": "This section discusses experiments across Closed and Open-Vocabulary settings.\nEvaluation Metric: We use Intersection Over Union (IoU), averaged over test sets, to evaluate models along with thresholded precision scores. Given a predicted segmentation mask M and ground truth mask G such that M, G \u2208 [0,1]H\u00d7K, IoU is calculated as the intersection of M and G divided by their union. P@X scores the percentage of predictions with IoU higher than threshold X.\nReferring Text Complexities: Referring queries often include various object attributes such as color, shape, relative position, or inter-object relationships to uniquely identify an object. As the number of attributes increases, so does the complexity of the text query. This requires the visual grounding model to account for all attributes in order to accurately identify the object to be retrieved. To quantify the complexity of referring queries, we use Named Entity Recognition (NER), which associates each word in a sentence with predefined entities like object names, colors, shapes, sizes, etc. Specifically, we employ a state-of-the-art NER model, GLINER [81], to categorize our test set into four groups based on the number of attributes present in the query. This allows us to compute IOU scores at varying levels of query complexity. For instance, the query, \"Please grab the blue pen on the right side,\" contains three attributes (object = pen, color = blue, position = right). More examples are provided in the supplementary material (Section 1)."}, {"title": "4.1. Datasets", "content": "We select two recent VG datasets for closed-vocabulary experiments, featuring cluttered indoor images with graspable objects and multiple instances, suitable for robotics.\nRoboRefIt [44] consists of 187 distinct real-world indoor scenes with 66 unique object categories. The resulting corpus contains around 50K tuples (RGB, text, mask). Two test splits are provided: Test A comprises samples with seen object categories as in the training set, while Test B comprises samples with unseen object categories. This corpus does not contain any annotations for grasp parameters, and thus can only be used for training visual grounding models.\nOCID-VLG [71] comprises 1763 highly cluttered indoor tabletop scenes and 31 unique graspable objects. Many scenes contain multiple instances of the same object and thus text queries use attributes such as object color, shape, relative position, and spatial relationships. The final dataset consists of roughly 89K (RGB, text, mask) tuples. Each tuple is also annotated with grasp parameters, but we only use the visual grounding masks for our experiments."}, {"title": "4.2. Baselines", "content": "We use two competitive VG baselines and two open-set RIS detectors for thorough comparison, addressing a gap in previous works to identify the best method for robotic VG.\nVL-Grasp (VL-Gr): Introduced in [44], VL-Grasp consists of a BERT text encoder and ResNet50 image encoder. The encoded output is concatenated and passes through a visual-lingual transformer with cross-modal attention. Finally, a decoder predicts a pixel-wise segmentation map. We train VL-Gr separately on RoboRefIt and OCID-VLG.\nCROG: Similar to our method, CROG [71] also uses the CLIP visual and text encoder to generate embeddings for referring text and RGB image. These embeddings pass through a multi-modal feature pyramid network and cross-modal attention layers, leading to a pixel-wise segmentation decoder. In contrast to our method, CROG finetunes the entire network including the CLIP layers. Although this baseline includes a decoder for predicting grasp parameters, we only use the VG part by eliminating the grasp loss. CROG is trained separately on RoboRefIt and OCID-VLG.\nGroundedSAM (GrSAM): This is a zero-shot baseline that combines an open-set object detector (Grounded DINO [42]) and a powerful segmentation model (SAM [34]). Grounded DINO takes as input the RGB image and a text query, outputting a bounding box over the predicted object. This passes through SAM to generate a segmentation mask. GrSAM demonstrated high performance on open-set object detection, and we use it without any further training [60].\nOwlViT + SAM (OwlSAM): This is another zero-shot baseline that combines an open-set object detector (OwlVIT [48]) with SAM. Similar to GroundedSAM, we use it without any further training as OwlVIT is trained on large datasets across diverse domains of visual grounding tasks."}, {"title": "4.3. Experimental Setup", "content": "Our lightweight model is trainable on a single RTX 5000 GPU whereas VL-Gr and CROG require four GPUs in parallel. Training employs pixel-wise binary cross-entropy loss, Adam optimizer [33], and a cosine learning rate scheduler. Zero-shot baselines use GPU-accelerated inference."}, {"title": "4.4. Closed Vocabulary", "content": "Our model outperforms all baselines on the RoboRefIt corpus (Tab. 1). The performance gap of VL-Gr and CROG between seen and unseen objects is substantial (25% and 14% respectively), indicating likely over-fitting to seen object categories. Our method achieves improved performance due to a compact and streamlined architecture design that leverages the strengths of frozen multi-modal embeddings from a VLM like CLIP. While CROG also uses CLIP as the backend feature extractor for the image and text modalities, it trains the entire CLIP model and thus loses the benefits of pre-training the VLM on millions of real-world images. CLIP was pre-trained with a contrastive loss to map images to their corresponding descriptive captions and, as a result, learns to transform an image and its corresponding caption to closer locations in the joint embedding space. Text queries in Referring Grasp Synthesis describe the referred object using multiple attributes like object category, color, shape, position, etc. By mapping these queries to the CLIP embedding space, the resulting multi-modal features are rich in semantics about the referred object. Thereafter, a hierarchical application of FiLM to fuse the embeddings and pass through a sufficiently large decoder effectively learns mappings to a pixel-level segmentation mask."}, {"title": "4.5. Open Vocabulary", "content": "Robots must grasp unseen objects in the real world, posing challenges due to the infinite variety of shapes and sizes of graspable objects. We address this by comparing models trained on RoboRefIt with open-set detection models in a zero-shot setting using a new, challenging corpus.\nData Creation: Given an RGB image, SAM [34] can segment all objects in the image. However, not all mask outputs correspond to meaningful objects. We collect a corpus of 120 cluttered environment images, manually validate segmentations produced by SAM, and crowd-source the (RGB-Mask) pairs to annotate referring text. Resulting corpus is called RoboRES (See supplementary material - Section 3).\nImproving open-set detection with language-conditioned guidance: We introduce a new method for zero-shot inference that leverages the capabilities of both language-conditioned segmentation and open-set detection models. During runtime, prediction from HiFi-CS is compared with the top three predictions of an open-set detection model. The entity with maximum overlap with our prediction is chosen as the output. We choose GroundedSAM as the open-set model and call this approach: GroundedSAM + HiFi-CS (GrSAM+HiFi-CS).\nFindings: Tab. 3 presents results of testing all models on RoboRES. HiFi-CS outperforms fine-tuned baselines, demonstrating improvements in zero-shot performance. As a smaller model, HiFi-CS averages 0.32 seconds per sample, making it the fastest baseline. It also shows strong performance at higher complexity levels (A=4). However, open-set detectors outperform fine-tuned language-conditioned segmentation models. This is expected, as models like GrSAM and OwlSAM are pre-trained on additional datasets for general segmentation tasks and likely encountered objects similar to our test set. As text complexity increases, performance of open-set detection models declines, while HiFi-CS continues to improve. A hybrid approach, combining GrSAM with HiFi-CS, capitalizes on the strengths of both techniques, resulting in significant improvements. Due to lightweight size of HiFi-CS, inference remains efficient when integrated with GrSAM."}, {"title": "5. Real World Experiments", "content": "We implemented a pipeline of visual grounding and grasping for our experiments. Visual grounding converted natural language instructions to object masks in RGB-D. The projected object level depth maps were used by the pre-trained AnyGrasp SDK [15] for generating candidate grasps. We use three VG baselines from our previous experiments for comparing performance in a real robot setting - HiFi-CS, GroundedSAM and GroundedSAM+HiFi-CS.\nExperimental Setup: We used five object classes: Fruit, Soda Can, Food Container, Spray Bottle, and Hardware. The first two categories are seen whereas the latter three are unseen by our pipeline. Object arrangement involved three levels with increasing number of distractors: Level 1 has one instance per object category, Level 2 has two instances per category, and Level 3 has three instances per category. We evaluated our pipeline with natural language commands using physical attributes visible to human eye. Our VG module captures images of the workspace across 5 views and predicts object masks. Top view mask was provided to AnyGrasp to output grasp poses. Experiments used a 7 DOF Franka Research 3 Arm, with RealSense D455 camera mounted at end-effector to capture RGB-D images. Motion and grasp poses were executed using velocity controller. All scenes used a standard table-top setup (Fig. 4).\nFindings: We used two metrics for evaluation: Segmentation Accuracy (SA) and Grasping Accuracy (GA), both scored through visual inspection. SA is 100 if a minimal referring query correctly segmented the required object, and we apply a penalty of 25 each time an additional attribute is required. SA is 0 if the model was unable to identify the object to grab. GA is 100 if the final grasp poses results in successful grasping and lifting of the object, otherwise, GA is 0. Grasping accuracy depends on the segmentation model, as an accurate segmentation mask increases the likelihood of a successful grasp. Tab. 4 shows the results obtained. Our proposed open vocabulary solution, which combines GroundedSAM with HiFi-CS, outperforms all baselines in both Segmentation Accuracy (SA) and Grounding Accuracy (GA). All methods perform worse on unseen objects (Food Container, Spray Bottle, Hardware) compared to seen objects (Fruit, Soda Can). In some trials with unseen objects, HiFi-CS fails to identify the correct object, causing GroundedSAM to default to the larger or more common object, regardless of the referring attributes in the text."}, {"title": "6. Conclusion and Future Work", "content": "This paper provides extensive comparisons of popular visual grounding techniques in closed and open-vocabulary robotic grasping. We introduce a language-conditioned segmentation model to generate object masks from complex text queries. Referred text in robotics often contains multiple object attributes required for accurate segmentation especially in presence of distractors of the target. Our proposed model uses an intuitive multi-modality fusion design to effectively utilize these attributes. Predicted masks can be used to construct object point clouds for grasp pose estimation. Our model outperforms competitive baselines in closed-vocabulary settings and can be combined with an open-set object detection model for open-vocabulary settings. We demonstrate this on a real robot across three difficulty levels. Our results show that language-conditioned models excel with longer text queries and, when paired with open-set detectors, improve zero-shot performance in visual grounding. Future work will focus on merging planning algorithms for open-vocabulary 6 DOF manipulations and adapting our method for visual grounding in navigation.\nLimitations: Multi-stage RGS is prone to errors, especially when VG misidentifies the target, resulting in incorrect grasps. To mitigate this, we use a hybrid language-conditioned and open-set segmentation model. Additionally, our system relies solely on a hand-mounted camera, and adding base cameras could improve grasp accuracy."}, {"title": "1. Analyzing Referring Text Attributes With Named Entity Recognition", "content": "Named Entity Recognition (NER) is a Natural Language Processing technique used to identify various entities within text input, which is closely related to our attribute analysis. NER models, typically trained on large annotated text datasets, learn to associate each word in a sentence with an entity label such as names, organizations, dates, colors, etc. For identifying and categorizing referring text in our test samples, we utilize the state-of-the-art GLiNER model. Given the referring text, we extract labels for Object, Color, Shape, and Position using the function illustrated in Fig. 1. Consequently, test sets are divided into four categories based on the number of attributes extracted by NER (examples provided in Tab. 1), and metrics are reported separately for each case as detailed in Section 4 of the main paper."}, {"title": "2. Ablation Studies", "content": "HiFi-CS has various hyperparameters that affect its overall performance. We perform ablation studies on five important aspects of our model: visual projections from the frozen VLM, the dimension of trainable decoder blocks, vision backend used for feature extraction, different types of multimodal fusion strategy and variations in text encoder.\nAll ablations are conducted using the RoboRefIt corpus for consistency, where we report the IoU across seen and unseen objects.\nVisual Projections: The CLIP VLM consists of multiple transformer blocks stacked sequentially. Input patches of the image pass through each transformer block, with each layer learning different levels of semantic information as the input propagates through the model. For a fixed version of CLIP (ViT-B/16), there are 12 transformer blocks in the vision encoder from which we can extract projections. We vary K, the set of transformer blocks chosen, between 4 to 6 to understand the impact on overall performance. This hyperparameter is crucial as the trainable decoder consists of K transformer blocks corresponding to the visual projections. Our results indicate that increasing the number of visual projections enhances performance on seen objects but saturates after K = 5 (Table 2). Since our objective is to perform well on unseen objects without over-fitting to any test set, we choose K = 5 for our model.\nDecoder Dimension: After FiLM conditioning, the merged multi-modal features pass through decoder transformer blocks. Each block is associated with an embedding dimensionality, which specifies the granularity of intermediate representations to be learned. Increasing the dimensionality also increases the model size. We vary this dimension D between 64, 128. Table 3 presents the results. Increasing the decoder size improves overall performance by allowing the decoder to learn better intermediate representations. However, increasing the size beyond D=128 causes training to diverge, indicating a saturation point.\nBackend CLIP Vision Transformer: The official implementation of CLIP provides various vision transformer backends. Larger models typically perform better than base models. We chose two different backend models for our ablation, namely ViT-B/16, and ViT-L/14, where 16 and 14 denote the patch dimensions used in the vision encoder. Table 4 shows our results. As expected, the larger vision transformer backend yields better results across both test splits.\nMulti-Modal Fusion Strategy: Table 5 shows the results of using two popular fusion methods. While using cross-attention mechanism, we observed that the model reached early saturation, with loss not decreasing even while gradually reducing the learning rate. The computationally expensive cross-attention mechanism might not effectively combine features that lie close in the joint dimension space, whereas a simple FiLM layer maintains the rich semantic information for visual grounding."}, {"title": "3. RoboRES Data Creation", "content": "To thoroughly benchmark our baselines in an open vocabulary setting with unseen samples, we created a new, complex test dataset to compare different visual grounding methods. The following steps outline the process of creating our corpus:\n1. Selection of Objects and Environment:\nWe selected a small set of graspable objects from day-to-day items.\nConsidering the wide range of environments where grasping robots can be deployed, we decided on five setups for capturing images: Table Top, Chair, Multi-layered Shelf, Drawer, and Human Hand.\n2. Arrangement of Objects:\nObjects were arranged with varying degrees of clutter:\nLow clutter (fewer than 4 well-spaced objects)\nMedium clutter (more than 4 closely spaced objects)\nHigh clutter (occluded objects present).\nWe also varied the lighting conditions: dark, dim, and bright, to ensure a holistic evaluation.\n3. Data Capture:\nThe environment and objects were set up, and images were captured using the RealSense D455 camera attached to our Franka Research 3 robotic arm.\nA total of 120 scenes were created with the given set of objects, varying clutter, lighting and background setup.\nWe found that using the gripper camera was not necessary as similar results were obtained with a simple iPhone camera.\n4. Mask Generation and Verification:"}, {"title": "4. Real world experiments", "content": "We performed all real world grasping experiments on the Franka Research 3 robotic arm. The experimental setup involved using five common object categories: Fruit, Soda Can, Food Container, Spray Bottle, and Hardware. For each category, there were three levels with an increasing number of distractors: Level 1 has one instance per object category, Level 2 has two instances per category, and Level 3 has three instances per category. For example, in Level 1, there are 5 objects, one from each category, with no distractors. In Level 2, there are 10 (5 x 2) objects, where each category has one target object and one distractor. In Level 3, there are 15 (5 x 3) objects, where each category has two distractors for the target object. This setup resulted in a total of 15 scenes, each designed to evaluate the model's ability to identify and grasp the target object in the presence of other items."}, {"title": "4.1. Minimal Referring Query (MRQ)", "content": "MRQ refers to the query with the least number of attributes required to uniquely identify the object of interest within a given scene. This concept is particularly relevant in scenarios with multiple objects, where the goal is to pinpoint a specific object. For instance, if there are two apples in a scene, each apple can be uniquely identified with just one additional attribute (apart from apple which is the object name). Possible MRQs for this scenario could be Give me the apple on the right or Give me the smaller apple. The MRQ is crucial for efficient and precise communication in robotic grasping tasks, minimizing query complexity while ensuring accurate object identification. In our experiments, if an MRQ results in the correct segmentation mask, we score that scene with 100 SA, as our model does not require redundant attributes to identify the target object accurately."}, {"title": "4.2. Implementing Referring Grasp Synthesis", "content": "The trials involved capturing RGB-D images with the robot, which provided both RGB (color) and depth information. These images, along with corresponding text queries, were input into the visual grounding model. This model segmented the referred object, producing the masked depth and RGB images, which were then processed by the Any-Grasp model to predict the grasp pose. The predicted 7 DOF grasp pose was subsequently executed by the robot. Table 7 showcases the predicted masks generated by GrSAM+HiFi-CS and grasp pose visualizations from AnyGrasp, with each image annotated with the corresponding input language query, displayed above each image. The examples include scenarios with multiple distractors to illustrate the model's robustness in complex environments."}, {"title": "4.3. Examining Segmentation Failure Cases", "content": "Table 8 highlights instances where our visual grounding model failed. In the first two rows, we use the same query, Can you grab the larger blue circular food container? for the images of the scene captured from different viewpoints. The model incorrectly identifies the smaller blue circular food container as the larger one in the second view. This discrepancy is due to the HiFi-CS model producing 2D segmentations, leading to perspective-dependent errors. In the top view, the blue container near the bottom looks bigger in perspective and our model correctly identifies this. However, in the second view, the blue container on the top looks larger than the blue container at the bottom due to a change in perspective. Therefore, the model misidentifies the smaller one as the larger one. Since the current approach does not utilize any 3D information, such segmentation inconsistencies may arise. Implementing 3D segmentation could mitigate these issues by providing more accurate, perspective-independent segmentations.\nIn the next two rows, we use the same image and input different queries: Grab the blue soda can on the bottom? and Grab the blue soda on the bottom? In this case, the model wrongly predicts the blue spray bottle on the bottom as the blue soda when the word can is omitted from the query. This example illustrates the model's sensitivity to specific object names. A minor change in the query can lead to incorrect predictions, highlighting the importance of precise language. To achieve more accurate results, it is necessary to include additional attributes in the queries. This would help the model to better distinguish between objects, reducing the likelihood of mispredictions due to subtle differences in phrasing."}, {"title": "4.4. Analysing Grasping Errors", "content": "We highlight the problems with using only one camera for our real world experiments in this section. The partial point cloud constructed using this camera works well for solid shapes like hardware adapters, as the solid edges of these objects are clearly represented in a top view. However, for curved objects like apples, soda cans, and spray bottles, the top view point clouds sometimes fail to accurately depict their exact curvature, resulting in a small offset during grasp pose execution. The widths of the soda can (6.6 cm) and food container (7.6 cm) are approximately the same as the maximum width of the gripper (8 cm), so even a tiny offset can lead to failure. Since the diameter of the spray bottle is smaller (5.3 cm), the offset does not cause an error. Another reason for the low grasping success rate is over-prediction in the visual grounding stage. Baselines such as GroundedSAM sometimes segment more than one object instance of the same/similar category, and although the segmentation mask contains the referred object, the best grasp pose might be executed on another object. Using a combination of our fine-tuned model, HiFi-CS, with an open"}]}