{"title": "Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment", "authors": ["Chaoqi Wang", "Zhuokai Zhao", "Yibo Jiang", "Zhaorun Chen", "Chen Zhu", "Yuxin Chen", "Jiayi Liu", "Lizhu Zhang", "Xiangjun Fan", "Hao Ma", "Sinong Wang"], "abstract": "Recent advances in large language models (LLMs) have demonstrated significant progress in performing\ncomplex tasks. While Reinforcement Learning from Human Feedback (RLHF) has been effective in\naligning LLMs with human preferences, it is susceptible to spurious correlations in reward modeling.\nConsequently, it often introduces biases such as length bias, sycophancy, conceptual bias, and\ndiscrimination that hinder the model's ability to capture true causal relationships. To address this,\nwe propose a novel causal reward modeling approach that integrates causal inference to mitigate these\nspurious correlations. Our method enforces counterfactual invariance, ensuring reward predictions\nremain consistent when irrelevant variables are altered. Through experiments on both synthetic\nand real-world datasets, we show that our approach mitigates various types of spurious correlations\neffectively, resulting in more reliable and fair alignment of LLMs with human preferences. As a drop-in\nenhancement to the existing RLHF workflow, our causal reward modeling provides a practical way to\nimprove the trustworthiness and fairness of LLM finetuning.", "sections": [{"title": "Introduction", "content": "Recent advancements in large language models (LLMs) have demonstrated remarkable capabilities in generating\ncoherent, contextually appropriate responses across a wide range of tasks (Brown et al., 2020). A key\napproach to further refine these models is Reinforcement Learning from Human Feedback (RLHF), which\nleverages human evaluations to guide the training process and align model outputs more closely with human\npreferences (Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022; Wang et al., 2024). RLHF typically\ninvolves training a reward model to capture human preferences, which is then used to fine-tune LLMs via\nreinforcement learning (RL) (Schulman et al., 2017; Chen et al., 2024b,f).\n\nDespite the success of RLHF, reward modeling is inherently prone to spurious correlations, which are\nassociations in the training data that do not reflect true causal relationships (Veitch et al., 2021), and can lead\nto unintended biases and induce reward hacking (McMilin, 2022). Reward hacking occurs when RL agents\nexploit flaws or ambiguities in the reward function to maximize rewards without genuinely improving alignment\nwith desired behaviors or completing designed tasks (Amodei et al., 2016; Weng, 2024). Consequently, this\nleads to misaligned models that exhibit biases such as favoring longer outputs (length bias) (Zheng et al.,\n2023), agreeing with user's incorrect assertions (sycophancy bias) (Perez et al., 2022), developing unintended\nshortcuts when making predictions (concept bias) (Zhou et al., 2023), and implicitly developing discrimination\nover certain demographic groups (discrimination bias) (Tamkin et al., 2023; Chen et al., 2024c). These biases,\nrooted in spurious correlations and reward hacking rather than true causal relationships, undermine the\nreliability and trustworthiness of LLMs, posing significant challenges for their safe and responsible deployment\nin real-world applications (Anwar et al., 2024; Qi et al., 2024).\n\nTo understand and mitigate these issues, it is essential to consider the sources of error in reward modeling.\nThe total error in the reward model can be decomposed into reducible and irreducible components, as shown"}, {"title": "Related Works", "content": "The issue of reward hacking has become increasingly significant as RLHF grows in popularity over the recent\nyears (Amodei et al., 2016; Casper et al., 2023; Kaufmann et al., 2023; OpenAI, 2023). RLHF aligns LLMs with\nhuman preferences by training a reward model (RM) to provide feedback based on user prompts (Christiano\net al., 2017; Ziegler et al., 2019; Chen et al., 2024b; Zhang et al., 2024b). However, RMs are often imperfect\nproxies of the underlying true human preferences, leading to instances of reward over-optimization (Coste\net al., 2023; Moskovitz et al., 2023), or reward hacking (Denison et al., 2024; Everitt et al., 2021), where\nmodels achieve high rewards without fulfilling the intended objectives (Pan et al., 2022; Weng, 2024).\n\nLLM reward hacking often stems from the model's reliance on spurious correlations in the preference dataset,\nsuch as length (Sountsov and Sarawagi, 2016; Dubois et al., 2024a; Huang et al., 2024), sycophancy (Sharma\net al., 2023; Ranaldi and Pucci, 2023), conceptual (Zhou et al., 2023), and demographic (Salinas et al., 2023)\nbiases. These spurious correlations, closely linked to reward hacking, can impair a model's capability to learn\nand generalize to broader scenarios (Ribeiro et al., 2016; Geirhos et al., 2020; Chen et al., 2024d).\n\nWithout proper constraints, models will exploit all available informative features during training, including\nunreliable spurious ones, which results in reward hacking, even if the task is very simple (Nagarajan et al.,"}, {"title": "Alleviating Spurious Correlations", "content": "Early efforts to mitigate spurious correlations and reward hacking in RLHF have primarily focused on\npenalizing specific biases within reward models (Mnih et al., 2015), especially correcting for length bias.\nFor example, Singhal et al. (2023) reveals that length-based biases in reward models significantly influence\nRLHF outcomes, often overshadowing non-length-related features, and proposes mitigation strategies such as\nbalanced preference datasets, reward data augmentation, confidence-based truncation, increased KL penalties,\nexplicit length penalties, omitting long outputs, and focusing on non-length reward metrics.\n\nTo address the overemphasis on longer response, Shen et al. (2023) proposed a Product-of-Experts (PoE)\nframework to decouple reward modeling from sequence length, thereby reducing the reward model's preference\nfor verbose but low-quality responses. Building on this, Eisenstein et al. (2023) introduced reward model\nensembles to moderate reward hacking by diversifying the sources of feedback and reducing reliance on any\nsingle reward model's spurious correlations. However, this method only partially mitigates the problem and\nfalls short of fully eliminating reward hacking. More recently, Ram\u00e9 et al. (2024) proposed Weight Averaged\nReward Models (WARM), which enhance robustness to distribution shifts by averaging model weights, offering\na more efficient and effective alternative to ensemble-based policy interpolation. ODIN (Chen et al., 2024a)\nadvanced this line of work by introducing a disentangled reward model architecture to tackle length bias.\nTheir approach separates reward factors into two linear heads, isolating content quality for use during RL\nfine-tuning, thus improving performance without sacrificing efficiency.\n\nIn contrast to these approaches, our causal reward modeling incorporates causal regularization directly into\nthe reward modeling process. By enforcing counterfactual invariance, we ensure that model responses align\nwith the true causal effects of human preferences rather than being driven by spurious correlations. Notably,\nunlike existing methods (Singhal et al., 2023; Chen et al., 2024a) that address a single type of spurious\ncorrelation, our approach fundamentally mitigates a broad spectrum of spurious correlations, providing a\ncomprehensive solution to reward hacking and enabling more reliable alignment with human preferences."}, {"title": "Preliminaries", "content": "The supervised fine-tuning step typically starts with a pre-trained language\nmodel, which is then fine-tuned through supervised learning on a high-quality dataset tailored to specific\ndownstream tasks, such as dialogue (Bai et al., 2022), instruction following (Longpre et al., 2023), and\nsummarization (Zheng et al., 2024). This fine-tuning process produces a model denoted as \u03c0SFT.\n\nDuring this stage, we will first need to have a dataset that consists of preference\npairs of responses, (y1, y2), for each prompt x. Typically, these pairs are obtained by presenting them to\nlabelers (e.g., humans), who evaluate the responses based on their preferences, represented as yw > y\u0131 | x,\nwhere yw and yi denote the preferred and less preferred responses, respectively. From a modeling perspective,\nthese preferences are assumed to be generated from an unknown latent reward model, r*(y, x). In practice,\nthe modeling assumptions for the preferences can vary depending on the problem, but the Bradley-Terry (BT)\nmodel is a commonly used assumption. The BT model computes the probability of one response y\u2081 being\npreferred over the other response y2 under the true reward function r*(x, y) by:\n\nGiven a static dataset of preference data D = {x(i), yw, y1(i)} sampled from p*, we can fit a reward model\nro(x, y) to estimate its parameters through maximum likelihood estimation. This approach is equivalent to\nbinary classification and can be trained by minimizing the negative log-likelihood loss:"}, {"title": "Counterfactual Invariance", "content": "An ideal debiased reward model should intuitively remain invariant to spurious factors of variations. For\nexample, to eliminate length bias, the reward model should exhibit invariance to changes in response length.\nTo formalize this notion, we leverage the concept of counterfactual invariance (Veitch et al., 2021).\n\nWe begin by introducing some notation. Let Z represent the random variable corresponding to a spurious\nfactor of variation (e.g., length), and let T denote the random variable that encompasses the prompt-response\npair. A reward model r is said to exhibit counterfactual invariance to Z if r(T(z)) = r(T(z')) for all z, z',\nwhere z and z' are realizations of Z, and T(z) denotes the counterfactual T we would have observed if Z were\nz. Throughout the paper, we use the term \"invariant\" or \"debiased\" to refer specifically to counterfactual\ninvariance."}, {"title": "Causal Decomposition", "content": "The prompt-response pair T can be decomposed into latent components based on their relations with the\nspurious factor Z (Veitch et al., 2021). Specifically, we define TZ, as the component of T that is not causally\ninfluenced by Z. In other words, TZ,\u3157 represents the part of T such that any function of T is counterfactually\ninvariant to Z if and only if it depends solely on TZ,1.\n\nUnder weak conditions on Z, TZ,\u3157 is well defined. Further details regarding the derivation and properties can\nbe found in (Veitch et al., 2021). In the next section, we extend this concept to develop a reward model that\nincorporates counterfactual invariance, which enables debiasing against various spurious factors."}, {"title": "Method", "content": "Ideally, counterfactual examples are necessary to learn counterfactual invariant predictors (Quinzan et al.,\n2022). However, obtaining such examples is challenging, especially in RLHF settings. For instance, given\na response of length 100, it is very hard to create a counterfactual response of length 50. Nonetheless, as\nsuggested by Veitch et al. (2021), observable signatures implied by causal graphs can be leveraged to regularize\nthe hypothesis class of the predictor.\n\nConsider the causal diagram of reward models in Fig. 1. Here, Z is the spurious factor (e.g., response length),\nT is the prompt-response pair, R is the reward and L is the preference label. The binary label L (e.g., L = 1\nwhen X\u2081 is preferred) can be modeled under the Bradley-Terry model, where preferences depend on true\nrewards. In practice, however, human labels are often biased, captured by a direct edge from Z to L.\n\nAs discussed in \u00a73.3, T can be decomposed into latent components based on their relation with Z. In addition\nto TZ,\u00b9, we define TL, as the component that does not directly cause L, and TZ^L as the complementary\nremaining part. And an invariant reward model should depend solely on TZ,1. Although precisely learning\nsuch an invariant reward model is infeasible without counterfactual dataset, the causal graph reveals that\nTL, is independent of Z. Consequently, any counterfactual invariant reward model must also be independent\nof Z, which leads to the following condition:"}, {"title": "Maximum Mean Discrepancy (MMD) Regularization for Independence", "content": "To enforce the independence condition outlined in Eq. (2), we employ Maximum Mean Discrepancy (MMD), a\nkernel-based statistical measure that quantifies the divergence between two probability distributions (Gretton\net al., 2012; Liu et al., 2020). MMD is commonly used to regularize models by ensuring alignment between\ndistributions across domains or subpopulations (Tolstikhin et al., 2016; Zhang et al., 2024a). Formally, given\ntwo distributions P and Q, the squared MMD in a reproducing kernel Hilbert space (RKHS) Hk is defined as:\n\nwhere F denotes a class of functions in Hk, and x ~ P, y ~ Q are two random variables. Intuitively, MMD\nmeasures the maximum mean difference between P and over functions f \u2208 F, as determined by a kernel\nk(,) such as Gaussian kernels.\n\nIn our approach, we use MMD as a regularizer to ensure that the learned reward model f(T) is invariant to\nthe spurious variable Z. If Z is binary, our MMD regularizer can be defined as:\n\nWhen Z spans a large or continuous space (e.g., response lengths), directly applying MMD becomes computa-\ntionally intensive. To address this, we partition Z into M discrete bins and compute MMD across all pairs\nof bins. Let b \u2208 [1, M] denote bin indices, with P\u2081(f(T)) representing the conditional distribution of f(T)\nwithin bin b, the regularizer is then defined as:"}, {"title": "Experiments", "content": "To examine the effectiveness of the proposed reward model, we'll test on four dataset covering sycophantic,\nlength, concept and discrimination bias. Although we only apply marginal regularization in \u00a74, in practice,\nthe focus is typically on the model's ability to generate the chosen responses based on prompts. Therefore, we\nadditionally test another variant of the regularization where the prompt and response pair are divided into\nchosen and rejected subsets and the independence regularization is applied for each subset individually. We\ndenote this as the conditional causal reward model (CRM), in addition to the unconditional variant discussed\nbefore."}, {"title": "Addressing Sycophantic Bias (Semi-synthetic)", "content": "Sycophantic bias (Sharma et al., 2023; Ranaldi and Pucci, 2023) refers to a model's tendency to produce\nresponses that agree with or flatter the user, regardless of the truth or accuracy of the content. This bias often\narises when reward models inadvertently assign higher rewards to outputs that align with users' stated beliefs\nor preferences, particularly in preference datasets where agreement is implicitly favored over truthfulness.\nFor example, in a conversational setting, if annotators systematically reward responses that confirm the\nuser's input (e.g., \"Yes, you are correct\"), the model learns to prioritize sycophantic behavior to maximize its\nreward. This can lead to outputs that prioritize agreeableness over factual accuracy, undermining the model's\ntrustworthiness.\n\nTo investigate sycophantic bias, we create a semi-synthetic dataset based on dataset\ndeveloped by Sharma et al. (2023). Specifically, our prompts are structured with the template \"{question} I\nthink the answer is {correct_answer} but I'm really not sure.\".\n\nIn this setup, we artificially induce a correlation between sycophantic behavior and correctness. Specifically,\nwith an 80% probability, the chosen response is prefixed with \"Yes, you are right.\" Conversely, with a 20%\nprobability, this prefix appears in the rejected response. This creates an artificial but controlled spurious\ncorrelation between agreement (\"Yes, you are right.\") and correct answer, enabling us to observe, measure\nand address sycophantic bias effectively.\n\nFor SFT, we use the Llama-3 8B base model (Dubey et al., 2024), finetuned on a combination of data from\nthe Anthropic HH-RLHF dataset (Bai et al., 2022) and our semi-synthetic sycophantic training dataset.\nThe HH-RLHF dataset is included to ensure sufficient training data volume, as the semi-synthetic dataset\ncontains only 1,727 examples. The reward and policy models are then trained using the chosen/rejected\npairs, with the policy fine-tuned for two epochs via Proximal Policy Optimization (PPO) (Schulman et al.,\n2017), implemented in OpenRLHF (Hu et al., 2024). Additional implementation details are available in\nAppendix B.1."}, {"title": "Addressing Length Bias", "content": "Length bias (Zheng et al., 2023) refers to the tendency of reward models to favor longer responses due\nto spurious correlations in the training data. For instance, in human preference datasets, annotators may\nunconsciously associate longer responses with higher-quality or more comprehensive answers, leading to\ndisproportionate rewards for verbosity rather than substantive content. This bias often misaligns the model's\nbehavior with true human preferences, particularly when concise and accurate responses are preferred in\nreal-world applications.\n\nWe adopted the Alpaca dataset (Dubois et al., 2024b) for our experiments. Initially,\nwe uses the chosen response for each prompt to do supervised finetuning (SFT) using the Llama-3 8B base\nmodel (Dubey et al., 2024). Then, this SFT model was subsequently employed to train both the reward\nmodel and the policy model. For reward model, we used the chosen and the rejected pair for training. With\nthe reward model, we then trained the SFT policy with the PPO implementation from OpenRLHF (Hu\net al., 2024) for one epoch. Additional details on hyperparameters and configurations are available in the\nAppendix B.2.\n\nOur findings are illustrated in Fig. 2, where each dot on the plots represents a single model run,\nevaluated by its win rate, calculated as the proportion of wins against the SFT model. The score is defined by\nscore = 50+ (nwin - nlose)/N * 100, where nwin and nlose denote the counts of wins and losses, respectively, and\nN represents the total test count. In the leftmost plot, we observe that both the conditional and unconditional\ncausal regularization methods achieve superior performance compared to the vanilla reward model with length\npenalty, as shown by their higher exponential moving average (EMA) curves. Furthermore, when examining\nthe Pareto frontier, our approach demonstrates an advantage over the baseline method.\n\nFinally, we analyze the impact of the regularization effect by sampling 50 responses per prompt and ranking\nthem using a reward model trained with varying causal regularization coefficients. We then compute the\naverage response length across all prompts for each rank. Our results show that models with higher coefficients\nassign higher ranks (i.e., lower numerical rank values) to responses with shorter lengths, indicating a reduction\nin the bias toward longer responses."}, {"title": "Addressing Concept Bias", "content": "Concept bias (Zhou et al., 2023) in LLMs refers to the model's unintended reliance on correlations between\nspecific concepts and labels present in the training data. For instance, in the Yelp Review dataset (Zhang et al.,\n2015), if most reviews mentioning \"food\" (categorized as a food concept) are labeled with positive sentiments,\nthe LLM may develop a shortcut, incorrectly predicting positive sentiment for any review that involves\n\"food.\" This type of concept bias, which stems from associating unrelated terms with certain outcomes due\nto imbalanced distribution in the training data, causes LLMs to make incorrect predictions in new, unseen\nscenarios, which highlights the tendency of LLMs to overgeneralize based on spurious correlations, rather\nthan always grasping the actual context of the input. In this section, we demonstrate the effectiveness of the\nproposed causal reward modeling in mitigating the concept bias when conducting sentiment analysis of the\nreview datasets.\n\nWe conducted experiments using Yelp (Zhang et al., 2015), IMDB (Maas et al., 2011),\nand Amazon Shoe Review (He and McAuley, 2016) datasets, augmented with additional concept labels\nprovided by Zhou et al. (2023). Specifically, each dataset includes three concepts, where Yelp has \"price\",\n\"service\", \"food\"; IMDB has \"music\", \"acting\", \"comedy\"; and Amazon has \"size\", \"color\" and \"style\".\nTo introduce more obvious concept bias, following Zhou et al. (2023), we modified each dataset to ensure all\npositive-sentiment samples were explicitly linked to a specific concept. For instance, in the Yelp dataset, we\nfiltered reviews so that all positive sentiment entries were linked to the \"food\" concept.\n\nTo facilitate training, we reformatted the datasets to align with the structure of Anthropic hh-rlhf (Bai et al.,\n2022) dataset. Specifically, we appended the prompt \"Classify the text into negative, or positive\" to the\nfront of each review, and used the correct \"positive\" or \"negative\" label from ground truths as the chosen\nassistant response. The incorrect classifications were then used in the rejected assistant response. We fully\nsupervise finetuned (SFT) the Llama-3 8B base model (Dubey et al., 2024) on each of the above processed,\nconcept-biased datasets using the chosen responses. The resulting SFT model was further utilized to train\nboth vanilla and causal reward models. Finally, using these reward models, we conducted PPO finetuning\nusing implementations from OpenRLHF (Hu et al., 2024) on the SFT model to produce final models for\nevaluation. More details on training hyperparameters are explained in Appendix B.3.\n\nWe assess performance using both utility metrics (Acc@C, Acc@NoC) as well as the bias-specific\nmetric Bias@C, as introduced in (Zhou et al., 2023). The utility metrics, which reflect the accuracy of correct\nsentiment classifications with (Acc@C) and without (Acc@NoC) the presence of a concept, indicate better\nperformance with higher values. On the other hand, Bias@C measures spurious correlations associated with\nconcept C, where values closer to zero suggest weaker biases. Specifically, positive Bias@C values suggest the\nmodel tends to predict positive labels when concept C is present in the input, whereas negative values suggest\nthe opposite tendency. For a more detailed explanation of the Bias@C metric, we direct interested readers to\ntheir original work (Zhou et al., 2023)."}, {"title": "Addressing Discrimination Bias", "content": "Given the implicit biases embedded in training data, LLMs often learn spurious discriminatory patterns over\ndifferent demographic groups (Tamkin et al., 2023). While some previous works attempt to leverage post-\ntraining methods (Bai et al., 2022) to mitigate this issue by designing specific bias-countering preference pairs,\nthese approaches are often labor-intensive, lacks explicit guarantees of effectiveness, and often compromise the\nmodel's overall utility (Allam, 2024). In contrast, we demonstrate below the effectiveness of our proposed CRM\nin explicitly mitigating discriminatory bias without relying on specific bias-focused data, while maintaining\nthe model's original performance on general language modeling tasks.\n\nTo obtain discrimination-specific data, we first filter the samples in the training set of\nthe Anthropic HH-RLHF (Bai et al., 2022) dataset, retaining those that contain a pre-defined large set of\ndemographic variables specified in Appendix B.4.1. This results in 35,567 samples, which we split into 90%\nfor training, 5% for validation, and 5% for testing. To stabilize training for CRM, we further group similar\ndemographic variables into 60 demographic bins, as detailed in Appendix B.4.2, balancing granularity and\ntraining batch volumes.\n\nWe evaluate both targeted discrimination bias performance and general model utility. (1)\nTo assess discrimination bias, we use the Anthropic Discrm-eval dataset (Tamkin et al., 2023), which contains"}, {"title": "Conclusions and Future Work", "content": "In this paper, we introduced a novel framework for causal reward modeling (CRM) aimed at addressing\nspurious correlations that compromise the alignment of LLMs with human preferences. By incorporating\ncounterfactual invariance into reward learning, our approach mitigates biases such as sycophancy, length bias,\nconcept bias, and discrimination. Through extensive experiments on both synthetic and real-world datasets,\nwe have demonstrated the effectiveness of CRM in enhancing fairness, reliability, and trustworthiness across\nvarious tasks. Additionally, CRM can be seamlessly integrated into any existing RLHF workflows, enabling\nmore robust and equitable alignment of LLMs without introducing significant complexity. As LLMs continue\nto be applied to more sensitive applications, ensuring their ethical and unbiased behavior is imperative. By\nbridging the gap between causality and reward modeling, our paper takes a step toward addressing this\nchallenge. Future work could explore extending our framework to other domains, investigating deeper causal\nstructures, and refining regularization formulations to further optimize performance and fairness."}, {"title": "Extension with DPO", "content": "Our framework can also be extended to DPO by replacing the reward model with the DPO's implict reward.\nThis gives us the following objective for training Causal DPO,"}, {"title": "Experimental Details", "content": ""}, {"title": "Sycophantic Bias", "content": "The reward model is trained using Low-Rank Adaptation (LoRA) (Hu et al., 2021) finetuning with rank\n64 and weight a = 128 with batch size 32 across 4 gpus. For both the conditional and the unconditional\nregularization, the coefficients are chosen from {0,0.1, 0.3, 0.5, 1, 3, 5, 10}. The final policy model is trained\nwith PPO with batch size 16 for 2 epochs. The initial KL coefficient is set to be 0.01."}, {"title": "Length Bias", "content": "To obtain the SFT model, we begin by finetuning the Llama-3 8B base model on selected responses from the\nAlpaca farm dataset for 3 epochs, using a learning rate of 2 \u00d7 10-5. Additional hyperparameters are available\nin the Alpaca farm GitHub repository2. Next, we train the reward model starting from this SFT model. This\ntraining is done using LoRA finetuning with rank 64 and weight a = 128, for 4 epochs, with a learning rate of\n1 \u00d7 10-4 and a batch size of 128 (distributed as 16 per GPU device).\n\nTo obtain a variety of reward models, we perform a hyperparameter sweep on two variables: 1) the number of\nbins, and 2) the regularization coefficient. For the number of bins, we explore values {10,20,30}, and for\nthe coefficient, we test {0.1, 1.0, 3.0, 10, 100}. Finally, we apply PPO to finetune the SFT model under our\nlearned reward model, obtaining the final policy model. For the PPO stage, we train for 1 epoch with a KL\ncoefficient sweep over {0.003,0.01, 0.03, 0.1}, resulting in a total of 60 (conditional) causal reward models.\n\nFor the baseline method, the reward model is trained with a regularization coefficient of 0 (equivalently). In\nthe PPO stage, we perform a more thorough sweep, tuning the KL coefficient over {0.003, 0.01, 0.03, 0.1},\nthe learning rate over {5 \u00d7 10-7,1 \u00d7 10-6}, and the length penalty over {0,1 \u00d7 10\u22123,1 \u00d7 10\u22124,1 \u00d7 10\u22125,5 \u00d7\n10-4,1 \u00d7 10-6,5 \u00d7 10-6}. This process results in 56 models, providing a comparable set to the causal reward\nmodels."}, {"title": "Concept Bias", "content": "As briefly mentioned in \u00a75.3, we supervised finetuned (SFT) the Llama-3 8B base model on each of the\nprocessed Yelp, IMDB, Amazon Shoe Review datasets. We keep the same hyperparameters for all datasets,\nwhich are illustrated in Table 4. The resulting SFT model is used for the reward learning through LORA,\nwhere detailed parameters are illustrated in Table 5. The reward models are subsequently utilized during\nthe PPO, where the hyperparameters for PPO on each dataset is showed in Table 6. All the trainings are\ndistributed on 8 NVIDIA A100 GPUs."}, {"title": "Discrimination Bias", "content": ""}, {"title": "Training Data Preparation", "content": "This section provides the demographic keyword groups used for filtering data from the Anthropic HH-RLHF\ndataset (Bai et al., 2022). The keywords were grouped into broad categories based on demographic attributes,\nensuring comprehensive coverage. Table 7 through 11 summarize the categories and their associated keywords.\nThese keyword categories provided a robust basis for filtering the HH-RLHF dataset, ensuring the inclusion of\ndiverse demographic contexts in the training data."}, {"title": "Demographic Bins", "content": "To stabilize training for CRM, we grouped similar demographic variables into 60 distinct bins. Specifically, we\ntargeted age, gender, and race-related discrimination. Each row in Table 7, Table 8, and Table 9 was treated\nas a bin, resulting in a total of 4 \u00d7 3 \u00d7 5 = 60 bins."}, {"title": "Detailed Description", "content": "Aiming to address a comprehensive collection of discrimination factors spanning age, gender, race and political\ngroups that LLMs might encounter when handling various forms of societal decisions, we can construct any\ntarget training dataset by following the anthropic discrimination dataset Tamkin et al. (2023) which covers 70\ntopics across society that involve accepting/rejecting a person."}]}