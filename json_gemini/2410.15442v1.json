{"title": "EXPLORING SOCIAL DESIRABILITY RESPONSE BIAS IN LARGE LANGUAGE MODELS: EVIDENCE FROM GPT-4 SIMULATIONS", "authors": ["Sanguk Lee", "Kai-Qi Yang", "Tai-Quan Peng", "Ruth Heo", "Hui Liu"], "abstract": "Large language models (LLMs) are employed to simulate human-like responses in social surveys, yet\nit remains unclear if they develop biases like social desirability response (SDR) bias. To investigate\nthis, GPT-4 was assigned personas from four societies, using data from the 2022 Gallup World Poll.\nThese synthetic samples were then prompted with or without a commitment statement intended to\ninduce SDR. The results were mixed. While the commitment statement increased SDR index scores,\nsuggesting SDR bias, it reduced civic engagement scores, indicating an opposite trend. Additional\nfindings revealed demographic associations with SDR scores and showed that the commitment\nstatement had limited impact on GPT-4's predictive performance. The study underscores potential\navenues for using LLMs to investigate biases in both humans and LLMs themselves.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are not only powerful tools for language tasks (e.g., topic extraction, sentiment analysis),\nbut also hold significant potential for creative applications beyond traditional linguistic boundaries. Their remarkable\nability to analyze and generate human-like text across diverse contexts has opened new avenues for innovation. One\nparticularly important application of LLMs lies in their potential to simulate human responses in social surveys Argyle\net al. [2023], Lee et al. [2024], Qu and Wang [2024] and experiments Hewitt et al. [2024], Mei et al. [2024], offering\na promising alternative for observing and understanding attitudes, perceptions, and behaviors. While debates persist\nregarding the nature of language\u2014whether it primarily serves as a tool for communication or thought Fedorenko et al.\n[2024]-these discussions should not deter social scientists from conducting empirical investigations into the capacity of\nLLMs to simulate human responses in survey contexts.\nUsing LLMs to simulate human responses provides substantial advantages for social science research. LLMs can offer\na quick, cost-effective way to generate data that can refine survey designs and enhance sample representativeness,\nespecially for underrepresented social groups. They can also fill in missing data and explore new possibilities by\nsimulating responses that may not have been captured in the original survey Kim and Lee [2023]. These features make\nLLMs a powerful tool for achieving more comprehensive and accurate analyses, thereby having the potential to advance\nsocial science research.\nHowever, when we embrace LLMs in social science research, it is important to comprehensively assess LLMs' ability\nto imitate a wide range of human aspects including perceptions, behaviors, and biases. While many studies have\nfocused on investigating LLMs' ability to simulate human beliefs Lee et al. [2024], perceptions Hwang et al. [2023],\nand behaviors Aher et al. [2023], Argyle et al. [2023], one critical area that has received less attention is whether LLMs\ncan inherit human biases commonly discussed in survey research. Such human bias possibly streams down to LLMs\ndue to the inherent bias embedded in the training and human-feedback data Bai et al. [2022], Pfohl et al. [2024]."}, {"title": "2 Understanding Social Desirability Response Bias in Large Language Models", "content": "Investigating bias in LLMs in their applications in social surveys is crucial. First, evaluating how well LLMs replicate\nbiases inherent in survey designs is critical for accurately simulating human behavior and improving the reliability of\nthese simulations. Second, unnoticed biases in LLMs, whether stemming from the models themselves or arising during\ntheir applications, can lead to flawed analysis, perpetuate harmful stereotypes, and compromise both future survey\ndesign and the integrity of AI technologies. By understanding whether specific biases exist in LLMs and how they align\nwith or differ from human biases, researchers can better interpret survey outcomes and enhance the utility of LLMs in\nsocial research.\nWithin the spectrum of biases pervasive in social surveys, social desirability response (SDR) bias Crowne and Marlowe\n[1960] remains a long-standing concern for social scientists. SDR bias is characterized by respondents' inclination to\nprovide answers that make them look good. Rooted in the tendency to conform to social expectations Schlenker and\nWeigold [1989], SDR bias has been widely observed and discussed in survey research across various data collection\nmodes Dodou and De Winter [2014], Holbrook et al. [2003], Kreuter et al. [2008]. Survey questions on sensitive\ntopics like politics, ethics, or behaviors are particularly susceptible to the SDR bias Johnson and Van de Vijver [2003],\nKrumpal [2013], Streb et al. [2008].\nTo collect truthful responses in social surveys, social scientists have tried multiple strategies. One effective strategy\nis to include a warning message in social surveys, which aims to encourage respondents to follow instructions or\nto think carefully Huang et al. [2012], Krosnick [2000]. However, while warning messages in a survey have been\nfound to increase respondents' motivation to generate truthful answers to survey questions, they can also induce\nreputation-management concerns that manifest themselves in more socially desirable attitudes and behaviors Clifford\nand Jerit [2015]. In other words, the presence of a warning message in the survey questionnaire plays a double-edged\nsword that can elevate the respondents' motivation or their SDR bias.\nThis current research investigates if the SDR bias will manifest in responses from LLMs when they are applied in the\nsurvey context. We use the commitment statement as a potential stimulus that induces social desirability bias in the\nresponses of LLMs. The commitment statement, as a type of warning message, is known to provoke SDR bias by\nemphasizing participants' commitments in responding to survey questions Clifford and Jerit [2015]. This heightened\nengagement can make respondents more conscious of how they present themselves, leading them to alter their answers\nto align with socially acceptable norms.\nComparing LLMs' responses between the two conditions, with and without the commitment statements, this study\nexamines the presence and patterns of SDR in LLMs. We investigate this phenomenon using two different SDR\nindicators: SDR index and civic engagement (CE, hereafter) index. Both SDR index and civic engagement activity\nhave been used to capture SDR bias in previous studies Clifford and Jerit [2015], Holbrook and Krosnick [2010]. SDR\nindex is specifically designed to measure the extent of social desirability bias in respondents, directly capturing the\ntendency to provide socially desirable answers rather than truthful ones. CE index, on the other hand, intends to measure\nactual behaviors related to civic participation, such as election participation, charity donations, and helping strangers.\nAlthough CE index is not specifically designed to measure SDR bias, the responses it captures can still be influenced by\nit Holbrook and Krosnick [2010]. Assuming that a commitment statement leads to SDR bias among human participants,\nit is hypothesized that:\nHla-b: Synthetic samples produced by LLMs exposed to a commitment statement will display social desirability bias,\nresulting in higher levels of a) SDR scores and b) CE scores compared to synthetic samples not exposed to a commitment\nstatement.\nThis study further examines the association between demographics and SDR bias. Previous studies have found that\ncertain demographic groups exhibit a higher tendency toward SDR bias than others. For example, SDR bias has"}, {"title": "3 Methods", "content": "This study used the dataset collected in the 2022 Gallup World Poll as the benchmark. The Gallup World Poll is a\nlarge-scale survey study conducted annually in over 140 societies, representing more than 99% of the world's adult\npopulation. It collects data on a wide range of topics such as Internet access, civic engagement, leadership approval,\nmedia freedom, and many other subjects related to human needs and potential. This study focuses on four societies:\nHong Kong, South Africa, the United Kingdom, and the United States. Our focus on these four societies is based on\ntheir commonalities and differences. English serves as a primary language in all of them, ensuring a consistent linguistic\ncontext. However, their distinct cultural backgrounds offer an opportunity to explore how these cultural differences may\ninfluence LLMs' responses with regard to SDR bias.\nFor each society under study, we randomly selected 500 respondents from the Gallup World Poll sample using stratified\nsampling based on age and gender. Due to some synthetic respondents generated by GPT providing invalid answers, the\nfinal valid sample sizes are as follows: 447 for Hong Kong, 440 for South Africa, 437 for the United Kingdom, and 454\nfor the United States."}, {"title": "3.1 Benchmark Data", "content": "been positively associated with age, with older individuals displaying higher levels of SDR bias compared to younger\nindividuals Ausmees et al. [2022], Dijkstra et al. [2001], Soubelet and Salthouse [2011]. Similarly, better-educated\ngroups have been shown to exhibit greater SDR bias than less-educated groups Clifford and Jerit [2015]. In addition\nto the main effect of demographics on SDR bias, a moderation effect between demographics and survey design was\nobserved. Specifically, the impact of a commitment statement on SDR bias was found to be greater among better-educated individuals Clifford and Jerit [2015]. This study investigates whether these patterns persist in synthetic\nsamples generated by LLMs.\nRQ1: How are demographics of synthetic samples associated with SDR bias?\nRQ2: Does the effect of a commitment statement on SDR bias vary across demographic groups?\nAdditionally, the study examines if SDR bias induced by a commitment statement will influence the predictive\nperformance of LLMs. LLMs with the commitment statement compared to those without the commitment statement\nmay generate responses that misalign with real human responses due to the induced SDR bias. This will improve our\nunderstanding of downstream effect of SDR bias and the role of a commitment statement in the predictive performance\nof LLMs.\nRQ3: Does a commitment statement affect the predictive performance of GPTs?"}, {"title": "3.2 Prompt Setting and Research Design", "content": "We employed GPT-4 (version gpt-4-1106-preview), recognized as one of the most powerful LLMs, to generate\nsynthetic individuals and their responses. To encourage the variance of responses, the temperature is set as 1.5.\nTo create realistic personas, we constructed detailed prompts that included three sets of variables from the Gallup\nWorld Poll. The first set of variables are demographic information provided by respondents in the survey, including\nage, gender, marital status, education level, annual household income, employment status, household size, perceived\neconomic circumstances. The second set of variables are survey contexts, including geolocation, year, and date\nof the interview. The third set of variables is the Community Basics Index consisting of seven items, including\nrespondents' perceptions about the public transportation system, roads and highways, air quality, and more. The\nCommunity Basics Index offers insights into respondents' social and physical surroundings. The Gallup World Poll\nsurvey items used in this study are available at Gallup Worldwide Research Methodology and Codebook https:\n//news.gallup.com/poll/165404/world-poll-methodology.aspx. An example prompt is as follows:\n\"I am [age] years old. My current marital status is [marital status]. As for education, I have [education level]. My\ncurrent employment status is [employment status]. My annual household income is [annual household income]. I am\nfrom [urbanity]. My HHSIZE (Total Number of People Living in Household) is [HHSIZE].\nI am residing in [country]. Today is [year and date]. In the city or area where I live, I am [public transportation] with\nthe public transportation systems.\nIn the city or area where I live, I am [road quality] with the roads and highways. In the city or area where I live, I am\n[education quality] with the educational system or the schools. In the city or area where I live, I am [air quality] with\nthe quality of air. In the city or area where I live, I am [water quality] with the quality of water. In the city or area\nwhere I live, I am [quality healthcare] with the availability of quality healthcare. In the city or area where I live, I am\n[affordable housing] with the availability of good affordable housing.\"\nThis study employed a within-subject experiment design where synthetic individuals were exposed to both the presence\nand absence of the commitment statement. The commitment message reads \"It is important to us that participants\nin our survey pay close attention to the materials. Are you willing to carefully read the materials and answer all of\nthe questions to the best of your ability?\" After the treatment, synthetic individuals were prompted to provide their\nanswers to targeted questions. Due to limitations in instruction-following abilities Qin et al. [2024] and the occurrence\nof hallucinations Huang et al. [2023], LLMs do not always adhere to instructions, often producing responses that do not\nalign with the predefined answer options. To mitigate this issue, we prompted them to answer each set of questions\nindependently. For instance, after the pre-setup, synthetic individuals responded to a set of SDR items. We then started\nover the pre-setup to have them respond to civic engagement activity items. Examples of prompts are available in the\nsupplemental document."}, {"title": "3.3 Measurements from Synthetic Samples", "content": ""}, {"title": "3.3.1 Social Desirability Response (SDR) Index", "content": "The SDR index was derived from [Ballard, 1992]. This index consists of 13 true/false items, with seven statements\ndescribing socially (un)desirable but uncommon behaviors and the other six representing socially (un)desirable but\ncommon behaviors. For example, the statement \u201cI sometimes feel resentful when I don't get my own way\" illustrates an\nuncommon behavior. Answering \u201cfalse\u201d to this question is considered a socially desirable response. Another statement,\n\"I am always courteous, even to people who are disagreeable,\u201d represents a common behavior. Answering \"true\" to\nthis question is considered socially desirable. The items are dichotomously scored, with a \u201c1\u201d indicating a socially\ndesirable response\u2014\u201ctrue\u201d for socially desirable items and \u201cfalse\u201d for socially undesirable ones. The SDR index score\ngenerated by synthetic samples was computed by summing the number of socially desirable responses, resulting in a\nscore ranging from 0 to 13 (M = 5.05, SD = 2.88)."}, {"title": "3.3.2 Civic Engagement (CE) Index", "content": "The CE index was derived from the measurements in the Gallup World Poll survey. This index consists of three items\nassessing civic activities in the past month, including donating money to charity, volunteering time to an organization,\nand helping a stranger. For instance, to measure donations to charity, an item asking \u201cHave you done any of the\nfollowing in the past month? How about donated money to a charity?\" was used. A binary scale \u201cYes\u201d or \u201cNo\u201d was\nused for each item. The CE index score generated by synthetic samples was computed by summing the number of civic\nengagement activities, resulting in a score ranging from 0 to 3 (M = 1.08, SD = .60)."}, {"title": "3.3.3 Analytic Plans", "content": "To investigate Ha-b, we used multilevel regressions to account for the within-subject effects of synthetic individuals,\nwith the treatment condition serving as the independent variable. To examine RQ1 and RQ2, we conducted a multilevel\nregression in which demographics, the treatment condition, and their interaction terms served as independent variables,\nwhile accounting for the within-subject effects of synthetic individuals. For RQ3, we computed the F1 score for each\ncivic activity. The F1 score can be calculated for each label (e.g., \"Yes\" or \"No\"). For simplicity, we reported the\nF1 score of the label that produced the higher value. For charity donations and volunteering, higher F1 scores were\nassociated with the \"No\" response, while for helping a stranger, higher F1 scores were associated with the \"Yes\"\nresponse. Thus, we reported these F1 scores."}, {"title": "4 Results", "content": ""}, {"title": "4.1 Reporting Socially Desirable? Effect of Commitment Statement on SDR bias (Hla\u2212b)", "content": "To assess whether the commitment statement induces social desirability response bias in LLMs, we examined two\nscales generated by GPT-4: the SDR index and the CE index. First, we examined this phenomenon based on the SDR\nindex. As Figure 1A illustrates, it is evident that the commitment statement facilitates SDR bias. SDR index was\nsignificantly higher when the commitment statement was present (estimated mean (hereafter, M) = 5.49, SE = 0.07, \u0421\u0406\n95% = [5.36, 5.63]) than when such statement was absent (M = 4.60, SE = 0.07, CI 95% = [4.47, 4.74]). Figure 1B\nillustrates the distribution of SDR index with and without the commitment statement across societies. The distribution\npattern was homogenous across the four societies, indicating that the synthetic sample exposed to the commitment\nstatement produced higher scores than those without it regardless of the geographical context of the study."}, {"title": "4.2 Main and Moderation Effects of Demographics on SDR Index (RQ1 & RQ2)", "content": "We examined the associations between demographic characteristics and SDR bias. Our analysis focused on the SDR\nindex, as the SDR bias was exclusively observed in this measure. This investigation revealed significant main effects of\nage and education on the SDR index. Specifically, synthetic individuals with older ages tended to display higher SDR\nscores, $b = .03$, $SE = .004$, $t(3461.56) = 7.47$, $p < .001$. Similarly, synthetic individuals with higher levels of education\nexhibited greater SDR scores, $b = .29$, $SE = .11$, $t(3517.71) = 2.54$, $p < .05$. Other demographics such as gender and\nincome were not significantly associated with the SDR index.\nWe further examined whether demographic characteristics interact with the commitment statement in influencing SDR\nindex. The impact of the commitment statement on the SDR index was positively moderated by age, $b = .01$, $SE =$\n$.005$, $t(1773) = 2.61$, $p < .01$. In other words, the presence of a commitment statement significantly increased SDR\nindex as age increased, whereas such increase was relatively modest in the absence of a commitment statement. Other\ndemographic variables, including education, gender, and income, did not have such significant interaction effects."}, {"title": "4.3 Limited Effect of Commitment Statement on the Predictive Performance of GPT-4 (RQ3)", "content": "Next, we examined whether the commitment statement affects the predictive performance of GPT-4. The F1 score,\nwhich measures the balance between precision and recall, was used to evaluate this performance. Figure 3 summarizes\nthe F1 scores for civic engagement activities between the two conditions across the four societies. Overall, the effect of\nthe commitment statement on the predictive performance of GPT-4 was limited. Across the three civic engagement\nactivities, F1 scores were similar between the two conditions, and this pattern was consistent across the four societies.\nComparing these response patterns provides more context about the performance of GPT-4. Two key patterns emerge.\nFirst, the response patterns from synthetic individuals were nearly identical across societies. Synthetic samples\npredominantly responded 'No' to both the charity donation and volunteer questions, while answering 'Yes' to the help\nstranger question. Second, GPT-4's predictive performance was particularly lower in the USA and UK, as its predictions\ndeviated more from the actual responses of individuals in these societies than Hong Kong and South Africa."}, {"title": "5 Discussion and Conclusions", "content": "This study investigates social desirability response (SDR) bias in large language models (LLMs). Using GPT-4 and the\n2022 Gallup World Poll data, we created synthetic samples from four different societies including Hong Kong, South\nAfrica, USA, and UK."}, {"title": "5.1 Do LLMs Acquire Social Desirability Bias? An Unresolved Question that Needs Further Attention", "content": "Our findings did not provide consistent evidence of SDR bias in LLMs when simulating human responses to survey\nquestions. On one hand, the presence of a commitment statement increased scores on the SDR index, indicating\npotential SDR bias. On the other hand, it lowered scores on the civic engagement (CE) index, suggesting an opposite\npattern from what is typically associated with SDR bias.\nThis inconsistency in our study suggests that a deeper understanding of SDR bias in LLMs is needed. A dual role of\nwarning messages has been documented in the literature. While commitment statements can unintentionally induce\nSDR bias Clifford and Jerit [2015], they can also enhance truthful responses by increasing participants' attention to the\nsurvey Hibben et al. [2022]. Our findings suggest that the effect of a commitment statement may vary depending on the\nnature of the survey items Krumpal [2013]. The SDR index presents hypothetical situations, allowing respondents to\nadjust their answers in socially desirable ways. In contrast, while individuals might exaggerate their engagement in\ncivic behaviors to appear more socially acceptable, doing so would involve lying, which could provoke discomfort and\nguilt. This tendency to report honestly on the CE index might be stronger in the presence of a commitment statement.\nGPT-4 might capture this nuanced aspect of human behavior and reflected the contradictory pattern through synthetic\nsamples.\nSDR bias is a complex phenomenon. It can be unconsciously encouraged through self-deception or strategically\nmotivated by impression management Paulhus [1984]. It can also manifest in various forms, such as overreporting\npositive behaviors, underreporting negative behaviors, altering attitudes on controversial issues, and supporting\nmainstream opinions Holtgraves [2017]. Although our study does not provide clear evidence of SDR bias in LLMs, this\nsingle study is insufficient to conclude that LLMs will not exhibit such bias when simulating human responses in social\nsurveys. Given the wide range of influences on SDR, from question framing to survey mode Heerwegh [2009], Kreuter\net al. [2008], Tourangeau and Yan [2007], future research should explore how other stimuli may trigger SDR in LLMs,\nbroadening our understanding of how these models respond to diverse survey contexts."}, {"title": "5.2 Associations between Demographics and SDR Bias", "content": "GPT-4 captured the associations between demographics and SDR bias. Consistent with findings from human research,\nolder and better-educated synthetic individuals exhibited greater SDR bias compared to their counterparts Ausmees\net al. [2022], Clifford and Jerit [2015], Dijkstra et al. [2001], Soubelet and Salthouse [2011]. These findings suggest\nthat GPT-4 mirrors how such bias interacts with demographic factors. Several studies have demonstrated that LLMs can\nintegrate demographic information to estimate perceptions and behaviors Argyle et al. [2023], Lee et al. [2024]. This\nstudy contributes to this line of research by extending its scope to more complex and subtle psychological phenomena\nsuch as SDR bias.\nIn addition to these main effects, our analysis revealed a moderation effect of demographics on the commitment\nstatement. Specifically, the commitment statement had a more pronounced effect on increasing SDR scores among\nolder synthetic personas. This novel finding, not previously explored in human research, highlights the potential of\nLLMs for social science exploration and suggests that similar results could be replicated with human participants. A\npossible explanation is that older adults may be more motivated by social expectations and norms, which can lead them\nto present themselves in a more socially desirable manner. The commitment statement likely heightened their awareness\nof social expectations, thereby amplifying their tendency to align their responses with perceived social acceptability."}, {"title": "5.3 Impact of SDR Bias on Predictive Performance", "content": "Overall, GPT-4 demonstrates a satisfactory level of performance in estimating human civic activities. Although GPT-4's\nresponses to civic activities systematically vary based on the condition of the commitment statement, this effect is\nmarginal in altering the predictive performance of GPT-4. However, a closer investigation reveals GPT-4's lack of\nsensitivity to cultural variations. In particular, there was a greater discrepancy between human and synthetic samples\nfrom the USA and the UK compared to those from Hong Kong and South Africa in responses to charity donations.\nWhile many human participants from the USA and UK reported that they donated their money to charity, GPT-4\nunderestimated this generosity.\nThese results are intriguing, especially considering recent findings on Western-centric biases in LLMs Naous et al.\n[2024], Qu and Wang [2024]. One hypothesis is that GPT-4 may be overcompensating for perceived biases by\nunderrepresenting prosocial behaviors in Western societies like the USA and UK. This could stem from efforts to correct\nfor overrepresentation of Western norms in training data, inadvertently leading to an underestimation of behaviors\nsuch as charitable giving, which are culturally significant in these regions. Another theoretical framework that might\nexplain this discrepancy is the Cultural Tightness-Looseness Theory Gelfand et al. [2011], which posits that societies\nvary in the strength of their social norms and tolerance for deviant behavior. GPT-4 may not accurately capture these\ncultural nuances, leading to homogenized responses. Additionally, the model's training data may lack sufficient cultural\nspecificity, causing it to generalize behaviors across societies without accounting for regional differences. Further\ninvestigation into the training data's composition and the model's cultural sensitivity is necessary to understand and\naddress these predictive inaccuracies."}, {"title": "5.4 Methodological Implications", "content": "Overall, our data provides inconclusive results about SDR bias in LLMs. The lack of empirical evidence from human\ndata further obscures our understanding of this bias in LLMs. With only a handful of studies examining the effects\nof warning messages on SDR bias, it remains unclear whether our synthetic data reflects SDR bias in the same way\nas human data. Although SDR bias has been studied for decades, its numerous influential factors such as survey\ndesign Clifford and Jerit [2015], items Holbrook and Krosnick [2010], modes Holbrook et al. [2003], and individual\ncharacteristics Crowne and Marlowe [1960]\u2013complicate its investigation.\nUsing GPT-4 simulations presents significant potential for social science research by allowing us to investigate\nunderexplored aspects of human bias more quickly and cost-effectively. LLMs' ability to replicate human behaviors\nand biases has been increasingly documented Argyle et al. [2023], Lee et al. [2024], Qu and Wang [2024], suggesting\nthat our synthetic data may reflect the pattern of human SDR bias to some extent. Our findings offer plausible scenarios\nrelated to SDR bias that may merit further investigation. This LLM research, inspired by human subject studies, has the\npotential to inspire future research involving human subjects to investigate previously unknown patterns of SDR bias.\nSuch a symbiotic research approach can deepen our understanding of both human and LLM behaviors and biases."}]}