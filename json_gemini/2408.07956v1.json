{"title": "RandomNet: Clustering Time Series Using Untrained Deep Neural Networks", "authors": ["Xiaosheng Li", "Wenjie Xi", "Jessica Lin"], "abstract": "Neural networks are widely used in machine learning and data mining. Typically, these networks need to be trained, implying the adjustment of weights (parameters) within the network based on the input data. In this work, we propose a novel approach, RandomNet, that employs untrained deep neural networks to cluster time series. RandomNet uses different sets of random weights to extract diverse representations of time series and then ensembles the clustering relationships derived from these different representations to build the final clustering results. By extracting diverse representations, our model can effectively handle time series with different characteristics. Since all parameters are randomly generated, no training is required during the process. We provide a theoretical analysis of the effectiveness of the method. To validate its performance, we conduct extensive experiments on all of the 128 datasets in the well-known UCR time series archive and perform statistical analysis of the results. These datasets have different sizes, sequence lengths, and they are from diverse fields. The experimental results show that the proposed method is competitive compared with existing state-of-the-art methods.", "sections": [{"title": "1 Introduction", "content": "Neural networks serve as fundamental learning models across disciplines such as machine learning, data mining, and artificial intelligence. Typically, these networks go 1 through a training phase during which their parameters are tuned according to specific learning rules and the data provided. A popular training paradigm involves backprop- agation for optimizing an objective function. Once trained, these networks can be deployed for a variety of tasks, including classification, clustering, and regression. A time series is a real-valued ordered sequence. The task of time series clustering assigns time series instances into homogeneous groups. It is one of the most important and challenging tasks in time series data mining and has been applied in various fields such as finance (Kumar et al., 2002), biology (Subhani et al., 2010; Fujita et al., 2012), climate (Steinbach et al., 2003), medicine (Wism\u00fcller et al., 2002) and so on. In this work, we consider the partitional clustering problem, wherein the given time series instances are grouped into pairwise-disjoint clusters. Existing time series clustering methods achieve good performance (Paparrizos and Gravano, 2015; Petitjean et al., 2011; Li et al., 2019), but since they form clusters based on a single focus, such as shape or point-to-point distance, they are sub- optimal for some specific data types. Here, we introduce a novel method named RandomNet for time series clustering using untrained deep neural networks. Differ- ent from conventional training methods that adjust network weights (parameters) using backpropagation, RandomNet utilizes different sets of random parameters to extract various representations of the data. By extracting diverse representations, it can effectively handle time series with different characteristics. These representations are clustered; the results from the clusters are then selected and ensembled to pro- duce the final clustering. This approach ensures that data only needs to pass through the networks once to obtain the final result, obviating the need for backpropagation. Therefore, the time complexity of RandomNet is linear in the number of instances in the dataset, providing a more efficient solution for time series clustering tasks. Given a neural network, the various sets of parameters in the network can be thought of as performing different types of feature extraction on the input data. As a result, these varied parameters can generate diverse data representations. Some of these representations may be relevant to a clustering task, producing meaningful clus- terings, while others may be less useful or entirely irrelevant, leading to less accurate or meaningless clustering. This concept forms the basis of RandomNet: by combining clustering results derived from all these diverse representations, the meaningful and latent group structure within the data can be discovered. This is because the noise introduced by irrelevant representations tends to cancel each other out during the ensemble process whereas the connections provided by relevant representations are strengthened. Therefore, efficient and reliable clustering can be achieved despite the randomness of the network parameters. To demonstrate the effectiveness of RandomNet, we provide theoretical analysis. The analysis shows that RandomNet has the ability to effectively identify the latent group structure in the dataset as long as the ensemble size is large enough. Moreover, the analysis also provides a lower bound for the ensemble size. Notably, this lower bound is independent of the number of instances or the length of the time series in the dataset, given that the data in the dataset are generated from the same mechanism. This provides the ability to use a fixed, large ensemble size to achieve satisfactory"}, {"title": "2 Background and Related Work", "content": "2.1 Definitions and notations Definition 1. A time series $T = [t_1, t_2,...,t_m]$ is an ordered sequence of real-value data points, where m is the length of the time series. Definition 2. Given a set of time series {Ti}i=1n and the number of clusters k, the objective of time series clustering is to assign each time series instance $T_i$ a group label $c_j$, where $j \\in$ {1, ..., k}. n is the number of instances in the dataset. We would like the instances in the same group to be similar to each other and dissimilar to the instances in other groups. 2.2 Related work There has been much work on time series clustering, and we categorize them into four groups: raw-data-based methods, feature-based methods, deep-learning-based methods, and others. Raw-data-based methods. The raw-data-based methods directly apply classic clustering algorithms such as k-means (MacQueen et al., 1967) on raw time series. The standard k-means algorithm adopts Euclidean distance to measure the dissimilarity of the instances and often cannot handle the scale-variance, phase-shifting, distortion, and noise in the time series data. To cope with these challenges, dozens of distance measures for time series data have been proposed. Dynamic Time Warping (DTW) (Berndt and Clifford, 1994) is one of the most pop- ular distance measures that can find the optimal alignment between two sequences. It is used in Dynamic time warping Barycenter Averaging (DBA) (Petitjean et al., 2011) 3 which proposes an iterative procedure to refine the centroid in order to minimize the squared DTW distances from the centroids to other time series instances. Similarly, K- Spectral Centroid (KSC) (Yang and Leskovec, 2011) proposes a distance measure that finds the optimal alignment and scaling for matching two time series. The centroids are computed, based on matrix decomposition, to minimize the distances between the centroids and the instances under this distance measure. Another approach, k-shape (Paparrizos and Gravano, 2015) proposes a shape-based distance measure based on the cross-correlation of two time series. The distance measure shifts the two time series to find the optimal matching. Each centroid is obtained from the optimization of the squared normalized cross-correlation from the centroid to the instances in the cluster. Feature-based methods. Feature-based methods transform the time series into flat, unordered features, and then apply classic clustering algorithms to the transformed data. Zakaria et al. (Zakaria et al., 2012) propose to calculate the distances from a set of short sequences to the time series instances in the dataset and use the distance values as new features for the respective instances. This set of short sequences, called U-shapelets, is found by enumerating all the subsequences in the data to best separate the instances. K-means are then applied to the new features for clustering. In the work by Zhang et al. (Zhang et al., 2016), instead of enumerating the subsequences, the shapelets are learned by optimizing an objective function with gradient descent. A recent work (Lei et al., 2019) proposes Similarity PreservIng Represent Ation Learning (SPIRAL) to sample pairs of time series to calculate their DTW distances and build a partially-observed similarity matrix. The matrix is an approximation for the pair-wise DTW distances matrix in the dataset. The new features are generated by solving a symmetric matrix factorization problem such that the inner product of the new feature matrix can approximate the partially-observed similarity matrix. Deep-learning-based methods. Many methods in this category adopt the autoencoder architecture for clustering. In autoencoder, the low-dimension hidden layer output is used as features for clustering. Among these, Improved Deep Embed- ded Clustering (IDEC) (Guo et al., 2017) improves autoencoder by adding an extra layer to the model. It not only employs a reconstruction loss but also optimizes a clus- tering loss specifically designed to preserve the local structure of the data. This dual loss strategy can capture the global structure and local differences, thereby improving the clustering process to better learn the inherent characteristics of the data. Deep Temporal Clustering (DTC) (Madiraju et al., 2018) specifically addresses time series clustering by using Mean Square Error (MSE) to measure the reconstruc- tion loss, and Kullback-Leibler (KL) divergence to measure clustering loss. Similarly, Deep Temporal Clustering Representation (DTCR) (Ma et al., 2019) adopts MSE for the reconstruction loss, while it uses a k-means objective function to measure the clustering loss. DTCR also employs a fake-sample generation strategy to augment the learning process. Clustering Representation Learning on Incomplete time-series data (CRLI) (Ma et al., 2021) further studies the problem of clustering time series with missing values. It jointly optimizes the imputation and clustering process, aim- ing to impute more discriminative values for clustering and to make the learned representations possess a good clustering property. 4 In the broader neural network literature, there is a class of methods that also use random weights known as the Extreme Learning Machine (ELM) (He et al., 2014; Peng et al., 2016; Wu et al., 2018), which uses a single-layer feed-forward network to map inputs into a new feature space. The hidden layer weights are set randomly but the output weights are trained. The idea is to find a mapping space where instances of different classes can be separated well. In the domain of time series classification, ROCKET (Dempster et al., 2020), MiniRocket (Dempster et al., 2021) and MultiRocket (Tan et al., 2022) adopt strategies involving the use of random weights to generate features for classification. They use multiple single-layer convolution kernels instead of a deep network architecture. Beyond the neural network and clustering fields, several works also adopt random- ized features or feature maps (Rahimi et al., 2007; Chitta et al., 2012; Farahmand et al., 2017). However, it is worth noting that all these methods diverge from our proposed approach in their network structures. Moreover, none of these methods incorporates ensemble learning, which forms the core of our approach. To the best of our knowl- edge, we are the first to propose using a network with random weights in time series clustering. Other methods. In our previous work (Li et al., 2019), we present a Symbolic Pattern Forest (SPF) algorithm for time series clustering, which adopts Symbolic Aggregate approximation (SAX) (Lin et al., 2007) to transform time series sub- sequences into symbolic patterns. Through iterative selections of random symbolic patterns to divide the dataset into two distinct branches based on the presence or absence of the pattern, a symbolic pattern tree is constructed. Repeating this process forms a symbolic pattern forest, the ensemble of which produces the final clustering result."}, {"title": "3 The Proposed Method", "content": "3.1 Architecture and algorithm Figure 1 shows the architecture of RandomNet. The method is structured with B branches, each containing a CNN-LSTM block, designed to capture both spatial and temporal dependencies of time series, followed by k-means clustering. Each CNN- LSTM block contains multiple groups of CNN networks and an LSTM network, and each group of CNN network consists of a one-dimensional convolutional layer, a Recti- fied Linear Units (ReLU) layer, and a pooling layer. The output of the CNN networks is flattened. In our experiments, we set the number of groups of the CNN network equal to log2 m, where m represents the length of the time series. We fix the number of filters of the 1D convolution to 8, the filter size to 3, and the pooling size to 2. We set the number of LSTM units to 8. The weights used within the network are ran- domly chosen from {\u22121,0,1}. We opt for this finite parameter set over a continuous interval (e.g., [-1,1]) for the purpose of simplifying the parameter space. Each branch produces its own clustering, however, some clusterings might be skewed or deviant due to the inherent randomness of the weights. To alleviate this problem, we propose a selection mechanism to remove any clusterings that contain clusters that are either too small or too large. Concretely, the method sets a lower bound Ir and an upper bound ur for the cluster size. The number of instances that violate the bounds in each clustering is counted as violation. For example, suppose a clustering contains two clusters with sizes 40 and 52, respectively. If the lower bound is 5 and the upper bound is 50, then the number of violations for this clustering is 52 - 50 = 2. The clusterings are sorted according to the number of violations and the method selects the top S clusterings for the ensemble. Here, $S = max(z_v, sr \u00d7 B)$, where $z_v$ is the number of clusterings with zero violation values, $s_r$ is a selection rate, and B is the number of branches in the method. Finally, we ensemble the results to form the final clustering. While the diversity of clustering results from a large number of different branches helps reveal various intrinsic patterns in the data, it introduces the challenge of combining these different results into a cohesive unified clustering. To address this challenge, we adopt the Hybrid Bipartite Graph Formulation (HBGF) (Fern and Brodley, 2004) to perform clustering ensemble. This technique builds a bipartite graph for the clusterings in the ensemble, where the instances and clusters become the vertices. If an instance belongs to a cluster, then there is an edge connecting the two respective vertices in the graph. Partitioning the graph gives a consensus clustering for the ensemble. HBGF has two main advantages. First, it can extract consensus from differences, identifying and strengthening the repeated patterns of grouping across the clustering set. Second, it has linear time complexity, which ensures the scalability of our model for large datasets. In our implementation, we use Metis (Karypis and Kumar, 1998) library to partition the graph."}, {"title": "3.2 Effectiveness of RandomNet", "content": "Given the network architecture, its parameters (weights) represent a form of feature extraction from the data and thus produce a kind of representation. With multiple random parameters, we can have multiple representations. Some representations are relevant to the clustering task. The instances that are similar to each other are more likely to be put in the same cluster under these relevant representations. Other representations are irrelevant to the clustering task. Under these representations, two similar instances may not be assigned in the same cluster. The intuition is that, in the ensemble, the effect of irrelevant representations can cancel each other out, and the effect of relevant representations can dominate the ensemble. Inspired by (Li et al., 2019) which is described in the previous section, we provide effectiveness analysis for RandomNet. We assume the data contains k distinct clusters which correspond to k different classes. We have the following theorem:"}, {"title": "4 Experimental Evaluation", "content": "4.1 Experimental setup To evaluate the effectiveness of RandomNet, we run the algorithm on all 128 datasets from the well-known UCR time series archive (Dau et al., 2019). These datasets come from different disciplines with various characteristics. Each dataset in the archive is split into a training set and a testing set. We fuse the two sets and utilize the entire dataset in the experiment. Some of these datasets contain varying-length time series. To ensure that all time series in a dataset have the same length, we append zeros at the end of the shorter series. For benchmarking purposes, we run kDBA (Petitjean et al., 2011), KSC (Yang and Leskovec, 2011), k-shape (Paparrizos and Gravano, 2015), SPIRAL (Lei et al., 2019), and SPF (Li et al., 2019) on the same datasets. These methods are used as representatives of the state-of-the-art for time series clustering. Additionally, we incor- porated deep-learning-based methods, Improved Deep Embedding Clustering (IDEC) (Guo et al., 2017) and DTC (Madiraju et al., 2018), for comparison. While DTC is specifically designed for time series data, as discussed in Section 2.2, IDEC is a general clustering method. We also compare our method with ROCKET (Dempster et al., 2020) and its variants, MiniRocket (Dempster et al., 2021) and MultiRocket (Tan et al., 2022), since we are interested in how other models that also used random parameters compare to ours. As they are all specifically designed for time series clas- sification, we adapt them to our use case by removing the classifier component and replacing it with k-means. All references to them will pertain to this adapted version. We do not include DTCR (Ma et al., 2019) in the comparison, as we are unable to reproduce the results reported in its paper, despite using the code provided by its authors. This issue has been similarly reported by others on the GitHub issue web- page for the project2. We do not include CRLI (Ma et al., 2021) since it is specially designed for incomplete time series data which is outside the scope of our study. Note that due to the complexity involved in training deep learning models, we have not included the time complexity for the two deep learning methods, DTC and IDEC,"}, {"title": "4.2 Hyperparameter analysis", "content": "To fine-tune and investigate the influence of hyperparameters on model performance, we conduct a series of experiments. We select 20 datasets from the UCR time series archive (Dau et al., 2019) and run each experiment 10 times and take the average Rand Index as the result. Number of branches. The number of branches B plays a pivotal role in our model, affecting both the quality of the clustering and the computational efficiency. We test B values ranging from 100 to 1000, in increments of 100, and keep all other settings default. The left plot of Fig. 2 shows that the average Rand Index improves with an increase in the number of branches until B 800. Increasing the number of branches beyond 800 only results in a rise in running time, without contributing to better clustering quality. Therefore, we set the default B for all datasets to 800. Selection rate. The selection rate sr controls the lower bound of the number of selected clustering. We test sr values ranging from 0.1 to 1, in increments of 0.1, and keep all other settings default. The middle plot of Fig. 2 shows slight changes in the average Rand index as sr changes. Since a larger sr will increase the running time of the model, we choose sr = 0.1 as the default value. Lower bound and upper bound. The lower bound Ir and the upper bound ur are crucial in detecting the number of violation, which affects the quality of clustering. We evaluate three pairs of Ir and ur, (0.1,1.8), (0.3, 1.5), and (0.5, 1.2), representing wide intervals, intermediate intervals, and narrow intervals, respectively. For simplicity, we present these as multipliers; the actual lower and upper bounds are obtained by multiplying these values with the average cluster size acs. Narrower intervals are more restrictive to the size of the clustering and thus will increase the number of violations. We keep all other settings as default."}, {"title": "4.3 Experimental results", "content": "Since we use 20 of the 128 datasets to select the hyperparameters, for the sake of fairness, we remove them in the following comparison and only show the results for the remaining 108 datasets. Comparison with k-means. As RandomNet uses k-means to generate clustering assignments, we are interested in how they compare. We also run k-means 800 times and use HBGF (Fern and Brodley, 2004) to ensemble the results, which is denoted as kmeansE. We run the methods under comparison on the 108 datasets and record the Rand Index. RandomNet significantly outperforms both k-means and kmeansE. It is noteworthy that kmeansE is significantly better than the standard k- means, indicating that employing ensemble methods can substantially improve the performance of time series clustering, even for the naive method that uses the original representation of time series. Comparing RandomNet with kmeansE further demon- strates that using the proposed deep neural network with random parameters for generating representations can indeed enhance the accuracy of k-means clustering and ensembles. Comparison with ROCKET and its variants. We compare our method with ROCKET and its variants, MiniRocket and Mul- tiRocket. Note that we remove the classifier components in these ROCKET variants and replace them with k-means to adapt them to our use case. RandomNet outperforms ROCKET variants in terms of average rank, and is especially significantly better than ROCKET and MultiRocket. This reflects the superiority of RandomNet, which is specially designed for time series clustering, in models based on random parameters. It is worth noting that MiniRocket is the best model among ROCKET variants. Therefore, we keep only MiniRocket in subsequent experiments. Comparison with the state-of-the-arts. RandomNet achieves the highest average Rand Index and average rank amongst all the baseline methods. Figure 5 depicts the critical difference diagram of the comparison between Ran- domNet and the state-of-the-art methods. The figure demonstrates that RandomNet significantly outperforms k-means, KSC, kDBA, SPIRAL, and two deep learning- based methods, IDEC and DTC. It also shows our proposed method is slightly better"}, {"title": "4.4 Ablation study", "content": "To verify the effectiveness of each component in RandomNet, we compare the per- formance of full RandomNet and its four variants on 108 UCR datasets, which are shown in Table 6. The four variants are, 1) RandomNet w/ GRU (replaces LSTM with GRU), 2) RandomNet w/o LSTM (removes LSTM), 3) RandomNet w/o LSTM & ReLU (removes LSTM and ReLU), and 4) RandomNet w/o LSTM & ReLU & pooling (removes LSTM, ReLU and pooling). The results show that full RandomNet is better than the four variants in average rand index and average rank, reflecting the effectiveness of each part of RandomNet. It is worth noting that pooling is important in the model. Removing pooling will significantly increase the running time and decrease the performance."}, {"title": "4.5 Visualizing clusters for different methods", "content": "Figure 6 shows the 2D embeddings of the Cylinder-Bell-Funnel (CBF) (Saito and Coifman, 1994) dataset using t-distributed Stochastic Neighbor Embedding (t-SNE) algorithm (Van der Maaten and Hinton, 2008), as well as cluster assignments by k- means, MiniRocket, and RandomNet compared with the true labels. We can see clearly that k-means and MiniRocket both have difficulty distinguishing the blue and green classes, which correspond to the Bell and the Cylinder classes, respectively."}, {"title": "4.6 Testing the time complexity", "content": "In real-world applications, the size of datasets and the length of time series can be huge, making linear time complexity with respect to the number of instances and length of time series an essential characteristic of any practical model. To test the scalability and effectiveness of our proposed method, we use the same mechanism to generate datasets of varying sizes. For different time series lengths, we supplement the original time series (length of 128) with random noise to reach the required length. In this experiment, we use the CBF dataset (Saito and Coifman, 1994). For testing linear complexity w.r.t the number of instances, the number of instances is set from 200 to 10,000 with a fixed time series length of 100. For testing linear complexity w.r.t the"}, {"title": "4.7 Analyzing noise sensitivity", "content": "We use three different datasets, SmallKitchenAppliances, ECG200, and FiftyWords, from three different application domains to test the noise sensitivity of the model. These datasets are injected with six levels of random Gaussian noise (scales of 0.05, 0.1, 0.2, 0.3, 0.4, and 0.5). This setting ensures that most values in the time series are valid, unlike in the previous section, where most values are noise. We evaluate the performance of RandomNet against the second-best model, SPF, by running each model 10 times and calculating the average Rand Index. As illustrated in Fig. 9, while both models exhibit a strong resilience to noise, our model is slightly better than SPF. For the SmallKitchenAppliances dataset, the per- formance of RandomNet has little effect as the noise level increases. On the contrary, the performance of SPF decreases more obviously. In the ECG200 dataset, both mod- els experience small fluctuations in performance at different noise levels, indicating similar effects on noise in this case. For the FiftyWords dataset, both models remain highly stable and show minimal performance differences despite the introduced noise. Overall, these observations highlight RandomNet's competitive ability to handle noise, confirming its effectiveness and robustness in noisy scenarios."}, {"title": "4.8 Finding the optimal number of clusters", "content": "In many real-world data mining scenarios, the true number of clusters (k) within the dataset is unknown, so whether the model has the ability to determine the optimal k is crucial. The Elbow Method is a widely accepted heuristic used in determining the optimal k. It entails plotting the explained variation as a function of k and picking the \"elbow\" of the curve as the optimal k to use. We apply the Elbow Method to the clustering performed by both k-means and RandomNet on the CBF dataset, which contains three classes. RandomNet can find an obvious \"elbow\" at k = 3, whereas for k-means, it is hard to locate a clear \"elbow\"."}, {"title": "5 Conclusion and Future Work", "content": "In this paper, we introduces RandomNet, a novel method for time series clustering that utilizes deep neural networks with random parameters to extract diverse representa- tions of the input time series for clustering. The data only passes through the network once, and no backpropagation is involved. The selection mechanism and ensemble in the proposed method cancel irrelevant representations out and strengthen relevant representations to provide reliable clustering. Extensive evaluations conducted across all 128 UCR datasets demonstrate competitive accuracy compared to state-of-the-art methods, as well as superior efficiency. Future research directions may involve integrat- ing more complex or domain-specific network structures into our method. Additionally, incorporating some level of training into the framework could potentially improve performance. We will also try to explore the potential of applying our method to multivariate time series or other data types, such as image data."}]}