{"title": "On Fairness of Unified Multimodal Large Language Model for Image Generation", "authors": ["Ming Liu", "Hao Chen", "Jindong Wang", "Liwen Wang", "Bhiksha Raj Ramakrishnan", "Wensheng Zhang"], "abstract": "Unified multimodal large language models (U-MLLMs) have demonstrated impressive performance in visual understanding and generation in an end-to-end pipeline. Compared with generation-only models (e.g., Stable Diffusion), U-MLLMs may raise new questions about bias in their outputs, which can be affected by their unified capabilities. This gap is particularly concerning given the under-explored risk of propagating harmful stereotypes. In this paper, we benchmark the latest U-MLLMs and find that most of them exhibit significant demographic biases, such as gender and race bias. To better understand and mitigate this issue, we propose a locate-then-fix strategy, where we audit show how the individual model component is affected by bias. Our analysis shows that bias originates primarily from the language model. More interestingly, we observe a \"partial alignment\" phenomenon in U-MLLMs, where understanding bias appears minimal, but generation bias remains substantial. Thus, we propose a novel balanced preference loss to balance the demographic distribution with synthetic data. Experiments demonstrate that our approach reduces demographic bias while preserving semantic fidelity. We hope our finding underscores the need for more holistic interpretation and debiasing strategies of U-MLLMs in the future.", "sections": [{"title": "1. Introduction", "content": "Multimodal Large language models (MLLMs) have demonstrated remarkable capabilities in visual understanding (Li et al., 2024c; Wang et al., 2024a). Recent research (Wu et al., 2024b;a) has focused on extending MLLMs' capabilities to image generation settings, enabling them to produce textual content and visual content. These unified MLLMs (U-MLLMs), e.g., VILA-U (Wu et al., 2024b), present both visual understanding and generation capability, where they can not only understand the semantics in images but also generate images with high-quality conditioning on user prompts in natural language. However, these U-MLLMs with unified capabilities may inadvertently reproduce or amplify biases at deployment, including gender and racial stereotypes embedded in their large-scale training data (Elazar et al., 2023; Chen et al., 2024).\nA common structure in U-MLLMs is an image tokenizer that transforms images into discrete tokens via an encoder-decoder framework (Esser et al., 2021). Specifically, the vision encoder compresses the input image into latent embeddings and then quantizes them into discrete tokens. Afterward, a decoder reconstructs the image from these tokens. This discrete tokenization bridges textual and visual modalities by analogizing image tokens to words in a sentence, enabling a single autoregressive objective that unifies text and image generation. While this design has proven effective for quality and scalability, it also opens additional channels for bias from the tokenizer.\nExisting work on debiasing in image generation has highlighted the social risks posed by skewed output distributions, and various methods have been proposed to reduce bias in image generation (Chuang et al., 2023; Bansal et al., 2022; Wang et al., 2024c; Gandikota et al., 2024). Nevertheless, many such methods are designed specifically for diffusion models, which leverage different principles for image generation (Shen et al., 2024; Gandikota et al., 2024)."}, {"title": "2. Preliminary", "content": "Structure of U-MLLMs We consider an autoregressive U-MLLM that, given a textual prompt x, first converts it into a sequence of text tokens {x1,...,x_T} and then generates a sequence of image tokens {z_1,..., z_{T_z} }, which an image decoder reconstructs into a final image y (see Figure 5 for the pipeline). Let \\theta = {\\theta_v, \\theta_l} denote the model parameters, where: (1) \\theta_v is the image tokenizer (encoder-decoder) responsible for converting input images into discrete tokens and decoding tokens back into images. (2) \\theta_l is the language model (LM) that processes and generates token sequences (both text and image tokens) in a unified autoregressive way.\nAs shown in Figure 6, the image encoder E_{\\theta_v} maps an image y into latent embeddings e, then quantizes the embeddings into a discrete token sequence {z_1,..., z_{T_z} }. Conversely, the image decoder D_{\\theta_v} inverts this process:\n{\u22481,..., ZT2} = E\u03b8\u2084(y), (1)\ny = Dev (21,..., ZT\u2082). (2)\nMeanwhile, the LM LMe, treats both text tokens and image tokens uniformly under a single next-token probability:\nPo(zt | X, Z<t) = LM\u04e9\u2081(\u2248t\u22121,..., 21; X). (3)\nThis design allows the U-MLLM to perform visual understanding by mapping an image to a semantic space (via Eq. (1)) and then interpreting those discrete tokens as inputs to the LM, and to perform image generation by autoregressively sampling {z} from Eq. (3) and reconstructing an image from the resulting token sequence via Eq. (2).\nDemographic Bias. When conditioning on neutral generation prompt (no explicit gender or race specified), the model could exhibit demographic bias by disproportionately generating output with a distribution skewing towards certain demographic groups (e.g., \u201cmale\u201d or \u201cfemale,\u201d \u201cAsian\u201d or \"Black\"). Figure 1 shows such an example where a prompt \"Please generate images of construction workers\" yield mostly images samples of one demographic group.\nFormally, let d \u2208 D where D = {d1,...,dx} is a set of demographic labels (e.g., {male, female} or {Asian, Black, White, Indian}). Given a neutral prompt x, such as \"a portrait of a construction worker\", an unbiased model would generate a list of images {Y1, Y2,...} with these demographic labels in a balanced distribution, for example, 50-50 for gender, or a uniform distribution for race. By"}, {"title": "3. Locating Bias", "content": "As shown in Figure 7, to identify the biases that might emerge from where, we analyze intermediate outputs - that is, the image tokens (produced by the LM) and the image embeddings (produced by the vision encoder) during the decoding step. We consider two main hypotheses regarding the origin of demographic bias as follows.\n3.1. Hypothesis I: The Bias is from Vision Encoder\nThe vision encoder transforms input images into a high-dimensional embedding space, as illustrated in Figure 6. This encoder itself may be a source of bias within U-MLLMs. To test this hypothesis, we focus on auditing the vision encoder, which is trained symmetrically alongside the decoder. By detecting the embedding output from the encoder, we aim to identify any inherent biases that could influence the overall fairness of the model.\nLinear Probing on Image Embeddings Figure 7 illustrated the overall pipeline to audit vision encoder: first, we sample a balanced set of images denoted as y from FairFace(K\u00e4rkk\u00e4inen & Joo, 2019) covering different demographic attributes (e.g., male, female). By feeding these images into the vision encoder, we obtained a set of image embeddings denoted as E = {e}. Each embedding e\u00a1 is labeled with the ground truth attribute di \u2208 {male, female} according to its corresponding image input yi. We then split E into training and testing subsets and train a linear classifier l(.) to predict demographic labels from embeddings ei. This gives us pairs (yi, di, ez)\u2014i.e. the image, the predicted demographic label, and the corresponding image embedding. The precision, F1 score, recall and precision in the test set are all high, indicating that the encoder's latent space preserves explicit demographic distinctions:\ndi = l(ei) \u2192 High Accuracy.\nSince the decoder is trained to invert the encoder, it should faithfully preserve these attributes. We also find that when we prompt the model with an attribute (e.g., \u201cAsian professor\u201d), the final generated images are overwhelmingly associated with that attribute. This also implies that the decoder consistently reflects whatever demographic semantics are either present in the tokens or passed by the encoder. Therefore, the chance that bias originated from the vision encoder is small.\n3.2. Hypothesis II: The bias is from the language model\nLet Xneutral be a neutral prompt (e.g., \"a photo of a professor\"), and let Xaug (di) denote an augmented version of the same prompt specifying a particular demographic di (e.g., \u201ca photo of a female professor\u201d). Assume that for each x, we sample a list of images {y1,..., ym} and each image Yi is associated with a sequence of image tokens generated from LM as z\u2081 = (Zi,1, Zi,2,...)."}, {"title": "4. Method", "content": "To effectively reduce bias in U-MLLM, a training dataset that explicitly includes diverse demographic attributes is of importance. In general, web-scale data under-represents certain groups or inherently skews toward certain stereotypes. By synthesizing balanced data\u2014where each demographic group appears in similar proportion- we can give the model a stronger signal to reduce its biases during training. This can ensure that the U-MLLM is trained on a controlled distribution counteracts the bias present in real-world data distribution.\n4.1. Training Data Generation\nWe leveraged diffusion model FLUX.1-dev(Labs, 2023) to synthesize our training data as follows:\n1. Base Prompt Selection. We begin by collecting a set of base prompts Xneutral (for example, \"a portrait of a {occupation}\u201d), where occupations are drawn from a publicly available dataset(Shen et al., 2024).\n2. Demographic Augmentation. For each base prompt Xneutral, we augment it with demographic attributes d \u2208 {d1, d2,...}, resulting in augmented prompts Xaug(di). For example, starting with the base prompt \"a photo of a nurse\" we generate augmented prompts such as \u201ca photo of a male nurse\u201d and \"a photo of an Asian nurse.\"\n3. Image Generation via Diffusion Model. For each Xaug (di), we feed it into FLUX.1-dev(Labs, 2023) model to obatin a set of images {Y1,Y2,...}. This step ensures diversity in the visual representations of each demographic group.\n4. Paired Dataset Creation. To create the dataset, we pair each generated set of images yk with its corresponding base prompt Xneutral. Before pairing, we use the ChatGPT-40 model to paraphrase the base prompt to introduce linguistic diversity while maintaining neutrality. For instance, the base prompt \"a photo of a nurse\" can be paraphrased as \"a photo of an individual in the nursing profession.\" The resulting synthetic dataset pairs each neutral prompt with a set of images, where each image corresponds to a specific demographic group:\nDsyn = {(xneutral, Yd\u0131, \u2026\u2026\u2026, Ydk ) }.\nBecause we explicitly injected demographic attributes into the prompts, Dsyn spans multiple genders, ethnicities, and roles-offering more balanced coverage than typical real-world data.\n4.2. Balanced Preference Loss\nExisting preference-optimization approaches focus on aligning the model's response to user-specified preferences (e.g., \"prefer polite over rude text\"). By contrast, in this study we seek balance among demographic attributes for instance, the model should equally generate {female, male} or"}, {"title": "5. Experiment", "content": "5.1. Experimental Setup\nModels. In our study, we evaluate the generation bias with respect to gender and race for latest U-MLLMs that can both understand and generate visual content. The models we considered include VILA-U(Wu et al., 2024b), Token-Flow(Qu et al., 2024), Emu3(Wang et al., 2024b), Janus(Wu et al., 2024a), Show-o(Xie et al., 2024), Janus-Pro(Chen et al., 2025). Since VILA-U was the only model with available training code when we began this study, we fine-tuned it into various variants for comparison. For VILA-U, we compare its variants: (1). Prompt engineering: original VILA-U model with a modified prompt to explicitly ask for diversity in image generation. (2). Understanding Finetuning(I \u2192 T): VILA-U finetuned on the dataset in image-to-text fashion. (3). Generation Finetuning(T \u2192 I): VILA-U finetuned on the same balanced data in text-to-image fashion.(4). Balanced Preference Loss: VILA-U finetuned using our proposed balanced preference optimization for visual generation. (Sec. 4).\nEvaluation Data. For evaluation, we collect a set of occupation-based generation prompts (e.g. \u201cnurse,\u201d \u201cdata engineer,\u201d \u201csenator\u201d) that have been used in previous studies(Shen et al., 2024) to assess diffusion models' biases concerning gender and race. These prompts are 50 in size and are publicly available. We utilized these prompts to evaluate various U-MLLMs under consistent conditions: We generate N = 160 images for each test prompt and use an attribute classifier(Shen et al., 2024) to determine the demographic attribute labels.\nFinetuning Data. For training, we collected a separate set of occupation-based generation prompts(Shen et al., 2024) (e.g., \"scientist,\" \"firefighter\") and generated synthetic images using a diffusion model, as described in Section 4. Our dataset comprises 1,000 training prompts, resulting in"}, {"title": "5.2. Experimental Results", "content": "Table 1 summarizes the results for generation bias evaluation on a range of U-MLLMs and especially the VILA-U model with a few debiasing strategies applied. We measure gender bias, race bias, and their intersection (G.\u00d7R.), together with semantics-preservation metrics. Lower bias scores indicate more fairness, while higher semantics scores indicate more fidelity in the generation."}, {"title": "5.3. Discussion", "content": "Qualitive Samples In Figure 3, we compare the VILA-U model with our method in different debiasing dimensions. Our approach evidently enables the model to generate more"}, {"title": "6. Related Work", "content": "Multimodal Generative Models Unified multimodal large language models (U-MLLMs) have advanced the start-of-the-art by bridging visual understanding and conditional"}, {"title": "7. Conclusion", "content": "Conclusion We examined demographic bias (e.g., gender, race) in unified multimodal large language models (U-MLLMs). We proposed a balanced preference optimization framework that combines supervised finetuning with a preference deviation penalty. Our experiments show that this method effectively reduces bias while maintaining high image quality and semantic fidelity. Moreover, our analysis of understanding versus generation bias underscores the importance of addressing both visual reasoning and token-level"}, {"title": "D. Related Work(continued)", "content": "Preference Optimization Direct preference optimization (DPO) (Rafailov et al., 2024) has emerged as a promising technique to address biases in machine learning models, especially LLM. Since then, numerous new loss functions have been proposed (Meng et al., 2024; Park et al., 2024; Hong et al., 2024; Ethayarajh et al., 2024; Azar et al., 2023). Recent advances (Amini et al., 2024) integrate preference modeling into loss functions to guide models toward balanced outputs, which have rarely been explored for MLLMs. Based on this, we introduce a novel balanced preference loss tailored for U-MLLMs. By leveraging demographic attributes during training, the proposed method balances the likelihood of generating outputs across groups without compromising the image quality."}, {"title": "E. Results for Debiasing TokenFlow", "content": ""}, {"title": "F. Limitation", "content": "Limitation Several questions remain open despite the bias reduction achieved by our approach. First, our study primarily centered on overt demographic categories (e.g., gender, race). Real-world scenarios may demand addressing intersectional or nuanced attributes (e.g., age, culture, or religion). Second, many models are not fully open-source, restricting the scope of our evaluations to two publicly available systems. Future research could broaden the range of tested models. Lastly, due to resource constraints, we did not explore alternative preference optimization objectives beyond our current framework. Building on our method to incorporate other debiasing approaches is a promising direction for future work."}, {"title": "B. Localizing Bias", "content": ""}, {"title": "C. BPO Algorithm", "content": ""}]}