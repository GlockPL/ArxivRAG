{"title": "Stream State-tying for Sign Language Recognition", "authors": ["Jiyong Ma", "Wen Gao", "Chunli Wang"], "abstract": "In this paper, a novel approach to sign language recognition based on state tying in each of data streams is presented. In this framework, it is assumed that hand gesture signal is represented in terms of six synchronous data streams, i.e., the left/right hand position, left/right hand orientation and left/right handshape. This approach offers a very accurate representation of the sign space and keeps the number of parameters reasonably small in favor of a fast decoding. Experiments were carried out for 5177 Chinese signs. The real time isolated recognition rate is 94.8%. For continuous sign recognition, the word correct rate is 91.4%.", "sections": [{"title": "1. Introduction", "content": "Sign language is one of the natural means of exchanging information for the hearing impaired. It is a kind of visual language via hand and arm movements accompanying facial expression and lip motion. The facial expression and lip motion are less important than hand gestures in sign language, but they may help to understand some hand gestures. Digitized devices can be used to measure the temporal and spatial information of hand gestures, the typical devices are data gloves, position trackers. In this paper, we use two CyberGloves and a position tracker, i.e., Pohelmus 3SPACE with two receivers positioned on the wrist of each CyberGlove and one fixed at thorax as input devices to measure gestures.\nChinese sign language is classified into two categories. One is hand gesture in which each gesture corresponds to a Chinese phrase. The other is fingerspelling in which each alphabet corresponds to a posture, and each Chinese sign corresponds to several postures performed continuously. Hand"}, {"title": "2. Basic units in hand gestures", "content": "Compared with human languages such as speech and handwriting, sign language also should have its own basic units. The aim to search the basic units in sign language is to build scalable systems with respect to large vocabulary size. The basic units in speech are phonemes, and the basic units in handwriting are the alphabets or strokes, etc. In contrast, the basic units in sign language are more complex. From the temporal and spatial analysis, for each time instant, hand shape, hand position and hand orientation are three measurable factors forming a hand spatial unit in whole sign space, we call it a spatial unit of a hand gesture. In addition, a hand gesture has a start position that corresponds to a spatial unit, transitional segment, and a stop position that also corresponds to a spatial unit. Therefore, a hand gesture can be viewed as a trajectory in a high dimensional space.\nFrom the analysis above the basic spatial units in hand gestures should include the following six data streams:\nright handshape, right hand position, right hand orientation, left handshape, left hand position and left hand orientation. And they forms a vector denoted by (X1(t),X2(t),X3(t),X4(t),X5(t),X6(t)).Each data stream is also a vector. The six data streams are observable and synchronous. The feature extraction approach for independent singer positions can be found in our previous work [24,27,28]. In Fig.1 (X1,X2,X3) and (X\u20191,X\u20192,X\u20193) denote the coordinates of an object point with respect to the Cartesian coordinate systems of the transmitter and receiver ,respectively. S is the position vector of the receiver with respect to Cartesian coordinate systems of the transmitter."}, {"title": "3. Tying", "content": "The aim of tying is to reduce computation load in decoding process. The tying can be at different levels.\nSharing the same HMM among some different HMMs is named as HMM tying in the whole sign space. This is the first level tying. Clustering different HMMs is more difficult than clustering different state models. The reason is that different HMMs may have different topologies, different transition probabilities and different state observation densities. So, the measure to distinguish different HMMs is difficult to choose.\nSharing the same state models among some different state models is named as state tying in the whole sign space. This is the second level tying. For this problem a measure to cluster the state models can be defined so that some state models are tied. Computation load of state models can be reduced using this approach, and the state tying approach [24] developed in speech recognition community can be used.\nThere are a lot of possible ways to cluster HMM models or state models in speech recognition. These approaches can be directly used for sign language recognition. However, tying at HMM level and state level for sign language recognition is not as effective as that for speech recognition. The reason has been descried in detail in section 2. Therefore, tying in each data stream or in sign subspace is proposed, where the data streams include hand shape, hand position and hand orientation. This is the third level tying. It is named as stream state tying. This level tying is more efficient for calculating state observation probabilities for sign language recognition. Unlike state tying in speech recognition the stream state tying is taken in each data stream of a hand gesture. The basic idea is as the following: first, a whole spatial vector is formed by using all stream vectors. For each hand gesture, its HMM is trained with training samples. After all HMMs have been trained, the observation probability densities in each data stream of all signs are tied with a few probability densities. The advantage of this approach is that the computation time is greatly reduced because the state observation probability density in a whole gesture space is the product of the state observation probability densities of the six streams. And because the state probability in log domain can be computed by summing all stream state probabilities in log domain as the following,\nlog b\u2081 (x) = \u2211log bis(x\u2081) (s=1 to 6)\nb\u2081(x) is the ith state observation probability, b\u2081\u2081(x\u2081) is the s 'th data stream observation probability in the ith state .For each data stream the stream state probabilities are clustered, the stream state probabilities belonging to the same class need to be computed only once. As the number of distinguishable patterns in each data stream is relatively small, for a given observation vector, after these six observation probabilities have been computed, the log likelihood of each sign can be easily gotten by a lookup table and by 5 times addition operations.\nIn addition, the other advantages of this approach are as follows,\nThe data stream of left hand and right hand can independently be used to select sign candidates during recognition, for example, the data stream of right-hand position and shape can be used to select the sign candidates without need of computation of all streams.\nFor the case of continuous sign recognition, even the network topology is linear, because the computation time for the state observation probabilities is relatively small, the probably active sign candidates can be quickly determined."}, {"title": "4. The algorithm for isolated sign recognition", "content": "Each sign has its own start position, and the start positions of different signs may be different. For isolated sign recognition observation data of several frames near the start position can be used to select the sign candidates, computation load can be greatly reduced in this way.\nFor the isolated sign language recognition, the algorithm for isolated sign recognition is proposed as the following:\nThe sign vocabularies are clustered according to parameters in start state of each stream model and its corresponding codebook. Suppose that each stream codebook is VQk (k=1-6), the sign vocabulary set is WordSet. A unique sign subset corresponding to each codeword i in VQk is denoted by SubSet(k,i). For different codewords such as i and j, their intersection set is empty, i.e.\nSubSet(k,i) \u2229 SubSet(k, j) = \u03c6\nand\n\u222aSubSet(k,i)=WordSet (i\u2208VQk)\nLet the conditional probability of the observation vector Ok with respect to the codeword i and stream kbe p(ok,i|k). Its posterior probability can be calculated as the following,\np(i|ok,k) = p(ok,i)/\u2211 p(ok, j) (j\u2208VQk)\nIf the posterior probability is greater than a threshold, then the sign subset SubSet(k,i) is active, otherwise inactive. For each stream observation vector ok if a codeword is active, then its corresponding sign subset is active. Let all active sign subsets in all stream k be WordCD(k)(k=1-6), the active sign subset in all streams will be their intersection set as the following,\nWordCD=\u2229WordCD(k)\n(k)\nSigns in active sign subset WordCD will be selected as candidates for further detailed match, and the result is obtained.\nDifferent stream combination coding schemes can be used to further reduce the computation time of the observation probabilities in whole sign space. The procedure can be formed as a hierarchical structure. For example, for left hand data stream when the intersection set of two different sign subsets in two streams is not empty, i.e. SubSet(k\u2081,i) \u2229SubSet(k2,i2)\u2260\u00a2, then the addition operation for signs in the intersection set only needs once to compute the whole observation densities. Based on the same principle, for the combinations over two streams, the similar approach can be"}, {"title": "5. Continuous Sign Recognition", "content": null}, {"title": "5.1 Movement epenthesis models", "content": "For all possibly reasonable combination of two signs the movement epenthesis models or sign transition models need to be trained, this leads to several sentence level training signs required. In addition, movement epenthesis is not well defined in the sign language books, which also leads to model the movement epenthesis even more difficult. To solve the problem HMMs are used to model the movement epenthesis. The parameters in each HMM of movement epenthesis need to be estimated by sentence level training samples. Because the HMM parameters of signs in a sentence have been estimated by isolated sign training samples, the parameters to be estimated are those of HMMs of movement epenthesis. To estimate these parameters, the HMMs of signs in the sentence and HMMs for movement epenthesis are linked HMM, for example, for a sentence consisting of two signs such as u and v, the whole HMM is shown as Fig.2. The sentence training samples are used to train the parameters in HMM of the movement epenthesis. During training the parameters of signs in the sentence are fixed, only parameters in the sign transition or movement epenthesis HMMs need to be estimated. Note that the transition probability from the last state of u to the first state of transition model CD (vu) is usually set to 1."}, {"title": "5.2 Tying of sign transition models", "content": "Sign transition models have redundancy, and some are impossible combinations between two signs. This kind of sign transition models can be omitted. The possible combinations between u and v are also too large to be managed. Therefore, model tying for sign transition models is necessary. The similar approach to tying sign models can be used to tie sign transition models.\nTo further simplify sign transition models, the following approach is proposed. The assumptions are\n1. The movement epenthesis between two signs is linearly changed over time.\n2. The period of movement epenthesis is relative shorter than that of a sign.\nFrom the first assumption sign transition model can obtained by using linear interpolation, \u03bbvu = (\u03bb\u03b1 + \u03bb\u2081)/2, where \u03bb\u03bb are the last state model of u and the first state model of v. This means that only one state HMM, i.e., GMM, is used to model the sign transition.\nFrom the second assumption, we know that the influence region of the sign transition is much smaller than that of a sign. This assumption is not always true, but for most of sign transitions it is hold."}, {"title": "5.3 Fast matching approach", "content": "Each sign has its own trajectory in sign space, if an observation vector was close to the trajectory, the sign would be probably active at that time, otherwise the sign inactive.\nFor the observation vector of each frame how to judge whether a sign is active becomes very important to speeding up the recognition process. If only a small fraction of signs is active at a frame, the most probably active signs are what are active at the previous frame due to the continuity of gestures. Only these active signs need to be further searched at the next frame, thus a large mount of computation load is reduced.\nThe approach to selecting the active signs at a frame is as the following: let U\u2081 be a basic sign unit not including sign transition unit. If a state belongs to the state set of units Ui, then denote j\u2208U\u2081.\nFor each frame the active score of stream s of the unit is computed as the following,\nPu (t,s) = max log by (s,O\u2081) (j\u2208U;)\nb\u2081(s,O,) is the state observation probability, and Pu (t,s) denotes the active score. This score can be used as an active measure to order the recognition units. A threshold also can be used to select the active units. The active unit will be further searched at the next frame.\nThe following problems may exist in this approach:\nSimilar with other fast match approaches, this approach may cause the pruning errors, because if one unit is incorrectly pruned at some frame, the errors may not be recovered. The algorithm needs computing all state probabilities of all basic sign units. The computation load is relatively large for a fast match approach.\nThe approach to selecting active sign transition units is as the following: to reduce the computation time in the sign transition units, Rich Get Richer strategy is used [26]. For sign transition unit vu, let pu be the last state probability of the unit u, let pv be the first state probability of the unit v, the look-ahead score of sign transition unit vu is approximated by pv|u=(logpu+logv)/2+lookahead (v), where the lookahead(v) score is the average matching score of the observation vectors at the following frames t+1,t+2,t+3 to the first state of unit v. The look ahead score can be used as an active measure and to order sign transition units. The active sign transition unit is further extended."}, {"title": "6. The search algorithm", "content": "A recursive transition network composed of the states of HMMs is used to model sentence level sign. Viterbi algorithm is employed for decoding. In Viterbi beam searches only the hypothesis whose likelihood falls within a beam of the most likely hypothesis is considered for further growth. The best beam size is determined empirically. By expanding the network to include an explicit arc from the end of each sign to the start of the next, the bigram language model has been incorporated to improve recognition accuracy in our system. To speed up the decoding process, the fast match approach discussed in previous section and the following pruning approach are used.\nTo conserve the computing and memory resources, it is imperative to prune the low-scoring partial paths. Two different beams are used, one is at state level, and the other is at sign level. The beam width at each level is determined empirically, and the beam threshold is computed with respect to the best path scoring at that level. For sign-to-sign transitions, if the cumulative score for the partial hypothesis in the last state of a sign exceeds the beam, no transitions are computed from this sign."}, {"title": "7. Experiment", "content": "The baud rate for both CyberGlove and 3-D tracker is set 38400. The number of states in HMM of each sign is 3 0r 5, which is determined by an adaptive approach. The raw gesture data, which in our case are values of 18-joint angles collected from the Cyberglove for each hand, the range of each angle value is within 0-255. For two hands, they are formed as a 48-dimensional vector appended with hand and position and orientation features. The dynamic range of each component is different. Each component value is normalized to ensure its dynamic range is 0-1 [24].\nThe HMM structure for each sign is left to right without skip. The hardware environment is Pentium III 450Hz."}, {"title": "7.1 Isolated sign recognition evaluation", "content": "For the case of isolated sign recognition 5177 signs in Chinese sign language were used as evaluation vocabularies. Each sign was performed 5 times by a sign language teacher. 4 times were used for training and one for test. Using the approach of cross validation test, the test times for each sign is 5. For different numbers of different patterns in each stream, the off-line recognition rates are listed in the Table 1. Where Lp, Lo, Ls, Rp, Ro, Rs, are the number of different patterns in data stream of left/right hand position, left/right hand orientation, left/right hand shape, respectively. The number of states in HMM is 3. When The number of states in HMM is 5, the results are listed in the Table 2. This shows that 3 states are better than the 5 states considering the computation load and recognition accuracy. Therefore 3 states for each HMM are used in the continuous recognition."}, {"title": "7.2 Continuous sign recognition evaluation", "content": "For the case of continuous recognition, the database of gestures consists of 5177 signs and 500 sentences. In general, each sentence consists of 2 to 15 signs. No intentional pauses were placed between signs within a sentence. Sign transition model discussed in section 5.2 was used to model the movement epenthesis.\nTo test the recognition performance at sentence level, one test was carried out described as the following. When 500 sentences were not used for any portion of the training. The 5177 words are used as basic units. Within 500 sentences, 264 sentences can be correctly recognized, the left 236 sentences have deletion (D), insertion (I), and substitution (S) errors, D=186, I=302, S=332, N=5162, N denotes the total number of signs in the test set, the word correct rate is 84.1%.\nThis shows that the movement epenthesis has affected on recognition performance at sentence level. To consider this effect, the sign transition HMMs were trained by the sentence level training samples. For the left 264 sentences, the training procedure for sign transition models was used for each of sentence. In the collected sentence samples, four in five are used for training and one for test. The word correct rate is 91.26%, where D=98, I=182, S=171, N=5162. The accuracy measure is calculated by subtracting the number of deletions, insertion, substitution, and errors from the total number of signs and divided by the total number of signs. The result shows that sign transition models are necessary for sentence level recognition. The recognition speed is about 2 times of real-time. When the number of sign transition models is set to 3 for all sign transition model, the word correct rate is 91.4%, where D=97, I=180, S=167, N=5162. The recognition rates are summarized in the Table 3."}, {"title": "8. Conclusion", "content": "We have presented a framework for large vocabulary sign recognition by stream state tying. For isolated sign recognition, we propose an algorithm for large vocabulary isolated sign recognition, the algorithm is 5-85 times faster than the approach without tying. As for the continuous sign recognition we propose a simplified sign transition model and a fast-matching approach to speeding up the decoding speed. Experiments have shown that these techniques are powerful for sign language recognition considering the problem of scalability."}]}