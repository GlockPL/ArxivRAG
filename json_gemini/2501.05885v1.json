{"title": "EDNet: Edge-Optimized Small Target Detection in UAV Imagery - Faster Context Attention, Better Feature Fusion, and Hardware Acceleration", "authors": ["Zhifan Song", "Yuan Zhang", "Abd Al Rahman M. Abu Ebayyeh"], "abstract": "Detecting small targets in drone imagery is challenging due to low resolution, complex backgrounds, and dynamic scenes. We propose EDNet, a novel edge-target detection framework built on an enhanced YOLOv10 architecture, optimized for real-time applications without post-processing. EDNet incorporates an XSmall detection head and a Cross Concat strategy to improve feature fusion and multi-scale context awareness for detecting tiny targets in diverse environments. Our unique C2f-FCA block employs Faster Context Attention to enhance feature extraction while reducing computational complexity. The WIoU loss function is employed for improved bounding box regression. With seven model sizes ranging from Tiny to XL, EDNet accommodates various deployment environments, enabling local real-time inference and ensuring data privacy. Notably, EDNet achieves up to a 5.6% gain in mAP@50 with significantly fewer parameters. On an iPhone 12, EDNet variants operate at speeds ranging from 16 to 55 FPS, providing a scalable and efficient solution for edge-based object detection in challenging drone imagery. The source code and pre-trained models are available at: https://github.com/zsniko/EDNet.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid advancement of commercial drones or unmanned aerial vehicles (UAVs) has brought transformative impacts across various sectors, including agriculture, aerial photography, shipping, security, and search and rescue [1]. This growth has heightened the demand for accurate and efficient automated object detection. Moreover, the integration of UAVs equipped with advanced sensing technologies offers significant potential for real-time monitoring of social interactions and transportation dynamics, thereby enhancing social intelligence and urban management [2].\nRecent advancements in deep learning, especially with the YOLO (You Only Look Once) [3] family of one-shot object detectors, have significantly enhanced detection capabilities. Traditional two-stage detectors like Faster R-CNN [4] excel in accuracy but fall short in speed. Transformers [5] offer high accuracy but are more computationally intensive and less suitable for edge-device deployment. Previous YOLO versions relied on the Non-Maximum Suppression (NMS) post-processing technique for removing redundant bounding boxes. YOLOv10 [6], the latest iteration of the YOLO series, eliminates post-processing by integrating dual label assignments and removing NMS, making it well-suited for real-time applications. Despite these advancements, detecting small objects in UAV imagery remains challenging due to resolution constraints and complex backgrounds, with objects of interest often comprise less than 10% of the total pixel count [7], compared to over 40% in standard object detection datasets like MS COCO.\nRecent advancements in UAV target detection using YOLO models have improved performance but present challenges for edge deployment. LV-YOLOv5 [8] integrates a vision transformer [9] to enhance detection, but its higher parameter count limits edge applicability. Slicing-aided techniques with YOLOv5 [10] and detection transformers [11] achieve high accuracy, but their heavy computational requirements make them unsuitable for edge computing. Various YOLOv5 improvements [12] [13] have enhanced feature extraction, dynamic heads, data augmentation, and attention mechanisms, but did not address parameter efficiency. YOLOv8 models show gains through modifications to the C2f module and loss functions [14] [15], achieving a mean Average Precision (mAP) of 41.3% but with increased complexity.\nWhile some research has shifted toward lightweight models for UAV vision [16], few have addressed the critical issue of edge deployment. VAMYOLOX [17], a more efficient framework also available in seven sizes, achieves 47.7%"}, {"title": "II. METHODOLOGY", "content": "The complete architecture of our proposed model (ED-Net), is illustrated in Fig. 2. The ConvBNSiLU (Conv2d, BatchNorm, SiLU) block, incorporating a 2D convolution, batch normalization, and Sigmoid Linear Unit (SiLU) activation [19], is a staple from YOLOv5 [20] and carries over to YOLOv8 [21] and YOLOv10 [6]. The Spatial-Channel Decoupled Downsampling (SCDown) block was first proposed in YOLOv10 and enhances efficiency by first using 1 \u00d7 1 convolution to adjust channel dimensions and then applying depthwise convolution for spatial downsampling, thus minimizing computational load while preserving crucial information. Attention mechanism is often used in object detection for better performance [22], hence Partial Self-Attention (PSA) [6] is proposed in the backbone, as a more computationally efficient alternative to traditional multi-head self-attention [23], as depicted on the right side of Fig. 2. Additionally, the Spatial Pyramid Pooling-Fast (SPPF) [24] layer, illustrated at the bottom of Fig. 2, leverages three concatenated max-pooling operations to extract features at multiple scales. Subsequent sections will detail the specific"}, {"title": "A. Backbone Improvement", "content": "We propose a novel and customized block, termed C2f-FCA, integrating an advanced Faster Context Attention (FCA) bottleneck. Our design starts with the FCA block, a sophisticated evolution of the FasterNet [25] architecture, which originally comprises a partial convolution followed by two 1 x 1 convolutions and a residual connection. This tailored configuration significantly enhances processing speed and efficiency. The Context Anchor Attention (CAA) is a sub-element derived from the Poly Kernel Inception Network [26]. Initially crafted for remote sensing applications, CAA is adeptly adapted here to improve feature extraction for drone-based target detection. SiLU [19] activation is also utilized to deliver smoother gradient transitions, promoting faster convergence and improving overall model performance. The FCA bottleneck not only improves performance but also reduces parameter count, making it more efficient.\nAs illustrated in Fig. 3, the FCA block incorporates 3 \u00d7 3 spatial mixing to blend spatial information from selected input channels, with drastically reduced computational complexity compared to traditional convolution. Additionally, it features a feed-forward network with two pointwise (1 \u00d7 1) convolution (PWConv) layers and the CAA. This combination captures long-range contextual relationships among distant pixels, enhancing feature representation, especially in complex scenes with multiple objects of the same category. To achieve this, we first apply average pooling and a pointwise convolution to extract local features. Following this, we use two depth-wise strip convolutions to approximate the effect of large-kernel depth-wise convolutions in a computationally efficient manner due to its lightweight property and reduces the parameter by two for 2D traditional depth-wise convolution with size ks. The attention mechanism employs a weighting matrix $A_{l-1} \\in \\mathbb{R}^{C_l \\times H_l \\times W_l}$ to prioritize channel importance. The operations are summarized in Fig. 3 and are captured by the following equations:\n$F_{pool}^{l-1}$ = Conv$_{1x1}(P_{avg}(X_{l-1}))$ (1)\n$F_1^{l-1}$ = DWConv$_{1\\times kb}(F_{pool}^{l-1})$, (2)\n$F_2^{l-1}$ = DWConv$_{k\\times 1}(F_1^{l-1})$. (3)\n$A_{l-1}$ = Sigmoid(Conv$_{1\\times 1}(F_2^{l-1})$)\nThe CAA applies attention weighting to the channels based on their importance before Adding the original input back to the processed features, forming a skip connection that aids in training stability and performance."}, {"title": "B. Neck Improvement", "content": "1) XSmall Detection Head: Drone cameras often capture vast scenes with tiny objects, presenting a challenge for effective target detection. The original YOLO downsamples feature maps through stages P1 to P5. For our input image size of 640, the resulting feature maps are 80 (P3), 40 (P4), and 20 (P5) pixels in resolution by the time they reach the detection heads. To improve the detection of small targets, we introduced an XSmall detection head, which features a resolution of 160 x 160 pixels. This head significantly reduces the down-sampling to just two stages, enabling it to retain more detailed and richer features of small targets. It is concatenated with features of the same scale from the backbone, as depicted in Fig. 2. Adding the XSmall head to the neck of improves feature fusion by integrating finer-grained, high-resolution features into the detection process. In the subsequent section, we will demonstrate how this XSmall detection head enhances detection precision for tiny objects.\n2) Cross Concat - A Novel Feature Fusion Strategy: We introduce a new concatenation scheme, Cross Concat Strategy (CCS), as depicted in Fig. 2 to improve feature fusion in the detection process. Unlike YOLOv10, which forwards the PSA output to the first upsampling block and again to the last stage before the large detection head, our approach uses the SPPF output for cross-concatenation, while keeping the PSA output connected to the first upsampling block. The SPPF block pools feature maps at different scales, capturing rich multi-scale contextual information crucial for detecting objects of varying sizes in drone imagery. This adjustment may help mitigate the potential loss of broader context when using attention mechanisms late in the process, providing the final detection layers with a more comprehensive understanding of the scene. The performance gains observed, as discussed in the results section, suggest that Cross Concat could be a more effective strategy for target detection in complex aerial environments."}, {"title": "C. Loss Function Improvement", "content": "Drones often operate in diverse flying conditions, which can vary due to changes in altitude and interference noise. To address these challenges and enhance accuracy, we have opted to replace the conventional box loss function with the WIoUv3 loss [27]. This choice enables more effective handling of noise by dynamically adjusting the focus across different samples, thus mitigating the impact of outliers.\n$L_{WIOU} = R_{WIOU} L_{IOU}$ (4)\n$R_{WIOU} = exp\\Big( \\frac{(x - x_{gt})^2 + (y - y_{gt})^2}{W^2 + H^2} \\Big)$ (5)\n$L_{WIoUv3} = rL_{WIOU}$ (6)"}, {"title": "III. EXPERIMENTAL RESULTS", "content": "A. Dataset\nWe utilized the VisDrone [1] dataset, a well-established and challenging benchmark for UAV-based object detection. The dataset comprises 6,471 training images and 548 validation images, encompassing 10 target categories: pedestrian, people, bicycle, car, van, tricycle, awning-tricycle, bus, and motorbike.\nB. Model Training\n1) Environment Setup: The details of the hardware and software configurations are outlined in Table I. Stochastic Gradient Descent (SGD) is employed as the optimizer, with a learning rate set to 0.01 and a momentum of 0.9. All models are trained for 200 epochs to ensure full convergence, and the best-performing model is selected during the training process.\n2) Evaluation Metrics: In object detection tasks, we utilize standard performance metrics: precision, recall, and mean Average Precision (mAP).\nPrecision (P) = $\\frac{TP}{TP + FP}$ (8)\nRecall (R) = $\\frac{TP}{TP + FN}$ (9)"}, {"title": "C. Model Comparison and Discussion", "content": "We evaluated the performance of EDNet against SOTA models, especially with variants also existing in YOLOv10: N, S, M, B, L, and X. To further optimize for edge deployment, we introduce a new tiny variant by reducing the depth scale factor from 0.33 to 0.2 and capping the maximum number of channels from 1024 to 512. This results in a model with just 1.78M parameters, significantly fewer than any YOLO variant, making it uniquely suited for deployment on resource-constrained devices, such as Raspberry Pi, without notably compromising detection performance. While small and medium variants could operate efficiently on modestly powered edge CPUs, larger variants may require more robust embedded GPUs, such as NVIDIA Jetson devices. This"}, {"title": "D. Ablation Experiment", "content": "We conducted the ablation study, each incremental addition demonstrates significant performance enhancements, affirming the effectiveness of our architectural choices. We pick EDNet-b to illustrate the ablation results in Table III, due to previously claimed reasons. The introduction of"}, {"title": "E. Model Deployment", "content": "We investigate the deployability of EDNet-Tiny across various environments within a generalized deployment setup, with an emphasis on ARM-based CPUs, which are prevalent in mobile devices. The model demonstrates a remarkable 3.2-fold increase in speed on edge processors, achieving latencies of 53.4 ms and 59.1 ms per image on the ARMv8 Avalanche and Firestorm performance cores, respectively, compared to its raw implementation with PyTorch on the same processors. Notably, on an ARMv8 Cortex-A76 processor within a Raspberry Pi 5, an exemplar of a resource-constrained embedded device, EDNet-Tiny processes images 3.1 times faster, with latencies of 129 ms versus 402 ms for the raw PyTorch model. Additionally, the 8GB version outperforms the 4GB version slightly, underscoring the importance of memory capacity in optimizing performance. Results illustrating latency improvements before and after optimization are presented in Fig. 7, highlighting the speed enhancements achieved through this process.\nWe further evaluate the inference performance of EDNet in the context of specialized mobile computing, as mobile phones can be conveniently integrated into drones for image capture. Drones can also be fully operated using onboard smartphones, which serve dual functions for vision and control [29]. The pre-trained EDNet models are implemented via the CoreML framework, facilitating accelerated AI inference by interfacing the CPU and the neural engine. All seven model sizes, ranging from Tiny to XL, are tested on the A14 System On Chip (SoC) of an iPhone 12 (released in 2020). Additionally, we assess the performance on the A16 SoC (2022) in an iPhone 15, the M1 SoC (2020) in a Mac and the M2 SoC (2022) in an iPad, which possess greater computational power that could approximate the capabilities of more recently released mobile devices. Notably, the performance curve for the iPhone 15 (red) is"}, {"title": "IV. CONCLUSION", "content": "In this paper, we presented EDNet, a novel framework optimized for small target detection in UAV imagery. ED-Net introduces key architectural innovations, including the uniquely designed C2f-FCA block for enhanced feature extraction with reduced computational complexity, and the XSmall detection head combined with the Cross Concat strategy for improved feature fusion, leading to gains in both accuracy and inference speed. The incorporation of WIoU also enhances bounding box regression.\nWith seven variants ranging from tiny to XL, EDNet consistently outperforms all YOLO iterations, achieving up"}]}