{"title": "SOK: VERIFIABLE CROSS-SILO FL", "authors": ["Aleksei Korneev", "Jan Ramon"], "abstract": "Federated Learning (FL) is a widespread approach that allows training machine learning (ML) models with data distributed across multiple devices. In cross-silo FL, which often appears in domains like healthcare or finance, the number of participants is moderate, and each party typically represents a well-known organization. For instance, in medicine data owners are often hospitals or data hubs which are well-established entities. However, malicious parties may still attempt to disturb the training procedure in order to obtain certain benefits, for example, a biased result or a reduction in computational load. While one can easily detect a malicious agent when data used for training is public, the problem becomes much more acute when it is necessary to maintain the privacy of the training dataset. To address this issue, there is recently growing interest in developing verifiable protocols, where one can check that parties do not deviate from the training procedure and perform computations correctly. In this paper, we present a systematization of knowledge on verifiable cross-silo FL. We analyze various protocols, fit them in a taxonomy, and compare their efficiency and threat models. We also analyze Zero-Knowledge Proof (ZKP) schemes and discuss how their overall cost in a FL context can be minimized. Lastly, we identify research gaps and discuss potential directions for future scientific work.", "sections": [{"title": "1 Introduction", "content": "Nowadays, the broad propagation of ML technologies is rapidly increasing. ML applications affect a variety of fields such as medicine, finance, marketing, education, and many others [19, 46, 27, 48]. Many ML approaches rely on a process of training on historical data: a model learns statistical patterns that later allow new predictions to be inferred. However, in some cases, data may contain private or confidential information; therefore, access to such data is limited, and applying ML must be done with extreme caution, either due to an interest in privacy of the data owners (DOs), e.g., individual persons caring about their privacy or companies caring about intellectual property, or for regulatory compliance, e.g., with the General Data Protection Regulations (GDPR).\nIn FL, multiple DOs, who are also sometimes referred to as clients, can train a model together, possibly under coordination of a central server, by exchanging encrypted messages without revealing their private data. As a result, researchers can benefit from a large amount of shared data and at the same time preserve privacy. However, while preserving privacy in FL allows protecting sensitive information, at the same time it produces an additional challenge in the verification of participants' behavior. Indeed, due to a possibility of malicious actions, it is important to ensure that all calculations are performed correctly even if the used data is private.\nFL is often divided into two categories: cross-device and cross-silo. In the cross-device FL setting, data comes from a large number of small and usually anonymous devices with low computational capacities. Anonymity complicates penalizing clients; a single client is free to abort or to violate the procedure at any time. In contrast, in this paper we focus on cross-silo FL where the number of parties is moderate; each party is usually a well-known and large entity"}, {"title": "2 Background", "content": ""}, {"title": "2.1 FL process", "content": "The majority of FL approaches contain two main categories of operations: local operations (both at the side of the clients and the server) and aggregation of clients' values. Additionally, there are also other operations specific to certain FL protocols, for example, where participants should draw random numbers, exchange cryptographic keys or select a subset of parties to communicate with.\nIn this paper, we consider both settings where the aggregation is coordinated or performed by a central server and settings where DOs perform the aggregation in a decentralized way.\nSome ML algorithms involve running an optimization algorithm, e.g., stochastic gradient descent (SGD). We refer to each iteration of such algorithm as an epoch."}, {"title": "2.2 Cross-silo FL properties", "content": "While analyzing the suitability of various algorithms for the cross-silo FL setting, we assume that:\n1. the number of participants is moderate (at most several thousands);\n2. all participants have an incentive to care about their reputation, they may only cheat in a way which can not be detected by others;\n3. all participants agree on the model to be trained (type of calculations to be executed);\n4. DOs possess computationally sufficiently powerful equipment;"}, {"title": "2.3 Adversarial attacks on FL", "content": "In a survey on FL threats by Rodr\u00edguez-Barroso et al. [49] two classes of adversarial attacks were distinguished:\n\u2022 privacy attacks, whose purpose is to infer sensitive information from the learning process;\n\u2022 attacks which aim at modifying the behaviour or output of the FL process.\nThe development of a FL protocol that is resistant to adversarial attacks requires applying a combination of various privacy enhancing technologies (PETs). For example, in order to prevent privacy attacks, authors of state-of-the-art solutions employ differential privacy (DP), multi-party computation (MPC), secure shuffling and Trusted-Execution Environment (TEE) among others.\nIn this paper, we study verification techniques that allow mitigating the second class of attacks, attacks on the federated learning process, such as data or model poisoning, when an adversary intentionally uses incorrect data or performs computations incorrectly to bias the resulting model. Authors of the considered papers applied commitment schemes, homomorphic hash functions, ZKP schemes and other methods to ensure that the federated model is computed correctly. A detailed analysis of these methods is presented in sections 3 and 4."}, {"title": "2.4 Verifiable FL", "content": "In the scope of this paper, we rely on a definition of Verifiable FL inspired by [60]:\nDefinition (Verifiable FL). FL is verifiable if selected parties are able to verify that the tasks of all participants are correctly performed without deviation.\nFollowing this definition, in contrast to the survey [53], we only include approaches that at least partly verify computa-\ntions of the FL process. For example, we do not analyze protocols which are focused only on verification of identity, ownership, or data provenance. We also exclude protocols considered in [44] that aim to prevent model poisoning attacks by analyzing distribution of values submitted by parties. Such methods efficiently mitigate some attacks, but do not allow to verify the correctness of individual computations or of the individual uses of the input data, e.g., an individual outlier input value is infrequent but possibly valid. Moreover, their efficiency depends on the domain and an attacker strength. On the other hand, we do include in our analysis several protocols devoted to verifiable federated private averaging and verifiable cross-device FL since the same verification techniques could be used in the cross-silo FL setting."}, {"title": "2.5 Threat models", "content": "In the scope of the considered works, authors usually rely on two widely-spread types of threat models: honest-but- curious (a.k.a. semi-honest) and malicious. According to the standard cryptography definitions, an honest-but-curious agent does not deviate from the protocol, but keeps a record of the protocol transcript and analyzes it to gain information about other users, while a malicious adversary can deviate from the prescribed protocol instructions and follow an arbitrary strategy to obtain greater benefits. However, in the context of FL, authors often adapt these definitions with additional properties. In order to thoroughly analyze miscellaneous flavors of the applied threat models we distinguish the following four categories:\n\u2022 honest (or trusted): always follows the protocol correctly and is trusted with sensitive information;\n\u2022 honest-but-curious: always follows the protocol correctly, but is not trusted with sensitive information;\n\u2022 forger: may try to forge different data, but otherwise follows the protocol, is not trusted with sensitive information;\n\u2022 malicious: can arbitrary deviate from the protocol and is not trusted with sensitive information.\nIn the scope of this paper, in order to describe different approaches in a rigorous manner, we specify the robustness of protocols to participant drop-outs, i.e. agents who register to participate but subsequently abandon the protocol, separately from the aforementioned categories of threat models. For example, to report that a protocol is robust against forging and drop-outs, we denote its threat model as \"forger + drop\"."}, {"title": "2.6 Blockchain technology", "content": "A blockchain is a sequence of blocks, which holds a complete list of transaction records like a conventional public ledger [55]. A transaction could be any action taking place on a blockchain network, for example, a transfer of digital"}, {"title": "2.7 Commitment scheme", "content": "A commitment scheme (CS) is a cryptographic primitive that allows parties to commit to values while keeping them hidden from others [6]. A party cannot modify the value after committing to it, but can later reveal it. CSs have two properties: hiding, meaning that a commitment reveals nothing about the original value, and binding, meaning that a party cannot compute the same commitment from different values, typically due to a computationally hard underlying problem. Lastly, some CSs are also homomorphic, meaning that there are two binary operations + and \u00b7 defined in the domain of original values and the domain of their commitments respectively, such that the following condition holds: $C(a + b) = C(a) \\cdot C(b)$, where a, b are values possessed by a party (or parties) and the commitment is represented by C."}, {"title": "2.8 Zero knowledge proof", "content": "A Zero-Knowledge Proof (ZKP) is a cryptographic method by which a party called the prover convinces another party, the verifier, about a statement over committed values [21]. A ZKP of a statement should satisfy three properties:\n\u2022 completeness: if the statement is true, then an honest verifier will be convinced by an honest prover;\n\u2022 soundness: if the statement is false, an honest verifier will be convinced with at most a negligible probability;\n\u2022 zero-knowledge: if the statement is true, a verifier is not capable of learning anything but the proven statement itself.\nSome ZKP can be turned into non-interactive proofs by replacing the verifier with a random oracle using the Fiat-Shamir heurisitic [15]. In the class of non-interactive ZKP, Zero Knowledge Succinct Non-interactive Arguments of Knowledge (zk-SNARKs) are of a particular interest due to the compact proofs relative to the size of the statement and fast verification [45].\nIn FL, ZKPs can be used to prove that a party correctly performed prescribed computations and, therefore, did not deviate from the FL protocol."}, {"title": "3 Verifiable cross-silo FL protocols analysis", "content": "In this section, we present a taxonomy of existing verifiable cross-silo FL protocols, analyze the efficiency of verification techniques and threat models, and discuss the impact of the cross-silo setting on verification. In the scope of this section, we refer to the number of clients as C and to the number of the aggregated vector dimensions as D.\nIn order to ensure that a FL protocol is executed correctly, one has to verify both the local computations of the clients, the aggregation, and the local computations of the server(s). We distinguish four categories of different verification techniques and describe each of them in details below. The full taxonomy is presented in Figure 1. Although each approach has specific characteristics, our categories allow observing general design patterns and infer conclusions about their efficiency. For this purpose, we assess computational and communication costs both per client and per server for each method. The comparison of asymptotic complexities of protocols and applied threat models is presented in Table 1 for approaches focused on verifiable aggregation and in Table 2 for approaches focused on verification of local computations. We emphasize that complexity metrics are calculated specifically for the verification overhead and do not reflect computation and communication which is needed even if no verification is performed. Lastly, we assume that public key infrastructure, ML model weights and seeds of PRGs are initialized before the training procedure and do not require a presence of a trusted party.\nIn order to fairly compare threat models of different approaches, we mapped the threat models described in the considered papers to definitions from Subsection 2.5 and assigned a suitable model ourselves in cases where authors did not explicitly describe it. In such cases, in tables 1 and 2, we use square brackets to denote assigned threat models and collusion markers. Additionally, some articles provided descriptions of multiple threat models, for example, separate"}, {"title": "3.1 Taxonomy description", "content": "Redundant aggregation (RA) based verification. This category consists of approaches that require the server to aggregate some redundant values in order to prove that the aggregation of clients' values is performed correctly. This feature leads to a computational cost of the server to be at least $O(C)$. Moreover, some protocols [54, 10, 25] are designed under the assumption that each party has one secret value, therefore a naive scaling of the approach to a FL setting where parties share multi dimensional data would lead to an additional factor D in the complexity."}, {"title": "Homomorphic property (HP) based verification", "content": "This category covers verification techniques which rely on the HP of underlying primitives: hash functions [24, 28] and commitment schemes [39, 14]. The general idea of such protocols is the following: clients compute hashes/commitments from their data and share them with each other, then all clients may verify the result of aggregation by checking that this result corresponds to the aggregation of hashes/commitments through homomorphism. As a consequence, both computational and communication costs of the server are O(1) if the ciphertext length does not depend on C or D. In order to verify the aggregation of clients' values, clients have to compute a hash/commitment in O(D) from their data and to aggregate hashes/commitments from other clients in O(C). In blockchain-based protocols clients upload hashes/commitments with a constant communication cost (to be multiplied with costs related to the blockchain infrastructure, see Section 3.2), while in other protocols, where clients have to exchange messages with each other, the communication cost per client is at least O(C). Exceptionally, in FedTrust [28] client costs have additional complexity factor D, as far as the hash is calculated for each component separately.\nIn VeriFL [24], apart from the main protocol, authors also demonstrated a way to optimize the verification. Leveraging the homomorphic property of the hash scheme and the repetitive nature of FL calculations, authors proposed an amortized verification which allows decreasing the number of hash function calls. Instead of checking that the product of hashes obtained from all clients is equal to the hash of the aggregated value obtained from the server after each epoch, one can draw a set of random coefficients to compute a linear combination of hashes obtained from all clients across multiple epochs and compare it with a hash of a linear combination of aggregated values obtained from the server using the same coefficients. As a result, in Table 1, the client computational cost of VeriFL differs from competing protocols by additional factor . Notable, since the core idea of all approaches of HP-based category is very similar, we observe that this optimization could be also applied to any of them.\nThere are also two protocols that lie at the intersection of the RA- and HP-based categories: VerifyNet [58] and SVFL [41]. In VerifyNet, clients share five additional values along with their gradients, which are later aggregated by the server. Aggregated values are shared back with clients so that they can verify the correctness of the gradients aggregation relying on the properties of the homomorphic hash function. The second approach, SVFL, is based on a homomorphic signature scheme. During the training procedure, the server performs a redundant aggregation of signatures and each client runs the verification algorithm based on the HP. Authors considered a malicious threat model for the server, however one should be careful while using signature-based verification techniques due to the concerns described above for similar approaches of the RA-based verification category [18, 59]."}, {"title": "ZKP based verification", "content": "The third category contains approaches which are based on ZKPs. The core principle could be described as follows: a party performs calculations and at the same time computes the proof, which is shared along with the result of calculations; other parties can later run the proof verification algorithm to ensure that the result was computed correctly. In contrast to previous categories, advanced ZKPs allow proving arbitrary computations, therefore such methods are suitable for proving the correctness of both aggregation of clients' values [56, 1, 51] and local computations [32, 26, 50]. Moreover, recent ZKP schemes provide a proof size that is sublinear in the amount of computations to prove.\nIn protocols focused on aggregation, authors build on different infrastructures and ZKP schemes. In GOPA [51], authors introduced a decentralized gossip approach where nodes publish proofs of their computations using E-protocols. In zkFL [56] authors apply more modern ZKP scheme, Halo2, and provide two versions of the protocol \u2013 with a centralized FL setting and a blockchain based one. In zkDFL [1] authors rely on the blockchain infrastructure and the Groth16 scheme. We also observed that authors of zkDFL and zkFL use different techniques to prove that each client value was indeed sent by one of the clients. In zkFL, all clients sign commitments to their local models and the server includes the signature verification in the proof of own computations. Interestingly, with this example one can notice that ZKPs help to eliminate the need for clients to communicate with each other to verify the integrity of aggregated data. In zkDFL, authors proposed to deploy a smart contract which checks that the result of redundant aggregation of local weights hashes performed by server is equal to the sum of hashes uploaded by clients (notably, computational burden of the aggregation verification is also transferred to the smart contract). Consequently, differences in the settings and chosen ZKP schemes result in different complexity metrics."}, {"title": "Data Embedding based verification", "content": "This category covers protocols, where participants embed additional values into their data before sharing it with untrusted parties; later, the result of calculations performed by an untrusted source is assumed to be correct if the corresponding additional values are computed correctly. The embedding principle leads to an increase in the size of the transmitted data and the complexity of the outsourced calculations, which depends on the size of embedded values, and require more expensive data preprocessing.\nOne of the best approaches focused on the verifiable aggregation from the complexity perspective is VFL [16]. In VFL clients encode their secret values as a polynomial function and embed an additional point (a\u017c, A) before interpolation, where ai is a parameter of client i and A is obtained from a pseudo random generator (PRG) using a\u017c as an input. In order to verify the result of aggregation performed by a malicious server, each client check that the evaluation of the aggregated polynomial function at the point ai corresponds to the output of PRG with $\\sum_{i=1}^{C} a_i$ as an input. In this protocol, transmitted data overhead is negligible, but each client has to compute a costly interpolation and stores large public parameters.\nIn PVD-FL [61], authors proposed a decentralized verifiable protocol which is based on the verifiable matrix multiplica- tion algorithm. Parties embed random vectors into their data and then check that these vectors were correctly multiplied, as a result, the developed algorithm allows to verify basic operations of ML. In contrast to other approaches surveyed in this paper, in PVD-FL authors aim to show correctness of both aggregation and local models calculation. However, they also mention that their protocol is still vulnerable against poisoning attacks."}, {"title": "3.2 Discussion", "content": "In this subsection, we discuss how various features of the cross-silo setting impact the development of a verifiable FL protocol. We highlight efficient schemes which cope with cross-silo FL challenges, describe the influence of the threat model choice on the efficiency and security of the protocol, and discuss advantages and disadvantages of the blockchain-based approaches.\nFirstly, we describe the link between cross-silo setting characteristics and the efficiency of protocols. One can notice that in tables 1 and 2 there are mainly two parameters determining communication cost: C and D. However, there is a large difference in their impact on the complexity. Since the number of participants in cross-silo settings is moderate while ML models typically have large sizes, a dependence on D is less desirable. Nevertheless, taking into account that clients anyway must send their local models to a server with O(D) communication cost, the overall FL complexity would become asymptotically worse only in cases when the verification overhead is larger than D. For example, such as in [54, 10, 28], where the communication cost is O(CD).\nSecondly, we observe that several approaches rely on a blockchain infrastructure [14, 39, 1, 50, 26]. This strategy offers several advantages. For instance, smart contracts enforce a transparent and verifiable distribution of incentives [50]. The use of smart contracts to perform aggregation also makes the presence of a distinct server unnecessary, thereby replacing a single party trust with blockchain trust guarantees. All blockchain-based approaches are also robust against limited drop-outs of aggregators, i.e. miners. Table 1 demonstrates that the verification overhead per client is generally smaller for blockchain-based approaches compared to non-blockchain protocols with similar verification"}, {"title": "4 ZKP for cross-silo FL", "content": "The development of new ZKP schemes has been an active area of research over the past decade. While the surge of new protocols has led to a broad variety of schemes to choose from, it also resulted in additional desirable characteristics, making it challenging to determine the most suitable choice for a specific application. In this context, we discuss the applicability of ZKPs in cross-silo FL and study how to prove calculations in this setting minimizing the cost."}, {"title": "4.1 Applicability", "content": "In this subsection, we discuss diverse characteristics of ZKP schemes, examining their implications within the context of the cross-silo FL setting. Specifically, we consider the time complexities for proving, verifying and preprocessing, the proof size, the common reference string (CRS) and the commit-and-prove property which some ZKP schemes feature, and the partition into transparent schemes versus trusted setup based schemes.\nComputational complexity of proving and verifying. In cross-silo FL, the verification of proofs generated during the training procedure presents a significant challenge: each party who wants to ensure the correctness of the protocol needs to execute a verification algorithm in order to check proofs coming from many participants. If the verification algorithm has a linear cost with respect to the size of the computations, the cost for the verifier is proportional to the combined computations of all other parties in the system, which is often too expensive. Furthermore, in scenarios where the protocol is publicly verifiable, e.g. when all proofs are stored on a bulletin board as demonstrated in [51, 10], verification of all proofs performed by an external party after the training procedure would become excessively time-consuming. Hence, we consider it is desirable that the verification time complexity is at most logarithmic in the total amount of computation to ensure feasibility within the FL setting. In contrast to the verification complexity, the proving complexity is of a slightly less priority since each party has to prove only its own computations once. Consequently, the"}, {"title": "4.2 Storage cost optimization", "content": "In the previous subsection, we highlighted that one of the more important criteria for designing a verifiable FL protocol is the total communication cost of ZKP schemes and the associated cost to store the proofs. If parties perform more computations than they want to include in a single proof, they can distribute their computations over multiple proofs. In this subsection we first study what is the best granularity of the proofs under different ZKP schemes and then discuss an alternative idea to prove computations based on recursive proof composition.\nAs in a ML algorithm the same operations are often repeated many times, e.g., for different data or for different epochs in iterative algorithms, we assume that one can produce a ZKP for all computations by repeatedly proving correct evaluation of a single circuit. Then, we introduce a parameter k representing how many evaluations of this circuit are included per ZKP. In order to understand the effect of grouping less or more computations together in a single proof, we define a function to compute the storage cost as a function of k and then find the optimal number k of grouped circuits to prove, i.e., we find the k with minimum function value.\nTo begin with, we introduce the following notations:\n\u2022 $c \\in N$ - the size of the minimal circuit in bits, i.e., the size of the smallest circuit so that the complete algorithm can be represented as a repetition of that circuit;\n\u2022 $n \\in N$ \u2013 the number of circuits to prove;"}, {"title": "5 Challenges and future directions", "content": "While verifiable cross-silo FL protocols is a well studied area with a plenty of solutions, our analysis reveals several challenges which have not been yet addressed by the research community. Below we discuss potential research gaps and future directions.\nFirstly, to the best of our knowledge, there are currently no verifiable FL protocols that fully support verification of both computations performed by clients and server at the same time. Exceptionally, PVD-FL [61] aims to achieve this, however the approach does not guarantee the correctness of all actions of participants and is still vulnerable to poisoning attacks. While some papers, such as [39], consider threats from both the server and clients, authors do not verify the clients' behavior, but analyze the distribution of their values to address potential attacks. Instead, we believe it would be an interesting direction to design a protocol that would enable verification (following the definition in 2.4) of actions of all participants. It would be later intriguing to compare different approaches from the threat models and the efficiency perspectives.\nSecondly, we observe that verifiable aggregation is primarily studied for the most popular type of aggregation \u2013 averaging of vectors possessed by DOs. Nevertheless, in certain settings, other U-statistics with a kernel of the degree two or larger (e.g. Kendall rank correlation coefficient) could be applied [4], introducing new challenges in the verification process. Based on our analysis, we can see that ZKP-based verification would potentially fit, however finding an efficient solution still remains an open problem.\nThirdly, among the papers considered in this SOK, there are no protocols that are robust against a collusion between client and server to bypass the verification. However, in real world scenarios such collusion might occur. Interestingly, in VerSA [25], the authors do not consider collusion attacks, justifying this choice with the impossibility result shown by Gordon et al. [22]. The authors of the latter paper demonstrated that in specific settings multiclient verifiable computation cannot be achieved in the presence of users colluding with the server. Nonetheless, the specific setting considered does not necessarily apply to FL. For instance, in contrast to the setting from [22], in a verifiable FL protocol clients may exchange messages with each other or prove computations interactively. As a result, this impossibility result does not necessarily show that a verifiable FL protocol cannot be developed to be robust against server-client collusion attacks. We believe that the development of such a protocol is an interesting direction for future research.\nFourthly, we observe that the repetitive nature of FL training is usually overlooked while developing a verifiable protocol. Nevertheless, this property opens up an opportunity to design various optimizations. For instance, in [24], the authors considered this property to combine verification of multiple epochs together, thereby reducing the computational cost of clients. Following similar ideas, in Subsection 4.2, we proposed an optimization based on the observation that instead of repetitive proofs in FL, one can group circuits together before proving, thereby optimizing communication costs. However, we believe that other optimizations, for example, for approaches from other taxonomy groups, require further analysis.\nLastly, as described in Subsection 4.2, to the best of our knowledge, there are no works exploring the applicability of recursive ZKP schemes in the context of FL. In recent years, there has been an active research in the ZKP community to develop such schemes [9, 36, 35]. We anticipate that they deserve a particular attention. Their characteristics may allow for new optimizations and significant reduction in complexities of verifiable protocols."}, {"title": "6 Conclusion", "content": "In this paper, we presented a systematization of knowledge on verifiable cross-silo FL. We proposed a new taxonomy distinguishing four categories of verification techniques. We described general design patterns for each category and provided an analysis of threat models, computational and communication costs both per client and per server for each protocol. We also discussed how various features of the cross-silo setting impact the verification process and highlighted advantages of ZKP-based protocols. As a continuation of our conclusions, we discussed the applicability of different ZKP schemes for cross-silo FL and optimization strategies to minimize the communication cost. Finally, we described several research challenges revealed in our analysis and indicated future scientific directions."}]}