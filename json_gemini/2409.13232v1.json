{"title": "Relationship between Uncertainty in DNNs and\nAdversarial Attacks", "authors": ["Abigail Adeniran", "Adewale Adeyemo"], "abstract": "Deep Neural Networks (DNNs) have achieved state-\nof-the-art results and even outperformed human accuracy in\nmany challenging tasks, leading to DNNs' adoption in a variety\nof fields including natural language processing, pattern recogni-\ntion, prediction, and control optimization. However, DNNs are\naccompanied by uncertainty about their results, causing them\nto predict an outcome that is either incorrect or outside of a\ncertain level of confidence. These uncertainties stem from model\nor data constraints, which could be exacerbated by adversarial\nattacks. Adversarial attacks aim to provide perturbed input to\nDNNs, causing the DNN to make incorrect predictions or increase\nmodel uncertainty. In this review, we explore the relationship\nbetween DNN uncertainty and adversarial attacks, emphasizing\nhow adversarial attacks might raise DNN uncertainty.", "sections": [{"title": "I. INTRODUCTION", "content": "As the burgeoning demand for intelligence in critical\narea increases, Deep neural networks (DNNs) have been\nused in a variety of domains, such as computer vision [1],\nspeech recognition [2], natural language processing [3],\nmachine translation [4] etc., where they have achieved\ncomparable outcomes and in some cases surpassing human\nexpert performance. However, the deployments of DNNs for\nmission critical applications are limited due to inexplicable\nDNN inference computation, sensitivity to domain shifts and\nvulnerability to adversarial attacks [5].\n\nTypically, the input layer of a DNN receive the data\nand a prediction is made at the output layer after multiple\nconvolution and pooling operations (see Fig. 1). The output\nof the DNN also displays the confidence score, explaining\nwhat a DNN knows and what it doesn't know. Uncertainty\nin DNNs are broadly divided into data uncertainty (epistemic\nuncertainty) and model uncertainty (Aleatoric uncertainty) [6].\nThe epistemic uncertainty explains the uncertainty produced\nby the model's shortcomings, such as errors in the learning\nphase, an unsuitable model structure, insufficient information\ndue to unidentified samples, or poor training data set\ncoverage. In contrast, Aleatoric uncertainty is caused by\ninherent random effects (such as noise) which change the\ndistribution of the data sample from the population [7].\n\nIn this review, we discuss the relationship between uncer-\ntainty in DNNs and Adversarial attacks. The remainder of the\npaper is structured as follows: Section II describes Uncertainty\nin DNNs, Section III describes Adversarial attacks on DNNs,\nsection IV describes Relationship between Uncertainty in\nDNNs and Adversarial Attacks and Section V concludes this\npaper."}, {"title": "II. UNCERTAINTY IN DNNS", "content": "The lack of trust in each DNN output is referred to\nas uncertainty [8]. While it is impossible to design a\nDNN algorithm with 100% confidence, it is imperative\nto understand what causes uncertainty, how to measure it,\nand how to minimize it. Since deep learning models don't\nprovide uncertainty estimates and frequently make overly\nor underly confident predictions, they do in fact need a\nmore thorough evaluation. The research community wants\nto ensure that DNNs accurately describe the probability that\ntheir results will be incorrect or fall outside of a specified\nrange of accuracy.\n\nTo make safe and informed judgments, DNNs would\nneed to produce both an output and a degree of certainty\nin those outcomes. This indicates that the DNNs would\nconvey information about their level of uncertainty and if it\nis low enough for the output to be trusted along with their\nfindings. In order to handle the decision-making process, the\nalgorithm may require human intervention after producing\nan output with a high level of uncertainty. Five factors that\ncause uncertainty in a DNN's predictions include: variability\nin real-world situations, errors inherent to the measurement\nsystems, errors in the architecture specification of the DNN,\nerrors in the training procedure of the DNN, and errors\ncaused by unknown data."}, {"title": "A. Types of Uncertainties", "content": "\u2022 Aleatoric Uncertainty: This is the uncertainty arising from\nthe natural stochasticity of observations. As aleatoric\nuncertainty refers to the inherent noise in all observations,\nit cannot be reduced. Aleatoric uncertainty cannot be\nreduced even when more data is provided. When it\ncomes to measurement errors, we call it homoscedastic\nuncertainty because it is constant for all samples. Input\ndata-dependent uncertainty is known as heteroscedastic\nuncertainty.\n\u2022 Epistemic Uncertainty: This describes what the model\ndoes not know because of the lack of training data.\nEpistemic uncertainty is due to limited data and knowl-\nedge. Epistemic uncertainty is reducible, which means it\ncan be lowered by providing additional data. Epistemic\nuncertainty can arise in areas where there are fewer\nsamples for training."}, {"title": "B. Prediction uncertainty", "content": "\"Prediction uncertainty\" is the uncertainty that is conveyed\nin the model's output and is determined by summing the\nepistemic uncertainty and the aleatoric uncertainty [10]."}, {"title": "III. ADVERSARIAL ATTACKS ON DNNS", "content": "Adversarial attacks on DNNs involves the addition of subtle\nmalicious perturbation on clean images to fool the DNN\nmodel and cause misclassification as shown in Fig. 3. The\nvulnerability of DNNs to adversarial attacks was first noted by\nSzegedy et al., [11] and Goodfellow et al., [12], after which\nseveral other researchers have proposed new adversarial attack\nmethods and defense methods. Adversarial attacks could be\ntargeted or non-targeted. Targeted attacks attempt to force the\nmodel to predict an input as a specified target class which\ndiffers from the true class, while non-targeted attacks only\nattempt to enforce the model to misclassify the input. In\nliterature, the three types of adversarial attacks include: white\nbox, black box, and gray box attacks."}, {"title": "IV. RELATIONSHIP BETWEEN UNCERTAINTY IN DNNS\nAND ADVERSARIAL ATTACKS", "content": "Adversarial machine learning attacks are premised on per-\nturbing the input to a DNN in a way that increases the\nlikelihood of incorrect decision-making and results in false\npredictions [13]. Adversarial attacks increase uncertainty in\nDNNs (especially Epistemic uncertainty). Adversarial attacks\nchange the pixel distribution of an input image, causing\nthe DNN to suffer from either over-confidence or under-\nconfidence predictions. The uncertainty of a DNN model\ncan be used to infer the goal of an adversary. Adversarial\ngoals can be classified into four groups based on how they\naffect the DNN's output integrity, i.e., confidence reduction,\nmisclassification, targeted misclassification, and source/target\nmisclassification [14]."}, {"title": "V. CONCLUSION", "content": "There is some degree of uncertainty in every machine-\nlearning project. While some uncertainty will always exist,\nthere are several strategies for identifying, calculating, and\nminimizing it. Adversarial attacks, on the other hand, increase\nthe uncertainty associated with a DNN model by changing\nthe distribution of the pixels in the input image (epistemic\nuncertainty), thus confusing the DNN model. it is imperative\nfor developers to create redundancy solutions that ensure\nDNNs deployed to production meet Service Level Agreements,\nparticularly for mission-critical systems."}]}