{"title": "FACTS: A FACTORED STATE-SPACE FRAMEWORK\nFOR WORLD MODELLING", "authors": ["Li Nanbo", "Firas Laakom", "Yucheng Xu", "Wenyi Wang", "J\u00fcrgen Schmidhuber"], "abstract": "World modelling is essential for understanding and predicting the dynamics of\ncomplex systems by learning both spatial and temporal dependencies. However,\ncurrent frameworks, such as Transformers and selective state-space models like\nMambas, exhibit limitations in efficiently encoding spatial and temporal struc-\ntures, particularly in scenarios requiring long-term high-dimensional sequence\nmodelling. To address these issues, we propose a novel recurrent framework, the\nFACTored State-space (FACTS) model, for spatial-temporal world modelling.\nThe FACTS framework constructs a graph-structured memory with a routing\nmechanism that learns permutable memory representations, ensuring invariance to\ninput permutations while adapting through selective state-space propagation. Fur-\nthermore, FACTS supports parallel computation of high-dimensional sequences.\nWe empirically evaluate FACTS across diverse tasks, including multivariate time\nseries forecasting and object-centric world modelling, demonstrating that it con-\nsistently outperforms or matches specialised state-of-the-art models, despite its\ngeneral-purpose world modelling design.", "sections": [{"title": "INTRODUCTION", "content": "World modelling (Schmidhuber, 1990b; 2015; Ha & Schmidhuber, 2018) aims to create an internal\nrepresentation of the environment for an AI system, enabling it to represent (Hafner et al., 2019;\n2023), understand (Schrittwieser et al., 2020; Hafner et al., 2020), and predict (Ha & Schmidhuber,\n2018; Micheli et al., 2022) the dynamics of complex environments. This capability is crucial for\nvarious domains, including autonomous systems, robotics, and financial forecasting, where accurate\npredictions depend on effectively capturing both spatial and temporal dependencies (Hafner et al.,\n2019; Ha & Schmidhuber, 2018). Consequently, spatial-temporal learning (Liu et al., 2024; Hochre-\niter & Schmidhuber, 1997; Wu et al., 2023a; Oreshkin et al., 2019) emerges as a key challenge in\nworld modelling, as approaches must balance the complexities of modelling high-dimensional se-\nquential data while maintaining robust long-term predictive power.\nDespite significant advancements, current spatial-temporal learning frameworks, used in world mod-\nelling, based on Transformers (Vaswani et al., 2017; Schlag et al., 2021) and RNNs (Schmidhuber,\n2015; Ha & Schmidhuber, 2018; Hafner et al., 2019; 2020) backbones, face limitations in fully\ncapturing the complexities of high-dimensional spatial-temporal data. Transformer, though pow-\nerful (Chen et al., 2022; Robine et al., 2023; Micheli et al., 2022), are inefficient for long-term\ntasks due to their quadratic scaling and limited context windows (Zhang et al., 2022). On the other\nhand, RNNs provide a more structured approach to sequential data. However, their efficacy is hin-\ndered by the vanishing gradients (Hochreiter, 1991; Pascanu et al., 2013). The primary challenges in\nspatial-temporal learning arise from the high dimensionality of the data and the necessity to preserve\nlong-term dependencies (Hochreiter et al., 2001; Tallec & Ollivier, 2018).\nRecently, there has been a growing interest in Structured State-Space Models (SSMs) for world\nmodelling (Gu & Dao, 2023; Hafner et al., 2023; Samsami et al., 2024) using latent state-space\nrepresentations. These representations allow for the modelling of underlying dynamics, where latent"}, {"title": "PRELIMINARIES", "content": "Structured-state space Models (SSMs) have their roots in the classic Kalman filter (Kalman, 1960),\nwhere they process a m-dimensional input signal $x(t) \\in R^m$ into a d-dimensional latent state $z(t) \\in$\n$R^d$, which is then projected onto an output signal $y(t) \\in R^n$. The general form of an SSM is\nexpressed as follows:\n\n\u017c(t) = A(t)z(t) + B(t)x(t)\n\ny(t) = C(t)z(t) + D(t)x(t),\n\nwhere \u017c(t) = $\\frac{dz(t)}{dt}$ indicates the time derivative of the state. The matrices $A(t) \\in R^{d \\times d}$, $B(t) \\in$\n$R^{d \\times m}$, $C(t) \\in R^{n \\times d}$, and $D(t) \\in R^{n \\times m}$ present the state, input, output, and feed-forward matrices,\nrespectively. In systems without direct feedthrough, D(t) becomes a zero matrix. Furthermore,\nsince the original system operates in a continuous domain, discretisation is often used (Wang et al.,\n2024b; Smith et al., 2023), resulting in the general discrete-time formulation of SSM:\n\nZt = AtZt-1 + Btxt\n\nYt = CtZt\n\nwith At, Bt, and Ct govern the dynamics driven by the input sequence $x_{<t}$, with different construc-\ntions (Wang et al., 2024b; Gu & Dao, 2023; Dao & Gu, 2024) influencing the expressiveness and\nefficiency of the model. If we denote the state vector with h, we can see that equation 3-equation 4\nform is equivalent to the RNN dynamics. Hence, similarly to RNNs, the system, in equation 3-4, is\ninherently sequential, which inhibits parallel processing.\nParallelisation and the selective mechanism As shown in Blelloch (1990); Smith et al. (2023) if\nBt constructed independently of $z_{t-1}$, the linear recurrence in equation 3 can be expanded as equa-\ntion 5:\n\nZ = \\Sigma A^sBxs,\n\nwhere $A_t:= A_{t+1}...\\bar{A}_{s+2}\\bar{A}_{s+1}; \\bar{A}_{t+1} = I; B_0x_0 = z_0$ with respect to some initialisation. This\nexpansion not only allows for parallel computation of the linear terms, but also reveals the direct\nconnection established between distant inputs/observations along the sequential dimension, e.g. xo"}, {"title": "PROPOSED FRAMEWORK: FACTS", "content": "In this section, we introduce our proposed framework: FACTS (FACTored State-space) model, a\nnovel class of recurrent neural networks designed with a structured state-space memory. FACTS\nis characterised by two key features: permutable state-space memory, which allows for flexible\nrepresentation of system dynamics with more complex structures and invariant recurrence with\nrespect to permutations of the input features, ensuring consistent modelling of underlying factors in\nthe world across different time steps.\nOne key intuition behind the \u201cpermutable state-space memory\u201d in FACTS is the principle of history\ncompression (Schmidhuber, 1992a; 2003), which emphasises the need to eliminate redundant infor-\nmation in sequence modelling while uncovering algorithmic regularities. This principle is essential\nfor effective long-sequence modelling with high-dimensional data, as it improves generalisation by\nreducing the accumulation of unnecessary information. Existing SSMs often address this challenge\nby imposing fixed structural constraints on their state spaces, such as diagonal or block-diagonal\nstructures, to capture invariant components that persist throughout the sequence (Gu & Dao, 2023;\nGupta et al., 2022a;b; Dao & Gu, 2024). However, these fixed structural priors assume that spe-\ncific dimensions of the state space correspond to consistent and specific factors over time. This\nassumption can be limiting in world-modeling scenarios where the relationship between state-space\ndimensions and input features evolves dynamically. For instance, in video sequence modeling, fac-\ntors may correspond to moving objects, and the spatial location of these objects (i.e., pixel positions)\nchanges from frame to frame. In such cases, the model needs to adapt to these changes, but current\nSSMs formulations struggle to maintain consistent factor representations due to their rigid structural\nconstraints, i.e., in equation 7 the matrices B(xt) must not only select relevant information for\nmodelling sequence dynamics but also account for the changing relative orders between subspaces\nof $z_{t-1}$ and $x_t$, which can evolve over time. This introduces additional complexity, leading to noise\nand redundancy that can hinder effective history compression."}, {"title": "FACTS FORMULATION", "content": "The FACTS framework is formalised as a class of structured state-space model, which can capture\nthe dynamic interactions between the latent factors and the input features. To facilitate this dynamic\nfactorisation, i.e., the process of identifying and disentangling meaningful factors from the input\ndata over time, at each time step t, we conceptualise the hidden state Zt as a graph and hence, the\nstate-space memory is represented as a set of nodes that correspond to the latent factors. The input\nfeatures Xt are also treated as another set of nodes. Formally, let\n\nZt = {z,z,...,z}\n\nXt = {x+,x,...,x}\n\nwhere Zt denote the set of k latent factors at time step t and m is the number of input features. By\nformulating both sets as nodes, FACTS is inherently invariant to permutations of both input features\nand factors. Then, to efficiently learn the optimal connections between these two sets, we propose\na graph-based routing mechanism that can effectively match input features with the corresponding\nfactors .i.e., learn edges between nodes in Zt and Xt, reflecting the strength of each correspondence\nbetween each input feature and each latent factor."}, {"title": "Dynamic selective state-space updates", "content": "In analogy with the standard SSM dynamics equation 3-\nequation 4, the evolution of the latent factors of FACTS is governed by the following modified\nstate-space dynamics:\n\nZt = At Zt-1 + Bt \u00a9 Ut\n\nYt = Dec(CtZt)\n\nHere, Zt represents the state-space memory at time t, which stores the latent factors. The terms At,\nBt, and Ct are selective state-space model parameters responsible for controlling the information\nflow between the previous memory Zt-1 and the input features Xt. The symbol denotes element-\nwise multiplication, while Dec is a permutation-invariant decoder applied to the latent factors.\nCompared to the standard SSM dynamics, i.e., equation 3-equation 4, we note two key differences:\n(i) FACTS relies on element-wise multiplication, instead of matrix multiplication, to conserve the\ninvariance properties. (ii) xt in equation 3 is replaced with Ut = (Zt-1, Xt), which is a key element\nin FACTS that models the interactions between the memory Zt-1 and the input features Xt."}, {"title": "The attention-based router", "content": "Before diving into the details of the different parts of equation 10\nand equation 11, we first introduce the routing mechanism used in this work. To maintain the\nrecurrent permutability of equation 10, the routing mechanism between memory and inputs must\ndynamically assign input features to consistent factors. This can be done using an attention-based\nrouting mechanism defined as follows:\n\nZt-1 \u039f\u03c6,\u03c8,\u03c6 \u03a7t = softmax($\\frac{'(Z_{t-1})\\psi^T(X_+)}{\\sqrt{d}}$)\u03c6(X))\n\nwhere the operator learns the relationships between the memory Zt-1 and input features Xt,\ndynamically determining which features correspond to which latent factors. The functions \u03c6, \u03c8,\nand represent the query, key, and value mappings, respectively, and are applied row-wise to the\nmemory and input features."}, {"title": "Factorisation process", "content": "The term Ut = U (Zt-1, Xt) in equation 10 is crucial for capturing the inter-\nactions between the memory and the input features. Note that in prior works (Gu & Dao, 2023; Dao\n& Gu, 2024) Ut is typically constructed as function of the current input Xt only. In this paper, we\nargue that, similar to the gating in RNNs vs LSTMs (Hochreiter & Schmidhuber, 1997), it is more\neffective use both Xt and Zt-1 to conserve long term dependencies. This interaction plays a key role\nin factorisation, which refers to the process of binding the input features to specific memory items,\neffectively uncovering the underlying factors. In the FACTS framework, the memory at the previous\ntime step Zt-1 serves as the prior over the latent factors, and Ut is the factor momentum that guides\nthe evolution of these factors across time. This factor momentum is computed as:\n\nUt = Zt-1 \u0421\u03c6\u03c5, \u03c8\u03c5, \u03c6\u03c5 \u03a7\u03c4"}, {"title": "Selectivity through memory-input routing", "content": "The selective state-space model parameters At, Bt,\nand Ct are constructed through interactions between the memory and the input features, ensuring\nthat both the memory and the inputs jointly decide which information should be retained or updated.\nThese parameters are computed as follows:\n\nAt = Zt-1 C, X\n\nBt = \u2206t \u00a9 (Zt-1 \u0421\u0444\u0432,\u0432,\u0432 \u0425+)\n\n\u0100t = exp(at)\n\nCt = Zt-1 C\u00a2c,\u03c8c,4c Xt\n\nHere, At is a step size introduced for discretisation, and the functions \u2206, \u03c8\u2206, and \u03c6\u25b3 (as well\nas their counterparts for Bt and Ct) are responsible for mapping the memory and inputs to their\nrespective selective parameters. The exponential function exp ensures that the selective parameters\nare non-negative, while a is a trainable scalar controlling the influence of At. By employing this\nselective mechanism, FACTS is capable of compressing long sequences in its state-space memory\nwhile maintaining the key properties of latent permutation equivariance and row-wise permutation\ninvariance. Hence, FACTS can efficiently capture meaningful factors, e.g., objects in video frames\nor independent sources in a signal, even as their relationships with input features change over time."}, {"title": "Linearisation", "content": "Although the framework presented so far in equation 10 has a permutable state-space\nmemory and equivariant through the memory-input routing, which we formally show in Section 3.2,"}, {"title": "THEORETICAL ANALYSIS OF FACTS", "content": "Here, we formally proof the permutation equivariance and invariance properties of FACTS. We first\nformally define the two fundamental properties, namely left permutation equivariant (L.P.E.) and\nright permutation invariant (R.P.I.) in Definitions 1 and 2, respectively.\nDefinition 1. Let $f: R^{n_1 \\times n_2} \\times R^{t \\times n_3 \\times n_4} \\rightarrow R^{N_1 \\times n_5}$ be a bivariate function with $n_1, n_2, n_3, n_4, n_5, t \\in$\nN. f is permutation equivariant (L.P.E.) if for all \u03c3\u03b5 Sn1, $M_1 \\in R^{n_1 \\times n_2}$, and $M_2 \\in R^{t \\times n_3 \\times n_4}$,\n\nf(\u03c3M1, M2) = \u03c3f (M1, M2),\n\nwhere Sk denotes the set of permutation matrices of size $R^{k \\times k}$.\nDefinition 2. Let $R^{n_1 \\times n_2} \\times R^{t \\times n_3 \\times n_4} \\rightarrow ]R^{n_1 \\times n_5}$ be a bivariate function with $n_1, n_2, n_3, n_4, n_5, t \\in$\nN. f is right permutation invariant (R.P.I.) if for all $\u03c3_1, \u03c3_2,...,\u03c3_t \\in S_{n_3}$, $M_1 \\in R^{n_1 \\times n_2}$, and\n$M_1, M_2,..., M_t \\in R^{n_3 \\times n_4}$,\n\nf(M1, [\u03c31M2, \u03c32M2,...,\u03c3\u1e6dM]) = f(M1, [M2, M2,...,M]).\n\nThese L.P.E. and R.P.I. properties, which formally describe the two fundamental aspects of FACTS:\npermutable memory and permutation-invariant recurrence (w.r.t. the features) with memory\nZt-1 and Xt serving as the left and right arguments of FACTS. They are thus essential not only for\nconstructing the routing mechanism but also for the overall design of FACTS.\nUsing Definitions 1 and 2, that by taking memory Zt and features Xt as the left and right arguments\nin FACTS (equation 20), we can show the following result:\nTheorem 1. FACTS as defined in equation 20 is L.P.E. and R.P.I.\nThe proof of Theorem 1 is available in Appendix B. Theorem 1 proves our main claim that FACTS:\ni) is invariant to input features permutation. ii) learns permutable state-space memory. Furthermore,\nit is possible to extend our results in Theorem 1 to the more general case, where A, B, U are L.P.E.\nand R.P.I. functions of Zt-1 and Xt. The main result is presented in Theorem 2.\nTheorem 2. if A, B,U are L.P.E. and R.P.I. functions of Zt-1 and Xt, any dynamics governed\nby equation 10 is L.P.E. and R.P.I."}, {"title": "EXPERIMENTS", "content": "We design experiments to evaluate the effectiveness of FACTS in world modelling. We frame world\nmodelling as a prediction task, where the model must predict future events in complex environ-\nments based on observed history, and evaluate a model's performance by its prediction accuracy.\nWe conduct experiments on two environments: the multivariate time series (MTS) benchmark for\nforecasting and the CLEVRER multi-object video dataset for video prediction."}, {"title": "LONG TERM FORECASTING", "content": "In many real-world applications, such as climate prediction, traffic flow management, or autonomous\nsystems, predicting future states over long horizons is crucial for effective decision-making. World\nmodeling, in these domains, often involves high-dimensional multivariate inputs\u2014such as interact-\ning agents, variables, or environmental factors requiring the system to account for their complex\ndependencies and interactions across time. Long-term forecasting in this context is challenging as\nit demands accurate representation of temporal dynamics over extended periods. Additionally, in-\nput features may lack a predefined order, or this order could change dynamically. For example, a\nsystem may receive data from sensors (e.g., temperature and pressure) without knowing which is\nwhich during testing. The world modeler must provide reliable long-term predictions even if the\ninput order changes unexpectedly, and generalize to unseen configurations, without learning every\npossible permutation of input features during training.\nBenchmark We use the open-source Time Series Library (TSLib)\u00b9, a widely-used benchmark for\ntraining and evaluating time-series models. TSLib provides standardized settings and a leader-\nboard of top-performing models, ensuring fair and consistent comparisons. Our focus is on long-\nterm multivariate time-series forecasting (MSTF) tasks, using 9 diverse real-world datasets: 4 ETT\ndatasets, Electricity, Weather, Exchange, Traffic, and Solar-Energy. Our approach FACTS is com-\npared against 8 baseline models, including state-of-the-art MSTF approaches that top the TSLib\nleaderboard (Wang et al., 2024c; Liu et al., 2024; Wu et al., 2023a; Nie et al., 2023; Zeng et al.,\n2023; Zhang & Yan, 2023; Zhou et al., 2022; Wu et al., 2021). Following the setup of iTransformer\nand S-Mamba, we fix the input sequence length to 96 and evaluate the models on prediction lengths\nof 96, 192, 336, 720. The datasets and experimental protocols are widely used in MSTF literature,\nproviding a robust evaluation framework. Prediction accuracy is measured using mean-squared error\n(MSE) and mean-absolute error (MAE) as the primary metrics."}, {"title": "FORECASTING WITH PREDEFINED ORDER (SCENARIO 1)", "content": "We use the exact same setup to Wang et al. (2024c); Liu et al. (2024); Wu et al. (2023a), with the\nexception of the pre- and post-processing modules (referred to as the \u201cembedders\u201d and \u201cprojectors\u201d\nin TSLib). In our implementation, we replace these with set functions to accommodate the output\nstructure of FACTS (c.f. Appendix C for more details). Note that in the standard setup of TSLib, the\narrangement of the input features in the test is not changed and is the identical to the arrangement\nto the one seen during the training. The average results over the different prediction windows of our\nproposed approach along with all competing methods are presented in Table 1 and the full results\nare available in Table 6 in Appendix D."}, {"title": "FORECASTING WITH UNKNOWN ORDER (SCENARIO 2)", "content": "To evaluate robustness under dynamic scenarios, we randomly permute the input features during\nthe test phase to simulate environments where the arrangement of agents or entities (e.g., robots,\nsensors) changes unpredictably. This mirrors real-world scenarios where input configurations vary,\nchallenging world models to adapt to unseen input orderings. We focus on top pretrained models"}, {"title": "OBJECT-CENTRIC WORLD MODELLING", "content": "Visual object-centric representation learning (OCRL) (Burgess et al., 2019; Greff et al., 2019; Lo-\ncatello et al., 2020; Nanbo et al., 2020) tackles the challenge of binding visual information to\nconsistent factors, even as object features dynamically permute with movement across pixels in\nvideos (Kipf et al., 2023). This aligns with FACTS' objective of identifying regularities in dynamic\nenvironments for history compression and future-event prediction, making OCRL an ideal evalua-\ntion benchmark. To evaluate how FACTS 1) leverages object information for future predictions and\n2) aligns its discovered factors with objects, we conduct two OCRL experiments, slot dynamics pre-\ndiction and unsupervised object discovery, set on the simulated CLEVRER multi-object videos (Yi\net al., 2020).\nSlot dynamics prediction The task involves having a world model capture object-centric dynamics\nin latent space: given the latent object representations of observed events (\"burn-in\"), the model\npredicts the future latent codes of the objects (\u201croll-out", "factors\" differently.\nResults We visualise the discovered factors by independently rendering each factor's dynamics back\ninto video while keeping the others fixed. As shown in Figure 5 (in the Appendix), FACTS primar-\"\n    },\n    {\n      \"title\": \"ABLATION STUDY\",\n      \"content\": \"FACTS requires the use of a set encoder and the predefined selection of the number of factors in the\nstate-space memory prior to training. Our ablation study aims to investigate the impact of different\nset encoders and the choice of the number of predefined factors on model performance.\nImpact of different set encoders We conduct our experiments on the Electricity dataset, testing\nfour prediction lengths: 96, 192, 336, and 720. Three different set encoders are evaluated, each\nemploying different priors: a Discrete-Fourier Transform (DFT) decomposer, a trainable Conv2d\nembedder, and a multi-scale Conv2d embedder (inspired by Wu et al. (2023a)). Table 3 presents the\ncomparison results, with further details of these embedders provided in Appendix C.2. The multi-\nscale periodic embedder consistently outperforms the others across all prediction lengths, while the\nDFT-based embedder shows declining performance as the prediction length increases. The standard\nerror further indicates that longer forecasting horizons amplify the impact of encoder choice, making\nit a more critical factor in model accuracy. This highlights the importance of using an unbiased,\nlearnable set encoder to improve generalisation.\nImpact of different number of factors We investigate the effect of the preset number of factors\n(k) on FACTS' performance, using the ETTm1 dataset with a 96-prediction length setting. Previous\nwork, such as Mamba (Gu & Dao, 2023) and xLSTM (Beck et al., 2024), shows that state-space\nmemory size significantly impacts performance. In FACTS, memory size is determined by the\nnumber of factors and the dimension of each factor (d). To isolate the effect of the number of\nfactors, we trained the model across various settings, gridded by different numbers of factors and\nfactor dimensions. As shown in Table 4, FACTS achieves consistent performance and is indeed\nrobust to the hyper-parameters.\"\n    },\n    {\n      \"title\": \"DISCUSSIONS & CONCLUSION\",\n      \"content\": \"In this work, we introduced FACTS, a novel recurrent framework designed for spatial-temporal\nworld modelling. FACTS is constructed permutable state-space memory, which offers the flex-\nibility needed to capture complex dependencies across time and space. By employing selective\nmemory-input routing, FACTS is able to dynamically assign input features to distinct latent factors,\nenabling more efficient history compression and long-term prediction accuracy. Furthermore, we\nformally showed that FACTS: i) is invariant to input features permutation. ii) learns permutable\nstate-space memory, maintaining consistent factor representations regardless of changes in the input\norder. Furthermore, through comprehensive empirical evaluations, FACTS demonstrated superior\nperformance on a variety of real-world datasets, consistently matching or outperforming specialized\nstate-of-the-art models in diverse tasks. Notably, FACTS maintained its predictive powers even in\"\n    },\n    {\n      \"title\": \"RELATED WORKS\",\n      \"content\": \"From a historical perspective, \u201cworld models\\\" or using models to learn environmental dynamics\nand leveraging them in policy training has an extensive literature, with early foundations laid in the\n1980s using feed-forward neural networks (FNNs) (Werbos, 1987; Munro, 1987; Werbos, 1989;\nRobinson & Fallside, 1989; Nguyen & Widrow, 1990) and in the 1990s with RNNs (Schmid-\nhuber, 1990b;a; 1991b;a). Notably, PILCO (Deisenroth & Rasmussen, 2011; McAllister & Ras-\nmussen, 2016) has emerged as a key probabilistic model-based method, using Gaussian processes\n(GPs) (MacKay et al., 1998) to learn system dynamics from limited data and train controllers for\ntasks like pendulum swing-up and unicycle balancing. While GPs perform well with small, low-\ndimensional datasets, their computational complexity limits scalability in high-dimensional scenar-\nios. To address this, later works (Gal et al., 2016; Depeweg et al., 2016) have adopted Bayesian\nneural networks (Kononenko, 1989), which have demonstrated success in control tasks with well-\ndefined states (Hein et al., 2017). However, these methods remain limited when modeling high-\ndimensional environments, such as sequences of raw pixel frames. In the context of reinforcement\nlearning, using recurrent models to learn system dynamics from compressed latent spaces has sig-\nnificantly improved data efficiency (Schmeckpeper et al., 2020; Finn et al., 2016). While the de-\nvelopment of internal models for reasoning about future states using RNNs dates back to the early\n1990s, subsequent works, such as \u201cLearning to Think": "Schmidhuber, 2015) and \u201cWorld Models"}]}