{"title": "MPruner: Optimizing Neural Network Size with CKA-Based Mutual Information Pruning", "authors": ["Seungbeom Hu", "ChanJun Park", "Andrew Ferraiuolo", "Sang-Ki Ko", "Jinwoo Kim", "Haein Song", "Jieung Kim"], "abstract": "Determining the optimal size of a neural network is critical, as it directly impacts runtime performance and memory usage. Pruning is a well-established model compression technique that reduces the size of neural networks while mathematically guaranteeing accuracy preservation. However, many recent pruning methods overlook the global contributions of individual model components, making it difficult to ensure that a pruned model meets the desired dataset and performance requirements. To address these challenges, we developed a new pruning algorithm, MPruner, that leverages mutual information through vector similarity. MPruner utilizes layer clustering with the Centered Kernel Alignment (CKA) similarity metric, allowing us to incorporate global information from the neural network for more precise and efficient layer-wise pruning. We evaluated MPruner across various architectures and configurations, demonstrating its versatility and providing practical guidelines. MPruner achieved up to a 50% reduction in parameters and memory usage for CNN and transformer-based models, with minimal to no loss in accuracy.", "sections": [{"title": "Introduction", "content": "Neural networks have revolutionized numerous fields, including computer vision and natural language processing (Abiodun et al. 2018; Elad, Kawar, and Vaksman 2023; Otter, Medina, and Kalita 2020), offering unprecedented levels of accuracy and functionality. However, with the success of neural networks also has come a significant increase in the size and complexity of neural networks, perhaps more so than strictly required: current neural networks often suffer from over-parametrization, where more neurons are used than necessary to effectively represent the dataset. Deployment of such over-parametrized models leads to unnecessary use of memory and power, which also translates to increased financial costs.\nOver-parametrization is a significant issue across all model architectures, and the recent success of transformer-based models (Vaswani et al. 2017; Lin et al. 2022) has magnified this trend. These models have achieved high performance on various tasks (Dosovitskiy et al. 2021; Khan et al. 2022; Shamshad et al. 2023) with extremely large networks, often exceeding billions of parameters. While transformers have opened a new chapter in performance, their size introduces new challenges, such as the need to fine-tune pre-trained models with data specific to new goals rather than training new models from scratch (Radiya-Dixit and Wang 2020; Tay et al. 2022; Zaken, Goldberg, and Ravfogel 2022). This trend forces users to maintain unnecessarily large pre-trained models (Fan et al. 2024), which limits the applicability of these powerful models, especially in resource-constrained environments like edge computing, where inference occurs on phones or other small devices. Consequently, there is a growing need for innovative solutions that try to find out the proper model size without sacrificing the benefits of advanced models.\nOne way to address this problem is through pruning (Cheng, Zhang, and Shi 2023; Han, Mao, and Dally 2016; Molchanov et al. 2016; Ma, Fang, and Wang 2023; Frantar and Alistarh 2023; Sun et al. 2024; Ganesh, Corso, and Sekeh 2020; Fan et al. 2021; Yang, Cao, and Zhao 2024; Liu et al. 2024), a well-known technique for compressing neural networks. While there is a substantial body of literature on this topic, most approaches focus on evaluating the importance of individual weights or optimizing an objective function subject to sparsity constraints. However, these methods often face two major challenges: (1) irregular memory access resulting from the selective pruning of weights within the same memory region, and (2) the lack of a multi-layer, cluster-wise approach with mathematical guarantees, due to the complexity and difficulty of analyzing the effects of layer clusters on model functionality. As a result, they offer a localized best-effort pruning solution, which is effective but fails to provide mathematical proof that the pruned size is optimal for the model and dataset."}, {"title": "Background and Related Works", "content": "A large body of work addresses neural network compression, aiming to reduce model size and computational demands while maintaining performance. Among them, three well-known compression methods are quantization, pruning, and knowledge distillation. These methods can be combined rather than chosen as alternatives, as they have orthogonal effects toward the same goal of compression. These techniques are crucial for deploying neural networks in resource-constrained environments, such as mobile devices and edge computing platforms. Our paper focuses on proposing a new pruning method to improve memory and power efficiency."}, {"title": "Pruning", "content": "Pruning is broadly categorized into unstructured and structured approaches (Cheng, Zhang, and Shi 2023), each with distinct characteristics and advantages. We focus on structured pruning (He and Xiao 2024), which is extensively studied both for CNNs (Li et al. 2017; Luo, Wu, and Lin 2017; He, Zhang, and Sun 2017; Liu et al. 2017; Lin et al. 2019; Huang and Wang 2018; Ganesh, Corso, and Sekeh 2020) and Transformers (Michel, Levy, and Neubig 2019; Voita et al. 2019; Kovaleva et al. 2019; Lagunas et al. 2021; Ma, Fang, and Wang 2023; Frantar and Alistarh 2023; Sun et al. 2024; Guo et al. 2023; Kim et al. 2024; Ashkboos et al. 2024; Fan et al. 2024; Men et al. 2024; Yang, Cao, and Zhao 2024; An et al. 2024; Fan et al. 2021). This approach simplifies the network's architecture by removing redundant or less critical structures, thereby enhancing overall efficiency. Common techniques in structured pruning include filter pruning, which targets entire filters in convolutional layers to achieve structured sparsity and improve computational efficiency. Neuron pruning, another structural method, involves removing entire neurons in fully connected layers, significantly reducing model size while preserving core functionality. While structured pruning offers the advantage of regular sparsity patterns that are more compatible with optimization on specialized hardware, it often requires careful adjustments to maintain model performance and can lead to changes in network architecture that impact training dynamics.\nAmong these approaches, some utilize inter-layer information, such as mutual information within the network, to identify optimal pruning locations. For instance, Mutual Information-based Neuron Trimming (MINT) (Ganesh, Corso, and Sekeh 2020) examines the relationships between filters in consecutive layers of CNN models, removing weights or filters to reduce layer size while considering their relative importance. In the context of transformer models, especially large language models (LLMs), Fan et al. (Fan et al. 2021) determine sparsity levels based on inputs. However, their method focuses primarily on local mutual information by evaluating similarities between adjacent layers and does not consider the contributions of multiple layers simultaneously. LLM-Pruner (Ma, Fang, and Wang 2023) employs structural pruning to selectively remove non-critical coupled structures based on gradient information. SliceGPT (Ashkboos et al. 2024) replaces each weight matrix with a smaller matrix using Principal Component Analysis. LaCo (Yang, Cao, and Zhao 2024) gradually merges similar layers from deep to shallow, setting a threshold to avoid excessive merging of layers. As a result, these methods concentrate on identifying which layers are least or most important and should be pruned or removed, rather than selecting unnecessary layers by analyzing the global multi-layer cluster-wise information of the entire model. Consequently, while they may perform well using local evidence, they cannot demonstrate that the pruned model is optimally sized for the given task.\nIn this context, MPruner introduces a novel multi-architectural layer pruning approach that significantly enhances the use of analytical metrics by incorporating a state-of-the-art layer similarity metric to guide pruning, overcoming the limitations of previous and related methods. Unlike earlier approaches, MPruner offers two key advantages. First, our method offers both mathematical and intuitive criteria to ensure that pruning safely preserves accuracy. This is achieved by leveraging a well-known metric, Centered Kernel Alignment (CKA) (Kornblith et al. 2019), to calculate layer similarities. Second, it calculates the global contribution of multi-layer clusters rather than focusing solely on pairs of adjacent layers. This allows our method to capture global information more effectively, ensuring a more comprehensive and accurate pruning process."}, {"title": "Centered Kernel Alignment (CKA) similarity score", "content": "Centered Kernel Alignment (CKA) (Kornblith et al. 2019) is a metric used to assess the similarity between two sets of representations. Equation 5 presents the formula for calculating the CKA value between the outputs of adjacent layers (i and j). It quantitatively measures how well the activations of one layer align with those of another layer or how similar two sets of activations are. Previous research (Davari et al. 2023) has demonstrated that CKA is effective for evaluating the similarity of different layers in neural networks, particularly due to its ability to capture complex and non-linear relationships between activation patterns compared to other methods. As a result, CKA is considered a powerful tool for analyzing and comparing neural network layers.\nThe process of calculating the CKA score, depicted in Equations 1-5, begins with the outputs of two different layers in the network. For example, if $x_i$ and $x_j$ are the outputs of two adjacent layers, Gram matrices K and L are first computed using Equation 1. These matrices are then centered using a centering matrix (Equations 2 and 3). Next, the Hilbert-Schmidt Independence Criterion (HSIC) (Chwialkowski and Gretton 2014) is calculated using Equation 4, which evaluates the independence between two random variables. Finally, the CKA score between the two layers is obtained using Equation 5. The CKA score offers practical benefits in terms of applicability, as it only requires the output vectors of the networks, eliminating the need to look into the network's detailed structure, such as weight values. In this sense, it can be applied to a wide range of architectures.\n$K = x_i x_i^T$,  $L = x_j x_j^T$\n$H = I_n - \\frac{1}{n} J_n$ (Centering Matrix)\n$K = HKH$,  $L = HLH$\n$HSIC(K, L) = \\frac{1}{(n-1)^2} tr(KHLH)$\n$CKA(x_i, x_j) = \\frac{HSIC(K, L)}{\\sqrt{HSIC(K, K) \\cdot HSIC(L, L)}}$"}, {"title": "MPruner: A Pruning Method for Determining Optimal Model Size", "content": "MPruner consists of three phases, as depicted in Figure 2. The first phase is the (1) analysis stage, where the entire model is examined to provide multi-layer cluster-wise information. The second phase is the (2) optimization stage, where unnecessary and redundant layers with minimal impact on the model's functionality are removed. The third phase is the (3) recovery stage, in which the model's performance is restored through retraining.\nThe overall mechanism is illustrated in Algorithm 1. It takes five inputs: the model M, the dataset D, the hook position L to specify where to calculate the CKA-based mutual information metrics, the CKA threshold T to determine the multi-layer clustering, the accuracy drop threshold y to determine when the algorithm should stop, and the freeze flag b to indicate whether retraining should involve the entire network or focus only on the pruned parts. The algorithm sets the pruning granularity to 1 (line 4), meaning that all layers except one in redundant layer clusters will be removed. If set to 2, it prunes layers by skipping one after pruning one, effectively removing half of the redundant layers.\nDuring each iteration, the algorithm first calculates and records the CKA values (line 6). If no layer clusters are found to perform similar functionalities in the model, the algorithm terminates (line 7). Otherwise, it invokes the pruner to remove redundant layers inside the multi-layer cluster according to the specified pruning granularity (line 8). The pruner then returns the pruned and untrained network $M_{p,u}$ and the layers $l_{sf}$ that should be frozen if retraining is required. After retraining the pruned network, the algorithm calculates the accuracy of the updated version (lines 10 and 11), then adjusts either the model or the pruning granularity based on the observed accuracy drop (line 12).\nInstead of specifying a sparsity level to prune the model parameters to a certain extent, the algorithm terminates when no more layers can be pruned or when the pruning granularity becomes too coarse. These conditions indicate that the network either has no layers left to prune or is no longer suitable for further pruning.\nTwo key functions in the algorithm are GetCandidates and Pruner. The GetCandidates function identifies layers within the network that perform similar functions. Algorithm 3 outlines the essential steps for identifying these similarities, utilizing Algorithm 2, which calculates the CKA values for all layers in the model and records them in the CKA matrix (VCKA). Once the CKA matrix is generated, pruning candidates are selected based on CKA value similarity: if the output vectors of two or more adjacent layers fall within a specified threshold, they are grouped together as a pruning candidate cluster (C).\nThe Pruner, as detailed in Algorithm 4, reduces the size of the target model based on the results from GetCandidates and the specified pruning granularity. If a layer cluster identified in the candidate set consists of two or more layers and the pruning granularity is set to 1, our approach initially attempts to remove all layers in the cluster except the first one, based on the evidence that these layers perform similar roles within the model. For instance, if n adjacent layers form a module ($p_i = l_1, ..., l_n$) and the output vectors from $l_1$ to $l_n$ fall within a similarity threshold or the given input values, merging these layers is expected to have minimal impact on the model's performance.\nHowever, we observe that excessive pruning sometimes leads to a noticeable drop in accuracy, particularly for CNN models. We believe this issue arises from having too many layers within a single cluster. To address this, we offer the option to adjust the pruning granularity to values other than 1. If the granularity is set to 2, the algorithm will attempt to remove every other layer in the cluster, effectively pruning the even-numbered layers.\nAs defined in Algorithm 1, our algorithm adjusts the pruning ratio if the pruned model experiences a significant accuracy drop due to the reduction of multiple layers and fails to meet the accuracy threshold. We have found that such adjustments are generally unnecessary for transformer models. However, CNN models (such as ResNet (He et al. 2016)) may require this step to mitigate accuracy loss."}, {"title": "Experiments", "content": "We conducted a range of evaluations by varying the inputs to MPruner. The primary goal of our experiments was not to argue that our method is superior to previous pruning techniques, but to confirm whether the multi-layer cluster pruning described in Section can achieve the optimal size, as supported by mathematical evidence measured by our CKA similarity score. All code and models were implemented in PyTorch. Due to the diversity of our experiments, we utilized multiple GPUs for analysis, optimization, recovery, and performance validation both before and after pruning. Specific details of the experimental setup are provided. As part of the inputs for MPruner, we set the CKA threshold (\u03c4) to 95%, 98%, and/or 99%. The retraining process was conducted with a learning rate of 0.00005, a batch size of 32, and for 3 epochs. To ensure a thorough comparison, we report all intermediate results, not just the final input and output models of our top-level pruning algorithm 1.\nOverall, we observed that our method effectively optimizes the size of both CNN and Transformer models with high scalability. The analysis stage operates in linear time, efficiently determining global multi-layer cluster-wise information regardless of the network's depth or width. Additionally, our approach demonstrated strong effectiveness for Transformer models, including both encoder-only and encoder-decoder architectures. We also found that our method can be seamlessly integrated with state-of-the-art magnitude pruning techniques (Sun et al. 2024) without causing any accuracy loss in Transformer models."}, {"title": "Conclusion and Future Works", "content": "We propose a novel pruning method that collapses layers, while regulating the pruning ratio according to specified accuracy thresholds, using multi-layer cluster-wise information to optimize neural network size. This method leverages layer similarities in architectures such as CNNs and transformers, removing unnecessary layers based on CKA similarity values between layer clusters. Our experiments show that this approach is highly effective for transformer models and efficiently reduces the size of large CNN models by eliminating unnecessary components. Additionally, our results suggest that our method can be seamlessly integrated with other pruning techniques to further reduce both model size and inference time.\nIn the future, we plan to explore alternative metrics beyond CKA for identifying layers to prune, aiming to improve compatibility with various architectures. We also intend to integrate our approach with advanced NAS tools and conduct further evaluations on large-scale language models, such as LLaMa and GPT, to assess the scalability and broader applicability of our method."}, {"title": "Supplementary Materials", "content": "The following sections present additional experiments involving MPruner that are not covered in the main text.\n\u2022 Hareware setting:\nCPU: Intel(R) Xeon(R) W5-2455X processor\nGPU: NVIDIA RTX A4000 16GB GPU\nMEMORY: 128 GB\n\u2022 Dataset: ILSVRC-2012\n\u2022 Model:\nmicrosoft/resnet152\nmicrosorf/restnet50\nThis experiment compares a model that was fine-tuned using Microsoft's ResNet152 for 3 epochs with a model that is compressed by MPruner. In this experiment, we set CKA thresholds as 98 and 99, followed by 3 epochs of retraining and unfreezing/freezing. The batch size was set to 64, the optimizer used was Adam, and the learning rate was 0.0002. The layer pruning process of the model was based on the similarity of the intermediate layers, specifically focusing on the ResNetBottleneck layer. The results of the experiment are in Table 4.\nThis experiment compares a model fine-tuned using Microsoft's ResNet50 for 3 epochs with a model that is compressed by MPruner. In this experiment, we set CKA thresholds as 95, 98, and 99, followed by 3 epochs of retraining and unfreezing/freezing. However, the threshold of 99 was excluded as no pruning occurred. The batch size was set to 64, the optimizer used was Adam, and the learning rate was 0.0002. The layer pruning process of the model was based on the similarity of the intermediate layers, specifically focusing on the ResNetBottleneck layer. The results of the experiment are in Table 5.\n\u2022 Hareware setting:\nCPU: 13th Gen Intel(R) Core(TM) i7-13700\nGPU: NVIDIA GeForce RTX 3070 TI 8GB\nMEMORY: 16GB\n\u2022 Dataset: Dair AI Emotion\n\u2022 Model: Bert-base / text classification\nThis experiment compares a model fine-tuned using BERT-base for text classification with 1 epoch to a model compressed by MPruner. Additionally, we conduct experiments with Wanda, comparing the results of applying Wanda alone versus using the integrated tool combining Wanda and MPruner. The sparsity levels for Wanda are 10%, 20%, 30%, 40%, and 50%. In this experiment, CKA thresholds of 98 and 99 were used, followed by 3 epochs of retraining and"}]}