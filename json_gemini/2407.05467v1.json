{"title": "The infrastructure powering IBM's Gen AI model development", "authors": ["Talia Gershon", "Seetharami Seelam", "Brian Belgodere", "Milton Bonilla", "Lan Hoang", "Danny Barnett", "I-Hsin Chung", "Apoorve Mohan", "Ming-Hung Chen", "Lixiang Luo", "Robert Walkup", "Constantinos Evangelinos", "Shweta Salaria", "Marc Dombrowa", "Yoonho Park", "Apo Kayi", "Liran Schour", "Alim Alim", "Ali Sydney", "Pavlos Maniotis", "Laurent Schares", "Bernard Metzler", "Bengi Karacali-Akyamac", "Sophia Wen", "Tatsuhiro Chiba", "Sunyanan Choochotkaew", "Takeshi Yoshimura", "Claudia Misale", "Tonia Elengikal", "Kevin O'Connor", "Zhuoran Liu", "Richard Molina", "Lars Schneidenbach", "James Caden", "Christopher Laibinis", "Carlos Fonseca", "Vasily Tarasov", "Swaminathan Sundararaman", "Frank Schmuck", "Scott Guthridge", "Jeremy Cohn", "Marc Eshel", "Paul Muench", "Runyu Liu", "William Pointer", "Drew Wyskida", "Bob Krull", "Ray Rose", "Brent Wolfe", "William Cornejo", "John Walter", "Colm Malone", "Clifford Perucci", "Frank Franco", "Nigel Hinds", "Bob Calio", "Pavel Druyan", "Robert Kilduff", "John Kienle", "Connor McStay", "Andrew Figueroa", "Matthew Connolly", "Edie Fost", "Gina Roma", "Jake Fonseca", "Ido Levy", "Michele Payne", "Ryan Schenkel", "Amir Malki", "Lion Schneider", "Aniruddha Narkhede", "Shekeba Moshref", "Alexandra Kisin", "Olga Dodin", "Bill Rippon", "Henry Wrieth", "John Ganci", "Johnny Colino", "Donna Habeger-Rose", "Rakesh Pandey", "Aditya Gidh", "Aditya Gaur", "Dennis Patterson", "Samsuddin Salmani", "Rambilas Varma", "Rumana Rumana", "Shubham Sharma", "Aditya Gaur", "Mayank Mishra", "Rameswar Panda", "Aditya Prasad", "Matt Stallone", "Gaoyuan Zhang", "Yikang Shen", "David Cox", "Ruchir Puri", "Dakshi Agrawal", "Drew Thorstensen", "Joel Belog", "Brent Tang", "Saurabh Kumar Gupta", "Amitabha Biswas", "Anup Maheshwari", "Eran Gampel", "Jason Van Patten", "Matthew Runion", "Sai Kaki", "Yigal Bogin", "Brian Reitz", "Steve Pritko", "Shahan Najam", "Surya Nambala", "Radhika Chirra", "Rick Welp", "Frank DiMitri", "Felipe Telles", "Amilcar Arvelo", "King Chu", "Ed Seminaro", "Andrew Schram", "Felix Eickhoff", "William Hanson", "Eric Mckeever", "Dinakaran Joseph", "Piyush Chaudhary", "Piyush Shivam", "Puneet Chaudhary", "Wesley Jones", "Robert Guthrie", "Chris Bostic", "Rezaul Islam", "Steve Duersch", "Wayne Sawdon", "John Lewars", "Matthew Klos", "Michael Spriggs", "Bill McMillan", "George Gao", "Ashish Kamra", "Gaurav Singh", "Marc Curry", "Tushar Katarki", "Joe Talerico", "Zenghui Shi", "Sai Sindhur Malleni", "Erwan Gallen"], "abstract": "AI Infrastructure plays a key role in the speed and cost-competitiveness of developing and deploying advanced AI models. The current demand for powerful AI infrastructure for model training is driven by the emergence of generative AI and foundational models, where on occasion thousands of GPUs must cooperate on a single training job for the model to be trained in a reasonable time. Delivering efficient and high-performing Al training requires an end-to-end solution that combines hardware, software and holistic telemetry to cater for multiple types of AI workloads. In this report, we describe IBM's hybrid cloud infrastructure that powers our generative AI model development. This infrastructure includes (1) Vela: an Al-optimized supercomputing capability directly integrated into the IBM Cloud, delivering scalable, dynamic, multi-tenant and geographically distributed infrastructure for large-scale model training and other AI workflow steps and (2) Blue Vela: a large-scale, purpose-built, on-premises hosting environment that is optimized to support our largest and most ambitious Al model training tasks. Vela provides IBM with the dual benefit of high performance for internal use along with the flexibility to adapt to an evolving commercial landscape. Blue Vela provides us with the benefits of rapid development of our largest and most ambitious models, as well as future-proofing against the evolving model landscape in the industry. Taken together, they provide IBM with the ability to rapidly innovate in the development of both Al models and commercial offerings.", "sections": [{"title": "Introduction", "content": "It is hard to overstate the important role of infrastructure in the successful development and efficient deployment of advanced AI models. Infrastructure selection and design impact the cost profile, speed, and efficiency of every stage of the AI life-cycle including data curation, pre-processing, tokenization, model training, adaptation, and inference. The emergence of generative AI and foundational models has led to a dramatic rise in the need for large-scale compute clusters with thousands of GPUs, which can be used together to train large models faster, provided that a sufficiently high-performing network and storage are available. Thus, the availability of a large-scale contiguous and high-performing infrastructure can have a significant impact on the time-to-value in the development of advanced models. In addition to hardware selections, the software we use to manage the infrastructure can also have a significant impact on time-to-value and the overall cost of achieving desired AI outcomes.\nWhile it is known in the industry that state-of-the-art models are generally trained at scale over high-performance infrastructures (e.g. as described in papers from Meta [30], [3]), few publications provide technical details on the design and operations of these systems. This document details IBM's hybrid-cloud-based approach to building world-class infrastructure to support IBM's model development activities at scale. This approach includes (1) The design and integration of AI-optimized supercomputing capabilities directly into IBM's Cloud to deliver scalable, dynamic, multi-tenant and geographically distributed infrastructure for large-scale model training and other AI workflow steps and (2) the design and deployment of large-scale, purpose-built, on-premises hosting environments that are optimized to support our largest and most ambitious AI model training tasks. The former (Vela) provides IBM with the dual benefit of high performance for internal use along with flexibility to adapt to evolving commercial opportunities. The latter (Blue Vela) provides us with the benefits of rapid development of our largest and most ambitious models, as well as future-proofing against the evolving model landscape in the industry."}, {"title": "Vela: An AI-optimized supercomputing infrastructure in IBM Cloud", "content": "In early 2023, IBM shared architectural details and design principles behind Vela, our first cloud-native AI-optimized supercomputer natively integrated into the fabric of IBM Cloud [6]. Some of these details are shown in Figure 1. Vela was designed to be flexible and scalable, capable of training today's large-scale generative Al models, and adaptable to new needs that may arise in the future. It was also designed such that its infrastructure could be efficiently deployed and managed anywhere in the world. The following sections describe some of the technology and innovations that enable Vela's high performance, its flexibility, and its operational resilience."}, {"title": "Vela Architecture", "content": "Vela is a horizontally scalable data center system with two-layer spine-leaf CLOS architecture. To support high availability, which is especially important for production cloud services such as watsonx.ai [2], we built network redundancy into the system. Each port of the network interface card (NIC) is connected to a different top-of-rack (TOR) switch, and each TOR switch is connected via two 100G links to four spine switches providing 1.6TBps cross rack bandwidth and ensures that the system can continue to operate despite failures of any given NIC, TOR, or spine switch.\nAs shown in Figure 1(a), Vela racks have six servers, where the industry norm is between 2 and 4 [4]. Our typical cloud racks are rated to provide 20kW of power to each rack from each of two redundant power distribution units (PDUs) (for a total of 40 kW available). Each GPU server draws a maximum of 6kW of power. Therefore, three servers can be accommodated per rack while preserving full power redundancy. The system, however, can accommodate greater density if a mechanism is in place to address potential failures of a power supply unit (PSU), where the servers are automatically throttled down to avoid overloading the healthy PDU. In order to enable this, we worked with our partners to develop a highly optimized power brake solution. When a PSU fails, the updated server firmware applies the power brake solution in about 2 seconds and the system throttles down to 3.2kW (taking each GPU from roughly 400W to 150W). Healthy PDU circuit breakers can tolerate power surges of up to 5 seconds. This allows us to essentially \u201cover-commit\" the amount of power available to each Vela rack safely. After extensive testing that showed all pertinent components were working safely with six servers per rack, without detrimental impact to the system or the workloads running on Vela, we proceeded to double the GPU density on Vela in 2023 within the same footprint originally allocated for the system in 2022."}, {"title": "Network", "content": "Vela was designed for large model training. To support bigger models, trained over ever-larger data sets, moving faster means using more GPUs per job. As more GPUs compute in parallel, we need commensurate network performance to ensure that GPU-to-GPU communication doesn't become a bottleneck to workload progress. We deliver this high performance networking by enabling two key technologies: Remote Direct Memory Access (RDMA) over Converged Ethernet (RoCE), and GPU-direct RDMA (GDR).\nRDMA allows one processor to access another processor's memory without having to involve either computer's operating systems. This leads to much faster communication between the processors by eliminating as many of the intervening processes as possible. GPU-direct RDMA (GDR) with ROCE, allows GPUs on one system to access the memory of GPUs in another system, using network cards, going over the Ethernet network.\nWe knew that AI training workloads would benefit from low latency RDMA communication paths on the nodes, resulting in more tokens processed per day and faster job completion times compared to TCP based communication. We also knew that achieving these benefits in a cloud native manner over a shared network infrastructure would come with its own challenges. While TCP communication works well over lossy, multi-pathed, Ethernet-based networks, network sensitive RDMA traffic requires quality of service (QoS) mechanisms to be supported by the underlying network infrastructure. We also knew we needed to offload as much network functionality as possible to the available hardware.\nFor ROCE to work in practice in our Ethernet-based production cloud, we needed to implement a robust congestion management mechanism. Hence, we deployed a mechanism that relies on isolating RoCE traffic in a traffic class, monitoring congestion experienced by this class in the network, and notifying the senders to reduce their traffic to alleviate congestion before packet losses occur. This process works by first marking the type of service (ToS) byte in the header of RoCE packets at the source. The first six bits of the ToS byte are reserved for the Differentiated Services Code Point (DSCP) tag and the last two bits are reserved for the Explicit Congestion Notification (ECN) tag. We mark RoCE traffic with a specific DSCP tag and an ECN value to enable the switches to process RoCE traffic via a dedicated queue and to indicate the occurrence of congestion in this queue in the ECN field, respectively. Second, Vela network switches are configured to monitor the buildup in the RoCE queue. They determine if there is congestion using a formula based on the length of the RoCE queue and mark the ECN field accordingly. Third, upon receiving a ROCE packet with congestion indication, the receiver sends a high priority message back to the sender known as the Congestion Notification Packet (CNP). Vela switches are configured to route such packets with high priority. Finally, upon receiving a CNP packet, the sender throttles its traffic injection rate to reduce congestion. The formula that the switches use to detect congestion of RoCE traffic is key to the effectiveness of this mechanism. In the Vela network, we tuned this formula to minimize packet losses.\nIn 2023, we enabled RoCE and GDR on Vela [5]. This upgrade, which was several years of research in the making, required simultaneous changes and enhancements to nearly every part of our cloud stack, from the system firmware to the host operating system, to virtualization, to the network underlay and overlay.\nAt the host level, new kernel drivers were needed with single-root I/O virtualization (SR-IOV) and RDMA support. At the network overlay level, IBM's proprietary software-defined network (SDN) was extended to provide RoCE hardware offloads and marking of ROCE traffic with a specific DSCP label which would be recognized by the network switches for traffic isolation. We also needed to enable ECN in the SDN. Network interface cards (NICs) on the Vela nodes were configured to enable congestion control mechanisms. The network underlay on Vela was configured with a performance-tuned quality-of-service (QoS) policy, which ensured that RoCE traffic identified with the DSCP marking on the packet headers would be isolated in its traffic class and that an appropriate amount of bandwidth would be allotted to RoCE traffic during times of congestion. Vela network switches were configured to monitor the RoCE traffic class and mark the ECN bits of outgoing RoCE packets to indicate congestion when queue buildup occurs. The congestion control mechanism built into the Vela cluster relies on any NIC receiving ECN marked packets to notify the sender to throttle before packet losses occur.\nThe RoCE deployment in the Vela cluster was tuned to perform well with equal cost multiple-path (ECMP) routing. The tuning effort included the QoS profile, the congestion control mechanism, switch buffers, and application properties such as flow characteristics. This end-to-end tuning was based on extensive studies conducted in a Research lab cluster built and configured like the Vela network."}, {"title": "Node Virtualization", "content": "When we designed Vela, we considered two consumption models: either make each node provisionable as bare metal (BM), or enable configuration of the node as a virtual machine (VM). It's generally accepted that bare metal is the path to maximizing AI performance, but VMs provide more flexibility [20]. Going the VM route would enable our service teams to easily provision and re-provision the infrastructure with different software stacks required by different stages of the AI workflow. VMs would make it easy for our support team to flexibly scale AI clusters dynamically and shift resources between workloads of various kinds in a matter of minutes. For example, Vela nodes are constantly moved between training and inferencing clusters depending on the demand for resources. The downside of virtualization, historically, is that it reduces node performance [31]. The research question to answer was: how close to bare metal performance could we achieve inside a VM?\nPCI-E device passthrough is the mechanism by which virtual machines directly access the physical devices installed on the host system. As a result, applications running inside VMs can take advantage of the full power of devices like GPUs and enable performant execution of applications such as gaming, CAD, and machine learning. We hypothesized that we could approach the bare metal performance of our GPU nodes by optimizing the way we pass the GPUs and virtual network functions into our VMs using Linux KVM.\nAs shown in Figure 1(b), modern AI compute nodes tend to have complex intra-node topology with multiple PCI-E switches connecting GPUs, network cards, and CPUs. These nodes are therefore challenging to virtualize with no performance loss. Before we began this optimization, we observed poor out-of-the-box AI workload performance and network performance (see Figure 5 VM Default columns). We provisioned and compared KVM-QEMU-based VMs for TCP, RoCE, and GDR communication models with default configuration and optimized configuration. The optimized configuration achieves 2-10x improvement in network performance over the default configuration (Figure 5).\nEnabling a performant VM-based execution environment for AI workloads required configuration changes at different system layers [11]. Specific optimizations were made in\n\u2022\tthe system BIOS (Virtual Machine Extensions like enable IOMMU, ACS, and SR-IOV support),\n\u2022\tthe network adapter (disable relaxed PCI ordering, increase maximum accumulative outstanding read bytes, and enable selective repeat, direct access to ATS from the NIC to GPU buffers using PCI-E peer-to-peer transactions, and ATS),\n\u2022\tthe hypervisor (enable NVFs, huge pages, ACS on the PCI controllers, and ATS on the NVFs, and increase maximum PCI read request size to 4KB),\n\u2022\tthe guest XML (enable huge pages, NUMA domains, Device-NUMA mapping, host-physical-bit model for large memory VMs, and ATS on PCI controllers), and\n\u2022\tthe guest operating system configurations (increase maximum PCI read request size to 4KB).\nWe implemented these ideas on a system with 8 NVIDIA A100-80GPUs and measured performance of various microbenchmarks and workloads on bare metal and virtual machines. Our results shows that the optimized VM configuration resulted in close to bare metal performance across all the experiments. For example, we ran the NMT-12 [15] model training job on single-node in VM and BM with the same configuration to demonstrate the virtualization overhead on real-world applications. The result shows that we can achieve 147K words-per-second (WPS) on BM and 140K WPS inside the VM, i.e., a virtualization overhead of about 5%. We also evaluated BM and VM performance with Cuda Samples [13], BERT-Large [7], MegaTron [22], and T5 11B [16], and the overhead of VM ranges from \"less than 0%\" to a maximum of 5%. By \"less than 0%\" we mean that VM execution can actually be faster; this is due to configurations such as large pages typically set for VMs, which is not the case in BM.\nVirtualization also plays an important role in multi-node communication. To evaluate the virtualization optimizations for network, we used two compute nodes, each with 8 NVIDIA A100-80GB GPUs and 4 Mellanox ConnectX-6 Dx dual-port cards (i.e. 800 Gbps aggregate bandwidth), with the IBM Cloud KVM hypervisor (Linux 5.4 at the time) and guest running Ubuntu 20.04 (with Linux 5.4) operating system, and the latest software stack from NVIDIA and Mellanox. To benchmark the network performance, we executed all_reduce_perf from nccl-tests (a micro-benchmark suite provided by NVIDIA) to evaluate the network performance with TCP, RoCE, and NVIDIA GPUDirect RoCE protocols.  shows that the optimizations/changes we applied at different system layers improved the network performance significantly. We also executed the distributed NMT-12 model training job and demonstrated that we can achieve similar performance in VM and BM environments. We tested several other distributed AI training jobs, and the performance loss is less than 5% in general.\nIn summary, we devised a way to expose all of the capabilities on the node (GPUs, CPUs, networking, and storage) into the VM and develop optimized VM configurations so that the virtualization overhead is less than 5%, which is the lowest overhead in the industry that we're aware of. These include configuring the bare-metal host for virtualization with support for Virtual Machine Extensions (VMX), SR-IOV, and huge pages. We also needed to faithfully represent all devices and their connectivity inside the VM, such as which network cards are connected to which CPUs and GPUs, how GPUs are connected to the CPU sockets, and how GPUs are connected to each other. These optimizations are part of our cloud control plane which creates the GPU VMs on the bare metal hosts. These, along with other hardware and software configurations, enabled our system to achieve close to bare metal performance."}, {"title": "Storage", "content": "During a large-scale model training job, several data- and I/O-intensive steps occur that can become a bottleneck to the overall training job progression in the absence of an optimized storage solution. The first one occurs when data, originally stored in an object store, must be accessed by the GPUs so that they can begin to compute. Loading data directly from object storage to each GPU's memory is slow due to the limited IOPs supported by typical cloud object storage. This bottleneck occurs both at the very beginning of the training job as well as every time the job stops and needs to be re-started again. As discussed later, component failures make this starting and stopping of jobs inevitable. A second I/O-intensive step of a model training job occurs during model \"checkpointing\". At periodic intervals during training, a record of the current set of model weights is sent to object storage, similar to occasionally \"saving\" the collective work of the GPUs. In both of these examples, a high-performance file system can be inserted between the object storage and the GPUs to act as an intermediating caching mechanism. In doing so, the data can be loaded into the GPUs much faster to start (or re-start) a training job, and model weights can be checkpointed to the file system at a much faster rate than when checkpointing directly to object storage. Thanks to unique technology in the file system we use, the checkpointed data can then be asynchronously sent to object storage but in a way that does not gate progress of the training job.\nTo realize these advantages, we use IBM Spectrum Scale [28], which we will refer to as \"Scale\" in the following text. At the core of Scale is IBM's \"General Parallel File System\" (GPFS [19]), a highly successful and proven parallel file system with a strong high-performance computing heritage.\nScale is deployed in Vela using a disaggregated storage model. The dedicated Scale storage cluster consists of tens of IBM Cloud Virtual Server Instances (VSIs) with two 1TB virtual block volumes attached to each instance. The virtual block volumes are hosted on a next-generation cloud-native and highly performant block storage service in IBM Cloud that can meet the high throughput requirements of model training workloads. A single file system is created using all attached devices. Hence the total capacity of the file system accessible to Vela is hundreds of terabytes, which can be extended at any time as needed to petabytes.\nAs stated above, the majority of the data used by the training jobs running on Vela originates in IBM's Cloud Object Storage (COS). Similarly, some data produced on Vela, like model checkpoints, need to end up in COS for cost efficiency purposes. We configure Active File Management(AFM) technology [23] to transparently connect filesets to object storage buckets. File system namespaces represent objects in buckets as files and brings data from the object storage into the file system on demand. When a file is written to the file system, AFM eventually moves it to object storage. This means that our 140TB file system capacity is essentially acting as a read-write cache over potentially petabytes of cost-effective object storage. When the cache is full, AFM automatically evicts the data and metadata that was not recently used.\nBefore we deployed this storage solution based on Scale, AI researchers using Vela could either use IBM COS directly or an NFS file system that was deployed for Vela. Compared to NFS performance, our Scale file system achieves a near 40x read bandwidth speedup (1GB/s vs 40GB/s with Scale), which directly helps with input data read operations. Also compared to IBM COS bandwidth, the Scale file system achieves a near 3x write bandwidth speedup (5GB/s vs 15GB/s with Scale), which accelerates the checkpoint and other data write operations. How do these numbers translate to real workload performance? . Several conclusions can be drawn from this comparison:\n\u2022\tBecause of the random I/O access patterns, concurrent accesses from multiple reads, and limited data reuse, the iteration time with NFS takes many steps to reach a steady state. In this experiment, it took more than 300 iterations. Because of the high bandwidth and low latency performance of Scale, and because of its ability to handle concurrent accesses, the iteration time reaches a steady state almost instantaneously.\n\u2022\tDuring steady state training, multiple clients access the file system at the same time. Since NFS has limited concurrency support, the step time tends to vary by almost 50% (e.g. from 6 seconds to 9 seconds). In the case of Scale, the step times varies between 4.8 seconds and 5.2 seconds, which is less than 10% variation.\n\u2022\tThanks to the overall higher performance of Scale, the step of the AI job is more than 10% faster on average than using NFS, which directly reduces AI model training times by 10%."}, {"title": "Vela Software Stack", "content": "Vela is operated by IBM Cloud as IaaS (Infrastructure as a Service). On top of this, IBM Research and IBM Cloud manage different Red Hat OpenShift clusters which are used for tasks that span the entire AI lifecycle, from data preparation to model training, adaptation, and ultimately model serving.\nLeveraging the OpenShift platform for these use cases offers several advantages to Al researchers, AI service providers, and platform administrators. First, it allows AI researchers to bring their own container with software that is necessary to run their workloads. Some of our AI researchers use stable PyTorch versions, others use the Megatron framework, and others use nightly builds of the latest software. Bringing your own container allows AI researchers to define and run their own experiments on the platform without needing explicit coordination with system administrators. OpenShift also simplifies system-level monitoring and debugging using a rich set of out-of-the-box capabilities and a collection of specialized operators that will be described below, as well as automated job restart orchestrated by our job scheduler in the event of failures.\nFor AI infrastructure service providers such as our cloud site reliability engineers (SREs), OpenShift offers a single platform to allocate appropriately-sized infrastructure to various services, for example hundreds of GPUs for training, individual GPU nodes for fine tuning, and a few GPUs for inference (which are further partitionable). This enables optimal use of resources for the workload and improves cost-efficiency of the infrastructure.\nFor Platform administrators, OpenShift provides constructs (namespaces, quotas) to provision and manage resources among multiple users and projects with fine grained access controls. Platform level APIs allow the administrators to scale the system up and down based on resource availability and workload needs. For example, in Vela, resources are moved between OpenShift clusters for AI training and inference services based on business needs.\nWhile the OpenShift platform already comes with a rich collection of capabilities to expose high performance infrastructure to workloads (such as a GPU Operator, Network operator) and a sophisticated ecosystem of tools for system management (such as monitoring, logging, alerting), in the section below we will discuss some key capabilities we have developed based on the specific needs of workloads running on Vela and how we leverage them along with IBM Cloud capabilities to substantially enhance monitoring and diagnostic functions."}, {"title": "OpenShift Operators", "content": "Autopilot: IBM Research has created and open-sourced a tool called Autopilot [17], which is a cloud native observability tool implemented as a set of daemons, each one running on a GPU node. It implements a set of health checks that evaluate the status of the system. These health checks focus mainly on subtle/software issues, discussed more in the next section (i.e., row-remapping or PCI-E link degradation), but also run connectivity tests (i.e., ping, iperf) to verify that NICs are reachable. At the time of publication, the complete list of health checks includes the following: PCI-E bandwidth measurement between host and device, remapped rows evaluation, GPU power throttle enablement, all DCGM diagnostics, GPU memory bandwidth evaluation through DGEMM and DAXPY workloads, ping and iperf. Any subset of health checks can be set up to run periodically and automatically on all nodes, but the user can also run them manually if needed. More extensive tests, namely DCGM diagnostics level 3, are also executed automatically only on nodes that have free GPUs. We added this deeper analysis because there have been episodes of subtle issues that are only revealed after running level 3 DCGM diagnostics, therefore we decided to run these more invasive health checks proactively and flag nodes with an ERR/PASS flag. Results from health checks are exported as Prometheus Gauges, so that users and admins can easily check the status of the system on Grafana. This has significantly accelerated the discovery of issues in the cluster, and enabled proactive remediation.\nMulti-NIC CNI: As discussed in the previous sections, Vela GPU nodes have multiple 100G network interfaces and IBM Cloud uses single root I/O virtualization (SR-IOV) [9] to expose multiple virtual interfaces per each physical interface. Multi-NIC CNI [18] is a container-native interface built on top of Multus CNI with several important functions: 1. It discovers all of the interfaces on each host and handles them as a pool, 2. it assigns virtual interfaces for pods on top of the SR-IOV interfaces for TCP communication without encapsulation, and 3. it passes physical SR-IOV interfaces into the pods for GDR communication. These actions ensure that the workloads can achieve line rate network performance for TCP and GDR communication while code is running inside the pod.\nCNSA: A Scale client cluster runs across Vela's GPU nodes in container-native mode leveraging the CNSA edition of Scale [24]. It uses Kubernetes operators to deploy and manage Scale in a cloud-native fashion as well as a CSI Plugin for provisioning and attaching persistent volumes based on Scale. The client cluster does not contain any locally attached storage devices and instead performs remote mount of the file system in the storage cluster [25]. Such an architecture allows compute and storage clusters to grow and shrink independently as workload requirements change.\nData scientists gain access to the Scale file system using the traditional Kubernetes process. They create a Persistent Volume Claim (PVC) describing the size of the volume and the storage class referring to Scale. By invoking Scale's REST API on the storage cluster, Scale's CSI plugin creates a new fileset [27], which later can be attached as a volume to any pod running in OCP."}, {"title": "Workload Performance on OpenShift", "content": "One of the concerns that is often expressed regarding the use of the OpenShift platform for performance-sensitive workloads is that it might introduce a resource overhead. We studied this by comparing the performance of various workloads running on OpenShift to those running directly in a virtual machines and found that the workload performance overhead is within the margin of error (i.e., under 5%). The smaller batch sizes result in more communication per iteration, which allows the study of network overhead introduced by OpenShift. The larger batches result in more computation per iteration (hence larger iteration times), which allows us to study the container virtualization overhead. Across all of the batch sizes, the iteration times with OpenShift are within 4% of the iteration time with VMs. Note that the step times are the same or even better with OpenShift compared to VMs in some cases. In the prior section on node virtualization, we summarized the overall virtualization overhead as varying between nearly 0% to up to 5%; the addition of an OpenShift layer does not meaningfully change this. The overall overhead is still contained within roughly 5% relative to bare metal.\nOpenShift does in fact run more processes on the node compared to a typical HPC scheduler such as monitoring agents, network operators, logging agents, etc but their cumulative CPU usage is within 2% and cumulative memory usage is under 4%. This a reasonably small overhead and it can be ignored for all practical purposes.\nWhen Vela first came online in 2022, we initially deployed with a traditional HPC scheduler called IBM Spectrum LSF and only a small (2 node) OpenShift cluster. We started to add capabilities like high performance networking using Multi-NIC CNI, optimized scheduling using the MCAD scheduler [1], container-native storage using IBM Storage Scale CNSA, advanced monitoring and automated health checks using Autopilot, and grew the platform over nine months to support thousands of GPUs under a single cluster. To the best of our knowledge, this is the largest OpenShift cluster with GPUs in production anywhere in the world.\nWhile a single cluster is good for resource consolidation, training workloads and inference workloads have distinct security and scaling requirements so we also deploy additional clusters on Vela for production watsonx.ai inference services."}, {"title": "Operational efficiency and resilience", "content": "It is easy to understand the direct relationship between having a high-performance infrastructure and training a model as quickly as possible. Equally important is the need for technology to enable operational efficiency. This includes supporting dynamicity and variability in the types of experiments that researchers want to run as well as the need for technology to detect and address a whole host of inevitable failures that are guaranteed to occur when training large models on a large and complex infrastructure. This section details an operational and technological approach for rapid experimentation and rapid failure resolution, which are both equally critical for AI model training agility."}, {"title": "Addressing component failures", "content": "In distributed training", "1": "n\u2022\tClear hardware failures", "failures": "We classify a failure as a clear hardware failure when the host crashes. We observed that on average about 2% of our hosts crashed per month over the past 2 years and as many as 5% in the worst case. A primary source for these failures is the failure of the board that holds the GPUs (called an HGX baseboard). A second source"}, {"title": "The infrastructure powering IBM's Gen AI model development", "authors": ["Talia Gershon", "Seetharami Seelam", "Brian Belgodere", "Milton Bonilla", "Lan Hoang", "Danny Barnett", "I-Hsin Chung", "Apoorve Mohan", "Ming-Hung Chen", "Lixiang Luo", "Robert Walkup", "Constantinos Evangelinos", "Shweta Salaria", "Marc Dombrowa", "Yoonho Park", "Apo Kayi", "Liran Schour", "Alim Alim", "Ali Sydney", "Pavlos Maniotis", "Laurent Schares", "Bernard Metzler", "Bengi Karacali-Akyamac", "Sophia Wen", "Tatsuhiro Chiba", "Sunyanan Choochotkaew", "Takeshi Yoshimura", "Claudia Misale", "Tonia Elengikal", "Kevin O'Connor", "Zhuoran Liu", "Richard Molina", "Lars Schneidenbach", "James Caden", "Christopher Laibinis", "Carlos Fonseca", "Vasily Tarasov", "Swaminathan Sundararaman", "Frank Schmuck", "Scott Guthridge", "Jeremy Cohn", "Marc Eshel", "Paul Muench", "Runyu Liu", "William Pointer", "Drew Wyskida", "Bob Krull", "Ray Rose", "Brent Wolfe", "William Cornejo", "John Walter", "Colm Malone", "Clifford Perucci", "Frank Franco", "Nigel Hinds", "Bob Calio", "Pavel Druyan", "Robert Kilduff", "John Kienle", "Connor McStay", "Andrew Figueroa", "Matthew Connolly", "Edie Fost", "Gina Roma", "Jake Fonseca", "Ido Levy", "Michele Payne", "Ryan Schenkel", "Amir Malki", "Lion Schneider", "Aniruddha Narkhede", "Shekeba Moshref", "Alexandra Kisin", "Olga Dodin", "Bill Rippon", "Henry Wrieth", "John Ganci", "Johnny Colino", "Donna Habeger-Rose", "Rakesh Pandey", "Aditya Gidh", "Aditya Gaur", "Dennis Patterson", "Samsuddin Salmani", "Rambilas Varma", "Rumana Rumana", "Shubham Sharma", "Aditya Gaur", "Mayank Mishra", "Rameswar Panda", "Aditya Prasad", "Matt Stallone", "Gaoyuan Zhang", "Yikang Shen", "David Cox", "Ruchir Puri", "Dakshi Agrawal", "Drew Thorstensen", "Joel Belog", "Brent Tang", "Saurabh Kumar Gupta", "Amitabha Biswas", "Anup Maheshwari", "Eran Gampel", "Jason Van Patten", "Matthew Runion", "Sai Kaki", "Yigal Bogin", "Brian Reitz", "Steve Pritko", "Shahan Najam", "Surya Nambala", "Radhika Chirra", "Rick Welp", "Frank DiMitri", "Felipe Telles", "Amilcar Arvelo", "King Chu", "Ed Seminaro", "Andrew Schram", "Felix Eickhoff", "William Hanson", "Eric Mckeever", "Dinakaran Joseph", "Piyush Chaudhary", "Piyush Shivam", "Puneet Chaudhary", "Wesley Jones", "Robert Guthrie", "Chris Bostic", "Rezaul Islam", "Steve Duersch", "Wayne Sawdon", "John Lewars", "Matthew Klos", "Michael Spriggs", "Bill McMillan", "George Gao", "Ashish Kamra", "Gaurav Singh", "Marc Curry", "Tushar Katarki", "Joe Talerico", "Zenghui Shi", "Sai Sindhur Malleni", "Erwan Gallen"], "abstract": "AI Infrastructure plays a key role in the speed and cost-competitiveness of developing and deploying advanced AI models. The current demand for powerful AI infrastructure for model training is driven by the emergence of generative AI and foundational models, where on occasion thousands of GPUs must cooperate on a single training job for the model to be trained in a reasonable time. Delivering efficient and high-performing Al training requires an end-to-end solution that combines hardware, software and holistic telemetry to cater for multiple types of AI workloads. In this report, we describe IBM's hybrid cloud infrastructure that powers our generative AI model development. This infrastructure includes (1) Vela: an Al-optimized supercomputing capability directly integrated into the IBM Cloud, delivering scalable, dynamic, multi-tenant and geographically distributed infrastructure for large-scale model training and other AI workflow steps and (2) Blue Vela: a large-scale, purpose-built, on-premises hosting environment that is optimized to support our largest and most ambitious Al model training tasks. Vela provides IBM with the dual benefit of high performance for internal use along with the flexibility to adapt to an evolving commercial landscape. Blue Vela provides us with the benefits of rapid development of our largest and most ambitious models, as well as future-proofing against the evolving model landscape in the industry. Taken together, they provide IBM with the ability to rapidly innovate in the development of both Al models and commercial offerings.", "sections": [{"title": "Introduction", "content": "It is hard to overstate the important role of infrastructure in the successful development and efficient deployment of advanced AI models. Infrastructure selection and design impact the cost profile, speed, and efficiency of every stage of the AI life-cycle including data curation, pre-processing, tokenization, model training, adaptation, and inference. The emergence of generative AI and foundational models has led to a dramatic rise in the need for large-scale compute clusters with thousands of GPUs, which can be used together to train large models faster, provided that a sufficiently high-performing network and storage are available. Thus, the availability of a large-scale contiguous and high-performing infrastructure can have a significant impact on the time-to-value in the development of advanced models. In addition to hardware selections, the software we use to manage the infrastructure can also have a significant impact on time-to-value and the overall cost of achieving desired AI outcomes.\nWhile it is known in the industry that state-of-the-art models are generally trained at scale over high-performance infrastructures (e.g. as described in papers from Meta [30], [3]), few publications provide technical details on the design and operations of these systems. This document details IBM's hybrid-cloud-based approach to building world-class infrastructure to support IBM's model development activities at scale. This approach includes (1) The design and integration of AI-optimized supercomputing capabilities directly into IBM's Cloud to deliver scalable, dynamic, multi-tenant and geographically distributed infrastructure for large-scale model training and other AI workflow steps and (2) the design and deployment of large-scale, purpose-built, on-premises hosting environments that are optimized to support our largest and most ambitious AI model training tasks. The former (Vela) provides IBM with the dual benefit of high performance for internal use along with flexibility to adapt to evolving commercial opportunities. The latter (Blue Vela) provides us with the benefits of rapid development of our largest and most ambitious models, as well as future-proofing against the evolving model landscape in the industry."}, {"title": "Vela: An AI-optimized supercomputing infrastructure in IBM Cloud", "content": "In early 2023, IBM shared architectural details and design principles behind Vela, our first cloud-native AI-optimized supercomputer natively integrated into the fabric of IBM Cloud [6]. Some of these details are shown in Figure 1. Vela was designed to be flexible and scalable, capable of training today's large-scale generative Al models, and adaptable to new needs that may arise in the future. It was also designed such that its infrastructure could be efficiently deployed and managed anywhere in the world. The following sections describe some of the technology and innovations that enable Vela's high performance, its flexibility, and its operational resilience."}, {"title": "Vela Architecture", "content": "Vela is a horizontally scalable data center system with two-layer spine-leaf CLOS architecture. To support high availability, which is especially important for production cloud services such as watsonx.ai [2], we built network redundancy into the system. Each port of the network interface card (NIC) is connected to a different top-of-rack (TOR) switch, and each TOR switch is connected via two 100G links to four spine switches providing 1.6TBps cross rack bandwidth and ensures that the system can continue to operate despite failures of any given NIC, TOR, or spine switch.\nAs shown in Figure 1(a), Vela racks have six servers, where the industry norm is between 2 and 4 [4]. Our typical cloud racks are rated to provide 20kW of power to each rack from each of two redundant power distribution units (PDUs) (for a total of 40 kW available). Each GPU server draws a maximum of 6kW of power. Therefore, three servers can be accommodated per rack while preserving full power redundancy. The system, however, can accommodate greater density if a mechanism is in place to address potential failures of a power supply unit (PSU), where the servers are automatically throttled down to avoid overloading the healthy PDU. In order to enable this, we worked with our partners to develop a highly optimized power brake solution. When a PSU fails, the updated server firmware applies the power brake solution in about 2 seconds and the system throttles down to 3.2kW (taking each GPU from roughly 400W to 150W). Healthy PDU circuit breakers can tolerate power surges of up to 5 seconds. This allows us to essentially \u201cover-commit\" the amount of power available to each Vela rack safely. After extensive testing that showed all pertinent components were working safely with six servers per rack, without detrimental impact to the system or the workloads running on Vela, we proceeded to double the GPU density on Vela in 2023 within the same footprint originally allocated for the system in 2022."}, {"title": "Network", "content": "Vela was designed for large model training. To support bigger models, trained over ever-larger data sets, moving faster means using more GPUs per job. As more GPUs compute in parallel, we need commensurate network performance to ensure that GPU-to-GPU communication doesn't become a bottleneck to workload progress. We deliver this high performance networking by enabling two key technologies: Remote Direct Memory Access (RDMA) over Converged Ethernet (RoCE), and GPU-direct RDMA (GDR).\nRDMA allows one processor to access another processor's memory without having to involve either computer's operating systems. This leads to much faster communication between the processors by eliminating as many of the intervening processes as possible. GPU-direct RDMA (GDR) with ROCE, allows GPUs on one system to access the memory of GPUs in another system, using network cards, going over the Ethernet network.\nWe knew that AI training workloads would benefit from low latency RDMA communication paths on the nodes, resulting in more tokens processed per day and faster job completion times compared to TCP based communication. We also knew that achieving these benefits in a cloud native manner over a shared network infrastructure would come with its own challenges. While TCP communication works well over lossy, multi-pathed, Ethernet-based networks, network sensitive RDMA traffic requires quality of service (QoS) mechanisms to be supported by the underlying network infrastructure. We also knew we needed to offload as much network functionality as possible to the available hardware.\nFor ROCE to work in practice in our Ethernet-based production cloud, we needed to implement a robust congestion management mechanism. Hence, we deployed a mechanism that relies on isolating RoCE traffic in a traffic class, monitoring congestion experienced by this class in the network, and notifying the senders to reduce their traffic to alleviate congestion before packet losses occur. This process works by first marking the type of service (ToS) byte in the header of RoCE packets at the source. The first six bits of the ToS byte are reserved for the Differentiated Services Code Point (DSCP) tag and the last two bits are reserved for the Explicit Congestion Notification (ECN) tag. We mark RoCE traffic with a specific DSCP tag and an ECN value to enable the switches to process RoCE traffic via a dedicated queue and to indicate the occurrence of congestion in this queue in the ECN field, respectively. Second, Vela network switches are configured to monitor the buildup in the RoCE queue. They determine if there is congestion using a formula based on the length of the RoCE queue and mark the ECN field accordingly. Third, upon receiving a ROCE packet with congestion indication, the receiver sends a high priority message back to the sender known as the Congestion Notification Packet (CNP). Vela switches are configured to route such packets with high priority. Finally, upon receiving a CNP packet, the sender throttles its traffic injection rate to reduce congestion. The formula that the switches use to detect congestion of RoCE traffic is key to the effectiveness of this mechanism. In the Vela network, we tuned this formula to minimize packet losses.\nIn 2023, we enabled RoCE and GDR on Vela [5]. This upgrade, which was several years of research in the making, required simultaneous changes and enhancements to nearly every part of our cloud stack, from the system firmware to the host operating system, to virtualization, to the network underlay and overlay.\nAt the host level, new kernel drivers were needed with single-root I/O virtualization (SR-IOV) and RDMA support. At the network overlay level, IBM's proprietary software-defined network (SDN) was extended to provide RoCE hardware offloads and marking of ROCE traffic with a specific DSCP label which would be recognized by the network switches for traffic isolation. We also needed to enable ECN in the SDN. Network interface cards (NICs) on the Vela nodes were configured to enable congestion control mechanisms. The network underlay on Vela was configured with a performance-tuned quality-of-service (QoS) policy, which ensured that RoCE traffic identified with the DSCP marking on the packet headers would be isolated in its traffic class and that an appropriate amount of bandwidth would be allotted to RoCE traffic during times of congestion. Vela network switches were configured to monitor the RoCE traffic class and mark the ECN bits of outgoing RoCE packets to indicate congestion when queue buildup occurs. The congestion control mechanism built into the Vela cluster relies on any NIC receiving ECN marked packets to notify the sender to throttle before packet losses occur.\nThe RoCE deployment in the Vela cluster was tuned to perform well with equal cost multiple-path (ECMP) routing. The tuning effort included the QoS profile, the congestion control mechanism, switch buffers, and application properties such as flow characteristics. This end-to-end tuning was based on extensive studies conducted in a Research lab cluster built and configured like the Vela network."}, {"title": "Node Virtualization", "content": "When we designed Vela, we considered two consumption models: either make each node provisionable as bare metal (BM), or enable configuration of the node as a virtual machine (VM). It's generally accepted that bare metal is the path to maximizing AI performance, but VMs provide more flexibility [20]. Going the VM route would enable our service teams to easily provision and re-provision the infrastructure with different software stacks required by different stages of the AI workflow. VMs would make it easy for our support team to flexibly scale AI clusters dynamically and shift resources between workloads of various kinds in a matter of minutes. For example, Vela nodes are constantly moved between training and inferencing clusters depending on the demand for resources. The downside of virtualization, historically, is that it reduces node performance [31]. The research question to answer was: how close to bare metal performance could we achieve inside a VM?\nPCI-E device passthrough is the mechanism by which virtual machines directly access the physical devices installed on the host system. As a result, applications running inside VMs can take advantage of the full power of devices like GPUs and enable performant execution of applications such as gaming, CAD, and machine learning. We hypothesized that we could approach the bare metal performance of our GPU nodes by optimizing the way we pass the GPUs and virtual network functions into our VMs using Linux KVM.\nAs shown in Figure 1(b), modern AI compute nodes tend to have complex intra-node topology with multiple PCI-E switches connecting GPUs, network cards, and CPUs. These nodes are therefore challenging to virtualize with no performance loss. Before we began this optimization, we observed poor out-of-the-box AI workload performance and network performance (see Figure 5 VM Default columns). We provisioned and compared KVM-QEMU-based VMs for TCP, RoCE, and GDR communication models with default configuration and optimized configuration. The optimized configuration achieves 2-10x improvement in network performance over the default configuration (Figure 5).\nEnabling a performant VM-based execution environment for AI workloads required configuration changes at different system layers [11]. Specific optimizations were made in\n\u2022\tthe system BIOS (Virtual Machine Extensions like enable IOMMU, ACS, and SR-IOV support),\n\u2022\tthe network adapter (disable relaxed PCI ordering, increase maximum accumulative outstanding read bytes, and enable selective repeat, direct access to ATS from the NIC to GPU buffers using PCI-E peer-to-peer transactions, and ATS),\n\u2022\tthe hypervisor (enable NVFs, huge pages, ACS on the PCI controllers, and ATS on the NVFs, and increase maximum PCI read request size to 4KB),\n\u2022\tthe guest XML (enable huge pages, NUMA domains, Device-NUMA mapping, host-physical-bit model for large memory VMs, and ATS on PCI controllers), and\n\u2022\tthe guest operating system configurations (increase maximum PCI read request size to 4KB).\nWe implemented these ideas on a system with 8 NVIDIA A100-80GPUs and measured performance of various microbenchmarks and workloads on bare metal and virtual machines. Our results shows that the optimized VM configuration resulted in close to bare metal performance across all the experiments. For example, we ran the NMT-12 [15] model training job on single-node in VM and BM with the same configuration to demonstrate the virtualization overhead on real-world applications. The result shows that we can achieve 147K words-per-second (WPS) on BM and 140K WPS inside the VM, i.e., a virtualization overhead of about 5%. We also evaluated BM and VM performance with Cuda Samples [13], BERT-Large [7], MegaTron [22], and T5 11B [16], and the overhead of VM ranges from \"less than 0%\" to a maximum of 5%. By \"less than 0%\" we mean that VM execution can actually be faster; this is due to configurations such as large pages typically set for VMs, which is not the case in BM.\nVirtualization also plays an important role in multi-node communication. To evaluate the virtualization optimizations for network, we used two compute nodes, each with 8 NVIDIA A100-80GB GPUs and 4 Mellanox ConnectX-6 Dx dual-port cards (i.e. 800 Gbps aggregate bandwidth), with the IBM Cloud KVM hypervisor (Linux 5.4 at the time) and guest running Ubuntu 20.04 (with Linux 5.4) operating system, and the latest software stack from NVIDIA and Mellanox. To benchmark the network performance, we executed all_reduce_perf from nccl-tests (a micro-benchmark suite provided by NVIDIA) to evaluate the network performance with TCP, RoCE, and NVIDIA GPUDirect RoCE protocols.  shows that the optimizations/changes we applied at different system layers improved the network performance significantly. We also executed the distributed NMT-12 model training job and demonstrated that we can achieve similar performance in VM and BM environments. We tested several other distributed AI training jobs, and the performance loss is less than 5% in general.\nIn summary, we devised a way to expose all of the capabilities on the node (GPUs, CPUs, networking, and storage) into the VM and develop optimized VM configurations so that the virtualization overhead is less than 5%, which is the lowest overhead in the industry that we're aware of. These include configuring the bare-metal host for virtualization with support for Virtual Machine Extensions (VMX), SR-IOV, and huge pages. We also needed to faithfully represent all devices and their connectivity inside the VM, such as which network cards are connected to which CPUs and GPUs, how GPUs are connected to the CPU sockets, and how GPUs are connected to each other. These optimizations are part of our cloud control plane which creates the GPU VMs on the bare metal hosts. These, along with other hardware and software configurations, enabled our system to achieve close to bare metal performance."}, {"title": "Storage", "content": "During a large-scale model training job, several data- and I/O-intensive steps occur that can become a bottleneck to the overall training job progression in the absence of an optimized storage solution. The first one occurs when data, originally stored in an object store, must be accessed by the GPUs so that they can begin to compute. Loading data directly from object storage to each GPU's memory is slow due to the limited IOPs supported by typical cloud object storage. This bottleneck occurs both at the very beginning of the training job as well as every time the job stops and needs to be re-started again. As discussed later, component failures make this starting and stopping of jobs inevitable. A second I/O-intensive step of a model training job occurs during model \"checkpointing\". At periodic intervals during training, a record of the current set of model weights is sent to object storage, similar to occasionally \"saving\" the collective work of the GPUs. In both of these examples, a high-performance file system can be inserted between the object storage and the GPUs to act as an intermediating caching mechanism. In doing so, the data can be loaded into the GPUs much faster to start (or re-start) a training job, and model weights can be checkpointed to the file system at a much faster rate than when checkpointing directly to object storage. Thanks to unique technology in the file system we use, the checkpointed data can then be asynchronously sent to object storage but in a way that does not gate progress of the training job.\nTo realize these advantages, we use IBM Spectrum Scale [28], which we will refer to as \"Scale\" in the following text. At the core of Scale is IBM's \"General Parallel File System\" (GPFS [19]), a highly successful and proven parallel file system with a strong high-performance computing heritage.\nScale is deployed in Vela using a disaggregated storage model. The dedicated Scale storage cluster consists of tens of IBM Cloud Virtual Server Instances (VSIs) with two 1TB virtual block volumes attached to each instance. The virtual block volumes are hosted on a next-generation cloud-native and highly performant block storage service in IBM Cloud that can meet the high throughput requirements of model training workloads. A single file system is created using all attached devices. Hence the total capacity of the file system accessible to Vela is hundreds of terabytes, which can be extended at any time as needed to petabytes.\nAs stated above, the majority of the data used by the training jobs running on Vela originates in IBM's Cloud Object Storage (COS). Similarly, some data produced on Vela, like model checkpoints, need to end up in COS for cost efficiency purposes. We configure Active File Management(AFM) technology [23] to transparently connect filesets to object storage buckets. File system namespaces represent objects in buckets as files and brings data from the object storage into the file system on demand. When a file is written to the file system, AFM eventually moves it to object storage. This means that our 140TB file system capacity is essentially acting as a read-write cache over potentially petabytes of cost-effective object storage. When the cache is full, AFM automatically evicts the data and metadata that was not recently used.\nBefore we deployed this storage solution based on Scale, AI researchers using Vela could either use IBM COS directly or an NFS file system that was deployed for Vela. Compared to NFS performance, our Scale file system achieves a near 40x read bandwidth speedup (1GB/s vs 40GB/s with Scale), which directly helps with input data read operations. Also compared to IBM COS bandwidth, the Scale file system achieves a near 3x write bandwidth speedup (5GB/s vs 15GB/s with Scale), which accelerates the checkpoint and other data write operations. How do these numbers translate to real workload performance? . Several conclusions can be drawn from this comparison:\n\u2022\tBecause of the random I/O access patterns, concurrent accesses from multiple reads, and limited data reuse, the iteration time with NFS takes many steps to reach a steady state. In this experiment, it took more than 300 iterations. Because of the high bandwidth and low latency performance of Scale, and because of its ability to handle concurrent accesses, the iteration time reaches a steady state almost instantaneously.\n\u2022\tDuring steady state training, multiple clients access the file system at the same time. Since NFS has limited concurrency support, the step time tends to vary by almost 50% (e.g. from 6 seconds to 9 seconds). In the case of Scale, the step times varies between 4.8 seconds and 5.2 seconds, which is less than 10% variation.\n\u2022\tThanks to the overall higher performance of Scale, the step of the AI job is more than 10% faster on average than using NFS, which directly reduces AI model training times by 10%."}, {"title": "Vela Software Stack", "content": "Vela is operated by IBM Cloud as IaaS (Infrastructure as a Service). On top of this, IBM Research and IBM Cloud manage different Red Hat OpenShift clusters which are used for tasks that span the entire AI lifecycle, from data preparation to model training, adaptation, and ultimately model serving.\nLeveraging the OpenShift platform for these use cases offers several advantages to Al researchers, AI service providers, and platform administrators. First, it allows AI researchers to bring their own container with software that is necessary to run their workloads. Some of our AI researchers use stable PyTorch versions, others use the Megatron framework, and others use nightly builds of the latest software. Bringing your own container allows AI researchers to define and run their own experiments on the platform without needing explicit coordination with system administrators. OpenShift also simplifies system-level monitoring and debugging using a rich set of out-of-the-box capabilities and a collection of specialized operators that will be described below, as well as automated job restart orchestrated by our job scheduler in the event of failures.\nFor AI infrastructure service providers such as our cloud site reliability engineers (SREs), OpenShift offers a single platform to allocate appropriately-sized infrastructure to various services, for example hundreds of GPUs for training, individual GPU nodes for fine tuning, and a few GPUs for inference (which are further partitionable). This enables optimal use of resources for the workload and improves cost-efficiency of the infrastructure.\nFor Platform administrators, OpenShift provides constructs (namespaces, quotas) to provision and manage resources among multiple users and projects with fine grained access controls. Platform level APIs allow the administrators to scale the system up and down based on resource availability and workload needs. For example, in Vela, resources are moved between OpenShift clusters for AI training and inference services based on business needs.\nWhile the OpenShift platform already comes with a rich collection of capabilities to expose high performance infrastructure to workloads (such as a GPU Operator, Network operator) and a sophisticated ecosystem of tools for system management (such as monitoring, logging, alerting), in the section below we will discuss some key capabilities we have developed based on the specific needs of workloads running on Vela and how we leverage them along with IBM Cloud capabilities to substantially enhance monitoring and diagnostic functions."}, {"title": "OpenShift Operators", "content": "Autopilot: IBM Research has created and open-sourced a tool called Autopilot [17], which is a cloud native observability tool implemented as a set of daemons, each one running on a GPU node. It implements a set of health checks that evaluate the status of the system. These health checks focus mainly on subtle/software issues, discussed more in the next section (i.e., row-remapping or PCI-E link degradation), but also run connectivity tests (i.e., ping, iperf) to verify that NICs are reachable. At the time of publication, the complete list of health checks includes the following: PCI-E bandwidth measurement between host and device, remapped rows evaluation, GPU power throttle enablement, all DCGM diagnostics, GPU memory bandwidth evaluation through DGEMM and DAXPY workloads, ping and iperf. Any subset of health checks can be set up to run periodically and automatically on all nodes, but the user can also run them manually if needed. More extensive tests, namely DCGM diagnostics level 3, are also executed automatically only on nodes that have free GPUs. We added this deeper analysis because there have been episodes of subtle issues that are only revealed after running level 3 DCGM diagnostics, therefore we decided to run these more invasive health checks proactively and flag nodes with an ERR/PASS flag. Results from health checks are exported as Prometheus Gauges, so that users and admins can easily check the status of the system on Grafana. This has significantly accelerated the discovery of issues in the cluster, and enabled proactive remediation.\nMulti-NIC CNI: As discussed in the previous sections, Vela GPU nodes have multiple 100G network interfaces and IBM Cloud uses single root I/O virtualization (SR-IOV) [9] to expose multiple virtual interfaces per each physical interface. Multi-NIC CNI [18] is a container-native interface built on top of Multus CNI with several important functions: 1. It discovers all of the interfaces on each host and handles them as a pool, 2. it assigns virtual interfaces for pods on top of the SR-IOV interfaces for TCP communication without encapsulation, and 3. it passes physical SR-IOV interfaces into the pods for GDR communication. These actions ensure that the workloads can achieve line rate network performance for TCP and GDR communication while code is running inside the pod.\nCNSA: A Scale client cluster runs across Vela's GPU nodes in container-native mode leveraging the CNSA edition of Scale [24]. It uses Kubernetes operators to deploy and manage Scale in a cloud-native fashion as well as a CSI Plugin for provisioning and attaching persistent volumes based on Scale. The client cluster does not contain any locally attached storage devices and instead performs remote mount of the file system in the storage cluster [25]. Such an architecture allows compute and storage clusters to grow and shrink independently as workload requirements change.\nData scientists gain access to the Scale file system using the traditional Kubernetes process. They create a Persistent Volume Claim (PVC) describing the size of the volume and the storage class referring to Scale. By invoking Scale's REST API on the storage cluster, Scale's CSI plugin creates a new fileset [27], which later can be attached as a volume to any pod running in OCP."}, {"title": "Workload Performance on OpenShift", "content": "One of the concerns that is often expressed regarding the use of the OpenShift platform for performance-sensitive workloads is that it might introduce a resource overhead. We studied this by comparing the performance of various workloads running on OpenShift to those running directly in a virtual machines and found that the workload performance overhead is within the margin of error (i.e., under 5%). The smaller batch sizes result in more communication per iteration, which allows the study of network overhead introduced by OpenShift. The larger batches result in more computation per iteration (hence larger iteration times), which allows us to study the container virtualization overhead. Across all of the batch sizes, the iteration times with OpenShift are within 4% of the iteration time with VMs. Note that the step times are the same or even better with OpenShift compared to VMs in some cases. In the prior section on node virtualization, we summarized the overall virtualization overhead as varying between nearly 0% to up to 5%; the addition of an OpenShift layer does not meaningfully change this. The overall overhead is still contained within roughly 5% relative to bare metal.\nOpenShift does in fact run more processes on the node compared to a typical HPC scheduler such as monitoring agents, network operators, logging agents, etc but their cumulative CPU usage is within 2% and cumulative memory usage is under 4%. This a reasonably small overhead and it can be ignored for all practical purposes.\nWhen Vela first came online in 2022, we initially deployed with a traditional HPC scheduler called IBM Spectrum LSF and only a small (2 node) OpenShift cluster. We started to add capabilities like high performance networking using Multi-NIC CNI, optimized scheduling using the MCAD scheduler [1], container-native storage using IBM Storage Scale CNSA, advanced monitoring and automated health checks using Autopilot, and grew the platform over nine months to support thousands of GPUs under a single cluster. To the best of our knowledge, this is the largest OpenShift cluster with GPUs in production anywhere in the world.\nWhile a single cluster is good for resource consolidation, training workloads and inference workloads have distinct security and scaling requirements so we also deploy additional clusters on Vela for production watsonx.ai inference services."}, {"title": "Operational efficiency and resilience", "content": "It is easy to understand the direct relationship between having a high-performance infrastructure and training a model as quickly as possible. Equally important is the need for technology to enable operational efficiency. This includes supporting dynamicity and variability in the types of experiments that researchers want to run as well as the need for technology to detect and address a whole host of inevitable failures that are guaranteed to occur when training large models on a large and complex infrastructure. This section details an operational and technological approach for rapid experimentation and rapid failure resolution, which are both equally critical for AI model training agility."}, {"title": "Addressing component failures", "content": "In distributed training", "1": "n\u2022\tClear hardware failures", "failures": "We classify a failure as a clear hardware failure when the host crashes. We observed that on average about 2% of our hosts crashed per month over the past 2 years and as many as 5% in the worst case. A primary source for these failures is the failure of the board that holds the GPUs (called an HGX baseboard). A second source, albeit less frequently observed, is the failure of the NVLink or the NVSwitch system. In both of these failure cases, the GPU system has to be removed from the cluster, and the system has to be fixed or replaced, in most cases by the vendor, before it can be returned back to the cluster. Another source of clear hardware failures have been the memory DIMM's of the host. These DIMM's can be replaced by our operations teams so that the system can return to operations relatively quickly compared to the other failures.\nWhen these kinds of failures occur, the host crashing causes the application to crash as well, which would normally require manual intervention to restart the application on a healthy set of nodes. To alert the system reliability engineers (SRE's), we developed Slack automation to send a message when the cloud control plane detects such as a host crash. In addition, if there are free hosts, our platform control plane (RedHat OpenShift host controller, which we developed) automatically restarts the VM so that the pool of resources provisioned in the cluster stays constant. Our job management system also automatically restarts the job on a healthy set of nodes, typically from a previous checkpoint.\nTo make sure that we can always restart"}]}]}