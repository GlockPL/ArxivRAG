{"title": "GENERATION OF GEODESICS WITH ACTOR-CRITIC REINFORCEMENT LEARNING TO PREDICT MIDPOINTS", "authors": ["Kazumi Kasaura"], "abstract": "To find the shortest paths for all pairs on continuous manifolds with infinitesimally defined metrics, we propose to generate them by predicting midpoints recursively and an actor-critic method to learn midpoint prediction. We prove the soundness of our approach and show experimentally that the proposed method outperforms existing methods on both local and global path planning tasks.", "sections": [{"title": "1 Introduction", "content": "On manifolds with metrics, minimizing geodesics, or shortest paths, are minimum-length curves connecting points. Various real-world tasks can be reduced to the generation of geodesics on manifolds. Examples include time-optimal path planning on sloping ground [Matsumoto, 1989], robot motion planning under various constraints [LaValle, 2006, Ratliff et al., 2015], physical systems [Pfeifer, 2019], the Wasserstein distance [Agueh, 2012], and image morphing [Michelis and Becker, 2021, Effland et al., 2021]. Typically, metrics are only known infinitesimally (a form of a Riemannian or Finsler metric), and their distance functions are not known beforehand. Computation of geodesics by solving optimization problems or differential equations is generally computationally costly and requires an explicit form of the metric, or at least, values of its differentials. To find geodesics for various pairs of endpoints in a fixed manifold, it is more efficient to use a policy that has learned to generate geodesics for arbitrary pairs. In addition, we aim to learn only from values of the infinitesimal metric.\nGoal-conditioned reinforcement learning [Schaul et al., 2015] to generate waypoints of geodesics sequentially from start to goal suffers from sparseness of the reward if an agent gets a reward only when it reaches a goal. To overcome this, a typical strategy is to give the agent appropriate rewards when it gets near its goal. However, to define reward values, we must have a global approximation of the distance function between two points beforehand. When manifolds have complex metrics, it may be difficult to find such an approximation. Furthermore, in trying to generate numerous waypoints, the long horizon makes learning difficult [Wang et al., 2020].\nTo overcome these difficulties, we propose a framework called the midpoint tree, which is a modification of the sub-goal tree framework [Jurgenson et al., 2020]. In this framework, a policy learns to predict midpoints of given pairs of points, and geodesics are generated by inserting midpoints recursively, as illustrated in Figure 1. Compared to sequential generation, such recurrent generation methods also have the advantage that they are naturally parallelizable."}, {"title": "2 Related Works", "content": "The original learning method for sub-goals prediction via a policy gradient in Jurgenson et al. [2020] has poor sample efficiency when recursion is deep. To improve on this, we propose an actor-critic learning method for midpoint prediction, which is similar to the actor-critic method [Konda and Tsitsiklis, 1999] for conventional reinforcement learning. We prove theoretically that, under mild assumptions, if the training converges in the limit of infinite recursion depth, the resulting policy can generate true geodesics. This result does not hold for generation by arbitrary intermediate points.\nWe experimentally compared our proposed method, on five path (or motion) planning tasks, to sequential generation with goal-conditioned reinforcement learning and midpoint tree generation trained by a policy gradient method without a critic. Two tasks involved continuous metrics or constraints (local planning), while the other three involved obstacles (global planning). In both local and global planning, our proposed method outperformed baseline methods for the difficult tasks."}, {"title": "2.1 Path Planning with Reinforcement Learning", "content": "One of the most popular approaches for path planning via reinforcement learning is to use a Q-table [Haghzad Klidbary et al., 2017, Low et al., 2022]. However, that approach depends on the finiteness of the state spaces, and the computational costs grow with the sizes of those spaces.\nSeveral studies have been conducted on path planning in continuous spaces via deep reinforcement learning [Zhou et al., 2019, Wang et al., 2019, Kulathunga, 2022]. In those works, the methods depend on custom rewards."}, {"title": "2.2 Goal-Conditioned Reinforcement Learning and Sub-Goals", "content": "Goal-conditioned reinforcement learning [Kaelbling, 1993, Schaul et al., 2015] trains a universal policy for various goals. It learns a value function whose inputs are both the current and goal states. Kaelbling [1993] and Dhiman et al. [2018] pointed out that goal-conditioned value functions are related to the Floyd-Warshall algorithm for the all-pairs shortest-path problem [Floyd, 1962], as this function can be updated by finding intermediate states. They proposed methods that use brute force to search for intermediate states, which depend on the finiteness of the state spaces. The idea of using sub-goals for reinforcement learning as options was suggested by Sutton et al. [1999], and Jurgenson et al. [2020] linked this notion to the aforementioned intermediate states. Wang et al. [2023] drew attention to the quasi-metric structure of goal-conditioned value functions and suggested using quasi-metric learning [Wang and Isola, 2022] to learn those functions.\nThe idea of generating paths by predicting sub-goals recursively has been proposed in three papers with different problem settings and methods. The problem setting for goal-conditioned hierarchical predictors [Pertsch et al., 2020] differs from ours because they use an approximate distance function learned from given training data, whereas no training data are given in our setting. Divide-and-conquer Monte Carlo tree search [Parascandolo et al., 2020] is similar to our learning method because it trains both the policy prior and the approximate value function, which respectively correspond to the actor and critic in our method. However, their algorithm depends on the finiteness of the state spaces.\nThe problem setting in Jurgenson et al. [2020] is the most similar to ours, but it remains different. In their setting, the costs of direct edges between two states are given, which are upper bounds for the distances. In our setting, we can only approximate distances between two states when they are close. Therefore, we must find waypoints such that the adjacent states are close. This is one reason why we use midpoint trees instead of sub-goal trees. In addition, because they focus on high-level planning, their trees are not as deep as ours; accordingly, we propose an actor-critic method whereas they use a policy gradient method without a critic."}, {"title": "3 Preliminary", "content": "After describing the general notions for quasi-metric spaces, we describe Finsler manifolds, which are an important example of such a space."}, {"title": "3.1 Quasi-Metric Space", "content": "We follow the notation in Kim [1968]. Let \\( X \\) be a space. A pseudo-quasi-metric on \\( X \\) is a function \\( d : X \\times X \\to \\mathbb{R} \\) such that \\( d(x, x) = 0 \\), \\( d(x, y) \\geq 0 \\), and \\( d(x, z) \\leq d(x, y) + d(y, z) \\) for any \\( x, y, z \\in X \\). A topology on \\( X \\) is induced"}, {"title": "3.2 Finsler Geometry", "content": "An important family of quasi-metric spaces is the Finsler manifolds. A Finsler manifold is a differential manifold \\( M \\) equipped with a function \\( F : TM \\to [0, \\infty) \\), where \\( TM = \\bigcup_{x\\in M} T_xM \\) is the tangent bundle of \\( M \\), and \\( F \\) satisfies the following conditions [Bao et al., 2000].\n1. \\( F \\) is smooth on \\( TM \\setminus 0 \\).\n2. \\( F(x, \\lambda v) = \\lambda F(x, v) \\) for all \\( \\lambda > 0 \\) and \\( (x, v) \\in TM \\).\n3. The Hessian matrix of \\( F(x, -)^2 \\) is positive definite at every point of \\( TM_x \\setminus 0 \\) for all \\( x \\in M \\).\nLet \\( \\gamma : [0, 1] \\to M \\) be a piecewise smooth curve on \\( M \\). We define the length of \\( \\gamma \\) as\n\\[ L(\\gamma) := \\int_0^1 F\\left(\\gamma(t), \\frac{d\\gamma}{dt}(t)\\right) dt. \\tag{1} \\]\nFor two points \\( x, y \\in M \\), we define the distance \\( d(x, y) \\) as\n\\[ d(x, y) := \\inf \\{L(\\gamma) | \\gamma : [0, 1] \\to M, \\gamma(0) = x, \\gamma(1) = y \\}. \\tag{2} \\]\nThen, \\( d \\) is a weakly symmetric quasi-metric [Bao et al., 2000]. A curve \\( \\gamma : [0, 1] \\to M \\) is called a minimizing geodesic if \\( L(\\gamma) = d(\\gamma(0), \\gamma(1)) \\).\nIt is known [Bao et al., 2000, Amici and Casciaro, 2010] that, for any point of \\( M \\), there exists a neighborhood \\( U \\subset M \\) such that any two points \\( p, q \\in U \\) can be uniquely connected by a minimizing geodesic inside \\( U \\). Note that \\( (U, d) \\) has the continuous midpoint property."}, {"title": "4 Theoretical Results", "content": "We briefly explain our main idea in \u00a7 4.1 and prove propositions justifying it in \u00a7 4.2 and \u00a7 4.3. We prove that the local assumptions in the proposition are satisfied in Finsler case in \u00a7 4.4 and consider environments with obstacles in \u00a7 4.5."}, {"title": "4.1 Midpoint Tree", "content": "The midpoint tree is a modification of the sub-goal tree proposed by Jurgenson et al. [2020]. In the sub-goal tree framework, a policy learns to predict an intermediate point between two given points, instead of the next point as in the sequential framework. Paths between start and goal points are generated by recursively applying this prediction to adjacent pairs of previously generated waypoints. In our midpoint tree framework, instead of predicting an arbitrary intermediate point, the policy predicts a midpoint. This modification enables appropriate generation even when metrics are only locally known.\nWe train the policy (actor) to predict midpoints by an actor-critic method. In other words, we simultaneously train a function (critic) to predict distances. In the following subsections, we describe the theoretical considerations of this actor-critic learning method."}, {"title": "4.2 Functional Equation", "content": "Let \\( (X, d) \\) be a pseudo-quasi-metric space. When we do not know \\( d \\) globally, we want to train a function (actor) \\( \\pi : X \\times X \\to X \\) to predict midpoints. We also train a function (critic) \\( V : X \\times X \\to \\mathbb{R} \\) to predict distances."}, {"title": "4.3 Iteration", "content": "Although we do not know the metric \\( d \\) globally, we assume that we know it infinitesimally, that is, we have a continuous function \\( C : X \\times X \\to \\mathbb{R} \\) that coincides with \\( d \\) in the limit \\( d(x, y) \\to 0 \\) or \\( C(x, y) \\to 0 \\). Formally, we assume the following conditions:\n1. For \\( x \\in X \\) and a series \\( (y_i)_{i \\in \\mathbb{N}} \\), if \\( C(x, y_i) \\to 0 \\), then \\( d(x, y_i) \\to 0 \\).\n2. For \\( x \\in X \\) and \\( \\varepsilon > 0 \\), there exists \\( \\delta > 0 \\) such that, for any \\( y, z \\in B_{\\delta}(x) \\),\n\\[ (1 - \\varepsilon) d(y, z) \\leq C(y, z) \\leq (1 + \\varepsilon) d(y, z). \\tag{18} \\]\nStarting with \\( C \\), we consider iterative construction of actors and critics to satisfy (3) and (4). Formally, we assume series of functions, \\( V_i : X \\times X \\to \\mathbb{R} \\) and \\( \\pi_i : X \\times X \\to X \\), indexed by \\( i = 0, 1, ..., \\) that satisfy the following conditions:\n\\[ V_0(x, y) = C(x, y), \\tag{19} \\]"}, {"title": "5 Experiments", "content": "In \u00a7 5.1, by building on the previous section's results, we describe our proposed learning algorithm for midpoint prediction. In the following subsections, we compared our method, which generates geodesics via policies trained by our algorithm, with baseline methods on several path planning tasks. The first two environments deal with asymmetric metrics that change continuously (local planning tasks), while the other three environments have simple and symmetric metrics but involve obstacles (global planning tasks)."}, {"title": "5.1 Learning Algorithm", "content": "Let \\( (X, d) \\) be a pseudo-quasi-metric space, and let \\( C \\) be a function satisfying Assumption 3.\nWe simultaneously train two networks with parameters \\( \\theta \\) and \\( \\phi \\): the actor \\( \\pi_{\\theta} \\), which predicts the midpoints between two given points, and the critic \\( V_{\\phi} \\), which predicts the distances between two given points. The critic learns to predict distances from the lengths of the sequences generated by the actor, while the actor learns to predict midpoints from the critic's predictions.\nFor learning efficiency, we train only one actor and one critic, unlike in \u00a7 4.3. Instead, we gradually increase the depth for data collection to train the critic, so that \\( C \\) is called for closer pairs of points. In this way, \\( V \\) and start by learning values of \\( V_0 \\) and \\( \\pi_0 \\) and gradually learn the values of \\( V_i \\) and \\( \\pi_i \\) for higher \\( i \\). Note that, since \\( C \\) is closer to the true distance \\( d \\) for pairs of closer points, \\( V_i \\) and \\( \\pi_i \\) converge more quickly in the region of pairs of closer points.\nThe network for actor \\( \\pi_{\\theta} \\) has a form for which the reparameterization trick [Haarnoja et al., 2018] can be applied, that is, a sample is drawn by computing a deterministic function of the input, parameters \\( \\theta \\), and an independent noise. Henceforth, we abuse notations and denote a sampled prediction from \\( \\pi_{\\theta}(\\cdot | s, g) \\) by \\( \\pi_{\\theta}(s, g) \\) even if it is not deterministic."}, {"title": "5.2 Tasks and Evaluation Method", "content": "We compared the methods in terms of their success rate on the following task. A Finsler manifold \\( (M, F) \\) with a global coordinate system \\( f : M \\to \\mathbb{R}^d \\) (and a free space \\( M_{\\text{free}} \\subset M \\)) is given as an environment. The number \\( n \\) of segments to approximate paths and a proximity threshold \\( \\varepsilon > 0 \\) are also fixed. For our method, \\( n \\) must be a power of two: \\( n = 2^{D_{\\text{max}}} \\), where \\( D_{\\text{max}} \\) is the midpoint trees' depth for evaluation. When two points \\( s, g \\in M_{\\text{free}} \\) are given, we want to generate a sequence \\( s = p_0, p_1, ..., p_n = g \\) of points such that no value of \\( C \\) for two consecutive points is greater than \\( \\varepsilon \\), where \\( C \\) is defined by (31) and, for cases with obstacles, (40). If the points generated by a method satisfy this condition, then it is considered successful in solving the task; otherwise, it is considered to have failed. Note that, for cases with obstacles, when \\( c_p > \\varepsilon \\), successes imply that all waypoints are lying on the free space.\nFor each environment, we randomly generated 100 pairs of points from the free space, before the experiment. During training, we evaluated the models regularly by solving the tasks for the prepared pairs and recorded the success rates. We ran each method with different random seeds ten times for the environment described in \u00a7 5.4.1 and five times for the other environments.\nFor each environment, the total number \\( T \\) of timesteps was fixed for all methods. Therefore, at Line 2 of our method, we continue learning until the number of timesteps reaches the defined value. Timesteps were measured by counting the evaluation of \\( C \\) during training. In other words, for sequential reinforcement learning (see the next subsection), the timesteps have their conventional meaning. For the other methods, one generation of a path (one cycle) with depth \\( D \\) is counted as \\( 2^D \\) timesteps."}, {"title": "5.3 Compared Methods", "content": "We mainly used success rate, not path length, for evaluation because metrics are only be defined locally and lengths thus cannot be calculated unless the success condition is satisfied. Furthermore, the evaluation by success rate allows for fair comparison with the baseline method using sequential generation. However, we also compared lengths of generated paths.\nThe baseline methods were as follows:\n* Sequential Reinforcement Learning (Seq): We formulated sequential generation of waypoints as a conventional, goal-conditioned reinforcement learning environment. The agent moves to the goal step by step in \\( M \\). If the agent is at \\( p \\), it can move to \\( q \\) such that \\( F(p, df_p^{-1}(f(q) - f(p))) = \\varepsilon \\). If \\( q \\) is outside the free space \\( M_{\\text{free}} \\), the reward \\( R \\) is set to \\( -c_p \\) and the episode ends as a failure. Otherwise, \\( R \\) is defined as\n\\[ R := - \\varepsilon + F(g, df_p^{-1}(f(g) - f(p))) - F(g, df_q^{-1}(f(g) - f(q))), \\tag{49} \\]\nwhere \\( g \\) is the goal.\nThe discount factor is set to 1. An episode ends and is considered a success when the agent reaches a point \\( p \\) that satisfies \\( F(p, df_p^{-1}(f(g) - f(p))) < \\varepsilon \\). When the episode duration reaches \\( n \\) steps without satisfying this condition, the episode ends and is considered a failure.\nWe used Proximal Policy Optimization (PPO) [Schulman et al., 2017] to solve reinforcement learning problems with this formulation.\n* Policy Gradient (PG): We modified the method in Jurgenson et al. [2020] to predict midpoints. For each \\( D = 1, ..., D_{\\text{max}} \\), a stochastic policy \\( \\pi_D \\) is trained to predict midpoints with depth \\( D \\). To generate waypoints, we apply the policies in descending order of the index. We train the policies in ascending order of the index by the policy gradient.\nTo predict midpoints, the value to be minimized in training is changed. Let \\( \\rho(\\pi_1, ..., \\pi_D) \\) be the distribution of \\( \\mathcal{T} := (p_0, ..., p_{2^D}) \\), where \\( p_0 \\) and \\( p_{2^D} \\) are sampled from the predefined distribution on \\( M \\) and \\( p_{2^{i-1}(2j+1)} \\) is sampled from the distribution \\( \\pi_i(\\cdot | p_{2^ij}, p_{2^i(j+1)}) \\). Let \\( \\theta_D \\) denote the parameters of \\( \\pi_D \\). Instead of minimizing the expected value of \\( \\sum_{i=0}^{2^D-1} C(p_i, p_{i+1}) \\) as in the original method, we train \\( \\pi_D \\) to minimize the expected value of\n\\[ \\mathcal{C} := \\left(\\sum_{i=0}^{2^{D-1}-1} C(p_i, p_{i+1})\\right)^2 + \\left(\\sum_{i=2^{D-1}}^{2^D-1} C(p_i, p_{i+1})\\right)^2. \\tag{50} \\]\nHere, we use\n\\[ \\nabla_{\\theta_D} \\mathbb{E}_{\\rho(\\pi_1, ..., \\pi_D)} [\\mathcal{C}] = \\mathbb{E}_{\\rho(\\pi_1, ..., \\pi_D)} [(\\mathcal{C} - b(p_0, p_{2^D})) \\nabla_{\\theta_D} \\log \\pi_D(p_{2^{D-1}} | p_0, p_{2^D})], \\tag{51} \\]\nwhere \\( b \\) is a baseline function.\nNote that, when the model is evaluated during training, if the current trained policy is \\( \\pi_D \\) (\\( 1 \\leq D < D_{\\text{max}} \\)), then evaluation is performed with depth \\( D \\) (\\( 2^D \\) segments). (The other methods are always evaluated with \\( n = 2^{D_{\\text{max}}} \\) segments.)\nFor our proposed method described in \u00a75.1, we tried two scheduling strategies to increase the trees' depth for training from zero to \\( D_{\\text{max}} \\), under the condition of fixed total timesteps.\n* Timestep-Based Depth Scheduling (Our-T): For each depth, training lasts the same number of timesteps.\n* Cycle-Based Depth Scheduling (Our-C): For each depth, training lasts the same number of calls to the data collection procedure (cycles).\nMore precisely, at Line 17 in Algorithm 1, for timestep-based depth scheduling, the depth is \\( \\lfloor t/t_a \\rfloor \\), where \\( t \\) is the number of timesteps at the current time and \\( t_a := \\lfloor T/D_{\\text{max}} \\rfloor + 1 \\). For cycle-based depth scheduling, the depth for the \\( c \\)-th call to the data collection procedure is \\( \\lfloor c/c_a \\rfloor \\), where \\( c_a := \\lfloor T/(2^{D_{\\text{max}}+1} - 1) \\rfloor + 1 \\). Note that Our-T provides more training for low depths than Our-C does.\nIn addition, we ran the following variants of our method."}, {"title": "5.4 Environments", "content": "We experimented in the following five environments."}, {"title": "5.4.1 Matsumoto Metric", "content": "The Matsumoto metric is an asymmetric Finsler metric that considers times to move on inclined planes [Matsumoto, 1989]. Let \\( M \\subset \\mathbb{R}^2 \\) be a region on the plane with the standard coordinates \\( x, y \\), and let \\( h : M \\to \\mathbb{R} \\) be a differentiable function that indicates heights of the field. The Matsumoto metric \\( F : TM \\to [0, \\infty) \\) is then defined as follows:\n\\[ F(p, (v_x, v_y)) := \\frac{\\sqrt{v_x^2 + v_y^2}}{\\alpha - \\beta}, \\tag{57} \\]\nwhere,\n\\[ \\beta := v_x \\frac{\\partial h}{\\partial x}(p) + v_y \\frac{\\partial h}{\\partial y}(p), \\quad \\alpha := \\sqrt{v_x^2 + v_y^2 + \\beta^2}. \\tag{58} \\]\nWe take the unit disk as \\( M \\) and let \\( h(p) := -||p||^2 \\). Intuitively, we consider a round field with a mountain at the center.\nFor this environment, we set the number of segments \\( n = 64 \\) (\\( D_{\\text{max}} = 6 \\)), the proximity threshold \\( \\varepsilon = 0.1 \\), and the total number of timesteps \\( T = 2 \\times 10^7 \\)."}, {"title": "5.4.2 Unidirectional Car-Like Constraints", "content": "Inspired by the cost function for trajectories of car-like non-holonomic vehicles [R\u00f6smann et al., 2017], we define an asymmetric Finsler metric for unidirectional car-like vehicles.\nLet \\( M := U \\times S^1 \\subset \\mathbb{R}^3 \\) be a configuration space for car-like vehicles, where \\( U \\subset \\mathbb{R}^2 \\) is the unit disk with the standard coordinate system \\( x, y \\) and \\( S^1 \\) is the unit circle with the coordinate \\( \\theta \\). We define \\( F : TM \\to [0, \\infty) \\) as follows:\n\\[ F((p, \\theta), (v_x, v_y, v_{\\theta})) := \\sqrt{v_x^2 + v_y^2} + c_c(\\xi^2 + h^2), \\tag{59} \\]\nwhere\n\\[ h := -v_x \\sin(\\theta) + v_y \\cos(\\theta), \\tag{60} \\]"}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we proposed a framework, called midpoint trees, to generate geodesics by recursively predicting midpoints. We also proposed an actor-critic method for learning to predict midpoints, and we theoretically proved its soundness. Experimentally, we showed that our method can solve path planning tasks that existing reinforcement learning methods fail to solve.\nOur experiments showed that effective depth scheduling depends on the task. Exploration of an efficient depth scheduling algorithm that considers learning progress is a future challenge. In addition, we tried only a straightforward actor-critic learning algorithm in this paper, while algorithms for actor-critic reinforcement learning have been intensively researched. Investigation of more efficient algorithms thus also remains for future work. For example, while we used an on-policy algorithm, off-policy algorithms [Degris et al., 2012] may be useful in our framework. The architectures we used for both actors and critics were also simple, but the quasi-metric learning method [Wang and Isola, 2022, Wang et al., 2023] may be useful for critics in our approach.\nIn our method, a policy must be learned for each environment. By modifying our method so that the actor and critic input information on environments as in Sartori et al. [2021], it may be possible to learn a policy that is applicable to different environments.\nWhile we assumed continuity of the policy function to prove our approach's theoretical soundness, the continuous midpoint property may only be satisfied locally. Further research is needed on the conditions under which our methods work. Even if our method does not work well globally, we could consider dividing manifolds, training policies for each region, and then connecting locally generated geodesics."}, {"title": "A Proofs of Lemmas", "content": "We describe proofs omitted from the main text."}, {"title": "A.1 Proof of Lemma 5", "content": "We first prove 1. For any sequences \\( x_i \\) and \\( y_i \\) that converge \\( x \\) and \\( y \\)"}, {"title": "GENERATION OF GEODESICS WITH ACTOR-CRITIC REINFORCEMENT LEARNING TO PREDICT MIDPOINTS", "authors": ["Kazumi Kasaura"], "abstract": "To find the shortest paths for all pairs on continuous manifolds with infinitesimally defined metrics, we propose to generate them by predicting midpoints recursively and an actor-critic method to learn midpoint prediction. We prove the soundness of our approach and show experimentally that the proposed method outperforms existing methods on both local and global path planning tasks.", "sections": [{"title": "1 Introduction", "content": "On manifolds with metrics, minimizing geodesics, or shortest paths, are minimum-length curves connecting points. Various real-world tasks can be reduced to the generation of geodesics on manifolds. Examples include time-optimal path planning on sloping ground [Matsumoto, 1989], robot motion planning under various constraints [LaValle, 2006, Ratliff et al., 2015], physical systems [Pfeifer, 2019], the Wasserstein distance [Agueh, 2012], and image morphing [Michelis and Becker, 2021, Effland et al., 2021]. Typically, metrics are only known infinitesimally (a form of a Riemannian or Finsler metric), and their distance functions are not known beforehand. Computation of geodesics by solving optimization problems or differential equations is generally computationally costly and requires an explicit form of the metric, or at least, values of its differentials. To find geodesics for various pairs of endpoints in a fixed manifold, it is more efficient to use a policy that has learned to generate geodesics for arbitrary pairs. In addition, we aim to learn only from values of the infinitesimal metric.\nGoal-conditioned reinforcement learning [Schaul et al., 2015] to generate waypoints of geodesics sequentially from start to goal suffers from sparseness of the reward if an agent gets a reward only when it reaches a goal. To overcome this, a typical strategy is to give the agent appropriate rewards when it gets near its goal. However, to define reward values, we must have a global approximation of the distance function between two points beforehand. When manifolds have complex metrics, it may be difficult to find such an approximation. Furthermore, in trying to generate numerous waypoints, the long horizon makes learning difficult [Wang et al., 2020].\nTo overcome these difficulties, we propose a framework called the midpoint tree, which is a modification of the sub-goal tree framework [Jurgenson et al., 2020]. In this framework, a policy learns to predict midpoints of given pairs of points, and geodesics are generated by inserting midpoints recursively, as illustrated in Figure 1. Compared to sequential generation, such recurrent generation methods also have the advantage that they are naturally parallelizable."}, {"title": "2 Related Works", "content": "The original learning method for sub-goals prediction via a policy gradient in Jurgenson et al. [2020] has poor sample efficiency when recursion is deep. To improve on this, we propose an actor-critic learning method for midpoint prediction, which is similar to the actor-critic method [Konda and Tsitsiklis, 1999] for conventional reinforcement learning. We prove theoretically that, under mild assumptions, if the training converges in the limit of infinite recursion depth, the resulting policy can generate true geodesics. This result does not hold for generation by arbitrary intermediate points.\nWe experimentally compared our proposed method, on five path (or motion) planning tasks, to sequential generation with goal-conditioned reinforcement learning and midpoint tree generation trained by a policy gradient method without a critic. Two tasks involved continuous metrics or constraints (local planning), while the other three involved obstacles (global planning). In both local and global planning, our proposed method outperformed baseline methods for the difficult tasks."}, {"title": "2.1 Path Planning with Reinforcement Learning", "content": "One of the most popular approaches for path planning via reinforcement learning is to use a Q-table [Haghzad Klidbary et al., 2017, Low et al., 2022]. However, that approach depends on the finiteness of the state spaces, and the computational costs grow with the sizes of those spaces.\nSeveral studies have been conducted on path planning in continuous spaces via deep reinforcement learning [Zhou et al., 2019, Wang et al., 2019, Kulathunga, 2022]. In those works, the methods depend on custom rewards."}, {"title": "2.2 Goal-Conditioned Reinforcement Learning and Sub-Goals", "content": "Goal-conditioned reinforcement learning [Kaelbling, 1993, Schaul et al., 2015] trains a universal policy for various goals. It learns a value function whose inputs are both the current and goal states. Kaelbling [1993] and Dhiman et al. [2018] pointed out that goal-conditioned value functions are related to the Floyd-Warshall algorithm for the all-pairs shortest-path problem [Floyd, 1962], as this function can be updated by finding intermediate states. They proposed methods that use brute force to search for intermediate states, which depend on the finiteness of the state spaces. The idea of using sub-goals for reinforcement learning as options was suggested by Sutton et al. [1999], and Jurgenson et al. [2020] linked this notion to the aforementioned intermediate states. Wang et al. [2023] drew attention to the quasi-metric structure of goal-conditioned value functions and suggested using quasi-metric learning [Wang and Isola, 2022] to learn those functions.\nThe idea of generating paths by predicting sub-goals recursively has been proposed in three papers with different problem settings and methods. The problem setting for goal-conditioned hierarchical predictors [Pertsch et al., 2020] differs from ours because they use an approximate distance function learned from given training data, whereas no training data are given in our setting. Divide-and-conquer Monte Carlo tree search [Parascandolo et al., 2020] is similar to our learning method because it trains both the policy prior and the approximate value function, which respectively correspond to the actor and critic in our method. However, their algorithm depends on the finiteness of the state spaces.\nThe problem setting in Jurgenson et al. [2020] is the most similar to ours, but it remains different. In their setting, the costs of direct edges between two states are given, which are upper bounds for the distances. In our setting, we can only approximate distances between two states when they are close. Therefore, we must find waypoints such that the adjacent states are close. This is one reason why we use midpoint trees instead of sub-goal trees. In addition, because they focus on high-level planning, their trees are not as deep as ours; accordingly, we propose an actor-critic method whereas they use a policy gradient method without a critic."}, {"title": "3 Preliminary", "content": "After describing the general notions for quasi-metric spaces, we describe Finsler manifolds, which are an important example of such a space."}, {"title": "3.1 Quasi-Metric Space", "content": "We follow the notation in Kim [1968]. Let \\( X \\) be a space. A pseudo-quasi-metric on \\( X \\) is a function \\( d : X \\times X \\to \\mathbb{R} \\) such that \\( d(x, x) = 0 \\), \\( d(x, y) \\geq 0 \\), and \\( d(x, z) \\leq d(x, y) + d(y, z) \\) for any \\( x, y, z \\in X \\). A topology on \\( X \\) is induced"}, {"title": "3.2 Finsler Geometry", "content": "An important family of quasi-metric spaces is the Finsler manifolds. A Finsler manifold is a differential manifold \\( M \\) equipped with a function \\( F : TM \\to [0, \\infty) \\), where \\( TM = \\bigcup_{x\\in M} T_xM \\) is the tangent bundle of \\( M \\), and \\( F \\) satisfies the following conditions [Bao et al., 2000].\n1. \\( F \\) is smooth on \\( TM \\setminus 0 \\).\n2. \\( F(x, \\lambda v) = \\lambda F(x, v) \\) for all \\( \\lambda > 0 \\) and \\( (x, v) \\in TM \\).\n3. The Hessian matrix of \\( F(x, -)^2 \\) is positive definite at every point of \\( TM_x \\setminus 0 \\) for all \\( x \\in M \\).\nLet \\( \\gamma : [0, 1] \\to M \\) be a piecewise smooth curve on \\( M \\). We define the length of \\( \\gamma \\) as\n\\[ L(\\gamma) := \\int_0^1 F\\left(\\gamma(t), \\frac{d\\gamma}{dt}(t)\\right) dt. \\tag{1} \\]\nFor two points \\( x, y \\in M \\), we define the distance \\( d(x, y) \\) as\n\\[ d(x, y) := \\inf \\{L(\\gamma) | \\gamma : [0, 1] \\to M, \\gamma(0) = x, \\gamma(1) = y \\}. \\tag{2} \\]\nThen, \\( d \\) is a weakly symmetric quasi-metric [Bao et al., 2000]. A curve \\( \\gamma : [0, 1] \\to M \\) is called a minimizing geodesic if \\( L(\\gamma) = d(\\gamma(0), \\gamma(1)) \\)."}, {"title": "4 Theoretical Results", "content": "We briefly explain our main idea in \u00a7 4.1 and prove propositions justifying it in \u00a7 4.2 and \u00a7 4.3. We prove that the local assumptions in the proposition are satisfied in Finsler case in \u00a7 4.4 and consider environments with obstacles in \u00a7 4.5."}, {"title": "4.1 Midpoint Tree", "content": "The midpoint tree is a modification of the sub-goal tree proposed by Jurgenson et al. [2020]. In the sub-goal tree framework, a policy learns to predict an intermediate point between two given points, instead of the next point as in the sequential framework. Paths between start and goal points are generated by recursively applying this prediction to adjacent pairs of previously generated waypoints. In our midpoint tree framework, instead of predicting an arbitrary intermediate point, the policy predicts a midpoint. This modification enables appropriate generation even when metrics are only locally known.\nWe train the policy (actor) to predict midpoints by an actor-critic method. In other words, we simultaneously train a function (critic) to predict distances. In the following subsections, we describe the theoretical considerations of this actor-critic learning method."}, {"title": "4.2 Functional Equation", "content": "Let \\( (X, d) \\) be a pseudo-quasi-metric space. When we do not know \\( d \\) globally, we want to train a function (actor) \\( \\pi : X \\times X \\to X \\) to predict midpoints. We also train a function (critic) \\( V : X \\times X \\to \\mathbb{R} \\) to predict distances."}, {"title": "4.3 Iteration", "content": "Although we do not know the metric \\( d \\) globally, we assume that we know it infinitesimally, that is, we have a continuous function \\( C : X \\times X \\to \\mathbb{R} \\) that coincides with \\( d \\) in the limit \\( d(x, y) \\to 0 \\) or \\( C(x, y) \\to 0 \\). Formally, we assume the following conditions:\n1. For \\( x \\in X \\) and a series \\( (y_i)_{i \\in \\mathbb{N}} \\), if \\( C(x, y_i) \\to 0 \\), then \\( d(x, y_i) \\to 0 \\).\n2. For \\( x \\in X \\) and \\( \\varepsilon > 0 \\), there exists \\( \\delta > 0 \\) such that, for any \\( y, z \\in B_{\\delta}(x) \\),\n\\[ (1 - \\varepsilon) d(y, z) \\leq C(y, z) \\leq (1 + \\varepsilon) d(y, z). \\tag{18} \\]\nStarting with \\( C \\), we consider iterative construction of actors and critics to satisfy (3) and (4). Formally, we assume series of functions, \\( V_i : X \\times X \\to \\mathbb{R} \\) and \\( \\pi_i : X \\times X \\to X \\), indexed by \\( i = 0, 1, ..., \\) that satisfy the following conditions:\n\\[ V_0(x, y) = C(x, y), \\tag{19} \\]"}]}]}