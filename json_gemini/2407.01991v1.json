{"title": "GENERATION OF GEODESICS WITH ACTOR-CRITIC\nREINFORCEMENT LEARNING TO PREDICT MIDPOINTS", "authors": ["Kazumi Kasaura"], "abstract": "To find the shortest paths for all pairs on continuous manifolds with infinitesimally defined metrics,\nwe propose to generate them by predicting midpoints recursively and an actor-critic method to learn\nmidpoint prediction. We prove the soundness of our approach and show experimentally that the\nproposed method outperforms existing methods on both local and global path planning tasks.", "sections": [{"title": "1 Introduction", "content": "On manifolds with metrics, minimizing geodesics, or shortest paths, are minimum-length curves connecting points.\nVarious real-world tasks can be reduced to the generation of geodesics on manifolds. Examples include time-optimal\npath planning on sloping ground [Matsumoto, 1989], robot motion planning under various constraints [LaValle,\n2006, Ratliff et al., 2015], physical systems [Pfeifer, 2019], the Wasserstein distance [Agueh, 2012], and image\nmorphing [Michelis and Becker, 2021, Effland et al., 2021]. Typically, metrics are only known infinitesimally (a form\nof a Riemannian or Finsler metric), and their distance functions are not known beforehand. Computation of geodesics\nby solving optimization problems or differential equations is generally computationally costly and requires an explicit\nform of the metric, or at least, values of its differentials. To find geodesics for various pairs of endpoints in a fixed\nmanifold, it is more efficient to use a policy that has learned to generate geodesics for arbitrary pairs. In addition, we\naim to learn only from values of the infinitesimal metric.\nGoal-conditioned reinforcement learning [Schaul et al., 2015] to generate waypoints of geodesics sequentially from\nstart to goal suffers from sparseness of the reward if an agent gets a reward only when it reaches a goal. To overcome\nthis, a typical strategy is to give the agent appropriate rewards when it gets near its goal. However, to define reward\nvalues, we must have a global approximation of the distance function between two points beforehand. When manifolds\nhave complex metrics, it may be difficult to find such an approximation. Furthermore, in trying to generate numerous\nwaypoints, the long horizon makes learning difficult [Wang et al., 2020].\nTo overcome these difficulties, we propose a framework called the midpoint tree, which is a modification of the sub-goal\ntree framework [Jurgenson et al., 2020]. In this framework, a policy learns to predict midpoints of given pairs of points,\nand geodesics are generated by inserting midpoints recursively, as illustrated in Figure 1. Compared to sequential\ngeneration, such recurrent generation methods also have the advantage that they are naturally parallelizable."}, {"title": "2 Related Works", "content": "The original learning method for sub-goals prediction via a policy gradient in Jurgenson et al. [2020] has poor sample\nefficiency when recursion is deep. To improve on this, we propose an actor-critic learning method for midpoint\nprediction, which is similar to the actor-critic method [Konda and Tsitsiklis, 1999] for conventional reinforcement\nlearning. We prove theoretically that, under mild assumptions, if the training converges in the limit of infinite recursion\ndepth, the resulting policy can generate true geodesics. This result does not hold for generation by arbitrary intermediate\npoints.\nWe experimentally compared our proposed method, on five path (or motion) planning tasks, to sequential generation\nwith goal-conditioned reinforcement learning and midpoint tree generation trained by a policy gradient method without\na critic. Two tasks involved continuous metrics or constraints (local planning), while the other three involved obstacles\n(global planning). In both local and global planning, our proposed method outperformed baseline methods for the\ndifficult tasks."}, {"title": "2.1 Path Planning with Reinforcement Learning", "content": "One of the most popular approaches for path planning via reinforcement learning is to use a Q-table [Haghzad Klidbary\net al., 2017, Low et al., 2022]. However, that approach depends on the finiteness of the state spaces, and the computational\ncosts grow with the sizes of those spaces.\nSeveral studies have been conducted on path planning in continuous spaces via deep reinforcement learning [Zhou\net al., 2019, Wang et al., 2019, Kulathunga, 2022]. In those works, the methods depend on custom rewards."}, {"title": "2.2 Goal-Conditioned Reinforcement Learning and Sub-Goals", "content": "Goal-conditioned reinforcement learning [Kaelbling, 1993, Schaul et al., 2015] trains a universal policy for various\ngoals. It learns a value function whose inputs are both the current and goal states. Kaelbling [1993] and Dhiman et al.\n[2018] pointed out that goal-conditioned value functions are related to the Floyd-Warshall algorithm for the all-pairs\nshortest-path problem [Floyd, 1962], as this function can be updated by finding intermediate states. They proposed\nmethods that use brute force to search for intermediate states, which depend on the finiteness of the state spaces. The\nidea of using sub-goals for reinforcement learning as options was suggested by Sutton et al. [1999], and Jurgenson\net al. [2020] linked this notion to the aforementioned intermediate states. Wang et al. [2023] drew attention to the\nquasi-metric structure of goal-conditioned value functions and suggested using quasi-metric learning [Wang and Isola,\n2022] to learn those functions.\nThe idea of generating paths by predicting sub-goals recursively has been proposed in three papers with different\nproblem settings and methods. The problem setting for goal-conditioned hierarchical predictors [Pertsch et al., 2020]\ndiffers from ours because they use an approximate distance function learned from given training data, whereas no\ntraining data are given in our setting. Divide-and-conquer Monte Carlo tree search [Parascandolo et al., 2020] is similar\nto our learning method because it trains both the policy prior and the approximate value function, which respectively\ncorrespond to the actor and critic in our method. However, their algorithm depends on the finiteness of the state spaces.\nThe problem setting in Jurgenson et al. [2020] is the most similar to ours, but it remains different. In their setting, the\ncosts of direct edges between two states are given, which are upper bounds for the distances. In our setting, we can\nonly approximate distances between two states when they are close. Therefore, we must find waypoints such that the\nadjacent states are close. This is one reason why we use midpoint trees instead of sub-goal trees. In addition, because\nthey focus on high-level planning, their trees are not as deep as ours; accordingly, we propose an actor-critic method\nwhereas they use a policy gradient method without a critic."}, {"title": "3 Preliminary", "content": "After describing the general notions for quasi-metric spaces, we describe Finsler manifolds, which are an important\nexample of such a space."}, {"title": "3.1 Quasi-Metric Space", "content": "We follow the notation in Kim [1968]. Let X be a space. A pseudo-quasi-metric on X is a function $d : X \\times X \\rightarrow \\mathbb{R}$\nsuch that $d(x, x) = 0$, $d(x, y) \\geq 0$, and $d(x, z) \\leq d(x, y) + d(y, z)$ for any $x, y, z \\in X$. A topology on X is induced"}, {"title": "3.2 Finsler Geometry", "content": "An important family of quasi-metric spaces is the Finsler manifolds. A Finsler manifold is a differential manifold M\nequipped with a function $F : TM \\rightarrow [0, \\infty)$, where $TM = \\bigcup_{x\\in M} T_xM$ is the tangent bundle of M, and F satisfies\nthe following conditions [Bao et al., 2000].\n1. F is smooth on TM \\ 0.\n2. $F(x, \\lambda v) = \\lambda F(x, v)$ for all $\\lambda>0$ and $(x, v) \\in TM$.\n3. The Hessian matrix of $F(x, \\cdot)^2$ is positive definite at every point of $TM_x \\backslash 0$ for all $x \\in M$.\nLet $\\gamma : [0, 1] \\rightarrow M$ be a piecewise smooth curve on M. We define the length of $\\gamma$ as\n$$L(\\gamma) := \\int_0^1 F \\left(\\gamma(t), \\frac{d\\gamma}{dt}(t)\\right) dt.$$\nFor two points $x, y \\in M$, we define the distance d(x, y) as\n$$d(x, y) := \\inf \\{L(\\gamma) | \\gamma : [0,1] \\rightarrow M, \\gamma(0) = x, \\gamma(1) = y\\}.$$\nThen, d is a weakly symmetric quasi-metric [Bao et al., 2000]. A curve $\\gamma : [0, 1] \\rightarrow M$ is called a minimizing geodesic\nif $L(\\gamma) = d(\\gamma(0), \\gamma(1))$.\nIt is known [Bao et al., 2000, Amici and Casciaro, 2010] that, for any point of M, there exists a neighborhood $U \\subset M$\nsuch that any two points $p, q \\in U$ can be uniquely connected by a minimizing geodesic inside U. Note that (U, d) has\nthe continuous midpoint property."}, {"title": "4 Theoretical Results", "content": "We briefly explain our main idea in \u00a7 4.1 and prove propositions justifying it in \u00a7 4.2 and \u00a7 4.3. We prove that the local\nassumptions in the proposition are satisfied in Finsler case in \u00a7 4.4 and consider environments with obstacles in \u00a7 4.5."}, {"title": "4.1 Midpoint Tree", "content": "The midpoint tree is a modification of the sub-goal tree proposed by Jurgenson et al. [2020]. In the sub-goal tree\nframework, a policy learns to predict an intermediate point between two given points, instead of the next point as in\nthe sequential framework. Paths between start and goal points are generated by recursively applying this prediction to\nadjacent pairs of previously generated waypoints. In our midpoint tree framework, instead of predicting an arbitrary\nintermediate point, the policy predicts a midpoint. This modification enables appropriate generation even when metrics\nare only locally known.\nWe train the policy (actor) to predict midpoints by an actor-critic method. In other words, we simultaneously train a\nfunction (critic) to predict distances. In the following subsections, we describe the theoretical considerations of this\nactor-critic learning method."}, {"title": "4.2 Functional Equation", "content": "Let (X, d) be a pseudo-quasi-metric space. When we do not know d globally, we want to train a function (actor)\n$\\pi : X \\times X \\rightarrow X$ to predict midpoints. We also train a function (critic) $V : X \\times X \\rightarrow \\mathbb{R}$ to predict distances."}, {"title": "4.3 Iteration", "content": "Although we do not know the metric d globally, we assume that we know it infinitesimally, that is, we have a continuous\nfunction $C: X \\times X \\rightarrow \\mathbb{R}$ that coincides with d in the limit $d(x, y) \\rightarrow 0$ or $C(x, y) \\rightarrow 0$. Formally, we assume the\nfollowing conditions:\nAssumption 3.\n1. For $x \\in X$ and a series $(y_i)_{i} \\in X^{\\mathbb{N}}$, if $C(x, y_i) \\rightarrow 0$, then $d(x, y_i) \\rightarrow 0$.\n2. For $x \\in X$ and $\\varepsilon>0$, there exists $\\delta>0$ such that, for any $y, z \\in B_{\\delta}(x)$,\n$$(1 - \\varepsilon)d(y, z) \\leq C(y, z) \\leq (1 + \\varepsilon)d(y, z).$$\nStarting with C, we consider iterative construction of actors and critics to satisfy (3) and (4). Formally, we assume series\nof functions, $V_i : X \\times X \\rightarrow \\mathbb{R}$ and $\\pi_i : X \\times X \\rightarrow X$, indexed by $i = 0,1,\\dots$, that satisfy the following conditions:\n$$V_0(x, y) = C(x, y),$$"}, {"title": "4.4 Finsler Case", "content": "We consider the case where (X, d) is a Finsler manifold (M, F) with a coordinate system (a diffeomorphism to a\nsubset) $f : M\\rightarrow \\mathbb{R}^d$. We define C as\n$$C(x, y) := F \\left(x, df^{-1}(f(y) - f(x))\\right),$$"}, {"title": "4.5 Free Space", "content": "Here, we consider the situation where there exist a free space (a path-connected closed subset) $M_{\\text{free}} \\subset M$, and we\nwant to generate paths inside $M_{\\text{free}}$. We modify d to\n$$d'(x, y) := d(x, y) + P(x,y)$$\nwhere\n$$P(x, y) := \\begin{cases} 0 & \\text{ if } x, y \\in M_{\\text{free}} \\text{ or } x, y \\notin M_{\\text{free}},\nc_P & \\text{ otherwise,} \\end{cases}$$\nand $c_P \\gg 0$ is a constant. Clearly, d' is also a quasi-metric. When $M_{\\text{free}}$ is compact and $c_P$ is large enough, midpoints\nof any pair of points in $M_{\\text{free}}$ with respect to d' lie in $M_{\\text{free}}$.\nLet\n$$C'(x,y) := C(x,y) + P(x,y).$$\nObviously, C' satisfies Assumption 3 with respect to d'."}, {"title": "5 Experiments", "content": "In \u00a7 5.1, by building on the previous section's results, we describe our proposed learning algorithm for midpoint\nprediction. In the following subsections, we compared our method, which generates geodesics via policies trained by\nour algorithm, with baseline methods on several path planning tasks. The first two environments deal with asymmetric\nmetrics that change continuously (local planning tasks), while the other three environments have simple and symmetric\nmetrics but involve obstacles (global planning tasks)."}, {"title": "5.1 Learning Algorithm", "content": "Let (X, d) be a pseudo-quasi-metric space, and let C be a function satisfying Assumption 3.\nWe simultaneously train two networks with parameters $\\theta$ and $\\phi$: the actor $\\pi_{\\theta}$, which predicts the midpoints between\ntwo given points, and the critic $V_{\\phi}$, which predicts the distances between two given points. The critic learns to predict\ndistances from the lengths of the sequences generated by the actor, while the actor learns to predict midpoints from the\ncritic's predictions.\nFor learning efficiency, we train only one actor and one critic, unlike in \u00a7 4.3. Instead, we gradually increase the depth\nfor data collection to train the critic, so that C is called for closer pairs of points. In this way, $V$ and $\\pi$ start by learning\nvalues of $V_0$ and $\\pi_0$ and gradually learn the values of $V_i$ and $\\pi_i$ for higher i. Note that, since C is closer to the true\ndistance d for pairs of closer points, $V_i$ and $\\pi_i$ converge more quickly in the region of pairs of closer points.\nThe network for actor $\\pi_{\\theta}$ has a form for which the reparameterization trick [Haarnoja et al., 2018] can be applied,\nthat is, a sample is drawn by computing a deterministic function of the input, parameters $\\theta$, and an independent\nnoise. Henceforth, we abuse notations and denote a sampled prediction from $\\pi_{\\theta}(\\cdot | s, g)$ by $\\pi_{\\theta}(s, g)$ even if it is not\ndeterministic."}, {"title": "5.2 Tasks and Evaluation Method", "content": "We compared the methods in terms of their success rate on the following task. A Finsler manifold (M, F) with a\nglobal coordinate system $f : M \\rightarrow \\mathbb{R}^d$ (and a free space $M_{\\text{free}} \\subset M$) is given as an environment. The number n of\nsegments to approximate paths and a proximity threshold $\\varepsilon>0$ are also fixed. For our method, n must be a power\nof two: $n = 2^{D_{\\text{max}}}$, where $D_{\\text{max}}$ is the midpoint trees' depth for evaluation. When two points $s, g \\in M_{\\text{free}}$ are given,\nwe want to generate a sequence $s = p_0, p_1,..., p_n = g$ of points such that no value of C for two consecutive points\nis greater than $\\varepsilon$, where C is defined by (31) and, for cases with obstacles, (40). If the points generated by a method\nsatisfy this condition, then it is considered successful in solving the task; otherwise, it is considered to have failed. Note\nthat, for cases with obstacles, when $c_P > \\varepsilon$, successes imply that all waypoints are lying on the free space.\nFor each environment, we randomly generated 100 pairs of points from the free space, before the experiment. During\ntraining, we evaluated the models regularly by solving the tasks for the prepared pairs and recorded the success rates.\nWe ran each method with different random seeds ten times for the environment described in \u00a7 5.4.1 and five times for\nthe other environments.\nFor each environment, the total number T of timesteps was fixed for all methods. Therefore, at Line 2 of our method,\nwe continue learning until the number of timesteps reaches the defined value. Timesteps were measured by counting the\nevaluation of C during training. In other words, for sequential reinforcement learning (see the next subsection), the\ntimesteps have their conventional meaning. For the other methods, one generation of a path (one cycle) with depth D is\ncounted as $2^D$ timesteps."}, {"title": "5.3 Compared Methods", "content": "The baseline methods were as follows:\n\\bullet Sequential Reinforcement Learning (Seq): We formulated sequential generation of waypoints as a conven-\ntional, goal-conditioned reinforcement learning environment. The agent moves to the goal step by step in M.\nIf the agent is at p, it can move to q such that $F(p, df_p^{-1}(f(q) - f(p))) = \\varepsilon$. If q is outside the free space\n$M_{\\text{free}}$, the reward R is set to $-c_P$ and the episode ends as a failure. Otherwise, R is defined as\n$$R := -\\varepsilon + F \\left(g, df_p^{-1}(f(g) - f(p))\\right) - F \\left(g, df_q^{-1}(f(g) - f(q))\\right),$$\nwhere g is the goal. The discount factor is set to 1. An episode ends and is considered a success when the\nagent reaches a point p that satisfies $F \\left(p, df_p^{-1}(f(g) - f(p))\\right) < \\varepsilon$. When the episode duration reaches n\nsteps without satisfying this condition, the episode ends and is considered a failure.\nWe used Proximal Policy Optimization (PPO) [Schulman et al., 2017] to solve reinforcement learning problems\nwith this formulation.\n\\bullet Policy Gradient (PG): We modified the method in Jurgenson et al. [2020] to predict midpoints. For each\n$D = 1,..., D_{\\text{max}}$, a stochastic policy $\\pi_D$ is trained to predict midpoints with depth D. To generate waypoints,\nwe apply the policies in descending order of the index. We train the policies in ascending order of the index by\nthe policy gradient.\nTo predict midpoints, the value to be minimized in training is changed. Let $\\rho(\\pi_1, ..., \\pi_D)$ be the distribution of\n$\\tau := (p_0, ..., p_{2^D})$, where $p_0$ and $p_{2^D}$ are sampled from the predefined distribution on M and $p_{2^{i-1}(2j+1)}$ is\nsampled from the distribution $\\pi_i \\left(\\cdot | p_{2^ij}, p_{2^i(j+1)}\\right)$. Let $\\theta_D$ denote the parameters of $\\pi_D$. Instead of minimizing\nthe expected value of $\\sum_{i=0}^{2^D-1} C(p_i, p_{i+1})$ as in the original method, we train $\\pi_D$ to minimize the expected\nvalue of\n$$C := \\frac{ \\left(\\sum_{i=0}^{2^{D-1}-1} C(p_i, p_{i+1})\\right)^2 }{2} + \\frac{ \\left(\\sum_{i=2^{D-1}}^{2^D-1} C(p_i, p_{i+1})\\right)^2 }{2}.$$\nHere, we use\n$$\\nabla_{\\theta_D} \\mathbb{E}_{\\rho(\\pi_1,...,\\pi_D)}[C_\\tau] = \\mathbb{E}_{\\rho(\\pi_1,...,\\pi_D)} \\left[\\left(C_\\tau - b \\left(p_0, p_{2^D}\\right)\\right) \\nabla_{\\theta_D} \\log \\pi_D \\left(p_{2^{D-1}} | p_0, p_{2^D}\\right)\\right],$$\nwhere b is a baseline function.\nNote that, when the model is evaluated during training, if the current trained policy is $\\pi_D$ ($1 \\leq D < D_{\\text{max}}$),\nthen evaluation is performed with depth D ($2^D$ segments). (The other methods are always evaluated with\n$n = 2^{D_{\\text{max}}}$ segments.)\nFor our proposed method described in \u00a75.1, we tried two scheduling strategies to increase the trees' depth for training\nfrom zero to $D_{\\text{max}}$, under the condition of fixed total timesteps.\n\\bullet Timestep-Based Depth Scheduling (Our-T): For each depth, training lasts the same number of timesteps.\n\\bullet Cycle-Based Depth Scheduling (Our-C): For each depth, training lasts the same number of calls to the data\ncollection procedure (cycles).\nMore precisely, at Line 17 in Algorithm 1, for timestep-based depth scheduling, the depth is $\\lfloor t/t_a \\rfloor$, where t is the\nnumber of timesteps at the current time and $t_a := \\lfloor T/D_{\\text{max}} \\rfloor + 1$. For cycle-based depth scheduling, the depth for the\nc-th call to the data collection procedure is $\\lfloor c/c_a \\rfloor$, where $c_a := \\lfloor T/(2^{D_{\\text{max}}+1} - 1) \\rfloor + 1$. Note that Our-T provides\nmore training for low depths than Our-C does.\nIn addition, we ran the following variants of our method."}, {"title": "5.4 Environments", "content": "We experimented in the following five environments."}, {"title": "5.4.1 Matsumoto Metric", "content": "The Matsumoto metric is an asymmetric Finsler metric that considers times to move on inclined planes [Matsumoto,\n1989]. Let $M \\subset \\mathbb{R}^2$ be a region on the plane with the standard coordinates x, y, and let $h : M \\rightarrow \\mathbb{R}$ be a differentiable\nfunction that indicates heights of the field. The Matsumoto metric $F : TM \\rightarrow [0, \\infty)$ is then defined as follows:\n$$F(p, (v_x, v_y)) := \\frac{\\sqrt{v_x^2 + v_y^2}}{\\alpha - \\beta},$$\nwhere\n$$\\beta := v_x \\frac{\\partial h}{\\partial x}(p) + v_y \\frac{\\partial h}{\\partial y}(p), \\quad \\alpha:= \\sqrt{v_x^2 + v_y^2 + \\beta^2}.$$\nWe take the unit disk as M and let $h(p) := -||p||^2$. Intuitively, we consider a round field with a mountain at the center.\nFor this environment, we set the number of segments $n = 64$ ($D_{\\text{max}} = 6$), the proximity threshold $\\varepsilon=0.1$, and the\ntotal number of timesteps $T = 2 \\times 10^7$."}, {"title": "5.4.2 Unidirectional Car-Like Constraints", "content": "Inspired by the cost function for trajectories of car-like non-holonomic vehicles [R\u00f6smann et al., 2017], we define an\nasymmetric Finsler metric for unidirectional car-like vehicles.\nLet $M := U \\times S^1 \\subset \\mathbb{R}^3$ be a configuration space for car-like vehicles, where $U \\subseteq \\mathbb{R}^2$ is the unit disk with the standard\ncoordinate system x, y and $S^1$ is the unit circle with the coordinate $\\theta$. We define $F : TM \\rightarrow [0, \\infty)$ as follows:\n$$F((p, \\theta), (v_x, v_y, v_\\theta)) := \\sqrt{v_x^2 + v_y^2} + c_c(h^2 + \\xi^2),$$\nwhere\n$$h := -v_x \\sin(\\theta) + v_y \\cos(\\theta),$$"}, {"title": "5.4.3 2D Domain with Obstacles", "content": "We consider a simple 2D domain with rectangular obstacles, as shown in Figure 4c, where the white area is the free\nspace, which is taken from an experiment in Jurgenson et al. [2020]. The metric is simply Euclidean. Note that we also\nconsider the outside boundary as unsafe.\nWe set $n = 64$ ($D_{\\text{max}} = 6$), $\\varepsilon=0.1$, $T = 4 \\times 10^7$, and the collision penalty $c_P = 10$."}, {"title": "5.4.4 7-DoF Robotic Arm with an Obstacle", "content": "This environment is defined for motion planning of the Franka Panda robotic arm, which has seven degrees of\nfreedom (DoFs), in the presence of a wall obstacle. The space is the robot's configuration space, whose axes\ncorrespond to joint angles. The metric is simply Euclidean in the configuration space. The obstacle is defined as\n$\\left\\{x > 0.1, -0.1 < y < 0.1\\right\\}$, and a state is considered unsafe if at least one of the segments connecting adjacent joints\nintersects the obstacle. We ignored self-collision.\nWe set $n = 64$ ($D_{\\text{max}} = 6$), $\\varepsilon=0.2$, $T = 4 \\times 10^7$, and $c_P = 10$."}, {"title": "5.4.5 Three Agents in the Plane", "content": "We consider three agents in $U := [-1,1] \\times [-1,1] \\subseteq \\mathbb{R}^2$. The configuration space is $M := U^3$ and the metric is\ndefined as the sum of Euclidean metrics of three agents. A state is considered safe if all distances of pairs of agents are\nnot smaller than $d_{\\text{thres}} := 0.5$, that is,\n$$M_{\\text{free}} := \\left\\{(p_0, p_1, p_2) \\in M | ||p_i - p_j|| \\geq d_{\\text{thres}} \\text{ for } i \\neq j \\right\\}.$$\nWe set $n = 64$ ($D_{\\text{max}} = 6$), $\\varepsilon=0.2$, $T = 8 \\times 10^7$, and $c_P = 10$."}, {"title": "5.5 Results and Discussion", "content": "Figure 3 shows the learning curves of the success rates for all methods averaged over random seeds. The error bars\nrepresent the standard errors. While Seq achieved the best success rate in the Matsumoto and 2D obstacles environments,\nits success rates were low and our proposed method had the best in the other environments. It may be much more\ndifficult to determine directions of the agent sequentially in higher dimensional environments than in two-dimensional\nones. Also, the approximation of distances in (49) may be close to the true values in the Matsumoto environment,\nbut not in the car-like environment. Note that, while it may be possible to improve learning success rate for Seq by\nengineering rewards, our method was successful without adjusting rewards.\nThe success rates for Inter decreased as training progressed in the Matsumoto and 2D obstacle environments, which\nmay have resulted from convergence to biased generation, as mentioned in Remark 2. The success rates for Cut did not\ndecrease as for Inter, while they are lower than those for our method."}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we proposed a framework, called midpoint trees, to generate geodesics by recursively predicting midpoints.\nWe also proposed an actor-critic method for learning to predict midpoints, and we theoretically proved its soundness.\nExperimentally, we showed that our method can solve path planning tasks that existing reinforcement learning methods\nfail to solve.\nOur experiments showed that effective depth scheduling depends on the task. Exploration of an efficient depth scheduling\nalgorithm that considers learning progress is a future challenge. In addition, we tried only a straightforward actor-\ncritic learning algorithm in this paper, while algorithms for actor-critic reinforcement learning have been intensively\nresearched. Investigation of more efficient algorithms thus also remains for future work. For example, while we used an\non-policy algorithm, off-policy algorithms [Degris et al., 2012] may be useful in our framework. The architectures we\nused for both actors and critics were also simple, but the quasi-metric learning method [Wang and Isola, 2022, Wang\net al., 2023] may be useful for critics in our approach.\nIn our method, a policy must be learned for each environment. By modifying our method so that the actor and critic\ninput information on environments as in Sartori et al. [2021], it may be possible to learn a policy that is applicable to\ndifferent environments.\nWhile we assumed continuity of the policy function to prove our approach's theoretical soundness, the continuous\nmidpoint property may only be satisfied locally. Further research is needed on the conditions under which our methods\nwork. Even if our method does not work well globally, we could consider dividing manifolds, training policies for each\nregion, and then connecting locally generated geodesics."}]}