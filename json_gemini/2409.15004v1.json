{"title": "ViBERTgrid BiLSTM-CRF: Multimodal Key Information Extraction from Unstructured Financial Documents", "authors": ["Furkan Pala", "Mehmet Yasin Akp\u0131nar", "Onur Deniz", "G\u00fcl\u015fen Eryi\u011fit"], "abstract": "Multimodal key information extraction (KIE) models have been studied extensively on semi-structured documents. However, their investigation on unstructured documents is an emerging research topic. The paper presents an approach to adapt a multimodal transformer (i.e., ViBERTgrid previously explored on semi-structured documents) for unstructured financial documents, by incorporating a BiLSTM-CRF layer. The proposed ViBERTgrid BiLSTM-CRF model demonstrates a significant improvement in performance (up to 2 percentage points) on named entity recognition from unstructured documents in financial domain, while maintaining its KIE performance on semi-structured documents. As an additional contribution, we publicly released token-level annotations for the SROIE dataset in order to pave the way for its use in multimodal sequence labeling models.", "sections": [{"title": "Introduction", "content": "Documents play a crucial role in our daily lives, serving as a means of communication and record keeping. They can be written, printed, or electronic, and are often used as official records or to provide information or evidence. These documents can be categorized based on their structure and the way they present information. Structured documents are highly organized, often containing tabular data and rich visual elements. Semi-structured documents have some level of organization, but may not follow a strict structure. Unstructured documents, on the other hand, do not have a predetermined structure and tend to be more text-intensive, with fewer visual or formatted elements such as tabular like layouts."}, {"title": "Related Work", "content": "Pipelines for information extraction from documents may contain many subtasks including but not limited to document classification, optical character recognition (OCR), named entity recognition (NER) and relation extraction (RE). Although there exist tasks with complex n-ary nested relations which could only be solved by sophisticated RE algorithms, \"most modern methods considered key information extraction (KIE) as a sequence tagging problem and solved through NER.\" However, some KIE datasets lack token-level annotations and researchers use in-house solutions to solve this issue. That is why in this study, we release a token-level annotation layer for SROIE dataset. Before the rise of deep neural networks, Conditional Random Fields (CRFs) were widely used for NER. CRFs are statistical models that take context into account by modeling dependencies and relationships between predictions as a finite state machine. Because the NER task can be seen as a sequence labeling task, the \"linear chain\" CRFs were a popular choice since each prediction is dependent on its immediate neighbors. However, as the meaning of a word can depend not only on its immediate surroundings but also on further words, models with larger context windows have become necessary.\nDeep neural networks, specifically recurrent neural networks (RNNs) and their variants, such as long short-term memory (LSTM) and gated recurrent units (GRUs) have become the state-of-the-art models for NER. In this context, Bidirectional LSTM-CRF (BiLSTM-CRF) models have been shown to be reliable and robust architectures for sequence tagging tasks as they can take advantage of both past and future tokens and capture sentence level meaning. In , it was observed that replacing a simple MLP with a BiLSTM-CRF layer significantly improved the performance of BERT on the NER task. Overall, deep neural networks have been able to achieve better performance on NER tasks than pure CRF architerctures by modeling larger context windows and learning more complex relationships between predictions.\nWith the introduction of large language models like ELMO , BERT, and GPT-3 , the performance of NER tasks has significantly improved. These models are pre-trained in an unsupervised manner on massive amounts of text and can learn rich representations of the language. They can be fine-tuned on specific NER tasks and provide contextualized word embeddings, which capture the meaning of words in the context of a sentence. This allows the model to better understand the relationships between words and make more accurate predictions. Additionally, these models can also be used in a transfer learning setting, where the pre-trained language model is used as a feature extractor for NER tasks, further improving performance. The use of these large language models in NER tasks has become a standard approach in the field of natural language processing, achieving state-of-the-art performance on many NER benchmarks.\nThe usage of multimodal models using textual, layout and visual information is a promising area of research in NER task. These models are able to take advantage of the layout and visual structure of documents, such as tables and figures, to improve their understanding of the text and make more accurate predictions. For example, by incorporating information about the position of entities within a table, these models can better identify and extract entities and their relationships. This approach could be especially useful in domains such as scientific literature, where tables and figures are commonly used to present data. Further research in this area could lead to significant advancements in the field of NER.\nIn this regard, [16] introduced a new method called Chargrid for representing documents by converting each page into a 2D grid of characters. While this representation is useful for preserving layout and spatial features in structured documents, it may not be as effective in dealing with unstructured documents. This is because the representation relies solely on character information to encode the text, which may not be sufficient for understanding the content of the document. Nevertheless, in unstructured documents, the meaning of a character is often dependent on the surrounding context and the word it belongs to, and encoding the text using only character information can lead to a loss of important information.\nAlthough, Chargrid representation is still useful in certain circumstances and can provide valuable insights when analyzing structured documents, it is not leveraging language models for extracting rich word embeddings. To address this deficiency, [6] modified the Chargrid architecture to represent a document as a grid of contextualized word piece embedding vectors, called BERTgrid. The BERTgrid method involves representing a document as a grid of contextualized word piece embedding vectors, which are obtained from a BERT language model. By using this representation, the spatial structure and semantics of the document become easily accessible to the neural network processing it. This allows for more efficient and effective analysis of the document by the neural network, as it is able to easily understand the meaning and context of the words within the document. The use of contextualized embedding vectors, which contain rich information about the language and context in which words are used, further enhances the ability of the neural network to accurately process and analyze the document.\nAlthough BERTgrid has been successful in using BERT to obtain contextualized word embeddings for key information extraction, the parameters of BERT are frozen during training, which limits the full potential of the language model. In an effort to address this issue, [19] proposed a new architecture called ViBERTgrid and a joint-training strategy to improve the accuracy of grid-based key information extraction models. ViBERTgrid incorporates a multi-modal feature extractor, which combines BERTgrid with a CNN to process the scanned page image with the layout and textual information. Using a joint-training technique that allows for the finetuning of both the BERT and CNN models, ViBERTgrid enables BERT to learn context-specific word embeddings more effectively."}, {"title": "Methodology", "content": "The approach, as shown in Fig. 2, consists of three main elements:\n1.  A multimodal backbone network that generates the ViBERTgrid feature map."}, {"title": "ViBERTgrid Feature Map", "content": "To create the ViBERTgrid representation, we first generate the BERTgrid representation and fuse it with an intermediate layer of image processing network, i.e., a backbone convolutional neural network (CNN). While BERTgrid provides textual and positional features, combining it with visual features makes the ViBERTgrid a multimodal and comprehensive embedding.\nBERTgrid Generation Following [6,19], given a document image, OCR engine outputs words and corresponding bounding boxes. Let us represent the words as a sequence $D = [w^{(1)}, w^{(2)}, ..., w^{(N)}]$ of length N. The bounding box for each word $w^{(i)}$ is then denoted as $B^{(i)} = (x_{min}^{(i)}, y_{min}^{(i)}, x_{max}^{(i)}, y_{max}^{(i)})$ where $(x_{min}^{(i)}, y_{min}^{(i)})$ and $(x_{max}^{(i)}, y_{max}^{(i)})$ are the top-left and bottom-right corner points for the bounding box. Then, we tokenize each word in $D$ using the wordpiece (sub-word) tokenizer, resulting in a sequence of sub-word tokens denoted as $T = [t^{(1)}, t^{(2)},..., t^{(M)}]$. It is important to note that the number of sub-word tokens $M$ is typically greater than the number of words $N$, as tokenization can often split a single word into multiple sub-word tokens. Because we are interested in getting word embeddings rather than token embeddings, we maintain the word indices for each token. This allows us to calculate the embedding for"}, {"title": "Early Fusion", "content": "To incorporate BERTgrid with visual features, we merge the generated BERTgrid G with an intermediate layer of the CNN. This allows us utilize the backbone image processing CNN as a new multimodal network. As the architecture of CNN, we use a pretrained ResNet34-FPN [9] network. The early fusion process follows the same approach as in [19] and outputs produce a highly informative multimodal feature map $P_{fuse} \\in R^{(\\frac{H}{S})\\times(\\frac{W}{S})\\times256}$ for document understanding tasks."}, {"title": "Word-Level Classification Head", "content": "Although key-information fields may span multiple words, the aim of this task is to classify each word into one of the predefined field labels such as company, address, date and total in the SROIE dataset. While C denotes the number of predefined field labels, we also have the other label for words we are not interested in, which makes C + 1 labels in total. Following [19], we use ROIAlign [8] to extract a 7x7x256 feature map $X_{i}$ for each word $w^{(i)}$ from $P_{fuse}$ features, by aligning them with the word bounding boxes. To allow error gradients to propagate directly from the output layer to the BERT encoder layers, we pass the embedding of each word $E(w^{(i)})$ to the word-level field type classification head through an additional skip connection. This time, we use a late fusion module in order to fuse $X_{i}$ and $E(w^{(i)})$ for each word. This module convolves each feature map $X_{i}$ with two 3x3 convolutional layers, generating a new 7x7x256 feature"}, {"title": "Classification Using Linear Layers", "content": "According to Lin et al. [19], incorporating two classifiers in parallel can enhance the convergence of the model. These classifiers are fully connected layers. The first classifier conducts binary classification on each word, predicting whether it belongs to any of the pre-defined C labels or not. The second classifier performs multi-class classification to assign one of the C + 1 labels to each word. It is worth noting that the first classifier only contributes to the loss during the training phase, and the second classifier is solely responsible for labeling each word during inference. To achieve this, we input a 1024-dimensional feature vector $t_{i}$ for each word $w^{(i)}$ into both classifiers in parallel, and obtain a 2-dimensional logits vector $o_{1}(w^{(i)})$ and a (C+1)-dimensional logits vector $o_{2}(w^{(i)})$ from the first and second classifiers, respectively. We can compute the losses for the first and second classifiers, denoted by $L_{word_{1}}$ and $L_{word_{2}}$, respectively, as follows:\n$L_{word_{1}} = \\frac{1}{N} \\sum_{i=1}^{N} CELoss(o_{1}(w^{(i)}), y_{1}(w^{(i)}))$\n$L_{word_{2}} = \\frac{1}{N} \\sum_{i=1}^{N} CELoss(o_{2}(w^{(i)}), y_{2}(w^{(i)}))$\nwhere $y_{1}(w^{(i)}), y_{2}(w^{(i)})$ represent the ground-truth labels for the word $w^{(i)}$ for the first and second classifier, respectively. N is number of words in the document detected by OCR. We used label weighting in cross entropy loss where $w_{c}$ is the weight for each label computed using the ENet weighting [25] as $w_{c} = ln(\\frac{k}{p_{c}})$, $p_{c}$ is the probability of label c in the train set and k = 1.02. Finally, we define the word-level classification loss as $L_{word} = L_{word_{1}} + L_{word_{2}}$."}, {"title": "Classification Using BiLSTM-CRF Layer", "content": "The architecture of a BiLSTM-CRF layer consists of two main components: a bidirectional LSTM (BiLSTM) layer and a conditional random field (CRF) layer. Given a list of words $D = [w^{(1)}, w^{(2)}, ..., w^{(N)}]$ in the document, we compute the fused feature vectors $X = [x_{1}, x_{2},\u2026\u2026, x_{N}]$ for each word. Then, we feed X into a BiLSTM layer to get two sequences of hidden states $\\overrightarrow{H}, \\overleftarrow{H}$ for forward and backward direction, respectively. We set the hidden size for the BiLSTM layer to 512, thus the hidden state $h^{(i)}$ for the word $w^{(i)}$ is a 512-d vector. At the end of the BiLSTM layer, these two hidden sequences are concatenated into a single sequence of hidden"}, {"title": "Auxiliary Semantic Segmentation Head", "content": "As reported in [19], training the network with a pixel-wise semantic segmentation loss results in a faster and more stable convergence. This approach has been utilized in several architectures such as Chargrid [16] and BERTgrid [6] as they rely on classifying each pixel into one of the predefined labels based on the bounding boxes of words. Semantic segmentation head has two classifiers. The first classifies pixels into 3 categories: inside a bounding box for a word with an interested label (from C categories stated in Sec. 3.2), inside a bounding box for a word with the \"other\" label, or not in any bounding box. The second classifier directly classifies each pixel into one of C + 1 labels. We use exactly the same structure for auxiliary semantic segmentation head with [19].\nOverall, our introduced architecture above differs slightly from [19] in that 1) ResNet34-FPN was used in CNN backbone instead of ResNet18-FPN, 2) single multi-class classifier was used instead of multiple binary classifiers at word-level classification and auxiliary semantic segmentation heads, 3) ENet loss weighting strategy was used instead of positive and negative word/pixel sampling."}, {"title": "Experimental Setup", "content": "We use two types of datasets (i.e., semi-structured and unstructured) in order to evaluate the proposed approach."}, {"title": "Datasets", "content": "SROIE: The ICDAR SROIE dataset is a publicly available collection of 973 receipts, comprising of 626 training and 347 testing samples. This dataset is highly used in semi-structured information extraction studies and consists of four types of entities: company, date, address, and total. However, direct use of this dataset within models requiring word bounding boxes, as is the case for our models, is not trivial. The ground-truth labels for key-information fields consist only of text, while the spatial information coming from OCR is separately provided for each receipt. Thus, one needs to match each word in the key-information fields with the corresponding word in OCR data. Such matching algorithms do exist [20,5], but they often result in poor matching, leading to suboptimal final performance. Consequently, we manually annotated the entire dataset by accurately identifying and assigning key-information labels to each word using the available OCR output. This approach rendered the dataset usable for ViBERTgrid training. However, evaluating the model's performance on the test set remains problematic due to discrepancies between OCR outputs and key-information fields, such as mismatched punctuation, extra or missing white spaces between words, and typos [12]. Nevertheless, [2] has identified and documented some of these errors over the test set, enabling us to manually rectify them and improve our model's performance.\nTransactional Documents: The unstructured dataset used in this study comprises two sets of Turkish money transfer order documents, previously introduced in [23]: UTD and UMTD. The UTD dataset contains 3500 money transfer order documents, which were selected from real banking flows. For consistency with the prior work, we used the splits provided in [23] as 2500 documents for training (UTDtrain), 400 for validation, and 600 for testing (UTDtest). It is reported that only 7% of UTD contains multiple transaction information within a single document. On the other hand, UMTD contains 1154 documents which are entirely multi-transactional. We used the same splits as in the original work: 954 documents for training (UMTDtrain) and 200 for testing (UMTDtest). Within the UMTDtest, [23] identified 54 out of 200 documents with tabular-like layouts and presented the results separately for tabular-like (TLL) and non-tabular-like (noTLL) documents. Similarly, we also present our results separately to observe the effect of the BiLSTM-CRF layer on these."}, {"title": "Hyperparameters & Evaluation", "content": "We pre-trained the BERT model on a vast corpus of the banking domain and utilized it in both architectures. While adopting the joint training methodology presented in [19], we made a modification to the optimization approach. In the original method, the CNN module was optimized using SGD while the BERT module was optimized using AdamW. However, in our implementation, we used two separate AdamW optimizers to train both the BERT and CNN models. This is because we found through our experiments that SGD often resulted in NaN gradients and was more difficult to converge compared to AdamW. The BERT optimizer has a learning rate of 5e-5 and weight decay of 0.01, while the CNN optimizer has a learning rate of 1e-4 and weight decay of 0.005. To prevent overfitting, we decayed the learning rate of both optimizers by a factor of 0.1 whenever there was no improvement in the validation micro F1 score for the past 5 epochs. We used a batch size of 2 due to large memory requirements of ViBERTgrid. As in , we used multi-scale training where the shorter size of an image is randomly selected from {320, 416, 512,608, 704} with a constraint that the longer side does not exceed 800. In the testing time, we set the shorter side of each image to 512 for transactional documents dataset and 704 for SROIE dataset.\nTo evaluate the performance of our models, we used the official SROIE evaluation script to obtain the macro F1 scores on the SROIE test set. For UTD and UMTD datasets, we adopted the field-level NER F1 score evaluation methodology used by [22]. To ensure the reliability of our results, we trained and tested each model 5 times and reported the mean and standard deviation of the F1 scores. For experiments with F1 scores too far from the mean, we re-conducted them to confirm the validity of the results.\nTo test the significance of our results, we used McNemar's test between the merged predictions of both architectures across five runs. We selected the significance level as a = 0.05."}, {"title": "Experiments & Results", "content": "In Table 1, we compare the performance of vanilla ViBERTgrid with ViBERTgrid BiLSTM-CRF architectures on the SROIE dataset. It is worth noting that we applied post-processing techniques on the predictions of both models using regular expressions from [2]. These techniques involve removing unwanted tokens from predictions, such as tax numbers from company names, and extracting date and total fields using rule-based approaches, such as regex pattern matching, when the model failed to predict or predictions were removed after cleaning. The post-processing techniques were applied consistently to both architectures."}, {"title": "Discussions & Conclusions", "content": "[23] provided valuable insights into the impact of visual features on relation extraction from unstructured financial documents. However, the NER stage was left unexplored with the assumption that visual features would be more valuable at the relation extraction stage. In this paper, we focused on the impact of using a multimodal transformer (i.e., ViBERTgrid previously explored on semi-structured documents) on the NER task from unstructured financial documents. However, the initial results showed that the original ViBERTgrid has a negative impact on unstructured documents compared to a pure textual baseline. The paper presented an approach to enhance the performance of ViBERTgrid on unstructured documents by extending it with a BiLSTM-CRF layer. As a result, our proposed ViBERTgrid BiLSTM-CRF model demonstrated a significant improvement in performance (up to 2 percentage points) on unstructured documents, while maintaining its performance on semi-structured documents, in the domain of financial and banking documents. As an additional contribution, we publicly released token-level annotations for the SROIE dataset to pave the way for its use in multimodal sequence labeling models."}, {"title": "Limitations", "content": "Although our study focused on named entity recognition task, the effectiveness of our approach on relation extraction remains unexplored. Therefore, as a future study, we plan to investigate the performance of our architecture on relation extraction and compare it with state-of-the-art multimodal relation extraction models."}]}