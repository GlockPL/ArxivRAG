{"title": "A COMPLEXITY-BASED THEORY OF COMPOSITIONALITY", "authors": ["Eric Elmoznino", "Thomas Jiralerspong", "Yoshua Bengio", "Guillaume Lajoie"], "abstract": "Compositionality is believed to be fundamental to intelligence. In humans, it underlies the structure of thought, language, and higher-level reasoning. In AI, compositional representations can enable a powerful form of out-of-distribution generalization, in which a model systematically adapts to novel combinations of known concepts. However, while we have strong intuitions about what compositionality is, there currently exists no formal definition for it that is measurable and mathematical. Here, we propose such a definition, which we call representational compositionality, that accounts for and extends our intuitions about compositionality. The definition is conceptually simple, quantitative, grounded in algorithmic information theory, and applicable to any representation. Intuitively, representational compositionality states that a compositional representation satisfies three properties. First, it must be expressive. Second, it must be possible to re-describe the representation as a function of discrete symbolic sequences with re-combinable parts, analogous to sentences in natural language. Third, the function that relates these symbolic sequences to the representation\u2014analogous to semantics in natural language-must be simple. Through experiments on both synthetic and real world data, we validate our definition of compositionality and show how it unifies disparate intuitions from across the literature in both AI and cognitive science. We also show that representational compositionality, while theoretically intractable, can be readily estimated using standard deep learning tools. Our definition has the potential to inspire the design of novel, theoretically-driven models that better capture the mechanisms of compositional thought.", "sections": [{"title": "1 Introduction", "content": "Compositionality is thought to be one of the hallmarks of human cognition. In the domain of language, it lets us produce and understand utterances that we have never heard before, giving us \u201cinfinite use of finite means\u201d (Chomsky, 1956). Beyond this, one of the most influential ideas in cognitive science is the Language of Thought hypothesis (Fodor, 1975; Quilty-Dunn et al., 2023), which conjectures that all thought involved in higher-level human cognition is compositional. Indeed, recent evidence from neuroscience supports the Language of Thought hypothesis and suggests that it is core to human intelligence (Dehaene et al., 2022).\nCompositionality has been equally influential in AI, right from its very genesis when approaches relied on symbolic models with structured, rule-based semantics. While \"Good Old-Fashioned AI\" has largely given way to deep learning, the idea that compositionality is central to intelligence has remained, motivating efforts in neurosymbolic AI (Garcez and Lamb, 2023; Marcus, 2003; Sheth et al., 2023), probabilistic program inference (Ellis et al., 2023; Lake et al., 2017), modular deep neural networks Andreas et al. (2016); Bengio (2017); Goyal and Bengio (2022); Goyal et al. (2021, 2020); Mittal et al. (2021); Pfeiffer et al. (2023); Schug et al. (2024), disentangled representation learning (Ahuja et al., 2022; Brehmer et al., 2022; Higgins et al., 2017; Lachapelle et al., 2022; Lippe et al., 2022; Sawada, 2018), object-centric learning (Locatello et al., 2020; Singh et al., 2023; Wu et al., 2024), and chain-of-thought reasoning (Hu et al., 2024; Kojima et al., 2022; Wei et al., 2022), to name only a few. One of the primary appeals of compositionality is that it enables a powerful form of out-of-distribution generalization, aptly named compositional generalization (Lake and Baroni, 2018). If a model is compositional with respect to a set of features in its training data, it need not observe all possible combinations of those features in order to generalize to novel ones (Bahdanau et al., 2019; Mittal et al., 2021;"}, {"title": "Definition 1 (Compositionality \u2013 colloquial)", "content": "The meaning of a complex expression is determined by its structure and the meanings of its constituents."}, {"title": "2 Compressing a representation", "content": "The definition that we will propose rests on the idea that compositional representations can be redescribed as a simple function of constituent parts. While there may be many ways to redescribe any given representation, a natural and principled way is through the lens of optimal compression and Kolmogorov complexity. We provide a brief introduction to Kolmogorov complexity below, but direct unfamiliar readers to Appendix A.\nKolmogorov complexity Kolmogorov complexity (Kolmogorov, 1965; Li et al., 2008) is a notion of information quantity. Intuitively, the Kolmogorov complexity of an object x, denoted K (x), is the length of the shortest program (in some programming language) that outputs x. A related notion is the conditional Kolmogorov complexity of x given another object y, denoted K(x|y), which is the length of the shortest program that takes y as input and outputs x. Kolmogorov complexity has many intuitive properties as a measure of information quantity. The smaller and the more \"structure\" an object has (regularity, patterns, rules, etc.), the more easily it can be described in a short program and the lower its complexity. Kolmogorov complexity therefore is deeply rooted in the idea of compression."}, {"title": "Compressing Z as a function of parts", "content": "Let us denote a representation by a matrix Z \u2208 R^{N\u00d7D}, where each row zn is a D-dimensional vector obtained by sampling iid from some data distribution and model p(x)p(z|x). For instance, p(x) could be a distribution over natural images, zn ~ p(z|x) could be the (often deterministic) output of some intermediate layer in a trained image classifier, and the resulting representation Z \u2208 R^{N\u00d7D} would be a matrix of these layer activations in response to many natural images.\nWe will argue that a natural way to think about compositional representations is: representations Z that can be significantly compressed as a function of constituent parts. In other words, the shortest program that outputs the representation, with length K(Z), has a very particular form: it first describes Z using short parts-based constituents, and then maps these parts to the high-dimensional representation. This program form is shown in Figure 1 and described in detail below. Crucially, the components of this program will be used in Section 3 to construct our formal definition of compositionality, in which representations that are more compressible as a function of constituent parts are more compositional. Before combining them into a definition of compositionality, we now describe the components of this program in the following steps."}, {"title": "3 Representational compositionality: a formal definition of compositionality", "content": "Our definition of compositionality is a ratio of constituent terms appearing in the decomposition of K (Z) in Equation (1):\nDefinition 2 (Representational compositionality)\nThe compositionality of a representation, denoted by C'(Z), is:\nC(Z) = K(Z)/(K(f) + K(Z|W, f)) = K(Z)/(K(pw) + K(W|pw) + K(f) + K(Z|W, f))\nwhere pw, W, and f are jointly obtained from the shortest program that compresses Z in Equation (1).\nCrucially, pw, W, and f are not free parameters: they are intrinsic to the representation in that they best compress Z (see the minimization in Equation (1)). Like Kolmogorov complexity, then, C'(Z) is intractable to compute because it requires an exponentially-large search over all possible tuples (pw, W, f). However, like Kolmogorov complexity, C(Z) can still be tractably estimated using efficient compression and optimization methods. While the primary contribution of this work is theoretical and aimed at justifying Definition 2, we outline a strategy for finding (pw, W, f) and estimating C(Z) in Appendix B. Importantly, however, we foreshadow a complementary definition for the compositionality of a language together with a representation, as opposed to a representation alone, which we will introduce in Section 3.1. This quantity is readily estimable in a wide rage of cases, and could prove useful in a number of practical applications such as tokenization.\nWe now unpack Definition 2 to see how it accounts for the problems of the colloquial Definition 1 and explains computational properties typically associated with compositionality."}, {"title": "Expressivity and compression", "content": "Effectively, representational compositionality says that the compositionality of a representation is a compression ratio that depends on two things: (1) the complexity of the representation, which appears in the numerator, and (2) the complexity of the semantics which construct the representation from its constituent parts, which appears in the denominator. When a representation is highly expressive (high K(Z)) but can nevertheless be compressed as a simple function of constituent parts (low K(Z|W)), representational compositionality says that the representation is highly compositional. Representational compositionality therefore formalizes a hypothesis in cognitive science that compositionality emerges from competing pressures for expressivity and compression (e.g., Kirby, 1999; Kirby et al., 2008, 2004, and references therein)."}, {"title": "Constituent \"parts\u201d are intrinsic to Z", "content": "Note that unlike the colloquial Definition 1, representational compositionality makes it clear where the \u201cconstituent parts\" (tokens in W), \"complex expressions\" (W), and \u201cstructure\u201d (f) associated with a representation come from: they are intrinsic properties of the representation. Compositional representations are those that are compressible in principle as simple functions of constituent parts, regardless of whether or not we know what that optimal compression scheme is."}, {"title": "Systematicity and generalization", "content": "Representational compositionality formalizes the intuition that the constituent parts of a compositional representation determine the meaning of the whole in a systematic way (Szab\u00f3, 2012, 2022). For instance, if f arbitrarily maps sentences w to their representations z in a way that does not take the structure or words of the sentence into account (as in the case of idioms), then its complexity K(f) is necessarily high and compositionality is low (we demonstrate this through experiments in Section 4.1). In addition, if f is inaccurate in how it maps sentences to their representations, the error K(Z|W, f) is high and the compositionality low. A representation that is highly compositional according to our definition thus benefits from the generalization ability of simple functions (low K(f)) that fit their data well (low K(Z|W, f)). Crucially, this ability of f to generalize to novel sentences and representation samples explains the fundamental relationship between compositionality and notions of systematicity from cognitive science (Szab\u00f3, 2022)."}, {"title": "Structure-preserving semantics", "content": "Representational compositionality explains the widely-held intuition that semantics functions f which are compositional are structure-preserving in how they map w \u2192 z (Montague et al., 1970). As explained in Ren et al. (2023), structure-preserving maps have lower Kolmogorov complexity, and thus higher compositionality according to our definition. In a structure-preserving map, each word in the sentence w independently affects a different subspace of the representation z so that pairwise-distances are similar in sentence-space and representation-space."}, {"title": "Modularity & compositionality", "content": "Representational compositionality explains the precise relationship between compositionality and structural modularity, which has been taken for granted in past work but is difficult to formally articulate (Goyal and Bengio, 2022; Lepori et al., 2023; Mittal et al., 2022). A modular semantics function f is simple because it decomposes knowledge into smaller reusable components that are algorithmically independent, and thus contributes to high compositionality under our definition. This also explains why natural language is highly compositional. Linguists typically model language using context-free grammars (Chomsky, 1956), in which a sentence hierarchically decomposes into a parse tree with a \u201cproduction rule\u201d applied at each node. The recursive application of these production rules, akin to a small number of modules in f, is then thought to determine the meaning of the sentence as a whole."}, {"title": "3.1 Compositionality of language systems", "content": "In Definition 2 of representational compositionality, W is not a free parameter, but rather a collection of sentences intrinsic to Z that minimize its description length. However, we can also consider the special case of languages in which the sentences are fixed to some WL that is external to the representation. Such cases are common in the real world; in a natural language such as English, W\u00b9 are the sentences that a person may utter while Z are the underlying neural activity patterns (thoughts) that those sentences elicit. We could then ask to what degree this language system composed of thoughts Z and given sentences WL is compositional. Expressing each thought with an arbitrary sentence would result in a non-compositional language system, whereas using sentences that accurately describe thoughts through simple semantics would be highly compositional. We define the compositionality of a language system as:\nDefinition 3 (Language system compositionality)\nThe compositionality of a language system L that maps sentences WL to their representation Z, denoted by CL (Z), is:\nCL(Z) = K(Z)/K(Z|WL) = K(Z)/(K(fL) + K(Z|WL, fL))\nwhere f\u00b9 is obtained from the shortest program that compresses Z given WL.\nThis definition opens the door to comparisons between the compositionalities of different real-world language systems, such as French and Japanese, which we attempt in Section 4.3."}, {"title": "4 Empirical results", "content": "In this section, we evaluate our compositionality definitions, C(Z) and CL(Z), on both synthetic and real-world datasets to see if they agree with intuitions.\nWhile no other formal definition of representational compositionality has been proposed, a heuristic commonly used to measure language system compositionality is topological similarity. For some language system (WL, Z), topological similarity computes a pairwise distance matrix between rows in WL and another distance matrix for Z, then correlates the two flattened matrices. Intuitively, if the pairwise distances of Z are preserved in WL, topological similarity scores the language system as compositional because the two spaces share structure. Throughout our experiments, we compare our definitions to topological similarity. As an aside, we note that our definition explains why topological similarity is a reasonable heuristic: simple semantics functions f (e.g., identity, linear) tend to preserve the structure of their inputs."}, {"title": "4.1 Synthetic representations", "content": "Our first set of experiments consider representations Z that are generated synthetically using known rules through: z ~ p(z; f (w)), w ~ pw(w). Since we know the underlying programs that generated the representations in this case, we know the ground truth complexity terms K(pw), K(W|pw), K(f), and K(Z|W, f) needed to compute C(Z) exactly. These experiments are therefore less about empirically estimating compositionality in practice and more about validating whether the definition matches with intuitions. The high-level methodology used to generate representations is described below, with additional details (such as derivations of complexity terms) provided in Appendix F.\nLookup table representations The simplest way to construct a representation from sequences of discrete tokens is to assign each token in the vocabulary a fixed embedding in a lookup table, and then concatenate these embeddings across the sequence (Figure 2a). Alternatively, the lookup table could assign each unique n-gram an embedding and we could concatenate the embeddings for consecutive n-sized chunks in the sequence. We call n the \u201cdisentanglement\" factor\""}, {"title": "Sentence length:", "content": "As representation dimensionality is held constant and sentence length increases, compositionality should intuitively increase. For instance, if sentences are of length 1, we are not tempted to call the representation compositional. The more the representation decomposes according to parts, the more compositional it should be. We see empirically that representational compositionality matches this intuition. This is because K(Z) increases with sentence length (there are more possible z values, for instance) and K(f)\u2014proportional to the size of the lookup table-is smaller (same number of table entries, each with lower-dimensional embeddings as sentence length grows). In contrast, topological similarity shows the opposite trend by decreasing with sentence length, thus violating intuitions."}, {"title": "Vocabulary size:", "content": "Vocabulary size has a more complex relationship to compositionality. If the vocabulary is too small relative to sentence length, then expressivity and compositionality are limited (e.g., with only one word in the vocabulary, nothing can be expressed). On the other hand, if the vocabulary is too large relative to sentence length, then compositionality is low because expressivity doesn't come from combining constituent parts (e.g., with one-word sentences and a large vocabulary, there is no notion of parts). For a given sentence length, then, compositionality should peak at some intermediate vocabulary size. This is precisely what we observe empirically with representational compositionality: a sharp increase in compositionality early on followed by a monotonic decrease as vocabulary size increases further. While topological similarity also decreases as a function of increased vocabulary size, it does not show the early increase, and is in fact largest for a vocabulary size of 1."}, {"title": "Representation dimensionality:", "content": "For a fixed sentence length and vocabulary, how does compositionality relate to representation dimensionality? We implemented this by increasing the dimensionality of the word embeddings that are"}, {"title": "Context-free grammar representations", "content": "While our lookup table experiments provide intuitions for representa-tional compositionality, they are unlikely to reflect the structure of representations in DNN and brains. For instance, The Language of Thought hypothesis (Fodor, 1975) posits that representations underlying human thought have a hierarchical structure akin to context-free grammars in natural language (Chomsky, 1956). In such grammars, the meanings of sentences decompose according to parse trees, where children merge into parents through production rules and leaves correspond to words. For instance, the sentence \u201cscared cats run\u201d decomposes according to \u201cADJECTIVE (scared) + NOUN (cats) \u2192 NOUN-PHRASE (scared cats)\" followed by \u201cNOUN-PHRASE (scared cats) + VERB (run) \u2192 VERB-PHRASE (scared cats run)\u201d, where symbols such as NOUN-PHRASE are parts of speech (similar to data types) and functions between parts of speech such as NOUN + VERB \u2192 VERB-PHRASE are production rules.\nTo model such systems using representational compositionality, we generated representations using simple synthetic grammars (Figure 2c). First, we assigned each word in the vocabulary an embedding and a part of speech tag, and we defined a grammar with a set of production rules. We then generated a dataset of sentences and parsed them using the grammar. Finally, the semantics were defined by embedding each word in the sentence and then applying a rule-specific function at every node in the parse tree until the root was reached, who's value we defined to be the representation. The rule-specific functions concatenated children embeddings and applied a linear projection with rule-specific weights.\""}, {"title": "4.2 Emergent languages from multi-agent training", "content": "Next, we further validate our compositionality metric by applying it to real-world representations. To avoid having to solve the difficult optimization problem involved in measuring C(Z) (which requires a minimization of K(Z) w.r.t. pw, W, f) we instead consider language systems in which W = WL is fixed and measure CL (Z) (see Section 3.1).\nOne interesting case of real language systems is those that emerge in multi-agent settings where agents must learn to communicate. We consider the setting of Li and Bowling (2019); Ren et al. (2020) in which a speaker and a listener learn to communicate in a simple object reference game, where objects have symbolic attributes analogous to color, size, shape, etc. Agents trained using reinforcement learning typically communicate successfully, but often learn non-compositional language systems that arbitrarily map sentences to objects. However, Li and Bowling (2019); Ren et al. (2020) have shown that compositionality can emerge through a multi-generation process called iterated learning (Kirby et al., 2015), where the agents' parameters are periodically reset and retrained on sentence/object pairs from the previous generation (see Figure 3a). Kirby et al. (2015) hypothesize that this occurs because iterated learning places an inductive bias for simple language systems that are more easily learnable across subsequent generations."}, {"title": "4.3 Natural languages", "content": "While it is commonly accepted that all natural languages are roughly equal in their expressive power (their ability to express ideas and thoughts), a highly debated question in linguistics is whether or not they are all equally compositional (Joseph and Newmeyer, 2012). For instance, while one camp suggests that high compositionality in one respect is generally balanced by low compositionality in another, other evidence suggests that languages which undergo significant outside contact experience a pressure for easier learnability and thus higher compositionality, such as in the case of English being exposed to non-native speakers. This question has been difficult to answer definitively, partly due to the absence of a principled and quantitative definition of compositionality."}, {"title": "5 Conclusion", "content": "We introduced a novel definition of compositionality, representational compositionality, that is grounded in algorithmic information theory. Through theoretical arguments and empirical experiments, we showed that this simple definition not only accounts for our many intuitions about compositionality, but also extends them in useful ways.\nIn virtue of being quantitatively precise, representational compositionality can be used to investigate compositionality in real-world systems. We demonstrated this in the case of emergent and natural language representations, but in a limited way that only considered language systems where the sentences describing a representation are externally defined. We note that this quantity can readily be applied to score tokenization schemes and may help guide improvements in tokenizer design, but even more intriguing is the possibility to operationalize our proposed measure CL to regularize and otherwise bias next generation tokenizers. We leave the exploration of this topic to future work. More generally however, measuring the compositionality of representations without a predefined mapping to sentences requires the development of additional machine learning tools, who's overall architecture we sketch out in Appendix B. The development of such tools is an important direction for future work, as it will allow us to investigate the compositionality of representations that emerge from different learning objectives, neural architectures, inductive biases, and brain regions. Further, these insights could potentially be used to validate or reject hypotheses about the role of compositionality in human cognition, such as the Language of Thought hypothesis (Fodor, 1975).\nRepresentational compositionality can also play an important role in the design and validation of machine learning models with principled inductive biases for compositionality. Namely, in addition to supporting a given task, a compositional representation must be easily describable as a simple function of constituent parts. There are both direct and indirect ways to achieve this that are grounded in our definition, both of which we intend to pursue in future work."}, {"title": "Appendix A Background on Kolmogorov complexity", "content": "Kolmogorov complexity was independently developed in the 1960s by Kolmogorov (1965), Solomonoff (1964), and Chaitin (1966), and defines a notion of \u201cinformation quantity\".\nIntuitively, the Kolmogorov complexity of an object is the length of the shortest program (in some programming language) that outputs that object. Specifically, given some finite string x, K(x) is the length l(r) (in bits) of the shortest binary program r that prints x and halts. Let U be a universal Turing machine that executes these programs. The Kolmogorov complexity of x is then:\nK(x) = min{l(r) : U(r) = x,r \u2208 {0,1}*}\nwhere {0, 1}* denotes the space of finite binary strings. A related notion is the conditional Kolmogorov complexity of a string x given another string y, which is the length of the shortest program that takes y as input and outputs x:\nK(x|y) = min{l(r) : U(r(y)) = z,r \u2208 {0,1}*}\nwhere r(y) denotes a program taking y as input. Finally, we can also define a \u201cjoint\u201d Kolmogorov complexity K(x, y), which denotes the length of the shortest program that jointly outputs both x and y. Surprisingly, joint Kolmogorov complexity is related to conditional Kolmogorov complexity (up to an additive logarithmic term, which we will ignore) by the Symmetry of Information theorem (Li et al., 2008):\nK(x,y) = K(yx) + K(x) = K(xy) + K(y).\nKolmogorov complexity has many intuitive properties that make it attractive as a measure of information quantity, and although it is less common than notions from Shannon information theory (Shannon, 2001), it is strictly more general (as we will show later below). The smaller and the more \u201cstructure\u201d an object has-regularity, patterns, rules, etc. the more easily it can be described by a short program and the lower its Kolmogorov complexity. Kolmogorov complexity therefore is deeply rooted in the idea of compression. For instance, a sequence with repeating patterns or a dataset that spans a low-dimensional subspace can be significantly compressed relative to its original size, and this results in low Kolmogorov complexity. In contrast, a random string devoid of any structure cannot be compressed at all and must in effect be \"hard-coded\", making its Kolmogorov complexity equal to its original size in bits.\nWhile powerful, Kolmogorov complexity has certain limitations. First and foremost, Kolmogorov is intractable to compute exactly because it requires a brute force search over an exponentially large space of possible programs. It is therefore often of conceptual rather than practical value, although it can nevertheless be upper-bounded using more efficient compression strategies. Second, Kolmogorov complexity depends on the programming language of choice. For instance, if a programming language has a built-in primitive for the object being encoded, Kolmogorov complexity is trivially small. This concern, however, is often overblown: given any two Turing-complete programming languages, the difference in Kolmogorov complexity that they assign to an object is upper-bounded by a constant that is independent of the object itself, because any Turing-complete programming language can simulate another (Fortnow, 2000; Gr\u00fcnwald and Vit\u00e1nyi, 2003). In practice, we can simply consider \u201creasonable\u201d Turing-complete programming languages that don't contain arbitrary object-specific primitives, in which case this simulation constant will be relatively small and the particular programming language of choice will have little effect. Finally, Kolmogorov complexity is only defined for discrete objects because no terminating program can output a continuous number with infinite precision. This concern is also less consequential in practice, because we can always represent continuous objects using finite (e.g., floating-point) precision."}, {"title": "Important properties for machine learning", "content": "In ML, we are often concerned with datasets and probabilistic models. Kolmogorov complexity relates to these two concepts in several interesting ways. First, we can ask about the Kolmogorov complexity of a finite dataset X = (x1,...,xn) where each sample is drawn iid from a distribution p(x). It turns out that if we have access to the true distribution p(x), optimal algorithms such as arithmetic coding (Witten et al., 1987) can encode each sample using only log2 p(xi) bits. Intuitively, this is because samples that occur more frequently can be encoded using shorter codes in order to achieve an overall better compression. We thus have that:\nK(X\\p) = - \\sum_{i=1}^{n}log_{2}(x_{i}).\nIf instead of access to the true distribution p(x) we only have a probabilistic model of the data po(x), we have that:\nK(X\\p) \\leq K(X|p_{e}) \\leq - \\sum_{i=1}^{n} log_{2} P_{\u03b8} (X_{i}),"}, {"title": "Appendix B Compressing a representation using discrete auto-encoders", "content": "To measure compositionality as defined in Definition 2, we must first compress K(Z) using the program form in Section 2. This involves finding a pw, W, and f that jointly minimize:\nK(Z) = \\min_{p_{w},W,f}K(p_{w}) + K(W|p_{w}) + K(f) + K(Z|W,f)\n= \\min_{p_{w},W,f}K (p_{w}) - \\sum_{n=1}^{N}log p_{w}(w_{n}) + K(f) - \\sum_{n=1}^{N}logp(z_{n}; f(w_{n})).\nWhile this is an intractable search problem, it can be turned into an easier optimization problem using modern deep learning tools. In particular, we can minimize at least some of the terms in Equation (1) by fitting a discrete auto-encoder"}, {"title": "VQ-VAE", "content": "The most popular method for training discrete auto-encoders is the Vector-Quantized Variational Auto-Encoder (VQ-VAE) (Van Den Oord et al., 2017). While the latent prior in a VQ-VAE is generally trained post-hoc, some work has managed to train the prior end-to-end along with the rest of the model (Cohen et al., 2022; Jones and Moore, 2020; Yasuda et al., 2021). The main challenge with VQ-VAEs is that they explicitly discretize in the latent space during training\u2014which is an inherently non-differentiable operation\u2014and then attempt to approximate gradients using imperfect estimators (Bengio et al., 2013; Jang et al., 2016). As a result, training is often unstable and fraught with degenerate solutions that collapse in the latent space (\u0141a\u0144cucki et al., 2020)."}, {"title": "Appendix C Assumptions in compressing a representation", "content": "In laying out our framework for measuring K(Z) in Section 2, we made several key assumptions.\nFirst, we assumed that the shortest program that outputs Z has a particular form. If it does not, then the estimated K(Z) can be far greater than the true one. However, we argue that the assumed program form is safe for the kinds of representations that we are interested in and the kinds of insights we wish to gain from estimating K(Z). Namely, we are interested in seeing if given neural representations share similar properties to conscious human thought, which is believed to have a symbolic structure where each thought is a composition of discrete concepts (Fodor, 1975). If a representation does not have this kind of structure, then our method would detect it in the form of a high estimated K(Z), even if this is an overestimate of the true Kolmogorov complexity due to incorrectly assuming the program form in Section 2.\nSecond, actually estimating K(Z) using Equation (1) requires a minimization over pw, W, and f. This optimization approach assumes that the pw and f which minimize K(Z) are DNNs. While this can seem unintuitive at first given the significant number of parameters in DNNs, it has been found that they converge to solutions that are remarkably simple and compressible (Blier and Ollivier, 2018; Goldblum et al., 2023; Rae, 2023; Sutskever, 2023), which likely explains their strong generalization abilities. We therefore believe that for neural representations with sufficient complexity, the assumption that they can be best compressed using DNNs is justified."}, {"title": "Appendix DExamples of compositional representations", "content": "To supplement and clarify the arguments in Section 3, it is easiest to gain further intuition for our definition of compositionality through concrete examples of different hypothetical representations. For each, we have strong intuitions about whether or not the representation is compositional, and we will see that our definition agrees with-and indeed extends these intuitions.\nExample 1, \u2193 C(Z): f is a lookup table from w to z Consider a representation Z that is sampled from a mixture of Gaussians, where the centroids are far apart but their locations lack any kind of structure (i.e., they are randomly distributed). To simplify things, let us assume that there are as many unique centroids as there are possible sentences. In such a case, the semantics function f would identify each centroid with a unique sentence and the resulting error K(Z|W, f) would be low. However, because these centroids lack any structure, f would have to define an arbitrary mapping from each sentence to its corresponding centroid. In other words, f would function as a lookup table from"}, {"title": "Example 2,\u2193 C(Z): Z is a smooth continuous function", "content": "The above example considered a case where the represen-tation had discrete structure that could be accurately modeled by sentences, and the source of low compositionality came from a high K(f). However, the compositionality can also be low if Z is inherently continuous, in which case modeling it using a discrete W is at best an approximation via quantization. In such a case, the error K(Z|W, f) would be high and the corresponding compositionality would be low. Note that it might be possible to compress Z using a low-dimensional continuous code rather than discrete sentences, from which an equivalent (perhaps even identical) definition of continuous compositionality could be derived, but in this work we consider only compositions of discrete parts."}, {"title": "Example 3,\u2193 C(Z): Z is simple", "content": "Most of the discussion thus far has focused on the denominator of C(Z) in Definition 2. However, a representation can also lack compositionality if the complexity of the numerator, K(Z), is low. If Z were very low-say it were a constant, for instance then it could be modeled using a simple f that achieves low error K(Z|W, f). However, we would certainly not be tempted say that the representation is compositional. In fact, it would be best compressed using a single word and an f that outputs a constant, rather than using complex sentences and simple compositional rules. Compositionality must therefore also increase with the expressivity of the representation, which is captured by the numerator K(Z)."}, {"title": "Appendix E Prequential coding", "content": "While the Kolmogorov complexity of a model K(pe) is difficult to measure directly, it turns out that we can jointly estimate K (D|pe) + K(pe) in cases where the model was fit to the data using a learning algorithm, as is the case in ML. From Equation (6), we have that:\nK(D\\pe) + K(p_{e}) = K(D,p_{\u03b8}).\nInstead of trying to estimate the terms on the LHS directly, we can estimate the RHS by finding the shortest program that jointly compresses both the dataset and the model, which we turns out to be easier through a compression algorithm called prequential coding illustrated in Figure E.1 and described below."}, {"title": "Appendix F Synthetic representations \u2014 experimental details", "content": "F.1 Lookup table representations\nGenerating the representations dsynthetic Z (and their ground-truth sentences W)We genera the program summa-rized intAlgorithm 1. In short", "following": "n\u2022 Generate a lookup table: We begin by constructing a lookup table from words (or n-grams) to their embeddings. This table has dimensions (K\u00ba, Mxq), where K is the vocabulary size, q is our disentanglement factor (i.e., the size of the n-grams), and D is the"}]}