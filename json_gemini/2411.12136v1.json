{"title": "Visualizing Loss Functions as Topological Landscape Profiles", "authors": ["Caleb Geniesse", "Jiaqing Chen", "Tiankai Xie", "Ge Shi", "Yaoqing Yang", "Dmitriy Morozov", "Talita Perciano", "Michael W. Mahoney", "Ross Maciejewski", "Gunther H. Weber"], "abstract": "In machine learning, a loss function measures the difference between model predictions and ground-truth (or target) values. For neural network models, visualizing how this loss changes as model parameters are varied can provide insights into the local structure of the so-called loss landscape (e.g., smoothness) as well as global properties of the underlying model (e.g., generalization performance). While various methods for visualizing the loss landscape have been proposed, many approaches limit sampling to just one or two directions, ignoring potentially relevant information in this extremely high-dimensional space. This paper introduces a new representation based on topological data analysis that enables the visualization of higher-dimensional loss landscapes. After describing this new topological landscape profile representation, we show how the shape of loss landscapes can reveal new details about model performance and learning dynamics, highlighting several use cases, including image segmentation (e.g., UNet) and scientific machine learning (e.g., physics-informed neural networks). Through these examples, we provide new insights into how loss landscapes vary across distinct hyperparameter spaces: we find that the topology of the loss landscape is simpler for better-performing models; and we observe greater variation in the shape of loss landscapes near transitions from low to high model performance.", "sections": [{"title": "1. Introduction", "content": "A central aim of machine learning (Simonyan and Zisserman, 2014; He et al., 2016; Krizhevsky et al., 2017; Vaswani et al., 2017; Devlin et al., 2018; Liu et al., 2019) is to learn the un-derlying structure of data. This learning process is governed by a loss function, denoted as L(0), where \u03b8 is the set of parameters (or weights) defining, e.g., a neural network. The loss function measures the difference between the outputs of a neural network and ground-truth values. In this way, the loss reflects how good (or bad) the current weights are at making correct predictions and how to adjust these weights during training. Given the important role that the loss function plays during learning, examining it with respect to a neural net-work's weights by visualizing the so-called loss landscape\u2014can provide valuable insights into both network architecture and learning dynamics (Goodfellow et al., 2014; Im et al., 2016; Li et al., 2018; Yao et al., 2020; Martin and Mahoney, 2021; Martin et al., 2021; Yang et al., 2022b, 2021; Zhou et al., 2023; Sakarvadia et al., 2024; Khan et al., 2024). Indeed, the loss landscape has been essential for understanding other aspects of deep learning, in-cluding generalizability (Cha et al., 2021; Yang et al., 2021) and robustness (Kurakin et al., 2016; Djolonga et al., 2021; Yang et al., 2022a). In addition, the loss landscape has been characterized in the context of scientific machine learning, e.g., to understand why different physics-informed architectures and loss functions are often brittle, exhibiting failure modes, and are hard to optimize (Krishnapriyan et al., 2021; Rathore et al., 2024; Xie et al., 2024).\nDespite its promise and appeal, loss landscape visualization is a complex and often bespoke process. Indeed, exploring and extracting insights from a loss landscape-which is inherently high-dimensional, with as many dimensions as the number of parameters in the model is challenging to do, especially when trying to visualize directly on a two-dimensional screen. Most efforts to date have focused on projecting the loss function down to one or two dimensions. Goodfellow et al. (2014) proposed a random-direction-based approach, where model parameters are interpolated along a one-dimensional path to see how the loss changes. Im et al. (2016) later introduced an extension of this method which involves projecting the loss landscape onto a two-dimensional space using barycentric inter-polation between triplets of points and bilinear interpolation between quartets of points. Li et al. (2018) continued improving the resolution of loss landscapes by introducing filter-wise normalization to remove the scaling effects incurred by previous approaches. A more so-phisticated approach to visualizing the loss landscape leverages the Hessian to define more relevant directions along which the model can be interpolated. More recently, Yao et al. (2020) used the top two Hessian eigenvectors as directions, thereby capturing more impor-tant changes in the underlying loss landscape. While various methods have been proposed, most applications have limited sampling to just one or two directions. Importantly, by restricting the sampling of loss landscapes to two dimensions, whether it be using random or Hessian-based directions, we ignore potentially informative information captured by ad-ditional dimensions (e.g., the eigenvectors associated with the dominant eigenvalues of the Hessian matrix).\nTowards characterizing higher-dimensional loss landscapes, here we take inspiration from topological data analysis (TDA). Specifically, we use a merge tree to encode the critical points of an n-dimensional neural network loss landscape, and we represent the merge tree as a topological landscape profile. The merge tree allows us to capture important"}, {"title": "2. Background", "content": ""}, {"title": "2.1. Topological Data Analysis", "content": "Topological data analysis (TDA) aims to reveal the global underlying structure of data.\nTDA is particularly useful for studying high-dimensional data or functions, where direct visualization (in two or three dimensions) is inherently not possible. We leverage ideas and algorithms from TDA to study the global structure of the loss function that is, the shape of the so-called loss landscape. Much of TDA is based on the more general idea of \"connectedness.\" In the context of a loss function, we are interested in the number of minima (i.e., unique sets of parameters for which the loss is locally minimized) and how \"prominent\u201d they are (i.e., measuring how many other sets of neighboring parameters have a higher loss than the parameter set that minimizes the loss function). Such information can be obtained from a persistence diagram (i.e., captured by the zero-dimensional persistent homology) and the so-called merge tree.\nA merge tree (Carr et al., 2003; Heine et al., 2016) tracks connected components of sub-level sets $L_{\\circledR}(v) = \\{x \\in D; x < v\\}$ as a threshold, v, is increased. The merge tree encodes changes in the loss landscape as nodes in a tree-like structure. The local minima"}, {"title": "2.2. Topological Landscape Profiles", "content": "To enable the visualization of higher-dimensional loss landscapes, we introduce a new topo-logical landscape profile representation that captures the minima and saddle points encoded by merge trees. This work builds upon Oesterling et al. (2013), who first introduced the idea of representing high-dimensional data clusters (and their nesting) as hills in a landscape, where the height, width, and shape of each hill encodes the coherence, size, and stability of each cluster. To construct the landscape profile, they first use a merge tree to encode the distribution (or density) of the data points. They then use this merge tree to construct the landscape profile, by representing maxima in the merge tree as hills in the landscape, where the size and shape of each hill are determined by characteristics like persistence and the number of points along the corresponding branch. In the context of loss functions, we are more interested in minima than maxima, so here we introduce a new version of this topological landscape profile, using the metaphor of valleys (or basins) rather than hills."}, {"title": "3. Methods", "content": "To construct our new topological landscape profile representations, we build on traditional loss landscape sampling approaches and leverage tools from TDA to capture the underlying shape (or topology) of the sampled loss landscapes. First, we select n vectors (n \u2264 m) to define an n-dimensional subspace (Figure 1.1), where m is the number of parameters in the model. We then sample a set of points from this subspace, where each point corresponds to a distinct set of parameters. We evaluate the loss for each set of parameters and represent the set of points (and their associated loss values) as an unstructured grid (Figure 1.2). We then compute a merge tree to capture the topology of the n-dimensional loss landscape (Figure 1.3), and we construct our final topological landscape profile based on this merge tree (Figure 1.4). In this section, we go into more detail about each of these steps."}, {"title": "3.1. Loss Landscape Construction and Representation", "content": "In this work, we limit our analysis to Hessian-based loss landscapes. We calculate the top n Hessian eigenvectors using PyHessian (Yao et al., 2020) (Figure 1.1) and then sample along the subspace spanned by these directions (Figure 1.2). The idea is that by using the eigenvectors associated with the top n largest eigenvalues, we can visualize the most significant local loss fluctuations for a given model. Given the n orthogonal directions, we generalize the approach taken by Li et al. (2018) by expanding the subspace beyond two dimensions. Formally, we perturb trained model parameters along the n directions and evaluate the loss L as follows:\n$f(a_1...a_n) = L(\\theta+ \\Sigma_{i=1}^{n}\\alpha_i\\delta_i),$                                                                                                            (1)\nwhere $a_1...a_n$ are the coordinates in the n-dimensional subspace, $\\delta_i$ is the i-th direction in that subspace, and \u03b8 is the original model. As such, each coordinate corresponds to a point associated with a computed loss value, and the collection of loss values forms an n-dimensional loss landscape. In this work, we use an equally spaced grid by taking each $a_i$ to be the set of equally spaced integers between 0 and r, where r is the resolution of each dimension in the grid. Here we use r = 41, such that the center of the grid corresponds to the original model, i.e., $\\Sigma_{i=1}^{n}\\alpha_i\\delta_i$ = 0.\nGiven an n-dimensional loss landscape, we can represent the sampled points as an unstructured grid, where each vertex in the grid is associated with n coordinates and a scalar loss value. Before we can characterize how the loss changes throughout the landscape (i.e., as parameters are perturbed from one vertex to the next), we need to define the spatial proximity (or connectivity) of vertices in the grid based on the similarity of their coordinates. Here we use a scalable, approximate nearest neighbor algorithm to construct a neighborhood graph representation of the loss landscape (Dong et al., 2011). The neighborhood graph, proposed by Jaromczyk and Toussaint (1992), of a dataset D is a graph G = (D, E) where two points u and v are connected by an edge (u, v) \u2208 E if they are similar. Here we focus on the k-nearest neighbor graph, where each point is connected to the k most similar points."}, {"title": "3.2. Topological Structures and Landscape Profiles", "content": "After defining the subspace and computing the loss landscape, we perform topological data analysis to extract and summarize the most important features. In this work, we use a merge tree to extract key information from the loss landscape, which we then use to define our topological landscape profile. We compute the merge tree for each loss landscape using the Topology ToolKit (TTK), developed by Bin Masood et al. (2021).\nGiven a merge tree, we then construct the topological landscape profile using the method proposed by Oesterling et al. (2013). In this representation, each branch (in the merge tree) ending in a local minimum is represented by a basin (in the landscape profile), and each sub-branch ending in a saddle point is represented as a sub-basin, below which other basins are placed. In either case, each basin (or sub-basin) is represented by a set of rectangles encoding the cumulative size of the branch (or sub-branch), from bottom to top, such that the top of the basin is as wide as the number of points found along the corresponding branch in the merge tree.\nWe introduce this topological landscape profile representation of loss functions to ef-fectively capture more information from higher-dimensional loss landscapes, in such a way that can still be visualized. While this topological representation and the merge tree used to create it both capture important features of the high-dimensional space, it also discards some important information by design. Here, we reincorporate some of this discarded in-formation back into our representation, for example, by using the loss values to color the different basins. As shown in Figure 2.C, we compute the average loss across the points in each basin, and we use darker blues to represent lower average loss values. Thus, deeper basins are represented by a darker blue color, evoking the idea of deeper ocean depths. In"}, {"title": "4. Empirical Evaluation", "content": ""}, {"title": "4.1. Visualizing Different Physical Constraints", "content": "In our first evaluation, we look at a set of physics-informed neural network (PINN) models trained to solve simple convection problems (Krishnapriyan et al., 2021). Here we aim to investigate the PINN's soft regularization and how it helps (or fails to help) the optimizer find an optimal solution to a seemingly simple convection problem. We show how the shape and complexity of our topological landscape profiles change as a physical \"wave speed\" parameter is increased and the PINN fails to solve this seemingly simple physical problem. Specifically, we consider the one-dimensional convection problem, a hyperbolic partial differential equation that is commonly used to model transport phenomena:\n$\\frac{\\partial u}{\\partial t}+ \\beta \\frac{\\partial u}{\\partial x} = 0, x \\in \\Omega, t\\in [0,T]$                                         (2)\n$u(x,0) = h(x), x\\in \\Omega$                                                                              (3)\nwhere \u03b2 is the convection coefficient and h(x) is the initial condition. The general loss function for this problem is\n$L(\\theta) = \\frac{1}{N_u} \\Sigma_{i=1}^{N_u} (\\hat{u} - u)^2 + \\frac{1}{N_f} \\Sigma_{i=1}^{N_f} (\\frac{\\partial \\hat{u}}{\\partial t} + \\beta \\frac{\\partial \\hat{u}}{\\partial x})^2 + L_B$                                                                                       (4)\nwhere $\\hat{u} = NN(\\theta, x, t)$ is the output of the NN, and $L_B$ is the boundary loss. While increas-ing the physical wave speed parameter, \u03b2, should not necessarily make this a harder problem to solve, it can make PINN models harder to train. Interestingly, Krishnapriyan et al. (2021) related these failure modes to changes in the corresponding loss landscape, showing that it becomes increasingly complicated, such that optimizing the model becomes increasingly difficult. Here we explore these failure modes in more detail using three-dimensional and four-dimensional Hessian-based loss landscapes, finding more variability in the shape of loss landscapes near the transition between high and low-performing models.\nIn Figure 3, we show a heat map corresponding to the average relative error across different values of the physical wave speed parameter and across different learning rates. Interestingly, we observe that the error increases with this physical parameter, but more slowly for higher learning rates. The smallest learning rate displays higher error rates even for smaller values of the physical parameter. When looking at the loss landscapes, we observe consistently more funnel-like loss landscapes for the smaller values of \u03b2, corresponding to lower error (Figure 3.1). In contrast, we observe a consistently more bowl-like loss landscape for the larger values of \u03b2, corresponding to higher error (Figure 3.3). The funnel-like landscapes correspond to when the PINN models find a physically reasonable solution, albeit constrained to a smaller space of solutions by the physical wave speed parameter."}, {"title": "4.2. Visualizing Loss Landscapes Over Training", "content": "In our second evaluation, we explore how loss landscapes change throughout training and across different learning rates. To do this, we study UNet models with a learnable CRF-RNN layer (Avaylon et al., 2022) trained on the Oxford-IIIT Pet dataset (Parkhi et al., 2012). We trained the models using five different random seeds across seven different learning rates for 30 epochs. For each checkpoint, we computed two-dimensional loss landscapes based on the top two Hessian eigenvectors. The model was perturbed using a distance of 0.01 and layerwise normalization was adopted (Li et al., 2018).\nIn Figure 4 and Figure B.6, we show the same heat map corresponding to average test accuracy over training and across different learning rates. We observe that the test accuracy improves over training, with some variation across the different learning rates. In Figure 4, we consider how the loss landscape changes over training. When looking at the loss landscapes for three different random seeds, after zooming in, we observe an initially shallow loss landscape but with the global minimum at a much higher loss compared to the end of training. As training proceeds, we see that the global minimum becomes lower, but the basin itself becomes deeper with the edges remaining at much higher loss values. As the global minimum continues to drop, we also observe additional flattening of the basin, such that all points have a much lower loss compared to the beginning of training. Interestingly, the flat basin at a much higher loss corresponds to a phase of learning where perturbing the model in any one direction doesn't really increase the already high loss. After five epochs, the much deeper basin reflects a less stable model, where perturbing the model results in relatively higher loss. As training proceeds, we observe a flattening of the basin, which"}, {"title": "5. Conclusion and Future Work", "content": "In this paper, we introduced a new topological landscape profile representation of neural network loss landscapes. To demonstrate the many different ways this new representa-tion of loss landscapes can be used, we explored several different machine learning exam-ples, including image segmentation (e.g., UNet-CRF) and scientific machine learning (e.g., PINNs). Along the way, we provided new insights into how loss landscapes vary across dis-tinct hyperparameter spaces, finding that the topology of the loss landscape is simpler for better-performing models and that this topology is more variable near transitions from low to high model performance. Moreover, by using a merge tree to extract the most important features from a computed loss landscape, we are able to construct a new representation encoding these features. By separating this new representation from the original space in which the loss landscape was sampled, our approach opens up the door to visualizing higher-dimensional loss landscapes.\nWhile we only explore up to four dimensions here, our approach can be extended to any number of dimensions. The limiting factor is sampling, which requires exponentially many more resources as the number of dimensions increases. However, future advances towards more efficient sampling could be combined with our current approach to reveal the higher-dimensional structure of loss functions. Complementary advances in sampling more global loss landscapes (combining multiple independently trained models) could also benefit from our new representations. In that case, we would expect to see more distinct basins in our topological landscape profiles."}]}