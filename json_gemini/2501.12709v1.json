{"title": "Practical quantum federated learning and its experimental demonstration", "authors": ["Zhi-Ping Liu", "Xiao-Yu Cao", "Hao-Wen Liu", "Xiao-Ran Sun", "Yu Bao", "Yu-Shuo Lu", "Hua-Lei Yin", "Zeng-Bing Chen"], "abstract": "Federated learning is essential for decentralized, privacy-preserving model training in the data-driven era. Quantum-enhanced federated learning leverages quantum resources to address privacy and scalability challenges, offering security and efficiency advantages beyond classical methods. However, practical and scalable frameworks addressing privacy concerns in the quantum computing era remain undeveloped. Here, we propose a practical quantum federated learning framework on quantum networks, utilizing distributed quantum secret keys to protect local model updates and enable secure aggregation with information-theoretic security. We experimentally validate our framework on a 4-client quantum network with a scalable structure. Extensive numerical experiments on both quantum and classical datasets show that adding a quantum client significantly enhances the trained global model's ability to classify multipartite entangled and non-stabilizer quantum datasets. Simulations further demonstrate scalability to 200 clients with classical models trained on the MNIST dataset, reducing communication costs by 75% through advanced model compression techniques and achieving rapid training convergence. Our work provides critical insights for building scalable, efficient, and quantum-secure machine learning systems for the coming quantum internet era.", "sections": [{"title": "Introduction", "content": "Deep learning has achieved remarkable success across various fields [1], including disease diagnosis [2], autonomous driving [3], and tackling critical scientific challenges [4, 5]. In particular, the advent of large language models [6, 7] has underscored the influence of empirical scaling laws related to model and dataset size [8]. The effectiveness of large language models like GPT-4 and Llama 3 stems largely from access to massive public datasets. However, high-quality private data, such as medical data and behavior data of users [9, 10], are often isolated among clients, complicating their integration into centralized learning systems.\nTo address the challenges of data privacy, federated learning has emerged as a decentralized paradigm, enabling collaborative model training while keeping client data local [11, 12]. This method has been effectively applied in privacy-sensitive areas, including healthcare [13], the Internet of Things [14], and personalized recommendations [15]. Concurrently, quantum machine learning has emerged as a transformative field [16\u201318], combining machine learning with quantum theory to exploit the advantages of both domains. Quantum federated learning [19] (QFL) extends federated learning into the realm of quantum machine learning, consisting of a cloud server and multiple clients that access quantum resources.\nTo further the development of QFL, several frameworks [20\u201322] have been proposed to enhance communication efficiency with quantum machine learning models, such as variational quantum circuits [23] or quantum neural networks (QNNs), which are considered promising models for the noisy intermediate-scale quantum era [24]. While communication efficiency is critical, ensuring data privacy remains a more significant concern in QFL. Although clients do not share raw local data, gradient inversion attacks [25] can still expose private information through shared gradients or model updates. Recent efforts have sought to mitigate these attacks through designing sophisticated quantum algorithms [26\u201329] or involving differential privacy techniques [30]. However, many of these approaches require extensive quantum resources beyond current capabilities or fail to offer perfect data privacy. Consequently, there is an ongoing demand for a practical QFL framework that provides quantum security for data privacy in the coming era of large-scale quantum computing.\nQuantum networks enable applications that surpass the classical ones [31\u201333] and many tasks have been demonstrated on quantum networks, which show quantum advantages [34\u201338]. Specifically, quantum key distribution (QKD) serves as a foundational component of the quantum internet [31], ensuring confidentiality of information transmission with information-theoretic security. With great progress made in recent years [39\u201345], QKD has laid the groundwork for integrating deep learning algorithms into quantum networks, paving the way for more practical privacy-preserving machine learning"}, {"title": "Results", "content": "Framework of QuNetQFL. The fundamental QFL setup consists of a central server and K quantum clients. Each client possesses a local dataset $D_k = \\{(x_i,y_i)\\}_{i=1}^{n_k}$, where $x_i$ denotes a quantum state and $y_i$ is the corresponding label. The dataset $D_k$ can be a native quantum dataset or a dataset generated through encoding classical data into quantum states [52], given by $x = \\phi(x)$ with $x \\in R^d$. In the $t$-th communication round, the server sends the global QNN parameters $\\theta^{t-1} \\in R^M$ to the clients. Each client then trains a local model by minimizing the local loss function $L(\\theta)$ that captures the error between predictions and true labels of its local dataset. After local training, each client computes an update $\\Delta\\theta_k^t = \\theta_k^t - \\theta^{t-1}$ based on its local loss, which is sent to the server. The server aggregates these updates using a weighted sum to form the global model update:\n$\\Delta\\theta^t = \\sum_{k=1}^{K} \\frac{n_k}{N} \\Delta\\theta_k^t$   (1)\nwhere $N = \\sum_{k=1}^{K} n_k$ is the total number of data instances across all involved clients. This aggregated update is then used to update the global model parameters with $\\theta^t = \\theta^{t-1} + \\Delta\\theta^t$. While this setup preserves data privacy by sharing only model updates, it remains vulnerable to gradient attacks that could reveal client information.\nTo ensure secure updates, QuNetQFL employs QKD to achieve a masking-based secure aggregation scheme (see Fig. 1 (b) for an illustration). This approach leverages a fully connected quantum network for secure quantum key exchanges between all participating clients. In the $t$-th communication round, a subset of clients $S_t \\subset \\{1,2,..., K\\}$ is selected to participate by the server. After achieving local training, clients in $S_t$ exchange quantum secret keys pairwise through the pre-negotiated QKD protocol in the quantum network, denoted by $QK_{ij}$ for each client pair $(i, j)$. In each communication round, the keys $QK_{ij}$ exchanged between client pairs are represented as $(M \\times q)$-bit binary strings, where $M$ is the model size and $q$ specifies the quantization bit length. The cost of quantum secure keys in the QuNetQFL scales as $O(K^2M)$. To integrate QKD into the framework, quantization is used to compress large models, balancing the communication cost (cost of quantum keys) with model training performance. Each key is treated as a vector with $M$ entries, where each entry is a q-bit signed integer, i.e., $QK_i \\in Z^M$, with the k-th entry $QK_{ij}[k] \\in [-(2^{q-1} - 1), (2^{q-1} - 1)]$.\nWe note that $QK_{ij} = QK_{ji}$, represents symmetric key sharing between each via QKD protocols. These keys are then used as a one-time padding mask as follows. The $i$-th client in $S_t$ computes the quantized weighted difference $\\Delta \\hat{\\theta}_i^t = \\theta_i^t - \\theta^{t-1}$, masks it with the shared secret keys, and uploads the masked local update:\n$\\Delta \\hat{\\theta}_i^t = [Q^q(p_i \\cdot \\Delta \\theta_i^t) + m_i] \\mod 2^q$, (2)\nwhere $m_i = \\sum_{j\\in S_t, i\\neq j}(-1)^{i>j}QK_{ij}, p_i = n_i/N_t$ with $N_t = \\sum_{i \\in S_t} n_i$ and $Q^q(\\cdot)$ is the $q$-bit quantization function. The server aggregates the updates by summing the quantized values from all selected clients:\n$\\Delta \\theta^t = \\sum_{i \\in S_t} \\Delta \\hat{\\theta}_i^t \\mod 2^q$ (3)\n$\\Delta \\theta^t = \\sum_{i \\in S_t} [\\sum_{i \\in S_t} Q^q(p_i \\cdot \\Delta \\theta_i^t) ] \\mod 2^q$, (4)\nwhere an essential property that $[\\sum_{i \\in S_t} m_i] \\mod 2^q = 0$ is utilized. This aggregation process, combined with quantization and masking, ensures that the updates remain private. Finally, the server updates the global model parameters as follows:\n$\\theta^t = \\theta^{t-1} + D^q(\\Delta \\theta^t)$, (5)\nwhere $D^q(\\cdot)$ is the de-quantization function, and the quantization imprecision is $O(1/2^q)$. The detailed pseudocode for this framework is given in Methods (see Algorithm 1).\nThis quantum-secured aggregation approach provides perfect privacy against gradient attacks, supporting secure and scalable QFL across distributed networks. The theoretical convergence analysis for QuNetQFL is provided in the supplementary information, offering both convergence guarantees and practical guidance for training, highlighting its practical advantages.\nSecurity analysis. (a) Threat model. We adopt a commonly used threat model in federated learning. All clients and the cloud server are considered honest-but-curious. That is, while they faithfully follow the prescribed protocol, they may attempt to infer other clients' local datasets from individual local model updates. In this context, we are not addressing the extreme scenario, which is also highly impractical, considering the numerous geographically distributed clients where only one client is honest, and all other clients conspire with the cloud server. Our scheme excludes this extreme collusion, as in current research [53]. If the server colludes with all but one of the clients during secure sum computation, the data of the colluding clients can be reliably removed from the sum, exposing the honest client's data. (b) Security guarantees. Our scheme is designed to safeguard each client's local dataset from other clients, the cloud server, and potential third-party attackers. However, protecting the global model is not the goal of this work."}, {"title": "Experimental quantum network", "content": "In QuNetQFL protocol, assume that in $t$-th round, a subset of total ordered clients $C_t$ is randomly sampled by the server, the index set of clients contained in $C_t$ is $S_t$. For $i \\in S_t$ and $c_i \\in C_t$ have a q-bit quantized local update vector $Q^q(\\Delta\\theta^t)$, where its $k$-th entry $Q^q(\\Delta\\theta^t)[k]$ is a q-bit signed integers. Then each selected client $c_i \\in C_t$ adds the keys distributed under quantum networks, as the one-time pad, to pair-wisely mask it and obtain the masked vector $\\Delta \\hat{\\theta}_i^t$ as shown in Eq. (2). Note that each $QK_{ij}[k]$ is a q-bit key distributed between client $i$ and client $j$ under quantum networks, which is truly random and secret. And $m_i[k]$ is also a q-bit truly random mask, which can be regarded as a q-bit signed integer. The result $\\Delta \\hat{\\theta}_i^t[k]$ is also a q-bit integer, which is perfectly masked. Thus, each client can mask their model to preserve their privacy from other clients and the server.\nExperimental quantum network. To experimentally generate quantum secure keys for demonstrating QuNetQFL with 3 or 4 clients, we established a quantum network with five participants. This includes four clients, Alice, Bob, Charlie, and David, who potentially require quantum secret keys for collaborative training, and one party, the untrusted Eve, responsible for measuring interference results, as shown in Fig. 2. Through this quantum network, each client pair can securely share quantum secret keys.\nHere, we take the key generation process of MDI QKD between Alice and Bob as an example, which applies similarly to other client pairs. The system frequency is 100 MHz. The relevant protocol description can be found in Methods. In Eve's site, to prepare weak coherent pulses with an extinction ratio exceeding 30 dB, two intensity modulators are utilized to chop the continuous light from a narrow linewidth continuous-wave laser source (NKT E15). Eve then sends the pulses into the loop after passing through a circulator and a 50: 50 beam splitter (BS). In the loop, two clients modulate the pulse train with phase modulators. Note that users only modulate one pulse train. Specifically, Alice only modulates the clockwise pulses and Bob only modulates the counterclockwise pulses. Then, the two pulses interfere at Eve's BS, and Eve records corresponding detection events by $D_1$ and $D_2$. The detection efficiencies of $D_1$"}, {"title": "Evaluation on QNN classifying quantum datasets", "content": "Detecting and certifying quantum resources, such as entanglement and quantum magic (nonstabilizerness), is essential in practical quantum technologies [54\u201357]. In this context, we utilized the QuNetQFL in an IID setting, employing QNN with a hardware-efficient ansatz (HEA) [58] to classify both high and low multipartite entangled states, as well as non-stabilizer and stabilizer states from two distinct quantum datasets respectively. Comprehensive details of the datasets and simulations are provided in the Methods section. Utilizing experimentally generated secret keys for quantum-secure aggregation, we demonstrated secure collaborative learning capabilities across three and four-client scenarios. Figure 3 illustrates comparative performances in these scenarios using 16-bit quantization (q = 16), against an ideal benchmark where all client data is centrally processed. The results highlight a notable improvement in the global model's test accuracy with the addition of a single client in both classification tasks, aligning it more closely with the benchmark's performance. This finding"}, {"title": "Evaluation on QNN classifying MNIST", "content": "To demonstrate the robustness and adaptability of QuNetQFL in real-world, heterogeneous data environments, we evaluated its performance across both IID and non-IID settings for four clients using QNNs on the classical MNIST dataset. We created multiple two-class subsets from MNIST, specifically {3,6}, {0,1}, {3,9}, and {3,5}, where, for instance, {3,9} represents images of digits \"3\" and \"9\". Each client was allocated 500 training samples from the MNIST training set, while the server test set comprised 500 samples from the MNIST test set. In the IID setting, each client's data contained an equal number of samples from each category. In contrast, the non-IID setting introduced imbalanced distributions to better reflect real-world scenarios where data varies across devices. The data splits among the four clients for two classes were configured as (200, 300), (300, 200), (167, 333), and (333, 167), as shown in Fig. 4 (d). To accommodate quantum resource constraints, we resized the original 28 \u00d7 28 images to 4 x 4 and encoded each into 4-qubit states via amplitude embedding. The classification task utilized a 4-qubit QNN with a three-layer HEA, as shown in Fig. 1 (d). Results in Fig. 4 show rapid convergence of the test loss within the first 40 communication rounds for both IID and non-IID settings, as depicted in Fig. 4 (a) and (b). A comparison of final test accuracies across both data distributions, as shown in Fig. 4 (c), reveals close performance levels with a close communication time (\u2248 40 rounds), highlighting the strong adaptability of our QFL framework across varying data distribution conditions."}, {"title": "Evaluation of the large-scale implementation of QuNetQFL", "content": "The QuNetQFL framework is flexible, supporting both quantum and classical machine learning models. If quantum computational resources are unavailable on either the clients or server, classical models can be used, ensuring scalability while benefiting from"}, {"title": "Discussion", "content": "In this work, we propose QuNetQFL, a quantum federated learning framework that leverages practical quantum networks to ensure information-theoretically secure communication among clients during model updates. QKD is employed to generate quantum secret keys, which serve as one-time pads to encrypt local model updates. To reduce costs, the framework integrates advancing quantization techniques. Additionally, QuNetQFL is versatile and can integrate other model compression techniques, such as low-rank tensor compression, weight pruning, and knowledge distillation, to further reduce"}, {"title": "Algorithm 1 Protocol of QuNetQFL", "content": "Input: The untrained global model parameters with $\\theta^0$, The data size distribution of clients $\\{n_k\\}_{k=1}^K$, the number of iterations T, quantization bit length q, the specific QKD protocols.\nOutput: Trained global model parameters $\\theta^T$\nInitialization: Randomly generate a set of T subsets $\\{S_t\\}_{t=1}^T$, where each $S_t$ contains an equal number of client indices.\nfor each round $t\\in [T]$ do\n1) Select the subset of clients' indexes $S_t$.\n2) For each client $i \\in S_t$:\na) Update the local model and compute the local update: $\\Delta \\theta_i^t = \\theta_i^t - \\theta^{t-1}$.\nb) Perform an MDI QKD protocol (see Box 1) with each other connected clients in the $S_t$ in the underlying quantum networks, and generate masking vector:\n$m_i \\leftarrow \\sum_{j\\in S_t, j \\neq i}(-1)^{i>j}QK_{ij}$.\nc) Compute masked and quantized local update:\n$\\Delta \\hat{\\theta}_i^t \\leftarrow [Q^q(p_i \\cdot \\Delta \\theta_i^t) + m_i] \\mod 2^q$.\n3) Aggregate updates: $\\Delta \\theta^t \\leftarrow [\\sum_{i \\in S_t} \\Delta \\hat{\\theta}_i^t] \\mod 2^q$.\n4) Update global model: $\\theta^t \\leftarrow \\theta^{t-1} + D^q(\\Delta \\theta^t)$.\nend for\nOutput the trained global model parameters $\\theta^T$."}, {"title": "secure federated learning", "content": "communication and quantum secret key costs.\nWe experimentally validated this approach using a five-party quantum network, achieving quantum secret key generation with key rates exceeding 30 kbps in both 3-client and 4-client scenarios, underscoring the feasibility of this method. Notably, twin-field QKD, based on the Saganc loop used in our experimental scheme, can reach distances of up to 200 km [61], suggesting the potential of our experimental scheme for large-scale QFL applications in metropolitan quantum networks. The framework takes advantage of advancing quantum network technologies, providing a platform for demonstrating the capabilities of quantum networks in large-scale distributed learning systems.\nLeveraging the generated quantum secret keys, we evaluated QuNetQFL on federated learning tasks with QNNs on both real-world classical and quantum datasets, demonstrating its applicability in the noisy intermediate-scale quantum era. We also evaluated the framework's scalability using classical models in multi-client scenarios, considering the scarcity of quantum computing devices. These results validate QuNetQFL as an effective solution for secure and scalable quantum federated learning, current quantum hardware limitations.\nWhile this work does not explicitly showcase quantum advantages in reducing computational or communication complexity, it provides a practical solution for quantum-secure federated learning. Our approach specifically addresses the privacy of shared local model updates, complementing other federated learning methods based on blind quantum computing or quantum homomorphic encryption [28, 30]. Future work will focus on reducing communication complexity by integrating these advanced quantum algorithms, balancing efficiency with practical implementation."}, {"title": "Box 1| MDI QKD Protocol subroutine", "content": "for Each client (i, j) pairs in $S_t \\times S_t$ do\n1) Randomly choose basis (X or Y) and prepare corresponding weak coherent states $\\left| e^{i k \\sqrt{\\mu}} \\right>$, where $\\mu$ is the pulse intensity.\n2) Send the states to an untrusted node, Eve, for measurement.\n3) Decide whether to flip the bit according to the measurement outcome.\n4) Estimate bit error rate by clients' announcements.\n5) Perform postprocessing steps (error correction and privacy amplification) to generate enough final secure keys.\nend for\nNote: Secret keys used in QuNetQFL can be generated during training or pre-generated to the overall process time. MDI QKD reduces the quantum resources required for client participation, and QuNetQFL is flexible to emerging QKD techniques."}, {"title": "Although QuNetQFL was primarily evaluated on variational quantum circuit models for the noisy intermediate-scale quantum era, it is also adaptable to large-scale quantum machine learning models", "content": "although QuNetQFL was primarily evaluated on variational quantum circuit models for the noisy intermediate-scale quantum era, it is also adaptable to large-scale quantum machine learning models [62], making it relevant for future fault-tolerant quantum computing. We anticipate that this work will inspire the development of more practical and scalable QFL schemes, further advancing quantum technologies in distributed learning systems."}, {"title": "Methods", "content": "Protocol of QuNetQFL. The detailed protocol for QuNetQFL is provided in Algorithm 1. Importantly, QuNetQFL is designed to be flexible, allowing clients to choose their local training algorithms based on resource constraints and computational requirements. Clients can employ either gradient-based methods or alternative non-gradient-based methods to update local models, particularly when obtaining the gradients of the QNN circuit is expensive.\nQuantization technique. To efficiently utilize the secret keys generated in quantum networks and further re-duce the communication overhead, we employ a quanti-zation technique in QuNetQFL, originally developed for homomorphic encryption [49] and later adapted for secure aggregation [63]. This method quantizes a scalar $s \\in R$ within $[-\\beta, \\beta]$ into a q-bit sign integer in the range $[-(2^{q-1}-1), 2^{q-1} - 1]$ using the q-bit Quantizer $Q^q(s)$:\n$Q^q(s) = sgn(s) \\cdot Round \\left(abs(s) \\cdot \\frac{(2^{q-1} - 1)}{\\beta}\\right)$, (6)\nwhere sgn() is the sign function, abs(\u00b7) denotes the absolute value, and Round(\u00b7) maps the input to the nearest integer. The corresponding de-quantized process for a quantized valued v is given by:\n$D^q(v) = sgn(v) \\cdot \\left(abs(v) \\cdot \\frac{\\beta}{(2^{q-1} - 1)}\\right)$. (7)\nQuantization is performed on the client side for local model updates, while de-quantization is executed on the cloud server for the aggregated global model. Model parameters are clipped to the range [-\u03b2, \u03b2] to ensure quantization accuracy. Note that to handle signs during de-quantization, the server employs the following adjustment: If $v > 2^{q-1} - 1$, it is updated as $v - 2^q$, otherwise, v remains unchanged."}, {"title": "Protocol of quantum network", "content": "We employed a four-phase MDI QKD protocol [43] between each pair of participants to generate quantum secret keys. To illustrate this process, we use the example of Alice and Bob with an untrusted third party, Eve. All other user pairs in the network follow this same protocol to obtain their quantum secret keys.\n(1) Alice and Bob choose the X basis with the probability $p_x$ and the Y basis with the probability $p_y (p_y = 1 \u2013 p_x)$. For the X basis, they prepare $\\left| e^{i k \\sqrt{\\mu}} \\right>$ where k is the logic bit value $(k_x \\in \\{0,1\\})$ and \u00b5 is the pulse intensity. For the Y basis, they prepare $\\left| e^{i (k_y+1/2) \\pi \\sqrt{\\mu}} \\right>$ where ky is the logic bit value $(k_y \\in \\{0,1\\})$.\n(2) Eve performs measurements on the pulses with a 50:50 BS and two single-photon detectors and records the detection results. One and only one detector clicks are defined as an effective event.\n(3) The above steps are repeated many times to accumulate sufficient data. Eve announces all effective events and the corresponding detector that clicks. For each effective event announced by Eve, if $D_2$ clicks, Bob flips his corresponding logic bit. Alice and Charlie retain only the logical bits from effective measurements, discarding others. Then they disclose their basis choices for effective events through authenticated classical channels.\n(4) Alice and Bob announce all their bit values in the Y basis to calculate the quantum bit error rate $E_y^\u2217$ in the Y basis to estimate the phase error rate under the X basis $E_\\phi^\u2217$, and the number of counts $n_X$ and $n_Y$ can also be obtained under X and Y bases, respectively.\n(5) Alice and Bob perform error correction on the remaining keys under the X basis and privacy amplification to obtain the final secret keys."}, {"title": "Simulation details of MDI QKD protocol", "content": "The final key rate of MDI QKD [43] can be given by\n$I = \\frac{n_x[1 - H(E_\\phi)] - A_{EC}}{2} - \\sqrt{\\frac{1}{N} log_2 \\frac{1}{\\epsilon_{EC}}} - \\sqrt{\\frac{1}{N} log_2 \\frac{1}{\\epsilon_{PA}}}$, (8)\nwhere $n_x$ is the number of total counts in the X basis and $E_\\phi$ is the upper bound of phase error rate under the X basis. $\\Lambda_{EC} = n_x f H(E_x)$ is the leaked information during error correction, where f is the error correction efficiency, $E_x$ is the bit error rate under the X basis and $H(x) = -x log_2 x - (1 - x) log_2 (1 - x)$ denotes the binary Shannon entropy. $E_{EC}$ and $E_{PA}$ are the failure probabilities for the error correction and privacy amplification, respectively and we set $E_{EC} = E_{PA} = 10^{-10}$.\n$E_x$ can be obtained from experimental results and $E_\\phi$ is bounded by the following inequality\n$1-2A \\leq \\sqrt{E_x(1 - E_x)} + \\sqrt{(1 \u2013 E_x) (1 \u2013 E_\\phi)}$, (9)\nwhere $A = (1 - | \\left< \\Psi_y | \\Psi_x \\right> |^2) / 2Q$. $Q = n_{tot} / N$ is the total gain, where $n_{tot}$ is the number of detection events and N is the number of pulses sent. $| \\Psi_x \\rangle ( \\left| \\Psi_y \\right> )$ is the basis-dependent state under the X (Y) basis. The fidelity can be expressed as\n|\\langle \\Psi_x | \\Psi_y \\rangle | = \\frac{1}{4} \\left[ (1 \u2013 i) (\\sqrt{\\mu} i \\sqrt{\\mu}) + (1 \u2212 i)\n(\u2212\\sqrt{\\mu} \u2013 i \\sqrt{\\mu}) + (1 + i) \\{ \\sqrt{\\mu} \\} \\right]\n(\u2212i \\sqrt{\\mu}) + (1+i) (\u2212i \\sqrt{\\mu}) \n-1 (10)\nConsidering the finite-key effect, Kato's inequality [64] is utilized. The upper bound of the expectation value $m_y^*$ is given by $m_y + \\Delta n_y$, where $m_y = n_y E_y^*$ represents the number of errors in the Y basis and $\\Delta n_y = V_m / ln \\epsilon_F$ with the failure probability $\\epsilon_F = 10^{-10}$. $E_y^* = m_y/n_y$ can be calculated and $E_\\phi$ can be derived according to Eq. 9. Thus we can get the number of phase errors $m_\\phi^* = n_x E_\\phi^* and m_p$ can subsequently be estimated by the inequality. Consequently, we can get the upper bound of phase error rate $E_\\phi = m_\\phi^*/n_x$.\nError correction and privacy amplification. To obtain the final keys from the raw keys, error correction is required to rectify bit errors, and privacy amplification is needed to eliminate the possibility of information leakage.\nThe raw keys of the participants are transformed into fully correlated bit strings by error correction, utilizing the Cascade key agreement mechanism [65]. Initially, the participants divide their keys into blocks with a variable length, determined by the estimated bit error rate. Based on the recommendation of [66] and our optimization, we designated the block length as 0.7/E with a growth factor of 2 and performed a total of 4 rounds.\nIn the first round, the participants calculate and compare the parity check values for each block. Subsequently, they use a dichotomy method to rectify blocks with errors. Following these procedures, each block will only contain an even number (including 0) of error bits.\nIn the following rounds, the participants first shuffle their keys with pre-shared random numbers, then proceed with block division and dichotomy error correction. After identifying and correcting new error bits, the participants backtrack their bit strings by applying the inverse sequence of random numbers and rectify blocks from previous rounds that corresponding to error bits. This is necessary because, after correcting an erroneous bit, the corresponding blocks from previous rounds will contain an odd number of errors, requiring additional error correction. Therefore, the additional error correction in the corresponding blocks is achievable. Through the above methods, we have achieved a stable error correction coefficient of $f < 1.2$ for our experimental data according to [66].\nAfter error correction, the keys of the participants become identical. However, the risk of information leakage remains. A privacy amplification process is required to compress the information and ensure confidentiality.\nOur privacy amplification algorithm is based on the fast Fourier transform (FFT) and inverse fast Fourier transform (IFFT) [67]. The final key length r is determined through finite key analysis and the length of raw keys n. We use an random binary (n - 1)-bit string $[V_0, V_1,\\dots, V_{n-2}]$ to construct a $r \\times (n - r)$ Toeplitz matrix $V_{r \\times(n-r)}$ and combine an $r \\times r$ identity matrix with it horizontally to obtain a modified matrix\n\\begin{equation}\nS_{r \\times n} = \\begin{pmatrix}\n1 & u_{r-1} & u_{r-2} & \\cdots & u_1 & v_0 \\\\\n & 1 & u_{r-1} & u_{r-2} & \\cdots & u_1 \\\\\n &  & 1 & \\cdots & u_2 & u_1 \\\\\n &  &  & \\ddots & \\vdots & \\vdots \\\\\n &  &  &  & 1 & u_{n-r} \\\\\n &  &  &  &  & 1\n\\end{pmatrix}\n(11)\n\\end{equation}\nAfterwards, $X_n = [X_0, X_1,\\dots, X_{n-1}]^T$ is defined as the vector of the raw keys and $Y_r = [Y_0, Y_1, \\dots, Y_{r-1}]^T$ is defined as the vector of the final keys. There is:\n$Y_r = S_{r \\times n} X_n = \\hat{X}_n + Y, (12)$\nwhere $\\hat{Y}$ can be obtain by $D_n \\times X$ [68]. Eq. 13 represents a circulant matrix $D_n$ extended from the Toeplitz matrix $V_{r \\times(n-r)}$. The vector $\\hat{X}_n = [0,\\dots,0, X_r,\\dots,X_{n-1}]$ is obtained by setting the first r values of the vector $X_n$ to zero. The calculation procedure is presented in Eq. 14.\n\\begin{equation}\nD_n = \\mathcal{P}_{r \\times r}(-1) \\mathcal{P}_{r \\times r}(-\\frac{1}{2}) \\mathcal{F}_{r \\times r}(-) \\mathcal{P}_{r \\times r}(-1) \\mathcal{P}_{r \\times r}(-\\frac{1}{2}) \\mathcal{F}_{r \\times r}(-)\n(13)\n\\end{equation}"}, {"title": "Y - D_n X_n", "content": "\\begin{equation}\nY = \\begin{aligned}\n&P_{(n-r) \\times(n-r)} \\times X_{n-r} \\\\\n=&D_n \\times X_n \\\\\n=&IFFT[FFT(D_n) \\cdot FFT(X)]\n\\end{aligned}\n(14)\n\\end{equation}\nThe privacy amplification method described above is applied to the experimental data, integrating the final key vector $Y_r$, resulting in a processing rate exceeding 10 Mbps."}, {"title": "Two quantum datasets", "content": "We introduced the details of two quantum datasets used to evaluate the performance of QuNetQFL in entanglement classification and nonstabilizerness classification tasks. Entanglement is a fundamental resource for quantum information processing tasks, especially in quantum communication. A given n-qubit state $|\\psi\\rangle$ is a product state if and only if $|\\psi\\rangle = \\left|\\psi_1\\right>\\otimes\\left|\\psi_i\\right>$, otherwise, it is entangled. For this task, we employed the NTangled dataset [69", "Tr[p_a^2": ".", "69": "we generated a balanced 3-qubit dataset consisting of low (CE = 0.05) and high (CE = 0.35) entangled states using a six-layer HEA. In an IID setting, each client accessed 160 states for training, while the server processed 200 test states with equal proportions from the two classes in both 3-client and 4-client scenarios.\nQuantum magic, or nonstabilizerness [70\u201372", "73": "."}]}