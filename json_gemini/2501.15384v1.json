{"title": "MetaOcc: Surround-View 4D Radar and Camera Fusion Framework for 3D Occupancy Prediction with Dual Training Strategies", "authors": ["Long Yang", "Lianqing Zheng", "Wenjin Ai", "Minghao Liu", "Sen Li", "Qunshu Lin", "Shengyu Yan", "Jie Bai", "Zhixiong Mat", "Xichan Zhu"], "abstract": "MetaOcc -3D occupancy prediction is crucial for autonomous driving perception. Fusion of 4D radar and camera provides a potential solution of robust occupancy prediction on serve weather with least cost. How to achieve effective multi-modal feature fusion and reduce annotation costs remains significant challenges. In this work, we propose MetaOcc, a novel multi-modal occupancy prediction framework that fuses surround-view cameras and 4D radar for comprehensive environmental perception. We first design a height self-attention module for effective 3D feature extraction from sparse radar points. Then, a local-global fusion mechanism is proposed to adaptively capture modality contributions while handling spatio-temporal misalignments. Temporal alignment and fusion module is employed to further aggregate historical feature. Furthermore, we develop a semi-supervised training procedure leveraging open-set segmentor and geometric constraints for pseudo-label generation, enabling robust perception with limited annotations. Extensive experiments on OmniHD-Scenes dataset demonstrate that MetaOcc achieves state-of-the-art performance, surpassing previous methods by significant margins. Notably, as the first semi-supervised 4D radar and camera fusion-based occupancy prediction approach, MetaOcc maintains 92.5% of the fully-supervised performance while using only 50% of ground truth annotations, establishing a new benchmark for multi-modal 3D occupancy prediction. Code and data are available at https://github.com/LucasYang567/MetaOcc.", "sections": [{"title": "I. INTRODUCTION", "content": "The core challenge in autonomous driving is to achieve comprehensive scene understanding through robust 3D perception. Despite significant progress, 3D object detection struggles with unconventional obstacles [1]. 3D occupancy prediction advances next-generation perception by providing geometric-semantic representations for complex scenes [2].\nFurthermore, perception performance is limited by sensor characteristics. While cameras provide dense semantic infor-"}, {"title": "II. RELATED WORKS", "content": "3D occupancy prediction advances scene understanding through dense geometric-semantic representation. Classical camera-based approaches [15], [19], [20] leverage 3D-based attention and depth-based modules for spatial features extraction, while BEV-based approaches [21], [22] trade spatial resolution for computational efficiency through height compression. TPVFormer [23] extends BEV to tri-perspective view with cross-view attention, while SparseOcc [24] introduces fully sparse 3D networks. Multi-modal approaches [13], [25], [26] predominantly focus on camera-LiDAR fusion through feature concatenation or attention mechanisms. Recent ad-vances explore geometric-semantic awareness [12], diffusion denoising [27], and temporal enhancement [28] to improve fusion effect. The complementary characteristics of 4D imaging radar and cameras show promising potential for occupancy prediction [4], yet related research remains limited."}, {"title": "\u0410. \u041e\u0441\u0441\u0438\u0440\u0430\u043f\u0441\u0443 Prediction Approach", "content": "3D occupancy prediction advances scene understanding through dense geometric-semantic representation. Classical camera-based approaches [15], [19], [20] leverage 3D-based attention and depth-based modules for spatial features ex-traction, while BEV-based approaches [21], [22] trade spatial resolution for computational efficiency through height com-pression. TPVFormer [23] extends BEV to tri-perspective view with cross-view attention, while SparseOcc [24] introduces fully sparse 3D networks. Multi-modal approaches [13], [25], [26] predominantly focus on camera-LiDAR fusion through feature concatenation or attention mechanisms. Recent ad-vances explore geometric-semantic awareness [12], diffusion denoising [27], and temporal enhancement [28] to improve fu-sion effect. The complementary characteristics of 4D imaging radar and cameras show promising potential for occupancy prediction [4], yet related research remains limited."}, {"title": "B. Occupancy Label Generation", "content": "Common methods [13], [15], [29] for generating 3D occupancy ground truth achieve high-quality semantic labels through point-wise LiDAR segmentation, yet require pro-hibitive annotation costs. Notably, OmniHD-Scenes dataset [4] extends pipeline that enhances semantic density of dynamic object features using non-keyframe point clouds without ad-ditional manual labeling. To address the labeling challenge, self-supervised approaches [16], [17] leverage neural rendering and geometric consistency. However, the unsupervised solu-tions face notable limitations in dynamic scenes and complex environments. Semi-supervised learning presents a promising direction by integrating open-set segmentor and LiDAR points with bbox annotation to generate pseudo-labels, enabling cost-effective training with mixed ground truth data."}, {"title": "III. METHOD", "content": "We propose MetaOcc, a surround-view occupancy prediction framework that fuses 4D radar and camera, as illustrated in Fig. 2. Multi-modal Spatial features are extracted indepen-dently through feature extractors. Subsequently, the extracted 3D features are processed by the MetaOcc Fusion Module (MFM), which employs local-global fusion mechanism to achieve efficient cross-modal integration. Finally, historical feature is incorporated through Temporal Alignment and Fu-sion (TAF) module before being fed into the occupancy head. To reduce annotation costs while maintaining performance, we further develop a semi-supervised training strategy that effec-tively combines ground truth and generated pseudo-labels."}, {"title": "A. Overview", "content": "We propose MetaOcc, a surround-view occupancy predic-tion framework that fuses 4D radar and camera, as illustrated in Fig. 2. Multi-modal Spatial features are extracted indepen-dently through feature extractors. Subsequently, the extracted 3D features are processed by the MetaOcc Fusion Module (MFM), which employs local-global fusion mechanism to achieve efficient cross-modal integration. Finally, historical feature is incorporated through Temporal Alignment and Fu-sion (TAF) module before being fed into the occupancy head. To reduce annotation costs while maintaining performance, we further develop a semi-supervised training strategy that effec-tively combines ground truth and generated pseudo-labels."}, {"title": "B. Camera & 4D Radar Feature Extractor", "content": "The feature extraction stage incorporates a dual-stream architecture to process camera and 4D radar, independently.\nCamera stream. The multi-view images are first processed through a 2D backbone network to extract features $F_c \\in \\mathbb{R}^{N_c\\times C_1 \\times H_1 \\times W_1}$, where $N_c$ denotes the number of cameras, and $C_1$, $H_1$, and $W_1$ represent the channel dimension, height, and width of image features, respectively. Different from conventional projection methods [13], we adopt 2D to 3D spatial attention mechanism [15], [21] based on multi-scale deformable attention to achieve cross-view feature interaction. Through spatial attention, $F_c$ are transformed into 3D features $F'_c \\in \\mathbb{R}^{C\\times H \\times W \\times Z}$, where $W$, $H$, and $Z$ correspond to reso-lution of occupancy ground truth. Furthermore, $F'_c$ are further refined through camera encoder based on 3D convolutions, which progressively integrates multi-scale features to enhance spatial representation.\n4D radar stream. Inherent sparsity of 4D radar results in insufficient occupancy density, which significantly impacts the performance of voxel-based methods [30] and limits effective 3D feature extraction. Although BEV-based solutions are sim-ple and effective, direct application to 3D feature extraction remains challenging. While existing attempts to extend BEV features into 3D space via channel to height plugin (CHP) [22] or interpolation techniques [14] seem straightforward, they often degrade geometric feature fidelity."}, {"title": "C. MetaOcc Fusion Module", "content": "MetaOcc Fusion Module combines Local Adaptive Fusion (LAF) and Global Cross-Attention Fusion (GCF). LAF ad-dresses the limitations of simple fusion strategies as concate-nation in capturing modal contributions, while GCF handles spatio-temporal misalignment through deformable attention mechanism for learnable dynamic feature offsets. This cas-caded design achieves comprehensive cross-modal feature fusion for occupancy prediction.\nLocal Adaptive Fusion. The LAF module employs a adaptive fusion mechanism to integrate 3D features $F_c'$ and $F_r'$ using a lightweight convolution network. By concate-nating multi-modal features and learning adaptive weights $W_{laf} \\in \\mathbb{R}^{C\\times H \\times W \\times Z}$ through convolution layers [13], our design effectively combines semantic features from visual perception and robust geometric representations from radar"}, {"title": "D. Temporal Alignment and Fusion", "content": "Temporal fusion is crucial for occupancy prediction, espe-cially in occluded environments. Inspired by [32], we design TAF module for effective temporal feature integration, as illustrated in Fig. 5. Given the 3D features of the sequence ${F_t, F_{t-k}, ..., F_{t-n}} \\in \\mathbb{R}^{C\\times H \\times W \\times Z}$, TAF first initializes $F_{ini}$ at timestamp $t$. For historical alignment, $F_{ini}$ are trans-formed to previous frames through coordinate transforma-tion $T(.)$ with global pose $P_{t \\rightarrow (t-k)}$, where trilinear grid sample $G_t$ is employed to obtain features $F_{(t-k)\\rightarrow t}$. Finally, we concatenate and refine the spatially-aligned features ${F_t, F_{(t-k)\\rightarrow t}, ..., F_{(t-n)\\rightarrow t}}$ to produce temporal-enhanced feature $F'$. The transformation and fusion process can be formulated as:\n$F_{(t-k)\\rightarrow t} = G_t(T(F_{ini}, P_{t \\rightarrow (t-k)}))$\n$F' = BottleNeck(Concat[F_t, ..., F_{(t-n)\\rightarrow t}])$\nwhere BottleNeck(\u00b7) consists of 3D convolution layers with ReLU activation and batch normalization."}, {"title": "\u0415. \u041e\u0441\u0441\u0438\u0440\u0430\u043f\u0443 Head", "content": "The occupancy prediction head consists of linear layers that map fused multi-modal features to occupancy distributions. Subsequently, we integrate cross-entropy loss $L_{ce}$ for basic supervision, while lovasz-softmax loss [13] $L_{lovasz}$ and scene-class affinity losses [33] $L_{scal}^{seo}$ and $L_{scal}^{sem}$ are introduced to enhance both geometric and semantic consistency. The overall loss function is formulated as:\n$L = \\lambda_1 L_{ce} + \\lambda_2 L_{lovasz} + \\lambda_3 L_{scal}^{seo} + \\lambda_4 L_{scal}^{sem}$\nwhere $\\lambda_1$, $\\lambda_2$, $\\lambda_3$, $\\lambda_4$ are loss weights of 1, 5, 1, 1."}, {"title": "F. Pseudo-Label Generation Approach for Semi-Supervised Training Procedure", "content": "Considering the advances of open-set segmentor in semantic understanding, we propose a semi-supervised framework for generating high-quality occupancy pseudo-labels. As shown in Fig. 6, our method takes multi-view images and LiDAR sequences as input to generate semantic occupancy represen-tations through text-prompt guidance. The detailed pipeline is presented in Algorithm 1.\nImage Semantic Segmentation. Pre-trained on over 10M images, Grounded-SAM [18] demonstrates strong zero-shot segmentation ability with flexible semantic control. We fine-tune the prompts T specifically for static scene elements, and apply the model to segment multi-view images I into detailed semantic masks $I_{k}^{seg}$.\nLiDAR Semantic Segmentation. Similar to the OmniHD-Scenes [4] dataset, we use B to separate dynamic objects $D_{obj}$ and static element $S_{ele}$ from LiDAR point clouds in keyframes. Furthermore, OmniHD-Scenes utilizes interpolated boxes $B^*$ to extract $D_{obj}$ in non-keyframes, combined with iterative closest point to reduce spatial alignment errors. We extend the interpolated boxes to separate $S_{ele}$ in non-keyframes, and further refine $B^*$ by computing maximum velocity errors between adjacent keyframes to minimize noise. Under normal conditions, separated $S_{ele}$ are projected onto $I_{k}^{seg}$ to obtain labels $S_{k}^{seg}$, where confidence-rank resolves multi-view semantic conflicts. However, conventional radius and voxel-based filtering fail to remove substantial noise around vehicles in rainy conditions, severely degrading label quality. We propose a feature-based filtering approach to address this challenge. First, we obtain a coarse drivable region $R_d$ based on ego-vehicle position and nearby interpolated boxes $B^*$. Then, we apply normal estimation of $S_{ele}$ to identify ground semantics within $R_d$, effectively removing non-ground semantic noise and producing clean static elements $S_{ele}^{clr}$, as shown in Fig. 6."}, {"title": "Temporal Enhancement", "content": "To address occlusion problems, we accumulate sequential information through temporal aggre-gation. Dynamic objects $D_{seq}$ are assembled based on tracking IDs, while static elements are aligned and concatenated in the global coordinate system $L_{seg}$ using pose information P.\nOccupancy Generation. We transform $L_{seg}$ to local frame $P_{seg}$, then generate occupancy pseudo-labels $\\hat{O}$ through knowledge-guided semantic voxelization with occlusion com-plexity priors, followed by nearest neighbor matching."}, {"title": "IV. EXPERIMENTS", "content": "Dataset. The OmniHD-Scenes dataset consists of 1.5K sequences captured by six multi-view cameras and 4D radars under diverse driving scenarios. Currently, the dataset in-clude 200 annotated sequences totaling 11921 key-frames with comprehensive occupancy ground truth labels, enabling the evaluation of multi-modal occupancy prediction. Based on OmniHD-Scenes, we construct OmniHD-SemiOcc dataset through a semi-supervised learning strategy that combines varying proportions of ground truth with generated pseudo-labels to reduce annotation efforts."}, {"title": "A. Implementation Details", "content": "Dataset. The OmniHD-Scenes dataset consists of 1.5K sequences captured by six multi-view cameras and 4D radars under diverse driving scenarios. Currently, the dataset in-clude 200 annotated sequences totaling 11921 key-frames with comprehensive occupancy ground truth labels, enabling the evaluation of multi-modal occupancy prediction. Based on OmniHD-Scenes, we construct OmniHD-SemiOcc dataset through a semi-supervised learning strategy that combines varying proportions of ground truth with generated pseudo-labels to reduce annotation efforts.\nMetrics. Both OmniHD-Scenes and OmniHD-SemiOcc benchmarks employ mean Intersection over Union (mIoU) for semantic accuracy and Scene Completion IoU (SC IoU) for geometric accuracy. The IoU metrics are formulated as:\n$IoU = \\frac{TP}{TP+FP+ FN}$\n$mIoU = \\frac{1}{C} \\sum_{i=1}^{C} \\frac{TP}{TP+FP+ FN}$\nwhere TP, FP, and FN denote true positives, false positives, and false negatives respectively, and C represents the number of semitic classes. SC IoU is calculated between free and occupied space."}, {"title": "B. Main Results", "content": "To validate the effectiveness of our proposed MetaOcc and semi-supervised training procedure, extensive experiments are performed on OmniHD-Scenes datasets.\nFull supervision results on OmniHD-Scenes Dataset. We compare MetaOcc with other methods on the OmniHD-Scenes dataset. All fusion-based methods adopt ResNet-50 backbone and 544x960 input resolution for fair comparison. As shown in Table I, our MetaOcc achieves superior performance with 32.75 SC IoU and 21.73 mIoU, surpassing previous methods by a large margin. Among camera-only methods, SurroundOcc achieves 28.61 SC IoU and 15.20 mIoU, while BEVFusion and M-CONet obtain 27.02 and 27.74 SC IoU with radar fusion respectively. MetaOcc outperforms SurroundOcc by improvements of 4.14 SC IoU and 6.53 mIoU, and sur-passes M-CONet with gains of 5.01 SC IoU and 5.65 mIoU. MetaOcc demonstrates consistent improvements across seman-tic categories, particularly in challenging categories including rider and traffic fence, validating the effectiveness of our multi-modal feature fusion. Furthermore, our single-frame version MetaOcc-S achieves 31.52 SC IoU and 20.92 mIoU, surpassing BEVFormer which uses R101-DCN backbone at 864\u00d71536 resolution, demonstrating the robustness of our approach.\nSemi-supervision results on OmniHD-Scenes Dataset. To investigate the effectiveness of semi-supervised learning, we evaluate MetaOcc with varying proportions of ground truth (GT) and pseudo annotations on OmniHD-Scenes test set. As shown in Table II, MetaOcc achieves 32.38 SC IoU and 14.46"}, {"title": "C. Ablation Study", "content": "To evaluate the effectiveness of component in MetaOcc, we conduct ablation experiments on the OmniHD-Scenes dataset. The overall ablation results are summarized in Table III, where the baseline uses voxel-based 3D backbone [30] and addition fusion. Performance progressively improves with the inclusion of each module, as will be discussed in detail later.\nAblation of RHS. To evaluate effective 4D radar feature extraction, we compare our RHS module with voxel-based and BEV-based methods. As shown in Table IV, the voxel-based Second [30] achieves 32.52 SC IoU and 21.34 mIoU, while BEV-based PointPillars with expand operators obtain slightly lower SC IoU but higher mIoU of 32.17 and 21.64. This suggests voxel-based methods favor geometric features while BEV-based approaches excel in semantic extraction. Our RHS module achieves superior performance with 32.75 SC IoU and 21.73 mIoU by effectively combining the advantages of voxel-based and BEV-based paradigms through height self-attention mechanism.\nAblation of MFF. The ablation studies of LAF and GCF demonstrate the effectiveness of our MFF design. Building upon RHS, LAF module further improves performance to 32.48 SC IoU and 21.36 mIoU through adaptive feature integration. The significant gains (+0.55 SC IoU, +0.41 mIoU) demonstrate the capability of LAF in capturing fine-grained local correspondences between modalities. GCF module fur-ther improves performance to 32.75 SC IoU and 21.73 mIoU through dual-stream deformable attention. The consistent im-provements (+0.27 SC IoU, +0.37 mIoU) demonstrate the"}, {"title": "V. CONCLUSIONS", "content": "In this work, we propose MetaOcc, a novel multi-modal oc-cupancy prediction framework that effectively fuses surround-view cameras and 4D radar with excellent attention mech-anisms. We first develop RHS to extract 3D features from sparse 4D radar points. Then, LVF and GCF are employed to capture modality contribution and address potential spatial-temporal misalignment. Afterward, we introduce TAF to in-tegrate historical features. Extensive experiments demonstrate that MetaOcc achieves superior performance in semantic and geometric accuracy on the OmniHD-Scenes dataset. Addi-tionally, our semi-supervised training strategy with open-set segmentor model significantly reduces annotation dependency while maintaining robust performance."}]}