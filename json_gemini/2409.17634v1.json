{"title": "P4Q: Learning to Prompt for Quantization in Visual-language Models", "authors": ["Huixin Sun", "Runqi Wang", "Yanjing Li", "Xianbin Cao", "Xiaolong Jiang", "Yao Hu", "Baochang Zhang"], "abstract": "Large-scale pre-trained Vision-Language Models (VLMs) have gained prominence in various visual and multimodal tasks, yet the deployment of VLMs on downstream application platforms remains challenging due to their prohibitive requirements of training samples and computing resources. Fine-tuning and quantization of VLMs can substantially reduce the sample and computation costs, which are in urgent need. There are two prevailing paradigms in quantization, Quantization-Aware Training (QAT) can effectively quantize large-scale VLMs but incur a huge training cost, while low-bit Post-Training Quantization (PTQ) suffers from a notable performance drop. We propose a method that balances fine-tuning and quantization named \u201cPrompt for Quantization\u201d (P4Q), in which we design a lightweight architecture to leverage contrastive loss supervision to enhance the recognition performance of a PTQ model. Our method can effectively reduce the gap between image features and text features caused by low-bit quantization, based on learnable prompts to reorganize textual representations and a low-bit adapter to realign the distributions of image and text features. We also introduce a distillation loss based on cosine similarity predictions to distill the quantized model using a full-precision teacher. Extensive experimental results demonstrate that our P4Q method outperforms prior arts, even achieving comparable results to its full-precision counterparts. For instance, our 8-bit P4Q can theoretically compress the CLIP-ViT/B-32 by 4 \u00d7 while achieving 66.94% Top-1 accuracy, outperforming the learnable prompt fine-tuned full-precision model by 2.24% with negligible additional parameters on the ImageNet dataset.", "sections": [{"title": "1 Introduction", "content": "Recent research in large-scale pre-trained Vision-Language Models (VLMs) [22, 19, 1] have achieved phenomenal performance in both computer vision and natural language processing, demonstrating their potential in learning open-world concepts. However, the deployment of these models on downstream application platform presents challenges due to their high computation and sample requirements. For example, the CLIP-ViT/B-32 [22] contains 152-MB parameters and demands 78G FLOPs for inference.\nDownstream application platforms such as medicine and transportation require the offline deployment of expert models due to data security. They require the VLMs to have excellent performance on the downstream data domain and low computation to match the constraints of the end-side device. While traditional fine-tuning methods enhance downstream performance by leveraging extensive data and task-specific instructions, they can incur prohibitive computational cost for fully optimizing"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Quantization", "content": "Quantized neural networks often possess low-bit weights and activations to accelerate model inference and save memory. The commonly used model quantization methods include quantization-aware training (QAT), and post-training quantization (PTQ). In QAT, MCN [30] builds a binarized convolutional neural network based on a projection function and a new updated rule during the backpropagation. Q-ViT [15] proposed an information rectification module and distribution-guided distillation to push the bit-width in a quantized vision transformer. While QAT often requires high-level expert knowledge and huge GPU resources for training or fine-tuning, especially the large-scale pre-trained model. To reduce the above costs of quantization, PTQ, which is training-free, has received more widespread attention and lots of excellent works arise. MinMax, EMA [11], Percentile [13], and OMSE [2] meth-ods are commonly used to compress or reduce the weights of the PTQ model. MinMax normalizes the weights and bias values in the model to a predefined range, such as [-1, 1], to reduce the storage space and increase the inference speed. EMA [11] uses a sliding average to recompute the values of some parameters in the model to a new value. By removing some low-weighted neurons, the size and computational cost of the model can be reduced in Percentile [13], which can be implemented based on a threshold that depends on the percentage of weights. OMSE [2] quantizes the model parameters using pruning methods to reduce the computational complexity and size of the model. FQ-ViT [17] propose Log-Int-Softmax to sustain that and simplify inference by using 4-bit quantization and the BitShift operator. But the traditional PTQ will cause great performance degradation. This paper proposes a prompt tuning method for PTQ."}, {"title": "2.2 Parameter-Efficient Fine-Tuning", "content": "Prompt tuning originates from the NLP. The high-level idea of prompting is to apply a function to modify the input text, so that the language model gets additional information about the task. Concretely, given a pre-trained language model, the task is often formulated as a \u201cfill-in-the-blank\" cloze test, such as asking the model to predict the masked token in \u201cNo reason to watch. It was [MASK]\u201d as either \u201cpositive\u201d or \u201cnegative\u201d for sentiment classification. The key lies in how to design the underlined part. However, the design of a prompting function is challenging and requires heuristics. Thus, continuous prompt learning methods [31, 14], which seek to address this problem by applying trainable prompts in a continuous space. A drawback of such methods compared to searching discrete tokens is the lack of a clear way to visualize what \u201cwords\u201d are learned for the vectors. Recent works [33, 26] adopt prompt tuning to Vision-Language Models (VLMs), such as CoOp [33]. CLIP [22] is a kind of VLM, which is trained under a large number of images and their text descriptions. It adopts a two-stream architecture, consisting of an image encoder and a text encoder that encode image and text inputs separately and produce individual vision and language representations embedded in a joint space using a contrastive loss. CoOp adopts prompt tuning to CLIP while freezing the encoders. Besides, prompt tuning has been applied to many downstream"}, {"title": "3 Methodology", "content": "In this section, we first revisit PTQ and CLIP in Sec. 3.1. Then, we describe our \"Prompt for Quantization\" (P4Q) method in Sec. 3.2. The overview of our framework is given in Fig. 2. In Sec. 3.3, the optimization process is shown."}, {"title": "3.1 Preliminaries", "content": "Post Training Quantization (PTQ). Assuming the quantization bit-width is b, the quantizer Q(xb) can be formulated as a function that maps a floating-point number x \u2208 R to the nearest quantization bin:\n\\(Q(x|b) : \\mathbb{R} \\rightarrow \\hat{x},\\)\n(1)\n\\(x =\begin{cases}\\{-2^{b-1},..., 2^{b-1}-1\\} & \\text{Signed,} \\\\{0..., 2^{b-1}\\} & \\text{Unsigned.} \\end{cases}\\)\n(2)\nThere are various quantizer Q(x|b), where uniform [11] are typically used. Uniform quantization is well supported on most hardware platforms. Its unsigned quantizer Q(x|b) can be defined as:\n\\(Q(x|b) = clip(\\lfloor\\frac{x}{s_x}\\rceil + zpx, 0,2^b - 1),\\)\n(3)"}, {"title": "3.2 Prompt for Quantization", "content": "CLIP is challenging to quantify by QAT due to its significant training burden and non-public pre-training dataset. Furthermore, naive PTQ causes significant quantization error and performance deterioration of CLIP from full-precision to low-bit. As shown in Fig. 3 (a) and (b), the distribution of image features is disturbed by PTQ. Fig. 3 (d) and (e) indicate the distortion of text features is even more serious. To alleviate the quantization error, we must calibrate the output features of the low-bit model. Since prompt tuning is effective and efficient, we suggest Prompt for Quantization (P4Q) to calibrate the output features of the low-bit CLIP quantized by PTQ. The P4Q details are introduced following.\nFollowing CLIP, we choose ViT-B/32 [5] and a text transformer as the image encoder and text encoder with the fixed weights. As shown in Fig. 2 (a), the image encoder and text encoder contain Li and Lt blocks respectively. The self-attention module in each block is conducted quantization following PTQ in sec. 3.1. It should be noted that there is no weights training in PTQ, i.e., the pre-trained full-precision weights of the encoders are directly quantized to low-bit weights. To relieve the distribution bias caused by quantization, we replace the hand-crafted prompt templates with a set of continual trainable vectors named learnable prompt P in the quantized textual stream. Specifically, P are concatenated with the embedding of a class name, formulating the text description tk (P) of the k-th class as:\n\\(t_k(P) = [P]_1[P]_2... [P]_M[CLS]_k,\\)\n(8)"}, {"title": "3.3 Optimization Process of P4Q", "content": "The key to P4Q is to provide an efficient training paradigm for PTQ. It retains the high efficiency of PTQ and introduces a learnable prompt and QAdapter to improve the quantization performance. The purpose of these learnable parameters is to realign the distributions of image and text features. In this subsection, a joint loss function in supervising prompt and adapter training are detailed below.\nFollowing the loss function of CLIP, the Cosine similarity of the quantized image feature and text feature is adopted as the classification loss Le. It can directly affect the distribution alignment of images and text features, which is formulated as:\n\\(L_c = E[-\\log\\frac{e^{\\langle v_i, w_{y_i}\\rangle /\\tau}}{\\sum_{k=1}^{K}e^{\\langle v_i, w_{k}\\rangle /\\tau}}]\\)\n(13)\nTo address the distributions of image and text features mismatch that occurred in the quantized CLIP, the image and text features's distributions of the full-precision model can be used as a reference. We introduce a knowledge distillation module that use similarity predictions of the full-precision teacher to optimize the learnable prompt and QAdapter. Knowledge distillation can help reduce the difficulty of optimizing the additional parameters in the quantized model and determine the optimization direction for the cross-modal alignment. The distillation loss is formulated as:\n\\(L_{dist} = \\frac{1}{n} \\sum_{i=1}^{n} P_s(y_i|x_i) \\log(P_t(y_i|x_i)) = \\frac{1}{n} \\sum_{i=1}^{n} \\log\\frac{e^{\\langle z_i, w_{y_i}\\rangle /\\tau}}{\\sum_{k=1}^{K}e^{\\langle v_i, w_{k}\\rangle /\\tau}} - \\log\\frac{e^{\\langle z_i, w_{y_i}\\rangle /\\tau}}{\\sum_{k=1}^{K}e^{\\langle z_i, w_{k}\\rangle /\\tau}},\\)\n(14)"}, {"title": "4 Experiment", "content": "In this section, we evaluate the performance of the proposed P4Q architecture. Extensive results reveal that P4Q outperforms the PTQ baseline by a considerable margin, achieving comparable results to ts full-precision counterparts."}, {"title": "4.1 Datasets and Implementation Details", "content": "Datasets. The experiments are carried out on the CIFAR100 [12] and ImageNet-1k [3] datasets. The CIFAR100 dataset consists of 60k images with a size of 32\u00d732 from 100 classes, which are split into 10 tasks with 10 classes. Each class consists of 500 training and 100 testing samples. The ImageNet-1k dataset contains samples sized 224\u00d7224 from 1000 classes. Each class consists of about 1,300 training and 50 test samples. More datasets appear in supplementary materials."}, {"title": "4.4 Generalization of P4Q", "content": "We investigate the generalization of P4Q by conducting cross-dataset transfer evaluation following CLIP [22]. We train a 4-bit P4Q use all data samples from the ImageNet and test its zero-shot performance on ImageNet-A [9], ImageNet-R [8], ImageNet-V2 [23], ImageNet-Sketch [25]. In general, P4Q achieves higher results on unsupervised target datasets than the PTQ baseline. The zero-shot performance of the 10-epoch and 20-epoch 4-bit P4Q models on ImageNet-V2 outperform the baseline by 1.47% and 1.65%, exhibiting P4Q's generalizability. On ImageNet-R, ImageNet-V2, and ImageNet-Sketch, 4-bit P4Q models also achieve comparable results to the baselines. The generalization capabilities of low-bit P4Q models can be attributed to several factors, including the adoption of the PTQ approach for quantizing the CLIP encoders. This approach minimizes alterations to the pretrained parameters, ensuring that the model retains its prior knowledge. Additionally, the process of distilling the low-bit model with its full-precision counterpart further contributes to maintaining its generalization performance.\nHowever, we observe that the training duration can impact the transfer performance of P4Q. As demonstrated by the 50-epoch 4-bit P4Q models on the ImageNet-A and ImageNet-V2, the improve-ment in zero-shot performance diminishes as training progresses. This phenomenon aligns with prior research where supervised adaptation methods on CLIP, such as linear prob [22] and prompt tuning [33] can result in increased accuracy on the seen training set at the expense of decreased performance on unseen test sets, a phenomenon known as the Generalization Gap. Our findings suggest that we can potentially trade off a reduction in performance on the training set for improvements in generalization by using a less extensively trained P4Q model."}, {"title": "4.5 Ablation Study", "content": "In the following experiments, we explore the best-performed structure of P4Q by ablating and tuning its parts. All tests are performed on CIFAR100 dataset.\nPrompt and QAdapter. We present the quantitative results of the proposed trainable prompt, QAdapter in Table 4a. As displayed, prompt and QAdapter improve the performance when applied alone, and the two methods further boost the performance considerably when combined. For example, the prompt improves the 4-bit base-line by 17.84% and the QAdapter achieves 20.23% improvement. Combining the prompt and QAdapter, the performance improvement achieves 23.15%. In Table 4a, we exhibit that QAdapter incurs al-most no additional computation or memory cost to the quantized baseline, with a negligible increase in FLOPs and 0.13MB increase in size, and prompt almost does not induce any additional memory cost.\nLoss analysis. We analyze the effectiveness of the proposed Le and Ldist in the 4-bit P4Q, trained 20 epochs. As shown in Table 4b, using the knowledge distillation loss Ldist and classification loss Le alone provides 20.30% increase to Top-1 accuracy in 4-bit models. Results validate the effectiveness of teacher similarity predictions and the contrastive supervision to a quantized student model. The two losses can further boost the performance considerably when combined together, achieving 1.78% increase than using the Le loss alone. We tune the trade-off parameter \u03bb of Ldist and obtain the optimal performance with \u03bb=1.\nHyper-parameters selection. We select the hyper-parameters prompt length M and adapt ratio \u03b1 using a 4-bit P4Q trained for 20 epochs. Model performance (Top-1 accuracy) with different hyper-parameter combinations {M, \u03b1} is presented in Fig. 5. Results indicate that performance initially improves and then declines as \u03b1 varies from 0 to 1. This demonstrates the necessity of image feature adaptation, with full adaptation (\u03b1 = 1) outperforming the vanilla base (\u03b1 = 0). Additionally, P4Q with prompt learning exhibits stronger performance than without (M = 0), but full prompt learning (M = 32) performs worse than all alternatives. Exploring the setups, we identify {\u039c,\u03b1} = {16, 0.2} as the combination that boosts P4Q's performance the most, achieving 68.12% Top-1 accuracy. Based on the ablative study above, we set hyper-parameters M and \u03b1 as 16 and 0.2 for the experiments in this paper."}, {"title": "5 Conclusion", "content": "This paper proposes a method that integrates fine-tuning and quantification named \u201cPrompt for Quantization\u201d (P4Q) for appling pre-trained vision-language models to downstream applications. We design a low-bit and lightweight adapter with learnable prompts to significantly improve the recognition performance of a PTQ model. Besides, we propose a joint loss function including contrastive loss and distillation loss, which can effectively align the cross-modal distributions of the quantized model to their full-precision counterparts. Overall, our P4Q quantized CLIP obtains a superior performance than prior arts, even achieving comparable results to its full-precision counterparts with negligible added parameters on CIFAR100 and ImageNet-1k. In our future work, we will explore the potential of our method in other applications."}]}