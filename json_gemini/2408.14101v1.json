{"title": "Estimating Causal Effects from Learned Causal Networks", "authors": ["Anna Raicheva", "Alexander Ihler", "Jin Tian", "Rina Dechter"], "abstract": "The standard approach to answering an identifiable causal-effect query (e.g., P(Y|do(X)) when given a causal diagram and observational data is to first generate an estimand, or probabilistic expression over the observable variables, which is then evaluated using the observational data. In this paper, we propose an alternative paradigm for answering causal-effect queries over discrete observable variables. We propose to instead learn the causal Bayesian network and its confounding latent variables directly from the observational data. Then, efficient probabilistic graphical model (PGM) algorithms can be applied to the learned model to answer queries. Perhaps surprisingly, we show that this model completion learning approach can be more effective than estimand approaches, particularly for larger models in which the estimand expressions become computationally difficult.\nWe illustrate our method's potential using a benchmark collection of Bayesian networks and synthetically generated causal models.", "sections": [{"title": "1 Introduction", "content": "Structural Causal Models (SCMs) are a formal framework for reasoning about causal knowledge in the presence of uncertainty [23]. When the full SCM is available, it is possible to use standard probabilistic inference [6, 8] to directly answer causal queries that evaluate how forcing some variable's assignment X = x affects another variable Y, written as P(Y|do(X = x)). However, in practice the full causal model is rarely available; instead, only a causal diagram \u2013 a directed graph capturing the causal relationships of the underlying SCM - is assumed to be known. Causal diagrams typically include both observable variables, which can be measured from data, and latent variables, which are unobservable and for which data cannot be collected. A linchpin of causal reasoning is that many causal queries can be uniquely answered using only the observable variables [28, 24, 25, 11] and consequently estimated using observational data.\nThe main approach developed in the past two decades for answering causal queries under such assumptions is a two-step process which we call estimand-based causal inference. The first step is to determine if the causal query is identifiable - i.e., uniquely answerable from the model's observational distribution \u2013 and if so, construct an expression (\"estimand\") that captures the answer symbolically using probabilities over only observed variables. Then, one can use the observed data to estimate the probabilities involved in the estimand expression. Over the past few years a variety of estimand-based strategies have been developed using different statistical estimation methods [14, 3].\nHowever, many of these approaches focus on specific, small, causal models, and do not examine the scalability of their approaches or provide significant empirical comparisons. In larger models, estimand expressions can become large and unwieldy, making them either computationally difficult, hard to estimate accurately, or both.\nIn this paper, we propose a straightforward yet under-explored alternative: to learn the causal model, including latent dependencies, directly from the observed data. Although the domains and distributional forms of these latent variables are unknown, recent work [34] has shown that, if the visible variables are discrete, any SCM is equivalent to one with discrete latent variables, and gives an upper bound on their domain sizes. This allows us to apply well-known techniques for learning latent variable models, such as the Expectation-Maximization (EM) algorithm, to build a Causal Bayesian Network (CBN) over the observed and latent variables. We can further apply model selection techniques such as the Bayesian Information Criterion (BIC, [16]) to select appropriate domain sizes. Then, given our learned model, we can use efficient algorithms for reasoning in probabilistic graphical models (PGMs) to answer one, or even many causal queries [6, 16].\nPerhaps surprisingly, we show that this approach is often significantly more effective than the estimand-based methodology. Furthermore, we provide structural conditions that help decide when each approach is likely to be more effective. The computational benefit of our approach is tied to the complexity of the causal graph: if the causal graph has low treewidth, then exact PGM algorithms (e.g., bucket elimination or join-tree scheme) are efficiently applicable. As an added benefit if there are multiple causal queries to perform on the same model, since the model is learned only once, the learning time can be amortized over all such queries.\nOur empirical evaluation incorporates a spectrum of models, including not only the small models common in causal effect literature, but also large models based on benchmark Bayesian networks and scalable classes of synthetic models; to the best of our knowledge this is the first such extensive empirical evaluation of causal effect queries.\nContributions. This paper presents a new path for answering causal effects queries by learning a Causal Bayesian Network that is consistent with a causal graph and observational data.\n\u2022 We propose to answer causal queries by directly learning the full causal Bayesian network via EM, followed by query processing using traditional PGM algorithms.\n\u2022 We provide a first of its kind, empirical evaluation of algorithms for causal effect queries on varied and large synthetic and real benchmarks.\n\u2022 We show empirically that our proposed learning approach gives more accurate estimates than the estimand-based alternatives.\nIn settings with high-dimensional estimand expressions but low treewidth causal models, our approach allows more accurate estimates by retaining more information from the causal graph, and allowing variance reduction without impacting computational tractability.\nThe rest of the paper is organized as follows. Section 2 provides background on related work and information and definitions, Section 3 outlines the main idea of our approach, Section 4 provides empirical evaluation, and we conclude in Section 5."}, {"title": "2 Background", "content": "We first provide some definitions and technical background. We use capital letters X to represent variables, and lower case x for their values. Bold uppercase X denotes a collection of variables, with |X| its cardinality and D(X) their joint domain, while \u00e6 indicates an assignment in that joint domain. We denote by P(X) the joint probability distribution over X and P(X = x) the probability of X taking configuration x under this distribution, which we abbreviate P(x). Similarly, P(Y|X) represents the set of conditional distributions P(Y | X = x), \u2200x.\nDefinition 1 (Structural Causal Model). A structural causal model (SCM) [23] is a 4-tuple M = (U,V, F, P(U)) where: (1) U = {U1, U2, ..., Uk} is a set of exogenous latent variables whose values are affected by factors outside the model; (2) V = {V1, V2, ..., Vn} is a set of endogenous, observable variables of interest whose values are determined by other variables in the model; (3) F = {fi : Vi \u2208 V} is a set of functions fi such that each fi determines the value vi of Vi as a function of Vi's causal parents PAi \u2282 U \u222a (V \\ Vi) so that fi: D(PAi) \u2192 D(Vi) and vi\u2190 fi(pai); and (4) P(U) is a probability distribution over the latent variables. The latent variables are assumed to be mutually independent, i.e., P(U) = \u03a0\u03c5\u2208U P(U).\nCausal diagrams. An SCM M induces a causal diagram which is a directed graph G = (VUU, E), where each node in the graph maps to a variable in the SCM, and there is an arc from every observed or latent node X in the graph to Vi iff X \u2208 PAi. An SCM whose latent variables connect to at most a single observable variable is referred to as Markovian, and one whose latent variables connect to at most two observable variables is called Semi-Markovian. It is known that any SCM can be transformed into an equivalent Semi-Markovian one such that answers to causal queries are preserved [27]. In the semi-Markovian case it is common to use a simplified causal diagram called an Acyclic Directed Mixed Graph (ADMG), which omits any latent variables with a single child, and replaces any latent variable with two children with a bidirectional dashed arc between the children, so that latent variables are no longer explicitly shown (see, e.g., Figure 1).\nThe SCM M also induces a Causal Bayesian Network (CBN) B = (G,P) specified by M's causal diagram G = \u3008VUU,E\u3009 along with its associated conditional probability distributions P = {P(Vi|PAi), P(U;)}. The distribution P(V, U) factors according to the causal diagram:\n\\(P(V,U) = \\prod_{V_i \\in V} P(V_i|PA_i) \\cdot \\prod_{U_j \\in U}P(U_j).\\)\n(1)\nThe observational distribution, P(V), is given by:\n\\(P(V) = \\sum_u P(V,U = u).\\)\n(2)\nCausal effect and the truncation formula. An external intervention, forcing variables X to take on value \u00e6, is modeled by replacing the mechanism for each X \u2208 X with the function X = x. This is formally represented by the do-operator do(X = x). Thus the interventional distribution of an SCM M by applying do(X) is,\n\\(P(V\\X,U | do(X = x)) = \\prod_{V_i \\notin X} P(V_i|PA_i) \\cdot P(U), X=x\\)\n(3)\ni.e., it is obtained from Eq. (1) by truncating the factors corresponding to the variables in X and setting X = x. The effect of do(X) on a variable Y, denoted P(Y|do(X)), is defined by marginalizing all the variables other than Y.\nCausal queries. While we normally have no access to the full structural causal model M and cannot evaluate these expressions directly, it is sometimes possible to evaluate the effect of an intervention given only the observational distribution P(V), specifically, if the answer is unique for any full model that is consistent with the graph and P(V) [23]. More formally:\nDefinition 2 (Identifiability). Given a causal diagram G, the causal effect query P(Y | do(X)) is identifiable if any two SCMs consistent with G that agree on the observational distribution P(V) also agree on P(Y | do(X)).\nIdentifiability makes the causal-effect query well-posed:\nDefinition 3 (Causal-effect query). Given a causal diagram G = (VUU, E), data samples from an observational distribution P(V), and an identifiable query P(Y | do(X)), the task is to compute the value of P(Y | do(X = x)).\nEstimand-based approaches. The now-standard methodology for answering causal-effect queries is to break the task into two steps. The first is the identifiability step: given a causal diagram and a query P(Y do(X)), determine if the query is identifiable and if so, generate an estimand, or algebraic expression in terms of the observational distribution P(V) that answers the query. A complete polynomial algorithm called ID has been developed for this task [27, 24]. The second step is estimation: use samples from the observational distribution P(V) to estimate the value of the estimand expression.\nA number of approaches have been applied to the estimation in the second step. A simple approach, called the plug-in estimator, replaces each term in the estimand with its empirical probability value in the observed data. More sophisticated approaches include the line of work by Jung et al., which apply ideas from data weighting and empirical risk minimization [12, 13] and double or debiased machine learning [14, 15]. Evaluating the estimand's value from a PAC perspective is analyzed in [4]."}, {"title": "3 Learning-Based Causal Inference", "content": "The approach we propose for the causal-effect task is to first learn a full CBN B = (G, P) given the causal diagram G = (VUU, E) and samples from the observational distribution P(V). Then, we can answer causal-effect queries based on the truncated formula Eq. (3) using probabilistic inference over the learned CBN. However, there could be many parameterizations P that are consistent with the same observational distribution P(V). Luckily, the identifiability property ensures that the problem remains well-posed: as long as the query is identifiable, any of these alternative parameterizations P will generate the same answer:\nProposition 1. Assume a given SCM M = (U,V, F, P(U)) having causal diagram G and observational distribution P(V). Any CBN B = (G,P) inducing the same observational distribution P(V) via Eq. (2) will agree with M on any identifiable causal-effect query P(Ydo(X = x)).\nWe note that if the query P(Y | do(X = x)) is not identifiable, then CBNs B = (G, P) with different parameters P inducing the same observational distribution P(V) may generate different answers to the query.\nConsequently, our problem reduces to the well-studied task of estimating the parameters of a Bayesian network that includes latent variables from observational data, given the network structure. A widely used algorithm for this purpose is Expectation-Maximization (EM), which aims to maximize the likelihood of the data. To apply EM, however, we also require knowledge of the latent variables' distributional forms.\nUsefully, in the case of discrete visible variables V, prior work has shown that any SCM can be transformed into an equivalent SCM in which the latent variables U take on discrete, finite domains [34], and provides an upper bound on the required latent domain sizes. This allows us to assume, without loss of generality, a discrete distribution for the Uj.\nHowever, the upper bound in [34] is conservative and often very large. Moreover, such large domain sizes are sufficient but may not be required for a given SCM. In practice, we use the discrete latent domain sizes as a complexity control mechanism, to impose an additional degree of regularity on the probability distribution over the visible variables. In order to select the appropriate amount of regularization, we take the domain sizes, k = {ku; }, as hyperparameters and select their values by minimizing the model's BIC score, which penalizes models with larger domain sizes for their increased flexibility and potential to overfit [6]:\n\\(BIC_{B,D} = -2 \\cdot LL_{B,D} + p \\cdot log(|D|)\\)\n(5)"}, {"title": "Algorithm 1: EM4CI", "content": "input: A causal diagram G = (UUV, E), U latent and V observables; D samples from P(V); Query Q = P(Y | do(X = x)).\noutput: Estimated P(Y | do(X = x))\n// k= latent domain size, BICB = BIC score of B, D, // LLB is the log-likelihood of B, D\n1. Initialize: \u0412\u0406\u0421\u0412 \u2190 \u221e,\n2. If \u00acidentifiable(G, Q), terminate.\n3. for k = 2, ..., to upper bound, do\n4. (B', LLB') \u2190 max {EM(G, D, k)|for i = 1 to 10}\n5. Calculate BICB, from LLB\n6. if BICB < \u0412\u0406\u0421\u0412,\n7. \u0412 \u2190 \u0412', \u0412\u0406\u0421\u0412 \u2190 \u0412\u0406\u0421\u0412\n8. else, break.\n9. endfor\n10: Bx=x \u2190 generate truncated CBN from B.\n11: return \u2190 evaluate PBX (Y)\nwhere D are the data samples from P(V), LLB,D is the log likelihood of the CBN model B learned via EM, and p is the number of free probability parameters in B.\nTo optimize our CBN over both the domain sizes k and probability parameters 0, we propose a practical algorithm that searches greedily in the space of k while optimizing @ by EM. Larger domain sizes can facilitate the learning of complex models, but require more samples and more time for convergence. For this reason, we prioritize searching the model space starting from smaller domain sizes. At the same time, having domain sizes slightly too large is unlikely to hurt performance significantly, so we adopt a simple strategy of keeping the latent domain sizes equal, i.e., set all ku\u2081 = k, and gradually increase the value k.\nOur learning strategy thus benefits from two sources of variance reduction compared to the simple plug-in estimates: first, from the graph structure itself, which imposes some regularity on P(V), and second, from preferring smaller latent domain sizes k when possible, which encourages learning lower-rank distributions when supported by the data.\nOnce a full CBN B = (G,P) is learned, any causal query P(Y | do(X = x)) can be answered based on Eq. (3). Let Gx denote the graph obtained by deleting all incoming arrows to X. Let Px=x = {P(Vi|PAi), P(U)}v;\u00a2x \u222a {P(X = x) = 1}. Let Bx=x = (Gx, Px=x) be the truncated CBN. Then we have:\nProposition 2. P(Y | do(X = x)) in the CBN B is given by P(Y) in the truncated CBN Bx=x, i.e.,\n\\(P_B(Y | do(X = x)) = P_{B_{X=x}}(Y).\\)\n(6)\nThe iterative learning EM4CI algorithm (Algorithm 1). Since we use EM for learning we call our algorithm EM for Causal Inference (EM4CI), described in Algorithm 1. Its input is a causal diagram G, samples D from P(V), and a query Q = P(Y | do(X = x)). After initialization, step 2 checks if the query is identifiable. This can be accomplished by any variant of the well-known ID algorithm [27, 24], or via the Do calculus [23]. If the query is not identifiable, the algorithm terminates with failure. Otherwise steps 3-9 provide the iterative learning scheme. Given samples D from P(V), we use EM to learn a CBN B over the graph G with latent domain size k.\nEM outputs the parameters for a network, B', and its corresponding log-likelihood, LLB', with which we calculate the BICB score. Since EM's performance is known to be sensitive to initialization, we perform ten runs of EM starting from randomly generated initial parameters, and retain the model with the highest likelihood as the candidate for the current latent domain size k. We then increase k, and repeat the process. If the BIC score decreases (step 6), we adopt the new candidate network and the process continues with increased k; otherwise the current B is selected as the final learned network. Step 10 generates the truncated CBN Bx=x, and step 11 employs a standard PGM inference algorithm, like join-tree decomposition [6, 8], to the CBN Bx=x to compute Px=x(Y).\nComplexity of EM4CI. Determining identifiability is polynomial in the graph size [27]. The complexity of EM (steps 3-9) depends on the sample size |D|, the variables' domain sizes, and the number of iterations needed for convergence which is hard to predict, but complexity-wise provide only a constant factor and can be ignored. The most extensive computation in each iteration of EM is the Expectation step, which requires probabilistic inference and can be accomplished in time and memory exponential in the treewidth of the graph. Otherwise, approximation algorithms such as belief propagation may be used. In our experiments, exact inference was always possible. Thus, each iteration of EM takes O(|D|\u00b7nlw), where n = |VUU is the number of variables, I bounds the variable domain sizes, and w is the treewidth. For T iterations we find that the complexity of EM is O(T\u00b7 |D|\u00b7 n \u2022 lw). Step 11 of probabilistic inference is O(nl) if performing exact inference. In summary, the complexity of algorithm EM4CI is O(T\u00b7|D|\u00b7n.lw). Thus, the algorithm is most effective for models with bounded treewidth. Note that the cost of the EM learning process can be amortized over multiple queries."}, {"title": "4 Empirical Evaluation", "content": "We perform an extensive empirical evaluation of EM4CI and compare against estimand-based approaches such as the brute-force empirical plug-in method and a state-of-the-art scheme called weighted empirical risk minimization (WERM) [13]. All experiments were run on a 2.66 GHz processor with 8 GB of memory.\nThe inputs to a causal inference algorithm are a triplet: 1) a causal diagram (of an underlying SCM), 2) data from the model's observational distribution, and 3) a query Q = P(Y | do(X)). We use two sources for our benchmarks: synthetically generated scalable model classes, and real domains from the academic literature of various fields with causal interpretations [1].\nSynthetic networks. We generate each triplet benchmark instance by first choosing a causal diagram and a query. Then, picking domain sizes of observed and latent variables, we generate the conditional probability tables (CPTs), one per variable and its parents, to yield the full CBN. Given the full model we generate samples from its observational distribution by forward sampling [6] over all variables, then discard the values of the latent variables. We compute the exact answer to the target query on the full CBN via an exact algorithm (e.g., join-tree [6, 8]).\nThe causal diagrams were selected in two ways. First, we take a set of 8 small diagrams from the causality literature [23, 14]; see Figure 2 and the Appendix. Second, we examine three scalable classes of graphs whose treewidths vary but can be controlled. These are: chain networks (treewidth 2; Figure 3c), diamond networks (treewidth 5; Figure 3a), and cone-cloud networks (treewidth O(\u221an), Figure 3b), abbreviated CH, D, CC respectively.\nIn the small diagrams (e.g., Figure 2), we set the domain size of the observed variables to d = 2, and of the latent variables to k = 10. For the chain, diamond, and cone-cloud models, we use (d, k) = (4, 10). The parameters of each CPT were generated by sampling from a Dirichlet distribution [6] with parameters a = [1,..., an]. We chose ai \u2208 [0, 1) uniformly at random in order to generate conditional probabilities near the edges of the simplex (i.e., far from uniform).\nUse-case benchmarks. We also test on four networks created for real-world domains. The \"Alarm\" network was developed for on-line monitoring of patients in intensive care units [2]; the \"A\" network, from the UAI literature, is synthetic but known to be daunting to exact algorithms [17]; the \"Barley\" network was built for a decision support system for growing barley without pesticides [18]; and the \"Win95\" model is an expert system for printer troubleshooting in Windows 95. Since no variables in these networks are specified as latent, we chose source vertices with at least two children to be latent confounding variables. We generated data by forward sampling and then discarding the latent variables' values.\nQueries. We hand selected identifiable queries, aiming for non-trivial ones and prioritizing those that appear to be complex. The queries and their corresponding estimands are given in the Appendix. We selected several identifiable queries per model to better evaluate the methods, as well as to emphasize that the learning time can be amortized over multiple queries."}, {"title": "4.2 Algorithms and performance measures.", "content": "EM4CI. In our EM4CI algorithm we used the EM implementation of SMILE: Structural Modeling, Inference, and Learning Engine package [1], written in C++. In SMILE, inference is carried out via join trees [22, 19], and used both in the E step of the EM algorithm [19] and for answering queries over the learned model.\nEstimand-based algorithms: Plug-In and WERM. We used the ID algorithm in the causal effect package [29] to compute the estimands. Subsequently, the plug-in method evaluates the estimand directly from the observational data, by plugging in the empirical conditional probabilities. Our plug-in implementation uses sparse table representations in Python, to ensure that the representation size of each estimated probability term is linear in the data size, rather than exponential in the number of variables. We also compare against a state-of-the-art scheme called WERM [13] using the author-provided implementation in R. A summary of the algorithms and implementations is given in Table 2.\nMeasures of performance. We report the results of EM4CI along the two phases of the algorithm; the learning phase (steps 3-9) done using the EM algorithm and the inference phase (steps 10-11) done using the join tree algorithm. For the learning process, we report the selected latent domain sizes and the total time at termination. For the inference phase we report the mean absolute deviation (mad) between the true answer and the estimated answer. The measure mad for a query P(Y | do(X)) is computed by averaging the absolute error over all single-value queries over all instantiations of the intervened and queried variables, P(Y = y | do(X = x)) for x, y \u2208 D(X) \u00d7 D(Y). We also report average inference time over the same set of query instantiations. In our experiments we increased k by twos (k = 2, 4, 6, 8, 10...) in the EM4CI algorithm."}, {"title": "4.3 Results", "content": "Results on small synthetic models. Results on models 1-8 (Figure 2) are presented in Table 3. Since these models are quite small we report the total time (learning plus inference) for EM4CI. We compare against the Plug-In estimand method (see estimand expressions in Table 1), which is guaranteed to converge to the exact answer. Therefore, we expect Plug-In to produce fairly accurate results on these small models if given enough samples. We observe that the accuracy of both methods are similar at both 100 and 1,000 samples, with EM4CI being more accurate on some cases, and Plug-In on others. EM4CI had better time performance for 100 samples, but for 1,000 samples the Plug-In was faster since learning time of EM4CI was longer."}, {"title": "WERM comparison.", "content": "The results for Models 1, 8, and 3' comparing WERM [13] to EM4CI are given in Table 4. We use domain size d = 2, and 1,000 and 10,000 samples. Again, EM4CI produced more accurate results in several instances, though the disparities are smaller than with the Plug-In method. EM4CI was faster than WERM with 1,000 samples but slower with 10,000 samples.\nUnfortunately, the available code for WERM is specific to these models, so we were unable to compare against it more generally. This also highlights a general lack of available estimand-based implementations applicable to general settings."}, {"title": "Results on large synthetic models.", "content": "In Tables 5 and 6 we show results on larger models of chains, diamonds, and cone-cloud graphs. The first two tables compare EM4CI to the plug-in method for a single query over all the models. Specifically, Table 5a presents time and accuracy results for 1,000 samples. We see that EM4CI was consistently more accurate, and in many cases significantly better (e.g., in 45-cone-cloud). We see the same trend for 10,000 samples in Table 5b. The superior accuracy of learning for model completion compared with the Plug-In is visualized well in Figure 4, which shows the accuracy trends for each class as a function of model sizes, for both 1,000 and 10,000 samples. We can see that generally, EM4CI using only 1,000 samples is even more accurate than Plug-In with 10,000 samples.\nAgain, we expect this improvement is due to the variance reduction of the estimation process. By its nature, model completion exploits more information from the causal graph than is apparent in the estimand expression alone, implicitly capturing the known conditional independence structure in its estimates of P(V). We can also include simple and easy-to-impose complexity control, in the form of the latent domain sizes, to further reduce variance. In contrast, it is difficult to impose meaningful regularity or variance reduction on the plug-in estimates of P(V) without creating significant computational burdens. Thus our results suggest that whenever the causal graph's treewidth is bounded and the estimand expression has large scope functions, we should prefer using model completion. Other settings may require more study to determine the best approach.\nFocusing on time, we see that the time of EM4CI is significantly more costly than Plug-In, with EM learning being the most time consuming part. Interestingly, while time grows with model size for both schemes, the inference time component remains efficient, likely due to the low treewidth of some of the models (e.g., the chains and the diamonds). In the 45-cone case, inference time is impacted more by model size, since its treewidth increases with the square root of the number of variables. Generally, for a single query, we find Plug-In has better time performance, and its time increases at a slower pace. In both methods, time also increases with sample size, e.g., when moving from 1,000 samples table to 10,000 samples."}, {"title": "Results on real-world data sets.", "content": "Table 7, compares EM4CI against Plug-In on a single query for all 4 networks. We observe the same pattern of performance as in the synthetic networks: EM4CI provides far more accurate results on all these models but with some time cost, attributed to learning the models. Evaluation for multiple queries for EM4CI and Plug-In are given in Table 8 for the A network. We find the same latent domain size (k\u0131rn = 4) is selected for both sample sizes. As before, learning time grows with sample size. Accuracy results are excellent and improve with increased sample sizes as well. Note that increased sample size improves accuracy but does not impact inference time, although more data may allow selection of"}, {"title": "5 Conclusion", "content": "In causal inference, the estimand-based approach has become standard: generating a potentially complex expression in terms of the visible distribution, and then estimating the required probabilities from observed data and evaluating the expression. While mathematically correct, this approach tends to ignore the difficulties inherent in estimating the complex, conditional probabilities required, and in computationally evaluating the resulting marginalized expression. These difficulties become increasingly apparent as model size increases.\nAn alternative path, relatively less explored, is to leverage the causal model structure more directly via learning. By performing model completion - i.e., learning a Causal Belief Network, including its latent confounding variables, from the observed data we exploit additional information about the co-dependence structure in the distribution, and can more easily apply complexity control and variance reduction strategies to the parameter estimation process. Then, once an approximate full model is available, traditional computationally efficient PGM algorithms can be applied to answer the query, either exactly or approximately. In settings where multiple queries are desired, their cost can be amortized by performing the learning process once.\nOur algorithm EM4CI uses the well-known EM algorithm to learn the model and its latent variable distributions, then uses tree-decomposition algorithms to answer the queries.\nWe carried out an extensive empirical evaluation, the first of its scale in the causal community, over a collection incorporating both synthetic networks and real world distributions. We evaluated and compared EM4CI's performance to the plug-in estimand approach, in terms of both accuracy and time.\nOur results appear conclusive: we show clearly that the model completion using EM4CI yields consistently superior results compared to the estimand plug-in. This benefit does come with a cost in time from the learning phase, which grows more quickly with networks size and number of samples compared to the plug-in approach. However as we show, learning time can be amortized over multiple queries on the learned models, making a collection of queries significantly more efficient.\nOur EM4CI approach relies in part on the efficiency of inference; when the treewidth of the graph is bounded, both learning and inference using EM4CI are likely to be effective. Additionally, since the structure of the graph is better exploited by model completion, we reduce the variance of our estimators, resulting in better performance for a given dataset size. Although in some cases, the estimand approach may generate a simple and easy-to-estimate expression, in larger models the expression is often complex. In such settings, the estimand expression loses structural information, forcing us to estimate high-dimensional probabilities and making it difficult to apply variance reduction strategies without creating a computationally difficult evaluation problem. Overall this suggests that model completion should be considered a competitive strategy for causal estimation."}]}