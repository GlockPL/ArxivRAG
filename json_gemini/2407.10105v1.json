{"title": "Hierarchical Multi-modal Transformer for Cross-modal Long Document Classification", "authors": ["Tengfei Liu", "Yongli Hu", "Junbin Gao", "Yanfeng Sun", "Baocai Yin"], "abstract": "Long Document Classification (LDC) has gained significant attention recently. However, multi-modal data in long documents such as texts and images are not being effectively utilized. Prior studies in this area have attempted to integrate texts and images in document-related tasks, but they have only focused on short text sequences and images of pages. How to classify long documents with hierarchical structure texts and embedding images is a new problem and faces multi-modal representation difficulties. In this paper, we propose a novel approach called Hierarchical Multi-modal Transformer (HMT) for cross-modal long document classification. The HMT conducts multi-modal feature interaction and fusion between images and texts in a hierarchical manner. Our approach uses a multi-modal transformer and a dynamic multi-scale multi-modal transformer to model the complex relationships between image features, and the section and sentence features. Furthermore, we introduce a new interaction strategy called the dynamic mask transfer module to integrate these two transformers by propagating features between them. To validate our approach, we conduct cross-modal LDC experiments on two newly created and two publicly available multi-modal long document datasets, and the results show that the proposed HMT outperforms state-of-the-art single-modality and multi-modality methods.", "sections": [{"title": "I. INTRODUCTION", "content": "As the number of accessible documents and publications grows exponentially, long document-related tasks have gained significant attention and made substantial progress. These tasks include long document summarization [1], long document question answering [2], long document machine reading comprehension [3], and long document classification (LDC) [4], [5]. As a fundamental task in Natural Language Processing (NLP), LDC plays an important role in many scenarios such as document management, document analysis, and personalized document recommendation [6], [7]. Recent studies in LDC have focused on the development of novel methods and techniques to better address the challenges posed by long documents.\nTransformer [8] has gained widespread popularity in various NLP tasks due to its exceptional performance [9]\u2013 [11]. However, the self-attention mechanism in Transformer has quadratic growth in memory usage and computational complexity with the sequence length. As a result, processing long documents with Transformer can be difficult and very expensive. To address this issue, two primary approaches have been proposed. One approach involves dividing long documents into manageable, isometric chunks using sequence cutting or sliding windows, followed by hierarchical modeling using Transformers. The other approach involves replacing the fully quadratic self-attention scheme of the Transformer with a sparse-attention mechanism. This modification enables the Transformer to handle documents with thousands of tokens or longer. However, it is important to note that these methods primarily focus on long text processing.\nIn recent years, researchers have made attempts to address multimodal document-related tasks such as visually rich document understanding [12], multimodal document quality assessment [13], and document image classification [14]. These attempts primarily focus on how to leverage and incorporate information from different modalities such as vision, language, and layout to obtain more effective multi-modal representations. However, most of these studies consider document pages as images and concentrate on the text stream extracted from these images using OCR [15], failing to explore the complex structure of long documents and the interaction between the texts and the embedding images.\nLong documents, such as scientific papers shown in Fig. 1 and other publications, typically feature long text sequences with explicit or implicit hierarchical structures and multiple embedding images. The challenge in cross-modal long document representation and classification is to fully exploit the hierarchical structure of the document and the complementary signals of visual and textual information. This problem differs from common multimodal tasks such as image-text retrieval [16], multi-modal emotion analysis [17], or visual question answering [18]. In those tasks, the relations between images and texts are simple and definitive, and the text length is typically short. To understand the nature of cross-modal long documents, consider a scientific document, as illustrated in Fig. 2. First, the texts of the long document are organized in a hierarchical manner, i.e., word-sentence-section, associated with several images. Second, as shown in Fig. 2, the relationships between the images and the texts vary across different levels. For example, a section is typically described by multiple images, while an image is often described by a number of sentences. Lastly, there may be weak correlations between the images and the texts, and some images may even be unrelated, which adds difficulties and poses challenges to the multi-modal representation of long documents.\nBased on the previous analysis, we introduce a new approach called Hierarchical Multi-modal Transformer (HMT) for Cross-modal Long Document Classification (CLDC). This framework is meticulously designed to address the intricate characteristics of multimodal long documents. Specifically, to effectively model the structured information inherent in long documents, we construct section-level and sentence-level features separately. This dual-level feature construction not only accurately captures the hierarchical structure of long documents but also facilitates the subsequent multi-granularity relationship capture between textual and visual modalities. For multimodal modeling, we employ state-of-the-art multimodal Transformers as our primary encoders. This architecture excels in performance and adeptly manages the weak correlations that often exist between text and images, thereby mitigating the potential impact of visual information on textual content. To fully leverage the hierarchical structure of the text, we introduce a dynamic mask transfer module. This innovative module ensures seamless information interaction between section-level and sentence-level multimodal Transformers, enabling higher-level structures to inform and enhance the processing at lower levels. These integrated modules synergistically work to organize and represent information in a hierarchical manner, effectively capturing complementary data from various modalities and maintaining a consistent and comprehensive flow of information.\nFig. 3 illustrates the detailed HMT architecture, which starts by utilizing pre-trained text and image models to capture the hierarchical text features, i.e., section and sentence features, and image features. Following this, two multi-modal transformers are employed to capture the complex relationships between the textual and visual features at different levels. These transformers are structured to handle hierarchical data, ensuring that both section-level and sentence-level interactions are effectively modeled. Specifically, to model the multi-scale relations between sentences and images, we introduce an enhanced transformer, namely Dynamic Multi-scale Multi-modal Transformer (DMMT). The DMMT is designed by assigning window masks of different sizes to the multi-head attention matrix, which allows it to capture varying levels of detail in the interactions. The multi-scale window branches are then dynamically fused with different weights, ensuring that the most relevant features are highlighted in the final representation. Finally, a dynamic mask transfer module is incorporated to fully leverage hierarchical text associations. This module enables effective interaction between the two multi-modal transformers, allowing insights from one level (e.g., section-level) to inform and enhance processing at another level (e.g., sentence-level). Considering that most of the existing multi-modal long document datasets only contain text sequences with less than 500 tokens and insufficient images, we construct two new multi-modal long document datasets in the field of scientific papers, which will be made available publicly to comprehensively test the proposed method. We summarize our contributions as follows,\n\u2022 The proposed Hierarchical Multi-modal Transformer (HMT) integrates text and image features at various levels of granularity for Cross-modal Long Document Classification (CLDC). To the best of our knowledge, this approach is the first of its kind to consider both the hierarchical structure and visual image information present in long documents for the task of CLDC.\n\u2022 Two well-designed techniques, namely Dynamic Multi-scale Multi-modal Transformer (DMMT) and Dynamic Mask Transfer (DMT) module, are proposed to effectively capture the complex relationships between sentences and images in long documents and enhance the interaction between the two multi-modal transformers.\n\u2022 Results from experiments conducted on two newly created and two publicly available multi-modal long document datasets indicate that the proposed method outperforms single-modal text methods and surpasses state-of-the-art multi-modal baselines.\nThe remainder of the paper is organized as follows. Related works are surveyed in Section II, and the HMT framework is presented in Section III. The evaluation is conducted in Section IV and the paper is concluded in Section V."}, {"title": "II. RELATED WORK", "content": "The task of LDC involves dealing with long documents with explicit or implicit hierarchical structures, and usually, their text contents can span thousands of tokens. Traditional document classification techniques may not be effective in such cases, as they are not capable of capturing long-range dependencies. The existing works for long document classification can be divided into two main categories.\nThe first category of methods for long document classification involves representing long documents in a hierarchical manner. For example, Pappagari et al. [19] divided the long text into manageable segments and fed each of them into the base model of BERT [20]. The inter-segment interactions are then modeled by applying a single recurrent layer or transformer to the segment representations. Similarly, Wu et al. [21] proposed a hierarchical interactive transformer, which models documents in a hierarchical manner and can capture the global document context for sentence modeling. Likewise, Liu et al. [5] proposed a hierarchical graph convolutional network for LDC, where a section graph and a word graph are constructed to model the macro and microstructure of long documents, respectively.\nAn alternative category of methods aims to address the computational complexity of the self-attention mechanism of the Transformer [8], which limits its ability to process long texts of thousands of tokens. So far, different sparsity mechanisms have been explored, e.g., the fixed pattern [4], [10], the learnable pattern [22], and the low-rank pattern [23].\nIn the case of fixed pattern sparsity mechanisms, the aim is to limit the scope of attention. This is achieved through window-based attention, global attention, and random attention techniques. For instance, Beltagy et al. [4] proposed a combination of windowed local-context attention and task-motivated global attention, successfully reducing the complexity from quadratic to linear. Zaheer et al. [10] further integrated random attention into the model proposed in [4] and also achieved linear complexity with respect to the number of tokens. The learnable pattern tries to capture both local and global contexts more effectively. For example, Kitaev et al. [24] introduced a learnable pattern using locality-sensitive hashing to identify the nearest neighbors of the attention query for attention. Zhang et al. [25] proposed a two-level attention schema that combines sliding window and pooling attention to reduce computational and memory costs. The low-rank pattern [23] observes that the self-attention matrices are low-rank. Thus, it chooses to linearly project key and value matrices into a low-dimensional space, e.g., from n to k, to achieve a O(nk) complexity.\nDespite clear advantages, these methods mainly focus on modeling text of long documents, while the cross-modal information is not considered.\nConsiderable research has been conducted to effectively organize and index document images. These approaches [14], [26] typically involve two streams of information: images and texts. In this pipeline, images serve as the visual representation of document pages, while the text is usually obtained by applying OCR [15] to the images. The rapid progress of deep learning has driven further advancements in this domain, enabling the automatic extraction of textual and visual features from document images [27]. While document image classification methods have made significant progress in recent years, they often overlook the real images within documents and do not take full advantage of the hierarchical structure of the text. As a result, there is still room for improvement in these methods. For this purpose, we propose a novel hierarchical multi-modal transformer for CLDC, in which the hierarchical structure of texts and the embedding images of long documents are utilized, instead of the page images.\nThe multi-modal transformer has proven to be effective in various multi-modal tasks, including image/video-text retrieval [16], [28], visual question answering [18], multi-modal emotional analysis [17], and visual grounding [29], resulting in impressive performance outcomes.\nCurrently, there are two main approaches for incorporating image data into a multi-modal transformer for text-image tasks. The first approach involves dividing the image into several patches, which are then treated as separate image inputs to the model. The second approach uses object detection techniques, such as Faster-RCNN [30], to extract region features from the image, and considers them as the input to the model. In some scenarios, to improve the cross-modal representation capability, additional data may also be incorporated. For instance, scene text embeddings extracted from the image [31] and class labels [32] can be utilized. In the context of text-video data, most existing approaches treat each image frame as a single feature, which is then used to interact with the text features. Our image-processing technique is similar to those used in text-video-related tasks, but with some notable differences. Firstly, our images are unordered. Additionally, the interaction objects for the images are not fine-grained word features, but high-level semantic features such as section and sentence features.\nWhile the multi-modal transformer has proven to be effective in a range of multi-modal tasks, it has not yet been applied to the CLDC task. To address this, we introduce the HMT model, which incorporates a multi-modal transformer and a dynamic multi-scale multi-modal transformer to capture the intricate relationships between image features and hierarchical text features such as section and sentence features. By leveraging these advanced modeling techniques, we aim to improve the performance of CLDC and push the boundaries of document understanding."}, {"title": "III. PROPOSED METHOD", "content": "In this section, we present the proposed Hierarchical Multi-modal Transformer (HMT) in detail. As shown in Fig. 3, the pre-trained text and image models are first employed to obtain initial hierarchical text features and image features. Next, the complex multi-modal relationships between these features are modeled using a hierarchical multi-modal transformer at both the section and sentence levels. Additionally, we introduce a dynamic mask transfer module to facilitate information exchange between the two multi-modal transformers. Finally, a fusion module is applied to aggregate the final multi-modal representations at both levels, enabling the classification of long documents.\nTo model the complex association of image features and text features at the section and sentence levels, we segment each long document t into l sequential sections in a fixed size without overlapping, denoted as {q1, q2, ..., q\u0131}. Each section contains r word tokens, i.e., qi = {Wio, Wi1, ..., Wir}, where wi\u2081 = [CLS] is the special start token for BERT-related encoders. Extra [PAD] tokens are appended to the end to meet the section length of r. Then, we apply the pre-trained BERT [20] ft(\u00b7, \u03c6), which has been widely utilized in LDC tasks [5], [19], to extract the ith section feature with word features as follows,\n$[p_i, x_{i1}, ..., x_{ir}] = f_t(q_i, \\phi), i = 1, ..., l.$\nwhere \u03c6 represents the parameters of the pre-trained BERT. The final hidden state of the [CLS] token, pi, is taken as the section feature, and the other tokens Xij,j = 1,...,r are taken as the word features. The final section features and word features of the long document can be represented as matrix features P = [P1,P2, ...,p\u0131] \u2208 Rl\u00d7d and X = [X11, ..., X1r, ..., X11,...,x\u0131r] \u2208 Rlr\u00d7d, where d is the dimension of the features.\nTo make the most of the fine-grained word features and avoid additional data input, the sentence features are directly obtained by aggregating the word features in each sentence by the proposed Sentence Token Generation (STG) block as shown in Fig. 4. This process is directed by the sentence mask Smask \u2208 Rir generated from the document. It is represented by an incrementally repeatable number sequence, in which the mask values of words in the same sentence keep fixed. The final sentence features are obtained by applying the max-pooling operation to the word features in each sentence, followed by a linear projection layer as follows,\n$s_i = maxpooling_c(X[S_{mask} == i\\cdot 1_{1\\cdot r}]), i = 1, ..., n.$\n$S_i = W_ss_i + b_s, i = 1, ..., n.$\nwhere maxpooling_c and n represent the column-wise max-pooling and the number of sentences, respectively. $1_{1\\cdot r} \\in R^{1r}$ is a vector in the size of lr with all elements being 1. Ws, bs are the trainable parameters of the linear projection layer. The final sentence features of the long document can be denoted as S = [81, 82, ..., Sn] \u2208 Rn\u00d7d.\nDifferent from the traditional multi-modal tasks, which focus on the local region features of images, we pay more attention to the global semantic representations for images vj,j = 1,2,...,m, where m is the number of images, in long documents, considering their better correspondences with the section features and sentence features. In light of the success of vision transformer (ViT) [33], which takes non-overlapping image patches as the input, we directly utilize the pre-trained CLIP model [34] with a ViT architecture as our image encoder fr(, 4). The CLIP model performs self-attention between patches and compiles the set of patch embeddings into a [CLS] embedding. We then leverage a fully connected layer to project all the image embeddings to the same dimension as the text embeddings, as follows,\nhj = Wh(fv(vj, \u03c8)) + bh, j = 1, ..., m.\nwhere \u03c8 represents the parameters of the pre-trained image encoder. The final image features of the long document can be formulated as H = [h1,h2,..., hm] \u2208 Rmxd. d is the dimension of the features.\nAs illustrated in Fig. 2, the text and image features of a long document have varying degrees of interdependence at different hierarchical levels, such as one-to-multiple and multiple-to-one relationships. To capture these complex and diverse relationships, we propose two multi-modal transformers, namely the Multi-modal Transformer and the Dynamic Multi-scale Multi-modal Transformer. Furthermore, we introduce a dynamic mask transfer module to enable information interaction between these two levels of multi-modal transformers.\nTo model the one-to-multiple relationships between sections and images, we directly concatenate the [CLS] token, section features P and image features H into a unified multi-modal sequence Fpv = [[CLS], P1, P2, \u2026, Pi, h1, h2, ..., hm]. Then, following the same architecture of the Transformer [8], we build our multi-modal transformer with a stack of multi-head self-attention layers followed by a feed-forward network, which can be denoted as follows,\n\u0395\u03c1\u03c5 - Fpv + MHSA(LN(Fpv)),\n\u00ca\u03c1\u03c5 \u2190 \u0395\u03c1\u03c5 + MLP(LN(Epv))\nwhere MHSA(\u00b7) denotes the multi-head self-attention layer, MLP() denotes the multi-layer perception layer, and LN(\u00b7) denotes the layer normalization. The final output of the [CLS] token of Fpv is adopted as the fused multi-modal representation in the section level, denoted as Ypv.\nFig. 5 illustrates the proposed Dynamic Multi-scale Multi-modal Transformer (DMMT), which consists of two branches: a single text transformer and a multi-scale multi-modal transformer. The former is designed to mitigate the negative impact of uncorrelated images on the final output, while the latter is responsible for capturing the multi-scale corresponding relationships between the sentence and image features.\nTo reduce the heterogeneous gap and enhance the representation power of multi-modal features, we first adopt a simple fusion strategy to generate multi-modal sentence representations. Specifically, given the max-pooling feature I of the image features and the sentence features [S1, S2, ..., Sn], the procedure is formulated as follows,\nI = maxpooling_c(H)\n$si = W_c[s_i; I] + b_c, i = 1, 2, ..., n.$\nwhere maxpooling_c represents the column-wise max-pooling. [;] denotes concatenation along the feature dimension. We \u2208 R2dxd and bc \u2208 Rd are the trainable parameters. The fused multi-modal sentence features can be represented as \u015c = [$1, $2, ..., $n].\nFig. 2 demonstrates that an image is usually depicted by several different sentences. To model the multi-scale correspondence relationships between sentences and images, we choose to limit the scope of attention of sentence elements by placing multi-scale window masks on the multi-head adjacency matrix. Concretely, given the multi-modal input sequence Fsv = [[CLS],\u015c, H], and the window mask matrix D\u2208 R(n+m+1)\u00d7(n+m+1) under one of the scales, the above process can be formulated as,\nF = MHSA(FS)\n$W_oConcat(A_1V_1, A_1 V_2, ..., A_hV_h)$\n$A_i = Softmax((\\frac{(F_{sv}W_q)(F_{sv}W_k)^TD}{\\sqrt{d}}))$\nwhere i = 1,2,..., h and the values of D are binary and set to 1 if they are within the attention span of the target element. MHSA(\u00b7) denotes the multi-head self-attention block. Fsv = LN(Fsv), Vi = FsW and Wa\u2208 Rdxdk, Wk \u2208 [Rdxdk, W \u2208 Rd\u00d7dv, W\u3002\u2208 Rd\u00d7d are the trainable parameters of MHSA. Especially, dk = d = d/h and h is the total number of heads in the MHSA block. Fig. 5 presents an architecture diagram of DMMT with nwin = 4 scale window mask matrices. It is noted that a fully-connected attention mask is included to capture the global information interaction between sentences and images. The text branch follows the same learning rules of Transformer [8], and the multi-modal branch owes the same learning procedure, but with a different MHSA mechanism as (6), which are denoted by T-Transformer(\u00b7), M-Transformer(\u00b7), respectively and formulated in (7) as follows,\n$Y_0 = T-Transformer([[CLS], \\hat{S}])$\n$Yi = M-Transformer(F_{sv}, D_i), i = 1, 2, ..., N_{win}.$\nIn order to fully leverage the hierarchical structure of long documents, we apply the principle of transferability to facilitate the migration of information from the section-image association to the sentence-image association, thereby enhancing the cross-modal interaction capability between sentence and image features.\nAs shown in Fig. 6, we first extract the multi-head section-image adjacency matrix Dpv \u2208 [Rhxlxm and the multi-head image-section adjacency matrix Dup \u2208 Rh\u00d7m\u00d7l from the multi-head adjacency matrix Apv \u2208 []Rh\u00d7(1+m+1)\u00d7(1+m+1) of Multi-modal Transformer. Since the operations keep the same, here we mainly take the first head of Dpv as an example to give the explanation.\nFor each section, we select the top-K images with the highest similarity scores as the critical image index set I+. The K is a dynamic number for each section, which is the smallest number that meets the condition in (10),\n$\\sum_{j\\in I^+} exp(D^1_{pv}(i, j)) / \\sum_{j=1}^m exp(D^1_{pv}(i, j)) > \\eta, i = 1, 2, ..., l.$\nwhere \u03b7 is a constant, and we set \u03b7 0.65 in all experiments.\nAccording to the critical image index set I+, i = 1, 2, ..., l, we can obtain the binary mask matrix Dou as follows,\n$D^1_{pv} = \\begin{cases} 1, j \\in I^+\\\\ 0, else \\end{cases}.$\nwhere i = 1, 2, ..., 1; j = 1,2,..., m. Then, we construct the transfer mask to model the relations between sentences and sections, denoted by Ts_p \u2208 Rn\u00d7l, in which if a sentence belongs to a section, the mask value will be set to 1, otherwise to 0, as follows,\n$[T_{s-p}]_{ij} = \\begin{cases} 1, if the sentence node i belongs to the section j\\\\ 0, else \\end{cases}.$\nIf we directly utilize Ts-p\u00d7 Dou, where \u00d7 denotes matrix multiplication, to finish the mask transfer, it will inevitably bring a lot of noise. This is mainly because the section features occasionally have completely different semantics from the included sentence features, meaning that the images associated with section features are not definitely related to the included sentence features. To make the obtained sentence-image mask matrix more discriminative, we further sparse the transfer mask Ts-p, only retaining the sentence node masks that are relevant to the corresponding section features. This process is accomplished by placing a threshold on the cosine similarity matrix between the sentence features and corresponding section features, as follows,\n$M_{s-p} = cos\\text{-similarity}(S, T_{s-p}P) > 0$\n$T_{s-p} = M_{s-p} \\times T_{s-p}$\nwhere Ms_p \u2208 Rn\u00d71. Given the sparse section-image mask matrix Dou \u2208 Rl\u00d7m and the sparse transfer mask Ts-p E Rnxl, the final sentence-image mask matrix can be formulated as,\n$D^1_{sv} = T_{s-p} \\times D^1_{pv}$\nThe same calculation procedures are applied to each head of the multi-head section-image attention matrix Dpv and the multi-head image-section attention matrix Dup. The final outputs can be expressed as Dov \u2208 Rnxm, i = 1, 2, ..., h, and Dus\u2208 Rmon, i = 1, 2, ..., h, respectively. Then, as shown in Fig. 6, we reorganize them as a whole multi-head mask matrix Damask \u2208 R(n+m+1)\u00d7(n+m+1), i = 1,2,..., h and integrate it into (6) to enhance the interaction ability of sentence and image features, as follows,\nF = MHSA(FS)\n$W_oConcat(A_1V_1, A_1 V_2, ..., A_h \\hat{V}_h)$\n$A_i = Softmax((\\frac{((F_{sv}W_q)(F_{sv}W_k)^T + D_{mask})}{\\sqrt{d}})W_m)$\nwhere i = 1,2,..., h and the dynamic weight Wm \u2208 R(n+m+1)\u00d7(n+m+1) is added to maximize the contribution of the transfer mask Dmask and minimize the influence of noise mask nodes on the multi-modal interaction at the sentence level. Accordingly, (7) can be changed as follows,\n$Y_0 = T-Transformer([[CLS], \\hat{S}])$\n$\\\\hat{y}_i = M-Transformer(F_{sv}, D_{mask}, D_i), i = 1, 2, ..., N_{win}.$\nFinally, based on (8) and (9), we can obtain the enhanced multi-modal representation \u0177su at the sentence level.\nTo obtain the final representation of the cross-modal long document, a column-wise max-pooling operation, denoted by maxpooling_c(.), is applied on the multi-modal representations at the section and sentence levels as follows,\nu = maxpooling_c([Y_{pv}, \\hat{y}_{sv}])\nwhere [,] represents the concatenation operation along the column dimension. Then, we apply the softmax function to output the probability of each label and adopt the cross-entropy loss function as the classification loss as follows,\nYc = softmax(Wuu+bu)\n$\\\\mathcal{L}_{cls} = - \\sum_{i\\in Y_D} \\sum_{j=1}^F Y_{ijl}ny_{c,ij}$\nwhere YD is the set of document indices referring to their labels, and F is the dimension of the output feature, which is equal to the number of classes. Y is the label indicator matrix."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we evaluate the proposed HMT model on two newly created and two publicly available multi-modal long document datasets and compare it with the state-of-the-art LDC methods, including both single text and multi-modal methods.\nIn the field of visually-rich document classification, several datasets have been developed, such as RVL-CDP [35] and Tobacco-3482 [36]. However, these datasets only contain images of document pages without the internal embedding images. On the other hand, the commonly used cross-modal document classification datasets typically consist of short text sequences, which are not suitable for evaluating the CLDC task\u2019s capabilities. To address this issue, we have constructed two new multi-modal long document datasets, namely MMaterials, and MAAPD, in addition to two public datasets that are relatively suitable for our task. These datasets contain structured long text sequences and multiple embedding images obtained by the Grobid\u00b9 and Fitz library, respectively. The specific statistics of the four datasets are shown in Table I.\nTo comprehensively assess the performance of various methods, we employ four commonly used metrics: Accuracy, Precision, Recall, and F1 score in our experiments. These metrics provide valuable insights into the classification results. Let TP, FP,TN, FN denote true positive, false positive, true negative, and false negative, respectively, and denote the total number of the samples as Na. These metrics are defined as follows.\nAccuracy = $\\frac{TP+TN}{N_a}$\nRecall = $\\frac{TP}{TP + FN}$, Precision = $\\frac{TP}{TP + FP}$\nF1-score = $\\frac{2 \\times Precision \\times Recall}{Precision + Recall}$\nHere, we adopt the Macro-F1 metric by first calculating the f1-score for each label and then finding their unweighted mean as the final score."}, {"title": "E. Ablation Study", "content": "An ablation study is conducted on the four datasets to examine the effects of each component of HMT. The results are presented in Table IV and Fig. 7. Specifically, we begin with the base model that utilizes only the section features. Then, we incorporate image features and use the Multi-modal Transformer (MMT) to model inter-modal correlations. Based on the results shown in Table IV, we can observe that integrating image information using the multi-modal transformer has led to significant performance improvements across all metrics on the four datasets. This indicates the benefits of incorporating image features into the model. Furthermore, by introducing the Dynamic Multi-scale Multi-modal Transformer (DMMT) to model the complex correlations between sentences and images, we achieve even better results compared to the former model. This highlights the importance of capturing multi-modal interactions at the sentence level to extract more discriminative features. Finally, by further incorporating the Dynamic Mask Transfer (DMT) module into the model to form the full architecture, we achieve the best results compared with other ablation models, which demonstrates the effectiveness of the hierarchical structure information, and simultaneously confirms the validity of our model in combining multiple images with the hierarchical text features of long documents.\nIn order to further demonstrate the impact of different parts of the DMMT module, as illustrated in Fig. 7, we present three variations of the DMMT module: -DMMT(M) represents the removal of multi-scale window masks; -DMMT(T) represents the removal of the text branch of DMMT; and -DMMT(D) represents the removal of the dynamic weight generation and assigns equal weights to all branches. The results show a significant decrease in performance when the multi-scale window masks are removed, indicating that modeling the multi-scale correspondence of sentence and image features is crucial for obtaining a more discriminative representation of long documents. Comparing the performance of DMMT with that of -DMMT(T) shows that the text branch in DMMT can effectively mitigate the influence of irrelevant images on the final representation of long documents. Moreover, the lack of the dynamic fusion mechanism also leads to performance degradation, demonstrating the significant role played by the dynamic fusion mechanism in achieving substantial improvements."}, {"title": "F. Hyperparameter Study", "content": "Section length r, the number of HMT layers N, and the number of multi-scale window masks nwin are the three major hyperparameters of our HMT model. To find the best configuration, we do extensive CLDC experiments on the validation set using various values for these hyperparameters.\n\u2022 Section Length r: In our model, the size of the section length r has a direct impact on the section and sentence representations, thus affecting subsequent cross-modal interactions. Limited by the maximum modeling length of pre-trained BERT, we select five different section lengths, i.e. 128, 192, 256, 384, 512 for each dataset and plot the F1-score as a function of r in Fig. 8(a). Intuitively, a shorter section length means more accurate text encoding. However, it may not always have a high degree of compatibility with image features. For MMaterials and MAAPD with multiple images, we find that the best results can be obtained when the section length is set to 256. For the datasets of Review and Food101, the overall performance is enhanced when the section length increases from 128 to 512. This can be explained that the image information of these two datasets is more skewed toward the expression of overall information, and to some extent, the more macroscopic text features will be semantically related to the image features.\n\u2022 Number of HMT Layers N: The influence of the layer numbers is illustrated in Fig. 8(b). We can see the decrease in our model performance when the number of layers increases from 1 to 4. We deem that this is primarily because both the initial text and image features are derived from the large-scale pre-trained models and classified as high-level semantic features. With the powerful interaction capabilities of the Transformer, it is possible to achieve effective information interaction with fewer layers. Additionally, the introduction of the multi-scale transformer further strengthens the information interaction ability of the single layer.\n\u2022 Number of Window Masks Nwin: Table V shows the results of the model using window masks with different numbers and scales. From the results, we can see that the best results are obtained when the multi-scale window masks are set to [3,5,7] for MMaterials, Review, and Food101. This suggests that increasing the number of multi-scale window masks does not always lead to better results. The plethora of multi-scale interactions will introduce additional noise information, limiting the final performance of the model. For the dataset of MAAPD, we find that the model performs best when the multi-scale window masks are set to [3,5,7,9,11],"}, {"title": "G. Computational Complexity", "content": "In this part, we present a comparison of the computational complexity of our proposed method with other baselines, except for traditional document classification methods that show relatively poor performance on the four datasets. Table VI summarizes the results, which indicate that our method has certain disadvantages in terms of time and space complexity compared to the single-modality LDC methods Hi-Transformer and HGCN. This is mostly due to the comparatively simple structures of these two models and the addition of pooling operations further expanding the advantages of the HGCN model. However, our method achieves significant performance improvements compared to these two methods. In addition, compared to other single-modality and multi-modality baselines, our method demonstrates smaller space complexity and competitive time complexity, validating that our model can achieve a favorable speed-accuracy trade-off."}, {"title": "H. Visualization", "content": "To better understand the efficacy of our proposed model, we visualize some examples of the datasets MMaterials and MAAPD and provide the outcomes of their classification under different model inputs. From the qualitative results shown in Fig. 9, we can find that both test samples produce inaccurate classification results when employing single-modality information, implying that the discrimination capability of single-modality information is subpar in some situations. Furthermore, we included a comparison with the high-performing single-modal method HGCN [5] and the multi-modal method MMDynamics [52]. Interestingly, both methods produced incorrect classification results on these two samples. Our analysis indicates that the significant ambiguity in the textual content poses a challenge for purely text-based methods to accurately interpret its meaning. Conversely, the multi-modal method MMDynamics, which focuses on fusing overall textual and visual features, does not sufficiently capture the multi-scale correspondences between these modalities. Conversely, our model achieves accurate predictions by incorporating hierarchical multi-scale modal interactions, demonstrating the complementarity between multi-modal information and confirming the efficacy of our proposed method in handling cross-modal long document data."}]}