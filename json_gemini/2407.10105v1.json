{"title": "Hierarchical Multi-modal Transformer for Cross-modal Long Document Classification", "authors": ["Tengfei Liu", "Yongli Hu", "Junbin Gao", "Yanfeng Sun", "Baocai Yin"], "abstract": "Long Document Classification (LDC) has gained significant attention recently. However, multi-modal data in long documents such as texts and images are not being effectively utilized. Prior studies in this area have attempted to integrate texts and images in document-related tasks, but they have only focused on short text sequences and images of pages. How to classify long documents with hierarchical structure texts and embedding images is a new problem and faces multi-modal representation difficulties. In this paper, we propose a novel approach called Hierarchical Multi-modal Transformer (HMT) for cross-modal long document classification. The HMT conducts multi-modal feature interaction and fusion between images and texts in a hierarchical manner. Our approach uses a multi-modal transformer and a dynamic multi-scale multi-modal transformer to model the complex relationships between image features, and the section and sentence features. Furthermore, we introduce a new interaction strategy called the dynamic mask transfer module to integrate these two transformers by propagating features between them. To validate our approach, we conduct cross-modal LDC experiments on two newly created and two publicly available multi-modal long document datasets, and the results show that the proposed HMT outperforms state-of-the-art single-modality and multi-modality methods.", "sections": [{"title": "I. INTRODUCTION", "content": "As the number of accessible documents and publications grows exponentially, long document-related tasks have gained significant attention and made substantial progress. These tasks include long document summarization [1], long document question answering [2], long document machine reading comprehension [3], and long document classification (LDC) [4], [5]. As a fundamental task in Natural Language Processing (NLP), LDC plays an important role in many scenarios such as document management, document analysis, and personalized document recommendation [6], [7]. Recent studies in LDC have focused on the development of novel methods and techniques to better address the challenges posed by long documents.\nTransformer [8] has gained widespread popularity in various NLP tasks due to its exceptional performance [9]\u2013[11]. However, the self-attention mechanism in Transformer has quadratic growth in memory usage and computational complexity with the sequence length. As a result, processing long documents with Transformer can be difficult and very expensive. To address this issue, two primary approaches have been proposed. One approach involves dividing long documents into manageable, isometric chunks using sequence cutting or sliding windows, followed by hierarchical modeling using Transformers. The other approach involves replacing the fully quadratic self-attention scheme of the Transformer with a sparse-attention mechanism. This modification enables the Transformer to handle documents with thousands of tokens or longer. However, it is important to note that these methods primarily focus on long text processing.\nIn recent years, researchers have made attempts to address multimodal document-related tasks such as visually rich document understanding [12], multimodal document quality assessment [13], and document image classification [14]. These attempts primarily focus on how to leverage and incorporate information from different modalities such as vision, language, and layout to obtain more effective multi-modal representations. However, most of these studies consider document pages as images and concentrate on the text stream extracted from these images using OCR [15], failing to explore the complex structure of long documents and the interaction between the texts and the embedding images.\nLong documents, such as scientific papers shown in Fig. 1 and other publications, typically feature long text sequences with explicit or implicit hierarchical structures and multiple embedding images. The challenge in cross-modal long document representation and classification is to fully exploit the hierarchical structure of the document and the complementary signals of visual and textual information. This problem differs from common multimodal tasks such as image-text retrieval [16], multi-modal emotion analysis [17], or visual question answering [18]. In those tasks, the relations between images and texts are simple and definitive, and the text length is typically short. To understand the nature of cross-modal long documents, consider a scientific document, as illustrated in Fig. 2. First, the texts of the long document are organized in a hierarchical manner, i.e., word-sentence-section, associated with several images. Second, as shown in Fig. 2, the relationships between the images and the texts vary across different levels. For example, a section is typically described by multiple images, while an image is often described by a number of sentences. Lastly, there may be weak correlations between the images and the texts, and some images may even be unrelated, which adds difficulties and poses challenges to the multi-modal representation of long documents.\nBased on the previous analysis, we introduce a new approach called Hierarchical Multi-modal Transformer (HMT) for Cross-modal Long Document Classification (CLDC). This framework is meticulously designed to address the intricate characteristics of multimodal long documents. Specifically, to effectively model the structured information inherent in long documents, we construct section-level and sentence-level features separately. This dual-level feature construction not only accurately captures the hierarchical structure of long documents but also facilitates the subsequent multi-granularity relationship capture between textual and visual modalities. For multimodal modeling, we employ state-of-the-art multimodal Transformers as our primary encoders. This architecture excels in performance and adeptly manages the weak correlations that often exist between text and images, thereby mitigating the potential impact of visual information on textual content. To fully leverage the hierarchical structure of the text, we introduce a dynamic mask transfer module. This innovative module ensures seamless information interaction between section-level and sentence-level multimodal Transformers, enabling higher-level structures to inform and enhance the processing at lower levels. These integrated modules synergistically work to organize and represent information in a hierarchical manner, effectively capturing complementary data from various modalities and maintaining a consistent and comprehensive flow of information.\nFig. 3 illustrates the detailed HMT architecture, which starts by utilizing pre-trained text and image models to capture the hierarchical text features, i.e., section and sentence features, and image features. Following this, two multi-modal transformers are employed to capture the complex relationships between the textual and visual features at different levels. These transformers are structured to handle hierarchical data, ensuring that both section-level and sentence-level interactions are effectively modeled. Specifically, to model the multi-scale relations between sentences and images, we introduce an enhanced transformer, namely Dynamic Multi-scale Multi-modal Transformer (DMMT). The DMMT is designed by assigning window masks of different sizes to the multi-head attention matrix, which allows it to capture varying levels of detail in the interactions. The multi-scale window branches are then dynamically fused with different weights, ensuring that the most relevant features are highlighted in the final representation. Finally, a dynamic mask transfer module is incorporated to fully leverage hierarchical text associations. This module enables effective interaction between the two multi-modal transformers, allowing insights from one level (e.g., section-level) to inform and enhance processing at another level (e.g., sentence-level). Considering that most of the existing multi-modal long document datasets only contain text sequences with less than 500 tokens and insufficient images, we construct two new multi-modal long document datasets in the field of scientific papers, which will be made available publicly to comprehensively test the proposed method. We summarize our contributions as follows,\n\u2022 The proposed Hierarchical Multi-modal Transformer (HMT) integrates text and image features at various levels of granularity for Cross-modal Long Document Classification (CLDC). To the best of our knowledge, this approach is the first of its kind to consider both the hierarchical structure and visual image information present in long documents for the task of CLDC.\n\u2022 Two well-designed techniques, namely Dynamic Multi-scale Multi-modal Transformer (DMMT) and Dynamic Mask Transfer (DMT) module, are proposed to effectively capture the complex relationships between sentences and images in long documents and enhance the interaction between the two multi-modal transformers.\n\u2022 Results from experiments conducted on two newly created and two publicly available multi-modal long document datasets indicate that the proposed method outperforms single-modal text methods and surpasses state-of-the-art multi-modal baselines.\nThe remainder of the paper is organized as follows. Related works are surveyed in Section II, and the HMT framework is presented in Section III. The evaluation is conducted in Section IV and the paper is concluded in Section V."}, {"title": "II. RELATED WORK", "content": "The task of LDC involves dealing with long documents with explicit or implicit hierarchical structures, and usually, their text contents can span thousands of tokens. Traditional document classification techniques may not be effective in such cases, as they are not capable of capturing long-range dependencies. The existing works for long document classification can be divided into two main categories.\nThe first category of methods for long document classification involves representing long documents in a hierarchical manner. For example, Pappagari et al. [19] divided the long text into manageable segments and fed each of them into the base model of BERT [20]. The inter-segment interactions are then modeled by applying a single recurrent layer or transformer to the segment representations. Similarly, Wu et al. [21] proposed a hierarchical interactive transformer, which models documents in a hierarchical manner and can capture the global document context for sentence modeling. Likewise, Liu et al. [5] proposed a hierarchical graph convolutional network for LDC, where a section graph and a word graph are constructed to model the macro and microstructure of long documents, respectively.\nAn alternative category of methods aims to address the computational complexity of the self-attention mechanism of the Transformer [8], which limits its ability to process long texts of thousands of tokens. So far, different sparsity mechanisms have been explored, e.g., the fixed pattern [4], [10], the learnable pattern [22], and the low-rank pattern [23].\nIn the case of fixed pattern sparsity mechanisms, the aim is to limit the scope of attention. This is achieved through window-based attention, global attention, and random attention techniques. For instance, Beltagy et al. [4] proposed a combination of windowed local-context attention and task-motivated global attention, successfully reducing the complexity from quadratic to linear. Zaheer et al. [10] further integrated random attention into the model proposed in [4] and also achieved linear complexity with respect to the number of tokens. The learnable pattern tries to capture both local and global contexts more effectively. For example, Kitaev et al. [24] introduced a learnable pattern using locality-sensitive hashing to identify the nearest neighbors of the attention query for attention. Zhang et al. [25] proposed a two-level attention schema that combines sliding window and pooling attention to reduce computational and memory costs. The low-rank pattern [23] observes that the self-attention matrices are low-rank. Thus, it chooses to linearly project key and value matrices into a low-dimensional space, e.g., from n to k, to achieve a O(nk) complexity.\nDespite clear advantages, these methods mainly focus on modeling text of long documents, while the cross-modal information is not considered."}, {"title": "B. Document Image Classification", "content": "Considerable research has been conducted to effectively organize and index document images. These approaches [14], [26] typically involve two streams of information: images and texts. In this pipeline, images serve as the visual representation of document pages, while the text is usually obtained by applying OCR [15] to the images. The rapid progress of deep learning has driven further advancements in this domain, enabling the automatic extraction of textual and visual features from document images [27]. While document image classification methods have made significant progress in recent years, they often overlook the real images within documents and do not take full advantage of the hierarchical structure of the text. As a result, there is still room for improvement in these methods. For this purpose, we propose a novel hierarchical multi-modal transformer for CLDC, in which the hierarchical structure of texts and the embedding images of long documents are utilized, instead of the page images."}, {"title": "C. Multi-modal Transformer", "content": "The multi-modal transformer has proven to be effective in various multi-modal tasks, including image/video-text retrieval [16], [28], visual question answering [18], multi-modal emotional analysis [17], and visual grounding [29], resulting in impressive performance outcomes.\nCurrently, there are two main approaches for incorporating image data into a multi-modal transformer for text-image tasks. The first approach involves dividing the image into several patches, which are then treated as separate image inputs to the model. The second approach uses object detection techniques, such as Faster-RCNN [30], to extract region features from the image, and considers them as the input to the model. In some scenarios, to improve the cross-modal representation capability, additional data may also be incorporated. For instance, scene text embeddings extracted from the image [31] and class labels [32] can be utilized. In the context of text-video data, most existing approaches treat each image frame as a single feature, which is then used to interact with the text features. Our image-processing technique is similar to those used in text-video-related tasks, but with some notable differences. Firstly, our images are unordered. Additionally, the interaction objects for the images are not fine-grained word features, but high-level semantic features such as section and sentence features.\nWhile the multi-modal transformer has proven to be effective in a range of multi-modal tasks, it has not yet been applied to the CLDC task. To address this, we introduce the HMT model, which incorporates a multi-modal transformer and a dynamic multi-scale multi-modal transformer to capture the intricate relationships between image features and hierarchical text features such as section and sentence features. By leveraging these advanced modeling techniques, we aim to improve the performance of CLDC and push the boundaries of document understanding."}, {"title": "III. PROPOSED METHOD", "content": "In this section, we present the proposed Hierarchical Multi-modal Transformer (HMT) in detail. As shown in Fig. 3, the pre-trained text and image models are first employed to obtain initial hierarchical text features and image features. Next, the complex multi-modal relationships between these features are modeled using a hierarchical multi-modal transformer at both the section and sentence levels. Additionally, we introduce a dynamic mask transfer module to facilitate information exchange between the two multi-modal transformers. Finally, a fusion module is applied to aggregate the final multi-modal representations at both levels, enabling the classification of long documents."}, {"title": "A. Feature Extraction", "content": "To model the complex association of image features and text features at the section and sentence levels, we segment each long document t into l sequential sections in a fixed size without overlapping, denoted as {q1, q2, ..., q\u0131}. Each section contains r word tokens, i.e., qi = {Wio, Wi1, ..., Wir}, where wi\u2081 = [CLS] is the special start token for BERT-related encoders. Extra [PAD] tokens are appended to the end to meet the section length of r. Then, we apply the pre-trained BERT [20] $f_t(\\cdot, \\phi)$, which has been widely utilized in LDC tasks [5], [19], to extract the ith section feature with word features as follows,\n\\[\\begin{equation}\\label{eq:1} [p_i, X_{i1}, ..., X_{ir}] = f_t(q_i, \\phi), i = 1, ..., l. \\end{equation}\\]\nwhere \\(\\phi\\) represents the parameters of the pre-trained BERT. The final hidden state of the [CLS] token, \\(p_i\\), is taken as the section feature, and the other tokens \\(X_{ij}, j = 1,...,r\\) are taken as the word features. The final section features and word features of the long document can be represented as matrix features \\(P = [p_1,p_2, ...,p_l] \\in \\mathbb{R}^{l \\times d}\\) and \\(X = [X_{11}, ..., X_{1r}, ..., X_{l1},...,X_{lr}] \\in \\mathbb{R}^{lr \\times d}\\), where d is the dimension of the features.\nTo make the most of the fine-grained word features and avoid additional data input, the sentence features are directly obtained by aggregating the word features in each sentence by the proposed Sentence Token Generation (STG) block as shown in Fig. 4. This process is directed by the sentence mask \\(S_{\\text{mask}} \\in \\mathbb{R}^{lr}\\) generated from the document. It is represented by an incrementally repeatable number sequence, in which the mask values of words in the same sentence keep fixed. The final sentence features are obtained by applying the max-pooling operation to the word features in each sentence, followed by a linear projection layer as follows,\n\\[\\begin{equation}\\label{eq:2} s_i = W_s{\\text{si}} + b_s, i = 1, ..., n. \\end{equation}\\]\nwhere maxpooling_c and n represent the column-wise max-pooling and the number of sentences, respectively. \\(1_{lr} \\in \\mathbb{R}^{lr}\\) is a vector in the size of lr with all elements being 1. \\(W_s, b_s\\) are the trainable parameters of the linear projection layer. The final sentence features of the long document can be denoted as \\(S = [s_1, s_2, ..., s_n] \\in \\mathbb{R}^{n \\times d}\\).\nDifferent from the traditional multi-modal tasks, which focus on the local region features of images, we pay more attention to the global semantic representations for images vj,j = 1,2,...,m, where m is the number of images, in long documents, considering their better correspondences with the section features and sentence features. In light of the success of vision transformer (ViT) [33], which takes non-overlapping image patches as the input, we directly utilize the pre-trained CLIP model [34] with a ViT architecture as our image encoder \\(f_v(\\cdot, \\psi)\\). The CLIP model performs self-attention between patches and compiles the set of patch embeddings into a [CLS] embedding. We then leverage a fully connected layer to project all the image embeddings to the same dimension as the text embeddings, as follows,\n\\[\\begin{equation}\\label{eq:3} h_j = W_h(f_v(v_j, \\psi)) + b_h, j = 1, ..., m. \\end{equation}\\]\nwhere \\(\\psi\\) represents the parameters of the pre-trained image encoder. The final image features of the long document can be formulated as \\(H = [h_1,h_2,..., h_m] \\in \\mathbb{R}^{m \\times d}\\). d is the dimension of the features."}, {"title": "B. Hierarchical Multi-modal Transformer", "content": "As illustrated in Fig. 2, the text and image features of a long document have varying degrees of interdependence at different hierarchical levels, such as one-to-multiple and multiple-to-one relationships. To capture these complex and diverse relationships, we propose two multi-modal transformers, namely the Multi-modal Transformer and the Dynamic Multi-scale Multi-modal Transformer. Furthermore, we introduce a dynamic mask transfer module to enable information interaction between these two levels of multi-modal transformers."}, {"title": "1) Multi-modal Transformer", "content": "To model the one-to-multiple relationships between sections and images, we directly concatenate the [CLS] token, section features P and image features H into a unified multi-modal sequence Fpv = [[CLS], p1, p2, \u2026, pi, h1, h2, ..., hm]. Then, following the same architecture of the Transformer [8], we build our multi-modal transformer with a stack of multi-head self-attention layers followed by a feed-forward network, which can be denoted as follows,\n\\[\\begin{equation}\\label{eq:4} \\hat{E}_{pv} \\leftarrow E_{pv} + MLP(LN(E_{pv})) \\end{equation}\\]\nwhere MHSA(\u00b7) denotes the multi-head self-attention layer, MLP() denotes the multi-layer perception layer, and LN(\u00b7) denotes the layer normalization. The final output of the [CLS] token of Fpv is adopted as the fused multi-modal representation in the section level, denoted as Ypv."}, {"title": "2) Dynamic Multi-scale Multi-modal Transformer", "content": "Fig. 5 illustrates the proposed Dynamic Multi-scale Multi-modal Transformer (DMMT), which consists of two branches: a single text transformer and a multi-scale multi-modal transformer. The former is designed to mitigate the negative impact of uncorrelated images on the final output, while the latter is responsible for capturing the multi-scale corresponding relationships between the sentence and image features.\nTo reduce the heterogeneous gap and enhance the representation power of multi-modal features, we first adopt a simple fusion strategy to generate multi-modal sentence representations. Specifically, given the max-pooling feature I of the image features and the sentence features [s1, s2, ..., sn], the procedure is formulated as follows,\n\\[\\begin{equation}\\label{eq:5} \\hat{F}_{sv} = MHSA(F_{sv}, D) \\end{equation}\\]\nwhere maxpooling_c represents the column-wise max-pooling. [;] denotes concatenation along the feature dimension. \\(W_c \\in \\mathbb{R}^{2d \\times d}\\) and \\(b_c \\in \\mathbb{R}^{d}\\) are the trainable parameters. The fused multi-modal sentence features can be represented as \\(S = [\\hat{s}_1, \\hat{s}_2, ..., \\hat{s}_n]\\).\nFig. 2 demonstrates that an image is usually depicted by several different sentences. To model the multi-scale correspondence relationships between sentences and images, we choose to limit the scope of attention of sentence elements by placing multi-scale window masks on the multi-head adjacency matrix. Concretely, given the multi-modal input sequence Fsv = [[CLS],\u015c, H], and the window mask matrix D\u2208 R(n+m+1)\u00d7(n+m+1) under one of the scales, the above process can be formulated as,\n\\[\\begin{equation}\\label{eq:6} \\hat{A}_i = \\text{Softmax}((\\hat{F}_{sv}W^Q)(\\hat{F}_{sv}W^K)^T D / \\sqrt{d}) \\end{equation}\\]\nwhere i = 1,2,..., h and the values of D are binary and set to 1 if they are within the attention span of the target element. MHSA(\u00b7) denotes the multi-head self-attention block.  denotes the layer normalization. \\(\\, \\) and \\(W_o\\in \\mathbb{R}^{d \\times d}\\) are the trainable parameters of MHSA. Especially, \\(d_k = d_v = d/h\\) and h is the total number of heads in the MHSA block. Fig. 5 presents an architecture diagram of DMMT with nwin = 4 scale window mask matrices. It is noted that a fully-connected attention mask is included to capture the global information interaction between sentences and images. The text branch follows the same learning rules of Transformer [8], and the multi-modal branch owes the same learning procedure, but with a different MHSA mechanism as (6), which are denoted by T-Transformer(\u00b7), M-Transformer(\u00b7), respectively and formulated in (7) as follows,\n\\[\\begin{equation}\\label{eq:7} Y_i = \\text{M-Transformer}(\\hat{F}_{sv}, D_i), i = 1, 2, ..., N_{\\text{win}}. \\end{equation}\\]"}, {"title": "3) Dynamic Mask Transfer", "content": "In order to fully leverage the hierarchical structure of long documents, we apply the principle of transferability to facilitate the migration of information from the section-image association to the sentence-image association, thereby enhancing the cross-modal interaction capability between sentence and image features.\nAs shown in Fig. 6, we first extract the multi-head section-image adjacency matrix Dpv \u2208 and the multi-head image-section adjacency matrix Dvp \u2208 from the multi-head adjacency matrix Apv \u2208 of Multi-modal Transformer. Since the operations keep the same, here we mainly take the first head of Dpv as an example to give the explanation.\nFor each section, we select the top-K images with the highest similarity scores as the critical image index set I+. The K is a dynamic number for each section, which is the smallest number that meets the condition in (10),\n\\[\\begin{equation}\\label{eq:10} \\sum_{j \\in I^+} \\exp(D_{pv}(i, j)) / \\sum_{j=1}^m \\exp(D_{pv}(i, j)) > \\eta , i = 1, 2, ..., l. \\end{equation}\\]\nwhere \u03b7 is a constant, and we set \u03b7 = 0.65 in all experiments.\nAccording to the critical image index set I+, i = 1, 2, ..., l, we can obtain the binary mask matrix as follows,\n\\[\\begin{equation}\\label{eq:11} D_{pv} = \\begin{cases} 1, j \\in I^+ \\\\ 0, \\text{else} \\end{cases} \\end{equation}\\]\nwhere i = 1, 2, ..., 1; j = 1,2,..., m. Then, we construct the transfer mask to model the relations between sentences and sections, denoted by \\(T_{s-p} \\in \\mathbb{R}^{n \\times l}\\), in which if a sentence belongs to a section, the mask value will be set to 1, otherwise to 0, as follows,\n\\[\\begin{equation}\\label{eq:12} [T_{s-p}]_{ij} = \\begin{cases} 1, \\text{if the sentence node i belongs to the section j} \\\\ 0, \\text{else} \\end{cases} \\end{equation}\\]\nIf we directly utilize \\(T_{s-p} \\times D_{pv}\\), where \\(\\times\\) denotes matrix multiplication, to finish the mask transfer, it will inevitably bring a lot of noise. This is mainly because the section features occasionally have completely different semantics from the included sentence features, meaning that the images associated with section features are not definitely related to the included sentence features. To make the obtained sentence-image mask matrix more discriminative, we further sparse the transfer mask T_s-p, only retaining the sentence node masks that are relevant to the corresponding section features. This process is accomplished by placing a threshold on the cosine similarity matrix between the sentence features and corresponding section features, as follows,\n\\[\\begin{equation}\\label{eq:13} \\text{Ms-p} = \\text{cos-similarity}(S, T_{s-p}P) > 0 \\end{equation}\\]\nwhere Ms_p \u2208 \\(i \\in \\mathbb{Z}^+\\). Given the sparse section-image mask matrix and the sparse transfer mask T_s-p E , the final sentence-image mask matrix can be formulated as,\n\\[\\begin{equation}\\label{eq:14} D_{sv} = T_{s-p} \\times D_{pv} \\end{equation}\\]\nThe same calculation procedures are applied to each head of the multi-head section-image attention matrix Dpv and the multi-head image-section attention matrix Dvp. The final outputs can be expressed as  \u2208 i = 1, 2, ..., h, and \u2208 i = 1, 2, ..., h, respectively. Then, as shown in Fig. 6, we reorganize them as a whole multi-head mask matrix Dmask \u2208 = 1,2,..., h and integrate it into (6) to enhance the interaction ability of sentence and image features, as follows,\n\\[\\begin{equation}\\label{eq:15} W_m) \\end{equation}\\]\nwhere i = 1,2,..., h and the dynamic weight Wm \u2208 is added to maximize the contribution of the transfer mask Dmask and minimize the influence of noise"}, {"title": "C. Model Training", "content": "To obtain the final representation of the cross-modal long document, a column-wise max-pooling operation, denoted by maxpooling_c(.), is applied on the multi-modal representations at the section and sentence levels as follows,\n\\[\\begin{equation}\\label{eq:17} u = \\text{maxpooling_c}([Y_{pv}, \\hat{Y}_{sv}]) \\end{equation}\\]\nwhere [,] represents the concatenation operation along the column dimension. Then, we apply the softmax function to output the probability of each label and adopt the cross-entropy loss function as the classification loss as follows,\n\\[\\begin{equation}\\label{eq:18} L_{cls} = - \\sum_{i \\in YD} \\sum_{j=1}^F Y_{ij}ln{\\hat{y}_{c,ij}} \\end{equation}\\]\nwhere YD is the set of document indices referring to their labels, and F is the dimension of the output feature, which is equal to the number of classes. Y is the label indicator matrix."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we evaluate the proposed HMT model on two newly created and two publicly available multi-modal long document datasets and compare it with the state-of-the-art LDC methods, including both single text and multi-modal methods."}, {"title": "A. Datasets", "content": "In the field of visually-rich document classification, several datasets have been developed, such as RVL-CDP [35] and Tobacco-3482 [36]. However, these datasets only contain images of document pages without the internal embedding images. On the other hand, the commonly used cross-modal document classification datasets typically consist of short text sequences, which are not suitable for evaluating the CLDC task's capabilities. To address this issue, we have constructed two new multi-modal long document datasets, namely MMaterials, and MAAPD, in addition to two public datasets that are relatively suitable for our task. These datasets contain structured long text sequences and multiple embedding images obtained by the Grobid\u00b9 and Fitz library, respectively. The specific statistics of the four datasets are shown in Table I.\n\u2022 Materials: We select 7 materials fields, i.e., composite material, battery separator, energy storage material, graphene, nanomaterial, silicon carbide, and titanium alloy, and download 9320 articles from the Internet. There are 6812 training samples, 1247 validation samples, and 1261 testing samples. Each sample has 4459 words and 9 images on average.\n\u2022 MAAPD: We expand the dataset AAPD [37] to a multimodal version, i.e., MAAPD, and use it in the CLDC task. As shown in Table I, 24573 samples are obtained by automatically parsing the XML files and extracting the image information. The dataset contains 20753 training samples, 2000 validation samples, and 2000 testing samples, which are assigned to one of 10 subject categories such as CS.CV, CS.cr, cs.ro. Each document includes an average of 13.5 images.\n\u2022 Review: The original online review dataset [38] has more than 44 thousand reviews, including 244 thousand images. However, the average text length of the samples is only 237.3 tokens. To better verify the effectiveness of our proposed model in processing long documents, we select the samples with text lengths larger than 256 and reorganize them as a new dataset, which contains 10352 training, 1294 validation, and 1294 testing samples with an average of 419.9 words, 27.8 sentences, and 4.1 images.\n\u2022 Food101: The UPMC Food101 dataset [39] contains web pages with textual recipe descriptions for 101 food. Each page is matched with a single image, which is obtained by querying Google Image Search for the given category. Typical examples of food labels include Filet Mignon, Pad Thai, Breakfast Burrito, and Spaghetti Bolognese."}, {"title": "B. Baselines and Metrics", "content": "We first compare the proposed HMT with several document classification methods, including Logistic"}, {"title": "E. Ablation Study", "content": "An ablation study is conducted on the four datasets to examine the effects of each component of HMT. The results are presented in Table IV and Fig. 7. Specifically, we begin with the base model that utilizes only the section features. Then, we incorporate image features and use the Multi-modal Transformer (MMT) to model inter-modal correlations. Based on the results shown in Table IV, we can observe that integrating image information using the multi-modal transformer has led to significant performance improvements across all metrics on the four datasets. This indicates the benefits of incorporating image features into the model. Furthermore, by introducing the Dynamic Multi-scale Multi-modal Transformer (DMMT) to model the complex correlations between sentences and images, we achieve even better results compared to the former model. This highlights the importance of capturing multi-modal interactions at the sentence level to extract more discriminative features. Finally, by further incorporating the Dynamic Mask Transfer (DMT) module into the model to form the full architecture, we achieve the best results compared with other ablation models, which demonstrates the effectiveness of the hierarchical structure information, and simultaneously confirms the validity of our model in combining multiple images with the hierarchical text features of long documents.\nIn order to further demonstrate the impact of different parts of the DMMT module, as illustrated in Fig. 7, we present three variations of the DMMT module: -DMMT(M) represents the removal of multi-scale window masks; -DMMT(T) represents the removal of the text branch of DMMT; and -DMMT(D) represents the removal of the dynamic weight generation and assigns equal weights to all branches. The results show a significant decrease in performance when the multi-scale window masks are removed, indicating that modeling the multi-scale correspondence of sentence and image features is crucial for obtaining a more discriminative representation of long documents. Comparing the performance of DMMT with that of -DMMT(T) shows that the text branch in DMMT can effectively mitigate the influence of irrelevant images on the final representation of long documents. Moreover, the lack of the dynamic fusion mechanism also leads to performance degradation, demonstrating the significant role played by the dynamic fusion mechanism in achieving substantial improvements."}, {"title": "F. Hyperparameter Study", "content": "Section length r", "r": "In our model", "N": "The influence of the layer numbers is illustrated in Fig. 8(b). We can see the decrease in our model performance when the number of layers increases from 1 to 4. We deem that this is primarily because both the initial text and image features are derived from the large-scale pre-trained models and classified as high-level semantic features. With the powerful interaction capabilities of the Transformer"}, {"title": "Hierarchical Multi-modal Transformer for Cross-modal Long Document Classification", "authors": ["Tengfei Liu", "Yongli Hu", "Junbin Gao", "Yanfeng Sun", "Baocai Yin"], "abstract": "Long Document Classification (LDC) has gained significant attention recently. However, multi-modal data in long documents such as texts and images are not being effectively utilized. Prior studies in this area have attempted to integrate texts and images in document-related tasks, but they have only focused on short text sequences and images of pages. How to classify long documents with hierarchical structure texts and embedding images is a new problem and faces multi-modal representation difficulties. In this paper, we propose a novel approach called Hierarchical Multi-modal Transformer (HMT) for cross-modal long document classification. The HMT conducts multi-modal feature interaction and fusion between images and texts in a hierarchical manner. Our approach uses a multi-modal transformer and a dynamic multi-scale multi-modal transformer to model the complex relationships between image features, and the section and sentence features. Furthermore, we introduce a new interaction strategy called the dynamic mask transfer module to integrate these two transformers by propagating features between them. To validate our approach, we conduct cross-modal LDC experiments on two newly created and two publicly available multi-modal long document datasets, and the results show that the proposed HMT outperforms state-of-the-art single-modality and multi-modality methods.", "sections": [{"title": "I. INTRODUCTION", "content": "As the number of accessible documents and publications grows exponentially, long document-related tasks have gained significant attention and made substantial progress. These tasks include long document summarization [1], long document question answering [2], long document machine reading comprehension [3], and long document classification (LDC) [4], [5]. As a fundamental task in Natural Language Processing (NLP), LDC plays an important role in many scenarios such as document management, document analysis, and personalized document recommendation [6], [7]. Recent studies in LDC have focused on the development of novel methods and techniques to better address the challenges posed by long documents.\nTransformer [8] has gained widespread popularity in various NLP tasks due to its exceptional performance [9]\u2013[11]. However, the self-attention mechanism in Transformer has quadratic growth in memory usage and computational complexity with the sequence length. As a result, processing long documents with Transformer can be difficult and very expensive. To address this issue, two primary approaches have been proposed. One approach involves dividing long documents into manageable, isometric chunks using sequence cutting or sliding windows, followed by hierarchical modeling using Transformers. The other approach involves replacing the fully quadratic self-attention scheme of the Transformer with a sparse-attention mechanism. This modification enables the Transformer to handle documents with thousands of tokens or longer. However, it is important to note that these methods primarily focus on long text processing.\nIn recent years, researchers have made attempts to address multimodal document-related tasks such as visually rich document understanding [12], multimodal document quality assessment [13], and document image classification [14]. These attempts primarily focus on how to leverage and incorporate information from different modalities such as vision, language, and layout to obtain more effective multi-modal representations. However, most of these studies consider document pages as images and concentrate on the text stream extracted from these images using OCR [15], failing to explore the complex structure of long documents and the interaction between the texts and the embedding images.\nLong documents, such as scientific papers shown in Fig. 1 and other publications, typically feature long text sequences with explicit or implicit hierarchical structures and multiple embedding images. The challenge in cross-modal long document representation and classification is to fully exploit the hierarchical structure of the document and the complementary signals of visual and textual information. This problem differs from common multimodal tasks such as image-text retrieval [16], multi-modal emotion analysis [17], or visual question answering [18]. In those tasks, the relations between images and texts are simple and definitive, and the text length is typically short. To understand the nature of cross-modal long documents, consider a scientific document, as illustrated in Fig. 2. First, the texts of the long document are organized in a hierarchical manner, i.e., word-sentence-section, associated with several images. Second, as shown in Fig. 2, the relationships between the images and the texts vary across different levels. For example, a section is typically described by multiple images, while an image is often described by a number of sentences. Lastly, there may be weak correlations between the images and the texts, and some images may even be unrelated, which adds difficulties and poses challenges to the multi-modal representation of long documents.\nBased on the previous analysis, we introduce a new approach called Hierarchical Multi-modal Transformer (HMT) for Cross-modal Long Document Classification (CLDC). This framework is meticulously designed to address the intricate characteristics of multimodal long documents. Specifically, to effectively model the structured information inherent in long documents, we construct section-level and sentence-level features separately. This dual-level feature construction not only accurately captures the hierarchical structure of long documents but also facilitates the subsequent multi-granularity relationship capture between textual and visual modalities. For multimodal modeling, we employ state-of-the-art multimodal Transformers as our primary encoders. This architecture excels in performance and adeptly manages the weak correlations that often exist between text and images, thereby mitigating the potential impact of visual information on textual content. To fully leverage the hierarchical structure of the text, we introduce a dynamic mask transfer module. This innovative module ensures seamless information interaction between section-level and sentence-level multimodal Transformers, enabling higher-level structures to inform and enhance the processing at lower levels. These integrated modules synergistically work to organize and represent information in a hierarchical manner, effectively capturing complementary data from various modalities and maintaining a consistent and comprehensive flow of information.\nFig. 3 illustrates the detailed HMT architecture, which starts by utilizing pre-trained text and image models to capture the hierarchical text features, i.e., section and sentence features, and image features. Following this, two multi-modal transformers are employed to capture the complex relationships between the textual and visual features at different levels. These transformers are structured to handle hierarchical data, ensuring that both section-level and sentence-level interactions are effectively modeled. Specifically, to model the multi-scale relations between sentences and images, we introduce an enhanced transformer, namely Dynamic Multi-scale Multi-modal Transformer (DMMT). The DMMT is designed by assigning window masks of different sizes to the multi-head attention matrix, which allows it to capture varying levels of detail in the interactions. The multi-scale window branches are then dynamically fused with different weights, ensuring that the most relevant features are highlighted in the final representation. Finally, a dynamic mask transfer module is incorporated to fully leverage hierarchical text associations. This module enables effective interaction between the two multi-modal transformers, allowing insights from one level (e.g., section-level) to inform and enhance processing at another level (e.g., sentence-level). Considering that most of the existing multi-modal long document datasets only contain text sequences with less than 500 tokens and insufficient images, we construct two new multi-modal long document datasets in the field of scientific papers, which will be made available publicly to comprehensively test the proposed method. We summarize our contributions as follows,\n\u2022 The proposed Hierarchical Multi-modal Transformer (HMT) integrates text and image features at various levels of granularity for Cross-modal Long Document Classification (CLDC). To the best of our knowledge, this approach is the first of its kind to consider both the hierarchical structure and visual image information present in long documents for the task of CLDC.\n\u2022 Two well-designed techniques, namely Dynamic Multi-scale Multi-modal Transformer (DMMT) and Dynamic Mask Transfer (DMT) module, are proposed to effectively capture the complex relationships between sentences and images in long documents and enhance the interaction between the two multi-modal transformers.\n\u2022 Results from experiments conducted on two newly created and two publicly available multi-modal long document datasets indicate that the proposed method outperforms single-modal text methods and surpasses state-of-the-art multi-modal baselines.\nThe remainder of the paper is organized as follows. Related works are surveyed in Section II, and the HMT framework is presented in Section III. The evaluation is conducted in Section IV and the paper is concluded in Section V."}, {"title": "II. RELATED WORK", "content": "The task of LDC involves dealing with long documents with explicit or implicit hierarchical structures, and usually, their text contents can span thousands of tokens. Traditional document classification techniques may not be effective in such cases, as they are not capable of capturing long-range dependencies. The existing works for long document classification can be divided into two main categories.\nThe first category of methods for long document classification involves representing long documents in a hierarchical manner. For example, Pappagari et al. [19] divided the long text into manageable segments and fed each of them into the base model of BERT [20]. The inter-segment interactions are then modeled by applying a single recurrent layer or transformer to the segment representations. Similarly, Wu et al. [21] proposed a hierarchical interactive transformer, which models documents in a hierarchical manner and can capture the global document context for sentence modeling. Likewise, Liu et al. [5] proposed a hierarchical graph convolutional network for LDC, where a section graph and a word graph are constructed to model the macro and microstructure of long documents, respectively.\nAn alternative category of methods aims to address the computational complexity of the self-attention mechanism of the Transformer [8], which limits its ability to process long texts of thousands of tokens. So far, different sparsity mechanisms have been explored, e.g., the fixed pattern [4], [10], the learnable pattern [22], and the low-rank pattern [23].\nIn the case of fixed pattern sparsity mechanisms, the aim is to limit the scope of attention. This is achieved through window-based attention, global attention, and random attention techniques. For instance, Beltagy et al. [4] proposed a combination of windowed local-context attention and task-motivated global attention, successfully reducing the complexity from quadratic to linear. Zaheer et al. [10] further integrated random attention into the model proposed in [4] and also achieved linear complexity with respect to the number of tokens. The learnable pattern tries to capture both local and global contexts more effectively. For example, Kitaev et al. [24] introduced a learnable pattern using locality-sensitive hashing to identify the nearest neighbors of the attention query for attention. Zhang et al. [25] proposed a two-level attention schema that combines sliding window and pooling attention to reduce computational and memory costs. The low-rank pattern [23] observes that the self-attention matrices are low-rank. Thus, it chooses to linearly project key and value matrices into a low-dimensional space, e.g., from n to k, to achieve a O(nk) complexity.\nDespite clear advantages, these methods mainly focus on modeling text of long documents, while the cross-modal information is not considered."}, {"title": "B. Document Image Classification", "content": "Considerable research has been conducted to effectively organize and index document images. These approaches [14], [26] typically involve two streams of information: images and texts. In this pipeline, images serve as the visual representation of document pages, while the text is usually obtained by applying OCR [15] to the images. The rapid progress of deep learning has driven further advancements in this domain, enabling the automatic extraction of textual and visual features from document images [27]. While document image classification methods have made significant progress in recent years, they often overlook the real images within documents and do not take full advantage of the hierarchical structure of the text. As a result, there is still room for improvement in these methods. For this purpose, we propose a novel hierarchical multi-modal transformer for CLDC, in which the hierarchical structure of texts and the embedding images of long documents are utilized, instead of the page images."}, {"title": "C. Multi-modal Transformer", "content": "The multi-modal transformer has proven to be effective in various multi-modal tasks, including image/video-text retrieval [16], [28], visual question answering [18], multi-modal emotional analysis [17], and visual grounding [29], resulting in impressive performance outcomes.\nCurrently, there are two main approaches for incorporating image data into a multi-modal transformer for text-image tasks. The first approach involves dividing the image into several patches, which are then treated as separate image inputs to the model. The second approach uses object detection techniques, such as Faster-RCNN [30], to extract region features from the image, and considers them as the input to the model. In some scenarios, to improve the cross-modal representation capability, additional data may also be incorporated. For instance, scene text embeddings extracted from the image [31] and class labels [32] can be utilized. In the context of text-video data, most existing approaches treat each image frame as a single feature, which is then used to interact with the text features. Our image-processing technique is similar to those used in text-video-related tasks, but with some notable differences. Firstly, our images are unordered. Additionally, the interaction objects for the images are not fine-grained word features, but high-level semantic features such as section and sentence features.\nWhile the multi-modal transformer has proven to be effective in a range of multi-modal tasks, it has not yet been applied to the CLDC task. To address this, we introduce the HMT model, which incorporates a multi-modal transformer and a dynamic multi-scale multi-modal transformer to capture the intricate relationships between image features and hierarchical text features such as section and sentence features. By leveraging these advanced modeling techniques, we aim to improve the performance of CLDC and push the boundaries of document understanding."}, {"title": "III. PROPOSED METHOD", "content": "In this section, we present the proposed Hierarchical Multi-modal Transformer (HMT) in detail. As shown in Fig. 3, the pre-trained text and image models are first employed to obtain initial hierarchical text features and image features. Next, the complex multi-modal relationships between these features are modeled using a hierarchical multi-modal transformer at both the section and sentence levels. Additionally, we introduce a dynamic mask transfer module to facilitate information exchange between the two multi-modal transformers. Finally, a fusion module is applied to aggregate the final multi-modal representations at both levels, enabling the classification of long documents."}, {"title": "A. Feature Extraction", "content": "To model the complex association of image features and text features at the section and sentence levels, we segment each long document t into l sequential sections in a fixed size without overlapping, denoted as {q1, q2, ..., q\u0131}. Each section contains r word tokens, i.e., qi = {Wio, Wi1, ..., Wir}, where wi\u2081 = [CLS] is the special start token for BERT-related encoders. Extra [PAD] tokens are appended to the end to meet the section length of r. Then, we apply the pre-trained BERT [20] $f_t(\\cdot, \\phi)$, which has been widely utilized in LDC tasks [5], [19], to extract the ith section feature with word features as follows,\n\\[\\begin{equation}\\label{eq:1} [p_i, X_{i1}, ..., X_{ir}] = f_t(q_i, \\phi), i = 1, ..., l. \\end{equation}\\]\nwhere \\(\\phi\\) represents the parameters of the pre-trained BERT. The final hidden state of the [CLS] token, \\(p_i\\), is taken as the section feature, and the other tokens \\(X_{ij}, j = 1,...,r\\) are taken as the word features. The final section features and word features of the long document can be represented as matrix features \\(P = [p_1,p_2, ...,p_l] \\in \\mathbb{R}^{l \\times d}\\) and \\(X = [X_{11}, ..., X_{1r}, ..., X_{l1},...,X_{lr}] \\in \\mathbb{R}^{lr \\times d}\\), where d is the dimension of the features.\nTo make the most of the fine-grained word features and avoid additional data input, the sentence features are directly obtained by aggregating the word features in each sentence by the proposed Sentence Token Generation (STG) block as shown in Fig. 4. This process is directed by the sentence mask \\(S_{\\text{mask}} \\in \\mathbb{R}^{lr}\\) generated from the document. It is represented by an incrementally repeatable number sequence, in which the mask values of words in the same sentence keep fixed. The final sentence features are obtained by applying the max-pooling operation to the word features in each sentence, followed by a linear projection layer as follows,\n\\[\\begin{equation}\\label{eq:2} s_i = W_s{\\text{si}} + b_s, i = 1, ..., n. \\end{equation}\\]\nwhere maxpooling_c and n represent the column-wise max-pooling and the number of sentences, respectively. \\(1_{lr} \\in \\mathbb{R}^{lr}\\) is a vector in the size of lr with all elements being 1. \\(W_s, b_s\\) are the trainable parameters of the linear projection layer. The final sentence features of the long document can be denoted as \\(S = [s_1, s_2, ..., s_n] \\in \\mathbb{R}^{n \\times d}\\).\nDifferent from the traditional multi-modal tasks, which focus on the local region features of images, we pay more attention to the global semantic representations for images vj,j = 1,2,...,m, where m is the number of images, in long documents, considering their better correspondences with the section features and sentence features. In light of the success of vision transformer (ViT) [33], which takes non-overlapping image patches as the input, we directly utilize the pre-trained CLIP model [34] with a ViT architecture as our image encoder \\(f_v(\\cdot, \\psi)\\). The CLIP model performs self-attention between patches and compiles the set of patch embeddings into a [CLS] embedding. We then leverage a fully connected layer to project all the image embeddings to the same dimension as the text embeddings, as follows,\n\\[\\begin{equation}\\label{eq:3} h_j = W_h(f_v(v_j, \\psi)) + b_h, j = 1, ..., m. \\end{equation}\\]\nwhere \\(\\psi\\) represents the parameters of the pre-trained image encoder. The final image features of the long document can be formulated as \\(H = [h_1,h_2,..., h_m] \\in \\mathbb{R}^{m \\times d}\\). d is the dimension of the features."}, {"title": "B. Hierarchical Multi-modal Transformer", "content": "As illustrated in Fig. 2, the text and image features of a long document have varying degrees of interdependence at different hierarchical levels, such as one-to-multiple and multiple-to-one relationships. To capture these complex and diverse relationships, we propose two multi-modal transformers, namely the Multi-modal Transformer and the Dynamic Multi-scale Multi-modal Transformer. Furthermore, we introduce a dynamic mask transfer module to enable information interaction between these two levels of multi-modal transformers."}, {"title": "1) Multi-modal Transformer", "content": "To model the one-to-multiple relationships between sections and images, we directly concatenate the [CLS] token, section features P and image features H into a unified multi-modal sequence Fpv = [[CLS], p1, p2, \u2026, pi, h1, h2, ..., hm]. Then, following the same architecture of the Transformer [8], we build our multi-modal transformer with a stack of multi-head self-attention layers followed by a feed-forward network, which can be denoted as follows,\n\\[\\begin{equation}\\label{eq:4} \\hat{E}_{pv} \\leftarrow E_{pv} + MLP(LN(E_{pv})) \\end{equation}\\]\nwhere MHSA(\u00b7) denotes the multi-head self-attention layer, MLP() denotes the multi-layer perception layer, and LN(\u00b7) denotes the layer normalization. The final output of the [CLS] token of Fpv is adopted as the fused multi-modal representation in the section level, denoted as Ypv."}, {"title": "2) Dynamic Multi-scale Multi-modal Transformer", "content": "Fig. 5 illustrates the proposed Dynamic Multi-scale Multi-modal Transformer (DMMT), which consists of two branches: a single text transformer and a multi-scale multi-modal transformer. The former is designed to mitigate the negative impact of uncorrelated images on the final output, while the latter is responsible for capturing the multi-scale corresponding relationships between the sentence and image features.\nTo reduce the heterogeneous gap and enhance the representation power of multi-modal features, we first adopt a simple fusion strategy to generate multi-modal sentence representations. Specifically, given the max-pooling feature I of the image features and the sentence features [s1, s2, ..., sn], the procedure is formulated as follows,\n\\[\\begin{equation}\\label{eq:5} \\hat{F}_{sv} = MHSA(F_{sv}, D) \\end{equation}\\]\nwhere maxpooling_c represents the column-wise max-pooling. [;] denotes concatenation along the feature dimension. \\(W_c \\in \\mathbb{R}^{2d \\times d}\\) and \\(b_c \\in \\mathbb{R}^{d}\\) are the trainable parameters. The fused multi-modal sentence features can be represented as \\(S = [\\hat{s}_1, \\hat{s}_2, ..., \\hat{s}_n]\\).\nFig. 2 demonstrates that an image is usually depicted by several different sentences. To model the multi-scale correspondence relationships between sentences and images, we choose to limit the scope of attention of sentence elements by placing multi-scale window masks on the multi-head adjacency matrix. Concretely, given the multi-modal input sequence Fsv = [[CLS],\u015c, H], and the window mask matrix D\u2208 R(n+m+1)\u00d7(n+m+1) under one of the scales, the above process can be formulated as,\n\\[\\begin{equation}\\label{eq:6} \\hat{A}_i = \\text{Softmax}((\\hat{F}_{sv}W^Q)(\\hat{F}_{sv}W^K)^T D / \\sqrt{d}) \\end{equation}\\]\nwhere i = 1,2,..., h and the values of D are binary and set to 1 if they are within the attention span of the target element. MHSA(\u00b7) denotes the multi-head self-attention block.  denotes the layer normalization. \\(\\, \\) and \\(W_o\\in \\mathbb{R}^{d \\times d}\\) are the trainable parameters of MHSA. Especially, \\(d_k = d_v = d/h\\) and h is the total number of heads in the MHSA block. Fig. 5 presents an architecture diagram of DMMT with nwin = 4 scale window mask matrices. It is noted that a fully-connected attention mask is included to capture the global information interaction between sentences and images. The text branch follows the same learning rules of Transformer [8], and the multi-modal branch owes the same learning procedure, but with a different MHSA mechanism as (6), which are denoted by T-Transformer(\u00b7), M-Transformer(\u00b7), respectively and formulated in (7) as follows,\n\\[\\begin{equation}\\label{eq:7} Y_i = \\text{M-Transformer}(\\hat{F}_{sv}, D_i), i = 1, 2, ..., N_{\\text{win}}. \\end{equation}\\]"}, {"title": "3) Dynamic Mask Transfer", "content": "In order to fully leverage the hierarchical structure of long documents, we apply the principle of transferability to facilitate the migration of information from the section-image association to the sentence-image association, thereby enhancing the cross-modal interaction capability between sentence and image features.\nAs shown in Fig. 6, we first extract the multi-head section-image adjacency matrix Dpv \u2208 and the multi-head image-section adjacency matrix Dvp \u2208 from the multi-head adjacency matrix Apv \u2208 of Multi-modal Transformer. Since the operations keep the same, here we mainly take the first head of Dpv as an example to give the explanation.\nFor each section, we select the top-K images with the highest similarity scores as the critical image index set I+. The K is a dynamic number for each section, which is the smallest number that meets the condition in (10),\n\\[\\begin{equation}\\label{eq:10} \\sum_{j \\in I^+} \\exp(D_{pv}(i, j)) / \\sum_{j=1}^m \\exp(D_{pv}(i, j)) > \\eta , i = 1, 2, ..., l. \\end{equation}\\]\nwhere \u03b7 is a constant, and we set \u03b7 = 0.65 in all experiments.\nAccording to the critical image index set I+, i = 1, 2, ..., l, we can obtain the binary mask matrix as follows,\n\\[\\begin{equation}\\label{eq:11} D_{pv} = \\begin{cases} 1, j \\in I^+ \\\\ 0, \\text{else} \\end{cases} \\end{equation}\\]\nwhere i = 1, 2, ..., 1; j = 1,2,..., m. Then, we construct the transfer mask to model the relations between sentences and sections, denoted by \\(T_{s-p} \\in \\mathbb{R}^{n \\times l}\\), in which if a sentence belongs to a section, the mask value will be set to 1, otherwise to 0, as follows,\n\\[\\begin{equation}\\label{eq:12} [T_{s-p}]_{ij} = \\begin{cases} 1, \\text{if the sentence node i belongs to the section j} \\\\ 0, \\text{else} \\end{cases} \\end{equation}\\]\nIf we directly utilize \\(T_{s-p} \\times D_{pv}\\), where \\(\\times\\) denotes matrix multiplication, to finish the mask transfer, it will inevitably bring a lot of noise. This is mainly because the section features occasionally have completely different semantics from the included sentence features, meaning that the images associated with section features are not definitely related to the included sentence features. To make the obtained sentence-image mask matrix more discriminative, we further sparse the transfer mask T_s-p, only retaining the sentence node masks that are relevant to the corresponding section features. This process is accomplished by placing a threshold on the cosine similarity matrix between the sentence features and corresponding section features, as follows,\n\\[\\begin{equation}\\label{eq:13} \\text{Ms-p} = \\text{cos-similarity}(S, T_{s-p}P) > 0 \\end{equation}\\]\nwhere Ms_p \u2208 \\(i \\in \\mathbb{Z}^+\\). Given the sparse section-image mask matrix and the sparse transfer mask T_s-p E , the final sentence-image mask matrix can be formulated as,\n\\[\\begin{equation}\\label{eq:14} D_{sv} = T_{s-p} \\times D_{pv} \\end{equation}\\]\nThe same calculation procedures are applied to each head of the multi-head section-image attention matrix Dpv and the multi-head image-section attention matrix Dvp. The final outputs can be expressed as  \u2208 i = 1, 2, ..., h, and \u2208 i = 1, 2, ..., h, respectively. Then, as shown in Fig. 6, we reorganize them as a whole multi-head mask matrix Dmask \u2208 = 1,2,..., h and integrate it into (6) to enhance the interaction ability of sentence and image features, as follows,\n\\[\\begin{equation}\\label{eq:15} W_m) \\end{equation}\\]\nwhere i = 1,2,..., h and the dynamic weight Wm \u2208 is added to maximize the contribution of the transfer mask Dmask and minimize the influence of noise"}, {"title": "C. Model Training", "content": "To obtain the final representation of the cross-modal long document, a column-wise max-pooling operation, denoted by maxpooling_c(.), is applied on the multi-modal representations at the section and sentence levels as follows,\n\\[\\begin{equation}\\label{eq:17} u = \\text{maxpooling_c}([Y_{pv}, \\hat{Y}_{sv}]) \\end{equation}\\]\nwhere [,] represents the concatenation operation along the column dimension. Then, we apply the softmax function to output the probability of each label and adopt the cross-entropy loss function as the classification loss as follows,\n\\[\\begin{equation}\\label{eq:18} L_{cls} = - \\sum_{i \\in YD} \\sum_{j=1}^F Y_{ij}ln{\\hat{y}_{c,ij}} \\end{equation}\\]\nwhere YD is the set of document indices referring to their labels, and F is the dimension of the output feature, which is equal to the number of classes. Y is the label indicator matrix."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we evaluate the proposed HMT model on two newly created and two publicly available multi-modal long document datasets and compare it with the state-of-the-art LDC methods, including both single text and multi-modal methods."}, {"title": "A. Datasets", "content": "In the field of visually-rich document classification, several datasets have been developed, such as RVL-CDP [35] and Tobacco-3482 [36]. However, these datasets only contain images of document pages without the internal embedding images. On the other hand, the commonly used cross-modal document classification datasets typically consist of short text sequences, which are not suitable for evaluating the CLDC task's capabilities. To address this issue, we have constructed two new multi-modal long document datasets, namely MMaterials, and MAAPD, in addition to two public datasets that are relatively suitable for our task. These datasets contain structured long text sequences and multiple embedding images obtained by the Grobid\u00b9 and Fitz library, respectively. The specific statistics of the four datasets are shown in Table I.\n\u2022 Materials: We select 7 materials fields, i.e., composite material, battery separator, energy storage material, graphene, nanomaterial, silicon carbide, and titanium alloy, and download 9320 articles from the Internet. There are 6812 training samples, 1247 validation samples, and 1261 testing samples. Each sample has 4459 words and 9 images on average.\n\u2022 MAAPD: We expand the dataset AAPD [37] to a multimodal version, i.e., MAAPD, and use it in the CLDC task. As shown in Table I, 24573 samples are obtained by automatically parsing the XML files and extracting the image information. The dataset contains 20753 training samples, 2000 validation samples, and 2000 testing samples, which are assigned to one of 10 subject categories such as CS.CV, CS.cr, cs.ro. Each document includes an average of 13.5 images.\n\u2022 Review: The original online review dataset [38] has more than 44 thousand reviews, including 244 thousand images. However, the average text length of the samples is only 237.3 tokens. To better verify the effectiveness of our proposed model in processing long documents, we select the samples with text lengths larger than 256 and reorganize them as a new dataset, which contains 10352 training, 1294 validation, and 1294 testing samples with an average of 419.9 words, 27.8 sentences, and 4.1 images.\n\u2022 Food101: The UPMC Food101 dataset [39] contains web pages with textual recipe descriptions for 101 food. Each page is matched with a single image, which is obtained by querying Google Image Search for the given category. Typical examples of food labels include Filet Mignon, Pad Thai, Breakfast Burrito, and Spaghetti Bolognese."}, {"title": "B. Baselines and Metrics", "content": "We first compare the proposed HMT with several document classification methods, including Logistic"}, {"title": "E. Ablation Study", "content": "An ablation study is conducted on the four datasets to examine the effects of each component of HMT. The results are presented in Table IV and Fig. 7. Specifically, we begin with the base model that utilizes only the section features. Then, we incorporate image features and use the Multi-modal Transformer (MMT) to model inter-modal correlations. Based on the results shown in Table IV, we can observe that integrating image information using the multi-modal transformer has led to significant performance improvements across all metrics on the four datasets. This indicates the benefits of incorporating image features into the model. Furthermore, by introducing the Dynamic Multi-scale Multi-modal Transformer (DMMT) to model the complex correlations between sentences and images, we achieve even better results compared to the former model. This highlights the importance of capturing multi-modal interactions at the sentence level to extract more discriminative features. Finally, by further incorporating the Dynamic Mask Transfer (DMT) module into the model to form the full architecture, we achieve the best results compared with other ablation models, which demonstrates the effectiveness of the hierarchical structure information, and simultaneously confirms the validity of our model in combining multiple images with the hierarchical text features of long documents.\nIn order to further demonstrate the impact of different parts of the DMMT module, as illustrated in Fig. 7, we present three variations of the DMMT module: -DMMT(M) represents the removal of multi-scale window masks; -DMMT(T) represents the removal of the text branch of DMMT; and -DMMT(D) represents the removal of the dynamic weight generation and assigns equal weights to all branches. The results show a significant decrease in performance when the multi-scale window masks are removed, indicating that modeling the multi-scale correspondence of sentence and image features is crucial for obtaining a more discriminative representation of long documents. Comparing the performance of DMMT with that of -DMMT(T) shows that the text branch in DMMT can effectively mitigate the influence of irrelevant images on the final representation of long documents. Moreover, the lack of the dynamic fusion mechanism also leads to performance degradation, demonstrating the significant role played by the dynamic fusion mechanism in achieving substantial improvements."}, {"title": "F. Hyperparameter Study", "content": "Section length r", "r": "In our model", "N": "The influence of the layer numbers is illustrated in Fig. 8(b). We can see the decrease in our model performance when the number of layers increases from 1 to 4. We deem that this is primarily because both the initial text and image features are derived from the large-scale pre-trained models and classified as high-level semantic features. With the powerful interaction capabilities of the Transformer, it is possible to"}]}]}