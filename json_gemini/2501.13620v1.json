{"title": "Cognitive Paradigms for Evaluating VLMs on Visual Reasoning Task", "authors": ["Mohit Vaishnav", "Tanel Tammet"], "abstract": "Evaluating the reasoning capabilities of Vision-Language Models (VLMs) in complex visual tasks provides valuable insights into their potential and limitations. In this work, we assess the performance of VLMs on the challenging Bongard Openworld Problems benchmark, which involves reasoning over natural images. We propose and evaluate three human-inspired paradigms: holistic analysis (global context processing), deductive rule learning (explicit rule derivation and application), and componential analysis (structured decomposition of images into components). Our results demonstrate that state-of-the-art models, including GPT-40 and Gemini, not only surpass human benchmarks but also excel in structured reasoning tasks, with componential analysis proving especially effective. However, ablation studies reveal key challenges, such as handling synthetic images, making fine-grained distinctions, and interpreting nuanced contextual information. These insights underscore the need for further advancements in model robustness and generalization, while highlighting the transformative potential of structured reasoning approaches in enhancing VLM capabilities.", "sections": [{"title": "Introduction", "content": "Human intelligence seamlessly integrates visual perception with language and higher-level cognition, enabling reasoning about visual information (Kunda, 2020). This ability, known as visual reasoning, supports tasks such as understanding spatial relationships, inferring object functions, and grounding language in the physical world. Despite advancements in natural language processing (NLP) (Zhao et al., 2023), achieving robust visual reasoning in machines remains a significant challenge, particularly in multimodal contexts (Cao et al., 2024).\nLarge Language Models (LLMs) have transformed NLP by excelling in tasks like machine translation, text generation, and question answering (Brown et al., 2020; Devlin, 2018). Their emergent abilities in areas such as code generation (Chen et al., 2021) and logical reasoning (Wei et al., 2022) suggest potential for bridging language and vision to tackle complex visual reasoning tasks.\nVisual reasoning tasks range from basic object recognition (Lin et al., 2014) to abstract inference (Zerroug et al., 2022; Teney et al., 2020; Barrett et al., 2018; Zhang et al., 2019). Complex tasks demand integrating perceptual data with higher-order cognition, such as spatial reasoning and relational understanding (Cao et al., 2024; Vaishnav et al., 2022). Bongard Problems (BPs) (Bongard, 1968), involving abstract rules distinguishing two image sets, exemplify such challenges by requiring both perceptual and conceptual reasoning.\nStructured reasoning tasks like BPs require the reasoning process to follow specific rules or structures. These tasks involve identifying patterns or relationships among image components and applying abstract reasoning based on those patterns. Vision-Language Models (VLMs), such as Phi-1.5 (Javaheripi et al., 2023), Llama-vision (Meta, 2024), Gemini (Google, 2024), and GPT-40 (OpenAI, 2024), provide a robust foundation for tackling BPs. By leveraging multimodal datasets, these models combine visual perception with higher-order cognitive processes, making them well-suited for structured reasoning tasks like BPs. This capability allows them to approach complex visual reasoning by breaking down information into identifiable, logical steps.\nThis paper investigates the ability of Vision-Language Models (VLMs) to solve natural image-based Bongard Problems using the Bongard Openworld Dataset (Wu et al., 2024) (example shown in Figure 1). Inspired by human problem-solving strategies, we evaluate three paradigms: holistic analysis, deductive rule learning, and componential analysis. Our findings demonstrate that VLMs"}, {"title": "Related Work", "content": "To evaluate the capabilities of VLMs, various benchmarks have been developed, focusing on different aspects of visual understanding and reasoning. These benchmarks play a crucial role in assessing model performance and identifying areas for improvement. Traditional benchmarks focussed on single image tasks like image captioning (Lin et al., 2014), OCR (Mishra et al., 2019; Singh et al., 2021), and VQA (Antol et al., 2015; Zhang et al., 2024; Lu et al., 2022; Gurari et al., 2018). For multi-image comprehension, researchers have developed interleaved image-text corpora, including MMC4 (Zhu et al., 2024) and OBELICS (Lauren\u00e7on et al., 2024; Lin et al., 2024), along with dedicated benchmarks like Q-Bench (Wu et al., 2023), Memontos (Wang et al., 2024), and DEMON (Li et al., 2023). While these benchmarks have contributed to progress in evaluating multi-image understanding, they often lack comprehensive coverage of complex reasoning scenarios. Other benchmarks like MMIU (Multimodal Multi-Image Understanding) (Meng et al., 2024) have been pivotal in evaluating VLMs comprehensively, incorporating diverse datasets covering areas like science and mathematics.\nDespite advancements in VLM benchmarks that involve language, purely visual reasoning tasks without linguistic input remain crucial for evaluating core visual understanding. Bongard problems (Bongard, 1968) present a unique challenge in this regard. Their significance lies in their focus on abstract visual concepts, the absence of explicit language, and the inherent multi-image relational reasoning required for their solution. Extensions such as Bongard-HOI (Jiang et al., 2022) and Bongard-OpenWorld (Wu et al., 2024), further extend these challenges to human-object interactions and real-world scenarios, highlighting models' few-shot reasoning capabilities.\nBeyond Bongard problems, other purely visual reasoning challenges have been proposed,"}, {"title": "Models", "content": "To evaluate our approach comprehensively, we employed a diverse range of Vision-Language Models (VLMs) that include both closed-source and open-source architectures. This diversity allowed us to assess the strengths and limitations of different model types and scales.\nThis selection encompasses state-of-the-art multimodal models, including GPT-40 (OpenAI, 2024), Gemini-2.0 (Google, 2024), Pixtral-12B (Agrawal et al., 2024), Llama-Vision-3.2 (11B, and 90B) (Meta, 2024), LLaVA (7B, 13B and 34B) (Liu et al., 2023), and LLaVA built on Llama 3 (8B) (XTuner, 2025). By leveraging this variety, we systematically analyzed model performance across different architectural designs and parameter scales, offering a holistic view of their capabilities. For the evaluation purpose, two metrics are used: classification accuracy and semantic similarity (more in Appendix C).\nFurther technical details, including model architectures and individual summaries, are elaborated in Appendix D."}, {"title": "Dataset", "content": "We evaluate our approach on the Bongard Open-World dataset (Wu et al., 2024), a benchmark designed to assess few-shot visual reasoning abilities. The full dataset contains 1001 cases, from which we have chosen a sample of 500 cases for our experiments. Each case in the dataset consists of six positive and six negative examples and a query image. These cases are governed by an underlying \"commonsense\" rule, with the rules themselves categorized into distinct visual concepts (further details can be found in Appendix B). Figure 1 illustrates the task: given a set of positive and negative examples, the goal is to determine the underlying rule and classify a query image accordingly."}, {"title": "Experiments", "content": "This paper investigates VLMs' ability to solve natural image-based Bongard Problems. We evaluate three paradigms: holistic analysis (processing all information together), deductive rule learning (deriving rules first), and componential analysis (examining components separately)."}, {"title": "Holistic Analysis", "content": "In this section, we describe our experiments using a holistic analysis approach, where the model is presented with all images (positive, negative, and test) simultaneously.\nThe holistic analysis experiments assess the ability of VLMs to directly infer patterns and rules from a set of images. We designed a prompt that presents the model with m positive examples, n negative examples, and a final query image.\nThe prompt instructs the model to:"}, {"title": "Deductive Rule Learning", "content": "This section describes our experiments using a deductive rule learning approach, which involves a two-step process: first, the model infers a general rule; second, it applies this rule to classify a query image.\nThe deductive rule learning experiments assess the ability of VLMs to explicitly formulate a rule before classifying a query image. This approach mirrors human deductive reasoning. We employed a two-stage prompting strategy. In the first stage, we presented the model with m positive examples and n negative examples and asked it to identify a rule that distinguishes the two categories. The prompt requested a concise summary of the identified rule (less than 20 words). The precise wording of this first-stage prompt is provided in Appendix E.2.\nIn the second stage, we presented the model with the previously generated rule summary and a query image. The prompt instructed the model to analyze the query image in the context of the given rule and classify it as either positive or negative. The prompt also requested a brief analysis of the query image and a restatement of the rule. The precise wording of this second-stage prompt is provided in Appendix E.2."}, {"title": "Componential Analysis", "content": "This section describes our experiments using a componential analysis approach (detailed in Appendix E.3). This method involves decomposing each image into its constituent visual elements and then reasoning based on these components (textual elements). This process is divided into three main stages: (1) detailed image description generation, (2) providing instruction on rule derivation, and (3) rule extraction and application based on these descriptions.\nThe componential analysis experiments aim to evaluate the ability of VLMs to reason based on structured descriptions of visual content. This approach simulates how humans might break down complex visual scenes into simpler parts for analysis. We employed a three-stage prompting strategy. In the first stage, we generate detailed descriptions of each image. This prompt instructs the model to analyze an image and provide a structured description, covering aspects such as scene, objects (living and inanimate), activities, contextual elements, visual patterns, emotional undertones, textual information, and a summary. In the second stage, we provide the models with instructions on how to analyze the outputs and derive the rule. In the final stage, which is purely textual, we provided the model with the descriptions generated in the first stage, along with information about which images belong to positive examples and negative examples, and the query image. The prompt then asks the model to derive a rule based on the descriptions of positive and apply this rule to classify the query image."}, {"title": "Rule-Based Evaluation", "content": "This section describes experiments designed to disentangle the image analysis (perception) and rule application (abstract reasoning) capabilities of VLMs. Unlike the previous experiments, where the models were tasked with both extracting and applying rules, here we provide the models with pre-extracted rules and assess their ability to apply these rules to classify query images.\nThese experiments isolate the rule application component of the Bongard problem-solving process. This experimental setup is conceptually similar to the Deductive Rule Learning experiments (Section 5.2), which also involved a two-step process: (1) inferring a general rule and (2) applying that rule to classify a query image. However, in this experiment, we bypass the first step (rule inference) and instead use the rule summaries generated by GPT-40 in the Deductive Rule Learning experiment. For each Bongard problem, we took the rule summary generated by GPT-40. We then presented the model with the query image and the corresponding rule summary, explicitly instructing it to classify the query image based on the provided rule. This approach allows us to specifically assess how well the models can understand and apply a given rule, independent of their ability to extract the rule themselves."}, {"title": "Results", "content": "Holistic analysis necessitates models with multi-image and multimodal capabilities. Among the models tested, only Gemini 2.0 (flash) (referred to Gemini here) and GPT-40 possess these features.\nTable 1 presents the overall accuracy, as well as the accuracy on positive and negative samples separately. As shown, Gemini achieves a slightly higher overall accuracy of 82.2% compared to GPT-40's 80.0%. Unlike GPT-40, Gemini shows a more balanced performance across both positive and negative samples, suggesting a more robust approach to handling cases where no clear rule might exist for the negative examples.\nThese results suggest that while GPT-4o may excel at recognizing patterns within the positive examples, its lower accuracy on negative samples indicates challenges in distinguishing subtle counter examples. On the other hand, Gemini's balanced performance reflects greater robustness in addressing the diverse reasoning demands of holistic analysis tasks."}, {"title": "Deductive Rule Learning", "content": "This two-stage approach allows us to evaluate the model's ability to not only identify patterns but also articulate them as explicit rules, providing insights into its deductive reasoning capabilities. We conducted these experiments using Gemini and GPT-40. Table 1 summarizes the overall accuracy, as well as the accuracy on positive and negative samples separately.\nThis time GPT-40 achieves a slightly higher overall accuracy compared to Gemini. Both models demonstrate balanced performance across positive and negative samples. GPT-40, in particular, excels in classifying positive samples, while Gemini achieves a more consistent performance across both categories.\nThese results indicate that the two-stage deductive approach enhances the models' ability to handle the complexities of both positive and negative examples, surpassing the performance seen in the holistic approach. GPT-40's superior accuracy on positive samples suggests its effectiveness in identifying and applying explicit patterns, while Gemini's balanced results highlight its robustness across diverse reasoning scenarios."}, {"title": "Componential Analysis", "content": "The componential analysis approach provides a framework for evaluating a broader spectrum of vision-language models (VLMs) by decomposing the complex multi-image reasoning task into manageable single-image description tasks. This design facilitates the inclusion of models that lack explicit multi-image processing capabilities but can effectively analyze individual images and process structured textual representations.\nWe conducted the three-stage componential analysis experiment using six VLMs: Pixtral-12B, Llava-Llama3-8B, Llama-Vision-11B, Llava 7B, GPT-40, and Gemini. The performance metrics, presented in Table 1, include overall accuracy and accuracy on positive and negative samples.\nThe results demonstrate a wide range of performance across the evaluated models. GPT-40 and Gemini achieve near-optimal accuracies, with overall accuracies of 92.8% and 93.6%, respectively. Notably, these results surpass human-level performance, which is reported at 91% on this task. This finding highlights the potential of advanced VLMs to excel in tasks traditionally dominated by human reasoning capabilities.\nOpen-source models, including Pixtral-12B, Llava-Llama3-8B, and Llama-Vision-11B, Llava 7B exhibit significant variability. Llava-Llama3-8B, for example, achieves a high accuracy on negative samples but an exceptionally low accuracy on positive samples, indicating a strong bias toward negative classifications. Similarly, Llama-Vision-11B demonstrates an inverse pattern. These discrepancies suggest that these models struggle to generalize effectively across different classes.\nThe superior performance of GPT-40 and Gemini indicates that structured, detailed descriptions significantly enhance reasoning capabilities in VLMs. Their ability to surpass human-level performance underscores the importance of integrating multimodal capabilities with robust processing pipelines. Furthermore, performance open-source models highlights the potential for continued advancements in accessible VLMs to achieve substantial progress in complex reasoning tasks."}, {"title": "Rule-Based Evaluation", "content": "The Rule-Based Evaluation experiment isolates the models' ability to apply explicitly provided rules to classify query images, bypassing the rule inference step. This approach offers a focused assessment of the rule application capabilities of the models. The results, presented in Table 2, reveal distinct trends across the evaluated models.\nPixtral-12B achieved the highest overall accuracy (83.8%), indicating a strong ability to comprehend and operationalize the provided rules effectively. Its relatively balanced performance on both negative and positive samples highlights its robust application of rules across varying contexts.\nThe results in Table 2 highlight that while certain models excel in confirming positive matches, their difficulties in handling negative cases expose potential biases in rule application mechanisms. Pixtral-12B's relatively balanced performance suggests it is more adept at applying rules across diverse scenarios, while models like Llama-Vision-xx and LLaVA-xx may overfit to confirmatory evidence.\nBy isolating the rule application component, this experiment provides valuable insights into the models' strengths and limitations. When combined with findings from the Deductive Rule Learning experiments (Section 5.2), this analysis helps to disentangle the factors contributing to model performance, distinguishing between deficits in rule extraction and rule application. Such insights are crucial for guiding improvements in visual-language model reasoning capabilities."}, {"title": "Ablation", "content": "To further analyze the reasoning process of the best-performing models, GPT-40 and Gemini, we conducted a semantic similarity analysis. This analysis aims to quantify the alignment between the descriptions of the query images and the rules identified for each category. We hypothesized that for correctly classified positive examples, the similarity between the description and rule should be high, while for negative examples, it should be lower.\nUsing cosine similarity of sentence embeddings generated using OpenAI embeddings, we computed the mean and standard deviation of the semantic similarity scores for both models and categories. Both models exhibit higher similarity (refer Table A.6) for positive examples compared to negative ones, confirming that correct classifications align with the identified rule. However, the small difference in similarity (~ 6%) suggests that the negative samples were designed to be closer to positive samples to make them challenging (also shown in Example 1)."}, {"title": "Analysis per Commonsense Category", "content": "To better understand the comparative capabilities of GPT-40 and Gemini, we analyzed their performance across ten commonsense reasoning categories in the Bongard Openworld dataset (details in Appendix B.3). This analysis (in Table A.5) highlights the models' strengths and limitations in handling diverse commonsense reasoning categories. Both models demonstrated robust performance across most categories indicating models' capability to process high-level conceptual patterns effectively.\nDifferences in performance were observed in categories requiring more nuanced or contextual understanding, such as \"Taste / Nutrition / Food\" and \"Human-Object Interaction.\" This suggests that while both models are adept at general reasoning tasks, their architecture and training data influence their performance on specific types of visual information. Notably, Gemini showed strengths in scenarios involving fine-grained object attributes, while GPT-40 performed better in tasks demanding contextual interpretation."}, {"title": "Analysis of Incorrectly Classified Samples", "content": "In this ablation study, we analyzed the misclassified samples from GPT-4o and Gemini during the Componential Analysis experiments to identify common limitations. The total 15 common test samples were incorrectly identified by both the VLMs. The errors revealed several recurring issues (more details in Table A.7), including challenges with synthetic images that lack natural textures, difficulties in making fine-grained distinctions, and struggles with recognizing specific image types, such as satellite views. Additionally, the models showed limitations in understanding nuanced contextual information, as evidenced by misclassifications related to artistic styles and the broader scene context. These findings highlight that while the models excel in many areas, they still face challenges in handling complex visual details and contextual reasoning, suggesting areas for improvement in VLMs' robustness and generalization."}, {"title": "Discussion", "content": "Our study investigates the application of Vision-Language Models (VLMs) to Bongard Open-World Problems (BPs), providing insights into the computational mechanisms underlying visual reasoning and their alignment with human cognitive strategies. We introduced three evaluation paradigms\u2014Holistic Analysis, Deductive Rule Learning, and Componential Analysis-designed to mirror distinct human approaches to learning and problem-solving. These paradigms allow us to evaluate VLM performance and understand how these models relate to human cognition.\nA critical aspect of human scene understanding is the dynamic interaction between the observer and their environment (Zelinsky, 2013; Malcolm et al., 2016; Gibson, 2015). This interactionist view posits that understanding a scene is an active process shaped by behavioral goals, rather than a passive reception of sensory input. This active engagement with the visual world is directly relevant to how both humans and VLMs approach BPs. Rather than processing individual images in isolation, both humans and VLMs benefit from considering the relationships between images in a given set.\nOur Holistic Analysis paradigm specifically targets this relational aspect by requiring VLMs to process all images and descriptions simultaneously. This approach allows the model to capture the overall context and directly identify the distinguishing rule, akin to human holistic problem-solving, where the \"gist\" of a scene is rapidly perceived, guiding subsequent attention and actions (Biederman, 1987; Li et al., 2002). Such holistic processing provides a crucial initial understanding, which can then be refined through more detailed analysis. This process heavily relies on working memory to maintain the \"gist\" while further analysis unfolds (Baddeley, 2012).\nFollowing this initial holistic understanding, human visual information processing is further refined through attentional mechanisms that guide gaze and cognitive processing (Posner, 1980). While eye movements can reflect attentional shifts, our Deductive Rule Learning paradigm directs the VLMs' attention on learning the underlying rule prior to evaluating query images. This mirrors human deductive reasoning, where a hypothesis is formed and then tested against new evidence. By focusing on rule inference before rule application, the VLM can store learned rules in long-term memory (Squire, 1992), allowing for efficient application to unseen instances.\nIn addition to holistic and deductive strategies, humans often use analytical approaches to break down complex problems. The Componential Analysis paradigm reflects this tendency by decomposing a complex problem into smaller, more manageable components. In this paradigm, VLMs analyze positive and negative examples separately, extracting individual rules for each set before comparing them to identify the distinguishing factor. This process aligns with cognitive theories that distinguish between System 1 (fast, intuitive perception) and System 2 (slow, deliberate reasoning) (Kahneman, 2011). Object recognition and categorization, a System 1 process involving perceptual mechanisms (Marr, 2010), precedes the more deliberate System 2 reasoning required to compare and contrast extracted rules. This analytical decomposition facilitates a more precise understanding of the underlying rules.\nOur findings, which show that VLMs can surpass human performance on BPs when applying these human-inspired paradigms, underscore the importance of attention and memory mechanisms in visual reasoning. Attention mechanisms, critical for selecting relevant information, are implemented in VLMs through architectures such as those described in (Bahdanau et al., 2016) and (Vaswani et al., 2017). These mechanisms allow VLMs to selectively focus on relevant image features, similar to human gaze patterns and attentional shifts. Meanwhile, memory mechanisms are essential for encoding visual information, maintaining it during rule formulation, and retrieving it for comparison. This is evident in the VLMs' ability to store learned rules (long-term memory) and maintain representations of images during analysis (working memory). The interplay between attention and memory is crucial for both human and artificial intelligence, providing deeper insights into the computational foundations of visual reasoning (Vaishnav and Serre, 2023).\nMoreover, our evaluation of both open-source and closed-source models highlights significant performance discrepancies, shedding light on the limitations of current VLMs in handling complex visual reasoning tasks. These limitations underscore the need for further model refinement, particularly in handling multi-image reasoning tasks and improving rule extraction capabilities. Finally, we demonstrate that effective reasoning in complex visual scenarios requires the complementary strengths of VLMs and Large Language Models (LLMs). While VLMs excel at extracting features from images, LLMs play a crucial role in higher-level reasoning tasks, indicating a promising direction for future research where the integration of both types of models can enable more sophisticated problem-solving in visual reasoning tasks."}, {"title": "Limitation", "content": "Despite the promising results of this study, several limitations must be considered. The Bongard Open-world dataset may not fully capture the complexity and diversity of real-world visual reasoning tasks, potentially limiting generalizability. Additionally, the reliance on pre-extracted rule summaries in some experiments restricts the evaluation of models' complete reasoning capabilities, particularly in rule extraction. The scalability of large models, such as GPT-40 and Gemini, is also a concern due to their high computational demands, and their ability to generalize to noisy, real-world data remains uncertain. Furthermore, the opacity of model decision-making processes highlights the need for better interpretability, and the risk of overfitting to task-specific knowledge could hinder broader applicability. Lastly, our evaluation primarily focuses on accuracy and semantic similarity, which may not fully capture the nuances of model reasoning. These limitations highlight areas for future research and refinement in applying VLMs to complex reasoning tasks."}, {"title": "Conclusion", "content": "This work investigates the effectiveness of Vision-Language Models (VLMs) in solving the challenging Bongard Openworld dataset, a task requiring complex visual reasoning. We proposed three human-inspired paradigms\u2014Holistic Analysis, Deductive Rule Learning, and Componential Analysis and demonstrated that VLMs, particularly GPT-40 and Gemini, can surpass human-level performance. Our results highlight that VLMs excel when combining detailed image analysis with structured rule-based reasoning. The study also emphasizes the complementary roles of VLMs and Large Language Models (LLMs), with VLMs excelling at visual feature extraction and LLMs crucial for higher-level reasoning. These findings provide valuable insights into model design and evaluation, suggesting a promising direction for future research in complex visual reasoning tasks and real-world applications."}, {"title": "Appendix", "content": "The insights gained from this work have broader implications beyond the specific task of solving the Bongard Openworld dataset. As the capabilities of Vision-Language Models (VLMs) continue to evolve, the methodologies and paradigms explored in this paper can contribute significantly to the development of models capable of more advanced reasoning and problem-solving in real-world applications. The ability to tackle complex visual reasoning tasks, such as the Bongard problems, is critical for a range of fields, from autonomous systems and robotics to medical imaging and beyond.\nOne of the most significant implications of our work is the demonstration that VLMs, when integrated with structured reasoning paradigms, can outperform human-level performance in specific problem-solving scenarios. This opens up new avenues for the deployment of AI systems in environments that require a blend of visual perception and reasoning, such as dynamic real-world settings where an agent must continually adapt to new and diverse scenarios. By surpassing human-level performance in tasks that require complex visual analysis, VLMs could enable more robust and adaptive AI systems that can be deployed in areas ranging from automated decision-making to real-time problem solving.\nFurthermore, our work also highlights the critical gap between the reasoning abilities of VLMs vs Large Language Models (LLMs). A combination of both is essential is needed for complex reasoning tasks. While VLMs excel at understanding and processing visual inputs, LLMs bring the reasoning and linguistic abilities necessary to contextualize and apply rules based on these inputs. This integration is essential for advancing research in cognitive modeling, where understanding the interplay between perception, reasoning, and language is central. Our findings encourage further exploration of hybrid architectures that seamlessly blend visual and language models, potentially driving progress in areas such as visual question answering, image captioning, and multimodal learning.\nFinally, our detailed evaluation of both open-source and closed-source models contributes to the ongoing dialogue about the accessibility, fairness, and transparency of AI systems. By examining the performance discrepancies across various architectures, we not only illuminate the current limitations of VLMs but also provide actionable insights for future improvements. This work could inspire the development of more efficient and capable models, particularly in the open-source community, and contribute to efforts aimed at making advanced AI more accessible and usable for a wide range of applications.\nIn summary, this work sets the stage for more advanced, human-like AI systems capable of solving complex visual reasoning tasks, with far-reaching applications in domains such as autonomous systems, healthcare, and cognitive modeling. The methodologies and findings outlined here provide a valuable foundation for future research and development in AI, pushing the boundaries of what is possible with multimodal learning and reasoning."}, {"title": "Dataset", "content": "We utilize a subset of 500 cases from the Bongard OpenWorld dataset (Wu et al., 2024), a benchmark for evaluating few-shot reasoning in machine vision. Each case presents a set of positive and negative examples, distinguished by an underlying \"commonsense\" rule. These rules, represented by numerical values (0-9), encode different visual and spatial concepts, as detailed below."}, {"title": "Commonsense Value Categories", "content": "The Bongard OpenWorld dataset categorizes the underlying rules into several visual concept categories, some of which are explicitly related to commonsense reasoning. Table A.3 provides a summary of these categories and example concepts."}, {"title": "Commonsense Value Distribution in Our Subset", "content": "Table A.5 provides a detailed breakdown of the distribution of commonsense values within our 500 test-cases subset. As observed, the value '0' (\"Anything else,\" marked as commonsense) is significantly more prevalent than other values. This distribution should be considered when analyzing the model's performance, as it may indicate a bias towards learning certain types of rules, particularly those related to the \"Anything else\" category."}, {"title": "Dataset Availability", "content": "The complete Bongard OpenWorld dataset is publicly available at Bongard-OpenWorld.github.io."}, {"title": "Evaluation Metrics", "content": "For evaluating the performance of the VLMs, we employ categorization accuracy on the binary classification task. This metric measures the percentage of test instances correctly classified as either positive or negative as present in the Bongard Problems.\nWe leverage a Semantic Similarity Evaluator to quantify the semantic relatedness between generated responses (e.g., question answers) and ground truth answers. This evaluator is inspired by the work of (Risch et al., 2021), which explores semantic answer similarity for evaluating question answering models."}, {"title": "Model Details", "content": "This section provides comprehensive details on the Vision-Language Models (VLMs) utilized in our experiments."}, {"title": "Vision-Language Models (VLMs)", "content": "OpenAI's GPT-40, accessed with vision capabilities via the openai library, processes both images and text seamlessly. It (OpenAI, 2024) excels in reasoning and instruction-following tasks, offering robust text generation and broad general knowledge. However, its closed-source nature and cost make it less accessible for extensive experimentation."}, {"title": "Configurations and Hyperparameters", "content": "For closed-source models, we utilized API calls by installing the openai, mistralai (for Pixtral), and google.generativeai libraries, enabling access to ChatGPT (GPT-40 with vision) (OpenAI, 2024), Pixtral (12B) (Agrawal et al., 2024), and Gemini (2.0-flash-exp) (Google, 2024), respectively. The open-source models, on the other hand, were accessed using the ollama (Ollama, 2025) library."}, {"title": "Holistic Analysis", "content": "1. Identify a rule that distinguishes the positive examples from the negative examples;\n2. Analyze the query image;\n3. Classify the query image as either positive or negative based on the identified rule.\nIt is explicitly requested to the VLM model to provide its analysis, the derived rule, a description of the query image, and the final classification category. Details are provided in Appendix E.1."}, {"title": "Deductive Rule Learning", "content": "This section describes our experiments using a deductive rule learning approach, which involves a two-step process: first, the model infers a general rule; second, it applies this rule to classify a query image.\nThe deductive rule learning experiments assess the ability of VLMs to explicitly formulate a rule before classifying a query image. This approach mirrors human deductive reasoning. We employed a two-stage prompting strategy. In the first stage, we presented the model with $m$ positive examples and $n$ negative examples and asked it to identify a rule that distinguishes the two categories. The prompt requested a concise summary of the identified rule (less than 20 words). The precise wording of this first-stage prompt is provided in Appendix E.2.\nIn the second stage, we presented the model with the previously generated rule summary and a query image. The prompt instructed the model to analyze the query image in the context of the given rule and classify it as either positive or negative. The prompt also requested a brief analysis of the query image and a restatement of the rule. The precise wording of this second-stage prompt is provided in Appendix E.2."}, {"title": "Componential Analysis", "content": "This section describes our experiments using a componential analysis approach (detailed in Appendix E.3). This method involves decomposing each image into its constituent visual elements and then reasoning based on these components (textual elements). This process is divided into three main stages: (1) detailed image description generation, (2) providing instruction on rule derivation, and (3) rule extraction and application based on these descriptions.\nThe componential analysis experiments aim to evaluate the ability of VLMs to reason based on structured descriptions of visual content. This approach simulates how humans might break down complex visual scenes into simpler parts for analysis. We employed a three-stage prompting strategy. In the first stage, we generate detailed descriptions of each image. This prompt instructs the model to analyze an image and provide a structured description, covering aspects such as scene, objects (living and inanimate), activities, contextual elements, visual patterns, emotional undertones, textual information, and a summary. In the second stage, we provide the models with instructions on how to analyze the outputs and derive the rule. In the final stage, which is purely textual, we provided the model with the descriptions generated in the first stage, along with information about which images belong to positive examples and negative examples, and the query image. The prompt then asks the model to derive a rule based on the descriptions of positive and apply this rule to classify the query image."}]}