{"title": "Anomaly Detection in Time Series of EDFA Pump Currents to Monitor Degeneration Processes using Fuzzy Clustering", "authors": ["Dominic Schneider", "Lutz Rapp", "Christoph Ament"], "abstract": "This article proposes a novel fuzzy clustering based anomaly detection method for pump current time series of EDFA systems. The proposed change detection framework (CDF) strategically combines the advantages of entropy analysis (EA) and principle component analysis (PCA) with fuzzy clustering procedures. In the framework, EA is applied for dynamic selection of features for reduction of the feature space and increase of computational performance. Furthermore, PCA is utilized to extract features from the raw feature space to enable generalization capability of the subsequent fuzzy clustering procedures. Three different fuzzy clustering methods, more precisely the fuzzy clustering algorithm, a probabilistic clustering algorithm and a possibilistic clustering algorithm are evaluated for performance and generalization. Hence, the proposed framework has the innovative feature to detect changes in pump current time series at an early stage for arbitrary points of operation, compared to state-of-the-art predefined alarms in commercially used EDFAs. Moreover, the approach is implemented and tested using experimental data. In addition, the proposed framework enables further approaches of applying decentralized predictive maintenance for optical fiber networks.", "sections": [{"title": "I. INTRODUCTION", "content": "Erbium-doped fiber amplifiers (EDFAs) are key elements in current long-haul optical fiber transmission networks. Since even short-term failures of an optical link cause cost-intensive loss of transmission capacity, high reliability of the installed EDFAs is of major importance. Core components of such amplifiers enabling amplification of optical data signals are erbium-doped fibers (EDFs) and pump lasers used to excite the erbium ions. A soft-failure of the pump laser caused by aging and degeneration processes will lead to performance degradation of the whole system.\nNowadays, commercially used EDFAs are typically operated with an integrated automatic gain control (AGC) stabilizing the amplifier gain by controlling the pump current and thus the pump power. Aging of the pump lasers leads to either sudden death or slow reduction of pump power at constant pump current due to wear-out that can be detected by monitoring the pump current and the emitted pump power over time. When the pump current required to obtain the maximum output power exceeds a threshold, an alarm raised.\nStandard techniques determine whether the pump current required to generate the maximum pump power exceeds a pre-defined threshold. However, aging detection does not work reliably when operating an optical amplifier below its maximum output power. In this paper, we propose a change detection framework to detect anomalies in the pump current time series allowing to detect aging of the pump at arbitrary points of operation and thus to reduce the size of the dataset provided to the evaluation algorithm. The proposed method is based on fuzzy clustering and is analyzed by means of experimental data."}, {"title": "II. FUZZY CLUSTERING PROCEDURES", "content": "Many different methods are known to detect anomalies in time series. The most common methods are based on stochastic time series analysis, mathematical models, classification, clustering, etc. [1] [2]. The task becomes even more complicated as the changes develop in bounds in a non-stationary environment. With smoothly and slowly developing changes in the time series, the definition of crisp sets is not feasible, whereas fuzzy clustering methods showed reliable learning and generalization behavior in various applications [3].\nThe aim of clustering is to identify data samples belonging to the same homogenous clouds of observations comprising similar data points. Such a clustering of real-world data is impaired by uncertainties caused by noise, missing values, etc which are forming \"smeared\" boundaries, so-called fuzzy sets. For this reason, the classical fuzzy c-means (FCM) algorithm was developed, as mathematically represented in Eq. (1) [3]\n$w_j(k) = \\frac{||x(k)-c_j||^{-2}}{\\sum_{l=1}^C ||x(k)-c_l||^{-2}}$\n$c_j = \\frac{\\sum_{k=1}^N w_j(k) x(k)}{\\sum_{k=1}^N w_j(k)}$\nwhere $w_j(k)$ denotes the weight j of data sample k. It is calculated as normalized Euclidian distance of the n-dimensional vector of features x(k) from the data sample k to the n-dimensional center $c_j$ of the cluster j. Therefore, $w_j(k)$ is a measurement of the affinity of a data sample to a cluster. As the algorithm aims to find the best fit of centers for all clusters, an update of the initial clusters need to be performed by weighting the data sample x(k) with the quadrating of $w_j(k)$ and normalizing over all data samples.\nIn the traditional approach, a data sample is assigned to a cluster with a crisp set. This means that the data sample is either in the cluster or not. In the case of a fuzzy cluster analysis, the data sample is assigned to a cluster with a membership value between in [0, 1]. These methods developed into two main directions: probabilistic and possibilistic clustering approaches. In [4], the authors propose to use both approaches for the task of anomaly detection in time series. In Eq. (2), a robust distance function $D_R(x(k), c_j)$ is defined, based on the standard activation function of an artificial neuron\n$D_R (x(k), c_j) = \\frac{B}{n} \\sum_{i=1}^{n} ln \\Big[cosh(\\frac{x_i(k) - c_{ji}}{B})\\Big];$\nand defines a scalar metric for the distance between a data sample x(k) and a cluster center $c_j$. It utilizes the hyperbolic cosine in combination with a natural logarithm to reduce the amplification of outliers, which are far away from the center of a cluster. Summarizing over all features n results in one metric per data sample to each cluster center, similar to the weights in Eq. (1). The parameter B controls the speed of change of the function and serves as a hyperparameter for further regularization.\nIntroducing the metric of Eq. (2) and utilizing it in the probabilistic clustering procedure (ProbCP), the following algorithm is defined by Eq. (3)\n$w_r^j(k) = \\frac{e^{(D_R(x(k)-c_j))^{-1}}}{\\sum_{l=1}^C e^{(D_R(x(k)-c_l))^{-1}}}$\n$c_j(k + 1) = c_j(k) + \\eta(k) w_r^j(k) tanh(\\frac{(x_i(k) - c_{ji}(k))}{B_i})$,\nwhere $w_r^j(k)$ serves as metric for the membership of a data sample x(k) to an existing cluster center $c_r$, which is similar to the classic fuzzy clustering represented in Eq. (1), whereas the distance is calculated using Eq. (2) rather than the Euclidian norm. The exponent B influences the shape of the distance function and represents an additional hyperparameter for regularization. In contrast to Eq. (1), the update of $c_j(k)$ in Eq. (3) is carried out iteratively. The prior value for the cluster center is accumulated with a gradient term, whereas $\\eta(k)$ denotes the learning rate of the algorithm and the weighted hyperbolic tangent resembles a gradient value. The learning rate $\\eta(k)$ can be adapted iteratively to reduce eventually occurring oscillating behavior of the algorithm.\nIn addition to the use of weights, the possibilistic clustering procedure (PossCP) makes use of already observed data samples for building the clusters. This results in a new metric $\\mu_j$ and a defined algorithm by Eq. (4)\n$w_{os}^j(k) = (1 + ((\\frac{D_R(x(k),c_j)}{\\mu_j})^{-1})^{(\\beta-1)})^{-1}$\n$\\tilde{c}_{ji}(k + 1) = \\tilde{c}_{ji}(k) + \\eta(k) w_{os}^j(k) tanh(\\frac{(x_i(k) - \\tilde{c}_{ji}(k))}{B_i})$\n$\\mu_j(k + 1) = \\frac{\\sum_{p=1}^{N} w_{os}^j(p)D_R(x(p),c_j(k+1))}{\\sum_{p=1}^N w_{os}^j(p)}$,\nwhere the weights $w_{os}^j(k)$ are calculated utilizing the pre-defined distance function of Eq. (2) and an exponential implementation of a hyperparameter for regularization, whereas the distance function is normalized by $\\mu_j$. This scalar metric observes the already presented data samples and adapts dynamically the distance at which the membership level takes the value 0.5, which results in $w_{os}^j(k) = 0.5$. The iterative update of the cluster centers is similar to the probabilistic clustering procedure, represented in Eq. (3)."}, {"title": "III. EXPERIMENTAL SETUP", "content": "The used data acquisition setup is shown in Fig. 1, where a wavelength-division multiplexing (WDM) signal is amplified by a commercial EDFA. The WDM signal contains ten channels, equally distributed in the conventional wavelength band (C-band). The input power level is set to a range from -35 dBm to 1 dBm. The gain is set to values ranging from 19 dB to 35 dB. Furthermore, the maximum output power is limited to 20 dBm. With this setup, a total dataset comprising 11,886 samples with 41 features is generated.\nThe features mainly contain monitoring values from the EDFA. The features are divided into three groups: optical, electrical and temperature readings. First, in the optical part we have the optical input power, the optical output power, the optical input power of the second stage and the optical output power of the first stage. Second, in the electrical part we have monitoring values about the currents and pump powers for the EDFA stages and more operating voltages. Finally, in the temperature part we have the temperature of the EDFA housing and the temperatures of the pump laser chips.\nWith the aim of detecting anomalies in the time series of pump current data, variations are introduced in these data. Pump degeneration is a well-researched process [5] [6]. Due to aging, the pump current required to reach a desired optical pump power starts to increase over time [7]. The ratio of actual pump current to nominal current can be used to model these effects and to create a specific data stream for evaluating the algorithms."}, {"title": "IV. CHANGE DETECTION FRAMEWORK", "content": "Artificial intelligence (AI) models can suffer from the curse of dimensionality. This means that the amount of features that can be learned by a machine learning algorithm is dependent on the algorithms capacity. Rather than overcoming this problem by using deep learning architectures, we propose a feature selection and reduction process. Combined with the clustering procedures, this approach results in a change detection framework (CDF), which is shown in Fig. 2.\nThe *Data Preprocessing Module* removes irrelevant, not explainable, null or repeated features from the dataset. Machine learning algorithms are sensitive to the scale of the data [8]. Therefore this module implements a standard scaler to normalize the data.\n*Feature Selection* uses a wide variety of methods which measure the impact of different features on the target value. Due to a missing target value in the task of clustering, these algorithms are not suitable [9] for the present problem. Therefore, we propose a method derived from information theory wherein information of a distribution is measured by it's entropy, as represented in Eq. (5).\n$H(X) = - \\sum_{k=1}^{N} P_k ln P_k,$\nwhere the entropy accumulates the weighted probabilities of events of a feature. Through the use of the natural logarithm, the units are called *nats*. Calculating the entropy for each feature and selecting all usable features with $H(X)_i > H_{min}$ and $H_{min} = 0$ results in a suitable feature selection method, whereas no transformations are performed."}, {"title": "V. RESULTS AND ANALYSIS", "content": "The learning behavior of the algorithms under different settings for the CDF is illustrated in Fig. 5. First of all, it is noticeable that the convergence criterium is reached within 30 iterations, respectively. Nonetheless, the classical FCM tends to soar in the first iterations for all given CDF configurations, which implies trouble in finding the best gradient for the provided dataset. In contrast, the ProbCP reduces the soaring and provides a more robust behavior for the CDF configuration with enabled EA and EA + PCA. Finally, the PossCP eliminates the soaring completely and resembles the most robust learning behavior for all three clustering algorithms and is independent over all CDF configurations.\nIn the experiment, the learning rate is set to a fixed value of $\\eta = 10^{-3}$. Using $||W^{t+1} - W^{t}|| < \\epsilon$ with $\\epsilon = 10^{-4}$, a suitable stop-criterium is created. The hyperparameters of the algorithms are set to B = 2 and $\\beta_1 = 1$. For the purpose of stochastic variance and the random initialization of the weight matrices, the number of runs equals n = 25 for averaging the results.\nFirst, the performance of the algorithms are evaluated on the raw data. The results are shown in Tab. I. The mean square error (MSE) serves as metric for the performance of the algorithm and is measured by the training ($MSE_{tr}$) and test ($MSE_{te}$) dataset. The results show that the FCM performs worse (29.3%) in terms of the training dataset as compared with ProbCP (0.0%) and PossCP (0.0%). The evaluation on the test dataset shows that no algorithm is able to generalize, which leads to unsatisfied performance.\nSecond, the performance of the algorithms is evaluated by using the data after the feature selection with EA. The results are shown in Tab. I. The EA slightly improves the performance of the algorithms on the training and test dataset. Nonetheless, the performance of the algorithms on the test dataset with 86.3%, 78.7% and 74.3% is insufficient.\nThird, we introduce the PCA to reduce the dimensionality of the dataset and to perform an orthogonal transformations on the features. As shown in Tab. I, the PCA has a positive impact on the performance of the algorithms on both, training and test dataset. The PCA enables the algorithms to generalize from the training dataset and improves the performance on the test dataset up to 30.3%, 22.7%, and 18.9% for the FCM, ProbCP and PossCP, respectively.\nFinally, the performance is determined for a combination of EA and PCA which is shown in Tab. I. The EA can improve the performance of the algorithms with PCA up to 3.0% in terms of the training dataset and up to 1.4% in terms of the test dataset. This is due to a change in the principle components, which are generated by the transformation and thus produce better learnability. Therefore the combination of the fuzzy clustering algorithms with the EA and PCA is the best performing setup for the CDF."}, {"title": "C. Comparison with other methods", "content": "To evaluate the performance of the CDF, a comparison to current state-of-the-art methods is done. In [11] an overview of available clustering methods in the field of machine learning is given. Therefore, a direct comparison of the CDF with algorithms of the same class is done: 1) K-Means clustering, 2) hierarchical (agglomerative) clustering and 3) Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH) clustering. K-Means clustering was originally invented for signal processing and utilizes vector quantization. Hereby, n observations are clustered into k clusters whereas the nearest mean serves as cluster prototypes. It has been successfully applied to the task of unsupervised anomaly detection in time series as shown in [12]. Hierarchical clustering uses a linkage distance, the Euclidian norm or any other distance metric, and recursively merges pairs of clusters to build a classification tree [13]. BIRCH is an online-learning algorithm with a high memory-efficiency. Similar to hierarchical clustering, a tree-like data structure is constructed. Lately, the successful application to detect anomalies in time series with lower dimensions has been proven [14]. In order to reduce the impact of randomness, the experiment was repeated 20 times and the metrics are averaged. As shown in Tab. II, the chosen performance metric for evaluation is MSE for training as well as test scenario. K-Means clustering has a poor learning behavior (39.87%) and is not able to generalize. With a test performance of 45.33%, an oscillating behavior is likely as we have a two-class problem. Hierarchical clustering shows similar results as the K-Means algorithm with a performance of 39.32% in training and 47.53% in test, respectively. BIRCH provides a better performance in training (31.67%) and test (34.17%) than the beforementioned algorithms. Of interest is the significant performance deviation between hierarchical clustering and BIRCH. Both algorithms construct a hierarchical data structure, but in contrast hierarchical clustering has a bottom up approach, whereas each observation starts in it's own cluster. More importantly, the proposed CDF outperforms all compared ones with the smallest values for MSE in training and test."}, {"title": "D. Experimental anomaly detection study", "content": "The purpose of the CDF is to detect anomalies in time series of the pump current, but the performance metrics of the training and test dataset are quantified measurements for shuffled datapoints. In order to evaluate the performance of the algorithms on the time series, we introduce the change-point detection (CPD) as a metric. The CPD is defined as the minimal change in the pump current which is detected by the algorithms with EA + PCA, shown in Tab. III. The value $I_o$ denotes the nominal pump current and I the actual pump current. Therefore $I/I_o$ can be interpreted as drift. The results show that the C-Means algorithm is able to detect a drift of 8.1% in the pump current. The Probabilistic and Possibilistic algorithms are able to detect a drift of 5.9% and 4.9% in the pump current, respectively. Therefore the PossCP enables the CPD significantly earlier than the predefined thresholds, which are typically set to 10%, especially for arbitrary operating conditions.\nWith the intention to implement the proposed algorithm in a live system, an additional anomaly identification test on different datastreams is performed. Therefore an introduction into inspection intervals needs to be done. A predicitve maintenance (PM) framework monitors the health state or condition of a system, represented through parameters and characterstic values, over a given time. Each point of observation in this specific timeline is called an inspection [15]. In the following experiment six different datastreams are tested on the CDF. The reference is represented as a datastream with no degradation at all (DR 0%), but with existing measurement noise. In addion, five datastreams is applied to the algorithm, each with a different rate of degradation (DR 10% - DR 50%). The timeline is set to contain a maximum of 150 inspections. In the transient states typically an oscillating behavior of the classified class will occure. To counteract this effect, a moving average filter is applied. It contains a sliding window technique (SWT), which is commonly used in PM tasks to capture time dependant information over a certain period of time, in this case 40 inspections. With the utilization of a filter, a transformation of the discrete classification value into a continuous class membership value is achieved. Therefore a class membership value under 0.5 is defined as OK with no anomalies and above 0.5 as not OK (nOK) containing anomalies. The experimental results of the applied datastreams onto the CDF are shown in Fig. 6. First, it is noticable that a noise loaded datastream (DR 0%) of normal operating conditions is correctly classified. Second, every degradation containing datastream is subject to a transmission from state OK to nOK. Third, the higher a degradation rate is the earlier the CDF is able to detect an anomaly. These three aspects indicate that the proposed CDF is detecting anomalies in a robust manner."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, a novel change detection framework based on entropy analysis, principle component analysis and fuzzy clustering algorithms is proposed to detect anomalies in time series of pump currents of EDFAs due to degeneration effects. A mentionable characteristic of the proposed method includes integrating multiple sensor data with arbitrary operational conditions in a consolidated framework. The fuzzy clustering based model takes normalized and cleaned sequence data as main inputs and eliminates stochastic non-informant data through entropy analysis. Furthermore, the dimensional reduced data stream is processed by principle component analysis to extract the features with the highest variance ratio and enabling generalization of the given data. Based on experimental measurements of drifting pump currents, the performance of the fuzzy clustering algorithms and the novel change detection framework shows significant generalization under arbitrary operational conditions. This leads to the ability to detect anomalies in the time series with up to 4.9% drift from the typical operating conditions. In addition, the experiments carried out show a robust behavior of the algorithm. In particular, it is possible to robustly detect anomalies in data streams that contain degenerative features as well as noisy data streams that resemble normal operating conditions. Detecting anomalies in time series of data resembles the important initial step enabling predictive maintenance. Reducing data dimensionality leads to decentralized implementations and a non-traffic-affecting working scenario of such a system. In our future work, the issue of analyzing anomalies in arbitrary operational conditions will be considered to propose a prognostic and diagnostic model for EDFAs."}]}