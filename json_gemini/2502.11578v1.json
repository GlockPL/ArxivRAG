{"title": "Language Complexity Measurement as a Noisy Zero-Shot Proxy for Evaluating LLM Performance", "authors": ["Birger Moell", "Johan Boye"], "abstract": "Large Language Models (LLMs) have made significant strides in natural language generation but often face challenges in tasks requiring precise calculations and structural analysis. This paper investigates the performance of state-of-the-art LLMs on language complexity measurement tasks, through the computation of the LIX readability metric and Average Dependency Distance (ADD). Using Swedish high school and university-level essays, we evaluate the models' abilities to compute LIX scores and perform dependency parsing, comparing their results to established ground truths. Our findings reveal that while all models demonstrate some capacity for these tasks, ChatGPT-01-mini performs most consistently, achieving the highest accuracy in both LIX computation and dependency parsing. Additionally, we observe a strong significant correlation -0.875 p 0.026 (N=6) between the models' accuracy in computing LIX and their overall performance on the Massive Multitask Language Understanding (MMLU) benchmark. These results suggest that language complexity measurement abilities can serve as a noisy zero-shot proxies for assessing the general capabilities of LLMs, providing a practical method for model evaluation without the need for extensive benchmarking datasets.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are advancing at an unprecedented pace. Until recently, models like ChatGPT demonstrated remarkable language generation capabilities but often struggled with mathematical and analytical tasks requiring a single correct answer (Wang et al., 2024a; Li et al., 2024; Mittal et al., 2024; Wang et al., 2024b).\nThis paper examines the performance of state-of-the-art LLMs on two analytical tasks related to language complexity: (1) computing the LIX readability metric and (2) performing dependency parsing to calculate the Average Dependency Distance metric. These tasks are particularly relevant as they test the mathematical capabilities of LLMs (in the case of LIX) and their structural reasoning abilities (in the case of dependency parsing). The LIX metric is especially intriguing because it involves counting the number of letters in tokens, a task that poses challenges given that tokens in LLMs are represented as numerical identifiers. This transformation should, theoretically, obscure information about the internal structure of words.\nWe evaluate six models: Gemini-1.5-Pro,\nGemini-2.0-flash, (developed by Google), Llama 70b, Llama70b 3.3 (from Meta), GPT40-mini, and 01-mini (released by OpenAI). Each model is presented with identical prompts for both tasks, and the outputs are compared against ground truth. The complete set of prompts is provided in the Appendix."}, {"title": "2 Language complexity metrics", "content": ""}, {"title": "2.1 Average Dependency Distance", "content": "Average Dependency Distance (ADD), suggested by Liu (2008), calculates the distance between each word (except the root word) and its head in a dependency tree (K\u00fcbler et al., 2009). For example, in the dependency tree for the sentence \"John made the pie in the fridge\" below, the distance from \u201cfridge\" to \"pie\" is 3. The ADD of the sentence is 12/7, or about 1.7."}, {"title": "2.2 Unlabeled Attachment Score", "content": "Accurate computation of the Average Dependency Distance (ADD) necessitates starting with a correct dependency tree. Therefore, evaluating how well various models perform dependency parsing is crucial. The quality of a dependency parser can be assessed by calculating the average Unlabeled Attachment Score (UAS) over a set of sentences. The UAS represents the proportion of dependency relations (i.e., the connections between words) that the parser correctly identifies when compared to the gold-standard (correct) tree, disregarding the dependency labels (no labels are shown in the tree above). In the example above, if a model would predict an arrow from \u201cmade\" to \"fridge\", rather than the correct arrow from \u201cpie\" to \"fridge\", the UAS would be 6/7, or about 0.86."}, {"title": "2.3 Swedish readability index LIX", "content": "LIX, a readability index suggested by Bj\u00f6rnsson (1968), is computed as A/B + 100C/A, where A is the number of words in the text, B the number of sentences, and C is the number of words longer than six letters. Higher LIX values indicate a more advanced text: Typically LIX<30 is considered an easy text, whereas LIX>50 is advanced, and LIX>60 very advanced (e.g., research papers)."}, {"title": "3 Method", "content": "We randomly selected 345 university-level and high-school essays\u00b9. All essays were written in Swedish before 2018 (to ensure that the author had not used any generative AI writing tool). From each essay, we randomly selected one paragraph (averaging 71 \u00b1 15 tokens) for the LIX calculations, and one sentence (averaging 26 \u00b1 8 tokens) for the dependency parsing experiment. Examples of paragraphs and sentences can be found in the Appendix.\nWe computed the ground-truth LIX values by a Python script implementing the LIX formula. The ground truth dependency trees were produced using the Stanza library\u00b2. We then asked the four models to compute the LIX score of each paragraph, and then to analyze our selected sentences and print their dependency trees. We instructed the models to print one word per row, on the following format:\n1, Han, 2, 1\ni.e., the word index, the word itself, the index of the headword, and the dependency distance between the word and its head. The exact prompts can be found in the appendix. Finally, we asked each model to compute the average dependency distance for each sentence."}, {"title": "4 Results", "content": ""}, {"title": "4.1 Lix", "content": "The LIX values calculated by the models highlight a clear trend in their ability to accurately compute readability scores. The errors in LIX values range from 7.4 to 20.9, with the best performance achieved by ol-mini, which recorded the lowest LIX error of 7.4. In contrast, llama-70b had the highest error at 20.9, indicating room for improvement in its computation of readability metrics.\nIntermediate performance was observed with models like Gemini-2.0-flash and GPT-40-mini, which achieved LIX errors of 10.42 and 9.2, respectively. These results suggest that newer and more advanced models tend to perform better in calculating LIX, aligning with improvements in their general capabilities.\nThe consistency of the results across models also indicates that LIX computation may serve as a valuable metric for assessing a model's ability to process linguistic features effectively. This evaluation is particularly relevant for applications in natural language understanding where readability and text complexity play crucial roles."}, {"title": "4.2 Correlation between MMLU and LIX error", "content": "To evaluate the relationship between the performance of large language models (LLMs) on the Massive Multitask Language Understanding (MMLU) benchmark and their linguistic complexity, as measured by the LIX Error, a Pearson correlation coefficient (r) was calculated.\nThe MMLU score represents the accuracy of the models on a standardized benchmark assessing general knowledge and reasoning abilities across multiple domains, while the LIX Error reflects the linguistic complexity or readability of the text generated by the models.\nThe Pearson correlation coefficient measures the linear relationship between these two variables, where r values range from -1 (perfect negative correlation) to +1 (perfect positive correlation), with 0 indicating no linear relationship."}, {"title": "4.3 Dependency parsing", "content": "All the models have a grasp of the fundamentals of dependency parsing: the words in the sentence are indexed from 1 up to the number of words, and the index of the head of any word (almost) always is a number in this range. To accurately measure the average UAS, however, one cannot simply go through all the words in a sentence and check that the model's predicted head word index matches that of the ground truth. This is because the model might tokenize the sentence differently from what is expected in the ground truth dependency tree, which would result in the words being numbered differently from the ground truth tree. As an example, we noticed that quoted words, like \u201ch\u00e5rda\" (Eng. \"hard\") was tokenized as three tokens (\", h\u00e5rda, \") by o1-mini, which is arguably more correct than the ground truth tokenization, which considered \"h\u00e5rda\" to be a single token.\nInstead of an index-based comparison, we carefully checked the dependency relations based on the words themselves to see if the model's predictions matched the ground truth. The results are shown in Table 2. We also calculated the Pearson correlation coefficient between MMLU and Add diff 1 to be -0.519, with a two-tailed p-value of 0.370. This indicates a moderate negative correlation that is not statistically significant."}, {"title": "4.4 Average dependency distance", "content": "Table 2 shows the average absolute difference between the ADD computed from the ground truth dependency trees and the ADD computed from the dependency trees produced by the various models (\"ADD diff 1\"). 01-mini is best in this category as well. In addition, we can note that the difference between the ADD values reported by 01-mini and the actual ADD values of its produced trees (\u201cADD diff 2\") is very small. That is, it is not necessary to use some external program to calculate the ADD of a sentence based on a dependency tree constructed by the model, but one can directly ask the model for the ADD value and expect to get a quite accurate result. Note that this is not the case for the other models. Both ADD diff 1 and ADD diff 2 are negatively correlated with MMLU scores (-0.83 and -0.63, respectively).\nTable 3 (in the appendix) shows that, quantitatively, 01-mini is the best-performing model of the five, though the quality of its inferences vary between different parts-of-speech. For example, 01-mini is good at inferring the head for adjectives and determiners (whose head is most often the noun that follows closely afterwards in the text), but bad at placing coordinating conjunctions correctly. A common example in the latter category are noun phrases consisting of two nouns with a conjunction in between, e.g. \u201corganisation och orientering\u201d (Eng.: \u201corganisation and orientation\u201d). The correct structure according to the Universal Dependencies guidelines (and Stanza) is:\ni.e., the first word of the conjunction is considered to be head of the whole phrase. However, no model is consistent in this regard: sometimes the first noun is considered the head, sometimes the second, and sometimes the conjunction (but arguably that this choice is somewhat arbitrary anyway).\nAnother phenomenon that all models get (consistently) wrong is the root node in the presence of a predicative, e.g., \"Varje project \u00e4r unikt\u201d (Eng. \u201cEvery project is unique\"). Though normally the main verb is the root word in each sentence, this is not the case when the main verb is a copula (the verb \"be\"). Instead, the predicative \u201cunique\u201d should be the root word. Every model got this wrong, indicating instead that \u201c\u00e4r\u201d (\u201cis\u201d) should be the root.\n01-mini predicted the role of punctuation signs in the sentence much better than the other models. Gemini, in particular, often skipped the punctuation signs altogether (although it was explicitly instructed to consider all punctuation signs as tokens), leading to a very poor result in this category."}, {"title": "5 Discussion", "content": "Understanding the inner workings of LLMs and evaluating their performance are tasks that hopefully can be done in parallel. In this work exploring language complexity and LLMs we showed that language complexity evaluation can be used to evaluate overall model performance. As these models become ubiquitous, quick methods for evaluation and understanding become more important. The strengths of our technique is its simplicity and language independence. The fact that we do not evaluate the content of language and instead evaluate the understanding of the structure of language makes our technique a reasonable candidate for evaluation of reasoning in LLMs. In the same way working memory tests are a noisy proxy for intelligence in humans, our tests can be seen as a working memory test for LLMs where models with better \"working memory\"; reasoning models outperform other models."}, {"title": "5.1 Conclusion", "content": "Language complexity metrics like LIX and ADD provide valuable insights into LLM capabilities. Despite advances in fluency, LLMs remain prone to errors in explicit calculations and syntactic fidelity. These findings suggest that complexity metrics can efficiently complement broader evaluation benchmarks."}, {"title": "6 Limitations", "content": "Although our findings demonstrate that language complexity measures (e.g., LIX) can serve as a useful\u2014albeit noisy\u2014zero-shot proxy for general LLM abilities, several limitations merit discussion:\nSingle run: Every LIX-computation and dependency-parsing was only done once per sentence and model.\nScope of Evaluation: First, our experiments focus exclusively on Swedish texts drawn from university and high-school essays. These sources may not capture the full breadth of linguistic styles and complexities found in other domains, genres, or languages. Consequently, our results may not generalize to languages with different morphological structures or to more specialized text types such as technical documents or colloquial dialogues.\nReliance on Proprietary Models: We primarily evaluated closed-source, proprietary models whose internal architectures, tokenization schemes, and training data are neither publicly accessible nor standardized.\nTokenization and Computational Fidelity: Our analysis relies on each model's tokenization process, which can differ substantially across architectures and affect the accuracy of LIX or ADD computations. Because we depend on the models to generate the appropriate tokens and numerical outputs, discrepancies in tokenization can introduce systematic errors in measured performance.\nRapid Model Evolution: Large Language Models evolve rapidly through fine-tuning and architectural enhancements. Performance measured at a particular snapshot in time may not generalize to newer versions of the same model. Ongoing and systematic evaluations are therefore essential to capture how fast-changing model iterations handle language complexity tasks.\nData Sampling: Finally, we used a limited number of paragraphs and sentences for LIX and dependency parsing evaluations. While this was sufficient to identify consistent errors and variance"}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Prompts used", "content": ""}, {"title": "Average Dependency Distance (ADD)", "content": "I would like you to print the dependency parsing result for a given Swedish sentence.\nPrint the result with one word on each row, on the following form:\n1, Han, 2, 1\n2, k\u00f6per, 0, 0\n3, en, 4, 1\n4, bok, 2, 2\nwhere the first number is the word index, the second column is the word itself, the third column is the index of the head word, and the last number is the dependency distance (i.e. the absolute difference between the index and the head word index). The root word should have head word=0 with a distance of 0. Finally, print the average of all the dependency distances in the sentence.\nHere is the sentence: \"text\""}, {"title": "Readability index (LIX)", "content": "Analyze the following Swedish text and compute a LIX readability score. Here's how to calculate LIX:\n1. Calculate the average sentence length (number of words / number of sentences)\n2. Calculate the percentage of long words (words with more than 6 characters) 3. Add these two numbers together to get the LIX score\nImportant: LIX scores typically range from 20 to 60, with: - 20-30 being very easy text - 30-40 being easy text - 40-50 being medium difficulty - 50-60 being difficult text Any score above 60 is extremely rare and likely indicates an error in calculation.\nPlease show your calculations and provide the final score in the format 'LIX=' followed by the number.\nHere is the text to analyze: (text)"}, {"title": "A.2 Example texts used for the LIX computations", "content": "ul\nFinns det n\u00e5gon mening med att studera histora?Jag anser att det g\u00f6r det. Som ett argument f\u00f6r det kan"}, {"title": "A.3 Example sentences used for dependency parsing", "content": "1. \u00c4ven om det r\u00f6r sig om att man som 2-\u00e5ring l\u00e4r sig att inte springa in ett tr\u00e4d f\u00f6r att man f\u00e5r v\u00e4ldigt ont d\u00e5.\n2. Indataenheter \u00e4r som det l\u00e5ter, saker som vi anv\u00e4nder f\u00f6r att skicka in data till datorn, tangentbordet \u00e4r ett bra exempel, scanner och gamepads \u00e4r tv\u00e5 andra exempel.\n3. L\u00e5ngt senare b\u00f6rjade en resturang, kallad som Bruno vid berget Vezuvio, att l\u00e4gga tomater, mozarella och persilja p\u00e5, eller om det var n\u00e5n annan krydda.\n4. Journalisten Bj\u00f6rn Almqvist har f\u00f6ljt n\u00e5gra av graffitim\u00e5larna som \u00e4r med i WUFC under flera \u00e5rs tid och fotat alla tunnelbanor som de har m\u00e5lat p\u00e5, alla v\u00e4ggar de har m\u00e5lat p\u00e5 och p\u00e5 deras resor runt om i v\u00e4rlden.\n5. N\u00e4r jag gick \u00f6ver statistik om emigration fr\u00e5n Vallonien fann jag att Sverige var bland de fem regioner dit vallonerna flyttat till mest, tillsammans med Flandern, Brasilien, Argentina och USA (Wisconsin, framf\u00f6rallt).\n6. Urbaniseringen, som detta kallas, har gjort s\u00e5 att h\u00e4lften av dagens befolkning bor i de femton st\u00f6rsta st\u00e4derna.\n7. Efter 2: a v\u00e4rldskriget s\u00e5 best\u00e4mde sig 51 l\u00e4nder att bilda FN, F\u00f6renta nationerna.\n8. P\u00e5 morgonen och lite in p\u00e5 dagen avdunstar vattnet ifr\u00e5n Atlanten som senare regnar ned p\u00e5 eftermiddagen.\n9. Jag skulle vilja s\u00e4ga att imperialismen b\u00f6rjade s\u00e5 l\u00e5ngt tillbaks som f\u00f6r2000\u00e5r sedan, och d\u00e5 t\u00e4nker jag fr\u00e4mst p\u00e5 rommarna som hade er\u00f6vrat stora delar Europa och \u00e4ven delar av Asien.\n10. M\u00e5nga av de invandrare som kommer till Sverige har inte varit n\u00e5gra outbildade bidragstagare i sitt hemland."}]}