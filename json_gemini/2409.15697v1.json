{"title": "DNAGRINDER: A LIGHTWEIGHT AND HIGH-CAPACITY GENOMIC FOUNDATION MODEL", "authors": ["Qihang Zhao", "Chi Zhang", "Weixiong Zhang"], "abstract": "The task of understanding and interpreting the complex information encoded within genomic sequences remains a grand challenge in biological research and clinical applications. In this context, recent advancements in large language model research have led to the development of both encoder-only and decoder-only foundation models designed to decode intricate information in DNA sequences. However, several issues persist, particularly regarding the efficient management of long-range dependencies inherent in genomic sequences, the effective representation of nucleotide variations, and the considerable computational costs associated with large model architectures and extensive pretraining datasets. Current genomic foundation models often face a critical tradeoff: smaller models with mediocre performance versus large models with improved performance. To address these challenges, we introduce dnaGrinder, a unique and efficient genomic foundation model. dnaGrinder excels at managing long-range dependencies within genomic sequences while minimizing computational costs without compromising performance. It achieves results that are not just comparable but often superior to leading DNA models such as Nucleotide Transformer and DNABERT-2. Furthermore, dnaGrinder is designed for easy fine-tuning on workstation-grade GPUs, accommodating input lengths exceeding 17,000 tokens. On a single high-performance GPU, it supports sequences longer than 140,000 tokens, making it a highly efficient and accessible tool for both basic biological research and clinical applications.", "sections": [{"title": "Introduction", "content": "Foundation models (aka large language models) such as BERT [1] and GPT [2], have demonstrated their stellar performance in learning the complex characteristics and structures of natural languages, making them well-suited for a variety of subsequent applications, such as sentiment analysis, text generation, and translation [3]. These foundation models have recently been adapted to analyze biological sequences as their deep structure and large-scale parameters are well suited for dealing with the intricacy of biological sequences and structures [4, 5, 6, 7, 8, 9, 10, 11]. Biological sequences composed of nucleotides like DNA and RNA, as well as amino acids forming peptides and proteins, are regarded as natural languages of life and can be effectively leveraged by using the technology of foundation models to uncover the underlying patterns and functions they encode [12]. Typically, these foundation models build robust feature representations from biological sequences through a process known as pretraining. Encoder-based models like BERT perform such pretraining by using a method called Masked Language Modeling (MLM), where they predict the actual words of some masked or corrupted ones in given sequences. By pretraining on millions of biological sequences, foundation models gain a comprehensive contextual understanding of the given sequences. Once trained, they only need a few fine-tuning steps to be effectively applicable to specific downstream tasks [13], including prediction of epigenetic marks, gene expressions, protein folding structures, and more.\nTo address these limitations, we introduce dnaGrinder, a refined genomic foundation model. dnaGrinder that utilizes an efficient pretraining strategy and incorporates multiple enhancements to overcome input length constraints, reduce time and memory usage, and improve model performance. In addition, we propose a novel approach to expanding the pretraining dataset by effectively increasing genome diversity rather than simply adding similar or repetitive sequences. Through extensive experiments on a dozen downstream benchmarks, we demonstrate that dnaGrinder achieves performance exceeding or comparable to state-of-the-art models while requiring fewer parameters and less GPU time for both pretraining and fine-tuning."}, {"title": "Methods", "content": "In this section, we present an overview of dnaGrinder's architecture, detailing its features and enhancements. We also discuss the specific implementation of the pretraining strategies employed to integrate these architectural improvements, providing insights into how these modifications contribute to the model's overall performance and efficiency."}, {"title": "Model", "content": "The dnaGrinder model employs an encoder-only transformer architecture. DNA sequences are first converted into numerical representations using Byte Pair Encoding (BPE) tokenization [18]. These numerical representations are then transformed into sequences of embeddings through an embedding layer. Unlike most encoder-only models that use absolute positional embedding [1] or rotary positional embedding [19], we utilize Attention with Linear Biases (ALiBi) [14], which is introduced at the beginning of the attention computation. To improve computational, memory, and inference efficiency, we employ sequence length warmup [20, 21] to the pretraining phase and adopt Flash Attention 2 [22] as our attention mechanism. We also experiment with several architectural enhancements, including the SwiGLU [15] activation function and token random replacement [5]. During the pretraining stage, we incorporate dynamic masking [23] to improve the model's learning capability."}, {"title": "Memory-efficient BPE tokenization", "content": "Byte Pair Encoding (BPE) [18] is a data compression algorithm that segments words by counting the co-occurrence frequency of subwords. For DNA data, BPE starts with a base vocabulary of four characters of base pairs (A, C, G, and T). In each iteration, it counts the frequency of each consecutive pair of character segments. The most frequent pair is identified and merged into a new subword, effectively reducing the number of distinct pairs. This process is repeated until the vocabulary reaches the desired size, which is 4,096 tokens in the current implementation of the model. By merging frequent pairs, BPE captures common patterns and motifs in the DNA sequences, improving both the efficiency of tokenization and the model's ability to learn meaningful representations. The final vocabulary, therefore, consists of the most frequent and representative tokens derived from the set of given sequences, enlarging the tokenization length and reducing computational complexity.\nCounting and merging pairs during BPE tokenization are memory-intensive processes that involve multithreading [24]. The memory consumption depends on both the number of sequences and the length of each sequence. For instance, the corpora file of DNABERT-2, where each sequence is 1,000 bp long, and the total size is 30GB, requires over 1TB of memory during training, according to the original authors.\nFor our model, with each sequence length set to 12,200 bp and a corpus size exceeding 118GB, it is impractical to load the entire corpus into memory at once. Our tests indicate that the maximum manageable corpus size per file for BPE tokenization, with each sequence length set to 12,200 bp, is approximately 20GB, which would still require around 1.8TB of memory. To address this issue, we split the corpus into smaller files and train iteratively on one file at a time, thereby managing memory usage more effectively.\nOur memory-efficient BPE tokenizer begins by processing the first file of sequences to generate an initial vocabulary of 4,096 tokens based on its content. For each subsequent file, the tokenizer updates its vocabulary using the content of the new file. Suppose tokens in the new file appear more frequently than some of the existing tokens in the vocabulary. In that case, the tokenizer replaces the less frequent tokens to maintain a vocabulary size of 4,096. If a token from the new file already exists in the current vocabulary, its frequency will be updated to include the new occurrences. However, if a token was previously excluded from the vocabulary, its earlier frequency is not retained, and only the frequency from"}, {"title": "Sequence Length Warmup (SLW)", "content": "During pretraining, encoder-based models usually randomly sample data from the entire dataset according to the batch size. This approach works well when the variance of sequence lengths is minimal, as observed in models such as DNABERT [4] and Nucleotide Transformer [5], which use fixed-length k-mer sequences. On the other hand, with BPE, even if the original sequence length is fixed, the number of tokens after tokenization varies. For instance, DNABERT-2 [7] restricts its pretraining sequences to a maximum of 128 tokens. Notably, despite the use of BPE, the length of DNABERT-2's pretraining sequence varies little, which does not significantly affect its training strategy.\nIn contrast, our model deals with sequences of 12,000 bp long. Given the use of BPE and a substantial portion of our training sequences from multispecies, the tokenized sequences have over 700 to more than 2300 tokens. If sequences are randomly sampled to form a batch for training, sequence lengths may vary considerably, and we need to pad these sequences to have the longest, uniform length in each batch. Because of the long sequence length resulting from padding, the pretraining is prolonged. Since the sequence lengths vary from one batch to the next, model performance fluctuates across batches. To address these issues, we adopt a sequence length warmup strategy, often used in pretraining decoder models [20, 21]. This strategy arranges the sequences in increasing order of their number of tokens, which helps to reduce training time and enhance stability as the variance in gradients increases.\nIn addition, we employ a data augmentation technique akin to that utilized in Nucleotide Transformer to generate a comprehensive set of training sequences. Initially, we partition the genome of each species into overlapping segments, each with 12,200 bp in length. Each segment is designed to overlap with its predecessor and successor by 100 bp at both the beginning and the end. From these overlapping segments, we extract a 12,000 bp segment. To enhance the diversity of the training set, the starting positions for these extracted segments are randomly selected within the initial 200 bp of the overlapping segments. This extraction process is repeated multiple times, resulting in a training sequence set comprising 300 billion tokens, which is consistent with the quantities typically employed in other genomic foundation models. Although different segments derived from the same genomic region of a species exhibit variability, BPE tokenization ensures that the tokenized sequences maintain comparable lengths. Finally, the augmented sequences originating from the same genome are organized together in the final pretraining dataset according to their respective sequence lengths.\nTo the best of our knowledge, our model is the first encoder-based architecture to incorporate SLW in pretraining. By leveraging SLW, we organize the pretraining process by the order of species. Our findings (Section 4) demonstrate that the model is capable of effectively learning sequence features and representations after being trained on a dataset comprising just 69.5 billion tokens.\nThe following observation can help appreciate SLW's contribution to model performance. Sequences characterized by a greater number of repeated elements (or simple patterns) exhibit lower complexities and reduced entropies, enabling them to be compressed into fewer tokens. This compression results in shorter sequence lengths in terms of token count. In the pretraining phase, sequences with low complexities are prioritized for processing over those with high complexities. The model first acquires the simpler patterns inherent in low-complexity sequences before advancing to the more intricate patterns in high-complexity sequences. This approach of initially focusing on simple patterns facilitates the model's ability to learn complex patterns within more intricate sequences. Consequently, this methodology enhances the overall pretraining process and, in turn, improves the performance of the model."}, {"title": "Attention with Linear Bias (ALiBi)", "content": "When a model is trained on short sequences, such as 512 tokens, its ability to handle longer sequences during inference is known as its extrapolation capability. This presents two challenges: first, the model meets position encodings that are not seen during training; second, the number of tokens processed by the attention mechanism during inference significantly exceeds those encountered during training. Popular approaches, such as Sinusoidal positional embeddings [25] and Rotary positional embeddings (RoPE) [19], either impose limitations on the maximum allowed input length or encounter difficulties in maintaining effective attention over long sequences. Specifically, RoPE has been found to have a decaying effect [26], where the model struggles to attend to tokens beyond 4,000-6,000 positions, even with extensive long-context pretraining. This decay in attention scores for distant tokens limits RoPE's effectiveness in handling extremely long input sequences, potentially impacting performance in tasks requiring long-range dependencies. According to the original paper [14], ALiBi surpasses T5 Bias and Rotary positional encodings in both training and inference speed, while performing comparably to Sinusoidal encodings.\nThe ALiBi method is straightforward. It assumes that as the distance between two tokens increases, their association decreases accordingly. Therefore, it penalizes attention scores based on the distance between the two tokens. A pre-defined bias matrix is added to the original attention score computation, which introduces a linear bias to the dot product between the query and key. This bias is an arithmetic sequence with a common difference of 1 and an initial term of -m(i \u2013 1):\nSoftmax(q_i K^T + m[\u2212(i \u2212 1), . . ., -2, -1, 0])\nOur experiments reveal the superior extrapolation ability of ALiBi, particularly in inference tasks like species classification that involve sequences ten times longer than those used during pretraining. Even though the pretraining phase utilized sequences of 12,000 bp, inference tasks were able to extend sequence lengths to 120,000 bp effectively. This aligns with observations in the original study [14], where the model's perplexity remained stable as inference token lengths increased. Notably, in the species classification task, only dnaGrinder and HyenaDNA (160K) successfully handled such long sequences on a single GPU, with dnaGrinder achieving a perfect classification accuracy of 100%, while HyenaDNA (160K) achieving a score of 64.22%. In contrast, models like DNABERT-2, NT-500M-1000g, NT-2500M-multi, and NT-50M-multi-V2 could not process sequences of this length, even with a batch size of 1 on a single GPU. To further illustrate dnaGrinder's extrapolation capability, we conducted GPU evaluations (Table 6) to determine the maximum token length it could handle across different GPUs."}, {"title": "Flash Attention 2", "content": "Flash Attention [22] is a fast and efficient vanilla attention enhanced by exploiting IO awareness to compute exact attention scores. Unlike sparse attention methods such as Big Bird [27] or approximated attention techniques like Linformer [28] and Performer [29], Flash Attention takes advantage of the different capacities and speeds of different memory types in GPUs to accelerate the overall attention computation. For example, SRAM is fast but has limited capacity, whereas High Bandwidth Memory (HBM) offers larger capacity but at slower speeds. By reducing the communication between these memory types, Flash Attention optimizes memory usage and improves computational efficiency.\nUnlike the Flash Attention Triton used in DNABERT-2, Flash Attention 2 [22] is twice as fast and optimized for inference, particularly for iterative decoding when the query is a short sequence (e.g., sequence length = 1). This improvement is especially beneficial for our model, as we train on long DNA sequences of 12,000 bp, but DNA sequences in many downstream tasks vary in length, with most not exceeding 1,000 bp. For instance, in DNABERT-2 downstream tasks, all sequences in the GUE dataset are shorter than 1,000 bp."}, {"title": "Architectural Enhancements", "content": "Beyond improving the primary methods of the model, we have also explored various latest architectural enhancements to optimize model performance. For instance, we experimented with different activation functions and further pretraining."}, {"title": "SwiGLU and GEGLU", "content": "DNABERT-2 replaces the ReLU activation function with GEGLU [15], a variant of GLU [30], which has been shown to boost the performance of Transformer models. However, the use of GEGLU increases the parameter size of our model from 63M to 110M due to the two separate linear transformations that the function uses. Specifically, the GELU activation function is applied to the first transformation, and the second transformation serves as a gating mechanism:\nGEGLU (x, W, V, b, c) = GELU(xW + b) \\otimes (xV + c)\nwhere W and V are the weight matrices, and b and c are the biases of the transformation. The symbol \\otimes represents element-wise multiplication, which modulates the output of the second transformation using the gating signal from the first. This structure leads to a significant increase in the number of parameters, as GEGLU requires separate linear transformations and associated biases for both the gating and output signals, which increases model capacity and computational complexity compared to ReLU and GELU.\nSwiGLU [15], another variant of GLU, prioritizes parameter size by simplifying the gate computation. To achieve parameter efficiency, SwiGLU utilizes a single linear transformation to compute the gating signal and applies this signal to the result of another linear transformation. This allows SwiGLU to maintain performance while reducing the complexity of the gating mechanism:\nSwiGLU(x, W, V, b, c, \\beta) = Swish_\\beta(xW + b) \\otimes (xV + c)\nwhere Swish is the Swish activation function with a parameter B, acting in place of GELU for the gating mechanism. Specifically, for an input dimension din and an output dimension dout, GEGLU requires 2 \u00d7 (din \u00d7 dout + dout) parameters. In contrast, SwiGLU achieves the same gating effect with a more parameter-efficient design, requiring only din \u00d7 dout + dout parameters, as the Swish activation allows for a more straightforward gate computation. This makes SwiGLU more parameter-efficient by simplifying gate computation without the additional weights and biases needed by GEGLU. Given the significant increase in parameter size introduced by GEGLU, we opted to use SwiGLU in our model, as it provides comparable performance while substantially reducing the model's overall complexity."}, {"title": "Further pretraining", "content": "Like DNABERT-2, we also explored further pretraining [31] using some downstream datasets. Our model was first pretrained on a general DNA dataset of multispecies reference genomes with human sequences updated with SNP variants. However, downstream classification tasks usually focus on specific regions of the genome, such as genic regions, to predict whether a sequence is a (core) promoter or contains a splicing site. These regions may have some intricate sequence features that the model needs to learn to deliver adequate performance.\nGiven that our model's pretraining sequences range from 729 to 2314 tokens in length, We employed in-domain further pretraining [31], where the model is further pretrained on all downstream datasets, including both the GUE and GUE-plus datasets from DNABERT-2, which contain 10 genomic problems including 36 classification tasks with sequences ranging from 70 to 10,000 bp. This approach contrasts with the further pretraining of DNABERT-2, which is constrained to only the GUE benchmark consisting of 28 classification tasks due to limitations on its pretraining input sequence length. Another difference from DNABERT-2 is that our model performed 100,000 steps, or about 0.41B tokens, of further pretraining, roughly equivalent to 3-4 epochs on the downstream datasets. In contrast, our model was only further pretrained for one epoch, processing approximately 0.176B tokens across 31,000 steps-about 70% fewer steps and 60% fewer tokens than DNABERT-2's further pretraining.\nSince DNABERT-2 plus, the further pretrained version, was not available for testing, we compared our model's performance with DNABERT-2 plus by using the MCC values for yeast epigenetic marks prediction tasks reported in the DNABERT-2 paper. We then calculated the MCC performance of our model and NT-v2-50M on these 10 classification tasks. The results (Table 5) show that even though our model was trained with fewer steps, it outperformed DNABERT-2 plus on half of the 10 tasks, achieving state-of-the-art performance on these tasks."}, {"title": "Datasets and Data Preparation", "content": "To facilitate effective training of the dnaGrider model, we constructed a comprehensive set of genomic sequences from multiple species."}, {"title": "Pretraining Datasets", "content": "To facilitate effective training of the dnaGrider model, we constructed a comprehensive set of genomic sequences from multiple species."}, {"title": "The human reference genome dataset", "content": "The latest Human Reference Genome (GRCh38.p14) covers approximately 92% of the human genome, encompassing 3.29 billion bp [32]. This comprehensive reference includes sequences from all autosomal, sex, and mitochondrial chromosomes. Although the first generation base model of Nucleotide Transformer (NT) replaces the reference genome sequence with 1000 Genome SNP data, introducing alterations to the DNA sequence, it retains 98% redundant content among the samples [33], which limits the NT's ability to learn from the diversity of the human genome.\nTo mitigate the impact of redundancy, we notice that there are abundant repetitive DNA sequences in genomic sequences. For example, about half of the human genome is repetitive [34]. Such repetitions complicate genomic analyses and mask significant genotypic variations. Therefore, we used the soft-masked assembly sequences from the UCSC Genome Browser to differentiate non-repeats and repeats identified by RepeatMasker [35] and Tandem Repeats Finder (with a period of 12 or less) [36].\nTo ensure that non-repetitive sequences constitute a substantial portion of each training sequence, we focused on preserving most non-repeating regions while minimizing the inclusion of repeating sequences. To the best of our knowledge, our approach is the first application in the context of genomic models. We initially removed all repetitive"}, {"title": "1000 Genome project data", "content": "The 1000 Genome Project dataset contains 3,202 samples, including 2,504 genomes of unrelated individuals and 602 samples from family trios [37]. These samples originate from 27 geographically structured populations representing African, American, East Asian, and European ancestries. The dataset utilizes the GRCh38.p14 version of the human reference genome as the template. This set of sequence data covers a total of 73,554,796 genetic variants, including filtered Single Nucleotide Variants (SNVs), insertions and deletions (INDELs), and Structured Variants (SVs) such as large deletions (DELs), insertions (INSs), duplications (DUPs), and inversions (INVs).\nTo achieve a broader and more diverse data augmentation, we downloaded the phased variant call format (VCF) files, where each allele includes one maternal allele and one paternal allele, representing the base pairs inherited from each parent. Unlike the Nucleotide Transformer dataset, which only includes SNVs and INDELs (<50 bp), our dataset also incorporates longer SVs (>50 bp). These SVs represent large-scale genetic alterations in the genome, which can significantly impact gene function and regulation, contributing to genetic diversity and disease susceptibility. To enhance data diversity, our training dataset construction considers both trails of alleles. We utilize both maternal and paternal genetic variants to generate our training data instead of considering only one trail at a time, like what Nucleotide Transformer did."}, {"title": "Multispecies reference genome data", "content": "The Multispecies Reference Genome dataset includes the reference genomes of 794 species, including a diverse array of organisms such as bacteria, fungi, invertebrates, protozoa, vertebrate mammals, and other vertebrates. We downloaded this dataset directly from NCBI, similar to Nucleotide Transformer, with the only difference being the exclusion of species with invalid reference links. In processing the data, any character different from a base pair, A, T, C, or G, was transformed into an 'N'. Each DNA chunk was processed to ensure all letters were in uppercase and restricted to base pairs or N, with any sequence containing 'N' being discarded at the end. This dataset forms the third sequence dataset for pretraining, providing a broad spectrum of genomic data across multiple species for comprehensive genomic studies."}, {"title": "Downstream Datasets", "content": "We utilized the GUE dataset from DNABERT-2, which consists of 28 sets of sequences for 7 classification tasks with sequence lengths ranging from 70 to 1000 bp. The seven genome sequence classification tasks we studied include core promoter detection, promoter detection, transcription factor prediction, and splice site detection for human sequences, transcription factor prediction for mouse sequences, epigenetic marks prediction for yeast sequences, and covid variant classification for virus sequences.\nIn addition, we incorporated downstream tasks from the Nucleotide Transformer (NT) model. Given that the GUE dataset and NT's downstream tasks overlap in the epigenetic mark prediction, and both datasets include tasks for promoter detection and splice site prediction (albeit with different data), we extended our evaluation to include two additional enhancer-related tasks from the NT model.\nTo further validate the capability of our model for handling sequences 10 times longer than those used in pretraining, we employed the species classification tasks from HyenaDNA. To ensure consistency and fairness, we selected the same five species used in HyenaDNA: hippo, human, lemur, mouse, and pig. We randomly sampled DNA sequences of"}, {"title": "Data Preparation", "content": "To enhance the model's ability to learn from diverse genomic data, we aimed to minimize redundancy by removing repetitive DNA sequences, which can impede the identification of key genomic features. These repetitive regions occupy a significant portion of the genome but offer little benefit to training, as they largely consist of duplicated content that lacks diversity. Our objective was to filter out these repetitive elements while preserving the most informative non-repetitive regions, ensuring that the input sequences were both relevant and met the necessary length for effective training."}, {"title": "Repeated and non-repeated content", "content": "Genomes across species contain repetitive sequences that are present multiple times within chromosomes. These repetitions, ranging from simple patterns like \"CGCGCG\" to more complex structures, can be categorized into different types, such as tandem repeats or interspersed repeats. Repeats give rise to redundancy and affect genome alignment and assembly, particularly during model pretraining, as they provide duplicated training tokens and position information. In other words, repeated sequences provide little information but incur extra computational burden to pretraining. Therefore, it is necessary to remove these repeats to focus on the unique and informative regions of the genome.\nIn processing the Human Reference Genome, we utilized the soft-masked assembly data. Initially, we removed all repeat regions, retaining only the non-repetitive sections and documenting their start and end positions. Upon analyzing these sequences, we observed that the majority were short and fragmented, with very few meeting the required input length for our model. To address potential issues associated with variations in input lengths, we implemented the following schemes:"}, {"title": "Filtering Short Sequences", "content": "We excluded non-repetitive sequences shorter than a specified length, which varied across chromosomes depending on the proportion of retained sequences relative to the total chromosome lengths. For instance, with a target sequence length of 12,200 bp, non-repetitive sequences shorter than 1,150 bp on chromosome 1 of the human genome were filtered out. This approach ensured that during the subsequent sequence extension phase, we avoided scenarios where short non-repetitive sequences constituted only a small fraction of the final sequence, thus avoiding excessive redundancy."}, {"title": "Extension", "content": "a) Rightward Extension: We started the process at position 0 on a selected chromosome, identifying the rightmost index of the first valid non-repetitive sequence. If this sequence was shorter than 12,200 bp, it was then extended to the right along the chromosome until it was 12,200 bp long. If this extension included one or more non-repetitive sequences, the subsequent operation began from the next non-repetitive sequence to the right that had not yet been included. Sequences exceeding 12,200 bp were split to ensure that each segment adhered to this length requirement.\nb) Handling 'N' Characters: In cases where an 'N' (representing unidentified bases) was encountered during rightward extension, the extension was halted, and the sequence was extended to the left to reach the required length of 12,200 bp. Given that 'N' constitutes only 5% of the human reference genome, such unidentified base pairs rarely appear on each chromosome. We did not observe any cases where the presence of 'N' prevented reaching the target length of 12,200 bp.\nAfter filtering and extension, the final retained set of sequences on chromosome 1 was equivalent to 50% of the original content, reflecting the proportion of non-repeated sequences on this chromosome. Although our data included some repeated sequences, we avoided fragmentation and redundancy. Our approach ensured that non-repeated content formed a substantial part of each training sequence, maximizing the inclusion of meaningful, non-redundant genomic data. The proportions of retained content per human autosomal chromosome plus the X chromosome, were as follows: [0.50, 0.50, 0.55, 0.53, 0.52, 0.51, 0.54, 0.56, 0.47, 0.56, 0.53, 0.54, 0.46, 0.44, 0.44, 0.49, 0.52, 0.51, 0.55, 0.56, 0.50, 0.57, 0.50], which was first introduced in [34].\nIn addition, considering that repetitive regions can also contain regulatory elements or genetic variants, we incorporated the complete human reference genome within the multispecies dataset. This inclusion was intended to fill potential gaps in the training data by providing the model with a thorough representation of human genomic features. Despite this inclusion, the proportion of the human genome constituted only 2.7% of the whole multispecies dataset, thereby minimizing the risk of excessive repetition while ensuring that the model benefits from a broad spectrum of genomic"}, {"title": "Parental genetic variants locus replacement", "content": "Upon obtaining human reference genome sequences of length 12,200 bp, we constructed the final pre-training set by extracting sequences of 12,000 bp long from these sequences with their starting positions randomly chosen from the first 0 to 199 bp.\nSubsequently, we randomly selected an individual from the 3,202 samples of the 1,000 Genomes dataset. We then identified all SNVs, INDELs, and SVs (Figure 2.a) of this individual that fell within this extracted 12,000 bp sequence. We replaced these variants at their corresponding positions on this extracted sequence [38]. In this replacement, because INDELs (<50 bp) and SVs (>50 bp) are variants of varying lengths, the final length of each 12,000 bp sequence will be different, especially considering that the start index is randomly selected from the first 200 bp. This approach achieves data augmentation by ensuring that the sequences vary significantly.\nIn contrast to the Nucleotide Transformer-500M-1000g model, which covers only SNVs and INDELs from just one parental lineage (either maternal or paternal, a detail not clarified in their paper), our approach incorporates variants from both maternal and paternal origins (Figure 2.a & 2.b). In other words, each extracted 12,000 bp sequence includes two parallel sequences of the maternal and paternal variants. This dual consideration is essential because 18.8% of the sequences of the 1000 Genome project are from family trios, and genetic variations from both parents contribute to the individual's overall genetic makeup. By including variants from both maternal and paternal origins, we aim to capture a more comprehensive representation of genetic variability and enhance the model's ability to account for inherited genetic differences."}, {"title": "Results", "content": "We compared dnaGrinder against five top-performing DNA foundation models to assess its performance comprehensively: HyenaDNA, DNABERT-2, NT-500M-1000g, NT-2500M-multi, and NT-50M-multi-V2.\nGiven that our pretraining data are from multispecies reference genomes and the human genome (version GRCh38) updated with 1000G SNP variants, we included DNABERT-2, NT-2500M-multi, and NT-50M-multi-V2, which were pretrained using multispecies reference genomes. We also included NT-500M-1000g, which was pretrained on the human GRCh38 genome sequences updated with 1000G SNP variants to align with our dataset.\nThe inclusion of NT-50M-multi-V2 in our comparison was motivated by the fact that it is representative of the second-generation NT models. It incorporates enhancements such as rotary positional encoding, the SwiGLU activation function, and the removal of MLP biases and dropout mechanisms\u2014similar features are used in our model. To the best of our knowledge, this is the first study to compare a model like ours with the second-generation NT.\nAdditionally, we included HyenaDNA in the comparison because it is a decoder-only model and employs a similar sequence length warmup strategy to ours during pretraining."}, {"title": "Setup and Metric", "content": "We assessed the models based on two criteria: computational efficiency and performance on downstream tasks. For computational efficiency, we compared the relative Floating-Point Operations (FLOPs)\u2014the sum of multiplication and addition operations performed during a forward pass. FLOPs were calculated using the H3 dataset from the yeast epigenetic marks prediction task, with sequences of 500 bp long. To assess performance, we used two metrics: Accuracy on 20 application tasks and Matthews Correlation Coefficient (MCC) specifically for the 10 yeast epigenetic marks prediction tasks, for a total of 30 tasks evaluated. Since the DNABERT-2 plus models were not publicly released, using MCC allows us to directly compare dnaGrinder's performance with the reported performance of DNABERT-2 plus in [7] (Table 5). This combination of metrics enables a comprehensive evaluation of each model's computational efficiency and task-specific performance."}, {"title": "Results on the GUE Benchmark and Enhancer Tasks", "content": "Table 2 summarizes the performance of six models compared by five evaluation metrics. Notably, dnaGrinder secured the top position in 11 tasks and ranked second in 12 tasks out of 30, achieving the highest overall performance among the six models evaluated. dnaGrinder outperforms the largest, state-of-the-art model NT-2500M-multi in the number of the top-2 tasks and outperforms the second state-of-the-art model DNABERT-2 in the average scores, while significantly surpassing other baselines (Table 2). This demonstrates dnaGrinder's exceptional efficiency and scalability in genomic sequence modeling without compromising performance.\nAmong the total 28 GUE benchmark problems, dnaGrinder achieved the best or second-best results in 21 tasks, ranked the best among all methods evaluated (Table 3). The dominance of dnaGrinder over other baselines is particularly notable in the human and mouse transcription factor prediction problems, reaching the highest or second-highest ACC scores in all ten tasks. Furthermore, dnaGrinder also achieved the highest or second-highest MCC prediction scores in 8 out of 10 tasks, showing strong performance on epigenetic marks prediction tasks. Despite only reaching the second-highest ACC score in 2 out of 6 core promoter and promoter detection tasks, dnaGrinder is ~40 times fewer in parameters and runs ~29 times fewer FLOPs when compared with NT-2500M-multi that achieved the highest ACC scores in 4 out of 6 core promoter and promoter detections tasks. This result indicated that dnaGrinder offered a favorable tradeoff between FLOPs, parameters, and model prediction tasks for promoter-related problems.\nWhile dnaGrinder performs similarly to DNABERT-2 and NT-50M-multi-V2 on the enhancer prediction task (Table 4), it achieved an accuracy of 68.50 in the enhancer type prediction task, surpassing the second-best model, NT-50M-multi-V2, by 4.75 points. This result showed dnaGrinder's superior ability to distinguish between different enhancer types, highlighting its robustness in handling"}]}