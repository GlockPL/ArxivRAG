{"title": "Interactive Visual Assessment for Text-to-Image Generation Models", "authors": ["Xiaoyue Mi", "Fan Tang", "Juan Cao", "Qiang Sheng", "Ziyao Huang", "Peng Li", "Yang Liu", "Tong-Yee Lee"], "abstract": "Visual generation models have achieved remarkable progress in computer graphics applications but still face significant challenges in real-world deployment. Current assessment approaches for visual generation tasks typically follow an isolated three-phase framework: test input collection, model output generation, and user assessment. These fashions suffer from fixed coverage, evolving difficulty, and data leakage risks, limiting their effectiveness in comprehensively evaluating increasingly complex generation models. To address these limitations, we propose DyEval, an LLM-powered dynamic interactive visual assessment framework that facilitates collaborative evaluation between humans and generative models for text-to-image systems. DyEval features an intuitive visual interface that enables users to interactively explore and analyze model behaviors, while adaptively generating hierarchical, fine-grained, and diverse textual inputs to continuously probe the capability boundaries of the models based on their feedback. Additionally, to provide interpretable analysis for users to further improve tested models, we develop a contextual reflection module that mines failure triggers of test inputs and reflects model potential failure patterns supporting in-depth analysis using the logical reasoning ability of LLM. Qualitative and quantitative experiments demonstrate that DyEval can effectively help users identify max up to 2.56 times generation failures than conventional methods, and uncover complex and rare failure patterns, such as issues with pronoun generation and specific cultural context generation. Our framework provides valuable insights for improving generative models and has broad implications for advancing the reliability and capabilities of visual generation systems across various domains.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent years have witnessed remarkable advances in visual generation models [2], [27], [30]. These models show impressive capabilities in generating vivid images and have found applications across diverse computer graphics domains [9], [12], [37], [40]. Despite their powerful capabilities, they still exhibit various generalization failures when deployed in real-world computer graphics scenarios. Given the vast range of images these models can generate, the potential generalization failures are highly varied and unpredictable [6]. Thoroughly testing these models to find significant failures reliably is crucial for enhancing their performance, yet this remains a major challenge.\nCurrent assessment approaches primarily rely on datasets and require significant manual effort, including sequential isolated three phases: the collection of test inputs, model generation, and human assessment of model outputs [1], [15], [18]\u2013[20], [31], [33], [36], [38]. These fashions are widely applied but difficult to evolve. As visual generation models advance and application scenarios expand, static datasets may become outdated, necessitating substantial resources to update and maintain their relevance. This limited scalability also manifests in the fixed coverage of test inputs, which restricts their ability to detect model deficiencies beyond predefined scenarios.\nInteractive testing methods have emerged as a promising alternative to address above limitations. Du et al. [6] propose an adversarial attack method for text-to-image models, which is adaptive based on tested model feedback but narrowly focuses on evaluating the robustness of noun changes in white-box models. Meanwhile, open-ended human-in-the-loop testing [10], [11], [28], [29] leverages Large Language Models (LLMs) and interactions with evaluators to generate challenging data for testing models on coherent, manually specified topics. Although effective in text-input text-output tasks or image understanding tasks, these methods cannot be directly applied to the visual generation domain due to the complexity of visual generation failures [3]. Evaluators will have a huge cost to mark failures of generated images in detail.\nIn this paper, we introduce DyEval, a novel interactive visual assessment framework that leverages an LLM through in-context learning to adaptively generate test inputs based on model feedback, effectively identifying failures in open domains (Fig. 1). DyEval features an intuitive visual interface that enables users to interac- tively explore and analyze model behaviors while maintaining a comprehensive overview of the testing process. DyEval is built upon a tree-based structure systematically recording the testing process. Each tree node contains test topics, specific prompts as test inputs, and corresponding evaluation results. The framework operates through two main components: First, in the test node construction phase, the LLM generates test inputs based on the current test topic and existing contexts. Evaluators then assess the generated images through the interactive interface, determining whether they pass or fail the specified criteria. Second, DyEval performs adaptive exploration based on the evaluation results. For nodes with high pass rates, the LLM proposes new test topics for deeper tree exploration. For nodes with lots of failures, DyEval employs contextual reflection, combining dynamic failure location to identify failure-inducing parts with a self-reflection module analyzing potential failure types and reasons based on existing test contexts. Benefiting from the dynamic interactivity of DyEval, we can also avoid the risk of data leakage [25], [26], which typically arises from public static test sets leading to targeted model optimizations and, consequently, skewed evaluation results.\nExperimentally, we showcase the efficacy of DyEval by evaluating various state-of-the-art text-to-image models from four perspectives: Object, Relation, Attribute, and Context (global attributes in the image, such as style). Quantitative experiments demonstrate that DyEval significantly outperforms traditional static evaluation methods, identifying max up to 2.56 times more failure cases when evaluating the same number of text-image pairs. Our experiments also reveal consistent patterns across different generation models. All tested models perform better with material objects over abstract ones, static over dynamic attributes, and explicit over implicit relations. While excelling at style-related tasks, they struggle with culture and knowledge-based generations, particularly with cultural nuances and implicit relations. In Parts- of-Speech (POS) analysis of text inputs, we find SDXL and SD3 show improvements over SD1-5 and SD2-1, but they continue to struggle with specific linguistic elements such as quantifiers and pronouns. Through case studies, DyEval uncovers unexpected testing perspectives and intricate failure patterns. The contextual reflection module identifies specific triggers for model failures, such as culturally specific words (e.g., \"kimono\") and certain text combinations. These findings provide valuable insights into the current limitations and potential areas for improvement in text- to-image generation models. In summary, DyEval offers a novel perspective in text-to-image model testing, significantly aiding evaluators in comprehensively understanding the boundaries of model capabilities and inspiring future enhancements. In this paper, our main contributions are listed below:\n\u2022 We propose DyEval, a novel dynamic interactive visual assessment framework for adaptive testing of text-to-image models, that enable efficient human-in-the-loop assessment and analysis. DyEval can flexibly adapt to any user-defined criteria like alignment, bias, and more, for both white-box and black-box models.\n\u2022 With the LLM-powered iteration system and interactive visual interface, DyEval gradually adapts tests based on user feedback to find a variety of model failures. Compared with static testing methods [?], [20], DyEval allows limitless test"}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 Assessments of Visual Generation Models", "content": "In the field of visual generation, there has been a significant increase in evaluation studies over the past years, especially in text-to-image generation. Currently, static evaluation [1], [15], [18]\u2013[20], [31], [33], [36], [38], [41] are the predominant method for evaluation, which employs a one-time assessment using pre-collected data sets, which can show the generalization ability of the model on specific data. Firstly, evaluators determine testing perspectives such as concept conjunction [23], and spatial relationships [35]. Then, they gather different test prompts according to their testing perspectives. There are four main ways that models get their prompts: existing text-image datasets [21], human usage records from online text- to-image tools like Midjourney [18], [31], [33], semi-automated generation using LLMs with predefined templates [1], [15], and combinations of them [19], [20], [36], [38]. Finally, evaluators assess the model outputs using established metrics and manual evaluation. Due to the high challenge of image generation tasks, manual assessment is indispensable and remains the most reli- able [16].\nThese works only provide static evaluations, they are unable to adapt to evolving models and human needs. Du et al. [6] propose a gradient-based adversarial attack method for text-to-image models, dynamically investigating the adversarial robustness of model inputs by adding or changing nouns. However, their approach is hard to generalize in the open domain.\nIn this paper, we offer a novel adaptive visual assessment method that enables users to specify test themes (e.g. culture, knowledge, count, and more), test aspects (e.g. alignment, bias, and more), and test metrics (e.g. Clip score, human assessment, and more) in the open domain and adjust the testing process interactively based on the model assessment feedback. This evaluation approach is useful in accommodating the open domain and evolving capabilities of the text-to-image models and can be seen as complementary aspects of current evaluations in text-to- image models, jointly enhancing the reliability and practical utility of the models.\nSeveral studies [4], [5], [14], [24] have utilized large language models (LLMs) as metrics to evaluate text-image alignment in text- to-image tasks. For instance, LLMScore [24] converts images into both image-level and object-level visual descriptions, subsequently employing LLMs to assess the alignment between the generated images and their corresponding texts through evaluation instruc- tions. X-IQE [4] uses large visual language models to generate textual explanations of generated image and text alignment. Several studies [5], [14] employ Visual Question Answering (VQA) to assess the alignment between text and images. In these approaches, a language model is utilized to automatically generate multiple question-answer pairs based on a given text input. The faithfulness of the image is then evaluated by determining whether existing VQA models can accurately respond to these questions using the corresponding image."}, {"title": "2.2 Human-LLM Collaborative Model Testing", "content": "Leveraging LLMs for human-aided evaluation, i.e. open-ended human-in-the-loop testing, has been well-recognized in text-input text-output tasks [10], [28], [29] and image-input understanding tasks [11]. They adopt strategies from software engineering, engaging individuals to generate test scenarios with assistance from LLMs. Utilizing human intervention to explore input sce- narios beyond conventional training and validation datasets, this testing methodology has effectively identified consistent failures in state-of-the-art models, even performing unproblematic on static benchmarks.\nFor text-input text-output tasks, they manually specify testing topics, using the powerful text generation, rich knowledge, and logical reasoning abilities of LLMs to create test inputs. Then, evaluators select and evaluate model errors manually and use this feedback to generate new test inputs. Similarly, text-to-image is also a text-input task, and its evaluation heavily depends on manual assessment, with no similar testing methods yet in the field.\nHowever, these methods cannot be directly applied to the text-to-image domain due to the complexity of image generation failures. Considering the complex dimensions outputs result in many types of failure [3] in image generation tasks, it would be highly costly for humans to detailly mark these failures. In this paper, we explore human-aided evaluation in the text-to-image task and introduce a contextual reflection module to alleviate this problem by automatically identifying minimal failure test inputs and conducting analysis based on LLMs."}, {"title": "3 DESIGN OBJECTIVES", "content": "Based on our analysis of limitations in current visual assessment fashions, we identify four key design objectives for our visual assessment framework for text-to-image models:\nO1 Dynamic coverage. Our visual assessment framework should overcome the fixed coverage limitation of static testing by supporting open-ended exploration of test scenarios. The ideal framework should enable visual exploration of dynamically gener- ated test cases across different aspects. Through human assessment feedback, Our framework can interactively adjust testing granularity based on discovered failure patterns. The visualization framework should support visual navigation between breadth-first exploration for comprehensive coverage and depth-first investigation for specific issues, enabling users to continuously expand the testing boundary without being constrained by predefined scenarios.\nO2 Evolutionary adaptability. To address the evolution difficulty in static datasets, the visual assessment framework should facilitate dynamic visualization of testing strategies that adapt based on model feedback and user needs. Through interactive visualizations and visual feedback loops, users can continuously refine test cases without requiring complete dataset reconstruction, ensuring the testing process remains effective as models advance.\nO3 Anti-leakage design. To prevent the data leakage issues common in static testing, the visual assessment framework should generate test inputs dynamically rather than fixed test sets. And test input distribution is broad enough to make it difficult to target optimization overfitting even if one knows how the test inputs are generated. Through visual monitoring of customizable"}, {"title": "4 METHODS", "content": "In this section, we present the formulation and overall pipeline in Sec. 4.1. Subsequently, we provide detailed descriptions of the two primary steps of DyEval: testing node construction (Sec. 4.2) and deepening node exploration (Sec. 4.3)."}, {"title": "4.1 Overview", "content": "Formulation. A test is defined as a combination of a text input i and the expected outputs X of a text-to-image model m that meet evaluator preferences and intentions [11], [28], [29]. These preferences include aesthetic appeal, semantic alignment, level of detail, fairness, and more. We define a bug as a specific text input where the visual generation model consistently fails to generate an accurate corresponding image. For each test pair (x, i) where x \u2208 X, the test(x, i) passes if x meets the evaluation expectations; otherwise, it fails. Assuming a distribution of output images given test inputs P(X|i, m), a test(i) fails when the pass rate is lower than the evaluator-expected pass rate p, termed as a bug:\n$\\mathbb{E}_{x \\sim P(X|i, m)}[test(x, i) \\text{ passes }] < \\rho.$\\nWe then define a topic as a coherent group of tests whose text prompts are united by an explainable concept [8], [34] and share similar expectations [11], [28]. Each topic(to) corresponds to a set of text inputs I that fit the specific topic to (e.g., \"dogs\"). For example, I contains text inputs related to dogs, and i \u2208 I. The text-to-image model m generates output images X based on these inputs:\n$topic(t_o) = \\{test(x, i | t_o, m) | i \\in I, x \\in X\\}.$\nDyEval aims to assist evaluators in revealing test topics and test inputs with high failure rates, thereby accurately finding the capability boundaries of the tested model.\nThe full process of DyEval is recorded as a test tree T with a maximum test depth $d_{max}$ and a maximum test width $W_{max}$, which also represents test contexts. A test node $t_{d,w} \\in T$ is defined as:\n$t_{d,w} = \\{t_{o_{d,w}}, topic(t_{o_{d,w}}), result_{d,w}, r_{d,w}\\}$,\nwhere d represents the index of depth, w represents the index of width, r signifies the reflection conducted by the LLM for each node, and resultd,w denotes the assessment results of test \u2208 topicd.w. For convenience, we also define fd,w as the parent node of td,w and cha,w as the children nodes of td,w. Additionally, several hyperparameters must be set for testing, including the number of test topics generated per iteration nt, the number of test inputs generated per iteration ni, and the number of images generated per test input nx."}, {"title": "4.2 Testing Node Construction", "content": ""}, {"title": "4.2.1 Testing Prompt Generation", "content": "In constructing a testing node, DyEval begins by utilizing the strong text generation ability of LLM to generate ni text prompts I based on the current topic. As illustrated in Fig. 2, these prompts correspond to the current test topic, such as \"DOG- human relationships.\" An example of a generated prompt is \"A dog wagging its tail while its owner scratches its belly.\" If the test tree T includes additional information related to the current topic, such as parent nodes f, these are also incorporated as test contexts for the LLM. DyEval inspires the LLM to generate test inputs that are semantically aligned with the topic, aiming to identify new potential failures in the model being tested. This can be formulated as:\n$I_{d,w} = InputGen(t_{o_{d,w}}, f_{d,w}),$\nwhere InputGen is a process that LLM takes the current topic tod.w and parent node fd,w as test contexts input and output testing inputs for text-to-image models. The prompt details for this function are provided in the supplementary material. To maintain semantic relevance, we verify the generated test inputs with the LLM and eliminate duplicates using an n-gram matching algorithm.\nAfter the generation of test inputs, the tested model m generates nx images per input, and evaluators will determine pass or fail based on the specific criteria. Given the complexity of image generation tasks, manual evaluation is essential and remains the most reliable method [16]. To reduce the labeling effort, we use CLIPScore [13] as an initial filter; any text-image pair with a CLIPScore below a certain threshold is automatically marked as a fail. To avoid mislabelling these results are also shown to the users, and they can refine as needed. Finally, these test processes are systematically recorded in the test tree T, with updates made to the relevant nodes td,w within T."}, {"title": "4.2.2 Contextual Reflection", "content": "For low average pass rate test nodes, we design a contextual reflection module to analyze further possible failure patterns of the text-to-image model m. This module consists of two components: a dynamic failure location that attributes failures to the smallest text component, called a failure trigger, and self-reflection for summarization and analysis by the logical ability of LLM.\nDynamic failure location. When a test test(x, i) fails, it is both costly and highly subjective for evaluators to label failures in detail. However, analyzing failure reasons and exploring the weaknesses of the test model is crucial for understanding and improving the model. To address this, we devise a divide-and-conquer strategy, called dynamic failure location, as illustrated in Fig. 2 and Alg. 1. This approach iteratively breaks down the text input i to pinpoint failure triggers. Initially, the input is split into two halves and tested. If both halves fail, further splitting continues. If both pass, one half is selected for further splitting and merged with the other one separately for testing.\nTo facilitate precise analysis and control of the text input and ensure semantic coherence after decomposition, we employ an LLM to transform text input i into a scene graph representation c [17]. Scene graph, as a structured representation method, explic- itly describes objects, their attributes, and relationships between objects within a scene. Converting visual content into scene graphs represents a crucial goal in visual understanding. The process Ftest subsequently transforms these scene graphs back into text inputs for evaluating text-to-image models, which is implemented using LLM..\nAs detailed in Alg. 1, after converting the test input i into scene graph co, dynamic failure location divides it into two subsets, C1 and C2. These subsets are then tested by converted into texts to determine if they meet evaluator requirements. If a subset fails, it undergoes further subdivision. If both pass, one is retained as a baseline while the other is further subdivided. All results are stored in the test node t for future reflection. Additional examples and specific task prompts of Ftest and Fsplit, and other details are available in the supplementary material.\nSelf-reflection. Leveraging the logical reasoning and analysis ability of LLM [32], [39], DyEval tries to analyze and summarize failure patterns and reasons for tested model m. The reflection, denoted as rd,w, is generated by the LLM using the current test node td,w from the test tree T:\n$r_{d,w} = Reflect(t_{d,w}),$\nwhere Reflect is a process where the LLM uses all available information from the current node t to analyze potential failure patterns. This reflective process allows DyEval to conduct a thor- ough analysis of failed test data, offering a deeper understanding of model performance and identifying potential areas for improvement. The specific prompt used is detailed in the supplementary materials."}, {"title": "4.3 Deepening Layer Exploration", "content": "For test nodes with a high average pass rate, the next step involves adaptively generating further exploration topics based on the current test context. While labeling text-image pairs is easy for humans, generating new topics is challenging and cost high. Therefore, we offload this creative task to an LLM, as illustrated in Fig. 2 on the right. The capabilities of LLM in language generation, knowledge, and summary analysis are leveraged to generate suggested test topics for the next layer using the current testing node td,w as context:\n$to_{d+1} = TopicGen(t_{d,w}),$\nwhere tod+1 is a set of nt test topics for children nodes of td,w. These new topics automatically are created as new nodes in T with parent node td,w and updated as the new nodes ta+1. By default, DyEval recommends test topics based on breadth-first, but evaluators can manually change the order for further exploration in the next testing loop. For instance in Fig. 2, after testing the node on the topic \"DOG-human relationships\", the LLM suggests a new set of child topics, such as \"Interactions between dogs and owners.\", \"The role of a dog in a family setting.\", and \"The role of a therapy dog.\u201d.\nBy deepening layer exploration, DyEval facilitates the genera- tion of subsequent test topics and inputs, guided by the evolving test tree T, until reaches a predefined limit of testing loops. This approach broadens the testing scope by suggesting more fine- grained topics related to the current one, enabling evaluators to refine their focus for future tests."}, {"title": "5 EXPERIMENTS", "content": "In this section, we start by introducing experimental settings in Sec. 5.1, and then conduct both qualitative and quantitative experiments on DyEval to answer the following questions: (1) Can DyEval find bugs effectively in the models being tested, especially compared with static methods (Sec. 5.2)? (2) What are the differences and similarities in failures among various text- to-image models (Sec. 5.3)?"}, {"title": "5.1 Experimental Settings", "content": "Hyper-parameters of DyEval. We evaluated four major open-source text-to-image models: Stable Diffusion v1-5 (SD1-5) [30], Stable Diffusion v2-1 (SD2-1) [30], Stable Diffusion XL (SDXL) [27], and Stable Diffusion v3 (SD3) [7]. Our study involved 23 participants, all proficient in English, without color blindness, and trained for the task. And remuneration to participants in excess of the local minimum hourly wage. In our participant evaluation process, we randomly reintroduce previous text-image pairs to test annotation consistency. The inter-rater reliability analysis of participants' assessments on text-image pairs yielded Kendall's tau coefficient of T = 0.7746 (p < 0.05), indicating strong agreement among raters after training. The parameters used were: number of test topics per iteration nt = 3, number of test inputs per iteration ni = 5, number of images generated per test input nx = 4, and max depth dmax = 3. We set the topic stop extension pass rate p to 0 to analyze text-to-image models fully. The LLM we used is GPT-3.5-turbo which is developed by OpenAI and used widely in agent and LLM applications [32], [39]. Detailed prompts in the supplementary material.\nAssessment criteria. Here we set the assessment criteria as text-image alignment and visual quality. A test(x, i) passes if the"}, {"title": "6 CONCLUSION AND DISCUSSION", "content": "We present DyEval, an interactive visual assessment framework that addresses the limitations of static datasets in evaluating rapidly evolving text-to-image models. Through extensive experiments, we demonstrate that our LLM-powered approach significantly outper- forms traditional evaluation methods by dynamically generating diverse test cases and providing interpretable insights into model behaviors. The visual analytics components of DyEval enable users to effectively explore and understand complex failure patterns, particularly in challenging scenarios involving cultural nuances, implicit relations, quantifiers, and pronouns.\nOur framework makes several key contributions to the visual analytics and computer graphics communities: it introduces a novel approach to adaptive model assessment that can evolve alongside advancing generation technologies; what's more, it demonstrates the effectiveness of combining LLM capabilities with interactive visualization for comprehensive model evaluation, and finally it provides a flexible foundation for analyzing various aspects of visual generation models.\nLimitations. Due to resource constraints, our current evalu- ation focuses primarily on open-source models, and we hope to experiment with commercial models like DALLE 3 to uncover more cutting-edge phenomena. The effectiveness of the framework partially depends on the quality of the underlying LLM, which may introduce biases in test case generation. Additionally, the current implementation requires significant human involvement in the assessment process, which could be resource-intensive for large-scale evaluations.\nFuture work. We envision extending the framework to evaluate other visual generation tasks, particularly text-to-video generation, to support cross-modal generation systems. Another promising di- rection is investigating the integration of Vision-Language Models (VLMs) as a more cost-effective alternative to current LLMs for test case generation and analysis. When text-to-image generation metrics become more advanced, we plan to incorporate them into DyEval to reduce manual evaluation effort while maintaining assessment quality. Furthermore, we aim to develop automated bug repair mechanisms to enable self-improving generation models."}]}