{"title": "Do We Need to Verify Step by Step? Rethinking Process Supervision from a Theoretical Perspective", "authors": ["Zeyu Jia", "Alexander Rakhlin", "Tengyang Xie"], "abstract": "As large language models have evolved, it has become crucial to distinguish between process supervision and outcome supervision-two key reinforcement learning approaches to complex reasoning tasks. While process supervision offers intuitive advantages for long-term credit assignment, the precise relationship between these paradigms has remained an open question. Conventional wisdom suggests that outcome supervision is fundamentally more challenging due to the trajectory-level coverage problem, leading to significant investment in collecting fine-grained process supervision data.\nIn this paper, we take steps towards resolving this debate. Our main theorem shows that, under standard data coverage assumptions, reinforcement learning through outcome supervision is no more statistically difficult than through process supervision, up to polynomial factors in horizon. At the core of this result lies the novel Change of Trajectory Measure Lemma a technical tool that bridges return-based trajectory measure and step-level distribution shift. Furthermore, for settings with access to a verifier or a rollout capability, we prove that any policy's advantage function can serve as an optimal process reward model, providing a direct connection between outcome and process supervision. These findings suggest that the empirically observed performance gap if any between outcome and process supervision likely stems from algorithmic limitations rather than inherent statistical difficulties, potentially transforming how we approach data collection and algorithm design for reinforcement learning.", "sections": [{"title": "1 Introduction", "content": "Reward signals play a central role in reinforcement learning, and it has been hypothesized that intelligence and its associated abilities could emerge naturally from the simple principle of reward maximization (Silver et al., 2021). Over the past decade, this idea has been powerfully demonstrated across diverse AI systems. In specialized domains like AlphaGo Zero (Silver et al., 2017), superhuman performance has been achieved by maximizing simple, well-defined environmental reward signals. The paradigm has also proven transformative for general-purpose AI systems, particularly in training large language models (LLMs) using reinforcement learning (Ouyang et al., 2022; Bai et al., 2022; Jaech et al., 2024). However, for these more open-ended systems, the challenge of reward specification is significantly more complex, requiring reward signals to be learned from human-annotated data through reward modeling rather than being manually specified.\nThis challenge of reward specification has led to the emergence of two fundamental supervision paradigms in reinforcement learning (e.g., Uesato et al., 2022; Lightman et al., 2023):\n\u2022 Outcome supervision: Reward feedback is provided only after the final output, based on the final outcomes or in the case of LLMs\u2014overall quality of the model's chain-of-thought (CoT).\n\u2022 Process supervision: Fine-grained reward feedback is provided based on the quality of each intermediate step (e.g., correctness of each step in the CoT in the case of LLMs).\nThe choice between these paradigms represents a fundamental trade-off in reinforcement learning system design. Process supervision offers several intuitive advantages: it provides more granular feedback, enables better interpretability of model decisions, and potentially allows for more efficient credit assignment in"}, {"title": "1.1 Our Results", "content": "This paper examines the statistical performance of reinforcement learning under outcome supervision-an emerging paradigm that has garnered significant attention in large language model research. Our findings challenge conventional wisdom that outcome supervision is inherently more difficult than process supervision due to its coarser feedback.\n(1) Our main results (Section 3) demonstrate that given a dataset of trajectories with only cumulative rewards (as in outcome supervision), we can transform the data into trajectories with per-step rewards, while only paying an additive error that scales with the state-action concentrability. As a result, we can transform any algorithm that takes trajectories with per-step rewards as input into an algorithm that takes trajectories with total rewards as input, with essentially no loss in statistical performance up to polynomial factors of the horizon.\n(2) We also provide (Section 4) a theoretical analysis of using Q-functions or advantage functions as reward functions, a popular approach in practice for mimicking process supervision from outcome supervision data (e.g., Wang et al., 2024; Luo et al., 2024; Setlur et al., 2024). We prove that the advantage function of any policy can serve as an optimal process reward model; in contradistinction, using Q-functions could lead to sub-optimal results.\nBeyond these two main messages, we make the following contributions:\n(1) Our key technical contribution-Change of Trajectory Measure Lemma (Lemma 3) -is applicable be-yond our main results. The change of measure is a fundamental operation in analyzing off-policy evaluation (e.g., Uehara et al., 2020), offline reinforcement learning (e.g., Xie et al., 2021a), and out-of-domain generalization (Dong and Ma, 2022). To our knowledge, Lemma 3 presents the first result concerning trajectory-level change of measure via step-level distribution shift.\n(2) We also extend our main results to the setting of preference-based reinforcement learning (Section 3.4). Namely, we transform preference-based trajectory data, generated according to the Bradley-Terry model, into a dataset of trajectories with per-step reward. In particular, for direct preference opti-mization (Rafailov et al., 2023), we improve the previous analyses and show that its sample complex-ity only scales with the state-action concentrability coefficients instead of trajectory concentrability coefficients potentially, an exponential improvement."}, {"title": "1.2 Notation", "content": "We use $a_n \\lesssim b_n$ or $a_n = O(b_n)$ whenever there exists some universal positive constant c such that $a_n \\leq c\\cdot b_n$.\nWe use $\u03c3: \\mathbb{R} \\rightarrow [0, 1]$ to denote the sigmoid function $x \\rightarrow \u03c3(x) = \\exp(x)/(1 + \\exp(x))$ ."}, {"title": "2 Background", "content": "In this section, we introduce key prerequisite concepts. We begin with the basics of Markov Decision Processes (Section 2.1). We then discuss the two aforementioned supervision paradigms in reinforcement learning (Section 2.2). Finally, we review the concepts of state-action and trajectory concentrability (Section 2.3)."}, {"title": "2.1 Markov Decision Processes", "content": "An MDP M consists of a tuple $(S, A, P, r^*, H)$. Here $H \\in \\mathbb{Z}^+$ denotes the horizon, $S = \\bigcup_{h=1}^H S_h$ denotes the layered state space, A denotes the action space, $P = (P_1,\\dots, P_H)$ with $P_h : S_h \\times A \\rightarrow \\Delta(S_{h+1})$ denoting the transition model, and $r^* : S \\times A \\rightarrow [0, 1]$ is the deterministic ground truth reward model. For simplicity, we let $s_1 \\in S_1$ be a fixed initial state.\nFor a trajectory $\u03c4 = (s_1, a_1, s_2, a_2,\\dots, s_H, a_H)$, we write $r(\u03c4) := \\sum_{h=1}^H r(s_h, a_h)$ to denote the total reward accumulated along the trajectory under deterministic reward model $r: S \\times A \\rightarrow \\mathbb{R}$ (which can be the ground truth reward model $r^*$ or any learned reward model). A (Markov) policy $\u03c0 : S \\rightarrow \\Delta(A)$ is a mapping from the state space S to a distribution on the action space A. The notation $\\Pr_{\u03c4 \\sim \u03c0}$ and $\\mathbb{E}_{\u03c4 \\sim \u03c0}$ stands for the probability and expectation with respect to trajectories \u03c4 sampled according to policy \u03c0 within the transition model given by P, starting from a fixed state $s_1$. For any given policy \u03c0, the occupancy measure of any state $s_h \\in S_h$ in layer h and any state-action pair $(s_h, a_h) \\in S_h \\times A$ are defined, respectively, as $d^\u03c0(s_h) := \\Pr_{\u03c4 \\sim \u03c0}(s_h \\in \u03c4)$ and $d^\u03c0(s_h, a_h) := \\Pr_{\u03c4 \\sim \u03c0}((s_h, a_h) \\in \u03c4)$. Additionally, we define the trajectory occupancy measure for any trajectory $\u03c4$, $d^\u03c0(\u03c4) := \\Pr_{\u03c4' \\sim \u03c0}(\u03c4 = \u03c4')$, as well as $\u03c0(\u03c4) := \\prod_{(s_h, a_h) \\in \u03c4} \u03c0(a_h \\mid s_h)$ (note that $d^\u03c0(\u03c4)$ and $\u03c0(\u03c4)$ are different when transitions is stochastic). For any policy \u03c0, we use $J(\u03c0)$ to denote the expected total reward of trajectories collected by under the ground truth reward $r^*$, i.e., $J(\u03c0) := \\mathbb{E}_{\u03c4 \\sim \u03c0}[r^*(\u03c4)]$. For a specific reward model r, we use $J_r(\u03c0)$ to denote the expected total reward of trajectories collected by under reward r, i.e., $J_r(\u03c0) := \\mathbb{E}_{\u03c4 \\sim \u03c0}[r(\u03c4)]$. We assume that $r^*(\u03c4) \\in [0, 1]$ for any trajectory \u03c4."}, {"title": "2.2 Outcome Supervision and Process Supervision", "content": "This paper focuses on two basic supervision paradigms in reinforcement learning: process supervision and outcome supervision. We analyze these approaches through the lens of statistical complexity rather than algorithmic implementation details.\nThe distinction lies in the temporal resolution of available reward signals:\n\u2022 Process supervision provides step-wise rewards during trajectory collection. More precisely, the offline data has the form\n$D_p := \\{(s_1, a_1, r_1, s_2, a_2, r_2,\\dots, s_H, a_H, r_H)\\},$  (1)\nwhere $r_h = r^*(s_h, a_h)$ denotes the ground truth reward value at step h. This setting is compelling compared to outcome supervision, especially for complex multi-step reasoning problems, as it provides more precise feedback that can pinpoint the location of suboptimal actions and allows to correct for cases where the agent makes mistakes in the middle of the reasoning path but reaches the correct final answer (Uesato et al., 2022; Lightman et al., 2023).\n\u2022 Outcome supervision reveals only the cumulative rewards for complete trajectories. The data for this setting has the form\n$D_o := \\{(s_1, a_1, s_2, a_2,\\dots, s_H, a_H, R)\\},$  (2)"}, {"title": "2.3 State-Action Coverage and Trajectory Coverage", "content": "The coverage condition-typically referred to as a bounded concentrability coefficient (Munos, 2003; Antos et al., 2008; Farahmand et al., 2010; Chen and Jiang, 2019; Jin et al., 2021; Xie and Jiang, 2021; Xie et al., 2021a; Bhardwaj et al., 2023)-has played a central role in the theory of offline (or, batch) reinforcement learning, and has recently gained growing attention in online reinforcement learning through a related concept of a coverability coefficient (Xie et al., 2022b; Liu et al., 2023; Amortila et al., 2024a,b).\nIn this paper, we use coverage conditions to capture the statistical complexity of different supervision paradigms. To motivate the importance of coverage notions, consider the following approach for imputing the missing rewards in outcome supervision. Suppose we minimize the trajectory-level regression objective over a class of reward functions $\\mathcal{R} = \\{r : S \\times A \\rightarrow \\mathbb{R}\\}$,\n$\\hat{r} = \\arg \\min_{r \\in \\mathcal{R}} \\sum_{(\u03c4, R) \\in D_o} (r(\u03c4) - R)^2,$  (3)\nwhere the outcome supervision data $D_o$ are collected by some reference policy $\u03c0_{off}$. With the learned reward model $\\hat{r}$, we can now employ an offline RL method of our choice. However, we have a (standard in the literature) mismatch: while $|\\sum_{h=1}^H \\hat{r}(s_h, a_h) - \\sum_{h=1}^H r^*(s_h, a_h)|$ is small for trajectories collected by $\u03c0_{off}$, we care about the error $|J(\u03c0) - J_{\\hat{r}}(\u03c0)|$ for some policy that differs from $\u03c0_{off}$, where $J(\u03c0)$ and $J_{\\hat{r}}(\u03c0)$ are defined in Section 2.1. A naive approach for capturing such a change of trajectory measure is to use the trajectory concentrability coefficient,\n$C_{traj}(\u03c0, \u03c0_{off}) := \\sup_{\u03c4} \\frac{d^\u03c0(\u03c4)}{d^{\u03c0_{off}}(\u03c4)},$  (4)\nwhere the supremum is over all possible trajectories.\nThis trajectory concentrability coefficient is usually considered to be prohibitively large, and its limitation has been widely studied in the literature on off-policy evaluation (e.g., Liu et al., 2018; Xie et al., 2019; Nachum et al., 2019; Uehara et al., 2020). An alterative approach is to express upper bounds (if possible) in terms of the state-action concentrability coefficient, commonly used in offline policy learning literature (e.g., Munos, 2003; Antos et al., 2008; Farahmand et al., 2010; Chen and Jiang, 2019) and defined as follows:\n$C_{sa}(\u03c0, \u03c0_{off}) := \\max_{h \\in [H]} \\sup_{(s_h, a_h) \\in S_h \\times A} \\frac{d^\u03c0(s_h, a_h)}{d^{\u03c0_{off}}(s_h, a_h)}$  (5)"}, {"title": "3 Outcome and Process Supervision: Similar Statistical Guarantees", "content": null}, {"title": "3.1 Learning a Reward Model from Total Reward", "content": "We present a simple approach to estimate rewards in an outcome supervision dataset of the form Eq. (2) using least squares regression, assuming that the learner has access to a class of reward models $\\mathcal{R}$. This transformation allows the learner to use outcome supervision data with methods designed for process reward data, as detailed below. We have the following theorem for the least squares estimate of the rewards:\nTheorem 1. Suppose the dataset $D_o$ is collected i.i.d. according to policy $\u03c0_{off}$ in the MDP $\\mathcal{M} = (S, A, P, r^*, H)$ with the ground truth reward model $r^* \\in \\mathcal{R}$. Then, with probability at least $1 - \u03b4$, for any policy \u03c0, the PRM reward model computed by Eq. (3) satisfies\n$|J(\u03c0) - J_{\\hat{r}}(\u03c0)| \\leq \\frac{H^{3/2} \\cdot C_{sa}(\u03c0, \u03c0_{off}) \\cdot \\log(|\\mathcal{R}|/\u03b4)}{\\sqrt{|D_o|}},$\nwhere $C_{sa}(\u03c0, \u03c0_{off})$ is the state-action concentrability coefficient defined in Eq. (5).\nThe proof of Theorem 1 is deferred to Appendix A.2. Theorem 1 yields an approach for transforming any offline RL algorithm which takes trajectories with per-step reward into an offline RL algorithm which takes trajectories with total reward as input. More precisely, we split the outcome supervised data, use the first part to estimate the reward function via least squares, and then use this estimate to impute the missing rewards on the second part of the data. We summarize this basic transformation in the following algorithm."}, {"title": "Algorithm 1 Offline Outcome-to-Process Transformation", "content": "1: Input: Offline dataset with total rewards $D_o = \\{(s_1, a_1,\\dots, s_H, a_H, R)\\}$, Reward model class $\\mathcal{R}$, Offline RL Algorithm A\n2: Split $D_o$ into two datasets $D_o^1$ and $D_o^2$ of equal size.\n3: Computer $\\hat{r}$ by solving Eq. (3) with dataset $D_o^1$ and the reward class $\\mathcal{R}$.\n4: Construct dataset $D_p = \\{(s_1, a_1, r_1,\\dots, s_H, a_H, r_H)\\}$ from $D_o^2$, where $\u2200(s_1, a_1,\\dots, s_H, a_H, R) \\in D_o^2$,\n$r_h = \\hat{r}(s_h, a_h), \u2200 h \\in [H]$.\n5: Call algorithm A with dataset $D_p$ and output learned policy $\\hat{\u03c0}$.\n6: Output: Learned policy $\\hat{\u03c0}$.\nNotice that the bound in the above theorem suffers from all-policy concentrability, regardless of which algorithm A is used with the transformed data. This occurs because the transformation fixes the learned reward model, requiring us to account for the distribution shift between $\u03c0_{off}$ and the data-dependent policy $\u03c0$ which can be any policy in the policy class \u03a0. This is a common issue in classical offline RL without specific methods like pessimism, particularly for the case of partial coverage. However, the concept of pessimism can also be applied to the outcome supervision setting in our paper, where we learn a specific reward model for each policy, as commonly done in the (process-supervision) offline RL literature (e.g., Xie et al., 2022a; Cheng et al., 2022; Uehara and Sun, 2021; Bhardwaj et al., 2023). Following this approach, we can transform model-based offline RL algorithms that use pessimism (Xie et al., 2022a; Bhardwaj et al., 2023) into algorithms that employ outcome supervision data. The sample complexity of these transformed algorithms scales with the single-policy concentrability $C_{sa}(\u03c0^*, \u03c0_{off})$, which depends only on the optimal policy $\u03c0^*$. We defer further details to Appendix B."}, {"title": "Statistical Efficiency", "content": "We now argue that there is no significant statistical edge for process supervision paradigm compared to the outcome supervision paradigm in the offline setting. The latter corresponds to standard offline RL problems (Levine et al., 2020; Jiang and Xie, 2024), for which a rich body of work exists analyzing sample complexity. Our \"equivalence\" argument primarily focus on coverage conditions, since different coverage notions (e.g., state-action-level vs. trajectory-level) can lead to exponential differences, as discussed in Section 2.3. While our results establish equivalence with respect to coverage conditions, we acknowledge they may still be subject to polynomial factors of H; removing such factors is an avenue for further research.\nIf we consider the worst-case scenario, it is easy to see that any algorithm which outputs an $\u03b5$-optimal policy requires at least $\\sup_\u03c0 C_{sa}(\u03c0, \u03c0_{off})/\u03b5^2$ number of samples. To see this, we may consider the two-armed bandit with action $a_1$ and $a_2$, and $r(a_1) = \u00b1\u03b5$, $r(a_2) = 0$, and policy $\u03c0 = \\text{Unif}\\{a_1, a_2\\}$. In the meantime, many classical offline RL algorithms, such as Fitted Q-Iteration (Antos et al., 2008; Munos and Szepesv\u00e1ri, 2008), the theoretical backbone of Deep Q-Network (DQN), require sample complexity that scales with $C_{sa}(\\Pi, \u03c0_{off})/\u03b5^2$ for obtaining an $\u03b5$-optimal policy (Chen and Jiang, 2019; Xie and Jiang, 2020). Hence Corollary 2 provides a transformation with the same sample complexity as in these works (up to polynomial in horizon factors), when encountering outcome supervision reward data.\nAs for the instance-dependent case (corresponding to the single-policy coverage discussed in Section 2.3), the lower bound result in Xie et al. (2021b) shows that any algorithm requires at least $C_{sa}(\u03c0^*, \u03c0_{off})/\u03b5^2$ number of samples to output an $\u03b5$-optimal policy, which only depends on the coverage of optimal policy $\u03c0^*$. Recent offline RL algorithms (Xie et al., 2021a; Cheng et al., 2022; Uehara and Sun, 2021; Bhardwaj et al., 2023) indeed reach that sample complexity in terms of the single-policy concentrability $C_{sa}(\u03c0^*, \u03c0_{off})$. Our results presented in Appendix B match the upper bound of these offline RL algorithms for the process supervision case and also enjoy the same sample complexity depending on the single-policy concentrability $C_{sa}(\u03c0^*, \u03c0_{off})$ ."}, {"title": "3.2 Change of Trajectory Measure", "content": "The proof of Theorem 1 relies on the following key change of trajectory measure lemma, which states that changing the measure of trajectory returns can be done at the price of state-action concentrability, up to logarithmic and polynomial-in-horizon factors. This lemma will be used for bounding the error between the true reward model $r^*$ and the learned reward model $\\hat{r}$. Thus, by setting $f = r^* - \\hat{r}$, we only need to show that the expectation of the absolute value of $f(\u03c4) := \\sum_{(s_h, a_h) \\sim \u03c4} f(s_h, a_h)$ is small for $\u03c4 \\sim \u03c0$ when controlling under $\u03c0_{off}$.\nLemma 3. (Change of Trajectory Measure Lemma) For MDP $\\mathcal{M} (S, A, T, f, H)$ with any function $f : S \\times A \\rightarrow [-1, 1]$, for any two policies $\u03c0$ and $\u03c0_{off}$, we have\n$\\frac{\\mathbb{E}_{\u03c4 \\sim \u03c0}[f(\u03c4)^2]}{\\mathbb{E}_{\u03c4 \\sim \u03c0_{off}}[f(\u03c4)^2]} \\leq H^3 C_{sa}(\u03c0, \u03c0_{off}) \\log(H C_{sa}(\u03c0, \u03c0_{off})),$  \nwhere $C_{sa}(\u03c0, \u03c0_{off})$ is defined in Eq. (5). Additionally, the following holds without the extraneous log factors:\n$\\mathbb{E}_{\u03c4 \\sim \u03c0} [|f(\u03c4)|] \\leq \\sqrt{H^3 C_{sa}(\u03c0, \u03c0_{off}) \\cdot \\mathbb{E}_{\u03c4 \\sim \u03c0_{off}}[f(\u03c4)^2]}$.\nThis lemma reveals a perhaps surprising insight: when the squared sum of some state-action value functions $[\\sum_{(s_h, a_h) \\sim \u03c4} f(s_h, a_h)]^2$ is small under the off-policy trajectory distribution $\u03c4 \\sim \u03c0_{off}$, we only need to account for state-action-level distribution shifts between $\u03c0_{off}$ and $\u03c0$ to bound the same squared sum under $\u03c4 \\sim \u03c0$. This holds true even though controlling such trajectory sums theoretically cannot prevent cases where individual terms have equal and large magnitude but opposite signs (i.e., where $|a| = |b| > 0$ but $a + b = 0$). We provide a proof sketch of Lemma 3 in the following. The detailed proof is deferred to Appendix A.1."}, {"title": "3.3 Proof Sketch of Lemma 3", "content": "In this section, we outline the key insights behind the proof of Lemma 3. The central observation is that controlling the trajectory-level variance of $f$ under a reference policy $\u03c0_{off}$ implies an automatic control of variance on prefixes and suffixes over the entire trajectory, with only a polynomial overhead in the horizon length H. This seemingly simple fact leads to perhaps surprisingly strong guarantees.\nInsight I: Trajectory-level bound controls the second moment on prefixes and suffixes. At first glance, small value $|f(\u03c4)|$ over the entire trajectory \u03c4 does not obviously guarantee that the value of $f$ on either (i) every prefix $\u03c4_{1:h}$ or (ii) every suffix $\u03c4_{h+1:H}$ is small. In principle, large positive and large negative portions of a single trajectory could \"cancel\" each other out, resulting in a small overall sum $|f(\u03c4)| = |f(\u03c4_{1:h}) + f(\u03c4_{h+1:H})|$.\nCrucially, however, thanks to the Markov property, we can argue that if f has small second moment (under $\u03c0_{off}$) and if a state $s_h$ is visited sufficiently often by $\u03c0_{off}$, f cannot have high variance on the prefix (leading up to $s_h$) and suffix (following $s_h$). Indeed, if the value of f on the prefix (or suffix) has large variance, then conditioned on passing through $s_h$ that is visited sufficiently often by $\u03c0_{off}$, the value of f on the entire trajectory also has large variance, which directly implies the large variance (hence, large second moment) of $f(\u03c4)$. Hence, even though the trajectory-level bound looks coarse, it forces each state $s_h$ to have relatively stable partial sums in both the prefix and suffix directions under $\u03c0_{off}$.\nInsight II: Layer-by-layer \"locking\u201d with only state-action coverage. Next, we want to argue that if all states in a trajectory satisfy the above low variance property (we call such states \"good\" states), then the reward of the entire trajectory cannot have large absolute value. We call this the \"locking in\" property here for brevity. In the following, we argue that \"locking in\" happens with high probability, even under policy \u03c0.\nAccording to the earlier argument, \"bad\" states (opposite of \"good\" states) cannot have large visitation probability under $\u03c0_{off}$. Then, by the definition of $C_{sa}(\u03c0, \u03c0_{off})$, which upper bounds the probability ratio between $\u03c0$ and $\u03c0_{off}$ at any state, we conclude that such bad states also have low probability under $\u03c0$, up"}, {"title": "3.4 Extension to Preference-Based Reinforcement Learning", "content": "In the previous section, we studied the statistical complexity of outcome supervision under the data format of Eq. (2), where the outcome reward is provided at the end of each trajectory. Preference-based rein-forcement learning (e.g., Knox and Stone, 2008; Akrour et al., 2012; Wirth et al., 2017) represents another well-established paradigm that extends outcome supervision and is commonly employed for learning from human preferences (e.g., Griffith et al., 2013; Christiano et al., 2017; Stiennon et al., 2020; Ouyang et al., 2022).\nRecent work on implicit reward modeling through single-step contrastive learning approaches (DPO; Rafailov et al., 2023) aims to eliminate the need for explicit reward modeling. While extensive prior work (e.g., Zhan et al., 2023; Liu et al., 2024; Song et al., 2024; Zhang et al., 2024) has focused on the sample complexity of these implicit reward modeling approaches, most existing bounds rely on trajectory-level change of measure. These results are also considered to depend on the trajectory concentrability under naive simplifications, which can grow exponentially with the horizon length (see detailed discussion in Section 2.3 and Section 3.2 of Xie et al. 2024).\nIn this section, we extend our main results to the preference-based reinforcement learning setting. As a direct application of our Change of Trajectory Measure Lemma (Lemma 3), we first provide a sample complexity bound of preference based RL which only scales with state-action concentrability instead of trajectory concentrability, a result applicable to standard explicit reward modeling approaches as well as implicit reward modeling approaches (i.e., DPO).\nIn preference-based RL, we suppose that for any trajectory \u03c4, the total reward along \u03c4 satisfies $r(\u03c4) \u2208 [0, V_{max}]$. To form the dataset $D$ of preferences, the learner collects two reward-free trajectories $\u03c4 = (s_1, a_1, s_2, a_2,\\dots, s_H, a_H)$ and $\u03c4' = (s'_1, a'_1, s'_2, a'_2,\\dots, s'_H, a'_H)$ according to policy $\u03c0_{ref}$, and receives the infor-mation about the order $(\u03c4_+, \u03c4_-)$ of \u03c4, \u03c4', based on the preference $y \\sim P(\u03c4 \\succ \u03c4')$. We adopt the Bradley-Terry model (Bradley and Terry, 1952):\n$P(\u03c4 \\succ \u03c4') = \\frac{\\exp(r(\u03c4))}{\\exp(r(\u03c4)) + \\exp(r(\u03c4'))}.$  (6)\nThe labeled preference dataset D consists of ordered samples $(\u03c4_+, \u03c4_-)$, where both trajectories are collected according to policy $\u03c0_{ref}$ and labeled according to the Bradley-Terry model Eq. (6)."}, {"title": "3.4.1 Improved Analysis of Preference-Based RL with Explicit Reward Modeling", "content": "We first provide the analysis for the case where an explicit reward modeling procedure is used for preference-based RL. Suppose the learner is given the preference-based dataset $D_{pref}$ and a reward class $\\mathcal{R}$, where for every reward model $r \u2208 \\mathcal{R}$ and any trajectory \u03c4, $r(\u03c4) \u2208 [0, V_{max}]$. In the following result, an analogue of Theorem 1, we transform the preference-based dataset into the reward model via maximum likelihood rather than the method of least squares.\nTheorem 4. Suppose $D_{pref} = \\{(\u03c4_+, \u03c4_-)\\}$ contains i.i.d. pairs of sequences collected according to $\u03c0_{ref}$ and ordered according to Eq. (6) with $r^* \u2208 \\mathcal{R}$. Let\n$\\hat{r} = \\arg \\min_{r \\in \\mathcal{R}} \\sum_{(\u03c4_+, \u03c4_-) \\in D_{pref}} \\log \u03c3(r(\u03c4_+) - r(\u03c4_-))$\n1 Our formal proof also needs to consider the \"good\" state-action-state tuples, which are similar to \"good\" states but involve the $(s_h, a_h, s_{h+1})$ tuple. We omit the details here for brevity, and readers can refer to the full proof in Appendix A.1."}, {"title": "3.4.2 Improved Analysis of DPO Algorithm", "content": "We now extend our main results to the implicit reward modeling setting and analyze the sample complexity of the DPO algorithm (Rafailov et al., 2023). DPO is a popular implicit reward algorithm that converts the two-step process of reward modeling and policy optimization into a single-step contrastive learning problem. DPO is commonly used in the token-level setup of LLMs, where actions (tokens) are directly appended to states (contexts) (Rafailov et al., 2023, 2024). In this case, the state-action concentrability coefficient essentially reduces to trajectory-level concentrability, as the last state is contains the trajectory. However, recent work indicates that DPO-style algorithms are applicable beyond the token-level setup, e.g., in environments with deterministic transition dynamics but still Markovian states, e.g., in robotics (Hejna et al., 2023; Xie et al., 2024). In these settings, our bounds with only state-action concentrability can be substantially tighter than existing trajectory-level ones.\nFollowing Xie et al. (2024), we assume deterministic ground-truth transition dynamics and consider the following KL-regularized objective (Xiong et al., 2023; Ye et al., 2024; Xie et al., 2024): for some positive number \u03b2,\n$I_\u03b2(\u03c0) := J(\u03c0) - \u03b2D_{KL}(\u03c0(\u03c4) || \u03c0_{ref}(\u03c4))$\n$= \\mathbb{E}_{\u03c4 \\sim \u03c0} [r^*(\u03c4) - \u03b2 \\log \\frac{\u03c0(\u03c4)}{\u03c0_{ref}(\u03c4)}]$.  (8)\nThe policy $\u03c0^*_\u03b2$ which maximizes $I_\u03b2(\u03c0)$ in Eq. (8) satisfies $\u03c0^*_\u03b2(\u03c4) \u221d \u03c0_{ref}(\u03c4) \\exp(r^*(\u03c4)/\u03b2)$ for any trajectory \u03c4. It is easy to verify that $\u03c0^*_\u03b2$ is a Markov policy. We assume the learner has access to a Markov policy class $\u03a0 \u2282 \\mathcal{A}^S$, and aims to find a policy $\\hat{\u03c0}$ that is nearly optimal with respect to the policy class \u03a0, i.e.\n$\\max_{\u03c0 \\in \u03a0} I_\u03b2(\u03c0) - I_\u03b2(\\hat{\u03c0}) \\leq \u03b5.$\nThe DPO algorithm (Rafailov et al., 2023) takes the dataset $D = \\{(\u03c4_+, \u03c4_-)\\}$ as input, and outputs the policy $\\hat{\u03c0} \u2208 \u03a0$ which maximizes the log likelihood, i.e.,\n$\\hat{\u03c0} \\leftarrow \\arg \\min_{\u03c0 \\in \u03a0} \\sum_{(\u03c4_+, \u03c4_-) \\in D} \\log \u03c3(\u03b2 \\log \\frac{\u03c0(\u03c4_+)}{\u03c0_{ref}(\u03c4_+)} - \u03b2 \\log \\frac{\u03c0(\u03c4_-)}{\u03c0_{ref}(\u03c4_-)}).$  (9)\nIn the following, we provide a refined analysis of the sample complexity of this algorithm. We first make the following assumptions."}, {"title": "4 Advantage Function Learning with Rollouts", "content": "In this section, we analyze a common empirical strategy for converting outcome-supervised data into process supervision by leveraging online rollouts. The central observation is that, given access to an environment that returns final outcomes, one can initiate rollouts from individual state-action pairs and use the resulting outcomes to approximate their \"quality.\" Multiple works have adopted variations of this idea, relying on Q-functions (e.g., Wang et al., 2024), advantage functions (e.g., Setlur et al., 2024), or other specialized value estimators (e.g., Luo et al., 2024).\nAlthough these methods have demonstrated empirical promise, their theoretical properties remain relatively unexplored. Establishing a theoretical foundation could reveal the assumptions and conditions under which these methods are effective and enable principled comparisons to alternative reward modeling approaches. In what follows, we present (to our knowledge) the first theoretical study of advantage-based reward learning with online rollouts. We show that the advantage function of any policy can serve as a valid process-based reward model, recovering the same optimal policy as the original environment. By contrast, we also prove a lower bound indicating that simply using the Q-function can fail: in certain cases, the Q-function-based reward model produces suboptimal or undesired policies."}, {"title": "4.1 Algorithm and Upper Bounds", "content": "For MDP is $\\mathcal"}]}