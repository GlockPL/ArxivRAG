{"title": "Questioning Internal Knowledge Structure of Large Language Models Through the Lens of the Olympic Games", "authors": ["Juhwan Choi", "YoungBin Kim"], "abstract": "Large language models (LLMs) have become a dominant approach in natural language processing, yet their internal knowledge structures remain largely unexplored. In this paper, we analyze the internal knowledge structures of LLMs using historical medal tallies from the Olympic Games. We task the models with providing the medal counts for each team and identifying which teams achieved specific rankings. Our results reveal that while state-of-the-art LLMs perform remarkably well in reporting medal counts for individual teams, they struggle significantly with questions about specific rankings. This suggests that the internal knowledge structures of LLMs are fundamentally different from those of humans, who can easily infer rankings from known medal counts. To support further research, we publicly release our code, dataset, and model outputs\u00b9.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are widely used for various natural language processing tasks, owing to their outstanding performance and vast knowledge base (Zhao et al., 2023; Minaee et al., 2024). However, understanding their internal knowledge structures remains challenging due to their black-box architecture (Singh et al., 2024). While previous research has made progress in understanding the characteristics of LLMs (Zhao et al., 2024; Xiao et al., 2024; Weller-Di Marco and Fraser, 2024; Liu et al., 2024; Nowak et al., 2024), their internal knowledge organization remains less explored (Templeton et al., 2024). In this paper, we aim to address the following question: \u201cDo LLMs organize their internal knowledge similarly to humans?\u201d\nTo investigate this, we examine the performance of LLMs using Olympic medal tallies from 1964"}, {"title": "2 Analysis Design", "content": ""}, {"title": "2.1 Data Collection", "content": "We first gathered the official medal tables from the Olympic Games website\u00b2, covering events from the 1960 Rome Olympics to the 2024 Paris Olympics\u00b3."}, {"title": "2.2 Task Configuration", "content": ""}, {"title": "2.2.1 Medal QA", "content": "Based on the collected data, we designed a question-answering (QA) task focused on obtaining the exact medal results for a specific team in a particular Olympic Games. For this, we constructed prompts for the LLMs in the following format: \u201cHow many medals did $TEAM get in the $YEAR $LOCATION $SEASON Olympics? Only provide the number of each medal.\". Appendix A.1 demonstrates provides an example of a complete conversation with an LLM based on this prompt.\nTo create questions for this task, we excluded the 2024 Paris Olympics as it is too recent to be included in the training data of LLMs, as well as the 1960 Summer and Winter Games, which were used as examples, as discussed in Section 2.3. This resulted in a total of 596 questions for the medal QA task.\""}, {"title": "2.2.2 Team QA", "content": "SThe second task focuses on asking the model to identify the team that achieved a specific ranking in a given Olympic Games. We constructed prompts for this task in the following format: \u201cWhich country ranked $RANK in the $YEAR $LOCATION $SEASON Olympics? Only provide the name of the country.\". Appendix A.2 provides a complete example of a conversation with an LLM based on this prompt.\nAs with the Medal QA task, we excluded the 2024 and 1960 Olympic Games from our raw data. Additionally, we limited our questions to the top 10 teams and excluded cases with joint rankings to avoid complications\u2075. This resulted in 304 questions for the team QA task.\""}, {"title": "2.2.3 Doubt Robustness", "content": "In addition to the two tasks described above, we also investigated the robustness of the models when faced with simple user feedback expressing doubt, such as \"Really?\". For this, we attached the following prompt after the model's response for each task: \"Really? Start the answer with \"Yes\" or \"No\". If you answer with \"No\", then provide the correct number of each medal/correct country name.\". This allowed us to observe the model's second response and measure its robustness in handling user doubt."}, {"title": "2.3 Experimental Setup", "content": "We used 12 different models, covering SOTA-level proprietary models and open-source models. Specifically, we used GPT (OpenAI, 2023, 2024), Claude (Anthropic, 2024), and Gemini (Google, 2024) models as proprietary models and LLaMA-3.1 (Dubey et al., 2024), Qwen-2 (Yang et al., 2024a), and Gemma-2 (Team et al., 2024) as open-source models. Figure 1 includes the exact version of the model we used for our experiment.\nWe experimented with each model with two-shot examples to facilitate the models to follow the prompt and produce responses in the desired format. Specifically, we used the results from the 1960 Rome and Squaw Valley Olympics. Note that these two-shot examples only contribute to the formatting of the output and do not provide useful clues to answer the given question, as we excluded 1960 games from our question data. The sample conversation in Appendix A.1 and A.2 includes the two-shot examples.\nWe implemented the experiment with LangChain (LangChain, 2023) and vLLM (Kwon et al., 2023) library. We used official API for proprietary models and vLLM for open-source models. We set the temperature of every model to 0, disabling the probabilistic language modeling, thus easing the reproduction of the experimental results. Please refer to our source code and data for more details."}, {"title": "3 Experimental Results", "content": ""}, {"title": "3.1 Performance Gap between Medal QA and Team QA", "content": "Figure 1 illustrates the results of our analysis. The most noticeable finding is the significant performance gap between the two tasks. While prior studies have suggested that LLMs often produce hallucinated responses when dealing with numerical data, our anal-ysis shows that SOTA-level LLMs such as GPT-4o, GPT-4-turbo, Claude-3.5-Sonnet, and Gemini-1.5-Pro demonstrate remarkable accuracy in retrieving the number of medals won by a specific team (Rawte et al., 2023, 2024).\nHowever, in the Team QA task, no model achieved an accuracy higher than 40%. The best performance came from GPT-4o-2024-08-06, which achieved an initial accuracy of 39.8%. This is particularly interesting since, for humans, inferring rankings from known medal counts is relatively straightforward. The underperformance of LLMs in this task suggests that, during pretraining, they may not organize or link related information in a structured manner, unlike humans.\nIn conclusion, our findings indicate that the internal knowledge structures of LLMs differ from those of humans. Furthermore, the models' inability to link related information efficiently during pretraining appears to hinder their ability to answer related queries. This observation highlights a fundamental limitation of the next-token prediction approach, which is the dominant method for training LLMs (Bachmann and Nagarajan, 2024)."}, {"title": "3.2 Evaluating Doubt Robustness with Doubt Matrix", "content": "Another key finding is the performance drop observed after user feedback expressing doubt. In Figure 1, the diamond and reversed triangles indicate the accuracy of the models' final responses after receiving doubtful feedback, as described in Section 2.2.3. In most cases, the models' performance declined when they altered their initial answers, even though the initial responses were correct. This suggests that LLMs are vulnerable to user doubt, even when no evidence supports the claim that the initial answer was wrong. Nonetheless, more recent models, such as GPT-4o and Claude-3.5-Sonnet, showed only minor differences in this regard. We denote the amount of this performance drop as doubt robustness and suggest that doubt robustness is another noteworthy factor for the evaluation of LLMs, as it is important to keep the original response and decision without the reason to alter it, to ensure the reliability of the model.\nTo explore this phenomenon further, we created a doubt matrix, similar to a confusion matrix, to analyze response changes in greater detail. We categorized responses into four cases: (1) correct initial and final responses, (2) correct initial but incorrect final responses, (3) incorrect initial and final responses, and (4) incorrect initial but correct final responses. Figure 2 shows an example of a doubt matrix, and Appendix C provides doubt matrices for all models across the two tasks. The doubt matrix shows that at least 28 responses, or 4.7% of total responses, changed after receiving doubtful feedback. Notably, there were more cases where"}, {"title": "4 Related Works", "content": "Researchers have investigated the internal functioning of LLMs using various approaches. Early studies in this field focused on the emergence of internal structures to process linguistic features such as syntax (Teehan et al., 2022). Another study explored how LLMs represent relationships between entities, showing that such relations can be approximated using a single linear transformation (Hernandez et al., 2024). Additionally, other researchers"}, {"title": "5 Conclusion", "content": "In this study, we explored the internal knowledge structure of LLMs using Olympic Games medal tallies. By analyzing the models' performance across two distinct tasks\u2014medal QA and team QA-we identified a significant disparity between their ability to recall numerical data (medal count) and their struggle to infer rankings, which is based on the medal counts. This suggests that while LLMs are adept at retrieving specific factual information, they may not organize or link related knowledge as humans do.\nAdditionally, we revealed a vulnerability in LLMs when exposed to doubtful user feedback. In many cases, models altered their correct initial responses, leading to degraded performance, which underscores the concept of doubt robustness. This issue reflects the models' vulnerability to user prompts that challenge their answers without evidence.\nOur findings highlight fundamental differences in how LLMs and humans organize knowledge, and they emphasize the need for further research into enhancing the robustness of LLMs. Future work could explore methods to better structure the internal knowledge of LLMs, making them more capable of handling related queries and less prone to altering correct answers due to unsupported challenges. We believe that incorporating graph-based approaches during pretraining may help improve LLMs' ability to organize and connect information, thereby enhancing their overall efficiency, both in terms of data usage and computational resources (Pan et al., 2024)."}, {"title": "Limitations", "content": "It should be noted that the experimental result in this paper does not indicate that LLMs do not have the reasoning ability to infer rankings given medal counts as input prompts. Various techniques such as chain-of-thought may be helpful for inferring rankings in such conditions (Wei et al., 2022; Kojima et al., 2022). Instead, we focus on the internal knowledge that LLMs organized during pretraining, without such sophisticated prompt design. This internal knowledge base is crucial for ensuring the quality of the LLM response, as the generated response may be affected by internal prior of the LLM, although the relevant information is given as input prompt (Jin et al., 2024b).\nAdditionally, we acknowledge that we did not suggest a method to alleviate the performance gap between medal QA and team QA tasks or improve the doubt robustness of LLMs. Instead, the purpose of this paper is to shed light on the importance of the internal knowledge structure of LLMs, thereby facilitating future studies in this direction. We hope this paper to become the cornerstone for future research."}]}