{"title": "Efficient Long Video Tokenization via Coordinated-based Patch Reconstruction", "authors": ["Huiwon Jang", "Sihyun Yu", "Jinwoo Shin", "Pieter Abbeel", "Younggyo Seo"], "abstract": "Efficient tokenization of videos remains a challenge in training vision models that can process long videos. One promising direction is to develop a tokenizer that can encode long video clips, as it would enable the tokenizer to leverage the temporal coherence of videos better for tokenization. However, training existing tokenizers on long videos often incurs a huge training cost as they are trained to reconstruct all the frames at once. In this paper, we introduce CoordTok, a video tokenizer that learns a mapping from coordinate-based representations to the corresponding patches of input videos, inspired by recent advances in 3D generative models. In particular, CoordTok encodes a video into factorized triplane representations and reconstructs patches that correspond to randomly sampled (x, y,t) coordinates. This allows for training large tokenizer models directly on long videos without requiring excessive training resources. Our experiments show that CoordTok can drastically reduce the number of tokens for encoding long video clips. For instance, CoordTok can encode a 128-frame video with 128\u00d7128 resolution into 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar reconstruction quality. We further show that this efficient video tokenization enables memory-efficient training of a diffusion transformer that can generate 128 frames at once.", "sections": [{"title": "1. Introduction", "content": "Efficient tokenization of videos remains a challenge in developing vision models that can process long videos. While recent video tokenizers have achieved higher compression ratios [1, 2, 9, 52, 61, 62] compared to using image tokenizers for videos (i.e., frame-wise compression) [43, 67], the vast scale of video data still requires us to design a more efficient video tokenizer.\nOne promising direction for efficient video tokenization is enabling video tokenizers to exploit the temporal coherence of videos. For instance, video codecs [26, 28, 32, 41] extensively utilize such coherence for video compression by"}, {"title": "2. Method", "content": "In this section, we present CoordTok, a scalable video tokenizer that can efficiently encode long videos. In a nutshell, CoordTok encodes a video into factorized triplane representations [20, 64] and learns a mapping from randomly sampled (x, y, t) coordinates to pixels from the corresponding patches. We provide the overview of CoordTok in Figure 2.\nProblem setup Let x be a video and D be a dataset consisting of videos. Our goal is to train a video tokenizer that encodes a video $x \\in D$ into tokens (or a low-dimensional latent vector) z and decodes z into x. In particular, we want the tokenizer to be efficient so that it can encode videos into fewer number of tokens as possible but still can decode tokens to the original video x without loss of information."}, {"title": "2.1. Encoder", "content": "Given a video x, we divide the video into non-overlapping space-time patches. We then add learnable positional embeddings and process them through a series of transformer layers [48] to obtain video features e.\nWe further encode video features e into factorized tri-plane representations [4, 64], i.e., $z = [z_{xy}, z_{yt}, z_{xt}]$, where $z_{xy}$ captures the global content in x across time (e.g., layout and appearance of the scene or object), $z_{yt}$ and $z_{xt}$ capture the underlying motion in x across two spatial axes (see Figure 9 for visualization). This design is efficient because it represents a video with three 2D latent planes instead of 3D latents widely used in prior approaches [9, 52, 61].\nWe implement our encoder based on the memory-efficient design of a recent 3D generation work [16] that introduces learnable embeddings and translates them to triplane representations. Specifically, we first introduce learn-able embeddings $z_0 = [z_{xy}^0, z_{yt}^0, z_{xt}^0]$. We then process them through a series of cross-self attention layers, where each layer consists of (i) cross-attention layer that attends to the video features e and (ii) self-attention layer that attends to its own features. In practice, we split each learnable embedding into four smaller equal-sized embeddings. We then use them as inputs to the cross-self encoder, because we find it helps the model to use more computation by increasing the length of input sequence. Finally, we project the outputs into triplane representations to obtain $z = [z_{xy}, z_{yt}, z_{xt}]$."}, {"title": "2.2. Decoder", "content": "Given the triplane representation $z = [z_{xy}, z_{yt}, z_{xt}]$, we implement our decoder to learn a mapping from (x, y, t) coordinate to the pixels of corresponding patch. We first divide the video into non-overlapping space-time patches. We note that the configuration of patches, e.g., patch sizes, may differ from the one used in the video encoder. We then randomly sample (x, y, t) coordinates by sampling N patches and extracting the center coordinates of sampled patches. We find that sampling only 3% of video patches can achieve strong performance (see Table 4 for the effect of sampling).\nAs inputs to the decoder, we use coordinate-based representations h that are obtained by querying each coordinate from triplane representations via bilinear interpolation. Specifically, let (i, j, k) be one of sampled coordinates. We extract $h_{xy}$ by querying (i, j) from $z_{xy}$, $h_{yt}$ by querying (j, k) from $z_{yt}$, and $h_{xt}$ by querying (i,k) from $z_{xt}$. We concatenate them to get coordinate-based representations h.\nGiven N coordinate-based representations $[h_1, ..., h_N]$, our decoder processes them through a series of self-attention layers, enabling each $h_i$ to attend to other representations $h_j$. This allows the decoder to aggregate and fuse the information from different coordinates. We then use a linear projection layer to process the output from each $h_i$ to pixels of the corresponding patch. Finally, we update the parameters of our encoder and decoder to minimize an $l_2$ loss between the reconstructed pixels and original pixels.\nTo further improve the quality of reconstructed videos, we introduce an additional fine-tuning phase where we train our tokenizer with both $l_2$ loss and LPIPS loss [66]. Specifically, instead of sampling coordinates, we randomly sample a few frames and use all coordinates within the sampled frames for fine-tuning. This enables the tokenizer to compute and minimize LPIPS loss, which requires reconstructing the entire frame. While we find that sampling frames instead of coordinates from the beginning of the training is harmful due to the lack of diversity in training data (see Table 4), we find that fine-tuning with sampled frames improves the quality of reconstructed videos."}, {"title": "3. Experiments", "content": "We design experiments to investigate following questions:\n\u2022 Can CoordTok efficiently encode long videos? Does encoding long videos lead to efficient video tokenization? (Figures 3 and 4 and Table 1)\n\u2022 Can CoordTok learn meaningful tokens that can be used for downstream tasks such as video generation? (Figure 6 and Table 2) Can efficient video tokenization improve video generation models? (Table 3 and Figure 5)\n\u2022 What is the effect of various design choices? (Figure 7 and Table 4)"}, {"title": "3.1. Experimental Setup", "content": "Implementation details We conduct all our experiments on the UCF-101 [40] dataset. Following the setup of prior works [9, 61], we use the train split of the UCF-101 dataset for training. For preprocessing videos, we resize and center-crop the frames to 128 \u00d7 128 resolution. We train our tokenizer using the AdamW optimizer [23] with a batch size of"}, {"title": "3.2. Long video tokenization", "content": "Setup To investigate whether training CoordTok to encode long videos at once leads to efficient tokenization, we consider a setup where tokenizers encode 128-frame videos. Because existing tokenizers cannot encode such long videos at once, we split videos into multiple 16-frame video clips, use baseline tokenizers to encode each of them, and then concatenate the tokens from entire splits. For CoordTok, we train our tokenizer to encode 128-frame videos at once. We provide more details in Appendix A.\nBaselines We mainly consider tokenizers used in recent image or video generation models as our baselines. We first consider MaskGIT-AE [5], an image tokenizer, as our baseline to evaluate the benefit of using video tokenizers for encoding videos. Moreover, we consider PVDM-AE [64], which encodes a video into factorized triplane representations and decodes all frames at once, as another baseline. Comparison with PVDM-AE enables us to evaluate the benefit of our decoder design because it shares the same latent structure with CoordTok. We further consider recent video tokenizers that encode videos into 3D latents, i.e., TATS-AE [9], MAGVIT-AE-L [61], and LARP [50], as our baselines. For a fair comparison, we train all baselines from scratch on UCF-101 or use the model weights trained on UCF-101 following their official implementations. We provide more details of each baseline in Appendix C."}, {"title": "3.3. Long video generation", "content": "Setup To investigate whether CoordTok can encode long videos into meaningful tokens, we consider an unconditional video generation setup where we train a model to produce 128-frame videos. Videos of length 128 are often considered too long to be generated at once, so several works use techniques such as iterative generation [64] for generating long videos. However, because CoordTok can efficiently encode long videos, we train our model to generate 128-frame videos at once. Specifically, we encode 128-frame videos into 1280 tokens with CoordTok and train a SiT-L/2 model [25], a recent flow-based transformer model, for 600K iterations with a batch size of 64. We then use the model to generate 128-frame videos using the Euler-Maruyama sampler with 250 sampling steps. We provide more implementation details in Appendix A.\nBaselines We consider recent video generation models that can generate 128-frame videos as baselines, i.e., MoCoGAN [45], MoCoGAN-HD [44], DIGAN [63], StyleGAN-V [37], PVDM-L [64], and HVDM [19]. We provide more details of each baseline in Appendix C."}, {"title": "3.4. Analysis and ablation studies", "content": "Effect of model Size In Figure 7a, we investigate the scalability of CoordTok with respect to model sizes. We evaluate three variants of CoordTok: CoordTok-S, CoordTok-B, and CoordTok-L. Each variant has a different size for the encoder and decoder (see Appendix A for detailed model configurations). We find that the quality of reconstructed videos improves as the model size increases. For instance, CoordTok-B achieves a PSNR of 25.2 while CoordTok-L achieves a PSNR of 26.9.\nEffect of triplane size In Figure 7b and Figure 7c, we investigate the effect of spatial and temporal dimensions in triplane representations. We evaluate CoordTok with varying spatial dimensions, i.e., 16\u00d716, 32\u00d732, and 64\u00d764, and varying temporal dimensions, i.e., 8, 16, and 32. In general, we find that using larger planes improves the quality of reconstructed videos, as the model can better represent details within videos using more tokens. This result suggests there is a trade-off between the number of tokens and the reconstruction quality. In practice, we find that reducing the spatial dimensions to 16\u00d716 while using a high temporal dimension of 32 strikes a good balance, achieving good quality of reconstructed videos with a relatively low number of tokens.\nEffect of triplane representations We now examine the effect of one of our key design choices: encoding videos"}, {"title": "4. Related Work", "content": "Video tokenization Many recent works have explored the idea of using video tokenizers to encode videos into low-dimensional latent tokens. Initial attempts proposed to directly use image tokenizers for videos [8, 31, 47] via frame-wise compression. However, this approach overlooks the temporal coherence of videos, resulting in inefficient compression. Thus, recent works have proposed to train a tokenizer specialized for videos [1, 2, 9, 15, 51, 52, 56, 57, 59, 61, 62]. They typically extend image tokenizers by replacing spatial layers with spatiotemporal layers (e.g., 2D convolutional layers to 3D convolution layers). More recent works have introduced efficient tokenization schemes with careful consideration of redundancy in video data. For instance, several works proposed to encode videos into factorized triplane representations [19, 64, 65], and another line of works proposed an adaptive encoding scheme that utilizes the redundancy of videos for tokenization [50, 58]. However, they still train the tokenizer through reconstruction of entire video frames, so training is only possible with short video clips split from the original long videos. Our work introduces a video tokenizer that can directly handle much longer video clips by removing the need for a decoder to reconstruct entire video frames during training. By capturing the global information present in long videos, we show that our tokenizer achieves more effective tokenization.\nLatent video generation Instead of modeling distributions of complex and high-dimensional video pixels, most recent video generation models focus on learning the latent distribution induced by video tokenizers, as it can dramatically reduce memory and computation bottlenecks. One approach involves training autoregressive models [9, 21, 56] in a discrete token space [31, 47]. Another line of research [49, 57, 60] also considers discrete latent space but has trained masked generative transformer (MaskGiT; [5]) for generative modeling. Finally, many recent works [1, 11, 13, 19, 24, 35, 64, 68] have trained diffusion models [14, 38] in continuous latent space, inspired by the success of latent diffusion models in the image domain [33]. Despite their efforts, the models are typically limited to processing only short video clips at a time (usually 16-frame clips), which makes it difficult for the model to generate longer videos. In this paper, we significantly improve the limited contextual length of latent video generation models by introducing an efficient video tokenizer."}, {"title": "5. Conclusion", "content": "In this paper, we have presented CoordTok, a scalable video tokenizer that learns a mapping from coordinate-based representations to the corresponding patches of input videos. CoordTok is built upon our intuition that training a tokenizer directly on long videos would enable the tokenizer to leverage the temporal coherence of videos for efficient tokenization. Our experiments show that CoordTok can encode long videos using far fewer number of tokens than existing baselines. We also find that this efficient video tokenization enables memory-efficient training of video generation models that can generate long videos at once. We hope that our work further facilitates future researches on designing scalable video tokenizers and efficient video generation models.\nLimitations and future directions One limitation of our work is that our tokenizer struggles more with dynamic videos than with static videos, as shown in Figure 8. We hypothesize this is due to the difficulty of learning to decompose dynamic videos into global content and motion. One interesting future direction could involve introducing multiple content planes across the temporal dimension. Moreover, future work may introduce an adaptive method for deciding the number of such content planes based on how dynamic each video is, similar to techniques in video codecs [12, 28, 42, 54] or an adaptive encoding scheme designed for a recent video tokenizer [58]. Lastly, we are excited about scaling up our tokenizer to longer videos from larger datasets and evaluating it on challenging downstream tasks such as long video understanding and generation."}]}