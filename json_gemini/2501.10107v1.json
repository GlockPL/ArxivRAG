{"title": "BBPOS: BERT-based Part-of-Speech Tagging for Uzbek", "authors": ["Latofat Bobojonova", "Arofat Akhundjanova", "Phil Ostheimer", "Sophie Fellenz"], "abstract": "This paper advances NLP research for the low-resource Uzbek language by evaluating two previously untested monolingual Uzbek BERT models on the part-of-speech (POS) tagging task and introducing the first publicly available UPOS-tagged benchmark dataset for Uzbek. Our fine-tuned models achieve 91% average accuracy, outperforming the baseline multi-lingual BERT as well as the rule-based tagger. Notably, these models capture intermediate POS changes through affixes and demonstrate context sensitivity, unlike existing rule-based taggers.", "sections": [{"title": "1 Introduction", "content": "Uzbek (a.k.a Northern Uzbek) is the second most-spoken language among all Turkic languages after Turkish (Johanson and Csat\u00f3, 2015). It has approximately 40 million native speakers and is the official language of the Republic of Uzbekistan. Although the official script for Uzbek is Latin, for historical reasons, it still heavily relies on Cyrillic script, both unofficially and officially. Uzbek is a morphologically rich language (MLR) and ranks as one of the most agglutinative languages in the world.\nAlthough Uzbek is a low-resource language, several language models, particularly BERT-based models, have been pre-trained for Uzbek in recent years (e.g. Mansurov and Mansurov, 2021; Mamasaidov and Shopulatov, 2023; Davronov and Adilova, 2024; Kuriyozov et al., 2024). These models vary in size, quality, and the script of the data on which they have been pre-trained. While some are community projects rather than formal academic publications and lack comprehensive evaluation, others have been assessed only in terms of Masked Language Modeling (MLM) accuracy, with comparisons to multilingual mBERT (Devlin et al., 2019). This limitation stems from the lack of publicly available benchmark datasets for Uzbek (Mansurov and Mansurov, 2021). The main goal of this paper is to fill this gap by creating a new dataset for a downstream task and evaluating models based on this benchmark.\nOne such downstream task is POS tagging, which lacks publicly available annotated datasets or pre-trained models for Uzbek. POS tagging, specifically with neural models, has the potential to impact linguistic analysis, corpus linguistics, and computational efficiency (Allaberganova and Kuriyozov, 2023). Existing rule-based solutions lack context sensitivity, a limitation that a BERT model can address effectively through its attention mechanism (Murat and Ali, 2024). Finally, the fine-tuning approach using pre-trained language models may be the most effective solution for low-resource languages, helping to bridge both the resource and accuracy gap.\nIn this work, we introduce the first BERT-based POS tagging models (BBPOS) for Uzbek, available for two actively used scripts, Latin and Cyrillic, together with a newly POS-tagged dataset of 500 sentences. Our models show an average accuracy of 91% based on 5-fold cross-validation."}, {"title": "2 Related Work", "content": "Rule-Based POS Taggers: Sharipov et al. (2023) present Uzbek Tagger a rule-based POS tagger tool that tags a word by looking up its root form from the dictionary. When it fails to find it, the tagger refers to the neighbouring words to make a decision using six custom grammatical rules. However, the tool only considers the immediate context, making it inferior to neural models (see Section 4).\nStatistical POS Taggers: Elov et al. (2023) demonstrate the application of Hidden Markov Models (HMMs) on Uzbek by manually tagging a small set of sentences, without developing a full model or dataset."}, {"title": "3 Experiments", "content": "3.1 Methods\nDue to the lack of a public dataset for POS tagging, we created our own dataset (see Section 3.2). We chose one pre-trained model for each script (see Section 3.3) and fine-tuned them with our dataset for the POS tagging task. As a baseline, we fine-tuned a multi-lingual mBERT model (Devlin et al., 2019). Each type of model was individually evaluated using a 5-fold cross-validation with a 80% - 20% train-test split. All BERT models were fine-tuned with the same hyperparameters (see Appendix A)."}, {"title": "3.2 Data", "content": "Tagset Selection: We used the Universal Part-of-Speech (UPOS) (Nivre et al., 2016), as it is a multilingual tagset that aims to cover similar linguistic features consistently across languages. Currently, it has been the foundation for 283 treebanks in 116 languages and our dataset is the first work to employ UPOS for Uzbek. There are 17 tags in the UPOS as shown in Table 1, and Uzbek can use all of them. Furthermore, it is easy to map UPOS to 12 word classes identified in traditional Uzbek grammar (see Appendix B).\nDataset Development: We collected 500 sentences (5,831 words), 250 sourced from news articles and 250 from fictional books. We manually annotated the data written in Latin script with UPOS tags. Then it was transliterated into a Cyrillic script to fine-tune the Cyrillic model. Table 1 shows the distribution of tags in the dataset and the number of unique words per POS (more details in Appendix C). As the sentences are ordered according to their genre, i.e., fiction and news, the datasets for each script were shuffled with the same seed before a 5-fold split for training and testing."}, {"title": "3.3 Models", "content": "Latin BERT: We chose the open source TahrirchiBERT (Mamasaidov and Shopulatov, 2023), a monolingual RoBERTa (Liu et al., 2019) model pre-trained on Uzbek Latin script. It is trained on large text data extracted from online blogs and scanned books (equivalent to 5B tokens \u2248 18.5GB). The dataset is fairly noisy due to the errors introduced by poor OCR applied to the books. Additionally, TahrirchiBERT does not handle the required pretokenization rules for the Latin script of Uzbek. Specifically, the modifier letters used in o\u02bb and g' letters and the glottal stop sign ' are treated as delimiter signs that cause incorrect word splits. The authors introduced a normalization specific to Uzbek Latin script, preventing some common spelling errors.\nCyrillic BERT: We fine-tuned UzBERT (Mansurov and Mansurov, 2021), a monolingual BERT model (Devlin et al., 2019) pre-trained solely on Cyrillic scripted Uzbek text. According to the authors, the model is trained on high-quality Cyrillic text data with 142M words (\u2248 1.9GB) and has not been evaluated on any downstream tasks due to the lack of public datasets. There are no Uzbek Cyrillic script-specific rules to be applied during the normalization and pretokenization stages, as each letter in the Uzbek Cyrillic alphabet is represented by a single alphabetic character."}, {"title": "4 Results", "content": "Table 2 shows accuracy and F1-score for all trained models together with the results obtained from the rule-based UzbekTagger on the POS-converted dataset (see Appendix D). It presents the mean and standard deviation for accuracy and F1-score of 5-fold cross-validation. The rule-based POS tagger with an average accuracy of 75% falls behind all BERT models. Both monolingual models outperform mBERT by a good margin overall. Table 2,"}, {"title": "5 Discussion", "content": "An interesting aspect of our experiments was how our models handled highly inflected words. They learned morphological features by detecting intermediate POS changes through affixes. For instance, in Figure 1 you can see how the word Kelmaganlardanmisiz? which corresponds to a whole sentence in English ('Are you one of those who did not come?') is tagged by our models. It also shows manual POS and morphological annotation for it. As you can see, our model's result resembles the morphological analysis rather than the simple POS labeling with which it was trained. In fact, according to Universal Dependencies (UD) guidelines, the word's POS relies solely on its lemma's POS.\nOur work on POS tagging has the potential for extension to data generation in morphological analysis, specifically in morpheme classification. However, this requires BERT models to be pre-trained using morphological or morphologically informed tokenizers rather than relying on subword tokenization methods like BPE and WordPiece which are statistical algorithms. Additionally, the success of neural models in learning aspects of Uzbek morphology could inspire the linguistic community to develop a unified and comprehensive POS tagset for Uzbek, one that considers how morphemes influence word-level POS shifts. Previous work on Turkish (\u00c7\u00f6ltekin, 2016) also discusses the guidelines for this.\nThe inconsistent representation of the letters o', g' and 'in texts, caused by the use of varying forms of apostrophes, poses a significant challenge for Latin Uzbek. This issue, as evidenced by the"}, {"title": "6 Conclusion", "content": "In this work, we introduced a new dataset for the low-resource Uzbek language tagged with the UPOS tagset and trained the first BERT-based POS taggers on it. We evaluated two monolingual Uzbek BERT models on the POS tagging downstream task, identifying potential improvements to pre-train Uzbek language models in the future. Our BBPOS models reached an average accuracy of 91% on 5-fold cross-validation, outperforming the baseline mBERT and the existing rule-based solution by far both in accuracy and F1-score. They show context sensitivity in handling ambiguous sentences with homonyms. They learned parts of speech for POS changing morphemes, generating enriched annotations with more linguistic information."}, {"title": "Limitations", "content": "We acknowledge the following limitations of the fine-tuned models:\n\u2022 Even though our fine-tuned models performed better than the rule-based tagger on the evaluation sets, we acknowledge that our models fail to tag overly inflected words as single tokens due to the subword tokenization used in them. The models can be used for synthetic data generation although with heavy human supervision to ensure quality and accuracy.\n\u2022 Additionally, due to the poor pretokenization of TahrirchiBERT, the Latin models fail at words containing the letters o', g', ', as they incorrectly split them into words treating the modifier letters as delimiters. This error is not evident during the validation and training stages of the token classification task as it is during inference.\nWe also acknowledge the following limitations of our benchmark dataset:\n\u2022 Our models failed to learn five out of seventeen POS tags due to the small representation in the initial dataset. Our benchmark needs to be enriched on those POS tags.\n\u2022 While not of major importance, our dataset is relatively small. The dataset is insufficient for training POS tagging models from scratch, such as HMM, CRF, RNN, or LSTM. While we trained an HMM model, its poor performance, achieving an accuracy of (40.7\u00b11) and an F1-score of (8.9 \u00b1 1.8), proved it to be an inadequate baseline and therefore it is not included in the results."}, {"title": "C Data Statement", "content": "We chose news and fiction genres to ensure broad domain coverage while preserving diversity in length, formality, and literary quality. All sentences were handpicked to ensure the quality of the data. News texts of the dataset were collected from the major news sites7. They cover various topics and reflect contemporary Uzbek language use. Fiction texts were chosen from the publicly available Uzbek works on the internet, including: \"Og'riq Tishlar\u201d and \u201cDahshat\u201d by Abdulla Qahhor, \"Shum Bola\" and \"Yodgor\" by G'afur G'ulom, \"Sofiya\u201d, \u201cHazrati Hizr Izidan\u201d, \u201cBibi Salima va Boqiy Darbadar\u201d, \u201cOlisdagi Urushning Aks-Sadosi\u201d and \u201cGenetik\" by Isajon Sulton, \"Buxoro, Buxoro, Buxoro...", "Ozodlik\" and \u201cLobarim Mening... \" by Javlon Jovliyev, \u201cKo\u2019k Tog": "Insonga Qulluq Qiladurmen", "Fano va Baqo\" and \u201cChodirxayol": "y Asqar Muxtor, \u201cAjinasi Bor Yo'llar", "Kecha va Kunduz\" and \"Qor Qo'ynida Lola": "y Cho'lpon. \nFigure 2 shows the number of words per sentence and the number of characters per word/sentence. The number of words per sentence ranges from 5 to 29, with an average of 11\u201312, likely reflecting natural linguistic patterns in Uzbek."}, {"title": "D Comparison with Uzbek Tagger", "content": "To compare BBPOS models with the rule-based POS tagger tool, we relabeled our golden dataset with 12 conventional Uzbek POS tagset using the conversion script we developed (see Section B). The token families that are excluded by the logic of UzbekTagger, such as punctuations, symbols and other (i.e. PUNCT, SYM, X) were eliminated from the dataset to the favor of UzbekTagger results. We then ran the untagged 5 evaluation folds, each containing 100 sentences, through UzbekTagger and compared the results against the relabeled golden dataset."}]}