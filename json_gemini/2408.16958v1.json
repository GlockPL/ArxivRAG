{"title": "Discovery of False Data Injection Schemes on Frequency Controllers with Reinforcement Learning", "authors": ["Romesh Prasad", "Malik Hassanaly", "Xiangyu Zhang", "Abhijeet Sahu"], "abstract": "While inverter-based distributed energy resources (DERs) play a crucial role in integrating renewable energy into the power system, they concurrently diminish the grid's system inertia, elevating the risk of frequency instabilities. Furthermore, smart inverters, interfaced via communication networks, pose a potential vulnerability to cyber threats if not diligently managed. To proactively fortify the power grid against sophisticated cyber attacks, we propose to employ reinforcement learning (RL) to identify potential threats and system vulnerabilities. This study concentrates on analyzing adversarial strategies for false data injection, specifically targeting smart inverters involved in primary frequency control. Our findings demonstrate that an RL agent can adeptly discern optimal false data injection methods to manipulate inverter settings, potentially causing catastrophic consequences.", "sections": [{"title": "I. INTRODUCTION", "content": "The increasing integration of distributed energy resources (DERs) helps introduce more clean energy and flexibility to the power system and plays a pivotal role in decarbonizing the future energy systems [1]. Under this trend, future power systems are transitioning from traditional all-synchronous generator (SG) systems to a mixture of SGs and a large number of inverter-based resources (IBRs). With a higher penetration level of IBRs, the power grid inertia will reduce, leading to an elevated concern of frequency stability. Interconnected smart inverters are a promising strategy to address the issue of inertia reduction, at the expense of emerging cybersecurity concerns. If interconnected smart inverters are compromised, they could serve as an interface for initiating cyber-adversarial events that may lead to catastrophic results.\nThere is an increasing interest in investigating how adversarial events can affect power systems by infiltrating and controlling smart inverters. For example, how smart inverters' settings can be altered by false data injection (FDI) under the operation mode of Volt-VAR, Volt-Watt, and constant power factor is studied in [2]. Detection and mitigation strategies for cyber-attacks such as FDI and denial-of-service (DOS) targeting the automatic generation control (AGC) system were developed in [3]. Tuyen et al. [4] presented a review of the cybersecurity risks for inverter-based power systems, including typical attacks, defense mechanisms, and detection and mitigation measures. Similar reviews can also be found in [5], [6]. Additionally, cyber-physical system (CPS) testbeds are developed to further facilitate research in this area: in [7], DoS attacks are executed in a testbed equipped with a real-time digital simulator (RTDS) for power, the network simulator-3 (NS-3) for communications, and devices like phasor measurement units and phasor data concentrators. This setup aims to investigate the vulnerability of voltage stability monitoring and control in a transmission system to these attacks. The work presented in [8] demonstrates that as the communication delay for a photovoltaic system exceeds the maximum tolerable level, it leads to a severe power imbalance and triggers an over-voltage event.\nThe escalating complexity of problem formulations and the intertwining of both physical and cyber systems have motivated researchers to seek solutions through machine learning-based approaches. The remarkable performance of reinforcement learning (RL) in sequential decision-making across various applications has garnered attention for its potential to address challenges within cyber-physical systems. For example, RL was used for automated incident handling against network-based attack [9]. Galowicz et al. [10], trained a communication network controller to take action based on signals such as signal-to-noise ratio thresholds.\nFor power system frequency control, RL has been utilized for secondary control such as single-area or multi-area AGC. For instance, a twin delayed deep deterministic policy gradient (DDPG) method is used to improve the performance of a single-area AGC in [11]. A multiagent DDPG technique is employed for multi-area load frequency control [12]. Overall, RL strategies have mostly focused on improving load recovery and control of the power grid.\nIn this work, we propose using an RL strategy from an adversary's perspective to discover hitherto unseen vulnerabilities. The vulnerability targetted is the risk that FDI could sufficiently compromise the frequency controller to induce frequency instabilities. We investigate the use of RL to learn optimal FDI adversarial strategies against the default droop control in the smart inverter. Equipped with the knowledge"}, {"title": "II. PROBLEM FORMULATION", "content": "This section presents the specific power system frequency control problem as well as the formulation of the interested FDI attacks.\nA. Power System Frequency Dynamics\nThe system frequency dynamics can be modeled by the swing equation [13]:\n$\\begin{aligned}\nM_i \\dot{\\omega}_i &= P_i - P_{e,i} - D_i \\omega_i - P_i^{IBR} &\\text{(1a)}\\\\\n\\dot{\\theta}_i &= \\omega_i &\\text{(1b)}\n\\end{aligned}$\nin which $i \\in N = \\{1, ..., n\\}$ is the bus index, $\\theta_i$ and $\\omega_i$ are the voltage phase angle and frequency deviation of bus $i$ respectively. $M_i$ and $D_i$ are the inertia and damping coefficients. The net power injection is denoted by $p_i$. By applying the direct current (DC) approximation in transmission system modeling, there is electric power $p_{e,i} = \\sum_{j \\in (N \\setminus \\{i\\})} B_{ij} \\sin(\\theta_i - \\theta_j)$ and $B_{ij}$ is the susceptance of line between bus $i$ and $j$. $p_i^{IBR}$ represents the power output from the inverter-based resource on bus $i$.\nTo maintain the system's frequency stability, the fast re- sponding IBRs on all $n$ buses participate in the system primary frequency control (PFC) following their designed droop rule, i.e., $p_i^{IBR} = k_i \\omega_i$. For simplicity, it is assumed that the IBRs have enough headroom to respond to both high and low- frequency events. Droop coefficients $k_i$ are properly designed so that under normal disturbances, the system frequency can be stabilized. The system considered throughout this work is the Kron-reduced IEEE New England transmission system with $n = 10$ buses on which both traditional synchronous machines and IBRs exist. The numerical integration of Eq. 1 is conducted with an explicit Euler integration with timestep $\\Delta t = 0.01$s similar to [14]. Fig. 1 demonstrates the effect of the frequency control starting from a random off-equilibrium initial condition.\nB. Adversarial Strategy Design\nThrough the cyber-connected smart inverters, an attacker could gain access to these key components of the power system and alter their internal logic to cause catastrophic disturbances. In this study, it is assumed that an attacker has already infiltrated the system allowing to tamper with the control logic of the droop controller by modifying droop coefficients (i.e. $p_i^{IBR} = k_i \\omega_i$, where $k_i$ are the modified droop coefficients). Other types of data tampering could also be envisioned and are left for future work [15]. The objective of the attacker is to maximize the disturbance created, specifically, given by:\n$\\begin{aligned}\n&\\underset{k_{i,t}: i \\in N, t \\in T}{\\text{maximize}} &\\quad \\sum_{i \\in N} \\sum_{t \\in T} \\delta \\omega_{i,t}\\\\\n&\\text{subject to} &\\quad \\text{(1)}, \\\\\\n& & \\quad P_i^{IBR} = k_{i,t} \\omega_{i,t}, \\forall t \\\\\\n& & \\quad ||k_{i,t} - k_i||_0 \\leq 1, \\forall t\n\\end{aligned}$  \\text{(2)}\nwhere $\\delta \\omega_{i,t} = |\\omega_{i,t}|-|\\omega_{base}|$ represents the difference between the frequency deviation after the FDI action $\\omega_{i,t}$ and the baseline without any attack $\\omega_{base}$ after a major frequency disturbance event. The design of this objective function is based on the intuition of maximizing additional disturbance by injecting false data, thereby exacerbating the consequences in the event of a system disturbance. To keep the FDI stealthy, the following constraint is placed on the type of FDI that can be conducted. Assumption: The adversary can only target one IBR at each control step, i.e., for any $t \\in T$, there can be at most one $i$ with $k_{i,t} \\neq k_i$. The stealth requirements are captured by the third constraint in Eq. 2.\nIn future works, additional requirements can be added to make the formulation more realistic, including that the tampering on any single IBR should not last more than $t_{detect}$ seconds cumulatively over $T$ to avoid being detected."}, {"title": "III. VULNERABILITY DISCOVERY VIA RL", "content": "To discover possible system vulnerability to FDI, we adopt the perspective of an adversary by train an RL agent to induce frequency instabilities through droop coefficient tampering. This vulnerability discovery process is depicted in the dashed red box in Fig. 2, as the focus of this paper; once vulnerabilities are identified, corresponding defense strategies could be devised to prevent elaborate FDI strategies. The defense mechanism and this overall adversarial designing process will be formally studied in our future works.\nA. Markov Decision Process Formulation\nA Markov decision process (MDP) needs to be formulated [16, Chapter 3], by defining the observation, action, and reward, in order to use RL to solve the aforementioned sequential decision-making problem.\nObservation $s_t$ collects all the information the adversary uses for decision-making. Here, for simplicity, it is assumed that the adversary has already gained full observability of the system. So, there is $s_t = [\\omega^N, \\theta^N]$, with voltage phase angle and frequency deviation of all buses at step $t$.\nAction $a_t = [i, k_i]$ is multi-categorical, it chooses which IBR to disturb ($i \\in N$) and how to tamper the droop coefficient ($k \\in k_i$).\nReward $r_t$ is defined corresponding to the objective function in (2), which reflects the frequency deviation relative to that of the original trajectory at step $t$: $r_t = I(\\omega_t^N, \\omega_{original}^N)$.\nBased on the MDP defined above, an environment can be built to train an RL agent to maximize the cumulative reward $R = \\sum_{t \\in T} r_t$, which quantifies the intensity of the frequency instability induced.\nB. Reinforcement Learning Algorithm\nDue to its promising performances in other contexts [17], the proximal policy optimization (PPO) algorithm is used to train the adversarial policy [18]. As a separate analysis (not shown here for the sake of brevity), an asynchronous actor-critic method [19] was also employed and led to lower cumulative reward than PPO. Within the PPO algorithm, the training maximizes the following surrogate objective:\n$\\mathcal{L}(\\phi) = E \\bigg[ \\min\\bigg( \\frac{\\pi_{\\phi}(a_t | s_t)}{\\pi_{old}(a_t | s_t)} A_t, clip(\\frac{\\pi_{\\phi}(a_t | s_t)}{\\pi_{old}(a_t | s_t)}, 1 - \\epsilon, 1 + \\epsilon) A_t \\bigg) - c_v L_v(\\phi) + c_H H_\\phi(S_t) \\bigg]$   \\text{(3)}\nwhere $y_t(\\phi) = \\frac{\\pi_{\\phi}(a_t | s_t)}{\\pi_{old}(a_t | s_t)}$ is the probability ratio, which we would like to increase if the advantage $A_t$ is positive and decrease if $A_t$ is negative. Clipping it by $1 \\pm \\epsilon$ encourages policy improvement within a trust region. $L_v(\\phi)$ is the value function loss, indicating how accurately the critic can estimate the expected reward. $H_\\phi$ is an entropy term that encourages the action diversity of the policy. The hyperparameters $c_v$ and $c_H$ control the relative importance of value function loss and entropy when compared with the policy loss. The policy is encoded with a neural net with 2 hidden layers of 64 neurons each. Batches of 64 environment steps are used for training. The entropy loss coefficient is chosen to be $c_H = 0.001$, the value loss coefficient is $c_v = 0.5$ and the learning rate is set constant equal to $3 \\times 10^{-4}$."}, {"title": "IV. CASE STUDY", "content": "A. Experimental Setup\nThe case study is conducted using the 10-bus Kron reduced IEEE New England system, with the implementation of the system frequency dynamics and default droop settings adopted from Ref. [14]. The software implementation of the RL algorithm relies on Stable-Baselines3 [20], and the training is done on the NREL HPC system. The environment defined by Eq. 1 is simulated for 500 steps (equivalent to 5s of total simulation time) and the objective of the agent is to maximize the cumulative reward over these 500 steps. The initial conditions for the phase and frequency of the buses are uniformly randomly sampled from a multivariate uniform distribution $\\mathcal{U}(-0.03, 0.03)$ superimposed with the equilibrium state of the system. The same initial condition is used for all realizations of the environment in order to guarantee a deterministic map between action and cumulative reward.\nAt every step, the agent chooses any one of the 10 generators and chooses to replace its original droop controller coefficient $k_i$ with $k_i$ for 1 step. For simplicity, $k \\in \\{-1, 0, 1\\}$ independently of the original value $k_i$.\nB. Validation with a simplified action space\nAlbeit multidiscrete, the number of possible action is $\\exp(\\frac{500}{\\Delta t} \\log \\frac{10}{\\epsilon})$ . The large action space prevents a brute force optimization strategy that would otherwise help validate the trained agent. Instead, a simplified action space is considered to derive a baseline performance evaluation for the policy obtained. The simplified action space chosen is a time-invariant action space, where the agent chooses one action type (one generator and one replacement value of the droop coefficient) throughout the 500 steps. Therefore, only 30 possible actions exist and can be easily enumerated to identify an optimal policy. The cumulative reward of the 30 different action types is shown in Fig. 3\nOverall, it is clear that replacing the droop coefficient with a negative number results in the highest rewards. This is an expected result given that a negative droop coefficient would enhance frequency deviations rather than mitigate them. In addition, the cumulative reward strongly depends on the bus chosen for the FDI. Perturbing the bus 7 with an $k = -1$ results in the highest cumulative reward $R = 584$. By contrast,"}, {"title": "V. CONCLUSION", "content": "In this work, a reinforcement learning approach to discover hitherto unseen FDI schemes that would perturb power systems has been demonstrated on a model system. All realizations of the RL agent were able to match or exceed the optimal cumulative reward of a simpler action space. The RL agent consistently identified a subset of the buses that needed to be perturbed which could inform how to prioritize cyber-defense strategies. The agent also discovered non-intuitive policies which is encouraging to inform the design of cyber detection and defense strategies. In one instance, the agent exploited a numerical instability in the environment simulations. Overall, the training realizations appeared sensitive to the initialization of the RL agent parameters and the function mapping the action to the cumulative rewards lacked smoothness. Future work will focus on regularizing this map through the rewards function adopted. The actions chosen by the agent were simplistic enough to be practically easy to detect. In future work, an adversarial framework will be presented to the agent to promote strategies that escape existing cyber defenses."}]}