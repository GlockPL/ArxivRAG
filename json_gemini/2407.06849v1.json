{"title": "TeVAE: A Variational Autoencoder Approach for Discrete Online Anomaly Detection in Variable-state Multivariate Time-series Data", "authors": ["Lucas Correia", "Jan-Christoph Goos", "Philipp Klein", "Thomas B\u00e4ck", "Anna V. Kononova"], "abstract": "As attention to recorded data grows in the realm of automotive testing and manual evaluation reaches its limits, there is a growing need for automatic online anomaly detection. This real-world data is complex in many ways and requires the modelling of testee behaviour. To address this, we propose a temporal variational autoencoder (TeVAE) that can detect anomalies with minimal false positives when trained on unlabelled data. Our approach also avoids the bypass phenomenon and introduces a new method to remap individual windows to a continuous time series. Furthermore, we propose metrics to evaluate the detection delay and root-cause capability of our approach and present results from experiments on a real-world industrial data set. When properly configured, TeVAE flags anomalies only 6% of the time wrongly and detects 65% of anomalies present. It also has the potential to perform well with a smaller training and validation subset but requires a more sophisticated threshold estimation method.", "sections": [{"title": "Introduction", "content": "Anomaly detection in time-series data has emerged as a common problem with a variety of real-world applications, from the medical field [16] to high-performance computing [24]. Findings from the field of anomaly detection are also of particular interest to the modern automotive industry. This industry is very diverse, including, for example, manufacturing, prototyping and testing. Furthermore, a car is a complex structure that can be segmented into different subsystems, one of them being the powertrain, which includes all components required for longitudinal dynamics. The powertrain is an important subsystem, as it is something a customer interacts with when driving or being driven. Therefore, testing the powertrain is an an integral part of the wider automotive powertrain development and is undertaken at different stages of development. Each of these stages is composed of many integration levels. These integration levels range from powertrain sub-component testing, such as the electric drive unit (EDU) controller or high-voltage battery (HVB) management system, to whole vehicle powertrain testing. For each integration level there is a special type of controlled environment, called a test bench. The use-case in this paper is on an endurance powertrain test bench, where the EDU and HVB on their own are tested under different conditions and loads for longer periods to simulate wear over time. Given the costly maintenance and upkeep costs of such test benches, it is desirable to keep downtime at a minimum and to avoid faulty records. Also, it is desirable to detect problems early to prevent damage to the testee. Online time-series anomaly detection is especially relevant, as it can provide timely insights into potentially harming behaviour that deviates from the norm.\nApplying anomaly detection to this real-world use case is especially challenging due to its complexity and highly dynamic setup. During powertrain testing several sensors record signals over time, leading to data in the form of multivariate time series. These signals not only have correlations within themselves over time but also between each other. In addition to that, some of the signals recorded also feature variable-state behaviour, meaning the same test done at different times will yield slightly different data. This is due to certain channels like the battery signals presenting slightly different behaviour depending on how warm or charged the battery is. All these characteristics exclude the use of simpler statistical models as they are not compatible with all of the mentioned data properties.\nGiven that evaluation is currently done manually by inspection, it is not feasible to analyse every single test, also evaluation tends to be delayed, only being undertaken days after the test is recorded, hence there is a clear need for automatic, fast and unsupervised evaluation methodology which can flag anomalous behaviour before the next testing procedure is started.\nTo achieve this, we propose a temporal multi-head attention-based variational autoencoder (TeVAE). TeVAE consists of a bidirectional long short-term memory (BiLSTM) variational autoencoder architecture that maps a time-series window into a temporal latent distribution [17] [24]. Also, a multi-head attention (MA) mechanism is added to further enhance the sampled latent matrix before it is passed on to the decoder. As shown in the ablation study, this approach avoids the so-called bypassed phenomenon [3], which is the first contribution. Furthermore, this paper offers a unique methodology for the reverse-window process. It is used for remapping the fixed-length windows the model is trained on to continuous variable-length sequences, i.e. time series. Moreover, we propose a set of metrics apt to online time-series anomaly detection that is not only interpretable and simple but is also compatible with discrete time-series anomaly detection. Lastly, the root-cause capability of TeVAE is investigated, for which a new metric, the root-cause precision, is also proposed.\nThis paper is structured as follows: First, a short background is provided in Section 2 on the powertrain testing methodology specific to this use case, as well as the theory behind VAEs and MA mechanisms. Then, related work in variational autoencoder-based time-series anomaly detection is presented in Section 3, followed by an in-depth introduction of the real-world data set and the approach we propose in Section 4. Then, several experiments testing different aspects of the proposed method are conducted and discussed in Section 5, along with the final results. Finally, conclusions from this work are drawn and an outlook into future work is provided in Section 6. The source code for the data pre-processing, model training as well as evaluation can be found under https://github.com/lcs-crr/TeVAE."}, {"title": "Background", "content": ""}, {"title": "Real-world Application: Automotive Powertrain Testing", "content": "During endurance testing a portfolio of different drive cycles is run, where a drive cycle is a standardised driving pattern characterised by the vehicle speed, which enables repeatability. For this type of testing, the portfolio consists exclusively of proprietary drive cycles, which differ from the public drive cycles used, for example, for vehicle fuel/energy consumption certification. The reason why proprietary drive cycles are used for endurance runs is that they allow for more extensive loading of the powertrain.\nGiven the presence of a battery in the testee, some time has to be dedicated to battery soaking (sitting idle) and charging. These procedures are also standardised using soaking and charging cycles, respectively, although, for the intents and purposes of this paper, they are omitted. What is left in the portfolio are eight dynamic drive cycles representing short, long, fast, slow and dynamic trips ranging from 5 to 30 minutes. Modelling the testee behaviour is further complicated through variable-state behaviour. This means that two measurements of the same drive cycle done one after another will look different, depending on initial states. A measurement is defined as an instance of a drive cycle and is in the form of a multivariate time series or sequence. Variable-state behaviour can be categorised into short-term reversible and long-term irreversible. On the one hand, there are channels like the battery temperature or state of charge (SoC) that contribute to the short-term reversible kind since the battery heats up and discharges as it is used. On the other hand, processes like battery ageing, also known as the state of health (SoH), contribute to long-term irreversible behaviour, which is not considered and modelled in this use-case.\nOn powertrain test benches, there are several control methods to ensure the testee maintains the given drive cycle, i.e. the vehicle speed profile. In this particular test bench, the regulation is done by the acceleration pedal position and the EDU revolutions per minute (rpm)."}, {"title": "Data Set", "content": "To enable the development of anomaly detection methodology that can deal with the above-mentioned challenges, a data set is created using the data for one of the testees. This real-world data set D consists of thousands measurement files, each of variable length and containing hundreds of (many redundant or empty) channels. The training subset Dtrain consists of M = 2785 unlabelled time series such that $D^{\\text{train}} = [S_1, ..., S_m, ..., S_M]$. Note that each time series in $D^{\\text{train}}$ has variable length $T_m$ and dimensionality $d_D$, such that $S_m \\in \\mathbb{R}^{T_m\\times d_D}$. For this work, a list of representative $d_D = 13$ channels is hand-picked in consultation with the test bench engineers. The chosen features along with their indices are as shown in Table 1.\nThe testing subset Dtest consists of N labelled time series such that $D^{\\text{test}} = [S_1, ..., S_n, ..., S_N]$, where each time series in Dtest has variable length Tn and dimensionality dp, such that $S_n \\in \\mathbb{R}^{T_n \\times d_D}$. The labelled anomaly-free portion of the testing subset Dtest accounts for Naf = 698 measurements, where a measurement is considered anomaly-free when the testee behaviour conforms to the norm.\nDue to the absence of labelled anomalies in the dataset, realistic anomalous events are intentionally simulated and recorded following the advice of test bench engineers. To this end, four anomaly types are recorded. Every anomaly type is recorded for every drive cycle at least once, leading to Na = 47 anomalous measurements that are all used as the labelled anomalous portion of the testing subset Dtest. Hence Dtest is made up of N = 745 measurements, representing an anomaly ratio of around $N_a/N = 6.3\\%$, however in reality this value is estimated to be much lower. This amount of anomalous data in relation to anomaly-free data is used as it approximately matches the anomaly ratio in public data sets [15, 1, 11, 24] and because the data set is not large enough to create a larger anomaly-free test subset.\nIn the first type, the virtual wheel diameter is changed, such that the resulting vehicle speed deviates from the norm. The wheel diameter is a parameter as resistances are connected to the shafts rather than actual wheels. This accounts for 16 time-series anomalies and the only channel that demonstrates anomalous behaviour is the vehicle speed, since:\n$U_{\\text{vehicle}} = r \\cdot w$ (1)\nwhere r is the wheel radius and w the angular velocity of the drive shaft. Logically, the anomalous behaviour is most visible at higher speeds."}, {"title": "Proposed Approach", "content": ""}, {"title": "Overview", "content": "To detect anomalies in multivariate time-series data, we propose a variational autoencoder architecture consisting of BiLSTM layers. The model architecture is illustrated in Figure 3. During training, the encoder $q_\\phi$, parameterised by $\\phi$, maps input window X to a temporal distribution with parameters $\\mu_z$ and $log \\sigma_z$ in the forward pass, Equation 13.\n$(\\mu_z, log \\sigma_z) = q_\\phi(X)$ (13)\nGiven the latent distribution parameters $\\mu_z$ and $log \\sigma_z$, the latent matrix is sampled from the resulting distribution, as shown in Equation 14. Note that the covariance is not modelled, hence $\\sigma_z$ only contains the diagonal of the covariance matrix.\n$Z \\sim \\mathcal{N} (\\mu_z, diag(\\sigma_z))$ (14)\nThen, the input window X is linearly transformed to obtain the query matrices $Q_i$ and key matrices $K_i$ for each head i. Likewise, the sampled latent matrix Z is also transformed to the value matrix $V_i$, as shown in Equation 15.\n$Q_i = XW_i^Q \\quad K_i = XW_i^K \\quad V_i = ZW_i^V$ (15)\nTo output the context matrix $C_i$ for each head i, the softmax of the through $\\sqrt{d_K}$ normalised query and key product is multiplied with the value matrix, Equation 16.\n$C_i = \\text{Softmax}(\\frac{QK^T}{\\sqrt{d_K}})V_i$ (16)\nThe final context matrix C is the result of the linearly-transformed concatenation of each head-specific context matrix $C_i$, as expressed in Equation 17.\n$C = [C_1, ..., C_h] W^O$ (17)\nThe decoder $p_\\theta$, parameterised by $\\theta$, then maps the context matrix C to $\\mu_X$ and $log \\sigma_X$, as shown in Equation 18. These can then be used to parametrise output distribution $\\mathcal{N}(\\mu_X, diag(\\sigma_X))$.\n$(\\mu_X, log \\sigma_X) = p_\\theta(C)$ (18)\nMapping the output to a distribution rather than a deterministic vector allows TeVAE to model uncertainty, which is assumed to be normally distributed."}, {"title": "Inference Mode", "content": "Despite the generative capabilities of VAEs, TeVAE does not leverage generation for anomaly detection. Rather than sampling a latent matrix as shown in Equation 14 during inference, sampling is disabled and only $\\mu_z$ is taken as the input for the multi-head attention mechanism, like in [22]. Equation 14, therefore, is replaced by Equation 19 for the forward pass.\n$Z = \\mu_z$ (19)\nThis not only accelerates inference by eliminating the sampling process but is also empirically found to be a good approximation of an averaged latent matrix if it were sampled several times like in [18]. The TeVAE layout during inference is shown in Figure 3, where the traced arrow designates the information flow from the encoder to the MA mechanism."}, {"title": "Threshold Estimation Method", "content": "Anomalies are by definition very rare events, hence an ideal anomaly detector only flags measurements very rarely but accurately. In the powertrain test bench scenario an algorithm is preferred that only flags a sequence it is sure is an anomaly, in other words, an algorithm that outputs very few to no false positives. A high false positive count would lead to a lot of stoppages and therefore lost testing time and additional cost. Of course, the vast majority of measurements evaluated will be anomaly-free hence it is paramount to classify them correctly, naturally leading to a high precision value. Also, there is no automatic evaluation methodology currently running at test benches, other than rudimentary rule-based methods, therefore a solution that plugs into the existing system that automatically detects some or most anomalies undetectable by rules-based approaches can already lead to time and cost savings. To achieve this, the threshold $\\tau$ is set as the maximum negative log-likelihood observed when the model is fed with unlabelled validation data."}, {"title": "Bypass Phenomenon", "content": "VAE, when combined with an attention mechanism, can exhibit a behaviour called the bypass phenomenon [3]. When the bypass phenomenon happens the latent path between encoder and decoder is bypassed and information flow occurs mostly or exclusively through the attention mechanism, as it has deterministic access to the encoder hidden states and therefore avoids regularisation through the DKL term. In an attempt to avoid this, [3] propose variational attention, which, like the VAE, maps the input to a distribution rather than a deterministic vector. Applied to natural language processing, [3] demonstrate that this leads to a diversified generated portfolio of sentences, indicating alleviation of the bypassing phenomenon. As previously mentioned, only [18] applies this insight in the anomaly detection domain, however, they do not present any evidence that it alleviates the bypass phenomenon in their work. TeVAE on the other hand, cannot suffer from the bypass phenomenon in the sense that information flow ignores the latent variational path between encoder and decoder since the MA mechanism requires the value matrix V from the encoder to output the context matrix. Assuming the bypass phenomenon also applies to a case where information flow ignores the attention mechanism, one could claim that TeVAE is not immune. To disprove this claim, the attention mechanism is removed from the model in an ablation study to see if anomaly detection performance remains the same. In this case, V = Z is instead directly input into the decoder. If it drops, it is evidence of the contribution of the attention mechanism to the model performance and hence is not bypassed. The results for this ablation study are shown and discussed in Section 5."}, {"title": "Reverse-window Process", "content": "Since the model is trained to reconstruct fixed-length windows, the same applies during inference. However, to decide whether a given measurement sequence $S_n \\in \\mathbb{R}^{T_n \\times d_D}$ is anomalous, a continuous reconstruction of the measurement is required. A trivial way to do so would be to window the input measurement $S_n$ using a shift of 1, input the windows into the model and chain the last time step from each output window to obtain a continuous sequence [5]. Considering the BiLSTM nature of the encoder and decoder, the first and last time steps of a window can only be computed given the states from one direction, making these values, in theory, less accurate, however. To overcome this, we propose averaging matching time steps in overlapping windows, which is called mean-type reverse-window method. This is done by pre-allocating an array with NaN values, filling it, and taking the mean for each time step while ignoring the NaN values, as depicted in Figure 4. This process and the general anomaly detection process are described in Algorithm 1. The input for this process is the output distribution parameters ($\\mu_X, log \\sigma_X$), so it essentially averages the distributions and hence can only be applied to the mean and variance parameters. Consider the distributions $\\mathcal{N}(\\mu_x, \\sigma_x^2)$ and $\\mathcal{N}(\\mu_y, \\sigma_y^2)$, both assumed to be independant and normally distributed. The sum of both distributions results in normal distribution $\\mathcal{N} (\\mu_{x+y}, \\sigma_{x+y}^2)$, obtained as shown in Equation 20 [13].\n$\\mu_{x+y} = \\mu_x + \\mu_y$ (20)\n$\\sigma_{x+y}^2 = \\sigma_x^2 + \\sigma_y^2$\nTherefore, the standard deviation of the resulting distribution $\\sigma_{x+y}$ is characterised by Equation 21.\n$\\sigma_{x+y} = \\sqrt{\\sigma_x^2 + \\sigma_y^2}$ (21)"}, {"title": "Root-cause Analysis", "content": "Once a measurement is predicted to be anomalous it is of great benefit if context is provided, like what channel had the biggest impact on the prediction. Depending on what subsystem within the system the anomaly originates, it may affect a different number of channels, in some cases even all. Some attempts to provide root-cause functionality are made in literature."}, {"title": "Results", "content": ""}, {"title": "Setup", "content": "The encoder and decoder both consist of two BiLSTM layers, with the outer ones having 512 hidden- and cell-state sizes and the inner ones 256. All other parameters are left as the default in the TensorFlow API.\nDuring training only, input windows are corrupted using Gaussian noise using 0.01 standard deviation to increase robustness [26].\nKey factors that are investigated in Section 5 are given a default value which applies to all experiments unless otherwise specified. These factors are training and validation subset size, which is set to 512h, reverse-window method, where the mean-type is used, the latent dimension size, which is set to dz = 64, the MA mechanism, which is set up as proposed in [25] with a head count of h = 8 and a key dimension size $d_K = [d_D/h] = 1$.\nThe optimiser used is the AMSGrad optimiser with the default parameters in the TensorFlow API.\nCyclical DKL annealing is applied to the training of TeVAE, to avoid the DKL vanishing problem [9]. The DKL vanishing problem occurs when regularisation is too strong at the beginning of training, i.e. the Kullback-Leibler divergence term has a larger magnitude in relation to the reconstruction term."}, {"title": "Online Evaluation Metrics", "content": "The results provided are given in the form of the calibrated and uncalibrated anomaly detection performance, i.e. with and without consideration of threshold $\\tau$, respectively. Recall that the threshold used is the maximum negative log-likelihood obtained from the validation set. The basis for all metrics are the number of true positives Ntp, number of false negatives Nfn and number of false positives Nfp.\nAs discussed in Section 1, a testing subset in discrete time-series anomaly detection problem has three types of time series: entirely normal, time-series anomalies and sub-sequence anomalies. Since anomalies are considered rare events, the number of anomaly-free time series Naf within Dtest is much larger than the number of time-series anomalies Nts and sub-sequence anomalies Nss, such that Naf >> Nts + Nss = Na and N = Naf + Nts + Nss. In the case of anomaly-free time series and time-series anomalies, traditional labels can easily be applied. An anomaly-free time series can be labelled as true negative or false positive and a time-series anomaly can be labelled as true positive or false negative. For partially anomalous time series, i.e. where the anomalous behaviour occupies a contiguous subset of time steps within the time series, it can be labelled as a true positive or a false negative, but also as a false positive, which occurs when an algorithm flags a time step early. Formally, this is the case when the first flagged time step is far enough ahead of the first ground-truth time step that the model cannot have had access to it. As is evident, the proposed metrics can only be applied to discrete time-series data sets where anomalous time series have at most one contiguous ground-truth anomalous sub-sequence of any length. Also, ensuring each time series contains at most one contiguous anomaly avoids ambiguity on how multiple sub-sequence anomalies within a time series should be detected and counted [27]. In addition to that, there can also be at most one contiguous ground-truth anomaly-free sub-sequence of any length within a sub-sequence anomalous time series, since systems that can dynamically return to anomaly-free behaviour (especially in a short amount of time) reap less benefit from automated anomaly detection than those which require human attention. Given that a predicted anomaly likely requests human attention and potentially a stoppage of the system, the adaptation of the metrics proposed above only takes into account the first predicted anomalous time step. For cases where the process may continue, there can be multiple contiguous predicted anomalous sub-sequences, hence the metrics cannot be applied.\nCalibrated metrics are the precision, recall and F\u2081 score. Precision P represents the ratio between the number of correctly identified anomalies (true positives) and the number of all positives (true and false), shown in Equation 22, recall R represents the ratio between the number of true positives and the number of all anomalies, shown in Equation 22, and F\u2081 score represents the harmonic mean of the precision and recall, shown in Equation 22.\n$P = \\frac{N_{\\text{tp}}}{N_{\\text{tp}} + N_{\\text{fp}}} \\quad R = \\frac{N_{\\text{tp}}}{N_{\\text{tp}} + N_{\\text{fn}}} \\quad F_1 = 2 \\cdot \\frac{PR}{P+R}$ (22)\nThe theoretical maximum F\u2081 score, F1,best, is also provided to aid discussion. This represents the best possible score achievable by the approach if the ideal threshold were known, i.e. the point on the precision-recall curve that comes closest to the P = R = 1 point, though, in reality, this value is not observable and hence cannot be obtained in an unsupervised manner. The precision and recall corresponding to the F1,best score are also provided.\nThe uncalibrated anomaly detection performance, i.e. the performance for a range of thresholds is represented by the area under the continuous precision-recall curve $A_{\\text{cont}}^{PR}$, Equation 23.\n$A_{\\text{cont}}^{PR} = \\int_{0}^{1} PdR$ (23)\nAs the integral cannot be computed for the continuous function, the area under the discrete precision-recall curve $A_{\\text{disc}}^{PR}$ is used which is done using the trapezoidal rule, Equation 24.\n$A_{\\text{disc}}^{PR} = \\sum_{k=1}^{K-1} \\frac{P_{k-1}+P_k}{2} \\Delta R_k$ (24)\nwhere K is the number of discrete points along the precision-recall curve, k the index of discrete points along the precision-recall curve and $\\Delta R_k$ the sub-interval length between indices k and k - 1.\nWhile the above metrics quantify binary anomaly detection performance, they do not provide information on the delay of detections, which plays a crucial role in online time-series anomaly detection. Therefore, we propose an additional metric, the detection delay $\\delta$, which represents the absolute delay between the first ground-truth anomalous time step t = tgt and the first predicted anomalous time step t = tp. The detection delay $\\delta$ is only calculated for anomalous time series since in anomaly-free time series there is no first ground-truth time step t = tgt, therefore it is calculated as shown in Equation 25.\n$\\delta = |t_p - t_{\\text{gt}}|$ (25)\nwhere, in case of a false negative, tp is equal to the last time step of the anomalous time series. This formula not only reflects the detection delay for true positives but also punishes false negatives by applying the maximum delay possible, i.e. $\\delta = T$, as well as false positives by applying the \\\"negative\\\" delay between the first early predicted time step t = tp and the first ground-truth time step t = tgt. For each anomalous test time series the detection delay is calculated and subsequently averaged to yield the average detection delay $\\overline{\\delta}$, shown in equation 26.\n$\\overline{\\delta} = \\frac{1}{N_{\\text{ts}} + N_{\\text{ss}}} \\sum_{i=1}^{N_{\\text{ts}} + N_{\\text{ss}}} \\delta_i$ (26)\nIn order to evaluate TeVAE's root-cause analysis performance we propose a corresponding metric. Recall that according to the method of counting labels, anomaly-free sequences can be labelled as true negatives or false positives and time-series anomalies can be labelled as true positives or false negatives. Sub-sequence anomalies can be labelled as true positives, false negatives or false positives, where the latter occurs when an anomaly is detected before it actually occurs. To quantify whether the most relevant channel has been flagged in the case of a predicted anomalous sequence, the root-cause true positive count $N_{\\text{tp}_{rc}}$ and root-cause false positive count $N_{\\text{fp}_{rc}}$ are introduced. Clearly, the root-cause channel can only be obtained for predicted anomalous sequences, hence why there is no root-cause true negative count $t_{nrc}$ or root-cause false negative count $N_{\\text{fn}_{rc}}$. The root-cause true positive count $N_{\\text{tp}_{rc}}$ represents the number of sequences labelled as true positives and the predicted root-cause channel is a subset of the list of ground-truth root-cause channels for a given anomaly type. Likewise, root-cause false positive count $N_{\\text{fp}_{rc}}$ represents the sum of three cases. The first case is an anomaly-free sequence labelled as a false positive, for which there are no ground-truth root-cause channels. The second case is a ground-truth time-series or sub-sequence anomaly that is labelled as a true positive, but the predicted root-cause channel is not a subset of the list of ground-truth root-cause channels for the relevant anomaly type. The third case is a ground-truth sub-sequence anomaly that is labelled as a false positive, due to a premature detection, for which there are no ground-truth root-cause channels. To aid the understanding of this concept, a diagram depicting what types of sequences can be labelled as what is shown in Table 2. As is evident, $N_{\\text{tp}} + N_{\\text{fp}} = N_{\\text{tp}_{rc}} + N_{\\text{fp}_{rc}}$. To summarise the $N_{\\text{fp}_{rc}}$ and $N_{\\text{fp}_{rc}}$ figures, we propose a new metric called the root-cause precision $P_{rc}$, shown in Equation 27.\n$P_{rc} = \\frac{N_{\\text{tp}_{rc}}}{N_{\\text{tp}} + N_{\\text{fp}}}$ (27)\n$P_{rc}$ denotes the number of correctly identified root-cause channels relative to the number of total detections, true or false."}, {"title": "Ablation Study", "content": "TeVAE is tested without the MA mechanism and with a direct connection from the encoder to the decoder to observe whether the absence of the MA impacts results. The anomaly detection performance of TeVAE and its counterpart without MA, henceforth referred to as NoMA model, are shown in Table 3.\nWhile the precision value of the NoMA model is slightly higher than the TeVAE, the recall value on the other hand is much lower. Overall, TeVAE has a significantly higher F\u2081 score, as well as a higher theoretical maximum F\u2081 score and higher uncalibrated anomaly detection performance, denoted by the Apr figure. Furthermore, TeVAE features a much lower average detection delay which is especially relevant for online time-series anomaly detection. In contrast to that, NoMA offers marginally higher root-cause precision. The results hence point towards an improvement brought about by the addition of the MA mechanism and therefore the bypass phenomenon can be ruled out."}, {"title": "Data Set Size Requirements", "content": "To evaluate how much data is required to train TeVAE to a point of adequate anomaly detection performance, it has been trained with 1h, 8h, 64h, and 512h of dynamic testing time. The results for this experiment are presented in Table 4.\nOn the one hand, as the training and validation subset increases in size, the precision value improves but on the other hand recall value decreases as the subset grows, though at a smaller scale compared to the increase in precision. This can be attributed to the fact that smaller subset sizes lead to a small validation set and therefore less data to obtain a threshold from. With a limited amount of data the validation set distribution is very different to the true data distribution, leading to a threshold that is very small and hence marks most anomalies correctly but also leads to a lot of false positives. Despite the decreasing recall, the F\u2081 score increases with a growing training and validation subset size. It can also be observed that both the F1,best and the Apr reach a point of diminishing returns after 8h of dynamic testing. Given that neither metric relies on the unsupervised threshold, it indicates that it plays a large role in the F\u2081 score. It further implies that the model quality remains largely the same from 8h onwards. Also, the F\u2081 score seems to approach the F1,best score as the subset grows, also backing the fact that with a small subset size, a good threshold cannot easily be obtained."}, {"title": "Reverse-window Process", "content": "To investigate the effect of the mean-type reverse-window method, it is compared with the first-type and last-type methods where the first and last values of each window are carried over, respectively, the results for which are shown in Table 5.\nAs is evident, the calibrated and uncalibrated detection performance is very similar throughout the different methods. In terms of average detection delay, it is evident, however, that the first-type method is significantly slower than the other two methods. Interestingly, despite the capacity to detect anomalies with much lower theoretical delay for most of the time steps in the sequence, the last-type actually yielded very similar average detection delays to the mean-type. The root-cause precision is very similar for last and mean-type reverse-windowing, with first-type scoring the highest. Furthermore, the mean-type reverse-window method results in a higher computational load, though negligible."}, {"title": "Hyperparameter Optimisation", "content": "As part of the hyperparameter optimisation of TeVAE, a list of key dimension sizes dk in combination with a list of latent dimension sizes dz is tested. Note that, given the unsupervised nature of the problem, this optimisation is not possible in a productive environment. The results depict theoretical and in reality unobservable anomaly detection performance. Alternatively, optimising for another metric like validation loss would be possible, however, there is no guarantee that said metric leads to good anomaly detection performance. Despite the larger learning capacity associated with a higher dk, the attention head concatenation is always transformed to an output matrix of dimensionality do = dz. For the two variables, values of 1, 8, 64, and 512 are tested; the results are shown in Tables 6 and 7.\nAs shown in the ablation study, the multi-head attention mechanism does positively impact anomaly detection performance, however Table 6 illustrates that once MA is implemented, the key dimen- sionality plays a small role, as all metrics are very comparable for all dk. When it comes to the latent dimension size dz, it is clear that for dz = dx = 1, the model cannot pass enough information through the two bottlenecks to yield good performance. Once dz = 8 is reached, the best anomaly detection performance ever observed in the experimentation is obtained, after which it drops slightly for dz = 64 and dz = 512. The same cannot be said for the root-cause precision Prc, which is lower for dz = 8 than for the other configurations."}, {"title": "Benchmarking", "content": "Of course, TeVAE is not the first model proposed for time-series anomaly detection. To relate its anomaly detection performance, it is compared with a series of other models based on variational autoencoders. The chosen subset of models is based on the work discussed in Section 3 which either linked source code or contained enough information for implementation. The models are implemented using hyperparameters specified in their respective publications. All models are trained on the 512h subset with early stopping, which is parameterised equally across all models. The anomaly detection process specified in Algorithm 1 is also applied to all models, along with the threshold estimation method. For VASP and LW-VAE no root-cause precision Pre is provided because the resulting anomaly scores cannot be decomposed. The results can be seen in Table 8.\nAs is evident, TeVAE outperforms all other models in F\u2081 score, R, Apr and 8, while providing nearly matching the best precision result using the unsupervised threshold. As stated in Section 4 a very high precision figure is important in this type of powertrain testing, however, the reduced precision is still considered tolerable. Also, it comes at the benefit of a much higher recall figure, which is reflected in the superior F\u2081 figure, though it still leaves room for improvement. Furthermore, the F1,best figure, which is obtained at P = 0.97 and R = 0.58, suggests that TeVAE has the potential to achieve even higher precision without sacrificing recall if the threshold were optimised."}, {"title": "Conclusion and Outlook", "content": "In this paper, a variational autoencoder (TeVAE) for unsupervised anomaly detection in automotive testing is proposed. Automotive testing is an especially challenging scenario due to its massive, diverse and multi-dimensional nature. In addition to that, the resulting data not only features variable states but also highly dynamic signals along with more static ones, adding to further complexity. It not only features an attention configuration that avoids the bypass phenomenon but also introduces a novel method of remapping windows to whole sequences. A number of experiments are conducted to demonstrate the online anomaly detection performance of the model, as well as to underline the benefits of key aspects introduced with the model. To this end, novel metrics are introduced to measure the detection delay, as well as the root-cause analysis capability of analysed anomaly detection approaches.\nFrom the results obtained, TeVAE clearly benefits from the MA mechanism, indicating the avoidance of the bypass phenomenon. Moreover, the proposed approach only requires a small training and validation subset size but fails to obtain a suitable threshold, as with increasing subset size only the calibrated anomaly detection performance increases. Despite the higher theoretical delay, mean-type reverse windowing performs comparably to its last-type in both detection performance and observed average detection delay, while outperforming the first"}]}