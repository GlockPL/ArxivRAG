{"title": "Artificial-Intelligence Generated Code Considered Harmful: A Road Map for Secure and High-Quality Code Generation", "authors": ["Chun Jie Chong", "Zhihao (Zephyr) Yao", "Iulian Neamtiu"], "abstract": "Generating code via a LLM (rather than writing code from scratch), has exploded in popularity. However, the security implications of LLM-generated code are still unknown. We performed a study that compared the security and quality of human-written code with that of LLM-generated code, for a wide range of programming tasks, including data structures, algorithms, cryptographic routines, and LeetCode questions. To assess code security we used unit testing, fuzzing, and static analysis. For code quality, we focused on complexity and size. We found that LLM can generate incorrect code that fails to implement the required functionality, especially for more complicated tasks; such errors can be subtle. For example, for the cryptographic algorithm SHA1, LLM gener-ated an incorrect implementation that nevertheless compiles. In cases where its functionality was correct, we found that LLM-generated code is less secure, primarily due to the lack of defensive programming constructs, which invites a host of security issues such as buffer overflows or integer overflows. Fuzzing has revealed that LLM-generated code is more prone to hangs and crashes than human-written code. Quality-wise, we found that LLM generates bare-bones code that lacks defensive programming constructs, and is typically more complex (per line of code) compared to human-written code. Next, we constructed a feedback loop that asked the LLM to re-generate the code and eliminate the found issues (e.g., malloc overflow, array index out of bounds, null dereferences). We found that the LLM fails to eliminate such issues consistently: while succeeding in some cases, we found instances where the re-generated, supposedly more secure code, contains new issues; we also found that upon prompting, LLM can introduce issues in files that were issues-free before prompting. Our study exposes the perils of LLM-generated code (and feedback loops), particularly in the critical security domain.", "sections": [{"title": "1 Introduction", "content": "Software security is of utmost importance in software engineering, as it directly affects the security and reliability of a digital society increasingly dependent on software. For example, the 2024 CrowdStrike bug that crashed 8.5 million Microsoft Windows devices [18] highlights the worldwide impact of software bugs. Human experts have been trained to write, review, and test code to ensure its quality, despite the fact that the process is time-consuming and error-prone. However, the security and quality of Artificial Intelligence (AI)-generated code is an under-studied area. With the advance of AI, the shift towards AI-assisted programming is rapidly gaining momentum, making the concerns more urgent.\nLarge Language Models (LLMs) are already widely used to assist developers in code completion and summarization [7, 12, 13], and in some cases, to automatically generate code from scratch to meet the requirements of specific tasks [44]. For example, GitHub Copilot [13] (\"Copilot\" for short), a widely-adopted AI coding assistant, has been available since 2022 [13]. While users report an improvement in productivity (81% and 88%, respectively reported by two independent user studies [3, 14]), an empirical investigation has shown that 40% of Copilot-generated programs are buggy [40]. The false sense of productivity when using LLM code generation in the workplace has mostly been driven by developers aiming to fulfill industry's internal performance metrics, such as task completion time and lines of code produced [26], rather than code quality.\nThe security implications of AI code generation are largely ignored by the industry: Copilot Voice has been introduced as a new feature that allows inexperienced developers to generate full programs by simply speaking to the AI assistant, marketed as a new way to \"write code without the keyboard\" [16]. Na\u00efve trust in AI code generation can lead to significant security vulnerabilities and degradation in code quality. Under-standing the traits and limitations of the LLM-generated code is important to facilitate the adoption of LLMs in future soft-ware engineering practice. In October 2023, the White House issues an Executive Order on \"Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\", which em-phasize the importance of security in such AI use cases [4, 10]."}, {"title": "2 Background", "content": "Motivated by the national goal, this work aims to study the security and quality of code generated by the state-of-the-art LLM model, OpenAI's GPT-40, which powers GitHub Copilot Enterprise [15]. Specifically, we create an evaluation framework, EXACT (Examination System for AI-Generated Code Testing), which uses a suite of program analyses, unit testing, fuzzing, complexity and size measurements, to assess code security and quality. EXACT compares these factors with the human-written code that implements the same functional-ity. We focus on three classes of representative coding tasks: over 200 LeetCode programming tests, fifteen commonly-used algorithms and data structure implementations, as well as popular cryptographic functions. For each of these tasks, we compare the security and quality of the human-written and LLM-generated code. We chose the C programming language due to its prevalence in systems and security-sensitive code.\nFor each comparison experiment, EXACT checks function-alities of LLM-generated and human-written code using an empirical method: if a predefined test suite (e.g., LeetCode online submission [19]) is available, we submit both programs to the test suite; otherwise, we use unit testing and AFL [2] to create test cases and compare the runtime behavior of the two programs. Note that fuzzing findings are used to assess both security and functionality. A recent study [53] demon-strates that LLM is not capable of solving complicated tasks (tasks that take a human more than 11 minutes). We observe a similar trait in our study: among the 21 LeetCode tasks that LLM failed to solve, 20 tasks are rated at medium or hard difficulty. LLM also implements the SHA-1 hash func-tion incorrectly, where it generates the wrong hash values for all the inputs. We also demonstrate the unreliability of both the AI-generated and human-written code in the presence of fuzzing, as they both fail to handle corner cases in our test suites (though human code performs slightly better).\nWe also found that the security and quality of LLM-generated code is lacking, compared to its human-written equivalent. The security issues found by Clang static ana-lyzer in LLM-generated code are consistently higher than human-written code: by 11.2% in LeetCode and 7.1% in algo-rithm tasks. Additionally, LLM-generated code is 1.19x more complex than human-written code in LeetCode and 1.26x in algorithm tasks.\nA feedback loop is a state-of-the-art practice carried out by various research groups [32, 36, 47] to improve LLM's code generation. Therefore, our study also investigates the effectiveness of a feedback loop in improving the security and correctness of the generated code by iteratively feeding the results of the evaluation back to the LLM model. We show that when we asked the model to fix the discovered security issues, it can introduce new bugs in code that was previously bug-free; moreover, the re-generated version has higher complexity per line. We also observe that by request-ing shorter line lengths, GPT-40 yields subsequent code with 11.3% lower cyclomatic complexity per line, and surprisingly,"}, {"title": "2.1 AI Code Generation", "content": "LLMs have been widely adopted for natural language tasks, and also for code generation. This adoption is easily justifi-able as LLMs have shown exceptional performance in code completion [43], translation [51], and full project code gen-eration [35]. With the introduction of large-scale pre-trained LLMs, such as OpenAI's GPT-40 that powers GitHub Copi-lot Enterprise [15], the performance of AI code generation has been further advanced to a degree that it is presumed to replace human programmers [5].\nThe inception of AI code generation has brought about both a revolution and a debate in the software community. On one hand, AI code improves developers' productivity by automating coding tasks, especially the repetitive and mun-dane ones [31]. However, the security implications of fully autonomous programming using LLMs [35] and automation of repetitive coding tasks [31] that have been proposed by recent research are not well understood.\nIn a Copilot study, 88% of human developers have reported that they are more productive when coding with the tool [3]. The drastic improvement in productivity demonstrates the potential of AI in software engineering. Indeed, the CEO of GitHub predicts that \u201csooner than later,\u201d \u201cCopilot will write 80% of code\" [5]. But, unfortunately, existing AI coding assistants have been shown to write incompetent code [40, 41, 49]. According to a study conducted by GitHub, 92% of the surveyed developers have used AI coding tools, and 70% of them believe that the tools offered an advantage in"}, {"title": "2.2 Automated Code Improvement", "content": "LLMs have been using Reinforcement learning (RL) for auto-mated code improvement. RL updates a model's parameters through interactions with human feedback or the environment, and has shown to be effective in improving various AI models' performance [32, 47]. Reinforcement learning from human feedback (RLHF) uses human feedback to fine tune a model's parameters [30]. Despite the fact that RLHF can be used to improve the performance of LLM code generation, scalable deployment in practice has been limited by the capability of human evaluators to provide meaningful feedback to the out-puts [36]. To cope with the lack of human feedback, OpenAI has proposed a fine-tuned model based on GPT 4, namely CriticGPT, to automatically critique the LLM-generated code, where the feedback is used to improve the subsequent gener-ations [36]. A pool of bugs and human feedback originated from OpenAI's previous RLHF pipeline is used as the training set of the fine-tuning of CriticGPT [36]. Likewise, GitHub Copilot has proposed a secondary LLM model to \u201capproxi-"}, {"title": "2.3 Implications of AI Code Generation", "content": "Na\u00efve trust in AI code-generation tools can lead to deterio-rated code quality and software security hazards. Due to the lack of metrics to evaluate the trustworthiness of generated code [49], existing quantitative evaluation of AI-generated code is limited. A user study conducted in 2023 has found significant degradation in code quality and a false sense of security when programmers use AI assistants [41]. Likewise, Pearce et al. conducted an empirical study on 1,689 Copilot-generated programs and found that 40% of the programs are vulnerable [40]. Another user study in 2024 has shown that a majority of GitHub Copilot users felt that the tool's sugges-tions were not always accurate, as the tool \"may give false code suggestions that mislead developers\" [14]. Fixing the errors in the generated code is a challenging task for most users because they did not author the code themselves. Con-sequently, when AI-generated code appears to be complex from the user's perspective, users often give up on fixing the code and resort to other online resources [48]. Indeed, strin-gent testing shows that, currently, even the most advanced LLM models, such as Claude 3.5 Sonnet [8] and GPT-40 [17], are only able to solve basic, straightforward tasks, that take human developers at most 11 minutes to solve [53]."}, {"title": "3 Motivation", "content": ""}, {"title": "3.1 Sample Task 1: Finding a Buffer's Size", "content": "As motivation for our study, we present a preliminary exper-iment we conducted on OpenAI GPT-40, where we found that the model fails a basic, yet critical, security task: finding the size of a buffer, even though the buffer size is given in the prompt in a way that a human can easily understand. The incorrect buffer size can lead to buffer overflow if it is larger, or leaking non-initialized memory if the buffer size is smaller than the actual buffer size in the buffer initialization func-tion. We present an example of wrong buffer size in Figure 1, where the buffer size is filled in as 26, but the actual buffer size should be 25 (as the length of an incomplete alphabet with the letter 'G' missing is 25). In our experiment, we asked GPT-40 to fill in the buffer size in the loop condition (the red circle in Figure 1) with various buffer size definitions in the referenced code in our prompt.\n\nWe found that GPT-40"}, {"title": "3.2 Sample Task 2: Implementing SHA1", "content": "We asked GPT-40 to complete a widely-used, security-critical algorithm: SHA1 (Secure Hash Algorithm 1). The SHA1 header file from OpenBSD's GitHub repository [23] was pro-vided in the prompt to make sure that the specifications are clear, and there are no discrepancies in function input/out-put formats between the GPT-40 and human implementation. We tested the correctness of GPT-40's SHA1 implementation by using the reference test vectors (predefined inputs with their expected outputs) provided by the National Software Reference Library (NSRL) [22]. GPT-40's implementation produced incorrect hash values for all the inputs. Such silent errors will not prompt any error message, but incorrect hash values can lead to security vulnerabilities and malfunctions"}, {"title": "3.3 Sample Task 3: Implementing Graph Data Structure", "content": "This task entailed generating a simple graph implementation, with functions for creating (allocating) a graph, adding and removing edges, printing the graph, and freeing (deallocat-ing) the graph memory from the heap. \nNote that the human-developed code, shown on the left, has checks for the value of $v$, whereas the LLM-generated code, on the right, does not contain such checks. The LLM code has two issues. First, it does not check the result of $malloc()$ for NULL, hence opening the first entry for potential NULL pointer dereferences if the memory allocation fails. Second, the LLM-generated code does not check for a potential nega-tive value of $v$. If a negative $v$ is passed as an argument, the call to $malloc()$ on line 5 will most likely result in a NULL pointer (since $malloc()$ takes an unsigned argument, the argument is interpreted as a positive value at the size of $size\\_t\\_max - abs(V) + 1$, where $size\\_t\\_max$ is the max value of the type), or, less likely, in a very large memory allocation. Therefore, memory allocation either fails or its size is in the control of a potential adversary. We refer to this as $malloc$ overflow, as a term used by Clang analyzer."}, {"title": "3.4 Motivations", "content": "Aligned with the national goal to manage the risk of genera-tive AI [4, 10], our research investigate the security of LLM-generated code. Our research is motivated by the following questions:\nResearch Question 1: Does LLM generate insecure code?\nResearch Question 2: For the same task, does LLM generate code that is more secure (or higher quality) than the human-written version?\nResearch Question 3: Can prompt engineering improve the security of LLM-generated code?"}, {"title": "4 Methodology", "content": "We select two categories of coding tasks to evaluate the secu-rity of LLM-generated code: (1) LeetCode problems, and (2) data structures and algorithms, including three widely-used cryptographic algorithms. We chose these categories because they are widely used in almost all software engineering inter-views and actual coding tasks, and they have relatively clear input-output interfaces.\nFor LeetCode problems, we selected all 202 problems from a well-known LeetCode solution repository on GitHub with 2.3k forks and 5.5k stars [20]. We use the same set of data"}, {"title": "4.1 LLM Code Generation", "content": "We use OpenAI's GPT-40 model to generate the full task solution for each coding task. As shown in Figure 4, for each coding task, we provide its task description as prompt to GPT-40 API, and collect the coding outputs.\nFor LeetCode problems, we locate the problem description in the LeetCode website using the problem title in the Leet-Code solution repository, and use the description as a prompt. The prompt is constructed in a way as close as possible to the actual task description given to a human developer.\nFor data structures and algorithms, we provided prompts for GPT-40 as follows: either the header files, or in the absence of header files, the function declarations (types) from the human-written code. This ensures that both GPT-40 and hu-man implement the same set of functionalities and eliminate the discrepancies in function signatures in code comparison."}, {"title": "4.2 Functionality Validation", "content": ""}, {"title": "4.2.1 LeetCode Problems", "content": "After obtaining the GPT-4o-generated code, we use test cases to validate the functionalities of the code against the human-written code for the same task. If test cases are available, such as LeetCode online submission [19], we submit GPT-40-generated code directly to the LeetCode platform (Sec-tion 5.1.1) for evaluation."}, {"title": "4.2.2 Data Structures and Algorithms", "content": "For data structures and algorithms, we employ unit testing and fuzzing to validate functionality, as follows.\nUnit testing. Test cases are manually written for unit test-ing with the help of CUnit [9]. We examine the human-written code to understand the functionality provided by the data structure or algorithm, such as insertion, deletion, sorting, etc. We then write test cases, divided into 2 categories: (1) regular cases, and (2) edge cases. Regular cases are typical scenar-ios that a function is expected to handle, such as inserting positive integers into a linked list, perform a merge sort on an unsorted array, etc. Edge cases are crafted in a way to test a function's protection against unexpected inputs, such as dequeuing from an empty queue, performing a breadth-first search on an empty graph, sorting an empty array, etc. We ran the same test cases on both GPT-40-generated and human-written code, since we prompt GPT-40 to implement the data structures and algorithms with the same header files or function declarations as used in the human-written code. By testing GPT-40-generated and human-written code with the same test cases, we verify that they are achieving the same functionalities.\nFuzzing. In addition to unit testing, we use AFL [2] to fuzz both GPT-40-generated code and human-written code. We create an entry point to fuzz each data structure and algorithm. This entry point is the main method in each program where it reads an input file that contains various instructions and input values. For example, the line insert 10 in an input file will carry out the insert operation in the binary search tree and insert the value 10 into the tree. The input file is used as a seed in fuzzing. Each data structure and algorithm has its own set of instructions to make sure that all implemented functionalities will be tested. Since we only focus on functionality validation, any invalid instructions mutated by AFL [2] in the process of fuzzing will be ignored in the entry point. However, values will be mutated during fuzzing and GPT-40-generated code and human-written code need to handle any kinds of invalid inputs. As it was not relevant to the comparison, the entry point code was not included in the security and complexity analysis."}, {"title": "4.2.3 Cryptographic Algorithms", "content": "For cryptographic algorithms, we employ a different way of validating the functionality. For one-way hashing algorithms such as SHA1 and Message Digest Algorithm 5 (MD5), we encrypt the predefined inputs from test vectors and compare the computed hash values with the expected hash values from the test vectors; we used existing sets of test vectors (strings) originating from NSRL [22] (as discussed in Section 3.2). For two-way algorithms such as Advanced Encryption Standard (AES), we encrypt the predefined inputs and decrypt the en-crypted values; we then validate the decrypted values against the original inputs."}, {"title": "4.3 Static Analysis", "content": ""}, {"title": "4.3.1 Security", "content": "We use the Clang static analyzer [1] to perform static analysis on both GPT-40-generated and human-written code. Clang provides a set of default checkers, such as null dereference, memory leak, and null arguments. In addition, Clang provides a range of experimental (advanced) checkers in several cat-egories, as follows. \u201cCore\u201d experimental checkers include detectors for pointer arithmetic, invalid casting/conversion, etc. \"Security\" experimental checkers look for errors such as array index out of bounds, malloc overflow, etc. \"Unix\" experimental checkers look for issues such as memory leaks or null pointers passed to string functions. While the exper-imental checkers might, in theory, emit a higher number of false positives, our manual validation of the reported errors indicate a negligibly low rate of false positives."}, {"title": "4.3.2 Complexity", "content": "We evaluate code complexity using several metrics: cyclo-matic complexity, normalized complexity, and lines of code. Cyclomatic complexity is used to measure the complexity of a program's control flow [37]. High cyclomatic complexity can indicate that the code is harder to understand and maintain, and potentially prone to errors. As cyclomatic complexity is an absolute value that characterizes an entire file, larger files naturally have higher complexity. Therefore, to gauge the complexity of typical code in a file, we also compute the normalized complexity, i.e., divide complexity by the num-ber of lines of code in that file \u2013 this indicates the typical complexity expected for a code snippet. We use SCC [25] to obtain the cyclomatic complexity and the lines of code in each C file. By \"lines of code\" we mean actual source code, excluding comments and blank lines."}, {"title": "5 Analysis Results", "content": "We now present our comparative analysis of GPT-40-generated and human-written code in terms of functionality (Section 5.1), security (Section 5.2), and complexity/code quality (Section 5.3)."}, {"title": "5.1 Functionality Analysis", "content": ""}, {"title": "5.1.1 LeetCode Problems", "content": "The LeetCode platform provides an online submission system [19] where submitted code has to pass all the test cases \u2013 only then it is considered a correct solution. All the human-written code that we gathered from the GitHub repository [20] had to pass all the test cases from LeetCode online submission system prior to being placed in the repository. Therefore, we only tested GPT-40-generated code through LeetCode online"}, {"title": "5.1.2 Data Structures and Algorithms", "content": "Section 4.2.2 discussed our strategy for writing unit test cases and seed files, used in unit testing and fuzzing, respectively. We discuss our findings next.\nUnit testing. All of the data structures and algorithms im-plemented by GPT-40 passed the unit testing. For human-written code, 3 out of 15 of the data structures and algorithms did not pass unit testing due to failing test cases and caus-ing segmentation faults. For example, in the implementation of doubly-linked list, the usage of == for double comparison leads to failing test cases. In the implementation of red-black trees, a missed return statement causes nullptr to be used as a buffer address, leading to null pointer dereference.\nFuzzing. In terms of unique hangs, GPT-40-generated code has 50% more hangs than human-written code, whereas for unique crashes, GPT-40 generated code has 23.9% more crashes than human-written code. We were unable to perform fuzzing on certain human-written code such as queue, doubly linked list, and red black tree due to the bug mentioned above"}, {"title": "5.1.3 Cryptographic Algorithms", "content": "We performed unit testing on AES, MD5, and SHA1 as dis-cussed in Section 4.2.3. Human-written code from OpenBSD generates all the correct hash values, as anticipated. GPT-40's implementations of MD5 and AES compute all values correctly. GPT-40's SHA1 implementation computes wrong hash values for all the inputs, which could be catastrophic if this implementation were to be used in real-world situations. Functionality issues like this have a high chance of going un-noticed, unless the LLM-generated code goes through strict testing."}, {"title": "5.2 Security Analysis", "content": "We perform static analysis on both GPT-40-generated and human-written code using Clang static analyzer [1]. \n\nOverall, GPT-40-generated code has 10.3% more security issues than human-written code. The three categories of issues that have relatively higher count are malloc overflow, array index out of bounds, and memory leak. We found that GPT-40-generated code has 32.8% more malloc overflow issues than human-written code. This is a consequence of GPT-40's tendency of assuming all inputs are valid (Figure 3). The counts for array index out of bounds issues are similar for both GPT-40 and human. \n\nOther than issues reported by Clang, we also look at the im-plementation details between GPT-40-generated and human written code on data structures and algorithms. We do not focus on the implementation details of LeetCode solutions because the main objective of a LeetCode problem is to pro-vide a solution that solves a very particular problem with a given set of constraints that passes all the test cases provided. We found that GPT-40 has a tendency of generating ex-act same code as human-written code. We will refer to this as \"code parroting\" for the rest of the paper. Note that we do not include any implementation details of human-written code in the prompts with the exception of header files or function declarations. In GPT-40-generated code, 2 out of"}, {"title": "5.3 Complexity Analysis", "content": "SCC [25] is the tool that we utilize to measure cyclomatic complexity and lines of code (LoC, excluding comments and blank lines) per file. We calculate mean, median, geometric mean, and trimmed mean for both complexity and normalized complexity (complexity per LoC).\nAs shown , GPT-40-generated code seems to have lower complexity on the surface. However, it would be incorrect to assume that this lower complexity indicates high-"}, {"title": "6 Feedback Loop", "content": ""}, {"title": "6.1 Rationale and Setup", "content": "State-of-the art practice suggests using a reinforcement loop (Section 2.2) to improve code quality when using LLMs. Therefore, we constructed a feedback loop as a way to im-prove GPT-40-generated code, starting by trying to eliminate the security issues identified in Section 5.2. For this experi-ment, we included 60 LeetCode files (solutions) generated by GPT-40, as follows: 30 files with security issues (found by Clang), and 30 files without security issues. \nWe ask GPT-40 to regenerate"}, {"title": "6.2 Results", "content": "Files with security issues. For files that did contain security issues, GPT-40 manages to lower the number of security is-sues found after one iteration on malloc overflow and null dereference from 40 to 24 and 3 to 2, respectively, as shown in Table 8. However, the security issues are not completely removed, even though we specifically asked GPT-40 to ensure the code will not have these security issues. Furthermore, for array index out of bounds issues, GPT-40 generated 3 more such issues. This is a critical finding since it shows the possi-bility that LLM may generate code that is the exact opposite of what users have specified in the prompt.\nFiles without security issues. For files without any se-curity issue before entering the loop, GPT-40 introduces 4 new malloc overflow issues.\nMalloc overflows account for 63.3% of the total security is-sues found across the code generated by GPT-40. However, GPT-40-generated code, still does not check the value of k and the result of malloc after an effort has been made in the prompt to ask LLM to make no such mistakes. This outcome is especially troublesome, since"}, {"title": "6.3 Why can't Prompt Engineering Fix Security Issues?", "content": "The results from the feedback loop allow us to draw a conclu-sion that LLM so far is not capable of consistently removing security issues even though the prompt has explicitly asked it to do so. LLM might even go against the prompt in producing more security issues in the code. First of all, these security issues are found by running static analysis. Users who utilize LLM for code generation might not even notice that the code poses serious security threats without running static analy-sis. Furthermore, even static analysis is not enough: some of the runtime errors found by LeetCode's online submis-sion system [19] were not identified during static analysis (Section 5.1.1). To conclude, even for users who are aware of potential issues and diligent in attempting to reduce or eliminate security risks in the code, the LLM route might be unfruitful: LLM regeneration may worsen the security problem in the generated code, e.g., as shown in Figure 11."}, {"title": "7 Related Work", "content": ""}, {"title": "7.1 AI-based Code Generation Security", "content": "Similar studies have been done on the security aspect of LLM code generation. Perry et al. [41] conducted a user study on two groups of participants: one without and the other with, access to an AI assistant. Participants were asked to solve a few specific tasks using multiple programming languages. Their results show that participants with access to the AI assistant wrote significantly less secure code, which aligns with our findings. We share a similar goal but take a different approach to addressing the AI assistant-introduced issues, as the comparisons in our study are between LLM and human in solving algorithmic and data structure tasks.\nWe discuss code parroting in Section 5.2. Code parrot-"}, {"title": "7.2 Benchmarks for LLM-Generated Code", "content": "Purple Llama CyberSecEval [28] and Cybench [53] are bench-marks designed to tackle the security issues arising from LLM-generated code. CyberSecEval selects insecure code tests from real-world open source codebases whereas Cy-bench gathers 40 Capture the Flag (CTF) tasks for evaluat-ing LLM-generated code. These benchmarks are orthogonal to our purpose in evaluating an LLM's coding capabilities. EXACT is a comprehensive evaluation framework that can in-corporate these benchmarks as a part of its testing if needed."}, {"title": "7.3 Code Improvement in LLM", "content": "As we discussed in Section 2.2, RL has been employed by LLMs to improve code generation. Due to the implementation of these blackbox components, we would not know for sure if the improvements have been made in the security aspect of code generation. However, our study has shown that LLM-generated code produces a significant amount of security risks. Prompt-based learning [33] is an approach where LLMs are guided to generate desired output with carefully designed prompts without re-training the models. This technique could be applied on code generation as well. For instance, whenever malloc appears in code generation, it should come with an allocation size check before using malloc and a NULL check for the result of malloc. This would be one of the desired outputs to drastically reduce malloc overflow issues. However, based on the results we have seen so far in our study, LLM-generated code still has a long way to go to achieve this level of security."}, {"title": "7.4 Privacy and Security in AI", "content": "Code generation is not the only domain that raises privacy and security issues regarding LLMs. Client-side prompt san-itization [29, 46, 52] is designed to protect private informa-tion shared with LLMs without affecting the performance of LLMs. Federated learning [38] can be used to train Machine Language models without sharing raw data that could be privacy-sensitive. Prompt injection is an attack where LLMs could generate harmful content due to malicious prompts; benchmarks [34] and fine-tuned models [42] can defend against such attacks. Our study shows the importance of scru-tinizing code security as well."}, {"title": "8 Discussion", "content": ""}, {"title": "8.1 Multiple LLMs", "content": "Our study focuses on a single LLM, GPT-40. We could extend our study by conducting the comparisons using the same human-written code with other LLMs such as Llama3, Claude, etc.; which would reveal differences between different LLMs in terms of code security. However, the focal point of our study is to bring attention to the differences between LLM-generated and human-written code. We believe that GPT-40, being the state-of-the-art of OpenAI's LLM, has successfully revealed the security threats brought upon by LLMs-based code generation; the issues we found are critical, yet likely overlooked when programming with the help of LLMs."}, {"title": "8.2 Coding Tasks", "content": "The coding tasks in our experiments are relatively small and independent as opposed to the code in large software projects such as an operating system kernel, a browser etc. Comparing code implemented for a subsystem can be an interesting fu-ture work since it requires more attention to interoperability, as these software has a plethora of components within them-selves. Another possible future work in terms of coding tasks could be asking LLM to fix known errors or identify possible issues in human-written code."}, {"title": "9 Conclusions", "content": "As AI-assisted code writing is spreading, it is imperative to study its impact and consequences on software security. We developed a methodology and framework, EXACT, for com-paring the security and quality of LLM-generated and human-written code for the same task. We expose and quantify the disadvantages of na\u00efvely using LLM-generated code in a critical domain such as security: the LLM-generated code might violate the expected functionality in subtle ways, or might contain security issues that do not manifest until late in program execution, and can be exploited by adversaries. In contrast, human code for equivalent tasks tends to contain guardrails and defensive constructs. We also show the poten-tial perils of using a feedback loop when programming with the help of LLMs. Our work sheds light on the importance of scrutinizing LLM-generated code (and code re-generated by prompting) for functionality, security, and quality issues. As long as LLM-generated code is prone to security risks it should be considered potentially harmful."}]}