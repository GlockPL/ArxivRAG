{"title": "Building Multi-Agent Copilot towards Autonomous Agricultural Data Management and Analysis", "authors": ["Yu Pan", "Jianxin Sun", "Hongfeng Yu", "Joe Luck", "Geng Bai", "Nipuna Chamara", "Yufeng Ge", "Tala Awada"], "abstract": "The ubiquity of sensors and IoT devices has led to an explosion in data availability in modern agriculture. The large volume and heterogeneity of the data, together with the complexity of data processing requirements, pose huge obstacles for achieving the principles of Findable, Accessible, Interoperable, and Reusable (FAIR). Current data management and analysis paradigms are to large extent traditional, in which data collecting, curating, integration, loading, storing, sharing and analyzing still involve too much human effort and know-how. The experts, researchers and the farm operators need to understand the data and the whole process of data management pipeline to make fully use of the data. The essential problem of the traditional paradigm is the lack of a layer of orchestrational intelligence which can understand, organize and coordinate the data processing utilities to maximize data management and analysis outcome. The emerging reasoning and tool mastering abilities of large language models (LLM) make it a potentially good fit to this position, which helps a shift from the traditional user-driven paradigm to AI-driven paradigm. In this paper, we propose and explore the idea of a LLM based copilot for autonomous agricultural data management and analysis. Based on our previously developed platform of Agricultural Data Management and Analytics (ADMA), we build a proof-of-concept multi-agent system called ADMA Copilot, which can understand user's intent, makes plans for data processing pipeline and accomplishes tasks automatically, in which three agents: a LLM based controller, an input formatter and an output formatter collaborate together. Different from existing LLM based solutions, by defining a meta-program graph, our work decouples control flow and data flow to enhance the predictability of the behaviour of the agents. Experiments demonstrates the intelligence, autonomy, efficacy, efficiency, extensibility, flexibility and privacy of our system. Comparison is also made between ours and existing systems to show the superiority and potential of our system.", "sections": [{"title": "I. INTRODUCTION", "content": "The core topics of modern agriculture is to meet the escalating needs for food, fuel, feed, and fiber in the era of climate change, limited natural resources and intricate dynamics of global ecosystems. To solve this challenge, data-informed precision agriculture is not only required but also necessary, in which the deployment of sensors, Internet of Things (IoT) devices [1], and advanced instrumentation [2] has enabled data collection in various modal, dimension, format, and resolution. While agricultural data has the potential to transform agricultural practice by providing valuable insights into crop productivity, soil quality, and sustainable farming techniques, in the meanwhile, the large volume and heterogeneity, together with the complexity of data processing requirements, pose huge obstacles for for achieving the principles of Findable, Accessible, Interoperable, and Reusable (FAIR), which in turn result in in-efficient utilization and exploration of the collected data.\nThere are existing efforts trying to revolutionize current practices of agriculture data management [3]\u2013[7], by im-plementing different data management platforms and tech-nologies. Nevertheless, current practice of agricultural data management and analysis is to large extent traditional in the sense that the whole data management pipeline, including data collecting, curating, integration, loading, storing, sharing and analyzing still involve too much human effort and know-how. The experts, researchers or the farm operators make every tiny decisions from where to put a file, to which analysis tool to use. In this process, they need to understand the details of the data and tools at their hands, including location, semantics and format of data, and the location, semantics and the interface of the relevant data processing tools to make fully use of the data. This human-driven paradigm incurs high cost of training and makes reuse of the data and duplication of the data processing pipeline difficult. The essential reason of the challenges faced by the traditional practice of agricultural data management lies in the fact that there's a lack of a layer of orchestrational intelligence which can understand, organize and coordinate each phase of the data processing pipeline to maximize the outcome.\nIn the meanwhile, Large Language Models (LLM) recently start to demonstrate reasoning and tool mastering capabilities. That is, augmented with a set of tools and given a task, LLM need to make plans for the task, select appropriate tools for each step, call the tools and observe the feedback from these tools. Also there can be a observe-and-take-action loop until the task can be accomplished. LLM's reasoning capability makes it feasible to make plans for a task or make decisions about which tool will be used in next step. The application arises naturally where we explore the possibility to fit LLM as the aforementioned orchestrational layer which can coordinate and harness each component of agricultural data management and coordinate each phase of the data processing pipeline. Imagining the tools are not isolated with each other, but form a network where each tool can connect with each other through their inputs and outputs. The tool using intelligence of LLM can probably find new paths in the complicated tool networks and as a result gain new patterns and insight. This will bring us a new data management paradigm in which human effort can be reduced while the outcome of data analysis can be probably maximized. In one word, LLM based copilot will act as a controller in this new data management paradigm, which helps human users to manage data, tools or ML models. Furthermore, as new architectures of LLM-based data management framework being proposed and as new tools, IoT devices or sensors being continuously incorporated, the systems adopting the new paradigm will hopefully grow into a full-fledged autonomous data management system, which can take care of mundane operations with minimal human interference, and help human users to make decisions in data management tasks."}, {"title": "II. RELATED WORK", "content": "Tool learning with Large Language Models (LLMs) has emerged recently as a promising paradigm for augmenting the capabilities of LLMs to accomplish complicated tasks, by utilizing LLMs reasoning abilities. Here the tool can be a program, a database, an API endpoint for a cloud based service, a sensor, hardware devices or anything of which the interface can be coded as plan text and thus can be recognized and utilized by LLMs.\nA large corpus of exiting work has been proposed to explore different aspects and phases of Tool learning with LLMs [8]\u2013[10]. Tool learning with LLMs is in fact an umbrella concept which encompasses a lot of subcategories, including augmenting LLMs with external knowledge [11]\u2013[14], augmenting LLMs with mathematical reasoning ability [15], [16], augmenting LLMs with program executors [17], [18], and augmenting LLMs with domain specific tool sets [19], [20]. Generally speaking, there are four phases in tool learning with LLMs: task planning, tool selection, tool calling and response generation [8]. In task planning, a task is decomposed into several sub-tasks, by utilizing LLM's reasoning capabilities [21]\u2013[26]. In tool selection, the most appropriate tool is selected by LLM for each phase generated from task planning [27], [28]. In tool calling phase, the selected tool is called, based on given interface of the tool. Again LLM will be used to format the calling interface, that is, configure the order and value of arguments for a method call [29]\u2013[31]. Nowadays, large language models such as GPT-4 can output function calling, given the description of the tool and the user input [32]. Lastly, the output of the tools can be combined and formatted for output, by utilizing LLM's language organization capability [33]. Each phase can utilize LLM and thus there will be several agents collaborating together for the whole process.\nThe existing works explore different frameworks to implement tool learning with LLMs, however, almost all the works don't decouple data flow from control flow, which may result in hallucination problem and in turn fail a task. In our work, we generate a meta-program graph, as a representation of the tools and their affiliated data and use three agents: controller, input formatter and output formatter to deal with control flow and data flow respectively. In this way, we can increase the predictability of the result generated by the LLM-based agents and thus increase the success rate of the task solving process."}, {"title": "B. Agricultural Data Management Systems", "content": "The digital revolution in agriculture has paved the way for advanced data management platforms specifically designed to tackle the unique challenges and demands of agricul-tural research and production. Several notable platforms have emerged in related sectors.\nCyVerse [34], initially developed for plant genomics, has ex-panded into a comprehensive platform, offering life scientists robust computational infrastructure to manage large datasets and conduct complex analyses. As a cloud-based solution, Cy-Verse supports collaborative efforts, enabling seamless sharing of data and tools. Its focus on scalability and interoperability has made it a crucial tool for modern agricultural data man-agement. GARDIAN [5], the CGIAR's (Consultative Group for International Agricultural Research) data discovery plat-form, allows users to find research datasets and publications from various CGIAR Centers and Programs. By aggregating data from multiple agricultural research projects, GARDIAN offers a holistic view of global agricultural research, fos-tering interdisciplinary collaboration. GEMS [4] integrates genomic, phenotypic, and environmental data, offering tools for analyzing and visualizing large-scale agricultural datasets, thereby linking genomics with environmental factors in agri-cultural research. TERRAREF [6] focuses on delivering high-resolution sensor data for plant research, using tools such as cameras and drones to gather detailed information on plant growth, health, and environmental conditions. This extensive dataset aids researchers in exploring the intricate relationships between plants and their environments. The evolution of Smart Farming into Agriculture 5.0 [35] emphasizes the critical role of data in optimizing farm management for sustainabil-ity and economic efficiency. Lastly, ADMA [7] introduces an innovative agricultural data management system designed according to FAIR principles, incorporating features such as intelligence, interactivity, scalability, flexibility, open-source pipeline management, and enhanced privacy and security.\nSeveral platforms and studies emphasize the importance of integrated data management systems for addressing specific agricultural topics or utilizing advanced technologies. For instance, Agricultural Remote Sensing Big Data [36] explores the management and application of remote sensing big data in agriculture, introducing a four-layer-twelve-level (FLTL) framework designed to handle remote sensing data for pre-cision farming and local farm analyses. Data Warehouse and Decision Support on Integrated Crop Big Data [37] presents a thorough analysis of data warehousing and decision support systems, demonstrating their capability in leveraging inte-grated crop big data through the development of a continental-level Agricultural Data Warehouse (ADW). Big Data and Machine Learning (ML) With Hyperspectral Information in Agriculture [38] reviews critical research utilizing big data, machine learning, and deep learning, with a specific focus on processing hyperspectral and multispectral agricultural data. It highlights the potential of ensemble machine learning and scalable parallel discriminant analysis in managing agricultural big data. Lastly, Blockchain for Sustainable e-Agriculture [39] discusses the intersection of blockchain technology and IoT in e-agriculture, emphasizing how blockchain can enhance agricultural value chains, strengthen IoT networks, and ensure data validation, security, and privacy."}, {"title": "III. FRAMEWORK", "content": "To support the FAIR principles and the proposed key features such as intelligence, autonomy, efficacy, efficiency, extensibility, flexibility and privacy, we design and implement a proof-of-concept system called Agricultural Data Manage-ment and Analytics Copilot (ADMA Copilot)."}, {"title": "A. Shift of Paradigm", "content": "Thanks to the emerging reasoning and tool learning ability of large language models, there will be a shift of agricultural data management paradigm. In the traditional paradigm, re-searchers need to understand every details of the data and tools (or ML models) at their hands, such as location, semantics and format of data, and the location, semantics and the interface of the tools (or ML models). When conducting data management, researchers need to manipulate the data, tools or ML models in person. So we can call the traditional paradigm as user-driven paradigm. On the other hand, in the new data management paradigm, an LLM-based copilot will take over to manipulate the data management pipeline, and the researchers will only send high-level requests to the copilot to initiate the pipeline and get involved when required. We can call the new paradigm as AI-driven paradigm."}, {"title": "B. System Components", "content": "There are two types of components in our framework: internal components and external components. Internal components are the integral parts of the ADMA Copilot, including a copilot server, three LLM-based Agents, a meta-program graph, a data&tool registry. The ADMA Copilot talks with external components such as the original ADMA system, sensors, external cloud storage and computing services, external tools & APIs, and finally, a web front-end. The following subsections, we'll introduce each main components.\n1) Copilot Server: The copilot sever is the hub of the whole system, which is responsible for coordinating other components. The copilot server receives instructions from the web front-end and output the results to the front-end. When receiving a task, the copilot server will search in its data & tool registry for relevant tools to use and build a meta-program graph. The copilot server also talks to three LLM-based agents and meta-program graph, to decide the tools to call in each step and to format the input/output of the meta-program. When the tool to call is determined, the copilot server will reach out to the specified tool, execute the call, and retrieve the results from the tool.\nThe design principle of the copilot server is to decouple the programmatic part from the \"intelligent\" part, by only keeping the programmatic part within its own logic and off-loading the intelligent part to the three LLM-based agents. In this way, the logic of copilot server can be kept at its minimal.\n2) LLM-based Agents: There are three LLM-based agents: a program controller, an input formatter and an output formatter, all of which will accept current task, meta-program graph and the execution history as input. The program controller is responsible for determining the control flow of the program, namely, which tool to execute in next step, by looking into the status of current meta-program graph and taking into account of the task to accomplish. The program controller has the highest requirements for the reasoning capability of the underlying LLM, because it needs to analyze current task and conduct the planning accordingly, and for some complicated tasks, the planning always requires high reasoning capability. The other two LLM-based agents, input formatter and output formmater, control the data flow, by translating the input to the value of the variable or translating the value to output, respectively.\nThree agents are coordinated by the copilot server and are triggered when required to keep the control flow and data flow of the program on track, thus finally solve the task proposed by the end user.\n3) Meta-Program Graph: Meta-program graph is a in-dispensable component which encodes the meta-structure of the tools and data. Through meta-program, the for-mentioned LLM-based agents can understand what a tool or data is about and make collected decisions on controlling the control flow and data flow in completing the task. It is the meta-program that makes data flow decoupled from the control flow and as a result, increase the predictability of the result.\nWe design the meta-program in a concise format, which contains all the necessary information about the data and tools, while keeping it small enough to fit in the context window of a typical LLM. Also the meta-program graph is extensible to contain hundreds or thousands of tools, which will help to solve complicated tasks containing tens or hundreds of steps of actions.\n4) Data & Tool Registry: The meta data about tools and data can be store in data & tool registry for the copilot to reference. When the number of tools grows really large, it is not possible to store the information about all the tools and affiliate variables in meta-program graph, so in this case, copilot will first retrieve relevant tools to construct the meta-program graph, and also retrieve the data to initialize relevant variables. Also the registry can store user and session information, which makes it act as a long-term memory of the copilot.\n5) ADMA: ADMA is short for Agricultural Data Man-agement and Analytics, which is the system we developed before [7]. Here we incorporate ADMA into the framework by connecting the copilot with ADMA. Through ADMA's API, the copilot can manipulate the data and the tools hosted on ADMA, just as a human user. In the copilot's perspective, ADMA can be considered as a set of external tools, which can be integrated into the data management pipeline. We'll demonstrate some use case of the copilot involving ADMA in the following section.\n6) Sensors & IoT Devices: ADMA Copilot can be con-nected to various sensors and IoT devices on the field, through APIs or specifically designed interface. The copilot will treat the sensors the same way as other tools, to collect on-field information such as air temperature, humidity, precipitation, wind speed, soil moisture, nitrogen, carbon dioxide, fertilizer application and etc, which is extremely important for certain tasks involving decision making. Here in our proof-of-concept system, we connect the copilot with John Deere [40] API and Realm5 [41] API, to collect field operation information and weather data respectively. We'll demonstrate some use case of the copilot calling sensors through these APIs in the following section.\n7) External Cloud Services: ADMA copilot can make full use of existing cloud storage and computing service, such as One Drive, Google Drive, Google Cloud, Amazon AWS or Dropbox. These cloud services provide more options for the user to store their data, run their program or host the ML/AI models. Copilot will register the interface for these cloud services and help the user to manage their data, tools or models on these service. For our proof-or-concept copilot, we connect it with Google Drive. Autonomous data movement between Google Drive and ADMA is demonstrated in the next section.\n8) External Tools: Copilot can be connected to various other external tools, through APIs or specifically designed interface. When new tools get connected, they will be reg-istered in the data & tool registry and later when needed, copilot can discover and call them. These feature assures the extensibility of our system and make it applicable to any application scenario.\n9) Web Front-end: A conversation based web front-end is provided to the end user, who can type in any data manage-ment instruction to the copilot. Once the copilot completes the instruction (or fails it), the final response will be displayed back to the end user on the web front. The specific format of the response will depend on the instruction. It can be in natural language, a table, a plot, a UI gadget, a web page or a image. The following section will contain several use cases about the results showing on the web front-end."}, {"title": "IV. EVALUATION", "content": "In this section, we demonstrate the use cases of ADMA Copilot to prove its core features such as intelligence, autonomy, efficacy, efficiency, extensibility, flexibility and privacy."}, {"title": "A. Interface", "content": "ADMA Copilot has a conversational interface, where the user can type in the instruction into the chat box at the bottom of the web page. Then the results will be displayed above the chat box. All the chat history within a session will be preserved. In Figure 4, when the user asks \"go to ADMA\u201d, then the main page of ADMA will be displayed. ADMA Copilot will also display the steps it has gone through."}, {"title": "B. Intelligence", "content": "ADMA Copilot can understand user intent in natural language. Sometimes, even the user input a fuzzy or somewhat ambigous instruction, the copilot will try its best to interpret the user's intent. Figure 5 illustrate a case in which the user type in the instruction: \"I want to know how to use ADMA.\u201d Then the copilot will display the documentation page of the ADMA because this is the result which is most appropriate in this context."}, {"title": "C. Autonomy", "content": "One of the key features of ADMA copilot is autonomy. It is designed to try its best to accomplish the tasks proposed by users autonomously, except extra user input is required. For each task, the program controller of the copilot will route on the meta-program graph and complete several steps until the task is finally accomplished."}, {"title": "D. Efficacy", "content": "The design of our copilot reduces the phenomena of hallucination of LLM to its minimal, by decoupling data flow from control flow. That is, when generating the calling interface of any tool, instead of asking LLM-based controller to do so, we use the value of the variable within meta-program graph. If there are not enough information, the copilot will ask for further input from the user, rather than fabricate some plausible value. This will make sure the predictability of the final results."}, {"title": "E. Efficiency", "content": "Autonomy not only brings about convenience, but also efficiency. For some complicated tasks which contain several steps, the copilot demonstrates its efficiency compared with human users. Here we conduct a comparison between ADMA copilot and human user, in terms of task completion time for several data management tasks."}, {"title": "F. Extensibility", "content": "To accommodate more tools, ADMA Copilot is designed to be able to incorporate new tools which will be incorporated in the future. Given a documentation of any new API or function interface, the only thing we need to do is to plug in the tool into the meta-program graph and inform the copilot how to use the new tools. Then all things left will be handled to the copilot without any changes as before. The copilot will make calls to the new tools when required. For instance, when we design a new user interface which can draw a map for the fields on our extension farm, we can just plug in the description of the UI tool into meta-program graph, and then when the user inputs \"I want to see the map for the field named 1863N\", the output will be like in Figure 9"}, {"title": "G. Flexibility", "content": "Thanks to the reasoning capability of the underlying LLM, our copilot demonstrates some degree of flexibility in terms of understanding the user's intent based on a wide spectrum of user input. The user can be free from precisely phrasing the instruction, and the user input can be fuzzy or even ambiguous. Nevertheless, the copilot will try its best to interpret the user's instruction and execute the task accordingly.\nIn Figure 10a, the user inputs \"download the file adma_test/test.txt on Google drive\u201d, then the copilot will download the specified file and display a download button on the interface. In Figure 10b, the user typs in \"I need to check the file adma_test/test.txt on Google drive\", the copilot can get the same intent as previous input and still display a download button on the interface."}, {"title": "H. Privacy and Authentication", "content": "ADMA Copilot will protect the user's data and privacy, by ask for user's credentials to authenticate when the user need to get access to protected services or data. In Figure 11a, the user want to \"list my Google drive root folder\", then the copilot decides it is a protected resource and ask for the user to go through the authentication process for Google drive. In Figure 11b, the user types in \"download the Realm5 data on 2024/5/1 on ADMA\", then the copilot decides this is also a protected data and asks the user to input ADMA token."}, {"title": "I. Comparison", "content": "By adopting similar evaluation frameworks as in [7], we compare ADMA Copilot with the existing agriculture plat-forms. The evaluation criteria and corresponding explanation are listed in Table I. These criteria cover a broad range of dimensions, from the basic requirements for a typical data management platform to more innovative features, which also take into consideration the principles of FAIR. The eleven dimensions of criteria are, namely, privacy and security, CRUD repository, interoperability, extensibility, data model, data granularity, analytical tools, tractability, ML Models, and Intelligence and Autonomy.\nWe make qualitative comparison between ADMA Copilot with existing agricultural data management platforms includ-ing CyVerse, GARDIAN, GEMS, TERRAREF and ADMA. Table II shows the comparison result. We mark \u00d7 when a platform satisfies certain criteria and leave the cell empty otherwise.\nADMA Copilot meets all the criteria under consideration. While this comparison is qualitative and may introduce bias due to the selection and specific definition of evaluation criteria, the results suggest that our platform excels in the chosen dimensions. This indicates that the design principles of our system address a broader range of potential user needs and interests, positioning it well for future requirements."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose and explore the idea of a LLM based multi-agent copilot for autonomous agricultural data management. Based on our previous developed platform: Agricultural Data Management and Analytics, we build a system called ADMA Copilot, which can understand user's intent, makes plan for data processing pipeline and accomplishes the task automatically, in which three agents: a LLM based controller, an input formatter and an output formatter collaborate together. Different from existing LLM based solu-tions, by defining a meta-program graph, our work decouples control flow and data flow to enhance the predictability of the behaviour of the agents. As a proof-of-concept product, ADMA Copilot can help to facilitate the shift of agricultural data management paradigm from traditionally human-driven paradigm to AI-driven paradigm. Experiments demonstrates the intelligence, autonomy, efficacy, efficiency, extensibility, flexibility and privacy of our system. Comparison is also made between ours and existing systems to show the superiority and potential of our system."}]}