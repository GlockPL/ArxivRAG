{"title": "Classifier-Free Guidance is a Predictor-Corrector", "authors": ["Arwen Bradley", "Preetum Nakkiran"], "abstract": "We investigate the theoretical foundations of classifier-free guidance (CFG). CFG is the dominant method of conditional sampling for text-to-image diffusion models, yet unlike other aspects of diffusion, it remains on shaky theoretical footing. In this paper, we first disprove common misconceptions, by showing that CFG interacts differently with DDPM (Ho et al., 2020) and DDIM (Song et al., 2021), and neither sampler with CFG generates the gamma-powered distribution $p(x|c)^\\gamma p(x)^{1-\\gamma}$. Then, we clarify the behavior of CFG by showing that it is a kind of predictor-corrector method (Song et al., 2020) that alternates between denoising and sharpening, which we call predictor-corrector guidance (PCG). We prove that in the SDE limit, CFG is actually equivalent to combining a DDIM predictor for the conditional distribution together with a Langevin dynamics corrector for a gamma-powered distribution (with a carefully chosen gamma). Our work thus provides a lens to theoretically understand CFG by embedding it in a broader design space of principled sampling methods.", "sections": [{"title": "Introduction", "content": "Classifier-free-guidance (CFG) has become an essential part of modern diffusion models, especially in text-to-image applications (Dieleman, 2022; Rombach et al., 2022; Nichol et al., 2021; Podell et al., 2023). CFG is intended to improve conditional sampling, e.g. generating images conditioned on a given class label or text prompt (Ho and Salimans, 2022). The traditional (non-CFG) way to do conditional sampling is to simply train a model for the conditional distribution $p(x | c)$, including the conditioning c as auxiliary input to the model. In the context of diffusion, this means training a model to approximate the conditional score $s(x, t, c) := \\nabla_x \\log p_t (x | c)$ at every noise level t, and sampling from this model via a standard diffusion sampler (e.g. DDPM). Interestingly, this standard way of conditioning usually does not perform well for diffusion models, for reasons that are unclear. In the text-to-image case for example, the generated samples tend to be visually incoherent and not faithful to the prompt, even for large-scale models (Ho and Salimans, 2022; Rombach et al., 2022).\nGuidance methods, such as CFG and its predecessor classifier guidance (Sohl-Dickstein et al., 2015; Song et al., 2020; Dhariwal and Nichol, 2021), are methods introduced to improve the quality of conditional samples. During training, CFG requires learning a model for both the unconditional and conditional scores ($\\nabla_x \\log p_t (x)$ and $\\nabla_x \\log p_t (x|c)$). Then, during sampling, CFG runs any standard diffusion sampler (like DDPM or DDIM), but replaces the true conditional scores with the \u201cCFG scores\"\n$\\S(x, t, c) := \\gamma\\nabla_x \\log p_t (x | c) + (1 - \\gamma)\\nabla\\log p_t(x)$,\nfor some $\\gamma > 0$. This turns out to produce much more coherent samples in practice, and so CFG is used in almost all modern text-to-image diffusion models (Dieleman, 2022). A common intuition for why CFG works starts by observing that Equation (1) is the score of a gamma-powered distribution:\n$p_{t,\\gamma}(x|c) := p_t(x)^{1-\\gamma}p_t(x|c)^{\\gamma}$,"}, {"title": "Preliminaries", "content": "We adopt the continuous-time stochastic differential equation (SDE) formalism of diffusion from Song et al. (2020). These continuous-time results can be translated to discrete-time algorithms; we give explicit algorithm descriptions for our experiments."}, {"title": "Diffusion Samplers", "content": "Forward diffusion processes start with a conditional data distribution $p_0(x|c)$ and gradually corrupt it with Gaussian noise, with $p_t (x|c)$ denoting the noisy distribution at time t. The forward diffusion runs up to a time T large enough that $p_T$ is approximately pure noise. To sample from the data distribution, we first sample from the Gaussian distribution $p_T$ and then run the diffusion process in reverse (which requires an estimate of the score, usually learned by a neural network). A variety of samplers have been developed to perform this reversal. DDPM (Ho et al., 2020) and DDIM (Song et al., 2021) are standard samplers that correspond to discretizations of a reverse-SDE and reverse-ODE, respectively. Due to this correspondence, we refer to the reverse-SDE as DDPM and the reverse-ODE as DDIM for short. We will mainly consider the variance-preserving (VP) diffusion process from Ho et al. (2020), although most of our discussion applies equally to other settings (such as variance-exploding). The forward process, reverse-SDE, and equivalent reverse-ODE for the VP conditional diffusion are (Song et al., 2020)\nForward SDE : $dx = \\frac{1}{2} \\beta_tx dt + \\sqrt{\\beta_t}dw$.\nDDPM SDE: $dx = \\frac{1}{2} \\beta_tx dt \u2013 \\beta_t\\nabla_x \\log p_t(x|c)dt + \\sqrt{\\beta_t}dw$\nDDIM ODE : $dx = -\\frac{1}{2} \\beta_tx dt - \\frac{1}{2} \\beta_t \\nabla_x \\log p_t(x|c)dt$.\nThe unconditional version of each sampler simply replaces $p_t(x|c)$ with $p_t(x)$. Note that the score $\\nabla_x \\log p_t (x|c)$ appears in both (4) and (5). Intuitively, the score points in a direction toward higher probability, and so it helps to reverse the forward diffusion process. The score is unknown in general, but can be learned via standard diffusion training methods."}, {"title": "Classifier-Free Guidance", "content": "CFG replaces the usual conditional score $\\nabla_x \\log p_t (x|c)$ in (4) or (5) at each timestep t with the alternative score $\\nabla_x \\log p_{t,\\gamma}(x|c)$. In SDE form, the CFG updates are\nCFGDDPM: $dx = -\\frac{1}{2} \\beta_tx dt - \\beta_t\\nabla_x \\log p_{t,\\gamma}(x|c)dt + \\sqrt{\\beta_t}dw$\nCFGDDim: $dx = -\\frac{1}{2} \\beta_tx dt - \\frac{1}{2} \\beta_t \\nabla_x \\log p_{t,\\gamma}(x|c)dt,$\nwhere $\\nabla_x \\log p_{t,\\gamma}(x|c) = (1 - \\gamma)\\nabla_x \\log p_t(x) + \\gamma\\nabla_x \\log p_t (x|c)$."}, {"title": "Langevin Dynamics", "content": "Langevin dynamics (Rossky et al., 1978; Parisi, 1981) is another sampling method, which starts from an arbitrary initial distribution and iteratively transforms it into a desired one. Langevin dynamics (LD) is given by the following SDE (Robert et al., 1999)\ndx = $\\frac{1}{2}$\u2207log p(x)dt + \u221a\u03b5dw.\nLD converges (under some assumptions) to the steady-state p(x) (Roberts and Tweedie, 1996). That is, letting ps(x) denote the solution of LD at time s, we have lims\u2192\u221e ps(x) = p(x). Similar to diffusion sampling, LD requires the score of the desired distribution p (or a learned estimate of it)."}, {"title": "Misconceptions about CFG", "content": "We first observe that the exact definition of CFG matters: specifically, the sampler with which it used. Without CFG, DDPM and DDIM generate equivalent distributions. However, we will prove that with CFG, DDPM and DDIM can generate different distributions, as follows:"}, {"title": "Counterexample 1", "content": "We first present a setting that allows us to exactly solve the ODE and SDE dynamics of CFG in closed-form, and hence to find the exact distribution sampled by running CFG. This would be intractable in general, but it is possible for a specific problem, as follows.\nConsider the setting where $p_0(x)$ and $p_0(x|c = 0)$ are both zero-mean Gaussians, but with different variances. Specifically, $(x_0, c)$ are jointly Gaussian, with $p(c) = \\mathcal{N}(0, 1)$, $p_0(x|c) = c + \\mathcal{N}(0, 1)$. Therefore\n$p_0(x) = \\mathcal{N}(0, 2)$\n$p_0(x|c = 0) = \\mathcal{N}(0, 1)$\n$p_{0,\\gamma}(x|c = 0) = \\mathcal{N}(0, \\frac{2}{\\gamma + 1})$\nFor this problem, we can solve CFGDDIM (7) and CFGDDPM (6) analytically; that is, we solve initial-value problems for the reversed dynamics to find the sampled distribution of $\\hat{x}_t$ in terms of the initial-value x. Applying these results to t = 0 and averaging over the known Gaussian distribution of x gives the exact distribution of $\\hat{x}_0$ that CFG samples. The full derivation is in Appendix A.1. The final CFG-sampled distributions are:\nCFGDDPM: $\\hat{x}_0 \\sim \\mathcal{N}\\left(0, \\frac{2^{\\frac{2\\gamma - 2}{\\gamma + 1}}}{2\\gamma - 1}\\right)$\nCFGDDim : $\\hat{x}_0 \\sim \\mathcal{N} \\left(0, 2^{1-\\gamma} \\right)$.\nThis shows that for any $\\gamma > 1$, the CFGDDIM distribution is sharper than the CFGDDPM distribution, and both are sharper than the gamma-powered distribution $p_{0,\\gamma}(x|c = 0)$. (Even though the distributions all have the same mean, their different variances make them distinct.) In fact, for $\\gamma \\gg 1$, the variance of DDPM-CFG is approximately $\\frac{2}{2\\gamma - 1}$, which is about twice the variance of $p_{0,\\gamma}(x|c = 0)$."}, {"title": "Counterexample 2", "content": "In the above counterexample, the CFGDDIM, CFGDDPM, and gamma-powered distributions had different variances but the same Gaussian form, so one might wonder whether the distributions"}, {"title": "CFG as a predictor-corrector", "content": "The previous sections illustrated the subtlety in understanding CFG. We can now state our main structural characterization, that CFG is equivalent to a special kind of predictor-corrector method (Song et al., 2020)."}, {"title": "Predictor-Corrector Guidance", "content": "As a warm-up, suppose we actually wanted to sample from the gamma-powered distribution:\n$p_{\\gamma}(x|c) \\propto p(x)^{1-\\gamma}p(x|c)^{\\gamma}$.\nA natural strategy is to run Langevin dynamics w.r.t. $p_\\gamma$. This is possible in theory because we can compute the score of $p_\\gamma$ from the known scores of p(x) and p(x | c):\n$\\nabla_x \\log p_{\\gamma}(x | c) = (1 - \\gamma)\\nabla_x \\log p(x) + \\gamma\\nabla_x \\log p(x | c)$.\nHowever this won't work in practice, due to the well-known issue that vanilla Langevin dynamics has impractically slow mixing times for many distributions of interest (Song and Ermon, 2019). The usual remedy for this is to use some kind of annealing, and the success of diffusion teaches us that the diffusion process defines a good annealing path (Song et al., 2020; Du et al., 2023). Combining these ideas yields an algorithm remarkably similar to the predictor-corrector methods introduced in Song et al. (2020). For example, consider the following diffusion-like iteration, starting from $x_\\tau \\sim \\mathcal{N}(0, \\sigma_\\tau)$ at t = T. At timestep t,\n1. Predictor: Take one diffusion denoising step (e.g. DDIM or DDPM) w.r.t. $p_t(x | c)$, using score $\\nabla_x \\log p_t (x | c)$, to move to time t' = t \u2013 \u2206t.\n2. Corrector: Take one (or more) Langevin dynamics steps w.r.t. distribution $p_{t',\\gamma}$, using score\n$\\nabla_x \\log p_{t',\\gamma}(x | c) = (1 - \\gamma)\\nabla_x \\log p_t (x) + \\gamma\\nabla_x \\log p_t (x | c)$."}, {"title": "SDE limit of PCG", "content": "Consider the version of PCG defined in Algorithm 1, which uses DDIM as predictor and a particular LD on the gamma-powered distribution as corrector. We take K = 1, i.e. a single LD step per iteration. Crucially, we set the LD step size such that the Langevin noise scale exactly matches the noise scale of a (hypothetical) DDPM step at the current time (similar to Du et al. (2023)). In the limit as \u2206t \u2192 0, Algorithm 1 becomes the following SDE (see Appendix B):\ndx = ADDIM(x, t) + \u2206LD\u2084(x, t, \u03b3) =: \u2206PCGddim (x, t, \u03b3),\nPredictor\nCorrector\nwhere ADDIM(x, t) = $-\\frac{1}{2} \\beta_t(x - \\nabla \\log p_t (x|c))dt$\n$\\Delta LD_G(x, t, \\gamma) = -\\frac{\\varepsilon}{2} ((1 - \\gamma)\\nabla_x \\log p_t(x) + \\gamma \\nabla_x \\log p_t(x|c))dt + \\sqrt{\\beta_t}dw$.\nAbove, ADDIM(x, t) is the differential of the DDIM ODE (5), i.e. the ODE can be written as dx = ADDIM(x, t). And \u2206LD\u2084(x, t, \u03b3), where G stands for \u201cguidance\u201d, is the limit as \u2206t \u2192 0 of the Langevin dynamics step in PCG, which behaves like a differential of LD (see Appendix B)."}, {"title": "Discussion and Related Works", "content": "There have been many recent works toward understanding CFG. To better situate our work, it helps to first discuss the overall research agenda."}, {"title": "Understanding CFG: The Big Picture", "content": "We want to study the question of why CFG helps in practice: specifically, why it improves both image quality and prompt adherence, compared to conditional sampling. We can approach this question by applying a standard generalization decomposition. Let p(x|c) be the \u201cground truth\u201d population distribution; let $p^* (x|c)$ be the distribution generated by the ideal CFG sampler, which exactly solves the CFG reverse SDE for the ground-truth scores (note that at $\\gamma = 1$, $p_\\gamma(x|c) = p(x|c)$); and let $p_\\gamma(x|c)$ denote the distribution of the real CFG sampler, with learnt scores and finite discretization. Now, for any image distribution q, let PerceivedQuality[q] \u2208 R denote a measure of perceived sample quality of this distribution to humans. We cannot mathematically specify this notion of quality, but we will assume it exists for analysis. Notably, PerceivedQuality is not a measurement of how close a distribution is to the ground-truth p(x|c) \u2013 it is possible for a generated distribution to appear even \u201chigher quality\" than the ground-truth, for example. We can now decompose:\nPerceived Quality [$p_{\\gamma}$] = PerceivedQuality[$p$] \u2013 (PerceivedQuality[$p$] \u2013 PerceivedQuality[$p_\\gamma$])."}, {"title": "Open Questions and Limitations", "content": "In addition to the above, there are a number of other questions left open by our work. First, we study only the stochastic variant of CFG (i.e. CFGDDPM), and it is not clear how to adapt our analysis to the more commonly used deterministic variant (CFGDDIM). This is subtle because the two CFG variants can behave very differently in theory, but appear to behave similarly in practice. It is thus open to identify plausible theoretical conditions which explain this similarity\u00b9; we give a suggestive experiment in Figure 6. More broadly, it is open to find explicit characterizations of CFG's output distribution, in terms of the original p(x) and p(x|c) \u2013 although it is possible tractable expressions do not exist.\nFinally, we presented PCG primarily as a tool to understand CFG, not as a practical algorithm in itself. Nevertheless, the PCG framework outlines a broad family of guided samplers, which may be promising to explore in practice. For example, the predictor can be any diffusion denoiser, including CFG itself. The corrector can operate on any distribution with a known score, including compositional distributions as in Du et al. (2023), or any other distribution that might help sharpen or otherwise improve on the conditional distribution. Finally, the number of Langevin steps could be adapted to the timestep, similar to Kynk\u00e4\u00e4nniemi et al. (2024), or alternative samplers could be considered (Du et al., 2023; Neal, 2012; Ma et al., 2015)."}, {"title": "Stable Diffusion Examples", "content": "We include several examples running predictor-corrector guidance on Stable Diffusion XL (Podell et al., 2023). These serve primarily to sanity-check our theory, not as a suggestion for practice. For all experiments, we use PCGDDIM as implemented explicitly in Algorithm 2\u00b2. Note that PCG offers a more flexible design space than standard CFG; e.g. we can run multiple corrector steps for each denoising step to improve the quality of samples (controlled by parameter K in Algorithm 2).\nFigure 1 illustrates the equivalence of Theorem 3: we compare CFGDDPM with guidance \u03b3 to PCGDDIM with exponent \u03b3' := (2\u03b3 \u2013 1). We run CFGDDPM with 200 denoising steps, and PCGDDIM with 100 denoising steps and K = 1 Langevin corrector step per denoising step. Corresponding samples appear to have qualitatively similar guidance strengths, consistent with our theory.\nIn Figure 5 we show samples from PCGDDIM, varying the guidance strength and Langevin iterations (i.e. parameters \u03b3 and K respectively in Algorithm 2). We also include standard CFGDDIM samples for comparison. All samples used 1000 denoising steps for the base predictor. Overall, we observed that increasing Langevin steps tends to improve the overall image quality, while increasing guidance strength tends to improve prompt adherence. In particular, sufficiently many Langevin steps can sometimes yield high-quality conditional samples, even without any guidance (\u03b3 = 1); see Figure 7 in the Appendix for another such example. This is consistent with the observations of Song et al. (2020) on unguided predictor-corrector methods. It is also related to the findings of Du et al. (2023) on MCMC methods: Du et al. (2023) similarly use an annealed Langevin dynamics with reverse-diffusion annealing, although they focus on general compositions of distributions rather than the specific gamma-powered distribution of CFG.\nNotice that in Figure 5, increasing the number of Langevin steps appears to also increase the \"effective\" guidance strength. This is because the dynamics does not fully mix: one Langevin step (K = 1) does not suffice to fully converge the intermediate distributions to pt,\u03b3."}, {"title": "Conclusion", "content": "In this paper, we have shown that while CFG is not a diffusion sampler on the gamma-powered data distribution $p_0(x)^{1-\\gamma}p_0(x|c)^{\\gamma}$, it can be understood as a particular kind of predictor-corrector, where the predictor is a DDIM denoiser, and the corrector at each step t is one step of Langevin dynamics on the gamma-powered noisy distribution $p_t(x)^{1-\\gamma'}p_t(x|c)^{\\gamma'}$, with $\\gamma' = (2\\gamma \u2013 1)$.\nAlthough Song et al. (2020)'s Predictor-Corrector algorithm has not been widely adopted in practice, perhaps due to its computation expense relative to samplers like DPM++ (Lu et al., 2022b), it turns out to provide a lens to understand the unreasonable practical success of CFG. On a practical note, PCG encompasses a rich design space of possible predictors and correctors for future exploration, that may help improve the prompt-alignment, diversity, and quality of diffusion generation."}, {"title": "1D Gaussian Counterexamples", "content": ""}, {"title": "Counterexample 1 Detail", "content": "Counterexample 1 (equation 10) has\n$p_0(x) \\sim \\mathcal{N}(0, 2)$\n$p_0(x|c = 0) \\sim \\mathcal{N}(0, 1)$.\nThe \u03b3-powered distribution is\n$p_{0,\\gamma}(x|c = 0) = p_0(x|c)^\\gamma p_{c=0}(x)^{1-\\gamma}$\n$\\propto e^{-\\frac{x^2}{2}-(2-1)\\frac{x^2}{4}} = e^{-\\frac{x^2}{\\frac{4}{2+1}}}$\n$\\sim \\mathcal{N}(0, \\frac{2}{\\gamma+1})$.\nWe consider the simple variance-exploding diffusion defined by the SDE\ndx = \u221atdw.\nThe DDIM sampler is a discretization of the reverse ODE\n$\\frac{dx}{dt} = \\frac{1}{2} \\nabla_x log p_t (x)$,\nand the DDPM sampler is a discretization of the reverse SDE\ndx = \u2212\u2207x log Pt,y(x)dt + dw.\nFor CFGDDim or CFGDDPM, we replace the score with CFG score \u2207x log Pt,y(x).\nDuring training we run the forward process until some time t = T, at which point we assume it is fully-noised, so that approximately\n$p_T(x|c = 0) \\sim \\mathcal{N}(0, T)$\n(in this case the exact distribution $p_T(x|c = 0) \\sim \\mathcal{N}(0, T + 1)$ so we need to choose T \u226b 1 to ensure sufficient terminal noise). At inference time we choose an initial sample $x_T \\sim \\mathcal{N}(0, T)$ and run CFGDDIM from t = T \u2192 0 to obtain a final sample xo.\nFor Counterexample 1, the CFGDDIM ODE has a closed-form solution (derivation in section A.5):\n: $\\frac{dx}{dt} = \\frac{1}{2} \\nabla_x log p_{t,y}(x)$\n$\\mathcal{CFG_{DDIM}}: \\frac{x_T\\left( \\frac{\\gamma }{(2(1+t))}-\\frac{(1-\\gamma) }{(2(2 + t))}\\right)}{\\sqrt{\\frac{(T+1)}{(2+t)}}}$"}, {"title": "Counterexample 2", "content": "Counterexample 2 (10) is a Gaussian mixture with equal weights and variances.\nc\u2208 {0, 1}, p(c = 0) = $\\frac{1}{2}$\n$p_0(x_0|c) \\sim \\mathcal{N}(\\mu(c), 1)$, \u03bc(0) = \u2212\u03bc, \u03bc(1) = \u03bc\n$p_0(x_0) \\sim \\frac{1}{2} p_0(x_0|c=0) + \\frac{1}{2} p_0(x_0|c=1)$.\nWe noted in the main text that if \u03bc is sufficiently large enough that the clusters are approximately"}, {"title": "Counterexample 3", "content": "We consider a 3-cluster problem to investigate why CFGDDIM and CFGDDPM often appear similar in practice despite being different in theory. Counterexample 3 (10) is a Gaussian mixture with equal weights and variances. We vary the variance to investigate its effect on CFG.\nc\u2208 {0, 1, 2}, p(c) = $\\frac{1}{3}$\n$p_0(x_0|c) \\sim \\mathcal{N}(\\mu(c), \u03c3)$, \u03bc(0) = \u22123, \u03bc(1) = 0, \u03bc(2) = 3\n$p_0(x_0) \\sim \\frac{1}{3} p_0(x_0|c=0) + \\frac{1}{3} p_0(x_0|c=1) + \\frac{1}{3} p_0(x_0|c=2)$.\nWe run CFGDDim and CFGDDPM with \u03b3 = 3, for \u03c3 = 1 and \u03c3 = 2. Results are shown in Figure 6."}, {"title": "Generalization Example 4", "content": "We consider a multi-cluster problem to explore the impact of guidance on generalization:"}]}