{"title": "LLM Surgery: Efficient Knowledge Unlearning and Editing in Large Language Models", "authors": ["Akshaj Kumar Veldanda", "Shi-Xiong Zhang", "Anirban Das", "Stephen Rawls", "Sambit Sahu", "Supriyo Chakraborty", "Milind Naphade"], "abstract": "Large language models (LLMs) have revolutionized various domains, yet their utility comes with significant challenges related to outdated or problematic knowledge embedded during pretraining. This paper addresses the challenge of modifying LLMs to unlearn problematic and outdated information while efficiently integrating new knowledge without retraining from scratch. Here, we propose LLM Surgery, a framework to efficiently modify LLM behaviour by optimizing a three component objective function that: (1) Performs reverse gradient on unlearning dataset (problematic and outdated information), (2) Performs gradient descent on the update dataset (new and updated information), and (3) Minimizes the KL divergence on the retain dataset (small subset of unchanged text), ensuring alignment between pretrained and modified model outputs. Due to the lack of publicly available datasets specifically tailored for our novel task, we compiled a new dataset and an evaluation benchmark. Using Llama2-7B, we demonstrate that LLM Surgery can achieve significant forgetting on the unlearn set, a 20% increase in accuracy on the update set, and maintain performance on the retain set.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) excel across various tasks [28, 37, 35, 30], but they rely on extensive internet-sourced data [31, 1], which often includes problematic content such as copyrighted material and personal information [36]. This can lead to LLMs inadvertently memorizing and generating such data [24]. Additionally, LLMs are limited by the temporal scope of their training data, lacking awareness of events or updates after training [14, 3], which risks generating outdated or incorrect information. Retraining LLMs to address these issues is computationally expensive; for example, pretraining Llama-3 requires 7.7 million GPU hours and produces significant CO2 emissions [4].\nTo ensure LLMs remain relevant and beneficial, it is essential to develop methods that allow models to (1) unlearn problematic or outdated data, (2) incorporate new information, and (3) maintain performance on standard benchmarks. Without these capabilities, LLMs pose significant legal and ethical risks [34], as shown by lawsuits filed by content creators, such as the New York Times, against companies using copyrighted material for training [26]. As LLMs become more widespread, the risk of perpetuating outdated or legally sensitive content grows, leading to severe consequences [36].\nTo address these challenges, we propose LLM Surgery, a framework for efficiently updating LLM knowledge to ensure it remains current, accurate, and legally compliant. Our key contributions are:"}, {"title": "2 Proposed Methodology", "content": "Given three datasets the unlearn dataset (Dunl), the update dataset (Dupd), and the retain dataset (Drtn) - our goal is to modify a LLM, Apre, so that the resulting LLM, 0surgery, meets the following criteria: (1) it unlearns specific information from Apre based on Dunl, (2) updates Apre with new knowledge from Dupd, and (3) maintains the same level of performance as Apre on standard benchmark tasks using Drtn. To achieve these goals efficiently, we propose the LLM Surgery framework.\nLLM Surgery Objective optimizes a objective function, as defined in Equation 1. This function is designed to accomplish three key objectives: (1) Unlearning: The first term in Equation 1 encourages the model to unlearn specific information by performing gradient ascent on Dunl. This ensures that the model parameters are optimized along the reverse gradient direction of the learned patterns in Dunl, effectively erasing targeted knowledge, (2) Updating: The second term facilitates learning of new information by applying gradient descent on the Dupd. This ensures the model integrates the updated knowledge, and (3) Retention: The third term minimizes the KL-divergence between the output distributions of surgery and Apre on Drtn. This ensures that the model retains essential knowledge that should remain unaffected by LLM Surgery. Mathematically, it can be expressed as follows:\n$L = \\frac{1}{|D^{unl}|} \\sum_{i=1}^{|D^{unl}|} \\sum_{t=2}^{|x^{unl}_{i}|} l(P[ \\cdot | x^{unl}_{i;<t}; \\theta^{surgery}], e^{unl}_{i;t}) + \\frac{1}{|D^{upd}|} \\sum_{i=1}^{|D^{upd}|} \\sum_{t=2}^{|x^{upd}_{i}|} l(P[ \\cdot | x^{upd}_{i;<t}; \\theta^{surgery}], e^{upd}_{i;t}) + \\frac{1}{|D^{rtn}|} \\sum_{i=1}^{|D^{rtn}|} \\sum_{t=2}^{|x^{rtn}_{i}|} 2KL(P[ \\cdot | x^{rtn}_{i;<t}; \\theta^{surgery}] || P[ \\cdot | x^{rtn}_{i;<t}; \\theta^{pre}])$ (1)\nIn this equation, i denotes the ith example in the dataset, where 1 \u2264 i \u2264 |D|, and |D| represents the size of the dataset. zunle Dunl, upd e Dupd, and n \u2208 Drtn represent sequences of tokens from the unlearning, update, and retain datasets, respectively. The notation P[\u00b7 | Xi;<t; 0] refers to the predicted probability distribution over the possible next tokens at position t, given the preceding tokens x<t and the model parameters 0. The vector et is the one-hot encoded representation of the true token at position t, and l(\u00b7, \u00b7) denotes the loss function, typically cross-entropy."}, {"title": "3 Experimental Setup", "content": "We conduct experiments using the Llama2-7B model [27] to evaluate the effectiveness of the LLM Surgery framework in unlearning information from Dunl, updating the model with new knowledge from Dupd, and retaining performance on Drtn. As a preliminary step, we continually-pretrain off-the-shelf Llama2-7B model on both Dunl and Drtn. The purpose of this continual-pretraining phase is to ensure that the model possesses the knowledge of Dunl and Drtn prior to performing LLM Surgery. We refer to this continually-pretrained model as opre.\nBaseline Model: A naive approach would involve continually-pretraining off-the-shelf Llama2-7B model using only the update dataset Dupd and the full retain dataset Drtn. This model is referred"}, {"title": "3.1 Data", "content": "Given the lack of publicly available datasets specifically tailored for our task, we compiled a novel dataset and evaluation benchmark. This benchmark includes multiple-choice question-answer (MCQA) examples generated using GPT-4, which consist of questions, three answer choices, and labels indicating the correct choice. Accuracy is used as the evaluation metric. Below, we describe the construction of the dataset and characteristics of the evaluation benchmark:\nUnlearn Dataset (Dunl): We derive the unlearning dataset from the original Wikipedia biographies in the WikiBio GPT-3 Hallucination Dataset [18], consisting of 238 true biographies. This dataset is divided into two subsets: (1) a subset of 119 biographies (50k tokens) used to simulate problematic data, such as personal information or copyrighted material, and (2) a subset of 119 biographies (50k tokens) representing outdated information. To assess the effectiveness of the LLM Surgery framework to unlearn this content from pre, we generate 2,400 MCQA examples based on the unlearn dataset and used them for evaluating surgery's performance on the unlearning data.\nUpdate Dataset (Dupd): The update dataset is constructed by generating 119 fictitious biographies (80k tokens) in Wikipedia style using GPT-4 [2]. These biographies correspond to the same 119 subjects in the outdated subset of the unlearning dataset. By generating these fictitious biographies, we ensure that pre has no prior exposure to this content before undergoing LLM Surgery. To assess the effectiveness of the LLM Surgery framework in incorporating this new information, we generate 2,400 MCQA examples based on the update set and used them for evaluating surgery's performance.\nRetain Dataset (Drtn): To ensure that the modifications to the model do not degrade the performance on unrelated tasks, we create a retain dataset comprising 2 million tokens from the Open-WebText dataset [12] and 1 billion tokens from the RedPajama-v1 dataset [8]. While both pre and ebaseline utilize the entire retain dataset, \u018fsurgery only uses a small subset of Drtn to preserve the model's original performance on unrelated tasks. To assess the effectiveness of the LLM Surgery framework in retaining performance, we developed an evaluation benchmark consisting of 20,000 MCQA examples from the 2 million tokens in the OpenWebText dataset to ensure that surgery's performance remains consistent with pre. Additionally, we evaluate surgery on the ARC-Easy [7] benchmark to assess its ability to retain performance comparable to off-the-shelf Llama2-7B model."}, {"title": "4 Results", "content": "Effectiveness of LLM Surgery In Table 1, we empirically demonstrate the effectiveness of the LLM Surgery framework by analyzing the accuracy of the continually-pretrained Llama2-7B model across the unlearn, update, and retain evaluation benchmarks, both before (pre) and after (surgery) applying LLM Surgery. The results indicate that \u0472surgery shows a 19% reduction in accuracy compared to pre on the unlearn benchmark. Additionally, on the unlearn benchmark, surgery exhibits"}, {"title": "Ablation Study 1: LLM Surgery with Gradient Descent Only (@gd)", "content": "We remove the gradient ascent and KL-divergence terms, applying only the gradient descent term. As expected, relying solely on gradient descent for the update dataset improved accuracy of Agd on the update benchmark from 44.6% to 66.8%, compared to pre (shown in Table 1). However, this approach resulted in Agd having a 10% higher accuracy on the unlearn benchmark compared to surgery, indicating insufficient unlearning. Moreover, accuracy on the retain set dropped by 8% relative to pre, suggesting that using gradient descent alone can lead to undesirable forgetting of previously learned tasks."}, {"title": "Ablation Study 2: LLM Surgery with Gradient Descent and KL-Divergence (@gd + kl)", "content": "To address the forgetting issue on Drtn, in this ablation, we include both the gradient descent and KL-divergence terms, omitting the gradient ascent term. This approach preserves the performance of 0gd + kl on Drtn, compared to pre. However, it fails to unlearn the targeted information, as shown by the lack of accuracy drop on the unlearn benchmark. The influence of the KL-divergence term likely prevents effective unlearning. This outcome is undesirable, as \u0189gd + kl retains the information it was meant to unlearn. In other words, the gradient ascent term is essential for effective unlearning, as it enables surgery to achieve a 19% reduction in accuracy on the unlearn benchmark, compared to pre, proving that all three terms are necessary for efficiently and effectively modifying LLM behavior."}, {"title": "Activation Pattern Analysis", "content": "As shown in Figure 1, the activation patterns remain consistent across the three datasets\u2014Dunl, Dupd, and Drtn\u2014for both the off-the-shelf Llama2-7B and epre. This is likely because Llama2-7B was pretrained on original Wikipedia biographies (Dunl), as well as OpenWebText and RedPajama-v1 (Drtn). The same knowledge is reinforced in pre prior to LLM Surgery, resulting in highly similar activation patterns for off-the-shelf Llama2-7B and Opre. After LLM Surgery, however, surgery shows distinct activation patterns across Dunl and Dupd, as LLM"}, {"title": "5 Related Works", "content": "Several methods have been proposed for unlearning in LLMs [16, 5, 11, 6, 17, 25]. One notable approach [32] shares our goal of removing harmful or hallucinated content but relies on costly fine-tuning with extensive input-output annotations and requires an additional annotated dataset to ensure coherence post-unlearning, limiting scalability. Another research direction focuses on editing structured knowledge by modifying model weights [29, 21, 23], adjusting architectures [13], or localizing knowledge [9, 19, 20, 10]. However, these approaches are typically effective for only a small number of concepts [15, 33] and fail to address unlearning of outdated information, leaving models vulnerable to information extraction attacks [22].\nIn contrast, our method unifies unlearning and editing, enabling the removal of problematic data while updating the model. We bypass costly annotations with a continual-pretraining objective, making our approach scalable to unstructured data and large token sets. Our work also addresses"}, {"title": "6 Conclusion", "content": "We propose LLM Surgery, an efficient framework for unstructured knowledge unlearning and editing in LLMs without retraining from scratch. The proposed LLM Surgery optimization function: (1) Performs reverse gradient on the unlearning dataset (problematic and outdated information), (2) Performs gradient descent on the update dataset (new and updated information), and (3) Minimizes the KL divergence on the retain dataset (small subset of unchanged text), ensuring alignment between the pretrained and modified model outputs. Due to the lack of publicly available datasets specifically tailored for our novel task, we compiled a new dataset and an evaluation benchmark. Using Llama2-7B, we demonstrate that LLM Surgery can achieve significant forgetting on the unlearn set, a 20% increase in accuracy on the update set, and maintain performance on the retain set."}]}