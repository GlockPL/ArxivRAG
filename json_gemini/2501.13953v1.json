{"title": "Redundancy Principles for MLLMs Benchmarks", "authors": ["Zicheng Zhang", "Xiangyu Zhao", "Xinyu Fang", "Chunyi Li", "Xiaohong Liu", "Xiongkuo Min", "Haodong Duan", "Kai Chen", "Guangtao Zhai"], "abstract": "With the rapid iteration of Multi-modality Large Language Models (MLLMs) and the evolving demands of the field, the number of benchmarks produced annually has surged into the hundreds. The rapid growth has inevitably led to significant redundancy among benchmarks. Therefore, it is crucial to take a step back and critically assess the current state of redundancy and propose targeted principles for constructing effective MLLM benchmarks. In this paper, we focus on redundancy from three key perspectives: 1) Redundancy of benchmark capability dimensions, 2) Redundancy in the number of test questions, and 3) Cross-benchmark redundancy within specific domains. Through the comprehensive analysis over hundreds of MLLMs' performance across more than 20 benchmarks, we aim to quantitatively measure the level of redundancy lies in existing MLLM evaluations, provide valuable insights to guide the future development of MLLM benchmarks, and offer strategies to refine and address redundancy issues effectively.", "sections": [{"title": "1. Introduction", "content": "Model Evaluation has always played a crucial role in the development of Multi-modal Large Language Models (MLLMs). Benchmarks serve not only as tools for assessing model accuracy but also as catalysts for driving innovation and improvements within the field. In recent years, with the rapid advancement of MLLMs, there has been an explosive growth in Visual Question Answering (VQA) Benchmarks. In the early stages, traditional model evaluation benchmarks such as GQA [13], VQA-V2 [2], VizWiz [4], and TextVQA [29] are characterized by relatively simple questions and answers, with responses often being a single word. This limits the depth of understanding and reasoning required from the models, making them less effective at evaluating the complex capabilities of modern MLLMs that are expected to handle more nuanced and context-dependent tasks. With the emergence of more powerful MLLMs [1, 6, 18, 21, 30, 32], traditional evaluation frameworks have become inadequate to meet the flexible evaluation requirements of these advanced models. In re-"}, {"title": "1.1. Identifying Redundancy", "content": "Redundancy is an intrinsic and multifaceted issue in MLLM benchmarks, appearing in several key forms:\n\u2022 Redundancy across dimensions (intra-bench): Tasks within the same benchmark may evaluate overlapping capabilities of MLLMs, leading to repetitive assessments.\n\u2022 Redundancy among instances (intra-bench): Certain instances closely resemble others, providing minimal additional differentiation or insight for model evaluation.\n\u2022 Redundancy across benchmarks within specific domains: Benchmarks targeting specific domains often exhibit overlapping objectives or scopes, resulting in duplicated efforts across different evaluation sets."}, {"title": "1.2. Ideal Redundancy Principles", "content": "Effective benchmarks should adhere to the following principles regarding redundancy:\n\u2022 Independence of dimensions: Ideal benchmarks should ensure that its dimensions are largely independent, minimizing overlap between them. However, some degree of redundancy may be inevitable when certain capabilities naturally require the interaction of multiple foundational skills, and redundancy should be carefully balanced to avoid excessive overlap while ensuring valid evaluation.\n\u2022 Optimal instance count: A well-designed benchmark should strike a balance in the number of instances it includes: neither too few nor too many, to ensure reliable and meaningful evaluations without introducing unnecessary redundancies.\n\u2022 Domain representativeness: A comprehensive benchmark targeted to a specific domain should meaningfully"}, {"title": "1.3. Benifits of Evaluating Redundancy", "content": "Evaluating and addressing redundancy offers several significant benefits, as shown in Fig. 1:\n\u2022 Optimizing benchmark design: 1). Determines whether certain dimensions within a benchmark warrant separate assessments or can be consolidated; 2). Identifies the minimal and sufficient number of instances required to accurately assess model performance; 3). Assesses the necessity of introducing new benchmarks within specific domains.\n\u2022 Enhancing efficiency in MLLM evaluation: 1). Determines whether a benchmark deviates from the domain's distribution; 2). Identifies the anchor benchmarks required to evaluate model performance within the domain.\nBy systematically addressing redundancy, we not only enhance the principles of benchmark design but also alleviate the resource demands of MLLM evaluation, creating a more streamlined and effective evaluation ecosystem."}, {"title": "2. Redundancy Framework", "content": "We present a framework for evaluating redundancy among MLLM capabilities, defined as specific tasks within a benchmark. Our framework is grounded in the following prior assumption:\nWhen evaluating similar capabilities, the performance rankings of MLLMs should exhibit strong correlation. Conversely, significant differences in these rankings suggest the evaluated capabilities are relatively independent.\nBased on this principle, we propose the Performance Correlation Redundancy Framework, which quantifies redundancy by measuring the correlation of MLLM performance rankings. To ensure robustness and generalization capability, we leverage the comprehensive data from VLMEvalKit [7], which includes diverse benchmarks and performance results from more than 100 MLLMs."}, {"title": "2.1. Dimensions Redundancy", "content": "Assume a benchmark consists of a set of dimensions, denoted as $X = \\{X_1,X_2, ..., X_m\\}$, where each $X_i$ represents a specific dimension. Let $N$ denote the number of MLLMs evaluated on these dimensions. For a given dimension $X_i$, we denote the ranking of the $N$ MLLMs on this dimension as $R_i$. To quantify the redundancy of $X_i$, we compute the average rank correlation between $R_i$ and the rankings $R_j$ of all other dimensions $X_j (j \\neq i)$. Formally, the redundancy $p(X_i)$ is defined as:"}, {"title": "2.2. Instances Redundancy", "content": "Let a benchmark contain M total instances (e.g., QA pairs). To evaluate redundancy, we begin by calculating the MLLM performance rankings obtained over the full set of all M instances, denoted as the ground-truth ranking $R_{GT}$. We then randomly sample a subset of the instances, comprising A% of the total M, and compute the corresponding MLLM rankings, denoted as $R_{sample}$. To quantify the redundancy of the benchmark at a sampling ratio of A%, we calculate the correlation coefficient between $R_{sample}$ and $R_{GT}$. This correlation reflects how representative the sampled subset is of the entire benchmark. To reduce the effect of randomness, the sampling process is repeated T = 100 times, and the average correlation result is recorded. We define the instance redundancy of the benchmark at sampling ratio A%, denoted as $p(A\\%)$, as follows:\n\u2022 A higher $p(\u0391\\%)$ indicates that the sampled instances are highly representative of the entire benchmark, and the remaining 1 - A% instances contribute little additional information, indicating redundancy."}, {"title": "2.3. Cross-Benchmark Redundancy", "content": "Consider $Y = \\{Y_1, Y_2,..., Y_l\\}$, a collection of l benchmarks within a specific domain (e.g., object hallucination, visual reasoning, visual perception). Let N represent the number of MLLMs evaluated across these benchmarks. For a given benchmark $Y_i$, let $K_i$ denote the ranking of the N MLLMs based on their performance on $Y_i$. To identify key anchor benchmarks within this domain (an anchor benchmark can serve as a representative over multiple other benchmarks), we focus on selecting benchmarks that demonstrate high redundancy with other benchmarks in the domain [43]. We define the redundancy of a benchmark $p(Y_i)$ as the average rank correlation coefficient between $K_i$ and the rankings $K_j$ of all other benchmarks $Y_j (j \\neq i)$ in the domain. Formally, $p(Y_i)$ is expressed as:"}, {"title": "2.4. Correlation Metrics", "content": "In this work, we adopt multiple metrics to describe the correlation between two set of performance numbers, including the Spearman Rank Correlation Coefficient (SRCC), the Pearson Linear Correlation Coefficient (PLCC), and the R2 Score (R-squared Coefficient of Determination).\n\u2022 SRCC is an evaluation metric that measures rank similarity, capturing how well the relative order between two rankings aligns.\n\u2022 PLCC quantifies linear similarity, assessing how closely the rankings follow a linear relationship.\n\u2022 R2 Score, on the other hand, evaluates the proportion of variance explained by the ranking relationship, serving as a measure of goodness-of-fit."}, {"title": "2.5. Top-K Analysis", "content": "Considering that the performance of top-tier MLLMs often garners greater attention on benchmarks, we can streamline the redundancy analysis by focusing only on the top-K MLLMs with the highest overall performance on a given benchmark, rather than incorporating all MLLMs in the calculation. By selecting the top-K models, we can better target the analysis of benchmark redundancy across different performance tiers. This approach also simplifies the process of maintaining and updating our framework as new MLLMs are introduced."}, {"title": "3. Experiment & Discussion", "content": "We use the evaluation results of hundreds of MLLMs obtained through the VLMEvalKit [7] as our data source for conducting experiments and analysis. All the data sources we used have been open-sourced on HuggingFace 1."}, {"title": "3.1. Exploring Dimension Redundancy", "content": "To comprehensively demonstrate the application of our redundancy framework in MLLM benchmarks, we conduct a detailed case study using the widely adopted and dimensionally diverse MMBench benchmark (v1.1) [24]. We categorize the MLLMs into two groups, Top-50 and Bottom-50, based on their overall performance in MMBench. This categorization enables us to highlight the differences in redundancy exhibited by MMBench when evaluating MLLMs with varying levels of capability. The results for the Top-50 and Bottom-50 groups are illustrated in Fig. 3 and Fig. 4, respectively, from which we derived several interesting insights."}, {"title": "3.2. Exploration Instance Redundancy", "content": "We include the evaluation results from 18 publicly available benchmarks in VLMEvalKit [7] in our experiments, with the average performance across benchmarks presented in Fig. 5. We adopt a similarity threshold of 0.95 for partitioning2, This leads to an intriguing conclusion: a majority of existing MLLM benchmarks exhibit significant redundancy in their instances when ranking both Top-50 and Bottom-50 MLLMs, with at least 50% of the instances being redundant. This indicates that many benchmarks could reduce their instance counts by half without significantly affecting the ranking of MLLMs being tested. The R2 score provides further insight, as it measures how effectively the final performance of MLLMs can be predicted using sampled instances. Compared to ensuring accurate ranking, achieving high accuracy in predicting the absolute performance of MLLMs requires a much larger number of instances. For example, both Top-50 and Bottom-50 MLLMs require over 90% of the instances to achieve an R2 score greater than 0.95. This distinction highlights that fewer instances are sufficient for reliable ranking than for precise performance prediction.\nWe also compare redundancy tendencies between Top-50 and Bottom-50 MLLMs, as shown in Figs. 5a and 5b. Notably, at the same 0.95 threshold for SRCC and PLCC, Bottom-50 MLLMs require significantly fewer instances than Top-50 MLLMs. This implies that accurately ranking higher-performing MLLMs (Top-50) demands more in-"}, {"title": "3.3. Exploring Cross-Benchmark Redundancy", "content": "To analyze cross-benchmark redundancy, we focus on the Math domain, specifically examining several popular mathematical benchmarks: MathVista [26], MathVision [40], MathVerse [31], and DynaMath [44]. We utilize the available evaluation results of 37 MLLMs listed on the Open-"}, {"title": "4. Redundancy Practice Recommendations", "content": "To ensure benchmarks are reliable and efficient, we recommend incorporating redundancy detection into the benchmark design process after its initial testing on a set of MLLMs. This critical step identifies potential redundancies across dimensions/instances/cross-benchmark overlaps, leading to more precise and meaningful evaluations.\nDimension Redundancy Check. Calculate the dimensional redundancy within the benchmark, with particular attention to dimensions exhibiting overall high redundancy. Analyze the redundancy heatmap to identify pairs of dimensions with exceptionally strong correlations, as these may indicate overlapping capabilities being assessed. For such cases, evaluate whether these dimensions are truly necessary or whether they assess similar or redundant skills.\nInstance Redundancy Check. Compute the instance redundancy curve to determine whether a smaller subset of benchmark instances can produce results comparable to the full instance set. If significant instance redundancy is identified, the benchmark should be reviewed, and redundant instances should be reduced. This not only streamlines the evaluation process but also optimizes resource usage without compromising the accuracy of results.\nCross-benchmark Redundancy Check. If the benchmark is intended to serve as a representative for a specific domain, measure its cross-benchmark redundancy relative to other benchmarks within the domain. Higher redundancy indicates stronger representativeness, making it a reliable choice for tasks requiring domain coverage. Conversely, if the goal is to fill a vacancy in the specific domain (e.g., focusing on a specific topic in mathematics that is not covered by previous benchmarks) maintaining low redundancy is a more favorable choice. For use cases focusing on core capabilities within a specific domain under limited resources, it is recommended to select the benchmark with the highest cross-benchmark redundancy. This ensures that the benchmark comprehensively covers the essential skills while minimizing unnecessary overlaps."}, {"title": "5. Conclusion", "content": "In conclusion, this paper addresses the pervasive issue of redundancy in MLLM benchmarks, impacting both the effectiveness and efficiency of model evaluation. We identify redundancy at three levels: dimension, instance, and cross-benchmark redundancy, and propose a framework with actionable guidelines to improve benchmark design. By promoting independence of dimensions, optimizing instance counts, and ensuring purposeful redundancy within specific domains, our framework streamlines evaluations and enhances reliability. Case studies further demonstrate its utility in refining current practices, paving the way for more efficient and accurate MLLM assessments."}, {"title": "6. Metrics Equation", "content": "To evaluate the consistency and accuracy of predictions, we employ three widely used metrics: the Spearman Rank Correlation Coefficient (SRCC), the Pearson Linear Correlation Coefficient (PLCC), and the Coefficient of Determination ($R^2$). These metrics provide complementary perspectives on model performance, capturing rank-based, linear, and variance-explained relationships, respectively. The mathematical definitions are detailed below.\n1) The SRCC measures the rank-based relationship between predicted and true values. It is defined as:"}, {"title": "2. Trends in Top-50 vs. Bottom-50 Redundancy:", "content": "\u2022 A clear pattern emerges when comparing the Top-50 and Bottom-50 redundancy maps. Nearly all Bottom-50 dimensions display significantly higher redundancy than their Top-50 counterparts. This observation supports our conclusion that dimensions tend to exhibit greater redundancy for Bottom-50 MLLMs compared to Top-50 MLLMs.\n\u2022 This phenomenon can be attributed to the overall underperformance of Bottom-50 MLLMs across various capabilities. As these models begin to improve, enhancements in their foundational abilities often lead to simultaneous progress across multiple dimensions. This results in a high degree of similarity in performance rankings, contributing to elevated dimensional redundancy.\n\u2022 In contrast, Top-50 MLLMs already possess relatively strong foundational capabilities. As a result, more challenging tasks across different dimensions introduce greater differentiation, reducing redundancy and creating more distinct performance profiles."}, {"title": "3. Implications for Redundancy Analysis:", "content": "\u2022 To ensure a reasonable and accurate evaluation during redundancy analysis, it is crucial to exclude MLLMs with consistently poor performance. Including such models could skew the analysis by disproportionately inflating redundancy, as their universal underperformance does not provide meaningful insights into inter-dimensional relationships."}]}