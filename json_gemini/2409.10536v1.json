{"title": "Evaluating the Potential Functions of an International Institution for AI Safety \u2013\nAn Institutional Analysis of the IAEA and IPCC, in the Context of Recent\nTrends in AI Safety", "authors": ["Arcangelo Leone de Castris (The Alan Turing Institute)", "Christopher Thomas (The Alan Turing Institute)"], "abstract": "Governments, industry, and other actors involved in governing AI technologies around the world\nagree that, while AI promises to deliver tremendous benefits to society, appropriate guardrails are\nrequired to mitigate the risks related to this technology. International institutions \u2013 including the\nOECD, the G7, the G20, UNESCO, and the Council of Europe \u2013 have already started developing\nframeworks for ethical and responsible AI governance. While these are important initial steps,\nalone they fall short of addressing the need for institutionalised international processes to identify\nand assess potentially harmful AI capabilities. Contributing to the ongoing conversation on how\nto address this gap, this chapter reflects on the opportunity to establish an international institution\nfor AI safety. Based on the analysis of existing international institutions addressing safety\nconsiderations in adjacent policy areas and the newly established national AI safety institutes in\nthe UK and US, this chapter identifies a list of concrete functions which could be performed by an\ninternational institution for Al safety. Understanding the impact of international governance\ninstitutions developed to address safety risks in other policy domains provides insights into the\npotential tools at the disposal on an international AI Safety Institution, be it a wholly new\ninternational body or a network of national safety institutions and/or other existing fora. Remaining\nagnostic to its institutional structure, our analysis suggests that the potential functions for an\ninternational institution for AI safety can be categorised into three domains: (a) technical research\nand cooperation, (b) safeguards and evaluations, and (c) policymaking and governance support.", "sections": [{"title": "Introduction", "content": "In response to the rapid commercialisation of generative AI systems, 2023 saw several calls for\nglobal AI safety regulation from prominent figures across industry and policy (Chowdhury, 2023;\nAltman et al., 2023; Suleyman & Schmidt, 2023; Milmo [Hassabis,] 2023; Prime Minister's\nOffice, 10 Downing Street, 2023; Gutierres, 2023). As a result, the relevant scholarly conversation\nhas increasingly focused on questions related to the establishment of international institutions for\nthe governance of advanced AI (Ho et al., 2023; Marcus & Reuel, 2023; Roberts et al., 2023;\nVeale et al., 2023). The run-up to the AI Safety Summit hosted by the UK in November 2023, in\nparticular, saw calls for international institutions for AI safety analogous to the International Panel\non Climate Change (IPCC) and the International Atomic Energy Agency (IAEA). However, critics\nargue that drawing inaccurate parallels between different policy domains risks distorting\nconversations around Al regulation and that an AI safety institution modelled on the IPCC or IAEA\nexamples would inherit the same problems that have limited the impact of these institutions\n(Broughel, 2023; Afina & Lewis, 2023; Stewart, 2023; De Pryck et al., 2022). However, even if\nthese examples are not directly applicable to the international governance of AI, considering\ngovernance models established in adjacent policy domains could still provide valuable lessons for\nthis nascent field (Milmo [Hassabis] 2023).\nTo investigate this assumption, in this chapter we offer an institutional analysis of the International\nAtomic Energy Agency (IAEA) and the International Panel on Climate change (IPCC) (Steinmo\net al., 1992). We investigate the functions and policy outcomes of these institutions to determine\ntheir relevance to governance processes for AI safety. We compare these findings with the latest\nefforts to establish national AI safety institutes in the UK and US. By contextualising the analysis\nof these case studies within current developments in AI safety and the broader AI governance\nlandscape, we extend the current literature, offering an interpretation of the potential functions that\nan international Al safety institution could perform. This analysis is underpinned by a mapping\nreview of the relevant literature, situating AI safety within the broader fields of AI ethics and\ngovernance, and providing a clear target for our analysis (Booth, 2016). Limited space in this\nchapter prevents us from offering a broader comparison to other international institutions (e.g. the\nInternational Financial Stability Board) and more recently emerging AI safety institutions (e.g.,\nthe newly established AI safety institutes in Canada, Singapore, and South Korea). These remain\nimportant avenues for further research.\nThe first section of this chapter sets out the relationship between contemporary interpretations of\nAI safety focused on \u2018advanced Al' and concepts of AI ethics and governance (Jobin et al., 2019),\nsituating the establishment of AI safety institutions within the broader global AI ethics and\ngovernance landscape. In Sections II and III, respectively, we offer a case study analysis of the\nInternational Atomic Energy Agency (IAEA) and the International Panel on Climate Change\n(IPCC), exploring their successes and failures as well as the relevance of these models for AI\nsafety. In Section IV we assess the potential of these analogies for informing the development of\nAI safety institutions. The IPCC and the IAEA are the focus of this short chapter due to their\nprominence in the global conversation around AI safety. Hence, we argue that there is a need for\na robust and timely analysis of their potential pros and cons. In section V, we analyse recent"}, {"title": "Situating 'safety' within the context of AI ethics and governance", "content": "The conversation around AI safety is closely connected to the two broader concepts of AI ethics\nand AI governance. AI ethics encompasses:\n\u201ca set of values, principles, and techniques that employ widely accepted standards of right and\nwrong to guide moral conduct in the development and use of AI technologies\u201d (Leslie, 2019).\nWithin AI ethics, \u201csafety\u201d is used to represent one of the principles that can guide moral conduct\nin the pursuit of ethical AI. The translation of abstract ethical principles into actionable\norganisational processes is one component of the broader category of AI governance, which refers\nto:\n\u201ca system of rules, practices, processes, and technological tools that are employed to ensure an\norganization's use of AI technologies aligns with the organization's strategies, objectives, and\nvalues; fulfills legal requirements; and meets principles of ethical AI followed by the organization\u201d\n(M\u00e4ntym\u00e4ki et al. 2022).\nIt is important to situate proposals for an international AI safety institute within the AI governance\ncontext, for two reasons: first, this context enables us to specify the role of an AI safety institute\nrelative to other, complementary mechanisms for Al governance; secondly, this context is\nimportant because AI safety regulation won't happen in a vacuum, rather it will sit within a broader\ncorpus of AI standards, principles, and international agreements, as well as a broader array of\nregulatory access points beyond Al governance such as platform governance, data protection,\nprocurement, international trade agreements, etc. As such, AI safety can be considered as a subset\nof Al governance.\nFor the purposes of this chapter, we characterise AI safety as a combination of both negative and\npositive conditions: (a) \u201cthe absence of unacceptable risk or harm caused by the use of AI\u201d, and\n(b) \"protections against the unacceptable risk or harm caused by the use of AI\u201d (Habli, 2023). In\nthis sense, the role of an AI safety institute would be to put in place protections, preventing or\nmitigating risks posed by harmful AI capabilities. The current conversations around \u2018AI safety'\nhave been shaped by the recent, rapid commercialisation of foundation models and, as a result,\nfocus primarily on \u2018advanced' AI capabilities (Marcus & Reuel, 2023). Due to their broad potential\napplications and rapid adoption, foundational models have intensified the governance debate\nfocused on ensuring that these systems benefit humanity, including by promoting sustainable and\nequitable development and managing their potentially negative global externalities (Ho et al.,\n2023). Beyond the scholarly literature, the focus on Al safety has gained significant political\ntraction, with AI safety institutions being set up across the world to address the risks from the"}, {"title": "Nuclear governance and the International Atomic Energy", "content": "Created in 1957, in the context of post-war nuclear proliferation, the International Agency for\nAtomic Energy (IAEA) marked the beginning of global nuclear governance. The IAEA is an\nintergovernmental forum representing 178 countries and serving as the global focal point for\nscientific and technical cooperation in the nuclear field.\nThe IAEA's mission stems from its statute (1956) and the Treaty on Non-Proliferation on Nuclear\nWeapons (NPT, 1970), which designates the IAEA as its verification agency, and encompasses"}, {"title": "Climate governance and the International Panel on Climate Change", "content": "Established in 1998 by the United Nations Environment Programme (UNEP) and the World\nMeteorological Organization (WMO), the IPCC is an intergovernmental institution bringing\ntogether 195 member states and several Observer Organisations. The IPCC's primary function is\nto provide governments with regular and independent assessments of climate change science that\nthey can use to inform climate policies. Although relevant from a policymaking perspective, the\nIPCC's climate science assessments are not \u2018policy-prescriptive' (Vardy et al., 2017; Bolin, 2007).\nThe highest organ of the Panel is its plenary. Here, government representatives meet to decide on\nstrategic matters including the work programme, structure and mandate of the working groups,"}, {"title": "Relevance of the IAEA and IPCC models for an International AI Safety Institute", "content": "The IAEA and IPCC models represent two of the most notable examples of how to govern global\npolicy issues. Considering the transnational nature and impact of AI technologies, policy observers\nhave looked at these two models as references to inform possible international AI safety processes.\nFor instance, both models deal in their own ways with questions related to risk assessment,\nmanagement, knowledge sharing, cooperation and foresight related to processes that can pose\nsignificant threats to our society. For example, useful parallels can be drawn between climate\nchange and AI regarding the challenge of dealing with the fundamental uncertainty characterising\nthe quality and impact of related risks. At the same time, however, the problems that the IAEA\nand IPCC address are substantively different from those raised by AI. While adaptation to\nexogenous processes is a key strategic priority in climate policy, and controlling nuclear\nproliferation for military purposes is at the core of the IAEA mission, the essence of AI safety is\nabout ensuring that the companies developing these technologies do so in a safe and responsible\nway. Therefore, if drawing connections between governance models in different policy domains"}, {"title": "Ongoing efforts to institutionalise AI safety", "content": "The AI Safety Summit hosted by the UK in November 2023 was arguably the first explicit\ninternational effort towards institutionalising Al safety functions. One of the objectives of the\nSummit was, in fact, to establish a process for international collaboration to address the risks of\n\u2018frontier AI.' Although the Summit failed to deliver actionable harmonised processes for\naddressing risks connected to advanced AI technologies, a noteworthy output of the first AI Safety\nSummit was the Bletchley Declaration. Signed by 28 countries and the EU, the statement formally\nrecognised the need for a global approach to understanding the impact of AI on society. Among\nother things, signatories committed to supporting an international network\n\u201c...of scientific research on frontier AI safety that encompasses and complements existing and new\nmultilateral, plurilateral and bilateral collaboration...to facilitate the provision of the best science\navailable for policy making and the public good\u201d (DSIT, 2023)\nConsidering that a follow-up AI Safety Summit has been convened in South Korea in May 2024,\nwith France next to take on hosting duties in Autumn 2024, this institutionalised process holds the\npotential to evolve into the first regular mechanism of intergovernmental cooperation specifically\ntargeted at Al safety. The Seoul Ministerial statement, signed by 27 countries and the EU\nreaffirmed the need for \u201ccollaborative international approaches to respond to rapid advancements"}, {"title": "Potential functions of an international AI safety institute", "content": "The UK and US national AI safety institutes represent an important step towards identifying and\nmitigating the potentially harmful impact of Al on society. However, the more advanced and\ndiffused AI technologies become, the more pressing the need for effective and dedicated\ninternational coordination on Al safety will be. With a view to establishing an international\ninstitution specifically focused on AI safety, the UKAISI and USAISI provide useful references\non the types of functions that such a body could perform, albeit reflecting UK and US perspectives\non what an Al safety institute can practically do. Building on the examples offered by the UKAISI\nand USAISI, our analysis of the IPCC and IAEA models, and gaps identified by the UN Interim\nReport on Al for Humanity (2023), we detail below a list of the core functions that an international\nAI safety institution could perform. The functions are clustered around three thematic categories:\na) technical research and cooperation; b) safeguards and evaluations; c) policymaking and\ngovernance support.\na) Technical research and cooperation\n\u2022\n\u2022\n\u2022\nAssessing Al safety science and horizon scanning\nRegularly assess the state of AI safety science globally, identify current and future risks,\nand assess their potential impact on society and the viability of solutions proposed to\naddress them.\nSupporting and enabling independent AI safety research\nStrategically identify priority research areas by curating a regularly updated research plan\nand providing financial and technical support to carry out foundational research.\nFacilitating international multistakeholder cooperation and knowledge sharing\nProvide a hub for coordinated investigative efforts across a network of national and\ninternational stakeholders committed to independent AI safety research, and enable the\ndissemination of technical information including safety and security incidents to\nsupport better decision-making of both AI users and developers.\nTechnical support and assistance to improve access, equity, and capability with regard to\nless technically advanced nations including access to compute, training and test data.\nSupport cooperation with less technically advanced nations to ensure global buy-in, and\ninhibit technological trajectories outside of agreed norms.\nIdentify and institute mechanisms to streamline input from national agencies on topics\nwithin their direct purviews\nEnsure integration with broader national AI policy initiatives and support the adaptation of\nnational approaches to continuous development and commercialisation of advanced AI\nsystems.\nCoordinating and aligning approaches between national AI safety institutions\nEnable the practical implementation of testing and evaluation and the adaptation of these\nfunctions to distinct contextual risks around the globe.\nb) Safeguards and evaluations\n\u2022\nDeveloping processes and methodologies for AI model evaluations\nDevelop an internationally shared set of tools and benchmarks for evaluating potentially\ndangerous capabilities of AI and mitigating risks. These will have to be applied consistently\nacross an international network of national Al safety institutes and other independent\norganisations performing AI testing to support global harmonisation of the ways Al risks\nare assessed.\nEvaluating AI models\nPerform testing and evaluation of strategically selected AI models developed across the\nworld. Testing and evaluation include red-teaming, human uplift evaluations, AI agent\nevaluations, etc.\nSetting AI safety standards and best practices\nLeverage the network of national AI safety institutes and other organisations committed to\nAl safety to set globally recognised safety standards and best practices for developing,\ndeploying, and procuring AI technologies.\nEstablishing powers to inspect models\nExtend current voluntary evaluation functions of national institutions for the purposes of\nverifying the appropriateness and compliance of organisations' safeguarding practices\nagainst agreed standards.\nHarmonising international standards and coordinating the development of international\nstandards, safety, and risk management frameworks for advanced AI\nEnsure the streamlined and effective development and application of norms for AI safety.\nPublishing evaluation best practices and methodologies\nHarmonise global AI safety evaluation methodologies and approaches, provide global\nleadership and support knowledge transfer.\nc) Policymaking and governance support\n\u2022\nInforming policymaking by providing evidence-based scientific assessments\nSynthesise and assess research developed by national AI safety institutes, and disseminate\nfindings across a network of international and national policymakers to support the\ndevelopment of innovative, evidence-based governance solutions.\nOffering technical guidance to support the development and implementation of norms and\nrules\nDevelop technical resources to facilitate regulators' rulemaking and enforcement on issues\nrelated to Al safety. Fill gaps in national and international approaches, support best\npractices, and ensure responsibility and accountability are incentivised at the national level.\nEnsuring Interoperability and alignment with regional, national, and industry norms\nEnable policymakers to integrate methodologies, standards and recommendations from the\nAI Safety Institute."}, {"title": "Conclusion", "content": "This chapter contributes to the ongoing scholarly and policy conversation around safety as a\ncomponent of international AI governance. It addresses the lack of institutionalised international\nprocesses to identify and assess potentially harmful AI capabilities by reflecting on what functions\nan international Al safety institute could perform. Based on the analysis of both existing\ninternational governance models addressing safety considerations in adjacent policy areas and the\nnewly established national AI safety institutes in the UK and US, our analysis identifies a list of\nconcrete functions that could be performed at the international level. While we do not imply that\ncreating a new body is the only way forward, understanding the structure of these bodies from a\nmodular perspective can help us to identify the tools at our disposal. These, we suggest, can be\ncategorised under three functional domains: a) technical research and cooperation, b) safeguards\nand evaluations, c) policymaking and governance support.\nThis analysis could be extended by future research in at least three complementary directions.\nFirst, by considering not only the functional aspect of institutions like the IPCC and IAEA, but\nalso learnings from their institutional design, including questions of funding, membership,\nstakeholder relationships, and governance. Second, further research should study a wider range of\ninstitutions, to gather more diverse perspectives to inform international AI safety regulation. With\nrespect to policy areas adjacent to AI safety, other organisations responsible for coordinating\ninternational efforts to mitigate different types of risk include the Financial Action Taskforce\n(FATF) and the European Organization for Nuclear Research (CERN). And, most importantly,\nseveral countries are now setting up national AI safety institutes \u2013 including Japan, Singapore and\nCanada. It will be interesting to analyse how their functions map against those of the UKAISI and\nUSAISI. Finally, it is important to consider that there are several options for establishing\ninstitutionalised processes for AI safety at the international level. These include setting up a new\ninternational institution for AI safety, consolidating a network of national safety institutions (as is\ncurrently being pursued), as well as building on existing fora such as GPAI, the UN or the OECD.\nThe pros and cons of each option require further consideration, and future developments should\nbe closely monitored."}]}