{"title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning", "authors": ["Vaidehi Patil", "Elias Stengel-Eskin", "Mohit Bansal"], "abstract": "User specifications or legal frameworks often require information to be removed from pretrained models, including large language models (LLMs). This requires deleting or \"forgetting\" a set of data points from an already-trained model, which typically degrades its performance on other data points. Thus, a balance must be struck between removing information and keeping the model's other abilities intact, with a failure to balance this trade-off leading to poor deletion or an unusable model. To this end, we propose UPCORE (Utility-Preserving Coreset Selection), a method-agnostic data selection framework for mitigating collateral damage during unlearning. Finding that the model damage is correlated with the variance of the model's representations on the forget set, we selectively prune the forget set to remove outliers, thereby minimizing model degradation after unlearning. We evaluate UPCORE across three standard unlearning methods consistently achieving a superior balance between the competing objectives of deletion efficacy and model preservation. To better evaluate this trade-off, we introduce a new metric, measuring the area-under-the-curve (AUC) across standard metrics. We find that UPCORE improves both standard metrics and AUC, benefitting from positive transfer between the coreset and pruned points while reducing negative transfer from the forget set to points outside of it.", "sections": [{"title": "1. Introduction", "content": "The widespread deployment of ML models, particularly large language models (LLMs), has raised significant concerns regarding data privacy, regulatory compliance, and ethical AI practices. These models are often trained on vast amounts of uncurated data scraped from the internet, inher-ently capturing sensitive, copyrighted, or undesirable content (Shokri et al., 2017; Carlini et al., 2019). As regulations like the European Union's General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) empower individuals with the \"right to be forgotten\", the need for efficient techniques that remove specific data or topics from trained models has become increasingly critical. Machine unlearning has emerged as a promising solution, enabling the targeted removal of data, concepts, or facts without the computational expense of retraining from scratch. Moreover, machine unlearning has benefits beyond compliance, addressing broader challenges such as mitigating harmful outputs, preserving intellectual property rights, and aligning LLMs with ethical and societal expectations (Jang et al., 2023). These practical uses have spurred growing interest in understanding, rethinking, and improving model editing and unlearning methodologies (Liu et al., 2024; Hase et al., 2024).\nGiven the growing adoption of LLMs, past work has proposed methods for developing and evaluating techniques for removing knowledge or skills from LLMs (Cao & Yang, 2015; Bourtoule et al., 2021; Nguyen et al., 2022) and steering their behavior in targeted ways (Sinitsin et al., 2020;"}, {"title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning", "content": "Meng et al., 2022). However, these editing methods often involve modifying the model in ways that lead to unintended consequences, such as reduced model utility on tasks or knowledge unrelated to the target. Thus, a balance must be struck between the successful unlearning of undesired information and maintaining the utility of the model by minimizing collateral damage. For this reason, current approaches generally evaluate unlearning or model editing methods based on their efficacy in altering intended knowledge - measured by how successfully a \"forget set\u201d is removed - and based on their collateral effects on unrelated behaviors, i.e. the accuracy on a \"retain set\". This kind of evaluation is especially crucial in realistic settings where unlearning happens on a topic. Such topic-based unlearning commonly arises in practical scenarios, for example, when deleting all information associated with an individual or with sensitive domains (Li et al., 2024). Here, the likelihood of over-generalization to similar topics beyond the domain being deleted is high.\nA key gap in existing research lies in understanding the specific data characteristics that drive overgeneralization and collateral effects during unlearning. While prior work (Sheshadri et al., 2024; Chowdhury et al., 2024) has measured damage resulting from unlearning, it does not investigate how attributes of the data - such as its variance - contribute to collateral damage or whether these attributes can be controlled to optimize the trade-off between deletion efficacy and utility retention. Focusing on a topic-based setting where the forget set comprises semantically coherent groups of information, we seek to address these questions:\n1.  What measurable attributes of the forget set drive collateral effects during the unlearning process?\n2.  Can these attributes be systematically controlled to optimize the trade-off between deletion effectiveness and model utility?\nWe investigate which properties of the forget data correlate with collateral damage during unlearning. Our analysis reveals a strong positive correlation between the variance of the model's hidden states corresponding to datapoints in the forget set (hidden state variance, or HSV), and the extent of collateral damage to the model after unlearning. In other words, unlearning a set of widely-distributed datapoints (as shown in Figure 1 (left)) leads to more damage than unlearning a more densely-distributed set. Building on this insight, we hypothesize that selectively curating a coreset with lower variance from the larger forget set can help optimize this trade-off, as shown in Figure 1 (right).\nTo this end, we introduce UPCORE, which constructs a core forget set by systematically identifying and pruning data points in the forget set that contribute most to the variance and thereby to collateral damage. UPCORE organizes"}, {"title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning", "content": "points into an Isolation Forest (Liu et al., 2008), which identifies anomalous points in a set. By pruning these points, we reduce the variance within the forget set, which we find leads to less damage. Crucially, in addition to reducing collateral damage, UPCORE in fact leverages it by identifying two separate kinds of collateral effects: (1) Negative collateral damage: Unintended degradation of unrelated model capabilities and (2) Positive collateral transfer: The intended impact on pruned data points removed to form the core forget set. This is illustrated in Figure 1, where pruned outlier points are still unlearned \u2013 despite not being a part of the coreset used for unlearning \u2013 due to positive transfer, and is further highlighted by our results in Table 1 and Table 3, which show that UPCORE results in better unlearning than a randomly selected subset while also having better knowledge retention on non-forget data. Moreover, UPCORE is method-agnostic: by focusing solely on the data, UPCORE can be applied to any data-driven unlearning framework.\nWe evaluate UPCORE in prompt completion and question-answering settings and across three standard unlearning methods: Gradient Ascent (Jang et al., 2023), Refusal (Ouyang et al., 2022) and Negative Preference Optimization (NPO) (Zhang et al., 2024c), applying each unlearning algorithm directly to the optimized core forget set obtained using UPCORE, rather than the entire forget set. We measure three critical dimensions: (1) Unlearning effectiveness, measured by the successful removal of targeted knowledge in the forget set, paraphrased versions of removed information as well as prompts attempting to jailbreak the model.; (2) Unintended damage, where we quantify collateral effects on unrelated model capabilities; and (3) Intended transfer, where we analyze the impact on the pruned data points that were removed from the core forget set. While we follow past unlearning work in the metrics we use to measure the trade-off between the competing objectives, we also note that the current suite of metrics measures performance at a fixed point during unlearning. This can make comparisons across methods hard, as the trade-off between deletion efficacy and model utility varies across unlearning epochs. To address this, in addition to showing improvements on standard metrics, we introduce a novel set of metrics that report the area-under-the-curve (AUC) for the standard unlearning metric suite, reporting not just the performance at one fixed timestep, but measuring how a method trades off deletion with model utility across checkpoints (see Figure 3).\nEmpirically, we find that across all three unlearning methods, UPCORE consistently has the highest AUC compared to baselines of unlearning on the complete forget set and choosing a random subset of forget points. In other words, UPCORE forms a Pareto frontier, doing a better job of maximizing unlearning effectiveness while also minimizing model damage. Moreover, UPCORE positively leverages generalization by transferring unlearning from the core set"}, {"title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning", "content": "to the high-variance outlier points that were removed from the core forget set. Notably, it consistently outperforms baselines across all unlearning methods; this holds true when comparing AUC across multiple metrics (e.g. ROUGE on a \"retain\" set, on neighborhood data closely related to the forget set but not in it, etc.). UPCORE's superior trade-off effectively generalizes to variations of the forgotten information, performing well on paraphrased versions of forgotten prompts as well as prompts intended to jailbreak the model. We also see these gains reflected in static evaluations of one checkpoint (as opposed to AUC, which evaluates across checkpoints); here, UPCORE obtains lower (better) ROUGE on the forget set than the random baseline while simultaneously incurring less model damage than the random and complete baselines, with the best (highest) ROUGE across all data not in the forget set."}, {"title": "2. Background and Related Work", "content": "Unlearning Methods for LLMs. Machine unlearning is broadly categorized into exact unlearning, which ensures the model is indistinguishable from one retrained without the forget data, and approximate unlearning, which efficiently modifies existing model parameters to approximate this effect without full retraining. Due to the cost of retraining LLMs, most unlearning applied to LLMs (including ours) falls into the second category. One such approach trains the model to output an uninformative response instead of knowledge in the \"forget set\u201d via RLHF (Ouyang et al., 2022), maximizing the probability of a predefined response like, \u201cI don't know\u201d (Wen et al., 2024). Yao et al. (2023) introduce a gradient ascent-based method for unlearning harmful content, outputting whitespace for harmful prompts. While effective in reducing harmful responses, this approach leads to notable performance degradation on normal prompts, underscoring the need for balance. Chen & Yang (2023) propose an unlearning framework that introduces an unlearning layer, demonstrating success in both classification and generation tasks with less extreme performance degradation. Eldan & Russinovich (2023) explore a novel network architecture designed to specifically unlearn copyrighted content in LLMs. On the evaluation side, Maini et al. (2024) present a benchmark for evaluating unlearning, which we use in our evaluation. Despite these advancements, balancing the trade-off between forgetting accuracy and preserving model utility remains an open challenge, motivating the need for more data-driven solutions. Our work addresses this gap by proposing a principled framework that focuses on minimizing collateral damage through data selection.\nModel Editing for Unlearning. Model editing provides an alternative approach to unlearning by directly modifying model weights to forget target facts (De Cao et al., 2021;"}, {"title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning", "content": "Dai et al., 2022; Mitchell et al., 2022; Meng et al., 2022). This method aligns with privacy requirements (Zhang et al., 2024a), avoids data-side interventions (Debenedetti et al., 2024), and protects against white-box extraction attacks. Following model editing work like Patil et al. (2024b), our framework employs LoRA-based weight updates for controlled unlearning via standard unlearning objectives (See Appendix B.6 for more details).\nCoreset Selection. Coreset selection identifies representative subsets that preserve key dataset properties, improving computational efficiency. Given the NP-hard complexity of the exhaustive search, methods have focused on optimizing coverage, diversity, or importance (Sener & Savarese, 2018; Tan et al., 2023). By recognizing unequal contributions of data points, coreset selection has proven effective in supervised learning (Wei et al., 2015; Killamsetty et al., 2021b;a), enabling efficient performance. Our work forms new connections between these methods and the problem of unlearning in LLMs, where preserving utility and minimizing collateral damage are critical."}, {"title": "3. Methods", "content": "We introduce UPCORE (Utility-Preserving Coreset Design for Unlearning), an approach motivated by the observation that certain data points in the forget set disproportionately contribute to collateral damage during unlearning, primarily by increasing data variance. To address this, UPCORE reformulates pruning for core forget set selection as an outlier detection task, where outlier data points i.e. points with the greatest influence on utility degradation are identified and pruned. By minimizing variance within the forget set, UPCORE reduces unintended negative effects, ensuring more effective and targeted unlearning."}, {"title": "3.1. Problem Definition", "content": "Let $D$ be the dataset used to train a model $M$, with $D_F \\subset D$ representing the forget set to be unlearned. Directly unlearning $D_F$, i.e., applying an unlearning algorithm $\\mathcal{U}$ to the model $M$ using only the data points in $D_F$, produces an updated model $M' = \\mathcal{U}(M, D_F)$. However, this often leads to significant performance degradation on the retained dataset $D \\setminus D_F$ due to over-generalization, where unlearning updates undesirably propagate beyond the forget set, impacting unrelated data points in $D \\setminus D_F$.\nTo address this issue, we aim to construct a pruned forget set $D_C \\subset D_F$, henceforth referred to as the core forget set, by removing points in $D_F$ that disproportionately drive over-generalization. The goal is to balance two competing objectives: (i) minimizing negative collateral damage, i.e., performance degradation on $D \\setminus D_F$, and (ii) maintaining deletion accuracy, i.e., ensuring that the unlearning $\\mathcal{U}$ effectively deletes the undesirable knowledge associated with"}, {"title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning", "content": "the original forget set $D_F$. More formally, given a damage metric $Damage_{(\\mathcal{U},D_C)}(M, M', D \\setminus D_F)$ that quantifies the impact of unlearning $D_C$ on $D \\setminus D_F$, and a deletion accuracy metric $DelAcc_{(\\mathcal{U},D_C)}(M', D_F)$ that evaluates the effectiveness of $\\mathcal{U}$ in forgetting $D_F$ after unlearning on $D_C$, the problem can be formulated as an optimization task:\n$D_C = \\underset{D_C \\subset D_F}{\\text{arg min}} \\big(\\lambda \\cdot Damage_{(\\mathcal{U},D_C)}(M, M', (D\\setminus D_F)) - DelAcc_{(\\mathcal{U},D_C)}(M', D_F)\\big)$,\nwhere $\\lambda > 0$ is a hyperparameter that controls the trade-off between the competing objectives."}, {"title": "3.2. Variance as a Measure of Collateral Damage", "content": "Building on prior work analyzing the cross-task generalization of forgetting methods (Zhang et al., 2024b), we investigate the relationship between attributes of the forget set $D_F$ and their impact on collateral damage during unlearning i.e. $Damage_{(\\mathcal{U},D_C)}(M, M', (D \\setminus D_F))$. Specifically, we identify the variance $Var(D_F)$ as a critical predictor of overgeneralization. To systematically evaluate this relationship, we analyze question-answer (QA) pairs generated from Wikipedia documents across diverse topics. Each topic-specific dataset acts as the forget set $D_F$, with unlearning applied sequentially, one topic at a time. For each forget set, we compute variance using the hidden states of the last token and the penultimate layer of the question. We then compute retain set performance as the model utility metric proposed by Maini et al. (2024), which measures performance on preserved data points after unlearning. The results, visualized in Figure 7a in Appendix B.2, demonstrate a strong negative correlation between HSV and model utility, indicating that variance is a potential driver of over-generalization. These findings underscore the importance of identifying and excluding points that lead to higher variance i.e. outliers to mitigate utility loss. In Appendix B.2 we show similar analyses for other attributes such as model confidence and gradient similarity but find no strong correlation"}, {"title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning", "content": "between utility degradation and these attributes."}, {"title": "3.3. UPCORE: Core Forget Set Selection", "content": "To achieve variance minimization in the forget set $D_F$, UPCORE frames the problem as an outlier detection task. UPCORE provides two key benefits: (1) It mitigates negative collateral damage by pruning outliers to form a more compact core forget set, and (2) It strategically exploits collateral over-generalization to extend unlearning beyond the core forget set, effectively removing the pruned points as well. As shown in Figure 1, what might traditionally be viewed as detrimental collateral damage \u2013 when it affects points outside the forget set ($D_F$) - can be turned to our advantage when it impacts untrained data points within the forget set that were pruned ($D_F \\setminus D_C$).\nTo detect these outliers, we use the Isolation Forest algorithm (Liu et al., 2008), an unsupervised learning technique that efficiently identifies anomalous data points. Isolation Forest works by recursively partitioning the dataset using random feature selections and random split values. Points that are isolated with fewer partitions, i.e. are isolated more easily, are considered outliers, as they differ significantly from the majority of the data. This makes the Isolation Forest algorithm particularly effective for high-dimensional data where traditional distance-based outlier detection methods may fail. These outliers, characterized by their isolation in the feature space, are likely to contribute to high variance and over-generalization during the unlearning process. UPCORE proceeds as follows (illustrated in Figure 2):\nStage 1: Hidden Feature Extraction: We extract hidden state representations $H$ from the model's penultimate layer (See Figure 2 left), corresponding to the final token of each question in $D_F$. These representations, which reflect the model's internal representation of the data, serve as input features for outlier detection. This step is guided by our analysis in Section 3.2, which highlights the strong link between hidden state variance and collateral damage.\nStage 2: Training the Isolation Forest and Computing"}, {"title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning", "content": "Anomaly Scores: We train an Isolation Forest model $\\mathcal{I}$ on the forget set $D_F$ to model its distribution, recursively partitioning the data to detect outliers (see Figure 2 middle). Points isolated more quickly and requiring fewer splits are flagged as outliers, indicating disproportionate contributions to variance in the hidden state space. For each $d \\in D_F$, $\\mathcal{I}$ assigns an anomaly score $score(d)$ based on the average path length $h(d)$ required to isolate the point across an ensemble of binary trees. Shorter path lengths correspond to higher anomaly scores, indicating points that contribute to variance and thus collateral effects. Additional details are provided in Appendix A.\nStage 3: Prune Outliers and Setting Stopping Criterion: To construct the pruned coreset $D_C$, we apply a threshold $\\tau$ on the anomaly scores from the Isolation Forest model: $D_C = \\{d \\in D_F | score(d) \\leq \\tau\\}$. Data points with scores above $\\tau$ are excluded as outliers, as they disproportionately contribute to variance. We hypothesize that removing these outliers will reduce utility degradation while preserving core information for forgetting. The threshold $\\tau$ is determined via a stopping criterion, which can be chosen as follows: (1) Coreset Size Control: Specify a desired coreset size $|D_C|$ to ensure an appropriate number of inliers (2) Proportional Pruning: Select the top $k\\%$ of points with the lowest anomaly scores to maintain a consistent pruning ratio. By selecting $\\tau$ based on user requirements, UPCORE provides fine-grained control over the trade-off, ensuring the construction of a robust coreset. In practice, we prune 10% of the data points in our main experiments and additionally conduct scaling experiments that vary the pruning percentage to analyze its impact on the trade-off dynamics.\nStage 4: Unlearning on the Coreset: After selecting the pruned coreset $D_C$, UPCORE applies the unlearning algorithm $\\mathcal{U}$ to the model $M$, resulting in $M_{UPCORE} = \\mathcal{U}(M, D_C)$ (See Figure 2 end). This process removes the influence of $D_F$ while minimizing utility degradation on $D\\setminus D_F$. By focusing on $D_C$, our approach ensures targeted unlearning and positively leverages collateral effects, as unlearning on $D_C$ also deletes much of $D_F$'s influence, even those parts not explicitly included in $D_C$."}, {"title": "4. Experimental Setup", "content": "Unlearning Methods and Baselines. We test UPCORE with three standard unlearning methods: gradient ascent, refusal, and negative preference optimization, applied to a Llama-3.1-8B (Dubey et al., 2024) base model. In all cases, models are trained using both the forget set (complete or sampled) and a retain set, which contains examples of data that should not be forgotten, providing a contrastive signal. We consider the following unlearning methods:\n\u2022 Gradient Ascent (Jang et al., 2023): Gradient Ascent"}, {"title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning", "content": "maximizes the training loss on the forget set $D_F$. For each $x \\in D_F$, the objective is to maximize the loss.\n\u2022 Refusal (Ouyang et al., 2022): Refusal trains the model to respond to sensitive prompts with neutral, non-informative answers, such as \"I don't know.\"\n\u2022 Negative Preference Optimization (NPO) (Zhang et al., 2024c): NPO is a stable form of DPO (Rafailov et al., 2024) designed for unlearning. It reduces the gap between the likelihood of the target data and the likelihood from the original model while ensuring the unlearned model remains closely aligned with the original (See Appendix B.5 for more details).\nWe evaluate UPCORE, which is a dataset selection method, against two other selection methods as baselines: (1) unlearning applied to the entire forget set (i.e. no selection), and (2) unlearning performed on a randomly subsampled subset of the forget set, matched in size to the coreset curated by UPCORE (i.e. random selection).\nDataset Design. We evaluate on factual questions across two settings, described below, and we include further details on these settings in Appendix B.3.\n\u2022 We consider factual prompt completions with brief answers, typically a single word or short phrase (e.g., Paris for the prompt \u201cThe capital of France is\u201d). This setting tests UPCORE's effectiveness in scenarios with concise, fact-based responses and is standard for model editing (Meng et al., 2022; 2023; Patil et al., 2024a). We source questions from Counterfact (Meng et al., 2022), a widely-used model editing benchmark. Following Patil et al. (2024a), we filter for single-token answers. In this setting, the base model's ROUGE score on each of the topics is 1.0.\n\u2022 We also consider a question-answering setting where the answers are potentially multi-token responses. Here, we source questions from TriviaQA (Joshi et al., 2017), a QA benchmark of trivia questions. This scenario tests UPCORE on longer-form generation; here, we create topics after filtering samples where the base model's ROUGE score is zero.\nW apply topic modeling to cluster questions into seven topic-based groups and one cluster in each setting is randomly chosen as the retain set, while the other six are used as separate forget sets, with performance averaged across them. For each topic, we also generate neighborhood QA pairs that are semantically related to the forget topic but do not directly overlap with it by prompting GPT-40 (Achiam et al., 2023) to produce 100 data points per topic. These pairs are automatically filtered with a sentence transformer model (Reimers & Gurevych, 2019) to verify that they have no overlap with the forget data. (see Appendix B.4 for details)."}, {"title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning", "content": "We argue that to systematically evaluate unlearning performance, we should be measuring this tradeoff across epochs. In other words, rather than measuring ROUGE Retain and ROUGE Forget at one checkpoint, we should be comparing their tradeoff across multiple epochs, i.e. measuring the area under the curve (AUC) between these two metrics. Visually, this is illustrated in Figure 3, where we show the tradeoff between the inverse ROUGE on the forget data (X axis) and the ROUGE on neighboring points (Y axis).\nTo this end, we introduce an AUC metric that integrates deletion effectiveness and model utility over time. Specifically, we construct a Pareto curve that plots utility metrics (e.g. ROUGE Retain, ROUGE Neighborhood, etc.) against deletion effectiveness (e.g. ROUGE Forget) as unlearning progresses. The AUC serves as a global metric that captures the trade-off between preserving useful knowledge and ensuring effective deletion. By also reporting standard metrics, we empirically validate that AUC correlates with improved unlearning performance across diverse settings (See Table 3). Furthermore, we also verify that it is negatively correlated with forget data variance (See Table 6)."}, {"title": "4.1. Metrics and Answer Extraction", "content": "Following prior work (Maini et al., 2024), we evaluate models according to a suite of metrics. First, given a reference answer and the model-produced answer, we compute ROUGE (Lin, 2004) to measure overlap. To quantify the amount of negative model damage, we compute ROUGE on the retain set \u2013 i.e. the set of examples used to teach the model what information to keep \u2013 the neighborhood data, and on the Real World and Real Authors datasets (Maini et al., 2024), which consist of topics different from those being unlearned. Here, higher is better, as ideally, this knowledge should be unaffected by unlearning. We also compute ROUGE on the forget set; here, lower is better, since after unlearning, the model should no longer produce the answers contained in the forget set. Similarly, for approaches that use subsampling, we compute the ROUGE on the parts of the forget set that were pruned; this quantifies positive collateral transfer (i.e. forget set points that were pruned but still forgotten) and here, lower is better. Finally, we compute model utility, which is the harmonic mean of the conditional probabilities $P(a | q)$ assigned by the model, normalized by raising it to the power $1/|a|$ to account for answer length, where q and a denote the question and answer, respectively, following common practice (Cho et al., 2014). Additionally, we evaluate truth ratios, which approximately measure the relative likelihood of the correct answer compared to an incorrect one (Maini et al., 2024), along with ROUGE scores across the Forget set along with its rephrase and jailbreak variants, Retain, Real-World, Real-Authors, and Neighborhood datasets.\nAUC Metric. While ROUGE and model utility provide a snapshot of model performance and are the standard evaluation metrics in unlearning, we argue that they are insufficient,"}, {"title": "5. Experimental Results and Discussion", "content": "Design. To evaluate whether UPCORE effectively expands the Pareto frontier for deletion effectiveness and utility retention, we compute and compare the Area Under the Curve (AUC) of models trained with UPCORE against those trained using the baseline methods detailed in Section 4. Each AUC value is calculated using two key metrics: (1) Deletion Effectiveness, quantified as (1-ROUGE) score on the forget set, plotted along the X-axis, and (2) Utility Retention along multiple dimensions, measured as the ROUGE score on various datasets containing datapoints that should not be deleted from the model, including neighborhood dataset and a combined model utility metric (Maini et al., 2024) which aggregates multiple metrics, plotted along the Y-axis. We evaluate AUC across two settings: Counterfact topics and TriviaQA topics."}, {"title": "5.1. UPCORE Balances Deletion and Model Utility", "content": "Results. Figure 3 illustrates the AUC metric, which measures the area under the curve for the Pareto frontier between forget performance and model utility and thereby quantifies the trade-off between them. A higher AUC indicates a better balance, where forget performance is maximized while minimizing unintended degradation in model utility. Here, the rate of utility degradation is slower when applying unlearning on the coreset designed by UPCORE.\nTable 1 quantifies these trends further across metrics on the Counterfact dataset, demonstrating that UPCORE consistently achieves higher AUC scores than both baselines: unlearning on the complete forget set and unlearning on a randomly subsampled set of the same size as the coreset generated by UPCORE. This trend holds across the three unlearning methods, highlighting the method-agnostic nature of UPCORE. Notably, UPCORE outperforms baselines by 3-7 AUC on Counterfact, indicating a superior trade-off between deletion and retention of useful knowledge."}, {"title": "5.2. Positive and Negative Transfer", "content": "Design. Here, we measure both positive and negative transfer. To assess whether unlearning on the core forget set"}, {"title": "5.3. Robustness to Rephrases and Jailbreaks", "content": "Design. To assess whether unlearning on the core forget set is robust to blackbox attacks attempting to extract the deleted information, we test generalization to synthetically-generated paraphrases and adversarial/jailbreak prompts that elicit the same information as in the forget set (see Appendix B.4 for examples and generation method). Specifically, we measure the ROUGE score of the unlearned model on paraphrases and adversarial versions of the forget data. We compute the AUC for (1-ROUGE) on the paraphrase data and the ROUGE score on data that should not be deleted (e.g. retain set, neighborhood data, etc.) in Table 5. Here, we also show the AUC using (1-ROUGE) on the jailbreak data; in both cases, we would like the model to have a high score for (1-ROUGE) on jailbreaking and paraphrase examples (Zou et al., 2023; Jin et al., 2024), indicating that the model generalizes to different phrasings of the forget data and that it is robust to attack. In other words, a higher AUC indicates better generalization.\nResults. As shown in Table 5, UPCORE results in higher AUC across both the settings and across all the utility datasets compared to the baselines, indicating a superior trade-off even with rephrases and jailbreak attacks. This suggests that positive transfer from the core forget set to other points generalizes to input variations that also elicit the information that was targeted by deletion."}, {"title": "5.4. Scaling the Coreset Size", "content": "Design. Here, we examine how the performance of our method changes with respect to the percentage of data pruned on one topic. Given the design of Isolation Forests, we can vary the percentage of pruned \u201coutlier\u201d points from 0% up to 50%, which we do in increments of 10, starting at 10% (as 0% is the complete set). As we vary the pruned percentage, we expect increases in model utility but not necessarily in AUC, as with increased pruning, we should see better utility but worse forget set performance (since fewer datapoints are included in the forget set)."}, {"title": "5.5. UPCORE Lowers Forget Set Variance", "content": "Design. To verify that UPCORE indeed leads to a lower variance compared to the random baseline, we report the hidden state variance of the forget set used in each baseline and in UPCORE."}, {"title": "6. Conclusion", "content": "We introduce UPCORE, a utility-preserving coreset selection framework for unlearning in LLMs that minimizes collateral damage while ensuring effective deletion. Through empirical analysis, we identified hidden state variance of the forget data as a key factor influencing model utility degradation. By leveraging it to prune outliers in the forget data, thereby forming the core forget set, UPCORE better balances deletion effectiveness and model performance retention on unrelated data. We propose the use of area-under-the-curve across the competing objectives along the unlearning trajectory as a metric to quantify the trade-off. Our results demonstrate that UPCORE substantially reduces unintended performance loss while leveraging positive transfer and is able to be combined with any data-driven unlearning method, highlighting its potential as a principled and generalizable approach to utility-aware unlearning."}, {"title": "A. Additional Background", "content": "A.1. Machine Unlearning Background.\nThe concept of machine unlearning (Cao & Yang, 2015) is typically divided into two categories: exact unlearning and approximate unlearning. Exact unlearning aims to completely remove information related to specific data, ensuring that the resulting model behaves identically to a model retrained from scratch without the forget data (Ginart et al., 2019). However, the computational infeasibility of retraining LLMs from scratch renders exact unlearning impractical for real-world applications. Approximate unlearning methods, on the other hand, focus on ensuring that the model parameters closely approximate those of a retrained model while maintaining computational efficiency (Guo et al., 2020; Chien et al., 2022; Pan et al., 2023; Yoon et al., 2025).\nA.2. Coreset Selection.\nUnlike prior work, which focuses on coreset selection for improving training efficiency or robustness, our approach leverages a novel perspective by applying coreset principles to the problem of machine unlearning. Specifically, while conventional methods (Maharana et al., 2024) aim to preserve model accuracy during training by selecting representative data, our framework, UPCORE, is designed to mitigate negative collateral damage during unlearning by identifying and pruning data points that disproportionately influence performance degradation. Furthermore, unlike general coreset selection approaches that primarily target classification or regression tasks (Lee et al., 2024; Wei et al., 2015), our method is tailored for unlearning settings where the goal is retaining model utility while ensuring the effective removal of unwanted information. Thus, our work extends the applicability of coreset selection beyond traditional use cases, offering a principled approach to balancing unlearning effectiveness with model performance.\nA.3. Anomaly Score in Isolation Forest:\nIsolation Forests produce anomaly scores for each point. More formally, the anomaly score for a data point d is defined as:\n$score(d) = 2^{- \\frac{h(d)}{c(n)}}$"}, {"title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning", "content": "where $h(d)$ is the average path length for $d$ across the ensemble of trees, $n$ is the size of the dataset $D_F$, and $c(n)$ is the average path length for a dataset of size $n$ in a"}]}