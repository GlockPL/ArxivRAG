{"title": "VLM's Eye Examination: Instruct and Inspect Visual Competency of Vision Language Models", "authors": ["Nam Hyeon-Woo", "Moon Ye-Bin", "Wonseok Choi", "Lee Hyun", "Tae-Hyun Oh"], "abstract": "Vision language models (VLMs) have shown promising reasoning capabilities across various benchmarks; however, our understanding of their visual perception remains limited. In this work, we propose an eye examination process to investigate how a VLM perceives images, specifically focusing on key elements of visual recognition, from primitive color and shape to semantic levels. To this end, we introduce a dataset named LENS to guide a VLM to follow the examination and check its readiness. Once the model is ready, we conduct the examination. Through this examination, we quantify and visualize VLMs' sensitivities to color and shape, and semantic matching. Our findings reveal that VLMs have varying sensitivity to different colors while consistently showing insensitivity to green across different VLMs. Also, we found different shape sensitivity and semantic recognition depending on LLM's capacity despite using the same fixed visual encoder. Our analyses and findings have potential to inspire the design of VLMs and the pre-processing of visual input to VLMs for improving application performance.", "sections": [{"title": "1 Introduction", "content": "Vision language models (VLMs) (Liu et al., 2023b; Dai et al., 2023; OpenAI, 2023; Chen et al., 2023) are composed of a visual encoder to process visual information with a large language model (LLM) for comprehension, akin to how the human visual system operates with the eyes and brain. While VLMs have shown promising performance on various tasks (Marino et al., 2019; Mishra et al., 2019; Sidorov et al., 2020; Krishna et al., 2016), our understanding of how these models perceive visual information remains limited. Prior works (Choe et al., 2022; Zhou et al., 2016; Akata et al., 2023; Prystawski et al., 2023; Zhu & Li, 2023; Allen-Zhu & Li, 2023) have tried to understand the behavior or decision of neural networks, which would help to achieve responsible AI including explainability and safety. As the need for model understanding is becoming increasingly important alongside significant advances in VLMs, we raise a fundamental question: How do VLMs see and recognize?"}, {"title": "2 LENS Dataset for Instruction and Readiness Check", "content": "Understanding primitive competency at different levels is vital for vision perception, as they are the basic building blocks of human awareness (Von Glasersfeld, 1989; Marr, 2010; Lowe, 2012). To understand the visual competency of VLMs as with humans, we propose an eye examination process involving three main steps: 1) instruction, 2) readiness check, and 3) examination. For steps 1 and 2, we introduce a dataset called LENS (Learning Element for visual Sensory), which has three primitive element categories: color, shape,"}, {"title": "2.1 Color LENS", "content": "The color element is designed to address specific queries categorized into either yes/no or Sample 1 or Sample 2, as shown in the first column of Fig. 2. The yes/no question type involves pairs of colors, prompting an assessment of whether the two colors are identical. The model should respond with yes or no, called format. The Sample 1 or Sample 2 question type includes two sets, each containing two colors. The model should choose the correct sample or respond with no answer if both samples have different colors. The challenge lies in determining which sample pair accurately matches in terms of color. Note that we provide two separate color sets for training and test data, respectively.\nWe finetune LLaVA-v1.5 (Liu et al., 2023a) and InstructBLIP (Dai et al., 2023) with LoRA (Hu et al., 2022) on our color LENS training data. In Table 1a, the performance of both LLaVA and InstructBLIP becomes higher after finetuning and reaches acceptable levels on our color test set, regardless of their model size. This higher performance indicates that the models understand the instructions and are ready for the examination regarding color. The instructions and readiness check steps are performed in the same way for shape and semantic elements as for color. To measure visual competency on a particular element, we use a model separately finetuned on the training set for that element. We will examine visual perception in terms of color in Sec. 3.1."}, {"title": "2.2 Shape LENS", "content": "The format for the shape element is the same as the color element but with color boxes replaced by shape images. We create these shape images using Bezier curves. The process begins with generating random points that are smoothly connected. The hyperparameters include the number of points, the radius around points, and the smoothness of the curve. We ensure that these hyperparameters do not overlap between the training and test sets. The second column of Fig. 2 shows the shape samples, with the readiness check results in Table 1b. The shape element examination is detailed in Sec. 3.3."}, {"title": "2.3 Semantic LENS", "content": "The semantic element has two groups, semantic and patch. The last column in Fig. 2 shows the samples for these groups, where the left is the semantic group and the right is the patch group. Semantic groups follow the same format as color or shape datasets but use images from ImageNet (Deng et al., 2009).\nThe patch group has 3 types of images: self-swap, cross-swap, and masking, as shown in Fig. 2. In all cases, we divide the images sampled from ImageNet into 4 \u00d7 4 patches. The self-swap requires finding the positions of swapped patches, the cross-swap requires identifying a patch from a different image. The masking requires locating the correct position for one of the missing patches. All questions in the patch group offer three options, including a no answer option.\nWe check the readiness on the semantic element in Table 1c. The examination of the semantic element is detailed in Sec. 3.5. Now, the model is ready; thus, we move to the examination."}, {"title": "3 Eye Examination for VLMs", "content": "During the examination, we evaluate the discrimination abilities of VLMs by asking whether two samples are identical in the context of multiple levels of vision primitives. In this paper, we conduct an eye examination on LLaVA-v1.5 (Liu et al., 2023a) and InstructBLIP (Dai et al., 2023)."}, {"title": "3.1 Examination: Color", "content": "Color is a crucial attribute for distinguishing objects and plays a central role in many aspects of computer vision (Gevers et al., 2012). By distinguishing subtle color differences beyond regular distinctions, we can perceive detailed information about an object, such as its curvature or whether it is behind glass. We explore how VLMs perceive and process a subtle difference in color information and understand the VLM's resolution for colors."}, {"title": "Definition 1. Sensitivity Area of Color (SAC).", "content": "Let $C_{ref}$ be the reference color, $C_{target}$ the target color, and $f(C_{ref}, C_{target}): \\{C_{ref}, C_{target}\\} \\rightarrow [0, 1]$ a score function that measures the similarity between colors. The function assigns a high value when the model recognizes that colors are similar.\nSensitivity Area of Color = $\\int f(C_{ref}, C_{target}) dC_{target}$.\nHowever, since calculating the integral is not feasible, we use numerical integration as $\\sum f(C_{ref}, C_{target})dA_i$ where $dA_i$ is the differential area. As shown in Fig. 3, we perform the calculation in polar coordinates, so $dA_i = rd\\theta dr$ where $r$ and $\\theta$ are the radius and angle, respectively. A low SAC value indicates that the model is capable of sensitively distinguishing the reference color. Conversely, a high SAC value stands for the model's insensitivity to the reference color."}, {"title": "3.2 Why do VLMs have different color sensitivities?", "content": "What component does affect the color sensitivity of VLMS?\nWe hypothesize that the capability of the visual encoder to perceive color significantly influences the decision-making process of LLMs, even more than LLMs themselves. To illus- trate this, consider a thought experiment on the color sensitivity that converts the RGB value to the text and then ask for the probability from LLMs without a visual encoder. For instance, we can prompt like \"Are (255, 0, 0) and (0, 255, 0) the same color?\" Since LLMs do not need to directly discrim- inate the colors themselves, we would expect the sensitivities to distinctive colors to be similar, leading us to question the role of the visual feature.\nWe measure cosine similarity between colors as follows:\nsim(Cref, Ctarget) = $\\frac{v(C_{ref}) \\cdot v(C_{target})}{||V(C_{ref})||_2||V(C_{target})||_2}$, where v(.) is a visual encoder. We fixed the reference color as red, green, or blue and varied the target color within the 24-bit RGB color space. Considering the vast size of the color space, 2563 \u2248 16.8M, we reduce the color space to 323 = 32,768. We apply min-max normalization to ensure values within the range [0, 1]."}, {"title": "3.3 Examination: Shape", "content": "Fundamental features such as edges, corners, and blobs are extensively employed in feature descriptions Krig & Krig (2016); Mikolajczyk et al. (2003); Mukherjee et al. (2015). Change of view results in fine changes to these features, allowing motion detection through subtle distinctions in shape. Therefore, we investigate how VLMs process shape information.\nWe examine the shape sensitivity of LLaVA and InstructBLIP similarly to the color sensitivity in Sec. 3.1. Given the reference shape of a circle, the target shape is set by adjusting the circle's eccentricity, size, or the number of regular polygon vertices. As the eccentricity increases (see Fig. 5a), the number of vertices decreases (see Fig. 5b), or the size of the circle increases and decreases (see Fig. 5c), the target shape deviates from a reference circle, denoted as Ref. in each figure. We measure the probability of shape sensitivity as follows."}, {"title": "Definition 2. Sensitivity Area of Shape (SAS).", "content": "Let $S_{ref}$ be the reference shape, and $S_{target}$ the target shape. We define a score function $f(S_{ref}, S_{target}): \\{S_{ref}, S_{target}\\} \\rightarrow [0, 1]$ that measures the similarity between shapes, assigning a high value when a model recognizes that shapes as similar.\nSensitivity Area of Shape = $\\int f(S_{ref}, S_{target}) dS_{target}$."}, {"title": "3.4 Why do VLMs have different shape sensitivities?", "content": "What component does affects the shape sensitivity of LLaVA and InstructBLIP? Since we use the same visual encoder regardless of model size, the difference likely stems from the capacity of the LLMs. To investigate this, we design an experiment to examine decision-making in LLMs by changing the text prompt and observing the token probability of shape sensitivity.\nThe eccentricity prompt is \"Is {r1} greater than or equal to {r2}?,\" where r\u2081 = 0 and r\u2082 as a real number between 0 and 0.9. The polygon prompt is \"Is {n\u2081} greater than or equal to {n2}?,\" where n2 = 3 and n\u2081 as an integer between 3 and 30. We append \"Answer with yes or no.\" to each text prompt and record the score of \"yes\" and \"no\" tokens.\nIn summary, we investigate the shape recognition ability of LLaVA and InstructBLIP as follows: (1) larger VLMs are more sensitive to shape, (2) decision-making of LLM influences the shape sensitivity, and (3) the model size of LLMs influences the numerical comparison ability, which is connected to the shape sensitivity."}, {"title": "3.5 Examination: Semantics", "content": "Semantics represents a foundational aspect of vision recognition. For instance, humans can categorize objects into corresponding semantic classes, regardless of color or shape variations. This ability extends to perceiving partially obscured objects; we can infer the form of the hidden parts if we recognize the category. In this regard, the role of semantics is critical in visual perception. Thus, we investigate how well LLaVA processes semantic-related information. In addition, the improvement of performance on the semantics in Table 1c is noticeable in a larger model. These results raise the question of why larger models have better performance on semantics."}, {"title": "3.6 Why do VLMs have different accuracies?", "content": "Given the same visual encoder, why do the larger VLMs have higher accuracies on the semantic dataset? We hypothesize that the performance gap stems from LLMs because of using the same visual encoder. To verify this hypothesis, we visualize how LLMs make decisions. Inspired by the weakly supervised object localization method (Choe et al., 2022), we assign probability to patches (see Fig. 7a) as follows.\n1. Given two images, one is used as the reference image, which is sample 1. The other is the source of a target.\n2. Crop the target source image into patches according to the patch size and stride. The cropped patch is used for sample 2.\n3. Ask VLMs if the reference and target patches share the same semantics and record the probability of \"yes\" tokens as the score.\nCompared to the results for the same and different classes in Fig. 7b, the score maps for the same class have high scores in object-present regions, contrasting with the noisy and structure-lacking maps for different classes. The larger model assigns a lower score to the background compared to the smaller model. It implies that larger LLMs achieve more accurate semantic recognition as reflected in Table 1c. Also, the kernel size affects the reliability of the captured objects and the noisiness of the score map."}, {"title": "4 Potential Application", "content": "Our findings suggest that VLMs can improve image recognition by applying simple pre-processing to input images. For example, in Fig. 8, the chart reasoning results from GPT-4 (OpenAI, 2023) vary according to graph colors or symbol shapes. Regarding the color, GPT-4 struggles to distinguish between similar shades of color, making it difficult for VLMs to match colors with their corresponding numerical values. As revealed by the color sensitivity check, this result is due to the model's lack of color competency rather than its reasoning ability. By adjusting the colors, GPT-4 provides accurate responses. Regarding the shape, GPT-4 is confused between triangle and rectangle symbols, indicating a lack of shape competency that may result in incorrect recognition outcomes. When we change the rectangle to a circle, GPT-4 provides the correct reasoning output. We hope that our VLM's eye examination methodology will not only assist in selecting and manipulating models and images but also facilitate enhancements in model performance by deepening our understanding of underlying factors."}, {"title": "5 Related Work", "content": "Large language models (LLMs) (OpenAI, 2023; Touvron et al., 2023a;b; Zhang et al., 2022b; Chowdhery et al., 2023; Roberts et al., 2020; Chiang et al., 2023) exhibit remarkable abilities in processing textual and linguistic information and generating reasonable responses, akin to human brain functions. Building upon the success of LLMs, vision language models (VLMs) (Dai et al., 2023; Liu et al., 2023b;a; Gao et al., 2023b; Zhang et al., 2024) have been emerged. These models represent a fusion of visual encoders with the reasoning abilities of LLMs, marking a significant evolution in the field. Unlike LLMS, VLMs are specially equipped to process visual information, i.e., images. As these models integrate visual capabilities, the amount of information a VLM processes increases, as does the variety of tasks it can handle. VLMs need to parse the information from given images, focusing on specific areas depending on the task or prompt. For instance, instruction-tuned VLMs, such as LLaVA (Liu et al., 2023b) and InstructBLIP (Dai et al., 2023), have demonstrated versatility in handling multiple tasks. We primarily focus on understanding how VLMS recognize and interpret visual information to improve our understanding of VLMs.\nBoth humans and neural networks exhibit similarities in terms of information processing (Alper et al., 2023; Zhang et al., 2022a). Numerous studies aim to understand human cognitive processes, often in highly con- trolled settings. For example, when developing the CIE color model (Wright-Guild experiment), participants were required to look through a small aperture, limiting their field of view to just 2 degrees. Similarly, in- vestigations of VLMs are frequently conducted in restricted environments (Zhu & Li, 2023; Allen-Zhu & Li, 2023). Inspired by prior work, we analyze changes under limited situations in terms of color, shape, and semantics, based on the test capabilities introduced by our LENS dataset."}, {"title": "6 Conclusion", "content": "We take a closer look at the elementary perception abilities of VLMs, especially LLaVA and InstructBLIP, through an eye examination process in the context of color, shape, and semantics. We introduce the LENS dataset to instruct and check models, ensuring they are in an appropriate state for examination. Fine- tuning and evaluating the models on LENS allows us to conduct a detailed analysis. Our examination results indicate that LLaVA and InstructBLIP are less responsive to the green spectrum regarding color sensitivity, resembling vision through a transparent green screen. We also discover that LLMs, which act as the brain of the VLMs, significantly influence shape sensitivity and the ability to make patch-wise semantic distinctions for recognition. We believe these intriguing findings and the proposed evaluation process will contribute to a deeper understanding of VLM behavior and advance their reasoning capabilities, as shown in potential application examples. We will conclude our work with discussions on limitations.\nLimitations. Although we finetune the model with LoRA in a parameter-efficient manner, our approach to integrating specific capabilities might not be optimal. There could be more suitable formats for samples and training methodology. In addition, while we investigate two VLMS, LLaVA-v1.5 and InstructBLIP, eye examination on additional VLMs would be an interesting future direction."}, {"title": "A Appendix", "content": "In this appendix, we include an additional discussion, the details of our LENS and experiments, and addi- tional qualitative results, which are not included in the main paper."}, {"title": "A Impact Statement", "content": "As research on Large Language Models (LLMs) and Vision-Language Models (VLMs) begins, there is a noticeable acceleration in the development speed within the AI industry. This advancement in research has the potential to enrich human life, yet it simultaneously brings rapid changes to our lifestyles. Despite the swift progress in large model research, a gap remains in our understanding of the precise operational principles of these systems. This lack of clarity and control over AI's deep integration into daily life is raising significant social concerns. Our project aims to address these issues, focusing specifically on analyzing the perception mechanisms of VLMs. For this reason, our goal is to move towards a more controllable system through a better understanding of VLMs by casting a fundamental question, \"How do VLMs perceive the world?\". However, it is important to recognize that such controllability does not always yield positive effects; it could potentially be exploited for more complex and malicious criminal activities. Currently, our analysis is in its nascent stage, but we hope that our research will contribute to the development of controllable and explainable AI, ultimately fostering a safer coexistence with AI in the world."}, {"title": "B Additional Discussion", "content": "Understanding the inside of AI is difficult because of overparameterization, non-linear mapping, non-convex optimization, etc. Many researchers have tried to provide and explain the behavior of AI in terms of theory (Allen-Zhu et al., 2019a;b), explainable algorithms (Ribeiro et al., 2016; Lundberg & Lee, 2017; Wu et al., 2023; Sundararajan et al., 2017; Petroni et al., 2019; Wu et al., 2020; Selvaraju et al., 2017), physics law (e.g., scaling low) (Kaplan et al., 2020; Gao et al., 2023a; Henighan et al., 2020; Hernandez et al., 2021; Cherti et al., 2023; Bubeck & Sellke, 2021), and computational experiments (Zhang et al., 2017; Prystawski et al., 2023; Akata et al., 2023).\nThe approach of deep learning theory explains the phenomena of AI in terms of the mathematical form. Thus, it is more rigorous than other approaches. However, deep learning theory includes impractical assumptions such as shallow or infinite-width networks. Explainable AI is to provide an understandable form to users. For example, previous methods have revealed how and why the model's decisions are made. The approach of physics law is to find certain laws that occur in models; the popular law is the scaling law that the larger the model and the more data we use, the better the performance. Similarly, the computational approach exists by observing the model's behavior. Some methods are inspired by philosophy, psychology, and cognitive science because they are based on large language models (LLMs) showing emergent intelligence and reasoning. These approaches might not be rigorous compared to the deep learning theory. Our approach is close to the computational experiments. We fine-tuned the pre-trained large model with our dataset and investigated the behavior of the model along with the visual input changes."}, {"title": "C Details of LENS", "content": "We propose the eye examination process of VLMs to understand how the model perceives the given visual signals. The LENS (Learning ElemeNt for visual Sensory) dataset includes three types of primitive visual information: color, shape, and semantic. The statistics of our LENS are in Table 4. Each LENS data consists of an image, a question with answer options, and a ground truth answer. The questions are randomly sampled from a set associated with each category. In this section, we provide the set of questions for each category of our LENS dataset.\nAdditionally, in Fig. 9, we show samples for the patch group in the semantic category. For the self-swap, two randomly selected patches are swapped within an image. The goal is to find the positions of swapped patches. In contrast, for the cross-swap, a single patch is randomly sampled from each image and swapped"}, {"title": "C.1 Color", "content": "The list of questions for color dataset in LENS."}, {"title": "C.2 Shape", "content": "The list of questions for shape dataset in LENS."}, {"title": "C.3 Semantic", "content": "The list of questions for the semantic dataset in LENS."}, {"title": "C.4 Patch", "content": "The list of questions for patch dataset in LENS."}, {"title": "C.5 Fine-Tuning Setup", "content": "We finetune VLMs with our constructed dataset, color, shape, and semantics, respectively. We chose LLaVA (Liu et al., 2023b) because it is efficient in terms of training time. We employ LoRA (Hu et al., 2022) during the finetuning because it is a resource-efficient algorithm. We set the training epoch as 2, batch size 128, and learning rate 0.0002 with cosine scheduling, Adam optimizer (Kingma & Ba, 2015) and gradient checkpointing. We use 8 A100 80G GPUs for our experiments."}, {"title": "D Color Pattern", "content": "In Sec. 3.2, we investigate the visual encoder of VLMs to understand their varying sensitivity to colors, no- notably the lesser sensitivity to green compared to the other colors. We evaluate CLIP ViT-L/14 336px (Rad- ford et al., 2021) used in LLaVA-v1.5 (Liu et al., 2023a) as described in the main paper. Here, v(.) represents the visual encoder (CLIP ViT-L/14 336px). We extract the hidden features, excluding the class token, that are injected into LLMs and average the tokens. We measure the cosine similarity between colors as following:\nsim(Cref, Ctarget) = $\\frac{v(C_{ref}) \\cdot v(C_{target})}{||V(C_{ref})||_2||V(C_{target})||_2}$\nWe choose the similarity measure as cosine similarity because the training method of CLIP uses pairwise cosine similarity. It is computationally infeasible to compare all colors because the number of colors is 2563. Thus, we divide R, G, and B into 32 bins which means the number of colors i 323. Specifically, the colors are (0, 0, 0), (0, 0, 8), ..., (248, 248, 240), (248, 248, 248). Finally, we apply min-max normalization to the results of cosine similarity to ensure values fall within [0, 1], providing better visualization and interpretation."}, {"title": "E Shape", "content": "In Fig. 11, we add the shape sensitivity graph result of InstructBLIP, Fig. 11g-11i. Since the original graph of InstructBLIP is very noisy, we perform polynomial fitting for a more comfortable comparison. As we mentioned in the main paper, InstructBLIP has a similar tendency to LLaVA, i.e., a larger model is more sensitive than a smaller one. In Table 11j, the values of SAS (sensitivity area of shape) also indicate consistent results with the graph result."}, {"title": "F Patch Analysis", "content": "As mentioned in the main paper, we provide the reference image and target cropped patch to VLM, ask if the given samples are the same semantically, and visualize the score map. Figure 12 shows the results. Let the original image size be 1536; then, we vary the patch size from 260 to 60 with a fixed stride size."}]}