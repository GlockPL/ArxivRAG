{"title": "BEYOND THE RAINBOW: HIGH PERFORMANCE DEEP\nREINFORCEMENT LEARNING ON A DESKTOP PC", "authors": ["Tyler Clark", "Mark Towers", "Christine Evers", "Jonathon Hare"], "abstract": "Rainbow Deep Q-Network (DQN) demonstrated combining multiple independent\nenhancements could significantly boost a reinforcement learning (RL) agent's per-\nformance. In this paper, we present \u201cBeyond The Rainbow\" (BTR), a novel al-\ngorithm that integrates six improvements from across the RL literature to Rain-\nbow DQN, establishing a new state-of-the-art for RL using a desktop PC, with a\nhuman-normalized interquartile mean (IQM) of 7.4 on Atari-60. Beyond Atari, we\ndemonstrate BTR's capability to handle complex 3D games, successfully training\nagents to play Super Mario Galaxy, Mario Kart, and Mortal Kombat with mini-\nmal algorithmic changes. Designing BTR with computational efficiency in mind,\nagents can be trained using a desktop PC on 200 million Atari frames within 12\nhours. Additionally, we conduct detailed ablation studies of each component, an-\nalyzing the performance and impact using numerous measures.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep Reinforcement Learning (RL) has achieved numerous successes in complex sequential\ndecision-making tasks, most rapidly since Mnih et al. (2015) proposed Deep Q-Learning (DQN).\nWith this success, RL has become increasingly popular among smaller research labs, the hobbyist\ncommunity, and even the general public. However, recent state-of-the-art approaches (Schrittwieser\net al., 2020; Badia et al., 2020a; Hessel et al., 2021; Kapturowski et al., 2022) are increasingly out of\nreach for those with more limited compute resources, either in terms of the required hardware or the\nwalltime necessary to train a single agent. This is a unique issue in RL compared to natural language\nprocessing or image recognition which have foundation models that can be efficiently fine-tuned for\na new task or problem (Lv et al., 2023). Meanwhile, RL agents must be trained afresh for each\nenvironment. Therefore, the development of powerful RL algorithms that can be trained quickly on\ninexpensive hardware is crucial for smaller research labs and the hobbyist community.\nThese concerns are not new. Ceron & Castro (2021) highlighted that Rainbow DQN (Hessel et al.,\n2018) required 34,200 GPU hours (equivalent to 1435 days) of training, making the research impos-\nsible for anyone except a few research labs, with more recent algorithms exacerbating this problem.\nRecurrent network architectures (Horgan et al., 2018), high update to sample ratio (D'Oro et al.,\n2022), and the use of world-models and search-based techniques (Schrittwieser et al., 2020) all in-\ncrease the computational resources necessary to train agents, many using distributed approaches\nrequiring multiple CPUs and GPUs (or TPUs), or requiring numerous days and weeks to train a\nsingle agent. These features have dramatically decreased RL's accessibility.\nFor this purpose, we develop \"Beyond the Rainbow\" (BTR), taking the same principle as Rain-\nbow DQN Hessel et al. (2018), selecting 6 previously independently evaluated improvements and\ncombining them into a singular algorithm (Section 3). These components were chosen for their\nperformance qualities or to reduce the computational requirements for training an agent. As a re-\nsult, BTR sets a new state-of-the-art score for Atari-60 (Bellemare et al., 2013) (excluding recurrent\napproaches) with an Interquartile Mean (IQM) of 7.4\u00b9 using a single desktop machine in less than\n12 hours, and outperforms Rainbow DQN on Procgen (Cobbe et al., 2020) in less than a fifth of the\nwalltime (Section 4.1). Further, we demonstrate BTR's potential by training agents to solve three\nmodern 3D games for the first time, Mario Kart Wii, Super Mario Galaxy and Mortal Combat, that\neach contain complex mechanics and graphics (Section 4.2). To verify the effectiveness and effect\nof the six improvements to BTR, in Section 5.1, we conduct a thorough ablation of each component,\nplotting their impact on the Atari-5 environments and in Section 5.2, we utilise seven different mea-\nsures to analyse the component's impact on the agent's policy and network weights. This allows us\nto more precisely understand how the components impact BTR beyond performance or walltime.\nIn summary, we make the following contributions to state-of-the-art RL.\n\u2022 High Performance (Section 4.1) - BTR outperforms the state-of-the-art for non-\nrecurrent RL on the Atari-60 benchmark, with an IQM of 7.4 (compared to Rain-\nbow DQN's 2.7), outperforming humans on 52/60 games. Furthermore, BTR out-\nperforms Rainbow DQN with Impala on the Procgen benchmark despite using a\nsmaller model and 80% less walltime.\n\u2022 Modern Environments (Section 4.2) - Testing beyond Atari, we demonstrate BTR\ncan train agents for 3 modern games: Super Mario Galaxy (final stage), Mario Kart\nWii (Rainbow Road), and Mortal Combat (Endurance mode). These environments\ncontain 3D graphics and complex physics and have never been solved using RL.\n\u2022 Computationally Accessible (Figure 5) - Using a desktop PC, BTR trains Atari\nagents for 200 million frames in under 12 hours, significantly faster than Rainbow\nDQN's 35 hours. This increases RL research's accessibility for smaller research\nlabs and hobbists without the need for GPU clusters or excessive walltime.\n\u2022 Component Impact Analysis (Section 5) - We conduct thorough ablations inves-\ntigating BTR without each component in terms of performance and other measures.\nWe discover that BTR widens action gaps (reducing the effects of approximation\nerrors), is robust to observation noise, and reduces neuron dormancy and weight\nmatrix norm (shown to improve plasticity throughout training)."}, {"title": "2 BACKGROUND", "content": "Before describing BTR's extensions, we outline standard RL mathematics, how DQN is imple-\nmented, and Rainbow DQN's extensions."}, {"title": "2.1 RL PROBLEM FORMULATION", "content": "We adopt the standard formulation of RL (Sutton & Barto, 2018), described as a Markov Decision\nProcess (MDP) defined by the tuple (S, A, P, R), where S is the set of states, A is the set of actions,\nP: S \u00d7 A\u2192\u25b3(S) is the stochastic transition function, and R : S \u00d7 A \u2192R is the reward function.\nThe agent's objective is to learn a policy \u03c0 : S \u2192 \u2206(A) that maximizes the expected sum of\ndiscounted rewards \u0395\u03c0[\u2211t=0tr(st, at)], where y \u2208 [0, 1) is the discount rate."}, {"title": "2.2 DEEP Q-LEARNING (DQN)", "content": "One popular method for solving MDPs is Q-Learning (Watkins & Dayan, 1992) where an agent\nlearns to predict the expected sum of discounted future rewards for a given state-action pair. To\nallow agents to generalize over states and thus be applied to problems with larger state spaces, Mnih\net al. (2013) successfully combined Q-Learning with neural networks. To do this, training minimizes\nthe error between the predictions from a parameterized network Qe and a target defined by\n$rt + \\gamma \\max\\limits_{a \\in A} Q_{o'}(S_{t+1}, a),$ (1)\nwhere Qo' is an earlier version of the network referred to as the target network, which is periodically\nupdated from the online network Qe. The data used to perform updates is gathered by sampling\nfrom an Experience Replay Buffer (Lin, 1992), which stores states, actions, rewards, and next states\nexperienced by the agent while interacting with the environment."}, {"title": "2.3 RAINBOW DQN AND IMPROVEMENTS TO DQN", "content": "In collecting 6 different improvements to DQN, Rainbow DQN (Hessel et al., 2018) proved cumula-\ntively that these improvements could achieve a greater performance than any individually. We briefly\nexplain the individual improvements, ordered by performance impact, most of which are preserved\nwithin BTR (see Table 1), for more detail, we refer readers to the extension's respective papers:\n1. Prioritized Experience Replay - To select training examples, DQN sampled uniformly\nfrom an Experience Replay Buffer, assuming that all examples are equally important to\ntrain with. Schaul et al. (2015) proposed sampling training examples proportionally to\ntheir last seen absolute temporal difference error, encouraging more training on samples\nfor which the network most inaccurately predicts their future rewards.\n2. N-Step - Q-learning utilizes bootstrapping to minimize the difference between the pre-\ndicted value and the resultant reward plus the maximum value of the next state (Eq. 1).\nN-step (Sutton et al., 1998) reduces the reliance on this bootstrapped next value by consid-\nering the next n rewards and observation in n timesteps (Rainbow DQN used n = 3).\n3. Distributional RL - Due to the stochastic nature of RL environments and agent's policies,\nBellemare et al. (2017) proposed learning the return distribution rather than scalar expecta-\ntion; this was done through modelling the return distributions using probability masses and\nthe Kullbeck-Leibler divergence loss function.\n4. Noisy Networks - Agents can often insufficiently explore their environment resulting\nin sub-optimal policies. Fortunato et al. (2017) added parametric noise to the network\nweights, causing the model's outputs to be randomly perturbed, increasing exploration dur-\ning training, particularly for states where the agent has less confidence.\n5. Dueling DQN - The agent's Q-value can be rewritten as the sum of state-value and ad-\nvantage (Q(s, a) = V(s) + A(s, a)). Looking to improve action generalisation, Wang\net al. (2016) split the hidden layers into two separate streams for the value and advantage,\nrecombining them with Q(s, a) = V(s) + (A(s, a) - \\frac{1}{|A|} \\sum_{a'} A(s, a')).\n6. Double DQN - In selecting the next observation's maximum Q-value (Eq. 1), this can\nfrequently overestimate the target's Q-value, negatively affecting the agent's performance.\nTo reduce this overestimation, Van Hasselt et al. (2016) propose utilising the online network\nrather than the target network to select the next action when forming targets, defined as:\n$r_t + Q_{o'}\\Big(S_{t+1}, \\arg \\max\\limits_{a \\in A} Q(s_{t+1},a)\\Big).$ (2)"}, {"title": "3 BEYOND THE RAINBOW - EXTENSIONS AND IMPROVEMENTS", "content": "Building on Rainbow DQN (Hessel et al., 2018), BTR includes 6 more improvements undiscovered\nin 2018.\u00b2 Additionally, as hyperparameters are critical to agent performance, Section 3.2 discusses\nkey hyperparameters and our choices. In the appendices, we include a table of hyperparameters,\na figure of the network architecture and the agent's loss function (Appendices C.2, D and D.2).\nFinally, the source code using Gymnasium (Towers et al., 2024) is included within the supplementary\nmaterial to help future work build upon or utilise BTR."}, {"title": "3.1 EXTENSIONS", "content": "Impala Architecture + Adaptive Maxpooling - Espeholt et al. (2018) proposed a convolutional\nresidual neural network architecture based on He et al. (2016) featuring three residual blocks\u00b3, sub-\nstantially increasing performance over DQN's three-layer convolutional network. Following Cobbe\net al. (2020), we scale the width of the convolutional layers by 2 to enhance its capabilities. We\ninclude an additional 6x6 adaptive max pooling layer after the convolutional layers (Schmidt &\nSchmied, 2021) found to speed up learning and support different input resolutions.\nSpectral Normalisation (SN) - To help stabilize the training of discriminators in Generative Ad-\nversarial Networks (GANs), Miyato et al. (2018) proposed Spectral Normalisation to help control\nthe Lipschitz constant of convolutional layers. SN works to normalize the weight matrices of each\nlayer in the network by their largest singular value, ensuring that the transformation applied by the\nweights does not distort the input data excessively, which can lead to instability during training.\nBjorck et al. (2021) found that SN could improve performance in RL, especially for larger networks\nand Schmidt & Schmied (2021) found SN reduced the number of updates required before initial\nprogress is made.\nImplicit Quantile Networks (IQN) - Dabney et al. (2018) improved upon Bellemare et al. (2017),\nlearning the return distribution over the probability space rather than probability distribution over\nreturn values. This removes the limit on the range of Q-values that can be expressed, and enables\nlearning the expected return at every probability.\nMunchausen RL - Boostrapping is a core aspect of RL; used to calculate target values (Eq. 1) with\nmost algorithms using the reward, rt, and the optimal Q-value of the next state, Q*. However, since\nin practice the optimal policy is not known, the current policy \u03c0 is used. Munchausen RL (Vieillard\net al., 2020) looks to leverage an additional estimate in the bootstrapping process by adding the\nscaled-log policy to the loss function (Eq. 3 where a \u2208 [0, 1] is a scaling factor, o is the softmax\nfunction, and 7 is the softmax temperature). This assumes a stochastic policy, therefore DQN is\nconverted to Soft-DQN with with \u03c0pt = \u03c3(\\frac{q}{\u03c4}). As Munchausen does not use argmax over the\nnext state, Double DQN is obsolete. Munchausen RL's update rule is\n$Q_o(s_t, a_t) = r_t + \\alpha \\tau ln \\pi_{o'}(a_t|s_t) + \\gamma \\sum_{a' \\in A} \\pi_{o'} (a'|S_{t+1})\\Big(Q_{o'}(S_{t+1}, a') \u2013 \\tau ln(\\pi_{o'} (a'|s_{t+1})) \\Big).$ (3)\nVectorization - RL agents typically take multiple steps in a single environment, followed by a\ngradient update with a small batch size (Rainbow DQN took 4 environment steps, followed by a\nbatch of 32). However, taking multiple steps in parallel and performing updates on larger batches\ncan significantly reduce walltime. We follow Schmidt & Schmied (2021), taking 1 step in 64 parallel\nenvironments with one gradient update with batch size 256 (Schmidt & Schmied (2021) took two\ngradient updates rather than the one we take). This results in a replay ratio (ratio of gradient updates\nto environment steps) of \\frac{1}{64}. Higher replay ratios have been shown to improve performance (D'Oro\net al., 2022), however we opt to keep this value low to reduce walltime."}, {"title": "3.2 HYPERPARAMETERS", "content": "How frequently the target network is updated is closely intertwined with batch size and replay ratio.\nWe found that updating the target network every 500 gradient steps\u2074 performed best. Given our\nhigh batch size, we additionally performed minor hyperparameter tests using different learning rates\nfinding that a slightly higher learning rate of 1 \u00d7 10\u207b\u2074 performed best, compared to 6.25 \u00d7 10\u207b\u2075 in\nRainbow DQN. In Appendix C.2, we clarify the meaning of the terms frames, steps and transitions.\nFor many years, RL algorithms have used a discount rate of 0.99, however, when reaching high\nperformance, lower discount rates alter the optimal policy, causing even optimally performing agents\nto not collect the maximum cumulative rewards. To prevent this, we follow MuZero Reanalyse\n(Schrittwieser et al., 2021) using y = 0.997. For our Prioritized Experience Replay, we use the lower\nvalue of a = 0.2, the parameter used to determine sample priority, recommended by Toromanoff\net al. (2019) when using IQN. Lastly, many previous experiments used only noisy networks or e-\ngreedy exploration, however, we opt to use both until 100M frames, then set e to zero, effectively\ndisabling it. We elaborate on this decision in Appendix G."}, {"title": "4 EVALUATION", "content": "To assess BTR, we test it on two standard RL benchmarks, Atari (Bellemare et al., 2013) and Proc-\ngen (Cobbe et al., 2020) in Section 4.1. Secondly, we train BTR agents for three modern games\n(Super Mario Galaxy, Mario Kart Wii, and Mortal Combat) with complex 3D graphics and physics\nin Section 4.2, never shown to be trainable with RL previously."}, {"title": "4.1 ATARI AND PROCGEN PERFORMANCE", "content": "We evaluate BTR on the Atari-60 benchmark following (Machado et al., 2018) and without life infor-\nmation (see Appendix J for the impact), evaluating every million frames on 100 episodes. Figure 1\nplots BTR against Rainbow DQN and DQN, achieving an IQM of 7.4 compared to Rainbow DQN's\n2.7 and DQN's 0.9. In comparison to human expert performance, BTR equals or exceeds them in\n52 of 60 and Rainbow DQN in 57. Importantly, we find that BTR appears to continue increasing\nperformance beyond 200 million frames, indicating that higher performance is still possible with\nmore time and data. Results tables and graphs can be found in Appendices A and B respectively."}, {"title": "4.2 APPLYING BTR TO MODERN GAMES", "content": "To demonstrate BTR's capabilities beyond standard RL benchmarks, we utilised Dolphin (Dolphin-\nEmulator, 2024), a Nintendo Wii emulator, to train agents for a range of modern 3D games: Super\nMario Galaxy, Mario Kart Wii and Mortal Combat. Using a desktop PC, we were able to train the\nagent to complete some of the most difficult tasks within each game. Namely, the final level in\nSuper Mario Galaxy, Rainbow Road (a notoriously difficult track in Mario Kart Wii) and defeating\nall opponents in Mortal Kombat Endurance mode (for details about the environments and setup, see\nAppendix K). For this, BTR required minimal adjustments: first, to input image resolution, 140x114\n(from Atari's 84x84) due to the game's higher resolution and aspect ratio, and second, to reduce the\nnumber of vectorized environments to 4 as a result of the games' memory and CPU requirements."}, {"title": "5 ANALYSIS", "content": "Given BTR's performance demonstrated in Section 4, in this Section, we ablate each component\nto evaluate their performance impact (Section 5.1). Using the ablated agents, we then measure\nnumerous attributes during and after training to assess each component's impact (Section 5.2)."}, {"title": "5.1 ABLATIONS STUDIES", "content": "BTR amalgamates independently evaluated components into a single algorithm. To understand and\nverify each's contribution, Figures 4 and B1 plot BTR's performance without each component on\nthe Atari-5 benchmark.\u2076\nWe find that Impala had the largest effect on performance, with the other components generally caus-\ning a less significant effect on final performance. However, when BTR's performance is compared\nbefore 200 million frames, we find Munchausen and Spectral Normalisation provide significant per-\nformance improvements (+24% and +25% at 40M frames, and +13% and +35% at 120M frames).\nWe compare the performance of components at different stages of training in Appendix E.\nFor vectorization and maxpooling, while their inclusion reduces performance, we find their sec-\nondary effects crucial to keep BTR computationally accessible. Omitting vectorization increases\nwalltime by 328% (Figure 5) by processing environment steps in parallel and taking fewer gradient\nsteps (781,000 compared to Rainbow DQN's 12.5 million). We find maxpooling makes the agent\nmore robust to noise as discussed in Section 5.2, and decreases the model's parameters by 77%."}, {"title": "5.2 WHAT ARE THE EFFECTS OF BTR'S COMPONENTS?", "content": "To help interpret the results in Section 5.1, we measure seven different attributes of the agent ei-\nther during or after training: action gaps and action swaps, linked to causing approximation errors\n(Bellemare et al., 2016); policy churn, which can cause excessive off-policyness (Schaul et al.,\n2022); score with additional noise indicating robustness and weight norm and SRank (Sokar et al.,\n2023), correlated with loss of plasticity.\nFor why Impala contributes to performance so strongly, Figure 6 shows that with Impala, BTR has\nfewer dormant neurons and higher weight matrix norms. Despite this, without BTR's other com-\nponents, Impala exhibits a notable drawback, learning a highly noisy and unstable policy. Table 2,\ndemonstrates that without IQN and Munchausen the agent experiences very low action gaps (ab-\nsolute Q-value difference between the highest two valued actions), causing the agent to swap its\nargmax action almost every other step. This is likely to result in approximation errors altering the\npolicy and causing a high degree of off-policyness in the replay buffer. This is particularly detri-\nmental in games requiring fine-grained control, such as Phoenix where the agent needs to narrowly\ndodge many projectiles, reflected in BTR's performance without these components.\nFurthermore, we find that spectral normalization and maxpooling are useful in dealing with noisy\nenvironments. To test this, we evaluate the performance of BTR's ablations when taking different\nquantities of e-actions, and find these two components prevent performance from dropping substan-\ntially 2. Lastly, we find Munchausen and IQN to have a significant impact on Policy Churn (Schaul\net al., 2022), with Munchausen reducing it by 5.6% and IQN increasing it by 2.4%. As a result,\nwhen these components are used together, they appear to reach a level of policy churn which does\nnot harm learning and potentially provides some exploratory benefits."}, {"title": "6 RELATED WORK", "content": "The most similar work to BTR, developing a computationally-limited non-distributed RL algorithm,\nis \"Fast and Efficient Rainbow\" (Schmidt & Schmied, 2021). They optimised Rainbow DQN to\nmaximise performance for 10 million frames through parallelizing the environments and dropping\nC51 along with hyperparameter optimisations. This differs from our goals of producing an algorithm\nthat scales across training regimes (up to 200 million frames) and domains (Atari, Procgen, Super\nMario Galaxy, Mario Kart and Mortal Combat), resulting in different design decisions.\nFor less computation-limited approaches, Ape-X (Horgan et al., 2018) was the first to explore highly\ndistributed training, allowing agents to be trained on a billion frames in 120 hours through using\n100 CPUs. Following this, Kapturowski et al. (2018) proposed R2D2 using a recurrent neural net-\nwork, increasing sample efficiency but slowing down gradient updates by 38%. Agent57 (Badia\net al., 2020a) was the first RL agent to achieve superhuman performance across 57 Atari games,\nthough required 90 billion frames. MEME (Kapturowski et al., 2022), Agent57's successor, focused\non achieving superhuman performance within the standard 200 million frames limit, achieved by\nused a significantly higher replay ratio and larger network architecture. Most recently, Dreamer-v3\n(Hafner et al., 2023) used a 200 million parameter model requiring over a week of training, achiev-\ning similar results as MEME. We detail some of the key differences between BTR, MEME and\nDreamer-v3 in Table 3. While these approaches perform equally to or better than BTR, all are in-\naccessible to smaller research labs or hobbyists due to their required computational resources and\nwalltime. Therefore, while these algorithms have important research value demonstrating the possi-\nble performance of RL agents, performative algorithms with a lower cost of entry, like BTR, are a\nnecessary component for RL to become widely applicable and accessible."}, {"title": "7 CONCLUSION AND FUTURE WORK", "content": "We have demonstrated that, once again, independent improvements from across Deep Reinforce-\nment Learning can be combined into a single algorithm capable of pushing the state-of-the-art far\nbeyond what any single improvement is capable of. Importantly, we find that this can be accom-\nplished on desktop PCs, increasing the accessibility of RL for smaller research labs and hobbyists.\nWe acknowledge that there are many more promising improvements we were not able to include in\nBTR, leaving room for more future work in a few years to create even stronger integrated agents.\nFor example, BTR does not add an explicitly exploration component, resulting in it struggling in\nhard-exploration tasks such as Montezuma's Revenge; therefore, mechanisms used in Never Give\nUp (Badia et al., 2020b) or other components may prove useful. Section 5.1 found that the neural\nnetwork's core architecture, Impala, had the largest impact on performance, an area we believe\nis generally underappreciated in RL. Previous work (Kapturowski et al., 2018) has incorporated\nrecurrent models enhancing performance, though we are uncertain how this can be incorporated into\nBTR without affecting its computational accessibility."}, {"title": "8 ETHICS AND REPRODUCIBILITY STATEMENTS", "content": "Our work does not involve human subjects or methodologies with direct ethical concerns such as\ndiscrimination, bias, or privacy violations. Additionally, we have no conflicts of interest, spon-\nsorship issues, or violations of legal or research integrity were present during the development of\nthis research. However, we acknowledge that by improving the accessibility and performance of\nReinforcement Learning (RL), our contributions may inadvertently provide more powerful tools to\nmalicious actors, thus, we urge the research community to remain vigilant regarding these issues.\nTo ensure reproducibility, we provide a detailed background of the work we build upon and clearly\nexplain all changes made to the base algorithm. Furthermore, Appendix C.2 provides all relevant\nenvironmental and algorithmic hyperparameters needed to reproduce our work. Additionally, we\nprovide clarity about often misunderstood terms (Appendix C.3), a detailed architecture diagram\n(Appendix D) and the exact hardware we tested our algorithms on (Appendix H). Most importantly,\nwe provide BTR's code within the supplementary material. Lastly, we provide many details regard-\ning the Wii games tested BTR on, including the minor changes from BTR, how the environment was\nsetup and the reward functions used."}, {"title": "C.2 ALGORITHM HYPERPARAMETERS", "content": "Table C5: Table showing the hyperparameters used in the BTR algorithm.\nHyperparameter\nLearning Rate\nDiscount Rate\nN-Step\nIQN Taus\nIQN Number Cos'\nHuber Loss K\nGradient Clipping Max Norm\nParallel Environments\nGradient Step Every\nReplace Target Network Frequency\nBatch Size\nTotal Replay Ratio\nImpala Width Scale\nSpectral Normalization\nAdaptive Maxpooling Size\nLinear Size (Per Dueling Layer)\nNoisy Networks \u03c3\nActivation Function\ne-greedy start\ne-greedy decay\ne-greedy end\ne-greedy disabled\nReplay Buffer Size\nMinimum Replay Size for Sampling\nPER Alpha\nOptimizer\nAdam Epsilon Parameter\nAdam \u1e9e1\nAdam \u1e9e2\nMunchausen Temperature \u0442\nMunchausen Scaling Term a\nMunchausen Clipping Value (lo)\nEvaluation Epsilon\nEvaluation Episodes\nEvaluation Every\nValue\n1e-4\n0.997\n3\n8\n64\n1.0\n10\n64\n64 Environment Steps (1 Vectorized Environment Step)\n500 Gradient Steps (32K Environment Steps)\n256\n1\n64\n2\n6x6\n512\n0.5\nAll Convolutional Residual Layers\nReLu\n1.0\n2M Frames\n0.01\n100M Frames\n1,048,576 Transitions (2\u00b2\u2070)\n200K Transitions\n0.2\nAdam\n1.95e-5 (equal to\n0.9\n0.999\n0.03\n0.9\n-1.0\n0.01 until 125M frames, then 0\n100\n1M Environment Frames (250K Environment Steps)\nbatch size)\n64"}, {"title": "C.3 CLARITY OF THE TERMS FRAMES, STEPS AND TRANSITIONS", "content": "Throughout the Arcade Learning Environment's history (ALE) (Bellemare et al., 2013; Machado\net al., 2018), there have been many ambiguities around the terms: frames, steps and transitions,\nwhich are sometimes used interchangeably. Frames refer to the number of individual frames the\nagent plays, including those within repeated actions (also called frame skipping). This is notably\ndifferent from the number of steps the agent takes, which does not include these skipped frames.\nWhen using the standard Atari wrapper, training for 200M frames is equivalent to training for 50M\nsteps. Lastly, transitions refer to the standard tuple (st, at, rt, St+1), where the timestep t refers to\na steps, not frames. We encourage researchers to make this clear when publishing work, including\nwhen mentioning values of different hyperparameters."}, {"title": "D BEYOND THE RAINBOW ARCHITECTURE & LOSS FUNCTION", "content": ""}, {"title": "D.1 ARCHITECTURE", "content": "Figure D4 shows the the neural network architecture of the BTR algorithm. The architecture is\nhighly similar to the Impala architecture (Espeholt et al., 2018), with notable exceptions:\n\u2022 Spectral Normalization Within each Impala CNN blocks, each residual layer (containing two\nConv 3x3 + ReLu) has spectral normalization applied, as discussed in Section 3.1.\n\u2022 Maxpooling Following the CNN blocks, a 6x6 adaptive maxpooling layer is added.\n\u2022 IQN In order to use IQN, it is required to draw Tau samples which are multiplied by the output\nof the CNN layers, as shown by the section \u2018IQN Samples' in figure D4.\n\u2022 Dueling Dueling (as included in the original Rainbow DQN) splits the fully connected layers into\nvalue and advantage streams, where the advantage stream output has a mean of 0, and is\nthen added to the value stream.\n\u2022 Noisy Networks As included in Rainbow DQN, Noisy Networks replace the linear layers with\nnoisy layers.\nLastly, the sizes of many of the layers given in Figure D4 are dependant upon the Impala width scale,\nof which we use the value 2. For example, the Impala CNN blocks have [16\u00d7width, 32\u00d7width,\n32xwidth] channels respectively. The output size of the convolutional layers (including the max-\npooling layer) is 6\u00d76\u00d732\u00d7width, as a 6x6 maxpooling layer is used. Lastly, the cos embedding\nlayer after generating IQN samples requires the same size as the output of the convolutional layers,\nhence the size is selected accordingly. Another benefit of the 6x6 maxpooling layer is following the\nproduct of the convolutional layers and IQN samples, the number of parameters is fixed, regardless\nof the input size. Figure D5 shows the numbers of parameters the ablated versions of BTR have."}, {"title": "D.2 LosS FUNCTION", "content": "The resulting loss function for the BTR algorithm remains the same as that defined in the appendix of\nthe Munchausen paper, which gave a loss function for Munchausen-IQN. As the other components\nin BTR do not affect the loss, the resulting temporal-difference loss function is the same. For self-\ncontainment, we include this loss function below:\n$TDBTR = r_t + \\alpha[\u03c4ln(\u03c0(a_t)|s_t)]_{\u03c40} + \u03b3\\sum_{a \\in A}\u03c0(a|s_{t+1})(Q_{o'} (S_{t+1}, a) \u2013 \u03c4ln(\u03c0(a|s_{t+1})) \u2013 Q(St, at)$ (D1)\nwith \u03c0(\u00b7|s) = softmax((s)) (that is, the policy is softmax with q\u02dc, the quantity with respect to which\nthe original policy of IQN is greedy). It is also worth noting here that due to the character conflict of\nboth Munchausen and IQN using 7 (Munchausen as a temperature parameter, and IQN for drawing\nsamples), we replace IQN's T with \u03c3. 10, \u03c4 and a are hyperparameters set by Munchausen. We use\nthe same values in BTR, also shown in our hyperparameter table in Appendix C.2."}, {"title": "E BTR WITH FEWER TRAINING FRAMES", "content": "Some of BTR's improvements provide a relatively small improvement after 200M frames, however\nwe want to point out their importance using fewer samples. Table E6 shows that many improvements\nprovide large benefits earlier in training."}, {"title": "F ANALYSIS OF BTR'"}]}