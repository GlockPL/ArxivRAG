{"title": "Flexible Coded Distributed Convolution Computing for Enhanced Fault Tolerance and Numerical Stability in Distributed CNNS", "authors": ["Shuo Tan", "Rui Liu", "Xianlei Long", "Kai Wan", "Linqi Song", "Yong Li"], "abstract": "Deploying Convolutional Neural Networks (CNNs) on resource-constrained devices necessitates efficient management of computational resources, often via distributed systems susceptible to latency from straggler nodes. This paper introduces the Flexible Coded Distributed Convolution Computing (FCDCC) framework to enhance fault tolerance and numerical stability in distributed CNNs. We extend Coded Distributed Computing (CDC) with Circulant and Rotation Matrix Embedding (CRME) which was originally proposed for matrix multiplication to high-dimensional tensor convolution. For the proposed scheme, referred to as Numerically Stable Coded Tensor Convolution (NSCTC) scheme, we also propose two new coded partitioning schemes: Adaptive-Padding Coded Partitioning (APCP) for input tensor and Kernel-Channel Coded Partitioning (KCCP) for filter tensor. These strategies enable linear decomposition of tensor convolutions and encoding them into CDC subtasks, combining model parallelism with coded redundancy for robust and efficient execution. Theoretical analysis identifies an optimal trade-off between communication and storage costs. Empirical results validate the framework's effectiveness in computational efficiency, fault tolerance, and scalability across various CNN architectures.", "sections": [{"title": "I. INTRODUCTION", "content": "In the rapidly evolving domain of distributed machine learning, Convolutional Neural Networks (CNNs) have become fundamental due to their exceptional capabilities in image feature extraction and classification [1]\u2013[3], as well as their versatility enabled by transfer learning [4]\u2013[6]. A significant trend in this field, particularly relevant to Internet of Things (IoT) applications, is the shift towards edge computing, where data processing is conducted directly on edge devices [7]. This paradigm reduces dependency on cloud resources, minimizing latency [8] and enhancing data privacy [9]. Deploying CNNs in distributed systems, especially on resource-constrained devices, poses significant challenges due to intensive computational requirements, particularly within convolutional layers (ConvLs). Convolution operations represent over 90% of the Multiply-Accumulate operations (MACS) in mainstream CNN architectures [10], including AlexNet [11], VGGNet [12], GoogleNet [13] and ResNet [14], and account for more than 80% of the computational time during inference [15]. Collaborative inference across multiple devices has emerged as a viable approach to mitigate the computational burden on individual devices and enhance CNN deployment efficiency [16]. However, inference latency is often significantly impacted by slow worker nodes, commonly referred to as stragglers. These stragglers, arising from hardware heterogeneity and variable network conditions [17], can lead to performance degradation and potential failures, particularly in IoT systems where data loss rates may exceed 70% per layer [15]. Coded Distributed Computing (CDC) has been introduced to enhance computational resilience and efficiency in distributed systems. By dividing a task into m subtasks and introducing redundancy through an (n,m) error correction code (with n > m workers), CDC allows the master node to obtain the desired solution by waiting for the fastest k workers (m < k< n), referred to as the recovery threshold, thereby significantly reducing computation time against the straggler effect. CDC has been successfully applied in Coded Matrix-Matrix Multiplication (CMMM) and other matrix multiplication-based algorithms [18]\u2013[23], due to the ease of linear decomposition of these operations. The application of CDC in distributed deep learning systems has garnered significant attention. Dutta et al. introduced a unified coded deep neural network (DNN) scheme utilizing Generalized PolyDot codes [24], which demonstrated the feasibility of deploying large DNNs on unreliable nodes prone to soft errors. Subsequent works, such as [15], [25], further integrated CDC at the model parallelism level in DNNs. However, these CMMM-based methods are not directly extensible to tensor convolutions in CNNs. Specifically, the decomposition of tensors \\(T \\in \\mathbb{R}^{d_1\\times\\ldots\\times d_n}\\) for CDC sub-tasks introduces multiple splitting dimensions, necessitating specialized partitioning schemes. Numerical stability is also a critical concern for CDC in DNNs due to the substantial propagation depth and cumulative errors associated with deep architectures [26]. Most existing CDC schemes\u2014including"}, {"title": "II. SYSTEM MODEL", "content": "The proposed Flexible Coded Distributed Convolution Computing (FCDCC) framework employs a master-worker architecture, consisting of a single master node and n homogeneous worker nodes. To enhance computational robustness, the system should be able to tolerate up to \\(\\gamma\\) straggler nodes, i.e., worker nodes that are slow or unresponsive. Key parameters in this framework include:\n\\( \\delta = \\frac{|\\mathcal{K}_{A K_B}|}{l} \\), where \\(k_A\\) and \\(k_B\\) are partitioning parameters along the spatial"}, {"title": "III. NUMERICALLY STABLE CODED TENSOR CONVOLUTION", "content": "To address the limitations of traditional CDC schemes in high-dimensional convolution operations, this section introduces two new techniques: (1) partitioning tensor lists into subgroups for parallel convolutions, and (2) utilizing tensor-matrix multiplication for encoding and decoding. These techniques establish NSCTC as the foundational component of the FCDCC framework.\n\\(T\\cdot M = [T_0,\\ldots,T_{U_k-1}] \\cdot \\begin{bmatrix} m_{0,0} & \\ldots & m_{0,U_n-1} \\\\ \\vdots & \\ddots & \\vdots \\\\ m_{U_k-1,0} & \\ldots & m_{U_k-1,U_n-1} \\end{bmatrix} = [\\sum_{i=0}^{U_k-1}m_{i,0}T_i,\\ldots,\\sum_{i=0}^{U_k-1}m_{i,U_n-1}T_i] = \\tilde{T}\\)\n\\(T_{A(i,j)} = T_A\\cdot G_{2i+j}\\)\n\\(T_{A(0,j)} + \\sum_{k=1}^{\\frac{k_A}{l}-1} \\sum_{l=0}^{l-1} R_{\\theta}^k (l,j)T_{A(k,l)},\\)\n\\(T_{B(i,j)} = T_B\\cdot G_{2i+j}\\)\n\\(T_{B(0,j)} + \\sum_{k=1}^{\\frac{k_B}{l}-1} \\sum_{l=0}^{l-1} (A)_k(l,j)T_{B(k,l)},\\)\n\\(T_{A(i,l_1)} *T_{B(i,l_2)} = (T_A \\cdot G_{2i+l_1}) * (T_B \\cdot G_{2i+l_2}) = (T_A * T_B) \\cdot (G_{2i+l_1} \\otimes G_{2i+l_2})\\)\n\\(T_{C_{i}} = T_C\\cdot ([G_0^{2i} \\otimes G_0^{2i+1}] \\otimes [G_0^{2i} \\otimes G_0^{2i+1}]) = T_C \\cdot G_E.\\)"}, {"title": "IV. FLEXIBLE CODED DISTRIBUTED CONVOLUTION COMPUTING", "content": "In this section, we embed the NSCTC scheme into the ConvLs of CNNs using Adaptive-Padding Coded Partitioning (APCP) and Kernel-Channel Coded Partitioning (KCCP) schemes. These partitioning methods decompose the convolution operations of input and filter tensors into encodable subtensors, which correspond to the tensor lists introduced in Section III. Furthermore, we propose an optimal partitioning strategy to achieve cost efficiency within the FCDCC framework.\n\\( \\hat{H} = (\\lfloor \\frac{H'}{k_A} \\rfloor - 1) \\times s + K_H,\\)\n\\( \\hat{S} = \\frac{H'}{k_A} \\times s.\\)\n\\(X' = X[:, i\\hat{S} : i\\hat{S} + \\hat{H}, :], \\text{ for } i \\in Z_{K_A} \\) \n\\(A_{i,j} = (R_{\\frac{2\\pi}{q}})^{(j \\cdot i)},\\text{ for } i \\in Z_{K_A}, j \\in Z_n.\\)\n\\(B_{ij} = (R_{\\frac{2\\pi}{q}})^{(j \\cdot i)},\\text{ for } i \\in Z_{K_A}, j \\in Z_n.\\)\n\\(K' = K[: \\frac{N}{K_B} \\cdot (i+1), :, :, :], :, : \\in Z_{K_B},\\)\n\\(\\tilde{K}_{(i,j)} = \\sum_{\\alpha\\in Z_{\\lfloor{K_B}/2\\rfloor} \\sum_{\\beta\\in Z_2} B(2\\alpha + \\beta, 2i + j)K'_{(\\alpha,\\beta)},\\) for \\(i \\in Z_n, j\\in Z_2.\\)\n\\(G = A \\otimes B = [A_0 \\otimes B_0 | \\ldots | A_{n-1} \\otimes B_{n-1}],\\)\n\\(\\tilde{Y}_{ (i,2\\beta_2+\\beta_1)} = \\tilde{X}_{ (i,\\beta_1)} * \\tilde{K}_{ (i,\\beta_2)},\\)"}, {"title": "V. RESULTS AND COMPLEXITY ANALYSIS", "content": "This section presents a comprehensive assessment of the FCDCC framework, focusing on its resilience to stragglers, condition number analysis, and complexity evaluations of the encoding, computation, communication, and decoding phases. All analyses are based on Multiply-Accumulate (MAC) operations and tensor entry counts. We also compare the FCDCC framework with existing model parallelism methods to evaluate its efficiency in distributed CNNs.\n\\( k_A = \\Theta(\\sqrt{Q}), k_B = \\Theta(\\sqrt{Q}).\\)"}, {"title": "VI. EXPERIMENTS", "content": "This section presents the experimental evaluation of the proposed FCDCC framework, conducted on Amazon EC2. The experiments focus on the inference of a single batch across various ConvLs of LeNet, AlexNet, and VGGNet models. Performance was assessed based on time efficiency, resilience to stragglers, and numerical stability."}, {"title": "VII. CONCLUSION AND FUTURE WORK", "content": "In this paper, we presented FCDCC framework, which integrates NSCTC with APCP and KCCP schemes. The FCDCC framework enhances numerical stability, system resilience, computational efficiency, and cost-effectiveness in distributed convolution computing. Our theoretical analysis and extensive experimental results on networks such as LeNet-5, AlexNet, and VGGNet demonstrate that FCDCC significantly improves computational performance compared to traditional uncoded and existing coded schemes. Future work includes refining the coding mechanisms, extending the CDC scheme to support pooling layers and nonlinear activation functions, and enhancing privacy protection to safeguard against colluding and malicious workers."}]}