{"title": "Can LLMs Generate Novel Research Ideas?", "authors": ["Chenglei Si", "Diyi Yang", "Tatsunori Hashimoto"], "abstract": "Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very first step of producing novel, expert-level ideas, let alone perform the entire research process. We address this by establishing an experimental design that evaluates research idea generation while controlling for confounders and performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, we obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we find LLM-generated ideas are judged as more novel (p < 0.05) than human expert ideas while being judged slightly weaker on feasibility. Studying our agent baselines closely, we identify open problems in building and evaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. Finally, we acknowledge that human judgements of novelty can be difficult, even by experts, and propose an end-to-end study design which recruits researchers to execute these ideas into full projects, enabling us to study whether these novelty and feasibility judgements result in meaningful differences in research outcome.", "sections": [{"title": "Introduction", "content": "The rapid improvement of LLMs, especially in capabilities like knowledge and reasoning, has enabled many new applications in scientific tasks, such as solving challenging mathematical problems (Trinh et al., 2024), assisting scientists in writing proofs (Collins et al., 2024), retrieving related works (Ajith et al., 2024, Press et al., 2024), generating code to solve analytical or computational tasks (Huang et al., 2024, Tian et al., 2024), and discovering patterns in large text corpora (Lam et al., 2024, Zhong et al., 2023). While these are useful applications that can potentially increase the productivity of researchers, it remains an open question whether LLMs can take on the more creative and challenging parts of the research process.\nWe focus on this problem of measuring the research ideation capabilities of LLMs and ask: are current LLMs capable of generating novel ideas that are comparable to expert humans? Although ideation is only one part of the research process, this is a key question to answer, as it is the very first step to the scientific research process and serves as a litmus test for the possibility of autonomous research agents that create their own ideas. Evaluating expert-level capabilities of LLM systems is challenging (Bakhtin"}, {"title": "Problem Setup", "content": "The central experiment of our work is a comparison of human- and LLM-generated ideas. While this goal is simple, there is no existing consensus on how to formulate the task of research ideation and evaluation, and we begin by defining the key aspects of our experiment design.\nWe think of research idea evaluation as consisting of three separate components: 1). the idea itself, generated in response to our instructions, 2). the writeup which communicates the idea, and 3). the evaluation of the writeup by experts. We outline our experiment design in each of these three parts with particular focus on potential confounders, such as the area of research, the format of a research idea, and the evaluation process.\nIdeation Scope and Instructions Research ideas can take many different forms. They can be simple tricks to improve model performance, or they may be large-scale research programs that form the basis of a Ph.D. thesis. Any experiment on ideation must carefully balance the realisticness and interestingness of a research idea with the practical realities of eliciting ideas from a large population. In our case, these tradeoffs are even more pronounced, as we have designed our ideation experiments so that the resulting ideas can be executed by experts in a follow-up set of experiments.\nThese constraints have led us to study prompting-based NLP research as a testbed for our study. Prompting research has been popular in recent years of NLP and AI research (e.g., Chen et al., 2023, Diao et al., 2024, Madaan et al., 2023, Qin et al., 2024, Schulhoff et al., 2024, Si et al., 2023, Wang et al., 2023, Wei et al., 2022, Yao et al., 2023, Yasunaga et al., 2024, Zhou et al., 2023, inter alia). This class of projects strikes a reasonable trade-off among our constraints. The most impactful prompting projects like chain-of-thought have had a major influence on LLM performance (Wei et al., 2022), and prompting projects are executable with minimal computing hardware.\nWe further structure our ideation process to avoid selection-bias-based confounders in ideation. If we simply ask LLMs and humans to produce ideas on \u2018prompting topics', we may find that LLMs and humans differ in the types of research ideas they produce (for example, LLMs may naturally suggest more projects on safer topics, which might be judged as less exciting by humans). This would"}, {"title": "Idea Writeup", "content": "lead us to simply measure misalignment in research topic preference between LLMs and humans, which is not the goal of our study. To address this possibility, we define a set of seven specific research topics extracted from the Call For Papers page of recent NLP conferences such as COLM. Specifically, our topics include: Bias, Coding, Safety, Multilinguality, Factuality, Math, and Uncertainty (see Appendix A for a complete description of these topics).\nEach human and LLM participant of the ideation experiment receives the same set of natural language instructions including the same topic description, idea template, and demonstration example to ensure a fair comparison. For human participants, we additionally allow them to select a preferred topic from the list, and for each selected topic, we generate a corresponding LLM idea. This exactly matches the idea topic distribution between the LLM and human participants, while ensuring that human experts are able to select topics according to their expertise.\nAn idea can only be evaluated if it is written up to be communicated, but this writing process introduces many additional potential confounders. Human researchers may write in ways that subtly signal quality research, such as including more examples and implementation details. The format of the writeup functions as a way to scaffold what contents should be included and the level of detailedness. Ideally, we want both human and LLM participants to provide all the necessary implementation details for their generated ideas.\nWe take inspiration from guidelines used in grant submissions and introduce a template to specify the structure and detailedness of idea proposals. Specifically, we construct a template that includes fields for the title, problem statement, motivation, proposed method, step-by-step experiment plan, test case examples, and the fallback plan. Both the LLM agent and the human idea writers are instructed to follow this template and our provided demonstration examples to produce a project proposal as the output (see Appendix B for the full template and Appendix C for the demo example).\nEven with these templates, there may be subtle writing style cues that affect the outcome measure. For example, humans may tend to write in a more engaging and informal tone. To reduce this possibility further, we developed a style normalization module that uses an LLM to convert all ideas into the same writing and formatting style without changing the original content. Our small-scale human study shows that such a normalization approach leads to a 50% accuracy for expert human judges who are asked to distinguish Al ideas from human ideas. Finally, the use of an LLM style anonymizer has the possibility of substantively changing the content of the ideas. To rule this out, the first author of this paper manually verified each human idea proposal to ensure all contents of the original ideas were preserved. We present the full prompt used in Appendix D."}, {"title": "Review and Evaluation", "content": "Reviewing research ideas is notoriously subjective, so we want to design a review form that defines all review criteria clearly to standardize and anchor the evaluations as much as possible. At the same time, we want our review criteria and measured variables to capture all the desiderata of high-quality research ideas.\nWe follow best practices from AI conference reviewing (e.g., ICLR and ACL) when designing the review form, where we define four breakdown metrics including novelty, excitement, feasibility, and expected effectiveness, apart from the overall score. For each metric, we ask for a numerical score on a 1-10 scale along with a free-text rationale. We provide clear definitions and grounding for each numerical scale to calibrate all reviewers' standards (see Appendix E for the full review form).\nOur blind review evaluation will compare ideas from three different conditions:\n1. Human Ideas: Idea proposals written by our recruited expert researchers."}, {"title": "Idea Generation Agent", "content": "We build a simple but effective LLM ideation agent to compare with the human expert baseline. Rather than focusing on innovating the agent itself, we adhere to a minimalist design principle, aiming to understand the current capabilities of LLMs in idea generation. Our research ideation agent has three essential components: paper retrieval, idea generation, and idea ranking, which we will describe in detail below."}, {"title": "Paper Retrieval for RAG", "content": "To ground idea generation, the agent needs to retrieve papers related to the given research topic, so that it will be aware of related works when generating new ideas. To do so, we leverage retrieval-augmented generation (RAG), which has demonstrated effectiveness on many knowledge-intensive tasks (Lewis et al., 2020, Shi et al., 2024). Concretely, given a research topic (e.g., \u201cnovel prompting methods that can improve factuality and reduce hallucination of large language models\"), we prompt an LLM to generate a sequence of function calls to the Semantic Scholar API. We use claude-3-5-sonnet-20240620 as the backbone model for our agent but the pipeline should generalize to other LLMs as well. The paper retrieval action space includes: {KeywordQuery(keywords), PaperQuery(paperId), GetReferences(paperId)}. Each action generation is grounded on the previous actions and executed results. We keep the top k=20 papers from each executed function call and stop the action generation when a max of N = 120 papers have been retrieved. We then use the LLM to score and rerank all retrieved papers based on three criteria: 1) the paper should be directly relevant to the specified topic; 2) the paper should be an empirical paper involving computational experiments;\u00b3 3) the paper is interesting and can inspire new projects. The LLM is prompted to score each retrieved paper on a scale of 1 to 10 based on these criteria and we use the top-ranked papers for the next step of idea generation.\""}, {"title": "Idea Generation", "content": "Our key insight for idea generation is to generate as many candidate ideas as possible. Our intuition is that only a small fraction of all generated ideas might be high-quality, and we should be willing to expend inference-time compute to generate more candidates so that we can later use a reranker to discover the \"diamond in the rough\". This aligns with existing results showing that scaling inference compute with repeated sampling can boost LLM performance on various coding and reasoning tasks (Brown et al., 2024, Li et al., 2022). Specifically, we prompt the LLM to generate 4000 seed ideas on each research topic. The idea generation prompt includes the demonstration examples and the retrieved papers. We craft k = 6 demonstration examples by manually summarizing exemplar"}, {"title": "Idea Ranking", "content": "The next step is for our ideation agent to rank all the remaining ideas so that we can find the best ones among them. To build such an automatic idea ranker, we use public review data as a proxy. Specifically, we scraped 1200 ICLR 2024 submissions related to LLMs (with keyword filtering) along with their review scores and acceptance decisions. We explored multiple ways of predicting the scores and decisions of these submissions and found that LLMs are poorly calibrated when asked directly to predict the final scores or decisions, but can achieve non-trivial accuracy when asked to judge which paper is better in pairwise comparisons.\nWe converted the ICLR submissions into our standard project proposal format and randomly paired up accepted and rejected papers and asked LLMs to predict which one is accepted. On this task, Claude-3.5-Sonnet achieves an accuracy of 71.4% with zero-shot prompting. For comparison, GPT-4o achieves 61.1% and Claude-3-Opus achieves 63.5%, and we do not observe significant gains from additional prompting techniques like few-shot or chain-of-thought prompting. We therefore choose the Claude-3.5-Sonnet zero-shot ranker.\nIn order to obtain reliable scores for all project proposals based on pairwise comparisons, we adopt a Swiss system tournament where all project proposals are paired with those whose accumulated scores are similar, and if the proposals are judged to be better, they gain an additional point. We repeat this for N rounds so the total score of each project proposal will be within the [0, N] range. As a sanity check, we use the Claude-3.5-Sonnet ranker to rank the 1.2K ICLR LLM-related submissions and compare the average review scores of the top 10 ranked papers and the bottom 10 ranked papers in Table 1. We see a clear separation between the top and bottom ranked papers, indicating the effectiveness of the LLM ranker. We choose N =5 for all our experiments since it gives the best ranking result on this validation set. The top-ranked project proposals from the agent will be directly used for the AI Ideas condition of the human study.\nSince our AI ranker is still far from perfect, we also introduce another experiment condition where the first author of this paper manually reranked the generated project proposals instead of relying on the LLM ranker, and we call this the AI Ideas + Human Rerank condition. As we show in"}, {"title": "Expert Idea Writing and Reviewing", "content": "In this section, we shift focus to the human branch of idea generation comparison. We present the details of our human study, including information about the recruited experts, the human idea generation task, and the subsequent review process."}, {"title": "Expert Recruitment", "content": "We recruit our expert participants (including for idea writing and reviewing) by sending sign-up forms to several channels, including: 1) the OpenNLP Slack channel with 1426 NLP researchers from 71 institutions (with consent from the channel manager); 2) Twitter (X); 3) Slack channels of various NLP groups by direct communication with the group members; and 4) official chat app of the NAACL 2024 conference. We also conducted in-person recruitment by giving out name cards and wearing T-shirts 5 with sign-up links at the NAACL 2024 conference as well as various other local NLP social events. Our study has been approved by the Stanford IRB (ID 74246).\nWe performed screening on all the US participants 6 based on their provided Google Scholar profiles. We set a minimum requirement of having published at least one paper at a major AI venue. 7 We reached out to all participants who satisfied this requirement with the consent form and followed up with the annotation documents for those who consented to participate.\nIn the end, we recruited N = 49 experts for writing ideas, and N = 79 experts for reviewing ideas. Note that 24 out of the 79 reviewers also participated in the idea writing, and we made sure no reviewer would review their own idea. This results in N = 104 total participants across the two tasks. Each idea writer is asked to write one idea within 10 days and we compensate $300 for each, with a $1000 bonus for the top 5 ideas as scored by the expert reviewers. Each idea reviewer is assigned 2 to 7 ideas to review and we collected N = 298 unique reviews in total. They are given one week to finish the reviews and we compensated $25 for each review written by the idea reviewers."}, {"title": "Expert Qualifications", "content": "Our pool of participants is highly qualified and diverse. The 49 idea writers come from 26 different institutions (Table 15) and the majority of them are current PhD students (Figure 3 left). The 79 reviewers come from 32 institutions (Table 16) and are mostly PhD students and Postdocs (Figure 3 right). We use their Google Scholar profiles to extract several proxy metrics, including the number of papers, citations, h-index, and i10-index at the time of their submission. Table 2 shows that our idea writers have an average of 12 papers and 477 citations, while every reviewer has published at least two papers and has an average citation of 635 and h-index of 7. Moreover, based on their survey"}, {"title": "Idea Writing", "content": "We report statistics of our idea writers' ideas to measure their quality. As shown in Table 3, idea writers indicate a moderately high familiarity with their selected topic (3.7 on a 1 to 5 scale), and indicate the task as moderately difficult (3 on a 1 to 5 scale). They spent an average of 5.5 hours on the task and their ideas are 902 words long on average. These indicate that participants are putting substantial effort into this task. 9 We also show the distribution of their selected topics in Table 4."}, {"title": "Idea Reviewing", "content": "We let all reviewer participants select their top two preferred topics as well as their preferred reviewing load (from 2 to 7). We then randomly assign them to ideas within their selected topics and all ideas are anonymized. In the assignment, we balance the number of ideas from each condition for each reviewer and ensure that each reviewer gets at least one human idea and one Al idea. Every idea is reviewed by 2 to 4 different reviewers. We also avoid assigning ideas written by authors from the same institution to avoid any potential contamination. Table 5 shows that each reviewer wrote an average of 3.8 reviews from 2 or 3 conditions, across 1 to 3 topics."}, {"title": "Test 1: Treating Each Review as an Independent Datapoint", "content": "In Test 1, we treat each review as an independent datapoint and aggregate all reviews from the same condition. We treat the Human Ideas as the baseline condition and compare it with AI Ideas and AI Ideas + Human Rerank using two-tailed Welch's t-tests with Bonferroni correction. We show the barplot in Figure 2 and the detailed numerical results in Table 7. Both AI Ideas (\u03bc=5.64\u00b1\u03c3=1.76) and AI Ideas + Human Rerank (\u03bc=5.81\u00b1\u03c3=1.66) are significantly better than Human Ideas (\u03bc = 4.84 \u00b1 \u03c3 = 1.79) on the novelty score (p < 0.01). In this particular test, the AI ideas in both conditions are also significantly better than human ideas on the excitement score (p<0.05), and the AI Ideas + Human Rerank condition is also significantly better than Human Ideas in terms of"}, {"title": "Test 2: Treating Each Idea as an Independent Datapoint", "content": "Since we collect multiple reviews for each idea, one could argue that we should not treat each review as an independent datapoint. To account for this potential confounder, we perform Test 2 where we average the scores of each idea and treat each idea as one datapoint. This way, the sample size for every condition will be N = 49, namely the number of ideas. We treat the Human Ideas as the baseline condition and compare it with AI Ideas and AI Ideas + Human Rerank using two-tailed Welch's t-tests with Bonferroni correction. As shown in Table 8, we still see significant results (p < 0.05) where both AI Ideas (\u03bc = 5.62 \u00b1 \u03c3 = 1.39) and AI Ideas + Human Rerank (\u03bc=5.78\u00b1\u03c3=1.07) have higher novelty scores than Human Ideas (\u03bc=4.86\u00b1\u03c3=1.26)."}, {"title": "Test 3: Treating Each Reviewer as an Independent Datapoint", "content": "Another possible confounder is that different reviewers might have different biases, for example, some reviewers may be more lenient than others. To account for such reviewer biases, we perform Test"}, {"title": "In-Depth Analysis of the Human Study", "content": "While the above main results highlight the promise of LLMs in generating novel research ideas, there are some additional nuances. In this section, we move beyond the statistical comparisons and dive"}, {"title": "Human Experts May Not Be Giving Their Best Ideas", "content": "We first investigate whether human experts are submitting their best ideas to us. We did a post-study survey to understand how idea-writing participants came up with their ideas. Out of the 49 participants, 37 of them came up with the idea on the spot, while the other 12 already had the idea before the study. Furthermore, we asked the survey question: \u201cHow does this idea compare to your past research ideas (ideas that you actually worked on)? Please answer with a percentile. E.g., this idea is one of my top 10% ideas.\u201d Our participants indicated that on average their submitted ideas are about the top 43% of all their past ideas. This implies that our collected ideas are likely the median-level ideas from these expert researchers, which is reasonable given that most of them came up with the idea within the 10-day time constraint of the task."}, {"title": "Reviewers Tend to Focus More on Novelty and Excitement", "content": "To gain a deeper understanding of the dynamics between the different metrics in the review process, we explore whether reviewers focus on specific aspects when evaluating the ideas. We compute the pairwise correlation between different metrics in Table 10. The overall score mostly correlates with the novelty score (r=0.725) and excitement score (r=0.854), while having almost no correlation (r <0.1) with the feasibility score. This implies that reviewers might be paying more attention to the novelty and excitement aspects of the ideas when they are reviewing."}, {"title": "Reviewing Ideas is Inherently Subjective", "content": "Finally, we acknowledge that reviewing is inherently subjective, and reviewing based on ideas rather than executed papers might be even more subjective. We investigate this using inter-reviewer agreement. Specifically, we randomly split reviewers of each paper into half, use one half to rank the top and bottom 25% of all ideas, and then measure agreement with the held-out set of reviewers. 11 As shown in the first block of Table 11, reviewers have a relatively low agreement (56.1%) despite the fact that we have provided detailed explanations for each metric in our review form. As a baseline comparison, the NeurIPS 2021 reviewer consistency experiment found 66.0% accuracy using this reviewer agreement metric in the balanced setting (Beygelzimer et al., 2021, Lu et al., 2024). We also computed the reviewer agreement using the same metric on the 1.2K ICLR 2024 submissions related to language models, which has a balanced accuracy of 71.9%. While our reviewer agreement is higher than random (50%), it is generally lower than conference reviewing, most likely due to the higher subjectivity involved when evaluating ideas without seeing the actual experiment results."}, {"title": "Limitations of LLMS", "content": "With our findings from the human study in mind, we now turn to LLM performance to provide insights that could inform future methods for improving idea generation systems. Our ideation agent is motivated by two potential strengths of LLMs: their ability to scale by generating a vast number of ideas - far more than any human could - and the possibility of filtering these ideas to extract the best ones from the large pool. In theory, this approach could lead to high-quality ideas by leveraging inference scaling. However, we present empirical evidence that this naive assumption about scaling idea generation has significant limitations."}, {"title": "LLMs Lack Diversity in Idea Generation", "content": "We adopted an over-generate and rank paradigm in idea generation. This raises the question: is there an upper limit to how many new ideas LLMs can generate? To answer this question, we take a closer look at 4000 generated seed ideas for each topic.\nWe encode all raw ideas with all-MiniLM-L6-v2 from Sentence-Transformers. For each idea, we compute its cosine similarity with all previously generated ideas on the same topic. We consider an idea as a duplicate if it has a similarity of above 0.8 with any of the previously generated ideas. In Figure 4, we show that as the agent keeps generating new batches of ideas, the percentage of non-duplicates in newly generated batches keeps decreasing, and the accumulated non-duplicate ideas eventually plateau. In fact, out of the 4000 generated seed ideas, there are only 200 non-duplicate"}, {"title": "LLMs Cannot Evaluate Ideas Reliably", "content": "Most prior works have adopted LLM-as-a-judge for evaluating research ideas (Lu et al., 2024) motivated by the observation that LLMs can have a higher agreement with human evaluators than the inter-human agreement. However, we offer some empirical evidence that LLMs cannot evaluate ideas reliably yet.\nConcretely, we use the average review score of each idea to rank the top and bottom 25% of all our collected human and AI ideas, and use this to benchmark various LLM evaluators. Specifically, we obtain the LLM predicted scores of all ideas and set the median score as the threshold to measure their accuracy on our balanced idea ranking data.\nIn the second block of Table 11, we compare several different LLM evaluators: 1) directly giving the review criteria and prompting for a final score (Baek et al., 2024, Li et al., 2024, Yang et al., 2024); 2) our pairwise ranker as described in Section 3.3; and 3) the \u201cAI Scientist\u201d reviewer agent (Lu et al., 2024). All of these LLM evaluators have a lower agreement than our expert reviewers' scores. Even the best LLM evaluator - our own Claude-3.5 pairwise ranker - only achieves an accuracy of 53.3%, lower than our inter-reviewer consistency of 56.1%.\nEven if AI-human agreement eventually matches or exceeds human-human agreement, simply meeting this baseline does not imply that AI-as-a-reviewer is meaningful, since we may be trading variance for bias, where AI reviewers are more consistent but rely on spurious correlations (Durmus et al., 2022). Our findings in Table 11 are consistent with these brittleness concerns, as we find a significant drop in AI-human agreement scores under our study compared to the original studies.\nFinally, even though Claude-3.5 pairwise agreements may seem close to human agreement, many other pieces of evidence throughout the paper leads us to be cautious about the use of LLM-as-a-judge"}, {"title": "Qualitative Analysis and Examples", "content": "In this section, we offer some qualitative analysis of human- and AI-generated ideas based on our collected reviews and present four pairs of randomly sampled human and Al ideas as case studies."}, {"title": "Analysis of Free-Text Reviews", "content": "Following recent practices of using LLMs to extract patterns from text corpora (Zhong et al., 2022, 2023), we use Claude-3.5 to extract and cluster the main points from all reviews. We then manually verified and labeled each cluster.\nMany reviews reinforce our quantitative finding that AI ideas tend to be more novel. For example, reviewers noted: \u201cThe idea of [...] is quite novel in an in-context learning setting.\u201d, \u201cThe idea of exploring [...] using an LLM-based iterative approach is novel.\u201d, \u201cThe idea of [...] when constructing prompts to improve cross-lingual transfer is one that I have not heard of before.\u201d, \u201cI like the idea to [...], and think it will be helpful for other researchers in the community.\u201d, \u201cCombining [...] is a unique way of attempting to preserve the gist of the information while likely losing specific identifiers.\", and \"Safeguarding using [...] is clearly novel. Similar ideas have not been seen in the related work.\".\nNext, we summarize some common failure modes of AI ideas:\""}, {"title": "Human Experts May Not Be Giving Their Best Ideas", "content": "1.  Human ideas are generally more grounded in existing research and practical considerations, but may be less innovative.  For example, these ideas might be applying existing techniques to new problems: \u201cMultilinguality as a debiasing method has already been considered in the literature, although not necessarily in the prompt engineering framework.\u201d Sometimes people apply incremental changes to existing techniques: \u201cThe overall idea shares quite a similar idea with program-of-thought (PoT). The only difference is that there is an additional step where an LLM is prompted to decide whether to use code or not.\u201d Some ideas try to combine existing techniques: \u201cQuery decomposition and RAG separately are well studied, if there is no existing work that combines both (which I'm not aware of), then it's reasonably novel.\u201d As some reviewers"}, {"title": "Problem Statement", "content": "1. Being too vague on implementation details. For example, one reviewer noted: \u201cI'm not super clear on the details of this lattice and how the model will be prompted, so I'm not super sure how well the model will complete these subtasks and how well-suited this particular structure is to completing the overall task.\u201d and another reviewer noted: \u201c'For analyzing the effectiveness of the method, the proposal only provides a very ad-hoc + hand-wavey suggestion to compare responses across predefined questions.\u201d In another case, the AI idea is criticized for not considering practical implementation details: \u201cI think in each of the steps, there is something hard to execute. For example, in step Constellation Formation, how do we do the weighted sum?", "noted": "It's unclear how CLIP is connected to the language model and how training a CLIP model would enable the LM to understand images.", "There's no mentioning on how to prompt the model to generate defensive strategies and refine the model's responses using these strategies.": "uch vagueness often makes it difficult for reviewers to make confident judgments: \u201cBecause this idea is too general and vague, I can't really answer the previous question. An idea needs a certain level of details to be determined if it fits for a conference/journal but this one misses them.", "example": "I'm not sure about the datasets picked. StereoSet is not a QA dataset; it simply contains statements. Also, I don't understand why Dialogue NLI responses require empathy.", "I'm concerned the datasets proposed are the right test cases for security of the code (since they are really just ML/programming problems, not system-level programming).\", and \\\"the choice of datasets might not be the best to show the effect of incorporating multiple perspectives, especially TruthfulQA and ScienceQA, which seems to have a single correct interpretation and answer.": "n another example, the benchmark datasets chosen are considered"}, {"title": "Making Unrealistic Assumptions", "content": "1.  Not adequately following existing best practices.  For example: \u201cThe proposal does not seem to include awareness of what has been previously tried, or more strategic ways to evaluate success/failures...\"\nWe contrast these with some of the unique strengths and weaknesses of human ideas:\""}, {"title": "Problem Statement", "content": "notes, human ideas tend to build on known intuitions and results: \u201cThere are already existing works on using available lexicons to improve the translation capabilities of LLMs in general.\u201d\n2.  Human ideas tend to be more focused on common problems or datasets in the field.  For example: \"The problem of models not handling negation properly is a very common problem, especially among propriety LMs such as claude-3-5-sonnet.\u201d, \u201cThe data exist. This project mainly entails plugging in these datasets to a prompt template and finetuning for a bit. There is little left unspecified, and it should be quite simple to execute on.\u201d, \u201cI haven't found any work using this idea to solve this specific problem, but [...] is definitely not new.\u201d, and \u201cWhile existing works have explored the problem of calibration in long-form answers (e.g. [...]), the specific method for calibration is different.\"\n3.  Human ideas sometimes prioritize feasibility and effectiveness rather than novelty and excitement.  For example, reviewers noted: \u201cI don't think this will be a groundbreaking finding, but it will probably work.\u201d and \u201cwhile the idea is promising and could lead to significant improvements, it may not be groundbreaking enough to be considered transformative or worthy of a best paper award\u201d."}, {"title": "Randomly Sampled Human and AI Ideas with Reviews", "content": "We randomly sample four pairs of ideas from different topics to ground our numerical results with actual examples. In each pair, there is one AI idea and one human idea. To save space, we include the full project proposal of each idea along with the full set of reviews in the Appendix, but we list their titles, topics, and average scores here for quick reference (we reveal whether each idea is AI-generated or human-written in Appendix X):\n1. Modular Calibration for Long-form Answers: Appendix P\nTopic: Uncertainty; Average Overall Score: 5.5\n2. Semantic Resonance Uncertainty Quantification: Calibrating LLM Confidence through Multi-Path Reasoning: Appendix Q\nTopic: Uncertainty; Average Overall Score: 6\n3. Translation with LLMs through Prompting with Long-Form Context: Appendix R\nTopic: Multilingual; Average Overall Score: 4\n4. Linguistic Pivot Constellation: Enhancing Cross-Lingual Transfer for Low-Resource Languages and Dialects: Appendix S\nTopic: Multilingual; Average Overall Score: 6.7\n5. LLM Directed Retrieval Querying for Improving Factuality: Appendix T\nTopic: Factuality; Average Overall Score: 4.7\n6. Semantic Divergence Minimization: Reducing Hallucinations in Large Language Models through Iterative Concept Grounding: Appendix U\nTopic: Factuality; Average Overall Score: 3.3\n7. Autoprompting: Generate Diverse Few-shot Examples for Any Application: Appendix V\nTopic: Coding; Average Overall Score: 5\n8. Temporal Dependency Unfolding: Improving Code Generation for Complex Stateful Systems: Appendix W\nTopic: Coding; Average Overall Score: 6.7"}, {"title": "Related Work", "content": "Research idea generation and execution. Several prior works explored methods to improve idea generation, such as iterative novelty boosting (Wang et al., 2024), multi-agent collaboration (Baek et al., 2024), and multi-module retrieval and revision (Yang et al., 2024). While some of them share similar components as our ideation agent, these works focus on improving the idea generation methods over vanilla prompting baselines, without comparisons to any human expert baselines. Beyond ideation, another line of work uses LLMs for executing experiments by generating code given the research problems (Huang et al., 2024, Tian et al., 2024), or combining idea generation with code generation to directly implement AI-generated ideas (Li et al., 2024, Lu et al., 2024). These works either use automatic evaluation on a pre-defined set of problems and benchmarks, setting a constrained problem space; or rely on proxy metrics like LLM evaluators, which are often unreliable"}]}