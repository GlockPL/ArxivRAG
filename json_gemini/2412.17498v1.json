{"title": "DRT-01: Optimized Deep Reasoning Translation via Long Chain-of-Thought", "authors": ["Jiaan Wang", "Fandong Meng", "Yunlong Liang", "Jie Zhou"], "abstract": "Recently, O1-like models have emerged as representative examples, illustrating the effectiveness of long chain-of-thought (CoT) in reasoning tasks such as math and coding tasks. In this paper, we introduce DRT-01, an attempt to bring the success of long CoT to neural machine translation (MT). Specifically, in view of the literature books that might involve similes and metaphors, translating these texts to a target language is very difficult in practice due to cultural differences. In such cases, literal translation often fails to convey the intended meaning effectively. Even for professional human translators, considerable thought must be given to preserving semantics throughout the translation process. To simulate LLMs' long thought ability in MT, we first mine sentences containing similes or metaphors from existing literature books, and then develop a multi-agent framework to translate these sentences via long thought. In the multi-agent framework, a translator is used to iteratively translate the source sentence under the suggestions provided by an advisor. To ensure the effectiveness of the long thoughts, an evaluator is also employed to judge whether the translation in the current round is better than the previous one or not. In this manner, we collect tens of thousands of long-thought MT data, which is used to train our DRT-01. The experimental results on literature translation demonstrate the effectiveness of the DRT-01. Using Qwen2.5-7B and Qwen2.5-14B as the backbones, the improvement brought by DRT-01 achieves 7.33~8.26 BLEU and 1.66~3.36 CometScore. Besides, DRT-01-7B can outperform QwQ-32B-Preview by 7.82 BLEU and 1.46 CometScore, showing its effectiveness.", "sections": [{"title": "1 Introduction", "content": "Recently, the emergence of the O1-like models shows great performance in reasoning tasks, e.g., math and coding tasks (OpenAI, 2024b; Qin et al., 2024; Huang et al., 2024; Zhang et al., 2024; Zhao et al., 2024). With the help of long thought, LLMs tend to explore, reflect and self-improve the reasoning processes to achieve more accurate answers.\nIn this paper, we introduce DRT-01 that aims to bring the success of long thought to neural machine translation (MT). There are two key points in achieving this goal:\ni) A suitable translation scenario to employ long thought in MT: Not all scenarios require long thought during translation. For example, in simple expressions, literal translation can meet most needs, and translation via long thought may be unnecessary.\nii) A method to synthesize MT data with long thought: Long thought SFT (supervised fine-tuning) data plays a vital role in simulating LLMs' long thought ability (Huang et al., 2024). Previous work typically pays much attention to how to synthesize long-thought SFT data in math and coding tasks (Qin et al., 2024; Huang et al., 2024; Zhao et al., 2024).\nFor i), inspired by Van den Broeck (1981), a possible scenario is translating sentences with similes or metaphors, where literal translation often fails to convey the intended semantics. Given that, we decide to mine such sentences from literature books. The mining process uses an advanced large language model (LLM) to first judge Q1: whether each literature sentence has any similes or metaphors. If has, the LLM will be asked to literally translate the sentence to a target language, and give a final judgment on Q2: whether literal translation is effective for native speakers of the target language to comprehend. If the answers of Q1 and Q2 are \u201cyes\u201d and \u201cno\u201d, respectively, the corresponding literature sentences will be reserved, and regarded as \"suitable to translate via long thought\". For ii), after collecting the literal sentences with similes or metaphors, the next question is how to synthesize long thought MT samples. Previous work typically utilizes Monte Carlo Tree Search (MCTS) (Qin et al., 2024; Zhao et al., 2024; Zhang et al., 2024) or data distillation (Huang et al., 2024) (from existing O1-like models) to collect long thought SFT samples. Nevertheless, MCTS is typically used in math and coding tasks where multiple reasoning behaviors should be considered, and the method emphasizes complex reasoning that might not be efficient for machine translation. Besides, utilizing existing O1-like models for data distillation might (1) constrain the potential quality of the long-thought data; and (2) have a data gap in MT since current O1-like models are typically optimized toward math and coding tasks.\nTherefore, we propose a multi-agent framework to synthesize MT data with long thought. In detail, there are three agents in the framework, i.e., a translator, an advisor and an evaluator. The synthesis process is iterative, consisting of the following three steps during each iteration: (1) the translator generates a new translation conditioned on the previous step's translation and the corresponding refinement suggestions from the advisor; (2) the advisor evaluates the current translation and offers detailed feedback; (3) the evaluator assesses the current translation and gives an evaluation score using predefined scoring criteria. Once the translation score provided by the evaluator reaches a pre-defined threshold or the number of iterations reaches a maximum value, the iteration will stop. After that, the translation and suggestions in every step could form the long-thought MT samples. To improve the readability and fluency of the long-thought data, we employ GPT-40 (OpenAI, 2024a) to reformulate the long-thought content.\nBased on the collected long-thought MT samples, we train (SFT) our DRT-01-7B and DRT-01-14B using the backbones of Qwen2.5-7B-Instruct and Qwen2.5-14B-Instruct (Qwen Team, 2024), respectively. Experimental results on literature translation demonstrate the effectiveness of the DRT-01. For example, DRT-01-7B outperforms Qwen2.5-7B-Instruct by 8.26 BLEU, 1.31 CometKiwi and 3.36 CometScore. It can also outperform QwQ-32B-Preview by 7.82 BLEU and 1.46 CometScore.\nOur main contributions are concluded as follows:\n\u2022 We propose DRT-01 aiming at building LLMs with long-thought machine translation ability. To achieve this, we mine literature sentences with similes or metaphors, and collect MT samples with long-thought processes.\n\u2022 To synthesize the long-thought MT samples, we propose a multi-agent framework that involves a translator, an advisor and an evaluator. These three agents collaborate in an iterative manner to produce long thoughts during MT. Lastly, GPT-40 is used to further improve the quality of the synthetic long-thought MT samples.\n\u2022 Experimental results on literature translation verify the effectiveness of our DRT-01. With the help of long thought, LLMs can learn to think during the machine translation."}, {"title": "2 DRT-01 Data", "content": "We focus on English-to-Chinese translation, and we introduce how to collect DRT-01 training data via three steps in this section: (1) collecting English sentences that tend to require long thoughts during translation (\u00a7 2.1); (2) synthesizing the long-thought translation process for the collected sentences by a designed multi-agent framework (\u00a7 2.2); (3) improving the readability and fluency of the long-thought content to form the final long-thought MT samples (\u00a7 2.3). In the end, we provide data statistics of the collected data to give a deeper understanding (\u00a7 2.4)."}, {"title": "2.1 Literature Book Mining", "content": "Following Kryscinski et al. (2022), we leverage the literature books from the Project Gutenberg public-domain book repository\u00b9, where the books are typically more than fifty years old and their copyrights have expired. About 400 English books are used to mine sentences with similes or metaphors.\nFirst, we extract all sentences from these books, and filter out too short or too long sentences, i.e., less than 10 words or more than 100 words, resulting in 577.6K literature sentences. Second, for each sentence, we use Qwen2.5-72B-Instruct (Qwen Team, 2024) to judge whether the sentence involves similes or metaphors, and discard the sentences that do not contain any ones. Third, for the remaining sentences, we let Qwen2.5-72B-Instruct literally translate them to Chinese, and then judge whether the translation satisfies native Chinese people. If the answer is negative, the corresponding sentence will be reserved, and regarded as \"suitable to translate via long thought\". In this manner, we finally collect 63K (out of 577.6K) literature sentences involving similes or metaphors whose literal"}, {"title": "2.2 Multi-Agent Framework", "content": "For each pre-collected sentence (denoted as s), we design a multi-agent framework to translate it from English to Chinese via long thought. As shown in Figure 1, our multi-agent framework includes three agents: a translator, an advisor, and an evaluator. The synthetic process is illustrated as follows:\n(1) Word-level Translation. The translator first identifies the keywords that lie in the sentence, and then provides their translations under the consideration of the context. The keywords are denoted as \\(W^{src} = \\{w_1^{src}, w_2^{src}, ..., w_k^{src}\\}\\), where \\(w_i^{src}\\) indicates the i-th keyword in s, and k is the number of keywords. The translation of keywords is denoted as \\(W^{tgt} = \\{w_1^{tgt}, w_2^{tgt}, ..., w_k^{tgt}\\}\\). This step enables the model to identify potential challenges in translating the entire sentence by breaking it down into sub-problems (i.e., word-level translation).\n(2) Preliminary Translation. The translator then provides a preliminary sentence translation (t\u2070) conditioned on both the source sentence (s) and its keyword bilingual pairs ((Wsrc, Wtgt)).\n(3) Translation Refine Loop. In the refine loop, three agents work together to refine the translation iteratively. In each iteration step k (start from k = 1), the advisor first evaluates the translation in the previous step, i.e., tk\u22121, and provides detailed feedback fk-1 for polishing it. Then, the evaluator gives an overall score of tk-1 conditioned on both pre-defined scoring criteria and fk-1, the score is denoted as sk-1. In the last of the iteration step, the translator takes its previous translation tk-1, the corresponding feedback fk-1 and overall score sk-1 into account to provide a new translation tk. The translation refine loop will stop when the overall score reaches a pre-defined threshold or the number of iteration steps meets the maximum."}, {"title": "2.3 Long Thought Reformulation", "content": "After the multi-agent collaboration, we obtain a long thought process:\n\\(P(s) : s \\Rightarrow (W^{src}, W^{tgt}) \\Rightarrow (t^0, f^0, s^0)\\ \\Rightarrow (t^1, f^1, s^1) \\Rightarrow ... \\Rightarrow (t^m, f^m, s^m)\\)\nwhere P(s) denotes the multi-agent thought process for s, and m is the number of iteration steps. To emphasize the valid thought process, translations without score change will be removed. That is, if s\u2071 is equal to si\u22121 (i = 1,2, ..., m), we will discard (t', fi, s\u2071) in P(s), resulting in:\n\\(P'(s) : s \\Rightarrow (W^{src}, W^{tgt}) \\Rightarrow (t^0, f^0, s^0)\\ \\Rightarrow (t^{r_1}, f^{r_1}, s^{r_1}) \\Rightarrow ... \\Rightarrow (t^{r_n}, f^{r_n}, s^{r_n})\\)\nwhere 1 < r\u2081 < r\u2082 < ... < rn \u2264 m, and n is the number of remaining steps. If n < 3, we will discard the whole sample, i.e., P(s) / P' (s).\nFor the remaining samples, we follow Qin et al. (2024), and leverage GPT-40 (OpenAI, 2024a) to modify and polish P'(s) into a self-reflection description. Finally, we obtain 22,264 machine translation samples with long thought."}, {"title": "2.4 Data Statistics", "content": "We split the collected 22,264 samples into training, validation and testing sets with 19,264, 1,000 and 2,000 samples, respectively. Table 1 shows the data statistics of DRT-01 data and previous 01-like data. For Marco-O1 CoT data (Zhao et al., 2024), since it is not fully released, we use its demo data to calculate the data statistics. As we can see, the average number of tokens in our synthesized thought reaches 500+ tokens, which is similar to previous math-oriented O1-like CoT data."}, {"title": "3 Experiments", "content": "Metrics. Following previous work, we adopt BLEU (Papineni et al., 2002), reference-free CometKiwi and reference-based CometScore (Rei et al., 2022) to evaluate the model translations. BLEU evaluates n-grams overlap between the generated translations and corresponding references, while CometScore evaluates the semantic similarity of translations against references. CometKiwi uses a language model to judge whether a translation conveys the semantics of the source sentence.\nTo calculate CometKiwi and CometScore, we leverage the official codes\u00b2 and the official models\u00b3. To calculate the BLEU score, we use the sacrebleu toolkit to calulate the corpus-level BLEU.\nBackbones. We adopt Qwen2.5-7B-Instruct and Qwen2.5-14B-Instruct (Yang et al., 2024) as the backbones of DRT-01.\nImplementation Details. Llama-Factory (Zheng et al., 2024) is used to instruct-tune LLMs. Following Wang et al. (2024), all LLMs are tuned on 8\u00d7NVIDIA A100 GPUs (40G) with le-5 learning rate and 32 (8\u00d74) batch size. We use the Deep-Speed optimization (Rasley et al., 2020), and set ZeRO-3 optimization. Following Qin et al. (2024), we set the number of training epochs to 3, and the training process costs 70 GPU hours and 124 GPU"}, {"title": "3.2 Main Results", "content": "Table 2 shows the results on literature translation. We compare our DRT-01-7B and DRT-01-14B with previous Qwen2.5-7B-Instruct, Qwen2.5-14B-Instruct, QwQ-32B-preview and Marco-01-7B. After instruction tuning on our collected data, DRT-01-7B outperforms Qwen2.5-7B-Instruct by 8.26 BLEU, 1.31 CometKiwi and 3.36 CometScore. DRT-01-14B outperforms Qwen2.5-14B-Instruct by 7.33 BLEU, 0.15 CometKiwi and 1.66 CometScore. Besides, DRT-01-14B achieves the best results in terms of all metrics, showing the effectiveness of long thought in machine translation.\nFigure 3 shows an example of DRT-01-14B. As we can see, the model learns the thought process in our data collection. DRT-01-14B first performs the word-level translation, and then attempts the preliminary translation. Next, it iteratively refines its translation until it thinks the translation is good enough."}, {"title": "4 Related Work", "content": "Recently, Ol-like models have shown great performance in reasoning tasks, especially math and coding tasks. After the emergency of OpenAI 01 model (OpenAI, 2024b), many efforts are given in reproducing OpenAI O1. For example, Qin et al. (2024) propose journey learning, a training paradigm, to encourage LLMs to learn not just shortcuts, but the complete exploration process. With only 327 training samples, the journey learning enhances LLMs' long-thought ability. Huang et al. (2024) explore the data distillation from existing O1-like models, and show the effectiveness of data distillation. Zhang et al. (2024) leverage Monte Carlo Tree Search (MCTS) to synthesize reasoning-enhanced code data, and train O1-Coder. Marco-01 (Zhao et al., 2024) is proposed to deal with open-ended text generation. Marco-o1 shows the effectiveness of a MCTS-enhanced inference method, and achieves great performance on math problems. Besides, Marco-o1 studies the machine translation cases with long thought, and shows the potentiality of O1-like models in dealing with MT."}, {"title": "5 Conclusion", "content": "In this paper, we introduce DRT-01, an attempt to bring the success of long-thought reasoning to neural machine translation (MT). Specifically, we synthesize the machine translation long-thought samples by a designed multi-agent framework and GPT-40 reformulation. To collect the source sentences that are suitable for translation via long thought, we mine sentences with similes or metaphors from existing literature books. To synthesize the long thought machine translation process for these sentences, a translator, an advisor and an evaluator collaborate to translate the source sentence iteratively. GPT-40 is further employed to enhance the readability and fluency of the thought process collected via the multi-agent framework. Based on the synthesized data, we train DRT-01-7B and DRT-01-14B models (using Qwen2.5-7B-Instruct and Qwen2.5-14B-Instruct as backbones), and show their effectiveness in literature translation."}]}