{"title": "Transferable Adversarial Facial Images for Privacy Protection", "authors": ["Minghui Li", "Jiangxiong Wang", "Hao Zhang", "Ziqi Zhou", "Shengshan Hu", "Xiaobing Pei"], "abstract": "The success of deep face recognition (FR) systems has raised serious privacy concerns due to their ability to enable unauthorized tracking of users in the digital world. Previous studies proposed introducing imperceptible adversarial noises into face images to deceive those face recognition models, thus achieving the goal of enhancing facial privacy protection. Nevertheless, they heavily rely on user-chosen references to guide the generation of adversarial noises, and cannot simultaneously construct natural and highly transferable adversarial face images in black-box scenarios. In light of this, we present a novel face privacy protection scheme with improved transferability while maintain high visual quality. We propose shaping the entire face space directly instead of exploiting one kind of facial characteristic like makeup information to integrate adversarial noises. To achieve this goal, we first exploit global adversarial latent search to traverse the latent space of the generative model, thereby creating natural adversarial face images with high transferability. We then introduce a key landmark regularization module to preserve the visual identity information. Finally, we investigate the impacts of various kinds of latent spaces and find that F latent space benefits the trade-off between visual naturalness and adversarial transferability. Extensive experiments over two datasets demonstrate that our approach significantly enhances attack transferability while maintaining high visual quality, outperforming state-of-the-art methods by an average 25% improvement in deep FR models and 10% improvement on commercial FR APIs, including Face++, Aliyun, and Tencent.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep face recognition (FR) systems [30, 40] have triumphed in both verification and identification scenarios and been widely applied across various domains, such as security [44], biometrics [28], and criminal-investigation [31]. Despite its promising prospect, FR systems pose a profound risk to individual privacy due to their capacity for large-scale surveillance [2, 46], e.g., tracking user relationship and activities by analyzing face images from social media platforms [13, 36]. Given the opacity of these FR systems, there is an urgent need for an effective black-box approach to protect facial privacy.\nRecent works have attempted to protect facial privacy using noise-based adversarial examples (AEs) [32, 38, 49] by adding carefully crafted adversarial perturbations to the source face images to deceive malicious FR systems. However, the adversarial perturbations are typically constrained to the lp norm within the pixel space, which makes adversarial face images have conspicuous discernible artifacts with poor visual quality [49].\nAnother solution is exploiting unrestricted adversarial examples [15, 35, 50, 61] to mislead malicious FR systems. Unlike noise-based methods, they are not confined by perturbation budgets, thereby maintaining superior image quality [4, 37, 48]. Recently proposed makeup-based approaches (e.g., AMT-GAN [15], CLIP2Protect [35]) have achieved state-of-the-art performance, as they can effectively embed adversarial noises in the makeup style by using generative adversarial networks (GANs) [10, 16] or GAN inversion. However, they rely on extra user-chosen guidance, such as reference images in AMT-GAN and textual prompts in CLIP2Protect, to make adversarial noises harmoniously integrated with face characteristics. Besides, these methods excessively focus on the modification of local attributes, leading to minor effects on facial identity characteristics, resulting in limited transferability."}, {"title": "2 RELATED WORK AND BACKGROUND", "content": "Recently, many works have been proposed to protect facial privacy against unauthorized FR systems [29, 42, 43]. The typical strategy involves the utilization of noise-based adversarial examples [29, 49, 57\u201360], where carefully crafted perturbations are added to face images to deceive malicious FR models. Oh et al. [29] proposed crafting protected face images from a game theory perspective in the white-box setting, which is impractical in real-world scenarios. Thus TIP-IM [49] introduced the idea of generating adversarial identity masks in the black-box setting. However, the perturbations are usually perceptible to humans and affect the user experience.\nAnother strategy is to leverage unrestricted adversarial examples [4, 18, 37, 48, 52, 56], which are not constrained by the perturbation norm in the pixel space and enjoy a better image quality [4, 37, 48]. Among these, makeup-based unrestricted adversarial examples are presented against unknown FR systems by concealing adversarial perturbations within natural makeup characteristics. Zhu et al. [61] made the first effort to utilize makeup to generate protected face images in the white-box setting. Afterwards, Adv-Makeup [50] synthesized imperceptible eye shadow over the orbital region on the face, which has limited transferability. AMT-GAN [15] generated adversarial face images with makeup transferred from reference images in a black-box manner, which has a higher attack success rate but suffers from obvious artifacts due to the conflict between the makeup transfer module and the adversarial noises. Recently, CLIP2Protect [35] traversed over the local latent space that controls the makeup style of a pre-trained generative model by using text prompts. However, all the above methods rely on guidance (e.g., text or image references) to make the adversarial noises distributed in a natural way. This is a disappointing constraint in real-world applications as the users usually have no desired target references. More importantly, the visual quality of output face images is largely affected by the references, as detailedly discussed in Section 3.2. DiffProtect [26] and Adv-Diffusion [24] employ the diffusion models [53] as the generative models and iteratively refine the latent representations of facial images, although yielding higher-quality adversarial facial images, exhibit limited attack transferability, rendering it impractical for real-world applications."}, {"title": "2.1 Facial Privacy Protection", "content": "Recently, many works have been proposed to protect facial privacy against unauthorized FR systems [29, 42, 43]. The typical strategy involves the utilization of noise-based adversarial examples [29, 49, 57-60], where carefully crafted perturbations are added to face images to deceive malicious FR models. Oh et al. [29] proposed crafting protected face images from a game theory perspective in the white-box setting, which is impractical in real-world scenarios. Thus TIP-IM [49] introduced the idea of generating adversarial identity masks in the black-box setting. However, the perturbations are usually perceptible to humans and affect the user experience.\nAnother strategy is to leverage unrestricted adversarial examples [4, 18, 37, 48, 52, 56], which are not constrained by the perturbation norm in the pixel space and enjoy a better image quality [4, 37, 48]. Among these, makeup-based unrestricted adversarial examples are presented against unknown FR systems by concealing adversarial perturbations within natural makeup characteristics. Zhu et al. [61] made the first effort to utilize makeup to generate protected face images in the white-box setting. Afterwards, Adv-Makeup [50] synthesized imperceptible eye shadow over the orbital region on the face, which has limited transferability. AMT-GAN [15] generated adversarial face images with makeup transferred from"}, {"title": "2.2 GAN Inversion", "content": "GAN inversion [1, 19, 39, 41, 47] intends to invert a given face image back to a low-dimensional manifold which is expressed as a latent space of a pre-trained GAN model, such that the image can be faithfully reconstructed. As the StyleGAN [22] models trained on a high-resolution face image dataset [21] exhibit exceptional image synthesis capabilities, various GAN inversion methods have been developed using different latent spaces based on StyleGANs. Generally, there are three typical latent spaces (i.e., W [1], W+ [39], and F [19]). They are the trade-off design between the reconstruction quality and editability [25]. W uses a mapping network to disentangle different features with a high degree of editability. However, it has limited expressiveness which restricts the range of images that can be faithfully reconstructed. Meanwhile, W+ feeds different intermediate latent vectors into each layer of the generator via AdaIN [17], alleviating the image distortion at the expense of editability. The latent space F consists of specific features which enjoy the highest reconstruction quality but suffer from the worst editability."}, {"title": "3 METHODOLOGY", "content": "In general, FR systems can operate with two modes: face verification and face identification. For verification, the FR systems identify whether two face images correspond to the same identity. For identification, the FR systems query the face database to identify whose representation is closest to the input image. In this paper, we consider both scenarios to sufficiently demonstrate the effectiveness of our approach. As seen in Fig. 1, if the user's source face image xs is directly posted to social media platforms, malicious FR systems could potentially trace the relationships and activities of the user by analysing the publicly available images.\nWith the help of facial privacy protection algorithms, users can obtain the protected face image xp that appears indistinguishable to human (i.e., naturalness) but can deceive the FR systems (i.e., transferablility). For the malicious FR systems, xp has the same identity as the target impersonated image xt in both face verification and face identification. Generally, the problem can be formulated as:\n $$ \\min_{X_p} L_{adv} = D(f_n(x_p), f_n(x_t)) \\\\ s.t. H(x_p, x_s) \\leq \\epsilon $$ \nwhere $D(\\cdot)$ represents a distance metric and $f_n(\\cdot)$ stands for a FR model that outputs a feature vector by extracting the feature representation of a face image. Contrary to noise-based adversarial examples where $H(x_p, x_s) = ||x_s \u2013 x_p||_p$ and $||\\cdot||_p$ is the $L_p$ norm, $H(x_p, x_s) \\leq \\epsilon$ quantifies the extent of unnaturalness of $x_p$ compared to $x_s$."}, {"title": "3.1 Problem definition", "content": "In general, FR systems can operate with two modes: face verification and face identification. For verification, the FR systems identify whether two face images correspond to the same identity. For identification, the FR systems query the face database to identify whose representation is closest to the input image. In this paper, we consider both scenarios to sufficiently demonstrate the effectiveness of our approach. As seen in Fig. 1, if the user's source face image xs is directly posted to social media platforms, malicious FR systems could potentially trace the relationships and activities of the user by analysing the publicly available images.\nWith the help of facial privacy protection algorithms, users can obtain the protected face image xp that appears indistinguishable to human (i.e., naturalness) but can deceive the FR systems (i.e., transferablility). For the malicious FR systems, xp has the same identity as the target impersonated image xt in both face verification and face identification. Generally, the problem can be formulated as:\n $$ \\min_{X_p} L_{adv} = D(f_n(x_p), f_n(x_t)) \\\\ s.t. H(x_p, x_s) \\leq \\epsilon $$ \nwhere $D(\\cdot)$ represents a distance metric and $f_n(\\cdot)$ stands for a FR model that outputs a feature vector by extracting the feature representation of a face image. Contrary to noise-based adversarial examples where $H(x_p, x_s) = ||x_s \u2013 x_p||_p$ and $||\\cdot||_p$ is the $L_p$ norm, $H(x_p, x_s) \\leq \\epsilon$ quantifies the extent of unnaturalness of $x_p$ compared to $x_s$."}, {"title": "3.2 Challenges and limitations", "content": "Existing works [15, 35] have experimentally demonstrated that the guidance of the reference image or text prompts plays an important role in maintaining the visual quality of adversarial face images during the process of generation. Without using guidance, the adversarial perturbations will be randomly generated and spread all over the face without any constraints, leading to obvious artifacts in the result images. If we simply fix the adversarial area, the noises may make the modified area extremely strange and distorted. Therefore, existing works have to use extra information to guide the distribution of adversarial noises such that they can harmonize with facial characteristics.\nMoreover, even with makeup information guidance, due to the incomplete disentanglement of facial attributes, the adversarial modifications concentrating on one kind of local semantics struggle to integrate seamlessly with other facial semantic information. As shown in Fig. 7, the visual quality can still be damaged in guidance-based schemes, especially when the reference is not appropriately chosen. Note that although CLIP2Protect [35] maintains the face quality well in some cases, the background behind the face has been significantly damaged. It should be emphasized that the image's visual quality should contain not only the facial information but also the background.\nBesides, these methods excessively focus on local attributes, leading to minor effects on facial identity characteristics. This limitation results in limited transferability and renders them less practical for real-world applications."}, {"title": "3.3 Our key insights", "content": "To address the aforementioned issues, we propose directly manipulating the entire facial space to harmoniously integrate adversarial noises, rather than solely using one kind of facial characteristic to guide the distribution of noises. In this way, all of the facial information will be adaptively assembled, including makeup characteristics, expressions, and face shape, as well as adversarial noises. In addition, by comprehensively optimizing every feature rather than excessively optimizing a particular attribute, the transferability of generated adversarial facial images is significantly improved. To this end, we first have the following observations."}, {"title": "3.4 Transferable Adversarial Facial Images", "content": "Drawing inspiration from the above analysis, we propose a brand-new generative framework (GIFT) to generate guidance-independent adversarial facial images with transferability for facial privacy protection. The pipeline of our approach is depicted in Fig. 5. We first employ GAN inversion to initialize the latent code that can faithfully reconstruct the source face image in F rather than W+ [35] latent space. Then, we conduct the global adversarial latent search, which takes the protected image and the target image as inputs to adversarially optimize the latent code of the protected face image. Furthermore, we introduce a key landmark regularization to preserve the visual identity of the protected image.\nLatent Code Initialization: The latent code is initialized based on GAN inversion. Given a source image xs and pre-trained encoder Eb from [19], we first calculate the initial latent code $w_{ini} = E_b(x_s)$ in the F latent space. Then we optimize the latent code wini by a reconstruction loss, which is defined as:\n $$ L_{rec}(w_{ini}) = L_{mse}(w_{ini}) + \\alpha L_{per}(w_{ini}) \\\\ = ||x_s - G(w_{ini})||_2 + \\alpha ||\\mathcal{F}(x_s) - \\mathcal{F}(G(w_{ini}))||_2 $$ \nwhere G refers to the pre-trained generative model, $L_{mse}$ and $L_{per}$ denote mean-squared-error (MSE) and perceptual loss, respectively. $\\alpha$ is the weight assigned to $L_{per}$. $\\mathcal{F}(\\cdot)$ represents an LPIPS [55] network used to compute the perceptual distance. As a result, we obtain the initialized latent code wf, which can faithfully reconstruct xs by G.\nGlobal Adversarial Latent Search: We utilize an ensemble training strategy with input diversity to search for a good adversarial optimization direction similar to [15]. We select N pre-trained FR models {fn ()}=1 which exhibit high accuracy in the public facial datasets, serving as white-box models to imitate the decision boundaries of potential target models in the black-box setting during performing global optimization. The adversarial loss is:\n $$ L_{adv} = \\frac{1}{N} \\sum_{n=1}^N D(f_n(T(G(w_f))), f_n(x_t)) $$ \nwhere D (x1, x2) = 1 \u2212 cos (x1, x2) is the cosine distance, fn () represents the n-th local pre-trained white-box model which maps an input face image to a feature representation. T (\u00b7) represents the transformation function including image resizing and Gaussian noising, and p is a predefined probability that determines whether the transformation will be applied to G\nKey Landmark Regularization: We first leverage a pre-trained semantic encoder from [51] to obtain the face semantic segmentation maps sems and semp for the source and protected image. We then take advantage of the semantic segmentation maps to regularize the optimization process, ensuring that the protected image preserves visual identity for humans. The regularization loss is defined as:\n $$ L_{sem} = L_{CE}(\\mathcal{M}(G(w_f)), \\mathcal{M}(x_s)) $$ \nwhere $L_{CE}$ () is the cross-entropy loss, M (\u00b7) represents the pre-trained semantic encoder. The total loss is:\n $$ L_{total} = L_{adv} + \\lambda_{adv} L_{adv} + \\lambda_{sem} L_{sem} $$ \nwhere $\\lambda_{adv}$ and $\\lambda_{sem}$ represent the hyper-parameters. The whole optimization process is outlined in Algorithm. 1."}, {"title": "4 EXPERIMENTS", "content": "Implementation Details: We employ StyleGAN2 pre-trained on FFHQ [21] as our generative model and use the Adam optimizer in all experiments. For Latent Code Initialization, we iteratively optimize the latent code for 1200 steps with a learning rate of 0.01. During the adversarial optimization process, we traverse over the global latent space for 50 iterations with a learning rate of 0.002 to generate protected face images. We set the hyper-parameters \u03b1, \u03bbadv and \u03bbsem to 10, 1, 0.01, respectively. More details can be seen in our supplementary.\nDatasets: We perform experiments for both face verification and identification tasks. Face Verification: We utilize CelebA-HQ [20] and LADN [11] as our test set. Specifically, for CelebA-HQ, we select a subset which contains 1000 images with different identities. For LADN, we divide the 332 images into 4 groups, with each group of images impersonating a target identity provided by [15]. Face Identification: we randomly select 500 images of 500 different identities in CelebA-HQ as the probe set, and the corresponding 500 images of the same identities along with the target image xp to form the gallery set.\nTarget Models: Following [15], we choose various deep FR models and commercial FR APIs to evaluate the transferability of the adversarial facial images generated by GIFT in the black-box settings. Specifically, the deep FR models include MobileFace [5], IR152 [7], IRSE50 [14], and FaceNet [33] and commercial FR APIs include Face++, Aliyun, and Tencent.\nCompetitors: We compare GIFT with recent noise-based and makeup-based facial privacy protection approaches. Noised-based methods include PGD [27], MI-FGSM [8], TI-DIM [9], and TIP-IM [49]. Makeup-based approaches include Adv-Makeup [50], AMT-GAN [15] and CLIP2Protect [35]. Among these methods, TIP-IM and CLIP2Protect are regarded as the state-of-the-art (SOTA) approaches against black-box FR systems in noise-based and unrestricted settings, respectively. Notably, TIP-IM employs a multi-target objective within its optimization to discover the best image from multiple targets. To maintain fairness in the comparison, we employ a single-target variant.\nEvaluation Metrics: We employ different evaluation strategies to calculate the protection success rate (PSR) for the verification and identification scenarios. For verification, we identify that two face images belong to the same identity if D (xp, xs) \u2265 \u03c4 and then calculate the proportion of successfully protected images in relation to all images. For identification, we report the Rank-N targeted identity success rate, which means that at least one of the top N images belongs to the target identity after ranking the distance for all images in the gallery to the given probe image. For commercial FR APIs, we directly record the confidence scores returned by FR servers. We also leverage FID [12], PSNR (dB) and SSIM [45] to evaluate the image quality. The FID quantifies the dissimilarity between two data distributions [54], commonly employed to assess the extent to which a generated dataset resembles one obtained from the real world. PSNR and SSIM are commonly utilized techniques for assessing the difference between two images."}, {"title": "4.1 Experiments Setup", "content": "Implementation Details: We employ StyleGAN2 pre-trained on FFHQ [21] as our generative model and use the Adam optimizer in all experiments. For Latent Code Initialization, we iteratively optimize the latent code for 1200 steps with a learning rate of 0.01. During the adversarial optimization process, we traverse over the global latent space for 50 iterations with a learning rate of 0.002 to generate protected face images. We set the hyper-parameters a, \u03bbadv and \u03bbsem to 10, 1, 0.01, respectively. More details can be seen in our supplementary.\nDatasets: We perform experiments for both face verification and identification tasks. Face Verification: We utilize CelebA-HQ [20] and LADN [11] as our test set. Specifically, for CelebA-HQ, we select a subset which contains 1000 images with different identities. For LADN, we divide the 332 images into 4 groups, with each group of images impersonating a target identity provided by [15]. Face Identification: we randomly select 500 images of 500 different identities in CelebA-HQ as the probe set, and the corresponding 500 images of the same identities along with the target image xp to form the gallery set.\nTarget Models: Following [15], we choose various deep FR models and commercial FR APIs to evaluate the transferability of the adversarial facial images generated by GIFT in the black-box settings. Specifically, the deep FR models include MobileFace [5], IR152 [7], IRSE50 [14], and FaceNet [33] and commercial FR APIs include Face++, Aliyun, and Tencent.\nCompetitors: We compare GIFT with recent noise-based and makeup-based facial privacy protection approaches. Noised-based"}, {"title": "4.2 Comparison Study", "content": "We present experimental results of GIFT in the black-box settings on four different pre-trained FR models on two public datasets under face verification and identification tasks. For face verification, we set the system threshold value at 0.01 false match rate for each FR model i.e., IRSE50 (0.241), IR152 (0.167), FaceNet (0.409), and MobileFace (0.302). The quantitative results in terms of PSR under the face verification scenario are displayed in Tab. 1. The results show that GIFT has the capability to achieve an average absolute gain of about 25% and 39% over SOTA unrestricted and noise-based facial privacy protection methods, respectively. We also provide PSR under the face identification scenario in Tab. 2. Consistently, GIFT outperforms recent methods in both Rank-1 and Rank-5 settings. Given that AMT-GAN and Adv-Makeup were initially trained for impersonating the target identity in the verification task, they have not been incorporated into Tab. 2.\nEvaluation on Image Quality. Tab. 3 shows the quantitative evaluations on image quality. Notably, we report the results that are averaged over 10 text prompts for CLIP2Protect. Although Adv-Makeup [50] achieves the lowest FID score, its transferability is limited due to perturbing only the eye region. Beyond Adv-Makeup,"}, {"title": "4.3 Visualization and Analysis", "content": "In this section, we employ Grad-CAM [34] to explore why the adversarial face images generated by GIFT exhibit better transferability. We present gradient visualizations of adversarial face images generated by AMT-GAN, CLIP2Protect, and GIFT in the black-box setting compared to the target images in Fig. 9. We can observe that the gradient responses of AMT-GAN and CLIP2Protect either concentrate on specific facial regions or focus on the background of the face image. In contrast, the gradient responses of GIFT are concentrated on the facial region, without being limited to local features, but rather covering the entire face. Therefore, the adversarial face images generated by GIFT exhibit superior performance, both on deep FR models and commercial FR APIs."}, {"title": "4.4 Ablation Study", "content": "The Effect of KLR: We investigate the effect of key landmark regularization on GIFT. As shown in Fig. 11, with the absence of KLR, the visual identity of the protected face image will change. The Effect of GALS: We analyze the effect of GALS on GIFT. As depicted in Fig. 10, compared to LALS, which only adversarially modifies the makeup style, GALS exhibits significantly better transferability.\nThe Effect of Latent Space: We study the effect of different latent spaces on GIFT. Both Fig. 12 and Tab. 4 indicate that, although W latent space results in better adversarial transferability, the generated adversarial face images exhibit poor image quality. In contrast, F latent space maintains high transferability while achieving the best image quality."}, {"title": "5 CONCLUSION", "content": "In this paper, focusing on protecting facial privacy against malicious FR systems, we propose GIFT, a guidance-independent generative framework to construct highly transferable adversarial facial images while maintain good visual effect. Specifically, we leverage Global Adversarial Latent Search to construct natural and highly transferable adversarial face images without extra guidance information. Moreover, we introduce a key landmark regularization method to preserve the visual identity. We further reveal the limitations of W+ latent space and the intriguing properties of the other two prevalent latent spaces W and F under the facial privacy protection scenario. Extensive experiments on both face verification and identification tasks demonstrate the superiority of GIFT against various deep FR models and commercial FR APIs."}]}