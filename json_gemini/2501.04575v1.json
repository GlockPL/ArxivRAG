{"title": "InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection", "authors": ["Yuhang Liu", "Pengxiang Li", "Zishu Wei", "Congkai Xie", "Xueyu Hu", "Xinchen Xu", "Shengyu Zhang", "Xiaotian Han", "Hongxia Yang", "Fei Wu"], "abstract": "Graphical User Interface (GUI) Agents, powered by multimodal large language models (MLLMs), have shown great potential for task automation on computing devices such as computers and mobile phones. However, existing agents face challenges in multi-step reasoning and reliance on textual annotations, limiting their effectiveness. We introduce InfiGUIAgent, an MLLM-based GUI Agent trained with a two-stage supervised fine-tuning pipeline. Stage 1 enhances fundamental skills such as GUI understanding and grounding, while Stage 2 integrates hierarchical reasoning and expectation-reflection reasoning skills using synthesized data to enable native reasoning abilities of the agents. InfiGUIAgent achieves competitive performance on several GUI benchmarks, highlighting the impact of native reasoning skills in enhancing GUI interaction for automation tasks.", "sections": [{"title": "1 Introduction", "content": "Graphical User Interface (GUI) Agents have emerged as powerful tools for automating tasks on computing devices, including mobile phones and computers. These agents can understand and interact with GUIs to execute complex operations, significantly enhancing user productivity and expanding the scope of automated task completion (Hu et al., 2024b; Hong et al., 2024; Zhang and Zhang, 2023; Qi et al., 2024; Xie et al., 2024; Vu et al., 2024; Yu et al., 2024; Wen et al., 2023).\nRecent developments in multimodal large language models (MLLMs) (Bai et al., 2023b; Li et al., 2024c; Team et al., 2024; Dai et al., 2022) have significantly advanced the potential of GUI Agents. MLLMs possess powerful visual understanding capabilities and can reason based on visual information, making them a promising foundation for building sophisticated GUI Agents. These models can interpret complex interface elements and adapt to a wide range of tasks, leading to more efficient and robust automation (Hong et al., 2024; Jiang et al., 2023; You et al., 2025; Nong et al., 2024; Vu et al., 2024).\nHowever, current MLLM-based GUI Agents face several critical challenges. A key limitation lies in their reasoning capabilities (Zhang and Zhang, 2023; Qi et al., 2024; Yu et al., 2024). While many existing GUI Agents can perform basic single-step reasoning, they struggle to effectively leverage information from previous steps. This lack of reflection on past experiences can lead to repetitive errors during task execution.\nAnother significant challenge lies in the reliance on the additional information of the GUIs. Many prior GUI Agent implementations rely on accessibility trees or Set-of-Marks (Yang et al., 2023a), to represent or augment the GUI's visual information. However, GUIs are inherently visual, and representing them primarily through text can lead to information loss or redundancy. Augmenting visual input with textual descriptions can also increase computational overhead. Furthermore, the availability and consistency of these textual representations vary across platforms, hindering practical deployment.\nTo address these limitations, we propose InfiGUIAgent, which is a MLLM-based GUI Agent trained through a two-stage supervised fine-tuning (SFT) methods with robust fundamental capabilities and native reasoning abilities. In stage 1, we collect data covering multiple tasks, such as vision-language understanding, GUI-specific QA, and tool use to improve fundamental capabilities such as GUI understanding and instruction grounding of the agents. In stage 2, we recognized two essential reasoning skills for GUI Agents: (1) Hierarchical reasoning, and (2) Expectation-reflection reasoning, and integrate these skills into the SFT data synthesized by MLLMs based on existing trajectories. Our main contributions are threefold:"}, {"title": "2 Related Works", "content": "Large Language Models (LLMs) (Floridi and Chiriatti, 2020; Touvron et al., 2023; Bai et al., 2023a; Xiao et al., 2021) have significantly enhanced the capabilities of AI systems in tackling a wide range of tasks (Hu et al., 2024c; Li et al., 2024d), thanks to their exceptional ability to process complex semantic and contextual information. The remarkable power of LLMs has also inspired exploration into their potential for processing multimodal data, such as images. Typically, the architecture of Multimodal Large Language Models (MLLMs) consists of three main components: a pre-trained large language model, a trained modality encoder, and a modality interface that connects the LLM with the encoded modality features. Various vision encoders, such as ViT (Dosovitskiy et al., 2021), CLIP (Radford et al., 2021), and ConvNeXt (Liu et al., 2022), extract visual features, which are integrated using techniques like adapter networks (Liu et al., 2023), cross-attention layers (Alayrac et al., 2022), and visual expert modules (Wang et al., 2023). These methods have facilitated the development of high-performing MLLMs, such as Qwen-VL (Bai et al., 2023b), GPT-4 Vision (OpenAI, 2023), BLIP-2 (Li et al., 2023) and InfiMM (Liu et al., 2024), thus opening new avenues for LLMs in processing GUI tasks."}, {"title": "2.2 MLLM-based GUI Agents", "content": "Agents are AI systems that perceive their environments, make decisions, and take actions to complete specific tasks. LLMs reaching human-level intelligence have greatly enhanced the ability to build agents. For GUI tasks, LLMs that read HTML code to perceive GUIs are developed (Wen et al., 2023). However, various works have shown that learning to interact with the visual form of the GUIs can show superior performance (Hu et al., 2024b). Therefore, MLLM-based GUI Agents are developed. ILuvUI (Jiang et al., 2023) fine-tuned LLaVA to enhance general GUI understanding, while AppAgent (Zhang et al., 2023) explored app usage through autonomous interactions. CogAgent (Hong et al., 2024) integrated high-resolution vision encoders, and Ferret-UI-anyres (You et al., 2025) employed an any-resolution approach. Building upon these works, our study focuses on developing a more lightweight agent with a simplified architecture for GUI tasks, aiming to improve ease of deployment."}, {"title": "3 Method", "content": "In this section, we introduce our two-stage supervised fine-tuning strategy for building InfiGUIAgent, as shown in Figure 1. In stage 1, we focus on improving fundamental abilities such as understanding and grounding, particularly considering the complexity of GUIs. In stage 2, we move on to improve the native reasoning abilities of agents for handling complicated GUI tasks."}, {"title": "3.1 Stage 1: Training for Fundamental Abilities", "content": "Considering the complexity of GUIs, which involve diverse data formats such as HTML code, high-resolution interfaces cluttered with small icons and text, general MLLMs lack fundamental abilities in both understanding GUI and grounding the actions. To address this, we first collected a range of existing visual-language and GUI datasets for supervised fine-tuning in stage 1. We gathered data covering several GUI tasks from multiple sources to ensure a comprehensive capabilities improvement (see Table 1). The datasets can be categorized into five parts:\n\u2022 GUI Understanding. Datasets focusing on GUI element recognition, layout comprehension, and semantic interpretation, including Screen2Words (Wang et al., 2021) and Screen Annotation (Baechler et al., 2024).\n\u2022 Grounding. Datasets capture various user interaction sequences and operation patterns, including GUIEnv (Chen et al., 2024), RICO Semantic Annotation (Sunkara et al., 2022), SeeClick-Web (Cheng et al., 2024), RICO SCA (Li et al.,"}, {"title": "3.1.1 Data Preprocessing and Standardization", "content": "Given the diversity of our data sources, we implemented comprehensive preprocessing steps to standardize the data format across all datasets. We normalized the coordinate system by following (Wang et al., 2024), mapping all spatial coordinates to a relative scale of [0, 1000]. This standardization facilitates consistent representation of both point and box annotations in JSON format, with points expressed as {\"x\" : x, \"y\" : y} and bounding boxes as {\"x1\": x1, \"y1\" : y1, \"x2\" : x2, \"y2\" : y2}. In this coordinate system, the origin {\"x\" : 0, \"y\" : 0} is located at the screen's top-left corner, with the x-axis extending rightward and the y-axis downward. The bottom-right corner corresponds to coordinates {\"x\" : 1000, \"y\" : 1000}. To enhance data quality, we implemented two additional preprocessing steps:\n\u2022 Instruction Enhancement. For datasets with ambiguous instructions, we developed standardized instruction templates to establish clear correspondence between commands and their expected outcomes.\n\u2022 Response Refinement. For entries with complex or inconsistent response formats, we utilized Qwen2-VL-72B (Bai et al., 2023b) to reformulate responses while preserving their semantic content. Each reformulation underwent validation to ensure accuracy and consistency."}, {"title": "3.1.2 Reference-Augmented Annotation", "content": "To better leverage the spatial information available in our collected datasets and enhance the model's visual-language understanding of GUIs, we implemented a reference-augmented annotation format."}, {"title": "3.2 Stage 2: Training for Native Reasoning", "content": "Building upon the foundational capabilities such as understanding and grounding, GUI Agents must also master advanced reasoning skills to effectively handle complex tasks. We identify two crucial reasoning skills: (1) Hierarchical reasoning, which enables planning and task decomposition, helping agents structure complex tasks into manageable subtasks and execute them efficiently (Huang and Chang, 2023; Zhang et al., 2024b; Huang et al., 2024), and (2) Expectation-reflection reasoning, which fosters adaptive self-correction and reflection"}, {"title": "3.2.1 Hierarchical Reasoning", "content": "Effective execution of GUI tasks demands both overarching strategic planning and meticulous tactical execution. To achieve this, we synthesize trajectory data with a hierarchical reasoning with two distinct layers:\n\u2022 Strategic Layer. Strategic layer is responsible for high-level task decomposition and sub-goal planning. This layer analyzes the overall task objective and determines the sequence of subtasks needed for completion.\n\u2022 Tactical Layer. Tactical layer handles the selection and grounding of concrete actions. Based on the strategic layer's planning, agent select appropriate GUI operations and adjusts their parameters to match the target."}, {"title": "3.2.2 Expectation-Reflection Reasoning", "content": "To enhance action consistency and foster autonomous self-correction, we incorporate Expectation-reflection reasoning into the training datasets. This iterative process enhances the agent's ability to adapt and learn from its actions through a structured reflection cycle:\n\u2022 Reasoning. After reflection (except the first step), the agents conduct hierarchical reasoning.\n\u2022 Action. After the reasoning, the agent takes the action.\n\u2022 Expectation. Following each action, the agent generates expected outcomes which are used to be verified at the next step.\n\u2022 Reflection. The agent evaluates whether its actions achieved the expected results and generating a textual summary of the reflection."}, {"title": "3.2.3 Agent-Environment Interface", "content": "We formulate the GUI interaction as a process where an agent interacts with a mobile environment. Let $s_t \\in S$ denote the environment state at step $t$, where $S$ represents the state space. The agent can observe the state through a screenshot observation $o_t$ and performs actions $a_t \\in A$, where $A$ is the action space. The environment transitions from $S_t$ to $s_{t+1}$ following $s_{t+1} \\sim P(s_t, a_t)$, where $P$ represents the transition probability function.\nThe agent receives a task goal $g$ and maintains access to a history window of size $n$. At each step $t$, the agent's input consists of:\n\u2022 Goal $g$\n\u2022 Current observation $o_t$\n\u2022 Historical context $H_t = \\{(O_i, r_i, a_i)\\}_{i=-n}^{t-1}$, where $r_i$ represents the reasoning process\nBased on these inputs, the agent generates a reasoning process $r_t$ and predicts an action $a_t$. The"}, {"title": "3.2.4 Modular Action Space", "content": "Given the diverse action spaces across collected datasets, we categorized and standardized the actions by unifying their names and parameters, merging similar operations where appropriate. The resulting action space $A$ consists of independent, composable operations that can be flexibly combined based on task requirements, as shown in Table 3. This modular design allows for dynamic action space configuration while maintaining a consistent interface across different platforms and scenarios."}, {"title": "3.2.5 Reasoning Process Construction", "content": "To construct high-quality reasoning data to stimulate the model's native reasoning capabilities, we leverage more capable MLLMs (e.g. Qwen2-VL-72B) to generate structured reasoning processes based on existing interaction trajectories. The construction process involves several key components:\n\u2022 Screenshot Description. For each observation $O_t$ in the trajectory, we generate a detailed description $d_t$. This step addresses the limitation that some MLLM models do not support interleaved image-text input formats well. To establish clear correspondence between observations (screenshots) and steps, we generate detailed descriptions to replace the screenshots, which helps facilitate the subsequent reasoning process construction.\n\u2022 Reflection. Given the previous expectation $e_{t-1}$ and current observation $o_t$, we generate a reflection $f_t$ that evaluates the outcome of the previous action.\n\u2022 Strategic Layer. The strategic reasoning consists of two parts: First, a summary is generated based on the $n$-step history $H_t = \\{(o_i, r_i, a_i)\\}_{i=-n}^{t-1}$ and current observation $o_t$. Then, the planning component is generated with access to the actual action $a_t$ to ensure alignment with the trajectory.\n\u2022 Tactical Layer. This layer's reasoning is constructed using the generated reflection $f_t$ and strategic layer output. The actual action $a_t$ from the trajectory is incorporated to ensure the tactical reasoning leads to appropriate action selection.\n\u2022 Expectation. For each state-action pair $(s_t, a_t)$, we generate an expectation $e_t$ based on current observation $o_t$, reasoning process $r_t$, and action $a_t$. Notably, we deliberately avoid using the next state $s_{t+1}$ in this generation process. Although using $s_{t+1}$ could improve the agent's accuracy in modeling state transitions, while using $s_{t+1}$ could lead to perfect expectations, such an approach might impair the agent's ability to handle expectation mismatches during deployment.\nWhile we avoid using $s_{t+1}$ in expectation generation to maintain robustness, we also explore the possibility of improving state transition modeling through a parallel next-state prediction task. Using the trajectory data, we construct additional training examples where the agent learns to predict the next state description $d_{t+1}$ given the current observation $o_t$ and action $a_t$. This auxiliary task helps the agent learn state transition dynamics, while keeping the expectation generation process independent of future states."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setting", "content": ""}, {"title": "4.1.1 Implementation Details", "content": "In stage 1, we sample 1M samples in total as illustrated in Table 1. In stage 2, we synthesized"}, {"title": "4.1.2 Evaluation Benchmarks", "content": "ScreenSpot. ScreenSpot (Cheng et al., 2024) is an evaluation benchmark for GUI grounding, consisting of over 1,200 instructions from iOS, Android, macOS, Windows, and Web environments, with annotated element types.\nAndroidWorld. AndroidWorld (Rawles et al., 2024) is a fully functional Android environment that provides reward signals for 116 programmatic tasks across 20 real-world Android apps. We find that Android World uses Set-of-Marks (SOM) (Yang et al., 2023b) to enhance the agent's grounding ability. However, when humans operate smartphones, their brains do not label elements on the screen. Over-reliance on SoM can lead to insufficient focus on pixel-level grounding ability. Therefore, in our experiments, agents respond to the raw image rather than the annotated image."}, {"title": "4.2 Main Results", "content": "ScreenSpot. Table 4 provides the results of different models across three platforms (Mobile, Desktop and Web) and two element types (Text and Icon) on ScreenSpot (Cheng et al., 2024). InfiGUIAgent-2B achieves highest accuracy of 76.3%, surpassing several strong baselines such as ShowUI (Lin et al., 2024) (75.1%) and UGround-7B (Gou et al., 2024) (73.3%), which is even with larger parameters size.\nAndroidWorld. Table 5 compares the success rates of InfiGUIAgent with open-source models on AndroidWorld (Rawles et al., 2024). InfiGUIAgent-2B achieves an overall success rate of 0.09, outperforming open-source models of similar size, such as ShowUI-2B (Lin et al., 2024) (0.07), and model with much more parameters such as LLaVa-OV-7B (Li et al., 2024b) (0.00) and Qwen2-VL-72B (Bai et al., 2023b) (0.05)."}, {"title": "5 Conclusion", "content": "In this work, we propose InfiGUIAgent, a novel MLLM-based GUI Agents. By constructing comprehensive training datasets with two-stage supervised fine-tuning, we enhance the model's ability to understand, reason, and interact with GUIs. Our"}]}