{"title": "GROOT: Generating Robust Watermark for Diffusion-Model-Based Audio Synthesis", "authors": ["Weizhi Liu", "Yue Li", "Dongdong Lin", "Hui Tian", "Haizhou Li"], "abstract": "Amid the burgeoning development of generative models like diffusion models, the task of differentiating synthesized audio from its natural counterpart grows more daunting. Deepfake detection offers a viable solution to combat this challenge. Yet, this defensive measure unintentionally fuels the continued refinement of generative models. Watermarking emerges as a proactive and sustainable tactic, preemptively regulating the creation and dissemination of synthesized content. Thus, this paper, as a pioneer, proposes the generative robust audio watermarking method (Groot), presenting a paradigm for proactively supervising the synthesized audio and its source diffusion models. In this paradigm, the processes of watermark generation and audio synthesis occur simultaneously, facilitated by parameter-fixed diffusion models equipped with a dedicated encoder. The watermark embedded within the audio can subsequently be retrieved by a lightweight decoder. The experimental results highlight Groot's outstanding performance, particularly in terms of robustness, surpassing that of the leading state-of-the-art methods. Beyond its impressive resilience against individual post-processing attacks, Groot exhibits exceptional robustness when facing compound attacks, maintaining an average watermark extraction accuracy of around 95%. Our audio samples are available\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "The advancements in generative adversarial networks (GANs) [9, 14, 15, 47] and diffusion models (DMs) [10, 35\u201337] have revolutionized the way of multimedia content generation. These models have significantly reduced the gap between generated content and authentic content, blurring the lines between what is real and what is artificial. To cope with the confusion on distinguishing between natural and synthesized content, deepfake detection has explored a variety of innovative approaches to widen the distinction. Regrettably, this effort not only enhances deepfake detection techniques but also drives the evolution of generative models. Generative models (GMs) learn from these distinctions and improve further.\nFor several decades, watermarking has served as a proactive solution for protecting the intellectual property rights of multimedia content and tracing its origins. This technique continues to be relevant and effective in modern contexts, particularly for proactively combating deepfake threats and sourcing corresponding GMs. Recent research has identified post-hoc watermarking and generative watermarking as two primary strategies for tagging AI-generated content. Post-hoc watermarking involves embedding the watermark after the content\u2019s generation, making it an asynchronous process. In contrast, generative watermarking integrates the watermarking process with content synthesis, utilizing the same GM for both tasks. Specifically, in terms of generative image watermarking, one of the solutions incorporates watermarks into the weights or structure of GMs [22, 43, 46]. Another explores the transferability of watermarks [8, 23, 28, 40, 42, 45], where the watermark is applied to the training data and consequently embedded in all generated images, leveraging its transferable characteristics.\nGenerative image watermarking [8, 42, 43] has rapidly gained prominence among researchers due to its superior fidelity and robustness. While this field flourishes within the realm of imagery, generative audio watermarking remains underexplored and lacks similar advancements. Chen et al. [3] and Ding et al. [7] pioneered the exploration of generative steganography in speech using autoregressive models. Building on this foundation, Cho et al. [5] and Li et al. [21] introduced generative audio steganography algorithms based on GANs. Designed to address particular applications such as model attribution and coverless steganography, these algorithms operate under the assumption that the distribution of generated content occurs through lossless channels-a condition that deviates significantly from real-world scenarios. The inherent limitation in robustness constrains the ability of these algorithms to proactively regulate the utilization of generated content. Furthermore, advancements of DMs have accelerated the audio synthesis, leading to their widespread application. However, the aforementioned research has not yet explored the potential of generative watermarking based on DMs.\nTo tackle these challenges, we proposed a generative robust audio watermarking (Groot) method tailored for diffusion-model-based audio synthesis. By directly generating watermarked audio through DMs, we can regulate and trace the use conditions of the generated audio and its originating DMs. As illustrated in Fig. 1, Alice\u2019s route demonstrates the conciseness but effectiveness of our proposed Groot method in comparison to the ordinary audio generation process, depicted as Bob\u2019s route. Specifically, the encoder converts the watermark into a format recognizable by DMs. Following joint optimization with a carefully designed loss function, the DMs can directly produce watermarked audio from the input watermark. A precise decoder is then employed to accurately extract the watermark from the generated audio. Our approach marries generative watermarking with proactive supervision, with the training overhead being exclusive to the encoder and decoder. This eliminates the necessity for complex retraining of DMs. Such a feature makes our method versatile and readily implementable as a plug-and-play solution for any diffusion model.\nIn a nutshell, the contributions can be summarized as:\n\u2022 New Paradigm. We pioneered an exploratory investigation and proposed a generative audio watermarking technique for proactively supervising generated content and tracing its originating models. We utilized a meticulously designed watermark encoder and decoder to directly synthesize watermarked audio through the diffusion models.\n\u2022 Vigorous Robustness. The robustness experiments validate that Groot exhibits remarkable resilience against not only individual post-processing attacks but compound attacks formed by arbitrary combinations of single attacks.\n\u2022 High Performance. We empirically validate several criteria, encompassing fidelity and capacity. It illustrates Groot maintains a superior quality in watermarked audio and can adapt to large capacities of up to 5000 bps."}, {"title": "2 RELATED WORK", "content": "2.1 Text-to-Speech Diffusion Models\nText-to-speech (TTS) is a technique that synthesizes the waveform from the transcriptions. Nowadays, TTS embraces its significant performance boost by relying on the advantage of diffusion models. A complete TTS synthesis process consists of two main stages designed as deep neural networks: text-to-spectrogram [2, 12, 27, 31] and spectrogram-to-waveform [4, 11, 18\u201320, 26]. The methods employed for text-to-spectrogram leverage diffusion models to generate the mel-spectrogram from Gaussian noise, with text input serving as the prompt. For spectrogram-to-waveform, these approaches synthesize the waveform by utilizing the mel-spectrogram as conditional input to the DMs. The proposed Groot primarily leverages the spectrogram-to-waveform process for watermarking. DMs generate watermarked audio directly by taking the watermark transformed into the latent variable as input.\n2.2 Audio Watermarking\nDeep-Learning-based Watermarking With the gradual advancement of deep learning in the field of audio watermarking, an increasing number of deep-learning-based audio watermarking techniques have emerged [1, 24, 25, 30, 33]. Concretely, Chen et al. [1] utilized invertible neural networks (INNs) for embedding the watermark into audio to boost robustness. Liu et al. [24] pioneered a watermarking framework to withstand audio re-recording. To detect speech generated by AI, Roman et al. [33] devised a localized watermarking technique. In addition, Liu et al. [25] proposed timbre watermarking, aiming to defeat the voice cloning attacks.\nGenerative Watermarking Diverging from post-hoc water-marking techniques, generative audio watermarking (or steganography) [3, 5, 7, 21] roots the watermark (or secret message) into GMs, facilitating the direct synthesis of watermarked (or stego) audio. Both [3] and [7] leverage autoregressive models to generate realistic cover speech samples. Specifically, [3] utilizes adaptive arithmetic decoding (AAD), whereas [7] employs the distribution copy method for embedding the secret message. Utilizing GANs, [21] directly generated stego audio from secret audio. For model attribution, [5] also employs GANs, which are trained to synthesize watermarked speech by incorporating a specific key and constraints. While these methods are constrained by transmission channels and limitations in model and practical application, the proposed Groot stands out as a plug-and-play generative watermarking method, enabling supervision through extracted watermarks. Moreover, it is capable of adapting to more robust scenarios."}, {"title": "3 PRELIMINARIES", "content": "The proposed Groot leverages vocoders based on DDPM [10] to generate the watermarked audio. The blurb of diffusion-model-based vocoders about generation is described as follows.\nIn the forward diffusion process, the normally distributed input s\u2080 is produced by gradually adding Gaussian noise to the original audio so ~ qdata(s\u2080). It follows a discrete Markov chain {st}\u1d57\u209c=\u2080 which is also Gaussian distributed, that is\n$$q(s_t | s_{t-1}) = N(s_t; \\sqrt{1 - \\beta_t}s_{t-1}, \\beta_tI),$$ \nwhere \u03b2t \u2208 (0, 1) is the variance scheduled at time step t, and I is an identity matrix. Let \u03b1t = 1 \u2212 \u03b2t, \u03b1\u0304\u209c = \u220f\u1d62\u208c\u2081 \u03b1\u1d62 and \u03f5 ~ N(0, I). For any time step of t, by re-parameterization, one has\n$$s_t = \\sqrt{\\bar{\\alpha}_t}s_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon.$$\nThe reverse denoising process generates an estimate of the audio waveform from the input s\u209c through a UNet-like network \u03b5\u03b8. Given that the denoising process q(st-1|st) follows a Gaussian distribution, according to Bayes\u2019 theorem, we can derive q(st-1|St) as follows:\n$$q(s_{t-1}|s_t) = \\frac{q(s_t | s_{t-1})q(s_{t-1}|s_0)}{q(s_t | s_0)}$$"}, {"title": "4 METHODOLOGY", "content": "Our proposed method, Groot, aims to seamlessly connect the input latent variables of DMs with the generation of watermarked audio content. To achieve this, we are confronted with three primary challenges: first, designing an architecture to incorporate the watermark into the input of DMs; second, developing a training approach that encourages DMs to naturally generate watermarked content; and third, devising a reliable architecture for extracting the watermark from the generated audio.\nTo solve the question above, solutions encompassing three main phases-watermarking, generating, and extracting-are illustrated in Fig. 2. Firstly, an encoder E(\u00b7) is contrived to transform the watermark w into a latent variable \u03c3. This transformed watermark is then combined with the origin latent variable st, serving as the input of DMs. The diffusion model employed is a pre-trained architecture, fixed and solely dedicated to generating audio content embedded with the watermark. Subsequently, an advanced decoder D(\u00b7) is developed to accurately extract the watermark from the synthetic audio. To ensure DMs incorporate the watermark into the audio content, we define a loss function that facilitates a joint training process. Consequently, the encoder E(\u00b7), a diffusion model(with its parameters fixed), and the decoder D(\u00b7) are simultaneously trained, guided by the criteria set forth by the loss function.\nBuilding upon the three main phases previously outlined, the subsequent section will detail the specific architectures of both the watermark encoder and decoder. Additionally, the methodologies for embedding and extracting watermarks, the approach to joint training, and the theory for watermark verification will be meticulously described, each in their respective segments.\n4.1 Watermark Encoder and Decoder\nThe watermark encoder aims to convert the watermark w to a latent variable \u03c3, which satisfies the distribution of DMs\u2019 input. The purpose of the watermark decoder is to distinguish features between the audio and the watermark, extracting the watermark w\u0302 from the watermarked audio x\u2080.\nThe watermark encoder is mainly composed of linear layers and Rectified Linear Unit (ReLU) activation functions. As depicted in Fig. 3, outputs of Fully Connected (FC) layers need to thoroughly get through the ReLU activation function. The input size of the first FC layer, namely the length of w is configurable, while the output of the last FC layer needs to adapt for the input size of DMs. Under the specific layout of the encoder, the watermark is converted to a latent variable \u03c3 with Gaussian distribution, which can be recognized as an input for diffusion models."}, {"title": "4.2 Watermark Embedding and Extracting", "content": "Watermark embedding: During the embedding process, the Gaussian latent variable st, sampled from the standard Gaussian distribution, continues to serve as the input for DMs. To root the watermark into DMs, the watermark encoder E(\u00b7) is utilized to transform the watermark w into the latent variable \u03c3 before generation process:\n$$\\sigma = E(w) \\in \\mathbb{R}^v,$$ \nwhere v = b \u00d7 ls, b is the batch size and ls represents the length of the Gaussian latent variable st. Subsequently, the latent variable \u03c3 is superposed to st to acquire the final latent variable xT for generating watermarked audio:\n$$x_T = s_T + \\sigma.$$\nThe generation process for watermarked audio x\u2080 is completed through the denoising process, directly derived from the final latent variable xT. The denoising distribution of this generation process adheres to the same format of Eq. (3), with the input being replaced by xT. In a consequence, the estimated audio can be obtained by\n$$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}}\\epsilon_\\theta (x_t, t, c)) + \\delta t_z;$$\nwhere the pretrained noise prediction network \u03f5\u03b8(xt, t, c) is utilized to approximate the denoising distribution. Here, c represents the mel-spectrogram, acting as a conditional input to assist in generating watermarked audio. And \u03b4tz represents the random noise, introduced to enhance randomness in the generation process and augment the diversity of the audio. Finally, a traditional sampler is employed to generate watermarked audio step by step. The intact watermarking and generating stage are formalized in Algorithm 1.\nWatermark extraction: The watermark extraction process is an independent process that does not necessitate the use of diffusion and denoising process. The watermark decoder D(\u00b7) is designed to disentangle features between audios and watermarks for recovering the watermark w\u0302:\n$$\\hat{w} = D(x_0) \\in \\mathbb{R}^u,$$\nwhere u = b \u00d7 l, b denotes the batch size, l denotes the length of the watermark. Our approach enables precise supervision of generated content and corresponding DMs by ensuring that the extracted watermark w\u0302 aligns with the embedded watermark. Essentially, through this innovative watermarking and extracting progress, the watermark becomes seamlessly integrated into the audio, maintaining its perceptual quality while ensuring content authenticity and traceability.\nDuring the inference process, the process of embedding watermarks is streamlined compared to the training process. Unlike the training procedure, where the latent variable \u03c3, derived from the watermark encoder, is added to the Gaussian latent variable sT, for inference, \u03c3 is directly fed into the diffusion model. Thus, as a coverless watermarking technique, our method allows for direct use of the watermark w as input to the pretrained DMs, as illustrated in Fig. 2. This can be formulated as:\n$$x_0 = G(E(w)),$$\nwhere G(\u00b7) symbolizes the generative progress employed by DMs. Ultimately, the pretrained watermark decoder skillfully extracts the watermark w\u0302 from the watermarked audio."}, {"title": "4.3 Joint Optimization of Training Process", "content": "Our Groot method places a paramount emphasis on the dual objectives of maintaining high-quality watermarked audio and achieving precise extraction of watermarks. To adeptly balance these critical aspects, we have implemented a joint optimization strategy. This approach focuses on conducting gradient updates for both the watermark encoder and decoder, while strategically keeping the parameters of DMs unchanged. Regarding the quality of watermarked audio, we employ the logarithm Short-Time Fourier Transform (STFT) magnitude loss Lmag (S0, X0) as in [41]. It utilizes an L\u2081 norm to constrain the log-magnitude, as defined by:\n$$L_{mag} = || log(STFT(s_0)) \u2013 log(STFT(x_0))||_1,$$\nwhere || ||1 denotes the L\u2081 norm, STFT(\u00b7) represents the STFT magnitudes, so and x0 represent the original audio and watermarked audio respectively. Furthermore, drawing upon [17], we integrate the mel-spectrogram loss Lmel (S0, x0), which measures the distance between the original and watermarked audio using the L1 norm. It is expressed as:\n$$L_{mel} = E_{\\phi(s_0), \\phi(x_0)} [||\\phi(s_0) \u2013 \\phi(x_0)||_1],$$\nwhere \u03c6(\u00b7) denotes the function of mel-spectrogram transformation. The total loss that constrains watermarked audio quality can be computed as:\n$$L_{Aud} = \\lambda_{mag}L_{mag} + \\lambda_{mel} L_{mel},$$\nwhere \u03bbmag and \u03bbmel are hyper-parameters for the log STFT magnitude loss and mel-spectrogram loss, respectively. Their target is to strike a balance between these loss terms.\nTo guarantee successful watermark recovery, binary cross-entropy stand as the indispensable choice:\n$$L_{WM} = - \\sum_{i=1}^k w_i log \\hat{w}_i + (1 \u2212 w_i) log(1 \u2013 \\hat{w}_i).$$\nOur final training loss is structured as follows, with both the watermark encoder and decoder optimized synchronously,\n$$L = \u03c4L_{Aud} + L_{WM},$$\nwhere \u03c4 is the hyper-parameters for the total audio quality loss LAud, used to control the trade-off between audio quality and watermark extraction accuracy."}, {"title": "4.4 Watermark Verification", "content": "Regulating generated contents and tracing associated DMs are achieved by verifying the existence of the watermark within the generated audio through test hypothesis [23, 43]. By assuming the watermark bit errors are independent to each other, with the previously defined watermark bits length l, the number of matching watermark bits k follow the binomial distribution:\n$$Pr(X = x) = \\sum_{i=k}^l {l \\choose i} \\xi^i (1 \u2013 \\xi)^{l-i},$$\nwhere \u03be is the probability that needs to be tested under hypotheses. In the common use of the binomial test, the null hypothesis Ho states the variable X is observed with a random guess of probability \u03be = 0.5, whereby the model is not watermarked. The alternative hypothesis H\u2081 states the watermark is produced by the owner."}, {"title": "5 EXPERIMENTAL RESULTS AND ANALYSIS", "content": "In this section, we conduct comprehensive experiments to rigorously evaluate our Groot method across several dimensions: fidelity and capacity, robustness. Moreover, we undertake a comparative analysis of Groot against the current state-of-the-art (SOTA) methods. The details of these experiments and their respective analyses are detailed below.\n5.1 Experimental Setup\n5.1.1 Dataset and Baseline. Our experiments were carried out on LJSpeech [13], LibriTTS [44], and LibriSpeech [29] datasets. LJSpeech is a single-speaker English speech dataset featuring approximately 24 hours of audio data with a sample rate of 22.05 kHz. LibriTTS and LibriSpeech both are multi-speaker English datasets comprising around 584 hours of audio data recorded at a sample rate of 24kHz and approximately 1000 hours of audio recordings with a sample rate of 16kHz, respectively. In addition, we conducted a comprehensive comparative evaluation of proposed Groot against existing SOTA methods, including WavMark [1], DeAR [24], AudioSeal [33], and TimbreWM [25]."}, {"title": "5.2 Ablation Study", "content": "To assess the efficacy of MGCNN as the foundational architecture of the watermark decoder, as detailed in Section 4.1, we conducted an ablation study with LJSpeech dataset by substituting all MGCNNs with conventional CNNs as a baseline. This investigation explored a range of capacities, specifically 100, 500, 1000, 2000 bits per second (bps). Fig. 6 showcases the watermark recovery accuracy employing different decoder structures, presented through a nested bar chart. The results indicate that MGCNNs outperform traditional CNNs significantly, showcasing a substantial enhancement in recovery accuracy as depicted in the bar chart. Across various capacities, the average accuracy attained with MGCNNs is an impressive 99.47%. Conversely, employing standard CNNs in the watermark decoder leads to a marked reduction to an average of just 89.31%. Specifically, at the 100 bps capacity, the accuracy of CNNs\u2019 is 11.14% lower compared to the performance with MGCNNs."}, {"title": "5.3 Fidelity and Capacity", "content": "5.3.1 The Performance of Proposed Groot on Fidelity and Capacity. Fidelity gauges the extent to which watermarking minimally impacts the perceptibility of the original generation. We validated the fidelity of watermarked audio with the evaluation metrics exhibited in Section 5.1.2. Table 1 elaborate the experimental results on fidelity across different datasets employing DiffWave. In this context, Benchmark refers to the generated audio, whose results are compared to the Ground Truth. In scenarios involving watermarked audio, the results are presented in comparison to the Benchmark. The experimental results demonstrate that the quality of watermarked audio remains impressively high across various datasets For LJSpeech, the STOI metric consistently holds at 0.96, and the lowest MOSL surpasses 3.3326 with only minimal reduction. While a slight numerical decline is observed in the multi-speaker LibriTTS and LibriSpeech datasets, attributed to resampling, this decrease does not materially affect the overall quality of the watermarked audio. Importantly, there is no discernible downward trend in audio quality as the capacity increases, even at a capacity of 2000 bps, the evaluation metrics show only a marginal decrease. We also expanded the functionality of Groot to encompass Aishell-3 Chinese datasets [34], undertaking experiments to assess its cross-lingual fidelity and capacity. Detailed specifics are provided in the Appendix.\nThe fidelity experiments conducted with different DMs on LJSpeech datasets are presented in Table 2. These results reveal only a minor degradation in audio quality when employing WaveGrad across four different capacities, with the average STOI and SSIM scores remaining approximately 0.89 and 0.73, respectively. Moreover, recovery accuracy remains consistently high at 99%. In the case of PriorGrad, the average STOI and SSIM scores are 0.95 and 0.84, respectively. It sustains an exceptional accuracy rate across all capacities, averaging at 99.80%.\nCapacity refers to the number of watermark bits that can be embedded. To evaluate the scalability of Groot in terms of watermark capacity, experiments were performed over a range of bps: 2500, 3000, 3500, 4000, 4500, and 5000 bps. Fig. 5 visualizes the impact on extracting accuracy as the watermark capacity increases. As observed, while LibriTTS exhibits minimal accuracy decline even at 5000 bps, the other two datasets experience noticeable decreases, albeit within an acceptable margin. Notably, from 2500 to 4500 bps, the proposed approach achieves an accuracy of approximately 99% with negligible impact on audio quality."}, {"title": "5.3.2 The Comparison With SOTA Methods on Fidelity and Capacity", "content": "Our evaluation commenced with a fidelity comparison between the proposed Groot and existing SOTA methods. The results are detailed in Table 3, with the capacity settings (16, 32, and 100 bps) conforming to the capacities reported in these SOTA methods. To effectively highlight the comparative outcomes, we introduce the prefix \"D-\" to denote the difference value between the watermarked audio and the original audio without watermark.\nThe analysis of the experimental results indicates that despite AudioSeal showing marginally smaller discrepancies in evaluation metrics, Groot consistently matches its audio quality. While WavMark and Groot are nearly indistinguishable in terms of STOI, Groot presents smaller variations across other metrics. In comparison with DeAR and TimbreWM, Groot showcases superior audio quality. To further affirm the superior capacity performance of Groot, we compared with DeAR and TimbreWM at 2500 bps. The results distinctly show that the watermark extraction accuracy for both DeAR and TimbreWm drastically falls to 50%, highlighting their limitation in adapting to high-capacity conditions. In stark contrast, Groot, achieves high recovery accuracies of 99.04%, 99.70%, and 99.81% across different diffusion models, all the while preserving considerably good audio quality."}, {"title": "5.4 Robustness", "content": "5.4.1 The Robustness of Proposed Groot and The Comparison With SOTA Methods Under Individual Attacks. To ensure the integrity and resilience of our watermark against potential attacks during dissemination, we undertook comprehensive robustness experiments. It is essential to emphasize that our DMs, alongside our watermark encoder and decoder, are proprietary technologies and remain confidential. Consequently, attacks are constrained to execute under a black-box assumption, focusing primarily on post-processing manipulations of the content. In light of this, we assessed the robustness of the watermarked audio against various disturbances, including random Gaussian noise, low-pass and band-pass filtering, stretch, cropping, and echo. These specific attacks are elaborately detailed in the Appendix. Table 4 summarizes the results of robustness for Groot and four SOTA methods against individual attacks.\nThe experimental results validate the excellent robustness of the proposed Groot against individual attacks. Groot demonstrates superior robustness against Gaussian noise and echo compared to SOTA methods, particularly at noise levels of 5 dB and 10 dB. Here, it achieves watermark extraction accuracy exceeding 99% with negligible degradation. Even undergoing echo effect, its recovery precision maintains an impressive rate above 98%. Although DeAR method attains perfect accuracy against band-pass filtering and stretching, its accuracy markedly declines after cropping and at a noise level of 5dB, barely surpassing 70%. Despite Groot does not always secure the highest recovery accuracy for certain attacks, it maintains high average accuracies across different DMs-98.68%, 98.56%, and 96.82%, respectively. This solid performance of Groot further confirms its strong robustness against individual attacks."}, {"title": "5.4.2 The Comparison With SOTA Methods Under Compound Attacks", "content": "With unpredictable transmission environments and the possibility of malicious interventions during dissemination, content is likely to encounter complex, combined attacks in real-world scenarios. To delve deeper into the robustness of Groot with increased rigor, we evaluated its performance under five composite attacks, each integrating two distinct types of individual attacks, alongside one composite attack that combines three single attacks. The specific attack combinations are detailed as follows. 1) low-pass filtering succeeded by Gaussian noise, 2) band-pass filtering followed by an echo attack, 3) cropping and subsequently stretching, 4) Gaussian noise followed by echo, 5) Gaussian noise coupled with band-pass filtering and, 6) Gaussian noise succeeded by band-pass filtering coupled with echo.\nThe results of the robustness experiments against compound attacks are presented in Table 5. Concretely, Groot illustrates exceptionally superior robustness against compound attacks 1 and 4, with watermark extraction accuracy far surpassing other SOTA methods. Despite facing multifaceted challenges of compound attacks 4 and 5, the recovery accuracy maintains at 99.22% and 99.24%, respectively. Even when confronted with compound attack 6, which amalgamates three separate attacks, Groot sustains an accuracy level of over 90%, peaking at 98.23%. Although DeAR exhibits reasonable robustness against compound attack 2, it fails to maintain the anticipated robustness against compound attacks 1 and 3. In contrast, Groot consistently delivers commendable accuracy across all composite attacks, underscoring its well-balanced and dependable robustness. Furthermore, Fig. 7 utilizes a radar chart to visually represent the robustness of various SOTA methods in countering compound attacks. A more extensive area within the radar chart signifies enhanced robustness. The radar chart for Groot distinctly reveals a larger coverage area, highlighting its superiority over SOTA methods."}, {"title": "6 CONCLUSION", "content": "In this paper, we propose Groot, a novel generative audio watermarking method, aimed at effectively addressing the challenge of proactively regulating the generated audio via DMs. Groot instills the watermark into DMs, enabling the watermarked audio can be generated from the watermark via DMs. Leveraging our designed watermark encoder and decoder, there is no requirement for retraining the DMs. Robustness has been enhanced precisely due to the meticulous watermark decoder and jointly optimized strategy. The experimental results and comparisons to SOTA methods further illustrate that Groot exhibits potent and well-balanced robustness capable of countering individual and even compound attacks while ensuring superior fidelity and capacity. Regarding future work, we will introduce the noise layer and incorporate the more suitable watermark encoder and decoder to boost the fidelity and robustness of the generative watermarking method."}]}