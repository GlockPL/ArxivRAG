{"title": "FLEX: END-TO-END TEXT-INSTRUCTED VISUAL NAVIGATION WITH FOUNDATION MODELS", "authors": ["Makram Chahine", "Alex Quach", "Alaa Maalouf", "Tsun-Hsuan Wang", "Daniela Rus"], "abstract": "End-to-end learning directly maps sensory inputs to actions, creating highly inte- grated and efficient policies for complex robotics tasks. However, such models are tricky to efficiently train and often struggle to generalize beyond their training scenarios, limiting adaptability to new environments, tasks, and concepts. In this work, we investigate the minimal data requirements and architectural adaptations necessary to achieve robust closed-loop performance with vision-based control policies under unseen text instructions and visual distribution shifts. To this end, we design datasets with various levels of data representation richness, refine feature extraction protocols by leveraging multi-modal foundation model encoders, and assess the suitability of different policy network heads. Our findings are synthe- sized in Flex (Fly-lexically), a framework that uses pre-trained Vision Language Models (VLMs) as frozen patch-wise feature extractors, generating spatially aware embeddings that integrate semantic and visual information. These rich features form the basis for training highly robust downstream policies capable of general- izing across platforms, environments, and text-specified tasks. We demonstrate the effectiveness of this approach on quadrotor fly-to-target tasks, where agents trained via behavior cloning on a small simulated dataset successfully generalize to real-world scenes, handling diverse novel goals and command formulations.", "sections": [{"title": "1 INTRODUCTION", "content": "A significant dimension of human reasoning is mediated through the combination of vision and language, facilitating our mobility in the physical world and our ability to follow directions. Such flexibility and concept understanding are highly desirable in autonomous robots, enabling interactions with humans and handling variants of complex real-world tasks from few representative examples. This inspires an exploration of the conditions necessary to equip robots with a human-like intuition and capacity to execute tasks across various contexts.\nEnd-to-end learning. Despite progress in end-to-end deep learning for autonomous navigation (Chib & Singh, 2023), and improvements in safety (Xiao et al., 2023) and generalization (Chahine et al., 2023; Quach et al., 2024), reasoning at the level of raw pixel and sensor data still suffers from significant limitations. End-to-end models remain very much black-box, inherently incapable of user interaction, and confined to the scope seen in training data.\nFoundation Models. Immense strides have been made in developing language-augmented founda- tional vision models with robust capabilities in open-world visual understanding, for tasks including classification (Radford et al., 2021; Yang et al., 2022), detection (Li et al., 2022c; Zhong et al., 2022), segmentation (Kirillov et al., 2023; Li et al., 2022a), and, captioning (Li et al., 2023; Wang et al., 2022). Within robotics, benefits have mostly been reaped in open-vocabulary detection and object manipulation (Chen et al., 2022; Liu et al., 2024), intermediate to high-level planning (Ahn et al., 2022) and even direct action prediction (Brohan et al., 2023). In navigation, similar approaches separating detection and control (Maalouf et al., 2023), or generating way-points plans (Shah et al., 2023) have been adopted to navigate to open-set goals."}, {"title": "2 PRELIMINARIES", "content": "End-to-end multi-modal imitation learning. The setup considered is that of an end-to-end control system f that generates commands $u \\in R^n$ where n is the dimension of the output vector. The system takes multi-modal input comprising of a RGB image $I \\in R^{h \\times w \\times 3}$, with h, w representing the frame height and width respectively, and a natural language text command T. f can be seen as the composition of a feature extraction backbone $\\phi$ and a policy head $\\pi$, such that $f = \\pi \\circ \\phi$, and yielding control commands through $u = f(I,T) = \\pi(\\phi(I,T))$.\nThroughout this work, we do not seek to train or fine-tune $\\phi$, but instead, investigate how architectural choices leveraging frozen VLM encoders can yield dense feature representations $F \\in R^{h' \\times w' \\times d}$ that integrate both spatial and semantic information tailored to robotics applications (Figure 1). We thus only train the policy network head $\\pi$, parameterized by weights $\\theta$ adopting the Imitation Learning (IL) paradigm of learning from expert demonstrations.\nIndeed, given a dataset $D = \\{(I_i, T_i, u_i)\\}_{i=1}^N$ consisting of N samples, where each sample contains an RGB image $I_i$, a natural language command $T_i$, and a ground truth control command $u_i \\in R^n$, the policy network $\\pi_{\\theta}$ is trained to minimize the Mean Squared Error (MSE) between the predicted control command $\\hat{u}_i = \\pi_{\\theta}(\\phi(I_i, T_i))$ and the ground truth label $u_i$. With the notation adopted the training objective $L$ is given in equation 1.\n\n$L(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\left|\\left|u_{i} - \\hat{u}_{i}\\right|\\right|_{2}}{2}$ (1)\nAutonomous Drone Fly-to-target Task. The scope of this research extends to a broad array of robotics tasks that rely on the use of both images and text. In the interest of cohesive illustration, we delve into a single running example throughout this manuscript. We explore quadrotor flight and more specifically a vision-based fly-to-target task where the goal can be specified by the human user via natural language. In this context, the control command $u \\in R^4$ comprises of scalar translation"}, {"title": "3 METHODS", "content": "3.1 TRAINING DATA\nA desirable property for an imitation learning system is to master a task from a handful of repre- sentative expert demonstrations without requiring extensive enumeration of use cases or intensive randomization and augmentation techniques. Hence, relying on internet-scale trained VLMs for fea- ture extraction largely mitigates the impediments on training dataset size, diversity and augmentation. We investigate the extent to which this statement holds, and limit ourselves to the use of a single simulated scene to generate four training datasets, evaluating the impact of diversity in the goal and text instruction phrasing on generalization capabilities of trained agents:\n1. One object and one command, containing demonstrations reaching a single goal object (red sphere) with a single command syntax (\"Fly to the red ball\").\nIM. One object and multiple commands, with the same goal object, but each run instructed with a lexical alternative of the instruction (as discussed in Appendix A.3).\n2. Two objects and one command, with red and blue spheres as the example goals and single command wording in either case (\"Fly to red/blue ball\").\n2M. Two objects and multiple commands, containing both colored spheres and variations of the syntax between demonstrations."}, {"title": "3.2 PATCH-WISE TEXT-VISION SPATIAL FEATURES", "content": "Generic image-text features. Robust OoD generalization relies on universal rather than domain- specific features for policy learning. Foundation model encoders leverage internet-scale data to learn generic features from a wide spectrum of contexts. Moreover, incorporating textual instructions demands an extractor capable of seamlessly integrating text inputs with visual features. Thus, a natural choice is pre-trained VLM as our feature extractor cornerstone. More specifically, BLIP-2 (Li et al., 2023) is used throughout this work, as it is specifically designed to fuse multi-modal information from large-scale textual and visual datasets, providing a cohesive representation.\nSpatial resolution for robotic tasks. Foundation models typically output a global feature vector representing the entire image. This coarse representation is unsuitable for robotics, where policy learning depends on fine-grained spatial information to effectively interpret and respond to the scene. Thus, we propose a method to extract spatial feature vectors for specific areas in an image. To obtain the global descriptor for a frame, we collect such features for multiple areas/patches covering the whole image. Specifically, given an input frame/image $I \\in R^{h \\times w \\times 3}$, an input text command T, and the patch-resolution $h' \\leq h, w' < w$, we provide a method which utilizes a multi-modal foundation"}, {"title": "4 EXPERIMENTS", "content": "4.1 FLY-TO-ANY-TARGET TASK\nTask description. The objective is to develop a vision-based quadrotor navigation agent capable of reaching arbitrary user-specified goals present in its field of view (FOV) while ensuring generalization across visual scenes, in simulation as well as in the real world.\nEvaluation protocol. A single test run consists of initializing a scene with a number of objects in the drone's FOV and providing text instructions about the goal to reach. The closed-loop inference is run for a fixed number of steps, 80, slightly larger than the average training sequence length. The test is successful if the agent can navigate towards the user-instructed object and center it in the middle of the frame. Failures, on the other hand, can be identified when the drone loses the object (the target exits the FOV and/or another visual cue is centered in on), or fails to approach or center in on the goal. The evaluation of the closed-loop performance of our system is based on monitoring the success rates on repeated runs in various evaluation configurations."}, {"title": "4.2 EXPERIMENTAL SETUP", "content": "4.2.1 SIMULATION\nSimulator. We use the PyBullet physics engine, building off the codebase presented in (Panerati et al., 2021). The drone dynamics we use are based on Bitcraze's Crazyflie 2.x nano-quadrotor. The physics simulation runs at 240Hz, and we allow low-level flight control to occur at the same frequency, although inference runs at a much slower rate of 3Hz. Indeed, for inference or data collection, we simulate the evolution of the system with constant commands for the number of steps corresponding to the desired period.\nBackground scenes. In addition to the in-distribution (InD) scene which we use for training, Samurai, we design a second very different-looking environment; Stadium. In stark contrast to the training environment which has tiled flooring, the Stadium environment ground is covered by green textured grass and field lines. Also, the Stadium stands include large portions of purple walls and its structure is distinctly different from that of the Samurai temples. The Stadium environment is used for scene generalization evaluation in simulation.\nTest scenarios. Each feature extractor, policy head, and dataset combination considered is tested in simulation on an increasingly demanding suite of scenarios. In each case, we gather the success rates over multiple runs, randomly initializing the positions of the potential target goals, the quadrotor distance to the objects, and the initial heading angle. Each scenario is run in both the InD (Samurai) and OoD (Stadium) scenes. The breakdown of scenarios considered (depicted in Figure 11) is as follows:\n1. Red and Blue Spheres: Easiest setup providing a measure of the mastery of the unaltered training task and performance changes based only on the change of scene and/or instruction phrasing.\n2. Mixed Color Spheres: Tests the generalization capability with respect to colors with a choice of two out of red, blue, green, yellow, and purple spheres appearing at initialization.\n3. Red shapes: Evaluates the sensitivity and adaptability to shapes of same color (red) with two out of a sphere, a cube, and a pyramid positioned in the quadrotor's initial FOV.\n4. Mixed Color Shapes: Similar to above with the object colors also randomized to be any of red, blue, green, yellow or purple.\n5. Open Dictionary: Hardest setup that goes beyond shapes and colors, with a range of objects in a more cluttered scene. Three objects are placed in the drone FOV picked amongst a red sphere, blue sphere, a light-colored Jeep, an Australian cattle dog, a brown Horse, a tall and narrow Palm Tree, a toy Space Rocket, and a whole Watermelon."}, {"title": "4.2.2 REAL-WORLD TRANSFER", "content": "Hardware. Our setup utilizes a DJI M300 RTK quadcopter interfaced with a DJI Manifold 2 computer and the DJI Onboard SDK, processing commands on a base station via Wifi to achieve a runtime frequency of just over 1 Hz with our highest resolution models. Flight tests are conducted on an urban university campus lawn, with targets including various cardboard cutouts positioned on tripods. More details are provided in Appendix A.4.\nTest setup. We deploy the system with the ViT policy head on the drone hardware in a series of tests with various props as targets and in different two-object initial configurations, in an urban campus environment. This is the ultimate challenge exposing the agents simultaneously to sim-to-real transfer, new scene generalization as well as new object instruction handling."}, {"title": "5 RESULTS", "content": "5.1 DATASET DESIGN\nThe degree of dataset richness required for generalisation is evaluated by training Flex instances of 256-patch resolution and a ViT policy head on all of the four datasets described in Section 3.1. The success rates on the simulation test cases are depicted in Figure 2. There is a clear gap"}, {"title": "5.3 POLICY NETWORKS", "content": ""}, {"title": "6 RELATED WORK", "content": "Learning via simulations. Neural network policies mapping perception to control have shown significant potential in autonomous navigation challenges (Pomerleau, 1988; Bojarski et al., 2016; Wang et al., 2023b; Chahine et al., 2023; Yin et al., 2023; Kaufmann et al., 2023). However, developing robust large-scale models is challenging as they require extensive training data that is time-consuming, costly, and poses potential safety risks (Kendall et al., 2019). Thus, training robot policies in simulation became a practical alternative. (Amini et al., 2022; Tedrake et al., 2019; Panerati et al., 2021; Shah et al., 2018). Still, simulated environments do not fully capture real-world intricacies, leading to transfer gaps that degrade performance and introduce safety risks. Training policies through intermediate visual abstraction (M\u00fcller et al., 2018; Toromanoff et al., 2020; Behl et al., 2020), aligns with our work but does not utilize multimodal foundation models.\nVLMs and foundation models in Robotics. Integrating foundation models into robotics has enhanced proficiency in dynamic, open-set environments. These models excel in applications like 3D mapping (Huang et al., 2023; Ding et al., 2023), detection and following systems (Maalouf et al., 2023; Liu et al., 2023; Ghiasi et al., 2022; Li et al., 2022a), control and planning (Tellex et al., 2020; Bisk et al., 2020; Ahn et al., 2022; Brohan et al., 2022; Li et al., 2022d), and 3D scene segmentation and understanding (Peng et al., 2023; Jatavallabhula et al., 2023). They also show versatility across multiple data modalities (Ramesh et al., 2021; Crowson et al., 2022; Patashnik et al., 2021; Ramesh et al., 2022; Maalouf et al., 2023), heralding a new era of robots capable of sophisticated reasoning and interaction. In autonomous vehicles, explainable and language-based representations provide introspection and counterfactual reasoning about events (Kim et al., 2019; Omeiza et al., 2021; Kuo et al., 2022; Tan et al., 2023; Zhong et al., 2023). However, none of these works have enabled autonomous vehicles to execute an end-to-end task given via a single text command.\nContrasting recent approaches with our goal. Recent advances in vision-based navigation, like RT-1 (Brohan et al., 2022) and RT-2 (Brohan et al., 2023), represent significant progress but differ fundamentally from our objectives. RT-1 was trained on over 130K real-world demonstrations, while RT-2 incorporates extensive internet-scale pre-training with models up to 55 billion parameters. Similarly, Vint (Shah et al., 2023) was trained from scratch on hundreds of hours of navigation datasets, and VLN-BERT (Hong et al., 2021) adapts BERT by training on 200K interactions via the PREVALENT dataset, totaling > 6 million image-text-action triplets. NavGPT (Zhou et al., 2024) leverages GPT models for zero-shot action prediction, and TG-GAT (Su et al., 2023) was designed for aerial navigation based on dialog history, trained in 6, 269 dialog-trajectory pairs. All these approaches are in stark contrast with our minimalist perspective.\nPatch features. Several methods have been proposed for extracting spatial feature descriptors using foundation models, but they face limitations: (i) some are not multimodal (Amir et al., 2021); (ii) others, fine-tuned for 2D-pixel alignment, causing the loss of many concepts (Ding et al., 2022); (iii) some rely on segmentation models like SAM (Kirillov et al., 2023), which can be inefficient and miss important regions (Jatavallabhula et al., 2023; Maalouf et al., 2024); and (iv) others don't fuse text queries with patch descriptors for semantic relation (Wang et al., 2023a)."}, {"title": "7 CONCLUSION", "content": "This work establishes the essential dataset and model requirements for robust generalization in text-instructed end-to-end visual navigation agents using pre-trained VLM encoders as multi-modal feature extractors. Our findings include the failure of training on a single data context (leads to over-fitting), and the adequacy of two examples to train models that handle a wide spectrum of similar use cases. We also advocate for simple text-space augmentations, which can improve performance in more nuanced test settings. We shed light on the shortcomings of low-resolution patch-wise feature extraction, with the fly-to-target task necessitating at least 8\u00d78 patches. Finally, we ascertain the superiority of the ViT architecture as a policy head, in terms of task success and flight behavior, while uncovering aspects of its robust context invariant decision process via similarity-based clustering.\nThe synthesis of these findings is Flex, a new minimalist training framework capable of producing user-interactive highly generalizing visual navigation agents. Our solution elegantly handles a suite of in-simulation challenges and proves readily deployable in the real-world, robustly achieving direct sim-to-real open dictionary out-of-distribution generalization."}]}