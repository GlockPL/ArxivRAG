{"title": "Renaissance: Investigating the Pretraining of Vision-Language Encoders", "authors": ["Clayton Fields", "Casey Kennington"], "abstract": "In the past several years there has been an explosion of available models for vision-language tasks. Unfortunately, the literature still leaves open a number of questions related to best practices in designing and training such models. In this paper we seek to answer several questions related to the pretraining of vision-language encoders through meta-analysis. In our first set of experiments, we show that we can save significant compute at no cost to downstream performance, by freezing large parts of vision-language models during pretraining. In our second set of experiments we examine the effect of basing a VL transformer on a vision model versus a text model. Additionally, we introduce a VL modeling platform called Renaissance that we use to conduct all of the experiments. This program offers a great deal of flexibility in creating, training and evaluating transformer encoders for VL modeling. The source code for Renaissance can be found at https://github.com/bsu-slim/renaissance.", "sections": [{"title": "1 Introduction", "content": "In the span of a few years, dozens of vision-language (VL) transformers have appeared in the literature with a bewildering array of architectures and training methods (see Fields and Kennington (2023) for a review). VL tasks, such as NLVR2 (Suhr et al., 2018) where the model is tasked with answering questions about images (see Figure 4 for an example) and image captioning require models to somehow represent and fuse both text and image information. Unfortunately, knowledge of best practices for training and implementing these models has lagged far behind the model development process. This stands in contrast to the NLP domain, where studies such as Rogers et al. (2021) and Kaplan et al. (2020) have thoroughly investigated the inner workings and best training practices for NLP transformers. To date, there have been only a handful of studies analyzing VL-transformers, such as Bugliarello et al. (2021), and the collected literature still fails to address some very basic questions concerning VL modeling with transformers.\nIn this paper we begin to address this gap by providing a systematic analysis geared toward shedding light on some basic aspects of training transformers for vision-language modeling. In particular, we focus on the pretraining and fine-tuning of transformer-encoder architectures. Transformer encoders are best suited toward discriminative tasks such as the NLVR2 benchmark that we mentioned in the opening paragraph and we do not address generative tasks like image captioning here. In our first set of experiments (Section 4), we ask whether it is possible to save compute by freezing parts of the model during pretraining and examining the effect on downstream performance. In our second and final set of experiments (Section 5) we compare the performance of a VL transformer based on a pretrained text encoder versus one based on a pretrained vision transformer. Both sets of experiments will help to establish best training practices for those interested in training VL transformers and hopefully also provide theoretical insight. To perform our experiments, we created a novel VL framework that we call Renaissance that streamlines the ability to evaluate different VL model types (e.g., 1-tower and 2-tower) against a suite of benchmarks.\nThe specific contributions of this paper can be summarized as follows:\n\u2022 We introduce a software platform Renaissance that offers a range of options for creating, training and testing vision-language transformer encoder models.\n\u2022 We demonstrate that a great deal of compute can be saved by freezing parts of two-tower encoder models during pretraining. In particular, freezing the visual module can actually lead to small increases in performance. When"}, {"title": "2 Related Work", "content": "2.1 Pretraining Vision-Language\nTransformers\nThe domain of vision-language modeling has seen major advancements in recent years with the adaptation of the transformer (Vaswani et al. 2017a) as VL models. The first examples of VL transformers to appear in the literature were adaptations of the popular BERT NLP model (Devlin et al. 2018). Some examples include VilBERT (Lu et al. 2019), LXMERT (Tan and Bansal 2019a) and VisualBERT (Li et al. 2019a). In the short space of time since these models were introduced a bewildering array of model variations have appeared in the literature. There are huge models designed for zero-shot inference such as Flamingo (Alayrac et al. 2022) and versatile models such as OFA (Wang et al. 2022) that can generate both text and images.\nWhile the literature is now replete with vision-language models, the analysis of their performance and the establishment of best practices has been mostly left open. The aformentioned study Bugliarello et al. (2021) examines pretraining of vision-language model. Bugliarello et al. (2023) is an effort by the same lead author, Emanuele Bugliarello, that provides an analysis of several models on what they term \"fine-grained\" tasks. Frank et al. (2021) examined the extent to which the vision and language modalities are actually integrated in VL transformers. As valuable as these studies have been however, they have barely scratched the surface of understanding vision-language transformers.\n2.2 Vision-Language Modeling Software\nVision language modeling has only recently come to prominence and the available software for it is still in a fairly primitive state. When using NLP models, researchers have a range of available software options that abstract many of the most difficult elements away from users. The most prominent example of this is the Huggingface model hub that specializes in NLP transformers. Though there are a few vision-language models available on Huggingface, there aren't many and they don't lend themselves to the modifications that research often demands. In addition to the Huggingface Hub, there have been some efforts toward creating software platforms primarily dedicated to multimodal modeling. LAVIS, introduced in Li et al. (2023) by Salesforce, is one such platform. Though well programmed and relatively straight forward to use, this program offers support very few VL models. Furthermore, implementing VL tasks is also a fairly involved task. The paucity of available software options led us to create the Renaissance platform for VL modeling that we introduce in the next section."}, {"title": "3 Renaissance: A Versatile Vision-Language Modeling Platform", "content": "We now describe the Renaissance program that we use to complete all of the experiments in this study. Because this is its first introduction, we will provide an extensive description of the program and its capabilities. In this section we also take the opportunity to introduce the pretraining tasks, fine-tuning tasks and the architectural elements required to understand the experimental procedures.\n3.1 Capabilities\nIn this section we describe the capabilities and various options available from the Renaissance platform. The most salient feature of the platform is its ability to easily change the basic architectural features of multi-modal transformers, then train and test them. By simply editing a configuration file, a user can choose a pretrained text encoder or a pretrained vision encoder from the Huggingface hub to insert into the model. In addition to the various architectural options, there are also a number of pretraining and fine-tuning tasks and options available. We will describe these in subsections below.\n3.1.1 Model Types\nOne-Tower Encoder Modeling A one-tower encoder model consists of an embedding layer, and a single transformer encoder module followed by a classification layer. Previous examples of one-tower encoders include models such as UNITER (Chen et al., 2020) and VisualBERT (Li et al., 2019b). In principle, one-tower encoders are very"}, {"title": "3.1.2 Training and Configuration Options", "content": "Beyond providing flexibility in basic architecture design, the program also provides several options for training and configuring models. The most salient of these features are discussed in the this subsection.\nRandom Weight Initialization Multi-modal models are often initialized with weights from pretrained text or image models. For instance Visual-BERT is initialized with the weights from the text model BERT (Devlin et al., 2019) and ViLT with weights from the image transformer ViT (Dosovitskiy et al., 2020). When doing research it is often useful to initialize model weights randomly and train from scratch. This is often useful for establishing baselines in experiments and as we show in Section 5, can have beneficial to the performance of one-tower models. Users can randomly initialize encoder weights by simply changing settings in a configuration file.\nManually Configure Model Dimensions By default, the dimensions of encoder modules is determined by the huggingface hub. However, when model weights are set to be randomly initialized users can manually specify the dimensions of encoder modules. This allows users to easily create completely novel architectures. As an example, consider a one-tower encoder where the encoder is based on ELECTRA-Small. By default, ELECTRA has a hidden size of 256, an embedding size of 128, an intermediate size of 1028 and 12 layers. Any of these numbers can be altered to create encoders of the desired shape and size.\nFreeze Modules During Training It is also easy to freeze the weights of any of the models modules during training. In addition to being useful for research purposes, this feature allows the user to significantly cut the compute costs of training. In practice, freezing the pretrained weights of a model's encoder module can be quite useful and is featured in our first set of experiments."}, {"title": "3.1.3 Pretraining Tasks", "content": "Currently, our program supports two pretraining tasks, masked language modeling and image-text matching. Models can be pretrained with either of these tasks individually or both in conjunction. Using both tasks in conjunction is a common approach found in the literature. Both tasks are briefly described in the list immediately below. A more thorough description can be found in Fields and Kennington (2023).\n\u2022 Masked language modeling (MLM) tasks the model with guessing a masked word based on the image features and the unmasked words. The MLM task was first introduced in Devlin et al. (2019). In the original task, the model's prediction is based only on the unmasked words in sequence of text. In the multimodal setting, the model's prediction is based on the unmasked words as well as the associated image.\n\u2022 Image-text matching is a binary task where the model is presented with an image-text pair and must determine if the text actually describes the image. Positive pairs are simply"}, {"title": "3.1.4 Downstream Vision-Language Tasks", "content": "In order to test and evaluate models, renaissance currently has five downstream vision-language tasks implemented. They are listed below with a brief description of each task.\n1. NLVR2 NLVR2 stands for Natural Language Reasoning for Real and was introduced in Suhr et al. (2018). Here a model will be given two images and must answer a true or false question about them. The addition of second image also makes this quite a challenging task. NLVR2 is very commonly used to benchmark VL models. An example from the dataset can be seen in Figure 4.\n2. SNLI-VE In the SNLI-VE task is a model is presented with an image text-pair and must determine if the image entails the sentence, contradicts the sentence or is neutral with respect to the sentence. It was introduced in (Xie et al., 2019). This task tends to be less challenging than the previous and requires less time to fine-tune and evaluate. Though it appears less commonly in the literature, its quick training time makes it very useful as a model development tool.\n3. Reference Resolution with RefCOCO In this final task a model is presented with an image that is segmented into several objects and a sentence describing one of these objects. The model must then determine which object the sentence is referring to. The RefCOCO dataset was introduced in (Kazemzadeh et al., 2014)."}, {"title": "3.1.5 Unimodal Downstream Tasks", "content": "Renaissance also supports downstream evaluation on pure NLP tasks and pure computer vision tasks. For pure NLP, Renaissance supports the GLUE tasks (Wang et al., 2018). GLUE is a set of natural language understanding tasks commonly used to benchmark NLP models. The program also supports image classification on the CIFAR10 (Krizhevsky et al., 2009). The unimodal capability will be useful in testing if and how multimodal training affects unimodal tasks."}, {"title": "3.2 Design and Implementation", "content": "Renaissance is entirely written in the Python programming language. Though popular and user friendly, using and maintaining large-scale python"}, {"title": "4 Experiment 1: Freezing Encoder Modules During Pretraining", "content": "4.1 Premise\nIn our first set of experiments, we ask what is the effect of freezing the weights of various parts of the model during pretraining? Specifically, if we initialize the vision and text modules of a two-tower encoder with pretrained models from their respective domains, can we freeze one or both of these modules during pretraining? Freezing both modules means that we would only be pretraining the cross-modal and output layers of the model. Pretraining is usually the most compute intensive aspect of model development and we can reduce the GPU memory use and the overall compute required by freezing parts of the model. The compute savings would allow researchers to pretrain models that might otherwise be too large for their hardware requirements. Alternatively, they might also train smaller models at higher batch sizes and possibly obtain better results.\nGiven that both vision and text encoder modules are pretrained in their respective domains, it makes intuitive sense that we might be able to skip at least some portion of their pretraining. These experiments should demonstrate empirically whether or not this is the case. Furthermore, the creators of the dual encoder\u00b9 model LiT (Zhai et al., 2022) found that they obtained slightly better results from freezing the model's vision encoder. This experiment"}, {"title": "4.2 Experimental Setup and Procedure", "content": "To begin, we use Renaissance to construct a set of two-tower models with ELECTRA-Small (Clark et al., 2020) as the text encoder and DeiT-Tiny (Touvron et al., 2021) as the image encoder. Both of these models are both quite small and efficient, and we chose them to expedite the training process. We set the cross-modal encoder module of each model to contain two sets of six transformer layers each with a hidden size of 256 and 4 attention heads. In total we pretrain four model variations, a baseline with both modules unfrozen, one with the text encoder frozen, one with the image encoder frozen and one with both encoder modules frozen. Each model will be pretrained for 100k steps at a batch size of 704 using the masked language modeling and image-text matching tasks described in Section 3.1.3. All models are trained using two NVIDIA L40s GPUs. We use two of the four pretraining datasets, MSCOCO and Visual Genome, which were described in the same section. Finally, we finetune and evaluate our models on three of the five VL tasks described in Section 3.1.4: SNLI-VE, NLVR2 and reference resolution with RefCOCO. In the interest of saving time and compute, we forego evaluating them on multimodal retrieval tasks and visual question answering. A crucial point to consider is that the no model weights are frozen during finetuning."}, {"title": "4.3 Results", "content": "The results for this experiment are summarized in Table 1. We see that we can obtain similar results by freezing one or both of the previously trained encoder modules during pretraining with only mild ill effect. On the SNLI-VE task, the difference between training the whole model and freezing one or both modules is very slight indeed. When freezing the vision module, the downstream results for SNLI-VE are essentially identical and we see only a slight drop in performance compared to the baseline model. We see a slightly different pattern on the NLVR2 task, however. Here we see the results for the baseline model and two models with a single encoder frozen having almost identical results. The model with the visual encoder produces the best score on the reference resolution task. The baseline model and the model with both modules frozen preform very nearly identically, while the"}, {"title": "5 Experiment 2: Text Encoder vs. Vision Encoder", "content": "5.1 Premise\nIn the previous set of experiments we focused on training two-tower models. In our final experiment we will examine the behavior of one-tower models. One-tower encoder models were also described in Section 3.1.1. To date, most of these models have been derived from text encoder models such as BERT (Devlin et al., 2019). A less explored approach is to base such models on transformer based vision models such as ViT (Dosovitskiy et al., 2020); this is the approach of the one-tower VL transformer called ViLT (Kim et al., 2021). In this experiment we ask if one strategy is superior to another when training and evaluating under otherwise similar conditions? More simply put, are one-tower encoders more effective when based on a vision encoder or a text encoder. In addition to providing guidance to future practitioners of VL modeling, answering this question should also provide interesting results from purely theoretical as well as practical perspectives."}, {"title": "5.2 Experimental Setup and Procedure", "content": "To make this experiment as fair a comparison as possible we select a vision transformer and a text transformer model as close in size to each other and architecture as possible. Toward this end we used BERT (Devlin et al., 2019) as our text-encoder model and ViT (Dosovitskiy et al., 2020) as our vision encoder model. The encoder towers in each of these models were consciously designed to have nearly identical dimensions with each encoder module containing 110M parameters. We employ patch embeddings for visual tokens and word-piece embeddings for visual tokens in all models. The resultant models will be close to identical, save that the weights of one are derived from vision pretraining"}, {"title": "5.3 Results", "content": "The results for this experiment are displayed in Table 2. According to our analysis there doesn't appear to be a significant advantage in basing a one-tower encoder model on either text or vision. Surprisingly, the randomly initialized model that we trained as a baseline scored the best on all three downstream tasks. These are very much unexpected results. Though we didn't have an intuition as to whether text or vision would perform better, we didn't expect the downstream results to be so similar and to be inferior to a randomly initialized variation. These results are especially notable since one of the few in depth analyses of vision-language models, Frank et al. (2021), indicates that the interaction between the visual and language modalities are not symmetric. That study used probing techniques to show that VL transformers learn to use vision-for-language more than language-for-vision. Our best explanation of this phenomenon is that that one-tower models do not make use of the individual visual or textual modalities, but instead converge to values not dependent on either.\nAnother notable conclusion of this experiment and the preceding ones, is that two-tower models are in general much more parameter efficient than one-tower models. The one-tower models used in this experiment are relatively large, each containing more than 100M parameters. While the two-tower models in the previous experiments contain less than 40M parameters. Nonetheless, the two-tower models outperform those in this final experiment using the same datasets for training and evaluation."}, {"title": "6 Future Directions", "content": "6.1 Renaissance\nAs this program evolves, we hope to incorporate a number of additional features that are not available in the current version. The capabilities that we plan to add are discussed below.\n6.1.1 Model Types\nThere are several model types, beyond one-tower and two-tower encoders, that we hope to support in future versions. These include, dual encoder, encoder-decoder and decoder only model types (see Fields and Kennington (2023) for explanations of and examples for each type). By virtue of adding these model types, we also hope to include the ability to generate text for tasks such as image captioning.\n6.1.2 Additional Tasks\nIn addition to more model architectures, we hope also to add additional tasks for both pretraining and finetuning. Some asks we intend to add are contrastive learning, reference resolution and visual question answering as pretraining tasks. Further we also hope to add downstream tasks such as image captioning to give the a wider variety of settings to use and evaluate various model architecture."}, {"title": "6.2 Analysis of VL Transformers and Pretraining", "content": "Because of the field of vision-language is rapidly evolving there are many possible future directions for research. We will mention a few. Though we have touched on some of the more basic aspects"}, {"title": "7 Conclusion", "content": "In this study, we have examined some basic features of pretraining vision-language transformers. In addition to the experiments that we've performed, we also introduced a flexible vision-language modeling framework called Renaissance, the source code for which can be found at https://github.com/bsu-slim/renaissance. In our first set of experiments we showed that pretrained vision and text modules can be frozen during vision-language pretraining with only small losses in downstream performance. This finding opens the possibility of training VL models whose size might normally exceed one's compute budget. In our second and final experiment we compared of effect of basing a one-tower encoder model on a text transformer versus a vision transformer. Surprisingly, our results indicate that neither strategy is superior to the other and that randomly initializing model weights yields the best results. We therefore recommend training one-tower models from scratch when possible. We conclude this study with the observation that multimodal modeling is a rapidly expanding pursuit and we hope that this paper is among the first of many that aim to shed light on this dynamic and exciting field of deep learning."}, {"title": "Limitations", "content": "The primary limitations of our study relate to size and scope. With greater resources, particularly compute resources, we would have been able to test more models on a wider variety of downstream tasks. These additional data would have added greater weight to our findings and given them broader applicability."}, {"title": "Ethics Statement", "content": "We have no pertinent ethical conflicts to report."}, {"title": "A Hardware", "content": "In all of the three studies we pertrain all of our models using two NVIDIA L40S GPUs each with 48GB of GPU memory. Where feasible we also used a server with two NVIDIA TITAN RTX GPUS with 24GB of memory and a server with two NVIDA TITAN Xp servers with 12GB of memory for finetuning."}]}