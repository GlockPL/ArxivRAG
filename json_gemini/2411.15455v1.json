{"title": "MUFM: A Mamba-Enhanced Feedback Model for Micro Video Popularity Prediction", "authors": ["Jiacheng Lu", "Mingyuan Xiao", "Weijian Wang", "Yuxin Du", "Yi Cui", "Jingnan Zhao", "Cheng Hua"], "abstract": "The surge in micro videos is transforming the concept of popularity. As researchers delve into vast multi-modal datasets, there is a growing interest in understanding the origins of this popularity and the forces driving its rapid expansion. Recent studies suggest that the virality of short videos is not only tied to their inherent multi-modal content but is also heavily influenced by the strength of platform recommendations driven by audience feedback. In this paper, we introduce a framework for capturing long-term dependencies in user feedback and dynamic event interactions, based on the Mamba Hawkes process. Our experiments on the large-scale open-source multi-modal dataset, Microlens, show that our model significantly outperforms state-of-the-art approaches across various metrics by 23.2%. We believe our model's capability to map the relationships within user feedback behavior sequences will not only contribute to the evolution of next-generation recommendation algorithms and platform applications but also enhance our understanding of micro video dissemination and its broader societal impact.", "sections": [{"title": "Introduction", "content": "The widespread adoption of portable devices has significantly contributed to the success of micro video platforms like TikTok. These devices make it easy for users to share their experiences, opinions, and thoughts in various formats, such as text, images, audio, and video. The resulting increase in user participation has led to the emergence of an important research area: micro video popularity prediction (MVPP).\nWe recognize that current approaches to predicting short video popularity often fall short by underutilizing the wealth of available data, particularly in overlooking the role of event-driven propagation within social networks. To address these shortcomings, we introduce the Mamba-Enhanced User Feedback Capture Model for Micro Video Popularity Prediction (MUFM), which advances popularity prediction by integrating refined recommendation systems with models that account for social network dynamics.\nMUFM is designed to make better use of the diverse data linked to short videos. It starts by using a retrieval system to find relevant micro videos from a multi-modal database, filtering content based on all available information including"}, {"title": "Related Works", "content": "Micro Video Popularity Prediction\nMicro Video Popularity Prediction (MVPP) has been studied using a variety of approaches. One prevalent method is feature engineering, which involves designing specific features to predict popularity. While this approach is widely adopted, it relies heavily on expert knowledge and high-quality feature selection, which can limit the scalability and flexibility of the models (Li et al. 2013) (Roy et al. 2013) (Wu et al. 2016).\nAlternatively, deep learning methods have emerged as powerful tools for modeling multi-modal data. Techniques such as HMMVED (Xie, Zhu, and Chen 2023) and MTFM (Zhao et al. 2024) harness the capabilities of neural networks like ResNet (He et al. 2015) and ViT (Dosovitskiy et al. 2021) for visual data, and BERT (Devlin et al. 2019) and AngIE (Li and Li 2024) for textual data. These models excel in capturing cross-modal correlations and predicting popularity by leveraging the strengths of different data modalities. However, these methods mainly target limited video information, fail to utilize the user behavior information that is more important for social network dissemination, and can't make sufficient use of multi-modal data. Therefore, the accuracy of the existing methods in the MVPP task is relatively low.\nHawkes Processes\nThe Hawkes Process is a self-exciting point process that defines the event intensity as a function of past events, capturing dependencies in time-series data (Hawkes 1971). The Hawkes Process is distinguished by its capability to model self-excitation, where events increase the likelihood of similar future occurrences, as well as mutual excitation, where distinct event types exert influence over one another (Laub, Taimre, and Pollett 2015). This dual functionality renders it particularly effective for applications in finance, social media analysis (Zhou, Zha, and Song 2013) (Rizoiu et al. 2017), and seismic activity modeling.\nRecent advancements have enhanced the original model's capabilities. Mamba Hawkes Process (MHP) combines the Mamba state space architecture with the Hawkes process to effectively capture long-range dependencies and dynamic interactions in temporal data (Gao, Dai, and Hu 2024). While traditional Hawkes processes model self-exciting event sequences, they often struggle with complex patterns, particularly in irregular or asynchronous data. MHP improves this by effectively modeling both history-dependent and independent components, naturally incorporating temporal differences to better predict event timing."}, {"title": "Preliminaries", "content": "Definitions & Problem Statement\nLet \\(V = \\{U_1, U_2,...,U_n\\}\\) represent the set of N micro videos available on an online video platform. Each video \\(v_i\\) consists of L modalities of content, denoted as \\(M_i = \\{m_1, m_2,..., m_l\\}\\), where l > 2. The goal of micro video popularity prediction is to forecast meta parameters related to video popularity, such as cumulative views, comments, and likes. These parameters are represented as \\(Y_i = [y_1, y_2, ..., y_h]\\), where h denotes the number of different popularity metrics for each video \\(v_i\\). The task is to predict the future values of these parameters, \\(Y_i\\), using all relevant modalities that influence the video's popularity trend after its release.\nIn this study, we focus on predicting comments as the primary popularity metric, as comments are less prone to manipulation and provide more detailed temporal information compared to other metrics like total views. The problem we address is to accurately predict \\(y\\), the comment parameter for a given video \\(v_i\\), using its multi-modal content information \\(M_i\\). Specifically, the goal is to forecast \\(Y_i\\) in the future based on the available modalities. We employ our model \\(F(v_i, M_i)\\) to make this prediction. We use normalized Mean Squared Error (nMSE) to evaluate the loss of our prediction model."}, {"title": "Methodology", "content": "Our methodological approach encompasses two modules: the video information extraction module founded on multi-modal information processing and database detection, and the social network dissemination simulation module for the video. In the multi-modal processing module of the video, we introduce LLM to retrieve related videos and process various modal information of the video via architectures such as ViT. Subsequently, we introduce a cross-attention mechanism to explore the interaction relationships between the retrieved videos and the target video. In our social network dissemination simulation module, we establish an event-focused Hawkes process model based on the Mamba architecture to reproduce the dissemination of the video within the group. A prediction network integrates all outcomes of the two modules and offers a prediction of the dissemination intensity of the video."}, {"title": "Micro Video Retrieval", "content": "In micro video dissemination, a video's popularity is often related to the performance of similar videos and is significantly influenced by recommendation algorithms and user behavior. We posit that incorporating similar video information can enhance micro video popularity prediction accuracy. To achieve this, we first employ a module to summarize and extract multi-modal info from micro videos, converting data into a textual description for similar content identification. To retrieve valuable instances for target video popularity prediction, we craft a video-to-text transformation process. This process uses vision and audio-to-text models to generate captions for video content, combined with original textual descriptions. The resulting composite text is used as an input prompt for LLMs, encoded into a retrieval vector representing the corresponding video. This method aligns micro video's audio-visual and textual modalities, addressing potential inconsistencies.\nThe micro video memory repository is a collection of reference pairs, denoted as (frames, text), with each element encoded for efficient retrieval. To generate a retrieval vector for a given micro video \\(V_i\\), we use BLIP-2 (Li et al."}, {"title": "Multi-Modal Process", "content": "For MVPP, it is fatal to extract the required feature information from the multi-modal information of the video. We perform frame-by-frame processing on all target videos, select certain interval frames for extracting image and audio information, and transform them into corresponding frame feature vectors through ViT and AST architectures. Further, we input the image-audio features of the video and the text description into a model with both forward and reverse attention mechanisms to capture its content features. Further, we compare it with the videos matched by the retriever in the database to further enhance this feature."}, {"title": "Multi-Modal Feature Extraction", "content": "To capture the key features of a micro video \\(v_i\\), we extract visual, audio and textual information. We start by selecting key frames \\(F_1, F_2, ..., F_k\\), where K is the total number of frames. Each frame \\(F\\in R^{H\\times W \\times C}\\) is divided into fixed-size patches and reshaped \\(F^* \\in R^{N\\times P^2 \\times C}\\)\nAfter implementing the Audio Spectrogram Transformer (AST) structure (Gong, Chung, and Glass 2021) to gain spectrum matrix \\(M\\in R^{nmel\\times na}\\), we split it into patches and linearly embedded each visual and audio patch, attaining K patch sequences. Through a transformer structure, we have \\(E^v\\) and \\(E^a\\), respectively. These feature matrices are concatenated to emphasize the relationship between visual and audio information:\n\\(E = [E^v ; E^a] \\in R^{K\\times(d_v+d_a)}.\\) (2)\nThe concatenated features \\(E^c\\) are then passed through a linear layer \\(W_c\\in R^{(d_v+d_a)\\times d}\\) with a ReLU activation function, generating an audio-visual input token \\(X^c\\). Similarly we can get textual input token \\(X^t\\) by process textual embedding \\(E^t\\) through layer \\(W_t \\in R^{d_t\\times d}\\).\n\\(X^c \\in R^{K\\times d} : X^c = ReLU(E^cW_c).\\) (3)\n\\(X^t \\in R^{n_w\\times d} : X^t = ReLU(E^tW_t).\\) (4)\nThe specific process and formula description of this part of the model can be found in Appendix 2."}, {"title": "Cross-modal Bipolar Interaction", "content": "Aligning audio-visual and textual modalities in micro videos presents challenges due to potential inconsistencies between textual descriptions and video content. To address this, we implement a cross-attention network comprising both positive and negative attention modules, designed to capture the similarities and differences between multi-modal information. The positive attention module focuses on identifying the most consistent features across different modalities, while the negative attention module highlights any inconsistent or contradictory information.\nWithin the positive attention module, the most aligned features between modalities are calculated using cross-modal attention vectors. For a given video \\(v_i\\), the audio-visually guided positive textual features \\(T^P\\) and the textually guided positive audio-visual features \\(C^P\\) are derived as follows:\n\\(T^P_i = ATT^P(X^qW^q_T,X^kW^k_T,X^vW^v_T)C,\\) (5)\n\\(= Softmax(\\alpha\\frac{Q K^T}{\\sqrt{d}})C,\\) (6)\nwhere \\(W^q_T, W^k_T, W^v_T\\) denote the query, key, and value projection matrices, respectively. Textually guided positive audio-visual features \\(C^P\\) can be gained with a similar method. The parameter \\(\\alpha\\) is used to balance the influence of positive and negative attention. Similarly, the negative audio-visually guided textual features \\(T^N\\) and textual guided audio-visual features \\(C^N\\) can be obtained using the same method.\nAfter this, we referred to the integration processing of hidden states in the MMRA (Geva et al. 2021) (Zhong et al. 2024) model to generate a comprehensive textual modal representation in FFN layers, incorporate audio-visual hidden states into textual hidden states to generate a comprehensive textual and audio-visual modal representation \\(T_i\\) and \\(C_i\\). Thus, we exploit expressive representations \\(T_i, C_i\\) by the attentive pooling strategy (Sun and Lu 2020)."}, {"title": "Retrieval Interaction Enhancement", "content": "We focus on extracting valuable insights from relevant instances retrieved to improve micro video popularity prediction (MVPP). To achieve this, we use an aggregation function that combines the comprehensive representations of these instances from the memory bank. This function assigns attention scores based on the normalized similarity scores obtained during retrieval as weights to construct the retrieved embeddings \\(X^r_i\\) and \\(X^r_t\\). Instances with higher similarity scores are prioritized, highlighting their relevance to the target micro video.\nSimilarly, we calculate \\(T_r\\) and \\(C_r\\), and then derive the final representations \\(T_i\\) and \\(C_i\\) using an attentive pooling strategy. To capture the popularity trends of the retrieved instances, we encode the label information through linear layers and aggregation, resulting in the retrieved label embedding \\(L_i\\). Finally, AUG-MMRA integrates all features from \\(T_i\\) and \\(C_i\\) to model cross-sample interactions. These feature interactions are constructed as\n\\(I = [\\Phi(C_i, C_T), \\Phi(C_i, T), ..., \\Phi(T_i, L)],\\) (7)\nwhere \\(I\\) denotes the process of inner products."}, {"title": "Multi-model Information Based SASRec", "content": "In our micro-video popularity prediction model, we use SASRec, a Transformer-based architecture, to capture users' sequential behavior patterns. It models short and long-term user preferences and incorporates multi-modal item features.\nThe model processes video frames, cover images, and titles extracted with pre-trained encoders. User interaction histories go into the SASRec module, where self-attention layers encode sequential behaviors. Multi-modal features are integrated to form the item scoring representation. The user embedding is combined with this.\nFor each user, a score for each item is calculated by a dot product. The final score for an item across all users is the average of individual scores. These are converted to probabilities with a sigmoid function. The model integrates multi-modal features and optimizes through backpropagation based on user behavior sequences. More details can be found in Appendix 5."}, {"title": "Mamba Hawkes Process (MHP) Architecture", "content": "We define the user's reaction sequence as \\(S = \\{(t_1, r_1), (t_2, r_2), ..., (t_n, r_n)\\}\\), where \\(\\Delta_i = t_i - t_{i-1}\\) represents the time difference, and it is expressed as \\(\\Delta = (\\Delta_1, \\Delta_2, ..., \\Delta_n)\\). The event is represented as a one-hot vector \\(r_i\\).We construct the MHP model using the following equation:\n\\(h(t + \\Delta t) = Ah(t) + Bu(t)\\) (8)"}, {"title": "MHP Model Architecture", "content": "Let \\(W_e\\) be the event embedding matrix of dimension D \u00d7 R, and the event embedding is defined as \\(xt_i = r_i(W_e)^T\\). The sequence embedding is \\((X_{t1}, X_{t2}, ..., X_{t_l})\\).We define the time-dependent matrices B(ti) and C(ti) as linear transformations. The state transition formulas for MHP are:\n\\(z_{t_i} = A(t_i)z_{t_{i-1}} + B(t_i)X_{t_i},\\) (9)\n\\(y_{t_i} = C(t_i)z_{t_i},\\)\nwhere the time-dependent coefficients are:\n\\(A(t_i) = exp(\\Delta A),\\) (10)\n\\(B(t_i) = (\\Delta A)^{-1}(exp(\\Delta_iA) - I) \\cdot \\Delta_i B(t_i).\\)\nThe final output O = \\((o_1, o_2, ..., o_n)\\) is passed through a neural network to generate the hidden representation h(t):\n\\(H = ReLU(OW_1+b_1)W_2 + b_2, h(t_j) = H(j, :)\\) (11)\nThe resulting matrix H contains hidden representations of all the events in the input sequence, where each row corresponds to a particular event.\nIntensity Function and Log-Likelihood The intensity function of the MHP is given by\n\\(\\lambda(t) = \\sum_{r=1}^R \\lambda_r(t),\\) (12)\nwhere\n\\(\\lambda_r = f_r(\\alpha_r(t - t_j) + w^T h(t_j) + b_r),\\) (13)\nand \\(f_r(x) = \\beta log(1 + exp(x/\\beta_r))\\) is the Softplus function. The log-likelihood function based on the sequence S is given by\n\\(l(S) = \\sum_{j=1}^n log (\\lambda(t_j H_j) - \\int_{t_1}^{ct_n} \\lambda(t H_t)dt \\) (14)\nNote: Here, log-likelihood function will play two important roles:\nWe learn the model parameters by maximizing the log-likelihood across all sequences:\n\\(max \\sum_{i=1}^N l(S_i),\\) (15)\nusing the ADAM optimization algorithm for an efficient solution.\nThen we use the MHP model as an evaluation module, combining it with multi-modal information to assign a comprehensive score that reflects the impact of platform recommendations on the popularity of short videos.\n\\(Likelihood_{MHP} = l(v_i), \\forall v_i \\in V.\\) (16)\nMore tricky details can be found in Appendix 6, please refer to it for more explanation."}, {"title": "Prediction Network", "content": "For each micro-video \\(v_i\\), the output layer in AUG-MMRA is fed a concatenated vector of the feature components, which is then passed through a fully connected layer with weights \\(W_{output} \\in R^{10d \\times d}\\).\n\\(Output = concat([C_i, T_i, C_r, T_r, I])W_{output}.\\) (17)\nAdditionally, for each micro-video, we obtain the recommendation score from SASRec, \\(Scores_{SAS}\\), and the recommendation likelihood \\(Likelihood_{MHP}\\) from MHP.\n\\(Prediction = [Output, Scores_{SAS}, Likelihood_{MHP}]W_{pred}.\\) (18)\nwhere \\(W_{pred}\\) is a trainable parameter matrix. In the model training phase, we employ mean squared error (MSE) as our loss function."}, {"title": "Experiments", "content": "Research Question In this section, we present experiments conducted to evaluate the effectiveness of MUFM on a real-world micro-video dataset, with the aim of addressing the following research questions:\nRQ1: How does MUFM perform compared to existing models and state-of-the-art methods?\nRQ2: What is the contribution of each component of MUFM to its overall performance in MVPP?\nRQ3: How do key hyperparameters affect the model's performance?\nRQ4: What insights can be gained from the results of MUFM?"}, {"title": "Implementation", "content": "Device All our experiments are conducted on a Linux server with 16 NVIDIA A100 Tensor Core GPUs using a multi-thread dataloader."}, {"title": "Evaluation Metrics", "content": "Since our short video popularity prediction is essentially a regression-like task, the degree of task completion is determined by the fit between the predicted popularity parameters and their actual values. We adopt the normalized Mean Square Error (nMSE) as the main parameter to measure the performance, which is defined as follows:\n\\(nMSE = \\frac{1}{N\\sigma_{y_i}^2} \\sum_{i=1}^N (y_i - \\hat{y_i})^2\\) (19)\nwhere N represent the total number of micro-video samples, \\(y_i\\) and \\(\\hat{y_i}\\) are the target and predicted popularity score for the i-th micro-video sample, and \\(\\sigma_{y_i}\\) is the standard deviation of the target popularity.\nIn addition, we measure Spearman's Rank Correlation (SRC) coefficient, Pearson linear correlation coefficient (PLCC), and Mean Absolute Error (MAE) as complementary metrics. SRC, PLCC, and MAE are defined as follows:\n\\(SRC = 1- \\frac{6 \\sum_{i=1}^N d_i^2}{N(N^2 - 1)}\\) (20)\n\\(PLCC = \\frac{1}{N} \\sum_{i=1}^N \\frac{(y_i - \\mu_y)(\\hat{y_i} - \\mu_{\\hat{y}})}{\\sigma_y \\sigma_{\\hat{y}}}\\) (21)\n\\(MAE = \\frac{1}{N} \\sum_{i=1}^N |y_i - \\hat{y_i}|,\\) (22)\nwhere \\(d_i\\) is the rank difference of a micro-video between the prediction and the popularity target. \\(\\sigma_y\\) and \\(\\sigma_{\\hat{y}}\\) stand for the standard deviation of the target and predicted popularity sequences for the i-th micro-video sample.\nA higher SRC value indicates a stronger monotonic correlation between targets and predictions, a higher PLCC value indicates a higher model performance, while a lower nMSE or MAE indicates a more precise prediction of the model. More details of baseline models and experiments are displayed in the Appendix."}, {"title": "Hyper-Parameter Settings", "content": "There are some main parameters in the training process, displayed in Table 2. We adopt the Bayesian Optimization (Jones, Schonlau, and Welch 1998), searching through the parameter hyper-space and find the lowest nMSE when setting [lr, batch_size, wdecay, dropout, a, K, S]=[1e-4, 64, 0.001, 0, 0.2, 10, 20]. We apply the above set of parameters to acquire our result and compare MUFM with other baselines. Hyper-parameter optimization and sensitivity analysis will be discussed in detail in the following paragraphs.\nRunning details: We have set the random seed to 2024 for reproducibility. The best performance is achieved after 6 epochs."}, {"title": "Performance Comparison", "content": "Baseline models To evaluate the superiority of the model, we conduct experiments with 9 competitive baselines of different methods. Our experimental baseline model includes: SVR (Khosla, Das Sarma, and Hamid 2014), HyFea (Lai, Zhang, and Zhang 2020), CLSTM (Ghosh et al. 2016), TMALL (Chen et al. 2016), MASSL (Zhang et al. 2023), CBAN (hin Cheung and man Lam 2022), HMMVED, MTFM and MMRA (Zhong et al. 2024). More details about our baseline model are displayed in Appendix 6, and all baseline code is included in the code appendix.\nPerformance Comparison The performance of various baseline models and our MUFM model on the dataset is presented in Table 3. The results demonstrate that MUFM consistently outperforms all baselines on pMicroLens dataset. These findings confirm the value of constructing a multi-modal pipeline integrated with User Feedback to boost prediction accuracy. Notably, compared to the current state-of-the-art model MMRA, our approach shows significant improvements by incorporating user-targeted recommen-"}, {"title": "Hyper-parameter Analysis", "content": "After initial adjustments, the model's performance showed low sensitivity to wdecay batch_size and dropout. Therefore, we focused on optimizing and analyzing other key parameters: K, S, \\(\\alpha\\), and lr.\nAs shown in Figure 2, more videos aggregated in the retrieval process enable the model to pay more attention to the shared characteristics from similar videos and attain effective information, which ultimately improves the performance of MUFM. The optimal value for S is 0.01. In terms of lr, increasing the learning rate requires more epochs to learn information and achieve the lowest nMSE. The optimal learning rate is found to be le-4.\nFigure 5 illustrates the best nMSE and SRC obtained when training with different values of K and \\(\\alpha\\). The model's performance is relatively insensitive both to the number of frames captured (K) and to the balance between positive and negative attention (\\(\\alpha\\)). MUFM performed best with K = 10 and \\(\\alpha\\) = 0.8.\nOverall, our model demonstrated strong robustness once the value of S was optimized. The best-performing hyperparameters are [lr, batch_size, wdecay, dropout, \\(\\alpha\\), K, S] = [1e-4, 64, 0.001, 0, 0.2, 10, 20]."}, {"title": "Ablation Study", "content": "In this section, we conduct an ablation study on MUFM to assess the impact of three critical components. We create the following variants for evaluation.\nnoSAS: This variant removes the SASRec, thereby eliminating user-targeted recommendations and leaving MHP as the sole recommendation mechanism.\nnoMamba: In this variant, the Mamba model in MHP is removed, eliminating context encoder and dependency capture through selective state space models (SSM).\nTherefore, the predicted recommendation likelihoods are attained from the user reaction sequence using merely the traditional Hawkes Process.\nnoHP: This variant excludes the Hawkes Process(HP) methodology. The predictive algorithm does not follow the Hawkes Process model, but solely depends on Mamba itself, which prevents modeling the phenomena of self-excitation or mutual inhibition between events.\nIn this analysis, we selected the top 50, 100, and 200 videos in the test samples with the highest ground truth popularity scores, as well as the bottom 50, 100, and 200 videos with the lowest scores. We then reported the average predicted popularity scores based on the number of comments. Additionally, we evaluated the entire test dataset using metrics such as nMSE, SRC, and MAE across four different model variants."}, {"title": "Conclusion", "content": "In this work, we propose MUFM, a multi-modal model for MVPP. MUFM introduces a retrieval-augmented framework and a Hawkes process model for the MVPP task. We align the visual audio and textual modalities to find similar videos. Cross-modal bipolar interactions are implied to address the presence of inconsistent information between models, as well as a retrieval interaction enhancement method to capture meaningful knowledge from relevant instances. The mamba-based Hawkes process model provides info on user behavior and improves our model precision. Experiments on the real-world micro-video dataset show our method is effective and outperforms the current state-of-the-art in MVPP."}, {"title": "Appendix 1: Micro Video Retrieval", "content": "The micro video memory repository is a collection of reference pairs, denoted as (frames, text), with each element encoded for efficient retrieval. To generate a retrieval vector for a given micro video Vi, we begin by using BLIP-2, a pre-trained large language model, to analyze the video's content and produce descriptive captions for its frames, resulting in the set \\(C_i = \\{c_1, . . ., c_l \\}\\), where L represents the total number of frames in video Vi.\nFor the audio component, we apply a similar process using the CLAP (Cross-Modal Language-Audio Pre-training) model, which generates descriptive textual annotations for the audio, denoted as ar. These annotations are derived from a predefined set of audio descriptions. The resulting synthetic captions Ci are then combined with the audio descriptions A and the original textual descriptions T\u2081 to form a comprehensive text prompt. This is achieved using the concatenation operator, leading to\n\\(P_i \\coloneqq C_i \\oplus A_i \\oplus T_i.\\) (23)\nThis synthesized text prompt Pi is then processed through a pre-trained semantic extraction model, specifically the UAE-Large architecture, to generate a retrieval vector Ri that encapsulates the key attributes of video V. This process is repeated for each micro video in the memory repository, assigning each one a unique retrieval vector that serves as its identifier within the system.\nThis method unifies the visual, audio, and textual modalities into a cohesive representation, enhancing the model's ability to predict micro video popularity. Additionally, it improves the accuracy and efficiency of content retrieval and analysis in the domain of micro form video dissemination.\nThe Bank Retriever mechanism is designed to identify the Top-S most similar videos from a memory bank by evaluating the similarity scores between the target video and stored instances. The process begins by receiving a query video \\(V_i = \\{v_1, v_2, ..., v_S \\}\\) and searching through the memory bank, which holds a collection of frame-text pairs. The retriever employs a maximum inner product search algorithm to scan the memory bank, selecting the most relevant videos based on their similarity to the query."}, {"title": "Appendix 2: Multimodal Process", "content": "To capture the key features of a micro video Vi, we extract visual, audio, and textual information. We start by selecting a series of key frames, denoted as \\(F_1, F_2, ..., F_k\\), where K represents the total number of frames.\nFor visual feature extraction, we apply a pre-trained visual model. Each frame \\(F \\in R^{H \\times W \\times C}\\) is divided into fixed-size patches and reshaped into a sequence of flattened 2D patches \\(F^* \\in R^{N \\times P^2 \\times C}\\), where (H, W) is the resolution of the original frame, C is the number of channels, (P, P) is the patch size, and N is the number of patches, which serves as the input sequence length for the Vision Transformer (ViT).\nFor audio features, we follow a similar approach using the Audio Spectrogram Transformer (AST). We first"}, {"title": "Appendix 3: Cross-modal Bipolar Interaction", "content": "Aligning audio-visual and textual modalities in micro videos presents challenges due to potential inconsistencies between textual descriptions and video content. To address this, we implement a cross-attention network comprising both positive and negative attention modules, designed to capture the similarities and differences between multi-modal information. The positive attention module focuses on identifying the most consistent features across different modalities,"}, {"title": "Appendix 4: Retrieval Interaction Enhancement", "content": "We focus on extracting valuable insights from relevant instances retrieved to improve micro video popularity prediction (MVPP). To achieve this, we use an aggregation function that combines the comprehensive representations of these instances from the memory bank. This function assigns attention scores based on the normalized similarity scores obtained during retrieval, using them as weights to construct the retrieved embeddings \\(X^r_i\\) and \\(X^r_t\\). Instances with higher similarity scores are prioritized, highlighting their relevance to the target micro video.\nSimilarly, we calculate Tr and Cr, and then derive the final representations Ti and C using an attentive pooling strategy. To capture the popularity trends of the retrieved instances, we encode the label information through linear layers and aggregation, resulting in the retrieved label embedding Li. Finally, AUG-MMRA integrates all features from Tr and C to model cross-sample interactions. These feature interactions are constructed as\n\\(I = [\\Phi(C_i, C_T), \\Phi(C_i, T), ..., \\Phi(T_i, L)],\\) (37)\nwhere \\(I\\) denotes the process of inner products."}, {"title": "Appendix 5: Multimodel Information Based SASRec", "content": "In our micro-video popularity prediction model, we adopt SASRec (Self-Attention Sequence Recommender), a Transformer-based architecture, to capture users' sequential behavior patterns. SASRec is crucial for modeling both short-term and long-term user preferences, incorporating multimodal item features to enhance recommendation accuracy.\nOur model processes various modalities, including video frames (Fi), cover images (Ii), and titles (Ti), which are extracted using pre-trained encoders. The user interaction histories are fed into the SASRec module, where self-attention layers encode sequential user behaviors, producing a sequence-based representation of user-item interactions."}, {"title": "Appendix 6: Mamba Architecture", "content": "Denote the sequence \\(S = \\{(t_1,r_1), (t_2, r_2), ..., (t_n, r_n)\\} \\)as the temporal users' reaction sequence, where R is the number of events. Let \\(\\Delta_i = t_i - t_{i-1}\\) represent the temporal differences, with \\(\\Delta_1 = t_1\\) for convention, hence the temporal sequence is represented by\n\\(\\Delta = (\\Delta_1, \\Delta_2, ..., \\Delta_n)\\)\nAdditionally, let ri be the one-hot vector of the events. Inspired by the discretization\n\\(h(t + \\Delta t) = Ah(t) + Bu(t)\\) (41)\nthe hidden states differ by time gap \\(\\Delta t\\) are related by the formula, we put the temporal differences into the equation directly to construct our Mamba Hawkes Process (MHP) structure. Now we state our construction.\nLet We be the event embedding matrix with dimensions D \u00d7 R, where D is the dimension of the hidden layers of Mamba blocks. The event embedding is defined as \\(X_{t_i} = r_i(W_e)^T\\)\n\\((X_{t_1}, X_{t_2}, ..., X_{t_n}) = (r_1, r_2, ..., r_n)(W_e)^T\\)\nIn the Mamba architecture, the matrices A, B and C are time-dependent and are obtained by linear projection from Xt. However, for the Hawkes Process, the approach is different, as it requires the use of temporal features. Specifically, we make B and C time-dependent, and A = (\u03941, \u03942, ..., \u0394\u03b7) is defined by the temporal differences as"}, {"title": "Appendix 8: Baseline Models", "content": "Here we elaborate how base models works.\nSVR: Uses Gaussian kernel-based Support Vector Regression (SVR) to predict micro-video popularity.\nHyFea: Employs the CatBoost tree model, leveraging multiple features (image, category, space-time"}]}