{"title": "Benchmarking LLMs in Political Content Text-Annotation: Proof-of-Concept with Toxicity and Incivility Data", "authors": ["Basti\u00e1n Gonz\u00e1lez-Bustamante"], "abstract": "This article benchmarked the ability of OpenAI's GPTs and a number of open-source LLMs to perform annotation tasks on political content. We used a novel protest event dataset comprising more than three million digital interactions and created a gold standard that includes ground-truth labels annotated by human coders about toxicity and incivility on social media. We included in our benchmark Google's Perspective algorithm, which, along with GPTs, was employed throughout their respective APIs while the open-source LLMs were deployed locally. The findings show that Perspective API using a laxer threshold, GPT-40, and Nous Hermes 2 Mixtral outperform other LLM's zero-shot classification annotations. In addition, Nous Hermes 2 and Mistral OpenOrca, with a smaller number of parameters, are able to perform the task with high performance, being attractive options that could offer good trade-offs between performance, implementing costs and computing time. Ancillary findings using experiments setting different temperature levels show that although GPTS tend to show not only excellent computing time but also overall good levels of reliability, only open-source LLMs ensure full reproducibility in the annotation.", "sections": [{"title": "1 Introduction", "content": "Despite the advantages of digital social media and the Internet, in a broader sense, for collective action and political engagement, an increase in incivility and toxicity in digital interactions has also been observed, becoming a recurrent research topic in recent years (e.g., Schmidt et al., 2024; Kim et al., 2021; Salgado et al., 2023, among others). In particular, digital interactions and online political discussions seem to be good grounds for incivility and toxicity (Schmidt et al., 2024).\nThe rapid advancements in AI capabilities since the early 2020s have brought about a paradigm shift in the landscape of computational social science research and the Natural Language Processing (NLP) field. These breakthroughs have revolutionised the traditional text-as-data approach in social sciences, which relied on machine learning using dictionaries, different forms of topic modelling, and supervised, semi-supervised, or unsupervised applications (Watanabe and Zhou, 2022, see also Gonz\u00e1lez-Bustamante, 2023). By leveraging the zero and few-shot capabilities of Large Language Models (LLMs) and deep learning, we can effectively handle large volumes of content that would otherwise necessitate substantial manual effort or augment and supplement traditional machine learning applications. This novel approach not only provides a distinct avenue to advance our comprehension of toxicity and incivility in digital interactions and political discourse in general but also showcases the potential of LLMs in shaping the future of computational social science research.\nThat landscape is changing, and at least in massification, an apparent leap forward occurred at the end of 2022 because of the adjusted version of GPT-3 with 175B parameters (Brown et al., 2020). Then, a number of more recent versions, such as GPT-4, in 2023, have shown outstanding perfor-"}, {"title": "2 Toxicity, Incivility and LLMs for Annotation Tasks", "content": "This section briefly reviews related work on toxicity and incivility and open-source LLMs for annotation tasks. Concerning the former, despite most literature tends to understand the use of disrespectful language as a common feature, there are a variety of conceptualisations that could include vulgarity, profanity, identity-based attacks, and hate speech, among others (Chen, 2017; Kim et al., 2021; Schmidt et al., 2024; Schmidt and Wiegand, 2017; Stoll et al., 2023).\nThe text-as-data approach appears as a relevant cornerstone due to the limited number of texts that human coders can annotate, considering time and resources (Schmidt et al., 2024). In this context, the Perspective API, developed by Jigsaw and Google's Counter Abuse Technology team, is one of the top-shelf options for categorising and classifying toxicity and incivility in the digital sphere. This algorithm was trained on millions of comments from Wikipedia, The New York Times and a variety of other sources labelled by crowdsource raters using distilled Bidirectional Encoder Representations from Transformers (BERT) models into Convolutional Neural Networks (CNNs). Perspective API has been used to moderate media content. For example, Goyal et al. (2022) highlighted the Perspective API as a machine learning solution in the content moderation workflow to tackle online harassment.\nIn academic research, to the best of our knowledge, the algorithm has been used to detect toxicity and uncivil comments on Twitter (Hopp et al., 2020; Theocharis et al., 2020; Orchard et al., 2024; Schmidt et al., 2024), Facebook (Hopp et al., 2020; Kim et al., 2021; Schmidt et al., 2024), news comments (Orchard et al., 2024; Schmidt et al., 2024) and Wikipedia content (Pavlopoulos et al., 2020).\nThe contribution of this paper is benchmarking Google's Perspective algorithm, OpenAI's GPTs and open-source LLMs as classifiers of toxicity, considering a gold standard based on human annotation. Indeed, LLMs are being used in social science in disciplines such as political science to study misinformation, replacing manual processes even instead of traditional NLP approaches and gaining insight into different dimensions of political speech (Linegar et al., 2023). As indicated above, researchers have been using GPTs through OpenAI's API for a variety of tasks (Gilardi et al., 2023; He et al., 2024), and despite different concerns related to reproducibility, privacy and openness of this pay-per-use form, this way tends to offer resources beyond those usually available to the average researcher in social sciences fields, it is straightforwardly to deploy without excessive computational requirements and also GPT-3.5 onwards, especially versions as from GPT-4, tend to excel on a number of tasks and outperform other options (Linegar et al., 2023).\nHowever, recent studies have shown that open-"}, {"title": "3 Data and Methods", "content": "We used a novel dataset that comprises more than 3.5 million messages posted on Twitter, rebranded as X, about protest events in Argentina (\u03b7 = 551,761) and Chile (n = 3,125, 254).\nThe Argentinian protests were against coronavirus and judicial reform measures during August 2020, while Chilean messages cover the social outburst stemming from protests against the underground fare hike in October 2019. We scraped all the messages using hashtags in both countries that reached more than 50,000 posts during those months. We used the now deprecated academic research access to the Twitter API and downloaded the messages in JavaScript Object Notation (JSON) format.\nIn order to guarantee the quality of our benchmarking, we created a gold standard using a balanced sample of messages from both countries. First, we classified toxicity in the full corpora using the Perspective algorithm, which implied 2,411 hours of computing, and the entire process was deployed on a Raspberry Pi 5, a small computer board with an ARM CPU and meagre power consumption. Then, we drew a random, balanced sample of 1,000 messages. This sampling process considered the level of toxicity using the Perspective score, which is basically a predicted probability of the algorithm's classification. Therefore, we balanced the sample by splitting it by country, considering 500 messages from Argentina and 500 from Chile, and then used quintiles of toxicity: Q1 from 0.00 to 0.20; Q2 from 0.21 to 0.40; Q3 from 0.41 to 0.60; Q4 from 0.61 to 0.80; and Q5 from 0.81 to 1.00.\nThe toxicity was manually annotated by a team of five high-skilled human coders using Labelbox (2024), an online platform that complies with the most relevant privacy and protection regulations. We used a binary codification performed using the core definitions of Perspective for toxicity as follows: Classify the category of the comment as either TOXIC or NONTOXIC. TOXIC: Rude, disrespectful, or unreasonable comments that are likely to make someone leave the discussion or stop sharing their perspective. NONTOXIC: Civil or nice comments that are unlikely to discourage conversation. Each message was hand-coded by two different coders of the team, therefore, we have two ground-truth labels for each sample observation. The inter-coder reliability was excellent: Cohen's \u03ba 0.944 and Krippendorff's a with a bootstrap of", "sections": [{"title": "3.1 Data and Gold Standard", "content": "We used a novel dataset that comprises more than 3.5 million messages posted on Twitter, rebranded as X, about protest events in Argentina (\u03b7 = 551,761) and Chile (n = 3,125, 254).\nThe Argentinian protests were against coronavirus and judicial reform measures during August 2020, while Chilean messages cover the social outburst stemming from protests against the underground fare hike in October 2019. We scraped all the messages using hashtags in both countries that reached more than 50,000 posts during those months. We used the now deprecated academic research access to the Twitter API and downloaded the messages in JavaScript Object Notation (JSON) format.\nIn order to guarantee the quality of our benchmarking, we created a gold standard using a balanced sample of messages from both countries. First, we classified toxicity in the full corpora using the Perspective algorithm, which implied 2,411 hours of computing, and the entire process was deployed on a Raspberry Pi 5, a small computer board with an ARM CPU and meagre power consumption. Then, we drew a random, balanced sample of 1,000 messages. This sampling process considered the level of toxicity using the Perspective score, which is basically a predicted probability of the algorithm's classification. Therefore, we balanced the sample by splitting it by country, considering 500 messages from Argentina and 500 from Chile, and then used quintiles of toxicity: Q1 from 0.00 to 0.20; Q2 from 0.21 to 0.40; Q3 from 0.41 to 0.60; Q4 from 0.61 to 0.80; and Q5 from 0.81 to 1.00.\nThe toxicity was manually annotated by a team of five high-skilled human coders using Labelbox (2024), an online platform that complies with the most relevant privacy and protection regulations. We used a binary codification performed using the core definitions of Perspective for toxicity as follows: Classify the category of the comment as either TOXIC or NONTOXIC. TOXIC: Rude, disrespectful, or unreasonable comments that are likely to make someone leave the discussion or stop sharing their perspective. NONTOXIC: Civil or nice comments that are unlikely to discourage conversation. Each message was hand-coded by two different coders of the team, therefore, we have two ground-truth labels for each sample observation. The inter-coder reliability was excellent: Cohen's \u03ba 0.944 and Krippendorff's a with a bootstrap of"}, {"title": "3.2 LLMs for Text-Annotation", "content": "During the gold standard creation process, we applied Perspective API to the whole corpus, therefore, we can compare it with the results of LLMs as classifiers for text annotation. Thus, we ran zero-shot classification tasks to identify toxicity using proprietary, closed-source OpenAI's GPTs and a number of open-source LLMs. We used the main recent GPTs, namely GPT-40, GPT-40 mini, GPT-4, GPT-4 Turbo and GPT-3.5 Turbo. We have not included o1-preview and o1-mini, released recently on September 12, 2024, since they are not fully available to all API users at the moment of writing this paper. On the other hand, we locally deployed a selection of open-source LLMs with the minimum temperature level to ensure reproducibility (Gruber and Weber, 2024, see also Weber and Reichardt, 2023).\nPrior to this study, we ran a preliminary benchmark using Perspective API as a proxy of ground-truth labels instead of the gold standard elaborated by human annotators. In these preliminary analyses, available in the Appendix, we used Perspective score with a laxer threshold of 0.55 and a standard of 0.70 and tenfold cross-validated performance and goodness-of-prediction indicators to obtain averaged metrics across the folds for each classifier in order to smooth performance fluctuations. This pre-proof-of-concept (pre-PoC) allowed us to exclude some models that only focused on programming tasks or were explicitly designed for embeddings. We privileged general-purpose models and those that provided a zero-shot classification straightforwardly following our prompt strategy rather than a chain-of-thought, which generally took over one minute per observation. In addition, we prioritised the selection of the state-of-the-art (SOTA) open-source LLMs, though we also included some models prior to Llama 3.1 release that could show good performance and trade-off considering computing time in our pre-PoC results. Therefore, we selected five SOTA open-source LLMs (i.e., Llama 3.1, Hermes 3, Gemma 2 9B, Gemma 2 27B and Mistral\nNeMo) and five models that could be slightly outdated (i.e., Nous Hermes 2 Mixtral, Nous Hermes 2, Mistral OpenOrca, Orca 2 and Aya) taking into account the accelerated pace of generative AI and NLP.\nOur prompt strategy was based on the labelling process by human coders and the core definitions of Perspective and Google. It comprised a query with the same message given to the coders as a system message in order to give context for the classification task. Along with providing texts of our balanced sample, we also listed the categories for the task as follows: Respond with only the category (TOXIC or NONTOXIC). Do not provide any additional analysis or explanation.\nIn sum, we ran a zero-shot classification considering our prompt strategy using five OpenAI's GPTs (i.e., GPT-40, GPT-40 mini, GPT-4, GPT-4 Turbo and GPT-3.5 Turbo), five SOTA open-source LLMs (i.e., Llama 3.1, Hermes 3, Gemma 2 9B, Gemma 2 27B and Mistral NeMo) and five ones before the release of Llama 3.1 (i.e., Nous Hermes 2 Mixtral, Nous Hermes 2, Mistral OpenOrca, Orca 2 and Aya) to benchmark all of them against Perspective API and our gold standard based on human annotations."}, {"title": "3.3 Reproducibility and Temperature Experiments", "content": "We set the temperature at a minimum to ensure our analysis's reproducibility and tried to avoid LLMs' hallucinations, as already mentioned. Higher temperatures make LLMs answer more creatively, and although there is no agreement about the role of this parameter on models' hallucinations, temperature increases are generally tested in hallucination evaluation (see Hao et al., 2024). Thus, we ran some additional classification tasks for the GPTS and open-source models with the best performance in order to test not only reproducibility but also how the randomness of temperature could influence the outcome.\nConsequently, we ran a zero-shot iteration with the temperature at the minimum and then two additional classifications considering low and high-temperature settings at 0.25 and 1.00, respectively,"}]}, {"title": "4 Results", "content": "Following the test by Reiss (2023). It is relevant to note that we set the same random number seed for response generation in all our tests. This allowed us to obtain bootstrapped Krippendorff's a estimates for an iteration under the same settings as the original zero-shot classification and for different temperature levels to test if the lower confidence interval is above the 0.80 threshold considered reliable (Krippendorff, 2019). This test also operates as a robustness check of our main analyses."}, {"title": "4.1 Benchmark and Error Rate Analysis", "content": "We calculated a variety of standard performance and goodness-of-prediction indicators for Perspective API at a standard of 0.70, the laxer threshold of 0.55 and the zero-shot classifications using the abovementioned GPTs and open-source LLMs. The performance metrics are: (i) accuracy that reports the proportion of correct predictions of the particular classifier in comparison with the human gold standard; (ii) precision that shows the ability of the classifier to identify positive predicted values to identify false negatives; (iii) recall or sensitivity that shows the proportion of correct classifications among true-positive cases; and (iv) F1-score, a combination of precision and recall. Perspective API, Google's model distilled from BERT family models, with a laxer cut-off threshold of Perspective score at 0.55, outperforms the GPTs and open-source LLMs we have tested, and it is the closest result to our gold standard with human coders. This finding offers interesting insights since the same model using a standard threshold of 0.70 was ranked at the bottom of the table, presenting one of the lowest performances with an F1-score slightly above 0.70, which is still acceptable. This suggests that the Perspective score as a probability measure tends to be too stringent and should be considered carefully when classifying messages.\nThen GPT-40 and Nous Hermes 2 Mixtral, with 47B parameters, show the highest F1-score in classifying toxicity. GPT-4-o was the most advanced, flagship OpenAI's model available until September 12, 2024, when ol-preview and o1-mini were released, though they are not widely available in the API yet. On the other hand, Nous Hermes 2 Mixtral is an open-source LLM from Nous Research trained on Mixtral and over GPT-4 synthetic data. The F1-score and accuracy of these models are above 80%. While Nous Hermes 2 Mixtral has a large number of parameters, making it challenging to deploy it on standard computers because of RAM requirements, GPT-40 is deployed directly through the OpenAI's API, however, as it is a proprietary model could present potential drawbacks already discussed."}, {"title": "4.2 Performance, Parameters and Computing Time", "content": "Figure 2 presents the relationship between each classifier's Fl-score, the number of parameters in the case of the open-source LLMs and computing time. While no statistically significant relationship exists between time and performance, the number of billions of parameters is significant (p = 0.003). Overall, the models with more parameters tend to show higher performance metrics.\nIn this sense, it is possible to identify some interesting cases, such as Nous Hermes 2 and Mistral OpenOrca, which exhibit good performance despite the lower number of parameters. Both also show an average time performance of 2.350 and 2.349 seconds per message, respectively. These values are considerably lower than Aya's, with 35B parameters and 4.137 seconds per message. It is important to consider that the average human annotation was 15.125 seconds, including labelling and revision time. Perspective API classification, for example, took an average of 1.175 seconds per observation.\nIn this context, the low average computing time of all GPTs is remarkable: the quickest was GPT-40 mini with only 0.511 seconds per observation, while the slower was GPT-4 with 0.852 seconds, which is still 5x quicker than the average open-source LLMs computing time. In sum, open-source LLMs are almost 6x faster than human coders, while Perspective API and GPTs are even quicker: nearly 13x and 24x, respectively."}, {"title": "4.3 Reproducibility and Temperature", "content": "We ran some experiments to test not only the reproducibility but also the influence of different temperature levels on the annotation task, iterating the models with the best performance. Figure 3 shows the 1,000 bootstrapped Krippendorff's a estimates for the original zero-shot classification iteration at the minimum temperature and iterations considering different temperature levels (i.e., 0.25 and 1.00).\nWhile GPTs never ensure entire reproducibility, their reliability is relatively high. Surprisingly, there are no extreme fluctuations setting different temperature levels, except for the case of GPT-4, which suggests that this model should not be used for this type of task. On the other hand, although open-source LLMs tend to show lower reliability under different temperature settings, these models ensure complete reproducibility at the minimum temperature."}, {"title": "5 Discussion and Limitations", "content": "This paper offers a benchmarking of Perspective API distilled from BERT family models, some OpenAI's GPTs and open-source LLMs for annotation and classification tasks on political content, specifically toxicity levels associated with online political discussions during events of protests. Although the models with better performance for this task are Perspective API using a laxer threshold, GPT-40 and Nous Hermes 2 Mixtral with 47B parameters, all the models tested showed F1-scores above 0.70, even some outdated versions such as GPT-3.5 Turbo. It is eye-catching that two of the best options evaluated were deployed locally. Perspective API was executed on a Raspberry Pi 5, allowing a striking carbon footprint reduction of 96% compared to if we had run it on a standard cloud computing service, and Nous Hermes 2 Mixtral was deployed locally using the Ollama server.\nIn the case of open-source LLMs, there is a trade-off between the number of parameters, computing time and model performance. Some interesting cases are Nous Hermes 2 and Mistral OpenOrca since both, with a small number of parameters, are able to classify toxicity with good accuracy and faster than larger models. This confirms the possibilities that some small models deployed locally with low costs for classification tasks could offer.\nIn this context, although GPTs surpass other models in terms of computing time, being 24x faster than human coders and more than 5x more rapid than average open-source LLMs, our experiments setting different levels of temperature showed that only through open-source models deployed locally at a minimum level of temperature is possible to ensure reproducibility in the annotation output.\nAlthough a prominent contribution of this work is that it offers insights into using LLMs for annotation in social science topics, it is relevant to note some limitations. For example, exploring some applications beyond zero-shot classification, such as classification tasks involving few-shot ontologies or chain-of-thought, is possible. In this sense, a potential avenue is incorporating additional context information and prompt variation in the pipeline to improve the classification's performance, as shown by Roy et al. (2023).\nIn sum, this work not only offers insights into the performance of different GPTs and open-source LLMs in annotation tasks of political content and toxicity, specifically offering some guidelines for the application, reproducibility and replicability of these models but also opens new avenues of research on the dynamic's toxicity and incivility in political phenomena in the digital sphere and for different applications on political speech in broader topics."}, {"title": "A Appendix", "content": "The toxicity classification task with the Perspective API was executed locally on a Raspberry Pi 5 with an ARM Cortex-A784 Core and 8GB of RAM. Raspberry Pi OS based on Debian GNU/Linux was used.\nThe open-source LLMs were deployed locally on an Intel Core i9-14900K CPU, NVIDIA GeForce RTX 4070 Super Windforce OC 12GB GPU and 64 GB of RAM (two memories DDR5-4800 of 32GB each). Windows Subsystem for Linux v2.1.5.0 and Ollama v0.1.44 were used for the preliminary analyses, and v0.3.10 for the final ones. It is relevant to note that Ollama suggests having 8GB of RAM to run 7B models, 16GB for 13B and 32GB for the 33B models. The largest models that we ran had almost 47B. An average laptop could run only small and medium models below 7B and between 7 and 13B parameters."}]}