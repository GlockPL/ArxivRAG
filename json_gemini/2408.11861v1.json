{"title": "Speaking the Same Language: Leveraging LLMs in Standardizing Clinical Data for AI", "authors": ["Arindam Sett", "Somaye Hashemifar", "Mrunal Yadav", "Yogesh Pandit", "Mohsen Hejrati"], "abstract": "The implementation of Artificial Intelligence (AI) in the healthcare industry has garnered considerable attention, attributable to its prospective enhancement of clinical outcomes, expansion of access to superior healthcare, cost reduction, and elevation of patient satisfaction. Nevertheless, the primary hurdle that persists is related to the quality of accessible multi-modal healthcare data in conjunction with the evolution of AI methodologies. This study delves into the adoption of large language models to address specific challenges, specifically, the standardization of healthcare data. We advocate the use of these models to identify and map clinical data schemas to established data standard attributes, such as the Fast Healthcare Interoperability Resources. Our results illustrate that employing large language models significantly diminishes the necessity for manual data curation and elevates the efficacy of the data standardization process. Consequently, the proposed methodology has the propensity to expedite the integration of AI in healthcare, ameliorate the quality of patient care, whilst minimizing the time and financial resources necessary for the preparation of data for AI.", "sections": [{"title": "1 Introduction", "content": "Using clinical datasets from disparate sources with different schema (data structures) and data dictionaries (information about fields, data types, and their meanings) can pose several challenges in AI development:\n\u2022 Data Inconsistency\nDifferent sources may have different conventions for documenting the same information. This inconsistency can lead to data mishandling, which in turn jeopardizes the accuracy of AI models.\n\u2022 Feature Mismatch\nIf different datasets use different features, or represent identical features differently, it can be difficult to reconcile them into a uniform model. Even similar types of data can be represented differently, making harmonization challenging.\n\u2022 Data Quality\nDatasets from different sources often have different levels of quality and reliability. Some datasets may contain more errors or inaccuracies than others, which can affect the performance of the Al models.\n\u2022 Information Loss\nDuring the process of standardizing or transforming the data to fit into a single schema, some important information could be lost.\n\u2022 Higher Complexity\nManaging data from disparate sources increases the overall complexity of data pre-processing. This also means increased time and resources spent on data wrangling.\n\u2022 Legal and Ethical Issues\nThe use of disparate data sources raises concerns about privacy and data security. Furthermore, the algorithms developed using the data can be unknowingly biased due to the variations in the quality of data from these different sources.\n\u2022 Interoperability Issues\nDifferent healthcare information systems tend to use different data standards, creating compatibility issues and limiting how freely data can be exchanged between systems.\nUltimately, such challenges may compromise the validity, generalizability, and reliability of AI solutions in the clinical field. This emphasizes the importance of a robust, harmonized data sourcing and integration strategy in clinical AI development.\nPrevious research has extensively explored the use of Al in clinical data standardization, primarily focusing on rule-based methodologies and conventional machine learning techniques [1, 2]. These approaches have paved the way for notable advancements in handling structured data. However, they often fall short when dealing with complicated domain-specific schemas, which constitute a significant portion of real-world clinical datasets. Techniques like natural language processing have been applied to extract information from unstructured text [3, 4] or data cleansing [5]. Yet, the application of Large Language Models (LLM) has been relatively under-explored in the current literature. The potential of these models for semantic understanding and context-aware data mapping suggests they could offer substantial improvements in the field of clinical data standardization.\nThis work is an attempt to leverage LLMs for mapping any raw dataset to a clinical data standard; HL7 Fast Healthcare Interoperability Resources (FHIR) [6]. We leverage a zero/few-shot learning approach to achieve data mapping for standardization.\nNote: In course of this work, we have used only the clinical data column names and corresponding data dictionaries (which defines the columns, their data types, applicable code values etc.) where available,"}, {"title": "1.1 A Brief Overview of Data Standardization for Clinical Data", "content": "The adoption of data standards for clinical data has become increasingly important with the rise of AI applications in healthcare. To develop accurate and effective Al models for tasks like diagnosis, prognosis, and treatment recommendations, high-quality structured clinical data is required. However, clinical data has historically been challenging to standardize due to its complexity and heterogeneity.\nIn recent years, FHIR has emerged as a leading standard for clinical data exchange. FHIR provides a common framework and set of APIs for representing and sharing clinical data in a standardized way. Some key benefits of FHIR include:\n\u2022 Structured data format based on resources with common fields for clinical concepts like patients, conditions, medications, etc. This enables integration and analysis across datasets.\n\u2022 Modular components can be used in flexible ways to represent various clinical workflows. This facilitates interoperability across systems.\n\u2022 Modern web standards and APIs for efficient data access and exchange.\n\u2022 Active open source community with rapid evolution of specifications.\nFHIR acts as a bridge for AI, allowing it to extract structured clinical data from electronic health records and other healthcare systems in a consistent format. This helps address the \"data wrangling\" challenges that often dominate healthcare AI projects. With more adoption of FHIR, higher-quality datasets will become available for developing, evaluating, and deploying AI algorithms."}, {"title": "2 Tasks and Data", "content": "This study utilized 14 clinical datasets (details on access requirements for each dataset can be provided upon request). The datasets encompassed a wide range of disease areas, including neurology, respiratory, ophthalmology, and insurance claims. The size and complexity of the datasets varied considerably."}, {"title": "2.1 Data", "content": "This study utilized 14 clinical datasets (details on access requirements for each dataset can be provided upon request). The datasets encompassed a wide range of disease areas, including neurology, respiratory, ophthalmology, and insurance claims. The size and complexity of the datasets varied considerably."}, {"title": "2.1.1 Ground-truth Generation and Curation", "content": "Ground-truth generation employed a semi-supervised approach leveraging LLMs. The resulting FHIR mapping was subsequently reviewed and refined by a team of four data curators with expertise in FHIR."}, {"title": "2.2 Methods", "content": "In this section, we describe the motivation behind our system, its overview and functionalities, the prompt engineering process."}, {"title": "2.2.1 System Inspiration", "content": "Our goal is to explore the ability of GPT-3.5 to provide a framework for defining a consistent data terminology across various datasets and institutions by utilizing Retrieval-Augmented Generation (RAG) [21]. This standardization is essential for interoperability, enabling healthcare organizations to exchange patient data seamlessly, researchers to aggregate and analyze data consistently, and developers to create applications that work with healthcare information reliably."}, {"title": "2.2.2 FHIR Standard and Clinical Data Mapping to FHIR", "content": "The core foundation of FHIR Standard is a set of modular components called \"Resources\". A Resource represents instance-level representation of any healthcare entity. All resources have a few elements (also called attributes) in common:\n\u2022 An identifier for the resource - typically a URL that defines where the resource is found\n\u2022 Common metadata"}, {"title": "2.2.3 System Overview", "content": "The system takes the data dictionary of a target dataset as input and generates an output mapping individual data elements that conforms to the FHIR standard, resulting in a standardized format for the dataset. This data dictionary typically includes definitions, descriptions, and explanations of terms, fields, and variables used in the dataset.\nAs shown in Figure 2, our system combines a retriever system, which extracts relevant document snippets from FHIR, and GPT-3.5, which produces answers using the information from those snippets. In essence, RAG helps the model to search and retrieve contextual information from FHIR to improve its responses. Specifically, the FHIR documents are split into text chunks using recursive character text splitting. The chunks are then embedded in a vector space using the OpenAI text-embedding-ada-002 embedding engine and stored in the Facebook AI Similarity Search (FAISS 1.7.4) vector database. The FAISS[23] vector database is used to find the k-most similar chunks to a given query at the query time. The original query, combined with the retrieved chunks is compiled into a prompt and passed to the GPT-3.5 for generating the answer. This provides the LLM with additional information that contains factual data, which can help to improve the quality of its responses. For vector similarity search we have used a chunk size of 2000, chunk overlap of 200."}, {"title": "2.2.4 Context for RAG", "content": "For RAG, we have prepared content based on the FHIR standard definitions that are available at the HL7 FHIR website. We have taken the resource, element and the corresponding descriptions of the resource-element pair as information that we have subsequently embedded into the vector database using chunking, for a subsequent retrieval during mapping, this retrieved context is passed along with the input data dictionary column and associated information to the LLM for an output FHIR mapping."}, {"title": "2.2.5 Prompt Engineering", "content": "Prompt engineering is an emerging field that focuses on creating and fine-tuning prompts to maximize the effectiveness of LLMs for various applications. It involves a comprehensive set of techniques that boost interaction with LLMs, enhance their safety, integrate domain knowledge, and work with external tools.\nIn this work we have taken an incremental approach with prompt engineering to create FHIR mapping. Initially we set out with basic prompting with minimal information in the prompt (viz. just the input data dictionary row and some simple instructions to map to FHIR), and then iterate over the input data dictionary sets after carefully examining output FHIR mapping generated. Our final prompt has the following structure: Definition of User Role, Initial Instructions, Placeholder for Retrieved Context from RAG, Example (one-shot learning example), Placeholder for data dictionary Input, Direction for Output Format, and Final Set of Instructions. Of these, we noticed the biggest change in output results occurred because of variations in Final Set of Instructions, as well content of the data dictionary."}, {"title": "2.3 Preliminary Evaluation", "content": "Our model predicts the structures for the data dictionary of a dataset. Each structure consists of several metadata blocks, with the first block usually called the resource. We evaluated the performance of our model based on the number of matched metadata blocks between the predicted structure and the ground-truth. We evaluated the performance for top-k = 20.\nA predicted structure is categorized as an 'Absolute Match' if all of its blocks align with those in the ground-truth. If the resource aligns but some or all other metadata elements do not, this leads to a 'Partial Match'. Alternatively, if none of the blocks match, or if the resource does not match even when"}, {"title": "Speaking the Same Language: Leveraging LLMs in Standardizing Clinical Data for AI", "content": "other block do, the predicted structure is considered a 'Mismatch'. Partial Score and Match Score are then defined as below:\n$Score = \\frac{S+P}{N} \u00d7 100$\n$P = \\sum_{i=1}^{P-SK} \\frac{intersection(pred_i, gt_i)}{union(pred_i, gt_i)}$\nWhere S and N represent the quantity of absolute matches and the total structures in the dataset dictionary, respectively. P is the fraction of matched blocks in all 'Partial Match' structures. predi and gti correspond to the predicted and ground-truth for structure i, respectively. K is the number of all 'Partial Match' structures. Moreover, we also calculate a Resource Match Score, which is calculated based on instances where the resources matched between ground-truth and Predicted results. Similar to Partial Score this is also a percentage value.\n$Resource Match Score = \\frac{S+K}{N} \u00d7 100$\nWhere S, K, and N represent the quantity of absolute matches, the quantity of 'Partial Match' structures, and the total structures in the dataset dictionary, respectively."}, {"title": "2.4 Observations", "content": "Table 4 presents the results for top-k = 20, averaged over 10 mapping iterations to account for the variations in results in individual mapping run.\nThe model exhibited strong overall performance with a mean score of 73.54 (SD = 0.16) and a low standard deviation of 0.11 for the resource match score (94.52). This consistency suggests that the model performs reliably across various datasets and exhibits minimal variability between runs.\nThe resource match score, which evaluates the alignment between predicted resources and the ground-truth, shows variability across datasets. While some datasets demonstrate high resource match scores (e.g., ADNI_DOD with 97.20), others exhibit lower alignment (e.g., CERA with 89.8). The consistency in"}, {"title": "Speaking the Same Language: Leveraging LLMs in Standardizing Clinical Data for AI", "content": "resource match scores, as indicated by small standard deviations, suggests that the model maintains a relatively stable performance in resource prediction across different datasets.\nsome datasets exhibit notably high scores, such as ADNI_DOD (77.93), COVASTIL (78.05), BIOFINDER1 (77.09), and BLAZE (79.59), while others, like ADNI (51.31), LUNG_PET_CT_DX (52.86) display lower performance. The standard deviations accompanying these scores provide insight into the consistency of the model's performance, indicating relatively small variations across individual runs. However, it's essential to note that while the model maintains consistency, there remains variability in its efficacy across different datasets, as evident in the significant deviation in scores, especially for datasets like LUNG PET CT DX, NLST, LIDC-IDRI, and CERA. This could be due to a number of factors, such as the quality of the data dictionary, and the specific FHIR profiles used. An informative and comprehensive dictionary could equip the LLM to interpret even cryptic column names effectively, facilitating better mapping and potentially mitigating performance variability. The variability in scores, highlights the importance of understanding dataset-specific characteristics that may influence the model's ability to accurately predict FHIR Mappings.\nFurther analysis is needed to understand the factors that contribute to the model's performance on different datasets. This could involve examining the characteristics of the datasets, such as the type of data, and the quality of the data dictionary."}, {"title": "3 Future Work", "content": "For our future work we have identified a few research areas:\n\u2022 Enriching the content for context retrieval, using FHIR examples, LLM generated content etc.\nFind out reason for lower score for certain datasets and improve the score for different datasets to get a more uniform score.\n\u2022 Work with different LLMs to have a comparative analysis of performance.\n\u2022 Work with different RAG methodologies (HyDE [24], Reranking etc.) to have a more in-depth understanding of how they are impacting the score."}, {"title": "4 Conclusion", "content": "In this work, we have explored a methodology of producing FHIR mapping utilizing the inherent parameterized knowledge available in LLMs, further bolstered by external non-parameterized knowledge sourced from curated documentation of FHIR Standards. Utilizing RAG with FAISS, relevant context information is retrieved and passed to the LLM during the inference process. The results indicate that this approach is a viable method for producing FHIR mapping for data elements from clinical datasets.\nThe distinct advantages of employing LLMs for FHIR mapping, in comparison to manual approaches, are as follows:\n1. Time Efficiency: LLMs can process and map data at a significantly faster rate than manual methods, leading to time savings in the mapping process. As an example, if it takes an expert 1 minute to map a single data field to FHIR, it would take 160 hours to map the total 9600 fields that we have mapped in the results. In our approach using LLM, we have been able to create that mapping in under 10 minutes. Usually for an expert, it will take more than a minute, considering time required for cross-referencing definition of a field, or further research for picking one resource or element over others. This is a significant advantage.\n2. Cost of Labor: The automation capabilities of LLMs reduce the need for extensive labor, thereby cutting down the costs associated with manual mapping. We explored Amazon Mechanical Turk pricing, which varies between $0.02 - $0.08 per object (in this case, we can assume an object is a single field and its FHIR mapping), at that rate our collection of 9600 fields would have cost $192 - $768 [25]. This is significantly higher than using OpenAI GPT [26] and Embedding Models."}, {"title": "Speaking the Same Language: Leveraging LLMs in Standardizing Clinical Data for AI", "content": "3. Expertise: LLMs can effectively utilize and apply complex rules and standards, such as those in FHIR, reducing the dependency on specialized expertise. FHIR standard also gets updated with new releases, that adds an additional burden for human experts to keep themselves updated. Our automated process alleviates the need of that process.\n4. Scalability: LLMs offer a scalable solution that can adapt to varying sizes and complexities of clinical datasets, a task that can be challenging and resource-intensive with manual methods."}, {"title": "5 Data Privacy and Safety considerations", "content": "As mentioned earlier, in course of this work, we have used only the clinical data column names and corresponding data dictionaries (which defines the columns, their data types, applicable code values etc.) where available, we have refrained from using actual clinical data as that is not a requirement for the experiments."}]}