{"title": "Speaking the Same Language: Leveraging LLMs in Standardizing Clinical Data for AI", "authors": ["Arindam Sett", "Somaye Hashemifar", "Mrunal Yadav", "Yogesh Pandit", "Mohsen Hejrati"], "abstract": "The implementation of Artificial Intelligence (AI) in the healthcare industry has garnered considerable attention, attributable to its prospective enhancement of clinical outcomes, expansion of access to superior healthcare, cost reduction, and elevation of patient satisfaction. Nevertheless, the primary hurdle that persists is related to the quality of accessible multi-modal healthcare data in conjunction with the evolution of AI methodologies. This study delves into the adoption of large language models to address specific challenges, specifically, the standardization of healthcare data. We advocate the use of these models to identify and map clinical data schemas to established data standard attributes, such as the Fast Healthcare Interoperability Resources. Our results illustrate that employing large language models significantly diminishes the necessity for manual data curation and elevates the efficacy of the data standardization process. Consequently, the proposed methodology has the propensity to expedite the integration of AI in healthcare, ameliorate the quality of patient care, whilst minimizing the time and financial resources necessary for the preparation of data for AI.", "sections": [{"title": "1 Introduction", "content": "Using clinical datasets from disparate sources with different schema (data structures) and data dictionaries (information about fields, data types, and their meanings) can pose several challenges in AI development:\n\u2022\nData Inconsistency\nDifferent sources may have different conventions for documenting the same information. This\ninconsistency can lead to data mishandling, which in turn jeopardizes the accuracy of AI models.\n\u2022\nFeature Mismatch\nIf different datasets use different features, or represent identical features differently, it can be\ndifficult to reconcile them into a uniform model. Even similar types of data can be represented\ndifferently, making harmonization challenging.\n\u2022\nData Quality\nDatasets from different sources often have different levels of quality and reliability. Some\ndatasets may contain more errors or inaccuracies than others, which can affect the performance\nof the Al models.\n\u2022\nInformation Loss\nDuring the process of standardizing or transforming the data to fit into a single schema, some\nimportant information could be lost.\n\u2022\nHigher Complexity\nManaging data from disparate sources increases the overall complexity of data pre-processing.\nThis also means increased time and resources spent on data wrangling.\n\u2022\nLegal and Ethical Issues\nThe use of disparate data sources raises concerns about privacy and data security. Furthermore,\nthe algorithms developed using the data can be unknowingly biased due to the variations in the\nquality of data from these different sources.\n\u2022\nInteroperability Issues\nDifferent healthcare information systems tend to use different data standards, creating compati-\nbility issues and limiting how freely data can be exchanged between systems.\nUltimately, such challenges may compromise the validity, generalizability, and reliability of AI solutions in\nthe clinical field. This emphasizes the importance of a robust, harmonized data sourcing and integration\nstrategy in clinical AI development.\nPrevious research has extensively explored the use of Al in clinical data standardization, primarily\nfocusing on rule-based methodologies and conventional machine learning techniques [1, 2]. These\napproaches have paved the way for notable advancements in handling structured data. However, they\noften fall short when dealing with complicated domain-specific schemas, which constitute a significant\nportion of real-world clinical datasets. Techniques like natural language processing have been applied\nto extract information from unstructured text [3, 4] or data cleansing [5]. Yet, the application of Large\nLanguage Models (LLM) has been relatively under-explored in the current literature. The potential of\nthese models for semantic understanding and context-aware data mapping suggests they could offer\nsubstantial improvements in the field of clinical data standardization.\nThis work is an attempt to leverage LLMs for mapping any raw dataset to a clinical data standard; HL7\nFast Healthcare Interoperability Resources (FHIR) [6]. We leverage a zero/few-shot learning approach\nto achieve data mapping for standardization.\nNote: In course of this work, we have used only the clinical data column names and corresponding data\ndictionaries (which defines the columns, their data types, applicable code values etc.) where available,"}, {"title": "1.1 A Brief Overview of Data Standardization for Clinical Data", "content": "The adoption of data standards for clinical data has become increasingly important with the rise of\nAI applications in healthcare. To develop accurate and effective Al models for tasks like diagnosis,\nprognosis, and treatment recommendations, high-quality structured clinical data is required. However,\nclinical data has historically been challenging to standardize due to its complexity and heterogeneity.\nIn recent years, FHIR has emerged as a leading standard for clinical data exchange. FHIR provides a\ncommon framework and set of APIs for representing and sharing clinical data in a standardized way.\nSome key benefits of FHIR include:\n\u2022\nStructured data format based on resources with common fields for clinical concepts like patients,\nconditions, medications, etc. This enables integration and analysis across datasets.\n\u2022\nModular components can be used in flexible ways to represent various clinical workflows. This\nfacilitates interoperability across systems.\n\u2022\nModern web standards and APIs for efficient data access and exchange.\n\u2022 Active open source community with rapid evolution of specifications.\nFHIR acts as a bridge for AI, allowing it to extract structured clinical data from electronic health\nrecords and other healthcare systems in a consistent format. This helps address the \"data wrangling\"\nchallenges that often dominate healthcare AI projects. With more adoption of FHIR, higher-quality\ndatasets will become available for developing, evaluating, and deploying AI algorithms. An example of\nFHIR representation of patient data and media data is shown in Figure 1 [6]."}, {"title": "2 Tasks and Data", "content": "This study utilized 14 clinical datasets (details on access requirements for each dataset can be provided\nupon request). The datasets encompassed a wide range of disease areas, including neurology, respiratory,\nophthalmology, and insurance claims. The size and complexity of the datasets varied considerably."}, {"title": "2.1.1 Ground-truth Generation and Curation", "content": "Ground-truth generation employed a semi-supervised approach leveraging LLMs. The resulting FHIR\nmapping was subsequently reviewed and refined by a team of four data curators with expertise in FHIR."}, {"title": "2.2 Methods", "content": "In this section, we describe the motivation behind our system, its overview and functionalities, the\nprompt engineering process."}, {"title": "2.2.1 System Inspiration", "content": "Our goal is to explore the ability of GPT-3.5 to provide a framework for defining a consistent data\nterminology across various datasets and institutions by utilizing Retrieval-Augmented Generation\n(RAG) [21]. This standardization is essential for interoperability, enabling healthcare organizations\nto exchange patient data seamlessly, researchers to aggregate and analyze data consistently, and\ndevelopers to create applications that work with healthcare information reliably."}, {"title": "2.2.2 FHIR Standard and Clinical Data Mapping to FHIR", "content": "The core foundation of FHIR Standard is a set of modular components called \"Resources\". A Resource\nrepresents instance-level representation of any healthcare entity. All resources have a few elements\n(also called attributes) in common:\n\u2022\nAn identifier for the resource - typically a URL that defines where the resource is found\n\u2022 Common metadata"}, {"title": "2.2.3 System Overview", "content": "The system takes the data dictionary of a target dataset as input and generates an output mapping\nindividual data elements that conforms to the FHIR standard, resulting in a standardized format for\nthe dataset. This data dictionary typically includes definitions, descriptions, and explanations of terms,\nfields, and variables used in the dataset.\nAs shown in Figure 2, our system combines a retriever system, which extracts relevant document\nsnippets from FHIR, and GPT-3.5, which produces answers using the information from those snippets. In\nessence, RAG helps the model to search and retrieve contextual information from FHIR to improve its\nresponses. Specifically, the FHIR documents are split into text chunks using recursive character text\nsplitting. The chunks are then embedded in a vector space using the OpenAI text-embedding-ada-002\nembedding engine and stored in the Facebook AI Similarity Search (FAISS 1.7.4) vector database. The\nFAISS[23] vector database is used to find the k-most similar chunks to a given query at the query time.\nThe original query, combined with the retrieved chunks is compiled into a prompt and passed to the\nGPT-3.5 for generating the answer. This provides the LLM with additional information that contains\nfactual data, which can help to improve the quality of its responses. For vector similarity search we have\nused a chunk size of 2000, chunk overlap of 200."}, {"title": "2.2.4 Context for RAG", "content": "For RAG, we have prepared content based on the FHIR standard definitions that are available at the\nHL7 FHIR website. We have taken the resource, element and the corresponding descriptions of the\nresource-element pair as information that we have subsequently embedded into the vector database\nusing chunking, for a subsequent retrieval during mapping, this retrieved context is passed along with\nthe input data dictionary column and associated information to the LLM for an output FHIR mapping."}, {"title": "2.2.5 Prompt Engineering", "content": "Prompt engineering is an emerging field that focuses on creating and fine-tuning prompts to maximize\nthe effectiveness of LLMs for various applications. It involves a comprehensive set of techniques that\nboost interaction with LLMs, enhance their safety, integrate domain knowledge, and work with external\ntools.\nIn this work we have taken an incremental approach with prompt engineering to create FHIR mapping.\nInitially we set out with basic prompting with minimal information in the prompt (viz. just the input\ndata dictionary row and some simple instructions to map to FHIR), and then iterate over the input data\ndictionary sets after carefully examining output FHIR mapping generated. Our final prompt has the\nfollowing structure: Definition of User Role, Initial Instructions, Placeholder for Retrieved Context from\nRAG, Example (one-shot learning example), Placeholder for data dictionary Input, Direction for Output\nFormat, and Final Set of Instructions. Of these, we noticed the biggest change in output results occurred\nbecause of variations in Final Set of Instructions, as well content of the data dictionary."}, {"title": "2.3 Preliminary Evaluation", "content": "Our model predicts the structures for the data dictionary of a dataset. Each structure consists of several\nmetadata blocks, with the first block usually called the resource. We evaluated the performance of\nour model based on the number of matched metadata blocks between the predicted structure and the\nground-truth. We evaluated the performance for top-k = 20.\nA predicted structure is categorized as an 'Absolute Match' if all of its blocks align with those in the\nground-truth. If the resource aligns but some or all other metadata elements do not, this leads to a\n'Partial Match'. Alternatively, if none of the blocks match, or if the resource does not match even when"}, {"title": "", "content": "other block do, the predicted structure is considered a 'Mismatch'. Partial Score and Match Score are\nthen defined as below:\n$Score = \\frac{S+P}{N}  \\times 100$\n$P = \\sum_{i=1}^{P} \\frac{intersection(pred_i, gt_i)}{union(pred_i, gt_i)}$\nWhere S and N represent the quantity of absolute matches and the total structures in the dataset\ndictionary, respectively. P is the fraction of matched blocks in all 'Partial Match' structures. $pred_i$ and\n$gt_i$ correspond to the predicted and ground-truth for structure i, respectively. K is the number of all\n'Partial Match' structures. Moreover, we also calculate a Resource Match Score, which is calculated\nbased on instances where the resources matched between ground-truth and Predicted results. Similar\nto Partial Score this is also a percentage value.\n$Resource Match Score = \\frac{S+K}{N}  \\times 100$\nWhere S, K, and N represent the quantity of absolute matches, the quantity of 'Partial Match' structures,\nand the total structures in the dataset dictionary, respectively."}, {"title": "2.4 Observations", "content": "Table 4 presents the results for top-k = 20, averaged over 10 mapping iterations to account for the\nvariations in results in individual mapping run.\nThe model exhibited strong overall performance with a mean score of 73.54 (SD = 0.16) and a low\nstandard deviation of 0.11 for the resource match score (94.52). This consistency suggests that the model\nperforms reliably across various datasets and exhibits minimal variability between runs.\nThe resource match score, which evaluates the alignment between predicted resources and the ground-\ntruth, shows variability across datasets. While some datasets demonstrate high resource match scores\n(e.g., ADNI_DOD with 97.20), others exhibit lower alignment (e.g., CERA with 89.8). The consistency in"}, {"title": "", "content": "resource match scores, as indicated by small standard deviations, suggests that the model maintains a\nrelatively stable performance in resource prediction across different datasets.\nsome datasets exhibit notably high scores, such as ADNI_DOD (77.93), COVASTIL (78.05), BIOFINDER1\n(77.09), and BLAZE (79.59), while others, like ADNI (51.31), LUNG_PET_CT_DX (52.86) display lower\nperformance. The standard deviations accompanying these scores provide insight into the consistency\nof the model's performance, indicating relatively small variations across individual runs. However, it's\nessential to note that while the model maintains consistency, there remains variability in its efficacy\nacross different datasets, as evident in the significant deviation in scores, especially for datasets like\nLUNG PET CT DX, NLST, LIDC-IDRI, and CERA. This could be due to a number of factors, such as the\nquality of the data dictionary, and the specific FHIR profiles used. An informative and comprehensive\ndictionary could equip the LLM to interpret even cryptic column names effectively, facilitating better\nmapping and potentially mitigating performance variability. The variability in scores, highlights the\nimportance of understanding dataset-specific characteristics that may influence the model's ability to\naccurately predict FHIR Mappings.\nFurther analysis is needed to understand the factors that contribute to the model's performance on\ndifferent datasets. This could involve examining the characteristics of the datasets, such as the type of\ndata, and the quality of the data dictionary."}, {"title": "3 Future Work", "content": "For our future work we have identified a few research areas:\n\u2022\nEnriching the content for context retrieval, using FHIR examples, LLM generated content etc.\n\u2022\nFind out reason for lower score for certain datasets and improve the score for different datasets\nto get a more uniform score.\n\u2022 Work with different LLMs to have a comparative analysis of performance.\n\u2022 Work with different RAG methodologies (HyDE [24], Reranking etc.) to have a more in-depth\nunderstanding of how they are impacting the score."}, {"title": "4 Conclusion", "content": "In this work, we have explored a methodology of producing FHIR mapping utilizing the inherent\nparameterized knowledge available in LLMs, further bolstered by external non-parameterized knowledge\nsourced from curated documentation of FHIR Standards. Utilizing RAG with FAISS, relevant context\ninformation is retrieved and passed to the LLM during the inference process. The results indicate that\nthis approach is a viable method for producing FHIR mapping for data elements from clinical datasets.\nThe distinct advantages of employing LLMs for FHIR mapping, in comparison to manual approaches,\nare as follows:\n1. Time Efficiency: LLMs can process and map data at a significantly faster rate than manual\nmethods, leading to time savings in the mapping process. As an example, if it takes an expert\n1 minute to map a single data field to FHIR, it would take 160 hours to map the total 9600\nfields that we have mapped in the results. In our approach using LLM, we have been able to\ncreate that mapping in under 10 minutes. Usually for an expert, it will take more than a minute,\nconsidering time required for cross-referencing definition of a field, or further research for\npicking one resource or element over others. This is a significant advantage.\n2. Cost of Labor: The automation capabilities of LLMs reduce the need for extensive labor, thereby\ncutting down the costs associated with manual mapping. We explored Amazon Mechanical Turk\npricing, which varies between $0.02 - $0.08 per object (in this case, we can assume an object\nis a single field and its FHIR mapping), at that rate our collection of 9600 fields would have\ncost $192 - $768 [25]. This is significantly higher than using OpenAI GPT [26] and Embedding\nModels.\n3. Expertise: LLMs can effectively utilize and apply complex rules and standards, such as those in\nFHIR, reducing the dependency on specialized expertise. FHIR standard also gets updated with\nnew releases, that adds an additional burden for human experts to keep themselves updated.\nOur automated process alleviates the need of that process.\n4. Scalability: LLMs offer a scalable solution that can adapt to varying sizes and complexities of\nclinical datasets, a task that can be challenging and resource-intensive with manual methods."}, {"title": "5 Data Privacy and Safety considerations", "content": "As mentioned earlier, in course of this work, we have used only the clinical data column names and\ncorresponding data dictionaries (which defines the columns, their data types, applicable code values\netc.) where available, we have refrained from using actual clinical data as that is not a requirement for\nthe experiments."}]}