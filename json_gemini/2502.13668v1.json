{"title": "PeerQA: A Scientific Question Answering Dataset from Peer Reviews", "authors": ["Tim Baumg\u00e4rtner", "Ted Briscoe", "Iryna Gurevych"], "abstract": "We present PeerQA, a real-world, scientific, document-level Question Answering (QA) dataset. PeerQA questions have been sourced from peer reviews, which contain questions that reviewers raised while thoroughly examining the scientific article. Answers have been annotated by the original authors of each paper. The dataset contains 579 QA pairs from 208 academic articles, with a majority from ML and NLP, as well as a subset of other scientific communities like Geoscience and Public Health. PeerQA supports three critical tasks for developing practical QA systems: Evidence retrieval, unanswerable question classification, and answer generation. We provide a detailed analysis of the collected dataset and conduct experiments establishing baseline systems for all three tasks. Our experiments and analyses reveal the need for decontextualization in document-level retrieval, where we find that even simple decontextualization approaches consistently improve retrieval performance across architectures. On answer generation, PeerQA serves as a challenging benchmark for long-context modeling, as the papers have an average size of 12k tokens.", "sections": [{"title": "Introduction", "content": "The number of scientific articles is increasing exponentially (Fire and Guestrin, 2019; Bornmann et al., 2020), leading to an increase in review work and leaving researchers with an ever-expanding number of publications to read to keep up with their field. Therefore, novel tools are required to support reviewing work and enable readers to consume information from scientific articles more efficiently (Brainard, 2020; Kuznetsov et al., 2024). Automatic Question Answering (QA) systems can provide such support, allowing researchers and reviewers to productively extract information from an article, particularly if integrated directly into the reading and reviewing interface (Zyska et al., 2023; Lo et al., 2024). QA systems can also improve the quality of peer review, e.g., by avoiding questions in a review that are addressed in the article but potentially overlooked by a reviewer. However, the development of QA models is limited by the availability of high-quality and realistic datasets in the scientific domain to measure the performance of methods. Collecting scientific QA data is challenging because it requires expert annotators who are difficult to recruit. Furthermore, naturally occur-"}, {"title": "Related Work", "content": "Peer Review Many tasks and applications leverage peer reviews as a data source, including argument mining (Hua et al., 2019; Cheng et al., 2020; Kennard et al., 2022), helpfulness and score prediction (Xiong and Litman, 2011; Gao et al., 2019), review generation (Yuan et al., 2022; D'Arcy et al., 2024), tagging and linking review comments with the paper (Kuznetsov et al., 2022; D'Arcy et al., 2024), rebuttal generation (Purkayastha et al., 2023), the study and analysis of peer review (Kang et al., 2018; Ghosal et al., 2022) and more general contexts such as document revision (Ruan et al., 2024). In PeerQA, we utilize peer reviews to source a scientific QA dataset.\nScientific QA QA datasets in the scientific domain can generally be categorized as larger-scale datasets that are (semi-) automatically created and small expert-annotated datasets.\nAmong the larger-scale but (semi-) automatically created QA datasets are PubMedQA (Jin et al., 2019), in which questions are sourced from article titles that are phrased as questions. Answers are either yes, no, or maybe, and a subset is expert-annotated. SciDefinition (August et al., 2022) uses templates to generate questions about the definition of scientific terms. Kulshreshtha et al. (2021) create a dataset in the ML and Biomedicine domain with questions sourced from Google's \"People also ask\" suggestions and answers from the search engine's span extraction feature. Wan et al. (2024) generate a large-scale, scientific QA dataset by distilling a generation model from GPT-4 instructed to output QA pairs given a paper. Auer et al. (2023) develop question templates to automatically generate questions that are answerable from the Open Research Knowledge Graph (Jaradeh et al.,"}, {"title": "PeerQA", "content": "Data Collection\nFigure 1 provides an overview of the data collection process. We use papers and peer reviews from NLPeer (Dycke et al., 2023) and extend this set with journals and conferences that publish peer reviews and camera-ready versions publicly. Specifically, the data from ARR 2022 (containing papers published at ACL and NAACL 2022), COLING 2020, ACL 2017, CoNLL 2016, and F1000 was curated in NLPeer, partially based on previous data collections (Kang et al., 2018; Kuznetsov et al., 2022) and published under a CC-BY-NC-SA 4.0 license. The data from the Geoscience domain is published under a CC-BY 4.0 license in two journals: Earth System Dynamics (ESD) and Earth Surface Dynamics (ESurf). For ICLR 2022/2023 and NeurIPS Datasets and Benchmark Track 2022, we retrieve papers and reviews from OpenReview. Since they are without any license, we do not publish them in our release but provide a download and processing script. All questions and answers in PeerQA are published under CC-BY-NC-SA 4.0.\nPaper Processing. We extract the full text of the camera-ready version of a publication, including equations and captions, using GROBID 0.8 (Lopez, 2008\u20132024), which also groups sentences into paragraphs, which we use later in our experiments.\nQuestion Processing. From the peer reviews of each paper, we extract an initial set of questions using all sentences ending in a question mark, resulting in 17910 questions. The resulting questions comprise three problems: First, they are noisy as"}, {"title": "Analysis", "content": "We report distributional statistics of the dataset in Figure 2. Notably, the average paper length is 11723 tokens, which provides an interesting benchmark for long-context generative models. Furthermore, questions are relatively long, with an average of 20.2 tokens (the average length in BioASQ, QASPER, and QASA is 13.2, 10.2, and 17.7, respectively). One reason for this is the question processing pipeline, particularly the decontextualization step. Reviewers construct questions potentially consisting of multiple sentences. During preprocessing, the question has been rephrased to contain all this information. We analyze the semantic similarity between the final and original questions, finding that 90% of questions have a similarity of more than 0.6 and 50% more than 0.82. This shows that our processed questions remain highly similar to the original questions in the review. On average, questions have 3.8 annotated answer evidence sentences. Besides, 30% of questions have non-consecutive answer evidence, i.e., the evidence is distributed non-contiguously over the paper. We run a topic model to understand which questions are contained in PeerQA, specifically BERTopic (Grootendorst, 2022). We find"}, {"title": "Experiments", "content": "Answer Evidence Retrieval\nWe set up the answer evidence retrieval task as an information retrieval problem: Given a query, the model computes a score for each passage in the paper, where a passage can be a paragraph or sentence. To evaluate the answer evidence retrieval task, we test models of various architectures, including cross-encoder (Nogueira and Cho, 2019), dense retrieval (Reimers and Gurevych, 2019), multi-vector dense retrieval (Khattab and Zaharia, 2020), sparse (Zamani et al., 2018) and lexical models. Specifically, a cross-encoder model concatenates the query and passage and outputs a relevance score. In contrast, dense approaches encode query and passage independently by the same or individual models, obtaining a high-dimensional representation for each. A score is computed via dot-product or cosine-similarity between the two representations. Multi-vector approaches represent a query and passage not by a single but by many representations, e.g., for each token. The relevance score is computed by taking the sum of the maximum score between each query and passage token. Lexical approaches use term matching and weighting between the query and passage. Building upon this, sparse models perform a semantic query and/or document expansion to overcome the lexical gap. Concretely, we evaluate: MiniLM-L12-v2 (Wang et al., 2020; Thakur et al., 2021), Contriever (Izacard et al., 2022), Dragon+ (Lin et al., 2023), GTR (Ni et al., 2022), ColBERTv2 (Santhanam et al., 2022), BM25 (Robertson and Zaragoza, 2009) and SPLADEv3 (Lassance et al., 2024).\nBesides various models, we investigate the impact of retrieving paragraphs or sentences. We use the paragraphs extracted by GROBID and mark any paragraph as relevant that contains a relevant sentence. Furthermore, we investigate a baseline to improve the decontextualization by prepending the title, which has been shown beneficial in cases where decontextualization is required (Wang et al., 2024).\nWe evaluate using Mean Reciprocal Rank (MRR) (Craswell, 2009), which considers the first relevant passage in a ranked list. While a typical question in PeerQA often has multiple answer evidence sentences they frequently belong to the same paragraph or are close to each other. Therefore, pointing a user to the respective paragraph in a real-world application would already be useful as further relevant information usually clusters around the same location. We also measure the quality of the entire ranking by evaluating Recall@10. We chose 10, as most questions have fewer relevant sentences"}, {"title": "Answerability and Answer Generation", "content": "We set up the answerability task as a binary classification problem: given a question and context, a model predicts whether a question is answerable or not. We label all questions as answerable with annotated answer evidence and all as unanswerable, which the authors flagged as such. The answer generation task is set up as a sequence-to-sequence task, i.e., given the question and the context, the answer needs to be generated. For both tasks, we employ instruction-tuned LLMs. For the answerability task, we prompt the model to either answer the question if sufficient evidence is provided or to generate No Answer. However, to obtain generations for all answerable questions, we remove the instruction to generate No Answer from the prompt for the answer generation task (see \u00a7G and \u00a7H for the prompts). We experiment with providing as context the gold passages (ablating retrieval errors), the top-k retrieved paragraphs (where k \u2208 {10, 20, 50, 100}), and the full text. This is a Retrieval Augmented Generation (RAG) (Lewis et al., 2020) setup, except we retrieve from a single, long document instead of a corpus. We truncate the paragraphs if required by the maximum context size of the models and decode greedily from the models. Specifically, we use Llama-3-8B-Instruct (Dubey et al., 2024), which we also extend to a 32k context size with dynamic rope-scaling, Command-R15, Mistral-7B-Instruct-v0.2 (Jiang et al., 2023), GPT-3.5-Turbo-0613-16k and GPT-40-0806 (OpenAI et al., 2023). We evaluate the answerability task as a binary classification problem. We evaluate with macro-F1 to counter the imbalance"}, {"title": "Results", "content": "Answer Evidence Retrieval\nTable 3 reports the retrieval results. Across models, we find that retrieving the paragraph yields higher scores than the sentence. Appending the title to the paragraph further improves results (except ColBERTv2's MRR), showing that decontextualiz-"}, {"title": "Answerability", "content": "We report precision, recall, and macro-F1 on the answerability task in Figure 3. We observe similar precision for all model and context settings. Precision for answerable questions is much higher than for unanswerable ones. When looking at recall, we find notable differences between the models. While Mistral and Command-R obtain relatively high recall on answerable questions and low recall on unanswerable questions, the Llama and GPT models obtain high recall on unanswerable questions and lower recall on answerable questions. This pattern can be explained: Mistral and Command-R tend to predict an answer more often, while Llama and GPT tend to predict the question is unanswerable, showing that all models have a bias towards one of the classes. Command-R and GPT-40 provide the best trade-off, shown by the highest macro-F1."}, {"title": "Answer Generation", "content": "Figure 4 reports the evaluation metrics comparing the generated answers to either the free-form reference answer, the GPT-4 augmented answer, or the gold paragraphs. Generally, models perform best with the gold answer evidence. Therefore, the annotated evidence provides a strong signal to answer the question. The scores achieved with the gold evidence represent an upper bound. However, higher scores might be possible with more context to better understand the gold answer evidence (or potentially unannotated but useful passages). Upon manual inspection of model errors, we find that lower performance is caused by evaluation failures or information in the free-form answer that is not supported by the evidence (i.e., information that is coming from the author's knowledge that might be general about the field or specific to the paper and did not make it into the camera-ready version). Generally, LLMs perform better in RAG, with fewer but relevant contexts, than in the full-text setting on PeerQA. This shows that despite LLMs' large context sizes, it is more effective to employ a retriever filtering relevant information than leaving this step to the internal workings of the LLM. A notable exception is GPT-40, which exhibits stable performance with increasing context sizes and increasing performance on answer correctness. GPT-40 is also the most recent and"}, {"title": "Error Analysis", "content": "powerful model in our evaluation, demonstrating the improved abilities of state-of-the-art models on long-context tasks. We further analyze the answer generation performance of the RAG setting by measuring the correlation between the retriever recall and the generation metric. We find mostly positive correlations between the retrieval and generation performance across models. While the correlation is not very strong (up to r = 0.42), it confirms that with increased retrieval performance, the generation improves (Salemi and Zamani, 2024).\nError Analysis. We analyze the lowest performing 80 generations of GPT-3.5 to better understand the errors and report them in Table 4. We find many low-scoring generations are correct despite at least one of the evaluation metrics providing a low score, for example, when the generation is more verbose or expresses the correct answer differently (Evaluation Error). However, we find the metric with the highest score for these generations to be above the 50th percentile in 91% of the cases. This shows that using different metrics against different ground truths is plausible and catches the alleged failures. Further, we observe the model is only partially correct when the free-form answer contains important additional details. In other cases, the model fails to reason correctly over the evidence, e.g., it arrives at an opposite conclusion than the correct answer. Similarly, when the evidence is only implicit or requires expert domain knowledge, the model fails. Lastly, there are also a few errors in the data. In 11.25% of cases, the gold evidence is not self-sufficient, i.e., more context from the paper would be required, e.g., to understand previously introduced concepts. These errors can likely be recovered through additional retrieval. Other times"}, {"title": "Conclusion", "content": "We introduced the PeerQA dataset to advance and study question answering on scientific documents. We sourced PeerQA's questions from peer reviews and obtained answer annotations from the paper authors. Our dataset supports three crucial tasks for developing QA systems: evidence retrieval, answerability, and answer generation. We analyzed the collected data and established baseline systems for all three tasks. For evidence retrieval, we find that decontextualization is key to improving performance. On the answerability task, we find that models tend to either over- or under-answer, showing a bias for one of the classes. Further, although models can fit the entire paper into context in the answer generation task, providing the model with the top passages from a retriever outperforms the full-text setting. We also show that with increased retrieval performance, the generation improves. Finally, our error analysis highlights the need for better evaluation metrics and model reasoning abilities."}, {"title": "Limitations", "content": "Dataset Size. General domain QA datasets usually comprise up to three magnitudes more data than PeerQA (e.g., NQ has 323k samples). However, collecting high-quality data in the scientific domain is challenging due to the requirement for expert annotators. Since science has many domains, it is impractical to collect training data for each of them. Instead, models need to generalize in an unsupervised manner, at most leveraging few-shot examples. Therefore, we introduce PeerQA as an evaluation resource to test the generalizability of models. The size is in line with other recent datasets such as HumanEval (Chen et al., 2021a) (164 examples), TruthfulQA (Lin et al., 2022) (817), and GPQA (Rein et al., 2024) (448). In addition, we release the unlabeled data, comprising 12k questions from 2.6k papers, that can be used for more annotations, unsupervised learning, and further study of reviews. Small evaluation datasets also have the advantage of reduced iteration time over experimental settings, lesser use of compute resources, and a smaller environmental impact."}, {"title": "Ethical Considerations", "content": "All annotators in PeerQA are authors of accepted articles at conferences or in journals. We do not collect any of their personal information or who has provided the answers. By the nature of our data collection protocol, we only contact authors who have provided their email publicly along with their publication and contact each author individually. Authors have participated voluntarily in the data collection, and we try to keep their workload low by only asking few questions (on average 2.8). Furthermore, the authors have largely already answered questions during peer review (see \u00a7C), making them familiar with the questions and answers, further reducing their workload.\nOne objective of PeerQA is to advance the study of peer review, including developing methods and tools to facilitate the authoring and reviewing of scientific articles. Particularly, LLMs have the potential to support authors and reviewers in their work (Kuznetsov et al., 2024). However, these models also have biases and weaknesses. For example, in our question answerability task, we clearly observe that some models are biased towards one class, i.e., predicting the question as answerable or unanswerable (see \u00a75.2). Therefore, these methods can only be used as assistants that support humans. PeerQA sheds light on these issues, raising awareness of potential weaknesses in these models and their careful application in science."}, {"title": "PDF extraction of Answer Evidence", "content": "Text extraction from PDF is not a perfect process. Unfortunately, this means that some annotated answer evidence (and therefore also answerable questions) must be discarded in our experiments since their evidence has not been extracted correctly."}, {"title": "Pre- & Post-Processing Prompts", "content": "Question Clean-Up &\nDecontextualization\nGiven the extracted question and previous sentences (context) from the peer review, we use the following prompt to decontextualize the question:\nThis is part of a scientific peer review\nwhere the reviewer raises a question\nregarding the paper.\n\"\"\"\n{context} {question}\n\"\"\"\nWrite the last question such that it\ncan be comprehended independently without\nthe context of the review. Resolve any\nreferences to the review. Respond with a\nsingle question.\nQuestion Decomposition\nIn case the constituency parser detects a conjunc-tion, we use the following prompt to decompose\nthe question:"}, {"title": "Answer Free-Form Augmentation with Evidence", "content": "To ensure a similar quality and verbosity of answers, we augment the free-form answers provided by the authors using the prompt below in case the question has annotated evidence. If it does not have annotated evidence, we use the prompt in \u00a7B.4.\nYou are a helpful scientific research\nassistant. Your task is to write clean\nanswers, given noisy answers from a\nscientific question answering dataset.\nThe question has been asked during a peer\nreview of a scientific article. Given\nthe question, background information\nextracted from the paper, and a noisy\nanswer, your task is to write a clean\nanswer. Write a concise answer that\ndirectly answers the question. Make\nsure all information in your answer is\ncovered by the background. Incorporate\nadditional information from the original\nanswer. Write the answer neutrally, i.e.,\nas a third person (and not the author)\nanswering the question. For example, use\n\"The authors\" instead of \"We\".\nQuestion: {question}\nBackground: {evidence}\nOriginal Answer: {answer}\nRephrased Answer:"}, {"title": "Answer Free-Form Augmentation without Evidence", "content": "You are a helpful scientific research assistant. Your task is to write clean answers, given noisy answers from a scientific question answering dataset."}, {"title": "Question Grounding", "content": "Figure 5 visualizes the similarity between the processed question and original review sentences. We use all-MiniLM-L6-v2 to compute the similarity. As detailed in \u00a73.1, we extract questions from the peer review and contextualize them with the preceding three sentences from the review. To understand whether our preprocessing has altered the original question or not, we compute the maximum similarity between the final processed question and the four sentences of the review (i.e., the question and the three proceeding questions). We find that 90% of questions have a similarity of at least 0.60, and 50% are more than 0.82 similar to the final processed question. This shows the quality of our cleaning, decontextualization, and decomposition steps: Questions are generally highly similar and, therefore, grounded in the original peer review."}, {"title": "Unlabeled Data", "content": "Besides the 579 questions with answer annotations, we additionally release all preprocessed and filtered 12k questions from 2.6k papers that have not been answered. Table 6 shows the breakdown per venue."}, {"title": "Answer Evidence Statistics", "content": "Figure 6 reports the number of answer evidence depending on the retrieval unit. Note that this only includes answer evidence that we could map into the text extracted from the PDF. Non-consecutive chunks are essentially the number of different locations in the paper with answer evidence."}, {"title": "Answerability Prompts", "content": "We use the following prompts to determine whether a question is answerable or not in the setting where we provide the full text (\u00a7G.1), the gold or retrieved paragraphs (\u00a7G.2).\nFull-Text\nRead the following paper and answer the question. If the paper does not answer the question, answer with \"No Answer\".\nQuestion: {question}\nPaper: {paper}\nAnswer:\nRAG\nRead the following paragraphs of a paper and answer the question. If the paragraphs do not provide any information\nto answer the question, answer with \"No\nAnswer\".\nQuestion: {question}\nParagraphs: {paragraphs}\nAnswer:"}, {"title": "Answer Generation Prompts", "content": "We use the following prompts to generate answers in the full-text (\u00a7H.1) and RAG (\u00a7H.2) setting.\nFull-Text\nRead the following paper and answer the question.\nQuestion: {question}\nPaper: {paper}\nAnswer:\nRAG\nRead the following paragraphs of a paper and answer the question.\nQuestion: {question}\nParagraphs: {paragraphs}\nAnswer:"}, {"title": "Model Sizes and Computational Resources", "content": "Answer Retrieval The number of parameters for each retrieval model is reported in Table 7. The retrieval experiments have been conducted on a Titan RTX 24GB.\nAnswerability & Answer Generation Sizes for the models used in the answerability and answer generation task are reported with the model names. The number of parameters for the proprietary GPT-3.5 and GPT-40 models are unknown, and we use it via the Azure API. We deploy the other models on a single A100 80GB GPU, except"}, {"title": "Evaluation Metric Details", "content": "Answer Evidence Retrieval To evaluate the answer evidence retrieval task, we use the mean reciprocal rank and recall implemented by the pytrec_eval (Van Gysel and de Rijke, 2018) package in version 0.5.\nUn/Answerability To compute the precision, recall, accuracy and F1 scores of the question answerability task, we use the classification report provided by scikit-learn (Pedregosa et al., 2011) version 1.4.0.\nFree-Form Answer Generation The generated answers are evaluated with Rouge (Lin, 2004), AlignScore (Zha et al., 2023) and Prometheus (Kim et al., 2024). For Rouge, we use the longest common subsequence (Rouge-L) between the generated answer and the reference answer. We use the rouge-score package in version 0.1.2 via Hugging Face's datasets package (Lhoest et al., 2021). We also stem the generated and reference answer before computing the metric with the Porter Stemmer. All reported Rouge-L scores are F1 metrics. For AlignScore, we use the fine-tuned checkpoint based on RoBERTa-large (Liu et al., 2019) and use the nli_sp mode, which splits the generation into sentences and uses a 3-way classification head to obtain scores. We use the original implementation in version 0.1.3. For Prometheus, we use the prometheus-eval package with version"}, {"title": "Answer Generation Error Analysis", "content": "As outlined in \u00a75.3, we conducted an error analysis on GPT-3.5's generations. Table 13 defines each error class, and Table 14 provides an example for each class.\nError Classes"}, {"title": "Answer Generation Correlation Analysis", "content": "Recall\nFigure 12 visualizes the relationship between the recall of the retrieval model (in this case SPLADEv3) at different cutoffs and the answer generation performance measured by different metrics.\nMean Evidence Position\nFigure 13 visualizes the Pearson correlation between the answer generation metric (Rouge-L, AlignScore, or Prometheus-2 compared to either the answer evidence, the annotated free-form answer or the GPT-4 augmented free-form answer as ground truth) and the mean token position of the answer evidence. All generations are taken from the full-text setting, i.e., where the entire paper text was given as input to the model. To compute the mean token position for each answer evidence, we compute the number of tokens in the paper before the evidence sentence. If a question has multiple answer evidence, we take the average position. We only find a weak relationship that is statistically insignificant in many cases. Nevertheless, some p-values show statistical significance, indicating that for some settings, the generation performance declines when the answer evidence is relatively towards the end of the paper. This finding is also consistent with related work such as Buchmann et al. (2024)."}, {"title": "Attributable Question Answering", "content": "We have considered answer generation based on the top retrieved paragraphs (RAG) or using the full context (\u00a74.2). In the RAG setup, the answer generation can generally be attributed to the retrieved passages (assuming the model is faithful to the context). However, when using the full text as context, attribution to the passage level is not trivial. Recently, attributable question answering has gained momentum (Bohnet et al., 2022; Gao et al., 2023; Malaviya et al., 2024), where in addition to generating an answer, the model is supposed to cite evidence supporting it. Therefore, we also conduct an experiment where the model is conditioned on the full text of the paper and is tasked to \"cite\" any paragraphs on which the generated answer is based. We prepend an id before each paragraph and include an instruction on how to cite. Specifically, we use the following prompt:\nRead the following paper and answer the question. Provide one or several evidence paragraphs that can be used to verify the answer. Give as few paragraphs"}]}