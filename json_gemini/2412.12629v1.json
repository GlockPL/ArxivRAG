{"title": "a2z-1 for Multi-Disease Detection in Abdomen-Pelvis CT: External Validation and Performance Analysis Across 21 Conditions", "authors": ["Pranav Rajpurkar", "Julian N. Acosta", "Siddhant Dogra", "Jaehwan Jeong", "Deepanshu Jindal", "Michael Moritz", "Samir Rajpurkar"], "abstract": "We present a comprehensive evaluation of a2z-1, an artificial intelligence (AI) model designed to analyze abdomen-pelvis CT scans for 21 time-sensitive and actionable findings. Our study focuses on rigorous assessment of the model's performance and generalizability. Large-scale retrospective analysis demonstrates an average AUC of 0.931 across 21 conditions. External validation across two distinct health systems confirms consistent performance (AUC 0.923), establishing generalizability to different evaluation scenarios, with notable performance in critical findings such as small bowel obstruction (AUC 0.958) and acute pancreatitis (AUC 0.961). Subgroup analysis shows consistent accuracy across patient sex, age groups, and varied imaging protocols, including different slice thicknesses and contrast administration types. Comparison of high-confidence model outputs to radiologist reports reveals instances where a2z-1 identified overlooked findings, suggesting potential for quality assurance applications.", "sections": [{"title": "1. Introduction", "content": "Image interpretation is a cornerstone for medicine, offering essential information for diagnosing and managing a wide range of conditions. Among imaging methods, computed tomography (CT) is especially valuable due to its detailed cross-sectional views of the body's internal structures. Abdomen-pelvis CT scans are particularly useful for identifying time-sensitive conditions like infections, blockages, or internal bleeding which require immediate medical attention, but can be difficult to interpret due to diversity in anatomy and potential pathology. The challenges in abdomen-pelvis CT interpretation, along with the increasing volume of imaging studies and a shortage of radiologists, underscore the urgent need for innovative solutions (Liu et al., 2022; Peng et al., 2022; Smith-Bindman et al., 2019). Advances in Al have positioned it as an exceptional tool to address these complex, data-intensive tasks, offering radiologists powerful support in managing growing workloads with greater accuracy and speed (Rajpurkar and Lungren, 2023). \u03a4\u03bf address this, we present a2z-1, an advanced AI model designed to assist in the accurate analysis of abdomen-pelvis CT scans by detecting 21 conditions.\nChallenges in CT Interpretation. The interpretation of medical images, particularly CT scans, is an extraordinarily complex task that challenges even the most experienced radiologists (Itri et al., 2018). This complexity is especially pronounced in abdomen-pelvis CT interpretation, where radiologists must simultaneously analyze multiple intricate anatomical structures and identify a diverse array of potential conditions. Despite the high level of expertise in the field, the challenging nature of this task inevitably leads to variability in interpretations and, at times, errors. Studies have shown that up to 14% of abdomen-pelvis CT reports undergo clinically important changes upon second review (Lauritzen et al., 2016). Furthermore, significant disagreements can occur both between different radiologists examining the same CTs and when the same radiologist reviews an CTs at different times. These disagreement rates can be as high as 26% and 32% respectively (Abujudeh et al., 2010), highlighting the subjective nature of image interpretation. A significant portion of radiological errors, estimated at 42%, occur when abnormalities are simply not noticed or recognized during the initial review of an image (Kim and Mansfield, 2014). These are known as perceptual errors,"}, {"title": "2. Results", "content": "The model maintained strong performance across various patient groups, imaging protocols, and test conditions.\n\u2022 Exceptional Performance in Detecting Critical Findings: a2z-1 was evaluated across 21 key findings in abdomen-pelvis CT scans, achieving high performance with an average AUC of 0.923 on external validation. The model delivered high performance for certain time-sensitive conditions such as small bowel obstruction (AUC 0.958) and acute pancreatitis (AUC 0.961).\n\u2022 Consistent Results Across Evaluation Scenarios and Patient Subgroups: a2z-1 was assessed across three institutions, including two external sites not used during training."}, {"title": "2.1. High Performance Across Diverse Conditions", "content": "Selection of Conditions. The a2z-1 model has been designed to detect a broad spectrum of abdominal conditions, with its performance assessed through Area Under the Curve (AUC) scores. The pathologies evaluated were chosen based on their clinical significance, guided by three main factors: urgency, potential for intervention, and prevalence. This approach ensures that the model is focused on conditions that demand timely treatment, have actionable outcomes, and are commonly encountered in clinical practice, allowing for robust performance measurement.\nAUC Performance Across Condition Categories. The a2z-1 model delivered solid performance across a variety of abdominal conditions, as shown by its AUC scores. For gastrointestinal conditions, the model achieved AUCs of 0.937 for appendicitis, 0.940 for diverticulitis, 0.855 for colitis, and 0.958 for small bowel obstruction. In vascular conditions, it showed AUCs of 0.876 for coronary artery calcification, 0.868 for aortic dissection, and 0.970 for unruptured aortic aneurysm. The model also performed well for hepatobiliary and pancreatic conditions, with scores of 0.940 for cholecystitis, 0.914 for biliary ductal dilatation, 0.890 for hepatic steatosis, 0.966 for liver cirrhosis, and 0.961 for acute pancreatitis (Figure 1). For renal conditions, the model showed AUCs of 0.949 for obstructive kidney stones, 0.873 for pyelonephritis, and 0.956 for hydronephrosis. Other abdominal conditions saw AUCs of 0.912 for hiatal hernia, 0.939 for splenomegaly, 0.960 for retroperitoneal hemorrhage, 0.921 for peritoneal free air (Figure 3), 0.920 for peritoneal abscess, and 0.891 for pneumonia. Figure 2 offers a visual summary of a2z-1's performance across the evaluated conditions.\nEvaluation Details. The model's effectiveness was evaluated through external validation, utilizing data from two independent health systems that did not contribute to the model's training. This external validation dataset consisted of 5444 studies from 4907 patients, providing a reliable sample size to assess the model's performance in varied real-world clinical environments. The ground truth for these cases was established through automated label extraction from the corresponding clinical radiology reports. Automated methods for label extraction from radiology reports have advanced significantly in recent years and are becoming highly accurate. We manually validated the quality of this ground truth by comparing the extracted labels to the actual content of the reports over a sample of reports that included every condition, achieving an average accuracy across findings of 99.4% in this subset, with a F1 score of 92.6%, recall of 99.05% and precision of 88.7%, ensuring a high level of fidelity. All reports were signed by US-certified"}, {"title": "2.2. Generalizability Across Radiology Sites/Practices", "content": "Internal and External Validation. A key strength of a2z-1 is its ability to maintain high performance across different clinical sites, demonstrating robust generalizability. In this evaluation, internal validation refers to testing the model on a separate dataset that was held out during training but comes from the same data source as the training set. This provides a baseline to assess the model's performance within a familiar clinical environment. In contrast, external validation refers to testing the model on independent datasets from two different states, which were not used during model development. These external validation sites allow us to assess the model's ability to generalize to distinct clinical settings and patient populations, ensuring real-world applicability.\nThe internal validation dataset, consisting of 9223 studies from 7234 patients, was used during model development to select the best-performing model. Importantly, the model was trained on a completely separate dataset with no patient overlap, ensuring that the internal validation results are unbiased. Meanwhile, the two external validation datasets come from two states different from the one used in model development, further testing the model's generalizability across diverse geographic locations.\nConsistent Performance Across Sites. As shown in Figure 5, the model demonstrates consistent performance across internal and external validation sets. For example, in detecting small bowel obstruction, a2z-1 achieves AUC scores of 0.979, 0.981, and 0.947 across the internal and the two external sites, respectively, showing only slight variation. Similarly, the detection of appendicitis is highly consistent, with AUCs of 0.941, 0.948, and 0.931 across the three validation sets, indicating reliable performance across different locations.\nVariations and Improved Performance. Interestingly, the model performs even better on some conditions in external validation, suggesting that"}, {"title": "2.3. Consistent Performance Across Patient Characteristics and Imaging Protocols", "content": "Analysis Overview. To evaluate the generalizability of the a2z-1 model, we conducted a comprehensive analysis using external validation data. The analysis focused on various patient demographics and imaging protocols, with relevant metadata extracted directly from DICOM files and associated radiology reports. These metadata fields, including patient age, sex, scan areas, contrast type used, and scanner manufacturer, allowed for a thorough assessment of model performance across critical subgroups. Table 1 shows the distribution of samples across different categories.\nPerformance Across Age and Sex. The results indicate that a2z-1 demonstrates strong consistency in performance across the different subgroups, even in the external validation setting. Across age groups, the model shows high performance, with AUC scores ranging from approximately 0.89 for patients 75 and above to 0.92 for those aged 45-59. Although there is a slight dip in performance for the youngest (under 18) and oldest (75 and above) populations, this may be due to anatomical differences or lower disease prevalence in these groups. Despite these small variations, the model remains highly effective across the majority of age categories, with performance particularly strong in early and middle-aged adult groups (18-59), where AUC scores consistently exceed 0.91. Sex-based analysis shows minimal variation in performance, with AUC scores of 0.92 for both males and females."}, {"title": "2.4. Confidence-Based Categorization for Streamlined Workflow Management", "content": "Confidence Levels and Threshold Setting. The a2z-1 model employs a confidence-based three-tiered categorization system\u2014\"likely,\" \"possible,\" and \"unlikely\"\u2014to balance precision and recall. The thresholds for each category were set using internal validation data, with the \"likely\" category targeting a precision of 0.8, and the \"possible\" category targeting a precision of 0.4 (after excluding cases already marked as \"likely\"). These thresholds were designed to ensure that the model prioritizes high-confidence cases for immediate attention while capturing additional cases with lower certainty for further review. This three-tiered system allows for better handling of potential findings without overwhelming the workflow, minimizing \"alarm fatigue.\"\nHigh-Confidence Findings in the Likely Category. The \"likely\" category, designed to capture high-confidence findings with a precision threshold of 0.8, ensures that the most critical cases are flagged accurately. In the external validation dataset, the model identified 224 out of 305 small bowel obstruction cases (73.44%) as \"likely.\" For appendicitis, 203 out of 418 cases (48.56%) were classified as \"likely,\" while 99 out of 148 cases (66.89%) for acute pancreatitis were captured in the \"likely\" category. This high-confidence filtering allows workflows to focus on cases that require prompt action without sacrificing accuracy.\nBroader Coverage with Reduced Workflow Disruption. The \"possible\" category, with a precision threshold of 0.4 among cases not categorized as \"likely,\" is designed to capture cases that are not entirely certain but may require more careful review. This classification helps identify findings that warrant additional scrutiny without necessarily being clear-cut positive cases. For appendicitis, 31.16% of remaining cases were categorized as \"possible,\" broadening the scope for further investigation. This tiered confidence system ensures that workflows are not overwhelmed while maintaining broad coverage for potential findings that may require attention."}, {"title": "2.5. Analysis of High-Confidence Model Predictions in External Validation", "content": "This analysis focuses on cases from the external validation dataset where the a2z-1 model predicted a pathology with high confidence (categorized as \"likely\") but the corresponding ground truth labels, derived from radiology reports, indicated the pathology was absent. The goal was to determine whether these divergences were true false positives or if errors in the report-to-label process or clinical ambiguity contributed to the discrepancies.\nLabeling Errors in Certain Cases. In some instances, the model's high-confidence predictions were correct, but the corresponding labels were incorrectly marked as negative due to errors in the report-to-label conversion process. For example, in one case, the model confidently predicted the presence of retroperitoneal hemorrhage. The report noted, \"subcapsular and perinephric hematoma,\" consistent with the model's prediction, but the ground truth label incorrectly indicated that no retroperitoneal hematoma was present. Similarly, in another case, the model accurately predicted an obstructive kidney stone, and the report explicitly stated, \"a 2 mm stone in the distal left ureter causing mild hydronephrosis,\" yet the label was marked negative. While the proportion of labeling errors were less than 1%, these were overrepresented in the model error analysis, suggesting that a portion of the model's false positives stemmed from inaccuracies in the labeling process rather than issues with the model's performance. This overrepresentation doesn't indicate a systemic problem with the labeling methodology, but rather highlights the challenges in accurately labeling edge cases or ambiguous instances where the report is not completely explicit.\nDetection of Similar Pathologies. In several instances, we observed that the model identified pathological findings that, while not explicitly mentioned in the radiological report, were similar to the actual conditions and carried similar clinical implications. This phenomenon suggests that the model may be capturing underlying patterns indicative of broader disease categories rather than specific diagnoses. For example, in one case, the model predicted colitis in a patient who was diagnosed with diverticulitis of the descending colon. Both conditions represent inflammatory processes affecting the colon, albeit with"}, {"title": "2.6. Analysis of High-Confidence Model Misses in External Validation", "content": "This analysis focuses on cases from the external validation dataset where the a2z-1 model predicted a low probability for a pathology that was labeled as positive based on the radiology report. The goal was to understand why the model failed to detect these findings and what contributed to the divergences.\nSubtle and Mild Cases. Analysis of false negative cases revealed that approximately 40% involved subtle or mild manifestations of the pathologies in question. In these instances, the radiological reports often characterized the findings as mild or early-stage, or expressed uncertainty about their presence, using phrases such as \"cannot exclude appendicitis.\" This pattern suggests that the majority of a2z-1 model's false negatives occur in cases where the pathological features are less pronounced and potentially subject to interobserver variability even among experienced radiologists. While these early or mild presentations may generally carry lower clinical urgency compared to more advanced cases, they nonetheless represent an important area for potential improvement in our model.\nLabeling Errors While less frequently than in false positive cases, we also identified labeling errors within false negative cases, including cases where biliary ductal dilatation or coronary artery calcification were labeled as present, but the report did not mention these findings.\nOvercalled Incidental Findings In some other cases, we found that the radiology report mentioned incidental findings such as \"borderline splenomegaly,\" which were not predicted by our model. However, on manual review, the spleen craniocaudal length was around 85 mm, which is below the threshold for diagnosing splenomegaly (Figure 10)."}, {"title": "3. Discussion", "content": "Towards World-Class AI For Medical Image Interpretation. To the best of our knowledge, a2z-1 is the first AI model to demonstrate excellent performance across a broad spectrum of abdomen-pelvis CT pathologies validated both on internal and external sites. This breadth of application, covering 21 actionable and time-sensitive conditions, sets it apart from most existing models, which tend to focus on narrower use cases. Our external validation, carried out across multiple health systems, demonstrates the robustness of a2z-1 and its ability to generalize across different datasets and patient populations.\nUnlike many existing studies that report results only on internal datasets, we conducted rigorous testing on independent external datasets to ensure the model's generalizability. The external validation shows that a2z-1 maintains its performance across different workflows, a key aspect for any model aiming to be broadly applicable. In particular, we achieve an average Area Under the Curve (AUC) of 0.92 across the 21 conditions. This performance level is significant because AUC values of this range typically reflect strong discriminative ability, ensuring reliable detection of both positive and negative cases.\nStrengths Compared to Existing AI Models. While AI models for chest X-ray (CXR) interpretation have made significant strides in covering a broad range of conditions (Jones et al., 2021), the interpretation of CT scans introduces an additional layer of complexity. CT images require the simultaneous analysis of multiple anatomical regions and structures, demanding a more intricate approach for AI-driven solutions (Schmidt, 2024). AI models have shown some early success (Brejneb\u00f8l et al., 2022; Hata et al., 2021; Rajpurkar et al., 2020; Vanderbecq et al., 2024), and generalist or foundation models are beginning to emerge, but these approaches have not yet demonstrated the rigorous performance necessary for comprehensive CT detection tasks, particularly in abdomen-pelvis imaging.\nOur focus with a2z-1 is on a structured, task-based evaluation of predefined, clinically actionable conditions. This deliberate focus ensures that the model is not only broad in scope but also precise in identifying time-sensitive pathologies. Importantly, we emphasize external validation across multiple sites, an approach that distinguishes our work from emerging generalist models that, while promising, have not yet demonstrated strong, consistent results in task-specific evaluations for CT scans.\nAnother example of a comprehensive approach in abdomen-pelvis CT is Merlin, a foundation model designed for broad abdominal imaging tasks (Blankemeier et al., 2024). While Merlin represents an early effort in this domain, its external validation remains limited, and its reported performance metrics are not directly comparable to those of a2z-1. By contrast, a2z-1 has been rigorously validated across multiple institutions, with condition-specific AUCs that provide detailed insights into its real-world applicability.\nBroader Workflow Enablement. While the model is highly relevant for radiologists, its utility is not limited to traditional radiology workflows. a2z-1's outputs could enable a range of applications, such as assisting emergency teams in triage scenarios, supporting quality control processes, selection of cases for peer review, or enabling automated alerts in high-volume settings like inpatient wards. The model's tiered confidence-based system\u2014categorizing findings into \"likely,\" \"possible,\" and \"unlikely\"\u2014offers flexibility, allowing different teams within the workflow to prioritize cases based on their specific needs.\nFor instance, high-confidence findings might be routed to specialists or flagged for immediate intervention, while lower-confidence findings could be used in secondary review processes. This flexible prioritization ensures that the model adapts to various workflows, whether in emergency departments, or quality assurance roles.\nCommercial Tools and Broader Applications. In the U.S., the only commercial AI tools approved for abdomen-pelvis CT interpretation are limited to three specific applications: BriefCase-Aortic Dissection Triage, BriefCase-Intra-abdominal Free Gas Triage, and Viz AAA. These tools are focused on triaging specific acute conditions, particularly in emergency settings,"}]}