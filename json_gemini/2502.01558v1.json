{"title": "Search-Based Adversarial Estimates for Improving Sample Efficiency in Off-Policy Reinforcement Learning", "authors": ["Federico Malato", "Ville Hautam\u00e4ki"], "abstract": "Sample inefficiency is a long-lasting challenge in deep reinforcement learning (DRL). Despite dramatic improvements have been made, the problem is far from being solved and is especially challenging in environments with sparse or delayed rewards. In our work, we propose to use Adversarial Estimates as a new, simple and efficient approach to mitigate this problem for a class of feedback-based DRL algorithms. Our approach leverages latent similarity search from a small set of human-collected trajectories to boost learning, using only five minutes of human-recorded experience. The results of our study show algorithms trained with Adversarial Estimates converge faster than their original version. Moreover, we discuss how our approach could enable learning in feedback-based algorithms in extreme scenarios with very sparse rewards.", "sections": [{"title": "1. Introduction", "content": "Sample inefficiency is a long-lasting challenge in deep reinforcement learning (DRL) affecting all well-known algorithms, such as Deep-Q-Network (DQN) (Mnih et al., 2015), Deep Deterministic Policy Gradient (DDPG) (Lillicrap et al., 2019) and Soft Actor Critic (SAC) (Haarnoja et al., 2018). The problem relates to the high number of transitions needed to learn an optimal policy (Sutton & Barto, 2018). Sample inefficiency has several causes, ranging from a high-dimensional observation or action spaces (Mnih et al., 2015), to an ineffective exploration strategy (Bellemare et al., 2016), or even an ill-designed reward signal (Rengarajan et al., 2022). In general, a good reward signal should be mathematically well-defined and encapsulate the learning objective perfectly (Sutton & Barto, 2018). Unfortunately, specifying a reward signal with these properties is generally a non-trivial task, and in some cases it might not be possible at all (Shakya et al., 2023; Zare et al., 2023; Rengarajan et al., 2022). It is possible to address this shortcoming by defining a sparse reward, that is, assigning a positive constant value only to certain transitions. As an example, consider the case of a robot moving in a room, whose objective is to move to a certain goal, given an initial position. If the distance between the robot and the goal position can be computed, then the inverse of that distance could be used as a good, dense reward signal; if this is not possible, though, a good solution would be to assign a value of 1 upon reaching the goal, and 0 otherwise. The latter case represents an example of sparse reward. Additionally, such reward is also delayed, as the robot will not observe it until the end of an episode.\nUsing sparse and delayed reward simplifies the task of specifying the goal of a task; unfortunately, the sporadic feedback turns the task into an harder one to learn. In particular, a sparse reward signal could prevent an agent from collecting meaningful experience, thus delaying or preventing learning (Lillicrap et al., 2019; Rengarajan et al., 2022)."}, {"title": "2. Related Work", "content": "A summary of the information discussed in this Section is reported in Table 1. The Table reports a list of approaches that are closely related to our proposal. For each source, we report a brief description of its main points, a list of compatible DRL algorithms, whether the optimality of expert is assumed, and a short overview of its main features.\nIn detail, Deep-Q-Learning from Demonstrations (DQfD) (Hester et al., 2017) and DDPG from Demonstrations (DDPGfD) (Vecerik et al., 2018) leverage human-collected trajectories and add a supervised margin loss term, a 1-step and an n-step double Q-Learning terms, and an L2 regularization term to the loss. Each term addresses and corrects a specific failure case, such as a non-representative dataset for the margin loss and stability of training for the double Q-learning terms.\nIn (Schmitt et al., 2018), authors collect experience from previously trained agents to train new ones. This approach assumes that an optimal policy exists and can be trained for the task at hand. As such, we point out that such method would be best applicable to e.g. transfer learning from simulations to autonomous agents such as robots acting in the real world.\nFor QDagger (Agarwal et al., 2022), authors suggest that suboptimal data coming from either pre-trained policies or human experts could be used to train a student network. Their method is based on adding an entropy-based penalty term to the loss function. Additionally, QDagger requires an offline pre-training phase, which we avoid in our proposal.\nIn (Rengarajan et al., 2022), the authors propose a new framework, Learning Online from Guidance Offline (LOGO), and derive a penalty term based on KL-divergence for Trusted Region Policy Optimization (TRPO) (Schulman et al., 2017a). More specifically, authors propose to alternate between a policy improvement and a policy guidance step. During policy improvement, the agent is trained using TRPO. Then, an additional step to guide the policy towards the expert while remaining in the truster region is performed. We point out that the algorithm is specifically tailored on one learning algorithm, TRPO, whereas we propose a generic method that is widely applicable; additionally, LOGO performs two gradient steps for each update, effectively increasing training time. Finally, in the case of partial observability authors propose to train an additional critic for the expert and a discriminator to estimate the current policy performance w.r.t. the expert. We highlight that training additional models might introduce instabilities and propagate errors that lead to catastrophic results.\nSimilarly, Advantage Weighted Actor Critic (AWAC) (Nair et al., 2021) derives a closed-form update to train a policy on a mixture of offline and online data, starting from a"}, {"title": "3. Preliminaries", "content": "In RL and DRL, our aim is to train a policy $\\pi_{\\theta} : S \\rightarrow A$ parameterized by a set of parameters $\\theta$ to select actions that, at any timestep t, maximize the expected sum of discounted future rewards\n$\\mathbb{E} [\\sum_{\\tau=t}^{\\infty} \\gamma^{\\tau-t} r_{\\tau}].$\nWe model our problem as a Markov decision process (MDP). Formally, the problem is defined as a 5-tuple (S, A, T, R, \u03b3), where we denote S C Rd as the d-dimensional state space, A as a continuous or discrete action space, T :S\u00d7A\u2192S the (unknown) transition dynamics, R: S \u00d7 A \u2192 Ra reward function, and \u03b3 \u2208 [0, 1) as the discount factor.\nAdditionally, our image-based experiments are modeled on a partially observable Markov decision process (POMDP). Such problems represent a generalization of MDPs where the full state of the environment is not observable by the agent. Formally, the problem is defined as a 7-tuple (S, A, T, R, \u03a9, \u039f, \u03b3) where, other than the previously defined S, A, T, and R, we introduce a set of (partial) observations \u2286 Rd, and a function O : S \u00d7 A \u2192 \u03a9 mapping the probability of observing \u03bf \u2208 \u03a9, given a certain state s \u2208 S and an action a \u2208 A."}, {"title": "3.2. Clipped Double-Q-Learning", "content": "Clipped Double-Q-Learning (cDQL) (Fujimoto et al., 2018) is an off-policy value function algorithm for DRL based on the concept of temporal difference (TD) error. The algorithm is a tweaked version of DQN that improves stability during training. At each timestep t, a policy $\\pi_{\\theta}$ greedily selects an action at following\n$a_t = \\arg\\max_{a \\in A} Q_\\theta^t(s_t, a)$\nwhere\n$Q^\\pi(s_t, a) = \\mathbb{E}[R_t | s_t, a, \\pi], R_t = \\sum_\\tau^\\infty \\gamma^{\\tau - t} r_\\tau$. Subsequently, the algorithm updates its Q-values using\n$Q^{t+1}(s_{t+1}, a) = \\mathbb{E}[r_t + \\gamma \\max_{a'\\in A} Q^t(s_t, a) | s_{t+1}, a]."}, {"title": "3.3. Advantage Weighted Actor Critic", "content": "Given a guidance policy $\\pi_\\beta$, AWAC aims at efficiently finding an optimal policy $\\pi^*$ by leveraging a dataset D collected from $\\pi_\\beta$. In particular, a parameterized policy $\\pi_{\\theta}$ is trained following\n$\\theta_{k+1} = \\arg\\max_{\\theta} \\mathbb{E}_{s, a \\sim D} [\\log \\pi_{\\theta}(a | s) \\exp{\\frac{1}{T} A^{\\pi_\\beta}(s, a)}]$ which is equivalent to adding a weighted regularization term $D_{KL}(\\pi_\\theta || \\pi_\\beta)$ to the off-policy loss (Equations 6 and ??)."}, {"title": "3.4. Bayesian online adaptation", "content": "BOA uses Bayesian statistics to update the belief of a DRL agent, given a parameterized policy $\\pi_{\\theta}$, a prior distribution $\\pi_\\rho(a | s_t)$ and an expert policy $\\pi_E$. By modeling the prior as\n$\\pi_\\rho(a_{\\pi_{\\theta}} \\mid s_t) \\sim Dirichlet(K, a_{prior})$ with $K = |A|$, $a_{prior, i} = \\mathbb{E}(\\pi_0(a) = i \\mid s_t)$ and we leverage Bayesian statistics to compute the posterior\n$\\pi_{\\theta}(a | a_{\\pi_E}, s_t) \\propto \\pi_E(a_{\\pi_E} | a_{\\theta}, s_t) \\pi_\\rho(a_{\\pi_\\theta} | s_t)$ which is still a Dirichlet with K components and $a_{posterior} = a_{prior} + c_t$, where ct is a vector storing the number of occurrences of each action sampled from $\\pi_E$. In substance, we can update the belief of a network following the \"suggestions\" of an arbitrary expert policy. In (Malato & Hautam\u00e4ki, 2024), $\\pi_E$ is chosen to be a simple search-based policy that, at each timestep t, retrieves the k-most similar stored latents to the current state st from a pre-encoded, small demonstration dataset."}, {"title": "4. Adversarial Estimates", "content": "As highlighted in Table 1, previous research incorporates offline experience, either sampled from a previously trained policy or collected by human contractors, to improve the"}, {"title": "4.1. Mathematical derivation", "content": "The first part of our derivation follows (Nair et al., 2021). We aim at solving a constrained optimization problem of"}, {"title": "5. Experiments", "content": "We use vanilla cDQL as main comparison for our experiments, and compare them to our improved agents. Additionally, we include HER, QDagger, LOGO and AWAC as additional baselines. As some of the agent we tested require pre-training, we train a simple Behavioral Cloning (BC) (Torabi et al., 2018) baseline and use it as expert policy. Each BC agent is trained on the same dataset that we leverage with our method. To select the best possible expert policy, we extensively tested each BC policy during training and retained only the one with the highest mean reward. Our source code for the experiment is available at [hidden information]\u00b9."}, {"title": "6. Results", "content": "The results of our experiments are shown in Figure 3. For each agent, we report the value of the reward on the y-axis and the number of timesteps on the x-axis. Our results are also publicly available on Weight&Biases (Biewald et al.)"}, {"title": "7. Conclusions", "content": "We have presented Adversarial Estimates, a new approach to improve the sample efficiency of a DRL agent based on"}, {"title": "Impact Statement", "content": "This paper presents a new approach for improving sample efficiency in Reinforcement Learning (RL). We believe that our contributions could pose another step towards making RL more applicable in real world scenarios. In particular, improving sample efficiency with the use of minimal data leads to better and faster convergence, hence lowering the costs of such applications and increasing their appeal. Simultaneously, using small datasets reduces the cost of gathering examples, leading to more contained time and monetary budgets."}]}