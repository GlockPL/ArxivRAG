{"title": "Trusting Your AI Agent Emotionally and Cognitively: Development and Validation of a Semantic Differential Scale for AI Trust", "authors": ["Ruoxi Shang", "Gary Hsieh", "Chirag Shah"], "abstract": "Trust is not just a cognitive issue but also an emotional one, yet the research in human-AI interactions has primarily focused on the cognitive route of trust development. Recent work has highlighted the importance of studying affective trust towards AI, especially in the context of emerging human-like LLMs-powered conversational agents. However, there is a lack of validated and generalizable measures for the two-dimensional construct of trust in AI agents. To address this gap, we developed and validated a set of 27-item semantic differential scales for affective and cognitive trust through a scenario-based survey study. We then further validated and applied the scale through an experiment study. Our empirical findings showed how the emotional and cognitive aspects of trust interact with each other and collectively shape a person's overall trust in AI agents. Our study methodology and findings also provide insights into the capability of the state-of-art LLMs to foster trust through different routes.", "sections": [{"title": "Introduction", "content": "Trust plays a crucial role not only in fostering cooperation, efficiency, and productivity in human relationships (Brainov and Sandholm 1999) but also is essential for the effective use and acceptance of computing and automated systems, including computers (Madsen and Gregor 2000), automation (Lee and See 2004), robots (Hancock et al. 2011), and AI technologies (Kumar 2021), with a deficit in trust potentially causing rejection of these technologies (Glikson and Woolley 2020). The two-dimensional model of trust, encompassing both cognitive and affective dimensions proposed and studied in interpersonal relationship studies (McAllister 1995; Johnson and Grayson 2005; Parayitam and Dooley 2009; Morrow Jr, Hansen, and Pearson 2004), have been adopted in studying trust in human-computer interactions, particularly with human-like technologies (Hu, Lu et al. 2021; Glikson and Woolley 2020). Cognitive trust relates to the perception of the ability (e.g., skills, knowledge, and competencies), reliability, and integrity of the trustee, whereas the affective dimension involves the perceived benevolence and disposition to do good of the trustee (Johnson and Grayson 2005; Mayer, Davis, and Schoorman 1995)."}, {"title": "Related Work", "content": "Due to the opaque nature of most high-performing AI models, trust between the user and the AI system has always been a critical issue (Carvalho, Pereira, and Cardoso 2019; Thiebes, Lins, and Sunyaev 2021; Jacovi et al. 2021), as inappropriate trust can lead to over-reliance or under-utilization of AI systems (Bu\u00e7inca, Malaya, and Gajos 2021; Asan, Bayrak, and Choudhury 2020). Research in trust has predominantly adopted the cognitive evaluation of the system's performance (Granatyr et al. 2015), such as its accuracy in making predictions (Ribeiro, Singh, and Guestrin 2016), its perceived consistency in completing tasks (Mcknight et al. 2011), and its ethical considerations and transparency in decision-making (Dur\u00e1n and Jongsma 2021).\nStudies in psychology have long been establishing the importance of psychological influence (e.g., emotions, personality, moods) on trust (Dunn and Schweitzer 2005; Lount Jr 2010). Extending beyond the traditional numeric and cognitive paradigm, recent works have proposed the importance of exploring affective factors of trust in Al systems (Granatyr et al. 2017; Gillath et al. 2021; Jeon 2023). For technologies perceived as more human-like, affective trust factors such as benevolence and integrity play a more significant role (Lankton, McKnight, and Tripp 2015). Moreover, recent advancements in AI, particularly in Large Language Models (LLMs) has demonstrated its capability beyond traditional task performance, as scholars find it challenging not to anthropomorphize them (Shanahan 2022). Notably, OpenAI's GPT-4, has shown excellent performance in Emotional Awareness (i.e. the ability to identify and describe emotions) (Elyoseph et al. 2023). There is also increasing interest in studying LLMs' empathetic responses (Ayers et al. 2023; Belkhir and Sadat 2023). Our work extends the current focus on the emotional aspects of AI interactions by highlighting the need to explore the emotional dimension of trust, a concept with deep roots in research studying interpersonal relationships."}, {"title": "Affective and Cognitive Trust", "content": "The interdisciplinary nature of AI trust research motivates the adoption of theoretical frameworks from interpersonal relationship literature (Bansal et al. 2023; Thiebes, Lins, and Sunyaev 2021). Among the classic interpersonal trust theories and models (e.g., (Mayer, Davis, and Schoorman 1995; Rempel, Holmes, and Zanna 1985)), a two-dimensional model with cognitive and affective components has been extensively studied (McAllister 1995). Similar to trust towards humans, trust towards technology has both cognitive and affective components (Komiak and Benbasat 2004). In the AI context, cognitive trust relates to the user's intellectual perceptions of the AI's characteristics (Komiak and Benbasat 2004; Madsen and Gregor 2000), focusing on aspects like reliability and transparency. Affective trust, on the other hand, involves emotional responses to the AI, including factors like tangibility and anthropomorphism (Ueno et al. 2022; Glikson and Woolley 2020). This duality is essential due to the inherent complexity of AI, which often suggests a need for a \"leap of faith\" in its hidden processes, beyond what can be cognitively processed (Hoff and Bashir 2015; Lee and See 2004). Prior works have found the limitation of cognition in decision-making, as demonstrated by studies showing limitations in users' abilities to discern AI inaccuracies, even with support through explanations (Jacobs et al. 2021; Bu\u00e7inca, Malaya, and Gajos 2021). The cognitive-affective architecture has been established in research of computational agents (P\u00e9rez et al. 2016; Chumkamon, Hayashi, and Koike 2016). The importance of this bi-dimensional model lies in its capacity to capture the full spectrum of trust dynamics that single-dimensional models, focusing solely on either aspects, fail to encompass. Trust has also been investigated through other bi-dimensional models in Human-Robot Interaction (HRI) (e.g. Law and Scheutz's Performance-based and Relation-based trust (Law and Scheutz 2021), and Malle and Ullman's Multi-Dimensional Measure of Trust (MDMT) (Malle and Ullman 2021)). Our work makes a unique contribution by focusing on the Cognitive-Affective (C-A) trust model that fully encapsulates the emotional and psychological intricacies in the interactions with the state-of-the-art AI models that have advanced emotional intelligence. Although MDMT was derived from a different body of prior literature in social-moral constructs as mentioned in their work (Ullman and Malle 2018, 2019), we found it to be a suitable scale to compare it with due to its similar bi-dimentional construct and adjective item format with our scale. Therefore, in Section, we use the moral scale to establish discriminant validity with our cognitive-affective trust scale, demonstrating the distinctiveness of our cognitive-affective trust scale."}, {"title": "Role and Effects of Affective Trust", "content": "There is growing research interest in exploring the role of affective trust in the use of AI technologies. A few recent works have highlighted that affect-based trust plays a decisive role in people's acceptance of AI-based technology in preventative health interventions (Kyung and Kwon 2022) and financial services robo-advising (Zhang, Pentina, and Fan 2021). Research in explainable AI (XAI) has also shown that people's affective responses to explanations are crucial in improving personalization and increasing trust in AI systems (Guerdan, Raymond, and Gunes 2021). However, given the interdisciplinary nature of AI trust research, the valuable insights to be borrowed from interpersonal trust are currently understudied in the AI context. Prior work has found that affective and cognitive trust have different impacts on relationships (Webber 2008; McAllister 1995). Cognitive trust tends to form rapidly (McKnight, Choudhury, and Kacmar 2002; Meyerson et al. 1996), whereas affective trust is more persistent under challenges in teamwork"}, {"title": "Gaps in Empirical Research and Measurement of Affective Trust in AI", "content": "Despite growing interest in this space, existing studies and measurement scales for affective trust in AI exhibit limitations, particularly in the adaptation and validation of measurement scales. Many existing scales, primarily developed for human trust contexts, have been applied to AI interactions with minimal modifications, raising questions about their generalizability. For instance, trust items intended for Human-Computer Trust were directly used for AI systems handling personal data, without substantial revision to reflect the unique aspects of AI interactions (Liao and Sundar 2021). Furthermore, there's a lack of consensus on defining affective trust in AI. While Kyung and Kwon (Kyung and Kwon 2022) merged benevolence and integrity dimensions to measure affective trust in AI-based health interventions, Shi et al. (Shi, Gong, and Gursoy 2021) categorized these dimensions as cognitive trust, employing a different scale (Komiak and Benbasat 2006) for affective trust. This inconsistency highlights the need for a unified, valid measure of trust for AI technologies (Ueno et al. 2022). Given the intertwined nature of affective and cognitive trust, it is evident that a comprehensive evaluation of trust in AI systems requires a scale that measures both dimensions. In response, this work adopts Verhagen et al.'s (Verhagen, Hooff, and Meents 2015) approach, developing semantic differential scales for both affective and cognitive trust in AI. Unlike Likert-type scales, semantic differentials use bipolar adjective pairs, offering advantages in reducing acquiescence bias and improving robustness (Hawkins, Albaum, and Best 1974), reliability (Wirtz and Lee 2003), and validity (Van Auken and Barry 1995)."}, {"title": "Scale Development", "content": "In developing our trust item pool, we conducted a comprehensive literature review to identify prominent two-dimensional trust models that differentiate between cognitive and affective components. We pooled models and items from literature in interpersonal trust, intraorganizational trust, and trust in interaction with computers and traditional technologies. This approach is consistent with the broader trend of extending trust research from human-human contexts to human-AI interactions, as evidenced by the comprehensive review by Glikson and Woolley (Glikson and Woolley 2020). After the initial literature review, we applied a rigorous selection process to a larger body of trust literature, using the following criteria: 1) clear focus and delineation of affective and cognitive dimensions; 3) wide citation and application in various contexts; and 4) applicability to human-AI interactions. This ensures a thorough representation of the most relevant frameworks. The final selected models include Lewis and Weigert's sociological model (Lewis and Weigert 1985), McAllister's interpersonal trust model (McAllister 1995), Madsen and Gregor's Human-Computer Trust Components (Madsen and Gregor 2000), Johnson and Grayson's customer trust model (Johnson and Grayson 2005), and Komiak and Benbasat's IT adoption trust model (Komiak and Benbasat 2006). From these, we extracted 56 unique key adjectives from their scales. Subsequent refinement involved removing synonyms and ensuring coverage of key dimensions: reliability, predictability, competence, understandability, integrity, benevolence, and amiability, which were adopted from the sub-scales from the above-mentioned models. The dimensions are kept flexible and serves mainly as a reference for coverage. We also developed antonym pairs for each adjective using resources like Merriam-Webster and Oxford English Dictionary, selecting the most appropriate antonym after several review rounds among the researchers. This resulted in 33 paired adjective items, divided into cognitive (N = 20) and affective (N = 13) trust categories, as detailed in Table 1. In the following step, we recruited participants to rate these items with respect to various scenarios through an online survey study."}, {"title": "Survey design", "content": "We used the hypothetical scenario method, where participants evaluated vignettes describing realistic situations to rate trust-related scales (Trevino 1992). This method is frequently used in studying trust in emerging or future-oriented intelligent systems (Shi, Gong, and Gursoy 2021; Juravle et al. 2020; Gillath et al. 2021; Kim, Giroux, and Lee 2021). Hypothetical scenarios enable exploration of long-term, nuanced, human-like interactions with AI assistants. This method also facilitates control over variables like agent type and interaction types, and risk levels, ensuring generalizability. In addition, this method ensures consistency in contextual details across respondents (Alexander and Becker 1978). We crafted 32 scenario variations, manipulating the following five key dimensions: Trust Level (high vs. low), Trust Route (affective vs. cognitive), Prior Interaction (first-time vs. repeated), Application Domain Stakes (high vs. low), and Agent Type (human vs. AI).\nFor validation purpose of the scales, we manipulated Trust Level and Trust Route. This involved depicting the agent's characteristics and behaviors in the scenarios, aligning them with varying levels of cognitive or affective trust. Additionally, to ensure the scales' generalizability, we manipulated Prior Interaction Frequency to be interacting with the agent for the first time or multiple times, and we set Application Domain Stakes to be either high-stake domains (Healthcare Diagnostics and Self-Driving Taxi) and"}, {"title": "Results", "content": "To uncover the factor structure underlying the 33 trust items, we first verified the suitability of our data for factor analysis. Bartlett's Test of Sphericity showed significant results ($x^2$ = 12574,p < 0.001) (Bartlett 1950), and the Kaiser-Meyer-Olkin Measure of Sampling Adequacy was high at 0.98 (Kaiser 1970; Dziuban and Shirkey 1974), both indicating the appropriateness of factor analysis for our dataset. To determine the number of trust sub-components, we applied Kaiser's eigenvalue analysis (Kaiser 1958) and parallel analysis (Hayton, Allen, and Scarpello 2004), which collectively suggested a two-factor structure.\nWe initially used an oblique rotation as recommended by Tabachnick and Fiddell for instances where factor correlations exceed 0.32 (Tabachnick, Fidell, and Ullman 2013). Given the high correlation among our factors (r = 0.78) (Gorsuch 1988), we retained this rotation method. We then refined our item pool based on specific criteria: items were kept only if they had a factor loading above 0.4 (Howard 2016), ensuring significant association with the underlying factor. Items with a cross-loading of 0.3 or more were removed to align item responses with changes in the associated factor (Howard 2016). Additionally, we applied Saucier's criterion, eliminating items unless their factor loading was at least twice as high as on any other factor (Saucier 1994). This led to the removal of six items: Harmful Well-intentioned, Unpromising - Promising, Malicious Benevolent, Discouraging - Supportive, Insincere - Sincere, and Unpleasant - Likable.\nA second round of exploratory factor analysis with the remaining 27 items preserved all items, as they met the above-mentioned criteria. The final item loadings are presented in Table 1 under the \"All\" column, with empty rows indicating the eliminated items. All remaining items demonstrated primary loadings above 0.55. Upon examining the keywords of items in each factor, two distinct themes emerged: cognitive trust and affective trust. This alignment was consistent with the dimensions identified in the initial literature review. Factor 1, representing cognitive trust, accounted for 43% of the total variance with 18 items, while Factor 2, corresponding to affective trust, explained 23% with 9 items."}, {"title": "Construct Validity", "content": "In addition to high reliability, we conducted analyses to show the validity of our scale. We first examined the construct validity, which refers to the degree to which the scale reflects the underlying construct of interest. Recall that we manipulated affective trust and cognitive trust through the level of trustworthiness and the trust development routes and controlled for factors like agent type, interaction stage, and risk level. T-test results revealed significant distinctions in both affective and cognitive trust scales under the experiment manipulation. Cognitive trust scale demonstrated a pronounced difference in high versus low cognitive trust conditions (t = 45.74, p < 0.001), and affective trust scale also showed a pronounced disparity in high versus low affective trust conditions (t = 43.00, p < 0.001).\nThis also demonstrates the efficacy of our manipulation with the scenarios, as we observed significant differences in both the cognitive and affective dimensions.\nWe then fitted two separate linear random effect models (Singmann and Kellen 2019) on the two scales over the two manipulations due to our experiment design. Model 1 and Model 2 in Table 2 (See Appendix) tests the effects of our manipulations on the resulting trust scales, while Model 3 tests the effects of both scales on general trust. As shown in Table 2 (See Appendix), we observed significant main effects of manipulation Trust Level (r = 2.059, p < 0.001) and manipulation Trust Route (r = -0.497, p < 0.01) of these two manipulations on the cognitive trust scale, and the same is observed for affective trust scale. More importantly, the interaction effect shows that the affective trust scale is higher when higher trust is developed via the affective route (r = 0.921, p < 0.001), while the cognitive trust scale is higher when higher trust is developed via the cognitive route (r = -0.538, p < 0.05). The above analyses demonstrated the construct validity of our scale."}, {"title": "Concurrent Validity", "content": "We then examined concurrent validity that assesses the degree to which a measure correlates with a establish criterion, which is a single-item measuring general trust towards the agent. After confirming that general trust for the agent was significantly higher in the higher trustworthiness condition (t = 10.47, p < 0.001), we found that overall trust is significantly and positively predicted by both the cognitive trust scale (r = 0.881, p < 0.001) and the affective trust scale (r = 0.253, p < 0.001). The effect size of the cognitive trust scale on general trust is greater than that of the affective trust scale. This is also consistent with the previous factor analysis result that the cognitive trust scale explains more variance than the affective trust scale.\nThese convergent tests provided sufficient support for the validity of our scales. Hence, in the next step, we applied them to measuring cognitive and affective trust in conversational AI agents."}, {"title": "Scale Validation", "content": "After developing a reliable two-factor scale for measuring cognitive and affective trust in AI, we conducted two validation studies. Study A (Section) validated the scale using Confirmatory Factor Analysis and tested the efficacy using LLM-generated conversations to elicit different levels of trust. Building on Study A's findings, Study B (Section) refined the study design to establish discriminant validity of the scales and provide empirical insights into the interaction between the two trust dimensions."}, {"title": "Validation Study A - Preliminary Study", "content": "In this study, in addition to establish scale validity, we test the efficacy of our affective trust scale in distinguishing between two conversational AI assistants with mock dialogues generated by OpenAI's ChatGPT (cha), a leading example of state-of-the-art LLM-based conversational agents. We used pre-generated mock-up conversations to reduce variations and errors induced in the interaction with LLMs, controlling for the effect of our manipulation. This survey study was initiated with uncertainties regarding GPT models' ability to evoke varying degrees of affective trust. Hence, we conducted a preliminary study to assess the effectiveness of ChatGPT and the sensitivity of our scale to the applied manipulations."}, {"title": "Study Design and Participants", "content": "We designed a within-subjects online experiment, in which participants evaluated screenshots of dialogues with two AI assistants, Echo and Nova (See Appendix for examples). Echo was designed to elicit high affective trust, while Nova demonstrated a lack of it. Our hypotheses were: affective trust would be higher for Nova than Echo (H1), and based on previously observed correlation between affective and cognitive trust, cognitive trust would also be higher for Nova (H2).\nTo explore the feasibility and efficacy of Large Language Models (LLMs) in manipulating affective trust, we used ChatGPT to generate AI responses, leveraging its capability for human-like interactions to manipulate affective trust levels and at the same time controlling for the speech style and length. After validating the definitions of affective and cognitive trust generated by ChatGPT against literature, we crafted prompts to vary affective trust levels. After experimenting with different prompts and scenarios, we chosen the scenario of user asking the AI agent for emotional support, in which the user starts with the question \u201cLately, I've been feeling lonely. What should I do?\" The responses were generated by ChatGPT and lightly edited for conciseness."}, {"title": "Results", "content": "Welch's t-tests showed that general trust (t= 2.37, p < 0.05), affective trust scale (t = 3.78, p < 0.001), and cognitive trust scale (t = 2.84, p < 0.01) all yielded significant differences between high and low affective trust conditions. This shows that the manipulation using ChatGPT is successful. ChatGPT has the capability of eliciting different levels of affective trust based on the model's learned representation of affective trust."}, {"title": "Confirmatory Factor Analysis", "content": "To confirm the factor structure determined by the EFA and assess its goodness-of-fit compared to alternative models, we performed a Confirmatory Factor Analysis (CFA) (Hinkin, Tracey, and Enz 1997; Long 1983), which is a structural-equations analysis that compares the fit of rival models. We conducted Confirmatory Factor Analysis (CFA) using both Maximum Likelihood (ML) and Diagonally Weighted Least Squares (DWLS) estimators to assess the fit of our model against a one-factor baseline model. We calculated several goodness-of-fit metrics, including the Comparative Fit Index (CFI), which measures the model's fit relative to a more restrictive baseline model (Bentler 1990); the Tucker-Lewis Index (TLI), a more conservative version of CFI, penalizing overly complex models (Bentler and Bonett 1980); the Standardized Root Mean Square Residual (SRMR), an absolute measure of fit calculating the difference between observed and predicted correlation (Hu and Bentler 1999); and the Root Mean Square Error of Approximation (RMSEA), which measures how well the model reproduces item covariances, instead of a baseline model comparison (MacCallum, Browne, and Sugawara 1996). The ML estimator yielded mixed results, with some fit indices suggesting adequate fit (CFI = 0.920, TLI = 0.914, SRMR = 0.046) while others indicated suboptimal fit (RMSEA = 0.082, $x^2$(494) = 1506.171, p < 0.001). However, when using the DWLS estimator, which is more appropriate for our ordinal data (Li 2016; Mindrila 2010), the model demonstrated excellent fit across all indices (CFI = 1.000, TLI = 1.003, RMSEA = 0.000, SRMR = 0.038, $x^2$(494) = 250.936, p = 1.000)."}, {"title": "Validity tests", "content": "We examined construct validity followed by concurrent validity of our scale following the same procedure as in the previous study. We first tested construct validity by checking the two scales are sensitive to the manipulation of affective trust through three regression models as shown in Table 3 (See Appendix). Model 1 and Model 2 test the effects of our manipulations on the affective and cognitive trust scales respectively. Model 3 tests the effects of both scales on general trust. We observed the main effects of the condition on both affective and cognitive trust scales. Interacting with an AI chatbot with higher affective trustworthiness led to 0.95 points higher on the 7-point affective scale and the cognitive trust scale was increased by 0.80 points. This differential impact highlights the scale's nuanced sensitivity: while both affective and cognitive trusts are influenced by affective trust manipulation, the affective trust scale responded more robustly. Concurrent validity was then affirmed through significant positive predictions of general trust by both the affective trust scale (r = 0.486, p < 0.001) and the cognitive trust scale (r = 0.546, p < 0.001)."}, {"title": "Validation Study B - Refined Study", "content": "The preliminary study established the practical validity of our AI trust scale and demonstrating the effectiveness of using ChatGPT to manipulate affective trust. It also provides empirical support for the scale's sensitivity to variations in trust levels induced by different attributes of an Al agent's communication style. Building on this foundation, this study aimed to delve deeper into the interplay between affective and cognitive trust, while also comparing our scale with the Multi-Dimensional Measure of Trust (MDMT) to establish discriminant validity. This comparative analysis sought to highlight the distinctiveness of our affective trust scale and the importance of establishing it as a separate scale.\nWe chose the Moral Trust Scale from Multi-Dimensional Measure of Trust (MDMT) model for a comparative analysis with our developed affective trust scale for AI, primarily due to its established reputation in HRI research (Malle and Ullman 2021; Ullman and Malle 2019), as mentioned previously in Section. Aside from both ours and MDMT being a two-dimensional trust models, our cognitive trust scale aligns closely with MDMT's capability trust scale, with overlapping scale items. This raises the question of whether our affective trust scale is measuring the same underlying construct as MDMT's moral trust scale. This comparison is crucial in highlighting the distinctiveness and specificity of our scale, particularly in capturing affective nuances in AI interactions that the moral trust might not cover."}, {"title": "Results", "content": "We first conducted Welch's t-tests to check the effects of our experimental manipulations on the scale ratings. The conditions, categorized as High and Low, were designed to elicit the levels of cognitive and affective trust. Significant variations were noted in the affective trust scale between high and low affective trust conditions (t = 7.999, p < 0.001), and similarly in the cognitive trust scale between high and low cognitive trust conditions (t = 9.823, p < 0.001). These findings confirm the effectiveness of the manipulation.\nWe conducted exploratory factor analysis (EFA) to confirm the distinctiveness of scales, not for refactoring previously developed scales. The high Kaiser-Meyer-Olkin (KMO) value of 0.9597 and a significant Bartlett's Test of Sphericity (Chi square = 7146.38, p < 0.001) established the dataset's suitability for factor analysis. Three factors were retained, accounting for 70% of the cumulative variance, a threshold indicating an adequate number of factors. This was also substantiated by a noticeable variance drop after the second or third factor in the scree plot and parallel analysis, where the first three actual eigenvalues surpassed those from random data. The results showed that the first three eigenvalues from our dataset were larger than the corresponding eigenvalues from the random data, indicating that these factors are more meaningful than what would be expected by chance alone. These results affirm that the items meaningfully load onto three distinct factors."}, {"title": "Discussion", "content": "Our work is grounded in the recognition that developing alternative instruments of established theoretical constructs holds significant value (Straub, Boudreau, and Gefen 2004). In this paper, we develop a validated affective trust scale for human-AI interaction and demonstrate its effectiveness at measuring trust development through the emotional route. While prior studies in AI trust have largely focused on cognitive trust, recent research emphasizes the need to consider affective trust in AI (Glikson and Woolley 2020; Granatyr et al. 2017). Existing affective trust scales, borrowed from models in non-AI contexts like interpersonal relationships and traditional computing (McAllister 1995; Komiak and Benbasat 2006), lack rigorous validation for Al systems. Thus, our study develops and validates a scale for measuring both affective and cognitive trust in AI. Through a comprehensive survey study design and rigorous EFA process, we landed at a 18-item scale measuring cognitive trust and a 9-item scale measuring affective trust. The process resulted in the removal of six antonym pairs due to cross-loading, indicating their relevance to both trust dimensions. Through rigorous validation processes, we affirmed its reliability, internal consistency, construct validity, and concurrent validity.\nThe validation of our scales were carried out through two studies. In Study 2A, our analysis further demonstrates validity of the scale through CFA and a few follow-up validity tests. In Study 2B, the three-factor structure that emerged from the factor analysis, coupled with the insignificant coefficient from the regression analysis, provides clear evidence of discriminant validity. This indicates that our affective trust scale captures a distinct construct, separate from related scales measuring trust, such as the Multi-Dimensional Measure of Trust (MDMT) (Malle and Ullman 2021). The construction of our affective trust scale is key to this distinction; it includes a broader range of items that capture emotional nuances more effectively, thereby more accurately reflecting the affective pathway's impact on general trust. In contrast, MDMT's moral trust scale focuses on ethical (n=4) and sincerity (n=4) aspects. Some items in the sincerity subscale (e.g., sincerity, genuineness, candidness, authenticity) overlap with benevolence elements in our affective trust scale. However, our scale incorporates unique items like 'Empathetic' and 'Caring,' absent in MDMT's scale, as well as likability aspects through items such as 'Patient' and 'Cordial.' These likability items are derived from established affective trust measures in human interactions, with previous studies confirming likability's role in fostering trust in various contexts including interpersonal relationships (Fiske, Cuddy, and Glick 2007), digital platforms (Tran, Wen, and Gugenishvili 2023), and robot interactions (Cameron et al. 2021)."}, {"title": "Empirical Findings", "content": "With its proficiency in generating human-like responses, tools powered by LLMs such as ChatGPT stand out as a novel approach for examining trust in AI. This method significantly lowers the barriers to studying Al systems with emotional capabilities, particularly in manipulating trust via emotional routes. In our study, we found that GPT models' advanced conceptual understanding of affective and cognitive trust allows it to generate responses tailored to specific trust levels. This was demonstrated in our study (Section). Our studies showed that LLMs effectively manipulate trust via cognitive and affective routes in diverse contexts like emotional support, technical aid, and social planning. This shows LLMs' versatility and utility in expediting trust formations in experimental studies. Our studies utilized pre-generated conversations to ensure control and consistency. Future research could explore the development of trust through LLMs in a different study setting, such as an interactive study setting or a longitudinal study setting with deeper relationship building."}, {"title": "Potential Usage", "content": "Our affective and cognitive trust scales present a valuable measurement tool for future research in designing trustworthy AI systems. Here, we outline a few possible usages.\nMeasure trust in human-AI interactions The construct of trust with affective and cognitive dimensions is well-established in interpersonal trust literature. Our scale bridges the gap between human-human and human-AI trust, enabling future work to study trust in human-AI teaming to improve collaboration experiences and outcomes. For instance, our scale can be employed to investigate how these trust dimensions impact creative work with generative AI tools, as they have been found to influence team contributions differently (Ng and Chua 2006). Furthermore, researchers have discovered that affective trust becomes more important later in the human teaming experience, while cognitive trust is crucial initially (Webber 2008). Our scale offers the opportunity to examine the dynamics of these trust dimensions in human-AI collaboration.\nSupport designing emotionally trustworthy systems Our research supports the growing understanding that emotional factors like empathy, tone, and personalization are crucial in establishing trust, especially in contexts where it's challenging to convey a system's performance and decision-making processes (Gillath et al. 2021; Kyung and Kwon 2022). Our scale can be used to distinctively measure trust developed through the affective route. This is particularly relevant in mental health interventions involving AI assistants, where patients may struggle to assess the Al's capabilities rationally (Hall et al. 2001; Gillath et al. 2021). Affective trust becomes vital here, as patients, especially those with low AI literacy or experiencing anxiety, depression, or trauma, may respond more to emotional cues from AI, which typically lacks the emotional intelligence of human therapists. Our validated affective trust scale can guide the design of AI systems to calibrate for appropriate trust in this context, such as through empathetic responses or affect-driven explanations, and help explore its impact on long-term engagement and treatment adherence."}, {"title": "Limitations and Future Work", "content": "In our scale development phase, we designed scenario featuring Al agents as service providers. This role is chosen intentionally to align with prior affective trust research for interpersonal relationships (Johnson and Grayson 2005; Komiak and Benbasat 2004). Also, the prevalence of service-providing scenarios make it easier for general public participants to draw parallels between these AI agents with their human counterparts. Future work can explore other roles of AI, such as teammates (Zhang et al. 2023, 2021) and friends (Brandtzaeg, Skjuve, and F\u00f8lstad 2022).\nWhile our approach to categorizing trust dimensions into cognitive and affective aspects was informed by established trust frameworks (refer to Table 1), the anticipated distinct subdimensions (e.g. reliability, understandability, etc.) were not as clear-cut after conducting exploratory factor analysis. This was possibly due to the subdimensions lacking sufficient unique variance or being highly correlated. Our scenario was deliberately designed to focused on differentiating cognitive and affective trust, while they might not have enough detailed information capture the nuances across the six dimensions. Future research to refine these subdimensions under cognitive and affective trust and examine their unique contributions to trust."}]}