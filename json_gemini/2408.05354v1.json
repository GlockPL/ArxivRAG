{"title": "Trusting Your AI Agent Emotionally and Cognitively: Development and Validation of a Semantic Differential Scale for AI Trust", "authors": ["Ruoxi Shang", "Gary Hsieh", "Chirag Shah"], "abstract": "Trust is not just a cognitive issue but also an emotional one, yet the research in human-AI interactions has primarily focused on the cognitive route of trust development. Recent work has highlighted the importance of studying affective trust towards AI, especially in the context of emerging human-like LLMs-powered conversational agents. However, there is a lack of validated and generalizable measures for the two-dimensional construct of trust in AI agents. To address this gap, we developed and validated a set of 27-item semantic differential scales for affective and cognitive trust through a scenario-based survey study. We then further validated and applied the scale through an experiment study. Our empirical findings showed how the emotional and cognitive aspects of trust interact with each other and collectively shape a person's overall trust in AI agents. Our study methodology and findings also provide insights into the capability of the state-of-art LLMs to foster trust through different routes.", "sections": [{"title": "Introduction", "content": "Trust plays a crucial role not only in fostering cooperation, efficiency, and productivity in human relationships (Brainov and Sandholm 1999) but also is essential for the effective use and acceptance of computing and automated systems, including computers (Madsen and Gregor 2000), automation (Lee and See 2004), robots (Hancock et al. 2011), and AI technologies (Kumar 2021), with a deficit in trust potentially causing rejection of these technologies (Glikson and Woolley 2020). The two-dimensional model of trust, encompassing both cognitive and affective dimensions proposed and studied in interpersonal relationship studies (McAllister 1995; Johnson and Grayson 2005; Parayitam and Dooley 2009; Morrow Jr, Hansen, and Pearson 2004), have been adopted in studying trust in human-computer interactions, particularly with human-like technologies (Hu, Lu et al. 2021; Glikson and Woolley 2020). Cognitive trust relates to the perception of the ability (e.g., skills, knowledge, and competencies), reliability, and integrity of the trustee, whereas the affective dimension involves the perceived benevolence and disposition to do good of the trustee (Johnson and Grayson 2005; Mayer, Davis, and Schoorman 1995). In the context of human-computer trust, cognition-based trust is built on the user's intellectual perceptions of the system's characteristics, whereas affect-based components are those which are based on the user's emotional responses to the system (Madsen and Gregor 2000).\nWhile AI trust research has largely centered on technical reliability and competency, there is a notable lack of work that explores the affective routes of trust development. The recent advancement of text-based Large Language Models (LLMs) have demonstrated a remarkable ability to take on diverse personas and skill-sets, recognizing and responding to people's emotional needs during conversation-based interactions. This capability is crucially aligned with the increasing focus on simulating Affective Empathy in human-AI interactions (Paiva et al. 2017; Welivita, Xie, and Pu 2021). In light of this, there is growing research interest in studying affective aspects of trust in AI (Glikson and Woolley 2020; Granatyr et al. 2017; Kyung and Kwon 2022; Zhang, Pentina, and Fan 2021; Guerdan, Raymond, and Gunes 2021). However, a critical gap exists in the lack of generalizable and accurate specialized measurement tools for assessing affective trust in the context of AI, especially with the enhanced and nuanced capabilities of LLMs. This highlights a need for a better measurement scale for affective trust to gain a deeper understanding of how trust dynamics function, particularly in the context of emotionally intelligent AI.\nIn this paper, we introduce a 27-item semantic differential scale for assessing cognitive and affective trust in AI, aiding researchers and designers in understanding and improving human-AI interactions. Our motivation and scale development process is based on a long strand of prior research on the cognitive-affective construct of trust that has been shown to be important in interpersonal trust in organizations, human trust in conventional technology and automation, and more recently in trust towards AI. Our use of OpenAI's ChatGPT to generate different levels of affective trust further demonstrates a scalable method for studying the emotional pathway to AI trust. Empirically, we contribute findings on the interplay and distinction between cognitive, affective, and moral trust. The paper is structured to highlight these contributions: Section describes the development and validation of our trust scale through an experimental survey study and factor analysis. The validation and application"}, {"title": "Related Work", "content": "Shifting Paradigm of AI Trust Research\nDue to the opaque nature of most high-performing AI models, trust between the user and the AI system has always been a critical issue (Carvalho, Pereira, and Cardoso 2019; Thiebes, Lins, and Sunyaev 2021; Jacovi et al. 2021), as inappropriate trust can lead to over-reliance or under-utilization of AI systems (Bu\u00e7inca, Malaya, and Gajos 2021; Asan, Bayrak, and Choudhury 2020). Research in trust has predominantly adopted the cognitive evaluation of the system's performance (Granatyr et al. 2015), such as its accuracy in making predictions (Ribeiro, Singh, and Guestrin 2016), its perceived consistency in completing tasks (Mcknight et al. 2011), and its ethical considerations and transparency in decision-making (Dur\u00e1n and Jongsma 2021).\nStudies in psychology have long been establishing the importance of psychological influence (e.g., emotions, personality, moods) on trust (Dunn and Schweitzer 2005; Lount Jr 2010). Extending beyond the traditional numeric and cognitive paradigm, recent works have proposed the importance of exploring affective factors of trust in Al systems (Granatyr et al. 2017; Gillath et al. 2021; Jeon 2023). For technologies perceived as more human-like, affective trust factors such as benevolence and integrity play a more significant role (Lankton, McKnight, and Tripp 2015). Moreover, recent advancements in AI, particularly in Large Language Models (LLMs) has demonstrated its capability beyond traditional task performance, as scholars find it challenging not to anthropomorphize them (Shanahan 2022). Notably, OpenAI's GPT-4, has shown excellent performance in Emotional Awareness (i.e. the ability to identify and describe emotions) (Elyoseph et al. 2023). There is also increasing interest in studying LLMs' empathetic responses (Ayers et al. 2023; Belkhir and Sadat 2023). Our work extends the current focus on the emotional aspects of AI interactions by highlighting the need to explore the emotional dimension of trust, a concept with deep roots in research studying interpersonal relationships."}, {"title": "Affective and Cognitive Trust", "content": "The interdisciplinary nature of AI trust research motivates the adoption of theoretical frameworks from interpersonal relationship literature (Bansal et al. 2023; Thiebes, Lins, and Sunyaev 2021). Among the classic interpersonal trust theories and models (e.g., (Mayer, Davis, and Schoorman 1995; Rempel, Holmes, and Zanna 1985)), a two-dimensional model with cognitive and affective components has been extensively studied (McAllister 1995). Similar to trust towards humans, trust towards technology has both cognitive and affective components (Komiak and Benbasat 2004). In the AI context, cognitive trust relates to the user's intellectual perceptions of the AI's characteristics (Komiak and Benbasat 2004; Madsen and Gregor 2000), focusing on aspects like reliability and transparency. Affective trust, on the other hand, involves emotional responses to the AI, including factors like tangibility and anthropomorphism (Ueno et al. 2022; Glikson and Woolley 2020). This duality is essential due to the inherent complexity of AI, which often suggests a need for a \"leap of faith\" in its hidden processes, beyond what can be cognitively processed (Hoff and Bashir 2015; Lee and See 2004). Prior works have found the limitation of cognition in decision-making, as demonstrated by studies showing limitations in users' abilities to discern AI inaccuracies, even with support through explanations (Jacobs et al. 2021; Bu\u00e7inca, Malaya, and Gajos 2021). The cognitive-affective architecture has been established in research of computational agents (P\u00e9rez et al. 2016; Chumkamon, Hayashi, and Koike 2016). The importance of this bi-dimensional model lies in its capacity to capture the full spectrum of trust dynamics that single-dimensional models, focusing solely on either aspects, fail to encompass. Trust has also been investigated through other bi-dimensional models in Human-Robot Interaction (HRI) (e.g. Law and Scheutz's Performance-based and Relation-based trust (Law and Scheutz 2021), and Malle and Ullman's Multi-Dimensional Measure of Trust (MDMT) (Malle and Ullman 2021)). Our work makes a unique contribution by focusing on the Cognitive-Affective (C-A) trust model that fully encapsulates the emotional and psychological intricacies in the interactions with the state-of-the-art AI models that have advanced emotional intelligence. Although MDMT was derived from a different body of prior literature in social-moral constructs as mentioned in their work (Ullman and Malle 2018, 2019), we found it to be a suitable scale to compare it with due to its similar bi-dimentional construct and adjective item format with our scale. Therefore, in Section, we use the moral scale to establish discriminant validity with our cognitive-affective trust scale, demonstrating the distinctiveness of our cognitive-affective trust scale."}, {"title": "Role and Effects of Affective Trust", "content": "There is growing research interest in exploring the role of affective trust in the use of AI technologies. A few recent works have highlighted that affect-based trust plays a decisive role in people's acceptance of AI-based technology in preventative health interventions (Kyung and Kwon 2022) and financial services robo-advising (Zhang, Pentina, and Fan 2021). Research in explainable AI (XAI) has also shown that people's affective responses to explanations are crucial in improving personalization and increasing trust in AI systems (Guerdan, Raymond, and Gunes 2021). However, given the interdisciplinary nature of AI trust research, the valuable insights to be borrowed from interpersonal trust are currently understudied in the AI context. Prior work has found that affective and cognitive trust have different impacts on relationships (Webber 2008; McAllister 1995). Cognitive trust tends to form rapidly (McKnight, Choudhury, and Kacmar 2002; Meyerson et al. 1996), whereas affective trust is more persistent under challenges in teamwork"}, {"title": "Gaps in Empirical Research and Measurement of Affective Trust in AI", "content": "Despite growing interest in this space, existing studies and measurement scales for affective trust in AI exhibit limitations, particularly in the adaptation and validation of measurement scales. Many existing scales, primarily developed for human trust contexts, have been applied to AI interactions with minimal modifications, raising questions about their generalizability. For instance, trust items intended for Human-Computer Trust were directly used for AI systems handling personal data, without substantial revision to reflect the unique aspects of AI interactions (Liao and Sundar 2021). Furthermore, there's a lack of consensus on defining affective trust in AI. While Kyung and Kwon (Kyung and Kwon 2022) merged benevolence and integrity dimensions to measure affective trust in AI-based health interventions, Shi et al. (Shi, Gong, and Gursoy 2021) categorized these dimensions as cognitive trust, employing a different scale (Komiak and Benbasat 2006) for affective trust. This inconsistency highlights the need for a unified, valid measure of trust for AI technologies (Ueno et al. 2022). Given the intertwined nature of affective and cognitive trust, it is evident that a comprehensive evaluation of trust in AI systems requires a scale that measures both dimensions. In response, this work adopts Verhagen et al.'s (Verhagen, Hooff, and Meents 2015) approach, developing semantic differential scales for both affective and cognitive trust in AI. Unlike Likert-type scales, semantic differentials use bipolar adjective pairs, offering advantages in reducing acquiescence bias and improving robustness (Hawkins, Albaum, and Best 1974), reliability (Wirtz and Lee 2003), and validity (Van Auken and Barry 1995)."}, {"title": "Scale Development", "content": "Initial Item Generation\nIn developing our trust item pool, we conducted a comprehensive literature review to identify prominent two-dimensional trust models that differentiate between cognitive and affective components. We pooled models and items from literature in interpersonal trust, intraorganizational trust, and trust in interaction with computers and traditional technologies. This approach is consistent with the broader trend of extending trust research from human-human contexts to human-AI interactions, as evidenced by"}, {"title": "Survey design", "content": "We used the hypothetical scenario method, where participants evaluated vignettes describing realistic situations to rate trust-related scales (Trevino 1992). This method is frequently used in studying trust in emerging or future-oriented intelligent systems (Shi, Gong, and Gursoy 2021; Juravle et al. 2020; Gillath et al. 2021; Kim, Giroux, and Lee 2021). Hypothetical scenarios enable exploration of long-term, nuanced, human-like interactions with AI assistants. This method also facilitates control over variables like agent type and interaction types, and risk levels, ensuring generalizability. In addition, this method ensures consistency in contextual details across respondents (Alexander and Becker 1978). We crafted 32 scenario variations, manipulating the following five key dimensions: Trust Level (high vs. low), Trust Route (affective vs. cognitive), Prior Interaction (first-time vs. repeated), Application Domain Stakes (high vs. low), and Agent Type (human vs. AI).\nFor validation purpose of the scales, we manipulated Trust Level and Trust Route. This involved depicting the agent's characteristics and behaviors in the scenarios, aligning them with varying levels of cognitive or affective trust. Additionally, to ensure the scales' generalizability, we manipulated Prior Interaction Frequency to be interacting with the agent for the first time or multiple times, and we set Application Domain Stakes to be either high-stake domains (Healthcare Diagnostics and Self-Driving Taxi) and"}, {"title": "Measurement and Variables", "content": "In our survey, we evaluated several key variables. For Affective and Cognitive trust, we used our semantic differential scale, where participants rated 33 adjective antonym pairs on a scale of -2 (most negative) to 2 (most positive). General trust was measured using a single-item questionnaire adapted from Yin (Yin, Wortman Vaughan, and Wallach 2019), where participants responded to the question \"how much do you trust this AI assistant to provide you with the guidance and service you needed\" on a 5-point Likert scale, ranging from 1 (\"I don't trust this agent at all\") to 5 (\"I fully trust this AI\"). AI literacy was assessed using items adapted from Wang (Wang, Rau, and Yuan 2022), all rated on a 5-point Likert scale from \"Strongly disagree\" to \"Strongly agree\", including items like \"I can identify the AI technology in the applications I use\" and \"I can choose the most appropriate AI application for a task\""}, {"title": "Participants", "content": "Amazon Mechanical Turk (MTurk) has been frequently used to recruit participants for online scenario-based studies related to AI technologies (Antes et al. 2021; Kim, Giroux, and Lee 2021; Kaur, Lampe, and Lasecki 2020). We recruited 200 participants from the United States through Amazon Mechanical Turk. The eligibility criteria included a minimum of 10,000 HITs Approved and an overall HIT Approval Rate of at least 98%. Each participant received a compensation of $2.20. The study involved repeated measures, collecting two sets of responses per participant for the two scenarios. Our quality control measures included a time delay for scenario reading, four attention checks, exclusions for uniform ratings or completion times more than two standard deviations from the mean, and a randomized sequence to control for order effects. After applying these criteria, we excluded 49 participants, resulting in 151 valid responses for the final analysis."}, {"title": "Results", "content": "Exploratory Factor Analysis To uncover the factor structure underlying the 33 trust items, we first verified the suitability of our data for factor analysis. Bartlett's Test of Sphericity showed significant results ($x^2$ = 12574,p < 0.001) (Bartlett 1950), and the Kaiser-Meyer-Olkin Measure of Sampling Adequacy was high at 0.98 (Kaiser 1970; Dziuban and Shirkey 1974), both indicating the appropriateness of factor analysis for our dataset. To determine the number of trust sub-components, we applied Kaiser's eigenvalue analysis (Kaiser 1958) and parallel analysis (Hayton, Allen, and Scarpello 2004), which collectively suggested a two-factor structure.\nWe initially used an oblique rotation as recommended by Tabachnick and Fiddell for instances where factor correlations exceed 0.32 (Tabachnick, Fidell, and Ullman 2013). Given the high correlation among our factors (r = 0.78) (Gorsuch 1988), we retained this rotation method. We then refined our item pool based on specific criteria: items were kept only if they had a factor loading above 0.4 (Howard 2016), ensuring significant association with the underlying factor. Items with a cross-loading of 0.3 or more were removed to align item responses with changes in the associated factor (Howard 2016). Additionally, we applied Saucier's criterion, eliminating items unless their factor loading was at least twice as high as on any other factor (Saucier 1994). This led to the removal of six items: Harmful - Well-intentioned, Unpromising - Promising, Malicious - Benevolent, Discouraging - Supportive, Insincere - Sincere, and Unpleasant - Likable.\nA second round of exploratory factor analysis with the remaining 27 items preserved all items, as they met the above-mentioned criteria. The final item loadings are presented in Table 1 under the \"All\" column, with empty rows indicating the eliminated items. All remaining items demonstrated primary loadings above 0.55. Upon examining the keywords of items in each factor, two distinct themes emerged: cognitive trust and affective trust. This alignment was consistent with the dimensions identified in the initial literature review. Factor 1, representing cognitive trust, accounted for 43% of the total variance with 18 items, while Factor 2, corresponding to affective trust, explained 23% with 9 items.\nReliability To test the internal reliability of the resulting items, we computed Cronbach's a for each scale. The cognitive trust scale (a = .98) and the affective trust scale (a = .96) both showed high internal consistency. We also tested the item-total correlation between each item and the average of all other items in the same sub-scale. All items' correlations exceed 0.6. In this development study, 18 items measuring cognitive trust and 9 items measuring affective"}, {"title": "Construct Validity", "content": "In addition to high reliability, we conducted analyses to show the validity of our scale. We first examined the construct validity, which refers to the degree to which the scale reflects the underlying construct of interest. Recall that we manipulated affective trust and cognitive trust through the level of trustworthiness and the trust development routes and controlled for factors like agent type, interaction stage, and risk level. T-test results revealed significant distinctions in both affective and cognitive trust scales under the experiment manipulation. Cognitive trust scale demonstrated a pronounced difference in high versus low cognitive trust conditions (t = 45.74, p < 0.001), and affective trust scale also showed a pronounced disparity in high versus low affective trust conditions (t = 43.00, p < 0.001). This also demonstrates the efficacy of our manipulation with the scenarios, as we observed significant differences in both the cognitive and affective dimensions.\nWe then fitted two separate linear random effect models (Singmann and Kellen 2019) on the two scales over the two manipulations due to our experiment design. Model 1 and Model 2 in Table 2 (See Appendix) tests the effects of our manipulations on the resulting trust scales, while Model 3 tests the effects of both scales on general trust. As shown in Table 2 (See Appendix), we observed significant main effects of manipulation Trust Level (r = 2.059, p < 0.001) and manipulation Trust Route (r = -0.497, p < 0.01) of these two manipulations on the cognitive trust scale, and the same is observed for affective trust scale. More importantly, the interaction effect shows that the affective trust scale is higher when higher trust is developed via the affective route (r = 0.921, p < 0.001), while the cognitive trust scale is higher when higher trust is developed via the cognitive route (r = -0.538, p < 0.05). The above analyses demonstrated the construct validity of our scale."}, {"title": "Concurrent Validity", "content": "We then examined concurrent validity that assesses the degree to which a measure correlates with a establish criterion, which is a single-item measuring general trust towards the agent. After confirming that general trust for the agent was significantly higher in the higher trustworthiness condition (t = 10.47, p < 0.001), we found that overall trust is significantly and positively predicted by both the cognitive trust scale (r = 0.881, p < 0.001) and the affective trust scale (r = 0.253, p < 0.001). The effect size of the cognitive trust scale on general trust is greater than that of the affective trust scale. This is also consistent with the previous factor analysis result that the cognitive trust scale explains more variance than the affective trust scale. These convergent tests provided sufficient support for the validity of our scales. Hence, in the next step, we applied them to measuring cognitive and affective trust in conversational AI agents."}, {"title": "Scale Validation", "content": "After developing a reliable two-factor scale for measuring cognitive and affective trust in AI, we conducted two validation studies. Study A (Section) validated the scale using Confirmatory Factor Analysis and tested the efficacy using LLM-generated conversations to elicit different levels of trust. Building on Study A's findings, Study B (Section) refined the study design to establish discriminant validity of the scales and provide empirical insights into the interaction between the two trust dimensions."}, {"title": "Validation Study A - Preliminary Study", "content": "In this study, in addition to establish scale validity, we test the efficacy of our affective trust scale in distinguishing between two conversational AI assistants with mock dialogues generated by OpenAI's ChatGPT (cha), a leading example of state-of-the-art LLM-based conversational agents. We used pre-generated mock-up conversations to reduce variations and errors induced in the interaction with LLMs, controlling for the effect of our manipulation. This survey study was initiated with uncertainties regarding GPT models' ability to evoke varying degrees of affective trust. Hence, we conducted a preliminary study to assess the effectiveness of ChatGPT and the sensitivity of our scale to the applied manipulations."}, {"title": "Study Design and Participants", "content": "We designed a within-subjects online experiment, in which participants evaluated screenshots of dialogues with two AI assistants, Echo and Nova (See Appendix for examples). Echo was designed to elicit high affective trust, while Nova demonstrated a lack of it. Our hypotheses were: affective trust would be higher for Nova than Echo (H1), and based on previously observed correlation between affective and cognitive trust, cognitive trust would also be higher for Nova (H2).\nTo explore the feasibility and efficacy of Large Language Models (LLMs) in manipulating affective trust, we used ChatGPT to generate AI responses, leveraging its capability for human-like interactions to manipulate affective trust levels and at the same time controlling for the speech style and length. After validating the definitions of affective and cognitive trust generated by ChatGPT against literature, we crafted prompts to vary affective trust levels. After experimenting with different prompts and scenarios, we chosen the scenario of user asking the AI agent for emotional support, in which the user starts with the question \u201cLately, I've been feeling lonely. What should I do?\" The responses were generated by ChatGPT and lightly edited for conciseness.\nIn addition to measuring affective and cognitive trust with our 27-item developed scale, we also included disposition to trust, AI literacy, age, and gender were included as control variables because previous studies have demonstrated their impacts on trust (Shi, Gong, and Gursoy 2021). AI familiarity was measured by 3 survey questions including \"I am familiar with using an AI-powered chatbot to help me with specific tasks\" on a 7-point Likert scale. Al literacy is measured by the same items as in the previous survey. Trust Disposition was measured by items adopted from prior work (Gefen 2000). General trust in each chatbot was measured using a one-item scale adapted from prior research (Ueno et al. 2022).\nWe conducted our experiment via Amazon MTurk, where participants viewed two screenshots, each depicting a three-"}, {"title": "Results", "content": "t-tests Welch's t-tests showed that general trust (t = 2.37, p < 0.05), affective trust scale (t = 3.78, p < 0.001), and cognitive trust scale (t = 2.84, p < 0.01) all yielded significant differences between high and low affective trust conditions. This shows that the manipulation using ChatGPT is successful. ChatGPT has the capability of eliciting different levels of affective trust based on the model's learned representation of affective trust.\nConfirmatory Factor Analysis To confirm the factor structure determined by the EFA and assess its goodness-of-fit compared to alternative models, we performed a Confirmatory Factor Analysis (CFA) (Hinkin, Tracey, and Enz 1997; Long 1983), which is a structural-equations analysis that compares the fit of rival models. We conducted Confirmatory Factor Analysis (CFA) using both Maximum Likelihood (ML) and Diagonally Weighted Least Squares (DWLS) estimators to assess the fit of our model against a one-factor baseline model. We calculated several goodness-of-fit metrics, including the Comparative Fit Index (CFI), which measures the model's fit relative to a more restrictive baseline model (Bentler 1990); the Tucker-Lewis Index (TLI), a more conservative version of CFI, penalizing overly complex models (Bentler and Bonett 1980); the Standardized Root Mean Square Residual (SRMR), an absolute measure of fit calculating the difference between observed and predicted correlation (Hu and Bentler 1999); and the Root Mean Square Error of Approximation (RMSEA), which measures how well the model reproduces item covariances, instead of a baseline model comparison (MacCallum, Browne, and Sugawara 1996). The ML estimator yielded mixed results, with some fit indices suggesting adequate fit (CFI = 0.920, TLI = 0.914, SRMR = 0.046) while others indicated suboptimal fit (RMSEA = 0.082, $x^2$(494) = 1506.171, p < 0.001). However, when using the DWLS estimator, which is more appropriate for our ordinal data (Li 2016; Mindrila 2010), the model demonstrated excellent fit across all indices (CFI = 1.000, TLI = 1.003, RMSEA = 0.000, SRMR = 0.038, $x^2$(494) = 250.936, p = 1.000). Robust fit indices, which account for non-normality in the data, also supported the model's fit using both estimators (ML: Robust CFI = 0.941, Robust TLI = 0.937, Robust RMSEA = 0.071; DWLS: Robust CFI = 0.998, Robust TLI = 0.997, Robust RMSEA = 0.035). The model fit significantly better than the baseline model using both ML ($x^2$(528) = 13132.525, p < 0.001) and DWLS ($x^2$(528) = 78914.753,"}, {"title": "Validity tests", "content": "We examined construct validity followed by concurrent validity of our scale following the same procedure as in the previous study. We first tested construct validity by checking the two scales are sensitive to the manipulation of affective trust through three regression models as shown in Table 3 (See Appendix). Model 1 and Model 2 test the effects of our manipulations on the affective and cognitive trust scales respectively. Model 3 tests the effects of both scales on general trust. We observed the main effects of the condition on both affective and cognitive trust scales. Interacting with an AI chatbot with higher affective trustworthiness led to 0.95 points higher on the 7-point affective scale and the cognitive trust scale was increased by 0.80 points. This differential impact highlights the scale's nuanced sensitivity: while both affective and cognitive trusts are influenced by affective trust manipulation, the affective trust scale responded more robustly. Concurrent validity was then affirmed through significant positive predictions of general trust by both the affective trust scale (r = 0.486, p < 0.001) and the cognitive trust scale (r = 0.546, p < 0.001)."}, {"title": "Validation Study B - Refined Study", "content": "The preliminary study established the practical validity of our AI trust scale and demonstrating the effectiveness of using ChatGPT to manipulate affective trust. It also provides empirical support for the scale's sensitivity to variations in trust levels induced by different attributes of an Al agent's communication style. Building on this foundation, this study aimed to delve deeper into the interplay between affective and cognitive trust, while also comparing our scale with the Multi-Dimensional Measure of Trust (MDMT) to establish discriminant validity. This comparative analysis sought to highlight the distinctiveness of our affective trust scale and the importance of establishing it as a separate scale.\nWe chose the Moral Trust Scale from Multi-Dimensional Measure of Trust (MDMT) model for a comparative analysis with our developed affective trust scale for AI, primarily due to its established reputation in HRI research (Malle and Ullman 2021; Ullman and Malle 2019), as mentioned previously in Section. Aside from both ours and MDMT being a two-dimensional trust models, our cognitive trust scale aligns closely with MDMT's capability trust scale, with overlapping scale items. This raises the question of whether our affective trust scale is measuring the same underlying construct as MDMT's moral trust scale. This comparison is crucial in highlighting the distinctiveness and specificity of our scale, particularly in capturing affective nuances in AI interactions that the moral trust might not cover.\nThe findings from the preliminary laid the groundwork for the more complex experimental designs in this study. This study refined the previous design into a 2x2 fully-crossed factorial model with between-subject design, contrasting high and low levels of affective and cognitive trust. Multi-turn Q&A conversations in each scenario were used to"}, {"title": "Results", "content": "t-tests for Manipulation Check We first conducted Welch's t-tests to check the effects of our experimental manipulations on the scale ratings. The conditions, categorized as High and Low, were designed to elicit the levels of cognitive and affective trust. Significant variations were noted in the affective trust scale between high and low affective trust conditions (t = 7.999, p < 0.001), and similarly in the cognitive trust scale between high and low cognitive trust conditions (t = 9.823, p < 0.001). These findings confirm the effectiveness of the manipulation.\nFactor Analysis We conducted exploratory factor analysis (EFA) to confirm the distinctiveness of scales, not for refactoring previously developed scales. The high Kaiser-Meyer-Olkin (KMO) value of 0.9597 and a significant Bartlett's Test of Sphericity (Chi square = 7146.38, p < 0.001) established the dataset's suitability for factor analysis. Three factors were retained, accounting for 70% of the cumulative variance, a threshold indicating an adequate number of factors. This was also substantiated by a noticeable variance drop after the second or third factor in the scree plot and parallel analysis, where the first three actual eigenvalues surpassed those from random data. The results showed that the first three eigenvalues from our dataset were larger than the corresponding eigenvalues from the random data, indicating that these factors are more meaningful than what would be expected by chance alone. These results affirm that the items meaningfully load onto three distinct factors.\nOur analysis used a factor loading threshold of 0.5 for clear factor distinctiveness. As shown in Table 4 (See Appendix), EFA resulted in two main factors aligned with cognitive and affective trust scales, and a third factor predominantly linked to the Moral Decision-Making Trust (MDMT) scale, particularly its Ethical (Ethical, Principled, Has Integrity) and Sincere (Authentic, Candid) subscales. Items on MDMT's scale showed lower factor loadings in the same analysis, particularly in the emotional dimension, suggesting a weaker representation of affective elements. These outcomes underscore the distinct nature of the MDMT scale from the affective trust scale. Despite the overall clear conceptual distinction, we noted that the MDMT's \"Sincere\" item and several cognitive trust items (Rational, Consistent, Predictable, Understandable, Careful, Believable) showed overlap across factors. This could be attributed to our study's design, which exclusively incorporates scenarios tailored to elicit affective and cognitive trust. This design choice was made to specifically examine these two types of trust, and also served as a way to determine if the moral trust scale reflects similar elements or different trust aspects not pertinent to our scenarios."}, {"title": "Regression Analysis", "content": "We conducted regression analysis to compare the predictive power of the scales on general trust. Table 4 (See Appendix) details this: Model 1 examines the effects of all three scales on general trust; Model 2 considers only cognitive and affective trust scales; and Model 3 includes the moral trust scale, excluding affective trust. This approach allows for comparison of the two related scales' contributions to general trust, while controlling for manipulation and other variables to observe in-group effects.\nThe results showed distinct contributions of each scale to general trust. Affective trust was a significant predictor in Model 1 (r = 0.364, p < 0.01) and Model 2 (r = 0.376, p < 0.01), whereas the moral trust scale showed non-significant correlations in all models. This suggests its limited relevance in scenarios dominated by emotional and cognitive cues. In contrast, the affective trust scale's significant impact highlights its ability to capture trust dimensions not addressed by the moral trust scale, demonstrating their distinctiveness."}]}