{"title": "Mechanisms of Generative Image-to-Image Translation Networks", "authors": ["Guangzong Chen", "Mingui Sun", "Zhi-Hong Mao", "Kangni Liu", "Wenyan Jia"], "abstract": "Generative Adversarial Networks (GANs) are a class of neural networks that have been widely used in the field of image-to-image translation. In this paper, we propose a streamlined image-to-image translation network with a simpler architecture compared to existing models. We investigate the relationship between GANs and autoencoders and provide an explanation for the efficacy of employing only the GAN component for tasks involving image translation. We show that adversarial for GAN models yields results comparable to those of existing methods without additional complex loss penalties. Subsequently, we elucidate the rationale behind this phenomenon. We also incorporate experimental results to demonstrate the validity of our findings.", "sections": [{"title": "I. INTRODUCTION", "content": "The advancement of large neural networks has significantly improved the performance of image-to-image translation tasks. Its high accuracy and flexibility attract many researchers in various fields. Industries, such as healthcare, automotive, and entertainment, utilize image-to-image translation technologies for different applications, including medical imaging, autonomous driving, and digital content creation [1], [2], [3]. In addition, researchers in academia and the private sectors are continuously innovating to explore new possibilities and advances in this area. Image-to-image translation encompasses a wide range of tasks, including edge-to-image, photo-to-painting, etc. [1], [4], [5]. All of these tasks need significant computational and data resources for the training model. Depending on the complexity of the model and the size of the dataset, training can take from hours to weeks.\nA myriad of methodologies have been advanced to address the image-to-image translation problem. Despite most existing models are able to solve the problem, they do not explain the mechanisms by which the network distinguishes content from style [6], [7], [8], [9], [10]. The nebulous definitions of content and style pose significant challenges in the mathematical characterization of the image translation process. Moreover, existing models for image-to-image translation often employ Generative Adversarial Networks (GANs) architecture, but encompass significant complexity, incorporating elements such as cycle loss, identity loss, and penalties on intermediate features. Rarely is the necessity of these intricate penalties examined.\nPreviously, we introduced a GAN-based model to transform food images using only GAN penalty without any additional penalties [4]. In this paper, we investigate the similarity between Generative Adversarial Networks (GANs) [11] and autoencoders [12] to elucidate the GAN model mechanism for image-to-image translation without imposing additional penalties. Subsequently, we show the rationale behind the efficacy of employing solely the GAN component for image-to-image translation tasks. We offer a clear explanation that substantiates the primary role of GAN components in addressing the image-to-image translation problem.\nWe have conducted a comprehensive review and analysis of the models employed for image generation and image-to-image translation. Our investigation focuses on identifying the efficacy of various components of the network. Notably, we discovered that the autoencoder and GAN models generate homologous output and provide an explanation for this phenomenon. This explanation also extends to the efficiency of GANs in the context of image-to-image translation. From our perspective, we employ a preliminary GAN for image-to-image translation. Furthermore, our findings elucidate why some examples in the network may fail.\nThis paper makes the following contributions: (i) We demonstrate that with a discriminator of sufficient capacity to distinguish between real and synthetic images, adversarial training for autoencoder models yields results similar to those of traditional autoencoder models. This is substantiated through experimental validation. (ii) We extend adversarial training to the image-to-image translation problem, illustrating that a straightforward GAN model can preserve common features and generate novel ones, whereas previous methods impose additional penalties to maintain common features. (iii) Our work provides a rationale for the efficacy of GANs in the image-to-image translation context, clarifying that the decomposition of texture and content signifies common and differentiating characteristics determined by the dataset. This offers a more precise and comprehensive understanding compared to previous studies.\nThe paper is structured as follows: The related works section gives a brief review of image generation and translation. The methods section provides our explanation, encompassing algebraic and geometric interpretations. Subsequently, the experiment section presents three experiments. The first experiment compares the performance of GANs and autoencoders, the second investigates the model's capability for image-to-image translation, and the third examines the constraints outlined in the methods section. Finally, conclusions are drawn based on our analysis."}, {"title": "II. RELATED WORKS", "content": ""}, {"title": "A. Generative Adversarial Networks (GANs)", "content": "GANs are widely utilized for image generation. These architectures are composed of a generator (G) and a discriminator (D) that compete in a min-max game during training. Numerous variations of GANs have been proposed to enhance their performance, such as CGAN [13], [14], [15], CVAE-GAN [16], VQ-GAN [17], StyleGAN [18], GigaGAN [19] and so on [20]. Additionally, extensive research has been conducted to address issues such as mode collapse and unstable training [20]. These contributions substantially advance the capability of GANs in producing high-fidelity images."}, {"title": "B. Image Translation", "content": "Gatys et al. proposed a seminal approach in which they demonstrated that style and content could be separated within a convolutional network. They used feature maps to capture the content and a Gram Matrix to capture the style [21]. The style transfer has become increasingly popular with a lot of researchers. Furthermore, numerous models have been introduced for image-to-image translation. CycleGAN [6], DualGAN [7], and similar models posited that the transformation between two domains should be invertible. These models used two GANs to learn invertible image translation. Other approaches like MUNIT [8], DRIT++ [22], TransferI2I [9], assumed that style and content are controlled by different sets of latent variables. Based on this assumption, they developed various network structures to achieve the desired translations. Palette employs a diffusion model for image-to-image translation [5]. However, its applicability is limited to tasks such as inpainting, colorization, and uncropping.\nZheng et. al. [23] addressed the issue of imbalanced image datasets using a multiadversarial framework. In addition, they introduce an asynchronous generative adversarial network to boost model performance. Yang et al. enhance the quality of the generated images through semantic cooperative shape perception [24]. Additionally, researchers apply various tech-niques such as multi-constraints, semantic integration, and a unified circular framework to refine image-to-image translation models by modifying model specifics [25], [26], [27], [25], [28], [29], [30]."}, {"title": "C. Network Explanation", "content": "Besides these models that provide methods for image-to-image translation, a variety of approaches have been suggested to clarify the fundamental processes driving the network's functioning from different analytical perspectives.\nClassification models are essential elements of GANs. The foundational theory underlying these models is vital for the proper function of GANs. Yarotsky established error limits for network [31], while Wang et al. determined error bounds for both multi-layer perceptrons and convolutional neural net-works. These studies demonstrate the theoretical correctness of convolutional neural networks [32].\nBeyond the classification model, Ye et al. introduced deep convolutional framelets as described in [33]. They utilized deep convolutional framelets to explain a model comparable to U-Net, proposing an approach that captures finer details than U-Net. This model helps to comprehend the roles of various components, such as the number of features, skip connections, and concatenation within the network.\nIn the context of generator networks, the variational autoencoder (VAE) and diffusion models are well explained [12], [34], [35]. The VAE focuses on minimizing the evidence lower bound (ELBO), whereas the diffusion model views the network's process as a Markov chain and derives its loss function based on the characteristics of a Markov chain. Generally, a GAN model trains a model that distinguishes the difference between real and fake. However, when GANS are applied to image-to-image translation tasks, a significant portion of the research centers on developing heuristic models, and much of the interpretation of these models is heuristic."}, {"title": "III. METHODS", "content": "The aim of this section is to elucidate the mechanism of adversarial training within the context of image-to-image translation challenges. Initially, we focus on a specific in-stance: the identity image translation task. Subsequently, we broaden our analysis to encompass the general image-to-image translation paradigm, providing a comprehensive explanation to demonstrate how GAN models can be applied to image-to-image translation tasks.\nThe task of recovering an image from a latent space is com-monly addressed through autoencoders. This issue is similar to the image reconstruction. However, in image reconstruction, the input image may exhibit certain defects that require correc-tion. In contrast, in our scenario, the input and output images are identical. Our findings demonstrate that employing either of the two methodologies yields similar results. Consequently, these conclusions can be extrapolated to the image translation problem.\nAutoencoders are widely employed to derive latent variables from input images. It is also used in image reconstruction applications. The main objective of an autoencoder is to learn a mapping function $G(x)$, capable of reconstructing the input image $x$. The generator $G$, comprises an encoder and a decoder, where the encoder is utilized to obtain the latent variable and the decoder reconstructs the image from the latent variable.\nAdversarial training, in this paper, is defined by the in-troduction of a mapping function $D$ which apparent the differences between authentic images $x$ and reconstructed images $G(x)$. It is similar to the discriminator function in GAN. The difference between the mapping function $D$ and the discriminator in GAN is that $D$ does not use binary output while the discriminator function in GAN requires binary output. The discriminator in GANs is a special case of the mapping function $D$. The training framework is a min-max game between $G$ and $D$, in which $D$ aims to maximize the loss function, while $G$ aims to minimize it.\nFig. 1 shows two distinct network architectures for genera-tive learning and autoencoder. The right is adversarial training. The left is the autoencoder. For the autoencoder, the goal is to employ a model to recreate the input data. Adversarial training involves alternating the learning of $G$ and $D$, where $G$ generates images and $D$ identifies the differences between the input and the generated output. A random variable $z$ is sampled from a Gaussian distribution and used exclusively to produce"}, {"title": "A. Similarity Between Autoencoder and Adversarial Training Under Certain Condition", "content": "In this subsection, we demonstrate that autoencoders and adversarial training yield similar results given two specific constraints. Firstly, the generator must have the ability to reconstruct the input image. Secondly, the mapping function D should accurately perceive the distinction between x and G(x).\n1) Algebraic Explanation: Let $I = {x^{(1)}, x^{(2)},...,x^{(m)}} = {x_1^{(1)}, x_2^{(1)},...,x_n^{(1)} , x_1^{(2)}, x_2^{(2)},...,x_n^{(2)},..., x_1^{(m)}, x_2^{(m)},...,x_n^{(m)}} \\in \\mathbb{R}^n$ be a set of data, where $x^{(i)}$ is the $i$-th sample. The optimization problem of the autoencoder is formulated as:\n$\\min_G L = \\frac{1}{m}\\sum_{x \\in I}||x - G(x)||$  (1)\nwhere $||\\cdot||$ is L\u2081 norm, which is the sum of the absolute values of the element of vector.\nThe adversarial training incorporates an additional mapping function $D$, which maps $x$ to a vector $D(x)$, with $D(x)$ belonging to $\\mathbb{R}^n$.After transformation, in the new space, $D(x)$ and $D(G(x))$ are linearly separable. It is important to note that the GAN requires a binary output from the discriminator, whereas the mapping function $D$ projects to a new space with dimension n.\nThe optimization problem of adversarial training is defined as follows:\n$\\min_G \\max_D L = \\frac{1}{m} \\sum_{x \\in I} ||D(x) - D(G(x))||$   (2)\nwhere $D(x)$ is $D(x) = [D_1(x), D_2(x),\\dots, D_n(x)]$.\nThe main difference between autoencoders and adversarial training is the presence of an auxiliary function $D$. This additional component augments the differences between the input data points $x$ and their generated data $G(x)$, which helps to train the generator. Both algorithms aim to make $G(x)$ close to $x$, leading to similar results. However, they might produce different results because, near the optimal solution, $D$ in adversarial training can become oscillating, causing $G$ to fluctuate around the optimum. In contrast, the autoencoder will converge to the optimal solution.\nIn (2), the training data is paired, which means $x$ and $G(x)$ must be considered together when computing the loss function. We will now demonstrate that adversarial training can be performed without paired data. If the function $D$ can maximize the loss function and perfect distinguish between $x$ and $G(X)$ on each feature, then there must be a function $D$ that $D_i(x) > D_i(G(x))$ and optimize the loss function at the same time. Then we have the following loss function:\n$L = \\frac{1}{m}\\sum_{x \\in I} \\sum_{i} [D_i(x) - D_i(G(x))]$. (3)\nRearranging the equation, we have:\n$L = \\frac{1}{m} \\sum_{x \\in I} \\sum_{i} D_i(x) - \\frac{1}{m} \\sum_{x \\in I} \\sum_{i} D_i(G(x))$.  (4)\nWe can define another function $D(x) = \\sum_i D_i(x)$, where $D(x) \\in \\mathbb{R}$. And the loss function can be written as:\n$L = \\frac{1}{m}\\sum_{x \\in I} D(x) - \\frac{1}{m} \\sum_{x \\in I} D(G(x))$.  (5)\nBecause $D$ only required to distinguish different features in $x$ and $G(x)$, we consider using random variables and distribution to model the problem. Let $P_{data}$ be the distribution of the data set and $p_g$ be the distribution of the generator's output, and replacing average with expectation, then we have\n$L = E_{x \\sim P_{data}(x)}[D(x)] - E_{x \\sim p_g(x)}[D(x)]$.  (6)\nThis is similar to the WGAN loss function [36]. From (2), we know that $G(x)$ will be push to $x$ when minimizing the loss function. Therefore, adversarial training should produce results similar to autoencoder models. The equation (6) tells us that if the discriminator $D$ can perfectly distinguish the data from $P_{data}$ and and $p_g$, the loss function will not depend on the order of x and G(x)."}, {"title": "2) Geometric Interpretation:", "content": "We also present a geometric interpretation of why adversarial training can produce results similar to the autoencoder. Fig. 2 shows the status of the early stage of the model training. After learning D(.) in the max part of the min-max optimization problem (2), we project x's and G(x)'s onto a new feature space where the set of x's and the set of G(x)'s are well clustered and can be separated by a hyperplane\u2014a linear boundary, similar to how the data with different labels are separated in the support vector machine (SVM). If we map the dividing surface to the original space, a nonlinear boundary will emerge to distinguish x's from G(x)'s. When solving the min part of the min-max optimization problem for G(\u00b7), G(x)'s will move toward the boundary, getting closer to x's, as demonstrated by the red arrows in Fig. 2. Through the alternating training of G and D, G(x)'s become closer to the set of x, effectively pushing both G(x)'s and x's toward the boundary. This process is likely to bring each pair of G(x) and x close to each other.\nFig. 3 illustrates the effects of G(\u00b7) and D(\u00b7) after training. Within the transformed space, D(x)'s and D(G(x))'s are distributed along the hyperplane. In the original space, the boundary is nonlinear, and x's and G(x)'s scatter close to each other.\nFrom this perspective, the result of adversarial training will be similar to the autoencoder when $I^s$ and $I^t$ are from the same distribution. This observation may contradict our initial expectations that GANs could generate any sample that fits the distribution of the dataset. However, our findings indicate that the adversarial model will produce the input data without imposing a reconstruction penalty between $x$ and $G(x)$."}, {"title": "B. Image-to-Image Translation", "content": "The network architecture is depicted on the left of Fig. 1. It incorporates two datasets: The first image dataset, $I^s$, is used as shape reference, where $I^s$ equals {$I_i|i = 1, ..., N$}, $I \\in \\mathbb{R}^{H \\times W \\times 3}$, $H$ and $W$ are the height and width of the images, 3 is the number of channels of an RGB image, and $N$ is the total number of images. The second image dataset, $I^t$, is used to provide texture information, where $I^t$ equals {$I_i|i = 1,...,M$}, $I \\in \\mathbb{R}^{H \\times W \\times 3}$, and $M$ the size of the second dataset. This dataset is provided to the discriminator D, to train the network.\nWe want to apply $I^s$ to facilitate the network to generate images with the same shapes as the images in $I^s$ while maintaining the textures from $I^t$. For example, zebras and horses share a common body shape but differ in texture. The dataset $I^s$ comprises horse images, whereas the $I^t$ consists of zebra images. Image translations aims at substitute the horse image texture with that of the zebra.\nIn the self-translation task, the mapping function D is required to verify that all features in both x and G(x) are identical. On the other hand, in the image-to-image translation task, the discriminator's role is to confirm that all features in the generated image match the distribution of the $I^t$."}, {"title": "IV. EXPERIMENTAL RESULTS AND DISCUSSIONS", "content": "We conducted three experiments. First, we verified our theoretical finding that GANs produce similar results with the autoencoder models when the reference images $I^s$ and texture images $I^t$ are the same. Second, we showcased the GAN model's capability to transform images from one domain to another. Lastly, we modified the dataset size and the generator configuration to examine the impact of the constraints as discussed in the Methods section.\nWe evaluated our model on various datasets, such as Animal FacesHQ (AFHQ) [37], Photo-to-Van Gogh, Photo-to-Monet from CycleGAN [6], and Flickr-Faces-HQ (FFHQ) [18]. The AFHQ dataset consists of 16130 images of animal faces, each with a 1024 \u00d7 1024 pixel resolution, covering three categories of animals: cat, dog, and wild. The Photo-to-Van Gogh and Photo-to-Monet has approximately 1000 images for each category. The FFHQ dataset is a high-quality collection of human facial images. It comprises 70000 images, all at a resolution of 1024 \u00d7 1024 pixels. In this study, we resized images to 512 \u00d7 512 resolution for all experiments.\nFor both the generator and discriminator, we utilized Style-GAN v2 [18] as the foundational architecture. Given that an additional encoder is required to encode the image into features, we used a simple convolutional network as the encoder, which comprises only convolution, downsampling, and ReLU activation."}, {"title": "A. Comparison Between GAN and Autoencoder", "content": "In this subsection, we used the AFHQ dataset to demon-strate the correctness of our analysis in the methods section.\nWe claim that GANs and autoencoders can produce similar results when the generator and discriminator have enough capacity. We used the mean square error between orignal image and generated image to evaluate the performance of the two models. Fig. 4 shows the reconstruction loss for three different models during the training phase.\nTo ensure a fair comparison between the GAN and the au-toencoder, we computed the reconstruction loss for generations after every 1000 images used to train the model. The green curve is generated by the autoencoder, the red curve by the GAN, and the yellow curve represents the reconstruction loss of the GAN model, when $I^s$ and $I^t$ differ. These findings suggest that when $I^s$ and $I^t$ are equivalent, both the GAN and autoencoder are effective in minimizing the reconstruction loss. Despite the fact that reconstruction loss is not utilized during the training of the GAN model, this reinforces the validity of our analysis in methods section."}, {"title": "B. Image-to-Image Translation Capability", "content": "When $I^s$ and $I^t$ are different, our method can be used for image-to-image translation and the same feature in both dataset will be preserved. Compared to other methods, the network is simpler, and we can predict the outcomes and provide explanations for the results.\nFig. 7 shows animal transfer examples. The first column displays the input, followed by four columns showing the outputs. The dogs faces are used as $I^s$ and cats faces as $I^t$. The generated cat face retains the same orientation as the dog face. In addition, the relative positions of facial features such as the eyes, nose, and ears remain uniform.\nA translation between an artwork and a photograph is also illustrated. In Fig. 8, the first column shows the input, while the subsequent columns show the output. It is noticeable that the objects remain the same, but the textures are different in the output. However, in the first row, the shape of the mountain appears slightly altered. According to our explanation, this happens because the input shape of the mountain is absent in the target dataset, causing the network to modify the mountain's shape."}, {"title": "C. Constraints Analysis", "content": "In the previous subsection, we demonstrated that our method can generate results similar to those of an autoencoder and also shows that the network has the capacity to solve image-to-image translation tasks. However, our method hinges on two critical conditions: first, the generator must be capable of completely reconstructing the input image; second, the discriminator must be able to perfectly distinguish between real and fake images whenever there is a discrepancy. In this subsection, we discuss the impact of these two conditions.\nWe consider the generator to be composed of two parts: an encoder and a decoder. If the encoder's capacity is insufficient, it can only retain certain features, implying that the generator will fail to produce an exact match of the input when dealing with a large dataset. Evaluating the condition on discriminator is inherently challenging, but it is known that smaller dataset"}, {"title": "1) Image Translation With High Dimension Features:", "content": "The results of human face transfer are shown in Fig. 9. The first column is the input image and the following columns are the corresponding output images. The output images look like a series of selfies of similar people with different detail texture. The global topology information, such as the positions of the eyes, nose, and mouth, is maintained in the same positions as the input. The detail features, such as skin folds and hair color, are randomly set.\nThe same model was been applied to the AFHQ dataset. However, the number of images is only 3000, while the human face dataset has 70000 images. The result is shown in Fig. 10. Compared to Fig. 9, the only difference is the colors of the output images in the same row. All other features remain the same.\nThe varying outcomes of the two experiments are due to differences in the sizes of the datasets. When the size of the dataset is relatively low, the discriminator possesses sufficient capability to distinguish differences, causing the output the GAN converge that of autoencoder. This demonstrates the validity of the analysis in the method section."}, {"title": "2) Image Translation with Low Dimension Features Transfer:", "content": "The experiment in this subsection is similar to previous subsection. The only difference is the intermediate feature decrease from 16 \u00d7 16 \u00d7 128 to 8 \u00d7 8 \u00d7 128, which makes the encoder not able to reserve all features from inputs. The result is shown in Fig. 11 and 12.\nIn face-to-face translation, the input image is in the first column, followed by the corresponding output images in the subsequent columns. Unlike in Fig. 9, the difference between each image in the same row is more pronounced. The image does not depict people with slightly different. Instead, Fig. 11 shows people of different sex, gender, and other details. The common feature is that they take selfies from the same angle and maintain the same pose.\nIn Fig. 12, discerning the similarity becomes even more challenging. The first column shows the input, while the rest display the output. It shows that within the same row, the animal species and angles of the photos differ. However, we observed that, at the beginning of the training process, the network retains the pose and angle of the input image for animal data. However, as training progresses, these features are discarded to enhance the realism of the output image if the capacity of the network is not insufficient. This is because the network is confused on which part of the feature should be preserved. This also shows that our analyses are correct."}, {"title": "V. CONCLUSION", "content": "Our study provides new insights into the effectiveness of GANs in tasks involving image-to-image translation. We have shown that adversarial training, when applied to autoencoder models, can achieve results comparable to traditional methods without the necessity for additional complex loss penalties. Furthermore, we explained the differences and similarities between GANs and autoencoders. We also incorporated ex-perimental results to demonstrate the validity of our findings."}]}