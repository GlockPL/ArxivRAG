{"title": "Survey and Taxonomy: The Role of Data-Centric AI in Transformer-Based Time Series Forecasting", "authors": ["Jingjing Xu", "Caesar Wu", "Yuan-Fang Li", "Gr\u00e9goire Danoy", "Pascal Bouvry"], "abstract": "Alongside the continuous process of improving AI performance through the development of more sophisticated models, researchers have also focused their attention to the emerging concept of data-centric AI, which emphasizes the important role of data in a systematic machine learning training process. Nonetheless, the development of models has also continued apace. One result of this progress is the development of the Transformer Architecture, which possesses a high level of capability in multiple domains such as Natural Language Processing (NLP), Computer Vision (CV) and Time Series Forecasting (TSF). Its performance is, however, heavily dependent on input data preprocessing and output data evaluation, justifying a data-centric approach to future research. We argue that data-centric AI is essential for training AI models, particularly for transformer-based TSF models efficiently. However, there is a gap regarding the integration of transformer-based TSF and data-centric AI. This survey aims to pin down this gap via the extensive literature review based on the proposed taxonomy. We review the previous research works from a data-centric AI perspective and we intend to lay the foundation work for the future development of transformer-based architecture and data-centric AI.", "sections": [{"title": "1 Introduction", "content": "Sequential data like languages and time series can be effectively modeled using recurrent neural networks (RNNs) [1], which connect features across different points in a sequence. Traditional fully connected neural networks (FCNs) lack this capability. However, RNNs face challenges such as vanishing and exploding gradients, which impede their ability to capture long-term dependencies. Solutions include using Rectified Linear Unit (ReLU) activation functions, initializing weights with identity matrices, and implementing gates to control information flow. Architectures like Gated Recurrent Units (GRUs) [2] and Long Short-Term Memory (LSTM) [3] networks use gate cells to manage long-term dependencies, but some challenges remain, particularly in training, which often considers only one direction of data leading to capture less detailed context. Bidirectional RNNs address this by processing data in both forward and reverse directions. Despite improvements, these models lack parallelism, leading to inefficiencies.\n\nThe Transformer [4] architecture offers an attractive solution to these issues by processing sequence data in parallel through an encoder-decoder architecture and applying multi-head attention mechanisms, thereby significantly enhancing efficiency and performance. Transformers have proven to be highly effective in processing long-term sequence data, such as lengthy sentences and speech in NLP [5] applications, as well as handling image and video data in CV applications [6, 7]. Their capabilities in the realm of long time series forecasting have also been recognized.\n\nNevertheless, the improvements made by these model-centric AI approaches (MCAI) can not always overcome insufficient data preparation and preprocessing. These steps, which are essential for developing precise models, were often neglected. Therefore, researchers are increasingly directing their efforts towards data-centric approaches to achieve better model performance, leading to the emergence of Data-Centric AI (DCAI) [8-13]. According to Andrew Ng, data-centric AI is the discipline of systematically engineering the data used to build an AI system [14]. It emphasizes the importance of data in the analysis compared to conventional model-centric AI. We"}, {"title": "2 Related Works and Background", "content": "2.1 Data-Centric Artificial Intelligence (Data-Centric AI)\n\nWith the development and movement [18, 19] of Data-Centric AI (DCAI), several surveys have emerged on this topic. However, none have specifically addressed the role of DCAI in Transformer models for time series forecasting. Some researchers [10] have introduced the concepts and principles to outline the foundations of DCAI. Others [9] have defined relevant terms and introduced a framework for DCAI, while another study [12] provides a overview of DCAI missions, detailing definitions, explanations, related tasks, and challenges. These surveys primarily focus on clarifying the definitions and frameworks or providing an overview of DCAI, without delving into detailed methods for each section. One comprehensive survey [13] examines the entire data lifecycle with representative methods but does not discuss specific use cases. Another survey [20] explores use cases with graph learning, and another [21] provides an epidemic forecasting survey from a DCAI perspective. However, none of these involve Transformer models. One paper [22] studies data movement in Transformers' training processes to improve GPU utilization, but it does not consider the data itself. Another study [23] addresses the performance issues of Vision Transformers (ViTs) in face recognition scenarios from a data-centric perspective. Nevertheless, the role of DCAI in Transformer-based time series forecasting remains insufficiently explored.\n\n2.2 Transformer for Time Series Forecasting\n\n2.2.1 Time Series Forecasting\n\nSurveys [24] and tutorials [25] discuss deep learning for time series forecasting from the perspective of model architectures, while another review [26] conducts experimental studies to compare the performance of different deep learning architectures. The Monash time series forecasting archive [27] provides a diverse collection of comprehensive time-series datasets across various domains, along with dataset characteristics analysis. However, these surveys do not conduct comprehensive analyses of Transformers in time series forecasting, and their dataset analysis and analysis pipelines are deficient.\n\n2.2.2 Transformer for Time Series\n\nThe use of Transformers for time series tasks arises from their powerful capability in handling sequential data. A survey [28] analyzes the development of time series Transformers from network modifications and application domain perspectives. A tutorial [29] provides details about Transformer architecture and its applications. However, these studies approach the topic from a model-centric AI perspective, leaving the survey of Transformers in time series from a data-centric AI perspective insufficient. Thus, we introduce the role of DCAI in Transformer-based time series forecasting to fill"}, {"title": "3 Taxonomy", "content": "We adopt the CRISP-DM and Transformer model workflow as the foundational framework to systematically structure the different phases of our survey, as illustrated in Figure 3. Input Data, Data-Model Interaction, and Output Evaluation will be detailed in Sections 4, 5, and 6 respectively. In the Input Data section, we cover data preparation and preprocessing to address RQ1: How are datasets preprocessed before being fed into models? In the Data-Model Interaction section, we discuss embedding, encoding, and modeling to address RQ2: How does the data and model interact with each other? In the Output Evaluation section, we explain model evaluation to address RQ3: How are models evaluated on the data?"}, {"title": "4 Input Data: How are datasets preprocessed before being fed into models?", "content": "4.1 Dataset Preparation\n\nWe gather datasets from typical models, utilizing open-access papers and open-source codes, resulting in 50 datasets across 24 models 1. These include 20 transformer models and 4 Large Language Models (LLMs) from 2020 to 2023. The transformer, an encoder-decoder architecture, is applied to various problems [56] such as NLP tasks, CV, and audio/speech processing. LLMs, which utilize transformer architecture, are pre-trained on large text datasets for NLP tasks. A recent survey [57] explores LLMs for time series data, making them relevant for comparison. The datasets and related models are shown in Tab. 1, which indicates that LLM-based time series models use more datasets than transformer-based time series models. These datasets do not have missing or corrupted data. Typically, missing data can be addressed through imputation, and corrupted data can be detected using anomaly detection algorithms. Most datasets for transformers-based TSF models are split into training, validation, and\n\n4.2 Data Preprocessing\n\nOnce data preparation has been completed, the next stage is data preprocessing. In this section, we follow the sequence of steps from the raw dataset to the model input. These steps are crucial for model performance to avoid \"Rubbish in, rubbish out\". The preprocessed data (model input) is then passed to the model [60]. In transformer-based time series models, data undergoes sequential preprocessing: organizing features, data reduction or augmentation, and data representation for the model. (N.B. Data representation (input embedding and position encoding) is part of the data-model interaction, discussed in the Sec. 5.)\n\n4.2.1 Data Features\n\nData Features includes feature transformations and feature engineering. Feature transformations transform a dataset into new distribution base on model's assumption. Feature Engineering extracts features from input datasets to improve the performance of the models. A common feature transformation is data normalization, which is frequently used in transformer-based time series forecasting (TSF) models. Models apply data normalization to adjust data to the same common scale or range. This keeps different datasets and models on the same level for comparison. There are several normalization solutions such as Z-normalization, Min-max normalization, Sigmoid normalization etc. [61]. However, further research is needed to analyze the different data normalization methods used in transformer-based time series models. Feature Engineering allows us to understand different features of time series data, which is essential for the performance of transformer-based forecasting models. Different features are discussed in [16, 62, 63]. Here, we focus on features applied in transformer-based time series forecasting. Below, we list some common feature engineering methods for transformer-based TSF models specifically.\n\nCovariate: In Dart [64], covariate time series refer to external data or variables that are not the target of forecasting but are useful for improving forecasting accuracy. For example, when modeling participants' heart rates using their weight, additional factors such as environmental temperature and measurement time also influence heart rates. These factors are called covariates. The meaning of covariates can vary depending on the context. In some contexts, input features or explanatory variables are considered covariates [65], similar to their definition in statistical dictionaries [66]. The paper by Davies [67] discusses the role of covariates in forecasting models. The Temporal Fusion Transformer (TFT) model [51] employs static covariate encoders to integrate covariates into the model.\n\nLag Features and Sliding/Rolling Window: Time lag refers to previous steps in the time series. An example of lag is shown in Fig.4. Lag-Llama [68] apply time lags as covariates to build the forecasting model. The Rolling Window technique involves moving a window of specified length across the data sequentially. In traditional"}, {"title": "5 Data-Model Interaction: How does the data and model interact with each other?", "content": "In this section, our aim is to address the interaction between data and the model. Specifically, we will focus on answering the question: How was the data prepared for the model, particularly for the encoder and decoder components? In transformer-based forecasting architectures, the encoder-decoder framework takes a given time series as input and produces a predicted time series as output. While the general flow of data through a transformer model is described in the paper [22], emphasizing hardware-level data movement during training, our paper primarily focuses on the data representation process within the transformer model.\n\n5.1 Data Representation: Input Embedding & Time Position Encoding\n\nFig. 6 illustrates the input embedding and time position encoding components in the transformer-based time series model. The inputs shown in the figure are preprocessed data prepared using sliding windows from Fig. 5. This preprocessed data includes a matrix of variables (features) and their corresponding timestamps. The variables (fea-tures) are represented using input embedding (the output is \"1\" in the Fig. 6), while the timestamps are represented using time position encoding (the output is \"2\" in the Fig. 6). Both representation processes result in matrices, which are then merged to form the input for the encoder. The input for the decoder is generated in the same manner. Input embedding techniques commonly include 1-D convolutional filters (with a kernel width of 3) on the input data, as used in models like Informer [49] and Autoformer [48], and linear projection methods, as seen in PatchTST [74]. Time position encoding typically applies temporal position encoding to represent timestamp information. The survey [30] provides a comprehensive overview of existing position information methods in Transformer models, while the paper[31] discuss various position encoding solutions for transformer-based time series models. The common sinusoidal position encoding is presented as follows, where t is the t-th timestamp, jis the j-th dimension of the model, d is the total dimension of the model.\n\n$P_{tj} = \\begin{cases} sin(\\frac{10000}{t^j}), &j \\in 2n: n \\in Z \\\\ cos(\\frac{10000}{t^{j-1}}), & j \\in 2n + 1 : n \\in Z \\end{cases}$ (1)\n\n5.2 Modeling\n\nAfter data representation, the resulting matrices are fed into the model's first layer, known as the attention layer. This marks the end of the data-model interaction. The main components of the transformer model (Attention, Add & Norm, and Feed For-ward) are illustrated in Fig. 7. In the context of MCAI, transformer-based time series models are classified into different types based on modifications to these components and the overall architecture [40, 71]."}, {"title": "6 Output Evaluation: How are models evaluated on the data?", "content": "Transformer-based time series models are generally evaluated from two perspectives: predictive performance and computational performance. Predictive performance is represented by metrics such as Mean Squared Error (MSE) and Mean Absolute Error (MAE). Computational performance is quantified by memory usage and computation time. These aspects are summarized in Table 2. In this part, we summarize metrics used in transformer-based forecasting models and we discuss the computational performance measurement solution.\n\n6.1 Predictive Performance\n\n6.1.1 Mean Squared Error (MSE) and Mean Absolute Error (MAE)\n\nMSE and MAE are used to measure the difference between predicted values and actual values. Lower values of MSE and MAE indicate higher accuracy of the model. Considering $\\hat{y_i}$ is predicted values and $y_i$ is the ground truth. The MSE and MAE is denoted as:\n\n$MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y_i})^2 , MAE = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y_i}|$ (2)\n\n6.1.2 Quantile Loss (p-risk)\n\nQuantiles divide a dataset into equal parts. Quantile is defined as $Q(p) = inf\\{x : F(x) \\geq \\rho\\},0 < p < 1$, where F(x) is the distribution function [75]. Normalized Quantile Loss, commonly used in transformer-based models, is discussed in [76, 77]."}, {"title": "7 Future Research Opportunities", "content": "7.1 Input Dataset\n\nThe Monash Time Series Forecasting Archive [27] offers a diverse range of comprehensive time-series datasets across various domains, accompanied by dataset characteristic analyses. However, there is still a lack of investigation of dataset exploration pipelines tailored specifically for transformer-based time series models. Additionally, research on determining optimal split ratios (training, validation, and testing) remains insufficient, with only a few papers such as [58, 59] discussing the subject. Moreover, while researchers have begun focusing on dataset augmentation in transformer-based time series models [73], dataset reduction for transformer models has so far been neglected. Dataset reduction is crucial due to the complexity (long timestamps with large multiple variables) of time series datasets. Furthermore, most transformer models primarily operate on common datasets such as electricity consumption, traffic, and exchange rates (as shown in Table 1). Some recent papers [84, 85] have explored transformer-based time series models in the financial domain. However, further investigation into the application of transformer-based time series models on diverse real-world datasets is desirable. Real-world datasets are often a mix of various data types, such as medical records including patients' images or speech data alongside numerical measurements [86]. As researchers are also delving into spatio-temporal data [57, 87], this signals a growing interest in transformer-based multimodal forecasting for using various data types.\n\n7.2 Input Data Representation\n\nIn this paper, the input data representation includes the input embedding and time position encoding. Another survey paper [30] has explored position information methods in transformer models, highlighting the importance in capturing sequential relationships effectively. Recent research in particular [31] has studied these methods to suit temporal data under the context of transformer-based time series models. Despite advancements in position encoding techniques, research on optimizing input embedding strategies tailored for transformer-based time series models remains insufficient.\n\n7.3 Evaluation\n\nMost transformer-based models use Mean Squared Error (MSE) and Mean Abso-lute Error (MAE) to assess forecast errors between predicted and observed values. Some models employ quantile loss to evaluate prediction intervals, while others utilize probability-based metrics such as likelihood and Continuous Ranked Probability Score (CRPS) to assess the alignment of predicted probability distributions with observed values. However, understanding the reliability and trustworthiness of these models remains challenging due to the opacity of neural networks within transformers. Methods for identifying, quantifying, and communicating uncertainties in model outputs are discussed in the book [88]. Surveys [89] explore uncertainty management techniques in NLP from both data and model perspectives. However, uncertainty management in"}, {"title": "8 Conclusion", "content": "This paper has explored the role of data-centric AI in transformer-based time series forecasting by addressing three key research questions and proposing a taxonomy. Firstly, in the Input Data section, we addressed RQ1 How datasets are preprocessed before being fed into models? by discussing the data preparation and preprocessing in transformer-based time series forecasting. Secondly, in the Data-Model Interaction section, we answered RQ2 How does the data and model interact with each other? by delving into the data representation within transformer-based time series forecasting models. Finally, in the Output Evaluation section, we addressed RQ3 How are models evaluated based on the data? Furthermore, we highlight future research opportunities based on these three reseach questions."}]}