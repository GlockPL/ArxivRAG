{"title": "Keep on Swimming: Real Attackers Only Need Partial Knowledge of a Multi-Model System", "authors": ["Julian Collado", "Kevin Stangl"], "abstract": "Recent approaches in machine learning often solve a task using a composition of multiple models or agentic architectures. When targeting a composed system with adversarial attacks, it might not be computationally or informationally feasible to train an end-to-end proxy model or a proxy model for every component of the system. We introduce a method to craft an adversarial attack against the overall multi-model system when we only have a proxy model for the final black-box model, and when the transformation applied by the initial models can make the adversarial perturbations ineffective. Current methods handle this by applying many copies of the first model/transformation to an input and then re-use a standard adversarial attack by averaging gradients, or learning a proxy model for both stages. To our knowledge, this is the first attack specifically designed for this threat model and our method has a substantially higher attack success rate (80% vs 25%) and contains 9.4% smaller perturbations (MSE) compared to prior state-of-the-art methods. Our experiments focus on a supervised image pipeline, but we are confident the attack will generalize to other multi-model settings [e.g. a mix of open/closed source foundation models], or agentic systems", "sections": [{"title": "Introduction", "content": "Recent AI research has shown the effectiveness of agentic architectures and systems composed of multiple models that decompose problems and create scaffolds in a solution pipeline[16, 29, 38, 27, 36, 6]. Alternatively consider an initial model doing a complex pre-processing step for a second model, for example a foundation model[8, 10, 1, 2, 34, 18, 37] that processes the input and passes its output to another model for a classification or some other task. In production systems, a service is often a pipeline of multiple pre/post-processing steps based on heuristics and machine learning models. Combining models this way has proven to be very effective and will likely increase over time with the rise of multi-agent systems.\nThe proliferation of real world AI systems and the horizon of ever more powerful methods has made securing these models against malicious or un-authorized use ever-more urgent. Model providers have responded to these security threats by implementing a mix of a) including safety fine-tuning [19] b) weaker side-car models that halt the model from responding based on detecting malicious queries or harmful outputs [35] c) closed model weights to prevent an attacker from developing white-box attacks [31] d) rate-limiting the users of a centrally served model to avoid black-box attacks [15].\nHowever, the conmingling of multiple models, of closed/open source, introduces new security vulnerabilities that are not precisely captured by existing threat models and complicates defense based on keeping the weights hidden or rate limiting the user to avoid the creation of proxy models."}, {"title": "Threat Model: Multi-Model System Attack With Partial Proxy Access", "content": "We introduce a new and realistic threat model for multi-agentic and multi-model applications that we first test in a vision modality.\nIn the simplest case, consider a system that is a composition of two models, e.g. h\u2081 and h2, so the overall output is \u0177 = h\u2082(h\u2081(x)). Specifically, we have black-box access to both models but it is only feasible to create a proxy model for h2 as shown in Figure 1. The proxy model for h2 allows us to perform gradient based attacks against h\u2082, so we can compute a $d_{adv}$ such that $h_2(X_{mod} + d_{adv}) \u2260 Y_{pred}$ where $X_{mod} = h_1(x)$ and $Y_{pred}$ is the predicted label of x. In the rest of the paper, we refer to $X_{mod}$ as the output of model h\u2081. The key difficulty in this scenario is that the transformation applied by h\u2081 might destroy the adversarial modification such that $h_1(x + adv) \u2260 X_{mod} + adv$ and therefore $h_2(h_1(x + d_{adv})) = Y_{pred}$.\nWe focus on the case when the modifications applied by the h\u2081 are reversible in the sense that $X_{mod}$, and $X_{mod} + adv$, can be \"re-inserted\" into x. Consider the case where h\u2081 is a segmentation model that detects a region of interest and crops the image and we have designed an adversarial perturbation attacking the cropped subset of the full image. That adversarial perturbation could be re-inserted into the original image inside the crop box. Formally, $h_1 : X \u2192 X$ and $h_2 : X \u2192 Y$, for some input modality X. This allows us to \"re-insert\" the adversarial sample $X_{mod} + d_{adv}$ crafted for h2 into x to create an adversarial sample for the whole system. Another example of a pair of models that satisfies this property could be a pair of natural language models; where the first model processes a piece of text, generating a new text string, that is then handed off to the second model for the final computation."}, {"title": "Our Contributions", "content": "To our knowledge, we propose the first attack specifically for a multi-model compositional problem where a proxy is only available for the last model. We observe that this is a more realistic scenario for industrial applications where it might not be feasible to create a proxy for each section of the system or where an adversary might not have access to information about the first sections of the system, for example the pre-processing of the data or an adversarial defense, but the last leg of the system might be approximated with an open source model or have been trained in a public dataset.\nWe provide an iterative method, which we name the Keep on Swimming Attack (KoS, pronounced chaos) to ensure that the attack survives the modifications applied by the non-proxy-able sections of the system, and show our attack has a higher success rate and lower noise levels than the natural baseline method, based on Expectation over Transformation (EoT) [4]. In Appendix A.1, we show how an end-to-end black-box attack was ineffective in this setting; it is this dead end that motivated us to design and develop the KoS Algorithm. Our method shows that even if a system has a secure and restricted section, there are instances in which the overall system can still be exploited with adversarial attacks."}, {"title": "Method", "content": "We can easily craft gradient based attacks for h\u2082 using well known methods[22, 11, 30, 28] if we have white-box access to h\u2082 or have created a reliable proxy model. However, since we only have black-box access to h\u2081 and cannot train a proxy model for that component, we cannot directly craft an end-to-end gradient based adversarial attack h2(h1(x)). Furthermore since the modifications applied by h\u2081 are specific to each sample and thus each adversarial sample iteration, there is no guarantee that adversarial modifications against h\u2082 will survive the transformation applied by h1.\nWe propose an iterative method, the Keep on Swimming Attack. Simply update the sample that we will attack for h2 when the adversarial perturbation has been removed by h\u2081, using the new output of h1.\nFormally, attack h2 and after K gradient based attack iterations, re-insert the adversarial perturbation attacking h\u2082 into the original input and pass it through h\u2081 to check if the attack is still adversarial. If the adversarial perturbation survived the transformation of h\u2081, e.g. h\u2081(x + $d_{adv}$) is still in the same domain of $X_{mod}$, which in our experiment means whether the cropping box coordinates are unchanged, and if we have reached our goal e.g. $h_2(h_1(x + d_{adv})) = Y_{target}$, we terminate and have achieved our objective of an end-to-end attack.\nElse if the adversarial transformation survived h\u2081 but has not yet reached the adversarial target, e.g. $h_1(x + d_{adv}) = X_{mod} + d_{adv}$ and $h_2(h_1(x + d_{adv})) \u2260 Y_{target}$ then we attack for K more iterations.\nElse if the adversarial sample was transformed/warped by h\u2081 and we have a new $X_{mod}$, so h\u2081 (x + \u03b4adv) = $X_{mod2}$, we just Keep on Swimming; we replace the adversarial sample that we had so far, $X_{mod} + adv$ with the new modified output of $X_{mod2}$ and keep attacking.\nThe attack finishes after a maximum number of iterations or when the end-to-end attack is successful. The algorithm is described in detail in Algorithm 1 and shown in Figure 2.\nIn Algorithm 1, the ReInsert(x, $X_{adv}$) operator takes the accumulated adversarial perturbations that have been applied to $X_{adv}$ and pastes it back into the original x. In our experiment this means pasting $X_{mod} + adv$ into the region of x from which we extracted $X_{mod}$. While our proposed attack pipeline uses a gradient based attack against h2, the pipeline is still valid for non-gradient based attacks.\nWhile our experiments focus on this specific modality, we believe in the general applicability of our framework and Algorithm 1. One example of an application could be a system that processes and answers questions about a text. A first non-proxy-able model extracts quotes from the text related to the question and the second proxy-able model generates an answer. Our method makes is suitable for agentic architectures and in general systems where there is a sequential combination of either models or heuristics in which we only have a partial information."}, {"title": "Experiments", "content": "In order to simulate the scenario proposed in this paper we focus on the problem of creating an adversarial attack to cause the numerical value of a check to be misread. The input for this system is an image of a check. The first model of the system (h\u2081) consists of a segmentation model that identifies the areas of the image with text. The output of model h\u2081 is the area of the image containing the check's numerical amount ($x_{mod}$), written in latin numerals 4. The second model of the system (h2) is an OCR (optical character recognition) system that identifies the numerals in the image. To simulate the target system we use the CRAFT [5] segmenter (h\u2081) to create cropped one line text image. To obtain the text in each image (h2), we used the publicly available Microsoft's Transformer based OCR for handwritten text[26]. We ran our experiments on a database of pictures of checks filled with handwritten information in which CRAFT was able to correctly identify and extract the numerical amount of the check. The attack objective is to transform the predicted numerical amount of one check to another value for a total of 20 attack samples.\nFor the attack, we assume black-box access to h\u2081 but not the possibility of creating a proxy. To create the adversarial sample for the image-to-text (OCR) section (h2) we use the \"I See Dead People\" (ISDP)[25]. This attack is grey-box since it has white-box access to the image encoder but not to the text decoder. In this case we had white-box access to the image encoder since we used the same OCR model as CRAFT but a proxy model for the image encoder could have been used making this attack entirely black-box. Note that this does not affect the results since ISDP was used with all"}, {"title": "Benchmarks", "content": "We compare our method with a baseline of only attacking h\u2082 and re-inserting the adversarial cropped image into the original large image (ISDP Baseline). We also compare our method with creating attacks that are robust to the transformation from h\u2081 using the method from [4] (Crop Robust ISDP). For the Crop Robust ISDP, we take a slightly larger crop than the one from the starting image, perform 10 random crops such that the text is always contained in the crop, attack each random crop independently, average the gradients and update the image to create the adversarial version. We found these hyperparameters to provide the best overall results for this method.\nWe compare the methods in terms of the attack success rate, the mean squared error (MSE) with respect to the original image, and computational cost. Table 1 shows the success rate of the KoS pipeline is considerably higher and the Levenshtein distance the final output of both the cropped and the full image are considerably lower than just using the ISDP attack or creating a version that is robust to cropping.\nThe KoS pipeline introduces more noise and takes more time than the Baseline ISDP but less than the Crop Robust ISDP attack. The key benefit of our method, that clearly Pareto dominates the other methods is our substantially higher attack success rate. We would note that we investigated these alternate baseline methods first and it was only our inability to craft successful attacks that required us to design the KoS attack."}, {"title": "Conclusion", "content": "We have shown how adversaries can use their knowledge of one model in a multi-model system to craft effective end-to-end attacks with the KoS algorithm. Further work is needed to study the convergence properties of KoS, and generalizing the attack to other settings like attacking a composition of LLMs. That said, these initial results already demonstrate the need for timely research into attacks and defenses in the threat model of Multi-Model Systems With Partial Proxy Access. If multi-agent and multi-model systems inherit the vulnerability of the most 'proxy-able' model, that suggests serious un-patched vulnerabilities already exist in the foundation model era, and we can expect the impact of such vulnerabilities to be amplified in the upcoming era of agentic AI."}, {"title": "Appendix", "content": "HopSkipJumpAttack\nWe attempted to use the HopSkipJumpAttack on the system but failed to produce samples where the attack is adversarial for human viewers, i.e. perturbations do not change the true label. Figure 3 shows one a sample where the initial number 25.86 is misread as the target output 100.00."}, {"title": "Visual Comparison of Adversarial Samples", "content": "Visual comparison of final cropped images for each attack pipeline converting 79.12 value to 100.00 and vice-versa showing if the attack was successful or not. The final adversarial sample is the whole check image but here we show the cropped versions to highlight visual differences on the adversarial modifications. One can observe the KoS samples have less noticeable perturbations in this particular sample as reflected by the lower average MSE from Table 1."}, {"title": "Social Impact Statement", "content": "Our paper takes an adversarial approach to disclose possible vulnerabilites for systems of machine learning models; we demonstrate a new attack on composed models. Using the attack would require a new attacker to obtain knowledge about the attacked system.\nUnlike papers that publish jailbreaks or zero-days, our disclosure cannot be used immediately off the shelf to attack production grade systems. That said, we are currently working on a generalization of this work that could be used to target systems currently in production.\nThis attack is very natural and well-motivated so it is possible or even likely similar attacks exist in-the-wild and are being used by real world attackers, so we believe introducing and studying the vulnerability in this proof-of-concept will allow for the design and deployment of effective defenses to this vulnerability.\nOne interpretation of our work, which we do not advocate for, is that releasing model weights could allow for attackers to break real world multi-model systems. Thus securing the modern AI stack requires locking down model weights. This would be an inversion of the well known Kerchoff's Principle from cryptography. We note, but do not advocate, for this interpretation which would no doubt have a significant social impact even though it is difficult to forecast if it would be positive or negative."}]}