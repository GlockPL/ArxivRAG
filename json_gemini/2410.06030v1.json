{"title": "An investigation of Quality Issues in Vulnerability Detection Datasets", "authors": ["Yuejun Guo", "Seifeddine Bettaieb"], "abstract": "Vulnerability detection is a crucial yet challenging task to identify potential weaknesses in software for cyber security. Recently, deep learning (DL) has made great progress in automating the detection process. Due to the complex multi-layer structure and a large number of parameters, a DL model requires massive labeled (vulnerable or secure) source code to gain knowledge to effectively distinguish between vulnerable and secure code. In the literature, many datasets have been created to train DL models for this purpose. However, these datasets suffer from several issues that will lead to low detection accuracy of DL models. In this paper, we define three critical issues (i.e., data imbalance, low vulnerability coverage, biased vulnerability distribution) that can significantly affect the model performance and three secondary issues (i.e., errors in source code, mislabeling, noisy historical data) that also affect the performance but can be addressed through a dedicated pre-processing procedure. In addition, we conduct a study of 14 papers along with 54 datasets for vulnerability detection to confirm these defined issues. Furthermore, we discuss good practices to use existing datasets and to create new ones.", "sections": [{"title": "1. Introduction", "content": "Detecting vulnerabilities plays a vital role in protecting software applications from potential security threats. By automating the detection process, organizations can quickly respond and mitigate risks, which is especially important given the increasing number of vulnerabilities in today's digital landscape. According to the Common Vulnerabilities and Exposures (CVE) Details\u00b9, 25,227 vulnerabilities were been reported in 2022 and 5910 have already emerged within three months in 2023.\nVulnerability detection is a challenging task that requires the analysis of vast amounts of code, often in various programming languages and with different styles and structures. Traditional approaches to vulnerability detection, such as rule-based [12] and signature-based [23] methods, rely on human experts to pre-define a set of rules or patterns, which can be time-consuming to develop and may not be effective in identifying new or previously un-known vulnerabilities. By contrast, deep learning (DL) has been proven to be a promising alternative given its ability to automatically learn complex patterns and features from a large amount of source code [4], [21], [28].\nDespite the significant progress in DL-based vulnerability detection, the availability of high-quality benchmark datasets remains a major challenge. As highly data-driven, DL models require a large number of data in the training procedure to tune parameters. Several datasets have been created and open-sourced in the literature (see Table 1 for more details) to facilitate the study on vulnerability detection. However, the quality of these datasets is often poor, which can lead to inaccurate, biased, or incomplete results of DL models for vulnerability detection. For instance, the quality of datasets can refer to the coverage of vulnerability types (please refer to Section 3.1.2 for more details). If the vulnerability of denial of service (DoS) is not involved in the training set, a detection model is unlikely to identify a code sample with this vulnerability as vulnerable since no related knowledge has been gained.\nIn this paper, we aim to reveal issues in existing datasets that need to be aware of when creating new datasets and addressed when using existing ones to improve the effectiveness of vulnerability detection models. Specifically, based on 54 existing datasets from 14 papers about vulnerability detection, we define two types of issues, critical and secondary, based on the degree that an issue can affect the detection performance and the difficulty to be addressed. Such critical issues include small sampling size and data imbalance, low coverage of vulnerability types, and bias in vulnerability distribution. Secondary issues, on the other hand, may also affect the model performance but can be addressed through a careful check and pre-processing before feeding into training a DL model. These issues include errors in raw data, mislabeling on source code, and noisy historical data. Finally, based on our findings, we provide actionable suggestions to researchers when using existing datasets as well as creating new ones."}, {"title": "2. Background and Related Work", "content": "DL-based vulnerability detection has become an increasingly important component of software security as it enables developers and security professionals to identify potential vulnerabilities more quickly and accurately than manual testing. The process involves the training of a DL model that analyzes source code for known patterns that may indicate the presence of a vulnerability, such as buffer overflows, SQL injection, or cross-site scripting [1]. Various DL models have been developed and proved to perform effectively on given test code, such as the graph neural network-based model Devign [28] and the large-scale pre-trained model CodeBERT [21].\nNonetheless, the effectiveness of DL-based vulnerability detection models highly depends on the quality of the data that is used for training. If the data is incomplete, inconsistent, or biased, the models may produce unreliable results leading to a false sense of security in addition to wasted resources. Therefore, ensuring the quality of data is essential for the success of vulnerability detection.\nThe quality issue in existing datasets has attracted the attention of researchers. A recent study [6] found that 20-71% of vulnerabilities are inaccurate in its considered four existing datasets and 17-99% data is duplicated. Jimenez et al. [10] demonstrated that noisy historical data that is labeled secure but actually is undiscovered vulnerabilities can cause the detection accuracy decrease by over 20%. Garg et al. [9] confirmed that taking the nosisy historical data into consideration helps to improve a DL model's performance on vulnerability prediction."}, {"title": "3. Issues in Existing Datasets For Vulnerability Detection", "content": "We categorize the issues in existing datasets for DL-based vulnerability detection into two types, critical and secondary, based on their impact on training a DL model with SOTA performance and the difficulty to be addressed by data pre-processing. Here, pre-processing refers to common operations to prepare data before training DL models, such as removing comments, removing empty lines, writing data into a unified format (e.g., JSON and PKL files), and splitting data for training, validation, and testing. Additionally, a study on 14 references listed in Table 1 is conducted to instantiate these issues."}, {"title": "3.1. Critical issues", "content": "This category includes inherent issues that can heavily affect a model's performance and cannot be addressed through pre-processing."}, {"title": "3.1.1. Small sampling size and data imbalance.", "content": "Since DL models are highly data-driven, feeding sufficient source code into training is crucial when training a DL model for vulnerability detection. However, existing datasets often have a small size. On the other hand, data imbalance has been proven to be a serious problem in the literature [7], [19], [24]. In vulnerability detection, this imbalance refers to the high ratio of secure code to vulnerable ones.\nFigure 1 shows the sampling information of 27 datasets provided by seven references. The datasets from two projects, LibPNG and LibTIFF, generally are small-sized. For example, the dataset LibPNG only includes 621 source code samples in total, as shown in Figure 1(a). Concerning the sampling bias, except the FFmpeg dataset by [28] in Figure 1(e), all datasets contain more secure code samples than vulnerable ones. Particularly, the imbalance ratio reaches 325 in the Asterisk dataset provided by [18] in Figure 1(b). Here, the imbalance ratio is calculated by $\\frac{\\text{#Secure}}{\\text{#Vulnerable}}$."}, {"title": "3.1.2. Low coverage of vulnerability types.", "content": "Vulnerability detection models are expected to detect a broad range of vulnerability types, including common ones such as denial of service (DoS) and cross-site scripting (XSS), as well as less common ones such as HTTP response splitting and cross-site request forgery (CSRF) vulnerabilities [1]. However, existing datasets often have limited coverage of vulnerability types, which can lead to false negatives or an overall poor detection.\nTable 2 lists the vulnerability types included in the 18 datasets provided by four references where the vulnerability of a source code sample is available. In total, 28 vulnerability types are covered by all references, which belong to 10 vulnerability type families [1], bypass something, XSS, DoS, directory traversal, code execution, gain privileges, overflow, gain information, SQL injection, and file inclusion. Other vulnerability families, such as HTTP response splitting and CSRF, are not covered by any dataset. In addition, note that each vulnerability family includes several vulnerability types, while in the list, most vulnerability types belong to the DoS (ID: 4-13) and the code execution (ID: 5-8, 15-23) vulnerability families. On the other hand, none of the four reverences could cover all listed vulnerability types."}, {"title": "3.1.3. Bias in vulnerability distribution.", "content": "Vulnerability distribution refers to the numerical proportion of vulnerability types in a given dataset. Bias in the distribution could force a vulnerability prediction model to learn more from majority types during the training procedure, thus the model will perform poorly on minority types.\nFigure 2 shows the vulnerability distribution of six datasets provided by [18]. The first observation is that each dataset covers a different set of vulnerability types, confirming the defined coverage issue in Section 3.1.2. Second, regardless of the dataset, vulnerabilities are distributed unevenly. Remarkably, in FFmpeg, nine out of 15 vulnerability types occupy less than 1% of source code from the entire dataset."}, {"title": "3.2. Secondary issues", "content": "This type of issue is mainly caused by errors in raw data or outdated information, which can be addressed through a comprehensive pre-processing to avoid affecting the model performance."}, {"title": "3.2.1. Errors in raw data.", "content": "Errors in raw data can significantly affect the accuracy of vulnerability detection algorithms. From existing datasets, we have found errors including empty source code files, extra lines, and inconsistent file formats.\n[18] includes 435, 195, and 205 empty source code files in Asterisk, Pidgin, and VLC, respectively. Without being informed, these empty files will be processed normally to train a detection model. However, since they do not provide any pattern, the detection model to be trained can be misled to learn real patterns for secure and vulnerable source code.\nAll the 18 datasets provided by [17], [18], [15], and [16] have the issue of containing an extra line at the beginning of a separate source code file. This extra line varies a lot across different datasets, such as [16]: such as \u201c}\u201d, \u201c*/\u201d, \u201c} EightBpsContext;", "};": "} AascContext;", ")": "Note that, without checking the source code files manually, this extra line is impossible to be removed during the pre-processing procedure. For example, the data pre-processing procedure generally filters comments by locating paired comment marks, such as \u201c//\u201d and \u201c\\*/\u201d for C programming language.\n[17] uses '.txt' for vulnerable code files and '.c' for secure code files. Depending on the framework, reading files can encounter errors."}, {"title": "3.2.2. Mislabeling on source code.", "content": "Mislabeling can severely degrade the performance of prediction models, which occurs due to the labeling manner. Generally, code files are extracted from open-source projects from GitHub and labels are manually given based on commit messages and descriptions in the national vulnerability database (NVD) [2]. Both commit messages and NVD entries are manually curated and analyzed, which is error-prone even with experienced software developers [3]. Another way is to use static code analysis tools as shown in Table 1, which is less accurate than the manual manner.\nFigure 3 shows an example of mislabeling in the LibPNG dataset provided by [18]. The code file (including green lines in the example) named \u201ccve-2016-10087.c\" indicates that the source code is vulnerable that allows context-dependent attackers to cause a NULL pointer dereference vectors. However, according to the commit message on GitHub\u00b2, the green lines are the patch of the corresponding vulnerable code (in red), thus, the label should be secure instead of vulnerable.\""}, {"title": "3.2.3. Noisy historical data.", "content": "Noisy historical data [9], [10] refers to the phenomenon that code labeled as secure might be identified as vulnerable in the future given that most vulnerabilities are discovered much later than when they are introduced. For example, the decode_main_header() function in FFmpeg is recently reported to have the null pointer dereference flaw\u00b3.\nHowever, when collecting data before the report, this function will be considered as secure."}, {"title": "4. Good Practices", "content": "Using existing datasets: Critical issues are difficult to be addressed by data pre-processing, but there are techniques to mitigate their impact on the model performance. For the sampling size issue, data augmentation [8] can be applied to increase the number of data during training. To force the model learn more from vulnerable code in imbalanced datasets, weighted loss functions, such as focal loss [19] and mean squared error loss [24], can be applied instead of the default cross entropy loss. Concerning the low vulnerability coverage issue, one can consider to merge several datasets that cover different vulnerabilities or just focus on detecting source code with included vulnerabilities. Last, code refactoring [26] and adversarial code attacks [25] can help to generate more similar code samples without changing the semantics to reduce the bias in vulnerability distribution.\nFor secondary issues, the quality of existing datasets can be improved by designing an advanced pre-processing method to remove errors and reduce noise from the raw data. Note that, the errors mentioned in Section 3.2.1 do not exist in all studied datasets and may not cover all cases. A thorough check should be made to develop the operations for a comprehensive pre-processing. To reduce noise, one should look into latest commit messages related to each code file and modify the labels when necessary.\nCreating new datasets: When collecting source code from open-source projects, all found issues should be kept in mind. An exhaustive pre-processing step is highly recommended to avoid errors and noise in data. When assigning labels to collected data, both static analysis tools and expert manual manner should be considered to avoid mislabeling. In addition, one should include as much information as possible, such as vulnerability type, source project, and commit id, instead of only including labels to allow for tracking and re-checking."}, {"title": "5. Conclusion", "content": "The accuracy and efficacy of deep learning (DL)-based vulnerability detection models highly rely on the quality of data used for training. Poor data quality can lead to unreliable results, false positives, and false negatives. In this paper, we define three critical and three secondary issues that occur in existing datasets. Furthermore, we provide actionable guidance to assist researchers in addressing these issues when using existing datasets or creating new ones with high quality."}]}