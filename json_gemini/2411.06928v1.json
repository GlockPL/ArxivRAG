{"title": "Electroencephalogram-based Multi-class Decoding of Attended Speaker's Direction with Audio Spatial Spectrum", "authors": ["Yuanming Zhang", "Jing Lu", "Zhibin Lin", "Fei Chen", "Haoliang Du", "Xia Gao"], "abstract": "Decoding the directional focus of an attended speaker from listeners' electroencephalogram (EEG) signals is essential for developing brain-computer interfaces to improve the quality of life for individuals with hearing impairment. Previous works have concentrated on binary directional focus decoding, i.e., determining whether the attended speaker is on the left or right side of the listener. However, a more precise decoding of the exact direction of the attended speaker is necessary for effective speech processing. Additionally, audio spatial information has not been effectively leveraged, resulting in suboptimal decoding results. In this paper, we observe that, on our recently presented dataset with 15-class directional focus, models relying exclusively on EEG inputs exhibits significantly lower accuracy when decoding the directional focus in both leave-one-subject-out and leave-one-trial-out scenarios. By integrating audio spatial spectra with EEG features, the decoding accuracy can be effectively improved. We employ the CNN, LSM-CNN, and EEG-Deformer models to decode the directional focus from listeners' EEG signals with the auxiliary audio spatial spectra. The proposed Sp-Aux-Deformer model achieves notable 15-class decoding accuracies of 57.48% and 61.83% in leave-one-subject-out and leave-one-trial-out scenarios, respectively.", "sections": [{"title": "I. Introduction", "content": "The human brain can extract the speech of the attended speaker amidst competing speakers and environmental noise [1], [2]. However, individuals with hearing impairments often struggle to understand speech, especially in noisy environments, necessitating the use of hearing aids [3], [4].\nModern hearing aids typically assume that the attended speaker is either the one producing the highest sound pressure level or the one located directly in front of the listener [5], [6]. However, these assumptions may not hold in practice, as the desired speaker could be masked by interference or positioned significantly away from the listener. Determining the direction of the attended speaker from interfering speakers using only the signals captured by the hearing aids' microphones is infeasible without prior information, such as the pre-recorded speech of the attended speaker. Recent advancements in auditory attention decoding (AAD) [7], [8] have introduced a viable approach to decode the direction of the attended speaker directly from brain signals, with electroencephalogram (EEG) being the most commonly used technique due to its non-invasive and convenient capture process [9], [10], [11], [12], [13], [14], [15].\nThe intricate nonlinear relationship between EEG signals and the attended audio poses a significant challenge for existing directional focus decoding methods. Rule-based methods, such as the filter bank common spatial patterns filter (FB-CSP) [16] and the Riemannian geometry-based classifier (RGC) [8], rely on second-order statistics to decode the attended direction. Deep neural network (DNN) approaches employ convolutional [7], recurrent [17], and self-attention [18] structures to directly extract the attended direction from EEG signals. The FB-CSP utilizes optimized filters to maximize the energy contrast of filtered signals between various directional focus classes [16]. The symmetric positive definite property enables the RGC to decode attended directions from the covariance matrices of EEG signals based on Riemannian distance [8]. These rule-based methods perform well with long EEG segments but degrade with shorter decision windows. A convolutional neural"}, {"title": "II. THE NJU AUDITORY ATTENTION DECODING DATASET WITH MULTI-DIRECTIONAL CUES", "content": "We have developed the NJU Auditory Attention Decoding Dataset, encompassing multiple alternative directions of attended speakers [21], [36]. Our prior research focused on binary directional focus decoding [21]. This paper further explores the feasibility of multi-class directional focus decoding.\nEEG data were collected from 28 normal-hearing subjects as they listened to two competing speakers reproduced via a loudspeaker array. Subjects were instructed to focus on one specific speaker while ignoring the other. All participants provided formal written consent approved by the Institutional Review Board of Nanjing Drum Tower Hospital (ethics approval number: 2022-065-09) prior to the experiments and received financial compensation upon completion. Due to device failure, data from seven subjects were excluded from further analysis.\nEEG data were recorded using the 32-channel EMOTIV Epoc Flex Saline system at a sampling rate of 1024 Hz in a low-reverberation listening room. The electrode placement of the recording device is depicted in Figure 1. EEG signals were down-sampled to 128 Hz and wirelessly transmitted to the PC USB receiver. This device has been used to capture EEG signals, yielding similar results to other high-density EEG collecting devices [37], [38].\nEach subject participated in 32 trials. In each trial, subjects were exposed to a pair of randomly selected stimuli, with directions randomly drawn from 15 possible competing speaker directions, i.e., \u00b0135\u00b0, \u00b1120\u00b0, \u00b190\u00b0, \u00b060\u00b0, \u00b145\u00b0, \u00b130\u00b0, \u00b115\u00b0 and 0\u00b0. Audio stimuli were presented at identical intensity through the loudspeaker using the vector-based amplitude panning (VBAP) algorithm [39], which panned the audio stimuli to their target directions. The loudspeakers were installed in an array measuring 2 meters in length and 1.5 meters in width. Notably, reproducing audio stimuli through loudspeakers and earphones yields similar quality of EEG signals and experimental outcomes [40], [41]. Subjects were instructed to attend to one of the two speakers while ignoring the other. Subjects were asked to close their eyes and remain still in the chair at the center of the loudspeaker array to minimize possible artifacts. Before each trial, subjects were given a question and instructed to find the answer in the attended audio stimuli. Subjects were allowed to take breaks after each trial to avoid listening fatigue. In total, there were 64 minutes (32 trials with 2-minute stimuli) of recordings per subject. However, our preprocessing removed some abnormal parts, therefore the available EEG recordings were shorter than 64 minutes per subject.\nAdditionally, an omnidirectional microphone was placed near each ear of the subject to capture the competing audio streams. It is important to note that determining the direction of the attended speaker using only the two-channel audio signals is not feasible due to existence of the competing speakers. However, the audio signals can be exploited as an effective auxiliary information to EEG for enhancing the decoding performance."}, {"title": "A. Experiment Setup", "content": "B. EEG Signal Preprocessing\nEEG preprocessing was performed using the EEGLAB toolbox [42]. The EEG signals were filtered through a bandpass filter between 1 Hz and 32 Hz, consistent with most previous AAD tasks [16], [27], [41]. EEG signals were decomposed"}, {"title": "C. Audio Signal Preprocessing", "content": "Audio recordings with an original sampling rate $f_s$ = 44.1 kHz were down-sampled to 8 kHz. The down-sampled audio signals were then transformed into the time-frequency domain via short-time Fourier transform (STFT) with $F$ frequency bins and $N$ time frames, with each time-frequency bin expressed as $Y_l(f, n)$ with $l$ the microphone unit index, $f$ the frequency index, and $n$ the time frame index. Subsequently, the minimum variance distortionless response (MVDR) beamformer [43] was applied to extract the spatial spectrum, which can be represented as\n$\\ P_{mvdr} = \\frac{1}{g^H(f,\\theta) R(f)^{-1}g(f,\\theta)}$,\nwhere $g(f,\\theta)$ is the steering vector, $R(f)$ is the correlation matrix of the microphone array defined as\n$\\ R(f) =  \\begin{bmatrix}\nY_{1,f}Y_{1,f}^H & Y_{1,f}Y_{2,f}^H\\\\\nY_{2,f}Y_{1,f}^H & Y_{2,f}Y_{2,f}^H\\\n\\end{bmatrix}$,\nwith $y_{l,f}$ a column vector written as\n$\\ Y_{l,f} =[Y_l(f,0),Y_l(f,1),..., Y_l(f, N - 1)], l=1,..., L$.\nThe left microphone unit is designated as the reference point. The angle begins at -90\u00b0 on the left-hand side and extends to 90\u00b0 on the right-hand side. The steering vector is then simplified to\n$g(f,\\theta)= [g_1(f,\\theta),g_2(f,\\theta)]\\  = [1, exp(-2\\pi if sin(\\theta)d / c)]$\nThe first ($l = 1$) and second microphone ($l = 2 $) are placed near the left and right ear, respectively."}, {"title": "III. PROPOSED METHODS", "content": "Figure 2 depicts the architecture of the models used to decode the direction of attended speakers. Figure 2 (a-1) illustrates the general pipeline of the EEG-based directional focus decoding. A DNN model is trained using EEG signals to predict the most probable direction of the attended speaker. Figure 2 (b-1) presents our proposed audio-EEG directional focus decoding method, in which a spatial spectrum is also provided as the second input of a DNN classifier to boost the directional focus decoding. The remaining sections of Figure 2 detail the structures of the CNN [7], LSM-CNN [21], and EEG-Deformer models (first row) [22], and their variants with a spatial fusion convolution layer (second row), which integrates spatial spectrum information with EEG features to improve decoding accuracy.\nLet $X \\in R^{C \\times T}$ be the input EEG with $C$ and $T$ representing the total number of EEG channels and time sample points respectively. The CNN model (Figure 2 (a-2)) directly takes $X$"}, {"title": "A. Overview of Models", "content": "as input and performs convolution along the channel dimension. An average pooling layer condenses the intermediate features, followed by fully connected layers to output the predicted class labels.\nLet $p \\in R^{N_1}$ be the spatial spectrum with $N_1$ the number of spatial sampling points of the spectrum. A fusion block is inserted before the convolution block in the Sp-Aux-CNN model (Figure 2 (b-2)) to leverage both EEG and audio spatial information. The fusion block first transforms the spatial spectrum into high-dimensional patterns and then concatenates them with EEG features to form a new tensor. The subsequent convolution block uses the fused tensor as input to determine the attended directions.\nThe remaining layers in LSM-CNN resemble those of the conventional network [7], with the only difference being the use of 3D convolution blocks (instead of 2D convolution), followed by an average pooling layer to condense the time dimension. The network uses a flatten layer and Fully Connected (FC) layers to generate the output, which is the attended direction.\nThe EEG-Deformer model (Figure 2 (a-4)) [22], which demonstrates superior performance in other BCI fields, is a new variant of transformer [20], [23], [24]. We adopted the EEG-Deformer to leverage the audio spatial spectrum by inserting a fusion block after the first EEG encoder. The remainder of Sp-Aux-EEG-Deformer is the same as the original."}, {"title": "B. The Learnable Spatial Mapping Module", "content": "Current data-driven methods primarily implement image or speech processing models for directional focus decoding [7], [18]. However, these methods fail to effectively leverage the spatial distribution of EEG electrodes, leading to suboptimal performance. Although EEG sensors (electrodes) are spatially distributed in 3D space, EEG signals are conventionally represented as multi-channel signals with additional descriptive text for each channel. Unfortunately, these texts cannot be directly utilized by DNN models. Therefore, it is crucial to design more effective mechanisms to exploit the spatial distribution of EEG sensors.\nIn addition to incorporating the connectivity between neighboring channels, our proposed LSM module reorganizes the spatial layout of EEG channels. This feature allows subsequent convolution layers to emphasize the intrinsic relationships among channels positioned within the 2D plane. This behavior mimics the simultaneous activation patterns observed in human brains, where neurons tend to activate in conjunction with neighboring neurons [45]. It is important to note that in the context of LSM, the term \"spatial\" refers to the 2D plane learned by the module, as opposed to the conventional 1D arrangement of EEG recordings."}, {"title": "C. Fusion of spatial spectrum and EEG features", "content": "The dotted box in Figure 2 (b-2) depicts the structure of the fusion block, which takes both spatial spectrum and EEG feature as inputs. An FC layer is applied to transform the spatial spectrum into a high-dimensional embedding, which is then reshaped to match the dimensions of the EEG features. The reshaped spatial spectrum embedding is concatenated with the EEG feature to form the output of the fusion block. This approach facilitates the neural network utilizing spatial cues of the competing speakers.\nSpecifically, let $Z$ be the intermediate EEG feature, e.g., the output of the LSM module. The fused feature $Z$ can be represented as"}, {"title": "IV. DATA PREPERATION & TRAINING", "content": "It has been revealed that the inherent long-range temporal correlation (LRTC) in EEG signals severely impacts DNN-based directional focus decoding models, resulting in overestimated decoding accuracy [29], [30], [31], [32], [33], [34], [35]. Speaker-related and audio-content-related features may also be captured by DNN models, necessitating carefully designed experiments to mitigate these effects. The participants are required to close their eyes during audio playback to minimize visual bias. However, eye movement components cannot be fully controlled during EEG experiments nor removed by preprocessing algorithms, resulting in unavoidable visual bias.\nIn our experiments, we made every possible effect to remove"}, {"title": "A. Cross-Validation", "content": "these biases. We demonstrate the employed LOO cross-validation paradigms in Figure 3. Figure 3 (a) illustrates the leave-one-trial-out (LOTO) scenario, where the training, validation, and test trials never overlap. Additionally, the DNN decoder is trained on the training trials of all subjects and evaluated on the held-out trials of the same subjects. That is to say, the LOTO decoder depends on specific subject groups.\nFigure 3 (b) depicts the leave-one-subject-out (LOSO) scenario. In this scenario, trials from one subject are used to validate and test the models, while trials from other subjects are used to train the DNN decoder. In other words, the DNN decoder never sees any trials from the test subject. Thus, the decoder cannot leverage subject-specific patterns to achieve higher decoding accuracy, making the leave-one-subject-out scenario more generalized and more challenging.\nFigure 3 (c) demonstrates the leave-one-moment+trial-out (LOMTO) condition. In this paradigm, trials adjacent to the validation or test trials (white trials in the figure) are removed. Figure 3 (d) and (e) depict the leave-one-class+trial-out (LOCTO) and leave-one-audio+trial-out (LOATO) conditions, respectively. The LOC+TO datasets follow these rules:\n\u2022\nTrials from one subject may have only one unique class"}, {"title": "B. Trial Labeling", "content": "We demonstrate five distinct trial labeling paradigms in Figure 4. In the first paradigm, all trials are labeled as their directions of attended speakers. In the second paradigm, trials without front-rear counterparts are excluded from the dataset, and the retained trials are labeled as their original attended directions, resulting in eight distinct classes. In the third paradigm, trials retained in the second paradigm are labeled according to the quadrant where they live, leading to a 4-class directional focus decoding dataset. In the last two paradigms, trials are labeled as left-right symmetric (the fourth one) or front-rear symmetric (the fifth one) pairs.\nThe proposed labeling paradigms facilitate the investigation of impact of number of alternative directions on the performance of directional focus decoding, which is discussed in next section."}, {"title": "C. Spatial Spectrum Computation", "content": "The spatial spectrum for each trial was precomputed following Equation (1), and all segments of a single trial were linked to the same spatial spectrum. This method is considered reasonable as it prevents the leakage of trial-specific spatial spectrum patterns into the evaluation sets."}, {"title": "V. RESULTS & DISCUSSION", "content": "Table 1 demonstrates the decoding accuracy of the CNN, LSM-CNN, and EEG-Deformer models on our proposed 15-class AAD dataset in the LOTO and LOSO paradigms with various window lengths. The results indicate that all DNN models cannot decode the attended direction from 15 alternatives using only listeners' EEG signals, and none significantly surpasses the chance level of a 15-class decoder, as shown in Table 1.\nHowever, all models achieve significantly better decoding performance by integrating audio spatial spectra. The simple network structure of the CNN model contributes to its worst decoding accuracy among three spatial-spectrum-informed models. More complicated networks, such as the Sp-Aux-LSM-CNN and Sp-Aux-EEG-Deformer, successfully surpass the chance level of a binary decoder (Wilcoxon sign-rank test, p < 0.001), indicating effective fusion of EEG and audio information. Specifically, the Sp-Aux-EEG-Deformer model achieves a decoding accuracy of 61.83% with a decision window length of 10 seconds in the LOTO scenario. Notably, the Sp-Aux-EEG-Deformer surpasses the Sp-Aux-LSM-CNN in all scenarios, demonstrating its superior modeling capabilities.\nBoth Sp-Aux-LSM-CNN and Sp-Aux-EEG-Deformer models attain a higher decoding accuracy on a longer decision window length, which is consistent with previous studies [7], [16], [47]. However, the Sp-Aux-CNN model achieved a deteriorated performance with longer EEG samples. An explanation is that the single-layer structure of the CNN model [7] contributes to a limited temporal receptive field, resulting in degraded feature recognition capability. Due to its performance deterioration, the CNN model is excluded from further analysis."}, {"title": "A. 15-class directional focus decoding accuracy", "content": "B. The impact of number of alternative directions\nAs shown in the first, third, and fifth rows of Table 1, none of the three DNN models surpass the chance level of a 15-class decoder (Wilcoxon sign rank test, p > 0.05). In addition to the 15-class labeling paradigm, we propose four paradigms with fewer alternative directions to validate the efficacy of our EEG dataset, as demonstrated in Figure 4. The binary front-rear paradigm is used in the next subsections and is not shown in Table 2.\nTable 2 presents the decoding accuracy of EEG-Deformer models and its Sp-Aux variants in the binary (2-class), quaternary (4-class), and octal (8-class) directional focus decoding scenarios. It is shown that EEG-Deformer achieves higher decoding accuracies as the number of alternative directions decreases Specifically, the EEG-Deformer model obtains decoding accuracies of 61.82%, 32.39%, and 22.16% in the binary, quaternary, and octal directional focus decoding experiments, respectively. Significant differences between the decoding accuracies of EEG-Deformer and the random guess decoder are found in the 2-class and 4-class decoding scenarios (Wilcoxon sign rank test, p < 0.05). Therefore, it is feasible to decode multi-class directional focus solely based on listeners' EEG signals, and the efficacy of our dataset is validated. Additionally, experimental results suggest that auditory-evoked EEG responses contain effective but limited information about the attended direction of the listeners, leading to degraded performance of 8-class and 15-class directional focus decoding. Experimental results in the last rows of Table 1 and Table 2 suggest that, rather than simply reducing the alternative directions to where competing speakers are present, the fusion of the spatial spectrum and EEG features results in more effective decoding of the attended direction. Specifically, the"}, {"title": "C. The impact of spatial spectrum", "content": "Sp-Aux-EEG-Deformer model achieves decoding accuracies of 64.75%, 60.91%, and 62.95% in the binary, quaternary, and octal scenarios. Additionally, the number of alternative directions also negatively impacts the decoding performance of the Sp-Aux-EEG-Deformer model when the spatial spectrum is fused.\nAlthough the advantage of fusing audio spatial cues in directional focus decoding has been demonstrated in Table 1 and Table 3, we will further discuss the impact of spatial spectrum fusion to provide extra insight into the fusion of spatial spectrum and EEG signals.\nAs depicted in Table 1 and Table 3, both Sp-Aux-LSM-CNN and Sp-Aux-EEG-Deformer models achieve significantly higher decoding accuracy by integrating the audio spatial spectrum and EEG features. Additionally, they achieve decoding accuracy above the chance level of a binary decoder, suggesting the effective integration and utilization of both features, as relying solely on audio features results in a binary random guess decoder.\nThe microphone array used in our experiment contains two units, one placed near the left ear and another near the right ear. Traditionally, a two-element microphone array cannot distinguish whether the sound source is coming from the front or rear. And our dataset includes some attended directions that are front-rear symmetric, such as 60\u00b0 versus 120\u00b0, and -45\u00b0 versus -135\u00b0. However, an array placed near the head provides not only the left/right spatial cues but also informs the DNN model of the front/rear information. (If not, the DNN model tends to become a four-class random guess decoder.) A convincing explanation is that listeners' heads affect the time delay of audio signals, resulting in divergent spatial spectra between front-incident and rear-incident audio waves. This variance is then recognized by the DNN model and reflected in the contrasting audio spatial features.\nAs depicted in Figure 4, we retain only \u00b1135\u00b0, \u00b1120\u00b0, \u00b060\u00b0, and \u00b145\u00b0 in the dataset and train DNN models on it to validate their capability to decode front-rear spatial cues. The proposed Sp-Aux-EEG-Deformer achieves a decoding accuracy of 62.95% with a standard deviation of 4.23% in the octal front-rear-symmetric dataset, exhibiting a strong capability to"}, {"title": "D. The impact of unseen subject", "content": "An ideal auditory BCI device is designed to operate on an unseen user, who is not included in its training set. Additionally, a LOTO decoder tends to decode LRTC components in the test trials, resulting in overestimated decoding accuracies [29], [30], [31], making it more valuable to train a directional focus decoder in the LOSO scenario. A LOSO decoder cannot leverage LRTC components which exist in adjacent trials of the same subject, since the validation and test trials never come from training subjects. (See Figure 3 for a schematic demonstration.)\nExtensive experiments confirm the degradation of a 15-class directional focus decoder in the LOSO scenario. Table 1 demonstrates the decoding performance of Sp-Aux-LSM-CNN and Sp-Aux-EEG-Deformer models from the joint input of EEG and spatial spectrum, significantly surpassing the chance level of a 15-class and binary decoder. (Wilcoxon sign-rank test, p<0.001) The Sp-Aux-EEG-Deformer achieves the highest LOSO decoding accuracy of 57.48% with a decision window length of 1 second. The Sp-Aux-LSM-CNN model also obtains a LOSO decoding accuracy of 55.44% with 1-second samples. Slight differences are observed between the LOSO decoding accuracies of Sp-Aux-LSM-CNN and Sp-Aux-EEG-Deformer models. Compared to the LOTO decoders, the LOSO ones achieve degraded performance in the 15-class and 8-class directional focus decoding tasks, which is consistent with previous findings [7], [16]. However, the Sp-Aux-LSM-CNN obtains a higher decoding accuracy in the LOSO scenarios than the LOTO scenarios."}, {"title": "E. The impact of unseen trials with additional constraints", "content": "To the best of our knowledge, previous research has not fully investigated the impact of the leave-one-trial-out cross-validation scenario with additional constraint conditions. Therefore, we conduct extensive experiments to find out potential biases. Specifically, we construct three leave-one-trial-out datasets with additional various constraints. A schematic description of the three datasets is shown in Figure 3.\nAs shown in the first two columns of Table 3, the Sp-Aux-LSM-CNN and Sp-Aux-EEG-Deformer models slightly deteriorate on the LOMTO dataset. Unlike binary directional focus decoding, adjacent trials in our 15-class dataset inherently possess distinct attention labels. Therefore, a 15-class decoder cannot leverage LRTC features to achieve higher decoding accuracy in these scenarios.\nThe LOCTO dataset poses a greater challenge for DNN models. The second two columns of Table 3 demonstrate a moderate degradation in the 15-class decoding accuracy of both LSM-CNN and EEG-Deformer models. It is noted that the Sp-Aux-LSM-CNN model obtains higher decoding accuracy on the held-out class dataset with a decision window length of 10 seconds, and the degradation is more severe with a shorter decision window length. The Sp-Aux-EEG-Deformer model also achieves a lower decoding accuracy in the LOCTO scenario, and the decoding performance degradation is more significant when the decision window length is set to 1 second.\nThe LOCTO dataset introduces a new limitation for DNN models by excluding trials with the same attended direction in the evaluation sets from the training set. Hence, the model cannot utilize features from the same-class training trials to obtain a higher decoding accuracy.\nFinally, both models are trained on the LOATO dataset. It is shown that both models significantly deteriorate on the LOATO dataset. However, they still successfully distinguish the attended direction from the alternatives and surpass the chance level of a binary decoder. This decline is attributed to the unseen attended audio in the validation and test sets and suggests a potential overfitting on the audio-dependent features.\nIn general, new limitations to leave-one-trial-out datasets pose significant challenges on DNN models. Nevertheless, we believe it necessary to conduct such experiments to fully evaluate the generalization capability and robustness of a data-driven directional focus decoding model. As our EEG dataset is not designed for these experiment conditions, e.g., unseen attended audio, we cannot construct a leave-one-audio+subject-out dataset to evaluate the performance of DNN decoders. Carefully designed EEG datasets are desired to meet the requirements for conducting such experiments.\nOverall, the fusion of spatial spectrum and EEG features is an effective step toward multi-class directional focus decoding. DNN models not only rely on EEG signals but also learn to leverage spatial cues embedded in the spatial spectrum to obtain higher decoding accuracy."}, {"title": "VI. CONCLUSIONS", "content": "This paper presents our newly recorded NJU auditory attention decoding dataset. To our knowledge, this is the first EEG recording dataset featuring 15 alternative directions for attended speakers. Using this dataset, we validate the feasibility of 15-class decoding of directional focus, which offers more effective information for subsequent speech processing compared to the commonly investigated binary decoding. We propose integrating the spatial spectrum with EEG features to enhance 15-class directional focus decoding. We introduce a"}]}