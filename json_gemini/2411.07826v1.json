{"title": "Efficient Federated Finetuning of Tiny Transformers with Resource-Constrained Devices", "authors": ["Kilian Pfeiffer", "Mohamed Aboelenien Ahmed", "Ramin Khalili", "J\u00f6rg Henkel"], "abstract": "In recent years, Large Language Models (LLMs) through Transformer structures have dominated many machine learning tasks, especially text processing. However, these models require massive amounts of data for training and induce high resource requirements, particularly in terms of the large number of Floating Point Operations (FLOPs) and the high amounts of memory needed. To fine-tune such a model in a parameter-efficient way, techniques like Adapter or LORA have been developed. However, we observe that the application of LoRA, when used in federated learning (FL), while still being parameter-efficient, is memory and FLOP inefficient. Based on that observation, we develop a novel layer finetuning scheme that allows devices in cross-device FL to make use of pretrained neural networks (NNs) while adhering to given resource constraints. We show that our presented scheme outperforms the current state of the art when dealing with homogeneous or heterogeneous computation and memory constraints and is on par with LoRA regarding limited communication, thereby achieving significantly higher accuracies in FL training.", "sections": [{"title": "Introduction", "content": "In recent years, Large Language Models (LLMs) have dominated various machine learning tasks, particularly next token prediction and text classification, while Vision Transformers (ViTs) have closed the gap with convolutional neural networks (CNNs) in vision tasks. However, these models require massive amounts of data (e.g., huge quantities of text for LLMs) and are very power-hungry for training, as they have billions of parameters to adjust [1]. As these LLMs, like GPT2 [2] or LLaMA [3], have billions of parameters and are trained on large quantities of text, they generalize well to many downstream tasks in the text domain. Similarly, in vision, multimodal Transformers can generalize well to detecting objects in an image [4, 5].\nIn many downstream applications, e.g., in next-word prediction or object classification on smartphones or internet of things (IoT) devices, deploying such large models imposes high resource requirements for inference, potentially causing high latency and high energy consumption. Although, through generalization, they can achieve the desired accuracy, for many tasks, they are not necessarily required, as tiny specialized models would suffice. In particular, we observe that tiny models that require ~ 60\u00d7 fewer resources (Floating Point Operations (FLOPs)) for inference compared to a lightweight GPT-2 124M model can perform similarly\u00b9 in next-token prediction tasks (refer to fig. 1). However, these specialized models require a sufficient amount of problem-specific data to serve as a replacement. In many cases, this problem-specific data resides on resource-constrained edge devices (such as smartphones or IoT devices), is privacy-sensitive, and cannot be centrally stored and processed. While in centralized training, several hundred-watt server GPUs are available, edge devices are very limited in their resources and can only spend a few watts on training. Additionally, such devices can have heterogeneous capabilities [6].\nTo make use of the devices' data, training must be performed on the devices themselves. In recent years, federated learning (FL) has emerged as a privacy-sensitive alternative to centralized training and has shown success in many domains such as smartphone applications, healthcare, and robotics [7, 8, 9, 10]. In this work, we study how tiny pretrained Transformer models can be adapted to downstream tasks using FL with heterogeneous resource-constrained devices.\nTo adapt large models to downstream tasks, recently, popular techniques like Adapter [11] and LoRA [12] have been introduced, mainly allowing the adaptation of such models in a parameter-efficient way. However, we observe that while being parameter-efficient, techniques like LoRA still require massive amounts of memory for training in the case of tiny models. The reason for this is that LoRA mainly reduces the memory overhead of gradients and optimizer states but does not lower the activation memory. For large models, this typically suffices as the weights and gradients account for most of the required training memory. We observe that the memory footprint of tiny language models and ViTs is mainly dominated by the activation memory (appendix A).\nWe compare LoRA against finetuning individual layers of a set of tiny neural networks (NNs) (Transformers with 3-12 layers, 3 heads, and an embedding size of 92) that were pretrained on OpenWebText [14] and trained in a federated manner to perform next-token prediction on Shakespeare [13] from the Leaf benchmark. From fig. 1, we can"}, {"title": "Related Work", "content": "Subset-based training: Resource constraints, and especially heterogeneous devices in FL, are tackled in a variety of works. A large branch of works applies submodel training [21, 19, 18, 17]. Caldas et al. [13] were among the first to propose such a scheme. Specifically, they randomly drop filters in CNNs to create a submodel with lower resource intensity. Both schemes in HeteroFL [18] and FJORD [19] hierarchically drop filters, such that devices with specific constraints always receive the same parameters. Additionally, in FJORD, each device alternates between subsets that are within its capabilities. However, both require that the most capable device is able to train the full NN model. FedRolex [17] addresses this issue by applying a rolling-window scheme over the parameter set, such that eventually all parameters receive training. Lastly, DepthFL [20] trains subsets by splitting the NN depthwise, using early exits for constrained devices in combination with self-distillation.\nBeyond training of subsets, freezing and quantization have been considered to address the heterogeneity challenges.\nFreezing in FL: In CoCoFL [22], freezing and quantization are used to lower the resources in heterogeneous training. However, it is assumed that the NN model is trained from scratch, i.e., there has to be a set of devices capable of training the first layers. Hence, the selection technique aims to propagate the gradients as far back as possible within the NN structure. Freezing has also been used to progressively increase the size of the NN and thereby save resources. Wang et al. propose ProgFed [23], where the NN is progressively increased by stacking layers on top of each other. Each time a new layer is stacked on the previously trained NN, the NN's head is removed and replaced. The authors show that this enables a significant reduction in required data upload and computations. However, eventually devices have to train the fully stacked NN as no freezing is applied, thereby requiring memory capabilities for fully training the NN model. This constraint is loosened by Successive Layer Training [24], where a model is progressively built up, but more and more early layers get frozen; thereby, the technique allows to obey to a given memory constraint. However, both techniques assume training a model in a federated"}, {"title": "Methodology", "content": "Problem statement: We consider an FL (synchronous cross-device) problem, where a single server and many devices c\u2208 C exist. Training is done on the devices for several rounds R, where the server is only responsible for aggregation. Furthermore, we require that a set of pretrained tiny Transformer architectures F exists, where each architecture Fr \u2208 F has a specific number of stacked layers l (specifically, a layer constitutes a multi-head attention module, as well as feedforward blocks). We assume all NNs in this set satisfy inference latency requirements. A fixed number of devices |C(r)| < |C| participate every round r < R. Further, we assume that devices are subject to constraints. Specifically, we assume that a device can only train with a limited amount of memory, can be restricted in its upload capabilities\u00b2, and can only perform a limited number of computations (FLOPs) per round. Without loss of generality, we assume that these constraints are known to the server and are static over time. We label Me as the peak memory available for training on device c. Similarly, we label Uc and Oc as the amount of upload a device can do and the computation that can be performed, respectively. Consequently, a device c can only participate in the training if the selected NN-configuration satisfies the given constraints.\nWe aim to maximize the accuracy of the FL system, given a set of devices C, with a set of (heterogeneous) constraints U = {Uc : \u2200c \u2208 C}, M = {Mc : \u2200c \u2208 C}, and O = {Oc : \u2200c \u2208 C} within a limited number of rounds R.\n\u00b2We assume that only limited bandwidth is available for a device to upload its weights within a reasonable time within a round."}, {"title": "NN-architecture selection with heterogeneous devices", "content": "We assume that each FL device c should be capable in participating in the training. Consequently a feasible NN structure Fi is required, where a device-specific t exits that allows all devices to apply training. Since there might be multiple architectures that allow this, we reduce the set F to Ffeasible by using\nFfeasible \u2190 {Fi \u2208 F | (\u2203t)[t \u2208 [1,...,l]^\\forallc \\in C [MFt \u2264 Mc/UFt \u2264 Uc\u2227OFt \u2264 Oc]]} (2)"}, {"title": "Heterogeneous freezing of pretrained NNs", "content": "We consider each NN architecture F\u2081 to have l layers, where a specific number of these layers can be trained while others are frozen. We define Ft as an NN architecture with t \u2264 l out of a total of l layers being trained. Specifically, in Ft, we freeze the first l \u2013 t layers, while the remaining t layers with indices [l \u2212 t + 1,...,l] are being trained. Each architecture F\u2081 can be trained with a different number of layers t\u2208 [1,...,1]. We refer to l and t as a training configuration. Each configuration has associated resource requirements. We label MFt as the memory required to train Ft. Similarly, we label Ort as the computation operations (FLOPs) required, and UFt for communication upload, respectively. In general, a device c can only apply training to a given NN-configuration Ft if\nMFt \u2264 McUFt \u2264 Uc \u2227 OFt \u2264 Oc (1)"}, {"title": "Experimental Evaluation", "content": "We evaluate our technique and state of the art based on a set of pretrained models F. Specifically, we pretrain Transformers with layers l\u2208 [3,6,9,12]. Each NN model Fi only varies in the number of layers. We use an embedding dimension of 96 and 3 heads per attention block for language modelling and 192 and 6 for ViTs. For language tasks, we use a sentencepiece tokenizer with a vocab size of 8192 and pretrain for 750K mini-batch steps on OpenWebText [14], using a context length of 256 and batch size 128. For vision tasks, we use a ViT [30] with patch size of 4 and a context length of 272. We pretrain on a downscaled (3 \u00d7 64 \u00d7 64) version of ImageNet [31] for 500K steps with a batch size of 256. We apply data augmentation techniques such as random flipping, rotation, as well as random cropping. For both, vision and text domain, we use an initial learning rate of n = 5\u00b710-4 and use a sinusoidal decay to \u03b7 = 5\u00b710-5 (1.10-4 and 1\u00b7 10-5 for ViT). We use AdamW [32] as optimizer (\u03b2\u2081 = 0.9, \u03b22 = 0.95), with weight decay of 0.1 for linear layers. In all cases, dropout of 0.05 is used.\nFederated training: We train a downstream task in a federated manner, where we distribute equal shares of Shakespeare from the Leaf benchmark [13] (next token prediction), and CIFAR10, and CIFAR100 [33] to devices c \u2208\u0404\u0421. Consequently, each device c has a local private dataset De. For CIFAR and Shakespeare, we use a total of |C| = 100 and 10 devices per round (|C(r)| = 10). We train for a total number of R = 75 rounds. In the case of Shakespeare, each device randomly picks a sentence using a context length of 256 from its local text, and trains for 8 batches with batch size 32. In the case of CIFAR, each device iterates once over its data samples using batch size 32. To have the same input resolution as the data used for pretraining, we upscale CIFAR data from 3 x 32 x 32 to 3 x 64 x 64. We distribute CIFAR100 and CIFAR10 in an non-independent and identically distributed (iid) fashion with Dirichlet a = 0.1 and a= 1.0, respectively. We use the same optimizer setup as used in pretraining and apply learning rate decay"}, {"title": "Ablation study", "content": "To evaluate how the NN selection eq. (3) behaves when having heterogeneous devices, we mix two groups with computation constraints [55, 75] and [75, 100] GFLOPs with different rates. I.e., a rate of 10% refers to 10% of devices having a constraint of 75 while 90% have 55. For ratios between 0% and 100%, we evaluate the accuracy on Shakespeare with all feasible Transformer architectures and what F\u2081 is picked by our technique based on the ratio.\nWe can observe in fig. 3 that maximizing the average number of trained layers (as in eq. (3)) robustly picks the NN that maximizes the accuracy. Depending on the average trained layers t (which depends on the mixing ratio), eq. (3) switches from a 3 to a 6 and 6 to a 9-layered NN, maximizing the accuracy."}, {"title": "State of the art comparison", "content": "We compare CAFF against several state of the art techniques: Heterogeneous LoRA [15], FedHM [16], HeteroFL [18], FJORD [19], FedRolex [17], and DepthFL [20]. We provide details about the configuration of the state of the art in appendix B."}, {"title": "Experimental results", "content": "Homogeneous results: We evaluate our technique and Heterog. LoRA as well as FedHM and FedRolex in a homogeneous setting with Shakespeare and CIFAR100. Specifically, we run several experiments with homogeneous memory constraints of 500, 700, and 900 (400, 600, and 800 for ViT) MB, upload constraints of 4, 6, and 8 (8, 12, and 16 for ViT) MB, as well as computation constraints of 60, 80, and 100 (75, 150, 200 for ViT) GFLOPs. We omit results for HeteroFL, FJORD, and DepthFL, as in the homogeneous case they all degrade to vanilla FedAvg. For completeness, we visualize a range of constraints in fig. 4. To have a fair comparison, we evaluate the technique with pretrained transformers with l\u2208 [3, 6, 9, 12] layers and present the respective highest gained accuracy in table 1. For our technique, we select the number of layers based on eq. (3).\nIn general, we can observe that CAFF achieves higher accuracies in almost all considered constraint settings, particularly with memory constraints. Here it can be observed that, to reach a certain accuracy, LoRA requires 2 - 3\u00d7 more memory compared to CAFF. However, LoRA achieves higher accuracies when upload constraints are applied. Additionally, it can be observed that FedRolex, when training a larger pretrained model than the devices can fully train within a round eq. (8), the accuracy heavily deteriorates. In fig. 4, we can see that with both Heterog. LoRA and FedHM lowering the rank for a specific NN impacts upload and computation but have minimal effect on the required peak memory. In contrast, CAFF supports a wider range of applicability across all three constraints.\nHeterogeneous results: To explore how our technique performs when devices have heterogeneous constraints, we explore a scenario where 50% of devices have higher constraints than the other 50% throughout all rounds. Specifically, we evaluate a setting an equal share of devices having memory constraints of [400, 600], [600, 800], and [400, 800] ([200, 400], [400, 600], and [200, 800] for ViT) MB. Similarly, we evaluate [2, 8], [4, 8] ([2, 4] and [2, 8] MB for ViT) for upload constraints, and [55, 75] and [75, 100] ([75, 100] and [125, 200] for ViT) GFLOPs for computation, respectively.\nWe observe that, across most evaluated scenarios, CAFF results in higher accuracy when devices have heterogeneous constraints. For CAFF, higher average resources lead to higher final accuracy, indicating that our technique efficiently uses available resources. However, with subset-training derived techniques, this is not always the case; for instance, with higher average computation in Shakespeare, many baselines actually perform worse or do not improve. For DepthFL we observe that the additional exit of the stronger devices significantly increases the upload and memory overhead, hence, for many evaluated scenarios (especially in case of language models), no suitable configuration is available.\nCan weaker devices make a contribution to the global model? To evaluate if weaker devices can contribute to the model, we change the data distribution from non-iid to a distribution that correlates with the devices' constraints (similar to [22], using a = 0.1). Specifically, we distribute the data"}, {"title": "Conclusion", "content": "In this work, we studied efficient training of pretrained tiny NNs in cross-device FL. We discovered that existing techniques like LoRA do not provide a good trade-off between resource usage and accuracy when considering peak memory and computation. Given a set of pre-trained NN architectures to choose from, we propose CAFF, an NN selection scheme and layer finetuning approach that outperforms LORA and other state-of-the-art techniques, improving accuracy and fairness in resource-constrained scenarios. We believe that our technique, particularly the NN selection scheme, contributes to practical design considerations for"}, {"title": "Technical details of constraint evaluation", "content": "Peak Memory: Peak memory is measured by summing the three major components that impact peak memory: the number of weights that must be kept in memory, the optimizer state of AdamW, and the activations that must be stored in memory. Activations that must be kept in memory are collected by traversing the compute graph during backpropagation in PyTorch [34].\nCommunication: The amount of data (in MB) that needs to be uploaded by a device is computed by tracking the parameters that change during training on the device (i.e., unfrozen parameters). Unchanged parameters do not need to be uploaded.\nComputation: The number of GFLOPs required for a single mini-batch during training is calculated by determining the forward and backward operations needed for major operations such as linear layers, dot products, addition, and layer normalization,"}, {"title": "Training memory components", "content": "We analyze the different memory components involved in training with layer freezing as used in CAFF and baselines. The components parameters, gradients and optimizer states, and activations are visualized for layer freezing and LoRA in fig. 5. It can be observed that LoRA consistently requires"}, {"title": "State of the art comparison", "content": "Heterogeneous LoRA [15]: LoRA applies a low-rank adapter to linear operations in the NN's layers. To support heterogeneity, the low-rank adapters are aggregated similarly to HeteroFL [18]. We freeze the token and position embedding table similar to layer finetuning. All LayerNorm modules receive training. We fully train the output linear layer as we have seed bad performance when using low-rank adapters.\nFedHM [16]: In FedHM, a lower complexity model is created by applying a singular value decomposition (SVD) to linear layers $w \\in \\mathbb{R}^{P \\times Q}$ such that $U, S, V^T = svd(w)$. Using a lower rank $z$, an approximation of $w \\sim U diag(S) \\cdot V^T$ can be constructed using two consecutive linear layers with $U diag(S) \\in \\mathbb{R}^{P \\times z}$ and $V^T \\in \\mathbb{R}^{z \\times Q}$. We pick the NN structure so that the least constrained device can fully train the NN without using SVD. All devices with less resources pick $z$ s.t.\nzs = max(z) \\quad s.t. \\quad MFz < Mc/UFz <Uc/OFz \u2264 Oc. (6)\nThe parameters $w$ are reconstructed on the server for aggregation by using $U \\cdot diag(S) \\cdot V^T$.\nHeteroFL: [18]: HeteroFL is a state-of-the-art technique that enables heterogeneous devices to train a common NN model. This is achieved by training a subset of the full NN architecture. Let $w \\in \\mathbb{R}^{P \\times Q}$ represent a linear layer in a Transformer NN. To lower the resources, constrained devices scale down the NN by using a subset of $w$ using a scale factor $s \\in (0, 1]$, such that $\\tilde{w} \\in \\mathbb{R}^{[sP] \\times [sQ]}$. Scaling down linear layers with $s$ results in a quadratic reduction in parameters (hence upload and computation), but only in a linear reduction in memory (as activations make up for most required memory, which decreases linearly with $s$. In HeteroFL, constrained devices receive always the same fixed subset of the full weights $w$. We introduce $I$ as the set of indices, that are used to create a subset. In HeteroFL, a device $c$ with a constraint $s$ uses a subset of size $[sP] \\times [sQ]$ by using $I_{Q} \\in \\{i|0 < i < [sQ] - 1\\}$ as output indices and $I_{P} \\in \\{i|0 < i < [sP] - 1\\}$ as input indices. HeteroFL"}, {"title": null, "content": "requires that a share of FL devices are capable of training the full NN weights. Therefore, we select the NN structure with the most layers that can still be fully trained (s = 1) by a single participating device. Remaining devices create a submodel $F_s$ that support their constraint by using\nsc = max(s) \\quad s.t. \\quad M_{\\tilde{w}s} < M_c/U_{\\tilde{w}s} <U_c/O_{\\tilde{w}s} \u2264 O_c. (7)\nFJORD [19]: FjORD applies the same strategy as HeteroFL with respect to indices. However, a constrained device switches on a mini-batch level between different levels of $s$.\nFedRolex [17]: Here, devices use a subset similarly to HeteroFL. However, while in HeteroFL, a constrained device always trains the same slice of weights $w$, in FedRolex, a rolling window approach is used. Output indices are selected per round $r$, such that for a linear layer $w$, $I$ is selected using\nI^{(r)} = \\{r, r + 1, ...,r + [scQ] - 1\\} (8)\nin case $r + [scQ] \u2264 Q$.\n\\{\u00ee, r + 1, . . ., Q \u2212 1 \\} \u222a \\{0, . . ., f + [scQ] \u2212 1 \u2212 Q\\} (9)\notherwise, where $r = r \\mod Q$. Thereby, FedRolex eventually trains all weights of an NN, even if no device can fully train all parameters within a round.\nDepthFL [20]: As we evaluate devices within two distinct resource groups, we configure DepthFL such that the weaker devices use a single early exit, while the stronger devices use the same early exit as the weaker devices, as well as the last exit of the NN structure. On the server, we evaluate the accuracy based on the first exit of the NN."}]}