{"title": "HW-TSC's Submission to the CCMT 2024 Machine Translation Tasks", "authors": ["Zhanglin Wu", "Yuanchang Luo", "Daimeng Wei", "Jiawei Zheng", "Bin Wei", "Zongyao Li", "Hengchao Shang", "Jiaxin Guo", "Shaojun Li", "Weidong Zhang", "Ning Xie", "Hao Yang"], "abstract": "This paper presents the submission of Huawei Translation Services Center (HW-TSC) to machine translation tasks of the 20th China Conference on Machine Translation (CCMT 2024). We participate in the bilingual machine translation task and multi-domain machine translation task. For these two translation tasks, we use training strategies such as regularized dropout, bidirectional training, data diversification, forward translation, back translation, alternated training, curriculum learning, and transductive ensemble learning to train neural machine translation (NMT) models based on the deep Transformer-big architecture. Furthermore, to explore whether large language model (LLM) can help improve the translation quality of NMT systems, we use supervised fine-tuning to train llama2-13b as an Automatic post-editing (APE) model to improve the translation results of the NMT model on the multi-domain machine translation task. By using these plyometric strategies, our submission achieves a competitive result in the final evaluation.", "sections": [{"title": "1 Introduction", "content": "Neural machine translation (NMT) [1-3] allows translation systems to be trained end-to-end without dealing with issues like word alignment, translation rules, and complex decoding algorithms that characterize statistical machine translation (SMT) [4] systems. Although NMT develops rapidly in recent years, it relies heavily on big data - large-scale, high-quality bilingual corpora. Due to the cost and scarcity of real corpora, synthetic data plays an important role in improving translation quality. Existing methods for synthesizing data in NMT focus on leveraging monolingual data during training. Among them, data diversification [5], forward translation [6], and back translation [7] are widely used to generate synthetic bilingual corpora. Such synthetic data can be used to improve the performance of NMT models. Although synthetic data is efficient, it inevitably contains noise and erroneous translations. Alternated training [8] introduces real data as a guide and alternately uses synthetic data and real data during the training process, which can prevent the training of the NMT model from being interfered with by noisy synthetic data. Another direction to improve the performance of NMT models is to use more efficient training strategies. Methods such as regularized dropout [9], bidirectional training [10], and curriculum learning [11] allow the NMT model to more effectively utilize limited data during the training process, while transductive ensemble learning [12] can aggregate the translation capabilities of multiple models into one model.\nThe powerful capabilities of LLM in logical reasoning and language generation promote the further development of machine translation. Despite the superior performance of translation models, existing models usually use beam search decoding [13] and top-1 hypothesis selection for inference. These techniques struggle to fully exploit the rich information in various N-best hypotheses, making them suboptimal for translation tasks that require a single high-quality output sequence. GenTranslate [14] utilizes the rich language knowledge and powerful reasoning capabilities of large language model (LLM) [15-18] to integrate the rich information in the N-best list from the basic model, generating higher-quality translation results.\nTo promote academic exchanges and connections between domestic and foreign research units and relevant industry partners, and to jointly advance the development of machine translation research and technology, we participate in the bilingual machine translation tasks and multi-domain machine translation tasks organized by CCMT2024. For these two translation tasks, we use training strategies such as regularized dropout [9], bidirectional training [10], data diversification [5], forward translation [6], back translation [7], alternated training [8], curriculum learning [11], and transductive ensemble learning [12] to train Neural machine translation (NMT) models based on the deep Transformer-big architecture [19-22]. Additionally, drawing inspiration from the GenTranslate method [14], we utilize supervised fine-tuning (SFT) [23] to train llama2-13b\u00b9 as an automatic post-editing (APE) model [24], aimed at enhancing the translation outputs of NMT models in the multi-domain machine translation task.\nNext, this paper expands on the details of our translation system in different translation tasks. The structure of the remaining sections is as follows: Section 2 describes the data size and data pre-processing; Section 3 provides an overview of our NMT system; Section 4 explains our APE system; Section 5 presents the parameter settings and experimental results; Section 6 summarizes our systems."}, {"title": "2 Dataset", "content": "According to the requirements of the CCMT 2024 outline, we train the NMT system from scratch on the bilingual translation task using the officially provided data.\n2.2 Data Pre-processing\nThe data pre-processing process is as follows:\n\u2022 Remove duplicate sentences or sentence pairs.\n\u2022 Remove invisible characters and xml escape characters.\n\u2022 Convert full-width symbols to half-width symbols.\n\u2022 Use jieba\u00b2 to pre-segment Chinese sentences.\n\u2022 Use mosesdecoder\u00b3 to normalize English punctuation.\n\u2022 Use opencc to convert traditional Chinese to simplified Chinese.\n\u2022 Use fasttext to filter other language sentences.\n\u2022 Use fast_align to filter poorly aligned sentence pairs.\n\u2022 Filter out sentences with more than 150 tokens in bilingual data.\n\u2022 Split long sentences in monolingual data into multiple short sentences.\n\u2022 Filter out sentence pairs with token ratio greater than 4 or less than 0.25.\n\u2022 When performing subword segmentation, joint Byte Pair Encoding7 [25] is used for mn\u2192zh, ti\u2192zh and uy\u2192zh translation tasks, and joint sentencepiece [26] is used for zh\u2192en and en\u2192zh translation tasks."}, {"title": "2.1 Data Size", "content": ""}, {"title": "3 NMT System", "content": "Transformer is the state-of-the-art model structure in recent MT evaluations. There are two parts of research to improve this kind: the first part uses wide networks (eg: Transformer-Big [27]), and the other part uses deeper language representations (eg: Deep Transformer [28]). For all MT tasks, we combine these two improvements, adopting the Deep Transformer-Big [19-22] model structure to train the NMT system. Deep Transformer-Big uses pre-layer normalization, features 25-layer encoder, 6-layer decoder, 16-heads self-attention, 1024-dimensional word embedding and 4096-dimensional ffn embedding."}, {"title": "3.1 System Overview", "content": ""}, {"title": "3.2 Regularized Dropout", "content": "Dropout [31] is a widely used technique for regularizing deep neural network training, which is crucial to prevent over-fitting and improve the generalization ability of deep models. Dropout performs implicit ensemble by simply dropping a certain proportion of hidden units from the neural network during training, which may cause an unnegligible inconsistency between training and inference. Regularized Dropout (R-Drop) [9] is a simple yet more effective alternative to regularize the training inconsistency induced by dropout. Concretely, in each mini-batch training, each data sample goes through the forward pass twice, and each pass is processed by a different sub model by randomly dropping out some hidden units. R-Drop forces the two distributions for the same data sample outputted by the two sub models to be consistent with each other, through minimizing the bidirectional Kullback-Leibler (KL) divergence [32] between the two distributions. In this way, the inconsistency between the training and inference stage can be alleviated."}, {"title": "3.3 Bidirectional Training", "content": "Many studies have shown that pre-training can transfer the knowledge and data distribution, hence improving the generalization. Bidirectional training (BiT) [10] happens to be a simple and effective pre-training method for NMT. Bidirectional training is divided into two stages, the early stage bidirectionally updates model parameters, and then tune the model normally. To achieve bidirectional updating, we only need to reconstruct the training samples from \"src\u2192tgt\" to \"src+tgt\u2192tgt+src\" without any complicated model modifications. Notably, BiT does not increase any parameters or training steps, requiring the parallel data merely."}, {"title": "3.4 Data Diversification", "content": "Data Diversification (DD) [5] is a data augmentation method to boost NMT performance. It diversifies the training data by using the predictions of multiple forward and backward models and then merging them with the original dataset on which the final NMT model is trained. DD is applicable to all NMT"}, {"title": "3.5 Forward Translation", "content": "Forward translation (FT), also known as self-training [6], is one of the most commonly used data augmentation methods. FT has proven effective for improving NMT performance by augmenting model training with synthetic parallel data. Generally, FT is performed in three steps: (1) randomly sample a subset from the large-scale source monolingual data; (2) use a \"teacher\" NMT model to translate the subset data into the target language to construct the synthetic parallel data; (3) combine the synthetic and authentic parallel data to train a \"student\" NMT model."}, {"title": "3.6 Back-Translation", "content": "An effective method to improve NMT with target monolingual data is to augment the parallel training data with back translation (BT) [7]. There are many works broaden the understanding of BT and investigates a number of methods to generate synthetic source sentences. Edunov et al. [29] find that back translations obtained via sampling or noised beam outputs are more effective than back translations generated by beam or greedy search in most scenarios. Caswell et al. [30] show that the main role of such noised beam outputs is not to diversify the source side, but simply to indicate to the model that the given source is synthetic. Therefore, they propose a simpler technique, Tagged BT. This method uses an extra token to mark back translated source sentences, which is generally outperform than noised BT."}, {"title": "3.7 Alternated Training", "content": "While synthetic bilingual data have demonstrated their effectiveness in NMT, adding more synthetic data often deteriorates translation performance since the synthetic data inevitably contains noise and erroneous translations. Alternated training (AT) [8] introduce authentic data as guidance to prevent the training of NMT models from being disturbed by noisy synthetic data. AT describes the synthetic and authentic data as two types of different approximations for the distribution of infinite authentic data, and its basic idea is to alternate synthetic and authentic data iteratively during training until the model converges."}, {"title": "3.8 Curriculum Learning", "content": "A practical curriculum learning (CL) [33] method should address two main questions: how to rank the training examples, and how to modify the sampling procedure based on this ranking. For ranking, we choose to estimate the difficulty of training samples according to their domain feature [11]. The calculation formula of domain feature is as follows, where $d_{in}$ represents an in-domain NMT model, and $d_{out}$ represents a out-of-domain NMT model.\n$q(x,y) = \\frac{\\log P(y|x; \\theta_{in}) \u2013 \\log P(y|x; \\theta_{out})}{Y}$ (1)\nFor the sampling procedure, we adopt a probabilistic CL strategy10 that takes advantage of the spirit of CL in a nondeterministic fashion without discarding the good practice of original standard training policy, like bucketing and mini-batching."}, {"title": "3.9 Transductive Ensemble Learning", "content": "Ensemble learning [34], which aggregates multiple diverse models for inference, is a common practice to improve the accuracy of machine learning tasks. However, it has been observed that the conventional ensemble methods only bring marginal improvement for NMT when individual models are strong or there are a large number of individual models. Transductive Ensemble Learning (TEL) [12] study how to effectively aggregate multiple NMT models under the transductive setting where the source sentences of the test set are known. TEL uses all individual models to translate the source test set into the target language space and then finetune a strong model on the translated synthetic data, which boosts strong individual models with significant improvement and benefits a lot from more individual models."}, {"title": "4 APE System", "content": "There is recently a surge in research interests in Transformer-based LLMs, such as ChatGPT [15, 16] and LLaMA [17, 18]. Benefiting from the giant model size and oceans of training data, LLMs can understand better the language structures and semantic meanings behind raw text, thereby showing excellent performance in a wide range of natural language processing (NLP) tasks. As shown in Figure 2, we use supervised fine-tuning to train LLM as an APE model to improve the translation quality of our NMT model on the zh\u2192en multi-domain machine translation task. Our APE system is inspired by the GenTranslate [14], but the difference is that we use source language text as part of the input information of LLM because we believe that adding source language text helps ensure the fidelity of the target language translation."}, {"title": "4.1 System Overview", "content": ""}, {"title": "4.2 Efficient LLM Finetuning", "content": "We choose Llama2-13b as the base LLM for our APE system. When performing supervised fine-tuning on this base LLM, full fine-tuning by retraining all model parameters is usually expensive and requires a long training period. Therefore, we adopt the popular LoRA-based efficient parameter fine-tuning method\u00b9\u00b9 [35]. This method freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. LoRA can lower the hardware threshold by up to 3x when using an adaptive optimizer because we do not need to compute gradients or maintain optimizer state for most parameters."}, {"title": "4.3 HypoTranslate Dataset", "content": "To build the APE training data for Efficient LLM fine-tuning, we first use the cometkiwi model12 [36] to select high-quality bilingual data. Specifically, we select bilingual data with a cometkiwi score greater than 0.8 on the zh\u2192en language pair. Then we use our trained NMT model as a base translation model to decode the N-best hypotheses from the source language text via a beam search algorithm, where the beam size N is set to 10."}, {"title": "5 Experiments", "content": "We use Pytorch-based Fairseq [37] open-source framework to train the NMT model, and use Adam optimizer [38] with \u03b21=0.9 and \u03b22=0.98 to guide the parameter optimization. During the training phase, each model uses 8 GPUs for training, batch size is 2048, update frequency is 4, learning rate is 5e-4, label smoothing rate is 0.1 and warm-up steps is 4000. We set dropout to 0.1 for high-resource translation tasks and 0.3 for low-resource translation tasks respectively. In addition, when applying R-Drop for training, we follow the setting of Let al [9], using reg_label_smoothed_cross_entropy as the loss function, and set reg-alpha to 5. Then, we use SacreBLEU [39] and COMET13 [35] to evaluate the overall translation quality of each NMT model.\nTo adapt LoRA-based efficient LLM fine-tuning [35], we use llama-recipes14 open source framework to train the APE model. The following is the configuration of LoRA: lora_rank is 32, lora_alpha is 64, lora_dropout is 0.05, lora_modules are \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\". We train 2 epochs with AdamW optimizer [36], with learning rate initialized to 1e-4 and then linearly decrease to le-5 during training. The batch size is set to 6, with accumulation iterations set to 8 (i.e., real batch size is 48), the context length is 4096, and the batch strategy is packing."}, {"title": "5.1 Setup", "content": ""}, {"title": "5.2 Bilingual MT Results", "content": "On en\u2192zh and zh\u2192en translation tasks, we use BiT and R-Drop to build a strong baseline system. Subsequently, we adopt the data augmentation methods of DD, FT and ST to improve the translation quality of baseline System. Next, we use AT guide model training with authentic bilingual data. Then, we use CL for domain adaptation. Finally, we train multiple NMT systems and integrate them using TEL as the final translation system."}, {"title": "5.2.1 en\u2192zh & zh\u2192en", "content": ""}, {"title": "5.2.2 mn zh, ti\u2192zh & uy\u2192zh", "content": "On mn zh, ti\u2192zh and uy\u2192zh translation tasks, we also use BiT and R-Drop to build a strong baseline system. The subsequent training method is similar to en\u2192zh and zh\u2192en translation tasks. The only difference is that we adopt DD and Tagged BT in the data augmentation stage, which is due to the lack of source language monolingual for these three tasks."}, {"title": "5.3 Multi-domain MT Results", "content": "On the zhen multi-domain machine translation task, we first select the best model (TEL) in the zh\u2192en bilingual machine translation task as the baseline model. Then, we use the collected high-quality domain-related bilingual data to fine-tune for domain adaptation. Finally, we use the APE model based on llama-13b to improve the translation results generated by the NMT model, and use the post-editing result as the final translation result."}, {"title": "6 Conclusion", "content": "This paper presents HW-TSC's submission to the bilingual machine translation task and multi-domain machine translation task of CCMT 2024. For both translation tasks, we use a series of training strategies to train NMT models based on the deep Transformer-big architecture. Additionally, for the multi-domain machine translation task, we use the powerful generation capabilities of LLM to post-edit the translation results of the NMT model to obtain better translations. Relevant experimental results also show the effectiveness of our adopted strategies. By using these enhancement strategies, our submission achieves a competitive result in the final evaluation."}]}