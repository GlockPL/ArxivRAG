{"title": "Decentralized Cooperation in Heterogeneous Multi-Agent Reinforcement Learning via Graph Neural Network-Based Intrinsic Motivation", "authors": ["Jahir Sadik Monon", "Deeparghya Dutta Barua", "Md. Mosaddek Khan"], "abstract": "Multi-agent Reinforcement Learning (MARL) is emerging as a key framework for various sequential decision-making and control tasks. Unlike their single-agent counterparts, multi-agent systems necessitate successful cooperation among the agents. The deployment of these systems in real-world scenarios often requires decentralized training, a diverse set of agents, and learning from infrequent environmental reward signals. These challenges become more pronounced under partial observability and the lack of prior knowledge about agent heterogeneity. While notable studies use intrinsic motivation (IM) to address reward sparsity or cooperation in decentralized settings, those dealing with heterogeneity typically assume centralized training, parameter sharing, and agent indexing. To overcome these limitations, we propose the CoHet algorithm, which utilizes a novel Graph Neural Network (GNN) based intrinsic motivation to facilitate the learning of heterogeneous agent policies in decentralized settings, under the challenges of partial observability and reward sparsity. Evaluation of CoHet in the Multi-agent Particle Environment (MPE) and Vectorized Multi-Agent Simulator (VMAS) benchmarks demonstrates superior performance compared to the state-of-the-art in a range of cooperative multi-agent scenarios. Our research is supplemented by an analysis of the impact of the agent dynamics model on the intrinsic motivation module, insights into the performance of different CoHet variants, and its robustness to an increasing number of heterogeneous agents.", "sections": [{"title": "Introduction", "content": "The paradigm of Multi-agent Reinforcement Learning (MARL) is rapidly emerging to be pivotal in a broad spectrum of practical applications such as resource management (Ying and Dayong 2005), autonomous vehicles (Cao et al. 2013), traffic signal control (Calvo and Dusparic 2018a), supply chain management (Fuji et al. 2018), robotics (Lillicrap et al. 2019), robot swarms (H\u00fcttenrauch, \u0160o\u0161i\u0107, and Neumann 2017), etc. These applications generally benefit from the efficient use of the diverse capabilities of heterogeneous agents. Moreover, the successful execution of tasks in these multi-agent systems requires the agents to adapt their behaviors to other agents for effective coordination rather than operating independently. The real-world deployment of these MARL systems typically involves the agents relying solely on the local environmental information and learning policies with infrequent environmental rewards (Wiewiora 2010; Wakilpoor et al. 2020).\nApplications such as package transport (Gerkey and Mataric 2002), traffic lights control (Calvo and Dusparic 2018b), disaster response (Michael et al. 2012), agriculture (Ju and Son 2019), etc. utilize agent heterogeneity such as distinct physical and behavioral traits of agents. Heterogeneity is also vital in multi-robot tasks as it enables efficient characterization and discovery of diverse behaviors, improving learning performance (Liu et al. 2022). On the other hand, the dependency on reward signals for the agent's learning process introduces the issue of reward sparsity (Hare 2019). Due to the lack of frequent feedback signals from the environment and the non-trivial nature of manually designing reward functions, MARL systems need to be robust enough to deal with infrequent environmental rewards.\nFurthermore, in addition to these challenges, the majority of real-world applications constrain the agents to act in a decentralized manner, and under partial observability, where each agent has a partial view of the shared environment. As a result, it is impractical for them to learn cooperative behaviors by utilizing a centralized algorithm that possesses global knowledge of all the agents and the state space (Iqbal and Sha 2019; Liu, Yeh, and Schwing 2019). In comparison to centralized training and full observability, the challenges of agent heterogeneity and reward sparsity are more severe in decentralized training under partial observability (Ma et al. 2022). Despite the aforementioned requirements for real-world deployments, most of the existing solutions to date either utilize global parameter sharing among all the agents or employ a centralized critic with global knowledge.\nTo the best of our knowledge, no prior research has addressed the issue of cooperative heterogeneous MARL in decentralized training settings under the practical constraints of real-world applications, such as partial observability and reward sparsity (see Section for more details). We propose CoHet, an algorithm that facilitates heterogeneous agent cooperation addressing the constraints required for real-world deployments. CoHet does not require any prior knowledge of agent heterogeneity (e.g speed, size, type, agent index). It employs an architecture for learning heterogeneous MARL"}, {"title": "Related Works", "content": "policies by utilizing a novel Graph Neural Network-based intrinsic motivation/reward calculation mechanism. In summary, our specific contributions are as follows:\nA Novel Intrinsic Reward Mechanism: We introduce a novel self-supervised intrinsic reward calculation algorithm CoHet - utilizing the underlying communication graph of a Graph Neural Network (GNN). In comparison to previous methods, CoHet accurately estimates the intrinsic rewards in the presence of agent heterogeneity (e.g. physical attributes/composition, behavioral differences) by using only the agents' local neighborhood information and their dynamics model predictions instead of relying on a centralized critic or external controller. We present two formulations of the CoHet algorithm, one that utilizes the neighborhood predictions (CoHetteam), and another that uses the agent's own predictions (CoHetself) for intrinsic reward calculation.\nIntegration with Established Algorithms: Our standalone intrinsic motivation architecture can be integrated with existing decentralized heterogeneous policy learning algorithms, thus enhancing performance in cooperative MARL benchmarks. We demonstrate this* by incorporating the state-of-the-art HetGPPO algorithm (Bettini, Shankar, and Prorok 2023a), leveraging its underlying GNN communication graph for intrinsic reward calculation. In contrast to previous heterogeneous policy learning techniques, this formulation requires no prior knowledge of the types of agent heterogeneity, indexing, etc.\nExtensive Validation and Scalability: We validate Co-Het in the presence of heterogeneous agents in six different scenarios in the Multi-agent Particle Environment (MPE) and Vectorized Multi-Agent Simulator (VMAS) benchmarks, showing superior performance to the baseline Het-GPPO across all scenarios. CoHet also outperforms the state-of-the-art MARL policy learning algorithm, Independent Proximal Policy Optimization (IPPO), in four of the six scenarios. We present findings on the impact of agent dynamics models on the intrinsic reward calculation, compare the two variants of the algorithm, and demonstrate its robustness to an increasing number of heterogeneous agents in a shared environment.\nThe subsequent sections will establish the necessary foundation of our work and explore our contributions in detail.\nIn response to the challenges posed by agent heterogeneity and the sparsity of rewards, most existing literature addresses one or the other. A prevalent method for emulating heterogeneous behavior involves augmenting each agent's observation space with a unique index that represents the agent's type (Foerster et al. 2016; Gupta, Egorov, and Kochenderfer 2017). These methods necessitate prior knowledge of the types of agent heterogeneity. Most of the literature on heterogeneous MARL focuses on tackling the problem of behavioral heterogeneity for physically identical agents (Wang et al. 2020a,b; Li et al. 2021; Lowe et al. 2020). They cannot be applied to all types of heterogeneity as they suggest solutions that are specific to problems within specific sub-classes of heterogeneity. Moreover, their use in partially observable systems is limited due to the lack of inter-agent communication. A notable study on heterogeneous MARL (Bettini, Shankar, and Prorok 2023a), proposes an algorithm termed HetGPPO (Heterogeneous GNN-based Proximal Policy Optimization), that is capable of learning heterogeneous decentralized policies in partially observable scenarios. It uses a GNN-based communication layer for sharing information among agents within local neighborhoods, thereby mitigating the effects of partial observability. Unlike previous methods, their reliance on only the local information enables Decentralized Training with Decentralized Execution (DTDE). Similar to their work, CoHet utilizes only the local neighborhood information under the constraints of DTDE.\nAlthough the aforementioned studies address the problem of agent heterogeneity, typically involving centralized training settings, parameter sharing or prior knowledge of agent heterogeneity, they do not consider the issues with reward sparsity. For tackling the sparse-reward problem in MARL, learning with the efficient exploration of the state space is a widely adopted approach (Liu et al. 2021; Mahajan et al. 2020; Christianos, Sch\u00e4fer, and Albrecht 2021). Dense scalar reward signals termed \"Intrinsic Motivation\", are often used to encourage exploration or coordination among agents (Du et al. 2019; Wu et al. 2021). Studies with a primary focus on cooperative tasks rely on the ability of agents to represent the dynamics of other agents in a shared environment to calculate these intrinsic rewards (Jaques et al. 2019; Stadie, Levine, and Abbeel 2015; Jeon et al. 2022). In case of the algorithm proposed by Ma et al. (2022), termed ELIGN (Expectation Alignment as a Multi-Agent Intrinsic Reward), the intrinsic rewards foster inter-agent coordination in a decentralized manner. However, a significant drawback of using an agent's own dynamics model as a proxy to calculate neighborhood predictions in ELIGN is that it becomes more challenging for agents to accurately model the dynamics of other agents in the presence of agent heterogeneity. Inaccuracies in the dynamics model can result in misleading alignment signals, and as a result, ELIGN scales poorly in the presence of heterogeneous agents. In contrast, CoHet utilizes the local neighborhood information passed via the underlying GNN-communication graph, to more accurately model the heterogeneity among agents.\nThere are only a couple of existing research works that simultaneously tackle agent heterogeneity under constraints of partial observability and reward sparsity (Zheng et al. 2020; Andres, Villar-Rodriguez, and Ser 2022). In the former work, heterogeneity is defined differently, referring to a mixture of on-policy, off-policy, and Evolutionary Algorithm (EA) agents and not the diverse physical and behavioral traits of agents. Moreover, their adoption of a local-global memory replay prevents them from undergoing training in a fully decentralized manner. The latter leverages intrinsic motivation in order to tackle reward sparsity similar to our work. They also address collaboration among heterogeneous agents. However, their utilization of a centralized"}, {"title": "Background", "content": "critic that merges all agent parameters into a single network constrains its applicability in the DTDE paradigm. Despite its significance, the lack of solutions in decentralized training settings limits the deployment of MARL agents in practical applications. CoHet addresses this notable research gap in the area of cooperative heterogeneous MARL, by fostering cooperation in a decentralized manner, under the real-world challenges of partial observability and reward sparsity.\nIn this section, we discuss the Markov games framework, which we use to formulate our problem, and the message-passing Graph Neural Network, which is used for differentiable inter-agent communication of local observations and predictions among agents.\nThe Markov games framework (Littman 1994) is a generalization of a Markov decision process to the case of multiple agents with cooperating or competing goals. The Partially Observable Markov Games (POMG) framework is used under conditions of partial observability. It is defined using the tuple\n$(V, S, O, A, \\{o_i\\}_{i \\in V}, \\{R_i\\}_{i \\in V}, T, \\gamma)$\nwhere,\n\\begin{itemize}\n    \\item $V = \\{1, 2, ..., N\\}$ is the set of all $N$ agents\n    \\item $S$ is the entire state space shared among all the agents\n    \\item $O = O_1 \\times O_2 \\times ... \\times O_n$ is the observation space of all the agents, where $O_i \\subset S$ and $\\forall i \\in V$\n    \\item $A = A_1 \\times A_2 \\times ... \\times A_n$ is the action space of all the agents\n    \\item $O_i \\in O_i$ is the observation instance of agent $i \\in V$, which is a partial view of the state space $S$\n    \\item $R_i: S \\times A \\times S \\rightarrow \\mathbb{R}$ is the reward function for agent $i$, where $i \\in V$\n    \\item $T: S \\times A \\times S \\rightarrow [0,1]$ is the stochastic state transition model, where each agent transitions to the next state, given the current state and the action taken\n    \\item $\\gamma$ is the discount factor such that $0 \\leq \\gamma \\leq 1$\n\\end{itemize}\nAdditionally, the reward that agent $i$ receives at time step $t$ can be denoted using $r_t = R_i(s_t, a_t, s_{t+1})$, where $a_t = (a_1, a_2,..., a_n)$ is the joint action taken by all the agents at time step t. The goal of each agent is to maximize the total expected discounted return $R_i = \\sum_{t=0}^T \\gamma^t r_t$ over the course of an episode with horizon $T$.\nThe message-passing technique (Kipf and Welling 2017) used by GNNs to transfer information from one node to another has proven to be an effective learning framework for understanding the patterns in a graph, the neighborhood of nodes, and the sub-graphs in large graphs (Zhou et al. 2021). In our problem formulation, given a graph $G = (\\nu, \\varepsilon)$,\n\\begin{itemize}\n    \\item $\\nu$ is the set of vertices we use to represent the agents\n    \\item $\\varepsilon$ is the set of edges discovered by inserting an edge $(i, j)$ from $i$ to $j$, if agent $j$ is within the observation radius of agent $i$\n    \\item $x_i$ represents the agent (i.e., node) attributes for each agent $i \\in V$. In our setting, this includes the non-absolute features of agent observations, which are found by removing the absolute features such as agent position, velocity, etc., from the agent observation $o_i$.\n    \\item $e_{ij}$ are the edge attributes for each edge $(i, j) \\in \\varepsilon$. The absolute position and the velocity of each agent are used to calculate the relative position and velocity. These are then concatenated to be used as the edge features.\n\\end{itemize}\nThe use of only the non-absolute observation features as agent embeddings allows the outputs of the message-passing GNN kernel to be invariant to geometric translations, thereby enhancing generalization. For the message-passing, at each time step, the agent embeddings and edge features are first computed and then utilized in the message-passing GNN kernel to learn the local sub-graph. Information is iteratively passed between adjacent nodes along the edges of the graph structure. In order to incorporate the Co-Het architecture on top of the underlying GNN formulation of HetGPPO, the inputs to our message-passing GNN kernel consist of the agent embedding $z_i = W_{\\Theta_1}(x_i)$ and edge attributes $e_{ij} = p_{ij} || v_{ij}$. Here, we represents a Multi-Layer Perceptron (MLP) encoder with parameters $\\Theta_1$. Using local information from all neighbors $j \\in N_i$, the GNN model output $h_i$ for agent $i$ is calculated in Equation 1.\n$h_i = \\psi_{\\Theta_2}(z_i) + \\bigoplus_{j \\in N_i} \\phi_{\\Theta_3}(z_j||e_{ij}) \\quad (1)$\nIn Equation 1, $\\psi_{\\Theta_2}$ and $\\phi_{\\Theta_3}$, are two MLPs parameterized by $\\Theta_i$ and the aggregation operator $\\bigoplus$ sums the $e_{ij}$ outputs for all the neighbors of agent $i$. Finally, two distinct MLP decoders take the GNN output $h_i$ and produce the value $V_i(O_N)$ and the action $a_i$, distributed according to $A_i \\sim \\pi_i(\\cdot | O_N)$ This formulation of GNN, allows us to utilize it for both intrinsic reward calculation and heterogeneous policy learning of HetGPPO, based on local neighborhood information."}, {"title": "The CoHet Algorithm", "content": "In this section, we introduce CoHet, a decentralized algorithm designed to enhance cooperation among heterogeneous agents in partially observable environments with sparse rewards. It provides a standalone self-supervised intrinsic reward architecture that can be incorporated with existing decentralized policy optimization algorithms. It fosters the learning of collaborative behaviors by reducing future uncertainty within each agent's neighborhood. CoHet encourages the agents to align their actions with their neighbors' predictions by imposing intrinsic reward penalties that deter deviations from such alignment. Furthermore, these calculated rewards serve as a source of dense reward signals that facilitate policy learning in numerous real-world tasks where manually designing reward functions is infeasible.\nAlgorithm 1: CoHet Algorithm\n\\begin{algorithmic}[1]\n    \\State Initialize models $\\omega_i, \\psi_i, \\phi_i, \\Omega_i, \\Gamma_i, f_i$ with random values $\\Theta_i$, where $i \\in \\{1,2,..., N\\}$\n    \\For{$k = 1, 2, ...$} \\do\n        \\State Initialize set of trajectories for all agents, $D_k \\leftarrow \\{\\}$\n        \\Comment{ACTION & VALUE CALCULATION}\n        \\For{$t = 0, 1, ..., T$} \\do\n            \\For{$i = 1, 2, ..., N$} \\do\n                \\State $x_i \\leftarrow \\text{trim}_{v, p}(o^t_i)$\n                \\State $z_i \\leftarrow \\omega_{\\Theta_1} (x_i)$\n                \\State $h_i \\leftarrow \\psi_{\\Theta_2} (z_i)$\n                \\For{each $j \\in N_i$} \\do\n                    \\State $e_{ij} \\leftarrow p_{ij} || v_{ij}$\n                    \\State $h_i \\leftarrow h_i \\oplus \\phi_{\\Theta_3} (z_j||e_{ij})$\n                \\EndFor\n                \\State $a_i \\leftarrow \\Omega_{\\Theta_4} (h_i)$\n                \\State $V \\leftarrow \\Gamma_{\\Theta_5} (h_i)$\n                \\State $a^t \\leftarrow a^t_1||a^t_2||...||a^t_n$\n            \\EndFor\n        \\EndFor\n        \\Comment{INTRINSIC REWARD CALCULATION}\n        \\For{$t = 0, 1, ..., T$} \\do\n            \\For{$i \\in \\{1, 2, ..., N\\}$} \\do\n                \\State $r^{int}_i \\leftarrow 0$\n                \\For{each $j \\in N^{t+1}_i$ } \\do\n                \\State $w_j \\leftarrow \\frac{d(i,j)}{\\sum_{k \\in N^{t+1}_i} d(i,k)}$\n                \\State $r^{int}_i \\leftarrow r^{int}_i + w_j \\times -||o^{t+1}_i - f_{\\Theta_6j} (o_j, a_j) ||$\n                \\EndFor\n                \\State $r^{total}_i \\leftarrow r^{ext}_i + \\beta \\times r^{int}_i$\n            \\EndFor\n            \\State $r^{total}_t \\leftarrow r^{total}_1||r^{total}_2|| ... ||r^{total}_n$\n            \\State $D_k \\leftarrow D_k \\cup (o^t, a^t, r^{total}_t, o^{t+1})$\n        \\EndFor\n        \\State Use $D_k$ to for Multi-PPO policy optimization\n    \\EndFor\n\\end{algorithmic}\nThe CoHet algorithm utilizes an underlying communication graph $G = (V, E)$ for passing both the ground truth observation of $o^t_i$ agent $i$ at time $t$, and its predicted next observation set $\\{ \\hat{o}^{t+1}_j | j \\in N^{t+1}_i\\}$ to all its local neighbors $N^{t+1}_i$ at the next time step. As previously mentioned, Co-Het's standalone reward calculation architecture presented in Figure 1a can be used alongside existing decentralized multi-agent policy optimization algorithms. However, integrating CoHet's decentralized heterogeneous intrinsic motivation architecture with established policy learning frameworks like HetGPPO, which accommodates policy heterogeneity in MARL scenarios, can be advantageous for deploying agents with varied physical or behavioral characteristics, such as varying sizes, speeds, action spaces, etc. Hence, in Figure 1b, we demonstrate the incorporation of the HetGPPO policy optimization architecture. In Algorithm 1, we start by initializing the model for each agent which includes the encoder $\\omega_i$, two multi-layer perceptrons (MLPs) $\\psi_i$ and $\\phi_i$, $\\pi$-decoder $\\Omega_i$, value decoder $\\Gamma_i$, and dynamics model $f_i$. At each training iteration, we begin by collecting the observations $o^t_i$. We trim the position $p$ and velocity $v$ from the observation to obtain the non-absolute features $x$ of each agent observation in Line 6, and use that to calculate the node embedding $z$ in Line 7. The position $p$ and velocity $v$ from the observations are used as edge embeddings in Line 10, this allows for GNN outputs to be invariant to geometric translations. Neighboring agent embeddings $\\{ z_j | j \\in N_i\\}$ are obtained through the GNNs underlying differentiable communication channel. These neighborhood embeddings, along with edge features are aggregated to calculate the GNN model output in Line 11, which is then decoded to produce action and value outputs in Line 13 and Line 14. The joint action $a_t$, of all the agents, is formed by concatenating individual agent actions $a^t_i$ at time $t$, in Line 16. Individual agent actions $a^t_i$, are later used for intrinsic reward calculation, and the concatenated actions of all agents $a_t$ are used for policy optimization.\nThe procedure for policy optimization as discussed above is visualized in Figure 1b, where the reward assigned to the agents at each time step is augmented with the dense intrinsic reward calculated via the process outlined in Figure 1a. Prior intrinsic motivation-based solutions have been shown to alleviate the effects of reward sparsity (as discussed in the 'Related Works' section). This augmentation of dense intrinsic reward signals allows the agents to learn from the interactions that do not result in any extrinsic/environmental rewards. In the CoHet algorithm, the dynamics model $f_{\\Theta_6}(o, a)$ of each agent $i$, are continually trained (Line 20 to Line 23) using the agents own experiences in the environment. As depicted in Figure 2, this is done in order to be used for intrinsic reward calculation. Agents achieve familiarity with environmental dynamics through continuous training in the dynamics model, which they use to predict the next observation for their neighbors. During the training of the dynamics model, the observation $o$ and the action $a$ are passed as input and the model outputs the prediction of the next observation $\\hat{o}_i$, where $i$ refers to the predicted next observation by agent $i$ for itself. We employ a three-layer MLP with ReLU non-linearities as the dynamics model and train them to minimize the mean squared error (MSE) between its prediction and the ground truth next observation $o^{t+1}_i$.\n$\\begin{aligned}\nd(i, j) &= (||p_i - p_j ||)^{-1} \\\\\nw_j &= \\frac{d(i, j)}{\\sum_{k \\in N^{t+1}_i} d(i, k)} \\\\\nr_{i}^{j,i}(o, a) &= f_{\\Theta_6i} (o_i, a_i) \\\\\nr^{int}_i &= \\sum_{j \\in N^{t+1}_i} w_j \\times ||o^{t+1}_i - \\hat{o}_i ||\n\\end{aligned}$\nIn addition, instead of giving equal weight to each neighbor's prediction in the calculation of the intrinsic reward, we utilize a Euclidean distance-based weighting to prioritize the predictions of the agents in close proximity. To achieve this, we calculate the inverse of the Euclidian distance between agent $i$ and agent $j$, obtained from their positions $p_i$ and $p_j$ in Equation 2 and use it to calculate the weights for each neighbor in Equation 3. Note that, the agent dynamics model predictions for the neighbors are communicated in the next"}, {"title": "Experiments", "content": "MARL tasks. We additionally compare with the state-of-the-art MARL baseline, IPPO (Independent Proximal Policy Optimization), which is applicable in decentralized training settings for heterogeneous agents under partial observability similar to HetGPPO. Unlike the two centralized critic-based heterogeneous MARL approaches discussed in the 'Related Works' section or widely used algorithms such as MADDPG (Lowe et al. 2020), MAPPO (Yu et al. 2022), COMA (Foerster et al. 2017), etc., these baselines along with CoHet address the more challenging problem of not relying on any centralized controller or prior knowledge of agent heterogeneity, but rather optimizing policies based on only the locally observable partial information available to these heterogeneous agents. Each of our evaluated tasks involves agents trained in a fully decentralized manner following the principles of the DTDE paradigm, acting under partial observability and reward sparsity. Furthermore, we analyze how each agent learns the dynamics model as time progresses and how it results in the reduction of the intrinsic reward penalty for misalignment. We compare the two variants of CoHet (CoHetteam and CoHetself) and how they perform. Furthermore, we demonstrate that the CoHet algorithm is robust to an increasing number of heterogeneous agents in the shared environment, an issue previously encountered in the intrinsic motivation-based methods (Ma et al. 2022).\nFor our experiments, we choose the MPE and VMAS settings that are extensively used for the performance evaluation of MARL agents in a variety of cooperative tasks (Lowe et al. 2020; Bettini, Shankar, and Prorok 2023b; Bou et al. 2023). In these scenarios, agents with different physical or behavioral traits have to cooperate to achieve a shared goal. The specifics of the environments are described below\n\\begin{itemize}\n    \\item VMAS Flocking: N agents flock around a single landmark, with obstacles spawned randomly in the environment. These agents with differing properties are rewarded for increasing their velocity and decreasing the inter-agent distance and are penalized for collisions.\n    \\item MPE Simple Spread: N agents aim to reach N different landmarks, receiving rewards if they can occupy the landmarks. Collisions incur penalties. Agents can occupy any landmark.\n    \\item VMAS Reverse Transport: N agents collaboratively push a heavy package within a rectangular enclosure toward a randomly positioned goal, with each agent receiving rewards if they can reach the goal.\n    \\item VMAS Joint Passage: Two agents are connected by an asymmetric mass linkage, start on one side of a wall. The goal is on the other side, which requires going through a narrow passage by aligning themselves perpendicular to the wall.\n    \\item VMAS Navigation: Similar to MPE Simple Spread, but here, landmarks are specifically assigned to each agent's color. To maximize rewards, each of the N agents must reach the landmark matching their color.\n    \\item VMAS Sampling: N agents start randomly in a grid environment with one-time rewards in the cells. Agents\n\\end{itemize}\nequipped with LIDARs with distinct ranges have partial observability.\nFor the empirical analysis of the CoHet algorithm, we use HetGPPO as our baseline as it is the only decentralized heterogeneous policy learning algorithm that does not rely on any prior knowledge of agent heterogeneity and applies to all heterogeneity classes. CoHet outperforms HetGPPO in each of the six cooperative scenarios involving agents with heterogeneous traits (e.g. size, speed, observation radius). Moreover, both the CoHetteam and CoHetself variants outperform the baseline in each of these cases. These results indicate that incorporating the GNN-based intrinsic motivation of CoHet enables the agents to surpass the baseline performance. In case of the two different variants of the algorithm, CoHetself only outperforms the rest in the MPE Simple Spread scenario, as shown in Figure 3e. On the other hand, CoHetteam outperforms in the rest of the scenarios VMAS Navigation, VMAS Reverse Transport, VMAS Sampling, VMAS Flocking, and VMAS Joint Passage, as depicted by the Figures 3a, 3b, 3c, 3d, and 3f, respectively. Differing convergence properties of the agent dynamics models required a different number of training steps in each of these environments. All the environments in Figure 3 were trained with a train batch size of 60000, in 4 random environment initializations, and for 1000 episodes where each episode lasted for a maximum of 200 environmental steps on a machine with an NVIDIA GeForce RTX 3090 GPU. For reproducibility of all our research, we include the details about the model hyper-parameters, and environment parameters in the supplementary materials and we also share the codebase.\nWe hypothesize that the enhanced performance of CoHet over HetGPPO in the multi-agent cooperative tasks of Figure 3 arises due to the effect of the intrinsic rewards acting as a penalty for misalignment with neighborhood predictions, thereby encouraging the agents to behave in a manner that reduces future uncertainty for their neighborhood. Moreover, the dense intrinsic rewards that foster collaborative actions in local neighborhoods also guide the agents during interactions with infrequent/sparse environmental feedback. Whereas CoHet allows agents to perform coordinated exploration even under reward sparsity, HetGPPO suffers when environmental (i.e. extrinsic) rewards are infrequent. We further evaluate our method against another state-of-the-art, Independent Proximal Policy Optimization (IPPO), where"}, {"title": "Conclusions and Future Work", "content": "each agent acts independently in the shared environment and optimizes their policies using PPO. IPPO has been demonstrated to outperform many fully observable critic models on several MARL benchmarks (de Witt et al. 2020). However, a drawback of using IPPO is that the other agents are considered as environmental components and are not explicitly represented in the IPPO critic. Conditioning the critic on local agent observations rather than the entire state also leads to non-stationarity during the training (Bettini, Shankar, and Prorok 2023a). Despite potential difficulties, it has the positive aspect of not requiring prior knowledge of agent heterogeneity or global information during training, allowing it to be a suitable baseline for our evaluation. In Table 1, we show that in 4 out of 6 of the cooperative scenarios (Simple Spread, Joint Passage, Sampling, and Flocking), where coordinated actions among agents are necessitated on top of independent exploration, CoHet variants outperform IPPO by a large margin. Furthermore, the results show that, on average, CoHet outperforms HetGPPO by a factor of approximately 3.19.\nCoHetteam vs. CoHetself: A Comparative Analysis\nCoHetteam utilizes the dynamics models of its surrounding agents to calculate intrinsic reward, whereas CoHetself employs the agent's own dynamics model and learns to follow its own predictions. As a result, the agents in CoHetteam learn to behave in accordance with their neighborhood predictions, and this can be challenging. As the agent dynamics models are trained using their own interactions with the environment, the presence of heterogeneous agents with differing physical/behavioral capabilities can result in a diverse set of dynamics models. Subsequently, the agents failing to align with the predictions of their local neighborhood, consisting of diverse agents, will be heavily penalized. Results indicate that, unlike previous methods, the novel GNN-based formulation in CoHet allows the agents to more accurately model their local heterogeneous neighborhood. In Table 2, we demonstrate that the intrinsic reward penalty for misalignment with neighbors in CoHetteam remains similar to the ones for CoHetself. In case of the performance, CoHetteam demonstrates better performance in all of the tasks that benefit from inter-agent cooperation, as it incentivizes the agents to adopt a more collaborative approach, irrespective of physical or behavioral heterogeneity. Interestingly, CoHetself exhibits superior performance over CoHetteam in only the MPE Simple Spread task. Since the landmarks are not specifically assigned to specific agents in this task, we find that the agents can gain an advantage from exploiting known areas of the environment where any of the landmarks exist, thus they are encouraged to exploit the parts of the environments where the errors of their own dynamics, models are minimal.\nAs the dynamics models for each agent continue to be trained, we anticipate that the agents will gain a better understanding of the environmental dynamics, leading to a decrease in the dynamics model Mean Squared Error (MSE) loss over time. Considering that each agent operates with their independent dynamics models, we should see a decline in the dynamics model loss for each. From our experiments, we observe this aforementioned gradual decline in the mean dynamics model loss for each agent, as depicted in Figure 4a for the MPE Joint Passage task. This decline is also observed in each task of Figure 3. As evident in Figure 4a, the agents effectively learn the environmental dynamics within 600 training episodes, and the dynamics model loss remains minimal unless an unknown aspect of the environmental dynamics is encountered. As a result of a better understanding of the environmental dynamics, each agent is more capable of predicting its neighbors' next observations. Furthermore, the agents gradually adapt to meet their neighbors' predictions. Therefore, the intrinsic reward, which in the case of CoHet is a penalty for misalignment, transitions from large negative values to extremely small negative values over time. This trend is demonstrated in Figure 4b for the same task. Interestingly, as the intrinsic reward is calculated using the dynamics model loss, we can see that it exhibits the opposite trend to that of the loss.\nAs discussed previously, CoHetteam motivates the agents to exhibit behaviors that match their neighborhood predictions. However, the diversity of heterogeneity types in the agent neighborhood can pose a challenge, potentially leading to a performance decline as the number of heterogeneous agents in the shared environment increases. We find that CoHetteam maintains its robustness despite the growth in the number of heterogeneous agents in shared environments, indicated by the Mean Episodic Rewards in Table 3 after 1000 environmental episodes. The mean episodic rewards remain consistent with the rewards for the single-agent case, indicating the robustness of the proposed architecture to an increasing number of heterogeneous agents.\nIn this paper, we propose the CoHet algorithm, a novel intrinsic reward calculation mechanism that utilizes a Graph Neural Network (GNN) and is suitable for decentralized heterogeneous MARL policy learning algorithms. It can be employed as a standalone module alongside existing decentralized policy optimization methods. Our proposed reward calculation architecture is applicable to practical multi-agent systems that operate under the constraints of partial observability and reward sparsity. The algorithm is validated in a series of cooperative multi-agent tasks that involve heterogeneous agents. We augment our research with an exploration of the effects of the agent dynamics model on the intrinsic reward module, compare the two variants of the CoHet algorithm and demonstrate that CoHetteam is robust against an increase in heterogeneous agents in the environment.\nA promising direction for future work includes exploring the alternative types of intrinsic motivation such as curiosity-driven or novelty-based rewards in the context of decentralized heterogeneous learning frameworks. Finding the proper balance between intrinsic and extrinsic rewards remains an open research problem. Future works can explore utilizing different weighting mechanisms that prioritize the predictions of the agents with the same sub-goals, and heterogeneity types. Ultimately, we opine that the collaboration between the agents in a MARL setting should take the need for decentralized training and agent heterogeneity into account."}]}