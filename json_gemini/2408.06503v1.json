{"title": "Decentralized Cooperation in Heterogeneous Multi-Agent Reinforcement Learning via Graph Neural Network-Based Intrinsic Motivation", "authors": ["Jahir Sadik Monon", "Deeparghya Dutta Barua", "Md. Mosaddek Khan"], "abstract": "Multi-agent Reinforcement Learning (MARL) is emerging as a key framework for various sequential decision-making and control tasks. Unlike their single-agent counterparts, multi-agent systems necessitate successful cooperation among the agents. The deployment of these systems in real-world scenarios often requires decentralized training, a diverse set of agents, and learning from infrequent environmental reward signals. These challenges become more pronounced under partial observability and the lack of prior knowledge about agent heterogeneity. While notable studies use intrinsic motivation (IM) to address reward sparsity or cooperation in decentralized settings, those dealing with heterogeneity typically assume centralized training, parameter sharing, and agent indexing. To overcome these limitations, we propose the CoHet algorithm, which utilizes a novel Graph Neural Network (GNN) based intrinsic motivation to facilitate the learning of heterogeneous agent policies in decentralized settings, under the challenges of partial observability and reward sparsity. Evaluation of CoHet in the Multi-agent Particle Environment (MPE) and Vectorized Multi-Agent Simulator (VMAS) benchmarks demonstrates superior performance compared to the state-of-the-art in a range of cooperative multi-agent scenarios. Our research is supplemented by an analysis of the impact of the agent dynamics model on the intrinsic motivation module, insights into the performance of different CoHet variants, and its robustness to an increasing number of heterogeneous agents.", "sections": [{"title": "Introduction", "content": "The paradigm of Multi-agent Reinforcement Learning (MARL) is rapidly emerging to be pivotal in a broad spectrum of practical applications such as resource management (Ying and Dayong 2005), autonomous vehicles (Cao et al. 2013), traffic signal control (Calvo and Dusparic 2018a), supply chain management (Fuji et al. 2018), robotics (Lillicrap et al. 2019), robot swarms (H\u00fcttenrauch, \u0160o\u0161i\u0107, and Neumann 2017), etc. These applications generally benefit from the efficient use of the diverse capabilities of heterogeneous agents. Moreover, the successful execution of tasks in these multi-agent systems requires the agents to adapt their behaviors to other agents for effective coordination rather than operating independently. The real-world deployment of these MARL systems typically involves the agents relying solely on the local environmental information and learning policies with infrequent environmental rewards (Wiewiora 2010; Wakilpoor et al. 2020).\nApplications such as package transport (Gerkey and Mataric 2002), traffic lights control (Calvo and Dusparic 2018b), disaster response (Michael et al. 2012), agriculture (Ju and Son 2019), etc. utilize agent heterogeneity such as distinct physical and behavioral traits of agents. Heterogeneity is also vital in multi-robot tasks as it enables efficient characterization and discovery of diverse behaviors, improving learning performance (Liu et al. 2022). On the other hand, the dependency on reward signals for the agent's learning process introduces the issue of reward sparsity (Hare 2019). Due to the lack of frequent feedback signals from the environment and the non-trivial nature of manually designing reward functions, MARL systems need to be robust enough to deal with infrequent environmental rewards.\nFurthermore, in addition to these challenges, the majority of real-world applications constrain the agents to act in a decentralized manner, and under partial observability, where each agent has a partial view of the shared environment. As a result, it is impractical for them to learn cooperative behaviors by utilizing a centralized algorithm that possesses global knowledge of all the agents and the state space (Iqbal and Sha 2019; Liu, Yeh, and Schwing 2019). In comparison to centralized training and full observability, the challenges of agent heterogeneity and reward sparsity are more severe in decentralized training under partial observability (Ma et al. 2022). Despite the aforementioned requirements for real-world deployments, most of the existing solutions to date either utilize global parameter sharing among all the agents or employ a centralized critic with global knowledge.\nTo the best of our knowledge, no prior research has addressed the issue of cooperative heterogeneous MARL in decentralized training settings under the practical constraints of real-world applications, such as partial observability and reward sparsity (see Section for more details). We propose CoHet, an algorithm that facilitates heterogeneous agent cooperation addressing the constraints required for real-world deployments. CoHet does not require any prior knowledge of agent heterogeneity (e.g speed, size, type, agent index). It employs an architecture for learning heterogeneous MARL"}, {"title": "Related Works", "content": "In response to the challenges posed by agent heterogeneity and the sparsity of rewards, most existing literature addresses one or the other. A prevalent method for emulating heterogeneous behavior involves augmenting each agent's observation space with a unique index that represents the agent's type (Foerster et al. 2016; Gupta, Egorov, and Kochenderfer 2017). These methods necessitate prior knowledge of the types of agent heterogeneity. Most of the literature on heterogeneous MARL focuses on tackling the problem of behavioral heterogeneity for physically identical agents (Wang et al. 2020a,b; Li et al. 2021; Lowe et al. 2020). They cannot be applied to all types of heterogeneity as they suggest solutions that are specific to problems within specific sub-classes of heterogeneity. Moreover, their use in partially observable systems is limited due to the lack of inter-agent communication. A notable study on heterogeneous MARL (Bettini, Shankar, and Prorok 2023a), proposes an algorithm termed HetGPPO (Heterogeneous GNN-based Proximal Policy Optimization), that is capable of learning heterogeneous decentralized policies in partially observable scenarios. It uses a GNN-based communication layer for sharing information among agents within local neighborhoods, thereby mitigating the effects of partial observability. Unlike previous methods, their reliance on only the local information enables Decentralized Training with Decentralized Execution (DTDE). Similar to their work, CoHet utilizes only the local neighborhood information under the constraints of DTDE.\nAlthough the aforementioned studies address the problem of agent heterogeneity, typically involving centralized training settings, parameter sharing or prior knowledge of agent heterogeneity, they do not consider the issues with reward sparsity. For tackling the sparse-reward problem in MARL, learning with the efficient exploration of the state space is a widely adopted approach (Liu et al. 2021; Mahajan et al. 2020; Christianos, Sch\u00e4fer, and Albrecht 2021). Dense scalar reward signals termed \"Intrinsic Motivation\", are often used to encourage exploration or coordination among agents (Du et al. 2019; Wu et al. 2021). Studies with a primary focus on cooperative tasks rely on the ability of agents to represent the dynamics of other agents in a shared environment to calculate these intrinsic rewards (Jaques et al. 2019; Stadie, Levine, and Abbeel 2015; Jeon et al. 2022). In case of the algorithm proposed by Ma et al. (2022), termed ELIGN (Expectation Alignment as a Multi-Agent Intrinsic Reward), the intrinsic rewards foster inter-agent coordination in a decentralized manner. However, a significant drawback of using an agent's own dynamics model as a proxy to calculate neighborhood predictions in ELIGN is that it becomes more challenging for agents to accurately model the dynamics of other agents in the presence of agent heterogeneity. Inaccuracies in the dynamics model can result in misleading alignment signals, and as a result, ELIGN scales poorly in the presence of heterogeneous agents. In contrast, CoHet utilizes the local neighborhood information passed via the underlying GNN-communication graph, to more accurately model the heterogeneity among agents.\nThere are only a couple of existing research works that simultaneously tackle agent heterogeneity under constraints of partial observability and reward sparsity (Zheng et al. 2020; Andres, Villar-Rodriguez, and Ser 2022). In the former work, heterogeneity is defined differently, referring to a mixture of on-policy, off-policy, and Evolutionary Algorithm (EA) agents and not the diverse physical and behavioral traits of agents. Moreover, their adoption of a local-global memory replay prevents them from undergoing training in a fully decentralized manner. The latter leverages intrinsic motivation in order to tackle reward sparsity similar to our work. They also address collaboration among heterogeneous agents. However, their utilization of a centralized"}, {"title": "Background", "content": "In this section, we discuss the Markov games framework, which we use to formulate our problem, and the message-passing Graph Neural Network, which is used for differentiable inter-agent communication of local observations and predictions among agents."}, {"title": "Markov Games", "content": "The Markov games framework (Littman 1994) is a generalization of a Markov decision process to the case of multiple agents with cooperating or competing goals. The Partially Observable Markov Games (POMG) framework is used under conditions of partial observability. It is defined using the tuple\n(V, S, O, A, {oi}i\u2208V, {Ri}i\u2208V, T, \u03b3)\nwhere,\n\u2022 V = {1, 2, . . ., N} is the set of all N agents\n\u2022 S is the entire state space shared among all the agents\n\u2022 O = O1 \u00d7 O2 \u00d7 . . . \u00d7 ON is the observation space of all the agents, where Oi \u2286 S and \u2200i \u2208 V\n\u2022 A = A1 \u00d7 A2 \u00d7 ... \u00d7 AN is the action space of all the agents\n\u2022 Oi \u2208 Oi is the observation instance of agent i \u2208 V, which is a partial view of the state space S\n\u2022 Ri : S \u00d7 A \u00d7 S \u2192 R is the reward function for agent i, where i \u2208 V\n\u2022 T : S \u00d7 A \u00d7 S \u2192 [0,1] is the stochastic state transition model, where each agent transitions to the next state, given the current state and the action taken\n\u2022 \u03b3 is the discount factor such that 0 \u2264 \u03b3 \u2264 1\nAdditionally, the reward that agent i receives at time step t can be denoted using ri = Ri(st, at, st+1), where at = (a1, a2,..., aN) is the joint action taken by all the agents at time step t. The goal of each agent is to maximize the total expected discounted return Ri =  \u2211t=0T \u03b3t rt over the course of an episode with horizon T."}, {"title": "Message-passing Graph Neural Network", "content": "The message-passing technique (Kipf and Welling 2017) used by GNNs to transfer information from one node to another has proven to be an effective learning framework for understanding the patterns in a graph, the neighborhood of nodes, and the sub-graphs in large graphs (Zhou et al. 2021). In our problem formulation, given a graph G = (V, E), \n\u2022 V is the set of vertices we use to represent the agents\n\u2022 E is the set of edges discovered by inserting an edge (i, j) from i to j, if agent j is within the observation radius of agent i\n\u2022 xi represents the agent (i.e., node) attributes for each agent i \u2208 V. In our setting, this includes the non-absolute features of agent observations, which are found by removing the absolute features such as agent position, velocity, etc., from the agent observation oi.\n\u2022 eij are the edge attributes for each edge (i, j) \u2208 E. The absolute position and the velocity of each agent are used to calculate the relative position and velocity. These are then concatenated to be used as the edge features.\nThe use of only the non-absolute observation features as agent embeddings allows the outputs of the message-passing GNN kernel to be invariant to geometric translations, thereby enhancing generalization. For the message-passing, at each time step, the agent embeddings and edge features are first computed and then utilized in the message-passing GNN kernel to learn the local sub-graph. Information is iteratively passed between adjacent nodes along the edges of the graph structure. In order to incorporate the CoHet architecture on top of the underlying GNN formulation of HetGPPO, the inputs to our message-passing GNN kernel consist of the agent embedding zi = Wo\u2081 (xi) and edge attributes eij = Pi,j || Vi,j . Here, we represents a Multi-Layer Perceptron (MLP) encoder with parameters \u03b8i. Using local information from all neighbors j \u2208 Ni, the GNN model output hi for agent i is calculated in Equation 1.\nhi = Voz (zi) + \u2295jen \u03c6\u03b8 (zj||eij)\n(1)\nIn Equation 1, \u03c8e, and \u0444\u04e9, are two MLPs parameterized by \u03b8i and the aggregation operator \u2295 sums the e outputs for all the neighbors of agent i. Finally, two distinct MLP decoders take the GNN output hi and produce the value Vi(ON) and the action ai, distributed according to ai ~ \u03c0i(\u00b7 | ON) This formulation of GNN, allows us to utilize it for both intrinsic reward calculation and heterogeneous policy learning of HetGPPO, based on local neighborhood information."}, {"title": "The CoHet Algorithm", "content": "In this section, we introduce CoHet, a decentralized algorithm designed to enhance cooperation among heterogeneous agents in partially observable environments with sparse rewards. It provides a standalone self-supervised intrinsic reward architecture that can be incorporated with existing decentralized policy optimization algorithms. It fosters the learning of collaborative behaviors by reducing future uncertainty within each agent's neighborhood. CoHet encourages the agents to align their actions with their neighbors' predictions by imposing intrinsic reward penalties that deter deviations from such alignment. Furthermore, these calculated rewards serve as a source of dense reward signals that facilitate policy learning in numerous real-world tasks where manually designing reward functions is infeasible."}, {"title": "Algorithm Description", "content": "The CoHet algorithm utilizes an underlying communication graph G = (V, E) for passing both the ground truth observation of of agent i at time t, and its predicted next observation set {\u00f4i | j \u2208 N+1} to all its local neighbors Nt+1 at the next time step. As previously mentioned, CoHet's standalone reward calculation architecture presented in Figure 1a can be used alongside existing decentralized multi-agent policy optimization algorithms. However, integrating CoHet's decentralized heterogeneous intrinsic motivation architecture with established policy learning frameworks like HetGPPO, which accommodates policy heterogeneity in MARL scenarios, can be advantageous for deploying agents with varied physical or behavioral characteristics, such as varying sizes, speeds, action spaces, etc. Hence, in Figure 1b, we demonstrate the incorporation of the HetGPPO policy optimization architecture. In Algorithm 1, we start by initializing the model for each agent which includes the encoder wi, two multi-layer perceptrons (MLPs) \u03c8i and \u03c6\u03af, \u03c0-decoder \u03a9\u2081, value decoder \u0393\u017c, and dynamics model fi. At each training iteration, we begin by collecting the observations of. We trim the position p and velocity v from the observation to obtain the non-absolute features x of each agent observation in Line 6, and use that to calculate the node embedding z in Line 7. The position pand velocity v from the observations are used as edge embeddings in Line 10, this allows for GNN outputs to be invariant to geometric translations. Neighboring agent embeddings {z} | j \u2208 N} are obtained through the GNNs underlying differentiable communication channel. These neighborhood embeddings, along with edge features are aggregated to calculate the GNN model output in Line 11, which is then decoded to produce action and value outputs in Line 13 and Line 14. The joint action at, of all the agents, is formed by concatenating individual agent actions at time t, in Line 16. Individual agent actions at, are later used for intrinsic reward calculation, and the concatenated actions of all agents at are used for policy optimization.\nThe procedure for policy optimization as discussed above is visualized in Figure 1b, where the reward assigned to the agents at each time step is augmented with the dense intrinsic reward calculated via the process outlined in Figure 1a. Prior intrinsic motivation-based solutions have been shown to alleviate the effects of reward sparsity (as discussed in the 'Related Works' section). This augmentation of dense intrinsic reward signals allows the agents to learn from the interactions that do not result in any extrinsic/environmental rewards. In the CoHet algorithm, the dynamics model for (o, a) of each agent i, are continually trained (Line 20 to Line 23) using the agents own experiences in the environment. As depicted in Figure 2, this is done in order to be used for intrinsic reward calculation. Agents achieve familiarity with environmental dynamics through continuous training in the dynamics model, which they use to predict the next observation for their neighbors. During the training of the dynamics model, the observation of, and the action at are passed as input and the model outputs the prediction of the next observation \u00f4i, where i refers to the predicted next observation by agent i for itself. We employ a three-layer MLP with ReLU non-linearities as the dynamics model and train them to minimize the mean squared error (MSE) between its prediction and the ground truth next observation ot+1.\nd(i, j) = (||Pi - Pj ||) \u22121\n(2)\nWj =\nd(i, j)\n\u2211k\u2208N+1 d(i, k)\n(3)\nrint (o, a) =\nj,i\n\u2211 Wjx ||0+1 - 0||\n(4)\n\u2208N\u2229N+1\n(5)\nIn addition, instead of giving equal weight to each neighbor's prediction in the calculation of the intrinsic reward, we utilize a Euclidean distance-based weighting to prioritize the predictions of the agents in close proximity. To achieve this, we calculate the inverse of the Euclidian distance between agent i and agent j, obtained from their positions pi and pj in Equation 2 and use it to calculate the weights for each neighbor in Equation 3. Note that, the agent dynamics model predictions for the neighbors are communicated in the next"}, {"title": "Experiments", "content": "In this section, we demonstrate that both variants of CoHet (CoHetself, CoHetteam) outperform the state-of-the-art decentralized heterogeneous MARL policy learning algorithm HetGPPO in each of the tasks evaluated on widely used VMAS and MPE benchmarks. The incorporation of the CoHet architecture leads to the learning of collaborative behaviors among heterogeneous agents, as evidenced by the improved performance over the baseline in these cooperative MARL tasks. We additionally compare with the state-of-the-art MARL baseline, IPPO (Independent Proximal Policy Optimization), which is applicable in decentralized training settings for heterogeneous agents under partial observability similar to HetGPPO. Unlike the two centralized critic-based heterogeneous MARL approaches discussed in the 'Related Works' section or widely used algorithms such as MADDPG (Lowe et al. 2020), MAPPO (Yu et al. 2022), COMA (Foerster et al. 2017), etc., these baselines along with CoHet address the more challenging problem of not relying on any centralized controller or prior knowledge of agent heterogeneity, but rather optimizing policies based on only the locally observable partial information available to these heterogeneous agents. Each of our evaluated tasks involves agents trained in a fully decentralized manner following the principles of the DTDE paradigm, acting under partial observability and reward sparsity. Furthermore, we analyze how each agent learns the dynamics model as time progresses and how it results in the reduction of the intrinsic reward penalty for misalignment. We compare the two variants of CoHet (CoHetteam and CoHetself) and how they perform. Furthermore, we demonstrate that the CoHet algorithm is robust to an increasing number of heterogeneous agents in the shared environment, an issue previously encountered in the intrinsic motivation-based methods (Ma et al. 2022).\nFor our experiments, we choose the MPE and VMAS settings that are extensively used for the performance evaluation of MARL agents in a variety of cooperative tasks (Lowe et al. 2020; Bettini, Shankar, and Prorok 2023b; Bou et al. 2023). In these scenarios, agents with different physical or behavioral traits have to cooperate to achieve a shared goal. The specifics of the environments are described below\n\u2022 VMAS Flocking: N agents flock around a single landmark, with obstacles spawned randomly in the environment. These agents with differing properties are rewarded for increasing their velocity and decreasing the inter-agent distance and are penalized for collisions.\n\u2022 MPE Simple Spread: N agents aim to reach N different landmarks, receiving rewards if they can occupy the landmarks. Collisions incur penalties. Agents can occupy any landmark.\n\u2022 VMAS Reverse Transport: N agents collaboratively push a heavy package within a rectangular enclosure toward a randomly positioned goal, with each agent receiving rewards if they can reach the goal.\n\u2022 VMAS Joint Passage: Two agents are connected by an asymmetric mass linkage, start on one side of a wall. The goal is on the other side, which requires going through a narrow passage by aligning themselves perpendicular to the wall.\n\u2022 VMAS Navigation: Similar to MPE Simple Spread, but here, landmarks are specifically assigned to each agent's color. To maximize rewards, each of the N agents must reach the landmark matching their color.\n\u2022 VMAS Sampling: N agents start randomly in a grid environment with one-time rewards in the cells. Agents equipped with LIDARs with distinct ranges have partial observability."}, {"title": "Empirical Results", "content": "For the empirical analysis of the CoHet algorithm, we use HetGPPO as our baseline as it is the only decentralized heterogeneous policy learning algorithm that does not rely on any prior knowledge of agent heterogeneity and applies to all heterogeneity classes. CoHet outperforms HetGPPO in each of the six cooperative scenarios involving agents with heterogeneous traits (e.g. size, speed, observation radius). Moreover, both the CoHetteam and CoHetself variants outperform the baseline in each of these cases. These results indicate that incorporating the GNN-based intrinsic motivation of CoHet enables the agents to surpass the baseline performance. In case of the two different variants of the algorithm, CoHetself only outperforms the rest in the MPE Simple Spread scenario, as shown in Figure 3e. On the other hand, CoHetteam outperforms in the rest of the scenarios VMAS Navigation, VMAS Reverse Transport, VMAS Sampling, VMAS Flocking, and VMAS Joint Passage, as depicted by the Figures 3a, 3b, 3c, 3d, and 3f, respectively. Differing convergence properties of the agent dynamics models required a different number of training steps in each of these environments. All the environments in Figure 3 were trained with a train batch size of 60000, in 4 random environment initializations, and for 1000 episodes where each episode lasted for a maximum of 200 environmental steps on a machine with an NVIDIA GeForce RTX 3090 GPU. For reproducibility of all our research, we include the details about the model hyper-parameters, and environment parameters in the supplementary materials and we also share the codebase.\nWe hypothesize that the enhanced performance of CoHet over HetGPPO in the multi-agent cooperative tasks of Figure 3 arises due to the effect of the intrinsic rewards acting as a penalty for misalignment with neighborhood predictions, thereby encouraging the agents to behave in a manner that reduces future uncertainty for their neighborhood. Moreover, the dense intrinsic rewards that foster collaborative actions in local neighborhoods also guide the agents during interactions with infrequent/sparse environmental feedback. Whereas CoHet allows agents to perform coordinated exploration even under reward sparsity, HetGPPO suffers when environmental (i.e. extrinsic) rewards are infrequent. We further evaluate our method against another state-of-the-art, Independent Proximal Policy Optimization (IPPO), where"}, {"title": "CoHetteam vs. CoHetself: A Comparative Analysis", "content": "CoHetteam utilizes the dynamics models of its surrounding agents to calculate intrinsic reward, whereas CoHetself employs the agent's own dynamics model and learns to follow its own predictions. As a result, the agents in CoHetteam learn to behave in accordance with their neighborhood predictions, and this can be challenging. As the agent dynamics models are trained using their own interactions with the environment, the presence of heterogeneous agents with differing physical/behavioral capabilities can result in a diverse set of dynamics models. Subsequently, the agents failing to align with the predictions of their local neighborhood, consisting of diverse agents, will be heavily penalized. Results indicate that, unlike previous methods, the novel GNN-based formulation in CoHet allows the agents to more accurately model their local heterogeneous neighborhood. In Table 2, we demonstrate that the intrinsic reward penalty for misalignment with neighbors in CoHetteam remains similar to the ones for CoHetself. In case of the performance, CoHetteam demonstrates better performance in all of the tasks that benefit from inter-agent cooperation, as it incentivizes the agents to adopt a more collaborative approach, irrespective of physical or behavioral heterogeneity. Interestingly, CoHetself exhibits superior performance over CoHetteam in only the MPE Simple Spread task. Since the landmarks are not specifically assigned to specific agents in this task, we find that the agents can gain an advantage from"}, {"title": "Reward Architecture Evaluation", "content": "As the dynamics models for each agent continue to be trained, we anticipate that the agents will gain a better understanding of the environmental dynamics, leading to a decrease in the dynamics model Mean Squared Error (MSE) loss over time. Considering that each agent operates with their independent dynamics models, we should see a decline in the dynamics model loss for each. From our experiments, we observe this aforementioned gradual decline in the mean dynamics model loss for each agent, as depicted in Figure 4a for the MPE Joint Passage task. This decline is also observed in each task of Figure 3. As evident in Figure 4a, the agents effectively learn the environmental dynamics within 600 training episodes, and the dynamics model loss remains minimal unless an unknown aspect of the environmental dynamics is encountered. As a result of a better understanding of the environmental dynamics, each agent is more capable of predicting its neighbors' next observations. Furthermore, the agents gradually adapt to meet their neighbors' predictions. Therefore, the intrinsic reward, which in the case of CoHet is a penalty for misalignment, transitions from large negative values to extremely small negative values over time. This trend is demonstrated in Figure 4b for the same task. Interestingly, as the intrinsic reward is calculated using the dynamics model loss, we can see that it exhibits the opposite trend to that of the loss."}, {"title": "Robustness to Increasing No. of Agents", "content": "As discussed previously, CoHetteam motivates the agents to exhibit behaviors that match their neighborhood predictions. However, the diversity of heterogeneity types in the agent neighborhood can pose a challenge, potentially leading to a performance decline as the number of heterogeneous agents in the shared environment increases. We find that CoHetteam maintains its robustness despite the growth in the number of heterogeneous agents in shared environments, indicated by the Mean Episodic Rewards in Table 3 after 1000 environmental episodes. The mean episodic rewards remain consistent with the rewards for the single-agent case, indicating the robustness of the proposed architecture to an increasing number of heterogeneous agents."}, {"title": "Conclusions and Future Work", "content": "In this paper, we propose the CoHet algorithm, a novel intrinsic reward calculation mechanism that utilizes a Graph Neural Network (GNN) and is suitable for decentralized heterogeneous MARL policy learning algorithms. It can be employed as a standalone module alongside existing decentralized policy optimization methods. Our proposed reward calculation architecture is applicable to practical multi-agent systems that operate under the constraints of partial observability and reward sparsity. The algorithm is validated in a series of cooperative multi-agent tasks that involve heterogeneous agents. We augment our research with an exploration of the effects of the agent dynamics model on the intrinsic reward module, compare the two variants of the CoHet algorithm and demonstrate that CoHetteam is robust against an increase in heterogeneous agents in the environment.\nA promising direction for future work includes exploring the alternative types of intrinsic motivation such as curiosity-driven or novelty-based rewards in the context of decentralized heterogeneous learning frameworks. Finding the proper balance between intrinsic and extrinsic rewards remains an open research problem. Future works can explore utilizing different weighting mechanisms that prioritize the predictions of the agents with the same sub-goals, and heterogeneity types. Ultimately, we opine that the collaboration between the agents in a MARL setting should take the need for decentralized training and agent heterogeneity into account."}]}