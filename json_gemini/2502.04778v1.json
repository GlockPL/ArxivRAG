{"title": "Behavior-Regularized Diffusion Policy Optimization for Offline Reinforcement Learning", "authors": ["Chen-Xiao Gao", "Chenyang Wu", "Mingjun Cao", "Chenjun Xiao", "Yang Yu", "Zongzhang Zhang"], "abstract": "The primary focus of offline reinforcement learning (RL) is to manage the risk of hazardous exploitation of out-of-distribution actions. An effective approach to achieve this goal is through behavior regularization, which augments conventional RL objectives by incorporating constraints that enforce the policy to remain close to the behavior policy. Nevertheless, existing literature on behavior-regularized RL primarily focuses on explicit policy parameterizations, such as Gaussian policies. Consequently, it remains unclear how to extend this framework to more advanced policy parameterizations, such as diffusion models. In this paper, we introduce BDPO, a principled behavior-regularized RL framework tailored for diffusion-based policies, thereby combining the expressive power of diffusion policies and the robustness provided by regularization. The key ingredient of our method is to calculate the Kullback-Leibler (KL) regularization analytically as the accumulated discrepancies in reverse-time transition kernels along the diffusion trajectory. By integrating the regularization, we develop an efficient two-time-scale actor-critic RL algorithm that produces the optimal policy while respecting the behavior constraint. Comprehensive evaluations conducted on synthetic 2D tasks and continuous control tasks from the D4RL benchmark validate its effectiveness and superior performance.", "sections": [{"title": "1. Introduction", "content": "Despite its huge success in industrial applications such as robotics (Kumar et al., 2021), game AI (Vinyals et al., 2019), and generative model fine-tuning (Ouyang et al., 2022), reinforcement learning (RL) algorithm typically requires millions of online interactions with the environment to achieve meaningful optimization (Haarnoja et al., 2018; Schulman et al., 2017). Given that online interaction can be hazardous and costly in certain scenarios, offline reinforcement learning, which focuses on optimizing policies with static datasets, emerges as a practical and promising avenue (Levine et al., 2020).\nHowever, without access to the real environment, RL algorithms tend to yield unrealistic value estimations for actions that are absent from the dataset (Levine et al., 2020; Fujimoto & Gu, 2021). Since RL necessitates querying the values of unseen actions to further refine its policy beyond the dataset, it is prone to exploiting the values of those unseen actions, leading to serious overestimation in value function. To manage this risk, one predominant approach is to employ behavior regularization (Fujimoto et al., 2019; Kumar et al., 2019; Wu et al., 2019; Tarasov et al., 2023), which augments the usual RL objectives by incorporating penalty terms that constrain the policy to remain close to the behavior policy that collects the datasets. In this way, the policy is penalized for choosing unreliable out-of-distribution (OOD) actions and thus exercises the principle of pessimism in the face of uncertainty.\nMost preliminary works in offline reinforcement learning assume explicit policy distributions (Fujimoto & Gu, 2021;"}, {"title": "2. Related Work", "content": "Offline RL. To improve beyond the interaction experience, RL algorithms need to query the estimated values of unseen actions for optimization. In offline scenarios, such distribution shift tends to cause serious overestimation in value functions due to the lack of corrective feedback (Kumar et al., 2020a). To mitigate this, algorithms like CQL (Kumar et al., 2020b), EDAC (An et al., 2021), and PBRL (Bai et al., 2022) focus on penalizing the Q-values of OOD actions to prevent the over-estimation issue. Behavior regularization provides another principled way to mitigate the distribution shift problem, by adding penalties for deviation from the dataset policy during the stage of policy evaluation, policy improvement (Fujimoto et al., 2019; Fujimoto & Gu, 2021; Ran et al., 2023), or sometimes both (Wu et al., 2019; Tarasov et al., 2023). Another line of research, also based on the behavior-regularized RL framework, approaches offline RL by performing in-sample value iteration (Kostrikov et al., 2022; Garg et al., 2023; Xu et al., 2023), thus eliminating OOD queries from the policy and directly approximating the optimal value function. Lastly, model-based offline RL methods (Yu et al., 2020; Sun et al., 2023; Luo et al., 2024) introduce learned dynamics models that generate synthetic experiences to alleviate data limitations, providing extra generalization compared to model-free algorithms.\nDiffusion Policies in Offline RL. There has been a notable trend towards the applications of expressive generative models for policy parameterization (Janner et al., 2022), dynamics modeling (Micheli et al., 2023; Alonso et al., 2024), and trajectory planning (Chen et al., 2021; Ajay et al., 2023; Gao et al., 2024). In offline RL, several works employ diffusion models to approximate the behavior policy used for dataset collection. To further improve the policy, they utilize in-sample value iteration to derive the Q-values, and subsequently select action candidates from the behavior diffusion model (Hansen-Estruch et al., 2023; Chen et al., 2023) or use the gradient of Q-value functions to guide the generation (Lu et al., 2023; Mao et al., 2024). However, the performances of these methods are limited, as the actions come from behavior diffusion. Alternatively, SRPO (Chen et al., 2024a) and DTQL (Chen et al., 2024b) maintain a simple one-step policy for optimization while harnessing the diffusion model or diffusion loss to implement behavior regularization. This yields improved computational efficiency and performance in practice; however, the single-step policy still restricts expressiveness and fails to encompass all of the modes of the optimal policy. A wide range of works therefore explore using diffusion models as the actor. Among them, DAC (Fang et al., 2024) formulates the optimization as a noise-regression problem and proposes to align the output from the policy with the gradient of the Q-value functions. Diffusion-QL (Wang et al., 2023) optimizes the actor by back-propagating the gradient of Q-values throughout the"}, {"title": "3. Preliminaries", "content": "Behavior-Regularized Offline Reinforcement Learning. We formalize the task as a Markov Decision Process (MDP) (S, A, T, R, \u03b3), where S is the state space, A is the action space, T(s'|s, a) denotes the transition function, R(s, a) is a bounded reward function, and \u03b3 is the discount factor. In offline RL, an agent is expected to learn a policy \u03c0 : S \u2192 \u0394(A) to maximize the expected discounted return $\\mathbb{E}_{\\pi}[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t)]$ with an offline dataset D = {(s_t, a_t, s_{t+1}, r_t)}, where $s_t, s_{t+1} \\in S$, $a_t \\in A$, and $r_t = R(s_t, a_t) \\in \\mathbb{R}$. We consider the behavior-regularized RL objective, which augments the original RL objective by regularizing the policy towards some behavior policy \u03bd:\n$\\max_{\\pi} \\mathbb{E}_{\\pi} [\\sum_{t=0}^{\\infty} \\gamma^t(r_t - \\eta \\text{KL} [\\pi(\\cdot|s_t)||\\nu(\\cdot|s_t)])]$,\nwhere \u03b7 > 0 controls the regularization strength. In offline RL, Eq. (1) is widely employed by setting \u03bd as the policy D that collects the dataset to prevent the exploitation of the out-of-dataset actions. Besides, when setting \u03bd as the uniform distribution, Eq. (1) equates to the maximum-entropy RL in online scenarios up to some constant.\nTo solve Eq. (1), a well-established method is soft policy iteration (Haarnoja et al., 2018; Wu et al., 2019). Specifically, we define the soft value functions as\n$V^{\\pi}(s) = \\mathbb{E}_{\\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t \\left(r_t - \\eta \\log \\frac{\\pi(a_t|s_t)}{\\nu(a_t|s_t)}\\right) \\right]$,\nwhere the expectation is taken w.r.t. random trajectories generated by \u03c0 under the initial condition $s_0 = s$ and $a_0 = a$. The soft Q-value function in this framework can be solved by the repeated application of the soft Bellman operator $B^{\\pi}$:\n$B^{\\pi} Q^{\\pi}(s, a) = R(s, a) + \\mathbb{E} \\left[ Q^{\\pi}(s', a') - \\eta \\log \\frac{\\pi(a'|s')}{\\nu(a'|s')} \\right]$,\nwhere $s' \\sim T(\\cdot|s, a)$ and $a' \\sim \\pi(\\cdot|s')$. For policy improvement, we can update the policy as follows:\n$\\max_{\\pi} \\mathbb{E}_{a \\sim \\pi(\\cdot|s)} [Q^{\\pi}(s, a)] - \\eta \\text{KL} [\\pi(\\cdot|s)||\\nu(\\cdot|s)]]$.\nNote that regularization is added for both Q-value functions and the policy. By iterating between policy evaluation and improvement, the performance of the policy defined by Eq. (1) is guaranteed to improve (Haarnoja et al., 2018)."}, {"title": "Diffusion Models", "content": "Diffusion models (Ho et al., 2020; Song et al., 2021) consist of a forward Markov process which progressively perturbs the data $x^0 \\sim q_0$ to data that follows the standard Gaussian distribution $x^N \\sim q_N(x)$, and a reverse Markov process that gradually recovers the original $x^0$ from the noisy sample $x^N$. The transition of the forward process $q_{n+1|n}$ usually follows Gaussian distributions:\n$q_{n|n-1}(x_n|x_{n-1}) = \\mathcal{N}(x_n; \\sqrt{1 - \\beta_n} x_{n-1}, \\beta_n I)$,\nwhere {$\\beta_n$}$_{n=1}^N$ is specified according to the noise schedule. Due to the closure property of Gaussian distributions, the marginal distribution of $x_n$ given $x^0$ can be specified as:\n$q_{n|0}(x_n|x^0) = \\mathcal{N}(x_n; \\sqrt{\\bar{\\alpha}_n} x^0, (1 - \\bar{\\alpha}_n) I)$,\nwhere $\\alpha_n = 1 - \\beta_n$, $\\bar{\\alpha}_n = \\prod_{n'=1}^n \\alpha_{n'}$. The transition of the reverse process can be derived from Bayes' rule,\n$q_{n-1|n}(x_{n-1}|x_n) = \\frac{q_{n|n-1}(x_n|x_{n-1})q_{n-1}(x_{n-1})}{q_n(x_n)}$.\nHowever, it is usually intractable, and therefore we use a parameterized neural network $p_{\\theta}^{n-1|n}$ to approximate the reverse transition, which is also a Gaussian distribution with parameterized mean:\n$p_{\\theta}(x^N) = \\mathcal{N}(0, I)$,\n$p_{\\theta}^{n-1|n}(x_{n-1}|x_n) = \\mathcal{N}(x_{n-1}; \\mu_{\\theta}(x_n), \\sigma I)$,\nwhere $I$ is the identity matrix, and $\\sigma = \\frac{1-\\alpha_{n-1}}{1-\\bar{\\alpha}_n} \\beta_n$. The learning objective is matching $p_{\\theta}^{n-1|n}(x_{n-1}|x_n)$ with the posterior $q_{n-1|n,0}(x_{n-1}|x_n, x^0)$:\n$\\mathcal{L}_{diff}(\\theta) = \\mathbb{E}_{n,x^0,x_n} \\left[ \\text{KL} \\left[ q_{n-1|n,0}(x_{n-1}|x_n, x^0)||p_{\\theta}^{n-1|n}(x_{n-1}|x_n) \\right] \\right]$\nwhere $x^0 \\sim q_0$, $x_n \\sim q_{n|0}(\\cdot|x^0)$. Simplified training objectives can be derived by reparameterizing $\\mu_{\\theta}$ with noise prediction or score networks (Ho et al., 2020; Song et al., 2021). After training, the generation process begins by sampling $\\hat{x}^N \\sim \\mathcal{N}(0, I)$, followed by iteratively applying $p_{\\theta}^{n-1|n}$ to generate the final samples $\\hat{x}^0$, which approximately follow the target distribution $q_0$.\nDiffusion Policy. Diffusion policies are conditional diffusion models that generate action a on a given state s. In this paper, we will use $p^{\\pi}$ and $p^{\\pi,s}$ to denote the diffusion policy and its state-conditioned version, respectively. Similarly, we will use $p_{n-1|n}^{\\pi,s,\\alpha_n}$ as a shorthand for the single-step reverse transition conditioned on $a^n$, i.e., $p_{n-1|n}^{\\pi,s,\\alpha_n} = p_{\\theta}^{n-1|n}(a_{n-1})$. At timestep t of the environment MDP, the agent observes the state $s_t$, drives the reverse diffusion process $a_{0:N} \\sim p^{\\pi}$ as defined in Eq. (8), and"}, {"title": "4. Method", "content": "4.1. Pathwise KL Regularization\nIn behavior-regularized RL, previous methods typically regularize action distribution, i.e. the marginalized distribution $p_{\\pi}^{s}$. Instead, we shift our attention to the KL divergence with respect to the diffusion path $a_{0:N}$, which can be further decomposed thanks to the Markov property:\n$\\text{KL} [p_{0:N}^{\\pi,s}||p_{0:N}^{\\nu,s}] = \\mathbb{E}_{a_{0:N} \\sim p_{0:N}^{\\pi,s}} \\left[ \\log \\frac{p_{0:N}^{\\pi,s}(a_{0:N})}{p_{0:N}^{\\nu,s}(a_{0:N})} \\right]$\n$= \\mathbb{E} \\left[ \\log \\frac{p_{\\pi,s}(a^N) \\prod_{n=1}^N p_{n-1|n}^{\\pi,s}(a^{n-1}|a^n)}{p_{\\nu,s}(a^N) \\prod_{n=1}^N p_{n-1|n}^{\\nu,s}(a^{n-1}|a^n)} \\right]$\n$\\mathbb{E} \\left[ \\log \\frac{p_{\\pi,s}(a^N)}{p_{\\nu,s}(a^N)} + \\sum_{n=1}^N \\log \\frac{p_{n-1|n}^{\\pi,s}(a^{n-1}|a^n)}{p_{n-1|n}^{\\nu,s}(a^{n-1}|a^n)} \\right]$\n$\\mathbb{E} \\left[ \\sum_{n=1}^N \\text{KL} \\left[ p_{n-1|n}^{\\pi,s,\\alpha_n}(a^{n-1})||p_{n-1|n}^{\\nu,s,\\alpha_n}(a^{n-1}) \\right] + \\log \\frac{p_{\\pi,s}(a^N)}{p_{\\nu,s}(a^N)} \\right]$.\nHere, the expectation is taken w.r.t. $a_{0:N} \\sim p_{0:N}^{\\pi,s}$, $p^{\\pi}$ is the diffusion policy trained via Eq. (9) on the offline dataset D to approximate the data-collection policy, and the last equation holds due to $p_{\\pi,s}^N = p_{\\nu,s}^N = \\mathcal{N}(0, I)$. In the following, we abbreviate KL $\\left[ p_{n-1|n}^{\\pi,s,\\alpha_n}(a^{n-1})||p_{n-1|n}^{\\nu,s,\\alpha_n}(a^{n-1}) \\right]$ with $l^{\\pi,s}_{n-1|n}(a^n)$.\nBased on this decomposition, we present the pathwise KL-regularized RL problem.\nDefinition 4.1. (Pathwise KL-Regularized RL) Let p\u03bd be the reference diffusion process. The pathwise KL-regularized"}, {"title": "4.2. Actor-Critic across Two Time Scales", "content": "To solve the pathwise KL-regularized RL (Eq. (11)), we can employ an actor-critic framework. Specifically, we maintain a diffusion policy p\u03c0 and a critic Q\u03c0. The update target of the critic Q\u03c0 is specified as the standard TD target in the environment MDP:\n$B^{\\pi}Q^{\\pi}(s, a) = R(s, a) + \\mathbb{E}[Q^{\\pi}(s', a^0) - \\eta \\sum_{n=1}^N l_{n-1|n}^{\\pi,s'}(a^n)]$,\ni.e., we sample diffusion paths a0:N at the next state s', calculate the Q-values Q\u03c0(s', a0) and the accumulated penalties along the path, and perform a one-step TD backup. For the diffusion policy, its objective can be expressed as:\n$\\max_{\\pi} \\mathbb{E}_{a^{0:N} \\sim p^{\\pi}} \\left[ Q^{\\pi}(s, a) - \\eta \\sum_{n=1}^N l_{n-1|n}^{\\pi,s}(a^n) \\right]$.\nAt first glance, one might treat the pathwise KL as a whole, and optimize the diffusion policy by back-propagating the gradient throughout the sampled path a0:N, similar to Diffusion-QL (Wang et al., 2023). Nevertheless, this requires preserving the computation graph of all diffusion steps and therefore incurs considerable computational overhead. Moreover, the inherent stochasticity of the diffusion"}, {"title": "4.3. Practical Implementation", "content": "Calculation of KL Divergence. Since each single-step reverse transition $p_{n-1|n}^{\\pi,s,\\alpha_n}$ is approximately parameterized as an isotropic Gaussian distribution (Eq. (8)), the KL divergence is analytically expressed as\n$\\text{KL} \\left[ p_{n-1|n}^{\\pi,s,\\alpha_n}||p_{n-1|n}^{\\nu,s,\\alpha_n} \\right] = \\frac{|\\mu_n^{\\pi,s}(a^n) - \\mu_n^{\\nu,s}(a^n)||^2}{2\\sigma^2}$.\nThat is, each penalty term is simply the discrepancy between the mean vectors of the reverse transitions, weighted by a factor depending on the noise schedule. The transition of the reverse process becomes Gaussian exactly in the continuous limit, in which case, the KL divergence can be computed through Girsanov's theorem (Oksendal, 2013), and the sum of mean squared errors (MSE) is replaced by an integral (Franzese et al., 2024). See appendix D for more details.\nSelection of States, Diffusion Steps and Actions. The policy improvement step requires training $V^{\\pi}$ and improving p\u03c0 on every (s, an, n) triplet sampled from the on-policy distribution of p\u03c0. However, we employ an off-policy approach, by sampling (s, a) from the dataset, n according to the Uniform distribution U[1, N] and an according to $q_{n|0}(\\cdot|a)$, which we found works sufficiently well in our experiments. We note that on-policy sampling (s, an, n) may further benefits the performance at the cost of sampling and restoring these triplets on the fly, similar to what iDEM (Akhound-Sadegh et al., 2024) did in its experiments.\nLower-Confidence Bound (LCB) Value Target. Following existing practices (Zhang et al., 2024a; Fang et al., 2024), we use an ensemble of K = 10 value networks {$\\psi_k$, $\\Phi_k$}$_{k=1}^K$ for both $V^{\\pi}$ and $Q^{\\pi}$ and calculate their target as the LCB of Eq. (12) and Eq. (14):\n$y_Q = \\text{Avg}_k[B^{\\pi}Q] - \\rho \\sqrt{\\text{Var}_k [B^{\\pi}Q]}$,\n$y_V = \\text{Avg}_k[TV] - \\rho \\sqrt{\\text{Var}_k [TV]}$\nwhere $\\Phi_k$ and $\\psi_k$ are exponentially moving average versions of the value networks. The objective for value networks is minimizing the mean-squared error between the prediction and the target:\n$\\mathcal{L}(\\{\\psi_k\\}_{k=1}^K) = \\mathbb{E}_{(s,a) \\sim D, n, a^n} (y_V - V_{\\psi_k}(s, a))^2$,\n$\\mathcal{L}(\\{\\Phi_k\\}_{k=1}^K) = \\mathbb{E}_{(s,a) \\sim D} (y_Q - Q_{\\Phi_k}(s, a))^2$,\nwhere n is sampled from U[1, N] and $a^n$ from $q_{n|0}(\\cdot|a)$. The pseudo-code for BDPO is provided in Algorithm 1."}, {"title": "5. Experiments", "content": "We evaluate BDPO with synthetic 2D tasks and also the D4RL benchmark (Fu et al., 2020). Due to the space limit,"}]}