{"title": "FedPT: Federated Proxy-Tuning of Large Language Models on Resource-Constrained Edge Devices", "authors": ["Zhidong Gao", "Yu Zhang", "Zhenxiao Zhang", "Yanmin Gong", "Yuanxiong Guo"], "abstract": "Despite demonstrating superior performance across a variety of linguistic tasks, pre-trained large language models (LMs) often require fine-tuning on specific datasets to effectively address different downstream tasks. However, fine-tuning these LMs for downstream tasks necessitates collecting data from individuals, which raises significant privacy concerns. Federated learning (FL) has emerged as the de facto solution, enabling collaborative model training without sharing raw data. While promising, federated fine-tuning of large LMs faces significant challenges, including restricted access to model parameters and high computation, communication, and memory overhead. To address these challenges, this paper introduces Federated Proxy-Tuning (FedPT), a novel framework for federated fine-tuning of black-box large LMs, requiring access only to their predictions over the output vocabulary instead of their parameters. Specifically, devices in FedPT first collaboratively tune a smaller LM, and then the server combines the knowledge learned by the tuned small LM with the knowledge learned by the larger pre-trained LM to construct a large proxy-tuned LM that can reach the performance of directly tuned large LMs. The experimental results demonstrate that FedPT can significantly reduce computation, communication, and memory overhead while maintaining competitive performance compared to directly federated fine-tuning of large LMs. FedPT offers a promising solution for efficient, privacy-preserving fine-tuning of large LMs on resource-constrained devices, broadening the accessibility and applicability of state-of-the-art large LMs.", "sections": [{"title": "Introduction", "content": "The emerging large language models (LMs) have demonstrated remarkable zero-shot and few-shot learning capabilities across various language tasks, such as text generation, question-answering, and machine translation. Large LMs, such as LLaMA (Touvron et al. 2023) and GPT-4 (Achiam et al. 2023), are trained on massive, diverse, and public datasets with up to hundreds of billion parameters. To adapt a general large LM for a specific task, it is usually fine-tuned on task-oriented datasets to meet the desired quality of service. For instance, PMC-LLaMA (Wu et al. 2023) is fine-tuned on medical data to achieve improved accuracy on medical-related questions. In practice, these datasets (e.g., user reviews and emails) are often distributed across devices, and collecting these datasets can be costly and may compromise user privacy.\nTo overcome this issue, federated learning (FL) (McMahan et al. 2017), which enables collaborative model training without sharing the raw data, is a de facto approach. However, there are several significant challenges for directly fine-tuning large LMs in FL: 1) Memory Overhead. Training large models requires significant memory, often exceeding 10 GB (Wang et al. 2023b; Rajbhandari et al. 2020). Most devices have RAM capacities of 4-8 GB (iLex 2024), which is insufficient for such tasks. 2) Computation Overhead. Even on a GPU-equipped device, local computations can take several hundred seconds per round. Consequently, a fine-tuning session may extend over several days. 3) Communication Overhead. In each FL round, participating devices are required to download the latest global model and then upload their local models.\nRecently, various parameter-efficient fine-tuning (PEFT) methods have been integrated into FL to overcome the aforementioned challenges (Zhao et al. 2023b,a; Che et al. 2023; Babakniya et al. 2023; Cai et al. 2023). These approaches assume that devices have white-box access to a large LM's parameters, focusing on updating only a small subset of parameters. In practice, however, these assumptions do not always hold due to the following reasons. 1) The pre-trained LMs could be proprietary (e.g., GPT-4), and thus their model weights are private, making it impossible to directly tune these models on edge devices in FL. 2) Even with PEFT methods, large LM fine-tuning still requires a huge memory footprint. For example, fine-tuning a LLaMA-13B model using the LORA (Hu et al. 2021) requires 34.8 GB VRAM. Such requirements exceed the capabilities of most resource-constrained devices in FL.\nTo bridge this gap, we introduce Federated Proxy-Tuning (FedPT), a lightweight federated fine-tuning method for large black-box LMs that requires access only to their predictions over the output vocabulary instead of their parameters. Specifically, as demonstrated in Figure 1, devices first collaboratively tune a smaller LM based on their private data. Then, with the small fine-tuned LM, the cloud server constructs a large proxy-tuned LM by leveraging the difference between the predictions of the small pre-trained and fine-tuned LMs (Liu et al. 2024a; Mitchell et al. 2023) to shift the original predictions of the larger pre-trained LM in the direction of tuning. After that, the server leverages knowledge distillation to transfer the knowledge from the large proxy-tuned LM to the small aggregated LM to obtain an updated small LM for further training. These steps are repeated iteratively until the large proxy-tuned LM meets predefined performance criteria. As will be shown later, even without access to the model weights of large LMs, FedPT can achieve comparable performance to directly federated fine-tuning of large LMs by leveraging the prediction distributions over the output vocabulary. Moreover, each device only needs to tune a smaller LM, which can effectively reduce the computation, communication, and memory overhead in FL.\nIn summary, this paper makes the following main contributions:\n1) We propose FedPT, a novel FL framework, that enables resource-constrained devices to fine-tune a large black-box LM collaboratively without sharing their private training data.\n2) We design a new federated fine-tuning strategy to achieve the same end as directly fine-tuning a larger LM at the server by tuning only smaller LMs at devices and applying proxy tuning and knowledge distillation to exchange the knowledge between the smaller LM and the larger proxy-tuned LM.\n3) We conduct extensive experiments on common benchmarks to evaluate the proposed framework. Experimental results demonstrate that FedPT can significantly reduce the computation, communication, and memory costs compared with directly fine-tuning large LMs while maintaining a similar model performance."}, {"title": "Problem Definition", "content": "Consider an FL system for fine-tuning the large LM, which consists of a cloud server and a set of N devices S (e.g., smartphones and IoT devices). Each device n \u2208 S has a local private dataset Dn, and the aggregated dataset of all devices is denoted as D := \u222a_{n\u2208S} D_n. For n \u2260 n' \u2208 S, the data distributions of Dn and Dn' could be different. The goal of the FL system is to find a global large LM \u03b8_l \u2208 \u211d^d with d in the scale of billions that solves the following optimization problem:\n$\\min_{\\theta_l \\in \\mathbb{R}^d} f(\\theta_l) := \\sum_{n \\in S} f_n(\\theta_l),$ (1)\nwhere f(\u03b8_l) represents the global loss, f_n(\u03b8_l) = \\mathbb{E}_{z\u2208D_n} [L(\u03b8_l; z)] is the local loss function of device n, z denotes a datapoint sampled from D_n, and L(\u03b8_l; z) represents the loss of model \u03b8_l on datapoint z.\nTo solve the optimization problem (1), FedIT (Zhang et al. 2024a) has been proposed, which integrates FedAvg (McMahan et al. 2017) with LoRA to alleviate the communication and memory overhead. However, it still faces the following challenges: (1) In practice, the white-box access to large LM \u03b8_l is not always available, making direct federated fine-tuning of the large LM infeasible. (2) Even with LoRA, fine-tuning the large LM continues to demand substantial memory and computational resources due to the vast size of the large LM."}, {"title": "Methodology", "content": "In this section, we introduce a new approach termed FedPT to address the aforementioned challenges. Instead of directly federated fine-tuning the large LM \u03b8_l to solve the optimization problem (1), the goal of FedPT is to construct a large proxy-tuned LM \\hat{\u03b8}_l that has similar performance as directly fine-tuning large LM \u03b8_l in each training round. Specifically, the large proxy-tuned LM \\hat{\u03b8}_l can be decomposed into three sub-models: a small pre-trained LM \u03b8_s^0, a small fine-tuned LM \u03b8_s, and a large pre-trained LM \u03b8_l^0. Here, the small LMs \u03b8_s^0, \u03b8_s \u2208 \u211d^{d_0} share the same vocabulary as the large pre-trained LM \u03b8_l^0 \u2208 \u211d^d while d \u226b d_0. Note that only the small LM \u03b8_s needs to be fine-tuned in FedPT. Essentially, FedPT aims to solve the following surrogate objective function:\n$\\min_{\\theta_s \\in \\mathbb{R}^{d_0}} f(\\theta_s) := \\sum_{n \\in S} \\frac{|D_n|}{|D|} -f_n(\\theta_s) + KL(p(\\hat{\\theta}_l), p(\\theta_s)).$ (2)\nThe first term is the weighted averaged local training loss of small LM \u03b8_s. The second term is the Kullback-Leibler (KL) divergence between the predicted probability distribution of the small LM p(\u03b8_s) and large proxy-tuned LM p(\\hat{\u03b8}_l).\nAlgorithm 1 provides the pseudo-code of FedPT. The total training process comprises T training rounds, with each round consisting of the following steps. At the beginning of t-th round, the cloud server randomly selects a subset S_t of K devices and broadcasts the latest small LM \u03b8_s^{t-1} to the selected devices (Lines 2-3). Then, each selected device k \u2208 S_t performs E epochs of local updates and sends the updated local model \u03b8_{s,k}^t back to the cloud server (Lines 4-7). Next, the cloud server aggregates the local models to obtain the small averaged LM \u03b8_s^{t} (Line 8). After that, the cloud server constructs the large proxy-tuned LM \\hat{\u03b8}_l^t based on the small averaged LM \u03b8_s^{t}, small pre-trained LM \u03b8_s^0, and large pre-trained LM \u03b8_l^0 (Line 9). The details of construction are introduced in Section 3.2. Finally, the cloud server obtains the latest small LM \u03b8_s^{t+1} by distilling the knowledge from \\hat{\u03b8}_l^t to \u03b8_s^{t} (Line 10). The details of knowledge distillation are explained in Section 3.3.\nIt is worth noting that FedPT inherits the privacy benefits of classic FL schemes by keeping the raw data on devices and sharing only model parameters. Additionally, FedPT is compatible not only with existing PEFT methods for large LMs, such as LoRA (Hu et al. 2021), LoHa (YEH et al. 2024), and adapter (Houlsby et al. 2019), but also with existing privacy-preserving techniques in FL, including secure aggregation and differential privacy."}, {"title": "Proxy Tuning", "content": "To deal with the huge computation, communication, and memory overheads and white-box model access requirement for the direct fine-tuning of LLMs in FL, we draw inspiration from proxy-tuning (Li et al. 2022; Liu et al. 2024a; Mitchell et al. 2023), which utilizes smaller LMs as proxies to guide the generation of larger LMs. For simplicity of notation, we will omit the superscript t without causing ambiguity in the following. At each training round, we fine-tune a small LM \u03b8_s, which shares the same vocabulary with the large pre-trained LM \u03b8_l^0. Subsequently, we add a logit offset, which is defined as the difference between logits from the small fine-tuned LM \u03b8_s and the pre-trained LM \u03b8_s^0, to every token of the large pre-trained model \u03b8_l^0 for guiding the prediction of the next word. Formally speaking, denote the input and generated sequence as x \u2208 X and y \u2208 Y, respectively. Let y_j be the j-th token in y and y_{<j} denote the sequence prefix from the beginning to the (j \u2013 1)-th token. Thus, the sequence-level distribution can be written as p(x, y) = \u220f_{j=1} p(y_j|x, y_{<j}). The probability distribution of the next word prediction from the large proxy-tuned LM \\hat{\u03b8}_l can be written as\n$p(y_j|x, y_{<j}; \\hat{\\theta}_l) := \\text{softmax} \\Big[g(y_j|x, y_{<j}; \\theta_l^0) + \u03b1 (g(y_j|x, y_{<j}; \\theta_s) - g(y_j|x, y_{<j}; \\theta_s^0)) \\Big],$ (3)\nwhere g(\u00b7) represents the logit function of the last layer of LM, and \u03b1 is a hyperparameter that controls the amount of modification to output distribution of the large pre-trained LM. A smaller value of \u03b1 results in predictions that closely resemble those of the large pre-trained LM, whereas a larger \u03b1 magnifies the contrast between the small fine-tuned LM and small pre-trained LM.\nNote that from Equation (3), we can obtain the following in the probability space:\n$\\frac{p(y_j|x, y_{<j}; \\hat{\\theta}_l)}{\\theta_l^0(y_j|x, y_{<j})} \\propto \\Big( \\frac{g(y_j|x, y_{<j}; \\theta_s)}{g(y_j|x, y_{<j}; \\theta_s^0)} \\Big)^\u03b1$ (4)\nFrom the above equation, it is evident that the small fine-tuned LM \u03b8_s plays a crucial role in guiding the large proxy-tuned LM \\hat{\u03b8}_l. Enhancing the fine-tuned \u03b8_s results in a more sophisticated large proxy-tuned LM. Moreover, as detailed in Appendices A.1 and A.2, the disparity between the large proxy-tuned LM and the directly fine-tuned large LM gradually diminishes with improvements in the small fine-tuned LM \u03b8_s."}, {"title": "Knowledge Distillation", "content": "According to the previous works (Li et al. 2022; Liu et al. 2024a; Mitchell et al. 2023), the proxy-tuned large LMs yield better performance compared with directly fine-tuned small LMs. Moreover, for knowledge-intensive tasks, proxy-tuning sometimes even surpasses the performance of directly fine-tuning the large LM, as it may preserve more learned knowledge than directly updating the model parameters of large LM (Liu et al. 2024a). Therefore, we further leverage the knowledge distillation (Sanh et al. 2019; Muhamed et al. 2021; Song et al. 2020) to transfer the general knowledge from the teacher model (i.e., large proxy-tuned LM \\hat{\u03b8}_l) to the student model (i.e., small LM \u03b8_s) in each training round. The objective of knowledge distillation is defined as follows:\n$\\theta_s = \\text{arg min}_{\\theta_s \\in \\mathbb{R}^{d_0}} \\mathbb{E}_{(x,y) \\sim D_{kd}} \\Big[ (1 - \u03bb) MLE(x, y; \u03b8_s) + \u03bb KL(p(x; \\hat{\\theta}_l), p(x; \u03b8_s)) \\Big],$ (5)\nwhere (x, y) is a datapoint sampled from a small public dataset D_{kd} in the cloud server, the first term MLE(x, y; \u03b8_s) represents the maximum likelihood estimation of the student model \u03b8_s, and the second term KL(p(x; \\hat{\u03b8}_l), p(x; \u03b8_s)) denotes the KL divergence between the predicted probability distribution of the teacher model \\hat{\u03b8}_l on and that of the student model \u03b8_s on the same data sample. Here, \u03bb is a hyperparameter used for balancing the two loss terms (see details in Appendix A.4)."}, {"title": "Experiments", "content": "We consider 10 devices in the experiments and experimentally validate the instruction-following task (Ouyang et al. 2022) as a conditional text-generation task where models generate responses based onthe given instructions. Other fine-tuning tasks, such as code generation (Roziere et al. 2023; Lai et al. 2023), can also be applied using our approach.\nModels. Our experiments utilize two distinct model families: GPT-2 (Radford et al. 2019) and LLAMA (Touvron et al. 2023), each available in various sizes. For GPT-2 family models, we use the GPT-2-760M model as the small LM and GPT-2-1.5B as the large LM. For LLaMA family models, we use LLaMA-7B as the small LM, while LLaMA-13B and LLaMA-30B serve as the large LM.\nFine-tuning Datasets. For the fine-tuning dataset D, we compile it from the \"databricks-dolly-15K\" (Conover et al. 2023), which contains 15,000 pairs of human-crafted instruction-following records. Specifically, we remove samples that surpass the models' context length. Then, we randomly allocate 1,000 samples for validation and 500 for testing, thereby retaining approximately 12,500 examples dedicated to training purposes. To simulate an FL setup, similar to FedIT (Zhang et al. 2024a), we employ two data partition strategies, pathological non-IID (McMahan et al. 2017) and Dirichlet non-IID (Hsu, Qi, and Brown 2019). We present the results on pathological non-IID distribution in the main paper, and the results on the Dirichlet distribution and further details about the data heterogeneity are provided in Appendix B.2. We utilize the Alpaca dataset (Taori et al. 2023a) as the public dataset Dkd for knowledge distillation.\nEvaluation Datasets. We evaluate the performance of our federated proxy-tuned model on the following three distinct instruction-following datasets: 1) Dolly: A 500-sample test set derived from the databricks-dolly-15K dataset. 2) Self-Inst (Wang et al. 2023a): A user-oriented instruction-following dataset with 252 samples. 3) S-NI: The SUPER-NATURALINSTRUCTIONS (Wang et al. 2022a) test set, which includes 9,000 samples across 119 tasks. Following (Peng et al. 2023; Gu et al. 2023), we divided this set into three subsets based on ground truth response lengths: [0, 5], [6, 10], and [11, +\u221e]. We use the [11, +\u221e] subset as the test set in our paper.\nEvaluation Metrics. We use two metrics to evaluate the model-generated responses: 1) Rouge-L score (Lin 2004): The Rouge-L score is used to assess the recall and relevance of text generated by a model by measuring the longest common subsequence of words compared to a reference text. Previous works (Wang et al. 2022b; Gu et al. 2023) have indicated that Rouge-L is appropriate for large-scale evaluation of instruction-following tasks. 2) GPT-4 feedback: We employ GPT-4-Turbo as a judge to evaluate model-generated responses from multiple perspectives, such as helpfulness, relevance, accuracy, and level of detail of their responses. The details are given in Appendix B.3.\nBaselines. We consider three baselines in our main experiments: 1) Base directly uses the base pre-trained large model on the server side. 2) FedAvg fine-tunes the small or large LM by the FedAvg algorithm (McMahan et al. 2017). This baseline is consistent with the recent work FedIT (Zhang et al. 2024a) that focuses on instruction-following tasks. 3) FedAvg+PT follows the same procedure as FedAvg to fine-tune a small LM. During text generation, it utilizes the large proxy-tuned LM, which incorporates the small fine-tuned model, a small pre-trained model, and a large pre-trained model to generate responses.\nHyperparameters. In all experiments, we use the most common PEFT technique, LoRA (Hu et al. 2021), for our local training (see Appendix A.3 for more details). Detailed parameters about LoRA can be found in Appendix B.6. We fine-tune the models for 20 communication rounds using the Prodigy optimizer (Mishchenko and Defazio 2024), with a batch size of 64 and an initial learning rate of 1. A cosine learning rate decay strategy (Loshchilov and Hutter 2016) is applied at each communication round, and safeguard warmup without bias correction is implemented. To save the memory footprint, all models are loaded into VRAM in half-precision mode, with checkpoints also saved in this format. For knowledge distillation in FedPT, the hyperparameter \u03bb is set to 0.1. 128 and 512 instances are sampled from the Alpaca dataset for the experiment on GPT-2 and LLaMA, respectively. More details are shown in Appendix B.5.\nExperiment Overview. We conduct experiments on GPT-2 and LLaMA models, recording checkpoints at each communication round and evaluating their performances on the three test datasets. For GPT-2 models, we conduct evaluations at communication rounds 1, 5, 10, and 15, as performance plateaued after round 10. For LLaMA models, evaluations are performed at communication rounds 1, 5, 10, 15, and 20. This results in 12 checkpoints for GPT-2 evaluation and 15 checkpoints for LLaMA evaluation. We search for the optimal \u03b1 for both FedAvg+PT and FedPT, selecting the best \u03b1 from {1.0, 1.3, 1.5, 1.8, 2.0} for the GPT-2 model and {1.0, 1.5, 2.0} for the LLaMA model. Finally, we obtain 720 evaluation results for the GPT-2 model and 600 evaluation results for the LLaMA model, totaling 1,320 evaluation results across all experiment settings."}, {"title": "Experimental Results", "content": "We first conduct a comprehensive comparison of FedPT and the baselines across Dolly, SelfInst, and S-NI datasets in terms of model size, resource consumption, and model performance. The final results are summarized in Table 1, and the detailed results during training are depicted in Figure 2.\nModel Size and Resource Costs. For both FedPT and FedAvg+PT, the large pre-trained model is only utilized on the server side, allowing small LMs to be deployed on each client. This setup significantly reduces memory, storage, and communication costs. As shown in Table 1, compared to FedAvg with LLaMA-13B, both FedPT and FedAvg+PT achieve a 44% reduction in VRAM usage and a 36% reduction in communications costs. Similarly, compared to FedAvg with GPT-2-1.5B, both algorithms attain a 38% reduction in VRAM usage and a 40% reduction in communication costs. Here we record the VRAM usage with the local training batch size of one. Note that VRAM usage may vary slightly due to different implementation details and hardware conditions. The reported VRAM consumption is not suitable for most devices but can be reduced through quantization (Dettmers et al. 2024) and CPU offloading (Ren et al. 2021). For LLaMA experiments, we employ the model parallelism with gradient accumulation to avoid VRAM overflow.\nModel Performance Comparison. From Table 1, we have the following observations for the base pre-trained and directly federated fine-tuning methods. First, the base pre-trained models suffer from inferior performance across all downstream datasets. Second, directly fine-tuning the large LM through the FedAvg method can significantly improve the model performance on specific downstream tasks.\nThen for the methods that use proxy-tuning, we have three observations. First, proxy-tuning can achieve performance comparable to the direct fine-tuning of the large LMs in the FL setting. For instance, although FedAvg+PT and FedPT only fine-tune GPT-2-760M locally, they achieve Rouge-L scores of 18.9 and 18.6, respectively. These scores surpass the 17.8 achieved by directly fine-tuning GPT-2-1.5B using FedAvg and are nearly as high as the 19.2 achieved by fine-tuning GPT-2-1.5B with FedAvg. Second, FedPT consistently outperforms FedAvg+PT. This superior performance is attributed to the use of a proxy model in FedPT to conduct knowledge distillation during the training process, thereby enhancing the performance of the aggregated smaller model. Third, we notice that FedPT exceeds the performance of FedAvg on LLaMA-13B for the SelfInst dataset. A similar phenomenon is also observed in (Liu et al. 2024a). This consistency suggests that proxy-tuning large models may better preserve knowledge more effectively than direct fine-tuning, which could potentially degrade performance on knowledge-intensive tasks. This highlights the great potential of the proxy-tuning approach.\nIn addition to the results on Rouge-L score, we also present the GPT-4 feedback results for LLaMA in Table 2. Due to the poor performance of the base pre-trained large model, we have excluded its GPT-4 score in Table 2. Additional results for GPT-2 are given in Appendix B.3. Note that we have the same observations for the GPT-4 evaluation results.\nScaling Law. We first investigate the performance of FedPT and FedAvg+PT when we scale up the size of a large model in Figure 3. Specifically, we evaluate FedPT on LLaMA-30B, reusing the fine-tuned LLaMA-7B from FedPT (7B-30B). Similarly, we evaluate FedAvg+PT on LLaMA-30B, reusing the fine-tuned LLaMA-7B from FedAvg (7B). This approach is designed to simulate a realistic scenario in which, during the training phase, only LLaMA-7B and LLaMA-13B are used. In the deployment phase, however, if more powerful models such as LLaMA-30B become available, we aim to evaluate whether the model trained with FedPT retains its advantage over FedAvg+PT. From Figure 3, we observe that performance improves for both FedPT and FedAvg+PT as the model size increases. The results for both methods show a clear positive correlation between model size and performance, highlighting the scaling law: larger models yield better results. Additionally, FedPT consistently outperforms FedAvg+PT, reinforcing the advantage of our proposed method as the number of model parameters increases.\nEffect of \u03b1. We then use various \u03b1 values in FedPT to investigate the effect of proxy-tuning weight \u03b1. As shown in (3), the generated logit follows g_\u03b8 + \u03b1(g_{\u03b8_s} \u2013 g_{\u03b8_s^0}). Intuitively, larger \u03b1 magnifies the influence of the difference between the fine-tuned small model and the pre-trained small model, making the predictions more responsive to the fine-tuning adjustments. Conversely, a smaller \u03b1 results in predictions more similar to the large pre-trained model, causing the predictions to adhere to the behavior of the original large pre-trained model closely.\nFigure 4 shows the results of \u03b1 \u2208 {1.0, 1.5, 2.0} for LLaMA on the three datasets. Specifically, \u03b1 = 1.5 yields the best performance for the Dolly and SelfInst dataset, while \u03b1 = 1.0 performs best for the S-NI dataset. This demonstrates the importance of carefully tuning \u03b1 to balance the trade-off between leveraging fine-tuning adjustments and maintaining the stability of the pre-trained model's predictions. Results for GPT-2 are provided in Appendix C.3."}, {"title": "Related work", "content": "Federated Fine-tuning of Large LMs. Although fine-tuned large LMs has demonstrated remarkable success across various domain-specific NLP tasks, deployment of large LMs is hindered by significant resource demand and data privacy concern. Federated fine-tuning of large LMs has been proposed as a promising technique to address the privacy concern, which enables multiple devices to fine-tune the large LM without sharing their private data. However, FL environments introduce stringent resource constraints, particularly on resource-constrained edge devices. This dilemma has catalyzed a shift towards integrating PEFT methods with the FL framework (Zhao et al. 2023b,a; Che et al. 2023; Babakniya et al. 2023; Cai et al. 2023). For example, federated prompt tuning is introduced in (Zhao et al. 2023b,a; Che et al. 2023), which only updates the soft prompt in each communication round of FL. Recent work (Xu et al. 2024) introduced a backpropagation-free FL framework for training large LMs so that it can reduce the required memory footprint effectively. However, these works are all based on the assumption of white-box access to large LMs. In contrast, FedPT only needs to fine-tune a small LM on each client while assuming black-box access to large LMs at the server, making it more appealing in practice.\nDecoding-time Tuning. Recent advancements in large LM applications have introduced a novel approach that \"tunes\" large LMs at decoding time. One such method, known as contrastive decoding (Li et al. 2023), improves text generation quality by subtracting the log probabilities of a smaller LM (called the amateur) from a larger LM (called the expert). This approach is subject to a plausibility constraint, ensuring that the generated text surpasses the quality produced by the large LM alone. Motivated by the logit-based tuning, a collaborative generation framework was proposed, merging logits from a small LM and a large LM through a learnable model to address privacy concerns (Zhang et al. 2024b). Similarly, a method that merges output probability distributions from a small LM and a large LM through a learned small network was developed in (Ormazabal, Artetxe, and Agirre 2023). The most recent studies (Mitchell et al. 2023; Liu et al. 2024a) utilize differences in logits as significant weights to recalibrate the conditional distributions within the large LM, thereby enhancing text generation capabilities. Specifically, one study analyzed the contribution of scaling up fine-tuning or pre-training (Mitchell et al. 2023), while another demonstrated the effectiveness of merging output logits from multiple LMs (Liu et al. 2024a, 2021). However, all of these studies focuses on centralized training, and hence their approaches are not applicable to the FL setting considered in our work."}, {"title": "Conclusion", "content": "In this work, we propose a novel FL framework called FedPT, designed for efficient fine-tuning of large LMs on resource-constrained devices without compromising privacy. A key advantage of FedPT is its ability to fine-tune large LMs without requiring access to their full model parameters. The experimental results confirm that FedPT achieves performance comparable to direct federated fine-tuning of large models, while significantly reducing resource costs in terms of storage, VRAM usage, and communication overhead."}, {"title": "Background", "content": "The general fine-tuning of the LMs process can be treated as a reinforcement learning (RL) process (Rafailov et al. 2024). We initial a policy \u03c0 = \u03b8_l, and then fine-tune \u03c0 to perform the task well. We denote the scalar-valued reward function r: X \u00d7 Y \u2192 \u211d, which represents the human preference of a response y to the query x. The RL goal is to maximize the expected rewards:\n$\\max_{\\pi} \\mathbb{E}_{x \\sim D, y \\sim \u03c0(\u00b7|x)} [r(x, y)],$ (6)\nwhere D is a fixed distribution (or dataset) of prompts. However, directly maximizing expected rewards can lead to a distribution collapse, which reduces the fluency and diversity of samples from the LLM (Korbak, Perez, and Buckley 2022). In order to solve the distribution collapse problem, one effective strategy is to include preserving the distributional properties of an LLM as part of the reward function (Korbak, Perez, and Buckley 2022; Mitchell et al. 2023; Ziegler et al. 2019; Liu et al. 2024b). The reformulated RL goal can be written as\n$\\max_{\\pi} \\mathbb{E}_{x \\sim D, y \\sim \u03c0(\u00b7|x)} [r(x, y) \u2013 \u03b2 KL(\u03c0(\u00b7|x)||\u03c0_0(\u00b7|x))],$ (7)\nwhere \u03c0_0 is the pre-trained model. The penalty Kullback-Leibler (KL) divergence term \u03b2 KL(\u03c0(\u00b7|x)||\u03c0_0(\u00b7|x)) can keep from moving too far from \u03c0_0. Here, \u03b2 controls the strength of the KL constraint to the pre-trained model. The closed-form solution (Ziegler et al. 2019; Korbak, Perez, and Buckley 2022; Raffel et al. 2020; Rafailov et al. 2024) can be shown as\n$\u03c0^*(y|x) = \\frac{1}{Z(x)} \u03c0_0(y|x) \\text{exp} \\Big( \\frac{1}{\u03b2} r_\u03c0 (x,y) \\Big),$ (8)\nwhere Z(x) = \u2211_y \u03c0_0(y|x) exp (\\frac{1}{\u03b2} r_\u03c0(x, y)). According to (Rafailov et al. 2024), we utilize the r_\u03c0(y|x) = \u03b2 \\text{log} (\\frac{\u03c0(y|x)}{\u03c0_0(y|x)}) as the reward function which is implicitly defined by the LM \u03c0 and \u03c0_0. Then, we can reformulate Equation (8) as\n$\u03c0^*(y|x) = \u03c0_0(y|x) \\text{exp} \\Big( \\text{log} \\frac{\u03c0(y|x)}{\u03c0_0(y|x)} \\Big),$ (9)\nFrom the above equation, we can observe that the base log probabilities represent the knowledge acquired during pre-training, whereas the capabilities gained through fine-tuning are reflected in the reward (e.g., the difference, calculated by subtracting the base log probabilities from the fine-tuned model log probabilities, indicates the improvement gained from fine-tuning.)."}, {"title": "Proxy-Tuning LLM", "content": "Based on the observation of Equation (9), we fine-tune a small pre-trained model \u03c0_{\u03b8_s}, which shares the same vocabulary with the large pre-trained model \u03c0_{\u03b8_l^0}. Instead of directly fine-tuning the large pre-trained"}]}