{"title": "Training Overhead Ratio: A Practical Reliability Metric for Large Language Model Training Systems", "authors": ["Ning Lu", "Qian Xie", "Hao Zhang", "Wenyi Fang", "Yang Zheng", "Jiantao Ma"], "abstract": "Large Language Models (LLMs) are revolutionizing the AI industry with their superior capabilities. Training these models requires large-scale GPU clusters and significant computing time, leading to frequent failures that significantly increase training costs. Despite its significance, this field lacks a metric for evaluating reliability. In this work, we introduce a novel reliability metric called Training Overhead Ratio (TOR) to evaluate the reliability of fault-tolerant LLM training systems. TOR is defined as the ratio of optimal training time to the observed training time of a system, serving as a practical tool for users to estimate the actual time required to train an LLM on a given system. Furthermore, our investigation identifies the key factor for enhancing reliability and present TOR equations for various types of failures encountered in practice.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have revolutionized the field of artificial intelligence, demonstrating remarkable capabilities across a wide range of tasks, including machine translation, text summarization, and conversational agents [1], [2]. The success of LLMs is primarily attributed to the scaling of model size and training data size, as evidenced by the scaling law [3]. To enhance model capacity, significant efforts have been invested in training increasingly larger models on trillions of tokens, requiring massive GPU clusters over extended periods.\nThis unprecedented scale of LLM training presents significant challenges in terms of system reliability. The month-long duration of training makes failures virtually inevitable. Moreover, the distributed and parallel nature of the training process exacerbates this issue, as a failure in a single computational node can halt the entire training system. For example, the Llama-3 training experienced 466 interruptions during a 54-day snapshot period [4]. These frequent failures extends the training time, creating a huge gap between the expected failure-free cost and actual one in practice.\nDespite the importance of reliability in LLM training systems, there is no widely accepted metric to effectively measure it. Traditional reliability engineering metrics, such as reliability and availability [5], fail to accurately model the LLM training systems. Reliability, typically modeled by failure rate, does not account for the overhead introduced by reliability improving operation such as checkpointing. Availability, calculated as the proportion of failure-free time, is not suitable for LLM training which requires a long time continuous computation. A system might achieve perfect availability by running indefinitely, yet be unreliable due to frequent data corruption. Another commonly used metric for LLM training system is Model FLOPs Utilization (MFU) [2]. However, it is designed to measure the system's efficiency, omitting the time waste, such as the interval between checkpoint and failure occurrence. This gap highlights the need for a more comprehensive and practical metric to measuring the reliability of LLM training systems.\nIn this paper, we introduced a new metric Training Overhead Ratio (TOR). It is defined as the ratio of the optimal training time to the observed training time of a system, where the optimal training time refers to the duration necessary to execute a task on an ideal, failure-free system devoid of any operational overhead. It considers all possible events that extends the training period, e.g., time wasted for roll-back, checkpoint saving overheads. Using this metric, system users can estimate how long a training time will cost on a given system. Furthermore, our investigation show that performance preservation ratio is crucial for improving the system's reliability. And we present TOR equations for various types of failures encountered in practice.\nIn summary, our contributions are: 1) We propose the first reliability metric for LLM training systems and provide a detailed equation for its calculation. 2) We identify key factors that enhance system reliability through investigation."}, {"title": "II. TRAINING OVERHEAD RATIO", "content": "We introduce a novel metric, termed the Training Overhead Ratio (TOR), designed to evaluate the reliability of fault-tolerant LLM training systems. TOR is defined as the ratio of the optimal training time to the actual observed training time, offering a quantitative assessment of a system's performance relative to its failure-free efficiency. Optimal training time is characterized as the duration necessary to execute a task on an ideal, failure-free system devoid of any operational overhead. In simpler terms, it represents the uninterrupted execution time of a task. It is mathematically represented as follows:\nTraining Overhead Ratio = $\\frac{\\text{Optimal Training Time}}{\\text{Observed Training Time}}$\nTOR ranges from 0 to 1, with higher values indicating a greater reliability of the system. It gives empirical comparisons among different training systems, enabling users to estimate the actual time required to train an LLM on a given system."}, {"title": "B. Performance Preservation Ratio", "content": "In this section, we introduce the concept of performance preservation ratio and demonstrates its crucial role for improving the system's TOR.\nIn any training task, the total workload, e.g., the number of FLOPs, is fixed. The equality for the computational effort between the optimal training and the failure-aware one is captured by the following equation:\nWopt. Topt = $\\int_{0}^{Tobs} Wobs(t) dt$,\nwhere wobs(t) represents the observed work rate at time t. For optimal training, we simplify the optimal work rate as a constant Wopt. Note that the optimal work rate does not imply optimal hardware utilization, e.g., 100% MFU. Instead, it represents the normal working rate without failures. Topt and Tobs denote the optimal and observed training time, which is required for TOR calculation. By dividing Wopt on both sides, Eq. 2 simplifies to:\nTopt = $\\int_{0}^{Tobs} r(t) dt$,\nwhere r(t) = $Wobs(t)/Wopt \\in [0,1]$ is the performance preservation ratio, representing the proportion of the optimal work rate achieved in practice at time t.\nEq. 3 provides a tractable method for estimating the optimal training time. Furthermore, it suggests that enhancing the TOR requires either maximizing r(t) or minimizing the duration of low r(t) values during training."}, {"title": "C. Failure Modeling", "content": "To further analyze the impact on the system reliability, or the performance preservation ratio, we develop a simple failure-recovery model based on industrial experience.\nModern LLM training systems encounter various types of failures during the training process [4]. We categorize these failures into two types based on their recovery patterns: fail-stop and fail-slow failures. Fail-stop failures instantly stop the training process and requires roll-back operation for recovery. In contrast, fail-slow failures allow the training process to continue but at a lower-than-expected performance level.\nWe focus our analysis on a single failure-recovery period, which can be viewed as a repeating unit throughout the training process. Both failure types share the following stages:\n\u2022\nSlow Recovery: Post-failure recovery where the system operates at a reduced functionality (r(t) < 1) for duration Tsr, with r(t) approximated as constant Rrs for simplicity.\n\u2022\nHealthy Run: Normal system runs without failures, where r(t) = 1 for duration Th."}, {"title": "III. CONCLUSION", "content": "In this paper, we propose the first reliability metric termed Training Overhead Ratio for LLM training system. This is the ratio of the optimal training time to the actual observed training time of a system. Furthermore, our investigation identifies the key factor for enhancing reliability and present TOR equations for various types of failures encountered in practice."}], "equations": ["Training Overhead Ratio = $\\frac{\\text{Optimal Training Time}}{\\text{Observed Training Time}}$", "Wopt. Topt = $\\int_{0}^{Tobs} Wobs(t) dt$", "Topt = $\\int_{0}^{Tobs} r(t) dt$", "TORfail-stop = $\\frac{Topt}{Tobs} = \\frac{\\int_{0}^{Tobs} r(t) dt}{Tobs} = \\frac{TsrRrs + Th}{Tsr + Th + NckptTckpt + Trb + Tr} = \\frac{MTBF \u2013 Tsr(1 \u2013 Rsr) - Trb - NckptTckpt}{MTBF + Tr}$", "TORfail-slow = $\\frac{TsrRrs + Th + TfsRfs}{Tsr + Th + NckptTckpt +Tfs + Tr} = \\frac{MTTR \u2013 Tsr(1 \u2013 Rsr) - NckptTckpt + TfsRfs}{MTTR+Tfs + Tr}$"]}