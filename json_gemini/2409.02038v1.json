{"title": "BEAVER: An Enterprise Benchmark for Text-to-SQL", "authors": ["Peter Baile Chen", "Fabian Wenz", "Yi Zhang", "Moe Kayali", "Nesime Tatbul", "Michael Cafarella", "\u00c7a\u011fatay Demiralp", "Michael Stonebraker"], "abstract": "Existing text-to-SQL benchmarks have largely been constructed using publicly available tables from the web with human-generated tests containing question and SQL statement pairs. They typically show very good results and lead people to think that LLMs are effective at text-to-SQL tasks. In this paper, we apply off-the-shelf LLMs to a benchmark containing enterprise data warehouse data. In this environment, LLMs perform poorly, even when standard prompt engineering and RAG techniques are utilized. As we will show, the reasons for poor performance are largely due to three characteristics: (1) public LLMs cannot train on enterprise data warehouses because they are largely in the \"dark web\", (2) schemas of enterprise tables are more complex than the schemas in public data, which leads the SQL-generation task innately harder, and (3) business-oriented questions are often more complex, requiring joins over multiple tables and aggregations. As a result, we propose a new dataset BEAVER, sourced from real enterprise data warehouses together with natural language queries and their correct SQL statements which we collected from actual user history. We evaluated this dataset using recent LLMs and demonstrated their poor performance on this task. We hope this dataset will facilitate future researchers building more sophisticated text-to-SQL systems which can do better on this important class of data. The project page is available at: https://peterbaile.github.io/beaver/.", "sections": [{"title": "1 Introduction", "content": "LLMs have shown their potential for solving text-to-SQL tasks on existing datasets, such as Spider, KaggleDBQA and Bird (Li et al., 2024; Sen et al., 2019; Yu et al., 2018; Lee et al., 2021). For example, on Spider, GPT-4 can achieve an execution accuracy above 85% (Gao et al., 2024). However, these datasets focus on tables collected from public sources and question-SQL pairs written by annotators. As such, they are not representative of real-world enterprise settings, for the following reasons.\nFirst, enterprise databases, typically designed for internal business use, often utilize more intricate schemas compared to tables from public datasets. Hence, understanding them may require database or business-specific knowledge. Public LLMs lack access to such knowledge and are unlikely to perform well on enterprise text-to-SQL tasks. As we will show in this paper, they often generate queries that contain incorrect/ insufficient columns or invalid values (e.g., in WHERE clauses).\nSecond, questions posed to enterprise databases are generally more complex than questions from public datasets. Public datasets are usually small and typically general-purposed. Therefore, the questions posed tend to be simple, and the information involved may only include one to two tables. In contrast, enterprise database schemas are more complex with corresponding complex SQL that involves joins and aggregates over multiple tables."}, {"title": "2 Dataset", "content": "As described in Section 1, existing public datasets do not reflect large data warehouses with high schema and query complexity. To study this issue, we have collected datasets from two enterprise data warehouses. We first formalize the text-to-SQL task and then describe the datasets."}, {"title": "2.1 Inputs and outputs", "content": "Following the standard problem setup of text-to-SQL, the input to an LLM includes a natural language question and a database of tables, and the output is a SQL statement whose execution answers the user's question. A database includes a set of tables, each defined by a schema that describes the names of columns and instances/ rows of the tables."}, {"title": "2.2 Data Warehouse 1: DW", "content": "The first data warehouse, called DW, contains 99 tables and 1544 columns from an existing SQL data warehouse. These tables deal with data about the physical plant of a university, including buildings, rooms and their use, along with age information and maintenance records. There are 61 pairs of natural language questions and corresponding SQL, in which users might ask questions such as \u201cWhat are the building names, department names, building street addresses, total number of rooms, and total area of all rooms for the electrical engineering and computer science department and the material science and engineering department?\u201d. Data is collected as follows.\nDatabases. Table information, including column names, column types, and rows, was collected for each database. Moreover, primary-key-foreign-key relationships were also added to the tables.\nSQL statements. To reflect the actual complexity of queries posed on enterprise databases, we first collected logs and reports from source organizations that include actual SQL statements executed by database users. From these logs and reports, we gathered SQL statements. To ensure diversity of SQL statements, we replaced literals and specific names (e.g., column name, table name) with placeholders (e.g., replace WHERE SESSION_LOCATION = 'Virtual' with WHERE PLACEHOLDER_COLUMN = PLACEHOLDER_LITERAL) to obtain patterns and only kept those SQL queries with distinct patterns."}, {"title": "2.3 Data warehouse 2: NW", "content": "The second data warehouse, called NW, includes 321 tables and 2309 columns from three separate SQL databases. These tables relate to networking and virtual machines in an enterprise infrastructure framework and describe networking policies, virtual machine status, IP addresses, and virtual machine migrations. There are 32 pairs of natural language questions and SQL queries, which include user questions about configurations, port number, and other attributes of virtual machines as well as migration and networking status to facilitate diagnosis and security monitoring. These question-SQL pairs were collected using the same approach as mentioned in Section 2.2."}, {"title": "2.4 Statistics", "content": "Table 1 summarizes the domain, dataset size, table statistics, and query complexity of our dataset (DW and NW combined) as well as several popular open-source datasets. Similar to Lan et al. (2023); Li et al. (2024), we measure query complexity in three dimensions: the average number of joins per query which indicates how many tables need to be combined to include sufficient information to answer the user question; the average number of aggregations per query which indicates the number of times aggregation keywords such as max, count, group by appear; and the nesting depth which indicates how deep sub-queries appear (e.g., SELECT FROM (SELECT ...) has a nesting depth of two). Compared to all existing datasets, BEAVER has the largest number of tables per database and the highest query complexity. To better understand the distribution of query complexity, we plotted the three dimensions of query complexity in all datasets. As shown in Figure 1, queries in BEAVER appear in the upper-right corner, meaning that they tend to have higher complexity in all three dimensions."}, {"title": "Natural language questions", "content": "Two graduate students and two database administrators from the data warehouse support group collectively constructed natural language questions for the collected SQL statements. The students first collaboratively generated the questions for the corresponding SQL statement. To ensure quality, these questions were then passed to the database administrators for review. If some questions lacked clarity, they were sent back to the students for editing. The above process repeats until all questions were approved by the database administrators."}, {"title": "2.5 Motivating example", "content": "Consider the following four tables extracted from the 99 tables in DW described in Section 2.2 and shown in Figure 2: the table Organization has a unique identifier and a name for each entity on campus; the table Room contains the unique ID for each building and each room in each building, and the area of each room in each building; the table Building_address has the actual address of each building; the table Buildings has the name of each building. Figure 2 also shows the join (primary-key-foreign-key) relationships (illustrated using green lines connecting different columns) among the four tables. While there might be better ways to store the data in these four tables, this particular schema is what exists in the data warehouse."}, {"title": "3 Benchmark", "content": "In the following sections, we extend beyond this example and benchmark off-the-shelf LLMs on our entire dataset (Section 3) and discuss the sources of error (Section 4)."}, {"title": "3.1 Experimental setup", "content": "Retrieval setting. As seen in Table 1, the average number of tables and columns per database in previous works (Yu et al., 2018; Li et al., 2024) is small, averaging 4.05 tables per database and 5.44 columns per table in Spider and 6.82 tables per database and 10.6 columns per table in Bird. This makes it feasible to fit schema information of all tables in a database within the input context limit. However, a key characteristic of enterprise databases is a large number of tables and a large number of columns per table, which makes it much more challenging to fit all this information in LLM's context. Moreover, trying to feed all tables to models can lead to under-utilization. Existing work (Liu et al., 2024) shows that models might not \"attend to\" all information in a lengthy prompt and that providing fewer tables can potentially outperform providing a large number of tables (Chen et al., 2024) due to less noise. Therefore, we introduce the retrieval setting. Given the input of a user question and a database, instead of feeding the user question and the schema of all tables in the database directly to LLMs for SQL generation, a retrieval system (e.g., embedding model) is first used to retrieve the top-k most relevant tables from the given database to the user question; only the schema of these top-k tables and the user question are provided as inputs to LLMs to generate the SQL statements.\nRetrieval-free setting. The retrieval setting described above is essential for understanding the end-to-end performance in a real-world enterprise setting where the underlying corpus consists of a large number of tables and a retrieval step is necessary. However, it may not accurately reflect models' ability to generate SQL because the top-k most relevant tables may not include sufficient information for models to derive the correct SQL. Therefore, to better understand models' SQL-generation capability, we further introduce a retrieval-free setting. Under this setting, the generative LLMs are provided with only the tables necessary for constructing the correct SQL queries. In the experiments, we regard the necessary tables as tables used in the gold SQL (i.e., gold tables).\nDatasets. We adapted the Spider and Bird datasets to make them more suitable for the retrieval setting. In these two datasets, tables are organized by topic, and each topic has its own database (around 5.03 tables per database) and queries that can be answered. To make these datasets suitable for the retrieval setting, we aggregated tables from all databases to construct a centralized table corpus for each dataset, resulting in 81 tables for Spider and 75 tables for Bird."}, {"title": "3.2 Evaluation metrics", "content": "End-to-end pipeline. Consider a user input that includes a user question q and a database D. q is provided to generative LLMs. Moreover, as described in Section 3.1, LLMs are also provided with either the gold tables in case of retrieval-free setting or the top-k (k = 10 in our case) tables T retrieved from D in the case of retrieval setting. LLMs then generate an SQL statement s. We denote the gold SQL statement as s* and the gold tables as T*.\nPerformance of table retrieval. The standard method for evaluating retriever models is to compare T and T* to compute recall @ top-k. Yet, this metric might not be informative enough. We notice that a SQL statement is unlikely to be generated correctly without all tables in T* provided in the input. Therefore, in addition to recall@k, we also measure the percentage of questions where the retriever can achieve perfect recall@k (i.e., recall@k = 1).\nPerformance of SQL generation. Execution accuracy (Yu et al., 2018; Li et al., 2024) is used to evaluate the end-to-end performance of the predicted SQL statement. Both the predicted SQL statement s and gold SQL statement s* were executed, producing outputs o and o*, respectively. Execution accuracy is 1 if o is the same as o* and 0 otherwise."}, {"title": "3.3 Overall performance", "content": "Table retrieval performance. From Table 2, we observe that recall@k and percentage of questions (%Q) with perfect recall@k on BEAVER is generally the worst. Recall@10 is 37.5% lower and %Q with perfect recall@10 is 55.9% lower on BEAVER compared to the average performance on Spider and Bird across all retriever models. Compared to Fiben, the more challenging dataset on public databases, recall@10 and %Q with perfect recall@10 is still 6.1% and 1.0% lower on average across all retriever models. This shows that identifying the correct set of tables that contain information to help answer the user question is much more challenging in an enterprise database.\nExecution accuracy under retrieval setting. As seen from Table 3, the end-to-end execution accuracy on BEAVER under the retrieval setting is the lowest across all datasets. Across Llama3 (70B and 8B) and GPT-3.5 Turbo models, performance on BEAVER is, on average, 56.6% lower than"}, {"title": "4 Error analysis", "content": "Section 3 provided an overview of the performance of off-the-shelf LLMs on BEAVER, indicating their limited capabilities of conducting text-to-SQL in a real-world enterprise setting. In this section, we discuss in detail the error sources during both table retrieval and SQL generation phases by examining all questions in our dataset. For table retrieval, we examined the performances of the best-performing retriever model UAE-Large-V1. For SQL generation, we inspected the performance of the two best-performing models (GPT-40 and Llama3-70B-Instruct) on our dataset."}, {"title": "4.1 Table retrieval phase", "content": "We observe three major error sources in table retrieval. Firstly, the retrieval model may not recall the set of tables with sufficient information to answer the user question (89.1%). For instance, given the user question \"What is the room, floor, building street address, city, state, and postal code of [person name]'s office?\" and a table corpus including the four tables shown in Figure 5, UAE retrieved table FAC_BUILDING_ADDRESS to cover \u201ccity, state, and postal code\", table FAC_BUILDINGS to cover \"building street address\", and table EMPLOYEE_DIRECTORY to cover \u201cperson name\". However, UAE did not retrieve table FAC_ROOMS to cover \"room, floor\".\nSecondly, the retrieval model may miss connecting tables (6.52%). This occurs when models retrieved a set of tables that can cover information in the user question, but they might not be connected through"}, {"title": "4.2 SQL generation phrase", "content": "In section 3, we discussed two settings of SQL generation: retrieval and retrieval-free. In the retrieval-free setting, only gold tables were provided to generative LLMs, and thus, models only need to leverage all input information to generate the correct SQL and do not need to filter irrelevant information or determine if given information is sufficient to answer the user question. Naturally, performance under this setting emphasizes models' SQL generation ability.\nOn the other hand, in the retrieval setting, top-k most relevant tables were provided as inputs to LLMs, yet, not all tables need to be used to answer user questions and some necessary tables might not be retrieved. Thus, not only do LLMs need to leverage their abilities to generate SQL statements, they also need to retrieve the most appropriate subset of tables from the input tables that include sufficient information to answer user questions. Therefore, the retrieval setting further requires retrieval ability.\nFor end-to-end table retrieval, models are given top-k most relevant tables and need to retrieve a subset of tables actually needed to answer user questions, which demonstrates their retrieval ability. We observe models not selecting tables to include sufficient information to answer user question to be the primary issue. For example, given the user question \"Show the department name, the course number, course descriptions, and the total number of courses per department.\" and top-k tables shown in Figure 7, GPT-40 retrieved table COURSE_DESCRIPTION to cover \u201ccourse descriptions\" and table DEPARTMENT to cover \u201cdepartment name\u201d, but did not retrieve table SUBJECT_CODE to cover \u201ccourse number and total number of courses\"."}, {"title": "5 Conclusion", "content": "Text-to-SQL is essential to bridge the gap between natural language question answering and table querying. The performance of off-the-shelf LLMs on existing datasets seems to suggest strong performance. However, existing datasets are not reflective of real-world enterprise settings and thus do not reflect LLMs capabilities on enterprise queries. The enterprise setting differs from existing public settings as it includes unseen domain-specific knowledge, a large number of tables that require an intermediate retrieval stage, higher query complexity, and schema complexity. As evidenced by results, the enterprise queries bring significant challenges to off-the-shelf models both in terms of table retrieval and SQL generation. We hope this serves as the foundation for future work that examines large-scale and complex text-to-SQL.\nIn this paper, we have only benchmarked on off-the-shelf LLMs with standard prompting techniques. As for future directions, we will conduct more in-depth analysis to understand models' generalizability to domain shifts using more metadata annotations and SOTA text-to-SQL techniques."}]}