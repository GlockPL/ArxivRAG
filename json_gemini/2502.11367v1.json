{"title": "Sparse Autoencoder Features for Classifications and Transferability", "authors": ["Jack Gallifant", "Shan Chen", "Kuleen Sasse", "Hugo Aerts", "Thomas Hartvigsen", "Danielle S. Bitterman"], "abstract": "Sparse Autoencoders (SAEs) provide potentials for uncovering structured, human-interpretable representations in Large Language Models (LLMs), making them a crucial tool for transparent and controllable AI systems. We systematically analyze SAE for interpretable feature extraction from LLMs in safety-critical classification tasks\u00b9. Our framework evaluates (1) model-layer selection and scaling properties, (2) SAE architectural configurations, including width and pooling strategies, and (3) the effect of binarizing continuous SAE activations. SAE-derived features achieve macro F1 > 0.8, outperforming hidden-state and BoW baselines while demonstrating cross-model transfer from Gemma 2 2B to 9B-IT models. These features generalize in a zero-shot manner to cross-lingual toxicity detection and visual classification tasks. Our analysis highlights the significant impact of pooling strategies and binarization thresholds, showing that binarization offers an efficient alternative to traditional feature selection while maintaining or improving performance. These findings establish new best practices for SAE-based interpretability and enable scalable, transparent deployment of LLMs in real-world applications.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have transformed natural language processing (NLP), demonstrating impressive performance on diverse tasks and languages, even in knowledge-intensive and safety-sensitive scenarios (Hendrycks et al., 2023; Ngo et al., 2022; Cammarata et al., 2021). However, the internal decision-making processes of LLMs remain largely opaque (Cammarata et al., 2021), raising concerns about trustworthiness and oversight, especially given the potential for deceptive or unintended behaviors. Mechanistic interpretability (MI), the study of the internal processes and"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Interpretable Feature Extraction", "content": "MI has evolved from neuron-level analysis to sophisticated feature extraction frameworks (Olah et al., 2020; Rajamanoharan et al., 2024). Early approaches targeting individual neurons encountered fundamental limitations due to polysemanticity, where activation patterns span multiple, often unrelated concepts (Bolukbasi et al., 2021; Elhage et al., 2022). While techniques like activation patching (Meng et al., 2022) and attribution patching (Syed et al., 2023) offered insights into component-level contributions, they highlighted the need for more comprehensive representational frameworks.\nSAEs address these limitations by providing more interpretable feature sets (Bricken et al., 2023; Cunningham et al., 2023). Recent scaling efforts have demonstrated SAE viability across LLMs from Claude 3 Sonnet (Templeton et al., 2024) to GPT-4 (Gao et al., 2024) with extensions to multimodal architectures like CLIP (Bhalla et al., 2024). Although these studies have revealed interpretable feature dimensions and computational circuits (Marks et al., 2024; Zhao et al., 2024), they focus mainly on descriptive feature discovery rather than systematic evaluation of their downstream applications. Our work bridges this gap by providing standardized evaluation frameworks for SAE-based classification and cross-modal transfer, establishing quantitative metrics and methods for feature utility across diverse tasks."}, {"title": "2.2 SAE-Based Classification and its Limitations", "content": "Reports have demonstrated that SAE-derived features can outperform traditional hidden-state probing for classification, particularly in scenarios with noisy or limited data with closed datasets (Anthropic Interpretability Team, 2024) or simplified tasks (Kantamneni et al., 2024). However, more recent studies, such as Wu et al. (2025), suggest that SAEs may not be superior, particularly for model steering (instead of classification). These seemingly conflicting results highlight a critical gap in the current understanding of SAE-based classification: a lack of systematic exploration of how hyperparameters, feature aggregation strategies, and other methodological choices impact performance.\nExisting evaluations often focus on narrow settings, making it unclear whether discrepancies arise from task differences, dataset choices, or specific configurations. This work addresses this gap by systematically evaluating SAE-based classification. We examine key hyperparameters and methodological choices like feature pooling, layer selection, and SAE width across diverse datasets and tasks, ensuring a fair comparison with established baselines."}, {"title": "3 Preliminaries", "content": "Notation and Setup: Let $\\mathcal{M}$ be a pretrained LLM with hidden dimension $d$. When $\\mathcal{M}$ processes an input sequence of tokens of length $n$, it produces hidden representations ${h_1, h_2, ..., h_n}$ for each layer, where each $h_t \\in \\mathbb{R}^d$. We consider three versions of Gemma 2 models (Team et al., 2024) in this work, the 2B, 9B and instruction-tuned variant, 9B-IT.\nSAE-Based Activation Extraction: We use pretrained SAEs provided by Gemma Scope (Lieberum et al., 2024), choosing the SAE with $L_0$ loss closest to 100. We extract each token's residual stream activations from layers that have been instrumented with the SAELens (Joseph Bloom and Chanin, 2024) tool. Specifically for the 2B model, we extract SAE features from layers 5, 12, 19 (early, middle, late) where 9B & 9B-IT models with layers 9, 20, and 31 from the residual stream.\nEach SAE has a designated width (i.e., number of feature directions). We evaluate 16K and 65K widths for the 2B model, and 16K and 131K for 9B and 9B-IT \u00b2, following the pretrained SAEs made available in Gemma Scope (Lieberum et al., 2024). Note: we do not train any SAEs ourselves; our workflow involves only extracting the hidden states and the corresponding pretrained SAE activations.\nPooling and Binarization Since SAEs generate token-level feature activations, an essential step in classification is aggregating these activations into a fixed-size sequence representation. Without pooling, the model lacks a structured way to combine token-level representations. Previous NLP works have explored various pooling strategies for feature aggregation in neural representations (Shen et al., 2018). However, it remains unclear which pooling method is most effective for LLMs' SAE features. We systematically evaluate different pooling approaches (displayed in 2, considering (1) Top-N"}, {"title": "4 Classification Tasks, Multimodal Transfer, and Hyperparameter Analysis", "content": "Here, we investigate best practices for using GemmaScope SAE features in classification tasks across model scale, SAE width, layer depth, pooling strategies, and binarization. We also briefly touch upon the cross-modal applicability of text-trained SAE features to a PaliGemma 2 vision-language model.\nDatasets: We evaluate performance on a suite of open-source classification benchmarks, including binary safety tasks (jailbreak detection, election misinformation, harmful prompt detection) and multi-class scenarios (user intent classification, scenario understanding, abortion intent detection from tweets, banking queries classification). Detailed dataset characteristics are in Appendix A.1."}, {"title": "4.1 Impact of Layer Depth and Model Scale", "content": "We evaluate gemma-2-2b, 9b, and 9b-it, using their early, middle, and late layers, with SAE widths of 16K/65K for gemma-2-2b and 16K/131K for gemma-2-9b and 9b-it, using different pooling strategies.\nWe extract token-level SAE features and train LR classifiers, comparing the results to TF-IDF and final-layer hidden-state baselines \u2075.\nFigure 3(a) depicts the layer-wise performance for the three model scales across our text-based classification tasks. We observe:\n\u2022 Layer Influence: Middle-layer activations typically produce slightly higher F1 scores than early- or late-layer features, indicating that mid-level representations strike a useful balance between semantic and syntactic information for classification tasks.\n\u2022 Model Scale: Larger models (9B, 9B-IT) achieve consistently higher mean performance (above 0.85 F1) compared to the 2B model. This aligns with the general larger hidden dimension in these models facilitating richer representations."}, {"title": "4.2 Pooling Strategies and Binarization", "content": "We next examine pooling and binarization strategies. Token level max activation pooling methods included no max pooling (top-0), top-20, and top-50 features per token. Binarization is applied after token aggregation.\nFigure 3(b) compares two feature selection strategies: (1) no max pooling with summation of all SAE features, and (2) selecting the top-N token level activations (here, 20 and 50), with and without binarization. LR classifiers are trained on the resulting features.\n\u2022 Binarization: Binarized and no max pooling of SAE features outperform both hidden-state probes and bag-of-words (dotted lines in Figure 3(b)). This indicates the effectiveness of SAE features, particularly when combined with binarization, for capturing relevant information.\n\u2022 Token level top-N Selection: Can outperform the binarized and no max pooling approach in certain settings, especially when N increases, and not binarized. However, the margin is typically small, and top-N selection demands additional computation to identify discriminative features.\nThese observations motivate our decision to adopt binarized and no max pooling as a default due to the reduced computational overhead whilst maintaining performance, while acknowledging that token-level top-N might excel for certain tasks.\nInterpretability and Layer-Wise Insights: We find that middle-layer SAE features often produce the highest accuracy across tasks. This trend echoes prior work suggesting that intermediate layers encode richer, more compositional representations than either early or late layers. Crucially, we find that binarizing the full set of SAE features offers a robust one-size-fits-all approach, whereas selecting a top-N subset can yield slightly higher performance but requires additional computational steps. From an interpretability perspective, the binarization strategy also grants a straightforward notion of \"feature activation\": whether or not a feature dimension was triggered above zero. Such a thresholding approach can facilitate more useful and usable feature-level analyses and potential explanations for model decisions."}, {"title": "4.3 Cross-Modal Transfer of Text-Trained SAE Features", "content": "Finally, we conduct a preliminary investigation into the cross-modal applicability of SAE features trained on text. Specifically, we tested whether features useful for text classification could also be beneficial in a vision-language setting.\nExperimental Setup: Instead of using text-based Gemma models directly, we use a Gemma-based"}, {"title": "5 Multilingual Classification and Transferability", "content": "This section evaluates the cross-lingual robustness of SAE features. We investigate whether features extracted from multilingual datasets are consistent with those found in monolingual contexts and explore the correlation between SAE feature transferability and cross-lingual prediction performance.\nWe conduct three primary experiments: (1) comparing native and cross-lingual transfer, (2) evaluating different feature selection methods, and (3) assessing the impact of training data sampling.\nDataset: We use the multilingual toxicity detection dataset (Dementieva et al., 2024), which contains text in five languages labeled with a binary toxicity label: English (EN), Chinese (ZH), French (FR), Spanish (ES), and Russian (RU)."}, {"title": "5.1 Native vs. Cross-Lingual Transfer", "content": "We first investigate the performance of SAE features when training and testing on the same language (native) versus training on one language and testing on another (cross-lingual).\nExperimental Setup: Following the best configurations from previous Section, we extract SAE features from gemma-2-9b and 9b-it (widths of 16K or 131K). We train linear classifiers on one language's data and test on the same or a different language. We also compare against a simpler SAE feature selection approach, the top-n mean-difference baseline (Mean-Diff), to determine if the entire feature set is necessary.\nResults and Discussion: Figure 4 presents the F1 scores. Pink bars show native SAE training, gold bars show English-trained models tested on other languages, and green bars show English-translated models tested on translated inputs Key findings:\n\u2022 Native Training Superiority: Native training consistently yields the highest F1 scores (e.g., EN \u2192 EN can reach over 0.99 F1).\n\u2022 English Transfer Effectiveness: Transferring SAE features trained on English (gold bars) achieves reasonable performance on ES, RU, and DE, but with a 15-20% F1 score decrease compared to native training. This indicates some cross-lingual features generalization internally inside of the models.\n\u2022 Direct Transfer Outperforms Translation: Translating foreign language inputs into English before classification does not outperform direct training on the original language data. Native language signals can be effectively transferred into a shared SAE feature space, proving valuable even without explicit translation."}, {"title": "5.2 Feature Selection Methods: Full SAE vs. Hidden States vs. Mean-Diff", "content": "Experimental Setup: We compare feature selection methods on gemma-2-9b and 9b-it, analyzing performance across different layers using: all SAE features (with binarization), last token hidden-state probing (baseline), and the top-N mean-difference (Mean-Diff) approach.\nResults and Discussion: Figure 5 shows the average F1 scores across layers.\u2076 SAE features achieve the highest macro F1 scores but exhibit greater variance, particularly due to DE \u2192 ZH transfer. Despite this, they remain the most preferable choice due to their superior peak performance. Hidden-state probing performs competitively with lower variance but does not reach the highest scores, making it a more stable alternative. Meanwhile, Mean-Diff top-N selection (Top-10, Top-20, Top-50) consistently lags behind SAE features and hidden states, offering similar variance but lower effectiveness, reinforcing the benefit of using the full SAE feature set.\u2077\nWe then examine the robustness of SAE feature extraction with varying amounts of training data."}, {"title": "6 Behavioral (Action) Prediction", "content": "This section examines whether smaller models can predict the output correctness (\"action\") of larger, instruction-tuned models in knowledge-intensive QA tasks. This relates to scalable oversight, where a smaller, interpretable model monitors a more capable system. We focus on predicting the 9B-IT model's behavior using features from smaller models and assess the impact of context fidelity.\nGoal and Motivation: We aim to determine whether smaller and/or base models (Gemma 2-2B, 9B) can predict their own behavior or that of a larger and/or fine-tuned model (9B-IT) on knowledge-based QA tasks, based on correct or incorrect factual information. This aligns with a scalable oversight scenario, where a smaller model monitors a more capable system when they share the same corpus and architecture.\nDatasets: We use the entity-based knowledge conflicts in question answering dataset (Longpre et al., 2022), which provides binary correctness labels for model responses. Open-ended generation is performed with vllm (Kwon et al., 2023), and answers are scored using inspect ai (AI Safety Institute) with GPT-40-mini as the grader."}, {"title": "7 Conclusion", "content": "We present a comprehensive study of SAE features across multiple model scales, tasks, languages, and modalities, highlighting both their practical strengths and interpretive advantages. Specifically, summation-then-binarization of SAE features surpassed hidden-state probes and bag-of-words baselines in most tasks, while demonstrating cross-lingual transferability. Moreover, we showed that smaller LLMs equipped with SAE features can effectively predict the actions of larger models, pointing to a potential mechanism for scalable oversight and auditing. Taken together, these results reinforce the idea that learning (or adopting) a sparse, disentangled representation of internal activations can yield significant performance benefits and support interpretability objectives.\nWe hope this work will serve as a foundation for future studies that exploit SAEs in broader multimodal, diverse languages, and complex real-world workflows where trust and accountability are paramount. By marrying strong classification performance with clearer feature-level insights, SAE-based methods represent a promising path toward safer and more transparent LLM applications."}, {"title": "8 Limitations", "content": "While our study demonstrates the effectiveness of SAE features for classification and transferability, several limitations remain.\nDependence on Gemma 2 Pretrained-SAEs Our analysis is restricted to SAEs trained with Jump ReLU activation on Gemma 2 models. This limits generalizability to other model architectures and training paradigms. Future work should explore diverse SAE training strategies and model sources.\nLimited Multimodal and Cross-Lingual Evaluation Our cross-modal experiments are preliminary, and further research is needed to validate SAE generalization across different modalities and low-resource languages.\nSensitivity to Task and Data Distribution SAE performance varies across datasets, and its robustness under adversarial conditions or domain shifts needs further study.\nInterpretability Challenges Despite improved feature transparency, the semantic alignment of SAE features with human-interpretable concepts remains an open question."}]}