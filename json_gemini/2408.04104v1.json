{"title": "Hardware-Assisted Virtualization of Neural\nProcessing Units for Cloud Platforms", "authors": ["Yuqi Xue", "Yiqi Liu", "Lifeng Nai", "Jian Huang"], "abstract": "Cloud platforms today have been deploying hardware\naccelerators like neural processing units (NPUs) for powering\nmachine learning (ML) inference services. To maximize the\nresource utilization while ensuring reasonable quality of service,\na natural approach is to virtualize NPUs for efficient resource\nsharing for multi-tenant ML services. However, virtualizing NPUs\nfor modern cloud platforms is not easy. This is not only due to\nthe lack of system abstraction support for NPU hardware, but\nalso due to the lack of architectural and ISA support for enabling\nfine-grained dynamic operator scheduling for virtualized NPUs.\nWe present TCloud, a holistic NPU virtualization framework.\nWe investigate virtualization techniques for NPUs across the\nentire software and hardware stack. TCloud consists of (1) a\nflexible NPU abstraction called vNPU, which enables fine-grained\nvirtualization of the heterogeneous compute units in a physical\nNPU (pNPU); (2) a vNPU resource allocator that enables pay-as-you-go computing model and flexible vNPU-to-pNPU mappings\nfor improved resource utilization and cost-effectiveness; (3) an\nISA extension of modern NPU architecture for facilitating fine-\ngrained tensor operator scheduling for multiple VNPUs. We\nimplement TCloud based on a production-level NPU simulator.\nOur experiments show that TCloud improves the throughput of\nML inference services by up to 1.4\u00d7 and reduces the tail latency\nby up to 4.6x, while improving the NPU utilization by 1.2x on\naverage, compared to state-of-the-art NPU sharing approaches.", "sections": [{"title": "I. INTRODUCTION", "content": "Machine learning (ML) is becoming the backbone for\nmany popular ML services, such as online recommendation\nand natural language processing [4], [7], [46], [49]. To\naccelerate these ML services, cloud platforms have employed\nhardware accelerators like neural processing units (NPUs) as\nthe mainstream compute engine [8], [15], [17], [21], [25], [26].\nNPUs are highly specialized to accelerate the common\noperations in deep neural networks (DNNs), such as matrix\nmultiplication and convolution. A typical NPU device is a\nperipheral board with multiple NPU chips, and each chip has\nmultiple NPU cores. Each NPU core has matrix engines (MEs) that leverage systolic arrays to perform matrix multiplications\nand vector engines (VEs) for generic vector operations. A\nwell-known example is the Google Cloud TPU [21].\nA common approach to using NPUs in cloud platforms\nis to assign an entire NPU chip to a single ML application\ninstance in a virtual machine (VM) or container via PCIe\npass-through [49]. However, this disables resource sharing\nand causes severe resource underutilization of NPUs. For\ninstance, prior studies [57] disclosed that a majority of the\nDNN inference workloads cannot fully utilize TPU cores, due\nto their imbalanced demands on MEs and VEs. Many DNN\nworkloads have diverse demands on the number of MEs and\nVEs (see \u00a7II-B). As a result, the one-size-fits-all approach is\nmuch less attractive for cloud platforms.\nTo address the utilization challenge and ease the resource\nmanagement for cloud platforms to accommodate diverse work-\nload demands, it is desirable to virtualize hardware devices and\nenable resource sharing among multiple tenants. Unfortunately,\nmodern cloud platforms have very limited virtualization support\nfor NPUs across the software and hardware stack.\nLack of system abstraction support for NPUs. Unlike the sys-\ntem virtualization of multi-core processors [3], [10], NPUS\nhave unique heterogeneous compute resources (i.e., MES\nand VEs). To circumvent this complexity, cloud platforms\ntoday expose homogeneous NPU cores to the user VMs.\nHowever, the existing abstraction at the NPU core level is too\ncoarse-grained, as user workloads may have diverse resource\nrequirements. We need a flexible system abstraction that allows\nusers to specify the ME/VE resources following the pay-as-you-go model [50]. Such an abstraction will simplify the NPU\nmanagement for cloud platforms, including NPU resource\n(de)allocation, resource mapping, and scheduling. Prior studies\ninvestigated the system virtualization for FPGAs [6], [34],\n[35], [61], [62] and GPUs [27], [56]. However, they cannot be\ndirectly applied to NPUs, as they target different architectures.\nLack of architectural support for NPU virtualization. Prior\nstudies enabled the time-sharing of an NPU device at\nthe task level, and support the preemption for prioritized\ntasks [12], [13]. However, the coarse-grained time-sharing\non the shared NPU board still suffers from severe resource\nunderutilization, due to the lack of support of concurrent\nexecution of multi-tenant workloads. Existing NPU sharing\napproaches either sacrifice isolation or suffer from high\npreemption overhead [16]. V10 [57] enabled NPU sharing\nbetween multiple DNN workloads. However, it is still based\non the time-sharing mechanism and suffers from operator\ninterference between multi-tenant ML instances, resulting in\npoor performance isolation. As we move towards fine-grained\nNPU virtualization, we need architectural support to achieve\nboth improved performance isolation and NPU utilization.\nLack of ISA support for virtualized NPUs. To simplify the\nhardware design, NPUs commonly employ VLIW-style ISAs,\nand the ML compiler explicitly exploits the parallelism of the\ncompute units [5], [29], [33]. However, this requires the number\nof compute units to be explicitly specified at the compilation\nstage, and the number cannot be changed at runtime. In this\ncase, the VLIW ISAs unnecessarily couple control flows of the\ncompute units (i.e., MEs). Even though some compute units of\na shared NPU become available, they cannot be utilized by the\nactive workload (except recompiling the DNN program). This is\ncaused by the fundamental tussle between dynamic scheduling\nand VLIW ISAs. As the collocated ML instances have various\ndemands on compute units at runtime, this limitation inevitably\ncauses either NPU underutilization or performance interference.\nWe need to rethink the NPU ISA design to facilitate dynamic\nresource scheduling for virtualized NPUs.\nIdeally, we wish to virtualize NPUs to enable flexible and\nfine-grained resource sharing and scheduling for improved NPU\nutilization and performance isolation. We present TCloud, a\nhardware-assisted system virtualization framework for NPUs.\nOur contributions. We first develop a simple yet flexible vNPU\nabstraction. We use VNPU to create a virtualized NPU device\nfor each ML instance. For each vNPU, the user can specify\nthe number of different types of compute units (MEs/VEs) on-\ndemand or follow the pay-as-you-go model in cloud computing.\nWe propose a new resource allocation mechanism that can\ndecide the optimized VNPU configuration for different ML\nworkloads, based on the analysis using ML compilers. As\ndifferent ML services have various ME/VE demands (see \u00a7II), such an abstraction enables fine-grained resource allocation,\nwhich benefits both end users and cloud platform operators\u00b9.\nTCloud can map vNPUs to physical compute units of NPU\ncores in different manners, based on the service level objectives\n(SLOs) of ML services. To maximize the NPU utilization while\nensuring performance isolation, TCloud enables fine-grained\nspatial sharing with resource harvesting. It also enables the\noversubscription of NPU cores by temporally sharing MEs/VEs\namong multiple vNPUs. Therefore, the idle compute units can\nbe opportunistically utilized by collocated workloads.\nTo facilitate the dynamic scheduling for collocated VNPUs,\nTCloud extends the VLIW-style ISA by reorganizing VLIW\ninstructions into independent micro-Tensor operators (\u00b5TOps in\n\u00a7III). TCloud introduces necessary architectural logic for fine-\ngrained dynamic scheduling of \u00b5TOps on the shared physical\nNPU cores. It allows one vNPU to harvest available compute\ncycles of MEs/VEs from collocated vNPUs, without causing\nmuch interference. This is impossible with conventional VLIW-\nstyle ISAs, as they strictly couple the control flows of the\n(statically) allocated compute units. Our new architectural\nsupport enables TCloud to offer the flexibility of NPU resource\nallocation and scheduling across the software (i.e., vNPU\nabstraction) and hardware (i.e., fine-grained \u00b5TOp scheduling)\nstack. TCloud requires minimum modifications to NPU chips\n(0.04% die area cost) as well as ML compilers.\nWe implement TCloud with a production-level NPU simu-\nlator following the typical TPU architecture. We collect the\ntraces of ML services as we run the MLPerf benchmarks [48]\nand the TPU reference models [23] on the real Google TPUs.\nOur experiments with multi-tenant ML instances show that\nTCloud can improve the throughput of ML inference services\nby up to 1.4x and reduce the tail latency by up to 4.6x,\nwhile improving the NPU utilization by 1.2\u00d7 on average, in\ncomparison with state-of-the-art NPU sharing approaches. We\nsummarize the contributions of TCloud as follows:\n\u2022\nWe conduct a thorough study of DNN inference workloads\non real NPU hardware, and investigate the NPU virtualization\nchallenges within both system and hardware stack (\u00a7II).\n\u2022\nWe propose a new system abstraction named vNPU for\nenabling fine-grained virtualization of the heterogeneous\ncompute units in NPU cores (\u00a7III-A).\n\u2022\nWe present a new NPU resource allocation scheme and enable\nflexible VNPU-to-pNPU mappings (\u00a7III-B and \u00a7III-C).\n\u2022\nWe extend the VLIW-style ISAs and NPU architecture for\nenabling fine-grained dynamic scheduling of virtualized\nNPUs for multi-tenant ML services (\u00a7III-D and \u00a7III-E).\n\u2022\nWe evaluate the efficiency and flexibility of our NPU\nvirtualization framework with real-world DNN traces (\u00a7V)."}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "A. NPU System Architecture\nAs shown in Figure 1, an NPU board has multiple NPU\nchips, each chip has multiple NPU cores, each core is connected\nto an off-chip HBM. An NPU core has two types of compute\nunits: matrix engines (MEs) that perform matrix multiplications\nwith systolic arrays; and vector engines (VEs) that perform\ngeneric vector operations. Each NPU core employs an on-chip\nSRAM to hide HBM access latency. A typical example of\nNPU architecture in production is Google TPU [32].\nTo run a DNN program on NPUs, ML compilers [14],\n[22], [47] generate a sequence of tensor operators, which are\nthen translated into device-specific machine instructions. An\nNPU core usually uses a VLIW-style ISA for simplifying the\nhardware. Each instruction contains multiple ME slots, VE\nslots, load/store slots for accessing the SRAM, and other slots\n(e.g., for DMA operations with HBM). The ML compilers can\nexploit the instruction-level parallelism with the knowledge of\nunderlying compute resource, such as the numbers of MEs/VEs.\nB. Characterization of ML Inference Services\nTo motivate NPU virtualization, we conduct a study of\nresource demands of ML inference workloads and their impact\non NPU utilization. We run various ML inference workloads\nfrom MLPerf benchmarks [48] and official TPU reference\nmodels [23] (see Table I), on a real Google TPUv4 board with\n8 cores. Each core has four MEs and two VEs. We profile\nthe number of MEs/VEs demanded by each workload with\nML compiler techniques, and the resource utilization with\nperformance counters on the TPU core. We vary the batch size\n(8 by default). The HBM footprint of benchmarks ranges from\n10.59MB to 22.38GB, which does not fully occupy the HBM on\nmodern NPU chips (e.g., 32GB/96GB on TPUv4/TPUv5p [53]).\nWe report the resource utilization on one TPU core, as all cores\nperform identical computations with data parallelism.\nDiverse demands on MEs/VEs. An ML inference workload\ncan have diverse resource demands over time, as different\noperators in a DNN model have vastly different demands\non MEs and VEs. For each workload, we analyze the DNN\nexecution graph generated by the ML compiler. By default,\nthe ML compiler picks the number of compute units for each\noperator to maximize the overall efficiency of the compute units"}, {"title": "C. NPU Virtualization: Challenges and Opportunities", "content": "System virtualization offers the opportunity for supporting\nmulti-tenancy and improving resource utilization. However,\nvirtualizing NPUs suffers from unique challenges.\nNew abstraction required for fine-grained virtualization.\nAs none of prior studies investigated NPU virtualization, it\nis unclear how the virtualized NPUs should be exposed to\napplication instances. By virtualizing NPUs, we need to provide\na simple yet effective abstraction, which can provide sufficient\nflexibility for users to specify the numbers of MEs and VES\nbased on the workload demand and target SLOs (see \u00a7III-B).\nFor instance, we should allocate more MEs than VEs to an\nME-intensive workload, and vice versa.\nHowever, even if we can allocate the most appropriate\nnumbers of MEs and VEs, the allocated resources still cannot\nbe fully utilized, due to the diverse resource demands of\ndifferent operators over time. A static allocation of MEs and\nVEs is insufficient. Instead, we need to enable dynamic resource\nscheduling. We should allow one workload to \"harvest\" the\nunderutilized compute units allocated to other workloads for\nimproving the overall utilization of the NPU core and the\nQuality-of-Service (QoS) of collocated ML inference services.\nUnfortunately, current NPU architectures do not support such\nfine-grained resource scheduling and harvesting.\nISA limitations for enabling virtualized NPU scheduling.\nThe fundamental limitations of modern NPU architectures\nprevent dynamic resource scheduling. To simplify the hardware\ndesign of NPUs, developers usually employ VLIW-style ISAs,\nand utilize ML compilers to exploit the instruction-level\nparallelism. However, the statically scheduled ISAs cannot fully\nexploit the hardware resources at runtime. They unnecessarily\ncouple the control flows of all MEs in a tensor operator, even\nthough different MEs can execute independently. As shown in\nFigure 8, the original VLIW program must execute each VLIW\ninstruction sequentially, creating false dependencies between\noperations on different MEs even though they do not have any\ntrue data dependencies. As the compiler explicitly specifies\nhow many MEs are being used, the allocated MEs cannot be\nchanged at runtime unless the DNN program is recompiled.\nFor example, if the compiler generates push/pop operations\nfor two MEs, these operations cannot be time-multiplexed on a\nsingle ME, since this will corrupt the intermediate states in the\nME. Hence, if only one ME is available, this DNN program\ncannot run until at least two MEs are available (Figure 9 left).\nIt also cannot utilize more than two MEs, even if more than\ntwo are available (Figure 9 right), because the push/pop\noperations for one ME share the intermediate data in this ME.\nTo address this problem and enable dynamic ME scheduling,\none may consider switching from VLIW to another ISA (e.g.,\nRISC) or employing superscalar out-of-order (OoO) execution\n(similar to a CPU core). However, they still lack the support\nfor dynamic ME scheduling since the compiler still needs to\nspecify which ME is the target of a push/pop instruction\nstatically. To remove such a constraint, we need to offer the\nflexibility for the NPU program to determine the target ME at\nruntime. Therefore, we need to rethink the contract between\nthe compiler and the NPU hardware by extending the ISA.\nArchitectural support for parallelizing ME/VE operations.\nOur key observation is that the execution of different MEs and\nVEs in a tensor operator is usually separable. Specifically, most\nDNN operators, such as matrix multiplication (MatMul) and\nconvolution, are partitioned by DNN compilers [14], [64] into\nmultiple tiles that can be computed independently. As shown\nin Figure 8, the original program computes a MatMul tile and\ndirectly applies a ReLU function to the results using 2 MEs and"}, {"title": "III. DESIGN AND IMPLEMENTATION", "content": "We design TCloud to achieve the following objectives:\n\u2022 Allocation flexibility: As DNN workloads have different\nresource and SLO requirements, we need to provide the\nflexibility for users to customize their NPU hardware.\n\u2022 NPU utilization: Since an individual ML inference workload\nunderutilizes NPU cores (\u00a7II-B), we need to enable fine-\ngrained NPU virtualization for improved NPU utilization.\n\u2022 Performance isolation: As we collocate DNN workloads on\nthe same NPU core, we must provide performance isolation.\nWe first present a new vNPU abstraction for NPU virtualiza-\ntion (\u015eIII-A). Based on this, we enable flexible vNPU resource\nallocation (\u015eIII-B) and vNPU-to-pNPU mappings (\u00a7III-C). We\nextend VLIW-style ISA (SIII-D) and NPU architecture (\u00a7III-E)\nfor enabling fine-grained resource scheduling for vNPUs.\nA. vNPU: The New Abstraction for NPU Virtualization\nWe design the vNPU abstraction with the goals of (1)\nallocating NPU hardware resource to a vNPU instance on\ndemand; (2) hiding the complexity from the ML programs with\nminimal changes to the guest software stack for compatibility.\nVNPU abstraction. A vNPU instance reflects the hierarchy\nof a physical NPU board. Figure 10 shows the configurable\nparameters of a vNPU. Each vNPU is exposed to the VM as a\nPCIe device. The guest NPU driver can query the hierarchy of\nthe vNPU, such as the number of chips, cores per chip, HBM\nsize, and others. The maximum vNPU size is capped by the\nphysical NPU size. If a guest VM requires more resources than\nis available on a physical NPU board, TCloud can allocate\nmultiple vNPU instances to it. The guest ML framework can\nhandle the data distribution across multiple vNPU cores in the\nsame way as that on physical NPUs. Take Google TPU for\nexample, TensorFlow already handles data parallelism across\nphysical NPUs. It can work in the same way with vNPUs.\nVNPU lifecycle. To create a vNPU instance, a user can\nspecify the vNPU configuration following the pay-as-you-go model [50]. Cloud providers can define various default\nconfigurations (e.g., small/medium/large vNPU cores as having\n1/4/8 MEs/VEs). TCloud can also learn an optimized VNPU\nconfiguration for a DNN workload with ML compilers (\u00a7III-B).\nAs shown in Figure 11, upon vNPU initialization, the guest\ndriver sends a request to the hypervisor through a para-\nvirtualized interface (\u00a7III-F) (1). The vNPU manager maps"}, {"title": "B. VNPU Allocation and Deallocation", "content": "Following the popular pay-as-you-go model [50], cloud plat-\nforms allow users to specify the vNPU configuration on demand.\nHowever, as ML inference workloads have diverse ME/VE\ndemands (see \u00a7II-B), specifying the number of MEs/VEs can\nbe challenging for users who are not NPU experts. Thus, we\nallow them to specify the total number of execution units\n(EUs), which is directly related to the cost of running the\nvNPU instance. TCloud provides the vNPU allocator, a compile-\ntime tool to improve the performance per cost of vNPUs by\nidentifying an optimized ME/VE ratio for the user workload.\nME/VE allocation. The ME/VE demands of a ML workload\ncan be reflected by how it runs on one ME and one VE. We\ndenote the ME active runtime as m, and that of VE as v. These\nnumbers can be obtained via profiling at the compilation stage.\nBased on our study in \u00a7II-B, for most DNN models, at least\none of ME/VE is active during the execution of an NPU core.\nThus, the time portion where only ME is active is $1 - v$, that of only VE is $1-m$, and that of concurrent ME/VE execution\nis $m + v - 1$. With Amdahl's Law, the normalized execution\ntime on $n_m$ MEs and $n_v$ VEs is\n$T = \\frac{1 - v}{n_m} + \\frac{1-m}{n_v} + \\frac{m+v-1}{min(n_m, n_v)}$   (1)\nwhere the concurrent part is bottlenecked by the minority type\nof EU. Let $n_m + n_v$ be the hypothetical speedup regardless\nof EU types, which means an EU can execute both ME and\nVE operators. Compared to real cases where each EU must\nrespect data dependencies and operator types, the hypothetical\nspeedup assumes all $n_m + n_v$ EUs are 100% utilized. Thus,\nthe hypothetical execution time on $n_m$ MEs and $n_v$ VEs is\n$T_h = \\frac{m+v}{n_m+n_v}$, and the total EU utilization can be quantified as\nthe ratio between hypothetical and estimated execution times:\n$U = \\frac{T_h}{T} = \\frac{\\frac{m+v}{n_m+n_v}}{\\frac{1 - v}{n_m} + \\frac{1-m}{n_v} + \\frac{m+v-1}{min(n_m, n_v)}}$   (2)\nTo isolate the impact of total ME and VE quantity, we\nsimplify the function by letting $k = \\frac{n_m}{n_v}$ be the ratio\nbetween the numbers of MEs and VEs. Without loss of\ngenerality, we assume $n_v \\geq n_m$, which means $k \\leq 1$. Then,\nwe can simplify Equation (2) with mathematical tools [38]:\n$U = \\frac{(m+v)k}{(n_m+n_v)(\\frac{1-v}{k} + 1-m + \\frac{m+v-1}{k})}$     $(k \\leq 1)$.    (3)\nTo find the value of k that maximizes U, we compute\nthe value of k where $\\frac{dU}{dk}$ = 0. This gives $k = \\sqrt{\\frac{m}{1 - m}}$\nfor m < 0.5. If m $\\geq$ 0.5, U will be monotonic, so k = 1\nmaximizes U. Similarly, for the case when $n_m \\geq n_v$, we\nderive $k = \\sqrt{\\frac{(1 - v)}{v}}$ for v < 0.5 and k = 1 for $v \\geq$ 0.5.\nConsequently, we have\n$\\frac{n_m}{n_v}$ = $\\begin{cases}\n\\sqrt{\\frac{m}{1 - m}}, & m < 0.5, \\\\\n\\sqrt{\\frac{(1 - v)}{v}}, & v < 0.5, \\\\\n1, & m \\geq 0.5 \\text{ and } v \\geq 0.5.\n\\end{cases}$    (4)\nThe case when both m < 0.5 and n < 0.5 does not exist since\nat least one of ME/VE will be active (m + n $\\geq$ 1). When m <\n0.5, for workloads with ME active time ratio m, we allocate\n$\\sqrt{\\frac{m}{1 - m}}$ times more MEs than VEs. When v < 0.5, for\nworkloads with VE active time ratio v, we approximate the\nallocated ME/VE quantity ratio to $\\sqrt{\\frac{(1 - v)}{v}}$. If m > 0.5\nand v > 0.5, we allocate the same number of MEs and VEs.\nNote that each vNPU will have at least one ME and one VE.\nMemory allocation. Users can use the compiler to estimate the\ntotal HBM capacity needed by a DNN workload. By default,\nthe SRAM capacity is allocated proportionally to the number\nof allocated MEs, as more MEs usually indicate larger tile\nsizes. Based on our study in \u00a7II-B, for many common ML\ninference services, the HBM bandwidth is less of a concern.\nThus, TCloud allows fair sharing of HBM bandwidth by\ndefault. For large models that demand large HBM capacity\nand bandwidth, the vNPU abstraction offers the flexibility for\nend users to allocate the demanded resources. The user may\nalso leverage existing tensor swapping techniques to support\nlarge DNN workloads with limited memory capacity [28], [63]."}, {"title": "C. VNPU Mapping", "content": "The vNPU manager attempts to balance the number of\nallocated EUs and the size of allocated memory. This minimizes\nthe chance that all EUs on one core are allocated but a large\nportion of its memory is not allocated, or vice versa. Thus,\nVNPUs with many EUs and small memory will be collocated\nwith vNPUs with few EUs and large memory. TCloud uses a\ngreedy algorithm for this by default.\nVNPU mapping schemes. TCloud provides the flexibility for\ncloud platforms to enable both hardware-isolated (spatial-\nisolated) mapping and software-isolated (temporal-sharing)\nmapping. With hardware-isolated mapping, a vNPU is mapped\nto dedicated EUs and SRAM, and the allocated hardware is\nnot shared with other vNPUs. With software-isolated mapping,\nmultiple vNPUs can temporally share the same EUs. TCloud\nuses priority-based scheduling for fair sharing and performs\ncontext switches between vNPUs (see \u00a7III-E).\nVNPU mapping policies. TCloud decides which vNPUs can\nbe mapped onto the same physical NPU (pNPU) as follows.\nWith hardware-isolated mapping, TCloud collocates a set of\nVNPUs as long as the total resource requirement (e.g., number\nof MEs/VEs, HBM capacity) does not exceed the pNPU. With\nsoftware-isolated mapping, TCloud aims to load-balance the\nPNPUs while allowing oversubscription. TCloud tracks the total\nresource requirement of assigned vNPUs on each pNPU, and\nassigns a new vNPU to the pNPU that suffers the least resource\nrequirement. TCloud can support other collocation policies [12],\n[41], [57] as well. At scale, TCloud can be integrated with\na cluster-wise VM/container orchestration framework such\nas KubeVirt/Kubernetes [18] to decide which VM should be\nplaced on what machine. Developing advanced vNPU/VM\ncollocation policies is orthogonal to our work.\nVNPU security isolation. TCloud enforces memory address\nspace isolation among collocated vNPUs with the conventional\nmemory segmentation scheme [2], [57] for both HBM and\nSRAM. TCloud divides the SRAM and HBM into fixed-sized\nsegments and maps each segment to the virtual address space of\na vNPU. For the NPU core in Table II, an SRAM/HBM segment\nis 2MB/1GB. There is no external fragmentation since the\nsegment size is fixed. The address translation is performed by\nadding the segment offset to the starting address of the physical\nsegment, which incurs negligible overhead. A page fault will\nbe triggered when an invalid access happens. This is sufficient\nsince ML frameworks like TensorFlow typically request a\ncontiguous chunk of memory for the entire lifetime of an ML\ninference service and have their own memory management\nmechanism. To isolate the vNPU instances as they communicate\nwith the host, TCloud uses IOMMU to enforce DMA remapping\n(\u00a7III-F). We leave side-channel mitigation to future work."}, {"title": "D. ISA Extension for NPU Virtualization", "content": "To support dynamic ME/VE scheduling (\u00a7II-C), we develop\nNeuISA, in which when the ML compiler maps a tensor\noperator onto MEs, it generates \u201csub-tasks\" for each ME,\nso the hardware can decide which \"sub-task\" can be executed\nat runtime based on the availability of MEs. NeuISA is still\nexpressive for compilers to exploit the instruction-level paral-\nlelism between MEs and VEs, and preserve the flexibility of\nsupporting fused operators and complex control-flow structures\nlike branches and nested loops in VLIW-style ISAs.\nSeparating ME control flow with \u00b5TOps. NeuISA decouples\nthe execution of independent MEs in a tensor operator by\nseparating the control flow of each ME and VE into independent\ninstruction sequences (see Figure 8), called micro-Tensor\nOperators (\u00b5TOps). To minimize changes to the existing VLIW\ncompiler and hardware, the instruction format inside a \u00b5TOP\nresembles the original VLIW ISA: an instruction contains\nmultiple slots, and each slot encodes an operation (such as a\npush/pop operation in an ME slot and an ALU operation in\na VE slot). However, the number of ME slots in a NeuISA\ninstruction differs from that of a traditional NPU ISA.\n\u03bc\u03a4\u039fp types. As shown in Figure 13, for a physical NPU core\nwith $n_x$ MEs and $n_y$ VEs, NeuISA defines two types of \u00b5TOps:\n(1) An \u039c\u0395 \u03bc\u03a4Op contains instructions with one ME slot and\n$n_y$ VE slots. An ME \u00b5TOp will only use one ME during\nexecution, which enforces that each \u039c\u0395 \u03bc\u03a4Op only contains\nthe control flow of one ME. To execute an operator on multiple\nMEs, the compiler generates multiple \u039c\u0395 \u03bcTOps. At runtime,\nthe hardware dynamically adjusts the number of MEs assigned\nto this operator by deciding how many \u039c\u0395 \u00b5TOps are being\nexecuted. The VE slots in an \u039c\u0395 \u00b5TOp enable instruction-\nlevel parallelism between MEs and VEs. VE slots are necessary\nbecause the VE needs to aggregate the outputs of the systolic\narray. They also enable operator fusions such as MatMul+ReLU\n(see Figure 8). (2) A VE \u00b5TOp contains instructions with no\nME slot and $n_y$ VE slots, which performs vector operations\nthat do not involve ME computation. The $n_y$ VE slots allow a\nVE \u00b5TOP to utilize all the VEs. Having multiple VE slots in\nan instruction does not increase the hardware complexity since\nthe original VLIW NPU architecture already supports this.\nSupporting fused operators with \u00b5TOp groups. The \u00b5TOps\ncan efficiently support basic tensor operators, such as tiled\nmatrix multiplication with each \u00b5TOp computing a different\ntile. However, ML compilers may generate fused operators that\ncannot be handled by \u00b5TOps alone, e.g., a matrix multiplication\nmay be executed with $n_x$ ME \u00b5TOps, while the succeeding\nfused normalization operator only needs a VE \u00b5TOP.\nTo support a fused operator, NeuISA organizes the \u00b5TOps\ninto a sequence of \u00b5TOp groups to express the dependencies\nbetween \u00b5TOps, as shown in Figure 13. Each group contains\""}, {"title": "E. Architectural Support for NeuISA", "content": "The \u00b5TOp design enables dynamic operator scheduling.\nIt allows a vNPU to harvest unused ME/VEs from other\ncollocated vNPUs in the same physical NPU core at runtime.\nHardware scheduler for NeuISA. Figure 17 shows the\npipeline design for fetching and scheduling \u00b5TOps. The NPU\ncore maintains the contexts of multiple vNPUs, including the\nPC pointers to the program and the vNPU configurations.\nEach time a new \u00b5TOp is ready or an existing \u00b5TOp finishes,\nthe \u00b5TOp scheduler selects the \u00b5TOps to be executed next.\nFor each vNPU, the \u00b5TOp scheduler retrieves the number of\nallocated MEs and the number of ready \u039c\u0395 \u03bc\u03a4Ops from the\nVNPU context. It selects a set of ready \u00b5TOps, and fetch their\ninstructions to the instruction queues.\nNext, the operation scheduler selects which operations from\nthe instruction queues will be executed at every cycle. The ME\noperations from the ME \u00b5TOp instruction queues are directly\nissued to the corresponding MEs. For the VE operations,\nthe scheduler selects which operations to issue from all VE\n\u00b5TOp instruction queues. To reclaim a harvested ME, TCloud\nperforms a context switch to preempt the harvesting \u00b5T\u041e\u0440.\nUpon a context switch, the register file and the intermediate\ndata in the MEs are saved to SRAM, which incurs negligible\noverhead compared to the length of an operator. The number\nof instruction queues should be large enough to support\nsimultaneous execution of all MEs/VEs. For an NPU core\nwith $n_x$ MEs and $n_y$ VEs, there are $n_x$ \u039c\u0395 \u00b5TOp instruction\nqueues and $n_y$ VE \u00b5TOp instruction queues.\n\u03bc\u03a4\u039fp scheduling policy. The \u00b5TOp scheduler can be con-\nfigured in either spatial-isolated or temporal-sharing vNPU\nscheduling mode, as discussed in \u00a7III-C.\nWith spatial-isolated mode, the scheduler aims to ensure\nperformance isolation. First, if a vNPU has $n_x$ MEs and at\nleast $n_x$ ready \u039c\u0395 \u00b5TOps, the scheduler will execute $n_x$ \u039c\u0395\n\u00b5TOps to fully utilize all the allocated MEs for this vNPU.\nIn this case, no MEs will be harvested from this vNPU. If\nthe allocated MEs are already being harvested by \u00b5TOps from\nother vNPUs, these \u00b5TOps will be preempted to reclaim the\nharvested MEs. Second, to improve utilization, if the vNPU\nhas more than $n_x$ ready \u039c\u0395 \u00b5TOps, and if another VNPU\ndoes not have enough \u039c\u0395 \u00b5TOps to utilize all its MEs, the"}]}