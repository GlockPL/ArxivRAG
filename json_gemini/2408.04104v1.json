{"title": "Hardware-Assisted Virtualization of Neural Processing Units for Cloud Platforms", "authors": ["Yuqi Xue", "Yiqi Liu", "Lifeng Nai", "Jian Huang"], "abstract": "Cloud platforms today have been deploying hardware accelerators like neural processing units (NPUs) for powering machine learning (ML) inference services. To maximize the resource utilization while ensuring reasonable quality of service, a natural approach is to virtualize NPUs for efficient resource sharing for multi-tenant ML services. However, virtualizing NPUs for modern cloud platforms is not easy. This is not only due to the lack of system abstraction support for NPU hardware, but also due to the lack of architectural and ISA support for enabling fine-grained dynamic operator scheduling for virtualized NPUs. We present TCloud, a holistic NPU virtualization framework. We investigate virtualization techniques for NPUs across the entire software and hardware stack. TCloud consists of (1) a flexible NPU abstraction called vNPU, which enables fine-grained virtualization of the heterogeneous compute units in a physical NPU (pNPU); (2) a vNPU resource allocator that enables pay-as-you-go computing model and flexible vNPU-to-pNPU mappings for improved resource utilization and cost-effectiveness; (3) an ISA extension of modern NPU architecture for facilitating fine-grained tensor operator scheduling for multiple VNPUs. We implement TCloud based on a production-level NPU simulator. Our experiments show that TCloud improves the throughput of ML inference services by up to 1.4\u00d7 and reduces the tail latency by up to 4.6x, while improving the NPU utilization by 1.2x on average, compared to state-of-the-art NPU sharing approaches.", "sections": [{"title": "I. INTRODUCTION", "content": "Machine learning (ML) is becoming the backbone for many popular ML services, such as online recommendation and natural language processing [4], [7], [46], [49]. To accelerate these ML services, cloud platforms have employed hardware accelerators like neural processing units (NPUs) as the mainstream compute engine [8], [15], [17], [21], [25], [26]. NPUs are highly specialized to accelerate the common operations in deep neural networks (DNNs), such as matrix multiplication and convolution. A typical NPU device is a peripheral board with multiple NPU chips, and each chip has multiple NPU cores. Each NPU core has matrix engines (MEs) that leverage systolic arrays to perform matrix multiplications and vector engines (VEs) for generic vector operations. A well-known example is the Google Cloud TPU [21].\nA common approach to using NPUs in cloud platforms is to assign an entire NPU chip to a single ML application instance in a virtual machine (VM) or container via PCIe pass-through [49]. However, this disables resource sharing and causes severe resource underutilization of NPUs. For instance, prior studies [57] disclosed that a majority of the DNN inference workloads cannot fully utilize TPU cores, due to their imbalanced demands on MEs and VEs. Many DNN workloads have diverse demands on the number of MEs and VEs (see \u00a7II-B). As a result, the one-size-fits-all approach is much less attractive for cloud platforms.\nTo address the utilization challenge and ease the resource management for cloud platforms to accommodate diverse workload demands, it is desirable to virtualize hardware devices and enable resource sharing among multiple tenants. Unfortunately, modern cloud platforms have very limited virtualization support for NPUs across the software and hardware stack.\nLack of system abstraction support for NPUs. Unlike the system virtualization of multi-core processors [3], [10], NPUS have unique heterogeneous compute resources (i.e., MES and VEs). To circumvent this complexity, cloud platforms today expose homogeneous NPU cores to the user VMs. However, the existing abstraction at the NPU core level is too coarse-grained, as user workloads may have diverse resource requirements. We need a flexible system abstraction that allows users to specify the ME/VE resources following the pay-as-you-go model [50]. Such an abstraction will simplify the NPU management for cloud platforms, including NPU resource (de)allocation, resource mapping, and scheduling. Prior studies investigated the system virtualization for FPGAs [6], [34], [35], [61], [62] and GPUs [27], [56]. However, they cannot be directly applied to NPUs, as they target different architectures.\nLack of architectural support for NPU virtualization. Prior studies enabled the time-sharing of an NPU device at the task level, and support the preemption for prioritized tasks [12], [13]. However, the coarse-grained time-sharing on the shared NPU board still suffers from severe resource underutilization, due to the lack of support of concurrent execution of multi-tenant workloads. Existing NPU sharing approaches either sacrifice isolation or suffer from high preemption overhead [16]. V10 [57] enabled NPU sharing between multiple DNN workloads. However, it is still based\non the time-sharing mechanism and suffers from operator interference between multi-tenant ML instances, resulting in poor performance isolation. As we move towards fine-grained NPU virtualization, we need architectural support to achieve both improved performance isolation and NPU utilization.\nLack of ISA support for virtualized NPUs. To simplify the hardware design, NPUs commonly employ VLIW-style ISAs, and the ML compiler explicitly exploits the parallelism of the compute units [5], [29], [33]. However, this requires the number of compute units to be explicitly specified at the compilation stage, and the number cannot be changed at runtime. In this case, the VLIW ISAs unnecessarily couple control flows of the compute units (i.e., MEs). Even though some compute units of a shared NPU become available, they cannot be utilized by the active workload (except recompiling the DNN program). This is caused by the fundamental tussle between dynamic scheduling and VLIW ISAs. As the collocated ML instances have various demands on compute units at runtime, this limitation inevitably causes either NPU underutilization or performance interference. We need to rethink the NPU ISA design to facilitate dynamic resource scheduling for virtualized NPUs.\nIdeally, we wish to virtualize NPUs to enable flexible and fine-grained resource sharing and scheduling for improved NPU utilization and performance isolation. We present TCloud, a hardware-assisted system virtualization framework for NPUs.\nOur contributions. We first develop a simple yet flexible vNPU abstraction. We use VNPU to create a virtualized NPU device for each ML instance. For each vNPU, the user can specify the number of different types of compute units (MEs/VEs) on-demand or follow the pay-as-you-go model in cloud computing. We propose a new resource allocation mechanism that can decide the optimized VNPU configuration for different ML workloads, based on the analysis using ML compilers. As different ML services have various ME/VE demands (see \u00a7II), such an abstraction enables fine-grained resource allocation, which benefits both end users and cloud platform operators\u00b9.\nTCloud can map vNPUs to physical compute units of NPU cores in different manners, based on the service level objectives (SLOs) of ML services. To maximize the NPU utilization while ensuring performance isolation, TCloud enables fine-grained spatial sharing with resource harvesting. It also enables the oversubscription of NPU cores by temporally sharing MEs/VEs among multiple vNPUs. Therefore, the idle compute units can be opportunistically utilized by collocated workloads.\nTo facilitate the dynamic scheduling for collocated VNPUs, TCloud extends the VLIW-style ISA by reorganizing VLIW instructions into independent micro-Tensor operators (\u00b5TOps in \u00a7III). TCloud introduces necessary architectural logic for fine-grained dynamic scheduling of \u00b5TOps on the shared physical NPU cores. It allows one vNPU to harvest available compute cycles of MEs/VEs from collocated vNPUs, without causing much interference. This is impossible with conventional VLIW-"}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "As shown in Figure 1, an NPU board has multiple NPU chips, each chip has multiple NPU cores, each core is connected to an off-chip HBM. An NPU core has two types of compute units: matrix engines (MEs) that perform matrix multiplications with systolic arrays; and vector engines (VEs) that perform generic vector operations. Each NPU core employs an on-chip SRAM to hide HBM access latency. A typical example of NPU architecture in production is Google TPU [32].\nTo run a DNN program on NPUs, ML compilers [14], [22], [47] generate a sequence of tensor operators, which are then translated into device-specific machine instructions. An NPU core usually uses a VLIW-style ISA for simplifying the\nhardware. Each instruction contains multiple ME slots, VE slots, load/store slots for accessing the SRAM, and other slots (e.g., for DMA operations with HBM). The ML compilers can exploit the instruction-level parallelism with the knowledge of underlying compute resource, such as the numbers of MEs/VEs."}, {"title": "B. Characterization of ML Inference Services", "content": "To motivate NPU virtualization, we conduct a study of resource demands of ML inference workloads and their impact on NPU utilization. We run various ML inference workloads from MLPerf benchmarks [48] and official TPU reference models [23] (see Table I), on a real Google TPUv4 board with 8 cores. Each core has four MEs and two VEs. We profile the number of MEs/VEs demanded by each workload with ML compiler techniques, and the resource utilization with performance counters on the TPU core. We vary the batch size (8 by default). The HBM footprint of benchmarks ranges from 10.59MB to 22.38GB, which does not fully occupy the HBM on modern NPU chips (e.g., 32GB/96GB on TPUv4/TPUv5p [53]). We report the resource utilization on one TPU core, as all cores perform identical computations with data parallelism.\nDiverse demands on MEs/VEs. An ML inference workload can have diverse resource demands over time, as different operators in a DNN model have vastly different demands on MEs and VEs. For each workload, we analyze the DNN execution graph generated by the ML compiler. By default, the ML compiler picks the number of compute units for each operator to maximize the overall efficiency of the compute units\nbased on the tensor shapes. We use this to quantify the ME/VE demands. Figure 2 shows that DNN inference workloads have various ME/VE demands over time. As we increase the batch size, we observe similar patterns (Figure 3). Due to space limitations, we only show the results of BERT and DLRM. The imbalanced demands are determined by the ML model architecture. For example, in Figure 4, ResNet is dominated by convolutions (ME-intensive operators), while DLRM contains many vector operators, which do not utilize the ME at all. For workloads that cannot run with large batch sizes due to insufficient memory, we do not show them in Figure 4.\nLow NPU resource utilization. The diverse demands on MEs/VEs inevitably cause NPU underutilization. We quantify the percentage of idleness of the MEs/VEs in Figure 5. Although workloads like DLRM and NCF may appear to be VE-intensive, at least 20% of their execution time still involves heavy ME computation. For ME-intensive models such as ResNet, many operators are also VE-intensive. To balance the demands on ME and VE, the ML compiler can perform operator fusion to pipeline the execution of ME and VE [14], [24], [57]. However, as such fusion opportunities are limited, most operators still have imbalanced ME/VE demands after fusion. Figure 6 shows an example of VE underutilization in\nan ME-intensive operator. Each pop operation takes 8 cycles to generate an 8 \u00d7 128 output vector from the ME, while each VE operation takes 1 cycle to post-process the output vector. As a result, the VE is idle for most of the time.\nWe also profile the HBM bandwidth utilization in Figure 7. While the peak bandwidth almost reaches the hardware limit (1.2TB/s on a TPUv4 chip), the average bandwidth is as low as 176-498GB/s. This is because different operators in a DNN model have varying bandwidth demands. For example, in DLRM, the embedding lookup consumes high bandwidth, while the multi-layer perceptron (MLP) has low bandwidth requirements. As we increase the batch sizes, the bandwidth consumption decreases for some workloads. For example, BERT is dominated by ME operators, which become\nmore compute-intensive with larger batch sizes; DLRM is VE-intensive, and VE operators have low compute intensity regardless of batch sizes. As some DNN operators underutilize the HBM bandwidth while other operators underutilize the compute resources, collocating DNN workloads on the same NPU core helps cloud platforms utilize both resources."}, {"title": "C. NPU Virtualization: Challenges and Opportunities", "content": "System virtualization offers the opportunity for supporting multi-tenancy and improving resource utilization. However, virtualizing NPUs suffers from unique challenges.\nNew abstraction required for fine-grained virtualization. As none of prior studies investigated NPU virtualization, it is unclear how the virtualized NPUs should be exposed to application instances. By virtualizing NPUs, we need to provide a simple yet effective abstraction, which can provide sufficient flexibility for users to specify the numbers of MEs and VES based on the workload demand and target SLOs (see \u00a7III-B). For instance, we should allocate more MEs than VEs to an ME-intensive workload, and vice versa.\nHowever, even if we can allocate the most appropriate numbers of MEs and VEs, the allocated resources still cannot be fully utilized, due to the diverse resource demands of different operators over time. A static allocation of MEs and VEs is insufficient. Instead, we need to enable dynamic resource scheduling. We should allow one workload to \"harvest\" the underutilized compute units allocated to other workloads for improving the overall utilization of the NPU core and the Quality-of-Service (QoS) of collocated ML inference services. Unfortunately, current NPU architectures do not support such fine-grained resource scheduling and harvesting.\nISA limitations for enabling virtualized NPU scheduling. The fundamental limitations of modern NPU architectures prevent dynamic resource scheduling. To simplify the hardware design of NPUs, developers usually employ VLIW-style ISAs, and utilize ML compilers to exploit the instruction-level parallelism. However, the statically scheduled ISAs cannot fully exploit the hardware resources at runtime. They unnecessarily couple the control flows of all MEs in a tensor operator, even though different MEs can execute independently. As shown in Figure 8, the original VLIW program must execute each VLIW\ninstruction sequentially, creating false dependencies between operations on different MEs even though they do not have any true data dependencies. As the compiler explicitly specifies how many MEs are being used, the allocated MEs cannot be changed at runtime unless the DNN program is recompiled. For example, if the compiler generates push/pop operations for two MEs, these operations cannot be time-multiplexed on a single ME, since this will corrupt the intermediate states in the ME. Hence, if only one ME is available, this DNN program cannot run until at least two MEs are available (Figure 9 left). It also cannot utilize more than two MEs, even if more than two are available (Figure 9 right), because the push/pop operations for one ME share the intermediate data in this ME.\nTo address this problem and enable dynamic ME scheduling, one may consider switching from VLIW to another ISA (e.g., RISC) or employing superscalar out-of-order (OoO) execution (similar to a CPU core). However, they still lack the support for dynamic ME scheduling since the compiler still needs to specify which ME is the target of a push/pop instruction statically. To remove such a constraint, we need to offer the flexibility for the NPU program to determine the target ME at runtime. Therefore, we need to rethink the contract between the compiler and the NPU hardware by extending the ISA.\nArchitectural support for parallelizing ME/VE operations. Our key observation is that the execution of different MEs and VEs in a tensor operator is usually separable. Specifically, most DNN operators, such as matrix multiplication (MatMul) and convolution, are partitioned by DNN compilers [14], [64] into multiple tiles that can be computed independently. As shown in Figure 8, the original program computes a MatMul tile and directly applies a ReLU function to the results using 2 MEs and 2 VEs. However, the instructions executed on the first ME/VE (colored blue) have no dependencies with the instructions on the second ME/VE (colored green). The two instruction groups can be separated and independently executed."}, {"title": "III. DESIGN AND IMPLEMENTATION", "content": "We design TCloud to achieve the following objectives:\nAllocation flexibility: As DNN workloads have different resource and SLO requirements, we need to provide the flexibility for users to customize their NPU hardware.\nNPU utilization: Since an individual ML inference workload underutilizes NPU cores (\u00a7II-B), we need to enable fine-grained NPU virtualization for improved NPU utilization.\nPerformance isolation: As we collocate DNN workloads on the same NPU core, we must provide performance isolation.\nWe first present a new vNPU abstraction for NPU virtualization (\u00a7III-A). Based on this, we enable flexible vNPU resource allocation (\u00a7III-B) and vNPU-to-pNPU mappings (\u00a7III-C). We extend VLIW-style ISA (\u00a7III-D) and NPU architecture (\u00a7III-E) for enabling fine-grained resource scheduling for vNPUs."}, {"title": "A. vNPU: The New Abstraction for NPU Virtualization", "content": "We design the vNPU abstraction with the goals of (1) allocating NPU hardware resource to a vNPU instance on demand; (2) hiding the complexity from the ML programs with minimal changes to the guest software stack for compatibility.\nVNPU abstraction. A vNPU instance reflects the hierarchy of a physical NPU board. Figure 10 shows the configurable parameters of a vNPU. Each vNPU is exposed to the VM as a PCIe device. The guest NPU driver can query the hierarchy of the vNPU, such as the number of chips, cores per chip, HBM size, and others. The maximum vNPU size is capped by the physical NPU size. If a guest VM requires more resources than is available on a physical NPU board, TCloud can allocate multiple vNPU instances to it. The guest ML framework can handle the data distribution across multiple vNPU cores in the same way as that on physical NPUs. Take Google TPU for example, TensorFlow already handles data parallelism across physical NPUs. It can work in the same way with vNPUs.\nVNPU lifecycle. To create a vNPU instance, a user can specify the vNPU configuration following the pay-as-you-go model [50]. Cloud providers can define various default configurations (e.g., small/medium/large vNPU cores as having 1/4/8 MEs/VEs). TCloud can also learn an optimized VNPU configuration for a DNN workload with ML compilers (\u00a7III-B). As shown in Figure 11, upon vNPU initialization, the guest driver sends a request to the hypervisor through a para-virtualized interface (\u00a7III-F) (1). The vNPU manager maps"}, {"title": "B. VNPU Allocation and Deallocation", "content": "Following the popular pay-as-you-go model [50], cloud plat-forms allow users to specify the vNPU configuration on demand. However, as ML inference workloads have diverse ME/VE demands (see \u00a7II-B), specifying the number of MEs/VEs can be challenging for users who are not NPU experts. Thus, we allow them to specify the total number of execution units (EUs), which is directly related to the cost of running the vNPU instance. TCloud provides the vNPU allocator, a compile-time tool to improve the performance per cost of vNPUs by identifying an optimized ME/VE ratio for the user workload.\nME/VE allocation. The ME/VE demands of a ML workload can be reflected by how it runs on one ME and one VE. We denote the ME active runtime as $m$, and that of VE as $v$. These numbers can be obtained via profiling at the compilation stage. Based on our study in \u00a7II-B, for most DNN models, at least one of ME/VE is active during the execution of an NPU core. Thus, the time portion where only ME is active is $1 - v$, that\nof only VE is $1 - m$, and that of concurrent ME/VE execution is $m + v - 1$. With Amdahl's Law, the normalized execution time on $n_m$ MEs and $n_v$ VEs is\n$T = \\frac{1 - v}{n_m} + \\frac{1 - m}{n_v} + \\frac{m+v-1}{min(n_m, n_v)}$\nwhere the concurrent part is bottlenecked by the minority type of EU. Let $n_m + n_v$ be the hypothetical speedup regardless of EU types, which means an EU can execute both ME and VE operators. Compared to real cases where each EU must respect data dependencies and operator types, the hypothetical speedup assumes all $n_m + n_v$ EUs are 100% utilized. Thus, the hypothetical execution time on $n_m$ MEs and $n_v$ VEs is $T_h = \\frac{m+v}{n_m+n_v}$, and the total EU utilization can be quantified as the ratio between hypothetical and estimated execution times:\n$U = \\frac{T_h}{T} = \\frac{\\frac{m+v}{n_m+n_v}}{\\frac{1 - v}{n_m} + \\frac{1 - m}{n_v} + \\frac{m+v-1}{min(n_m, n_v)}}$\nTo isolate the impact of total ME and VE quantity, we simplify the function by letting $k = \\frac{n_m}{n_v}$ be the ratio between the numbers of MEs and VEs. Without loss of generality, we assume $n_v \\geq n_m$, which means $k \\leq 1$. Then, we can simplify Equation (2) with mathematical tools [38]:\n$U = \\frac{(m + v)k}{(1 - m)k^2 + k + m}k \\leq 1$.\nTo find the value of k that maximizes U, we compute the value of k where $\\frac{dU}{dk} = 0$. This gives $k = \\sqrt{\\frac{m}{1-m}}$ for $m < 0.5$. If $m \\geq 0.5$, U will be monotonic, so k = 1 maximizes U. Similarly, for the case when $n_m \\geq n_v$, we derive $k = \\sqrt{\\frac{1 - v}{v}}$ for $v < 0.5$ and k = 1 for $v \\geq 0.5$. Consequently, we have\n$\\frac{n_m}{n_v} = {\\sqrt{\\frac{m}{1-m}}, m < 0.5 \\\\  \\sqrt{\\frac{1 - v}{v}}, v < 0.5 \\\\ 1, m \\geq 0.5 \\\\ and v \\geq 0.5.$\nThe case when both m < 0.5 and n < 0.5 does not exist since at least one of ME/VE will be active (m + n \u2265 1). When m < 0.5, for workloads with ME active time ratio m, we allocate $\\sqrt{\\frac{m}{1 - m}}$ times more MEs than VEs. When v < 0.5, for workloads with VE active time ratio v, we approximate the allocated ME/VE quantity ratio to $\\sqrt{\\frac{1 - v}{v}}$. If m > 0.5 and v > 0.5, we allocate the same number of MEs and VEs. Note that each vNPU will have at least one ME and one VE.\nMemory allocation. Users can use the compiler to estimate the total HBM capacity needed by a DNN workload. By default, the SRAM capacity is allocated proportionally to the number of allocated MEs, as more MEs usually indicate larger tile sizes. Based on our study in \u00a7II-B, for many common ML inference services, the HBM bandwidth is less of a concern. Thus, TCloud allows fair sharing of HBM bandwidth by default. For large models that demand large HBM capacity and bandwidth, the vNPU abstraction offers the flexibility for end users to allocate the demanded resources. The user may also leverage existing tensor swapping techniques to support large DNN workloads with limited memory capacity [28], [63]."}, {"title": "C. VNPU Mapping", "content": "The vNPU manager attempts to balance the number of allocated EUs and the size of allocated memory. This minimizes the chance that all EUs on one core are allocated but a large portion of its memory is not allocated, or vice versa. Thus, VNPUs with many EUs and small memory will be collocated with vNPUs with few EUs and large memory. TCloud uses a greedy algorithm for this by default.\nVNPU mapping schemes. TCloud provides the flexibility for cloud platforms to enable both hardware-isolated (spatial-isolated) mapping and software-isolated (temporal-sharing)\nmapping. With hardware-isolated mapping, a vNPU is mapped to dedicated EUs and SRAM, and the allocated hardware is not shared with other vNPUs. With software-isolated mapping, multiple vNPUs can temporally share the same EUs. TCloud uses priority-based scheduling for fair sharing and performs context switches between vNPUs (see \u00a7III-E).\nVNPU mapping policies. TCloud decides which vNPUs can be mapped onto the same physical NPU (pNPU) as follows. With hardware-isolated mapping, TCloud collocates a set of VNPUs as long as the total resource requirement (e.g., number of MEs/VEs, HBM capacity) does not exceed the pNPU. With software-isolated mapping, TCloud aims to load-balance the PNPUs while allowing oversubscription. TCloud tracks the total resource requirement of assigned vNPUs on each pNPU, and assigns a new vNPU to the PNPU that suffers the least resource requirement. TCloud can support other collocation policies [12], [41], [57] as well. At scale, TCloud can be integrated with a cluster-wise VM/container orchestration framework such as KubeVirt/Kubernetes [18] to decide which VM should be placed on what machine. Developing advanced vNPU/VM collocation policies is orthogonal to our work.\nVNPU security isolation. TCloud enforces memory address space isolation among collocated vNPUs with the conventional memory segmentation scheme [2], [57] for both HBM and SRAM. TCloud divides the SRAM and HBM into fixed-sized segments and maps each segment to the virtual address space of a VNPU. For the NPU core in Table II, an SRAM/HBM segment is 2MB/1GB. There is no external fragmentation since the segment size is fixed. The address translation is performed by adding the segment offset to the starting address of the physical segment, which incurs negligible overhead. A page fault will be triggered when an invalid access happens. This is sufficient since ML frameworks like TensorFlow typically request a contiguous chunk of memory for the entire lifetime of an ML inference service and have their own memory management mechanism. To isolate the vNPU instances as they communicate with the host, TCloud uses IOMMU to enforce DMA remapping (\u00a7III-F). We leave side-channel mitigation to future work."}, {"title": "D. ISA Extension for NPU Virtualization", "content": "To support dynamic ME/VE scheduling (\u00a7II-C), we develop NeuISA, in which when the ML compiler maps a tensor operator onto MEs, it generates \u201csub-tasks\" for each ME, so the hardware can decide which \"sub-task\" can be executed\nat runtime based on the availability of MEs. NeuISA is still expressive for compilers to exploit the instruction-level parallelism between MEs and VEs, and preserve the flexibility of supporting fused operators and complex control-flow structures like branches and nested loops in VLIW-style ISAs.\nSeparating ME control flow with \u00b5TOps. NeuISA decouples the execution of independent MEs in a tensor operator by separating the control flow of each ME and VE into independent instruction sequences (see Figure 8), called micro-Tensor Operators (\u00b5TOps). To minimize changes to the existing VLIW compiler and hardware, the instruction format inside a \u00b5TOP resembles the original VLIW ISA: an instruction contains multiple slots, and each slot encodes an operation (such as a push/pop operation in an ME slot and an ALU operation in a VE slot). However, the number of ME slots in a NeuISA instruction differs from that of a traditional NPU ISA.\n\u03bc\u03a4\u039fp types. As shown in Figure 13, for a physical NPU core with $n_x$ MEs and $n_y$ VEs, NeuISA defines two types of \u00b5TOps: (1) An \u039c\u0395 \u00b5\u03a4Op contains instructions with one ME slot and $n_y$ VE slots. An ME \u00b5TOp will only use one ME during execution, which enforces that each \u039c\u0395 \u00b5\u03a4Op only contains the control flow of one ME. To execute an operator on multiple MEs, the compiler generates multiple \u039c\u0395 \u00b5TOps. At runtime, the hardware dynamically adjusts the number of MEs assigned to this operator by deciding how many M\u0395 \u00b5TOps are being executed. The VE slots in an \u039c\u0395 \u00b5TOp enable instruction-level parallelism between MEs and VEs. VE slots are necessary because the VE needs to aggregate the outputs of the systolic array. They also enable operator fusions such as MatMul+ReLU (see Figure 8). (2) A VE \u00b5TOp contains instructions with no ME slot and $n_y$ VE slots, which performs vector operations that do not involve ME computation. The $n_y$ VE slots allow a VE \u00b5TOP to utilize all the VEs. Having multiple VE slots in an instruction does not increase the hardware complexity since the original VLIW NPU architecture already supports this.\nSupporting fused operators with \u00b5TOp groups. The \u00b5TOps can efficiently support basic tensor operators, such as tiled matrix multiplication with each \u00b5TOp computing a different tile. However, ML compilers may generate fused operators that cannot be handled by \u00b5TOps alone, e.g., a matrix multiplication may be executed with $n_x$ ME \u00b5TOps, while the succeeding fused normalization operator only needs a VE \u00b5TOP.\nTo support a fused operator, NeuISA organizes the \u00b5TOps into a sequence of \u00b5TOp groups to express the dependencies between \u00b5TOps, as shown in Figure 13. Each group contains\nup to $n_x$ \u039c\u0395 \u00b5TOps, allowing the operator to utilize all the allocated MEs, and up to one VE \u00b5TOp, as one VE \u00b5TOp already contains $n_y$ VE slots to utilize all the VEs. All \u00b5TOps in one \u00b5TOp group may execute concurrently, but each group must execute sequentially to preserve data dependency. As an example, a fused operator may contain one \u00b5TOp group doing a MatMul+ReLU with multiple \u039c\u0395 \u00b5TOps, followed by a \u00b5TOp group doing normalization with a single VE \u00b5TOp.\nNeuISA control flow. As NeuISA inherits the VLIW semantic inside each \u00b5TOp, it intrinsically supports conditional branches and loops inside a \u00b5TOp. It is also desirable to have branches across \u00b5TOp groups. For example, an operator contains a nested loop in which the inner-most loop is a matrix multiplication that can be mapped to a \u00b5TOp group. In this case, we need to support loops across multiple \u00b5TOp groups.\nNeuISA defines special control instructions that can be in-voked in each \u00b5TOP (see Figure 14). The u Top.nextGroup instruction can be used to specify the target \u00b5TOp group that should be executed next. It may be executed by more than one \u00b5TOps in the same group as long as they specify the same target group index. Otherwise, an exception will be raised. Figure 15 shows a loop structure example. The loop counter Count is stored in the on-chip SRAM. The loop body contains \u03bc\u03a4\u039f\u03a1 group 0-2. In group 2, Count is incremented and examined at the end of a \u00b5TOp. If this is not the last loop iteration, uTop.nextGroup is executed to loop back to group 0."}, {"title": "E. Architectural Support for NeuISA", "content": "The \u00b5TOp design enables dynamic operator scheduling. It allows a vNPU to harvest unused ME/VEs from other collocated VNPUs in the same physical NPU core at runtime.\nHardware scheduler for NeuISA. Figure 17 shows the pipeline design for fetching and scheduling \u00b5TOps. The NPU core maintains the contexts of multiple vNPUs, including the PC pointers to the program and the vNPU configurations. Each time a new \u00b5TOp is ready or an existing \u00b5TOp finishes, the \u00b5TOp scheduler selects the \u00b5TOps to be executed next. For each vNPU, the \u00b5TOp scheduler retrieves the number of allocated MEs and the number of ready \u039c\u0395 \u00b5TOps from the VNPU context. It selects a set of ready \u00b5TOps, and fetch their instructions to the instruction queues.\nNext, the operation scheduler selects which operations from the instruction queues will be executed at every cycle. The ME operations from the ME \u00b5TOp instruction queues are directly issued to the corresponding MEs. For the VE operations, the scheduler selects which operations to issue from all VE \u00b5TOp instruction queues. To reclaim a harvested ME, TCloud performs a context switch to preempt the harvesting \u00b5T\u041e\u0440. Upon a context switch, the register file and the intermediate data in the MEs are saved to SRAM, which incurs negligible overhead compared to the length of an operator. The number of instruction queues should be large enough to support simultaneous execution of all MEs/VEs. For an NPU core with $n_x$ MEs and $n_y$ VEs, there are $n_x$ \u039c\u0395 \u00b5TOp instruction queues and $n_y$ VE \u00b5TOp instruction queues.\n\u03bc\u03a4\u039fp scheduling policy. The \u00b5TOp scheduler can be con-figured in either spatial-isolated or temporal-sharing vNPU scheduling mode, as discussed in \u00a7III-C.\nWith spatial-isolated mode, the scheduler aims to ensure performance isolation. First, if a vNPU has $n_x$ MEs and at least $n_x$ ready \u039c\u0395 \u00b5TOps, the scheduler will execute $n_x$ \u039c\u0395 \u00b5TOps to fully utilize all the allocated MEs for this vNPU. In this case, no MEs will be harvested from this vNPU. If the allocated MEs are already being harvested by \u00b5TOps from other vNPUs, these \u00b5TOps will be preempted to reclaim the harvested MEs. Second, to improve utilization, if the vNPU has more than $n_x$ ready \u039c\u0395 \u00b5TOps, and if another VNPU does not have enough \u039c\u0395 \u00b5TOps to utilize all its MEs, the scheduler allows the unused MEs to be harvested. A ready VE \u00b5TOp is always executed, as it does not occupy any MEs.\nWith temporal-sharing mode, as the NPU is oversubscribed,\nthe scheduler maintains fair sharing with the best effort. It uses a priority-based preemptive policy similar to that in previous works [16", "57": "."}]}