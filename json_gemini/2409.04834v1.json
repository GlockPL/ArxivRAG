{"title": "Reducing Events to Augment Log-based Anomaly Detection Models: An Empirical Study", "authors": ["Lingzhe Zhang", "Tong Jia*", "Kangjin Wang", "Mengxi Jia", "Yong Yang", "Ying Li*"], "abstract": "As software systems grow increasingly intricate, the precise detection of anomalies have become both essential and challenging. Current log-based anomaly detection methods depend heavily on vast amounts of log data leading to inefficient inference and potential misguidance by noise logs. However, the quantitative effects of log reduction on the effectiveness of anomaly detection remain unexplored. Therefore, we first conduct a comprehensive study on six distinct models spanning three datasets. Through the study, the impact of log quantity and their effectiveness in representing anomalies is qualifies, uncovering three distinctive log event types that differently influence model performance. Drawing from these insights, we propose LogCleaner: an efficient methodology for the automatic reduction of log events in the context of anomaly detection. Serving as middleware between software systems and models, LogCleaner continuously updates and filters anti-events and duplicative-events in the raw generated logs. Experimental outcomes highlight LogCleaner's capability to reduce over 70% of log events in anomaly detection, accelerating the model's inference speed by approximately 300%, and universally improving the performance of models for anomaly detection.", "sections": [{"title": "1 Lay Abstract", "content": "As software systems become more complex, detecting problems or unusual behaviors (called anomalies) in these systems is both critical and difficult. Most current methods for finding these anomalies rely on processing huge amounts of log data, which can slow down the process and may lead to inaccurate results due to unnecessary or noisy logs. Despite this, the impact of reducing log data on anomaly detection hasn't been studied much.\nTo address this gap, we conduct a detailed study using six different models and three datasets to see how the quantity and quality of log data affect the ability to detect anomalies. We discover that not all log events are equally important-some help models perform better, while others have little or even negative effects. Based on these findings, we develop a tool called LogCleaner.\nLogCleaner is designed to automatically reduce the number of log events while still maintaining the important information needed for detecting anomalies. It works as a middle layer between the software system and the detection models, continuously removing unnecessary events (which we call \"anti-events\" and \"duplicative-events\") from the raw logs.\nOur experiments show that LogCleaner can remove more than 70% of log events without hurting the ability to detect anomalies. In fact, by reducing the noise, it speeds up the models by about 300% and improves their overall performance. LogCleaner offers a practical solution for developers and engineers looking to make anomaly detection faster and more accurate."}, {"title": "2 Introduction", "content": "Modern software systems are becoming increasingly complex, leading to more frequent failures that can cause considerable losses even during short periods of unavailability [9, 48]. Detecting anomalies accurately has therefore become critical for ensuring reliable and continuously available services. System logs provide valuable runtime information about software states and events, making them an indispensable resource for log-based anomaly detection approaches. With the ability to pinpoint failures and prevent further deterioration, log-based anomaly detection have garnered significant attention as important ways to maintain highly secure and resilient software systems in the face of rising complexity.\nIn recent years, anomaly detection based on system logs has gained significant research attention. These log-based anomaly detection models can be broadly classified into two categories: supervised models[2, 3, 10, 26, 30, 40, 47, 49] and unsupervised models[1, 7, 11, 19, 20, 22, 23, 33, 42]. Supervised models, such as RobustLog[49], necessitate labeled data comprising both normal and abnormal instances to construct their predictive frameworks. In contrast, unsupervised models detect deviations relying solely on standard data. They are primarily split into deep neural network-based[7, 22, 33, 42] and graph-based models [1, 11, 19, 20, 22].\nDespite the promising results demonstrated by these anomaly detection methods, they directly leverage extensive log data generated by software systems, leading to the following practical challenges:\n\u2022 Inefficient Inference: With an increasing number of logs, the model's inference speed tends to slow down. If a substantial portion of these original logs consists of irrelevant entries, it can result in unnecessary degradation of inference speed and resource wastage. [43].\n\u2022 Misleading by Noise Logs: It is acknowledged that having more logs provides a wealth of information, but in reality, many logs are of low quality, and some even contain noise. This can mislead the model [25, 49].\nIn fact, not all logs generated by software systems are essential. However, the quantitative effects of log reduction on the effectiveness of anomaly detection remain unexplored. The significance of various log types and the subsequent performance trade-offs post their elimination remain uncertain.\nTo fill this significant gap, we conduct an empirical study to quantify the impact of log reduction on anomaly detection The investigation spans six anomaly detection models (LR[2], SVM[26], Decision Tree[3], Isolation Forest[14], RobustLog[49], PLELog[40]) applied to three datasets[18] (HDFS, BGL, Thunderbird). We design two approaches: a retry-based method and a clustering-based approach, to validate the extent of log event reduction possible under constrained model performance degradation thresholds. The results reveal that for anomaly detection models, log events can be significantly reduced, and the reduction of logs can even enhance model effectiveness. In extreme cases, such as the Thunderbird dataset, a single log event (originally 1406 log events) can identify most anomalies.\nFurthermore, this work conducts an in-depth analysis of reducible log events for anomaly detection. The events are categorized into anti-events and duplicative-events based on whether their removal improves or does not affect model performance. Additionally, whose removal degrades model effectiveness are identifies as key-events.\nBuilding on the findings of the empirical study, we introduce LogCleaner, a comprehensive methodology designed for the automatic reduction and reporting of anti-events and duplicative-events in log events, specifically tailored for anomaly detection. LogCleaner is divided into an profiling and an online component. In the profiling part, it utilizes historical logs, applying TF-IDF to eliminate sporadic log events, then using mutual information to reduce anti-events. Finally, it employs a graph-based clustering approach to eliminate duplicative-events, resulting in a reduced event set. In the online part, it functions as middleware between software systems and models, streamlining raw generated logs to reduced logs using the reduced event set. The reduced logs are then employed for anomaly detection. Additionally, whenever there is a variation in the code of the software system, the system's logs and existing labels are re-extracted for re-profiling. This re-profiling process enables LogCleaner to adapt effectively to potential future anomalies.\nWe evaluate LogCleaner's effectiveness across the aforementioned models and datasets. Results show that LogCleaner can reduce over 70% of log events in anomaly detection, accelerate the model's inference speed by approximately 300%, while universally improve the performance of models for both anomaly detection. In summary, the contributions are as follows:\n\u2022 We conduct a comprehensive study to quantify the impact of log event reduction on anomaly detection model effectiveness. Our findings reveal the remarkable extent to which the number of log events can be reduced without compromising model performance. Furthermore, our empirical study categorizes log events into key-events, anti-events, and duplicative-events based on the impact of their removal on model performance.\n\u2022 Inspired by the findings, we introduce LogCleaner, an efficient methodology for the automatic reduction of log events in the context of anomaly detection. Serving as middleware between software systems and models, LogCleaner continuously updates and filters anti-events and duplicative-events in the raw generated logs.\n\u2022 We validate LogCleaner's effectiveness across 6 anomaly detection models on 3 datasets. Experiments demonstrate LogCleaner universally improves detection model performance while reducing over 70% of log events and accelerating the model's inference speed by approximately 300%."}, {"title": "3 Background", "content": "This section provides background on log-based anomaly detection, introduces the models that will be used in the empirical study, and outlines the common overall framework for log-based anomaly detection."}, {"title": "3.1 Anomaly Detection", "content": "Anomaly detection[1-3, 7, 10, 11, 19, 20, 26, 30, 40, 42, 49] aims to identify irregularities in system behavior. Detection methods are broadly categorized into supervised and unsupervised models. Supervised models [2, 3, 10, 26, 30, 40, 49], like RobustLog[49], require labeled data that includes both normal and abnormal examples to form predictive frameworks. PLELog[40] addresses the issue of insufficient labels via probabilistic label estimation and designs an attention-based GRU neural network to detect anomalies. Loglizer [17] offers a comprehensive toolkit featuring several machine-learning-based log analysis models designed for automated anomaly detection, including linear regression (LR)[2], SVM[26], Decision Tree [3], Isolation Forest[14]. In contrast, unsupervised models [1, 7, 11, 19, 20, 42] identify deviations based only on standard data. In this paper, we focus on supervised models"}, {"title": "3.2 The Common Workflow", "content": "Despite that the target and approaches of anomaly detection are quite different, they share share common workflow [19, 24]. As shown in figure 1, the framework consists of three steps: (1) log parsing, (2) log grouping, (3) anomaly detection."}, {"title": "3.2.1 Log parsing.", "content": "Raw logs consist of semi-structured text encompassing various fields like timestamps and severity levels. For the benefit of downstream tasks, log parsing is employed to transform each log message into a distinct event template, which includes a constant part paired with variable parameters. For example, the log template \"E0,('instruction', 'cache', 'parity', 'error', 'corrected')\" can be extracted from the log message \"2005-06-03-15.42.50.363779 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\" in figure 1. There are many log parsing methods, based on frequent pattern mining[5, 34, 37, 44], clustering[12, 35, 38, 39], and heuristics[16, 21, 31]. This paper utilizes the Brain [44] implemented by Logparser[15, 51]."}, {"title": "3.2.2 Log grouping.", "content": "After being parsed into event templates, log data can be organized into sequence groups using session, sliding, or fixed windows. Determining an optimal window size is challenging. For instance, a small window size might impede the models' ability to recognize anomalies that stretch across multiple sequences. Conversely, a large window size could lead to log sequences encompassing multiple anomalies, thereby complicating the detection process [24]. This study adopts both session-based and fixed windows of 100 logs, aligning parameters with those presented in the survey [24]."}, {"title": "3.2.3 Anomaly detection.", "content": "After converting log events into sequences, they are processed by the previously mentioned anomaly detection models. These models undergo profiling training and then facilitate online prediction."}, {"title": "4 Study Design", "content": "This section outlines the datasets and models under evaluation and provides an overview of the methodology adopted for the empirical study."}, {"title": "4.1 Datasets", "content": "In our assessment of models for log-based anomaly detection, three datasets are employed [18]: HDFS, BGL and Thunderbird. The details of each dataset are as follows:\nHDFS dataset originates from over 200 Amazon EC2 nodes. It encompasses a total of 11,175,629 log messages. These messages are grouped into distinct log windows based on their block_id, representing individual program executions within the HDFS system. Notably, 16,838 log blocks (amounting to 2.93%) within this dataset signify system anomalies.\nBGL dataset is derived from a supercomputing system and was gathered by Lawrence Livermore National Labs (LLNL). It comprises a total of 4,747,963 log messages. Every message within the BGL dataset has been manually categorized as either normal or anomalous. Notably, of these, 348,460 log messages (representing 7.34%) are marked as anomalous.\nThunderbird dataset is an open collection of logs sourced from the Thunderbird supercomputer at Sandia National Labs (SNL). This dataset encompasses both regular and anomalous messages, each of which has been manually classified. While the Thunderbird dataset encompasses a massive collection of over 200 million log messages, this paper opts to use an initial continuous subset of 10 million log lines for the sake of computational efficiency. Notably, this subset includes 353,794 anomalous log messages, constituting 3.53% of the total."}, {"title": "4.2 Evaluated Models", "content": "In this study, we evaluate the six representative models described in section 3.1. The source code for all anomaly detection models[6, 17, 40] are public. In terms of log parsing, we employ the Brain[44] method as implemented by Logparser[15, 51]. For log grouping, we adopt different strategies based on the dataset: session-based windows are applied to the HDFS dataset, while for the BGL and Thunderbird dataset, we utilize fixed windows comprising 100 logs."}, {"title": "4.3 Approach", "content": "As previously mentioned, the aim of study is to quantify the effect of log event reduction on the effectiveness of anomaly detection models. To achieve this, we introduce two empirical study methodologies: the Retry-based approach and the Cluster-based approach."}, {"title": "4.3.1 Retry-based approach.", "content": "The core concept behind the Retry-based approach is to iteratively remove one log event at a time and then retrain the model to assess its effectiveness. If the model's effectiveness decreases after the removal, that particular log event is retained; otherwise, it's deemed useless."}, {"title": "", "content": "As illustrated in figure 2, this approach initially employs the previously mentioned log parsing tool to extract event sequences and templates. Following this, the respective log grouping algorithm is utilized to organize these events into groups. Sequentially, it attempts to eliminate one log event from the event templates (designated as the target removal event), and simultaneously remove the corresponding event from each event group. It's crucial to note that after removing an event, the log grouping algorithm isn't re-executed. This ensures the preservation of labels for each event group. Thus, even if an event group is devoid of any events, its associated label is still retained.\n$f1 < (1-a) x f1max$ (1)\nThe regenerated event groups are subsequently fed into the respective model for retraining and testing. From this, it obtains metrics such as precision, recall, and the F1-score. If the model's performance meets the criteria defined in equation 1, it indicates that the removal of that particular event impacts the model's effectiveness. As a result, this event is reintegrated into both the event templates and event groups. On the other hand, if the event doesn't significantly influence the performance, it is deemed redundant and removed, with the F1-score at that point recorded as $f 1 max$. Here, a represents the permissible threshold for performance degradation. It's important to highlight that minor temporary performance dips during the model's training and testing phase don't necessarily signify a permanent degradation in model efficacy. Such deviations might merely be due to natural random fluctuations. Thus, even though it establishes a threshold with a, the overall model effectiveness might not necessarily decline upon the completion of experiments."}, {"title": "4.3.2 Clustering-based approach.", "content": "The Retry-based approach can produce near-optimal results. However, its necessity to retrain the model every time an event is removed becomes prohibitively time-consuming when dealing with datasets that have numerous events and a large volume of log events. This is because most model training durations are directly proportional to the volume of log events. For instance, considering the Thunderbird dataset, which consists of 1,406 event templates, if the SVM model initially takes around 10 minutes for each train-test iteration, completing the entire experiment will demand almost 10 days of computational time. Moreover, it's essential to highlight that many experiments require multiple runs to ensure consistent and reliable results.\nThus, to expedite the categorization and filtering of irrelevant log events, we introduce the Clustering-based approach. As illustrated in figure 3, it begins by extracting all log templates. Sequentially, it identifies each log event within the event templates as the Target Test Event. For each event group, only the corresponding Target Test Event is retained. These single event groups are then subjected to the specific model for retraining and testing. The resulting precision, recall, and F1-score are documented for every iteration. Ultimately, it obtains the precision, recall, and F1-score associated with each individual log event. Using a clustering algorithm (KMeans in this paper), based on the precision, recall, and F1-score, it classifies log events into two categories: irrelevant and relevant events. Finally, within the scope of relevant events, the Retry-based approach is executed."}, {"title": "4.4 Research Questions", "content": "The objective of this research is to quantify the impact of log event reduction on the performance of anomaly detection models. Several research questions are formulated to guide the investigation, leveraging the experimental approaches previously described.\nRQ1: To what extent can each anomaly detection method reduce log events? We aim to assess the extent to which each method can reduce log events while maintaining the performance of existing anomaly detection approaches. For this purpose, useless otherwise mentioned, we conduct experiments on the studied models across various datasets with a = 0.02. This parameter choice ensures that the natural random fluctuations do not lead to false identification of an event as relevant, while also ensuring that the model's performance does not degrade significantly.\nRQ2: How does log event reduction impact the performance of existing anomaly detection approaches? In RQ1, we conduct experiments using a = 0.02, indicating a slight decline in the model's performance. However, as previously analyzed, this isn't necessarily the case. Hence, in this research question, we will further delve into a quantitative analysis of the impact of event reduction on the performance of anomaly detection approaches."}, {"title": "5 Empirical Results", "content": "This section presents and addresses the research questions proposed in section 4.4."}, {"title": "5.1 RQ1: Reduction Extent of Log Events for Anomaly Detection Methods", "content": "For RQ1, we conduct experiments on the studied models across various datasets with a = 0.02, to investigate the potential reduction in the volume of log events and lines. The experimental results are shown in table 1."}, {"title": "5.2 RQ2: Performance Following Event Reduction", "content": "For RQ2, we conduct a detailed analysis of the performance of anomaly detection models with and without event reduction, setting \u03b1 = 0.02.\nAs demonstrated in table 3, the performance of nearly all models improved with event reduction. Some even experienced significant enhancements. For instance, RobustLog on the Thunderbird dataset initially has an F1-Score of 69.2%. However, after event reduction, it soares to 100%. For the Isolation Forest model, its original performance on the Thunderbird dataset is nearly negligible with an F1-Score of 0.02%. However, after event reduction, this score improved to 17.3%. Even more impressively, for the LR model on the BGL dataset, by balancing Precision and Recall (Precision shifted from 19.3% to 98.2%, and Recall shifted from 82.9% to 45.6%), the overall F1-Score nearly doubles compared to the initial performance."}, {"title": "5.3 RQ3: Types of Log Events That Can Be Reduced", "content": "In RQ1 and RQ2, we discover that, for anomaly detection, the model performance can improve to varying degrees with a significant reduction in log events. Thus, in RQ3, we delve deeper into this phenomenon by case study.\nInitially, we examine the reasons for the enhancement in model performance after event reduction. In a particular instance with the LR model in the HDFS dataset, upon removing the entry \"E3,[*]Served block[*]to[*]\", there's a notable improvement: Precision by 0.63%, Recall by 22.88%, and F1-Score by 13.49%. The reason for such a phenomenon is that this work discovers that this\""}, {"title": "6 LogCleaner", "content": "Our empirical study identifies three types of log events that have different effects on the models. However, the experiments presented earlier required continuous model execution to determine the log events that can be reduced. Moreover, these methods do not provide an opportunity to reintroduce eliminated log events, even though they may represent potential future anomalies. In this section, we introduce LogCleaner, an automated approach to reduce log events without relying on model execution. Additionally, it allows for the reintroduction of some reduced log events when the system encounters false negatives, providing the model with the minimum log event set for current-state detection.\nAs demonstrated in figure 6, LogCleaner is divided into an profiling and an online component. In the profiling phase, it aims to automatically generate a reduced event set. To achieve this, raw logs are initially parsed into event templates and grouped into event groups with corresponding labels. Subsequently, TF-IDF is applied to filter out infrequently occurring events. The remaining events (Frequency Events) undergo processing by the Anti-Event Optimizer, which utilizes both event groups and associated labels, employing mutual information to eliminate anti-events. The events surviving this stage (Relevant Events) then go through the Duplicative-Event Separator, where the OPTICS algorithm clusters similar events, retaining only one event within each cluster. Finally, the reduced event set are generated based on the retained events.\nIn the online phase, LogCleaner serves as middleware between software systems and models, streamlining raw generated logs into reduced logs using the reduced event set generated in profiling phase. These reduced logs are then utilized for anomaly detection. Additionally, whenever there is a variation in the code of the software system, the system's logs and existing labels are re-extracted for re-profiling. This re-profiling process enables LogCleaner to adapt effectively to potential future anomalies."}, {"title": "6.1 Anti-Event Optimizer", "content": "In RQ3, it can be discovered that certain anti-events have no correlation with the occurrence of system anomalies or the specific anomalies that are triggered. Consequently, these anti-events negatively impact the model's classification performance.\nThe analysis suggests that the presence or absence of such log events bears no relation to labels. Therefore, mutual information, a method from the feature selection domain, can be employed to estimate the relationship between each log event and its corresponding label.\n$MI(e; 1) = \\Sigma_{(e, l)} p(e, l) log \\frac{p(e, l)}{p(e)p(l)}$ (2)\nAs illustrated in equation 2, all events are denoted as e \u2208 \u0395, all labels as I \u2208 L, p(e, l) represents the joint probability distribution of E and L, while p(e) and p(l) are the marginal probability distributions of E and L, respectively.\nUltimately, as depicted in equation 3, the mutual information for each event e is represented as the average of its mutual information with all labels 1. Among them, events with MI(e; L) \u2264 \u03b8anti are deemed as anti-events.\n$MI(e; L) = \\frac{1}{|L|} \\Sigma_{l\u2208L} MI(e; 1)$ (3)"}, {"title": "6.2 Duplicative-Event Separator", "content": "For duplicative-events, LogCleaner initially constructs an appear graph based on the co-occurrence patterns of log events. This entails representing each log event as a vector (ei). When two log events"}, {"title": "7 Experiment and Evaluation", "content": "This section evaluates the overall results, conduct an ablation study of LogCleaner, and assess the influence of hyperparameters.\nWe perform experiments on the 6 models and 3 datasets previously discussed. Unless otherwise specified, LogCleaner utilizes TF-IDF to filter out events with a frequency below 0.1. The Anti-Event Optimizer's threshold, \u03b8anti, is set to 0, while the threshold Odup for the Duplicative-Event Separator is set to 2."}, {"title": "7.1 Overall Evaluation Results", "content": "For anomaly detection, as illustrated in table 4, the number of reducible events is significant across all datasets. For the Thunderbird and BGL datasets, the events are reduced by approximately 70%. While this doesn't quite match the results from the previous empirical study, LogCleaner operates quickly and the reduced events are applicable across all models."}, {"title": "7.3 Effectiveness of Event Reduction Components", "content": "To evaluate the effectiveness of each component, we conduct an ablation study for anomaly detection. We assess under various configurations: utilizing TF-IDF alone, combining TF-IDF with the Anti-Events Optimizer (Anti.), and employing both TF-IDF and Anti-Events Optimizer (Anti.) alongside the Duplicative-Events Separator (Dup.). As presented in table 7, each component plays a role in reducing the number of events, with the Duplicative-Events Separatorr (Dup.) having the most pronounced effect.\nWe also validate the performance changes of models on the BGL and Thunderbird datasets. As illustrated in figure 7, in some instances, the model's performance remains unaffected with the addition of more components. However, for models such as LR and Isolation Forest in the BGL dataset and RobustLog in the Thunderbird dataset, the effectiveness of the models increases with the addition of components. Notably, the Anti-Event Optimizer (Anti.) plays the most significant role in this improvement."}, {"title": "7.4 Influence of Hyperparameters", "content": "To verify whether LogCleaner have chosen the optimal hyperparameters, experiments by varying the hyperparameters are carried out. As depicted in figure 8, it's evident that while in HDFS, as Odep increases, the number of event reductions rises, the model's performance diminishes. Conversely, in BGL, as odep increases, the model's performance remains consistent, but the number of event reductions drastically decreases. Thus, a setting of dep = 2 is a relatively optimal parameter."}, {"title": "8 Related Work", "content": "8.1 Log-based Anomaly Detection\nLog analysis for anomaly detection is a well-established research area[4, 7, 8, 13, 19, 20, 29, 33, 40, 49]. These methodologies typically involve extracting templates and key information from logs, followed by constructing models for anomaly detection and classification. There are mainly two types of models in this domain: graph-based and deep-learning models.\nGraph-based models leverage log events parsed from log files to create a graph-based representation. They detect conflicts and anomalies by comparing event sequences against this graph. For instance, LogFlash[20] utilizes a real-time streaming process for log transitions, enhancing the speed of anomaly detection. HiLog[19] performs an empirical study on four anti-patterns that challenge the assumptions underlying anomaly detection models, proposing a human-in-the-loop approach to integrate human expertise into log-based anomaly detection.\nDeep-learning models, conversely, use various neural networks to model sequences of log events. LogRobust[49] applies Term Frequency-Inverse Document Frequency (TF-IDF) and word vector-ization to convert log events into semantic vectors, thus improving the accuracy of anomaly detection. UniParser [29] employs a token encoder and a context encoder to learn patterns from log tokens and their adjacent contexts."}, {"title": "8.2 Log Compression & Placement", "content": "Given the substantial volume of logs generated by modern systems, assisting developers in adding appropriate logging statements is a promising research area, as highlighted in prior studies [43, 45, 46, 50]. Errlog[45], LogEnhancer[46], and Log20[50] enhance debugging capabilities by strategically inserting supplementary logging statements into the source code. Concurrently, LogReducer [43] leverages eBPF to manage logging overhead in performance-critical areas, ensuring that logging remains effective.\nHowever, the process of archiving massive volumes of logs over extended periods can introduce substantial storage overhead. To address this challenge, several studies have focused on log compression techniques to reduce storage requirements. Approaches such as Nanolog[41], CLP[36], and Cowic[27] construct dictionaries for fields in logs and replace strings by referencing these dictionaries. Additionally, LogZip [28] and RoughLogs [32] employ sophisticated statistical models to identify and reduce redundancy in logs."}, {"title": "9 Discussion", "content": "9.1 Application of LogCleaner\nLogCleaner has been implemented for a subset of users in Apache IoTDB, yielding positive feedback. Beyond its application as described in this paper, some users have employed LogCleaner to identify key events and alert developers about unnecessary print statements in the logs that can be removed. Developers can selectively delete these prints to enhance system performance. It has aided developers in discovering that over 50% of the log print statements in the system are unnecessary. As a result, the performance of Apache IoTDB has improved by approximately 8%."}, {"title": "9.2 Threats to Validity", "content": "The major threats to the validity can be identified as following.\nLimited models. In the empirical study, we mainly evaluate six representative models that have publicly available source code. In the future, we plan to re-implement more log-based detection models that have not released their source code, based on the descriptions provided in their papers. Subsequently, a larger-scale evaluation will be conduct.\nImplementation. We primarily utilize publicly available implementations of the studied models. The implementation of LogCleaner is also based on popular libraries, and three authors have thoroughly reviewed the source code to ensure accuracy and reliability.\nLimited datasets. The experiments are conducted on three log datasets. While they are widely used in existing studies on log-based anomaly detection, they may not fully represent all characteristics of log data. In future research, we plan to conduct experiments on additional datasets to cover a broader range of real-world scenarios."}, {"title": "10 Conclusion and Future Work", "content": "In this paper, we examine event reduction's effect on log-based anomaly detection models. Through empirical study on six models across three datasets, we identify three distinctive log event types that impact model performance differently. Based on these findings, we propose LogCleaner: an efficient methodology for the automatic reduction of log events in the context of anomaly detection. Serving as middleware between software systems and models, LogCleaner continuously updates and filters anti-events and duplicative-events in the raw generated logs. This approach not only accelerates the model's inference speed but also enhances the effectiveness of model classification.\nIn future research, we intend to leverage reinforcement learning to enhance the efficacy of log reduction. Furthermore, we also aspire to integrate LLM to pinpoint key events."}]}