{"title": "Thank You, Stingray: Multilingual Large Language Models\nCan Not (Yet) Disambiguate Cross-Lingual Word Sense", "authors": ["Samuel Cahyawijaya", "Ruochen Zhang", "Holy Lovenia", "Jan Christian Blaise Cruz", "Elisa Gilbert", "Hiroki Nomoto", "Alham Fikri Aji"], "abstract": "Multilingual large language models (LLMs)\nhave gained prominence, but concerns arise re-\ngarding their reliability beyond English. This\nstudy addresses the gap in cross-lingual seman-\ntic evaluation by introducing a novel bench-\nmark for cross-lingual sense disambiguation,\nStingrayBench. In this paper, we demon-\nstrate using false friends\u2014words that are or-\nthographically similar but have completely dif-\nferent meanings in two languages\u2014 as a pos-\nsible approach to pinpoint the limitation of\ncross-lingual sense disambiguation in LLMs.\nWe collect false friends in four language pairs,\nnamely Indonesian-Malay, Indonesian-Tagalog,\nChinese-Japanese, and English-German; and\nchallenge LLMs to distinguish the use of them\nin context. In our analysis of various mod-\nels, we observe they tend to be biased toward\nhigher-resource languages. We also propose\nnew metrics for quantifying the cross-lingual\nsense bias and comprehension based on our\nbenchmark. Our work contributes to develop-\ning more diverse and inclusive language mod-\neling, promoting fairer access for the wider\nmultilingual community.", "sections": [{"title": "1 Introduction", "content": "Multilingual large language models (LLMs) have\nbecome integral tools in a variety of tasks and\nlanguages (Bang et al., 2023; Yong et al., 2023;\nZhang et al., 2023a; Lovenia et al., 2024; Cahyaw-\nijaya, 2024; Cahyawijaya et al., 2024). While\nthese LLMs have remarkable capabilities, there\nare growing concerns about the reliability of their\nresponses especially in languages other than En-\nglish. Most evaluations address cross-lingual gen-\neralization in LLMs by assessing their ability on\nthe set of downstream tasks as the one used in\nEnglish (Cahyawijaya et al., 2021; Adelani et al.,\n2023; Kabra et al., 2023; Zhang et al., 2023b; Ade-\nlani et al., 2024; Cahyawijaya et al., 2024; Zhang\nand Eickhoff, 2024), many even directly translated\nfrom the source corpora (Hu et al., 2020; Cahyawi-\njaya et al., 2021; Winata et al., 2023; Cahyawijaya\net al., 2023a; Bandarkar et al., 2024; Singh et al.,\n2024). These evaluations reflect the cross-lingual\ngeneralization in the downstream application level,\nbut fail to capture the basic understanding of seman-\ntic meaning across different languages. This lack\nof semantic understanding further extends to the\nunexplained bias of multilingual LLMs towards cer-\ntain languages or language families which causes\nthe LLMs to respond in their preferred languages,\nleading to a significant misrepresentation of users'\nintent (Nomoto, 2023; Nomoto et al., 2024).\nOur work aims to explore the cross-lingual eval-\nuation of semantic meaning in LLMs and under-\nstand its underlying causes. We focus on the con-\ncept of \"false friends\", which are words or phrases\nthat sound similar in two languages but have dis-"}, {"title": "2 Related Works", "content": "2.1 Cognates and False Friends in NLP\nHomologous words that show systematic sound\ncorrespondences indicating common ancestry are\nknown as cognates (Atkinson, 2013). For example,\nbaru in Malay and bago in Tagalog are cognates\nbased on the systematic r-g sound correspondence,\nboth meaning 'new'. However, cognates do not nec-\nessarily have the same meaning, as is the case with\nbibir meaning 'lip' in Malay and bibig \u2018mouth' in\nTagalog, which show the same r-g correspondence.\nThe study of cognacy contributes to understanding\nthe historical lineage of languages and the recon-\nstruction of proto-languages (Campbell, 2013).\nMany recent works focus on the identification\nof cognates in genetically related languages (Bat-\nsuren et al., 2019, 2021; Bafna et al., 2022; Dinu\net al., 2023; Akavarapu and Bhattacharya, 2024).\nOne of the factors that make cognate identifica-\ntion non-trivial is the presence of false friends (or\nfalse cognates). False friends are words that are\northographically or phonetically similar but do not\nshare the same meaning (Allan, 2009). While many\nfalse friends are indeed cognates, some are not true\ncognates and can be mistaken for cognates. For\nexample, an Indonesian-Malay false friend polisi\n'police (Indonesian), policy (Malay)' traces back\nto different ancestor languages, i.e. Dutch (politie\n'police') and English (policy). Besides posing a\nchallenge to cognate identification, false friends\nalso constitute a major obstacle for translators, lan-\nguage learners and especially machine translation\nsystems. Studying false friends is not easy because\nit requires bilingual proficiency and as a result,\nfalse friends have received little attention. Current\nstudies on false friends focus on their collection\nand identification Ljubesic and Fi\u0161er (2013); Cas-\ntro et al. (2018); Uban and Dinu (2020). In our\npaper, we deal with false friends in multiple lan-\nguage pairs and use them as a tool to understand the\nproficiency of multilingual large language models."}, {"title": "2.2 Word Sense Disambiguation", "content": "Word Sense Disambiguation (WSD) task aims to\ndetermine the correct meaning of a polysemous\nword in a given context (Bevilacqua et al., 2021,\n2020; Blevins and Zettlemoyer, 2020) The task has\nalso been extended to a multilingual setting (Nav-\nigli et al., 2013; Pasini, 2021; Pasini et al., 2021; Su\net al., 2022), leveraging multilingual lexical knowl-\nedge bases (Navigli and Ponzetto, 2012; Bond and\nFoster, 2013). It has been a challenging task in NLP\nsince its early recognition by Weaver (1949) and re-\nmains critical in recent works that investigate \"the\ncurse of multilinguality\u201d (Conneau, 2019; Berend,\n2023) and universal representations across lan-\nguages (Wu et al., 2019; Wendler et al., 2024; Fer-\nrando and Costa-juss\u00e0, 2024; Zhang et al., 2024).\nBuilt upon previous studies, our work explores the\nability of multilingual LLMs to disambiguate word\nsense across languages. The closest related task\nis the Word-in-Context (WiC) task (Pilehvar and\nCamacho-Collados, 2018; Raganato et al., 2020),\nwhere they ask to classify the word usage given"}, {"title": "3 StingrayBench", "content": "3.1 Dataset Construction\nTo construct StingrayBench, native speakers of\nChinese, English, German, Indonesian, Japanese,\nMalay, and Tagalog are asked to list down com-\nmon words that existed between the following lan-\nguage pairs together with their meanings: English-\nGerman, Indonesian-Malay, Indonesian-Tagalog,\nand Chinese-Japanese. For this, the follow-\ning resources dealing with false friends are con-\nsulted: Wiktionary's lists of false friends, Ka-\nmus Komunikatif Nusantara: Indonesia-Malaysia,\nMalaysia-Indonesia (Mohd Sharifudin bin Yusop\nand Al Mudra, 2015), and Kamus Kata: Bahasa\nMelayu Malaysia-Bahasa Indonesia (Rusdi Abdul-\nlah, 2016). Some words in these resources are\nrejected as they turned out not to be false friends\nafter scrutiny. Moreover, it is not always easy to\nfind common words with identical spellings and\ncharacters, except for Indonesian-Malay. There-\nfore, we allow the use of words differing in capi-\ntalization (e.g. arm-Arm) in English-German, the\nuse of words with one edit distance (e.g. aku-ako)\nin Indonesian-Tagalog, and the use of words with"}, {"title": "3.2 Task Formulation", "content": "Using the sentences collected above, we propose\ntwo task formulations of different semantic granu-\nlarities as follows. Notice that we prompt the model\nin English as it is language-neutral for most of the\nlanguage pairs except for the English-German case.\nSemantic Appropriateness Given the data con-\nstruction as described above, we want to test the\nmodels' competence for sentence comprehension.\nIn this task, we prompt the model with: Which\nsentence is more semantically appropriate?. The\nfirst two options are the two sentences for the lan-\nguage pair respectively. A third option, that both\nsentences are appropriate, is also included. It is the\ncorrect option for the true cognates scenario but\nalso serves as a confounding option for the false\ncognates subset. We provide the example of the\nprompt and the target completion for the semantic\nappropriateness task in Figure 2.\nUsage Correction In this task formulation, we\nemphasize the usage of the specific cognate words\nby prompting the models with: Is the usage of\n[WORD] in this sentence correct? [SENTENCE].\nWe expect this task to be simpler as 1) the options\nare binary with no confounding options; and 2) spe-\ncific cognates are mentioned in the prompt which\npotentially serves as a task hint. We provide the\nexample of the prompt and the target completion\nfor the usage correction task in Figure 2."}, {"title": "3.3 Measure of Cognate Understanding\nAbility in LLMS", "content": "We define cognate understanding as the ability of\nLLMs to be able to correctly comprehend the se-\nmantic meaning of a cognate for both \"true cog-\nnate\" and \"false friend\". This is done through\nprobing LLMs with questions that ensure the un-\nderstanding of LLMs to the semantic nuances of\nthe cognate, which can be either \"true cognate\"\nor \"false friend\", in the context of the relevant\nlanguage pairs as described in Section 3.2. Us-\ning the tasks in StingrayBench, we measure the\nper-language accuracy of LLMs on each task and\nconduct further analysis as described below.\nStingray Plot To measure cognates understand-\ning ability of LLMs on a certain language pair\n< L1, L2 >, we need to take into account the\ncognate understanding quality on both L1 and\nL2. To do so, we derive our analysis based on\na 2-dimensional vector space and introduce the\nStingray plot. As shown in Figure 3, the Stingray\nplot presents two different contours: (1) a U-shaped\nangular contour with a minimum value of 0 at either\n0\u00b0 and 90\u00b0 angle and a maximum value of 1 at 45\u00b0\nangle; and (2) the radial contour with a minimum\nvalue of 0 at the bottom left corner and a maximum\nvalue of 100 at the top right corner. Using this"}, {"title": "4 Experiment Setting", "content": "4.1 Data Subsets\nWe utilize the collected StingrayBench for our\nevaluation which covers four language pairs, i.e.,\nEnglish-German, Indonesian-Malay, Indonesian-\nTagalog, and Chinese-Japanese. For each language,\nwe split the data into two different subsets based\non the phenomenon observed, i.e., true cognate\nand false friend subsets. As there are only limited\namount of data, we aggregate the score from mul-\ntiple tasks to improve the reliability of the LLMs\nprediction. The statistics of the StingrayBench per\nlanguage pair and per subset are shown in Table 1."}, {"title": "4.2 Model", "content": "Our evaluation covers a wide variety of LLMs,\nfrom monolingual, bilingual, and multilingual\nLLMs. For multilingual LLMs, we incorporate\nBLOOMZ (Le Scao et al., 2023; Muennighoff\net al., 2023), mT0 (Muennighoff et al., 2023), Aya-\n101 (Singh et al., 2024; \u00dcst\u00fcn et al., 2024), Aya-\n23 (Aryabumi et al., 2024), Qwen-2.5 (Yang et al.,\n2024; Team, 2024), Command-R (Cohere For AI,\n2024a,b), and GPT-40 mini (OpenAI et al., 2024).\nWe also explore LLMs with lower language cover-\nage or specifically adopted for certain languages in-\ncluding Phi-3 (Abdin et al., 2024), Cendol LLaMA-\n2 (Touvron et al., 2023; Cahyawijaya et al., 2024),\nCendol mT5 (Xue et al., 2020; Cahyawijaya et al.,\n2024), SEALLM v3 (Nguyen et al., 2024), SEA-\nLion v2.1 (Ong and Limkonchotiwat, 2023), Chat-\nGLM2 (GLM et al., 2024), and Yi (AI et al., 2024).\nWe exhaustively explore different size variations of\neach LLM with the scale ranging from 0.3B to 70B\nparameters to better understand the effect of scaling\non the cognate understanding of LLMs. The list of\nLLMs covered in our study is shown in Table 2."}, {"title": "4.3 Evaluation & Inference", "content": "For the inference, we conduct zero-shot prompting\nby prompting LLMs to answer the given prompt di-\nrectly using each of the corresponding chat formats\nsupported in each LLM. We perform two different\ntypes of inference: (1) likelihood-based inference;\nand (2) generation-based inference.\nLikelihood-based To perform likelihood-based\ninference, we follow the zero-shot prompting im-\nplementation from prior works (Cahyawijaya et al.,\n2023b,a; Zhang et al., 2023a; Lovenia et al., 2024).\nFor binary classification tasks, we use the label with\nthe highest marginal likelihood given the prompt.\nFor multiple-choice tasks, we provide the choices\nafter the query and take the answer choice label,\ni.e., A, B, or C, with the highest likelihood. We opt\nfor the likelihood-based for open-source LLMs as\nwe cannot perform this on the API-based LLMs.\nGeneration-based To generalize and ensure the\nrobustness of our results, we also do inference with\na generation-based approach. The prompts are\nshown in 2 and with an additional sentence that\nasks LLMs to limit their answers to \"A, B or C\" or\n\"Yes or No\u201d. To get the final result, we post-process\nthe generated responses: as an example, for the\nsemantic appropriateness task, LLMs sometimes\nanswer \"A and B\" instead of the option \"C\u201d. We\ntest all LLMs listed in Table 2. Nonetheless, we\nalso note that in some LLMs, they often fail to\nfollow the given instructions."}, {"title": "5 Analysis and Discussion", "content": "We show the stingray plot of the language-and-\ntask aggregated results from our experiment in Fig-\nure 4. We observe a clear distinction of LLMS'\ncognate understanding between true cognate and\nfalse friend subsets, and provide further analysis of\nthis behavior in the following section."}, {"title": "5.1 Do LLMs Understand True Cognates?", "content": "We showcase the breakdown performance per lan-\nguage pair on the true cognate subset in Figure 5a.\nAlthough, some smaller-scale LLMs does not per-\nform as good, but larger LLMs tend to yield strong\ncognate comprehension score. Some LLMs such\nas Aya-23 (35B), ChatGLM2 (6B), Phi-3 Small,\nQwen 2.5, Yi 1.5 (34B), and GPT-40-mini even\nachieve almost perfect scores with average cog-\nnate comprehension scores \u226590%. This indicates\nthat most LLMs understand the semantics of a true\ncognate and can incorporate it properly in both\nlanguages in the corresponding language pair."}, {"title": "5.2 Can LLMs Distinguish False Friend?", "content": "While all LLMs show low cognate bias on the false\nfriend subset as shown in Figure 6, most LLMs\nperform very poorly on the cognate comprehen-\nsion score in most language pairs. For instance,\nmost LLMs yield comprehension scores that are\nclose to a random baseline as shown in Figure 5b.\nThis signifies that most existing LLMs could not\neven distinguish the sense of false friends across\ndifferent languages emphasizing an urgent need for\na more advanced method on cross-lingual sense\ndisambiguation in multilingual LLMs.\nLanguage Representation Matters Despite the\nquality on the false friend subset is generally low\nacross all LLMs, most LLMs show higher perfor-\nmance on the English-German language pair, in-\ncluding Qwen2.5 (14B and 32B), Yi (34B), Aya 23\n(35B), and GPT-40-mini. This result indicates that\nmost of the existing multilingual LLMs, despite\nfurther tuning on other languages, are still English-\ncentric. This observation is consistent with the\nfact that most existing multilingual LLMs are pri-\nmarily trained on English data (Xue et al., 2020;\nMuennighoff et al., 2023; Chowdhery et al., 2023;\n\u00dcst\u00fcn et al., 2024; Aryabumi et al., 2024), high-\nlighting the need for enhanced representation of\nnon-English languages.\nLanguage Similarity Affects False Friend Dis-\nambiguation We observe that the performance\nof the Indonesian-Tagalog language pair tends to\nbe higher than the performance of the Indonesian-"}, {"title": "6 Conclusion", "content": "Our work presents a comprehensive evaluation\nof cross-lingual sense disambiguation in multilin-\ngual LLMs. Through the introduction of Stingray-\nBench, we measure and analyze semantic un-\nderstanding across languages. By studying false\nfriends and true cognates, we have identified\nkey factors contributing to semantic biases. Our\nmethodology, including the stringray plot and eval-\nuation metrics, i.e., cognate bias and cognate com-\nprehension score, offers a novel approach to un-\nderstanding cross-lingual sense disambiguation in\nmultilingual LLMs. The generalization of our find-\nings across various language pairs highlights the\nsignificance of this work. Our StingrayBench is\nnot only suitable for measuring the cross-lingual\nsense disambiguation in LLMs, but also a suitable\ntestbed for investigating language selection bias in\nmultilingual LLMs. We believe that our contribu-\ntions provide a foundation for further enhancing the\ncross-lingual capabilities of LLMs, ultimately im-\nproving their reliability and performance in diverse\nlinguistic contexts and advancing the development\nof more inclusive and unbiased multilingual LLMs."}, {"title": "Limitation", "content": "Dataset Size Despite the enormous efforts on\nannotating with multiple native speakers across dif-\nferent language pairs, due to the limited amount\nof available false friends and true cognates across\ndifferent language pairs, our StingrayBench con-\nsists of only around 150-200 samples per language\npairs. To cater to this limitation, we try to increase\nthe task, allowing probing of multilingual LLMs\nwith bigger sample sizes. We leave further explo-\nration on how to increase the amount of data of\nfalse friend and true cognate to future work.\nBenchmark Coverage Due to the difficulty\nin finding annotators, our StingrayBench only\ncovers four language pairs, i.e., English-German,\nIndonesian-Malay, Indonesian-Tagalog, and\nChinese-Japanese. There are many other poten-\ntial language pairs that can be covered in the\nbenchmark, such as Sloven-Croatian, Spanish-\nPortuguese, etc. We expect future work to extend\nthe generalization of our benchmark and findings\nto other language pairs."}, {"title": "Ethics Statement", "content": "This work introduces a novel benchmark for cross-\nlingual sense disambiguation and evaluation in mul-\ntilingual large language models (LLMs), aiming\nto uncover biases and limitations in their seman-\ntic understanding across languages. Throughout\nthe development of this benchmark, several ethical\nconsiderations were taken into account.\nInclusivity and Fairness The primary motiva-\ntion of our work is to highlight and address the\nbiases present in multilingual LLMs, particularly\ntoward high-resource languages. We recognize\nthat current language technologies often underper-\nform speakers of low-resource languages, which\ncould reinforce language hierarchies and contribute\nto the marginalization of these linguistic com-\nmunities. By incorporating language pairs such\nas Indonesian-Malay and Indonesian-Tagalog, we\nstrive to promote inclusivity and fairness in the eval-\nuation of LLMs and advocate for broader linguistic\ndiversity in NLP research.\nBias and Misrepresentation One of the key\ngoals of our research is to identify bias in cross-\nlingual semantic disambiguation, especially con-\ncerning the handling of false friends and true cog-\nnates. We understand that biases in LLMs can"}, {"title": "A Annotation Guideline", "content": "A.1 Annotation Objective\nThe goal of this annotation task is to create a dataset\nthat distinguishes between false friends and true\ncognates across various language pairs. Annotators\nwill work with native speakers to identify and cat-\negorize common words, construct sentences, and\ntranslate them to ensure accurate representation.\nA.2 Word Selection and Criteria\nFalse Friends: Words with the same spelling or\ncharacters but different meanings in the respec-\ntive languages. True Cognates: Words with the\nsame spelling, characters, and meanings in both\nlanguages. For collecting the common words, an-\nnotators incorporate the following sources:\n\u2022 Wiktionary's lists of false friends 6\n\u2022 Kamus Komunikatif Nusantara: Indonesia-\nMalaysia, Malaysia-Indonesia (Mohd Shari-\nfudin bin Yusop and Al Mudra, 2015)\n\u2022 Kamus Kata: Bahasa Melayu Malaysia-\nBahasa Indonesia (Rusdi Abdullah, 2016)\nA.3 Annotation Process\nAnnotation Flow\n\u2022 Translation and Replacement For false\nfriends, given a false friend word, an anno-\ntator will make the correct sentence in their\nlanguage and translate it into English. The an-\nnotator of the other language in the pair will\nthen translate the English translation into their\nnative language. The target false friend word\nwill be replaced with a different word that con-\nveys the intended meaning. This will result in\na semantically odd sentence.\n\u2022 Cognate Agreement and Translation: For\ntrue cognates, both annotators will first agree\non an English sentence. The English sentence\nwill then be translated into their respective\nnative languages to construct the true cognate\nsentence pair.\nAllowed Variations\n\u2022 English-German: Words differ in capitaliza-\ntion with a maximum of one edit distance.\n\u2022 Chinese-Japanese: Words with different char-\nacters developed from the same origin.\n\u2022 Indonesian-Malay: Exact match words.\n\u2022 Indonesian-Tagalog: Words with maximum\nof one edit distance.\nA.4 Dataset Entries\n\u2022 False Friends: Each false friend will typically\nhave two entries one for the correct usage\nand one for the incorrect usage.\n\u2022 True Cognates: Each true cognate will have\none entry, as both native language sentences\ntranslate correctly.\nA.5 Examples\nFalse Friend (Indonesian-Tagalog):\n\u2022 Indonesian sentence: Pagi ini saya begitu\nsenang\n\u2022 English translation: This morning, I feel so\nhappy\n\u2022 Tagalog translation: Ako ay masaya ngayong\numaga\n\u2022 Replacement: Ako ay masaya ngayong pagi\n(\"Today's stingray, I feel happy\")\nTrue Cognate (Indonesian-Malay)\n\u2022 English sentence: \"That apple has many cater-\npillars.\"\n\u2022 Indonesian sentence: Apel itu banyak ulat\n\u2022 Malay sentence: Epal itu ada banyak ulat.\nA.6 Additional Guidance for Annotators\n\u2022 Ensure a clear understanding of the word's\nmeaning and context.\n\u2022 Construct sentences that are natural and gram-\nmatically correct in your native language.\n\u2022 Pay attention to the nuances and potential vari-\nations in word usage.\n\u2022 For false friends, aim for a semantically odd\ntranslation to highlight the semantic differ-\nences between the two sentences.\n\u2022 Collaborate effectively with your partner an-\nnotator to ensure accurate translations and rep-\nresentations."}, {"title": "B Dataset Card", "content": "Dataset Name: StingrayBench\nB.1 Dataset Description\nOverview StingrayBench is a dataset designed to\nevaluate models' understanding of semantic appro-\npriateness and cognate word usage across multiple\nlanguage pairs. The dataset focuses on false friends\nand true cognates, which are words with similar\nspellings or characters but different meanings or\nadditional meanings in different languages.\nLanguage Pairs The dataset covers the follow-\ning language pairs:\n\u2022 English-German (EN-DE)\n\u2022 Chinese-Japanese (ZH-JA)\n\u2022 Indonesian-Malay (ID-MS)\n\u2022 Indonesian-Tagalog (ID-TL)\nB.2 Dataset Construction\nNative speakers of the respective languages were\ninvolved in constructing the dataset. They listed\ncommon words between language pairs and their\nmeanings, consulting resources on false friends.\nThe words were then categorized as false friends\nor true cognates.For each word, annotators created\nsentences in their native language and provided En-\nglish translations. The sentences were designed\nto showcase the correct and incorrect usage of the\ntarget words. In the case of false friends, the sen-\ntences were manipulated to produce semantically\nodd translations.\nB.3 Dataset Statistics\nThe dataset contains a total of 705 entries, includ-\ning: 259 true cognate entries and 446 false friend\nentries. The distribution of entries across language\npairs is as follows:\n\u2022 EN-DE: 196 entries (98 true cognates, 98 false\nfriends)\n\u2022 ZH-JA: 165 entries (51 true cognates, 114\nfalse friends)\n\u2022 ID-MS: 186 entries (52 true cognates, 134\nfalse friends)\n\u2022 ID-TL: 158 entries (58 true cognates, 100 false\nfriends)\nB.4 Task Formulation\nSemantic Appropriateness In this task, models\nare prompted to determine which sentence is more\nsemantically appropriate. The prompt includes two\nsentences from the language pair and a third option\nindicating that both sentences are appropriate. This\ntask aims to test the model's comprehension and\nunderstanding of the semantic nuances between the\nlanguage pairs.\nUsage Correction The usage correction task fo-\ncuses on the correct usage of specific cognate\nwords. Models are prompted with a sentence con-\ntaining a cognate word and asked to determine if\nthe word's usage is correct. This task provides a\nmore targeted evaluation of the model's ability to\nhandle cognate words accurately.\nB.5 Example Prompts and Completions\nSemantic Appropriateness\nPrompt:\nWhich sentence is more semantically appropriate?\nA. \"Ich habe einen Arm.\" (German)\nB. \"I have an Arm.\" (English)\nC. \"Both sentences are appropriate.\"\nTarget Completion: \"C. Both sentences are\nappropriate.\"\nUsage Correction\nPrompt:\nIs the usage of \"pagi\" in this sentence correct?\n\"Ako ay masaya ngayong pagi.\" (Tagalog)\nTarget Completion:\n\"No, the usage of 'pagi' is incorrect. 'Pagi' means\n'stingray' in Tagalog, and the sentence should use\n'umaga' for 'morning'.\"\nB.6 Dataset Licensing Information\nTo promote accessibility, encourage collaboration,\nand facilitate knowledge sharing, StingrayBench\nwill be made available to the public under the Cre-\native Commons Attribution-ShareAlike 4.0 Inter-\nnational (CC-BY-SA 4.0) license. This license\nensures that the dataset is accessible and can be\nutilized by a wide range of individuals and organi-\nzations including for commercial users."}]}