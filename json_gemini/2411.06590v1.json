{"title": "CriticAL: Critic Automation with Language Models", "authors": ["Michael Y. Li", "Vivek Vajipey", "Noah D. Goodman", "Emily B. Fox"], "abstract": "Understanding the world through models is a fundamental goal of scientific research. While large language model (LLM) based approaches show promise in automating scientific discovery, they often overlook the importance of criticizing scientific models. Criticizing models deepens scientific understanding and drives the development of more accurate models. Automating model criticism is difficult because it traditionally requires a human expert to define how to compare a model with data and evaluate if the discrepancies are significant- both rely heavily on understanding the modeling assumptions and domain. Although LLM-based critic approaches are appealing, they introduce new challenges: LLMs might hallucinate the critiques themselves. Motivated by this, we introduce CriticAL (Critic Automation with Language Models). CriticAL uses LLMs to generate summary statistics that capture discrepancies between model predictions and data, and applies hypothesis tests to evaluate their significance. We can view CriticAL as a verifier that validates models and their critiques by embedding them in a hypothesis testing framework. In experiments, we evaluate CriticAL across key quantitative and qualitative dimensions. In settings where we synthesize discrepancies between models and datasets, CriticAL reliably generates correct critiques without hallucinating incorrect ones. We show that both human and LLM judges consistently prefer CriticAL's critiques over alternative approaches in terms of transparency and actionability. Finally, we show that CriticAL's critiques enable an LLM scientist to improve upon human-designed models on real-world datasets.", "sections": [{"title": "1 Introduction", "content": "A longstanding goal of artificial intelligence research is to automate the discovery of scientific models [16, 27]. The rapid development of LLMs with remarkable reasoning capabilities and general knowledge has created exciting new opportunities within this domain. Recent work has shown that LLM-based scientific agents can propose research ideas [23], discover scientific models [17], and implement experiments [12, 18]. These results highlight the promise of using LLMs to automate many important aspects of scientific discovery. However, they overlook the crucial role that model criticism, or understanding the limitations of a model, plays in driving scientific progress. Model criticism deepens our understanding and often motivates new models. Furthermore, automated methods for criticism can improve the reliability of LLM-based scientific discovery systems, as LLMs are prone to systematic hallucinations [18, 29] that could undermine the broader goal of automating scientific discovery.\nModel criticism is hard to automate because it is inherently dependent on the model and problem domain. In particular, it involves (1) determining which aspects to compare between the model and data and (2) evaluating the significance of any differences. Each of these tasks typically requires substantial human expertise [7]. While leveraging LLMs is an initially appealing approach to automation, it introduces new challenges: LLMs might also hallucinate the critiques themselves, undermining the effectiveness of automated model criticism."}, {"title": "2 Background", "content": "In this section, we discuss model criticism techniques from different domains. Crucially, we can often formalize finding discrepancies as identifying suitable test statistics, using those statistics to compute discrepancies between model predictions and data, and validating their significance using domain knowledge.\nRegression analysis In regression analysis, we begin with a dataset D = {X, Y} of input features X and targets Y; our goal is to predict Y from X. Given model predictions Ypred, we perform model diagnostics that target the standard assumptions of linear regression (e.g., linearity, homoscedasticity, uncorrelated errors). For example, to evaluate whether homoscedasticity holds, we can plot the residuals against the input features. We can then either informally assess whether the pattern in the residuals indicates a significant departure from homoscedasticity or perform statistical tests.\nComputational models Computational models often make simplifying assumptions that can lead to systematic errors, even after the parameters of these models are calibrated. This might be due to imperfect physical knowledge or systematic measurement errors; these systematic errors are often known as model inadequacies. Bayarri & Berger [2] introduce a framework for understanding these inadequacies that involves defining domain-specific evaluation criteria or performing sensitivity analyses and checking whether these accord with scientific intuition. Another very influential approach is to cast this as a statistical modeling problem and directly build a statistical model of"}, {"title": "Bayesian statistical models", "content": "In statistical modeling, we model the data as a probability distribution. More formally, a statistical model defines a joint probability distribution p(Y, \u03b8|X, H) over observed variables y and latent variables \u03b8; we use H to indicate a specific class of statistical models and the dataset D = {X, Y} can include both observations Y that we model as random variables and additional quantities X that we treat as fixed. By marginalizing out the latent variables, we obtain the posterior predictive distribution\n$$p(Y^{pred} | D, H) = \\int P(Y^{pred} | \\theta, X, H)p(\\theta | D, H)d\\theta \\qquad (1)$$\nA common technique for evaluating such a model is a posterior predictive check (PPC) [4, 8, 20, 22]. In brief, PPCs ask if the posterior predictive distribution captures important properties of the data. Concretely, to perform a PPC, we first draw samples from the posterior predictive distribution, {Y^{pred}_m ~ p(Y^{pred} | D, H). We then choose a test statistic T(X, Y^{pred}) that can reveal some property of the data that is not well-captured by the model samples. To compare the posterior predictive samples against the dataset, we compute the test statistic over both samples (forming a null distribution) and data. For a PPC to be useful, the test statistic must be chosen in a model-dependent way and choosing an appropriate test statistic is an important step in many applied modeling settings [3, 9, 25]. For example, when criticizing a Poisson model, one might check for over-dispersion by computing the variance-to-mean ratio. Crucially, posterior predictive checks do not require human intervention, since they automatically generate a quantitative measure of the significance of any discrepancy via the posterior predictive p-value; we discuss this in more detail in Section 3.1."}, {"title": "3 Method: CriticAL", "content": "In this section, we describe CriticAL, our system for finding systematic discrepancies between a scientific model and dataset. We provide a brief overview here; for a schematic overview, see Figure 1. CriticAL takes as input: dataset metadata, a symbolic representation of a model (e.g., program) and model samples. Given these, CriticAL produces significant discrepancies. Each discrepancy is represented as a test statistic implemented as a Python function, an executable artifact that programmatically expresses the discrepancy, and a natural language criticism."}, {"title": "3.1 Automatically proposing and evaluating discrepancies", "content": "As we saw in Section 2, we can often formalize finding discrepancies between model predictions and data as identifying suitable test statistics. Designing"}, {"title": "Evaluating significance of discrepancies via hypothesis tests", "content": "We now describe how CriticAL uses the test statistics to identify significant discrepancies. In brief, we use model samples to approximate a null distribution over the test statistic and then compute an empirical p-value. We assume the user can generate data from the model {Y^{pred}_i}_{i=1}^m. This is not restrictive requirement and how the user generates the model samples is a design choice; for example, we can do this for any model that describes a generative process for the data.\nWe describe how to construct an empirical p-value pk given Tk and {Y^{Pred}_i}_{i=1}^m below.\n1. We approximate the null distribution of the test statistic by computing the test statistic over the model samples {T(X, Y^{pred})}_{i=1}^m.\n2. We locate the test statistic of the observed data T(X, Y) within this null distribution to obtain an empirical p-value. That is, we compute\n$$P(T(X, Y^{pred}) > T(X, Y)|D, H) \\approx \\frac{1}{m} \\sum_{i=1}^m \\mathbb{1}_{\\{T(X, Y^{pred}_i) \\ge T(X, Y)\\}\\} \\qquad (2)$$"}, {"title": "Instantiating the framework for Bayesian models", "content": "In our experiments, we focus our evaluation on Bayesian models because they are widely used in scientific settings [6, 10]. In our context, Bayesian models are also appealing because they can be expressed symbolically as probabilistic programs [11, 24] and we can choose the model samples to be posterior predictive samples {Y^{pred}_m}_{m=1}^n (Equation 1). The corresponding posterior predictive p-value has an intuitive interpretation: how atypical is y under the posterior distribution p(Y^{pred}|D, H) with respect to the discrepancy measure defined by Tk?"}, {"title": "3.2 Interfacing with LLM science agents via natural language criticism", "content": "In many situations, we might want to integrate CriticAL within a broader scientific discovery system, involving either human or LLM scientists. Therefore, CriticAL also produces natural language criticism. This design choice is motivated by several considerations. By offering critiques in natural language, which is flexible and generic, the system provides an additional medium for users to interpret results, which can be useful in fields where training in formal modeling is less common. Second, this design choice is natural given recent advances in LLM-based agents for scientific discovery and modeling [12, 17].\nWe prompt an LLM to produce natural language criticism hk that summarizes the discrepancy implied by test statistic Tk and its p-value pk. Specifically, we ask the LLM to synthesize the test statistic in a way that's informative to a colleague revising some initial model. For examples of the natural language critiques produced, see Section A.2 and for the prompt see Figure 9.\nWe can easily integrate these three artifacts within an LLM-based scientific discovery system. Specifically, we provide the system with (1) a Python implementation of the test statistic Tk, (2) the natural language hk, and (3) the initial model; in our experiments these models will be probabilistic programs in pymc or stan [1, 5]. We use the LLM-based system for generating probabilistic programs introduced by Li et al. [17].\nIn general, these hypothesis tests are cheap relative to the cost of model fitting. For example, posterior inference is the dominating cost for Bayesian models and performing posterior predictive checks is cheap given posterior samples. Thus, CriticAL will generally introduce minimal overhead to the overall cost of an AI scientist system."}, {"title": "4 Experiments", "content": "In this section, we present experimental results that evaluate key quantitative and qualitative properties of our system. We begin by illustrating the pitfalls of a naive LLM in a synthetic regression setting. We then systematically study CriticAL's ability to avoid hallucinations and discover true discrepancies by analyzing its true and false positive rates in a setting where we synthesize discrepancies between models and datasets. We then evaluate the transparency and interpretability of our system in human user and LLM evaluations, as well as the actionability of the natural language criticism in helping an LLM-based system to revise models."}, {"title": "4.1 Experiment 1: Naive LLM-based critic hallucinates in synthetic regression task", "content": "In an initial case study, we show that a naive LLM critic consistently hallucinates but CriticAL does not. Specifically, we compare the model revision changes induced by the critiques produced by CriticAL and the naive approach, in a setting where we adversarially introduce spurious \u201cdistractor\" features into a dataset. For an overview, see Figure 2.\nGenerating a regression dataset with spurious features We generate a synthetic dataset inspired by the radon dataset, a commonly used dataset in regression analysis. We generate the target,"}, {"title": "4.2 Experiment 2: Statistical analysis of hallucinations and true discoveries", "content": "A reliable critic system should avoid hallucinations (i.e., generating false positives) and discover true discrepancies when they exist. In this section, we study this through a statistical lens and characterize"}, {"title": "4.3 Experiment 3: Analyzing key qualitative properties of test statistics for real-world model-dataset pairs", "content": "In the previous sections, we evaluated CriticAL's statistical properties. However, users interacting with an LLM-based critic system may care just as much about key qualitative properties such as"}, {"title": "4.4 Experiment 4: CriticAL generated criticism drives model improvements", "content": "Model criticism should ideally be actionable and aid a user (either LLM or human) in model revision. In our final experiment, we use the model criticism generated by CriticAL in the previous section"}, {"title": "5 Conclusion", "content": "We introduced CriticAL, a framework for automated model criticism that leverages LLMs to identify discrepancies between a model and dataset and then applies hypothesis tests to assess the significance of discrepancies. CriticAL serves as a lightweight verifier, validating both scientific models and critiques within a hypothesis testing framework. Our experiments demonstrate that CriticAL reliably identifies true discrepancies without hallucinating false critiques. Furthermore, both human and LLM judges preferred CriticAL's critiques over alternative approaches. CriticAL critiques enabled an LLM-based system to substantially improve upon expert designed models. By automating model criticism, CriticAL represents a step toward more reliable automatic scientific discovery systems.\nWhile our evaluation was limited to Bayesian models, which are commonly used in scientific domains, CriticAL's design is versatile: the only requirements are the ability to sample data from the model and a symbolic representation of the model. An exploration of other common classes of scientific models [6] is an exciting direction for future work."}, {"title": "6 Acknowledgements", "content": "This work was supported in part by AFOSR Grant FA9550-21-1-0397, ONR Grant N00014-22-1-2110, and an NSF Expeditions Grant, Award Number (FAIN) 1918771. EBF is a Chan Zuckerberg Biohub - San Francisco Investigator.\nWe thank Omar Shaikh and Justin Shen for valuable feedback on this paper. We also thank Peter Yoon, Daniel Same, and Artem Khan for discussions."}]}