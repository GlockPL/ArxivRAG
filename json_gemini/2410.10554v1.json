{"title": "ROSAR: An Adversarial Re-Training Framework for Robust Side-Scan Sonar Object Detection", "authors": ["Martin Aubard", "L\u00e1szl\u00f3 Antal", "Ana Madureira", "Luis F. Teixeira", "Erika \u00c1brah\u00e1m"], "abstract": "This paper introduces ROSAR, a novel framework enhancing the robustness of deep learning object detection models tailored for side-scan sonar (SSS) images, generated by autonomous underwater vehicles using sonar sensors. By extending our prior work on knowledge distillation (KD), this framework integrates KD with adversarial retraining to address the dual challenges of model efficiency and robustness against SSS noises. We introduce three novel, publicly available SSS datasets, capturing different sonar setups and noise conditions. We propose and formalize two SSS safety properties and utilize them to generate adversarial datasets for retraining. Through a comparative analysis of projected gradient descent (PGD) and patch-based adversarial attacks, ROSAR demonstrates significant improvements in model robustness and detection accuracy under SSS-specific conditions, enhancing the model's robustness by up to 1.85%. ROSAR is available at https://github.com/remaro-network/ROSAR-framework.", "sections": [{"title": "I. INTRODUCTION", "content": "With the growing interest in deep-sea exploration for oceanographic research [1] and energy infrastructure (e.g., gas pipelines [2], wind turbine structures [3]), the development of underwater monitoring systems, particularly autonomous underwater vehicles (AUVs), has seen significant advancements over the last decade. Due to the unique underwater environment, common sensors used in terrestrial and aerial robotics, such as cameras and LiDAR, are limited in their underwater applications. Consequently, sonar, which operates based on sound, is the most commonly used sensor underwater, overcoming limitations related to luminosity and reflection. However, despite its broad use in underwater robotics, sonar is susceptible to underwater environmental noise from other sonars, marine animals, and the deep sea.\nAs the trend moves toward implementing deep learning (DL) models onboard for real-time detection and decision-making [4], ensuring the reliability of these models becomes crucial. Therefore, operators must trust that the DL models will consistently provide accurate detections even under such challenging conditions. Nonetheless, ensuring this trust has been an ongoing challenge for several years due to the unpredictable underwater noise. Previous work has focused on sonar noise filtering to reduce noise in sonar images [5], [6], which can result in information loss. Inspired by trends in generative models, current efforts focus on using generative adversarial networks (GANs) [7] to extend noisy datasets, thereby improving model robustness [8], [9]. While useful for data enhancement, these strategies neglect to examine how the model's behavior is influenced under adverse conditions.\nGiven the widespread use of DL, a growing research line for neural network verification (NNV) has emerged, aiming to rigorously validate DL models against specific safety and robustness criteria, contributing to significant insights into their reliability [11]. In computer vision, NNV is commonly used for classification tasks, ensuring that the DL model consistently outputs the correct class even if a certain amount of noise is present in the input data [12]. However, its application to more complex models like object detection remains limited, primarily due to computational constraints.\nRecognizing these challenges, particularly in the context of side-scan sonar (SSS) imagery, our approach is focused on leveraging NNV (through adversarial attacks) for robustness improvement. Expanding on our previous work on knowledge distillation (KD) [10] applied to the YOLOX model [13], we introduce an extended framework designed to enhance the robustness of object detection models against SSS-specific noise. This framework involves defining specific safety or robustness properties and retraining the model using counter-examples (CEs) generated in cases when these properties are violated."}, {"title": "II. RELATED WORK", "content": "Object Detection aims to accurately detect and classify objects on an image (or video). When deploying a neural network model for object detection into an embedded system, the typical trade-off is between choosing efficient (but less accurate) models and accurate (but less efficient) models. Focusing on improving the efficiency of the embedded model, our previous work [10] leverages KD [14] to distillate the knowledge from a teacher (larger model) to a student (smaller model), achieving an improvement in the accuracy of the smaller model, while maintaining its efficiency. However, while [10] focuses on knowledge distillation, it does not address the issue of model robustness. Thus, ROSAR is designed to ensure accurate output predictions, even in the presence of noises absent from the training dataset.\nAdversarial attacks on neural networks have gained significant attention in recent years, especially in safety-critical applications such as autonomous driving and industrial robotics, where the DL output prediction must be robust for safe operation. Szegedy et al. [15] first demonstrated that neural networks are vulnerable to adversarial attacks, i.e., minor perturbations to the input data can cause the model to make incorrect predictions with high confidence. Goodfellow et al. introduced the fast gradient sign method (FGSM) [16] to craft adversarial examples by leveraging model gradients. Unlike more straightforward methods like FGSM, which applies a single step of gradient ascent, Madry et al. [17] developed the projected gradient descent (PGD) method. This iterative approach performs multiple iterations of small perturbations, refining adversarial attacks and enhancing the success rate of the attack. Expanding on these concepts, Zhang et al. proposed alpha-beta-CROWN [18], a robust neural network verifier that certifies the robustness of neural networks against adversarial attacks, by combining branch-and-bound techniques with linear bounds propagation, guaranteeing tight robustness. Furthermore, as pre-check prior to complete verification, the tool uses the PGD attack as an efficient, but incomplete method for falsifying the safety of a network.\nAdversarial patch attacks, introduced by Brown et al. [19], search for a specific patch that, when displayed on an image, can deceive the model in both classification and regression tasks. Unlike typical attacks, such as the PGD, which modify the entire image, the adversarial patch is a localized modification designed to cause misclassification. It does not require access to the entire image, and can be effective across various objects and scenes. Wu et al. [20] introduced a method for generating adversarial patches effective in both digital and real-world attacks on object detectors. This pioneering work focuses on the transferability of patch attacks across various models, including a total variation penalty to ensure patch smoothness. Building on these advancements, the DPatch [21] method refines adversarial patch strategies by introducing targeted (predicting a specific incorrect class) and untargeted patches (causing the model to make any incorrect prediction). More recently, Shrestha et al. [22] developed an adversarial patch specifically for the YOLOv5 model, achieving an 80% success rate on the VisDrone dataset designed for unmanned aerial vehicle (UAV) applications. Their approach integrates total variation loss, printability loss, patch saliency loss, and patch objectiveness loss during the patch generation process, significantly enhancing the success rate of the attack against object detectors.\nSurprisingly, the current object detection literature based on sonar images does not yet show significant interest in adversarial attacks, particularly in the context of adversarial patch attacks. Despite the current lack of previous works focusing on adversarial attacks for sonar images, Q. Ma et al. proposed the noise adversarial network (NAN) [23], which generates noise for sonar datasets and applies it to the Faster R-CNN object detection model, improving detection robustness by 8.9% mean average precision and introduced the Lambertian adversarial sonar attack (LASA) [24] improving SSS classifier robustness."}, {"title": "III. METHODOLOGY", "content": "Our proposed framework, illustrated in Fig. 1, is designed with two primary objectives: (1) leveraging KD to enhance the efficiency and accuracy of the YOLOX object detection model and (2) increasing the model's robustness against noise. While the first objective has been covered in [10], this paper centers on the second objective, which integrates the KD-enhanced model into an adversarial retraining loop. Validation is conducted on field-collected noisy SSS images, with robustness assessment using adversarial datasets generated by PGD and adversarial patch attacks.\nPGD Attack. Using the alpha-beta-CROWN tool, the PGD attack is conducted based on the safety properties defined in Section V. If the model violates these properties then the tool generates a counter-example, producing an adversarial image that triggers the violation. To characterize robustness, binary search is used to determine a noise tolerance upper bound, below which correct predictions are maintained, as detailed in Section VI-A."}, {"title": "IV. DATASETS", "content": "The lack of open-source sonar datasets often forces underwater robotics researchers to collect and annotate their own data, a time-consuming and expensive process that limits the ability to compare scientific results and the reproducibility of experiments [25]. Thus, to validate our proposed method, we introduce SWDD-Validation, which is composed of three novel open-source SSS datasets: SWDD-Clean, SWDD-Surface, and SWDD-Noisy, all of them extending our previously published dataset SWDD [10]. The datasets are available at https://zenodo.org/records/10528135.\nIn this paper, we train the original model with the SWDD dataset and validate it using the three proposed datasets, aiming to evaluate how the robustness of the model may vary under different sonar and noise conditions. Similarly to the SWDD dataset, the three new datasets are the results of wall inspection surveys, collected at the Porto de Leix\u00f5es harbor using a Klein 3500 sonar mounted on a Light Autonomous Underwater Vehicle (LAUV) [26]. The datasets, providing meta-data on the number of images, bounding boxes, sonar frequency, range per transducer, and the total resolution of the generated images. provides a sample image from each dataset: the SWDD-Clean dataset, which includes data from the same mission as the original SWDD dataset; the SWDD-Surface dataset, captured while the LAUV was on the surface during windy weather, featuring a non-straight wall and wave-induced variations; and the SWDD-Noisy dataset, collected under stormy conditions, where the SSS transducer intermittently exited the water, resulting in data loss represented by black lines in the images. To clarify the features of SSS images, the yellow line in the center represents the nadir gap between the two SSS transducers, indicating areas where seafloor data is absent. Additionally, the yellow line outside the nadir gap denotes the presence of a wall."}, {"title": "V. ADVERSARIAL ATTACK", "content": "Neural network verification (NNV) is an emerging area of formal methods that enables practitioners to mathematically prove whether certain properties hold for a given neural network. These properties are usually defined as a set of constraints that restrict the inputs and outputs of the network. Given a neural network with input-output function $f : R^n \\rightarrow R^m$, and a property of interest $P := (P_{in}, P_{out})$, the goal of NNV is to check whether $\\forall x.x \\models P_{in} \\Rightarrow f(x) \\models P_{out}$ holds. If it holds, then the neural network $f$ is said to satisfy $P$. In the case of violation, the verification tools generally provide a counter-example, which is an input $x$ such that $x \\models P_{in}$ but $f(x) \\nvDash P_{out}$.\nVarious properties of interest, such as safety or robustness, can be formulated depending on the verification objective. A classical robustness property asserts that the network's output is robust against small input perturbations. More specifically, considering some $L_p$ norm, an input $\\bar{x}\\in R^n$, and positive real constants $\\epsilon$ and $\\delta$, the network $f$ is locally robust for input $x$, iff $\\forall x. ||x - \\bar{x}||_p \\leq \\epsilon \\Rightarrow ||f(x) - f(\\bar{x})||_p \\leq \\delta$. The most commonly used norm is the $L_\\infty$ norm.\nIn this paper, we define and analyze two safety properties, both of which describe the robustness of our YOLOX model. Since YOLOX is an object detection model, outputting a fixed number of bounding box proposals along with objectness scores and class confidence scores for each bounding box, one needs to account for these factors when formalizing the safety properties. Accordingly, our robustness properties are designed to assess whether adversarial noise in the input images can compromise the output, leading to instances where some predicted bounding boxes are effectively fooled.\ni. Our first property $P_1$ expresses that the network is robust against random $L_\\infty$ noise in the input. This type of noise simulates the random perturbations that can be present in side-scan sonar images across the whole waterfall image. \nOur constraints allow noise in each pixel and each channel by a portion of 0 < \\epsilon < 1. The property is violated in case there is a noisy input, within the $\\epsilon$ perturbation bound, for which either the predicted bounding box objectness score falls below the objectness threshold $\\acute{g}_{obj}$ or the predicted class for the bounding box changes. Formally, for a 3D input image of size $h\\times w\\times c$ and a maximum perturbation bound $\\epsilon$, attacking the bounding box $b$, with objectness score $y^{obj}_b$, having $N$ classification confidence scores and $y^{class,p}_b$ being the confidence score of the correct class, the robustness property is defined as follows:\n$P_1 := \\\\ \\bigwedge_{i,j,k=1,1,1}^{h,w,c} (1 - \\epsilon) \\cdot x_{i,j,k} \\leq \\hat{x}_{i,j,k} \\leq (1 + \\epsilon) \\cdot x_{i,j,k}  \\\\  y_b^{obj} > \\acute{g}_{obj} \\\\ \\bigwedge_{l=1,l\\neq p}^N y_b^{class,p} > \\acute{y}_b^{class,l} $\nii. Our second property $P_2$ expresses that the network is robust against dark horizontal lines in the input image, mimicking the noise that is present in the SWDD-Noisy dataset. To verify the model performance against this black line phenomenon, we formalize the robustness property $P_2$ for a randomly generated line configuration $L \\subseteq \\{1, ..., h\\}$ and some $0 < \\epsilon < 1$ as follows:\n$P_2 := \\\\ \\bigwedge_{i\\in\\{1,...,h\\}\\setminus L} \\\\ \\bigwedge_{j=1,k=1}^{w,c} (x_{i,j,k} = \\hat{x}_{i,j,k}) \\\\ \\bigwedge_{i\\in L} ( \\epsilon \\cdot x_{i,j,k} \\leq \\hat{x}_{i,j,k} \\leq \\epsilon \\cdot x_{i,j,k})  \\\\ y_b^{obj} > \\acute{g}_{obj} \\\\ \\bigwedge_{l=1,l\\neq p}^N y_b^{class,p} > \\acute{y}_b^{class,l} $\nAs an alternative to NNV, adversarial attacks offer an incomplete but often more efficient way of falsifying the robustness of neural networks. An adversarial attack tries to find the input $\\hat{x}$, which violates the robustness property from above, resulting in unexpected output, i.e., fooling the network. Since the formal verification of a neural network is an NP-complete problem, analyzing real-world-sized networks, such as YOLOX and other object detection models, is a challenging and in most cases infeasible task (considering limited amount of resources). Thus, in this paper we only provide insights into the robustness of the networks by assessing the success rate of different adversarial attack methods against them. Using adversarial attacks, we can easily show unsafety (i.e. the lack of adversarial robustness), which is the case in most instances. However, the result of this analysis is not a formal guarantee due to the incompleteness of these methods."}, {"title": "VI. EXPERIMENTAL EVALUATION", "content": "As outlined in Section III, the adversarially retrained models are validated using two approaches: (1) the SWDD-Validation datasets to assess the improvement of the retrained model compared to the baseline results (Table II), and (2) the PGD attack embedded in binary search to compute the robustness bounds considering the $P_1$ and $P_2$ safety properties. Both validation processes are conducted using the KD-Nano-L-ViT model [10], resulting from the KD of the YOLOX-ViT-L model into the YOLOX-Nano model.\nAdversarial retraining begins with creating adversarial datasets, which are subsequently integrated into the retraining loop to enhance model robustness. This section details the generation of these adversarial datasets and evaluates the retrained model under both PGD and patch attack scenarios.\nOur study uses the alpha-beta-CROWN tool to assess whether the PGD attack can produce a counter-example, signifying a violated safety property. Due to current computational constraints, we cannot verify the safety property to ensure complete compliance. However, based on extensive testing, we consider the safety property satisfied if the PGD attack does not produce a counter-example within two minutes. We employ a binary search method to approximate the noise threshold at which the model fails.\nThe binary search, outlined in Algorithm 1 is applied to both safety properties ($P_1$ and $P_2$), where the function eval_prop takes as input the safety property. The input parameters differ depending on the property: for $P_1$, the lower and upper bounds are set to 0.0 and 0.08, respectively, with a maximum of 5 iterations; for $P_2$, the bounds are 0.60 and 1.0, with also up to 5 iterations. The algorithm iterates over all detected bounding boxes for each input image, initializing the search bounds. The midpoint (mid) is calculated by bisecting the interval in each iteration. The property check, performed by the eval_prop function, is conducted for the perturbation bound mid. If a counter-example is found within the specified time limit, the upper bound is adjusted downward, and the search range is halved. If no counter-example is found, the lower bound is adjusted upward accordingly. This iterative process continues for the designated number of iterations, with the final threshold bound saved as the average of the maximal perturbation bound where the property holds and the minimal perturbation bound where the property fails. The counter-examples found during each iteration are saved in the allocated adversarial dataset, resulting in two separate datasets \u2013 one for $P_1$ and one for $P_2$, named P1-SWDD (1017 images) and P2-SWDD (1462 images).\nDue to some limitations of integrating YOLOX into the patch generation framework, for this experiment, we opted to use the YOLOv5 model for adversarial patch dataset generation and subsequently apply this dataset in the adversarial retraining loop using the KD-Nano-L-ViT model. The adversarial patch attack on the YOLOv5 model requires initial training with the SWDD dataset. To align with the size of the YOLOX model used in this study, we select the YOLOv5-nano model and train it for 300 epochs. The resulting model weights are then incorporated into the adversarial patch framework, as explained in [22]. By applying this method, ROSAR generates the adversarial Patch-SWDD dataset, which consists of 151 images. The adversarial dataset comprises the SWDD dataset with the adversarial patch in the dataset ground truth location for every bounding box, corresponding for the classes wall and noWall.\nWe applied adversarial retraining with the three adversarial datasets to fine-tune the KD-Nano-L-ViT model, initially trained on the SWDD dataset for 300 epochs. To enhance the model's robustness, the retraining process leverages transfer learning with the P1-SWDD, P2-SWDD, and Patch-SWDD datasets. Since the retraining process focuses on adversarial retraining rather than initial training, we aim to make the model retain the knowledge acquired during the initial training. Consequently, the retraining is conducted by comparing the performance across four different epochs: 5, 10, 15, and 20 epochs. The results of the adversarial retraining are illustrated , where Val. indicates the validation dataset, epoch specifies the number of epochs used for retraining, %TP is the percentage of true positive bounding boxes, FP is the number of false positive bounding boxes, and AP is the average precision. Furthermore, the first row of each validation dataset represents the metrics for the original KD-Nano-L-ViT model trained with the SWDD dataset.\nThe results demonstrate how adversarial retraining \u2013 employing both adversarial patch and PGD methods \u2013 enhanced the model's performance across various metrics. Notably, there is an improvement in the model's performance on all three validation datasets (SWDD-Clean, SWDD-Surface, and SWDD-Noisy). While the retrained models exhibit a reduction in %TP, they also show a marked decrease in FP, indicating a reduction in overfitting, suggesting that the retrained models offer more reliable detections than the original. The patch retraining has a lower %TP than the two other models, where the P2-SWDD has the highest %TP. Thus, as an inference comparison with the SWDD-Validation dataset, the two PGD retraining datasets have higher %TP, whereas the patch retraining dataset has the lowest FP. Based on the results , for robustness validation we have chosen the three models retrained with 15 epochs.\nThe robustness validation process evaluates whether the retrained models have enhanced performance compared to the original KD-Nano-L-ViT model concerning the properties $P_1$ and $P_2$. This process uses the PGD attack with the aim to fool the model considering the two safety properties. Similarly to the approach in Section VI-A, where counter-examples were generated, for each successful attack, we establish the threshold noise level at which the model was fooled. This phase focuses on determining the robustness boundary value for the adversarially retrained model using the binary search method in Algorithm 1. Respectively to the used property, the model retrained on the P1-SWDD dataset is compared to the original model under property $P_1$, and the model retrained on the P2-SWDD is compared with the original model under property $P_2$. The model retrained on the Patch-SWDD is compared using both safety properties due to the patch attack disregarding the safety properties."}, {"title": "VII. CONCLUSION", "content": "This paper presented ROSAR, a novel framework to enhance the robustness and efficiency of DL object detection models tailored explicitly for SSS images. The framework leverages KD for embedded systems, previously validated in our earlier work, while focusing on improving model robustness through adversarial retraining. We addressed the challenges of SSS-specific noise and limited data availability by introducing three distinct SSS datasets and generating adversarial datasets using PGD and patch attacks. Our extensive experiments demonstrate that adversarial retraining improves detection accuracy and robustness under SSS conditions and that model retraining with PGD attack returns better model robustness. While the Patch-SWDD dataset slightly reduced mean robustness compared to the original model, it significantly improved detection metrics and provided greater stability, ensuring consistent robustness across various instances. Given the computational constraints, our current methodology focused on fooling the bounding box candidate with the highest confidence value. Future work should expand this approach to consider multiple candidates simultaneously, thereby providing a more comprehensive robustness assessment. Furthermore, ROSAR can be adapted to address additional safety properties, such as interference caused by data transmission during SSS data collection. This framework is not limited to SSS application but can be applied to any vision-based applications where safety properties can be mathematically formalized. This research lays a solid foundation for advancing the use of DL models in underwater robotics, particularly in challenging SSS environments."}]}