{"title": "Harmonic Reasoning in Large Language Models", "authors": ["Anna Kruspe"], "abstract": "Large Language Models (LLMs) are becoming\nvery popular and are used for many different\npurposes, including creative tasks in the arts.\nHowever, these models sometimes have trouble\nwith specific reasoning tasks, especially those\nthat involve logical thinking and counting. This\npaper looks at how well LLMs understand and\nreason when dealing with musical tasks like\nfiguring out notes from intervals and identify-\ning chords and scales. We tested GPT-3.5 and\nGPT-40 to see how they handle these tasks. Our\nresults show that while LLMs do well with note\nintervals, they struggle with more complicated\ntasks like recognizing chords and scales. This\npoints out clear limits in current LLM abilities\nand shows where we need to make them better,\nwhich could help improve how they think and\nwork in both artistic and other complex areas.\nWe also provide an automatically generated\nbenchmark data set for the described tasks.", "sections": [{"title": "Introduction", "content": "Generative AI, particularly Large Language Mod-\nels (LLMs), is increasingly utilized across a range\nof applications, extending beyond textual outputs to\ninclude artistic creations such as image generation,\ntext composition, and music. This proliferation of\napplications has sparked interest in cross-modality,\na growing field of research exploring the transfer\nand application of generative models across dif-\nferent sensory domains. Among these, musical\ngeneration tools like Suno\u00b9 and Udio\u00b2 have demon-\nstrated an implicit understanding of musical rules,\nraising questions about whether LLMs, which have\nbeen exposed to vast amounts of music-related lit-\nerature during training, exhibit a similar grasp of\nmusical concepts.\nWestern music is characterized by a rigorously\nstructured system encompassing rhythm, harmony,"}, {"title": "Related work", "content": "The integration of reasoning capabilities into large\nlanguage models (LLMs) represents a significant\nfrontier in artificial intelligence research, focusing"}, {"title": "Methodology", "content": "In this section, we will describe our experimen-\ntal design. All generated questions were tested on\nGPT-40. The interval experiments were also run\non GPT-3.5 for comparison. Each experiment was\nperformed three times to account for randomness\nin the models. Evaluation then consisted of com-\npairing each result to the expected one to obtain\naccuracy, accounting for enharmonic variants."}, {"title": "Interval problems", "content": "To investigate the capability of Large Language\nModels (LLMs) in processing and understanding\nmusical intervals, we utilized the music213 Python\nlibrary to automatically construct a series of prob-\nlems. These problems are formulated to challenge\nthe LLM's ability to determine and name intervals\nbased on the question format, \u201cWhat is a <inter-\nval> up/down from <note>?\u201d. Intervals were lim-\nited to not be greater than an octave.\nWe introduced various configurations to increase\nthe complexity of the task:\nDirection: Only upward intervals vs. upward and\ndownward intervals\nOctave limitation: Intervals remaining within the\nsame octave (c to b) vs. intervals transcending\noctaves\nAccidentals: Sharps only vs. sharps and flats\nWe hypothesize that in each case, the first variant\nis more commonly seen in textbooks, and therefore\neasier to solve for LLMs presumably trained on\nsuch material. (The pitch classes Bb and Eb may\nbe an exception for accidentals).\nA comprehensive dataset comprising 500 ques-\ntions for each configuration combination was com-\npiled, ensuring a robust evaluation of the LLM's\nability across different musical scenarios. Prior to\ntesting, the LLM was briefed to expect questions\nabout musical intervals and was instructed to for-\nmat its responses in a table to standardize output."}, {"title": "Chords and Scales Problems", "content": "In the domain of chords and scales, the focus of the\nexperiments was on the LLM's ability to identify\nand name various types of chords and scales. Us-\ning the music21 Python library, chords and scales\nwere randomly constructed to create a diverse set\nof musical problems. The used chord and scale\ntypes are given in Table 1 (appendix); definitions\nwere used as in music21.\nThe evaluation was structured into two main ex-\nperiments: In the first, the types of chords and\nscales were explicitly given to the LLM before it\nwas asked to identify them. Interestingly, it was\nobserved that the provided types were quickly \u201cfor-\ngotten\" or disregarded by the model in subsequent\ntasks, indicating a potential limitation in its short-\nterm recall or application of explicitly given infor-\nmation. In the second experiment, the types were\nnot provided, increasing the difficulty level and re-"}, {"title": "Interval Results", "content": "The results of the interval experiments are shown\nin Figure 1 for all described configurations, tested\non GPT-3.5 and GPT-40.\nThe results indicate that all the tested config-\nurations influence the LLM's performance, with\nthe direction of interval movement showing the\nstrongest impact. This suggests a challenge in the\nLLM's ability to generalize from upward to both\nupward and downward interval calculations, which\nis less common in textbook material but critical for\na robust understanding of musical intervals.\nThe more state-of-the-art model GPT-40 dis-"}, {"title": "Chords and Scales results", "content": "In the exploration of chords and scales using the\nLLM, experiments were structured to assess how\nwell the model could recall and apply its knowledge\nunder various configurations. Only GPT-4o was\ntested in this harder task, where prior knowledge\nwas crucial for success. The results are shown in\nFigure 2.\nChords Results When the LLM was informed\nof the chord types, it recognized basic versions\nfairly accurately. However, without prior informa-\ntion, the model often generated unusual responses.\nThe presence of enharmonic notes significantly re-\nduced accuracy, suggesting that such variations,\nthough known to the model, are not commonly\npresented in textbooks and hence, are more chal-\nlenging for the model. Surprisingly, the model per-\nformed reasonably well with inversions, although\nthese results were still inferior to the basic chord\nrecognition tasks. In the most complex scenarios,\ncombining multiple variations, the model's perfor-\nmance dropped dramatically, achieving only about\n15% accuracy without prior information on possi-\nble chord types. It was also noted that the model\ntends to forget which types were previously named,\nhighlighting a possible limitation in its short-term"}, {"title": "Conclusion", "content": "This study has explored the capabilities of Large\nLanguage Models (LLMs) like GPT-3.5 and GPT-\n40 in understanding and processing musical tasks,\nparticularly focusing on the identification of in-\ntervals, chords, and scales. Our results indicate\nthat while LLMs perform adequately in simpler\ntasks such as identifying note intervals, their per-\nformance significantly declines in more complex\ntasks involving chord and scale recognition.\nThe experiments revealed that LLMs are heavily\ninfluenced by the configurations of the tasks, with\nchanges in interval direction, octave constraints,\nand accidentals presenting significant challenges.\nThis suggests that LLMs might be relying heavily\non patterns observed during training rather than\ndeveloping a deeper, conceptual understanding of\nmusical theory. The most advanced model, GPT-40,\nshowed some improvements over GPT-3.5, indicat-\ning that model size and training data quality do\ninfluence performance. However, the nature of this"}, {"title": "Future work", "content": "In this work, we focused on reasoning about har-\nmony. Future work could analyze other aspects of\nmusic-domain reasoning, e.g. rhythm, or explore\nnon-western music. (Yuan et al., 2024) reported\nsimilarly bad results on GPT-4, but showed that\nIn-Context Learning, Chain-of-Thought, and Role-\nplaying improved results. Furthermore, there are\nsome clues that more sophisticated models demon-\nstrate more emerging reasoning capabilities (see\ne.g. the differences between GPT-3.5 and GPT-40)."}]}