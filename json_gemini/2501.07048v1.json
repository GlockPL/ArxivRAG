{"title": "Unveiling the Potential of Text in High-Dimensional Time Series Forecasting", "authors": ["Xin Zhou", "Weiqing Wang", "Shilin Qu", "Zhiqiang Zhang", "Christoph Bergmeir"], "abstract": "Time series forecasting has traditionally focused on univariate and multivariate numerical data, often overlooking the benefits of incorporating multimodal information, particularly textual data. In this paper, we propose a novel framework that integrates time series models with Large Language Models to improve high-dimensional time series forecasting. Inspired by multimodal models, our method combines time series and textual data in the dual-tower structure. This fusion of information creates a comprehensive representation, which is then processed through a linear layer to generate the final forecast. Extensive experiments demonstrate that incorporating text enhances high-dimensional time series forecasting performance. This work paves the way for further research in multimodal time series forecasting.", "sections": [{"title": "Introduction", "content": "Time series forecasting plays a vital role in various domains such as finance Wang et al. (2023); Zheng et al. (2023); Gabrielli et al. (2023), traffic Eldele et al. (2024); Zhao et al. (2023); Zhang et al. (2023), healthcare Angelopoulos et al. (2024); Shen et al. (2023); Saldanha et al. (2024), and weather prediction Cheng et al. (2024); Auer et al. (2024); Chatzigeorgakidis et al. (2024). Traditional deep learning-based methods focus on time series analysis, relying purely on the historical numerical values of individual series. However, they often overlook the potential of textual data, particularly in high-dimensional time series forecasting, which involves a large number of channels and complex, diverse temporal patternsDonoho et al. (2000); Barigozzi et al. (2024). In such a scenario, textual information can provide valuable supplementary insights.\nThe recent success of Large Language Models in Natural Language Processing and Computer Vision has sparked interest in using textual data to enhance time series forecasting Xi et al. (2023); Jia et al. (2024), however, their data are low-dimensional. Thus, incorporating textual information into high-dimensional time series forecasting remains unexplored. Our work aims to bridge this gap by being the first to integrate textual data into high-dimensional time series forecasting, introducing a novel challenge and paving the way for future research in this field. We propose that integrating numerical time series data with textual information allows models to develop a more comprehensive understanding of underlying dynamics, ultimately improving forecasting accuracy.\nIn our work, we propose a general framework - TextFusionHTS, integrating textual information into high-dimensional time series forecasting, focusing on the challenges of extracting and capably fusing textual data with time series inputs. Our key contributions are as follows:\n(1) Framework for Text-Integrated High-Dimensional Time Series Forecasting: We introduce a new framework TextFusionHTS that integrates textual data into high-dimensional time series analysis. By integrating text-based information, we open up new possibilities for high-dimensional time series forecasting and pave the way for further research in multimodal time series analysis;"}, {"title": "Related Works", "content": ""}, {"title": "Time Series Forecasting", "content": "Time series forecasting methods have employed various deep learning techniques, including Linear Wang et al. (2024); Zeng et al. (2023); Darlow et al. (2023), Convolutional Neural Networks Cheng et al. (2024); Zhang et al. (2023); Luo and Wang (2024), and Transformer Piao et al. (2024); Liu et al. (2024b); Boussif et al. (2024); Chen et al. (2024). Most of them mainly solve the challenge of capturing short-term and long-term dependencies. PatchTST Nie et al. (2022) has proven to be a leading technique that divides the series window into patches, allowing it to capture the complex global and local relationships within individual series. Recently, LLM-based methods, e.g., TimeLLM Jin et al. (2023), TEMPO Cao et al. (2023), TEST Sun et al. (2023), and TimeCMA Liu et al. (2024a) have explored adapting LLMs for time series tasks by converting time series data into text formats that LLMs can effectively process."}, {"title": "Multimodal Time Series Forecasting", "content": "Several methods have explored integrating different data types. Fu et al. (2024) introduced a framework that incorporates video information into time series forecasting, while Multimodal Time-series Analysis Xi et al. (2023) fuses textual, visual, and audio data to forecast gifting behavior on live-streaming platforms. GPT4MTS Jia et al. (2024) proposes a news multimodal dataset based on GDELT and provides a pipeline for data extraction and processing. These efforts demonstrate the potential of multimodal forecasting, though they often encounter challenges due to the lack of published code or data. Additionally, MST-GAT Ding et al. (2023) utilizes graph attention mechanisms to manage multimodal inputs, specifically designed for anomaly detection in time series data. While these methods show promise in integrating textual data, the performance of fusing different modalities in high-dimensional time series forecasting remains unclear."}, {"title": "Method", "content": "Inspired by the dual-tower architecture commonly used in multimodal methods Koh et al. (2024); Sun et al. (2024), we propose a framework TextFusionHTS that integrates Large Language Models with time series models, as shown in Figure 1."}, {"title": "Time Series Representation Learning", "content": "We first use a patch-based method, PatchTST, to extract the representation of the time series data. The input series $x \\in \\mathbb{R}^l$ with $l$ length is divided into $p$ patches. Using the patch-based method is crucial in dynamically using static textual data. Following PatchTST, dividing time series into patches allows the model to capture local temporal patterns within each patch. Since textual data is static, it's important to assign textual data to different interactions with various time series patches of the series. For example, the text may provide context that is more relevant to certain periods or patterns in the series. Therefore, using a patch-based method ensures that the model can apply the static text data in a context-sensitive manner for different time series windows."}, {"title": "Text Representation Learning", "content": "To incorporate textual data, we use Meta Llama-3.1 8B, the state-of-the-art open-source LLM, to process the text description corresponding to the time series. The text is tokenized and converted into a set of toke embeddings. Let the set of token embeddings be ${t_i \\in \\mathbb{R}^{d_{tx}}|i = 1, 2, ..., n}$ where $d_{tx}$ is the dimension of text representation, $n$ is the number of tokens. We compute the average of all token embeddings to get full semantic meaning of text,\n$z_{tx} = \\frac{1}{n} \\sum_{i=1}^{n} t_i \\in \\mathbb{R}^{d_{tx}}, Q = z_{tx}$"}, {"title": "Fusion Module", "content": "We apply a cross-attention mechanism where the time series representations (K and V) are combined with the textual representation (Q). This operation results in a fused representation $z = CrossAttention(Q, K, V) \\in \\mathbb{R}^{d}$ that integrates information from both modalities.\nFinally, we use a Feed Forward transformation to map the fused representation z to the forecast values $y \\in \\mathbb{R}^{h}$, resulting in the final predictions in h horizon."}, {"title": "Experiments", "content": ""}, {"title": "Experimental Settings", "content": "We collect two high-dimensional time series forecasting datasets with textural data. Wiki-People records daily traffic of Wikipedia webpages from July 2015 to December 2016. It was published in the Kaggle competition. We keep 3,857 full series published in the English language in the Wikipedia agent and crawl text data from the corresponding pages. News Moniz and Torgo (2018) is originally a large dataset of news items and their respective social feedback on Facebook. The collected data relates to a period of 8 months, between November 2015 and July 2016, accounting for 26,612 news items on the 'Obama' topic. The time interval is 20 minutes. Given that the time interval is 1 day for Wiki-People and 20 minutes for News, we set the input lengths to 7 and 9, corresponding to 1 week and 3 hours, respectively. The forecasting horizon h is set to {7, 14, 21, 28, 35} for Wiki-People and {1, 3, 9, 12, 15} for News.\nAll implementations are done in PyTorch and trained on a single NVIDIA A100 GPU. To ensure model convergence during training, we set the number of training epochs to 100 and employ an early stopping mechanism. Specifically, if the change in validation loss is less than $1 \\times 10^{-4}$, the training stops, and testing begins."}, {"title": "Result Comparison", "content": "To assess the impact of incorporating textual information with our proposed framework, we conduct the following experiments for comparison: 1) using purely time series data as input, with PatchTST as the time series model; 2) using both time series data and textual data as input, with the framework introduced in Section 3.\nThe Table 1 summarizes the overall results across two datasets, showing that integrating textual data into our framework consistently improves performance in both Mean Absolute Error (MAE) and Weighted Absolute Percentage Error (WAPE). While the only exception, is that when the forecasting horizon h is set to 21 on the Wiki-People dataset. When the time series data alone fails to provide adequate context, the additional textual data acts as a valuable complement, enhancing forecasting accuracy."}, {"title": "Strategies for Text Feature Extraction", "content": "To investigate the relation between text feature extraction and horizon. We compare three strategies for extracting textual features: using the [cls] token, the [bos] token, and the average of all token embeddings. Following are the results on the News dataset."}, {"title": "Conclusion", "content": "In this paper, we investigated the potential of incorporating textual data into high-dimensional time series forecasting. Our experiments demonstrate that using textual data can notably enhance forecasting performance, particularly for longer horizons where time series data alone may fall short in providing sufficient context. This approach effectively addresses the limitations of traditional time series models, significantly boosting their predictive capabilities. These findings underscore the importance of multimodal approaches in time series forecasting. For future work, we plan to develop more advanced models capable of fusing additional modalities, such as images and audio, to further enhance forecasting accuracy. Additionally, we aim to conduct more comprehensive experiments on diverse datasets to fully explore the benefits of integrating multiple sources of information."}]}