{"title": "Leveraging Metamemory Mechanisms for Enhanced Data-Free Code Generation in LLMs", "authors": ["Shuai Wang", "Liang Ding", "Yibing Zhan", "Yong Luo", "Zheng He", "Dapeng Tao"], "abstract": "Automated code generation using large language models (LLMs) has gained attention due to its efficiency and adaptability. However, real-world coding tasks or benchmarks like HumanEval and StudentEval often lack dedicated training datasets, challenging existing few-shot prompting approaches that rely on reference examples. Inspired by human metamemory-a cognitive process involving recall and evaluation-we present a novel framework (namely M2WF) for improving LLMs' one-time code generation. This approach enables LLMs to autonomously generate, evaluate, and utilize synthetic examples to enhance reliability and performance. Unlike prior methods, it minimizes dependency on curated data and adapts flexibly to various coding scenarios. Our experiments demonstrate significant improvements in coding benchmarks, offering a scalable and robust solution for data-free environments. The code and framework will be publicly available on GitHub and HuggingFace.", "sections": [{"title": "1 Introduction", "content": "Recently, large language models (LLMs) have made significant strides in understanding, generation, reasoning, and translation [22, 30, 32, 38, 51, 52]. By pretraining on vast amounts of textual data, the general LLMs can process language in ways that closely resemble human abilities, enabling them to perform a wide range of tasks, e.g., answering questions [34], writing [40], translation [50], and summarizing documents [41]. In many instances, general LLM's performance has matched or even exceeded human capabilities [23, 36]. In the field of code generation, general LLMs have also shown great potential [13, 19]. The general LLMs can generate code from natural language descriptions or complete and optimize existing code. This capability makes them a valuable tool for developers, particularly for tasks like automating scripts, designing algorithms, or adding code annotations [12, 16]. Since the pretraining data for the general LLMs is predominantly natural language text, with relatively little code data, they exhibit notable limitations in specialized coding scenarios [43]. To improve performance, pretraining or fine-tuning LLMs specifically for code generation is a widely adopted approach [6, 31]. Nonetheless, pretraining or fine-tuning requires considerable computational resources, posing challenges for many organizations and individuals. Conversely, in-context learning prompting has emerged as a more cost-effective and efficient alternative for adapting LLMs to code generation tasks [9, 18, 28].\n[Problem] Recently, few-shot methods [4, 18, 25, 47] based on in-context learning prompting have been widely proven to significantly improve code generation tasks for LLMs (including general LLMs and code LLMs). Few-shot prompting typically adds a few examples to the normal prompting (i.e., the user's requirement description) to guide LLMs in addressing the requirements or problems presented in the normal prompting. Although there are extensive researches on few-shot prompting methods [11, 18, 29, 46], most of them focus on retrieving the best matching examples from a training set to pair with the normal prompting, thereby improving the generation code performance of LLMs, as illustrated in Figure 1. Using few-shot prompting for LLM inference is particularly difficult for real-world data-free coding scenarios.\n[Motivation] Therefore, we are considering generating relevant examples from the LLM's own knowledge rather than relying on examples retrieved from the training set or external sources to guide the LLM in addressing the current problem. Our preliminary research shows that existing methods can directly prompt LLMs to generate relevant examples, but they do not guarantee the authenticity and accuracy of the recited [37] or analogized [48] content. It is worth noting that this is especially important for programming problems. Unlike natural language, programming languages have strict syntax and semantic rules [3, 39] that must be precisely followed for the computer to correctly understand and execute the code. So, how can we ensure the reliability of the examples generated by the LLM? This issue advances our research on metamemory [8]. Metamemory refers to an individual's awareness and regulation of their own memory processes, including the ability to recall, evaluate, and control their memory content.\n[Method] To better improve the performance of the one-time code generation of LLMs, especially for real-world data-free coding scenarios, we propose a metamemory morkflow (M\u00b2WF) framework inspired by the flow of human metamemory. Our M2WF framework consists of four main stages: Recall, Evaluation, Planning, and Guidance, as illustrated in Figure 1. Initially, the recall stage prompts the LLM to recall K relevant programming problems and provide their corresponding implementation steps and code. Subsequently, the evaluation stage assesses the confidence level of each recalled programming problem and its associated code, selecting the top M examples with the highest confidence. Following this selection, the planning stage utilizes the top M examples to formulate an implementation plan for the original problem. Ultimately, the implementation plan guides the LLM in solving the original programming problem.\n[Contributions] The main contributions are summarized as follows:\n\u2022 We have proposed a novel metamemory workflow (M\u00b2WF) framework by considering the flow of human metamemory. This is particularly beneficial for real-world data-free coding scenarios or benchmark tasks without training sets.\n\u2022 Our proposed M2WF framework has strong versatility and flexibility and can be easily plug-and-played. Moreover, the M2WF ensures reliability by carefully evaluating the recalled examples and adaptively tailoring such recall&evaluation process for each programming problem.\n\u2022 Extensive experimental results have demonstrated that M2WF framework can significantly improve the quality of code generation by LLMs, with the pass@1 score sometimes increasing by over 29.43%."}, {"title": "2 Related Work", "content": "2.1 Metamemory\nMetamemory, a concept introduced by John Flavell in the 1970s, refers to an individual's awareness and understanding of their own memory processes [8]. This cognitive ability to monitor and regulate memory emphasizes the efficient management of recalled information [21, 26, 27]. It involves three key aspects: recall memory information, evaluating the accuracy of recall content, and adjusting memory strategies based on the evaluations. Through this dynamic process, individuals can improve information acquisition, enhance learning outcomes, and effectively solve complex problems [24, 33], as illustrated in Figure 2.\nInspired by the flow of human metamemory, we propose a novel metamemory workflow (M\u00b2WF) framework to improve the performance of one-time code generation in LLMs.\n2.2 Code Generation for LLMS\nThe rise of LLMs has significantly advanced the development of automated code generation. However, due to programming languages' unique syntax and semantic structures, it is particularly challenging for LLMs to generate high-quality code. To improve the performance of automated code generation, existing research has primarily focused on pre-training or fine-tuning code LLMs [6, 31], post-processing methods [7, 49], advanced decoding [44], and in-context learning prompting [9, 11, 18, 46].\nPre-training or fine-tuning LLMs usually use hundreds of millions or even tens of billions of parameters within the transformer [42] architecture or its variants to train. Post-processing (i.e., code repair) methods primarily use secondary or multiple corrections to the code generated by LLMs. Compared to pre-training or fine-tuning LLMs and post-processing methods, in-context learning"}, {"title": "3 Background", "content": "In this section, we first define some symbols. Then, we formalize some related code generation methods (i.e., normal prompting, few-shot prompting [4, 18, 25, 47], analogical prompting [48]) and analyze their drawbacks.\n\u2022 Symbol definition. We denote the reasoning layer of the pre-trained LLM with parameters \u03b8 as P, and lowercase letters x, y, to denote a language sequence, i.e., x = (x[1],\uff65\uff65\uff65, x[n]), where each x[i] is a token. Note: in the formalization process, we omit the representation of the encoding and decoding processes, focusing solely on the representation of the reasoning process in LLMs.\n\u2022 Normal prompting (NP). The normal prompting is using the LLM to map the input x (i.e., the user's requirements) to y. This process can be formalized as: $y \\sim P_{NP} (y|x)$. Although LLMs can directly meet user's requirements by inputting x, their understanding is far less profound than that of humans. Especially, in complex coding scenarios or when clear prompts are lacking, the LLM's output may remain superficial and fail to address the core of the programming problem [45].\n\u03b8\n\u2022 Few-shot prompting (FSP). Few-shot prompting uses a few examples retrieved from the training set (or manually created examples) as references to guide an LLM in generating responses that meet the user's requirements. This process can be formalized as: $y \\sim P_{FSP} (y|c, x)$, where c represents a small number of examples. Although few-shot prompting is highly flexible and effective [4, 18, 25, 47], it is especially challenging to use for LLM"}, {"title": "4 Method", "content": "To improve the quality of one-time code generation in real-world data-free scenarios or benchmark tasks without training sets, we propose a novel metamemory workflow (M\u00b2WF) framework inspired by the metamemory mechanism in section 2.1. Our proposed M2WF framework mainly consists of four stages: recall, evaluation, planning and guidance, with its overall framework demonstrated in Figure 4.\n4.1 Recalling\nRecalling is the psychological process by which humans retrieve past information from their recollections, and it sometimes refers to the information that has been retrieved. The method of recollection involves two steps: retrieval and extraction. First, humans make decisions or identifications based on new information, extracting relevant content from the retrieved information. In the M2WF framework, recalling relevant examples is the initial step, akin to how humans retrieve related memories when faced with new information. This process enhances the LLM's understanding of the recalled content and provides relevant experiential insights or solutions for new information. When presented with a new programming problem x (i.e., the input content), the M2WF directs the LLM to identify K-related programming problems from its memory based on the given programming problem x. Additionally, our recall examples expand on the principle of analogical method [48] by guiding the LLM to provide implementation steps for these recall-related programming problems, e.g.,\n${x^{i}, \\hat{s^{i}}, y^{i}} \\sim P_{recall} (\\hat{x^{i}}, \\hat{s^{i}}, \\hat{y^{i}}|d_{re}, x, x^{i-1}, \\hat{s^{i-1}}, \\hat{y^{i-1}}), i \\in [1, K]$\nWhere, re is an abbreviation for recalling; $x^{i}$ represents the relevant programming problem of the i-th recollection; $s^{i}$ represents the implementation steps for the i-th related programming problem; $y^{i}$ represents the Python3 code for the i-th related programming problem; $d_{re}$ represents the instruction requirements for recalling related programming problems. More specifically, our instruction $d_{re}$ for recalling related examples is:\n4.2 Evaluation\nAfter extracting the recalled content, humans evaluate their memory ability, specifically judging the accuracy of the retrieved information. This evaluation is often based on factors such as personal experience and the use of memory strategies. During the evaluation stage, we mimic human metamemory assessment processes by prompting the LLM to assess its confidence in the recalled relevant programming problems, including their implementation steps and the corresponding Python3 code. We set relevant instructions, established the confidence score range from 0 to 100, and selected the top M (M \u2264 K) examples with the highest confidence, e.g.,\n$C^{i} \\sim P_{eval}(C^{i}|d_{ev}, x^{i}, \\hat{s^{i}}, \\hat{y^{i}}), i \\in [1, K], C^{i} \\in [0, 100]$\n${\\lbrace x^{j}, s^{j} \\rbrace} \\sim P_{select} (x^{j}, s^{j}|d_{se}, x^{i}, \\hat{s^{i}}, C^{i}), j \\in [1, M]$\nWhere, ev is an abbreviation for evaluation; se is an abbreviation for selection; $C^{i}$ represents the confidence score for the i-th related programming problem; $d_{ev}$ and $d_{se}$ represent the evaluation instruction requirements and the selected instruction requirements, respectively. Our evaluation instruction [$d_{ev}, d_{se}$] can be described in detail as follows:\n4.3 Planning\nThe planning stage is the third key step in the M\u00b2WF framework. At this stage, the LLM can use past successful experiences and steps to formulate a detailed plan for the new programming task. Specifically, we prompt the LLM to provide an implementation plan s for the original programming problem based on the implementation steps ${\\lbrace \\hat{s^{j}}, j \\in M \\rbrace}$ from the selected M examples, e.g.,\n$s \\sim P_{plan}(s|d_{pl}, {\\hat{s^{j}}}, x)$\nWhere, pl is an abbreviation for planning;$d_{pl}$ represents the planning instruction requirements. Our evaluation instruction $d_{pl}$ can be described in detail as follows:\n4.4 Guidance\nThe guidance phase is the final step in the M2WF framework and a crucial part of solving the original programming problem. During this phase, the provided implementation plans s guide the LLM in writing correct Python3 code y to solve the original programming problem x, i.e.,\n$y \\sim P_{guid}(y|d_{gu}, x, s)$\nWhere, gu is an abbreviation for guidance; $d_{gu}$ represents the instruction requirements for writing the Python3 code for the original programming problem. The instruction requirement $d_{gu}$ for our guidance phase is as follows:"}, {"title": "5 Experiments", "content": "In this section, we will explore the effectiveness of the M2WF framework and compare it with few-shot prompting methods. Our experiments aim to answer the following research questions:\n\u2022 RQ1: What impact does increasing the number of recalled related examples have on the performance of the M2WF framework?\n\u2022 RQ2: What impact does select multiple recalled examples with higher confidence have on the performance of our proposed M2WF framework?\n\u2022 RQ3: What is the impact of each stage of our proposed M2WF framework on overall performance?\n\u2022 RQ4: How do the open-source and closed-source LLMs based on our proposed M2WF framework perform in real-world data-free coding scenarios or benchmark tasks without training sets?\n\u2022 RQ5: How do larger parameter LLMs and smaller parameter LLMs based on the M2WF framework perform in real-world data-free coding scenarios or benchmark tasks without training sets?\n\u2022 RQ6: How does our proposed M\u00b2WF framework perform in real-world data-free coding scenarios or benchmark tasks without training sets compared to other methods (e.g., CoT prompting [14], Analogical prompting [48])?\n\u2022 RQ7: How does the our proposed M2WF framework perform in comparison to few-shot retrieval method (i.e., AceCoder [18])?\n\u2022 RQ8: Can the our proposed M2WF framework be applied to multilingual benchmarks?\n5.1 Datasets\nTo better evaluate the performance of the M2WF framework, we selected four benchmarks, i.e., HumanEval [6], StudentEval [2], codeforces [19, 48], MultiPL-E [5].\nHumanEval [6] benchmark was manually constructed by OpenAI researchers in 2021 and only includes 164 test samples.\nHumanEval+ [20] benchmark, introduced in 2023, enhances the original HumanEval benchmark by incorporating additional test cases and addressing some of the errors.\nStudentEval [2] benchmark, released in 2023, is a test benchmark containing 1,675 prompts across 48 questions, written by 80 students who had completed only one semester of a Python programming course. The StudentEval benchmark identifies four key disjoint subsets for each question participant: first success (the correct code was generated on the first attempt), first failure (the first attempt was unsuccessful, and the participant moved on to the"}, {"title": "5.2 Models", "content": "To better validate the performance of the M2WF framework, we select two open-source LLMs (Mistral-7B-Instruct-v0.2, DeepSeek-Coder-V2) and two closed-source LLMs (ChatGPT and GPT-4).\nChatGPT (GPT-3.5-turbo, [1]) is a language model developed by OpenAI based on a generative pre-trained transformer model. ChatGPT can understand and generate natural language text, supporting a variety of applications, including dialogue generation, text composition, and creative writing.\nGPT-4 (GPT-4-turbo-2024-04-09, [1]) is the fourth-generation generative pre-trained transformer model developed by OpenAI. Compared to GPT-3 and ChatGPT, GPT-4 has improvements in language understanding, generation quality, and several other aspects.\nMistral-7B-Instruct-v0.2 1 is a seven billion parameter model distributed under the Apache License, and it is available for instruction-following and text completion tasks.\nDeepSeek-Coder-V2 [10] is an open-source mixture of expert (MoE) code language models that achieves performance comparable to GPT-4 Turbo on specific coding tasks. Specifically, DeepSeek-Coder-V2 was further pre-trained from intermediate checkpoints of DeepSeek-V2 with an additional 60 trillion tokens. This ongoing pre-training significantly enhances DeepSeek-V2's coding and mathematical reasoning abilities while maintaining strong performance on general language tasks."}, {"title": "5.3 Baselines", "content": "To demonstrate that the M2WF framework can significantly improve the performance of LLMs in generating code on one-time, especially for benchmark tasks (e.g., HumanEval, HumanEval+, and StudentEval) without training sets, we have compared the M2WF framework with several baselines and state-of-the-art approaches.\nNormal prompting (i.e., the user's requirements) instructs the LLM to generate the corresponding code directly without adding any extra instructions.\nCoT prompting [14] adds \"Let's think step by step\" after the normal prompting, instructing the LLM to generate the final code step by step.\nAnalogical prompting [48] instruct LLMs to provide multiple relevant examples based on the original question, and then use these relevant examples to solve the original problem."}, {"title": "5.4 Evaluation Metric", "content": "We follow the evaluation metric pass@k used in reference [6, 17, 35] to assess the performance of the M2WF framework. The calculation method for pass@k is:\npass@k := E 1\nProblems (n-c)\nk\nn\nIn Eq. (6), n represents the number of code generations for a given problem; c represents the number of generated n codes that passed the test. In the experiment, we evaluated the performance of M2WF framework on four A100 GPUs with 80GB of memory each."}, {"title": "5.5 Ablation Studies", "content": "In this subsection, we investigate the effects of recalling K-related examples, selecting the top M recall examples with the highest confidence, and three stages (i.e., recalling stage, evaluation stage, planning stage). The experimental results are shown in Figure 5 and Table 2.\n(Answer to RQ1) the impact of models using M2WF framework on recalling K-related examples in the HumanEval benchmark. In Figure 5, we kept the value of M constant to analyze the performance of recalling K-related examples. When M = 1, and 3, the scores of the Mistral-7B-Instruct-v0.2 model exhibit a trend of initially increasing and then decreasing. When M = 2, the scores of the Mistral-7B-Instruct-v0.2 model show a decreasing trend. The DeepSeek-Coder-V2 model and GPT-4 model display a similar trend. This indicates that recalling a larger number of examples does not necessarily lead to the best results with the M2WF method. Note: In the experiment, we do not analyze the performance of recalling K examples when M = 4, 5, 6, and 7. This is due to the token length limitations of the LLMs used in the experiment, which prevented us from conducting experiments with more recall related examples.\n(Answer to RQ2) the impact of using M2WF method for selecting the top M recall examples with the highest confidence on the HumanEval benchmark. In Figure 5, we kept K constant to analyze the performance of selecting the top M recall examples with the highest confidence. When K = 5,6,7, and 8, it is clear that the scores of the Mistral-7B-Instruct-v0.2 model exhibit a trend of initially increasing and then decreasing. The scores of the DeepSeek-Coder-V2 model and GPT-4 model show a similar pattern. Therefore, we can conclude that selecting either a higher or lower number of recall examples with the highest confidence does not lead to better performance with the M2WF framework. Note: In the experiment, we do not analyze the cases for K = 2, 3, and 4 because selecting only a few of the top M recall examples with the highest confidence does not sufficiently demonstrate the reliability of the experiment.\n(Answer to RQ3) The performance of the Mistral-7B-Instruct-v0.2 model based on M2WF framework at different stages. Since the M2WF framework involves inputting all instructions at once and producing results for each stage at one-time, to investigate the performance of each stage (mainly the recall stage, evaluation stage, and guidance stage), we divided the M2WF framework into two steps. For example, when exploring the impact of the recall stage, we first input recall instructions to prompt the LLM to recall relevant programming problems. After obtaining the recalled results, we introduce random noise to the recalled programming problems. Specifically, we add noise every 10 characters, with a noise level set to 0.5. We then instruct the LLM to evaluate the recalled programming problems (i.e., the programming problems with noise) and provide a corresponding implementation plan for the original programming problems to guide the LLM in solving them. Using a similar approach, we assessed the performance of each stage, with specific results shown in Table 2. From Table 2, it is clear that each stage plays a crucial role. If one of the stages is affected by noise, the performance of the Mistral-7B-Instruct-v0.2 model based on M2WF method will decline, e.g., when the recall stage is affected, the performance of the model drops by 11.97%. Similarly, the performance of the Mistral-7B-Instruct-v0.2 model based on M2WF method is affected in other stages (i.e., evaluation stage and planning stage) as well."}, {"title": "5.6 Results", "content": "We validated the effectiveness of the M2WF framework on benchmark tasks without training sets. The experimental results on the HumanEval, StudentEval, and HumanEval+ benchmarks are shown in Tables 3, 4, and 5, leading to the following conclusions:\n(Answer to RQ4) whether in open-source or closed-source models, our M2WF framework consistently achieves good performance. In Tables 3, 4, and 5, we can see that the M2WF framework based on the Mistral-7B-Instruct-v0.2 (open-source model) achieves average results of 54.29, 29.38, and 35.62 on HumanEval, StudentEval, and HumanEval+, respectively. This represents improvements of 5.66%, 19.38%, and 8.86% compared to the Mistral-7B-Instruct-v0.2 model using normal prompting. The M2WF framework also shows similar improvements on the DeepSeek-Coder-V2 model. The M2WF framework based on ChatGPT achieves average results of 84.65, 37.38, and 79.88 on HumanEval, StudentEval, and HumanEval+, respectively, while the average results for ChatGPT with normal prompting are 76.79, 28.88, and 64.74. Compared to normal prompting, the M2WF framework based on ChatGPT improves results by 10.24%, 29.43%, and 23.39%, respectively. A similar improvement is observed with the closed-source GPT-4 model. This demonstrates that the M2WF can achieve significant improvements in both open-source and closed-source models.\n(Answer to RQ5) the M2WF framework performs well on both larger and smaller parameter models. As shown in Tables 3, 4, and 5, the M2WF based on the DeepSeek-Coder-V2 of the larger parameter achieves results of 93.49, 55.20, and 84.01 on HumanEval, StudentEval and HumanEval+, respectively. Compared to the DeepSeek-Coder-V2 with normal prompting, the M2WF framework improves performance by 5.52%, 5.38%, and 9.35%. Similarly, the M2WF framework also performs well on the Mistral-7B-Instruct-v0.2 of the smaller parameter. This indicates that the M2WF framework achieves significant improvements across LLMs with both larger and smaller parameters."}, {"title": "6 Discussion", "content": "In this section, we will explore a performance comparison between the M2WF framework and few-shot retrieval method (i.e., AceCoder [18]), and analyze whether LLMs based on the M2WF framework can demonstrate performance improvements in other programming languages.\n(Answer to RQ7) performance comparison between M2WF framework and few-shot retrieval method. The AceCoder [18] method is one of the representatives of few-shot retrieval theory in"}, {"title": "7 Conclusion", "content": "Existing few-shot retrieval methods have achieved good results in one-time code generation tasks in LLMs. However, few-shot retrieval methods are particularly challenging for real-world data-free coding scenarios or benchmark tasks without training sets. In this work, inspired by human meta-memory processes, we propose a metamemory workflow (M\u00b2WF) framework to improve the performance of one-time code generation in LLMs. This framework not only eliminates the need to retrieve relevant examples from the training set but also ensures the reliability of the LLM's recalled content. Through extensive experimental results, we can see that M2WF framework significantly improves performance. In the future, we plan to apply the M2WF framework to actual software development to enhance development efficiency."}, {"title": "8 Limitations", "content": "Although our proposed M2WF framework has shown significant improvements in code generation tasks, it also has some limitations: 1) When using the LLM's API, the LLM may refuse to recall programming problems; 2) For benchmarks that lack function names, the code generated by the M2WF framework may not conform to the evaluation format; 3) The M2WF framework involves four stages and is based on one-time input and output, which significantly increases the number of tokens for both input and output, as shown in Table 7."}]}