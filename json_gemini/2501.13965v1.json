{"title": "ZKLORA: Efficient Zero-Knowledge Proofs for LORA Verification", "authors": ["Bidhan Roy", "Peter Potash", "Marcos Villagra"], "abstract": "Low-Rank Adaptation (LoRA) is a widely adopted method for customizing large-scale language models. In distributed, untrusted training environments, an open source base model user may want to use LoRA weights created by an external contributor, leading to two requirements: (1) the base model user must confirm that the LoRA weights are effective when paired with the intended base model, and (2) the LoRA contributor must keep their proprietary weights private until compensation is assured.\nWe present ZKLORA, a zero-knowledge verification protocol that relies on succinct proofs and our novel Multi-Party Inference procedure to verify LoRA-base model compatibility without exposing LoRA weights. ZKLORA produces deterministic correctness guarantees and validates each LoRA module in only 1-2 seconds on state-of-the-art large language models. This low-latency approach enables nearly real-time verification and promotes secure collaboration among geographically decentralized teams and contract-based training pipelines. The protocol ensures that the delivered LoRA module works as claimed, safeguarding the contributor's intellectual property while providing the base model user with verification of compatibility and lineage.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have attained remarkable success [1, 2], but verifying fine-tuned modifications such as LoRA [4] in an untrusted, distributed training environment can be difficult when the updated weights must remain private. Traditionally, one might re-run an entire forward pass or inspect thousands of parameters to ensure correctness, which is infeasible for massive models. ZKLORA addresses this by generating a zero-knowledge proof of correctness for each LoRA module, guaranteeing that the private LoRA genuinely fits the base model. Crucially, the verification"}, {"title": "2 Preliminary Results", "content": "We benchmarked ZKLORA across several LLMs and smaller models with different numbers of LORA modules. The input for inference is a batch of size 3 with sequence length 5. Our central question is how verification times, as well as settings and proof generation times, grow with the number of LORA modules, while also considering each LoRA's average parameter size. Figure 1 and Table 1 detail this trade-off."}, {"title": "3 ZKLORA", "content": "ZKLORA's design reflects the synergy between LoRA's parameter-efficiency and zero-knowledge cryptographic protocols: LoRA significantly shrinks the parameter footprint being proven, while the zero-knowledge aspect maintains confidentiality of the contributor's proprietary weights. By merging these ideas, ZKLORA enables trust-driven collaboration across decentralized infrastructures, contract-based training, and other scenarios where proof-of-correctness is essential but the LORA code remains private. Our approach also builds on incremental verification concepts [7] and advanced proof systems such as Nova [6] and HyperNova [5], which allow us to scale proofs to large neural networks. Ultimately, this combination provides a practical pipeline for parameter-efficient fine-tuning while verifying correctness in a succinct and minimally intrusive manner.\nWe implement a protocol that not only supports multi-party inference with partial activations exchanged between a base model user and a LoRA contributor, but also produces cryptographic proofs that the LoRA transforms are valid. The overall workflow is shown in Figure 2, while Figure 3 gives a deeper look at how Multi-Party Inference with LoRAs functions within an individual module. In addition, pseudocode for ZKLORA is in Algorithm 1. To begin the Multi-Party Inference, the base model user puts the dataset chosen for inference into the base model's first module. The forward pass continues through until the base model until it hits a module that uses remote LORA weights. When this occurs the base model user sends partial activations to the LoRA contributor for processing. These exchanged activations, shown conceptually in Figure 2, correspond to \"Base Acts\" from the base model user and \"LORA Acts\" from the LORA contributor.\nAfter the multi-party inference finishes, the LoRA contributor shifts to a proof generation phase. At this stage, each LoRA module is compiled into a constraint system describing the LORA transformations, and a key setup procedure yields the proving key, verification key, and possibly a structured reference string if the underlying zero-knowledge scheme requires one. The contributor then creates a \u201cwitness\u201d by running partial activations through these constraints and finally produces the proof files.\nOnce the proof generation is done, the base model user receives each proof and runs a fast verification procedure, typically requiring about 1-2 seconds per module. As Figure 3 suggests, this does\nnot require the LoRA contributor to reveal the actual low-rank matrices. Instead, the contributor\nonly sends updates and proofs that these updates conform to the declared LoRA transformations.\nIf any single proof fails, the base model user can reject the entire LORA submission; otherwise, the\nsystem is accepted as consistent with the underlying base model."}, {"title": "4 Related Work", "content": null}, {"title": "4.1 Low-Rank Adaptation (LoRA)", "content": "Low-Rank Adaptation (LoRA) [4] is a technique for parameter-efficient fine-tuning of large language models (LLMs) that injects small, low-rank adapter matrices into specific layers of a pre-trained model. By isolating the fine-tuning process to these low-rank components, LoRA drastically reduces memory overhead compared to full-model fine-tuning. This design choice is especially appealing for massive LLMs where training or even storing all parameters can be prohibitive [3].\nBeyond the clear advantage of reduced storage, LoRA also facilitates swapping multiple domain- specific adapters into a single base model, making it straightforward to maintain separate skill sets without instantiating an entire new copy of the model. These adapters can target specialized tasks (e.g., medical or legal text) with minimal overhead, driving LoRA's widespread adoption. Yet verifying that a proprietary LoRA truly aligns with a base model (without revealing the adapter) remains problematic\u2014precisely the gap ZKLORA fills."}, {"title": "4.2\nIncrementally Verifiable Computation", "content": "In a decentralized world, trust is a resource that is hard to achieve. In decentralized computation, we need to make sure the computations are both done and done correctly. In a seminal paper by Valiant (2008) [7], it was shown that proofs of knowledge can be used to assert the correct execution of general computations. That is, if M is a machine that runs for t steps producing a sequence of configurations c0, C1, ..., Ct, then there exists an efficient and effective way to produce a computationally sound proof for the computation c0 \u21e8 ct. This idea is referred to as Incrementally Verifiable Computation or IVC.\nThe main goal of IVC is to produce compact, updatable proofs of correctness for a sequence of computations, so that each new step can be verified on its own while building on the guarantees of previous steps. This technique significantly reduces the verification overhead for long or evolving computations, which is invaluable in scenarios like decentralized networks, outsourced computation, and any application requiring frequent correctness checks.\nKothapalli et al. (2022) [6] introduced the proof system NOVA and the idea of recursive proofs,\nwhich are proofs that can \"prove the correctness of other proofs.\" Recursive proof composition is\nkey to IVC where each proof attests to the correctness of both a step's output and the validity of\nthe previous step's proof.\nHyperNova [5] is a novel recursive argument system optimized for customizable constraint sys- tems (CCS) that generalizes and improves upon prior approaches like Nova. It achieves efficiency through a folding scheme where the prover's cryptographic costs are minimized and achieves zero- knowledge without relying on zkSNARKS. An IVC system allows the construction of proofs in zero-knowledge where the proofs reveal no information about the underlying computation or its inputs beyond the validity of the claim [7]."}, {"title": "5 Conclusion", "content": "ZKLORA provides a fast, robust mechanism to ensure that private LoRA modules remain effective when combined with a large base model. Our evaluations indicate that each LoRA module's forward computation can be verified in less than 2 seconds, even for multi-billion-parameter LLMs. This efficiency bridges the gap between privacy-preserving LoRA development and practical, real-time validation in large-scale deployments. In terms of future work, the most relevant and immediate"}]}