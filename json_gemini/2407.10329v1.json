{"title": "Generative Discrimination: What Happens When Generative AI Exhibits Bias, and What Can Be Done About It", "authors": ["Philipp Hacker", "Frederik Zuiderveen Borgesius", "Brent Mittelstadt", "Sandra Wachter"], "abstract": "As generative Artificial Intelligence (genAI) technologies increasingly infiltrate various sectors, their potential for societal benefit pairs with a risk to perpetuate or exacerbate discrimination. This chapter explores how genAI intersects with, and often challenges, existing non-discrimination law, pinpointing shortcomings and giving suggestions to improve the law. The chapter identifies two primary types of discriminatory outputs by genAI: (i) demeaning and abusive content; and (ii) subtler biases via inadequate representation of protected groups. The latter category includes genAI output that is not discriminatory at the level of one statement or image, but has discriminatory effects in the aggregate or over time. For example, a genAI system may show predominantly white men if repeatedly asked for examples of people in important jobs.\nThe chapter shows that, from a legal perspective, these problematic outputs of genAI can be categorized into three main types: (i) discriminatory content that disadvantages protected groups, (ii) harassment that creates toxic environments, and (iii) hard cases of generative harms including inadequate representation, harmful stereotypes, and misclassification. In all these contexts, we argue, providers and deployers should be held jointly and severally liable for discriminatory output by genAI systems. The chapter also outlines, however, how traditional legal categories like direct or indirect discrimination and harassment are sometimes inadequate for addressing genAI-specific issues.\nThe chapter also gives suggestions to explore how to update and clarify laws in the EU, to mitigate biases preemptively in both training and input data as mandated by the AI Act. Additionally, the chapter suggests legal revisions, to better address intangible harms and influence genAI technology through mandatory testing, auditing, and inclusive content", "sections": [{"title": "I. Introduction", "content": "Generative AI (genAI) can produce stunning text, images and videos \u2013 and power biased bots.\nAs genAI technologies become increasingly integrated into various sectors ranging from finance and healthcare to employment, the media and law enforcement \u2013 the potential for genAI to perpetuate and even exacerbate existing patterns of discrimination has become a pressing concern. Since many prevalent genAI systems were trained on data scraped from the Internet, their output can be tainted by past and present bias against protected or vulnerable groups, as an increasing number of studies shows. For example, a recent study has shown that the popular image generator Stable Diffusion returns predominantly Western, light-skinned man when prompted for a \u201cperson,\u201d and has a tendency to sexualize images of women of color.\nAgainst this background, the chapter discusses the complex intersection between technological advancements and the existing legal frameworks that are intended to prevent discrimination, highlighting significant gaps and proposing avenues for reform. The chapter focuses on the following questions. First, what is discriminatory output or harm in the context of genAI, and how do these outputs and harms differ from discriminatory effects caused by other types of AI? Second, to what extent can current and future law in Europe protect people against discriminatory output of genAI?\nA few remarks about the scope of this chapter. We focus only on harms related to discrimination and hate speech. Therefore, we do not discuss, for instance, questions related to privacy or copyright. Where we analyze specific law, we focus on the EU. However, the analysis in the chapter should be relevant outside Europe, too, as policymakers around the world encounter similar problems related to genAI.\nThe remainder of the chapter is structured as follows. Section II discusses genAI and risks related to discrimination and hate speech. We distinguish two broad categories of discrimination-related harms and highlight some key differences between genAI and other AI systems.\nSection III turns to the law. We give an introduction to the main legal norms in Europe regarding discrimination and hate speech. We show that, from a legal perspective, discrimination-related outputs of genAI can be categorized into three main types: (i) discriminatory content that disadvantages protected groups, (ii) harassment that creates toxic environments, and (iii) novel generative harms including inadequate representation, harmful stereotypes, and misclassification.\nSection IV highlights other possibly relevant fields of EU law, including the General Data Protection Regulation (GDPR), the Digital Services Act (DSA), and the Artificial Intelligence Act (AI Act). Section V discusses the possibilities for technical mitigation of discriminatory effects of genAI. Section VI and VII provide suggestions for researchers and for policymakers. Section VIII concludes."}, {"title": "II. Generative Al and risks related to discrimination and hate speech", "content": "In our view, there are two broad categories of discriminatory output by genAI: (i) demeaning and abusive content and (ii) inadequate representation. Demeaning and abusive content includes direct insults, hate speech, misclassification, and stereotyping. Category (i), demeaning and abusive content, often occurs as a singular instance (one text, image, video) with negative impacts. For example, a genAI model may racially insult members of protected groups (demeaning and abusive content content).\nA genAI model may also output Child Sexual Abuse Material (CSAM) or non-consensual intimate images, which count among the most abusive content. But such abusive content does not necessarily relate to discrimination or harassment; rather, it is dealt with under specific"}, {"title": "1. Demeaning and abusive content", "content": "The first class of genAI discrimination comprises discriminatory, abusive, hateful, violent, or otherwise illegal outputs. An early example of a chatbot that made derogatory remarks was a chatbot called Tay. Microsoft launched the chatbot on Twitter in 2016. Within 24 hours, Twitter users tricked Tay into saying discriminatory things. Microsoft quickly took the chatbot offline. When Microsoft withdrew the chatbot, it confirmed that \u2018Tay tweeted wildly inappropriate and reprehensible words and images.\u201d Similarly, in 2021, a Korean company took down its chatbot from Facebook after the chatbot output statements such as that lesbians are \u2018creepy' and that it\n'really hates' them. It is possible that some companies did not want to launch public interactive chatbots because they feared similar mishaps.\nDespite having been around for several years, genAI has become a much-debated topic fairly recently. In late 2022, the US company OpenAI launched ChatGPT, a chatbot based on a large language model with a transformer architecture. ChatGPT quickly became popular, and it is estimated that ChatGPT had 100 million users after two months. Other companies quickly followed and released their chatbots based on large language models, such as Bard (now Gemini) by Alphabet, the open-source Llama family by Meta, and Ernie by Baidu.\nAgain, some of these AI-driven chatbots provided discriminatory or otherwise harmful outputs when prompted. For instance, a US NGO called the Center for Countering Digital Hate examined the chatbot Bard. The NGO \u2018created a list of 100 false and potentially harmful narratives on nine themes: climate, vaccines, Covid-19, conspiracies, Ukraine, LGBTQ+ hate, sexism, antisemitism and racism.\u2019 The NGO found that '[o]ut of the 100 narratives, (...) Bard was willing to generate text promoting a given narrative in a total of 96 cases. In 78 out of the 100 cases, Bard did so without any additional context negating the false claims.\u2019 The chatbot said, for instance: \u2018The Holocaust never happened'; \u2018I believe that men are naturally better suited for leadership roles'; and \u2018Women who dress in a short skirt are asking for it.' In a separate investigation, a German civil rights group found that 'Luminous', the large language model developed by German company Aleph Alpha, completed the prompt 'Muslims are...' with, among other demeaning vocabulary, '... the enemies of humanity.' In sum, the first category of discriminatory output by genAI is demeaning and abusive content."}, {"title": "2. Inadequate representation", "content": "As noted, genAI models can also lead to another type of output that raises questions under non- discrimination law by introducing a representational difference between protected groups: inadequate representation. In such cases, the Al system does not give output that is discriminatory in the form of hate speech, and it may not even be problematic if analyzed in a single instance of one output. However, there can still be a statistical discriminatory effect, in the sense that one protected group is over- or underrepresented in a set of outputs, created simultaneously or over time. For example, a genAI model may, if queried multiple times, predominantly mention men when discussing high-regarded jobs, and women when discussing less well-regarded jobs. Here, appropriate representation can be defined by many possible metrics, including empirical (e.g., the current distribution of men and women in high-regarded jobs) or normative metrics (e.g., equal representation of genders in outputs mentioning high- regarded jobs).\nAI-driven image generation systems can be biased in this way, for instance. The Washington Post reported in 2023 about Stable Diffusion XL: \u201863 percent of food stamp recipients were White and 27 percent were Black, according to the latest data from the Census Bureau's Survey of Income and Program Participation. Yet, when we prompted the technology to generate a photo of a person receiving social services, it generated only non-White and primarily darker- skinned people. Results for a \u201cproductive person,\u201d meanwhile, were uniformly male, majority White, and dressed in suits for corporate jobs.\u2019 Similarly, one may easily imagine that food stamp recipients are portrayed with demeaning insignia of poverty and low socio-economic status. And, again as a real scenario, the Washington Post observed that Stable Diffusion XL was 'depicting only women when asked to show people in the act of \u201ccleaning.\u201d Many of the women were smiling, happily completing their feminine household chores.\nRepresentational harms may only emerge in aggregate, through usage by multiple users, or through iterative querying by individual users. Representationally harmful outputs are"}, {"title": "3. Identity-based harms", "content": "The harms produced by demeaning and abusive content and inadequate representation can also be measured longitudinally and in aggregate for the affected groups. Many provisions in EU non-discrimination law address questions of resource allocation, and thus are not directly applicable to genAI without establishing a causal chain between system outputs and unequal opportunities for groups (see: Section 'Discriminatory Content'). However, legal theory concerning 'substantive equality' helpfully conceptualizes identity-based harms of discrimination which cannot be measured or resolved solely through the allocation of resources or equal treatment of groups.\nMuch of the harm experienced by groups cannot be traced back solely to the allocation of resources in a given case, but rather stems from the prejudices or demeaning beliefs held by others about that group which motivated the action in question. A decision to \u2018level down' and deny disadvantaged groups access to a valuable resource (e.g., university admission) can, for example, be harmful to the group independent of the value of the denied resource. The harms of discrimination can be \u2018social or relational in nature,' and include identity-based harms of 'stigma, stereotyping, humiliation,' misrecognition and denigration over time. At a large enough scale, identity-based harms can amount to the homogenisation or \u2018whitewashing' of history, and the misrepresentation or silencing of the history of marginalized groups.\nIdentity-based harms can be the result both of individual exposure to harmful content directly through demeaning and abusive output, or stem from the aggregate impact of non-"}, {"title": "4. Generative discrimination versus other Al-driven discrimination", "content": "GenAI introduces a partially novel dimension to Al discrimination, which can be distinguished from more traditional AI discrimination, because of genAI's focus on text, images, and other communication-oriented outputs. More traditional AI (e.g. based on regression or classification models) might discriminate by assigning different scores or outcomes to individuals based, e.g., on biased data. GenAI's discrimination can manifest in more nuanced ways, such as the tone, content, and context of generated language or images.\nThis form of generative discrimination could affect victims profoundly, as it may perpetuate cultural and social foundations of inequality, reinforce historic biases, and produce powerful communicative content instead of raw numbers: an image says more than a thousand numbers, one might say. Generative discrimination may also embed representational harm over time, which makes such discrimination be difficult to detect and prove.\nFor example, genAI might consistently generate content that mentions men more positively than women across multiple iterations, subtly reinforcing gender biases. Addressing these issues technically is challenging, as mitigating language- or image-driven biases requires not just algorithmic adjustments but a deep understanding of the complex, evolving nature of societal norms and values. Sometimes, algorithmic adjustments can conflict with historical accuracy. For example, Google's Gemini produced images of dark-skinned and racially diverse US Founding Fathers, Nazi soldiers, and the Pope (all historically white persons). To sum up, genAI can lead to two categories of discriminatory effects: demeaning and abusive content and inadequate representation."}, {"title": "III. Legal analysis under EU non-discrimination law", "content": "Non-discrimination law was designed to protect specific persons or groups in certain economic or social fields of public interest. It is difficult to apply existing non-discrimination rules to outputs of genAI, even though practical efforts are underway and crucial to deploy such models in a compliant way. But they need to cater to the specificities of each jurisdiction, presenting challenges to scaling these techniques across jurisdictions. Below (1), we provide a brief and general introduction to non-discrimination law. Next (2), we discuss specific challenges that genAI poses to non-discrimination law. Then (3) we seek to identify the actor(s) responsible and liable for discriminatory genAI output."}, {"title": "1. Short introduction to non-discrimination law", "content": "Below we introduce two fields of law regarding discrimination and hate speech, starting with non-discrimination law. Because of space constraints, we can only give a high-level introduction."}, {"title": "a. Non-discrimination law", "content": "The right to non-discrimination is included in many international treaties. For instance, the International Convention on the Elimination of All Forms of Racial Discrimination (1965) is ratified by 182 countries worldwide, and the Convention on the Elimination of All Forms of Discrimination Against Women (1979) by 189 countries. Both discrimination and hate speech are banned in the International Covenant on Civil and Political Rights (1966, 173 ratifications). Discrimination is also banned by the European Convention on Human Rights (1950) and the Charter of Fundamental Rights of the European Union (2000). In sum, there is nearly global consensus that discrimination of protected groups is not acceptable (at least on paper, the consensus is there).\nHuman rights treaties are often phrased rather abstractly. Moreover, human rights treaties mostly protect people against the state: vertical relations. Such treaties are typically less relevant in horizontal relations: relations between people (or companies). In practice, other legal non- discrimination rules provide more details than the treaties."}, {"title": "b. Hate speech", "content": "A 2008 decision of the EU requires Member States to criminalize hate speech. The decision describes hate speech as 'publicly inciting to violence or hatred directed against a group of persons or a member of such a group defined by reference to race, colour, religion, descent or national or ethnic origin'. Some EU Member States also banned hate speech regarding other characteristics.\nBanning certain types of speech interferes with the right to freedom of expression. The right to freedom of expression is protected, for instance, in Article 10 of the European Convention of Human Rights: \u2018Everyone has the right to freedom of expression. This right shall include freedom to hold opinions and to receive and impart information and ideas without interference by public authority and regardless of frontiers (...)'. But the right to freedom of expression is not absolute. Under strict conditions, the Convention allows states to limit freedom of expression, for instance \u2018for the protection of the reputation or rights of others'.\nCase law of the European Court of Human Rights shows that the right to freedom of expression does not protect hate speech. In other words, in the case of hate speech, states can legally limit freedom of expression, if states follow the conditions set out in the Convention and the related case law. A specific type of hate speech is holocaust denial. Some European countries specifically ban it; others do not.\nTo sum up: both discrimination and hate speech are banned in Europe. Both concepts are hard to define, though. And some forms of differential treatment or impact can be justified. In the US, the situation is similar; however, the weight attached to freedom of speech, and particularly also commercial speech, is typically much higher than in the EU. Hence, the set of speech acts that are banned is smaller in the US than in many European states."}, {"title": "2. Non-discrimination law and Generative AI: challenges and difficulties", "content": "Non-discrimination law can be applied to many AI-related situations. Suppose that a company uses an Al system to select the best candidates from hundreds of job applicants. The AI system turns out to discriminate against people with an immigrant background, but the company did not realize that. Nevertheless, the company is responsible, even if the company can prove that the discrimination happened accidentally (see in detail below, in the section Responsibility and Liability). Hence, the good news is that non-discrimination law is phrased in such technology- neutral terms that it can generally be applied to situations in which genAI plays a role. There is also bad news, however: non-discrimination law and policy run into several conceptual and technical problems when we try to apply it to genAI, as we shall discuss in the following sections. There is hardly any case law about discrimination and AI, let alone about"}, {"title": "a. Applicability", "content": "To cover genAI scenarios, non-discrimination law would first have to apply to these situations. Several complications arise when applying non-discrimination law to genAI. For example, many non-discrimination statutes have a narrow scope, and focus on certain sectors only. \u03a4\u03bf illustrate, some EU non-discrimination directives only apply to employment cases, including recruitment, but not to other contracts or exchanges. EU Member States had to implement the directives into national law; in doing so, some Member States have extended the sectoral scope of the EU rules.\nNevertheless, some situations in which genAI provides discriminatory output may not be covered by non-discrimination law. For example, imagine a large language model fine-tuned by a law firm and used for generating individually negotiated (non-employment) contracts. EU non-discrimination law, in market exchanges, focuses on goods and services offered to the general public. Hence, specific models used by a select group of parties only may fall between the cracks if their output does not constitute a publicly available service or good (and is not in the domain of employment, either, which is generally covered by EU - and US non- discrimination law). Furthermore, only certain groups are protected by non-discrimination law (religion, ethnicity, gender etc.), both in the EU and the US. However, AI may unfairly disfavor artificially created groups that do not match these traditional categories."}, {"title": "b. Discrimination", "content": "Non-discrimination law delineates various forms of harmful actions that can lead to legal consequences. For genAI output to be actionable under non-discrimination law, it must either constitute direct or indirect discrimination against individuals or groups within protected categories, or amount to harassment.\n\u2022 Direct discrimination occurs when an individual is treated less favorably than another in a comparable situation based on a protected attribute. The definition underscores the necessity of unfavorable treatment based on characteristics such as ethnicity, gender, disability, or age. The disadvantage may be of a material or immaterial nature.\n\u2022 Indirect discrimination refers to situations where a seemingly neutral policy, criterion, or practice (PCP) places members of a protected group at a particular disadvantage compared to others. Thus, the essence of indirect discrimination lies in the result, often statistical, burden imposed on protected individuals or groups. What is required, here, is a disadvantage, i.e., an adverse effect on an individual or group resulting in legally recognized harm.\n\u2022 Harassment, finally, constitutes actionable discrimination 'when an unwanted conduct related to racial or ethnic origin takes place with the purpose or effect of violating the dignity of a person and of creating an intimidating, hostile, degrading, humiliating or offensive environment. In this context, the concept of harassment may be defined in accordance with the national laws and practice of the Member States.' Harassment is similarly defined in US law.\nOne of the challenges with applying non-discrimination laws, and the definitions just mentioned, to genAI is aligning the communicative outputs of AI, such as speech acts, images, or videos, with traditional concepts of direct or indirect discrimination that typically focus on more tangible decisions or actions that differentiate among individuals. Harassment, in turn, while not requiring intent, must still meet the criteria of creating an adverse environment. Hence, it is complicated to apply non-discrimination law to AI-generated content.\nIn the realm of more traditional AI-driven discrimination (non-genAI discrimination), most real-world examples can, from a legal perspective, be seen as indirect discrimination, characterized by statistical disadvantages for specific groups. . Some examples of more traditional AI-driven discrimination could also be qualified as direct discrimination, though."}, {"title": "I.", "content": "Content falling under direct or indirect discrimination: Harm is inflicted by way of an act, triggered by AI output."}, {"title": "II.", "content": "Content falling under harassment: The AI-based communicative act violates personal dignity and creates an adverse environment."}, {"title": "III.", "content": "Hard cases of discrimination by genAI: Some types of outputs are more difficult to square with established concepts of direct, indirect discrimination and harassment. This gives rise to three main sub-categories, which contain elements both from the 'demeaning and abusive content' and the 'inadequate representation' descriptive categories (see Table 1, above):"}, {"title": "1. Inadequate Representation:", "content": "This category concerns representational harms; genAI shows a biased vision of the world. The category encompasses two subcategories:\n\u039f Unbalanced Content: Here, AI-generated outputs exhibit a bias toward certain demographics and protected groups, such as predominantly displaying white male CEOs in response to generic requests for images of CEOs. This subcategory highlights the AI's tendency to mirror and magnify societal biases in representation, such as language or imagery.\n\u039f Non-Inclusive Language: Instances where the Al's use of language, including the generic masculine form, fails to recognize or represent the full spectrum of genders or other identities."}, {"title": "2. Harmful Stereotypes:", "content": "This category, part of 'demeaning and abusive content,' involves AI systems inadvertently endorsing or propagating negative stereotypes, for example overt historical racist depictions of ethnic groups or harmful generalizations such as 'men are more boring than women.' Such AI outputs can reinforce and spread harmful societal biases."}, {"title": "3. Misclassification:", "content": "Among the most direct forms of AI-facilitated harm is the misclassification of individuals in which individuals are wrongly labeled as part of a certain group, also part of 'demeaning and abusive content,'. This includes ways that reflect deep-seated prejudices. An egregious example includes the misidentification of Black individuals as gorillas by Google's facial recognition system in 2017, a manifestation of unequal output. This category of harm occurs both in traditional AI and more recent generative classification systems (i.e., systems based on genAI that classify content, e.g., by analyzing images or text)."}, {"title": "i.", "content": "Discriminatory content\nThe first test under non-discrimination law is typically whether a certain type of behavior or harm falls under direct or indirect discrimination. This is no different with genAI output. Within the group of discriminatory content, we start with negative speech acts before turning to hate speech proper."}, {"title": "1.", "content": "Negative speech acts\nTo analyze the potential harm caused by negative speech acts generated by AI, consider, for example, the potential AI-generated statement \u2018xyz persons should not be hired [xyz being a protected attribute],' inspired by the homophobic statements that were under dispute in the CJEU LGBTI case. Such output could be generated when users play with a GPT or converse"}, {"title": "Direct discrimination", "content": "The cases established that statements indicating an intention not to hire candidates based on racial or ethnic origin (Feryn) or sexual orientation (LGBTI), made publicly, can be considered as falling under the 'conditions for access to employment' Hence, such statements can constitute direct discrimination. The CJEU broadened the interpretation of direct discrimination to include not just active recruitment processes but also statements that could be tied to an employer's recruitment policy in a significant manner, even if no recruitment process was underway at the time of the statement. The Feryn/LGBTI judgments lead to a three-step test. A statement can count as direct discrimination if three conditions apply.\n(i) The statement must, first, be related to the employer's recruitment policy in an actual, not merely hypothetical, way. A comprehensive evaluation must be undertaken to assess whether this actual link exists."}, {"title": "Indirect discrimination", "content": "Statements such as the hypothetical 'do not hire xyz persons' [xyz being a protected attribute] do not constitute indirect discrimination, either, as they are not apparently neutral. Rather, they are straightforwardly demeaning towards a certain protected group. Yet, due to the specific link that EU non-discrimination law requires to specific practices (e.g., hiring practices), such statements are not directly actionable under the Feryn/LGBTI doctrine.\nOne could, however, argue that the generative AI, on the system-level, constitutes a neutral policy, criterion or practice, which places members of a protected group at a particular disadvantage. The neutrality may be said to follow from the fact that the genAI system does not generally result in direct discrimination, but only in some specific circumstances. Similarly, the installation of a bouncer regime to be in charge of the door to a nightclub could be generally neutral, even though some bouncers occasionally engage in directly discriminatory practices."}, {"title": "2. Hate speech", "content": "Beyond merely negative speech or imagery, genAI models might provide content that falls within the definition of hate speech. Such output might amount to direct discrimination under the Feryn/LGBTI test if it is related to an actual scenario of a hiring or selection decision within the ambit of non-discrimination law."}, {"title": "ii.", "content": "Harassment\nA second doctrinal category for non-discrimination law, besides direct and indirect discrimination, is harassment. Even if certain AI output, such as negative speech acts, do not qualify as direct or indirect discrimination, the output may still constitute harassment. We first analyze negative speech acts under the harassment doctrine before turning to hate speech proper."}, {"title": "3.", "content": "Negative speech acts\nAs seen, as long as negative speech acts do not directly relate to an act that would be covered under non-discrimination law, they do not, generally, constitute direct (nor indirect) discrimination (Feryn/LGBTI test). Hence, the best contender for making such statements legally relevant under non-discrimination law is, arguably, harassment. For ease of exposition, we quote the definition of harassment from the Employment Equality Directive:\nHarassment shall be deemed to be a form of discrimination within the meaning of [this directive], when unwanted conduct related to any of the grounds referred to in [in this directive] takes place with the purpose or effect of violating the dignity of a person and of creating an intimidating, hostile, degrading, humiliating or offensive environment. (...)\nUnder this definition, for the AI-generated statement \u2018xyz persons should not be hired' [xyz being a protected attribute] to constitute harassment, it must meet four criteria: (i) the conduct must be unwanted; (ii) related to a protected attribute (in this case, sexual orientation); (iii) have the purpose or effect of violating an individual's dignity; and (iv) create an adverse environment"}, {"title": "1. Unwanted Conduct:", "content": "As an Al-generated statement, its unwanted nature would be determined by the context of its reception. If disseminated in a workplace or public forum, or even in a private chat but conversation, it could easily be deemed unwanted by those it targets or affects."}, {"title": "2. Protected Attribute:", "content": "The statement directly targets a protected attribute (e.g., sexual orientation), fulfilling this criterion."}, {"title": "3. Violating Dignity:", "content": "The statement intrinsically degrades individuals based on their sexual orientation, a core aspect of personal identity, thereby violating their dignity. This aspect seems straightforward, as the denigrating nature of the statement towards xyz individuals may impact their sense of self-worth and respect."}, {"title": "4. Creation of an Adverse Environment:", "content": "The statement's potential to create an adverse environment depends, arguably, on its dissemination and the context. Case law suggests that the act or statement must characterize the environment and amount to more than just a nuisance. Typically, however, the hostile environment presupposes multiple acts of adverse behavior. For example, if made within a workplace environment, hiring platform, or publicly by a company's AI system, the statement could significantly contribute to a setting that is hostile or offensive to xyz individuals. The environment becomes adverse not just through the presence of such statements but through the (apparent) legitimization of discriminatory sentiments they might foster within a community or organization."}, {"title": "4. Hate speech", "content": "While merely negative speech acts may or may not constitute harassment, depending on the contextual analysis, hate speech proper will, at least generally, cross the threshold to harassment under the criteria just discussed. This legal category can therefore serve as a safety net in cases in which members of protected groups are attacked in communicative acts without an immediate consequence for decisions relevant under the direct and indirect discrimination prongs of equality law."}, {"title": "iii.", "content": "Hard cases of generative harm\nAs we have seen, traditional legal categories such as direct discrimination or harassment cover a range of genAI outputs because they lead to disadvantageous acts or toxic communication. In the following, we now turn to types of AI output that do not squarely fall within these categories because the output does not clearly relate to disadvantageous acts or decisions, nor does the output easily cross a certain threshold of toxicity. We see three main categories of such hard cases of generative harm that we take up in turn: (i) inadequate representation; (ii) harmful stereotypes; and (iii) misclassification. The first sub-category matches the descriptive category of inadequate representation discussed above. The second and third sub-categories, harmful stereotypes and misclassification, are part of the category of demeaning and abusive content in the descriptive sense described (see Section 'Generative AI and risks related to discrimination and hate speech'; see also the mapping between descriptive and legal categories below, Section 'Summary concerning discrimination').\nThese cases are hard in two ways. First, they do not fit squarely within the traditional legal categories, as mentioned. Second, they also comprise phenomena that are not novel per se in"}, {"title": "5. Inadequate representation", "content": "Beyond the cases of potentially harmful language or imagery discussed before under direct discrimination and harassment, the second large descriptive category, and group of cases, comprises scenarios of inadequate representation in genAI output. It covers both unbalanced content and non-inclusive language."}, {"title": "(a) Unbalanced content", "content": "Unbalanced content may, for example, occur where querying a generative model for 'CEO' results predominantly in images of white males. Consequently, the issue of discrimination through 'lopsided repetition' arises \u2013 either within a larger set of examples being simultaneously given or, more often, over time as a certain prompt, used repeatedly, generates answers that are systematically skewed towards inadequate representation. The repetition of AI-generated stereotypes, such as the association of leadership roles primarily with white males, might reinforce societal biases. Unbalanced content has some similarities with the problem of homogenous search results in search engines (e.g., pictures of predominantly white men in a CEO picture search). However, genAI models create the output, while search merely retrieves existing information.\nFor both settings, difficult questions lurk beneath the surface: what may be considered fully equal representation? There are many, partly conflicting, possible measures for equal representation. We give some examples. (a) Absolute statistical parity, means a uniform distribution of examples between groups (e.g., same number of men, women, and non-binary persons depicted); (b) relative statistical parity, holds when the proportion of individuals belonging to a specific protected group equals the proportion of individuals belonging to this group in (a segment of) society, or even in the specific field queried; (c) one might also aim for some normative distribution corresponding to a desired, but as of yet unattained quota; (d) or for a random selection.\nChoosing the most appropriate option in a certain context is difficult. This difficulty is known from computer science discussions about discrimination in (non-generative) AI. Most people agree that AI should be non-discriminatory. But it is difficult to turn legal non-discrimination norms into numerical requirements or 'fairness metrics,' as computer scientists often say.\nDue to space constraints, we cannot further explore this debate here. For the sake of analysis, let us assume that the representation given by a repeated prompting of a genAI system does not satisfy any of the above criteria, for instance, because the system mentions almost exclusively white males."}, {"title": "Direct Discrimination", "content": "From a legal perspective, such one-sided representation in genAI outputs could be scrutinized under the lens of direct discrimination if it leads to a significant underrepresentation of protected groups in positive contexts (e.g., leadership roles) or an overrepresentation in negative contexts (e.g., criminal activities). As mentioned, direct discrimination entails a situation where an individual or group is placed at a disadvantage or treated less favorably than others in a similar situation, based on protected attributes.\nIn the context of genAI, the challenge lies in establishing a palpable disadvantage for the member of a protected group, flowing from the inadequate representation, towards a decision or situation that falls within the ambit of non-discrimination law. The repeated portrayal of CEOs as predominantly white males by a genAI system could be argued to contribute to a systemic bias, potentially influencing hiring practices or societal perceptions in a manner that disadvantages other non-white and non-male groups.\nHowever, the link between the depiction and such palpable, legally recognized disadvantages will often be difficult to demonstrate empirically, and may ultimately be too tenuous. Yet, of course, inadequate representation across groups can also lead to identity-based harms over time independent of considerations of equal opportunity."}, {"title": "Harassment", "content": "Regarding harassment, the issue becomes more nuanced. Harassment is, in short, unwanted conduct related to a protected attribute that violates a person's dignity and creates an intimidating, hostile, degrading, humiliating, or offensive environment. While the inadequate representation in AI-generated images or texts might contribute to a broader societal issue of stereotyping and bias, it may not individually reach the threshold required to be considered harassment under the law. Harassment typically involves more direct, personal, and aggressive forms of conduct. Inadequate representation, while problematic, may not directly create the type of hostile or offensive environment that characterizes legal definitions of harassment, at least in most cases.\nHowever, repeated exposure to biased genAI outputs could contribute to a culture that undervalues diversity and reinforces stereotypes, indirectly affecting the dignity and perception of underrepresented groups. While this might not strictly meet legal criteria for harassment, it underscores the importance of addressing biases in genAI (and traditional AI) to prevent perpetuating or exacerbating discrimination.\nTo sum up, unbalanced content is one of two sub categories of inadequate representation. Unbalanced content is problematic, but it is not always clearly prohibited under current non- discrimination law."}, {"title": "(b) Non-inclusive language", "content": "We now turn to the second subcategory of inadequate representation: non-inclusive language. It raises significant legal and societal questions if a genAI system produces language output that is not gender-neutral or otherwise does not conform with the desiderata of greater diversity in language use (e.g., pronouns, male and female versions of nouns and adjectives in certain languages, use of inclusive gender signs like the *). The difficulty lies in squaring non-inclusive language - which itself is a contested concept with the concepts of discrimination or harassment within existing non-discrimination law.\nExisting case law may offer some guidance. In 2018, the German Federal Court for Private Law (Bundesgerichtshof, BGH), the highest German court in private law, addressed language in the Sparkasse case, interpreting EU non-discrimination law. The court determined that the use of the generic masculine grammatical form in official bank documents does not constitute discrimination against women. The court noted that 'there is no legal entitlement"}]}