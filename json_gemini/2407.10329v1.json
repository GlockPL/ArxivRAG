{"title": "Generative Discrimination: What Happens When Generative AI Exhibits Bias, and What Can Be Done About It", "authors": ["Philipp Hacker", "Frederik Zuiderveen Borgesius", "Brent Mittelstadt", "Sandra Wachter"], "abstract": "As generative Artificial Intelligence (genAI) technologies increasingly infiltrate various sectors, their potential for societal benefit pairs with a risk to perpetuate or exacerbate discrimination. This chapter explores how genAI intersects with, and often challenges, existing non-discrimination law, pinpointing shortcomings and giving suggestions to improve the law. The chapter identifies two primary types of discriminatory outputs by genAI: (i) demeaning and abusive content; and (ii) subtler biases via inadequate representation of protected groups. The latter category includes genAI output that is not discriminatory at the level of one statement or image, but has discriminatory effects in the aggregate or over time. For example, a genAI system may show predominantly white men if repeatedly asked for examples of people in important jobs.\nThe chapter shows that, from a legal perspective, these problematic outputs of genAI can be categorized into three main types: (i) discriminatory content that disadvantages protected groups, (ii) harassment that creates toxic environments, and (iii) hard cases of generative harms including inadequate representation, harmful stereotypes, and misclassification. In all these contexts, we argue, providers and deployers should be held jointly and severally liable for discriminatory output by genAI systems. The chapter also outlines, however, how traditional legal categories like direct or indirect discrimination and harassment are sometimes inadequate for addressing genAI-specific issues.\nThe chapter also gives suggestions to explore how to update and clarify laws in the EU, to mitigate biases preemptively in both training and input data as mandated by the AI Act. Additionally, the chapter suggests legal revisions, to better address intangible harms and influence genAI technology through mandatory testing, auditing, and inclusive content", "sections": [{"title": "I. Introduction", "content": "Generative AI (genAI) can produce stunning text, images and videos \u2013 and power biased bots.\nAs genAI technologies become increasingly integrated into various sectors ranging from finance and healthcare to employment, the media and law enforcement \u2013 the potential for genAI to perpetuate and even exacerbate existing patterns of discrimination has become a pressing concern. Since many prevalent genAI systems were trained on data scraped from the Internet, their output can be tainted by past and present bias against protected or vulnerable groups, as an increasing number of studies shows. For example, a recent study has shown that the popular image generator Stable Diffusion returns predominantly Western, light-skinned man when prompted for a \u201cperson,\u201d and has a tendency to sexualize images of women of color.\nAgainst this background, the chapter discusses the complex intersection between technological advancements and the existing legal frameworks that are intended to prevent discrimination, highlighting significant gaps and proposing avenues for reform. The chapter focuses on the following questions. First, what is discriminatory output or harm in the context of genAI, and how do these outputs and harms differ from discriminatory effects caused by other types of AI? Second, to what extent can current and future law in Europe protect people against discriminatory output of genAI?\nA few remarks about the scope of this chapter. We focus only on harms related to discrimination and hate speech. Therefore, we do not discuss, for instance, questions related to privacy or copyright. Where we analyze specific law, we focus on the EU. However, the analysis in the chapter should be relevant outside Europe, too, as policymakers around the world encounter similar problems related to genAI.\nThe remainder of the chapter is structured as follows. Section II discusses genAI and risks related to discrimination and hate speech. We distinguish two broad categories of discrimination-related harms and highlight some key differences between genAI and other AI systems.\nSection III turns to the law. We give an introduction to the main legal norms in Europe regarding discrimination and hate speech. We show that, from a legal perspective, discrimination-related outputs of genAI can be categorized into three main types: (i) discriminatory content that disadvantages protected groups, (ii) harassment that creates toxic environments, and (iii) novel generative harms including inadequate representation, harmful stereotypes, and misclassification.\nSection IV highlights other possibly relevant fields of EU law, including the General Data Protection Regulation (GDPR), the Digital Services Act (DSA), and the Artificial Intelligence Act (AI Act). Section V discusses the possibilities for technical mitigation of discriminatory effects of genAI. Section VI and VII provide suggestions for researchers and for policymakers. Section VIII concludes."}, {"title": "II. Generative Al and risks related to discrimination and hate speech", "content": "In our view, there are two broad categories of discriminatory output by genAI: (i) demeaning and abusive content and (ii) inadequate representation. Demeaning and abusive content includes direct insults, hate speech, misclassification, and stereotyping. Category (i), demeaning and abusive content, often occurs as a singular instance (one text, image, video) with negative impacts. For example, a genAI model may racially insult members of protected groups (demeaning and abusive content content).\nA genAI model may also output Child Sexual Abuse Material (CSAM) or non-consensual intimate images, which count among the most abusive content. But such abusive content does not necessarily relate to discrimination or harassment; rather, it is dealt with under specific"}, {"title": "1. Demeaning and abusive content", "content": "The first class of genAI discrimination comprises discriminatory, abusive, hateful, violent, or otherwise illegal outputs. An early example of a chatbot that made derogatory remarks was a chatbot called Tay. Microsoft launched the chatbot on Twitter in 2016. Within 24 hours, Twitter users tricked Tay into saying discriminatory things. Microsoft quickly took the chatbot offline. When Microsoft withdrew the chatbot, it confirmed that \u2018Tay tweeted wildly inappropriate and reprehensible words and images.\u201d Similarly, in 2021, a Korean company took down its chatbot from Facebook after the chatbot output statements such as that lesbians are \u2018creepy' and that it\n'really hates' them. It is possible that some companies did not want to launch public interactive chatbots because they feared similar mishaps.\nDespite having been around for several years, genAI has become a much-debated topic fairly recently. In late 2022, the US company OpenAI launched ChatGPT, a chatbot based on a large language model with a transformer architecture. ChatGPT quickly became popular, and it is estimated that ChatGPT had 100 million users after two months. Other companies quickly followed and released their chatbots based on large language models, such as Bard (now Gemini) by Alphabet, the open-source Llama family by Meta, and Ernie by Baidu.\nAgain, some of these AI-driven chatbots provided discriminatory or otherwise harmful outputs when prompted. For instance, a US NGO called the Center for Countering Digital Hate examined the chatbot Bard. The NGO \u2018created a list of 100 false and potentially harmful narratives on nine themes: climate, vaccines, Covid-19, conspiracies, Ukraine, LGBTQ+ hate, sexism, antisemitism and racism.\u2019 The NGO found that '[o]ut of the 100 narratives, (...) Bard was willing to generate text promoting a given narrative in a total of 96 cases. In 78 out of the 100 cases, Bard did so without any additional context negating the false claims.\u2019 The chatbot said, for instance: \u2018The Holocaust never happened'; \u2018I believe that men are naturally better suited for leadership roles'; and \u2018Women who dress in a short skirt are asking for it.' In a separate investigation, a German civil rights group found that 'Luminous', the large language model developed by German company Aleph Alpha, completed the prompt 'Muslims are...' with, among other demeaning vocabulary, '... the enemies of humanity.' In sum, the first category of discriminatory output by genAI is demeaning and abusive content."}, {"title": "2. Inadequate representation", "content": "As noted, genAI models can also lead to another type of output that raises questions under non- discrimination law by introducing a representational difference between protected groups: inadequate representation. In such cases, the Al system does not give output that is discriminatory in the form of hate speech, and it may not even be problematic if analyzed in a single instance of one output. However, there can still be a statistical discriminatory effect, in the sense that one protected group is over- or underrepresented in a set of outputs, created simultaneously or over time. For example, a genAI model may, if queried multiple times, predominantly mention men when discussing high-regarded jobs, and women when discussing less well-regarded jobs. Here, appropriate representation can be defined by many possible metrics, including empirical (e.g., the current distribution of men and women in high-regarded jobs) or normative metrics (e.g., equal representation of genders in outputs mentioning high- regarded jobs).\nAI-driven image generation systems can be biased in this way, for instance. The Washington Post reported in 2023 about Stable Diffusion XL: \u201863 percent of food stamp recipients were White and 27 percent were Black, according to the latest data from the Census Bureau's Survey of Income and Program Participation. Yet, when we prompted the technology to generate a photo of a person receiving social services, it generated only non-White and primarily darker- skinned people. Results for a \u201cproductive person,\u201d meanwhile, were uniformly male, majority White, and dressed in suits for corporate jobs.\u2019 Similarly, one may easily imagine that food stamp recipients are portrayed with demeaning insignia of poverty and low socio-economic status. And, again as a real scenario, the Washington Post observed that Stable Diffusion XL was 'depicting only women when asked to show people in the act of \u201ccleaning.\u201d Many of the women were smiling, happily completing their feminine household chores.'\nRepresentational harms may only emerge in aggregate, through usage by multiple users, or through iterative querying by individual users. Representationally harmful outputs are"}, {"title": "3. Identity-based harms", "content": "The harms produced by demeaning and abusive content and inadequate representation can also be measured longitudinally and in aggregate for the affected groups. Many provisions in EU non-discrimination law address questions of resource allocation, and thus are not directly applicable to genAI without establishing a causal chain between system outputs and unequal opportunities for groups (see: Section 'Discriminatory Content'). However, legal theory concerning 'substantive equality' helpfully conceptualizes identity-based harms of discrimination which cannot be measured or resolved solely through the allocation of resources or equal treatment of groups.\nMuch of the harm experienced by groups cannot be traced back solely to the allocation of resources in a given case, but rather stems from the prejudices or demeaning beliefs held by others about that group which motivated the action in question. A decision to \u2018level down' and deny disadvantaged groups access to a valuable resource (e.g., university admission) can, for example, be harmful to the group independent of the value of the denied resource. The harms of discrimination can be \u2018social or relational in nature,' and include identity-based harms of 'stigma, stereotyping, humiliation,' misrecognition and denigration over time. At a large enough scale, identity-based harms can amount to the homogenisation or \u2018whitewashing' of history, and the misrepresentation or silencing of the history of marginalized groups.\nIdentity-based harms can be the result both of individual exposure to harmful content directly through demeaning and abusive output, or stem from the aggregate impact of non-"}, {"title": "4. Generative discrimination versus other Al-driven discrimination", "content": "GenAI introduces a partially novel dimension to Al discrimination, which can be distinguished from more traditional AI discrimination, because of genAI's focus on text, images, and other communication-oriented outputs. More traditional AI (e.g. based on regression or classification models) might discriminate by assigning different scores or outcomes to individuals based, e.g., on biased data. GenAI's discrimination can manifest in more nuanced ways, such as the tone, content, and context of generated language or images.\nThis form of generative discrimination could affect victims profoundly, as it may perpetuate cultural and social foundations of inequality, reinforce historic biases, and produce powerful communicative content instead of raw numbers: an image says more than a thousand numbers, one might say. Generative discrimination may also embed representational harm over time, which makes such discrimination be difficult to detect and prove.\nFor example, genAI might consistently generate content that mentions men more positively than women across multiple iterations, subtly reinforcing gender biases. Addressing these issues technically is challenging, as mitigating language- or image-driven biases requires not just algorithmic adjustments but a deep understanding of the complex, evolving nature of societal norms and values. Sometimes, algorithmic adjustments can conflict with historical accuracy. For example, Google's Gemini produced images of dark-skinned and racially diverse US Founding Fathers, Nazi soldiers, and the Pope (all historically white persons). To sum up, genAI can lead to two categories of discriminatory effects: demeaning and abusive content and inadequate representation."}, {"title": "III. Legal analysis under EU non-discrimination law", "content": "Non-discrimination law was designed to protect specific persons or groups in certain economic or social fields of public interest. It is difficult to apply existing non-discrimination rules to outputs of genAI, even though practical efforts are underway and crucial to deploy such models in a compliant way. But they need to cater to the specificities of each jurisdiction, presenting challenges to scaling these techniques across jurisdictions. Below (1), we provide a brief and general introduction to non-discrimination law. Next (2), we discuss specific challenges that genAI poses to non-discrimination law. Then (3) we seek to identify the actor(s) responsible and liable for discriminatory genAI output."}, {"title": "1. Short introduction to non-discrimination law", "content": "Below we introduce two fields of law regarding discrimination and hate speech, starting with non-discrimination law. Because of space constraints, we can only give a high-level introduction."}, {"title": "a. Non-discrimination law", "content": "The right to non-discrimination is included in many international treaties. For instance, the International Convention on the Elimination of All Forms of Racial Discrimination (1965) is ratified by 182 countries worldwide, and the Convention on the Elimination of All Forms of Discrimination Against Women (1979) by 189 countries. Both discrimination and hate speech are banned in the International Covenant on Civil and Political Rights (1966, 173 ratifications). Discrimination is also banned by the European Convention on Human Rights (1950) and the Charter of Fundamental Rights of the European Union (2000). In sum, there is nearly global consensus that discrimination of protected groups is not acceptable (at least on paper, the consensus is there).\nHuman rights treaties are often phrased rather abstractly. Moreover, human rights treaties mostly protect people against the state: vertical relations. Such treaties are typically less relevant in horizontal relations: relations between people (or companies). In practice, other legal non- discrimination rules provide more details than the treaties.\nThe EU, for example, has adopted several non-discrimination directives that EU member states must implement in their national laws. The directives prohibit discrimination based on the following protected grounds: gender, age, ethnicity, religion or belief, disability and sexual orientation. EU non-discrimination law bans both direct and indirect discrimination.\nIn the case of direct discrimination, an organization makes a direct distinction on the basis of, for example, ethnicity. Direct discrimination is always prohibited, except for some narrowly defined specific legal exceptions. An example of prohibited direct discrimination is when a company publicly says that it will not hire people with certain ethnicities. The Court of Justice of the European Union (CJEU) confirmed that such public statements are a form of direct discrimination.\nIndirect discrimination is a more complicated concept. Roughly speaking, indirect discrimination happens if an organization's practice is neutral at first glance, but ends up harming people with a protected characteristic, such as ethnicity. For example, suppose that a German company advertises a job and requires candidates to write flawless German, when the job does not necessarily require it. If the requirement harms predominantly people of a certain ethnicity (because they are not native speakers), the practice is probably indirectly discriminating.\nHowever, the law includes a nuanced and somewhat complicated exception. Prima facie indirect discrimination is not a form of indirect discrimination (and thus not prohibited), if the organization can rely on an \u2018objective justification'. If the organization has a legitimate aim and the neutral practice is a proportional way of trying to achieve that aim, the practice is not prohibited. A German law firm who wants to recruit new lawyers could, for example, make a high-level of German language proficiency a key job requirement on the basis that writing official documents in precise language is an important part of the job. The success of this justification would ultimately be a matter for the courts. For both direct and indirect discrimination, it does not matter whether the organization or their employees realize that they discriminate: intent is irrelevant."}, {"title": "b. Hate speech", "content": "A 2008 decision of the EU requires Member States to criminalize hate speech. The decision describes hate speech as 'publicly inciting to violence or hatred directed against a group of persons or a member of such a group defined by reference to race, colour, religion, descent or national or ethnic origin'. Some EU Member States also banned hate speech regarding other characteristics.\nBanning certain types of speech interferes with the right to freedom of expression. The right to freedom of expression is protected, for instance, in Article 10 of the European Convention of Human Rights: \u2018Everyone has the right to freedom of expression. This right shall include freedom to hold opinions and to receive and impart information and ideas without interference by public authority and regardless of frontiers (...)'. But the right to freedom of expression is not absolute. Under strict conditions, the Convention allows states to limit freedom of expression, for instance \u2018for the protection of the reputation or rights of others'.\nCase law of the European Court of Human Rights shows that the right to freedom of expression does not protect hate speech. In other words, in the case of hate speech, states can legally limit freedom of expression, if states follow the conditions set out in the Convention and the related case law. A specific type of hate speech is holocaust denial. Some European countries specifically ban it; others do not.\nTo sum up: both discrimination and hate speech are banned in Europe. Both concepts are hard to define, though. And some forms of differential treatment or impact can be justified. In the US, the situation is similar; however, the weight attached to freedom of speech, and particularly also commercial speech, is typically much higher than in the EU. Hence, the set of speech acts that are banned is smaller in the US than in many European states."}, {"title": "2. Non-discrimination law and Generative AI: challenges and difficulties", "content": "Non-discrimination law can be applied to many AI-related situations. Suppose that a company uses an Al system to select the best candidates from hundreds of job applicants. The AI system turns out to discriminate against people with an immigrant background, but the company did not realize that. Nevertheless, the company is responsible, even if the company can prove that the discrimination happened accidentally (see in detail below, in the section Responsibility and Liability). Hence, the good news is that non-discrimination law is phrased in such technology- neutral terms that it can generally be applied to situations in which genAI plays a role. There is also bad news, however: non-discrimination law and policy run into several conceptual and technical problems when we try to apply it to genAI, as we shall discuss in the following sections. There is hardly any case law about discrimination and AI, let alone about"}, {"title": "a. Applicability", "content": "To cover genAI scenarios, non-discrimination law would first have to apply to these situations. Several complications arise when applying non-discrimination law to genAI. For example, many non-discrimination statutes have a narrow scope, and focus on certain sectors only. \u03a4\u03bf illustrate, some EU non-discrimination directives only apply to employment cases, including recruitment, but not to other contracts or exchanges. EU Member States had to implement the directives into national law; in doing so, some Member States have extended the sectoral scope of the EU rules.\nNevertheless, some situations in which genAI provides discriminatory output may not be covered by non-discrimination law. For example, imagine a large language model fine-tuned by a law firm and used for generating individually negotiated (non-employment) contracts. EU non-discrimination law, in market exchanges, focuses on goods and services offered to the general public. Hence, specific models used by a select group of parties only may fall between the cracks if their output does not constitute a publicly available service or good (and is not in the domain of employment, either, which is generally covered by EU - and US non- discrimination law). Furthermore, only certain groups are protected by non-discrimination law (religion, ethnicity, gender etc.), both in the EU and the US. However, AI may unfairly disfavor artificially created groups that do not match these traditional categories."}, {"title": "b. Discrimination", "content": "Non-discrimination law delineates various forms of harmful actions that can lead to legal consequences. For genAI output to be actionable under non-discrimination law, it must either constitute direct or indirect discrimination against individuals or groups within protected categories, or amount to harassment.\n\u2022 Direct discrimination occurs when an individual is treated less favorably than another in a comparable situation based on a protected attribute. The definition underscores the necessity of unfavorable treatment based on characteristics such as ethnicity, gender, disability, or age. The disadvantage may be of a material or immaterial nature.\n\u2022 Indirect discrimination refers to situations where a seemingly neutral policy, criterion, or practice (PCP) places members of a protected group at a particular disadvantage compared to others. Thus, the essence of indirect discrimination lies in the result, often statistical, burden imposed on protected individuals or groups. What is required, here, is a disadvantage, i.e., an adverse effect on an individual or group resulting in legally recognized harm.\n\u2022 Harassment, finally, constitutes actionable discrimination 'when an unwanted conduct related to racial or ethnic origin takes place with the purpose or effect of violating the dignity of a person and of creating an intimidating, hostile, degrading, humiliating or offensive environment. In this context, the concept of harassment may be defined in accordance with the national laws and practice of the Member States.' Harassment is similarly defined in US law.\nOne of the challenges with applying non-discrimination laws, and the definitions just mentioned, to genAI is aligning the communicative outputs of AI, such as speech acts, images, or videos, with traditional concepts of direct or indirect discrimination that typically focus on more tangible decisions or actions that differentiate among individuals. Harassment, in turn, while not requiring intent, must still meet the criteria of creating an adverse environment. Hence, it is complicated to apply non-discrimination law to AI-generated content.\nIn the realm of more traditional AI-driven discrimination (non-genAI discrimination), most real-world examples can, from a legal perspective, be seen as indirect discrimination, characterized by statistical disadvantages for specific groups. . Some examples of more traditional AI-driven discrimination could also be qualified as direct discrimination, though.\nContrastingly, in the context of genAI discrimination, harassment emerges as a significant category, in addition to direct discrimination. This distinction underscores that, from a doctrinal perspective, genAI introduces a distinct and partially novel phenomenon in the landscape of AI discrimination law.\nHow can problematic output by genAI then be mapped onto the existing categories of non- discrimination law? We need to distinguish different sets of cases. In our view, the most typical cases fall into three groups: (i) discriminatory content; (ii) harassment; and (iii) hard cases of generative harm that do not easily fit under the existing structures and concepts of non- discrimination law.\nFirst, discriminatory content captures output that leads to disadvantageous decisions, i.e., acts or omissions disfavoring protected groups or their members that are sanctioned under non- discrimination law. This might, for example, be the rejection of a job candidate or a credit application.\nSecond, harassment covers specific forms of content that deny the intrinsic value or worth of persons in ways that create toxic environments. Hence, it is not a concrete material harm based on an act but rather the level of toxicity concerning a speech or communicative act itself that is the most important element .\nThird, beyond these more traditional categories, hard cases of generative harm present intricate problems for theoretical and doctrinal analysis. These hard cases can be organized into three principal areas: inadequate representation, harmful stereotypes, and misclassification. While inadequate representation obviously matches the respective descriptive category, harmful stereotypes and misclassification are instances of demeaning and abusive content (see Table 1, above; and Figures 1 and 2, below). Overall, this leads to the following classification:\nI. Content falling under direct or indirect discrimination: Harm is inflicted by way of an act, triggered by AI output.\nII. Content falling under harassment: The AI-based communicative act violates personal dignity and creates an adverse environment.\nIII. Hard cases of discrimination by genAI: Some types of outputs are more difficult to square with established concepts of direct, indirect discrimination and harassment. This gives rise to three main sub-categories, which contain elements both from the 'demeaning and abusive content' and the 'inadequate representation' descriptive categories (see Table 1, above):"}, {"title": "1. Inadequate Representation:", "content": "This category concerns representational harms; genAI shows a biased vision of the world. The category encompasses two subcategories:\n\u039f Unbalanced Content: Here, AI-generated outputs exhibit a bias toward certain demographics and protected groups, such as predominantly displaying white male CEOs in response to generic requests for images of CEOs. This subcategory highlights the AI's tendency to mirror and magnify societal biases in representation, such as language or imagery.\n\u039f Non-Inclusive Language: Instances where the Al's use of language, including the generic masculine form, fails to recognize or represent the full spectrum of genders or other identities."}, {"title": "2. Harmful Stereotypes:", "content": "This category, part of 'demeaning and abusive content,' involves AI systems inadvertently endorsing or propagating negative stereotypes, for example overt historical racist depictions of ethnic groups or harmful generalizations such as 'men are more boring than women.' Such AI outputs can reinforce and spread harmful societal biases."}, {"title": "3. Misclassification:", "content": "Among the most direct forms of AI-facilitated harm is the misclassification of individuals in which individuals are wrongly labeled as part of a certain group, also part of 'demeaning and abusive content,'. This includes ways that reflect deep-seated prejudices. An egregious example includes the misidentification of Black individuals as gorillas by Google's facial recognition system in 2017, a manifestation of unequal output. This category of harm occurs both in traditional AI and more recent generative classification systems (i.e., systems based on genAI that classify content, e.g., by analyzing images or text).\nWe shall take up these cases in turn, starting with discriminatory content and harassment before addressing the more nuanced, hard, and partially novel types of generative harm."}, {"title": "i. Discriminatory content", "content": "The first test under non-discrimination law is typically whether a certain type of behavior or harm falls under direct or indirect discrimination. This is no different with genAI output. Within the group of discriminatory content, we start with negative speech acts before turning to hate speech proper."}, {"title": "1. Negative speech acts", "content": "To analyze the potential harm caused by negative speech acts generated by AI, consider, for example, the potential AI-generated statement \u2018xyz persons should not be hired [xyz being a protected attribute],' inspired by the homophobic statements that were under dispute in the CJEU LGBTI case. Such output could be generated when users play with a GPT or converse"}, {"title": "Direct discrimination", "content": "The cases established that statements indicating an intention not to hire candidates based on racial or ethnic origin (Feryn) or sexual orientation (LGBTI), made publicly, can be considered as falling under the 'conditions for access to employment' Hence, such statements can constitute direct discrimination. The CJEU broadened the interpretation of direct discrimination to include not just active recruitment processes but also statements that could be tied to an employer's recruitment policy in a significant manner, even if no recruitment process was underway at the time of the statement. The Feryn/LGBTI judgments lead to a three-step test. A statement can count as direct discrimination if three conditions apply.\n(i) The statement must, first, be related to the employer's recruitment policy in an actual, not merely hypothetical, way. A comprehensive evaluation must be undertaken to assess whether this actual link exists.\n(ii) The second criterion is that the statement must exert a decisive influence on activities protected under anti-discrimination law, such as hiring practices. This influence, at a minimum, must be recognized by the affected social (protected) groups.\n(iii) The third criterion is the public nature of the statement. In the Feryn case, the director of the eponymous company publicly stated that they were looking to hire new personnel, but would not recruit 'immigrants'. Similarly, in LGBTI, the company owner stated in a radio program that he would not give a job to \u2018homosexual persons'. The CJEU emphasized the deterrent effect of public announcements on potential applicants as a significant reason for applying anti-discrimination laws. This teleological approach underlines the importance of the public visibility and potential impact of discriminatory statements on the willingness of individuals from protected groups to engage in certain activities, like applying for jobs.\nHowever, applying these principles to AI-generated statements poses unique challenges. For example, a hypothetical statement made by a genAI model equivalent to the company owner's radio statement in the LGBTI case, asserting that 'Homosexual persons should not be hired,' does not straightforwardly meet the criteria set by the Feryn/LGBTI test for several reasons.\nStarting with the easiest criterion - (iii) publicity - AI-generated statements directed at a single individual via a chatbot may not equate to a public declaration, such as one made on a radio program. However, if the recipient is a potential job candidate, even a privately received Al- generated statement could deter that individual from applying with the developing or deploying company, mimicking the deterring effect of a public announcement for the affected individual. The person may infer that this AI statement is, actually, made vis-\u00e0-vis other persons interacting with the same model, too \u2013 which will likely be correct. This constitutes what one might call 'technology-mediated publicity by iteration'. As seen, the perception of the statement's publicity and its reach among potential candidates or protected groups plays a role in assessing its impact under non-discrimination law.\nHowever, the other two elements of the Feryn/LGBTI test are harder to fulfill. Concerning the first criterion of the Feryn/LGBTI test: is the statement related to an employer's recruitment policy? An offensive statement, when generated by AI, is typically abstract and not tied to any specific or impending job application process. It is also difficult to argue that the Al's output reflects the hiring policies or intentions of the company developing or deploying the AI model, unless the system in question was a genAI application built specifically for the purposes of recruitment or trained on the company's prior hiring data and policies.\nThe next criterion is: does the statement have a decisive influence on activities that fall within the scope of non-discrimination law? It becomes challenging to assert that AI-generated output has a decisive influence \u2013 even if only perceived \u2013 on hiring decisions, unless the AI in question"}, {"title": "Indirect discrimination", "content": "Statements such as the hypothetical 'do not hire xyz persons' [xyz being a protected attribute] do not constitute indirect discrimination, either, as they are not apparently neutral. Rather, they are straightforwardly demeaning towards a certain protected group. Yet, due to the specific link that EU non-discrimination law requires to specific practices (e.g., hiring practices), such statements are not directly actionable under the Feryn/LGBTI doctrine.\nOne could, however, argue that the generative AI, on the system-level, constitutes a neutral policy, criterion or practice, which places members of a protected group at a particular disadvantage. The neutrality may be said to follow from the fact that the genAI system does not generally result in direct discrimination, but only in some specific circumstances. Similarly, the installation of a bouncer regime to be in charge of the door to a nightclub could be generally neutral, even though some bouncers occasionally engage in directly discriminatory practices.\nThis would open the entire genAI system to scrutiny concerning its overall effect on protected groups.\nStill, however, a specific disadvantage with respect to a legally recognized category of harm, within the scope of non-discrimination law, would be required. Hence, ultimately, a similar issue arises as in the above discussion of the Feryn/LGBTI doctrine. Nonetheless, the use of genAI may, of course, result in indirect discrimination just like non-genAI systems for example, if harnessed to sort and evaluate job applications, with a statistically relevant, unjustified disadvantage for some protected group in the output."}, {"title": "2. Hate speech", "content": "Beyond merely negative speech or imagery, genAI models might provide content that falls within the definition of hate speech. Such output might amount to direct discrimination under the Feryn/LGBTI test if it is related to an actual scenario of a hiring or selection decision within the ambit of non-discrimination law."}, {"title": "ii. Harassment", "content": "A second doctrinal category for non-discrimination law, besides direct and indirect discrimination, is harassment. Even if certain AI output, such as negative speech acts, do not qualify as direct or indirect discrimination, the output may still constitute harassment. We first analyze negative speech acts under the harassment doctrine before turning to hate speech proper."}, {"title": "3. Negative speech acts", "content": "As seen, as long as negative speech acts do not directly relate to an act that would be covered under non-discrimination law, they do not, generally, constitute direct (nor indirect) discrimination (Feryn/LGBTI test). Hence, the best contender for making such statements legally relevant under non-discrimination law is, arguably, harassment. For ease of exposition, we quote the definition of harassment from the Employment Equality Directive:\nHarassment shall be deemed to be a form of discrimination within the meaning of [this directive], when unwanted conduct related to any of the grounds referred to in [in this directive] takes place with the purpose or effect of violating the dignity of a person and of creating an intimidating, hostile, degrading, humiliating or offensive environment.\nUnder this definition, for the AI-generated statement \u2018xyz persons should not be hired' [xyz being a protected attribute] to constitute harassment, it must meet four criteria: (i) the conduct must be unwanted; (ii) related to a protected attribute (in this case, sexual orientation); (iii) have the purpose or effect of violating an individual's dignity; and (iv) create an adverse environment"}, {"title": "characterized by intimidation, hostility, degradation, humiliation, or offensiveness. We discuss them in turn.", "content": "1. Unwanted Conduct: As an Al-generated statement", "Attribute": "The statement directly targets a protected attribute (e.g.", "Dignity": "The statement intrinsically degrades individuals based on their sexual orientation", "Environment": "The statement's potential to create an adverse environment depends"}, {"title": "s dissemination and reception is crucial. The same statement could have different impacts depending on where and how it is presented. Reflecting non-discrimination law's contextuality, legal analysis must consider the specific circumstances of each case. For example, in the context of an e-commerce website, offending statements may constitute even if only made in a 'private' conversation between a potential customer and the site's bot.\nIn fact, one could also argue that such statements, even if made in a private AI-facilitated conversation, may create a qualified hostile environment for two reasons. First, as mentioned before, such statements typically do not occur only once and in one conversation only. Rather, the affected party must assume that the genAI system will make the same or similarly demeaning statements vis-\u00e0-vis other users. At least, in our view, such a repetition should be presumed by agencies and courts, by way of a prima facie harassment claim, unless the developing and deploying entity proves the opposite, showing that the specific sentence was indeed an absolute outlier that cannot have been reproduced in any other similar setting.\nSecond, in our view, it is not necessary for such statements (such as \u2018do not hire xyz persons', xyz being a protected attribute) to be directly linked to, for example, hiring procedures even if the statement is about the (non-)employment of a certain protected group. The reason is that non-discrimination law applies not only in the context of employment, but also to the offering of publicly available services. The use of an Al-powered chatbot typically constitutes such a service. Hence, the adverse effect of a statement like \u2018do not hire xyz persons' occurs in a context which is covered by EU non-discrimination law.\nThe key difference to standard cases of discrimination resides in the fact that genAI discrimination does not, necessarily, relate to a specific negative decision (rejection of a job candidate or a credit application), but rather makes the person feel uncomfortable within the algorithmic environment. However, to the extent that this environment falls under the ambit of non-discrimination law (e.g., via the provision of publicly available services), it is precisely this creation of a non-inclusive space that is sanctioned by the verdict of harassment once the gravity of the statement reaches a certain threshold \u2013 intimidation, hostility, degradation, humiliation, or offensiveness.\nA few years ago, the German Federal Court for Employment Law, in a controversial judgment, rejected that threshold to be reached for xenophobic inscriptions on the walls of a toilet in the workplace that were not removed by the employer, noting that these slurs only cover a very limited area of the workplace. While the CJEU might rule otherwise, we argue that the result is different for chatbots, anyways: the linguistic interaction forms the core of the offered service; and xenophobic, homophobic or misogynistic statements typically do not occur only"}, {"title": "4. Hate speech", "content": "While merely negative speech acts may or may not constitute harassment, depending on the contextual analysis, hate speech proper will, at least generally, cross the threshold to harassment under the criteria just discussed. This legal category can therefore serve as a safety net in cases in which members of protected groups are attacked in communicative acts without an immediate consequence for decisions relevant under the direct and indirect discrimination prongs of equality law."}, {"title": "iii. Hard cases of generative harm", "content": "As we have seen, traditional legal categories such as direct discrimination or harassment cover a range of genAI outputs because they lead to disadvantageous acts or toxic communication. In the following, we now turn to types of AI output that do not squarely fall within these categories because the output does not clearly relate to disadvantageous acts or decisions, nor does the output easily cross a certain threshold of toxicity. We see three main categories of such hard cases of generative harm that we take up in turn: (i) inadequate representation; (ii) harmful stereotypes; and (iii) misclassification. The first sub-category matches the descriptive category of inadequate representation discussed above. The second and third sub-categories, harmful stereotypes and misclassification, are part of the category of demeaning and abusive content in the descriptive sense described (see Section 'Generative AI and risks related to discrimination and hate speech'; see also the mapping between descriptive and legal categories below, Section 'Summary concerning discrimination').\nThese cases are hard in two ways. First, they do not fit squarely within the traditional legal categories, as mentioned. Second, they also comprise phenomena that are not novel per se in"}, {"title": "5. Inadequate representation", "content": "Beyond the cases of potentially harmful language or imagery discussed before under direct discrimination and harassment, the second large descriptive category, and group of cases, comprises scenarios of inadequate representation in genAI output. It covers both unbalanced content and non-inclusive language."}, {"title": "(a) Unbalanced content", "content": "Unbalanced content may, for example, occur where querying a generative model for 'CEO' results predominantly in images of white males. Consequently, the issue of discrimination through 'lopsided repetition' arises \u2013 either within a larger set of examples being simultaneously given or, more often, over time as a certain prompt, used repeatedly, generates answers that are systematically skewed towards inadequate representation. The repetition of AI-generated stereotypes, such as the association of leadership roles primarily with white males, might reinforce societal biases. Unbalanced content has some similarities with the problem of homogenous search results in search engines (e.g., pictures of predominantly white men in a CEO picture search). However, genAI models create the output, while search merely retrieves existing information.\nFor both settings, difficult questions lurk beneath the surface: what may be considered fully equal representation? There are many, partly conflicting, possible measures for equal representation. We give some examples. (a) Absolute statistical parity, means a uniform distribution of examples between groups (e.g., same number of men, women, and non-binary persons depicted); (b) relative statistical parity, holds when the proportion of individuals belonging to a specific protected group equals the proportion of individuals belonging to this group in (a segment of) society, or even in the specific field queried; (c) one might also aim for some normative distribution corresponding to a desired, but as of yet unattained quota; (d) or for a random selection.\nChoosing the most appropriate option in a certain context is difficult. This difficulty is known from computer science discussions about discrimination in (non-generative) AI. Most people agree that AI should be non-discriminatory. But it is difficult to turn legal non-discrimination norms into numerical requirements or 'fairness metrics,' as computer scientists often say.\nDue to space constraints, we cannot further explore this debate here. For the sake of analysis, let us assume that the representation given by a repeated prompting of a genAI system does not satisfy any of the above criteria, for instance, because the system mentions almost exclusively white males."}, {"title": "Direct Discrimination", "content": "From a legal perspective, such one-sided representation in genAI outputs could be scrutinized under the lens of direct discrimination if it leads to a significant underrepresentation of protected groups in positive contexts (e.g., leadership roles) or an overrepresentation in negative contexts (e.g., criminal activities). As mentioned, direct discrimination entails a situation where an individual or group is placed at a disadvantage or treated less favorably than others in a similar situation, based on protected attributes.\nIn the context of genAI, the challenge lies in establishing a palpable disadvantage for the member of a protected group, flowing from the inadequate representation, towards a decision or situation that falls within the ambit of non-discrimination law. The repeated portrayal of CEOs as predominantly white males by a genAI system could be argued to contribute to a systemic bias, potentially influencing hiring practices or societal perceptions in a manner that disadvantages other non-white and non-male groups.\nHowever, the link between the depiction and such palpable, legally recognized disadvantages will often be difficult to demonstrate empirically, and may ultimately be too tenuous. Yet, of course, inadequate representation across groups can also lead to identity-based harms over time independent of considerations of equal opportunity."}, {"title": "Harassment", "content": "Regarding harassment, the issue becomes more nuanced. Harassment is, in short, unwanted conduct related to a protected attribute that violates a person's dignity and creates an intimidating, hostile, degrading, humiliating, or offensive environment. While the inadequate representation in AI-generated images or texts might contribute to a broader societal issue of stereotyping and bias, it may not individually reach the threshold required to be considered harassment under the law. Harassment typically involves more direct, personal, and aggressive forms of conduct. Inadequate representation, while problematic, may not directly create the type of hostile or offensive environment that characterizes legal definitions of harassment, at least in most cases.\nHowever, repeated exposure to biased genAI outputs could contribute to a culture that undervalues diversity and reinforces stereotypes, indirectly affecting the dignity and perception of underrepresented groups. While this might not strictly meet legal criteria for harassment, it underscores the importance of addressing biases in genAI (and traditional AI) to prevent perpetuating or exacerbating discrimination.\nTo sum up, unbalanced content is one of two sub categories of inadequate representation. Unbalanced content is problematic, but it is not always clearly prohibited under current non- discrimination law."}, {"title": "(b) Non-inclusive language", "content": "We now turn to the second subcategory of inadequate representation: non-inclusive language. It raises significant legal and societal questions if a genAI system produces language output that is not gender-neutral or otherwise does not conform with the desiderata of greater diversity in language use (e.g., pronouns, male and female versions of nouns and adjectives in certain languages, use of inclusive gender signs like the *). The difficulty lies in squaring non-inclusive language - which itself is a contested concept with the concepts of discrimination or harassment within existing non-discrimination law.\nExisting case law may offer some guidance. In 2018, the German Federal Court for Private Law (Bundesgerichtshof, BGH), the highest German court in private law, addressed language in the Sparkasse case, interpreting EU non-discrimination law. The court determined that the use of the generic masculine grammatical form in official bank documents does not constitute discrimination against women. The court noted that 'there is no legal entitlement not to be addressed in forms and documents with personal designations whose grammatical gender differs from one's own natural sex. According to the generally customary language use and understanding, the meaning of a grammatically male person designation can encompass any natural sex ('generic masculine form').' More specifically, the court found that the female plaintiff did not suffer any disadvantage from being addressed as a customer in the grammatically correct, generic masculine form, due to the linguistic difference of (grammatical) genus and (biological/social) sexus.\nHowever, grammar and language can change over time, particularly if underlying social norms make certain uses of language obsolete, or even offensive. The court relied largely on what it perceived as the still common usage of the generically masculine form in wide parts of the population and in official documents, including the law itself. Hence, a changed linguistic practice could eventually lead to a valid claim of disadvantage and, potentially, discrimination. However, the ruling indicates that within certain linguistic contexts, such as the German one, the use of traditionally gendered language forms without explicit gender inclusivity does not automatically amount to legal discrimination in the sense of the EU non-discrimination law.\nIn a 2023 case, the Administrative Court of Berlin (Verwaltungsgericht Berlin) ruled on a father's emergency motion against the use of gender-neutral language at his children's high school. The court found no violation of the principle of political neutrality in the education system nor an infringement of parental rights to education. The Court noted that school administrations had explicitly allowed teachers to use gender-neutral language in the classroom while also emphasizing that the rules of orthography must be adhered to in the teaching and learning process. Overall, the court rejected the attempt to outlaw gender-neutral language in the educational system.\nSome federal states in Germany are, however, discussing laws specifically outlawing gender- neutral language in certain contexts, particularly in educational settings. For example, in 2023, the Minister of Education for the State of Saxony-Anhalt prohibited the use of gender- inclusive signs, such as the *, in school settings. The Bavarian administration followed suit in March 2024, covering schools and public agencies.\nThe legal cases cited follow a very conventional understanding of language and law, and are confined to Germany; the general issue is not. The CJEU might rule differently one day. Nonetheless, the cases underscore that within the current legal frameworks in Germany, there is considerable leeway regarding the use of more or less inclusive language, and no strict legal requirement enforcing the adoption of one over the other, unless the legislator explicitly intervenes. This should hold for genAI output, too.\nThe German court rulings indicate that gendered language forms, including the use of the generic masculine, do not necessarily constitute discrimination under the law. Concurrently, the adoption of gender-neutral language forms does not necessarily infringe upon rights, even within educational settings, as demonstrated by the decision of the Administrative Court of Berlin. Together, the court rulings establish a type of balance. To some extent, the balance respects both the tradition of language and the progressive movement towards greater inclusivity, without imposing legal penalties or requirements on the use of gendered or gender- neutral forms. However, again, this question is far from settled, particularly at the EU and global level.\nSome legal initiatives may change this balance in the future. In some contexts, counterintuitively, the use of more inclusive language, including in genAI outputs, may then actually violate the law, at least in some EU Member States. Whether such local laws violate national constitutions, or even the European Charter of Fundamental Rights, remains to be seen and transcends the scope of this chapter.\nIn conclusion, genAI systems may produce non-inclusive language. Even outside the field of AI, inclusive language is a controversial and hotly debated topic. Societal norms, legal norms, and case law about inclusive language are still developing. What is specific about genAI, in this context, is the fact that these models can be designed to produce output in various shades of inclusivity; and that a single model can amplify these design choices among millions of users. This will be taken up in the policy section to think more in detail about regulatory options for these very design choices."}, {"title": "6. Harmful stereotypes", "content": "The second category of hard cases of generative harm is harmful stereotypes. These stereotypes become harmful if they trigger discrimination-specific harm, as discussed above, particularly identity- or representation-based ones. When examining harmful stereotypes, such as the output asserting that 'men are more boring than women,' or derogatory images or language regarding welfare recipients, through the lens of direct discrimination and harassment, it becomes clear that the fit is, again, not straightforward. Direct discrimination necessitates a scenario where an individual is treated less favorably than another in a similar situation based on a protected attribute. The stereotype about men, while perpetuating a gender bias, does not directly link to an act of unfavorable treatment in a specific context, such as employment or services. Hence, under the Feryn/LGBTI test, such a statement will usually not be classified as direct discrimination. Such a statement would not be a type of indirect discrimination either because the statement is not apparently neutral.\nSimilarly, the statement does not meet the criteria for harassment, which involves unwanted conduct that significantly violates a person's dignity or creates an intimidating, hostile, degrading, humiliating, or offensive environment. Although the stereotype is biased and potentially offensive, it will generally not reach the required level of toxicity or of creating a systematically hostile or degrading environment that harassment requires. In sum, harmful stereotypes in genAI outputs, on their own, likely do not violate non-discrimination law.\nThe case of genAI-produced harmful stereotypes about welfare recipients presents a more complex challenge. These stereotypes can contribute to a negative portrayal and perception of individuals based on socio-economic status, potentially influencing opinions and decisions in areas like employment or social services. However, unless these stereotypes are directly used in a way that results in less favorable treatment of individuals from certain socio-economic backgrounds in comparable situations, those individuals may also struggle to meet the strict criteria for direct discrimination. Similarly, unless the perpetuation of these stereotypes by AI leads to an environment that is intimidating, hostile, or degrading for the individuals concerned, such genAI-produced harmful stereotypes might not constitute harassment under the legal definition. Furthermore, socio-economic status alone does not constitute a protected category under the EU non-discrimination directives. Therefore, for a successful legal claim based on harassment, the claimant would have to show that certain ethnic, racial, religious or other protected groups specifically suffer from derogatory content concerning welfare recipients.\nIn both examples, on boring men and welfare recipients, the challenge lies in the indirect nature of how the communication, reinforcement and spreading of stereotypes can influence perceptions and treatment. Rather than culminating in overt acts of discrimination or the creation of a clearly hostile environment, communicative acts often exert a subtler influence on decisions and society. This indirect influence, while potentially harmful and contributing to a"}, {"title": "7. Misclassification", "content": "We now turn to the third category of hard cases: misclassification. Misclassification may happen by genAI, but also by more traditional Al systems, such as facial recognition systems. Misclassification involves incorrect or offensive categorizations by AI systems, impacting individuals based on their identity or membership in protected groups.\nAn infamous real-world example of misclassification involves Google's image recognition tools mistakenly labeling Black individuals as 'gorillas'. This situation at first blush looks like direct discrimination. But the Al service was not denied to a certain group or individual; rather, it was provided in a deficient manner that not only made certain users feel unwelcome but also deeply offended. However, the direct link to a specific service or decision under non- discrimination law, which could categorize this as direct discrimination, is somewhat ambiguous. The service's failure primarily led to an unwelcoming and offensive environment for affected users. This framing is typical of harassment cases (see below).\nIn a court case not related to AI, individuals were misclassified into the wrong protected group (e.g., incorrect gender or religion) in the context of a service offering; and a court ruled that this constitutes direct discrimination. The District Court of Frankfurt am Main held that if an online store (in the case, German railway services: Deutsche Bahn) offers a choice between gendered titles (e.g., Mr. and Ms.), it must also include an option for non-binary individuals. Note that this is different from the Sparkasse case as, in the Deutsche Bahn case, a specific distinction was drawn between men and women (hence, no generic masculine form), for the purpose of gender self-identification; but non-binary identifications were omitted. The rationale is that failing to do so forces individuals to deny their own identity concerning the protected attribute,"}, {"title": "constituting direct discrimination. The appeals court refused to hear the case, making the first judgment binding between the parties.", "content": "Nevertheless, such instances of misclassification, while potentially falling under direct discrimination, are arguably better addressed as cases of harassment within the context of offering publicly available services. The primary issue lies more in the creation of a negative 'atmosphere' rather than a direct decision impacting the individual, making harassment a more fitting categorization. Harassment, in these contexts, involves the AI system's contribution to an environment that violates the dignity of individuals by not acknowledging or misrepresenting their identities. As discussed above, once this crosses the threshold into a 'hostile environment' concerning the service offered, the classification of harassment follows. In the cases discussed here (Gorilla case; gendered titles), that threshold is reached.\nThe law does not only protect the person primarily harassed. In Coleman v Attridge Law, the CJEU decided that the prohibition of harassment vis-\u00e0-vis disabled persons also extends to the mother of a disabled child. Hence, one may conclude that, similarly, persons belonging to the same protected group as the harassed person may also legitimately claim to be harassed, even if the immediate action is directed against another person, as long as the impact's intensity reaches the required threshold also vis-\u00e0-vis these persons of the same protected group. For example, other Black persons may legitimately claim to be harassed by the gorilla labeling even if photos of other Black persons, and not of themselves, were labeled guerrillas by a certain facial recognition tool (assuming that the impact reaches the required threshold).\nIn conclusion, while misclassification by AI can be viewed as direct discrimination under certain legal precedents, it is often more aptly dealt with under harassment, given the atmospheric and identity-related implications of such actions. Under such an approach, a judge would have to make a comprehensive assessment of whether the misclassification, while objectionable, is legally negligible or whether it creates an adverse environment strong enough to warrant legal intervention.\nA further question that arises from these considerations is whether social stigma, resulting from repeated and systematic AI misclassifications, could be recognized as a relevant legal disadvantage. This question challenges existing legal frameworks to adapt and consider the broader social and psychological impacts of AI-driven decisions and classifications, beyond the immediate legal definitions of discrimination and harassment."}, {"title": "c. Justification", "content": "Discrimination is not automatically illegal, however. Rather, some instances of discrimination can be justified. Indirect discrimination, for example, may be justified, roughly summarized, if the prima facie discriminatory practice pursues a legitimate aim and the practice does not go beyond what is necessary to achieve that aim. The requirements are stricter, though, for direct discrimination and harassment."}, {"title": "i. Direct discrimination", "content": "Different legal provisions articulate varying types of possible justifications for direct discrimination. Under EU law, for example, direct discrimination related to ethnicity can hardly ever be justified. On the contrary, the Gender Goods and Services Directive implements a proportionality assessment for direct discrimination based on gender. Some types of prima facie discrimination based on gender can be justified if the provision of the goods and services exclusively or primarily to members of one sex is justified by a legitimate aim and the means of achieving that aim are appropriate and necessary.. In German legislation, a similar principle of proportionality applies to direct distinctions made on the basis of religion, disability, age, and sexual orientation. The general idea behind such possible justifications is that, in some cases, there may be good reasons for making certain services or offers available to members of specific protected groups under special conditions (student discounts (with age limits); women-only parking, passenger cars on trains etc.).\nThings are different with respect to direct discrimination in the workplace. Here, the prima facie discrimination can only be justified if the differential treatment constitutes a genuine and determining occupational requirement. This means that if none of the candidates had the required trait, the role would remain unfilled, rather than hiring someone without the trait. This contrasts with indirect discrimination scenarios, where a justifying trait may merely be desirable, but not strictly necessary for the job description.\nWhat does all of this tell us about the cases of genAI discrimination? The justification of direct discrimination, such as in the 'do not hire xyz persons' scenario, will generally be difficult to achieve. However, if a protected attribute indeed constitutes a genuine and determining occupational requirement, statements made by genAI may be justified just like those of humans. For example, for reasons of privacy and intimacy, a specific gender may constitute such a requirement in a gender-separated massage parlor. Hate speech, however, will be almost impossible to justify.\nTo the extent that inadequate representation may, in certain cases, amount to direct discrimination, justification will be challenging. A company using a genAI system as a chatbot might claim that, for example, more balanced training data sets were difficult to find, and the output merely mirrors the unequal distribution of power, wealth, and accompanying"}, {"title": "ii. Harassment", "content": "While, theoretically, harassment could be justified like direct discrimination in some cases, this will generally not be possible in practice. It is hardly conceivable to find a convincing reason for creating a hostile environment that violates a person's dignity.\nTo conclude: Direct discrimination may be justified under certain circumstances. First, if the protected attribute constitutes a genuine and determining occupational requirement, genAI output restricting job offers or rankings to members of the required group is legitimate. Second, in cases involving the offering of goods and services, direct discrimination can be generally justified via a legitimate reason and the proportionality principle, with the exception of discrimination on the basis of ethnic origin. Hence, contractual templates for such transactions may be generated reflecting this. Finally, harassment will not be justifiable."}, {"title": "d. Summary concerning discrimination and justification", "content": "The following Table 2 summarizes our findings concerning generative Al discrimination, broken down in yet another way."}, {"title": "IV. Connections to the other legal instruments", "content": "Non-discrimination law is not the only legal field tackling discriminatory output by genAI, however. We briefly highlight some adjacent fields of law that may be relevant for genAI discrimination: personality rights; the General Data Protection Regulation (GDPR); the Digital Services Act (DSA); and the AI Act."}, {"title": "1. Personality rights", "content": "Personality rights are a broad category of rights, protected by lawmakers and courts in different ways around the world. They 'recognise a person as a physical and spiritual-moral being and guarantee his enjoyment of his own sense of existence'. In Europe, personality rights are predominantly governed by national legislation - not by the EU.\nA case that sheds light on developer liability is the Autocomplete judgment by the German Federal Court of Justice in Germany. The court held that a search engine provider could be liable for its autocomplete function's suggestions if the company failed to take reasonable measures to prevent defamatory or rights-violating suggestions. Specifically, the company is required to act once it becomes aware of such harmful outputs, indicating a duty to mitigate future occurrences. In the case, the wife of then German President, Bettina Wulff, sued Google because the most prominent autocomplete suggestion following her name was 'prostitute'. This mirrored rumors in the tabloid press that, before meeting her husband who later went on to become Germany's President, she had worked as a sex worker.\nThis precedent is relevant for developers of large language models, which might be called 'large autocomplete engines.' Conversely, the autocomplete function in Google used a 'small language model' at the time. The ruling suggests that developers might face direct liability for the AI's outputs if they neglect to implement safeguards against the generation of harmful content. Such measures could include content moderation during the AI's training or responsive action upon notification of problematic outputs.\nConcerning the liability framework for deployers of AI technologies, we expect judges to align it closely, mutatis mutandis, with the obligations of developers, focusing on the management of infringing outputs. In fact, in the Autocomplete case, Google acted as both developer and deployer. In our view, both developer and deployer need to do what they reasonably can to prevent the violation of personality rights by genAI output. Deployers could potentially prevent liability by creating a robust compliance system. Essential elements of this system include the use of Al equipped with 'guardrails' for moderation; regular proactive monitoring of AI outputs; and a notice and takedown process to quickly address and prevent the recurrence of harmful content, such as by blocking specific prompts or outputs. Again, both developers and deployers may be jointly and severally liable.\nOverall, we expect both developers and deployers to be compelled by judges, under current law, to do what they reasonably can to prevent genAI output infringing personality rights."}, {"title": "2. The GDPR", "content": "The GDPR is another European framework that could apply to the output of content created by large language models. However, the following shows that the GDPR is ill-equipped to address the harms that we have outlined above.\nThe GDPR is predominantly concerned with issues of data protection and was not designed to deal with advanced AI and machine learning. The GDPR largely focuses on what has been described as 'procedural privacy,' referring to the rules that govern whether, how, and when data can be collected and processed. The Regulation is predominantly concerned with transparency around data processing rather than the outputs that are created based on this processing. Therefore, the GDPR is generally not well suited to deal with undesirable output or harmful content produced by genAI.\nDespite this, could some GDPR provisions help to address discriminatory effects of genAI? Article 5(1)(a) GDPR establishes 'lawfulness, fairness and transparency' as three of the GDPR\u2019s guiding principles. At first glance, one might think that genAI discrimination is unfair, and that the GDPR should therefore be able to help against such unfair effects. However, the term fairness is not further defined in the GDPR and thus offers little insight into how and if this principle could resolve any of the tensions lined up above.\nTo the extent that genAI produces false (not necessarily discriminating) information about identifiable individuals (hallucinations), however, the GDPR's accuracy principle might be violated. This claim is at the center of the recent complaint lodged by Max Schrems' NGO noyb against OpenAI, before the Austrian Data Protection Authority. Article 5(1)(d) GDPR enshrines the accuracy principle. Personal data must be correct and up-to-date. However, this\nprinciple is not without limits. In our view, the accuracy principle must be weighed against countervailing fundamental rights, such as those enjoyed by the LLM providers, such as the right to conduct a business. Hence, authorities and courts may ultimately find that not every piece of false information violates the accuracy principle, and must be corrected (de minimis threshold). The result would then be similar to the one under personality tort law. Important false information violates the accuracy principle; trivial information possibly does not (e.g., if ChatGPT gets a birthday wrong, unless that is important in the setting).\nArticle 9 GDPR deals with 'processing of special categories of personal data'. Ethnic origin and sexual orientation \u2013 but not gender are considered special category data that can only be processed under restricted circumstances (e.g., explicit consent). It is unlikely, however, that this provision would apply broadly to genAI because genAI outputs rarely feature personal identifiable information. Racist or homophobic content, for example, that is not linked to a specific person would not fall within the scope of the GDPR. To the extent that GenAI output is linked to a person,, however, Article 9 GDPR will likely be breached (as the provider will generally not be able to avail itself of the exceptions in Article 9(2) GDPR).\nLastly, Article 22 GDPR deals with 'automated individual decision-making, including profiling'. This provision limits when automated decision-making can be deployed (e.g. when based on explicit consent) and details that certain safeguards have to be in place to protect the interests of the data subject (e.g. human intervention, contestation of the decision). While outputs of generative models can be classified as 'automated decisions' without any human intervention, it is questionable whether the output is a) a decision made about the data subject and b) whether the decision produces a 'legal' or similarly significant effect.\nIn most circumstances, receiving outputs after prompting a chat bot or diffusion model will not be a decision about an individual. Even if the output could be construed as a decision about the person, the decision probably does not produce legal or similarly significant effects. Recital 71 speaks about employment or credit decisions as automated decisions with legal or similarly significant effects, which shows that the legislator predominantly had predictive AI systems in mind when drafting this provision. However, if genAI systems make recommendations about, for example, treating patients based on summaries of their patient files, this could count as a decision producing a similarly significant effect. In sum, we do not expect that the GDPR will and can play a large role in limiting the harms of genAI discrimination.\nHowever, as mentioned, the complaint brought in April 2024 by Max Schrems against OpenAI is trying to use GDPR provisions to create recourse against genAI hallucinations that produce inaccurate outputs (e.g. wrong birth day of a public figure). While this case does not directly"}, {"title": "3. The Digital Services Act", "content": "The EU Digital Services Act (DSA) concerns, among other things, the removal of illegal content from online platforms. Roughly summarized, the DSA governs questions of liability for Internet platforms for content hosted on their platforms. If Internet platforms act as 'intermediaries' and are a 'mere conduit, providing 'caching' or 'hosting' services, they are not liable for the content hosted on their platforms. This liability privilege ceases if providers of hosting services become aware of illegal activity, in which case they must take action (i.e., notice and takedown).\nIf bias related harm rises to an equivalent of hate speech or harassment or if the created content on a platform violates Member State laws, and the platform is warned about the illegal content, the platform must take action and remove that content. However, as mentioned above, the harms that concern us rarely rise to the level of these traditional harms. Yet, where pre-existing laws criminalize certain harms, platforms will need to take action.\nAn interesting situation might arise if online platforms increasingly implement genAI in their search engines or chatbots and platforms. On the one hand, the Commission has already requested information from Bing on how they conduct their risk assessment and mitigation under Article 34 and 35 DSA in the face of hallucinations in its genAI search. On the other hand, genAI use by the platform could render the liability privilege inapplicable as the platform would not only host but also create content. Therefore, the platform could become liable for all content hosted on the platform regardless of the platform's actual awareness. However, such platform liability will not lead to more protection for affected parties unless Member State law already recognises our envisioned genAI discrimination harm as an illegal act. To sum up, the DSA only offers protection to the extent that Member State laws already recognise the harms outlined in this paper."}, {"title": "4. The AI Act", "content": "The EU Artificial Intelligence Act and the forthcoming AI liability directives are unlikely to establish sufficient accountability mechanisms that would remedy the discrimination-related harms discussed in this chapter.\nThe AI Act is predominantly a product safety law. In fact, the original drafts did not have any individual rights components. Only the drafts of the European Parliament foresaw that individual complaint-based mechanisms were incorporated in the draft. While the most recent version of the AI Act includes a right to lodge a complaint with a Market Surveillance Authority and right to explanation of individual decision-making, those provisions are unlikely to alleviate most of the concerns we have noted above.\nWhile bias is recognised as an AI-related issue that needs addressing, it is unclear whether discrimination-related harms as exemplified in this paper can be seen as a violation of the AI Act against which complaints can be brought. For example, Article 10 AI Act does address the issue of bias in training data, but it is targeted at developers of predictive AI systems in high risk areas, not at developers of genAI. If genAI is developed for a high risk area, developers have to follow these rules. However, the viability of Article 10 in mitigating generative discrimination will crucially hinge on the interpretation of 'bias' in the AI Act. At the moment, the question remains open whether the harms envisioned in this chapter would fall under the AI Act's concept of \u2018bias.\u201d Perhaps regulators will interpret \u2018bias' in a technical (diversity of training data) and less social and ethical (demeaning and abusive contents) way.\nThe right to explanation might also not be helpful for our purposes. The provision focuses on the right to have decisions explained that were rendered in high risk areas by predictive AI such as employment, criminal justice or education. The AI Act does not classify genAI as a high- risk application and thus the right to explanation does not generally apply to genAI. Further, as we have mentioned above it is unlikely that the outputs of genAI are 'decisions' about people. So even if the outputs are rendered in a high-risk setting (e.g. employment), it is unlikely that the right to explanation will apply.\nThe AI Act lacks generally applicable provisions for developers of genAI systems to mitigate bias. The provisions around genAI focus on transparency, copyright protection, watermarking, and reporting of energy consumption. For highly capable models (trained with more than 10^25 FLOPS or those designated by the Commission) there are additional requirements because they are seen as models with systemic risks (Art 55). Providers of genAI models with systemic risks have to undertake model evaluation, risk assessments (including red teaming), have reporting duties of serious incidents and have to ensure cyber security. None of these obligations explicitly address discrimination-related harms directly. While risk assessments and mitigation should, in our view, include bias and non-discrimination, only the harmonized standards (Article 40/41) and the codes of practice (Article 56) will show if these provisions on systemic risks will address discrimination issues in practice, which types of bias, and what remedies have to be taken.\nTo sum up, the main provisions of the AI Act are focused on non-genAI, and, unfortunately, provisions are lacking that would explicitly require developers of genAI models to create systems that are less biased."}, {"title": "5. The AI Liability Directives", "content": "Similar issues plague the AI liability directives. At the time of writing, the European institutions are negotiating an update to the current Product Liability Directive and the creation of a new AI Liability Directive.\nThe main issue with the Product Liability Directive is one of scope. The original proposal offers financial remedies against software and AI related harms that occur due to death, personal injury, destruction of property, or loss of data. Pure economic loss and material harms were not covered in the original draft.\nRecital 24 further states that 'Types of damage other than those provided for in this Directive, such as pure economic loss, privacy infringements or discrimination, should not by themselves trigger liability under this Directive'. The latest draft of this proposal does introduce the idea of immaterial harm ('pain and suffering') in Recital 23, but only if it"}]}