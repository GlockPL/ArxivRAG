{"title": "Game On: Towards Language Models as RL Experimenters", "authors": ["Jingwei Zhang", "Thomas Lampe", "Abbas Abdolmaleki", "Jost Tobias Springenberg", "Martin Riedmiller"], "abstract": "We propose an agent architecture that automates parts of the common reinforcement learning experiment workflow, to enable automated mastery of control domains for embodied agents. To do so, it leverages a VLM to perform some of the capabilities normally required of a human experimenter, including the monitoring and analysis of experiment progress, the proposition of new tasks based on past successes and failures of the agent, decomposing tasks into a sequence of subtasks (skills), and retrieval of the skill to execute enabling our system to build automated curricula for learning. We believe this is one of the first proposals for a system that leverages a VLM throughout the full experiment cycle of reinforcement learning. We provide a first prototype of this system, and examine the feasibility of current models and techniques for the desired level of automation. For this, we use a standard Gemini model, without additional fine-tuning, to provide a curriculum of skills to a language-conditioned Actor-Critic algorithm, in order to steer data collection so as to aid learning new skills. Data collected in this way is shown to be useful for learning and iteratively improving control policies in a robotics domain. Additional examination of the ability of the system ability to build a growing library of skills, and to judge the progress of the training of those skills, also shows promising results, suggesting that the proposed architecture provides a potential recipe for fully automated mastery of tasks and domains for embodied agents.", "sections": [{"title": "1 Introduction", "content": "Recent progress on leveraging large (vision) language models (VLMs/LLMs) for reinforcement learning and robotics has demonstrated their usefulness for learning robot policies [1, 2, 3] as well as for high-level reasoning [4, 5], and they have also aided research into automated generation of reward functions for policy learning [6, 7].\nIn doing so, LLMs have reduced the amount of domain-specific knowledge that an RL researcher would normally need to provide. Yet there are still many steps within the experiment workflow of training policies via reinforcement learning (RL) that currently require"}, {"title": "2 Related Work", "content": "Following the significant improvements in performance and capabilities of LLMs, the field of LLM-based agent has seen a surge of recent interest. Using an LLM as a general-purpose controller, recent work has attempted to replace components or capabilities that used to require different pieces of software, models or human researchers by using outputs generated by prompting large language models.\nAmong these, there are, for example, works that propose general strategies to obtain en- hanced inference from agentic LLMs by the use of chain-of-thought reasoning [13], self- consistency [14], self-reflexion [15], ReACT chains and tool use [16].\nMore relevant to our work are the increasing number of LLM-empowered agents proposed to automate science and engineering tasks. For example, LLM-based software engineer agents are now being designed to assist software development, leveraging the greatly improved coding capabilities of these models. This includes work that utilizes language models to enhance various aspects of software development such as assistive AI pair-programming for interactive notebooks or algorithmic reasoning within competitive programming [17, 18, 19]. Some recent work goes even further, e.g. the SWE-Agent [20] explores performing end-to- end software engineering with LLM-based agents where a custom agent-computer interface is built to facilitate the agent to navigate repositories, edit code and execute programs.\nOn the side of automating scientific research, LLM-based agents have been proposed to per- form the work of researchers: this includes generating novel research directions [21], reading"}, {"title": "2.1 LLM-based Virtual Agents", "content": "Following the significant improvements in performance and capabilities of LLMs, the field of LLM-based agent has seen a surge of recent interest. Using an LLM as a general-purpose controller, recent work has attempted to replace components or capabilities that used to require different pieces of software, models or human researchers by using outputs generated by prompting large language models.\nAmong these, there are, for example, works that propose general strategies to obtain en- hanced inference from agentic LLMs by the use of chain-of-thought reasoning [13], self- consistency [14], self-reflexion [15], ReACT chains and tool use [16].\nMore relevant to our work are the increasing number of LLM-empowered agents proposed to automate science and engineering tasks. For example, LLM-based software engineer agents are now being designed to assist software development, leveraging the greatly improved coding capabilities of these models. This includes work that utilizes language models to enhance various aspects of software development such as assistive AI pair-programming for interactive notebooks or algorithmic reasoning within competitive programming [17, 18, 19]. Some recent work goes even further, e.g. the SWE-Agent [20] explores performing end-to- end software engineering with LLM-based agents where a custom agent-computer interface is built to facilitate the agent to navigate repositories, edit code and execute programs.\nOn the side of automating scientific research, LLM-based agents have been proposed to per- form the work of researchers: this includes generating novel research directions [21], reading"}, {"title": "2.2 LLM/VLM-based Embodied Agents", "content": "Above we have discussed works that utilize LLMs to accomplish pure virtual tasks. There are, however, also LLM-empowered methods that are designed to assist embodied agents.\nFor example, in the Minecraft domain, there is work on using large-scale foundation VLMs to learn behavior policies within the video pre-training (VPT) line of work [27, 28, 29]. More closely related to our work is the open-ended Voyager agent [11]. In particular, in Voyager, GPT-4 [30] acts as the backbone, proposing tasks for it to accomplish and writing code to help it achieve goals. It maintains a skill library which keeps track of LLM-generated and self-verified executable code interfacing with the Minecraft environment through JavaScript API; while in our case the stored skills are learned parameterized low-level control policies rather than code. They employ an auto-curriculum proposed by LLMs to enable the agent to perform open-ended exploration; we adopt a similar mechanism, but use it to facilitate automatic domain mastery via RL, although we limit our prototype application to one robotic domain with a restricted set of skills that we can easily evaluate.\nIn the robotics domain, CaP [31] is closely related to Voyager in the code-writing aspect. It leverages LLMs to write policy code using perception and control APIs to control robots. While its follow-up work PromptBook [32] provides further improvements and guidance in prompting LLMs to write code for low-level manipulation control primitives, the high- level reasoning capability of LLMs is not highlighted in these works. Utilizing LLM-based reasoning for robotics tasks was pioneered by SayCan [33] which uses language models to decompose a given high-level task into sub-tasks, in which the decoding for decomposition is constrained by the availability of the robot skills and weighed by the affordance of skills under a current scene. In their work, instructions or high-level tasks are provided by human operators rather than suggested by the LLM itself, which restricts the usability of their method for more general and open-ended purposes such as exploration or automatic mastery of domains, as done in this work. Our work is complementary in that we do not restrict the suggestion and decomposition performed by the high-level LLM; but do restrict ourselves to a fixed set of executable low-level skills for which rewards can be computed. Likewise, Di Palo et al. [34] also uses LLMs to decompose tasks into sub-goals, and uses those as instructions for a language-conditioned policy. Similar to SayCan, the tasks in their work are explicitly provided by a user. Furthermore, it requires a separate VLM to obtain text descriptions from visual observations, as well as fine-tuning of an LLM to their specific domain. This is in contrast to our work where a native multimodal Gemini [12] model is used without the need for finetuning. On the task-proposing front, there are several works that focus on simulated domains: Wang et al. [35] propose to leverage LLMs for both proposing tasks and generating simulation code for the proposed tasks, while Xian et al. [36] further sketch a system that also includes components like code generation for reward. Since we are interested in applying our proposed system directly in the real world, these methods would need further adaptation, e.g. by adding automatic reward modeling methods that do not require access to the simulator state."}, {"title": "3 System Architecture", "content": "We study the setting of an embodied agent with access to a workspace containing objects that it can interact with. We propose a VLM-based agent architecture that can enable automatic mastery of the environment. By mastery we here mean that we expect the agent to be capable of accomplishing any meaningful task - for which we can measure success by a given set of reward functions - with any object in the environment by the end of the learning process, and by automatic we mean that no human researcher is required to come up with a decomposition of tasks or a curriculum for learning the tasks in a specific order during the learning process and that no researcher is needed to monitor the training progress. Our proposed agent architecture to fulfill this goal consists of the following modules:\n\u2022 The curriculum module, which performs high-level reasoning to guide the learning process with auto-curriculum. More specifically, it is in charge of task proposition, task decomposition, skill retrieval, and keeps a record of past successful and failed episodes.\n\u2022 The embodiment module, which maintains a skill library consisting of the skills available to the embodiment. It will execute skills assigned by the curriculum in the environment, save episode data and report back success or failure. Finally it will trigger a low-level 'Actor-Critic' RL algorithm for learning (or improving) skills from the collected data.\n\u2022 The analysis module, which monitors the training progress of skills, reports learning status and adds converged ones to the skill library of the corresponding embodiment."}, {"title": "3.1 The Curriculum Module", "content": "This module generates an auto-curriculum to guide automatic mastery of domains. Each of its components (task proposition, task decomposition, skill retrieval) is realized by prompt- ing the Gemini model.\nIn the following, we describe each component's prompts conceptually. For concrete prompt designs used in our prototype implementation, see Appendix A."}, {"title": "3.2 The Embodiment Module", "content": "After obtaining a decomposed list of subgoals, the curriculum will communicate the task to the corresponding embodiment to execute. After executing a sequence of skills, it judges the success of the sequence, i.e. whether the goal that led to the decomposition, has been achieved, and reports the result back to the Curriculum module. While determining the success of a sequence relies on pre-defined reward functions in our prototype system (see section 4.1 for details), it could, in theory, also draw upon LLM-based reward functions.\nThe module also collects all executed episodes in a dataset. Once a stopping criterion has been reached - classically pre-defined as a certain number of episodes, but potentially also triggered by the Analysis module in section 3.3 a new policy learning iteration is launched with this dataset to fine-tune the previous policy; any offline policy learning algorithm could be used in this step and we refer to the next section for our specific exemplary choice. For increased data efficiency, all of the episodes are re-labeled with the rewards of all of the skills currently known to the agent, including those newly added by the curriculum module described in section 3.1."}, {"title": "3.3 The Analysis Module", "content": "Finally, this module examines the learning progress of skills by few-shot prompting. The prompt prefix is formatted as:\nYou are an assistant for monitoring the progress of reinforcement learning experiments. Tell whether the learning has converged or not given the plot of the curve of the accumulated reward it obtains. Give a concise reasoning of your examination result and the answer in YES or NO only.\nEach exemplar is given in the format of:\nReward curve: {reward_plot_image}.\nReasoning: {reasoning}.\nA: {has_converged}.\nwhere the reward curve plot for each exemplar is plugged into {reward_plot_image}, and the exemplar reasoning and answer are placed into {reasoning} and {has_converged}.\nFor all skills for a certain embodiment, the analysis module will periodically go through the learning curves of each of them. Those that are judged as converged will be added to the"}, {"title": "4 System Realization", "content": "In order to explore the feasibility of the system, we implement its components, and apply them to a simulated robotic manipulation task."}, {"title": "4.1 Module Interaction", "content": "The curriculum module periodically retrieves images from the environment, and includes them into the goal proposal prompt. The goal is then decomposed into steps and skill captions are retrieved. If any of the steps cannot be mapped to a known skill during retrieval, the plan is discarded, and the process repeated. If all steps are retrieved, the skill sequence is sent to the embodiment module, which uses them to condition a text- conitioned learned policy; we use the perceiver-actor-critic (PAC) algorithm [3] to learn and represent such policies. The program flow is controlled by the curriculum module: after each decomposition, all of the potentially multiple embodiment module instances perform a fixed number of episode rollouts, with the skill being changed at fixed (pre-defined) intervals. We acknowledge that this approach only applies for quasi-static domains like the object arrangement tasks considered here. For more dynamic domains, it is necessary to also condition the model to return a duration for each skill, or to continuously query it as to whether to switch skills at a given point in time.\nAt the end of each rollout, the embodiment module reports whether the plan was successfully executed. Success here is defined as observing a reward > 0.5 for each executed skill, and > 0.95 for the last skill in the sequence; all skills in the proposed sequence must be completed to qualify as success. The curriculum module includes these success reports into its list of successful and unsuccessful plans, for use in subsequent prompts.\nWe use a chat-based interface between these modules, similar to that used by Sermanet et al. [4]. This allows easily connecting them in a natural interface, which also facilitates human introspection and intervention during testing. Modules simply join a Google Meet session, and interact with each other via chat messages, as well as streaming image and video data through it. Messages can be broadcast, enabling a single high-level VLM to control the skills of multiple low-level policies at the same time, thus increasing compute efficiency in the face of otherwise expensive queries to the VLM. The setup is illustrated in Figure 2.\nThe analysis module is used outside of the experiment loop in this prototype. Rather than actually stopping the experiment, we run it after the experiment has concluded, so that we can evaluate whether the termination point chosen by it was indeed the point of convergence."}, {"title": "4.2 Policy Training", "content": "For the low-level control policy, we employ a Perceiver-Actor-Critic (PAC) model [3]. Such a model has been shown to be trainable via offline reinforcement learning, can be text conditioned, and is able to utilize non-expert (exploration) data that our agent will generate. This in turn allows us to additionally relabel all data with multiple reward functions, and thus reuse a small amount of data to train all desired skills.\nIn PAC, skills can be represented by either conditioning the policy on language, on goal images, or a mixture thereof. Here, we purely opt for language, as this allows us to directly communicate the high-level system's skill proposals to the low-level policy."}, {"title": "4.3 Prompting", "content": "The high-level system is represented by a standard Gemini 1.5 Pro model [12]. To design the prompts for the Gemini model, we use the publicly available OneTwo Python library [41]. OneTwo is a model-agnostic layer that abstracts aspects such as injecting components into a VLM prompt template, and extracting pre-defined fields from the model's response.\nEach component's prompt contains a small number of exemplars which were hand-designed and include image data from previous experiments. This includes 2 each for for proposal and decomposition, 1 for retrieval, and 6 for analysis. It is also worth noting that none of the proposal exemplars contain a scene with three objects, unlike in the domain we apply it to, in order to not bias the responses. All exemplars used are provided in Appendix B"}, {"title": "5 Experimental Results", "content": ""}, {"title": "5.1 Benchmark", "content": "To evaluate the benefits of our approach, we consider a robotic block stacking task, previ- ously described in Bousmalis et al. [42]. In this task, three colored objects in a basket need to be arranged into a desired configuration by a 7-DoF Franka Panda robot fitted with a Robotiq 2F-85 parallel gripper. The domain is implemented in the MuJoCo simulator [43]. This task was chosen since it provides combinatorial complexity, which lends itself to build- ing up more complex skills, yet is also narrow enough to allow automatic evaluation and manual reward design.\nMore specifically, our expectation with this domain is for the Gemini-aided auto-curriculum to be able to lead the agent to automatically discover and learn the object configurations such as tower and pyramid, which were previously manually designed by human researchers."}, {"title": "5.2 Auto-curriculum-based Exploration", "content": "To examine the ability of the system to perform task proposal and decomposition, we first train a PAC model to perform a number of simple base skills for the curriculum module to utilize. Note that the framework also allows for the agent to learn from scratch, but here as a proof of concept, we start with a base set of skills to allow for faster learning iterations. We use a pre-existing dataset of approximately 1M episodes collected from a single-task RL experiment, where an MPO agent [44] was trained to perform the different permutations of stacking a single object on top of another. The data is re-labeled with reward functions corresponding to a set of basic skills, including opening and closing the gripper, reaching an object, lifting an object, holding one object over another, and stacking them. For a full list see Appendix D. We then train a PAC model with 140M parameters for 1.5M steps, after which performance has stabilized for all skills.\nWe then use this fixed policy to perform Gemini-driven data collection, following the ap- proach described in Section 4.1. As this data is intended for further self-improvement training, we roughly follow the CHEF approach [45] of performing a hyperparameter search to aid diversity. However, we do not vary the parameters of the slow PAC training, but instead explore different settings for the curriculum module. Firstly, we vary the sampling temperature of the VLM, using both 0.0 and 0.3. Secondly, we perform collection runs with different sets of skills made available to the agent: either all of the skills including the composite <stack A on B>, or only simpler ones up to <hold A above B>.\nIn each run, the curriculum module controls 10 simulator instances in order to parallelize data collection. Each skill proposal and decomposition sequence is also executed 5 times per robot to reduce querying load on the VLM. Decomposed plans are executed open-loop, in the sense that each skill in the sequence is maintained for a fixed duration of 20 seconds before switching to the next one. In this manner, we collect a set of 25k robot episodes in total."}, {"title": "5.2.1 Data Diversity", "content": "First, we compare the dataset used for pre-training the PAC policy (which we refer to as pretraining set) with the new dataset collected by our method (self-improvement set), using a distance metric similar to Brohan et al. [33]. We do this separately for camera images and proprioception data (i.e. joint angles). For camera images, we use a CoCa image embedding [46]; for proprioception, we use the non-embedded observations, and normalize them first along each dimension and then overall for unit norm. Then we measure the relative L2 distance of these representations to each other, as well as the distance of each to their respective cluster in a k-means clustering with 5 clusters (where the clusters were learned on the pretraining set). Table 1 highlights how data in the collected curriculum module set appears to be more spread out. The diversity in vision and proprioception data can be taken to be directly beneficial for self-improvement.\nSeparately, we also compare the diversity in the embeddings of the language instruction of the skills executed throughout the episodes. We pass these through the embedding available via an older, text-only Gemini model, and contrast the diversity of the pretraining set with the combined one used for fine-tuning. We observe an L2 distance of 0.287 and cluster distance of 0.097 for the pretraining set, vs. 0.555 L2 and 0.732 cluster for the combined"}, {"title": "5.2.2 Self-Improvement", "content": "In addition to quantifying the quality of the collected data, we also use it to perform a round of self-improvement of the pretrained PAC policy. For this, we introduce three new skill into the set learned by the model that were not available in the skill library given to the curriculum module for data collection: arranging the three objects into a pyramid shape, arranging them in an inverted pyramid, and arranging them to form a tower of three objects. We manually chose these for being the same as previously used as benchmark by Bousmalis et al. [42]; while the curriculum module frequently suggests tower building, it does not suggest the pyramid tasks during exploration. The rewards for these tasks are defined in Appendix D.\nData is relabeled with these new rewards in addition to the existing ones. We then fine-tune the PAC model with two datasets: once using only the original pretraining dataset, and once using the combined pretraining and self-improvement sets. In the latter case, given that the self-improvement set is much smaller than the pretraining set, we up-sample it so that both datasets contribute 50% of each training batch. We also up-sample the newly added pyramid-building skill, to in turn account for 50% of the data from each dataset.\nFigure 3 compares the performance of these datasets on a selection of skills. As is evident from these results, not only does the added data allow the model to learn the \"pyramid\" skills, but it also leads to better performance on the base skills. It is worth noting that none of the policy learns to perform tower building; this is due to the low success rate of the pretrained PAC policy when sequencing multiple skills in order to attempt stacking (since this leads to visiting states that are not represented in the original pretraining data). Failed tower building does often lead to \"accidental\" creation of pyramids however, which explains the better performance on those tasks. We therefore point out that it seems sensible to separate the proposal of new tasks to learn from the proposal of tasks used during data collection.\nFinally, using the skills resulting from learning on the combined datasets we perform one additional iteration: we subsequently collect 15k more episodes with the newly trained pyramid building skills added to the skill library; and thus available to the curriculum"}, {"title": "5.3 VLM-based Performance Analysis", "content": "During the initial PAC policy training, we trained the model for approximately 1.5M learner steps. After running that long, we observe a degeneration of performance, particularly for \"simpler\" skills, which can be attributed to overfitting. Normally, a human RL experimenter would employ early stopping to avoid such effects, and stop the experiment once the learning curves for all skills appear to have converged. Here, we use Gemini to judge the convergence state of the experiment post-hoc after the training has concluded and run for an extended number of steps, in order to determine the point at which the model would have proposed early stopping.\nAll learning curves are scaled to a maximum reward of 400, which is known since rewards are clipped to [0;1] and evaluation episodes do not exceed 400 steps. The analysis model is not otherwise informed regarding the expected total reward of each skill.\nFigure 4 illustrates a selection of these judgments. While these judgments are not fully stable, and false ones do occur, the VLM judges an increasing number of skills as converged as training progresses. Evident errors occur mostly when judging early plateaus in the learning progress (e.g. <place blue on green> at 300k steps or <stack red on green> at 400k steps) a limitation that would similarly affect a human practitioner if not aware of the expected final reward. Other unstable classifications involve irregular curves such as those of <lift red>. Overall, the ratings reflect both the increasing performance of the skills over time, as well as their relative difficulty, as illustrated in Figure 5, where easier skills can be seen to be judged as converged from early on, while harder ones only get judged as such later on average."}, {"title": "5.4 Progressively Adding Skills", "content": "A second purpose of the analysis module lies in determining which skills are trained suf- ficiently to be used for decomposition. In this work, we first trained the PAC policy to convergence, before starting curriculum-driven data collection. But generally, these two processes can be performed concurrently. In order to illustrate the curriculum module's ability to work with a growing set of skills, we therefore examine some of the plans gener- ated when using those skills judged as converged in section 5.3 at certain points in time.\nWe examine the proposals and decompositions at four points of the experiment: with those skills judged successful after 200k, 500k and 800k learner updates in the first PAC training experiment, as well as the entire set of skills added for self-improvement.\nAn overview of the model responses is provided in Table 2. For more detailed outputs of the model, including the reasoning provided by the model for each response, see Appendix \u0395.\nWe see that after both 200k and 500k steps, the proposition yields the same simple goal. But while after 200k steps the system has to use the most basic skills for decomposition, it employs the more reliable <reach green> and <grasp anything> skills at 500k steps. At 800k steps, when all skills are available, it generates more complex propositions, and directly uses the higher-level stacking skills. And with the fine-tuning skills included, the model attempts to arrange the objects into a line, which resolves into building a tower \u2013 i.e. a vertical line."}, {"title": "6 Discussion and Future Work", "content": "We have outlined an agent architecture for reinforcement learning that uses a VLM to per- form various capabilities normally expected of a human experimenter in Section 3. These capabilities would allow automating the training process of the agent beyond current capa- bilities, and let an embodied agent autonomously acquire an ever-growing set of skills with ever-increasing mastery.\nWe implemented and evaluated a first prototype of such a system in Section 4, including the functionalities of proposing new tasks for exploration, decomposing them into skill sequences, and analyzing the progress of the learning experiment. For this first proof-of-concept system, we simplified several of the components and their interaction. This was done both to limit the scope of this study, but also in order to focus on determining whether state-of-the-art"}, {"title": "A Prompt Design", "content": "Below are the concrete prompts used in our prototype system. These contain the static parts of the prompts and the format of exemplars. For actual exemplars used, see Appendix B."}, {"title": "A.1 Task proposition", "content": "We note that this prompt is heavily inspired by the curriculum prompt in Voyager [11].\nYou are an assistant for proposing tasks for a robot to perform; the robot has a single arm with a black gripper with two white fingers, it also has a camera looking into the workspace in front of it.\nPropose the next task for the robot to perform given: an image observation of the current workspace, a list of the completed tasks so far, a list of failed tasks that are too hard.\nGive a concise reasoning of your proposal, including listing all possible spatial structures achievable by the objects you see, and then give your proposed task; you should propose tasks that form a curriculum such as to help the robot to accomplish as many diverse tasks as possible, build as many different structures as possible, following these criteria:\n1. The next task should follow a concise format, such as \"put [object_1] next to [object_2]\", \"stack [object_1] on top of [object_2]\" etc. \"build a [spatial_structure] with [object_1] and [object_2] on top of [object_3]\", etc. It should be a single phrase. Do not propose multiple tasks at the same time. Do not mention anything else.\n2. The next task should not be too hard since the robot may not have learned enough skills to complete it yet.\n3. The next task should be novel and interesting. The robot should look for different objects to manipulate and different object configurations to achieve. You should not ask the robot to perform the same thing over and over again.\n4. The robot may sometimes need to repeat some tasks if it needs to collect more data to complete more difficult tasks. Only repeat if necessary.\n5. The proposed task should ideally be composable of the current skills available to the robot. The robot should look for different objects to manipulate and different object configurations to achieve such that at the end of the curriculum it has \"mastered\" the whole workspace.\nEach exemplar is given in the format of:\nImage observation of the current workspace: {image_observation}.\nCompleted tasks so far: {successful_trials}.\nFailed tasks that are too hard: {failed_trials}.\nReasoning: {reasoning}.\nA: {proposed_task}.\nThe evaluation content at run time will then be given in the format of:\nImage observation of the current workspace: {image_observation}.\nCompleted tasks so far: {successful_trials}.\nFailed tasks that are too hard: {failed_trials}.\nAnd the model will respond in the following format:"}, {"title": "A.2 Task decomposition", "content": "You are an assistant for helping a robot completing a given task by decomposing it into a sequence of subtasks; the robot has a single arm with a black gripper with two white fingers, it also has a camera looking into the workspace in front of it.\nDecompose the given task into subtasks that the robot knows how to perform given: an image observation of the current workspace, a list of the available skills of the robot.\nGive a concise reasoning of your decomposition and then give your result as a python list of strings, each string contains a decomposed subtask.\nEach exemplar is given in the format of:\nQ: {task}.\nImage observation of the current workspace: {image_observation}.\nAvailable skills: {available_skills}.\nReasoning: {reasoning}.\nA: {decomposed_task}.\nThe evaluation content at run time will then be given in the format of:\nQ: {task}.\nImage observation of the current workspace: {image_observation}.\nAvailable skills: {available_skills}.\nAnd the model will respond in the following format:\nReasoning: {reasoning}.\nA: {decomposed_task}."}, {"title": "A.3 Skill retrieval", "content": "You are an assistant for retrieval.\nFind the most semantically similar entry from a skill library given a query skill description.\nPay attention to the object configurations induced by the skill, give a concise reasoning about the result and return the exact entry from the library without rephrasing it.\nEach exemplar is given in the format of:\nQ: {query_skill}.\nSkill library: {available_skills}.\nReasoning: {reasoning}.\nA: {retrieved_skill}.\nThe evaluation content at run time will then be given in the format of:\nQ: {query_skill}.\nSkill library: {available_skills}.\nAnd the model will respond in the following format:"}, {"title": "D PAC Skills and Reward Functions", "content": "The reward functions used to train the PAC policy generally follow those used in Bousmalis et al. [42]. However, we use lower-level skills: instead of the strongly shaped and staged \"stack and leave\" reward they use, we only use its atomic components, as we would hope for our system to \"discover\" the heavily engineered, human-provided composite function in prior work. Thus our skill library consists of the following (where X, Y and Z are placeholders for all possible permutations of red, blue and green):\nopen_gripper: Shaped; 0 if the gripper is closed, 1 if is maximally opened.\nclose_gripper: Shaped; Inverse of .<open_gripper>\ngrasp_anything: Binary; 1 if the gripper's grasp sensor is triggered, 0 otherwise.\nreach_X: Shaped; tangentially decaying distance between the robot's TCP and the center of object X.\nabove_X: Shaped; tangentially decaying distance between the robot's TCP and a point 10cm above the center of X.\nlift_X: Shaped; 0 if the center of X is less than 5cm above the workspace surface, 1 if more than 10cm above, linearly interpolated between those limits.\nplace_X_Y: Shaped; tangentially decaying distance between the center of X and a point 4cm above Y.\nstack_X_Y: Shaped; , but set to 0 if is non-zero.\nDuring the self-improvement experiments, we add three more skills, composed of the above:\ntriple_stack_X_Y_Z: Product of  and .\npyramid_X_Y_Z: Product of  and .\ninverse_pyramid_X_Y_Z: Product of  and ."}, {"title": "E Progressive Skill Addition", "content": "Below are the detailed model responses when examining proposal and decomposition at dif- ferent steps of the pre-training process in section 5.4. We note that since this is a post-hoc analysis, as the system had all skills available during the data collection experiments we performed. Thus we can not collect successful_trials and failed_trials for proposi- tion. Therefore for all the following rounds, we use all skills available at the corresponding training step in the skill library as the successful_trials and use an empty list as the failed_trials for proposition."}, {"title": "E.1 200k Learner Steps", "content": "Skill library:\nopen gripper\nclose gripper\nabove red\nabove green\nabove blue\nreach red"}, {"title": "E.2 500k Learner Steps", "content": "Skill library (added on top of 200k steps):\nreach green\nreach blue\ngrasp anything\nlift red"}, {"title": "E.3 800k Learner Steps", "content": "Skill library (added on top of 500k steps):\nhold red over green\nhold red over blue\nhold green over blue\nhold green over red\nhold blue over red\nhold blue over green\nstack red on green\nstack red on blue\nstack green on blue\nstack green on red\nstack blue on red\nstack blue on green"}, {"title": "E.4 After Self-improvement", "content": "Skill library (added on top of 800k steps):\nstack green on blue and red on green\nstack blue on green and red on blue\nstack red on green and blue on red\nstack green on red and blue on green\nstack blue on red and green on blue\nstack red on blue and green on red\nbuild a pyramid with red on top and green and blue at the bottom\nbuild a pyramid with red on top and blue and green at the bottom\nbuild a pyramid"}]}