{"title": "EvoP: Robust LLM Inference via Evolutionary Pruning", "authors": ["Shangyu Wu", "Hongchao Du", "Ying Xiong", "Shuai Chen", "Tei-Wei Kuo", "Nan Guan", "Chun Jason Xue"], "abstract": "Large Language Models (LLMs) have achieved remarkable success in natural language processing tasks, but their massive size and computational demands hinder their deployment in resource-constrained environments. Existing structured pruning methods address this issue by removing redundant structures (e.g., elements, channels, layers) from the model. However, these methods employ a heuristic pruning strategy, which leads to suboptimal performance. Besides, they also ignore the data characteristics when pruning the model.\nTo overcome these limitations, we propose EvoP, an evolutionary pruning framework for robust LLM inference. EvoP first presents a cluster-based calibration dataset sampling (CCDS) strategy for creating a more diverse calibration dataset. EvoP then introduces an evolutionary pruning pattern searching (EPPS) method to find the optimal pruning pattern. Compared to existing structured pruning techniques, EvoP achieves the best performance while maintaining the best efficiency. Experiments across different LLMs and different downstream tasks validate the effectiveness of the proposed EvoP, making it a practical and scalable solution for deploying LLMs in real-world applications.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), such as Llama (Touvron et al., 2023b) and OPT (Zhang et al., 2022a), have revolutionized natural language processing (NLP) by achieving state-of-the-art performance across a wide range of tasks (Touvron et al., 2023a; Zhang et al., 2022a; Scao et al., 2022; Chowdhery et al., 2023; Thoppilan et al., 2022; Zeng et al., 2023; Chowdhery et al., 2023). However, the success of these models comes at a significant computational cost. LLM consists of billions of parameters, requiring substantial memory, storage, and energy resources. This computational burden limits their deployment in resource-constrained environments.\nTo address these challenges, network pruning (LeCun et al., 1989) has emerged as a critical technique for optimizing LLMs by reducing their size while preserving their performance. Pruning involves selectively removing redundant or less important parameters (e.g., elements (Sun et al., 2024), channels (Ashkboos et al., 2024), or layers (Song et al., 2024)) from a pre-trained model, which enables faster inference and lower energy consumption. Importantly, pruning is feasible in LLMs because these models are often over-parameterized, containing more parameters than necessary to achieve their performance.\nUnfortunately, existing pruning techniques still face different challenges. Element-wise pruning achieves the finest-grained pruning to obtain a quite high sparsity, e.g., 50%, but requires specific computation kernels and hardware support to accelerate the inference. Channel-wise pruning trades off the hardware efficiency and sparsity. However, the complex channel dependency increases the difficulty of determining the pruning channel, thus resulting in performance drops. Layer-wise pruning adopts heuristic algorithms to search the pruning pattern and maintain the hardware friendliness for faster inference. But it often fails to find the optimal pruning patterns.\nTo address the challenges, we follow layer-wise pruning and introduce a novel evolutionary pruning framework, EvoP. EvoP first presents a cluster-based calibration dataset sampling strategy for collecting a more diverse calibration dataset. EvoP then combines the diverse calibration dataset with an evolutionary pruning pattern search method to identify the optimal pruning patterns that generalize well across tasks. Extensive experiments demonstrate that the proposed EvoP outperforms"}, {"title": "2 Background", "content": "2.1 Network Pruning\nNetwork pruning is a key technique in the LLM era, which aims to reduce neural networks' size and computational complexity by removing unnecessary or redundant structures, such as elements (Sun et al., 2024), channels (Ashkboos et al., 2024), or layers (Song et al., 2024). Existing methods target different levels of pruning granularity, from fine-grained element-wise pruning to coarse-grained layer-wise removal, offering a trade-off between model compression, hardware efficiency, and task performance.\nThis paper formally defines the network pruning problem to understand and unify these approaches better. Given a sparsity hyperparameter \u03b8, a pre-trained backbone model M with parameters W, and a calibration dataset D with n data, the goal of network pruning is to find a pruning pattern p in the Pruning Pattern Space P (PPS) that minimizes the loss of the pruned model Mp over the calibration dataset D,\n$$min_{p \\in P} L(f(M, p), D), \\text{s.t.} ||p||_0 \\le \\theta \\cdot ||W||_0$$\n, where L(\u00b7) is the loss function, p \u2208 {0, 1}m is a binary vector of zeros and ones, indicating a pruning pattern over m pruning components, which can"}, {"title": "2.2 Challenges of Different Pruning Techniques", "content": "The above formulation provides a unified framework for understanding and comparing different pruning methods. However, each pruning granularity introduces unique challenges. Element-wise pruning is a kind of unstructured pruning, which is not a hardware-friendly technique, requiring specific computation kernel designs and implementations. For example, 2:4 pruning (Frantar and Alistarh, 2023; Sun et al., 2024; Zhang et al., 2024b; Zhou et al., 2021; Zhang et al., 2022b) relies on specific sparse matrix multiplication operators and is currently only supported by GPUs with NVIDIA Ampere architecture. Besides, the pruning pattern space P of the element-wise pruning can be enormous, resulting in the proposed solutions being generally heuristic algorithms on each weight matrix (Wang, 2020; Jiang et al., 2018).\nChannel-wise pruning removes the entire channel for better hardware efficiency (Ashkboos et al., 2024; Ma et al., 2023). However, complex dependencies between channels often make it chal-"}, {"title": "3 Observations", "content": "This section presents two observations regarding algorithms and data, which motivate us to propose corresponding methods."}, {"title": "3.1 Observation 1: Sub-optimal Pruning Pattern of Heuristics in Large PPS", "content": "Existing state-of-the-art layer-dropping techniques adopt heuristic algorithms to select layers to drop. For example, SLEB (Song et al., 2024) enumerates all unpruned layers and removes the one where the model without the layer achieves the minimum perplexity loss on the calibration dataset. Then, SLEB repeats this process until the given sparsity is reached. This simple yet effective technique is successful because the SLEB algorithms can achieve as optimal as ideal solutions when the pruning pattern space is not large enough (small sparsity).\nTo better support the above claims, we conducted an analysis experiment on the Tinyllama model (Zhang et al., 2024a) over the calibration dataset in Figure 1. We follow the data selection of SLEB to build the calibration dataset from WikiText-2 (Merity et al., 2017). We collect three sets of perplexity losses over different sparsities: the SLEB, the Ideal that enumerates the whole pruning pattern space, and the Gene using the genetic algorithm to search.\nAs expected, when the sparsity is below around 35%, SLEB can easily find the global optimal since the pruning pattern space is small, only seven layers need to be selected at most. However, when the size of the pruning pattern space increases (larger sparsity), SLEB gradually gets stuck in the local optimal. Therefore, this motivates us to propose a new algorithm to further optimize the pruning in the scenarios with larger pruning pattern space. Noted, the size of the pruning pattern space is not only related to the given sparsity but also depends on the original model size, which can be calculated"}, {"title": "3.2 Observation 2: Low Data Diversity of Calibration Dataset", "content": "The above analysis shows the potential to approach the global optimal. However, the global optimal might differ when pruning on different calibration datasets. First, we analyze the semantic distribution of WikiText-2 and visualize the distribution in Figure 2. We use the BERT-base (Devlin et al., 2019) model to encode each sentence and TSNE to reduce the embedding dimensions. Figure 2 demonstrates that some sentences are semantically similar and naturally clustered. We also use KMeans to cluster those sentences and draw the colors, proving this claim.\nBased on the data characteristics, we further show the impact of different calibration datasets on the global optimal pruning patterns. Since Figure 1 has demonstrated the Gene can achieve comparable performance to the Ideal, we would regard the Gene's results as the Ideal's. We collect the perplexity of SLEB and Gene using the samples from each cluster in Figure 3. Experimental results show that different calibration datasets may lead to different optimal. This motivates us to propose a new data sampling method to help the pruning algorithms reach a better optimal."}, {"title": "4 EvoP: An Evolutionary Pruning Framework", "content": "Figure 4 shows the overview of EvoP, which consists of two key steps: (1) Build a robust calibration dataset based on the Cluster-based Calibration Data Sampling (CCDS), and (2) Determine the optimal pruning pattern with the Evolutionary Pruning Pattern Search (EPPS). Below, we show the details of each step.\n4.1 CCDS: Cluster-based Calibration Dataset Sampling\nTo achieve a more robust pruning pattern, EvoP first partition the raw dataset (e.g., WikiText-2 (Merity et al., 2017) or C4 (Raffel et al., 2020)) into k clusters based on input semantic similarity. To ensure the clustering effect and semantic integrity, we first divide the dataset into chunks with the same number of sentences and then cluster the data at the chunk level. BERT (Devlin et al., 2019) is applied to generate the embedding of each chunk since it can get information-rich representations compared to unidirectional pre-trained language models. Then, clustering algorithms such as KMeans are used to cluster these embeddings.\nThe generated clusters separate inputs with different computational requirements, allowing for more diverse and representative data sampling. For each cluster Cj (where j = 1,2,. . . ,k), We randomly select n representative samples, each of which may contain multiple chunks. These samples are used to evaluate the performance of different pruning patterns during the search process. Sampling across all clusters ensures data diversity, improving the"}, {"title": "4.2 EPPS: Evolutionary Pruning Pattern Search", "content": "Unlike heuristic methods in SLEB (Song et al., 2024), which rely on predefined rules (e.g., magnitude-based pruning), we proposed a revolutionary pruning pattern search algorithm (Algorithm 4.2) to explore search space of possible pruning patterns systematically and adaptively. First, a population of pruning patterns is initialized, where each pattern is represented as a binary vector p \u2208 {0, 1}m, with m is the number of layers in the LLM. A value of 1 indicates that the layer is pruned, while 0 means it is retained. Then, for each pruning pattern p, the fitness is computed as the average loss on the k \u00d7 n representative samples. The loss is measured using the task-specific objective (e.g., cross-entropy loss for language modeling). EvoP uses perplexity to measure the loss and select the top patterns with the lowest loss (highest fitness) to generate the next generation3. This mimics natural selection, where the fittest individuals survive. New pruning patterns are generated"}, {"title": "5 Experiments", "content": "In this section, we first introduce the dataset and the experimental setup. Then, we present the results of our EvoP and different pruning techniques on five downstream tasks. We also conduct analysis experiments to show our EvoP and those pruning techniques' in-domain and out-of-domain performance. Finally, we show the ablation studies.\n5.1 Datasets and Experiments Setups\nDatasets. Following SLEB (Song et al., 2024), we also use the library LM-Harness (Gao et al., 2024) to evaluate our EvoP and other pruning techniques"}, {"title": "5.2 Results on Downstream Tasks", "content": "Table 1 shows the performance on six downstream tasks. Overall, experimental results demonstrate that the proposed EvoP outperforms existing state-of-the-art pruning techniques on the representative downstream tasks across different sparsity levels. Compared to the lower sparsity (10%), the higher sparsity (20%) represents a larger pruning pattern space, which increases the difficulty of the network pruning problem. From the results, our EvoP achieves a better performance on higher-sparsity cases, consistently achieving better performance compared to baselines. The above results indicate EvoP's robustness in balancing sparsity and performance.\nTable 1 also presents the inference speedup of the pruned model with different pruning techniques compared to the dense model. Wanda achieves the same inference speed as that of the dense model. This is due to the fact that Wanda requires customized implementations using hardware-friendly codes. Otherwise, running Wanda's pruned model is like running the dense model with half-zero weights. SliceGPT with lower sparsity runs a bit slower than the dense model but a bit faster when with high sparsity. This might lie in that the extra overheads of SliceGPT are quite large; only when applying large sparsity would the efficiency benefits be dominant. For SLEB and our EvoP, these two are all layer-wise pruning, which only removes some layers. The pruned model would be quite hardware-friendly; thus, the speedup gains as much as the given sparsity. Our EvoP only removes layers according to the searched pruning pattern. Thus, its speedup is exactly the same as that of SLEB."}, {"title": "5.3 Results on Language Modeling", "content": "Table 2 and Table 3 show the in-domain and out-of-domain perplexity results on the wikitext-2 and C4 datasets. Experiments in Table 2 show that our EvoP consistently outperforms other pruning techniques, especially on the sparsity of 10%, demonstrating its effectiveness in maintaining in-domain performance. Similar conclusions can also be drawn from out-of-domain results."}, {"title": "5.4 Ablation Study", "content": "We conducted the ablation study of different components with different sparsity on the C4 calibration dataset. As shown in Table 4, CCDS can help find a better pruning pattern on the low sparsity level, while EPPS contributes more to the performance improvements on the high sparsity level. This may be due to the fact that at the low sparsity level, the pruning pattern space is not large enough, and heuristic algorithms can easily find the same optimal pruning pattern as the EPPS. However, at the high sparsity level, a large pruning pattern space is better for demonstrating the effectiveness of the EPPS, thus achieving more performance improvements."}, {"title": "6 Related Work", "content": "Language Models Compression. There are several main techniques for language model compression: knowledge distillation (Sun et al., 2020; Touvron et al., 2021; Pan et al., 2021), quantization (Yao et al., 2022; Gholami et al., 2021; Xiao et al., 2023), network pruning or sparsity, (He et al., 2024; Frantar and Alistarh, 2023; Ashkboos et al., 2024; Sun et al., 2024; Zhang et al., 2024b; Song et al., 2024; Liu et al., 2023) and early exit (Huang et al., 2024; Li et al., 2023; Xin et al., 2020). Knowledge distillation methods transfer knowledge from a large, complex model (called the teacher model) to a smaller, simpler model (called the student model). They must either learn the output distribution of teacher models (Agarwal et al., 2024) or design multi-task approaches (Liang et al., 2023) to ensure that student models retain the knowledge and generative capabilities of the teacher models. Quantization methods compress language models by reducing the precision of the numerical values representing the model's parameters (e.g., weights and activations). For example, OneBit quantized the weight matrices of LLMs to 1-bit (Xu et al., 2024). Early exit methods allow a model to terminate its processing early during inference if it has already made a confident prediction, avoiding the need for additional layers of computation (Chen et al., 2024). Network pruning, also known as sparsity techniques, refers to methods employed to compress language models by eliminating less significant structures within the network, such as individual weights, neurons, or layers (Frantar and Alistarh, 2023; Ashkboos et al., 2024; Song et al., 2024). The primary objectives"}, {"title": "7 Conclusion", "content": "In this paper, we proposed EvoP, an evolutionary pruning framework for robust LLM inference. EvoP leverages a cluster-based calibration dataset sampling and an evolutionary pruning pattern searching to jointly help further search the optimal pruning pattern for the model. Comprehen-"}, {"title": "8 Limitations", "content": "The search time is the main limitation of the proposed EvoP, which is longer than existing pruning methods and usually takes about hours to run on one NVIDIA A100 GPU. However, this search time should be accepted. The reasons are: 1) This is an offline process and only requires running once for multiple deployments; 2) With the searched pruning pattern, the pruned model can consistently improve its performance, making it worthwhile to spend the time cost; 3) Furthermore, parallel or distributed computing can accelerate the evolutionary pruning pattern searching process, while existing implementations lack parallelism."}]}