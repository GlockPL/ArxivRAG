{"title": "HiQuE: Hierarchical Question Embedding Network for Multimodal Depression Detection", "authors": ["Juho Jung", "Chaewon Kang", "Jeewoo Yoon", "Seungbae Kim", "Jinyoung Han"], "abstract": "The utilization of automated depression detection significantly enhances early intervention for individuals experiencing depression. Despite numerous proposals on automated depression detection using recorded clinical interview videos, limited attention has been paid to considering the hierarchical structure of the interview questions. In clinical interviews for diagnosing depression, clinicians use a structured questionnaire that includes routine baseline questions and follow-up questions to assess the interviewee's condition. This paper introduces HiQuE (Hierarchical Question Embedding network), a novel depression detection framework that leverages the hierarchical relationship between primary and follow-up questions in clinical interviews. HiQuE can effectively capture the importance of each question in diagnosing depression by learning mutual information across multiple modalities. We conduct extensive experiments on the widely-used clinical interview data, DAIC-WOZ, where our model outperforms other state-of-the-art multimodal depression detection models and emotion recognition models, showcasing its clinical utility in depression detection.", "sections": [{"title": "1 Introduction", "content": "The diagnosis of depression in clinical settings often involves the use of interview-based instruments [66], in which mental health experts conduct clinical interviews with patients, assessing their symptoms [53, 66, 68, 89]. Due to the gradual and varied manifestation of depressive symptoms among individuals [13, 17, 18, 48], clinicians employ a structured interview process, which involves specific questionnaires and criteria, to detect a wide range of verbal and non-verbal symptoms of depression in patients' speech, behavior, facial expressions, and immediate responses during conversations [46, 71, 79].\nInterview-based diagnostic methods have proven highly effective in interpreting patient responses [29, 53]. Mental health experts strategically incorporate follow-up questions in conjunction with primary questions to gather additional information from patients, thereby enhancing their understanding of the exhibited depressive symptoms [68]. In a case where a response from an initial follow-up question is insufficient for diagnosis, further follow-up questions can be employed to synthesize the patient's responses. By employing hierarchical questions during clinical interviews, clinicians can obtain a comprehensive understanding of depressive signals and the patient's overall condition, leading to improved diagnostic accuracy [17, 68].\nUsing clinical interview data, many scholars have proposed methods that can detect depression by analyzing revealed verbal (e.g., textual) or non-verbal (e.g., visual or acoustic) signals. Some studies have delved into visual cues, encompassing facial expressions and head poses [4, 5, 31, 51]. Additionally, a series of investigations has focused on acoustic and textual cues, with the goal of diagnosing depression based on linguistic patterns, vocal qualities, pitch, and loudness [2, 43, 63, 82, 91]. Nevertheless, these approaches have treated the entire input sequence as a singular entity, disregarding the structured nature of clinical interviews. Given the use of structured questionnaires and conversational exchanges during these interviews, considering the interview data as a single input sequence can be less effective for depression detection in"}, {"title": "2 Related Work", "content": "Researchers have identified distinctive features across various modalities, including acoustic patterns, visual characteristics, and language usage in clinical interviews. For instance, individuals with depression often exhibit specific acoustic features, such as slower speaking rates, lower pitch ranges, and reduced loudness [6, 9, 20, 36, 75, 76, 79] as well as visual features, including discernible facial expressions characterized by sadness, minimal head movement [5], unstable facial expressions [14, 78], and irregular eye-gazing patterns [33, 40]. Moreover, they often reveal negative emotions in language, utilize a higher frequency of first-person pronouns, and exhibit intense focus on specific words [3, 61, 85]. These findings highlight the crucial role of incorporating multiple modalities for effective depression detection to develop a comprehensive understanding of an individual's symptoms [17, 18, 48]."}, {"title": "2.2 Automatic Depression Detection", "content": "There have been a considerable number of proposals to detect depression by developing machine learning algorithms or natural language processing techniques [41, 47]. Initially, a substantial efforts were dedicated to extracting representative features [69, 70] and creating single-modality models for depression detection [30, 51, 77, 82, 83]. Furthermore, as Multimodal Sentiment Analysis (MSA) [49, 67] gained momentum with the recognition of various verbal and non-verbal symptoms of depression in psychological research, researchers made significant attempts to incorporate context-aware attention [10] and multimodal attention [26] to capture diverse"}, {"title": "3 Clinical Interview Dataset", "content": "To train our proposed method for the depression detection task, we use the DAIC-WOZ dataset [28], which is a subset of the widely used dataset called Distress Analysis Interview Corpus (DAIC) [74]. The DAIC-WOZ dataset comprises clinical interviews conducted to diagnose psychological distress disorders. These interviews involve Wizard-of-Oz interactions, where an Al virtual interviewer named Ellie is controlled by a human interviewer located remotely. The dataset consists of speech samples from 189 participants, including audio/visual features, raw audio files, and interview transcripts. Following the prescribed guidelines, we split the dataset into 107 training samples, 35 validation samples, and 47 test samples."}, {"title": "3.1 Data Augmentation with Random Sampling", "content": "The DAIC-WOZ dataset suffers from a significant class imbalance, with a higher proportion of non-depression samples. Some prior studies addressed this issue by employing data augmentation techniques like random masking [7, 43, 65]. Inspired by these, we tripled the size of the depression dataset by randomly masking 10 out of 85 questions in each 85 \u00d7 N question-embedded interview sequence, aligning it with the size of the non-depression dataset during training. Specifically, we first segmented the interview sequences into question-answer (Q-A) pairs based on timestamps, starting from the interviewer's question to the participant's response. Then, we randomly masked ten Q-A pairs per interview, corresponding to the interviewer's questions. Unused questions"}, {"title": "4 Hierarchical Question Embedding Network", "content": "Suppose we have a set of depression dataset $C = {c_i}_{i=1}^{C}$, where $c_i$ contains the multi-modal inputs including audio, video, and text sequences; $X_a \\in R^{L_a \\times d_a}$, $X_v \\in R^{L_v \\times d_v}$, and $X_t \\in R^{L_t \\times d_t}$, where $L_m$ represents the sequence length and $d$ indicates the feature dimension. Given the hierarchical structure of interview questions, we segment the interview sequence into question-answer pairs. Specifically, each input sequence is defined as $S = {s_i}_{i=1}^{n}$, where $s_i = (question_i, answer_i)$. We then annotate the input sequence S with corresponding hierarchical positions in the hierarchical question embedding process, denoted as $\\hat{S} = {(s_i, pos_i)}_{i=1}^{n}$, where $pos_i$ indicates the hierarchical position of the $question_i$. Finally, the proposed model predicts an individual $c_i$ depression symptom $\\hat{y} \\in {normal, depression}$."}, {"title": "4.2 Overall Architecture", "content": "The proposed method, HiQuE, as shown in Figure 2, consists of three layers: (i) Question-Aware Module, (ii) Cross-Modal Attention, and (iii) Depression Detection. HiQuE categorizes interview sequences into main and follow-up questions using a hierarchical question embedding process. Audio, visual, and text features are extracted separately, and the Question-Aware Module generates Question-Aware representations for each feature. These are combined in the Cross-Modal Attention layer to create a final multimodal representation, which the Depression Detection layer uses to predict the presence of depression."}, {"title": "4.3 Hierarchical Question Embedding Process", "content": "As depicted in Figure 3, interviewer Ellie's questions are categorized into 85 topics based on content, following Gong et al.'s approach [27]. Each question is associated with a specific topic code, such as labeling \u201cHow has seeing a therapist affected you?\u201d as therapist_affect and \u201cWhere are you from originally?\u201d as origin. These questions are further categorized into 66 primary and 19 follow-up questions based on content and order. For a complete list of the questions, please refer to Table 5. Finally, we systematically tag each question based on its hierarchical order, specifically when a follow-up question follows a primary question or when a follow-up question follows a previous follow-up question. For instance, where the question sequence is \"What did you study at school?", "Are you still working in that?\u201d, and \u201cHow hard is that?": "the hierarchical order would be primary \u2013 follow-up - follow-up.\nAn overall process of hierarchical question embedding is depicted in Figure 3. Interview sequences are represented as unimodal raw sequences $X_m$, where $m$ denotes modality \u2208 {a, v, t}, respectively. Sequences $X_m$ are partitioned into segments $S = {s_i}_{i=1}^{n}$ based on question and answer boundaries. Note that the number of segments, n, may vary for each sample due to differences in the type and number of questions employed during each interview. Then, each segment is split into a question segment and an answer segment; $S = {(q_1, a_1) ... (q_i, a_i) ... (q_{85}, a_{85})}$. After partitioning, segments are labeled with topic codes corresponding to each question and given hierarchical position embeddings based on their relationships. Specifically, as shown in Figure 3, we assign the previous question's Topic id to the follow-up question. These hierarchical positions are incorporated into the representation before feeding them into the model. This embedding process ensures uniform vector shapes by replacing unused questions with zero vectors, resulting in 85-dimensional representations for all samples."}, {"title": "4.4 Feature Extraction", "content": "For audio feature extraction, we utilize the open-Source Media Interpretation by Large feature-space Extraction (openSMILE) [22], along with the extended Geneva Minimalistic Acoustic Parameter Set [21]. These features encompass 88 functionals, including loudness, MFCCs, and other characteristics that aid in discerning emotions in speech. Consequently, each interviewee's audio features are represented as 85 \u00d7 88-dimensional vectors, where 85 denotes the question embedding dimension. These audio features are then processed using a transformer encoder."}, {"title": "4.4.2 Visual Feature", "content": "Due to privacy concerns, the dataset only offers visual features extracted via the Constrained Local Neural Fields (CLNF) algorithm [8], a widely-used approach for facial landmark localization and face recognition. To address the variation in interview duration for each answer, we first extract 68 facial landmarks from each frame (at a rate of 1 frame per second) within each segment, considering their respective x and y coordinates. We then compute mean and variance vectors within each segment and concatenate the x and y coordinates. This results in 85 \u00d7 272-dimensional vectors per participant, with zero vectors used for segments where a face is not detected."}, {"title": "4.4.3 Text Feature", "content": "For text feature extraction, we segment the interview transcripts into sections corresponding to individual answers for each question. We next leverage the pre-trained RoBERTa [42] to generate text features from each answer segment. Given the RoBERTa's strength in robustly capturing contextual information and semantic nuances in various NLP task [38], it demonstrated superior performance compared to other embedding methods and large language models (LLMs) as shown in Section 5.3. We extract features from the final layer, focusing on the [CLS] token, resulting in an 85 \u00d7 768-dimensional vector for each answer, where 85 represents the dimensionality of the question embedding."}, {"title": "4.5 Question-Aware Module Layer", "content": "In Figure 2, a transformer encoder with h multi-heads is utilized to capture attention between questionnaire responses. Initially, a stack of 1-dimensional convolutional layers is applied to process local information, converting varying shapes of HIQ Visual Rep. (85 \u00d7 275), HIQ Visual Rep. (85 \u00d7 88), and HIQ Visual Rep. (85\u00d7768) into uniform shapes of 85 \u00d7 4 denoted as $U_m$, $m \u2208 t, a, v$.\nSubsequently, the question-aware self-attention mechanism guides the transformer encoder to focus on important segments and relationships among the question-embedded sequences. Given that each representation is embedded based on 85 questions, self-attention allows the model to focus on important questions within the question-embedded representation. As a result, this particular attention mechanism enables HiQuE to extract meaningful information, represented as Q-A M Rep; M\u2208 {Audio, Visual, Text}, in the form of 85 \u00d7 85 matrices, for depression detection from each question. We analyze this unimodal attention score to identify the significant components of intra-modality. The same input is employed for self-attention as query (Q), key (K), and value (V) in the following equations:\n$MultiHead(Q, K, V) = Concatenate(head_1, ..., head_h)$ (1)\n$head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$ (2)\n$Q-A m Rep. = MultiHead(U_m, U_m, U_m) + U_m$ (3)"}, {"title": "4.6 Cross-Modal Attention Layer", "content": "In Figure 2, the multimodal transformer encoder with h multi-heads integrates information from two modalities using a cross-attention mechanism [34]. This mechanism allows the model to discern crucial relationships between $m_1$ and $m_2$ modalities, with $m_1$ serving as the source (query) and $m_2$ as the target (key and value). Furthermore, since the information in the two modalities differs, we conduct bidirectional cross-attention between $m_1$ and $m_2$ (i.e., audio-visual, visual-text, and text-audio) to allow the model to learn relevant information across modalities as follows:\n$U_{m1,m2} = MultiHead (U_{m1}, U_{m2}, U_{m2}) + U_{m1}$ (4)\n$U_{m2,m1} = MultiHead(U_{m2}, U_{m1}, U_{m1}) + U_{m2}$ (5)\nGiven that the input to the cross-modal attention layer is $U_m$, $m\u2208 t, a, v$ from the question-aware module layer, each input representation has a shape of 85 \u00d7 85. This allows us to analyze the multimodal attention score to identify significant components between different modalities."}, {"title": "4.7 Depression Detection Layer", "content": "In the last stage, the audio-visual, visual-text, and text-audio cross-modal representations are transformed into a final multimodal representation after layer normalization, concatenation, and GAP (global average pooling), as follows:\n$\\tilde{U} = \\sum GAP(U_{m1,m2} + U_{m2,m1})$ (6)\nFinally, multimodal representation is fed into HiQuE's depression detection layer to detect depression as follows:\n$Y = softmax(HiQuE(\\tilde{U}))$ (7)\nwhere the HiQuE prediction layer comprises a fully connected layer and a dropout layer. Since the depression detection task can defined as a binary classification problem, we employed the cross entropy as the loss function as follows:\n$Loss = - \\frac{1}{b} \\sum_{i=1}^{b} [y_i log(\\hat{y_i}) + (1 - y_i) log(1 - \\hat{y_i})]$ (8)\nwhere b represents the batch size, i is an index representing each example within the batch, $y_i$ is the actual label where 0 represents normal and 1 represents depression, and $\\hat{y_i}$ is the softmax function that represents the model's prediction or probability."}, {"title": "5 Experiments", "content": "We use Tensorflow [1] to implement the proposed model. The dropout rate, batch size, epochs, and learning rate were set to 0.5, 8, 100, and 0.0002, respectively. The maximum sequence length was set to 85 since all sequences are embedded into 85 questions. All weights are randomly initialized in both HiQuE and baselines."}, {"title": "5.1 Baseline Methods", "content": "To evaluate the overall performance of the proposed model, we compare its performance against five state-of-the-art multimodal models for depression detection and emotion recognition as follows:\n(i) Tensor Fusion Network (TFN) [90], (ii) bidirectional LSTM / 1D CNN-based model (BiLSTM-1DCNN) [41], (iii) Multimodal Transformer (MulT) [73], (iv) MISA [35], and (v) D-vlog [88]. Since these models were specifically designed to analyze multimodal fusion methods, we have categorized them as \"Modality-Aware\".\nWe further utilize seven context-aware multimodal models for depression detection and emotion recognition to compare the analysis of the hierarchical structure of clinical interviews: (i) bidirectional contextual LSTM (bc-LSTM) [55], (ii) Emotion Recognition [64], (iii) Sequence Modeling [2], (iv) Topic Modeling [27], (v) Context-aware deep learning (Context-Aware) [37], (vi) Speechformer [11] and (vii) GRU/BiLSTM-based [65]. As these methods consider the context of interview questions and answers or focus on the topics of questions and the timing of their appearance during the interview, we have categorized them as \"Context-Aware\".\nNote that we extract multimodal features from the entire interview sequence to train the five modality-aware methods and seven context-aware methods. All models were trained on the same data partition to ensure fairness and evaluated using the hyperparameters that showed the best performance."}, {"title": "5.2 Experimental Results", "content": "To provide a comprehensive assessment of the models' performance, particularly in the context of an imbalanced dataset (i.e., DAIC-WOZ), we report experimental results with various metrics including the weighted average and geometric mean scores (G-mean score). Table 1 shows the Macro Average precision / recall / F1-score, Weighted Average precision / recall / F1-score, and G-mean score of the baseline models and the proposed model, respectively.\nAs shown in Table 1, HiQuE achieves the best depression detection with a macro average F1-score of 0.79, a weighted average F1-score of 0.82, and a G-mean score of 0.790. As macro-average treats each class equally, while weighted-average gives weight based on class size, the result that HiQuE excels in both metrics highlights HiQuE's robustness and effectiveness against an imbalanced dataset. showcasing its ability to capture distinct depression indicators.\nAmong the baseline models, GRU/BiLSTM-based [65] achieves the highest performance with a macro average F1-score of 0.75, weighted average precision of 0.86, and G-mean score of 0.765. This underscores the effectiveness of analyzing speech characteristics"}, {"title": "5.3 Text Embedding Performance Comparison", "content": "We chose to use the pre-trained RoBERTa [42] as an encoder due to its higher performance as shown in in Table 2, in comparison with other popular embedding techniques and large language models (LLMs). The high performance of ROBERTa is due to its robust representations and comprehensive contextual understanding.\nLLMs also showed a comparable performance as shown in Table 2, but we decided not to use them due to practical challenges related to privacy and stability, particularly in mental health applications."}, {"title": "5.4 Generalization to Unseen Questions", "content": "To assess HiQuE's generalizability, we further utilized the E-DAIC-WOZ [58] and MIT Interview dataset [50]. The E-DAIC-WOZ [58] comprises audio-visual recordings of semi-clinical interviews conducted in English, featuring numerous questions absent in the DAIC-WOZ dataset. However, it does not provide the transcript of interviewer's questions, making it difficult to determine the specific questions asked. The MIT Interview dataset [50] includes 138 interview videos of internship-seeking students from MIT, featuring facial expressions, language use, and prosodic cues. Moreover, it provides ground truth labels for stress level and job interview performance, rated by nine independent judges. This dataset encompasses multimodal features influencing mental states during job interviews [50].\nWe adapt our model to these datasets by extracting text from the audio using the whisper [56] and mapping unseen questions to the predefined list (Table 5) based on the BERT-score similarity."}, {"title": "6 Analysis", "content": "To analyze the importance of each modality (i.e., audio, visual, and text) for detecting depression, we compare the performance of models that are trained with different sets of modalities. For the unimodal models (i.e., A, V, T), we first simply utilize a hierarchical question embedding process followed by a question-aware module layer for each input modality. We then add global average pooling and fully connected layers with softmax activation function to generate predicted labels (i.e., depressed or not). As shown in Figure 4, the model trained with text achieves the highest performance (0.71 of macro average F1-score) among the unimodal models. This implies that the text modality contains the most useful information in depression detection, which can be linked to the results of the prior"}, {"title": "6.2 Intermodal Interaction Analysis", "content": "By examining the attention score distributions across different modalities, as depicted in Figure 5, we highlight the significance of each modality in depression detection. Notably, questions directly related to emotions or past experiences, such as \"Tell me about an event or something that you wish you could erase from your memory?\" or \"Tell me about the last time you felt really happy?", "Have you been diagnosed with depression?\u201d or \u201cHow have you been feeling lately?": "had the highest impact on text.\nWe also explore the impact of individual modalities (i.e., audio, visual, and text) when the model fails to make accurate predictions. Depressed patients are often misclassified as normal when interviewees exhibit cheerful tones or frequent laughter (resulting in high audio and visual attention scores, Figure 5), or when there are no clear indicators of depression during the interview. Conversely, our model tends to misclassify normal as depression when negative words are frequently used, particularly when participants express recent feelings of anxiety and depression. In these cases, as highlighted in Figure 5, the text attention score predominantly influences the incorrect predictions."}, {"title": "6.3 Ablation Study", "content": "To highlight the benefits of our proposed hierarchical question embedding process, we conducted an ablation study with three distinct cases: Non-Question Embedding (N.Q.E), only Question Embedding (Q.E.), and Hierarchical Question Embedding (H.Q.E.), as shown in Table 4. In the case of N.Q.E, the entire interview sequence is treated as a single sequence for the depression detection model. Specifically, in this case, the input sequences are cropped from the beginning of the utterance to the end of the interview conversation. When only Q.E is applied, the interview is segmented into question-answer pairs. Notably, this procedure only divides the sequence into questions and aligns them with the respective question topics without incorporating hierarchical positional embedding. The improvement presented in Table 4 highlights that, for depression detection with clinical interviews, extracting useful information based on a question-driven approach is more effective than considering the entire interview sequence as a single sequence. In the last scenario H.Q.E, hierarchical position embedding is introduced following the question embedding procedure. To elaborate, after dividing the interview into question-answer pairs using question embedding, a hierarchical relationship (primary or follow-up) among the questions is tagged through hierarchical position embedding. As shown in Table 4, our proposed hierarchical question embedding process effectively forces the model to capture hierarchical relationships and the importance of the questions."}, {"title": "6.3.2 Model Components", "content": "To assess the effectiveness of each layer in the HiQuE, we conducted an ablation study on the Question-Aware Module Layer and the Cross-Modal Attention Layer. As shown in Table 4, without \"Q-A Module Layer\", each audio, visual, and text representation undergoes hierarchical question embedding and feature extraction processes before entering the Cross-Modal Attention Layer, which incorporates bidirectional cross-attention (e.g., $U_{a,v}$ - $U_{v,a}$) as illustrated in Figure 2. Given that the Question-Aware Module Layer assesses the relevance, importance, and mutual influence of the 85 embedded questions for each modality, its absence results in a performance degradation of the model. Without \"C-M Attention Layer\", the HIQ Audio Rep., HIQ Visual Rep., and HIQ Text Rep. go through the Question-Aware Module Layer, and are then concatenated before entering the Depression Detection Layer. Since the Cross-Modal Attention Layer computes relevant information from different modalities, the result highlights the effectiveness of considering information from both different modalities for accurate depression detection. Interestingly, as depicted in Table 4, the setting without \u201cQ-A Module Layer\u201d achieves a higher macro average F1-score and weighted average F1-score compared to the setting without \u201cC-M Attention Layer\u201d. This reveals that in detecting depression, it is more important to learn relevant information and interactions between modalities than to analyze the relationships and importance of each question. Furthermore, Table 4 also presents the performances of our proposed model with and without data augmentation. The results confirm that data augmentation enhances performance by balancing the sizes of depression and non-depression cases in the training set."}, {"title": "7 Case Study", "content": "In this section, we present a case study on samples from our test set to assess the effectiveness of the decision-making process of HiQuE. Specifically, we examine the verbal (i.e., text) and non-verbal (i.e., audio and video) signals for the two cases: a depressed individual (381P) and a non-depressed individual (470P). Our analysis focuses on the distinct attributes of audio, text, and visual attention scores for each individual. For a fair comparison, we apply a normalization technique to the amplitude and time of the audio waves, allowing for unbiased and consistent analysis and comparison.\nFigure 6 showcases how the model integrates text, audio, and visual features during the decision-making process for each questionnaire response. In the case of the primary question \u201cWhat\u2019s one of your most memorable experiences?", "Can you tell me about that?\", we observe the comprehensive exploration and understanding of various responses exhibited by HiQuE in detecting depression. In the case of a non-depressed individual (470P), detailed explanations, expressions of excitement, and smiling faces are evident in the answer to the follow-up question. Note that HiQuE also gives the highest attention score 0.6 to visual features. Furthermore, apart from <laughter>, the audio waves display symmetrical patterns without irregular fluctuations, indicating a more wide range of tones and amplitudes. On the other hand, the depressed individual (381P) encounters difficulties recalling memorable experiences when responding to the follow-up question. Instead of positive recollections, this individual shares memories of regrettable past incidents. By examining the audio wave of the depressed individual, we observe unstable fluctuations in amplitude while his/her facial expressions remain neutral. For this reason, HiQuE assigns the highest attention score of 0.5 to text features, followed by attention scores of 0.3-0.4 for audio features.\nOur analysis of the attention scores for each modality during the model\u2019s diagnostic process demonstrates that HiQuE effectively incorporates the interview structure through its hierarchical question embedding layer. The case study provides further evidence\"\n    },\n    {\n      \"title\"": "8 Conclusion"}, {"content": "In this paper, we presented HiQuE, a novel hierarchical question embedding model for multimodal depression detection. HiQuE efficiently captures the hierarchical structure of questions in clinical interviews and explores the correlations between different modalities to extract valuable information for depression detection. Through a comprehensive case study, we confirmed that the HiQuE focuses on questions specifically related to depression and makes its final decision by utilizing attention scores. This approach allows the model to mimic the expertise of clinical professionals during clinical interviews, where the interaction of questionnaire responses plays a crucial role. Given HiQuE's demonstrated generalizability to unseen questions, future plans involve extending its applicability to additional speech-related tasks and exploring the advantages of hierarchical question embedding further."}]}