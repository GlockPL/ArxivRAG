{"title": "Universal Inceptive GNNs by Eliminating the Smoothness-generalization Dilemma", "authors": ["Ming Gu", "Zhuonan Zheng", "Sheng Zhou", "Meihan Liu", "Jiawei Chen", "Qiaoyu Tan", "Liangcheng Li", "Jiajun Bu"], "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable success in various domains, such as transaction and social networks. However, their application is often hindered by the varying homophily levels across different orders of neighboring nodes, necessitating separate model designs for homophilic and heterophilic graphs. In this paper, we aim to develop a unified framework capable of handling neighborhoods of various orders and homophily levels. Through theoretical exploration, we identify a previously overlooked architectural aspect in multi-hop learning: the cascade dependency, which leads to a smoothness-generalization dilemma. This dilemma significantly affects the learning process, especially in the context of high-order neighborhoods and heterophilic graphs. To resolve this issue, we propose an Inceptive Graph Neural Network (IGNN), a universal message-passing framework that replaces the cascade dependency with an inceptive architecture. IGNN provides independent representations for each hop, allowing personalized generalization capabilities, and captures neighborhood-wise relationships to select appropriate receptive fields. Extensive experiments show that our IGNN outperforms 23 baseline methods, demonstrating superior performance on both homophilic and heterophilic graphs, while also scaling efficiently to large graphs.", "sections": [{"title": "1 Introduction", "content": "Graph Neural Networks (GNNs) [11, 12, 17, 31, 37] have attracted substantial attention in recent years, achieving notable success across various domains, such as transaction networks [19] and social networks [8]. Most GNN models update a node's representation by recursively aggregating information from its neighborhoods, typically within a k-hop subgraph, where k is the number of GNN layers. Depending on the graph type, GNN methods are broadly classified into two categories: homophilic GNNs (homoGNNs) [10, 12, 17, 37] and heterophilic GNNs (heteGNNs) [5, 23, 33, 50]. HomoGNNs operate under the homophily assumption, which posits that adjacent nodes tend to share similar labels (homophily). In contrast, heteGNNs are designed for heterophilic graphs, where connected nodes are more likely to have differing labels.\nIn practical graph applications, industry professionals typically rely on domain expertise to estimate the homophily ratio of graph data based on direct neighbors and then select state-of-the-art GNN methods accordingly. While straightforward, this approach can be restrictive, as the homophily ratio can vary significantly across different orders of neighboring nodes, as illustrated in Figure 1(a). For instance, in the Cora dataset, which is generally considered homophilic based on direct neighbors, homophily decreases sharply with increasing neighborhood order. Conversely, in the"}, {"title": "2 Related Works", "content": "Homophilic Graph Neural Networks. Graph Neural Networks (GNNs) have demonstrated remarkable abilities in managing graph-structured data, particularly under the assumption of homophily. Traditional GNNs can be broadly categorized into two categories. Spectral GNNs, such as the Graph Convolutional Network (GCN) [17], leverage various graph filters to derive node representations. In contrast, spatial GNNs aggregate information from neighboring nodes and combine it with the ego node to update representations, employing methods such as attention mechanisms [37] and sampling strategies [12]. Unified frameworks [25, 51] have been proposed to integrate and elucidate these diverse message-passing approaches. Recent advancements have introduced multi-hop techniques to address the limitations of traditional GNNs in capturing long-range dependencies, with examples including skip connections [12], residual connections [4], and jumping knowledge mechanisms [45]. However, these methods primarily target homophilic graphs and are less effective when dealing with heterophilic graphs.\nHeterophilic Graph Neural Networks. Addressing the challenges posed by heterophilic graphs, several innovative approaches have been proposed for graph neural networks. These include: (1) Neighborhood Extension: Techniques such as high-order neighborhood concatenation [50], new neighborhood discovery [29], neighborhood refinement [46], and global information capture [18]. (2) Neighborhood Discrimination: Methods including ordered neighborhood encoding [33], ego-neighbor separation [50], and hetero-/homo-phily neighborhood separation [28]. (3) Fine-Grained Information Utilization: Strategies such as multi-filter signal usage [23], intermediate layer combination [50], and refined gating or"}, {"title": "3 Notations and Preliminaries", "content": "3.1 Notations\nGiven an undirected attribute graph G(V, X, &, A), the node set V = {01, 02, ..., UN} comprises N nodes attributed with the feature matrix X = [X0, X1,\u2026, XN] \u2208 RN\u00d7D, while the edge set & is represented by the adjacency matrix A \u2208 RN\u00d7N. Aij = 1 if (vi, vj) \u2208 &, otherwise Aij = 0. The degree matrix is denoted as D = diag(d1, d2, \u2026\u2026\u2026, d\u0145) \u2208 RN\u00d7N, with d\u00a1 = \u2211 aij representing the degree of node vi. Therefore, the re-normalization of the adjacency matrix \u0100 = \u0189\u00af\u00bd (A+IN)\u0189\u00af\u00bd, where IN is the identity matrix. The symmetrically normalized graph Laplacian matrix is L = IN-A. Denote C = [C1, C2, , cN] \u2208 RN\u00d7C as the semantic labels of the nodes in one-hot format, and C is the number of labels. As for the relationships between semantic labels and graph structures, edge homophily [50] is computed as: he = (1/|E|) \u03a3(vi,vj)\u2208& I(ci = cj).\n3.2 Preliminaries"}, {"title": "3.2.1 Smoothness of GNNs", "content": "Oono and Suzuki [27] describe the smoothness characteristic of GNNs with information loss from X in its research on asymptotic behaviors of GNNs from a dynamical systems perspective. They take into consideration all the non-linearity, convolution filters and linear transformation layers of the most typical message passing method GCN, and demonstrate that when it extends with more layers, the representation (i.e., H(k) = (AH(k-1)W(k)), see Section 4.1 for detail) exponentially approaches information-less states, which they describe as a subspace M in Definition 3.1 that is invariant under the dynamics.\nDefinition 3.1 (subspace). Let M := {EB | B \u2208 RM\u00d7D} be an M-dimensional subspace in RN\u00d7D, where E \u2208 RN\u00d7M is orthogonal, i.e. ETE = IM, and M \u2264 N.\nFollowing the notations in [27], we denote the maximum singular value of W(1) by s1 and set s := supl\u2208N\u2081 S\u012e. Denote the distance that induced as the Frobenius norm from X to M by dM(X) := infy\u2208M ||X - Y||F = D. The following Corollary 3.2 can be interpreted as the information loss as layer I goes.\nCOROLLARY 3.2 (OONO AND SUZUKI [27]). Let 0 = \u03bb1 \u2264 \u03bb2 \u2264\u00b7\u00b7\u00b7 \u2264 \u03bb be the eigenvalues of A, sorted in ascending order. Suppose the multiplicity of the largest eigenvalue \u03bb is M(\u2264 N), i.e., \u03bb\u03bc-\u039c < \u03bb\u03c0-\u039c+1 = ... = \u03bb and the second largest eigenvalue is defined as \u03bb := max=1 maxN-M |\u03bb\u03b7| < |AN| = 1. Let E to be the eigenspace associated"}, {"title": "with AN-M+1,\u00b7\u00b7\u00b7, AN. Then we have \u03bb < \u03bb\u03bc = 1, and", "content": "dm (H(1)) \u2264 sdm (H(1-1)), (1)\nwhere M := {EB | B \u2208 RM\u00d7D}. Besides, if s1x < 1, the output of the 1-th layer of GCN on G exponentially approaches M.\nA smaller distance from the representations to the subspace M (i.e., dM(H(1))) indicates greater smoothness with larger information loss [27]. This is because the subspace M denotes the convergence state of minimal information retained from the original node features X with the only information of the connected components and node degrees of A. This means, as for any Y \u2208 M, if two nodes vi, vj \u2208 V are in the same connected component and their degrees are identical, then the corresponding column vectors of Y are identical, meaning unable to distinguish them using Y."}, {"title": "3.2.2 Generalization of GNNs", "content": "As discussed in existing works [15, 34, 38], a model's generalization capability can be governed by the Lipschitz constant of the neural network as in Definition 3.3.\nDefinition 3.3 (Lipschitz constant). A function f : Rn \u2192 Rm is called Lipschitz continuous if there exists a constant L such that\n\u2200x, y \u2208 R, ||f(x) - f(y)||2 \u2264 L||x - y||2, (2)\nwhere the smallest L for which the previous inequality is true is called the Lipschitz constant of f and will be denoted \u00ce.\nA smaller L(f) means the GNN has better generalization capability [36]. Please note that, this paper does not discuss the generalization ability on graph domain adaption [20]. Instead, we discuss the generalization ability concerning inherent structural disparity [26] and data distribution shifts from the training set to the test set [36]."}, {"title": "4 Methodology", "content": "4.1 Revisiting Message Passing\nGenerally, most graph neural networks (GNNs) capture multi-order neighborhood information by recursively stacking message-passing (MP) layers. This process is typically divided into two stages: the aggregation stage and the combination stage [44]:\nh(0) = \u03c7\u03c5\u03c2 (3)\nm(k) = AGG(k) ({h(k-1) | u\u2208 N(v)}), (4)\nh(k) = COM(k) (hk), mk)), (5)\nwhere hk) is the hidden representation and mk) is the message for node v in the k-th layer. Here, N(\u0e02) denotes the set of neighbors adjacent to node v, while AGG() and COM() represent the aggregation and combination function, respectively. Denoting H(k) = [hk),h(k),..., h(k)] \u2208 RN\u00d7F, the most widely used GCN implementation can be written as H(k) = (AH(k-1)W(k)), where \u03c3(\u00b7) is the activation function."}, {"title": "4.1.1 Smoothness-Generalization Dilemma", "content": "Obtaining information from a k-hop neighborhood typically requires k layers of MP. Despite its computational efficiency [17], it introduces an unexpected limitation. The following Theorem 4.1 reveals a dilemma between the smoothness and generalization ability. Please refer to Appendix A.1 for the proof."}, {"title": "THEOREM 4.1", "content": "Given a graph G(X, A), let the representation obtained via k rounds of GCN message passing on symmetrically normalized A be denoted as H(k) = (AH(k-1) W(k)), and the Lipschitz constant of this k-layer graph neural network be denoted as LG. Given the distance from X to the subspace M as dm(X) = D, then the distance from Hk to M satisfies:\ndm(Hk)) \u2264 LGkD, (6)\nwhere LG = || \u03a00 W(i) ||2, and a < 1 is the second largest eigenvalue of A.\nCOROLLARY 4.2. V\u0139G, \u20ac > 0, \u2203k* = [(log in)/log 1], such that\ndm(H)) < e, where [.] is the ceil of the input.\nRemark. As D is constant with respect to X, we observe that the distance is upper-bounded by three factors: the second largest eigenvalue A of A, the Lipschitz constant LG corresponding to the norm of the product of all weight matrices W(i), and the layer depth k. Based on this, several conclusions can be drawn.\nFirst, there exists a dilemma between the smoothness and generalization ability of the network, which leads to a performance drop as layer depth increases. From Section 3.2.1, we know that a smaller distance to M indicates greater smoothness with information loss. The state of extremely small distance with indistinguishable representations is referred to as oversmoothing. Since limx\u2192\u221e \u03bbk = 0, LG has to rise when k increases to prevent dM (H()) from converging into 0. This is evidenced by the upper bound of the Lipschitz constant continuing to increase as training progresses [15] However, a large \u00ceG implies reduced generalization, leading to a significant performance gap between training and test accuracy [36]. Consequently, either oversmoothing or poor generalization will occur at large k.\nSecond, oversmoothing is inevitable in high-order neighborhoods within the GCN message-passing framework. From Corollary 4.2, we see that for any given LG, there exists a k such that the distance from the representations to the subspace M is smaller than any arbitrarily small e. Thus, oversmoothing becomes unavoidable for sufficiently large k, as LG computing from the learning parameter weight matrices can not be infinitely large.\nNote that several works [4, 27, 30] have analyzed the inevitability of oversmoothing in GNNs. However, they primarily focus on oversmoothing itself and fail to connect it with generalization. Our work is the first to elucidate the dilemma between smoothness and generalization by proposing a tighter upper bound for dm(H)) that bridges these concepts."}, {"title": "4.1.2 Dilemma Hinders Universality of GNNs", "content": "This dilemma has significant negative implications for the universality of GNNs, as analyzed through both homophilic and heterophilic settings below.\nIn homophilic settings, the dilemma primarily affects high-order neighborhoods through inevitable oversmoothing, whereas low-order neighborhoods are less impacted. This can be intuitively understood by recognizing that in low-order homophilic neighborhoods, smoothing and generalization are aligned, as message passing pulls together the representations of nodes with the same label, which is"}, {"title": "4.2 Proposed Inceptive Message Passing Framework", "content": "To address the aforementioned dilemma, we propose a new message-passing architecture called Inceptive Message-Passing Graph Neural Networks (IGNN). Rather than introducing new modules into the original message-passing framework, this architecture transforms its cascade-dependent structure into an inceptive one. Our experiments in Section 5 demonstrate that IGNN, even with the simplest vanilla GCN aggregation, outperforms all baselines across both homophilic and heterophilic datasets, confirming that the dilemma is indeed the fundamental issue limiting the universality of GNNs.\n4.2.1 Inceptive GNNs (IGNN). Our proposed framework highlights three simple yet effective principles to form an inceptive GNN: (1) Separative Neighborhood Transformation; (2)Inceptive Neighborhood Aggregation; and (3) Relative Neighborhood Learning.\nSeparative Neighborhood Transformation (SN). The key is to avoid sharing or coupling transformation layers across neighborhoods:\nhok) = f(k) (x) = x0W(k), (7)\nwhere f(k) (.) represents the transformation for the k-th neighborhood. The absence of SN implies all k-hop neighborhood transformations either share the same parameters We or are cascade-coupled in a multiplicative manner, such as \u220f Wi, as shown in Table 1. This design aims to capture the unique characteristics of each neighborhood and decouple their learning processes, enabling personalized generalization with distinct Lipschitz constants for different neighborhoods. This is crucial, as various hops exhibit different levels of homophily [2].\nInceptive Neighborhood Aggregation (IN). This core design simultaneously embeds different receptive fields, such as various hops"}, {"title": "or customized relational neighborhoods:", "content": "m(k) = AGG(k) ({h(k) | u\u2208 N(k)}), (8)\nwhere AGG() represents the neighborhood aggregation function for each neighborhood, or in other words, the receptive field. The simplest approach involves partitioning the k-th order rooted tree of neighborhoods into k distinct neighborhoods N(k) = N(Ak) with N(0) = {0}. Additionally, neighborhoods identified through techniques such as structure learning can be incorporated into this framework as customized relational neighborhoods. The inceptive nature of the architecture prevents higher-order neighborhood representations from being computed based on lower-order ones, i.e., m(k) = (h(k-1)) as in Equation (4). This avoids cascading the learning of different hops, which could propagate errors across all hops if one becomes corrupted. Moreover, it prevents the product-type amplification of the Lipschitz constant, as described in 4.1, which would otherwise limit the network's generalization ability.\nNeighborhood Relationship Learning (NR). Up to this stage, all mk) are learned separately. Therefore a neighborhood-wise relationship learning module is added to learn the correlations among neighborhoods, including their commonalities or differences:\nhu = REL ({mk) | 0 \u2264 k \u2264 K}), (9)\nwhere REL() is the relationship learning function of multiple neighborhoods. The relationships among different neighborhoods represent a new characteristic in our framework compared to original framework while the ego feature is included as m(0).\nVarious relationships can be learned through diverse techniques, such as concatenation with learnable transformation [12], deep set embedding with mean/max/sum pooling [48], and attention like ordered gating mechanism [33], etc., can be applied. Based on the mechanism selected, IGNN can be divided into three variants: concatenative, residual and attentive IGNN-s. These methods are typically considered different, but we will show their consistency as variants of inceptive GNNs below.\n4.2.2 Variants of IGNNs. Here, we demonstrate that several seemingly unrelated network structure designs in GNNs surprisingly create an inceptive variant, with a brief comparison in Table 1. From the success of these various architectures in their own fields, we can indirectly realize the importance of introducing inceptive architectures to the universality of GNNs.\nResidual IGNN. Residual connection [13] is a widely used technique to train deep neural networks. Kipf and Welling [17] first leverage it in vanilla GCN as: H(k) = \u03c3(\u00c2H(k\u22121)W(k)) + H(k-1). It is easy to observe that the expansion expression of the H(k) will cover all \u00c2\u00b9, 1 < i < k, which is an inceptive variant with an IN design. Besides, some methods [4, 10] adopt an initial residual connection, constructing connections to the initial representation H(0); H(k) = \u03c3(AH(k-1)W(k)) + H(0), where H(0) = \u03c3(XW(0)). Leaving out all non-linearity for simplicity, we can derive the expression for H(k) in terms of X as: H(k) = 0 Ak-ixw(0) (\u03a0=i+1 W(j)), where k+1 W(j) = I. This formulation is also an inceptive variant of IN design. However, these two variants still involve a large"}, {"title": "number of cascade-coupled weight matrix multiplications for either low- or high-order neighborhoods, which may suffer from the aforementioned dilemma.", "content": "Attentive IGNN. Different from the above variant, attentive IGNN-s leverage the attention mechanism to realize node-wise personalized neighborhood relationship learning, defined as:\nh(k) = a(k)m(k) + a(k)h(k-1), (10)\nwhere a(k) = 9i (mk), h(k-1)), i \u2208 {1, 2}. g() is the mechanism function. Several methods such as DAGNN [21], GPRGNN [5], ACMGCN [23] and OrderedGNN [33] all incorporate this attentive architecture through different attention mechanism design to realize the IN and NR design. However, as can be seen from Table 1, these methods either share the same We across all neighborhoods or cascade couple the weight matrices of different hops, which is unable to allow distinct Lipschitz constant for their sub-networks to adapt to their different extent of smoothness.\nConcatenative IGNN. This variant is defined as a concatenation of multi-neighborhood messages with a learnable transformation:\nh = 0 ((||=(mk)))), (11)\nwhere || is the concatenation operator. Although simple, its power is surprisingly strong, as it can achieve various relationships with learnable parameters, such as mean or sum neighborhood pooling, and even more complex relationships found in existing methods, like the general layer-wise neighborhood mixing of MixHop [1], personalized PageRank (PPR) of APPNP [10], and Generalized PageRank (GPR) weights of GPRGNN [5] (see Proof A.3). It is worth noting that, the exact simple architecture has been widely adopted by many previous works targeting various problems other than heterophily, such as MixHop [1] for layer-wise neighborhood mixing, SIGN [9] for scalability improvement, and IncepGCN [30] for oversmoothing. However, surprisingly, no one has discovered their consistent effectiveness in heterophilic/universality graph learning.\n4.2.3 Putting All Together: Simplest IGNN. Taking vanilla GCN aggregation as AGG(\u00b7) and concatenation with learnable transformation as REL(\u00b7), the simplest variant of IGNN is defined as:\nh(k) = x0W(k), (12)\nmu\nm(k) = \u03c3(Auhuk)), (13)\nho,k = \u03c3 ((||=0(mk)))W), (14)\nwhere \u00c3\u00ba = I. Its matrix format is H\u2081G,k = \u03c3((||\u00af\u2030\u03c3(Aixw(i)))W), where W(i) \u2208 RDXF, and W \u2208 RkF\u00d7F'. Unless otherwise stated, all IGNN below refers to this simplest implementation."}, {"title": "4.3 How Can IGNN Benefit Universality", "content": "Here, we theoretically demonstrate the universality of IGNN."}, {"title": "4.3.1 Eliminating the Smoothness-Generalization Dilemma across Hops", "content": "As theoretically analyzed in Section 4.1.2, the inherent dilemma in original message-passing framework will hinder the universality of GNNs in different ways for homophilic and heterophilic settings. The following Theorem 4.3 shows that IGNN can release the dilemma and thus benefit both settings for improving universality.\nTHEOREM 4.3. Given a graph G(X, A), let the representation of IGNN be denoted as H\u2081G,k = \u03c3((||=06(\u00c2\u00bfXW(i)))W), and the Lipschitz constant of it be denoted as LIG. Given the distance from X to the subspace M as dm (X) = D and W = \\, then the distance from HIG.k to M satisfies:\ndM(HIG,k) \u2264 \u03a3\u03bb\u03afw(i) WiD, (15)\ni=0\nwhere \u03bb < 1 is the second largest eigenvalue of A, and LIG = || \u03a30 W(i) Will2.\nRemark. Theorem 4.3 demonstrates the effective elimination of the dilemma from two perspectives. From the global perspective, the overall Lipschitz constant of the entire network is effectively shrunk to avoid an extreme decrease in the generalization of the network. It is a cascade multiplication LG = || \u041f=0 W(i) ||2 in the message passing framework, which will grow exponentially as the layer depth increases since each high-order neighborhoods suffering from oversmoothing all demand large \u00ceG, and the cascade multiplication will lead to excessive growth in magnitude. While the Lipschitz constant is a summation of individual multiplication of various two terms in IGNN, whose increase in magnitude will be much smaller than that of cascade multiplication. From the local perspective, a more flexible personalized trade-off between smoothness and Gener-alization is enabled for each neighborhood. Since IGNN gives each neighborhood isolated transformation layers, making it possible to learn individual sub-network Lipschitz constant for their distinct magnitude of ak and homophily level [2]. High-order neighborhoods with extremely small ak demand a large Lipschitz constant to mitigate information loss, while low-order or homophilic ones with relatively large ak can enjoy sufficient small Lipschitz constant to guarantee the generalization of robustness of their representations."}, {"title": "4.3.2 Adaptive Graph Filters", "content": "Many existing works has highlighted the significance of high-frequency signals for heterophilic graph learning [3, 23]. Different from their practice of designing hand-crafted high-pass and low-pass filters, IGNN has the ability to adaptively learn arbitrary-pass graph filters. Despite the simple architecture, IGNN has expressive power beyond the polynomial graph filters [6] as in Theorem 4.4. See proof in Appendix A.2.\nTHEOREM 4.4. One layer of a K-hop concatenative IGNN, i.e., H = ((||(Aixw(i)))W)), can express a K order polynimial graph filter (0) with arbitrary coefficients \u03b8\u2081.\nRemark. Theorem 4.4 shows that IGNN can achieve a K-order polynomial graph filter, which can be viewed as a simplified case of it. Since polynomial graph filters have been proven able to approximate any graph filter [32], this suggests that IGNN can learn arbitrary graph filters without the need for manually designing low-pass or high-pass filters. Apart from the capability of polynomial"}, {"title": "5 Experiments", "content": "In this section, we aim to answer the following research questions through extensive experiments on multiple real-world datasets:\n\u2022 RQ1: How does the proposed IGNN method perform compared to the state-of-the-art methods?\n\u2022 RQ2: What are the contributions of the three principles?\n\u2022 RQ3: How is the dilemma resolved crossing different neighborhood orders?\n5.1 Datasets, Baselines and Experiment Settings\nDatasets. Following recent works [22], we select 13 representative datasets of various sizes up to millions of nodes, excluding those too small or too class-imbalanced [49], as follows: (i) Heterophily: Roman-empire, BlogCatalog, Flickr, Actor (a.k.a., Film), Squirrel-filtered, Chameleon-filtered, Amazon-ratings, Pokec; (ii) Homophily: PubMed, Photo, wikics, ogbn-arxiv, ogbn-products. The Statistics of the datasets can be found in Table 3 and 4.\nBaselines. We selected 23 representative baseline models, as shown in Table 2. These models are categorized into four main types: Graph-agnostic base models, Homophilic GNNs, Heterophilic GNNs and Graph transformers. The GNNs are further divided into two subtypes: None-inceptive GNNs, and Inceptive GNNs.\nExperimental Settings. To ensure consistency with existing methods, we randomly construct 10 splits for each dataset, with proportions of 48% for training, 32% for validation, and 20% for testing. We report the mean performance and standard deviation of classification accuracy across these 10 splits. For the large-size datasets (ogbn-arxiv, pokec and ogbn-products), we use the public splits. We set the dimensionality of node representations to 512 or their attribute dimensionality if it is smaller and use a learning rate of 0.001 with a weight decay of 0.00005. The network is optimized using the Adam optimizer [16]. The neighborhood range varies from [1,2,4,8,10,16,32,64] hops. The best hyperparameters"}, {"title": "5.2 Performance Analysis (RQ1)", "content": "From Table 3 and 4, it is evident that IGNN consistently outperforms existing baseline methods. Several key observations can be made: A subset of homophilic GNNs, which happen to be one of the inceptive variants, outperforms most recent heterophilic GNNs, highlighting the strength of inceptive architectures in addressing the dilemma hindering universality. Specifically, the average ranks (A.R) of inceptive homophilic GNNs exceed those of all non-inceptive heterophilic GNNs, and in many cases, surpass those of inceptive heterophilic GNNs. These homophilic GNNs have been largely overlooked in previous studies focused on heterophily or universality, as their original designs did not incorporate specialized mechanisms for these properties-only DAGNN and GCNII have specific features to mitigate oversmoothing. Surprisingly, the mere incorporation of an inceptive variant is sufficient to achieve superior performance, even without modifications to the message-passing process tailored for heterophily. This finding strongly suggests that the key factor limiting traditional message-passing frameworks in achieving universality or handling heterophily is the trade-off dilemma between smoothness and generalization & robustness, rather than the specifics of how information is transmitted between neighbors.\nHeterophilic GNNs with inceptive architectures demonstrate better performance compared to other heterophilic models, while graph transformers also show strong results across most baselines, regardless of whether the inceptive architecture is incorporated, indicating their ability to handle universality. On the one hand, inceptive heterophilic GNNs are all attentive variants, each employing different attention mechanisms. Interestingly, although they are all the same attentive variant, these models exhibit significant differences in performance, indicating that the design of the attention mechanism plays a critical role in the architecture's effectiveness. On the other hand, graph transformers excel likely because they move beyond the traditional message-passing paradigm, utilizing both global and local attention mechanisms to learn pairwise propagation weights. This is beneficial for two reasons: (i) they adopt a structure-learning approach that is agnostic to homophily or heterophily, and (ii) their attention modules, being independent of the message-passing process, inherently enjoy strong generalization and robustness, rendering them unaffected by the dilemma.\nIGNN outperforms all baselines with or without inceptive architectures, while the performance of inceptive GNNs also varies, suggesting that the effectiveness of these models is significantly influenced by whether all three principles are integrated and how they are implemented. In particular, concatenative variants (e.g., IGNN, SIGN, and IncepGCN) generally outperform residual and attentive variants, with the ordered gating mechanism of OrderedGNN standing out as evidence that order information is crucial for capturing neighborhood-wise relationships and learning their interactions. However, two concatenative variants-JKNet and MixHop-show lower performance due to their unique designs: original JKNet does not include ego features without propagation, and MixHop requires stacking layers on top of inceptive architectures, partially reintroducing the dilemma. Furthermore, most inceptive GNNs fail"}, {"title": "5.3 Ablation Studies of SN, IN and NR (RQ2)", "content": "Table 5 presents the ablation of the three principles, with all variants utilizing the GCN aggregation It is important to note that SN cannot be applied without NR, so the ablations do not include any combinations of SN without NR. Several key conclusions can be drawn: First, applying All Principles yields the Best Performance. The best performance is achieved when all principles are"}, {"title": "applied, as IGNN obtains the highest average rank (Rank 1) (line 6 vs. others). Second, JKNet-GCN shows a significant performance gap depending on IN (line 3 vs. line 5), where the difference lies in whether ego feature transformation is included", "content": "This indicates that incorporating ego representation into the final representation enhances generalization, as the absence of propagation in this step helps avoid the dilemma. Third, SN and NR demonstrate excellent synergy, yielding significantly improved results when used together. Although IN is incorporated in lines 4\u20136, adding either SN or NR alone (lines 4, 5) does not lead to the best improvement compared to incorporating both, as seen in IGNN (line 6).\n5.4 Performance of Different Neighborhood Orders (RQ3)\nFigure 3 illustrates the performance of various methods across different neighborhood orders in both homophilic and heterophilic graphs. In the homophilic context (photo), most methods addressing the oversmoothing issue, such as GCNII and GPRGNN, effectively mitigate the problem, with several inceptive methods, including IGNN and OrderedGNN, demonstrating competitive performance. Conversely, in the heterophilic scenario (squirrel), most methods consistently struggle with high-order neighborhoods, as evidenced by a trend of initial improvement followed by a decline in performance. In contrast, IGNN exhibits a notable increase in performance that stabilizes thereafter, highlighting the effectiveness of the inceptive design in breaking cross-hop dependencies. This design enables each hop to independently assess its own generalization capability, thereby enhancing overall performance in varying structures."}, {"title": "6 Conclusion", "content": "In this paper, we propose a universal message-passing framework called Inceptive Graph Neural Network (IGNN). Through theoretical analysis, we reveal the limitations imposed by cascade dependency in multi-hop learning, which leads to a smoothness-generalization dilemma that hinders learning in both homophilic and heterophilic contexts. Our framework eliminates this dilemma by leveraging three key principles: separative neighborhood transformation, inceptive neighborhood aggregation, and neighborhood relationship learning. These principles enable the model to learn independent representations across multiple receptive hops. This approach enhances the ability of GNNs to adapt to varying levels of homophily across multiple hops, with personalized generalization capabilities for each hop, making IGNN a versatile solution."}, {"title": "THEOREM 4.1", "content": "Given a graph G(X, A), let the representation obtained via k rounds of GCN message passing on symmetrically normalized A be denoted as H(k) = \u03c3(AH(k-1) W(k)), and the Lipschitz constant of this k-layer graph neural network be denoted as LG. Given the distance from X to the subspace M as dm(X) = D, then the distance from Hk to M satisfies:\ndm(Hk)) \u2264 LG\u03bbkD, (6)\nwhere LG = || \u03a0ki=1 W(i) ||2, and \u03bb < 1 is the second largest eigenvalue of A."}, {"title": "COROLLARY 4.2.", "content": "V\u0139G, \u20ac > 0, \u2203k* = [(log ())/log \u03bb], such that\ndm(H())) < e, where [.] is the ceil of the input."}, {"title": "THEOREM 4.3.", "content": "Given a graph G(X, A), let the representation of\nIGNN be denoted as HIG,k = \u03c3((||Ki=0 \u03c3(\u00c2iXW(i)))W), and the Lipschitz constant of it be denoted as LIG. Given the distance from X to\nthe subspace M as dm (X) = D and W = \\, then the distance\nfrom HIG.k to M satisfies:\ndM(HIG,k) \u2264 \u03a3i=0 \u03bbi ||W(i) Wi||2D, (15)\nwhere \u03bb < 1 is the second largest eigenvalue of A, and LIG =\n|| \u03a3i=0 W(i) Wi||2."}, {"title": "Given U invariant under A, U is also invariant under \u00c2\u00b9. Similar to the derivation of Equation (18), we have", "content": "d\u00b2M (HIG,k) = d\u00b2M (o (||Ki=06(\u00c2\u00bfXW(i))))) = d\u00b2M (o ((Ki=0(AiXW(i)))Wi\n< d\u00b2M\u03a3Ki=0 (AiXW(i))Wi ||o=0(\u00c2\u00bfXW(i)))Wi) (23)\n= doM\u03a3Ki=0 (AiXW(i)))Wi (17)\n< \u03a3Ki=0 (\u03bbiXW(i))WiW (17)\n\u2264 \u03a3i=0 \u03bbi ||W(i) Wi||2dM(X) (24)\nwhere WM(i) is the M dimensional projection of W(i)."}, {"title": "Recall the Theorem 3.1 in Khromov and Singh [15] as following", "content": "Theorem A.3. Similar"}]}