{"title": "Improving Deep Reinforcement Learning by Reducing the Chain Effect of Value and Policy Churn", "authors": ["Hongyao Tang", "Glen Berseth"], "abstract": "Deep neural networks provide Reinforcement Learning (RL) powerful function approximators to address large-scale decision-making problems. However, these approximators introduce challenges due to the non-stationary nature of RL training. One source of the challenges in RL is that output predictions can churn, leading to uncontrolled changes after each batch update for states not included in the batch. Although such a churn phenomenon exists in each step of network training, how churn occurs and impacts RL remains under-explored. In this work, we start by characterizing churn in a view of Generalized Policy Iteration with function approximation, and we discover a chain effect of churn that leads to a cycle where the churns in value estimation and policy improvement compound and bias the learning dynamics throughout the iteration. Further, we concretize the study and focus on the learning issues caused by the chain effect in different settings, including greedy action deviation in value-based methods, trust region violation in proximal policy optimization, and dual bias of policy value in actor-critic methods. We then propose a method to reduce the chain effect across different settings, called Churn Approximated ReductIoN (CHAIN), which can be easily plugged into most existing DRL algorithms. Our experiments demonstrate the effectiveness of our method in both reducing churn and improving learning performance across online and offline, value-based and policy-based RL settings, as well as a scaling setting.", "sections": [{"title": "Introduction", "content": "One fundamental recipe for the success of Deep Reinforcement Learning (DRL) is powerful approximation and generalization provided by deep neural networks, which augments the ability of RL with tabular or linear approximation to large state spaces. However, on the other side of this benefit is less control over the function dynamics. Network outputs can change indirectly to unexpected values after any random batch update for input data not included in the batch, called churn in this paper. This change is particularly problematic for an RL agent due to its non-stationary nature, which can exacerbate instability, suboptimality and even collapse. Therefore, it is of significance to understand and control these undesired dynamics to address learning issues and improve performance.\nConsistent efforts have been devoted by RL community to gain better understandings of the learning dynamics from different perspectives [Achiam et al., 2019, Kumar et al., 2022, Liu et al., 2023, Lyle et al., 2022b]. Recently, Schaul et al. [2022] studied a novel churn phenomenon in the learning process of typical value-based RL algorithms like DoubleDQN [van Hasselt et al., 2016]. The phenomenon reveals that the greedy actions of about 10% of states in the replay buffer change after a single regular batch update. Such a dramatic churn can persist throughout the learning process of DoubleDQN, causing instabilities."}, {"title": "Prior Work", "content": "In the past decade, a significant effort has been made to understand the learning issues of DRL agents and propose improvements that make DRL more stable and effective. The early stage of this effort studied bias control for value approximation with deep neural networks introducing many improvements after DQN [Mnih et al., 2015] and DDPG [Lillicrap et al., 2015] to address overestimation or underestimation for value-based methods [van Hasselt et al., 2016, Bellemare et al., 2017, Hessel et al., 2018] and deep AC methods [Fujimoto et al., 2018, Haarnoja et al., 2018, Lan et al., 2020, Kuznetsov et al., 2020, Chen et al., 2021] respectively. Additional works dig deeper to diagnose the learning issues regarding instability and generalization, related to the Deadly Triad in DRL [van Hasselt et al., 2018, Achiam et al., 2019], stabilizing effect of target network [Zhang et al., 2021b, Chen et al., 2022, Pich\u00e9 et al., 2022], difficulty of experience replay [Schaul et al., 2016, Kumar et al., 2020, Ostrovski et al., 2021], over-generalization [Ghiassian et al., 2020, Pan et al., 2021, Yang et al., 2022], representations in DRL [Zhang et al., 2021a, Li et al., 2022, Tang et al., 2022], delusional bias [Lu et al., 2018, Su et al., 2020], off-policy correction [Nachum et al., 2019, Zhang et al., 2020, Lee et al., 2021], interference [Cobbe et al., 2021, Raileanu and Fergus, 2021, Bengio et al., 2020] and architecture choices [Ota et al., 2020].\nAmong these directions, one notable thread is to understand the learning dynamics of DRL agents with a focus on the non-stationary nature of RL. A prominent phenomenon of representation ability loss is studied in [Dabney et al., 2021, Igl et al., 2021, Kumar et al., 2021, 2022, Ma et al., 2023], which reveals how representations become less useful in later stages of learning, leading to myopic convergence. Further, empirical studies in [Nikishin et al., 2022, D'Oro et al., 2023, Sokar et al., 2023, Nauman et al., 2024] demonstrate that the loss of approximation ability becomes severe and leads to collapse when high replay-ratios are adopted for better sample efficiency, while network resets and normalization methods can be simple and effective remedies. This is further identified as"}, {"title": "Preliminaries", "content": "Reinforcement Learning (RL) is formulated within the framework of a Markov Decision Process (MDP) $(S, A, P, R, \\gamma, \\rho_0, T)$, defined with the state set $S$, the action set $A$, the transition function $P : S \\times A \\rightarrow \\mathcal{P}(S)$, the reward function $R : S \\times A \\rightarrow \\mathbb{R}$, the discounted factor $\\gamma\\in [0, 1)$, the initial state distribution $\\rho_0$ and the horizon $T$. The agent interacts with the MDP by performing actions from its policy $a_t \\sim \\pi(s_t)$ that defines the mapping from states to actions or action distributions. The objective of an RL agent is to optimize its policy to maximize the expected discounted cumulative reward $J(\\pi) = \\mathbb{E}_\\pi[\\sum_{t=0}^T \\gamma^t r_t]$, where $s_0 \\sim \\rho_0 (s_0), s_{t+1} \\sim P (s_{t+1} | s_t, a_t)$ and $r_t = R(s_t, a_t)$. The state-action value function $q^{\\pi}$ defines the expected cumulative discounted reward for all $s, a \\in S \\times A$ and the policy $\\pi$, i.e., $q^{\\pi} (s, a) = \\mathbb{E}_\\pi[\\sum_{t=0}^T \\gamma^t r_t | S_0 = s, a_0 = a]$.\nPolicy and value functions are approximated with deep neural networks to cope with large and continuous state-action space. Conventionally, $q^{\\pi}$ can be approximated by $Q_\\theta$ with parameters $\\theta$ typically through minimizing Temporal Difference (TD) loss [Sutton and Barto, 1988], i.e., $\\mathcal{L}(\\theta) = \\mathbb{E}_{s,a\\sim D} d_\\theta(s, a)^2$ where $D$ is a replay buffer and $d_\\theta(s, a)$ is a type of TD error. A parameterized policy $\\pi$ with parameters $\\phi$ can be updated by taking the gradient of the objective, i.e., $\\phi \\leftarrow \\phi+\\alpha\\nabla_\\phi J(\\pi_\\phi)$ with a step size $\\alpha$. Value-based methods like Deep Q-Network (DQN) [Mnih et al., 2015] trains a Q-network $Q_\\theta$ by minimizing $\\mathcal{L}(\\theta)$ where $d_\\theta(s, a) = Q_\\theta(s, a) - (r + \\gamma \\max_{a'} Q_{\\theta^-} (s', a'))$ and $\\theta^-$ denotes the target network. For policy-based methods, TD3 [Fujimoto et al., 2018] is often used to update a deterministic policy with Deterministic Policy Gradient (DPG) theorem [Silver et al., 2014]: $\\nabla_\\phi J(\\pi_\\phi) = \\mathbb{E}_{s\\sim D} [\\nabla_\\phi \\pi_\\phi(s) \\nabla_a Q_\\theta(s,a)|_{a=\\pi_\\phi(s)}]$; Soft Actor-Critic (SAC) [Haarnoja et al., 2018] learns a stochastic policy with the gradient: $\\nabla_\\phi J(\\pi_\\phi) = \\mathbb{E}_{s\\sim D}[\\nabla_\\phi log\\pi_\\phi(a|s)+(V\\alpha log \\pi_\\phi(a|s)-\\nabla_a Q_\\theta(s,a))\\nabla_\\phi f_\\phi(\\epsilon; s))|_{a=f_\\phi(\\epsilon;s)}]$, with noise $\\epsilon$ and implicit function $f_\\phi$ for re-parameterization."}, {"title": "A Chain Effect of Value and Policy Churn", "content": "In this section, we present a formal study on value and policy churn and their impact on learning. We first introduce an intuitive overview of how churn is involved in DRL (Section 4.1). Then, we propose the definitions of the value and policy churn (Section 4.2), followed by a chain effect that reveals how the churns interplay and bias parameter update (Section 4.3)."}, {"title": "Generalized Policy Iteration under Churn", "content": "Generalized Policy Iteration (GPI) [Sutton and Barto, 1988] is widely used to refer to the general principle of learning in an Evaluation-Improvement iteration manner, which applies to almost all RL methods. In the context of DRL, i.e., with network representation and mini-batch training, the value and policy networks' outputs can have unexpected changes, i.e., the churn, after each mini-batch training for the states not included in the batch. Such churns are neglected in most DRL methods, let alone their influence on the practical learning process. In Figure 1, we extend the classic GPI diagram by taking churn into consideration, to show how it is involved in the learning process intuitively.\nIn the evaluation process, the parameterized Q-network $Q_\\theta$ approximates the value of the current policy via repeated mini-batch training. Under the impact of churn, the Q-network is not likely to"}, {"title": "Definition of Value and Policy Churn", "content": "A deep neural network can have the form $f_\\theta : \\mathcal{X} \\rightarrow \\mathcal{Y}$ with parameters $\\theta$. The network is optimized for a set of input data $B_{train} = \\{x_i\\}$ with a loss function, leading to a parameter update of $\\theta \\rightarrow \\theta'$. Given a reference set of input data $B_{ref} = \\{\\bar{x}_i\\}$ (where $B_{ref} \\cap B_{train} = \\emptyset$) and a metric $d$ for the output space, the churn is formally defined as:\n$C_f(\\theta,\\theta', B_{ref}) = \\frac{1}{|B_{ref}|} \\sum_{\\bar{x}_i \\in B_{ref}} d(f_\\theta(\\bar{x}_i), f_{\\theta'}(\\bar{x}_i))$.\nArguably, churn is an innate property of neural networks and it is closely related to problems like interference [Liu et al., 2020, 2023] and catastrophic forgetting [Lan et al., 2023] in different contexts.\nIn this paper, we focus on the churn in Q-value network $Q_\\theta$ and policy network $\\pi_\\phi$. We then obtain the definitions of the Q-value churn ($C_Q$, w.r.t. $\\theta \\rightarrow \\theta'$) and the policy churn ($C_\\pi$, w.r.t. $\\phi \\rightarrow \\phi'$ , using a deterministic policy for demonstration) for an arbitrary state-action pair $\\bar{s}, \\bar{a} \\in B_{ref}$ as follows:\ncq(\\theta, \\theta', \\bar{s},\\bar{a}) = Q_{\\theta'}(\\bar{s}, \\bar{a}) - Q_\\theta(\\bar{s}, \\bar{a}), c_{\\pi}(\\phi, \\phi', \\bar{s}) = \\pi_{\\phi'}(\\bar{s}) - \\pi_\\phi(\\bar{s}).\nThen, the definitions regarding $B_{ref}$ can be generalized to the batch setting by aggregating data in $B_{ref}$: $C_Q(\\theta,\\theta', B_{ref}) = \\frac{1}{|B_{ref}|} \\sum_{\\bar{s},\\bar{a} \\in B_{ref}} c_Q(\\theta,\\theta', \\bar{s},\\bar{a})], C_\\pi(\\phi,\\phi', B_{ref}) = \\frac{1}{|B_{ref}|} \\sum_{\\bar{s} \\in B_{ref}} c_\\pi(\\phi,\\phi', \\bar{s})]$. Without loss of generality, we carry out our analysis mainly regarding $\\bar{s}, \\bar{a}$ for clarity in the following."}, {"title": "How the churns CQ, C\u03c0 are caused by parameter updates", "content": "First, we look into the relationship between the Q-value churn $C_Q$, the policy churn $C_\\pi$ and the network parameter updates $\\Delta_\\theta = \\theta' - \\theta, \\Delta_\\phi = \\phi' - \\phi$. For $\\Delta_\\theta, \\Delta_\\phi$, we use typical TD learning and DPG for demonstration: $\\Delta_{\\theta} = \\frac{\\alpha}{|B_{train}|} \\sum_{s,a\\in B_{train}} \\nabla_\\theta Q_\\theta(s, a)d_\\theta(s, a)$, and $\\Delta_{\\phi} = \\frac{\\alpha}{|B_{train}|} \\sum_{s,a\\in B_{train}} \\phi\\pi_\\phi(s)\\nabla_a Q_\\theta(s,a)|_{a=\\pi_\\phi(s)}.\\newline$\nNow we characterize $C_Q$ and $C_\\pi$ as functions of $\\Delta_\\theta, \\Delta_\\phi$ with the help of Neural Tangent Kernel (NTK) expression [Achiam et al., 2019]. For clarity, we use $B_{train} = \\{s, a\\}$ and $B_{ref} = \\{\\bar{s}, \\bar{a}\\}$ and abbreviate $|B_{train}|, |B_{ref}|$ and step size $\\alpha$ when context is clear. Concretely,\n$C_Q(\\theta,\\theta') = \\nabla_\\theta Q_\\theta(\\bar{s},\\bar{a}) \\Delta_\\theta + O(||\\Delta_\\theta||^2) \\approx \\nabla_\\theta Q_\\theta(\\bar{s},\\bar{a})^T\\nabla_\\theta Q_\\theta(s, a) d_\\theta(s, a)$\n$C_\\pi(\\phi,\\phi') = \\nabla_\\phi \\pi_\\phi(\\bar{s}) \\Delta_\\phi + O(||\\Delta_\\phi||^2) \\approx \\nabla_\\phi \\pi_\\phi(\\bar{s})^T\\nabla_\\phi \\pi_\\phi(s) \\nabla_a Q_\\theta(s, a)|_{a=\\pi_\\phi(s)}$\nEq. 2 shows that the value and policy churn is mainly determined by the kernels of the Q-network $k_\\theta$ and the policy network $k_\\phi$, along with the TD error and the action gradient. This indicates that churn is determined by both the network's property itself and the learning we performed with the network."}, {"title": "From Single-step Interplay to The Chain Effect of Churn", "content": "In addition to the first piece of understanding (U.I) that presents how parameter updates cause the churns, we discuss how the churns affect parameter updates backward with two more pieces of understanding (U.II) and (U.III), finally shedding light on a chain effect of churn."}, {"title": "How CQ, C\u03c0 deviates action gradient and policy value", "content": "First, we introduce two types of deviation derived from the value and policy churn: (1) Action Gradient Deviation ($\\mathcal{D}^Q_\\theta$), the change of action gradient regarding the Q-network for states and actions that are affected by the Q-value churn $C_Q$; (2) Policy Value Deviation ($\\mathcal{D}^\\pi_\\phi$), the change of Q-value due to the action change for states that are affected by policy churn $C_\\pi$. Formally, the two types of deviation are:\n$\\mathcal{d}^Q_\\theta(\\theta,\\theta', \\bar{s}) = \\nabla_a Q_{\\theta'}(\\bar{s}, \\bar{a})|_{a=\\pi(\\bar{s})} - \\nabla_a Q_\\theta(\\bar{s}, \\bar{a})|_{a=\\pi(\\bar{s})}.$\n$\\mathcal{d}^\\pi_\\phi(\\phi,\\phi', \\bar{s}) = Q(\\bar{s}, \\pi_{\\phi'}(\\bar{s})) - Q(\\bar{s}, \\pi_\\phi(\\bar{s})).$\nOne thing to note is the two types of deviation show the interplay between the value and policy churn, as the value churn derives the deviation in policy ($C_Q \\overset{derive}{\\longrightarrow} \\mathcal{D}^\\pi_\\phi$) and the policy churn derives the deviation in value ($C_\\pi \\overset{derive}{\\longrightarrow} \\mathcal{D}^Q_\\theta$), as denoted by the superscripts. This interplay between the policy and value can be shown better with the expressions below (derivation details in Appendix A):\n$\\mathcal{d}^Q_\\theta(\\theta,\\theta') = \\nabla_a c_Q(\\theta,\\theta')|_{a=\\pi(\\bar{s})}, \\mathcal{d}^\\pi_\\phi(\\phi,\\phi') \\approx (\\nabla_a Q_\\theta(\\bar{s},\\bar{a})|_{a=\\pi_\\phi(\\bar{s})})^T c_\\pi(\\phi,\\phi').$\nSince the action gradient and policy value play key roles in parameter updates, the deviations in them naturally incur negative impacts on learning."}, {"title": "How parameter updates are biased by CQ, C\u03c0 and the deviations DQ\u03b8 D\u03c0\u03d5", "content": "Let us consider a segment of two consecutive updates, denoted by $(\\theta^-, \\phi^-) \\rightarrow (\\theta, \\phi) \\rightarrow (\\theta', \\phi')$. The churns occurred during the last update $(\\theta^-, \\phi^-) \\rightarrow (\\theta, \\phi)$ participate in the current update $(\\theta, \\phi) \\rightarrow (\\theta', \\phi')$ about to perform. Concretely, the churns affect the following aspects: (1) Q-value estimate, (2) action selection in both TD error and policy objective, and (3) the gradient of network parameters.\nFrom these aspects, we can deduce the difference between the parameter updates under the impact of the value and policy churn (denoted by $\\tilde{\\Delta}_\\theta, \\tilde{\\Delta}_\\phi$) and the conventional ones $\\Delta_\\theta, \\Delta_\\phi$. As a result, we can find that the value and policy churn, as well as the deviations derived, introduce biases in the parameter updates. We provide the complete discussion and derivation in Appendix A.2.\nThe analysis on the update segment $(\\theta^-, \\phi^-) \\rightarrow (\\theta, \\phi) \\rightarrow (\\theta', \\phi')$ can be forwarded, and taking the three pieces of understanding together, we arrive at the chain effect of churn."}, {"title": "The Chain Effect of Churn", "content": "(U.I) Parameter updates cause the value and policy churn, (U.II) which further leads to the deviations in the action gradient and policy value; (U.III) the churns and the deviations then bias following parameter updates.\nAs the cycle illustrated in Figure 2, the value and policy churn and the parameter update bias accumulate and can amplify each other throughout the learning process. Intuitively, the parameter update chain could derail and fluctuate under the accumulating churns and biases, thus preventing stable and effective learning. We concretize our study on the consequences in the next section."}, {"title": "Reducing Value and Policy Churn in Deep RL", "content": "In this section, we show concrete learning issues caused by churn in typical DRL scenarios (Section 5.1), followed by a simple plug-in method to reduce churn and address the issues (Section 5.2)."}, {"title": "Consequences of the Chain Effect of Churn in Different DRL Scenarios", "content": "Since churn is involved in most DRL methods as illustrated by Figure 1, we concretize our study and focus on several typical DRL scenarios below."}, {"title": "Greedy action deviation in value-based methods", "content": "Value-based methods like DQN train a Q-network $Q_\\theta$ and compute the policy by choosing the greedy action of $Q_\\theta$. A consequence of computing the action greedily is that changes in the values will directly cause changes in the action distribution [Schaul et al., 2022]. Similarly to Eq. 4, this deviation can be formalized as:\n$\\mathcal{D}^a_\\theta(\\theta,\\theta', B_{ref}) = \\frac{1}{|B_{ref}|} \\sum_{\\bar{s} \\in B_{ref}} \\mathbb{I}\\{\\arg\\max_a Q_{\\theta'}(\\bar{s},a)\\} \\neq \\mathbb{I}\\{\\arg\\max_{\\bar{a}} Q_\\theta(\\bar{s},\\bar{a})\\}$. We suspect that this deviation introduces instability and hinders learning, and we focus on whether reducing churn can improve the performance of value-based methods."}, {"title": "Trust region violation in policy gradient methods", "content": "Trust region plays a critical role in many policy gradient methods for reliable and efficient policy updates. Proximal Policy Optimization (PPO) [Schulman et al., 2017] uses a clipping mechanism as a simple but effective surrogate of the trust region for TRPO [Schulman et al., 2015]: $Clip(r(\\phi_{old}, \\phi), 1 - \\epsilon, 1 + \\epsilon)$ and $r(\\phi_{old}, \\phi) = \\frac{\\pi_{\\phi}(a|s)}{\\pi_{\\phi_{old}}(a|s)}$.\nWith respect to policy churn, even though the PPO policy conforms to the trust region for the states in the current training batch, it could silently violate the trust region for other states, including previously updated ones. Consider the policy update $\\phi \\rightarrow \\phi'$, it is highly likely to have $r(\\phi, \\phi') = \\frac{\\pi_{\\phi'}(a|s)}{\\pi_{\\phi}(a|s)} \\neq 1$ and thus $r(\\phi_{old}, \\phi') = \\frac{\\pi_{\\phi'}(a|s)}{\\pi_{\\phi_{old}}(a|s)} = r(\\phi_{old}, \\phi)r(\\phi,\\phi') \\neq r(\\phi_{old}, \\phi)$. Since we have no information about $r(\\phi, \\phi')$, there is no guarantee for the trust region $1 - \\epsilon \\leq r(\\phi_{old}, \\phi') \\leq 1 + \\epsilon$ to be respected after churn. Intuitively, this silent violation is hazardous and detrimental to learning."}, {"title": "Dual bias of policy value in Actor-Critic methods", "content": "Deep AC methods interleave the training between the actor-network and the critic-network. Unlike the two scenarios above, where either the value churn or the policy churn raises a learning stability issue, we present the dual bias of policy value that stems from the bilateral effect of churn. The dual bias exists in the policy value as $Q_{\\theta'}(\\bar{s}, \\pi_{\\phi'}(\\bar{s})) \\neq Q_\\theta(\\bar{s}, \\pi_\\phi(\\bar{s}))$. In the context of AC methods, the policy value is used for the target computation of the critic $r_t + \\gamma Q_{\\theta'} (s_{t+1}, \\pi_{\\phi'} (s_{t+1}))$ and the optimization objective of the actor $\\nabla Q_{\\theta'} (s, \\pi_{\\phi'}(s))$. Thus, the dual bias steers the training of the actor and the critic.\nGiven these negative consequences of churn, a question is raised naturally: how can we control the level of churn to mitigate the issues without introducing complex trust regions or constraints?"}, {"title": "A Regularization Method for Churn Reduction", "content": "In this section, we propose a regularization method to reduce value and policy churn, called Churn Approximated ReductIoN (CHAIN). To combat the prevalence of churn's negative influence on DRL, our method should be simple to implement and easy to use with different RL methods.\nBased on the definitions of the value churn ($C_Q$) and the policy churn ($C_\\pi$) in Section 4.2, we propose two corresponding loss functions $\\mathcal{L}_{QC}$ and $\\mathcal{L}_{PC}$ for churn reduction. Formally, for parameterized networks $Q_{\\theta_t}, \\pi_{\\phi_t}$ at time $t$ and a reference batch $B_{ref}$ sampled from replay buffer, we have:\n$\\mathcal{L}_{QC} (\\theta_t, B_{ref}) = \\frac{1}{|B_{ref}|} \\sum_{\\bar{s},\\bar{a} \\in B_{ref}} (Q_{\\theta_t} (\\bar{s}, \\bar{a}) - Q_{\\theta_{t-1}} (\\bar{s},\\bar{a}))^2$\n$\\mathcal{L}_{PC}(\\phi_t, B_{ref}) = \\frac{1}{|B_{ref}|} \\sum_{\\bar{s} \\in B_{ref}} d (\\pi_{\\phi_t} (\\bar{s}), \\pi_{\\phi_{t-1}}(\\bar{s}))$\nwhere $d$ is a policy distance metric, and we use mean square error or KL divergence for deterministic or stochastic policies. Ideally, the regularization should be imposed on the post-update network parameters $\\theta_{t+1}, \\phi_{t+1}$. Since they are not available at time $t$, we regularize $\\theta_t, \\phi_t$ and use $\\theta_{t-1}, \\phi_{t-1}$ as the targets for a convenient and effective surrogate.\nBy minimizing $\\mathcal{L}_{QC}$ and $\\mathcal{L}_{PC}$, we can reduce the value and policy churn and suppress the chain effect further. This allows us to use the churn reduction regularization terms along with standard RL objectives, and arrives at DRL with CHAIN finally:\n$\\underset{\\theta_t}{\\text{minimize }} \\mathcal{L}(\\theta_t, B_{train}) + \\lambda_Q \\mathcal{L}_{QC}(\\theta_t, B_{ref})$\n$\\underset{\\phi_t}{\\text{maximize }} J(\\phi_t, B_{train}) - \\lambda_\\pi \\mathcal{L}_{PC}(\\phi_t, B_{ref})$\nwhere $B_{train}, B_{ref}$ are two separate batches randomly sampled from $D$, and $\\lambda_Q, \\lambda_\\pi$ are coefficients that control the degree of regularization. CHAIN serves as a plug-in component that can be implemented with only a few lines of code modification in most DRL methods. The pseudocodes are omitted here, and we refer the readers to Algorithm 1 in the Appendix."}, {"title": "Automatic adjustment of \u03bb\u03bf, \u03bb\u03c0", "content": "To alleviate the difficulty of manually selecting the regularization coefficients, we add a simple but effective method to adjust $\\lambda_Q, \\lambda_\\pi$ adaptively during the learning process. The key principle behind this is to keep a consistent relative scale (denoted by $\\beta$) between the churn reduction regularization terms and the original DRL objectives. Concrete, by maintaining the running means of the absolute Q loss $|\\mathcal{L}_Q|$ and the VCR term $|\\mathcal{L}_{QC}|$, $\\lambda_Q$ is computed dynamically as $\\lambda_Q = \\beta \\frac{|\\mathcal{L}_Q|}{|\\mathcal{L}_{QC}|}$, which is similar for $\\lambda_\\pi$. This is inspired by our empirical observations and the recent study on addressing the scale difference across different domains [Hafner et al., 2023].\nAnother thing worth noting is that CHAIN helps to mitigate the loss of plasticity via churn reduction. This connection can be established by referring to the NTK expressions in Eq. 2: reducing churn encourages $k_\\theta, k_\\phi$ to 0 and thus prevents the empirical NTK matrix from being low-rank, which is shown to be a consistent indicator of plasticity loss [Lyle et al., 2024]."}, {"title": "Experiments", "content": "In the experiments, we aim to answer the following questions: (1) How large is the value and policy churn in practice, and can our method effectively reduce churn? (2) Does our method's reduction of churn address learning issues and improve performance in terms of efficiency and episode return? (3) Does CHAIN also improve the scaling abilities of deep RL?\nWe organize our experiments into the four subsections below that correspond to the three DRL scenarios discussed in Section 5.1 as well as a DRL scaling setting. Our experiments include 20 online RL tasks from MinAtar, MuJoCo, DMC, and 8 offline RL datasets from D4RL, as well as 6 popular algorithms, i.e., DoubleDQN, PPO, TD3, SAC, IQL, AWAC. We provide the experimental details in Appendix B and more results in Appendix C."}, {"title": "Results for CHAIN DoubleDQN in MinAtar", "content": "We use DoubleDQN (DDQN) [van Hasselt et al., 2016] as the value-based method and MinAtar [Young and Tian, 2019] as the experiment environments. MinAtar is an Atari-inspired testbed for convenient evaluation and reproduction. We build our DoubleDQN based on the official MinAtar code with no change to the network structure and hyperparameters. We implement CHAIN DDQN by adding a few lines of code to apply the value churn reduction regularization in the standard training of the Q-network (Eq. 7). For CHAIN DDQN, $\\lambda_Q$ is set to 50 for Breakout and 100 for the other tasks. For CHAIN DDQN with automatic adjustment of $\\lambda_Q$ (denoted by the suffix \u2018Auto\u2019), the target relative loss scale $\\beta$ is set to 0.05 for all the tasks.\nFirst, to answer Question (1), we report the value churn and the greedy action deviation of DDQN in Figure 3. Each point means the average metric across randomly sampled states and the whole learning process. As expected, we can observe that the value churn accumulates as more training updates take place, leading to the growth of greedy action deviation. With CHAIN, the churn and deviation are reduced significantly. We refer the readers to Figure 10 for more statistics on the value churn.\nFurther, we show the learning curves of CHAIN DDQN regarding episode return in Figure 4. We can see that CHAIN consistently achieves clear improvements over DDQN in terms of both sample efficiency and final scores, especially for Asterix and Seaquest. Moreover, CHAIN (Auto) matches or surpasses the results achieved by manual coefficients, which supports Question (2) positively. In the next subsection, we evaluate how much CHAIN can improve policy gradient-based RL algorithms."}, {"title": "Results for CHAIN PPO in MuJoCo and DMC", "content": "Corresponding to the second DRL scenario discussed in Section 5.1, we focus on the policy churn in Proximal Policy Optimization (PPO) [Schulman et al., 2017], and try to answer the first three questions for policy gradient-based RL algorithms. We build the experiments on the public implementation of PPO from CleanRL [Huang et al., 2022] and use the continuous control tasks in MuJoCo and DeepMind Control (DMC) as the environments for evaluation. Following the same principle, we implement CHAIN PPO by adding the policy churn reduction regularization to the standard PPO policy training (Eq. 6), with no other modification to the public PPO implementation.\nFirst, to understand the level of churn while PPO is training, we compare PPO and CHAIN PPO with different choices of $\\lambda_\\pi$ in terms of policy churn and episode return. In summary, we observed that PPO also exhibits clear policy churn and CHAIN significantly reduces it throughout learning, which answers Question (1). Figure 11 shows the details for this analysis. Note that more policy churn makes it more likely to violate the trust region as $C_\\pi \\propto r(\\phi, \\phi')$ discussed in Section 5.1. In turn, we also observed CHAIN PPO consistently outperforms PPO in Ant-v4 and HalfCheetah-v4 across different choices of $\\lambda_\\pi$.\nFurther, we aim to answer Question (2) and evaluate whether CHAIN can improve the learning performance of PPO in terms of episode return. For CHAIN PPO (Auto), we set the target relative loss scale $\\beta$ to 0.1 for MuJoCo tasks and 0.02 for DMC tasks. The results are reported in Figure 5 and Figure 6 for MuJoCo and DMC, respectively. The results show that CHAIN PPO outperforms PPO in most cases with higher sample efficiency and final episode return, often significantly. We believe that our results reveal a promising direction to improve more trust-region-based and constraint-based methods in DRL by addressing the issues caused by churn."}, {"title": "Results for Deep Actor-Critic Methods with CHAIN in MuJoCo and D4RL", "content": "Next, we continue our empirical study and evaluate our method for deep"}]}