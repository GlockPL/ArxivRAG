{"title": "Recent Advances in Large Langauge Model Benchmarks against Data\nContamination: From Static to Dynamic Evaluation", "authors": ["Simin Chen", "Yiming Chen", "Zexin Li", "Yifan Jiang", "Zhongwei Wan", "Yixin He", "Dezhi Ran", "Tianle Gu", "Haizhou Li", "Tao Xie", "Baishakhi Ray"], "abstract": "Data contamination has received increasing at-\ntention in the era of large language models\n(LLMs) due to their reliance on vast Internet-\nderived training corpora. To mitigate the risk of\npotential data contamination, LLM benchmark-\ning has undergone a transformation from static\nto dynamic benchmarking. In this work, we\nconduct an in-depth analysis of existing static\nto dynamic benchmarking methods aimed at\nreducing data contamination risks. We first\nexamine methods that enhance static bench-\nmarks and identify their inherent limitations.\nWe then highlight a critical gap-the lack of\nstandardized criteria for evaluating dynamic\nbenchmarks. Based on this observation, we\npropose a series of optimal design principles\nfor dynamic benchmarking and analyze the lim-\nitations of existing dynamic benchmarks. This\nsurvey provides a concise yet comprehensive\noverview of recent advancements in data con-\ntamination research, offering valuable insights\nand a clear guide for future research efforts.\nWe maintain a GitHub repository to continu-\nously collect both static and dynamic bench-\nmarking methods for LLMs. The repository\ncan be found at this link 1.", "sections": [{"title": "1 Introduction", "content": "The field of natural language processing (NLP)\nhas advanced rapidly in recent years, fueled by\nbreakthroughs in Large Language Models (LLMs)\nsuch as GPT-4, Claude3, and DeepSeek (Achiam\net al., 2023; Liu et al., 2024; Wan et al., 2023).\nTrained on vast amounts of Internet-sourced data,\nthese models have demonstrated remarkable capa-\nbilities across various applications, including code\ngeneration, text summarization, and mathematical\nreasoning (Codeforces, 2025; Hu et al., 2024).\nTo develop and enhance LLMs, beyond advance-\nments in model architectures and training algo-\nrithms, a crucial area of research focuses on effec-"}, {"title": "2 Background", "content": "2.1 Data Contamination\nData contamination occurs when LLM's training\ndata Dtrain contains information that improperly\noverlaps with evaluation benchmark data Dtest,\ncompromising the validity of performance measure-\nments. We summarize existing work and provide a\nformal definition of data contamination.\nExact contamination occurs when there is any\nexact duplicate in the benchmark dataset\n\u2203d s.t. d \u2208 Dtrain and d \u2208 Dtest\nIn other word, there exist a data point d that both\nin Dtrain and Dtest. Common cases include verba-\ntim test examples appearing in training corpora,\ncode snippets from benchmark implementations, or\ndocumentation leaks.\nSyntactic contamination occurs when a test data\npoint could be found in the training dataset after a\nsyntactic transformation, such that\n\u2203d s.t. Fsyntactic(d) \u2208 Dtrain and d \u2208 Dtest\nwhere Fsyntactic denotes syntactic transformations\nlike punctuation normalization, whitespace modifi-\ncation, synonym substitution, morphological vari-\nations, or syntactic paraphrasing while preserving\nlexical meaning."}, {"title": "2.2 Contamination from LLM Training", "content": "Unlike traditional models with clear separations\nbetween training and evaluation data, LLMs are\npre-trained on massive, diverse datasets often\nscraped from the web (e.g., FineWeb (Penedo et al.,\n2024)) which increases the risk of evaluation data\noverlap. In the post-training phase, models are fur-\nther fine-tuned on large human-annotated (Mukher-\njee et al., 2023; Kim et al., 2023) or synthetic\ndatasets (Ding et al., 2023; Teknium, 2023; Wang\net al., 2023) that may resemble evaluation tasks,\nfurther compounding contamination risks. Al-\nthough retrieval-based detection methods (Team\net al., 2024; Achiam et al., 2023) exist, the sheer\nscale and complexity of training corpora make it\ndifficult to entirely exclude evaluation data. Addi-\ntionally, many LLMs keep their training data pro-\nprietary (Dubey et al., 2024; Yang et al., 2024),\ncomplicating the accurate assessment of their true\nperformance and highlighting the need for fair and\nreliable benchmarks. This opacity further exacer-\nbates data contamination, as it impedes the com-\nmunity's ability to verify and mitigate potential\noverlaps between training and evaluation data."}, {"title": "2.3 LLM Benchmarking", "content": "As LLMs evolve into general-purpose task solvers,\nit is crucial to develop benchmarks that provide a\nholistic view of their performance. To this end, sig-\nnificant human effort has been dedicated to build-\ning comprehensive benchmarks that assess vari-\nous aspects of model performance. For example,\ninstruction-following tasks evaluate a model's abil-\nity to interpret and execute commands (Zhou et al.,\n2023; Qin et al., 2024; Huang et al., 2024), while\ncoding tasks assess its capability to generate and\nunderstand programming code (Chen et al., 2021;\nAustin et al., 2021; Jimenez et al., 2024; Code-\nforces, 2025; Aider, 2025). Despite their useful-\nness, static benchmarks face challenges as LLMs\nevolve rapidly and continue training on all avail-\nable data (Villalobos et al., 2022). Over time, un-\nchanging benchmarks may become too easy for\nstronger LLMs or introduce data contamination\nissues. Recognizing this critical problem, contami-\nnation detectors have been developed to quantify\ncontamination risks, and dynamic benchmarks have\nbeen proposed to mitigate these issues."}, {"title": "3 Static Benchmarking", "content": "In this section, we summarize a collection of static\nbenchmarks that have been widely used to evalu-\nate various aspects of model performance. These\nbenchmarks cover a broad range of tasks includ-\ning math, language, coding, reasoning, knowledge,\nsafety, instruction following, and reading compre-\nhension. They serve as standardized evaluation\ntools to measure model abilities in areas such as\narithmetic problem-solving, natural language un-\nderstanding, program synthesis, commonsense rea-\nsoning, factual knowledge retrieval, toxicity detec-\ntion, and more. Table 2 provides an overview of\nthese benchmarks along with the corresponding\ntask categories and key references.\n3.1 Problem Formulation\nA static benchmark is given by D = (X, Y, S(.)),\nwhere D represents the seed dataset, consisting\nof input prompts X, expected outputs y, and a"}, {"title": "3.2 Static Benchmark Application", "content": "Math Math benchmarks evaluate a model's abil-\nity to solve multi-step math problems. Datasets\nsuch as GSM8K (Cobbe et al., 2021) and\nMATH (Hendrycks et al., 2021) require models\nto work through complex problems. Recent chal-\nlenges like AIME 2024 (of America, 2024) and\nCNMO 2024 (Society, 2024) further test a model's\ncapacity to tackle diverse and intricate math tasks.\nKnowledge Knowledge benchmarks evalu-\nate LLM internal knowledge. NaturalQues-\ntions (Kwiatkowski et al., 2019) and Trivi-\naQA (Joshi et al., 2017) focus on retrieving\nreal-world information, while multi-domain tasks\nare covered by MMLU (Hendrycks et al., 2020),\nBBH (Suzgun et al., 2022), and AGI Eval (Zhong\net al., 2023). Recent extensions like MMLU-\nRedux (Gema et al., 2024) and MMLU-Pro (Wang\net al., 2024b) refine these assessments further."}, {"title": "3.3 Methods for Mitigation", "content": "Due to the nature of LLM training data collection\nand the public availability of these static benchmark\ndatasets, there is a risk that LLMs may inadver-\ntently encounter and use them, leading to data con-\ntamination. To address this issue, several methods\nhave been proposed to enhance static benchmark-\ning and mitigate the impact of data contamination.\n3.3.1 Canary String\nCanary strings are deliberately crafted, unique to-\nkens embedded within a dataset to serve as markers\nfor data contamination. When a model's output\nunexpectedly includes these tokens, it strongly in-\ndicates that the model has memorized portions of\nits training data rather than learning to generalize.\nFor instance, the BIG-Bench dataset incorporates\nthese strings so that model developers can identify\nand filter out such instances (Jacovi et al., 2023).\nLimitations The effectiveness of canary strings\ndepends on model trainers being aware of and re-\nsponsive to these markers. If a developer aims\nto leak benchmarking data to boost scores, this\nmethod will not work.\n3.3.2 Encryption\nEncryption methods secure evaluation data by mak-\ning it inaccessible to unauthorized parties, prevent-\ning its accidental inclusion in training sets. Jacovi\net al. (2023) propose encrypting test data with a\npublic key and a \u201cNo Derivatives\" license to block\nautomated crawling and reuse. Yang et al. (2023)\nshow that even advanced decontamination methods\ncan be defeated by minor text variations, empha-\nsizing the need for robust encryption. Similarly,\nTRUCE (Chandran et al., 2024) leverages confiden-\ntial computing and secure multi-party computation\nto enable private benchmarking, ensuring that test\ndata and model parameters remain confidential.\nLimitation While these methods effectively pro-\ntect against data leakage, they depend on strong key\nmanagement, they introduce extra computational\noverheads. These methods are vulnerable if encryp-\ntion is compromised or private key is exposed.\n3.3.3 Label Protection\nLabel protection involves keeping the true answers\nof a test set hidden from public access so that only\nan authorized evaluator can use them during model\nassessment. This approach is common in bench-\nmarks such as GLUE (Wang, 2018), SuperGLUE\n(Wang et al., 2019), and OpenAI's HumanEval\n(Chen et al., 2021), etc., where the test labels are\nwithheld to prevent models from learning or mem-\norizing them during training. The key advantage\nof this method is its ability to maintain evaluation\nintegrity by preventing model exposure to answers,\nthereby mitigating data contamination risks.\""}, {"title": "3.3.4 Post-hoc Detection", "content": "Post-hoc detection mitigates data contamination\nby identifying overlaps between Dtrain and Dtest.\nThis is typically done through n-gram matching at\nvarious levels, such as tokens (Touvron et al., 2023)\nor words (Radford et al., 2019; Brown et al., 2020;\nChowdhery et al., 2023). However, exact matching\noften leads to false negatives, prompting the use\nof more robust techniques like embedding-based\nsimilarity (Riddell et al., 2024; Lee et al., 2023;\nGunasekar et al., 2023) and improved mapping\nmetrics (Li et al., 2024d; Xu et al., 2024).\nBeyond direct overlap detection, post-hoc meth-\nods also analyze model behavior under different\nconditions, such as memorization through masked\ninputs (Ranaldi et al., 2024; Chang et al., 2023),\npartial completions (Anil et al., 2023; Golchin and\nSurdeanu, 2024), or preference for original over\nparaphrased test cases (Duarte et al., 2024; Golchin\nand Surdeanu, 2023; Zong et al., 2024). For in-\nstance, Dekoninck et al. (2024) propose CONSTAT,\nwhich detects contamination by comparing model\nperformance across benchmarks.\nLimitations Post-hot detection methods face sev-\neral limitations. Full access to the training dataset is\noften restricted due to legal and privacy constraints,\nmaking overlap detection challenging. Addition-\nally, assumptions about model behavior, such as\nhigher memorization or lower perplexity for con-\ntaminated instances, may not hold across different\nmodels and tasks."}, {"title": "4 Dynamic Benchmarking", "content": "Due to the inherent limitations of static benchmark-\ning schemes, they face challenges in providing a\ntransparent yet faithful evaluation of LLMs. To\naddress this, dynamic benchmarking has been pro-\nposed.\n4.1 Problem Formulation\nA dynamic benchmark is defined as Bdynamic =\n(D,T(\u00b7)), D = (X, Y, S(\u00b7)) where D represents\nthe static benchmark dataset. The transforma-\ntion function T(\u00b7) modifies the data set during the"}, {"title": "4.2 Evaluation Criteria", "content": "While many dynamic benchmarking methods have\nbeen proposed to evaluate LLMs, the evaluation\ncriteria for assessing these benchmarks themselves\nremain non-standardized. To this end, we propose\nthe following evaluation criteria to assess the qual-\nity of a dynamic benchmarking algorithm.\n4.2.1 Correctness\nThe first criterion for evaluating the quality of dy-\nnamic benchmarking is Correctness. If the cor-\nrectness of the generated dataset cannot be guaran-\nteed, the benchmark may provide a false sense of\nreliability when applied to benchmarking LLMs,\nleading to misleading evaluations. We quantify the\ncorrectness of dynamic benchmarks as:\nCorrectness = EN [Ei=1 S(Vi, G(Xi))]\nwhere Xi and Vi represent the input and output of\nthe ith transformation, respectively. The function\nG(\u00b7) is an oracle that returns the ground truth of\nits input, ensuring an objective reference for cor-\nrectness evaluation. For example, the function G(\u00b7)\ncould be a domain-specific an annotator. This equa-\ntion can be interpreted as the expected alignment\nbetween the transformed dataset's outputs and their\ncorresponding ground truth values, measured using\nthe scoring function S(.). A higher correctness\nscore indicates that the dynamic benchmark main-\ntains correctness to the ground truth.\n4.2.2 Scalability\nThe next evaluation criterion is scalability, which\nmeasures the ability of dynamic benchmark-\ning methods to generate large-scale benchmark\ndatasets. A smaller dataset can introduce more\nstatistical errors during the benchmarking process.\nTherefore, an optimal dynamic benchmark should\ngenerate a larger dataset while minimizing associ-\nated costs. The scalability of a dynamic benchmark\nis quantified as:\nScalability = EN [Ei=1 ||T(D)||\n||D|| \u00d7 Cost(T)"}, {"title": "4.2.3 Collision", "content": "One of the main motivations for dynamic bench-\nmarking is to address the challenge of balancing\ntransparent benchmarking with the risk of data con-\ntamination. Since the benchmarking algorithm is\npublicly available, an important concern arises: If\nthese benchmarks are used to train LLM, can they\nstill reliably reflect the true capabilities of LLMs?\nTo evaluate the robustness of a dynamic benchmark\nagainst this challenge, we introduce the concept\nof collision in dynamic benchmarking. Collision\nrefers to the extent to which different transforma-\ntions of the benchmark dataset produce overlapping\ndata, potentially limiting the benchmark's ability to\ngenerate novel and diverse test cases. To quantify\nthis, we propose the following metrics:\nCollision Rate = Ei,j=1, i\u2260j ||Di \u2229 Dj ||\n||D||\nRepeat = E1k | k = min[N Dj Di\nj=1"}, {"title": "4.2.4 Stable of Complexity", "content": "Dynamic benchmarks must also account for com-\nplexity to help users determine whether a perfor-\nmance drop in an LLM on the transformed dataset\nis due to potential data contamination or an in-\ncrease in task complexity. If a dynamic transforma-\ntion increases the complexity of the seed dataset,\na performance drop is expected, even without data\ncontamination. However, accurately measuring the\ncomplexity of a benchmark dataset remains a chal-\nlenging task. Existing work has proposed various\ncomplexity metrics, but these are often domain-\nspecific and do not generalize well across differ-\nent applications. For example, DyVal (Zhu et al.,\n2024a) proposes applying graph complexity to eval-\nuate the complexity of reasoning problems. For-\nmally, given a complexity measurement function\n\u03a8(.), the stability can be formulated as:\nStability = Var(\u03a8(D))"}, {"title": "4.2.5 Diversity", "content": "Besides the aforementioned criteria, another im-\nportant factor is the diversity of the transformed\ndataset. This diversity can be categorized into\ntwo components: external diversity and internal\ndiversity: External diversity measures the varia-\ntion between the transformed dataset and the seed\ndataset. Internal diversity quantifies the differences\nbetween two transformation trials.\nExternal Diversity = E1O(Di, D)\nInternal Diversity = E,j=1,i\u2260jO(Di, Dj)\nwhere (\u00b7) is a function that measures the diversity\nbetween two datasets. For example, it could be\nthe N-gram metrics or the reference based metrics,\nsuch as BLEU scores."}, {"title": "4.2.6 Interpretability", "content": "Dynamic benchmarking generates large volumes\nof transformed data, making manual verification\ncostly and challenging. To ensure correctness, the\ntransformation process must be interpretable. Inter-\npretable transformations reduce the need for exten-\nsive manual validation, lowering costs. Rule-based\nor manually crafted transformations are inherently\ninterpretable, while LLM-assisted transformations\ndepend on the model's transparency and traceabil-\nity. In such cases, additional mechanisms like ex-\nplainability tools, or human-in-the-loop validation\nmay be needed to ensure reliability and correctness."}, {"title": "4.3 Existing Work", "content": "Building on the task formats of static benchmarks,\ndynamic benchmarks have been introduced to as-\nsess LLM capabilities while minimizing data con-\ntamination and ensuring fairness. Table 2 summa-\nrizes recent dynamic benchmarks for LLM eval-\nuation. Dynamic benchmarks can be categorized\ninto four types based on their construction process:\ntemporal cutoff, rule-based generation, LLM-based\ngeneration, and hybrid approaches. Temporal cut-\noff follows a data collection process similar to static\nbenchmarks, with the key difference being that data\nis gathered from newly released information. Rule-\nbased and LLM-based generation approaches cre-"}, {"title": "5 Discussions", "content": "Key Insights. Our analysis highlights two key in-\nsights regarding data contamination in LLM bench-\nmarking. First, static benchmarks become less\neffective as training corpora grow. The proba-\nbility of contamination increases with Prcontam X\n|Dtrain|\u00b7|Dtest|-1, rendering traditional benchmarks\noutdated for models trained on web-scale data.\nData privacy and commercial concerns further com-\nplicate contamination issues. Second, traditional\nstatic methods fall short in preventing contamina-\ntion, prompting the creation of dynamic bench-\nmarks. However, our study identifies a lack of\nstandardized criteria for evaluating these dynamic\nbenchmarks. We hope our proposed criteria will\noffer valuable insights and guide the development\nof more effective benchmarks.\nCurrent Challenges. Static benchmarking meth-\nods face challenges due to a lack of transparency\n(e.g., label protection) and high assumptions about\ncontaminated models (e.g., post-hoc detection). Al-\nthough dynamic benchmarks address these limita-\ntions, they introduce new issues, such as balancing\ncorrectness with scalability. We also observed that\nsome dynamic benchmarks neglect complexity con-\ntrol, resulting in inefficiencies in evaluation.\nFuture Directions. Dynamic benchmarking offers\na new approach for evaluating LLMs, but our study\nreveals a lack of standardized criteria. Future ef-\nforts should focus on standardizing these criteria\nfor dynamic benchmarks."}, {"title": "6 Conclusion", "content": "This survey reviews the literature on data contami-\nnation in LLM benchmarking, analyzing both static\nand dynamic approaches. We find that static meth-\nods, though consistent, become more vulnerable\nto contamination as training datasets grow. While\ndynamic approaches show promise, they face chal-\nlenges in reliability and reproducibility. Future\nresearch should focus on standardized dynamic\nevaluation, and practical mitigation tools."}]}