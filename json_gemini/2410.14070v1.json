{"title": "FaceSaliencyAug: Mitigating Geographic, Gender and Stereotypical Biases via Saliency-Based Data Augmentation", "authors": ["Teerath Kumar", "Alessandra Mileo", "Malika Bendechache"], "abstract": "Geographical, gender and stereotypical biases in computer vision models pose significant challenges to their performance and fairness. In this study, we present an approach named FaceSaliency Aug aimed at addressing the gender bias in Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). Leveraging the salient regions of faces detected by saliency, the propose approach mitigates geographical and stereotypical biases in the datasets. FaceSaliency Aug randomly selects masks from a predefined search space and applies them to the salient region of face images, subsequently restoring the original image with masked salient region. The proposed augmentation strategy enhances data diversity, thereby improving model performance and debiasing effects. We quantify dataset diversity using Image Similarity Score (ISS) across five datasets, including Flickr Faces HQ (FFHQ), WIKI, IMDB, Labelled Faces in the Wild (LFW), UTK Faces, and Diverse Dataset. The proposed approach demonstrates superior diversity metrics, as evaluated by ISS-intra and ISS-inter algorithms. Furthermore, we evaluate the effectiveness of our approach in mitigating gender bias on CEO, Engineer, Nurse, and School Teacher datasets. We use the Image-Image Association Score (IIAS) to measure gender bias in these occupations. Our experiments reveal a reduction in gender bias for both CNNs and ViTs, indicating the efficacy of our method in promoting fairness and inclusivity in computer vision models.", "sections": [{"title": "1 Introduction", "content": "Computer vision models and their application have exhibited various social biases such as gender bias [4, 3], geographical bias [34, 37] and racial bias [4, 12]. For example, the facial recognition system is less accurate for people with dark skin and also for women [4]. Another example is oxygen therapy; it is a common medical practice, monitored using a pulse oximeter, which measures oxygen levels in the blood by passing infrared light"}, {"title": "3 Proposed Approach", "content": "In this section, we introduce six data augmentation strategies for the search space and outline the proposed approach based on this search space."}, {"title": "3.1 Search Space - Data Augmentation", "content": "The search space encompasses six proposed data augmentation techniques, each detailed below. It's worth noting that these augmentation methods operate on the salient region of the image, which is detected using methodologies outlined in previous works."}, {"title": "3.1.1 Row Slice Erasing (RSE)", "content": "RSE technique augments the salient region x of image I by element-wise multiplication with a binary mask M. The mask M contains values of 0 or 1, denoting exclusion or inclusion of pixels, respectively. Slices of size S are randomly selected from the salient region to generate the binary mask. Horizontal slices of the mask are filled alternately with O's and 1's. Refer to Fig. 3(a) for visualization.\n$\\x = x \\cdot M$ \n(1)"}, {"title": "3.1.2 Column Slice Erasing (CSE)", "content": "Similar to RSE, CSE involves applying augmentation to the salient region x of the image I. The augmented salient part is obtained using a binary mask M generated by selecting slices of size S from the salient region. However, in this strategy, vertical slices of the mask are alternately filled with O's and 1's. See Fig. 3(b) for a visualization."}, {"title": "3.1.3 Row Column Slice Erasing (RCSE)", "content": "RCSE combines RSE 3.1.1 and CSE 3.1.2 techniques. RSE and CSE are sequentially applied. RCSE is illustrated in Fig. 3(c)."}, {"title": "3.1.4 Partially Saliency Erasing (PSE)", "content": "PSE divides the salient region into four parts and randomly erases one or more squares (Fig. 3(d)). Mathematically, a binary mask M is split into four equal parts, with a random number of parts filled randomly with O's or 1's. Element-wise multiplication generates the augmented image $\\hat x$ (Equation 1)."}, {"title": "3.1.5 Horizontal Half Saliency Erasing (HHSE)", "content": "HHSE horizontally divides the salient region into two parts and randomly erases one part (Fig. 3(e)). The mask M is partitioned into two segments, with one filled with O's and the other with 1's. This process generates the augmented image x through element-wise multiplication (Equation 1)."}, {"title": "3.1.6 Vertical Half Saliency Erasing (VHSE)", "content": "VHSE vertically divides the salient region into two parts and randomly erases one part (Fig. 3(f)). The mask M is split vertically into two equal-sized segments, with one filled with O's and the other with 1's. This process generates the augmented image x via element-wise multiplication (Equation 1)."}, {"title": "3.2 FaceRandAug:", "content": "Given an input image I and a salient region x detected within I, FaceRandAug randomly selects one of the following erasing strategies from the given data augmentation list with equal probability: RSE, CSE, RCSE, PSE, HHSE and VHSE. Let S denote the selected erasing strategy. The augmented salient region \u00ee is obtained as follows:\n$\\hat x =\\{\\begin{array}{ll}  Erased Salient Region using S, & \\text{if S is selected} \\\\  x, & \\text{otherwise} \\end{array}$\nwhere x represents the augmented salient region. The selection process of FaceRandAug ensures that each erasing strategy has an equal chance of being selected, similar to the searching process of RandAug [9]. This method provides a flexible way to incorporate various erasing strategies into the data augmentation pipeline for facial mask selection."}, {"title": "4 Experiments", "content": "In this section, we discuss experiments settings, results and insights."}, {"title": "4.1 Experimental Setting", "content": "For measuring data diversity in terms of geographical and stereotypical biases, we use two variants of the Image Similarity Score (ISS) namely ISSIntra and ISSCross [34]. ISSIntra measures data diversity within dataset and ISSCross measures data diversity among different datasets [34]. Five datasets (i.e: Flickr Faces HQ (FFHQ), WIKI, IMDB, Labelled Faces in the Wild (LFW), UTK Faces and Diverse Dataset) with same setting as used in work [34]."}, {"title": "4.2 Result", "content": "We performed experiments on two tasks: measuring dataset diversity across five datasets and evaluating gender diversity in four occupation datasets. For dataset diversity, we calculated Intra-dataset Image Similarity Score (ISSintra), demonstrating enhanced diversity across all datasets (Table 2), and the proposed also shown superior performance than RSMDA [22] except for LFW dataset. For a more detailed analysis of query combinations with language-location pairs, Table 1 presents the ISSintra for various queries across different language-location pairs. We compare the baseline ISS (intra and across) values with our proposed approach. The queries include CEO, Engineer, Nurse, Politician, and School Teacher, each evaluated with multiple language-location pairs. Overall, our proposed approach demonstrates higher diversity, as measured by ISS scores, as shown in Table 3. Additionally, we also analyzed Inter-dataset Image Similarity Score (ISSinter) and ISSintra for five occupation datasets. Our approach generally showed greater diversity, except for the school teacher dataset, as shown in Table 3, potentially due to a female bias identified by Mandal et al. (2023) [37]. Particularly, our approach consistently outperformed the baseline for \"CEO\" and \"Engineer\" occupations in both ISSintra and ISScross. Additionally, across various datasets, our approach achieved slightly better results compared to the baseline. Moreover, our approach yielded higher mean ISS scores across all queries, indicating its effectiveness in enhancing diversity. Overall, our approach achieved impressive diversity scores.\nIn addition, we assessed gender bias using the Image-Image Association Scores (IIAS) on both masked (obscured) and unmasked (non-obscured) data, utilizing different CNNs and ViTs. Our proposed approach consistently reduced bias across all scenarios, with particularly significant reductions observed in the unmasked dataset. Notably, when the data was well-balanced (Unbiased) in terms of gender, our approach achieved an approximately 53-fold reduction in bias. Furthermore, our approach demonstrated superior performance across all classes compared to the baseline, as indicated by the larger reductions in bias for the \"Ours\" columns compared to the \"Baseline\" columns. Additionally, while our approach exhibited substantial reductions in bias across all occupations compared to the baseline, the magnitude of reduction varied across occupations and data types. For instance, the reduction in bias for the \"Nurse\" occupation was more pronounced in the unmasked dataset compared to the \"School Teacher\" occupation. Furthermore, our approach achieved particularly pronounced bias reduction for ViTs compared to CNNs, highlighting its effectiveness across different model architectures. While the reduction in bias was slightly less pronounced in the masked dataset compared to the unmasked dataset, our approach still exhibited substantial improvements. Overall, our approach"}, {"title": "4.2.1 Masked Bias reduction", "content": "In Fig. 4, the results reveal a clear difference in bias reduction between models for the masked scenario, where gender-specific features are hidden. For occupations such as CEO and Engineer, the Baseline CNN (Masked Biased) and Baseline ViT (Masked Biased) models display positive IIAS scores, indicating the presence of gender bias. The Baseline ViT (Masked Biased) model, in particular, exhibits higher bias in these occupations, with significantly positive scores compared to the CNN models. When applying our augmentation method, there is a noticeable reduction in bias. The CNN + Our Augmentation (Masked Biased) and ViT + Our Augmentation (Masked Biased) models show lower IIAS scores, indicating that the proposed augmentation strategy effectively reduces the bias in these occupations. Specifically, the ViT + Our Augmentation (Masked Biased) model significantly outperforms the Baseline ViT, reducing bias in both CEO and Engineer occupations. For the Nurse and School Teacher occupations, which typically reflect higher stereotypical associations, the bias is much more pronounced in the Baseline ViT (Masked Biased) model. However, with our augmentation, particularly in the ViT + Our Augmentation (Masked Biased) model, the bias is substantially reduced, bringing the IIAS scores closer to zero. This demonstrates that our augmentation method is effective in masking gendered features and reducing stereotypical associations in these biased roles. The CNN + Our Augmentation (Masked Biased) model similarly reduces bias, though the effect is more moderate compared to the ViT-based model."}, {"title": "4.2.2 Unmasked Bias reduction", "content": "In Fig. 5, where the unmasked scenario allows for gender-specific visual cues, the IIAS scores across all occupations are generally higher, reflecting stronger gender biases when the models have access to these cues. For CEO and Engineer, the Baseline CNN (Unmasked Biased) and Baseline ViT (Unmasked Biased) models show significant positive scores, indicating that bias is more evident when gender-related features are not masked. Once again, our augmentation method shows substantial improvements. The CNN + Our Augmentation (Unmasked Biased) and ViT + Our Augmentation (Unmasked Biased) models reduce the biases for CEO and Engineer, with ViT + Our Augmentation (Unmasked Biased) being particularly effective in lowering the IIAS scores, though the scores are higher than in the masked scenario. This indicates that while the augmentation reduces bias, the effect is slightly diminished in the unmasked scenario compared to the masked one. For the Nurse and School Teacher occupations, the Baseline ViT (Unmasked Biased) model shows the highest bias, with very negative IIAS scores, reflecting strong stereotypical associations. However, the ViT + Our Augmentation (Unmasked Biased) model effectively reduces these biases, significantly lowering the IIAS scores. The CNN + Our Augmentation (Unmasked Biased) model also reduces bias but does not perform as well as the ViT-based model. Overall, these results suggest that the augmentation method is effective in reducing bias in the unmasked scenario, though masking gender-specific features (as seen in Fig. 4) yields stronger bias mitigation."}, {"title": "4.3 Trade-off between accuracy vs bias", "content": "The trade-off between accuracy and bias in our proposed method reveals that while a significant reduction in bias is achieved, as illustrated in Table 4, the accuracy performance is also competitive. In Table 5, we observe that our method exhibits notable accuracy across various architectures in the CIFAR10 and CIFAR100 datasets, and most of the models demonstrate superior performance compared to existing methods. However, this competitive performance is not uniform, as there is one specific instance where the accuracy was lower than the baseline. This indicates that while our saliency-based masking technique effectively mitigates bias, it may introduce slight fluctuations in accuracy in certain contexts, necessitating further investigation into the balance between bias reduction and model performance. Moreover, it shows that the accuracy and bias trade-off also depends on the nature of the dataset, model, and application context."}, {"title": "4.4 Real-World Application", "content": "The FaceSaliency Aug method offers practical solutions for reducing bias in various computer vision models, with key applications including:\nFacial Recognition Systems: Common in security and surveillance, facial recognition systems often exhibit demographic biases, leading to errors like marking male instead of female or white instead of non-white. FaceSaliency Aug can help reduce these biases, ensuring more accurate and inclusive recognition across diverse groups [4, 39].\nHealthcare: In facial-based diagnostics, biased models may lead to misdiagnosis. FaceSaliency Aug can help ensure healthcare models are more equitable by addressing biases related to gender and ethnicity [39].\nHuman Resources: Automated HR tools, like resume screening and facial analysis in interviews, may perpetuate gender biases. FaceSaliencyAug can promote fairness in hiring by minimizing these biases."}, {"title": "5 Conclusion", "content": "We introduced FaceSaliency Aug, a new method aimed at diversifying data to lessen geographical and stereotypical biases and diminish gender bias in CNNs and ViTs. By leveraging salient regions detected by our Saliency, we implemented a unique data augmentation technique that randomly masks these regions and restores the original image, thereby increasing data diversity and reducing biases. Our experiments across various datasets, including FFHQ, WIKI, IMDB, LFW, UTK Faces, and Diverse Dataset, showed improved diversity metrics, as evaluated by ISSintra and ISSinter algorithms. Additionally, our method effectively reduced gender bias in datasets such as CEO, Engineer, Nurse, and School Teacher, as indicated by reductions in IIAS for both CNNs and ViTs. The proposed augmentation has also shown impressive performace on CIFAR10 and CIFAR100 across various architectures. Overall, our research emphasizes the importance of addressing biases in computer vision models for fairness and inclusivity. By introducing innovative data augmentation, we've demonstrated significant enhancements in dataset diversity and reductions in gender bias across various occupations, showing promise for real-world applications of computer vision systems."}]}