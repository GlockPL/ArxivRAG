{"title": "From Linguistic Giants to Sensory Maestros: A Survey on Cross-Modal Reasoning with Large Language Models", "authors": ["Shengsheng Qian", "Zuyi Zhou", "Dizhan Xue", "Bing Wang", "Changsheng Xu"], "abstract": "Cross-modal reasoning (CMR), the intricate process of synthesizing and drawing inferences across divergent sensory modalities, is increasingly recognized as a crucial capability in the progression toward more sophisticated and anthropomorphic artificial intelligence systems. Large Language Models (LLMs) represent a class of Al algorithms specifically engineered to parse, produce, and engage with human language on an extensive scale. The recent trend of deploying LLMs to tackle CMR tasks has marked a new mainstream of approaches for enhancing their effectiveness. This survey offers a nuanced exposition of current methodologies applied in CMR using LLMs, classifying these into a detailed three-tiered taxonomy. Moreover, the survey delves into the principal design strategies and operational techniques of prototypical models within this domain. Additionally, it articulates the prevailing challenges associated with the integration of LLMs in CMR and identifies prospective research directions. To sum up, this survey endeavors to expedite progress within this burgeoning field by endowing scholars with a holistic and detailed vista, showcasing the vanguard of current research whilst pinpointing potential avenues for advancement. An associated GitHub repository that collects the relevant papers can be found at https://github.com/ZuyiZhou/Awesome-Cross-modal-Reasoning-with-LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, the advent of Large Language Models (LLMs) has revolutionized the field official intelligence, showcasing unprecedented capabilities in natural language understanding and generation. These models, such as GPT-4 [1], Gemini [2], Claude [3], and Llama 2 [4], have demonstrated remarkable proficiency in tasks ranging from text completion to question answering. Despite the great success of LLMs, the community has also acknowledged some critical limitations in their capabilities. Some work indicates that LLMs have constraints in understanding the physical world and performing complex reasoning [5], [6], [7]. Furthermore, the concept of extending powerful LLMs to multimodal models has gained traction within the realm of multimodal learning [8], [9], [10]. As a result, recent developments in methodologies for Cross-Modal Reasoning (CMR) with LLMs have shown significant progress alongside notable challenges.\nCross-Modal Reasoning with Large Language Models (CMR with LLMs) integrates the capacity of LLMs to comprehend and infer new information across diverse modalities, such as text, image, and sound. Reasoning is a cognitive process that entails a systematic analysis and assessment of information utilizing logical structures, critical thinking principles, and empirical evidence to formulate informed conclusions, facilitate decision-making processes, and resolve complex problems [11]. In particular, CMR [12], [13] encompasses the comprehensive analysis and reasoning across various modalities. Through harnessing the interplay and interdependencies among multiple modalities, CMR aims to extract significant insights and make coherent inferences. In the context of LLMs, this capability is derived from their extensive training on diverse datasets, allowing them to make connections and generate responses that span different types of data. The development of CMR has profoundly impacted and transformed a multitude of specialized domains. This progress embraces an array of multimodal tasks, including visual question answering [14], [15], vision-and-language navigation [16], [17], image captioning [18], [19], video captioning [20], [21], etc. Fig. 2 illustrates several examples showcasing the application of LLMs in CMR.\nAs shown in Fig. 1, methods for CMR with LLMs can be roughly classified into four groups according to the roles of LLMs in CMR, as follows: (1) LLM as multimodal fusion engine: LLMs act as integrative frameworks that consolidate and harmonize data from diverse modalities. Through sophisticated algorithms and neural network architectures, these models [9], [22], [23] discern and interpret the multifaceted semantics of tasks, enabling a comprehensive understanding that transcends the limitations of single-modality processing. (2) LLM as textual processor: LLMs apply advanced natural language processing techniques to distill and extract critical information from extensive textual data, which enables the extraction of responses from voluminous textual data [24], [25]. Moreover, LLMs can enrich and optimize textual expression for CMR, such as enriching answers [26] or prompts [27], [28]. (3) LLM as cognitive controller: LLMs architect the reasoning trajectory, orchestrating the sequence of cognitive steps required to address complex tasks. This involves generating structured plans, reasoning pathways [29], [30], or even executable code [31], [32] that guide the model towards the resolution of tasks. (4) LLM as knowledge enhancer: LLMs enrich tasks with implicit knowledge from extensive pre-training while also integrating explicit domain-specific information through external databases or auxiliary tools. LLMs may produce textual representations of visual content [27] or facilitate auxiliary modules [33], [34] for enhanced content comprehension and reasoning. Additionally, LLMs can extract pertinent information from web sources or search engines [29], [35].\nTo the best of our knowledge, this is the first survey that provides an investigation into the roles of LLMs in facilitating CMR. To clarify a comprehensive context for our analysis, we furnish a brief overview of published surveys partially related to our work: Some surveys [36], [37] vigorously examine the nature of LLMs, thoroughly categorizing and elucidating their distinct attributes. Some other surveys [38], [39], [40], meanwhile, elaborate on the reasoning methodologies associated with LLMs, even though their inquiries do not encompass the multimodal perspective. Concentrating primarily on multimodal component incorporation strategies, [41] reviews the state-of-the-art landscape of multimodal LLMs. Different from the above surveys, we review extensive research on Cross-Modal Reasoning with LLMs to provide a comprehensive and up-to-date survey. The objective of this survey is to comprehensively outline the contemporary state-of-the-art developments in CMR with LLMs. Our contribution can be succinctly summarized as follows:\n1) Innovative Taxonomy: We introduce a novel taxonomy for existing approaches for CMR with LLMs, providing a systematic understanding of mixed elements and methodologies in this interdisciplinary domain.\n2) Comprehensive Analysis: We undertake a thorough exploration of the existing models and research in CMR with LLMs, incorporating diverse perspectives, methodologies, and tasks, thus delivering a comprehensive synopsis of current advancements in the field.\n3) Extensive Resources: We collect and discuss abundant resources on CMR with LLMs, including state-of-the-art methods and practical tasks. We hope this survey can serve as a valuable resource for anyone interested in the intersection of cross-modal reasoning and large language models.\n4) Discussion of Challenges and Future Directions: We highlight the existing challenges in CMR with LLMs and then sketch potential future paths and research opportunities. We aim to kindle novel research ideas and foster growth in this burgeoning interdisciplinary field."}, {"title": "2 \u03a4\u0391\u03a7\u039f\u039d\u039fMY FOR CROSS-MODAL REASONING WITH LARGE LANGUAGE MODELS", "content": "As illustrated in Fig. 1, we present a taxonomy of the implementation approaches for cross-modal reasoning (CMR) utilizing large language models (LLMs). From the perspective of the roles of LLMs in CMR, these methods are roughly classified into four categories: Multimodal Fusion Engine, Textual Processor, Cognitive Controller, and Knowledge Enhancer."}, {"title": "2.1 Multimodal Fusion Engine", "content": "LLMs exhibit the capacity to effectively integrate natural language with diverse modalities, including visual or audio inputs. This integration enhances their reasoning abilities, enabling them to furnish more nuanced and informed responses. A Multimodal Fusion Engine (MFE) guarantees the precision of these responses by meticulous mapping of multimodal features into a language-aligned feature space, thus mitigating the risk of generating inaccurate answers or hallucinations. By adopting this integrated approach, LLMs harness insights from various heterogeneous sources, enriching their comprehension and reasoning capabilities, thereby culminating in responses that are more sophisticated and comprehensive.\nMFE plays a crucial role in integrating multimodal data to facilitate cross-modal reasoning. It serves as a fundamental component of LLMs, bridging the gap between various modalities. Presently, three commonly employed methods have been identified to accomplish this integration: Prompt Tuning, Instruction Tuning, and Pre-training.\n\u2022 Prompt Tuning: The specialization in prompt tuning involves the careful design and calibration of tailored textual prompts to guide LLMs toward specific operational areas. Precision in crafting these prompts is vital as it aims to evoke desired responses or behaviors from the models. In the realm of prompt tuning for LLMs, there are two main categories based on prompt formatting: discrete prompt tuning and continuous prompt tuning. Discrete prompt tuning involves creating and using distinct text templates or natural language prompts, while soft prompt tuning utilizes continuous vectors or embeddings as the foundation for prompts. Currently, a combination of these two types of prompts is often employed in CMR tasks. At the heart of prompt tuning lies the transformation of visual information into tokens to construct effective prompts [42], [43], [44].\n\u2022 Instruction Tuning: In the context of LLMs, instruction tuning [10], [45] refers to the practice of adjusting and improving the pedagogical approach utilized for these models. The goal is to enhance the models' understanding of specific directives provided by users. This typically involves clarifying and making task descriptions more precise, or rewording directives in a manner that helps the model accurately interpret the desired outcome. Models can improve performance on cross-modal reasoning (CMR) tasks by fine-tuning instructions from multiple perspectives. This includes adjusting granularity levels [46], [47], [48], optimizing region-level visual-language combinations [49], [50], [51], or integrating both approaches [52], [53].\n\u2022 Multimodal Pre-training: Significant progress has been made in the development of large-scale multimodal pre-training foundation models, driven by innovative pre-training objectives and model architectures. These foundation models primarily leverage several approaches for multimodal tasks, including next token prediction [54], [55], [56], masked language modeling [57], [58], and encoder-decoder structures [57], [58], [59]."}, {"title": "2.2 Textual Processor", "content": "The profound linguistic and semantic expertise possessed by LLMs is harnessed to enhance textual processing, particularly within cross-modal reasoning tasks. In such contexts, modules often generate textual data to facilitate communication between components. Due to the varying designs of these modules, the generated textual content may not always align seamlessly with the specific tasks' requirements. The role of the Textual Processor (TP) is to refine and harmonize this textual information, transforming it into coherent and contextually appropriate natural language expressions or generating customized text suited to precise needs [24]. By prioritizing consistency and fluency, LLMs demonstrate exceptional proficiency in integrating diverse information sources and producing text that meets a wide range of contextual and functional requirements [28], [33], [60].\n\u2022 Semantic Refiner: As semantic refiners, LLMs leverage their language capabilities to refine and distill textual information. In these cases, an expansive corpus of textual data from diverse, heterogeneous sources is provided as input to the LLMs. LLMs then meticulously process and seamlessly integrate this textual compendium, leveraging its sophisticated language understanding to generate concise, semantically refined output that succinctly encapsulates the key conceptual essence distilled from the broader informational corpus.\n\u2022 Content Amplifier: The capabilities of LLMs can be harnessed to expand and enrich textual information. By leveraging their advanced language generation abilities, LLMs can produce supplementary text that offers enhanced contextual framing, detailed descriptions, and illustrative examples. This enriched output serves as valuable input for subsequent models or as fully developed answers tailored to specific tasks."}, {"title": "2.3 Cognitive Controller", "content": "Large Language Models (LLMs) act as Cognitive Controllers (CC), coordinating reasoning processes by generating inputs for various inference tools. They serve as a bridge between the language model and other tools, ensuring seamless integration for proficient reasoning and inference generation. This enhances cognitive processing and resource utilization.\nTo understand LLMs as Cognitive Controllers, we explain their role in cross-modal reasoning. LLMs break down complex tasks into manageable parts and delegate them to suitable computational tools or modules [32], [35], [61], [62]. They can create strategic outlines or identify specific modules for engagement, primarily using programming constructs and natural language interfaces.\n\u2022 Programmatic Construction: LLMs convert cross-modal reasoning tasks into structured programs, thereby enabling the precision crafting of reasoning sequences and facilitating the generation of standardized responses. Notably, the application VisProg [32] utilizes GPT-3 to create a visual script, in which each line of code is meticulously mapped to a module that addresses a designated sub-task. Additionally, LLMs can also be tasked with the provision of parameter identifiers for inputs requisite for module functionality, employing crafted contextual exemplars as benchmarks to adeptly maneuver through complexities [35].\n\u2022 Linguistic Interaction: Contrasting with the rigidity of programmatic directives, natural language presents a more dynamic modality for interaction, enabling models to curate responses that are both broader in scope and logically coherent. Typically, these models formulate reasoning pathways either by generating elaborative monologues [24], [30] or by orchestrating contextual dialogues with peer models, which facilitate a seamless flow of reasoning [63], [64], [65]. This modality allows for a nuanced and flexible approach to cross-modal reasoning that is adaptable to emergent complexities and user expectations."}, {"title": "2.4 Knowledge Enhancer", "content": "LLMs can amalgamate and condense information derived from extensive knowledge bases, thus enabling them to generate outputs rich in informative content. This capability is attained through a methodical, iterative process [26], wherein LLMs conduct searches within the knowledge base, appraise the contextual cues and historical data, assess the adequacy of the available information to fulfill tasks or address queries, and ultimately present the response in a user-friendly manner.\nCross-modal reasoning pertains to the integration of data from various modalities, such as text, images, and audio, to derive coherent and meaningful deductions. Nonetheless, accomplishing successful cross-modal reasoning often necessitates additional knowledge beyond what is presently available. In this context, LLMs assume a pivotal role as Knowledge Enhancers (KE). LLMs represent formidable AI models that have undergone extensive training on copious amounts of textual data, exhibiting human-like language generation capabilities. Employing these models enables the supplementation of existing knowledge bases with augmented contextual information, thereby facilitating cross-modal reasoning. According to the source of provided knowledge, these methods can be divided into two groups: Implicit Cognition and Augmented Knowledge.\n\u2022 Implicit Cognition: Within the realm of CMR tasks, LLMs are employed directly, harnessing the extensive knowledge acquired during their comprehensive training phases. This deployment capitalizes on the LLMs' sophisticated language comprehension abilities, derived from their exposure to a vast corpus of textual data, to facilitate reasoning across diverse modalities including text, imagery, and auditory inputs. Notably, LLMs can generate textual interpretations of visual content [27] or provide prompts [33], [34] that support auxiliary modules in their processing tasks, thereby enhancing comprehension and reasoning about the multimodal content.\n\u2022 Augmented Knowledge: LLMs also serve as reservoirs of external knowledge, augmenting CMR tasks with additional information that enriches the existing knowledge base. In this capacity, LLMs function as dynamic tools for the retrieval or generation of pertinent textual content, effectively supplementing the information pool available for CMR undertakings. This might involve leveraging LLMs to source relevant textual data from digital repositories, websites, or Search Engines [29], [35], thereby broadening the scope of knowledge accessible for informed reasoning and analysis."}, {"title": "3 LLM AS MULTIMODAL FUSION ENGINE", "content": "Fig. 5 illustrates the utilization of LLMs as a Multimodal Fusion Engine(MFE) within the context of cross-modal reasoning. A typical CMR task encompasses a textual task description accompanied by multimodal data inputs. The transformation of multimodal data into an array of tokens, which serve as the LLMs' inputs, is critical for achieving LLM-based information fusion. These tokens are frequently comprised of specialized prompts, along with code or machine languages that are easily interpretable by LLMs. The LLMs are adept at extracting vast amounts of textual information and refining it to formulate a concise answer.\nThe current MFE can be categorized into three methods: prompt tuning, instruction tuning, and multimodal pre-training. On a broader scale, models that utilize prompt tuning and instruction tuning for training their modules follow a specific prompting process. For a clearer understanding, we utilize Fig.6 to illustrate the distinction between prompting and pre-training. Additionally, we leverage the differences in tuning to delineate between the two major categories of prompting methods. The development of effective prompts is a focal point in various research endeavors, often employing prompt tuning\u2014a method involving the creation and refinement of prompt tokens [66]. This procedure can be categorized into prompt tuning and instruction tuning (outlined in Fig. 7)."}, {"title": "3.1 Prompt Tuning", "content": "Fig. 8 exemplifies various prompts and their corresponding responses within a Visual Question Answering (VQA) system. Upon examination of the figure, it becomes evident that cross-modal discrete prompts amalgamate multimodal data with textual components to create prompts comprising distinct tokens. These tokens, in turn, direct LLMs to generate responses that align with the provided tokens. In contrast, cross-modal continuous prompts are scrupulously designed to represent multimodal data by utilizing specialized CMR tasks.\nEnhancements to commanding LLMs for CMR often involve prompt tuning [67], a process that optimizes LLMs with task-specific datasets using newly introduced prompt tokens. This progression commences with extracting multimodal data from varied inputs, subsequently, unique prompts are forged from textual input via techniques such as prompt engineering [68], [69]. Appropriate tuning techniques are implemented to train the models and construct the final answers. In the process of discrete prompts tuning, LLMs are typically kept frozen while semi-freezing, or partial tuning is predominant in the sphere of continuous prompts and instructions tuning.\nCAT [60] integrates multimodal controls for image captioning by converting visual prompts into mask prompts and using a visual chain-of-thought technique to generate and refine captions based on genre.\nKAT [70] combines implicit knowledge extraction from GPT-3 [55] with explicit contrastive learning via CLIP to develop prompts that guide LLMs, focusing on visually-aligned entities. REVIVE [71] extends KAT by incorporating regional features to create context-aware prompts, retrieving both implicit and explicit knowledge, and merging visual data for answer generation.\nVisual ChatGPT [72] enables users to interact with ChatGPT via text and image inputs, leveraging multiple Visual Foundation Models (VFMs) to handle CMR tasks with a prompt system that details each VFM's function and use case.\nPaLM-E [73], derived from PaLM [74], demonstrates exceptional performance in a variety of embodied reasoning tasks, including sequential robotic manipulation planning, VQA, and Cross-Modal Captioning (CMC). In PaLM-E, an approach is introduced whereby the Language and Vision model is kept fixed, and only the input encoders are trained to evaluate and compare different-modality encoders. The objective is to establish a strong grounding of the model in observations and enhance its embodiment capabilities. This training methodology can be considered as a type of input-conditioned soft-prompting technique.\nVL-T5 [75] is a multimodal model that builds upon T5 [76]. The model effectively leverages text prefixes to accommodate a wide range of tasks and integrates visual embeddings obtained from a Faster R-CNN [77] into a bidirectional multimodal encoder. By utilizing the technique known as Prefix Tuning [78], VL-T5 constructs continuous prompts using diverse prefixes and visual embeddings.\nBLIP-2 [79] presents a two-stage framework designed to mitigate the modality gap in CMR. In the first stage, a querying transformer is pre-trained using a fixed image encoder to facilitate vision-language representation learning. In the second stage, either OPT [80] or FlanT5 [81] is employed for generative vision-to-language learning. Specifically, FlanT5 is pre-trained using a prefix language modeling loss, where continuous prompts are constructed by combining prefix text and visual representation, while the generation target is represented by the suffix text.\nVPGTrans [82] is a framework that accelerates transfer learning while preserving performance. It leverages a learned mapping between visual content and a soft prompt from a specific LLM. The soft prompt is converted to a different LLM, allowing the converter to merge with the projector trained on another dataset. Word embeddings from both models serve as soft prompts, exhibiting similarity.\neP-ALM [83] is a model that effectively integrates perceptual encoders with language models through the use of continuous prompts, while simultaneously minimizing the number of trainable parameters. This is achieved through the implementation of a singular linear projection layer, which connects the two modules. Leveraging the hierarchical representation encoded in the pre-trained models, eP-ALM tactfully injects [CLS] tokens from multiple layers of the perceptual model [84] into several layers of LLM. Such an approach enables the language model to effectively integrate perceptual input, thus contributing to contextually relevant outputs.\nVCOT [85] is a framework that integrates the efficiency, robustness, and multi-step reasoning abilities of Chain of Thought (CoT) with the multimodal capabilities of vision-language models. It aims to resolve logical inconsistencies in sequential datasets by generating synthetic multimodal infillings. To accomplish this, VCoT employs GPT-3 to produce a summary that optimizes the likelihood of the projected output obtained from a multipoint foveation process, augmented by diverse few-shot exemplars."}, {"title": "3.2 Instruction Tuning", "content": "Instruction tuning is a specialized methodology applied to the training of machine learning models, with a particular emphasis on LLMs. This approach incorporates task-specific directives, designed to enhance the zero-shot performance of these models. Cross-modal instruction tuning is an extension of this method, focusing on reinforcing the models' ability to comprehend and respond efficiently to instructions that amalgamate visual or other data modalities. The primary objective of this technique is to bolster the models' capabilities in executing intricate tasks that necessitate CMR, which combines diverse data modalities.\nSignificantly, instruction tuning presents a notable divergence from traditional prompting systems, primarily due to its emphasis on the necessity of high-quality instruction tuning data during the intensive fine-tuning stage of the models. Nonetheless, it warrants mention that the utility of instruction tuning data is not confined to an individual task but rather spans a range of cognate tasks. Consequently, the acquisition of instruction-following data emerges as a vital constituent in effectuating successful instruction tuning. For instance, MultiInstruct [86] is a comprehensive multimodal instruction tuning benchmark dataset, encompassing 62 diverse tasks across 10 categories in a unified sequence-to-sequence format. Derived from 21 existing open-source datasets, each task is accompanied by five expert-written instructions. utilizing MultiInstruct for instruction tuning, OFA [87], demonstrated enhanced zero-shot performance when evaluated on the extensive NATURAL INSTRUCTIONS dataset [88].\nInstructBLIP [89] compiles a broad array of accessible vision-language datasets, converting them into an instruction-tuned format to enhance the diversity of the instruction-tuning data.InstructBLIP advances vision-language instruction tuning and assesses generalizability. It leverages a Q-Former [79] to extract instruction-sensitive visual features and applies language modeling loss for response synthesis.\nTo generate instruction-following data that engages visual content, LLaVA [10] harnesses the language-only GPT-4 or ChatGPT, characterized as a formidable mentor. More specifically, when encoding an image into its visual features to instigate a textual prompt, LLaVA employs two varieties of symbolic representations: Firstly, Captions, which typically illustrate the visual scenario from multiple vantage points; Secondly, Bounding boxes that generally localize the objects present in the scene, with each box encoding the object concept along with its spatial location. LLaVA-1.5 [90] builds upon LLaVA [10] by incorporating an improved MLP cross-modal connector and leveraging academic task data to enhance multimodal understanding and data efficiency. It is capable of scaling to high-resolution inputs and achieves state-of-the-art performance with reduced data and a more streamlined architecture.Video-ChatGPT [91] elevates video discourse by integrating video representations with an LLM, drawing from vision-language model methodologies for video-related tasks. It repurposes LLaVA [10] on prediction tokens in line with its intrinsic autoregressive paradigm.\nLLaVA-Med [92] introduces a curriculum learning approach to adapt LLaVA [10] for the biomedical domain, leveraging a self-generated biomedical multimodal instruction-following dataset. Similarly, in the biomedical field, MedVInt [93] employs a tripartite architecture comprising a visual encoder, language encoder, and multimodal decoder. As a generative learning model, MedVInt aligns a pre-trained vision encoder with a large language model through visual instruction tuning.\nGPT4Tools [62] leverages GPT-3.5 to create instruction datasets for tool utilization, enhancing LLMs' ability to perform tool-related tasks. It fine-tunes LLMs using an auto-regressive training paradigm and applies LoRA [94] to optimize rank-decomposition factors in the Transformer architecture, preserving the original model parameters. While prefix and suffix prompts are part of the tuning process, they are not discussed here to focus on the fine-tuning method.\nMiniGPT-4 [22] integrates a static visual encoder with Vicuna [95] through a linear projection layer, facilitating the alignment of visual and textual features to optimize learning efficiency. The model undergoes extensive pretraining with aligned image-text pairs, fostering a sophisticated understanding of the interaction between visual and linguistic modalities. Building upon this foundation, MiniGPT-5 [9] introduces the concept of generative vokens, allowing training on raw imagery without the need for detailed annotations. It employs a bifurcated training strategy, incorporating classifier-free guidance, a dedicated mapping module, and supervisory losses to ensure precise alignment between tokens and their corresponding image features.\nVideoChat [96] converts videos into textual and embedded formats to enhance multimodal understanding in LLMs. It features a comprehensive video-centric dataset with detailed textual and dialogic annotations. A joint training protocol, leveraging existing image instruction data [10], [22], [96], improves the system's spatial perception and reasoning for both static images and dynamic videos.\nLaVIN [97], an efficient multimodal language model employing Mixture-of-Modality Adaptation (MMA), adeptly facilitates CMR tasks with minimal pre-training while preserving strong NLP capabilities. Demonstrating high efficiency and robust performance, LaVIN is ideally suited for chatbot applications and its resources are openly available for scholarly research and development.\nVideo-LLaMA [98] is designed for video-grounded conversations by integrating a language decoder with pre-trained unimodal models for visual and audio inputs. It employs cross-modal pre-training to capture the relationships between multimodal data and is fine-tuned using curated instruction data from various sources [10], [22], [96] to enhance its conversational abilities.\nInstruction tuning in ChatBridge [23] enhances the model's multimodal understanding and adherence to human instructions, promoting better zero-shot generalization across various multimodal tasks. This process involves a specialized dataset with multimodal instructions for targeted refinement.\nDetGPT [99] synergizes multimodal models with open-vocabulary object detectors to seamlessly integrate visual and textual modalities. The approach exploits extant corpora dedicated to image captioning and object detection tasks. The enhancement of its interpretative faculties concerning human directives, as well as its competency in generating corresponding object enumerations, is achieved through precision fine-tuning with the aid of a selectively assembled query-answer instruction dataset. The fine-tuning is particularly concentrated on the refinement of the linear projection layer.\nMacaw-LLM [100], an integrative model for CMR, incorporates modality, alignment, and cognitive modules. Human-verified instruction-response pairs are generated by GPT-4. The model utilizes a one-step fine-tuning method for instruction adaptation, promoting coherent cross-modality alignment and reducing error transmission.\nLLaMA-Adapter [101] refines LLaMA into an instruction-following model using learnable prompts and zero-initialized attention for parameter-efficient fine-tuning [102], improving response quality and benchmark performance. LLaMA-Adapter V2 [103] further enhances this by refining bias in linear layers, applying joint training with disjoint parameters, and integrating visual data to improve zero-shot multimodal reasoning and task execution.\nMiniGPT-v2 [104] builds vision-language knowledge by using weakly-labeled datasets for diversity and fine-grained datasets for precision. It integrates a frozen ViT [84] with LLaMA-2 [4], mapping visual tokens to the LLaMA-2 space. Fine-tuning with multi-modal instruction datasets improves conversational abilities, with varying data sampling ratios for fine-grained and new instructions.\nPandaGPT [105] integrates ImageBind's [106] multimodal encoders with Vicuna's language models to handle vision- and audio-based tasks. It aligns the feature spaces of ImageBind and Vicuna using 160k image-language pairs, optimizing a linear projection matrix and LoRA [94] weights while keeping the original parameters unchanged.\nMultiModal-GPT [107] uses a vision encoder, perceiver resampler, and linguistic decoder, applying LoRA to modify self-attention and cross-attention mechanisms for next-token prediction and sequence generation.\nYing-VLM [108], built on BLIP-2 [79] and Ziya-13B [109], is tuned with the M\u00b3IT dataset through a dual-phase process, aligning visual features with text via image captioning and instruction tuning for CMR tasks.\nPolite Flamingo [110] is a method enhancing dataset politeness for vision-language tasks by transforming annotations for instruction tuning. The method involves the filtration and transformation of raw annotations into a more polite style, which subsequently serves as the basis for visual instruction tuning.\nNEXT-GPT [111], designed for high adherence to user directives, yields pertinent multimodal outputs via advanced instruction tuning. Said tuning entails paired training with user instructions and target outputs, applying LoRA [94] for precise parameter updates. Reinforced by optimization using premium annotations, the model incorporates decoding-level fine-tuning, aligning output tokens with the intended multimodal context.\nChatSpot [112], a multimodal interaction model, achieves language-image alignment through a streamlined MLP approach, foregoing additional models or post-processing. This facilitates granular user interactions with image regions, enhancing region-specific instruction comprehension through precise tuning. Trained on the MGVLID, it demonstrates improved task performance.\nBLIVA [113] leverages a Q-Former [79] and a fixed image encoder for instruction-tailored visual feature extraction. The resultant features serve as inputs for an LLM which, following pre-training with image-caption pairs, aligns itself with visual information. This pre-trained LLM is then capable of generating descriptive image representations.\nBuboGPT [114] employs visual grounding for enhanced CMR, using a two-stage training approach with a curated instruction dataset to improve multimodal comprehension. The first stage synchronizes the linear projection layer output with the lexical embedding space, while the second conditions the LLM to interpret directives and produce modality-specific responses.\nVisionLLM [115] features a unified linguistic directive, an image tokenizer, and an LLM-driven task decoder, facilitating the performance of assorted tasks steered by textual prompts. This consolidated instruction set is adept at handling both standalone vision and composite vision-language tasks, enhancing task adaptability."}, {"title": "3.3 Multimodal Pre-training Foundation Models", "content": "Multimodal pre-training is a methodology used to train models to acquire knowledge from various data sources such as images", "121": "a cutting-edge Visual Language Model (VLM)", "122": "endeavors to leverage the Visual Prompt Generator (VPG) for improved performance in CMR tasks", "84": ".", "123": "by converting image features into language embeddings", "elements": "a Visual Encoder that generates feature vectors from images, an Image Prefix module that translates these vectors into embeddings, an autoregressive Language Model integrating these embeddings, and Adapter layers tuned in situ within the transformer language model. Throughout the training, the Language Model's architecture remains static, utilizing pre-existing GPT-J weights [124", "125": "an efficient encoder-decoder model, leverages a pre-trained expert library for handling multimodal inputs and generating text. It employs a vision encoder for RGB images and labels, alongside an autoregressive text decoder. Key trainable elements\u2014the Experts Resampler and the Adaptor\u2014normalize input variability and enhance vision-language processing. Pre-trained weights remain largely fixed to preserve domain knowledge, optimizing Prismer for tasks like image captioning and visual question answering. Training focuses on sequentially predicting text tokens.\nPaLI [126", "127": "module for image processing, and the mT5 [128"}, {"84": "backbones could yield even better results. Training PaLI involves various tasks, including pure language understanding tasks, to maintain mT5's language competencies without forgetting. Notably, only the language component gets updated during training as the vision component remains static.\nUtilizing a dual-encoder architecture like CLIP [129", "130": "LAVILA [131", "132": "leverages the pre-trained CoCa [133", "121": "OpenFlamingo [134", "135": "an in-context instruction-tuned multimodal model, derives from OpenFlamingo [134", "136": "is designed to train on diverse data types such as images, text, observations, and actions. The information is then converted into sequential tokens to effectively handle this multi-modal data. During deployment, Gato utilizes these tokens to generate contextual responses or actions. By employing a joint visual-language model, Gato is capable of understanding and generating responses using both text and visual data, enabling it to handle tasks that involve the comprehension of both visual and textual information.\nThe enhanced PaLI-X [137", "138": "remains unchanged in the initial stage. In the subsequent stage, potential augmentations to the visual component correspond with model progression and escalated image resolution parameters. The PaLI-X model employs an extensive UL2 [139", "140": "combines visual and textual information using a vision encoder based on ViT [84"}]}