{"title": "MetaDecorator: Generating Immersive Virtual Tours through Multimodality", "authors": ["Shuang Xie", "Yang Liu", "Jeannie S.A. Lee", "Haiwei Dong"], "abstract": "MetaDecorator, is a framework that empowers users to personalize virtual spaces. By leveraging text-driven prompts and image synthesis techniques, MetaDecorator adorns static panoramas captured by 360\u00b0 imaging devices, transforming them into uniquely styled and visually appealing environments. This significantly enhances the realism and engagement of virtual tours compared to traditional offerings. Beyond the core framework, we also discuss the integration of Large Language Models (LLMs) and haptics in the VR application to provide a more immersive experience.", "sections": [{"title": "METADECORATOR FRAMEWORK", "content": "In response to this, our work introduces MetaDecorator, the framework that endows users with the capability to customize their virtual experiences using text prompts. This framework shown in consists of two main stages. To provide the interactive and personalized experience, MetaDecorator first capitalizes on recent advancements in 2D content generation using diffusion models for skybox or panoramic images employed in virtual tours. It utilizes user-inputted text prompts to guide the aesthetic of the synthesized scene or modify the partial region or objects, thereby facilitating the creation of diverse and tailor-made virtual environments.\nUnlike conventional methods that treat 3D scenes as disjointed assemblies of multi-view images, MetaDecorator emphasizes the joint optimization of the entire scene to improve the immersive user experience. To do that, MetaDecorator adopts image-based 3D reconstruction with geometry constraints by fully utilizing the high quality RGB images and camera poses available at virtual tours. The Lidar data would be used to improve the reconstructed quality if the virtual tour is obtained by advanced sensors. To enable easy integration with VR applications, MetaDecorator outputs polygonal meshes which also facilitate the render speed in edge computation, such as VR devices. With this high extensibility, MetaDecorator can incorporate LLMs to better understand user preferences and interact with the environment in more sophisticated ways, such as automatically furnishing virtual spaces with user prompts. Additionally, integrating haptic feedback into these applications can further enhance the user's sense of presence within the virtual tours."}, {"title": "CASE STUDY: GENERATING IMMERSIVE VIRTUAL TOURS", "content": "Building on the framework outlined earlier, we present a case study demonstrating a pipeline for enhancing indoor panoramic images with user-specified elements and embedding for metaverse applications, allowing for the personalization of virtual tours. The pipeline follows two main stages as shown in : indoor panoramic image decoration and panoramic NeRF rendering. In the first stage, skybox images in the virtual tour applications are converted to panoramic images. Then the panoramic images are partially or fully decorated based on user-provided text prompts using stable diffusion In the second stage, a NeRF model is used to represent a 360\u00b0 view of the decorated image into 3D spaces, which is subsequently converted into a 3D mesh object. This mesh object is optimized for efficient rendering, facilitating its seamless integration into metaverse applications."}, {"title": "MetaDecorator Keeps Geometric & Semantic Consistency", "content": "To create a more immersive and realistic decorating experience, MetaDecorator ensures geometric and semantic consistency throughout the 2D content generation process.\nSpecifically, the 2D content generation begins by taking input from existing virtual tour 360\u00b0 images and user inputs to generate decorated images. The 360\u00b0 images are first converted into seamless panoramic representations from skybox images, ensuring style consistency during the decoration process. Depth, edge, and instance segmentation information are then extracted from the panoramic image, which is crucial for maintaining geometric and semantic consistency in the decorated images. When the user provides a prompt, the system attempts to identify a specific region for decoration based on the semantic segmentation results. If a specific region cannot be identified, the entire image will be decorated. Additionally, users have the option to input a style image as a reference, which enhances control over the artistic outcome by influencing the overall aesthetic direction of the final decorated image.\nThe 2D content generation process utilizes the inputs mentioned above to create new decorated images that harmoniously blend user-specified decorative elements, artistic style, and the underlying geometric structure of the scene. This is achieved by employing a stable diffusion model in conjunction with ControlNet ControlNet provides geometric guidance by analyzing depth and edge information to understand the scene's structure. Throughout the iterative refinement process, ControlNet generates intermediate outputs at different resolutions, ensuring that the decorations seamlessly integrate with the existing geometry. The diffusion model incorporates text and style embeddings derived from the user's prompt and reference images, guiding the integration of these elements into the final image while adhering to geometric constraints provided by ControlNet.\nThe 2D content generation is an iterative process that allows users to gradually adjust the image until it meets their needs. For example, a user can first apply a specific style to the entire scene based on a reference image and then interactively decorate individual elements, such as a sofa, by specifying styles or products based on prompts or additional references. The example of decorated images generated with this process is shown in ."}, {"title": "MetaDecorator Supports Metaverse Applications", "content": "Once the decorated panoramic image is obtained, we propose the Depth Prior and Constraint Panoramic NeRF (DP-NeRF) to reconstruct the 3D space, creating a seamless and immersive experience. DP-NeRF integrates the depth image generated in the previous step with the decorated RGB image to further constrain geometric consistency during the 3D reconstruction process. Additionally, it leverages the depth information to optimize the training process, enhancing both efficiency and quality. The details of this optimization process are discussed in the next section.\nThough DP-NeRF marks a significant advancement in image-based 3D reconstruction, their implicit volumetric representations differ from widely used polygonal meshes and lack support from common 3D software packages. To overcome this, a coarse to fine two steps approach to generate the high quality mesh. The coarse geometry is first generated by using the marching cubes algorithm which is applied to the ray tracing results derived from the NeRF model. An iterative surface refinement process then adjusts vertex positions and face density based on re-projected rendering errors, refining both appearance and geometry We finally convert the model into the high quality mesh with texture images for real-time rendering. As a result, this method not only preserves rendering quality and performance for green Al, but also facilitates easy integration with common 3D software and metaverse applications."}, {"title": "ENHANCEMENT: TRAINING EFFICIENTLY IMPROVEMENT FOR GREEN AI", "content": "While NeRF is one of the leading approaches for high-quality image-based 3D reconstruction, it is notably time-intensive, often requiring several hours or even an entire day to complete the training for a single decorated panoramic image. To enhance the efficiency of NeRF training for green Al and improve the user experience, we propose DP-NeRF as illustrated in . Traditional NeRF training for a single 360\u00b0 image involves simulating ray tracing for panoramic images using a camera distortion matrix. However, this approach results in many rays tracing into empty space, which significantly slows down the training process. To accelerate the training process, we first establish an occupancy grid based on the method in where the occupancy of each grid cell is initialized using depth priors obtained from previous steps (illustrated in ). The occupancy of each grid cell is determined by how close the cell is to the nearest 3D point and the viewpoint. If a grid cell is close to a 3D point, it will have a higher occupancy, while a cell farther away will have lower occupancy. Additionally, the influence of the viewpoint on this occupancy is adjusted by a weight factor, which typically falls within a small range.\nAfter constructing the occupancy grid using depth priors, the NeRF model is trained from multiple simulated viewpoints, following the strategy outlined in During both the training and inference phases, only grid cells with high occupancy value are considered for ray tracing, and the occupancy values are dynamically updated throughout the training process. This approach significantly increases the number of effective training points, thereby accelerating both training and inference.\nTo further enhance training efficiency, we apply constraints on both the RGB and depth outputs (shown in ), which introduces stronger depth control and directly informs the NeRF training. The use of depth constraints not only improves the accuracy of the scene reconstruction but also reduces the likelihood of artifacts, leading to a more robust and reliable model."}, {"title": "FUTURE IMMERSIVE EXPERIENCES", "content": "Introducing LLMs for Better User Interaction\nThe next frontier in creating immersive user experiences, particularly in AR/VR applications, lies in enabling natural interaction with multi-modal objects. LLMs have revolutionized user experiences in web and mobile applications, making tasks like searching more intuitive and efficient. With thoughtfully designed LLMs, we can transform the quality of generated environments to precisely align with user needs. Moreover, LLMs can unlock new dimensions of interactivity, such as dynamically retrieving 3D models from database or even generating new models based on user prompts. These models can then be seamlessly embedded into the virtual environment, ushering in a new era of customization, realism, and immersive user experiences. Imagine virtual tours where every element can be tailored in real-time, creating a truly engaging and personalized adventure."}, {"title": "Adding Haptic Textures for Immersive Experiences", "content": "Tactile sensation is a critical sensation for users when delving into an immersive virtual environment. Specifically, in the reconstructed 3D environments, different objects (such as walls, windows, etc.) and furniture (e.g., sofas, tables, chairs) have various textures when touched, leading to the haptic texture design. The intuition is that when we slide our fingers on the surface, we basically feel the spatial characteristics of the objects by mechanoreceptors (thousands of sensory receptors detecting stimuli such as touch, pressure, and vibration). By simplification, the principle is that if the design the haptic vibration pattern (such as haptic tones in metaphor [15]) is carefully done, by considering the aforementioned compelling spatial tactile characteristics, we are able to design different haptic textures for the common objects and furniture in virtual decorated environments, including bricks, wood, tiles, etc."}]}