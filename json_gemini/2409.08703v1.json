{"title": "NeSHFS: Neighborhood Search with Heuristic-based Feature Selection for Click-Through Rate Prediction", "authors": ["Dogukan Aksu", "Ismail Hakki Toroslu", "Hasan Davulcu"], "abstract": "Click-through-rate (CTR) prediction plays an important role in online advertising and ad recommender systems. In the past decade, maximizing CTR has been the main focus of model development and solution creation. Therefore, researchers and practitioners have proposed various models and solutions to enhance the effectiveness of CTR prediction. Most of the existing literature focuses on capturing either implicit or explicit feature interactions. Although implicit interactions are successfully captured in some studies, explicit interactions present a challenge for achieving high CTR by extracting both low-order and high-order feature interactions. Unnecessary and irrelevant features may cause high computational time and low prediction performance. Furthermore, certain features may perform well with specific predictive models while under-performing with others. Also, feature distribution may fluctuate due to traffic variations. Most importantly, in live production environments, resources are limited, and the time for inference is just as crucial as training time. Because of all these reasons, feature selection is one of the most important factors in enhancing CTR prediction model performance. Simple filter-based feature selection algorithms do not perform well and they are not sufficient. An effective and efficient feature selection algorithm is needed to consistently filter the most", "sections": [{"title": "1. Introduction", "content": "The development of the Internet has provided various platforms for reaching potential customers, including search engines, e-commerce websites, mobile apps, and social media platforms. Consequently, online advertising has emerged as a dominant sector in advertising (Yang & Zhai, 2022).\nPredicting the probability of a user clicking on a specific ad is critical in online advertising. There are various performance metrics such as Click-Through-Rate (CTR) and Convergence-Rate (CVR) used to determine the relevance between ads and users.\nA lot of practitioners and researchers have explored various approaches and techniques to enhance Click-Through Rate (CTR) prediction over the past decades. Many of these successful methodologies leverage machine learning, utilizing historical click data to forecast future user behavior (Yang et al., 2024; Duan et al., 2024; Demsyn-Jones, 2024; Ma et al., 2024a; Wang et al., 2024). Consequently, the selection and quality of features play a pivotal role in determining the performance of these prediction models. Features serve as the building blocks upon which these models operate, influencing their ability to accurately capture patterns and relationships within the data. Therefore, understanding the relevance and impact of different features is crucial for optimizing the predictive power of CTR models."}, {"title": "3. Proposed Methodology", "content": "Identifying the optimal subset of features for achieving the highest AUC score requires evaluating all feature subsets, hence, it takes an exponential amount of time. Therefore, rather than looking for the global optimum solution, it is more practical to efficiently determine a local optimum feature set that generates an AUC score close to the best one. Our proposed method is a customized heuristic to explore a very small portion of all possible feature subsets in order to obtain strong local optima. This exploration is done by utilizing the significance scores of features and by defining neighborhood concepts among feature subsets based on the scores of features.\nIn the NeSHFS method, we first calculate feature scores using chi-square and ANOVA methods for sparse and dense features, respectively. Subsequently, these sparse and dense features are ranked based on their scores independently. Next, a certain number of features are removed from both feature sets, with the specific numbers determined according to the sizes of the sparse and dense feature sets. Following each removal, CTR prediction is conducted on a dataset containing the remaining features, utilizing models such as DeepFM and FiBiNet. This process iterates until no features remain.\nFollowing this global search, a refined search is conducted to explore potentially better feature sets. This involves selecting the top k (in our case, 3) feature sets that produced the highest AUC scores during the global search. After that, small neighborhoods of these sets are generated by iteratively adding or removing features one by one. These refined feature sets are then subjected to the same CTR prediction process. As a result, the feature set producing the highest AUC score from the final neighborhood search step is returned, which corresponds to a local optimum.\nThe architecture of the proposed NeSHFS is given in Fig. 1."}, {"title": "4. Evaluations and results", "content": "This section presents the outcomes of the proposed NeSHFS method, focusing on Click-Through Rate (CTR) prediction. Utilizing the DeepCTR framework, we evaluate the efficiency of our proposed method using benchmark datasets, including the Huawei Digix 2022 dataset, the Criteo dataset, and the Avazu dataset. Our primary evaluation metric employs the DeepFM model for CTR prediction. Additionally, we employ the FiBiNeT model to further validate our proposed method as another deep predictive model, specifically on the Huawei Digix 2022 dataset. Furthermore, to validate the efficiency of our feature selection technique, we compare the proposed solution with the Genetic Algorithm (GA).\nAll experiments are conducted on an NVIDIA GTX 950M GPU. A batch size of 256 is utilized, and approximately 1 million randomly selected unbalanced data points are employed due to hardware limitations. To evaluate the efficiency of the proposed method and predict Click-Through Rates (CTR), an 80/10/10 split is used for training, validation, and testing, respectively.\nTo prevent over-fitting, we use early stop, and its patience is set to three on validation loss. Other parameters, such as the number of hidden units in the Deep Neural Network (DNN), 11 regularization, 12 regularization, and dropout, were used with their default values.\nEach experiment was conducted five times, and the average results are presented in the result tables alongside the corresponding number of features. The numbers in parentheses within the number of features column represent the number of numerical and categorical features, respectively.\nDuring the experiments, i and \u012f values are set to 5 and 3, respectively. Furthermore, defined parameters and their values are summarized in Table 2."}, {"title": "5. Discussion", "content": "The proposed NeSHFS method has shown promising results in the context of Click-Through Rate (CTR) prediction across multiple datasets. Our evaluations, conducted using the DeepCTR framework and the DeepFM and FiBiNet models, highlight the method's effectiveness in both feature selection and model performance enhancement.\nThe NeSHFS method demonstrates a significant improvement in prediction accuracy, as evidenced by the AUC scores. For instance, on the Huawei Digix 2022 dataset, the AUC score increased from 0.77044 to 0.78838, a notable 2.33% improvement, while reducing the number of features from 25 to 5. This reduction also led to a decrease in training time from 515.40333 seconds to 378.45179 seconds, showcasing the method's efficiency in handling large datasets with fewer computational resources.\nWhile the Genetic Algorithm (GA) yielded slightly better AUC scores, its runtime was substantially longer, making it impractical for real-world applications. The GA required over 398,000 seconds for feature selection on the Digix dataset, compared to NeSHFS, which achieved faster results with less computational overhead. This demonstrates NeSHFS's advantage as a lightweight and robust feature selection technique.\nOur experiments revealed distinct patterns in feature importance across datasets. For example, in the Digix dataset, slot_id, C12, and device_id were the most critical categorical features, while u_refreshTimes and 13 were the most influential numerical features. This distinction underscores the necessity of tailored feature selection processes for different types of data, which NeSHFS effectively addresses through its dual-ranking system.\nThe general search process effectively identified high-performing feature subsets by iteratively removing the least important features. The subsequent neighborhood search further refined these subsets, leading to even higher AUC scores. This two-step approach proved beneficial in maximizing model performance and ensuring that the feature sets were optimized for each specific dataset."}, {"title": "6. Conclusions", "content": "In this paper, we introduced NeSHFS, a heuristic-based feature selection method. Our approach involves ranking numerical features by their ANOVA scores and categorical features by their chi-square scores. NeSHFS iteratively selects and removes an appropriate number of features based on their decreasing ranking scores, both from numerical and categorical feature sets. These selected feature subsets are then employed in CTR prediction using a chosen model and dataset. The feature sets yielding the highest prediction results undergo further refinement through a local search mechanism. This involves iteratively"}]}