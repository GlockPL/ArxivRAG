{"title": "Re-Attentional Controllable Video Diffusion Editing", "authors": ["Yuanzhi Wang", "Yong Li", "Mengyi Liu", "Xiaoya Zhang", "Xin Liu", "Zhen Cui", "Antoni B. Chan"], "abstract": "Editing videos with textual guidance has garnered popularity due to its streamlined process which mandates users to solely edit the text prompt corresponding to the source video. Recent studies have explored and exploited large-scale text-to-image diffusion models for text-guided video editing, resulting in remarkable video editing capabilities. However, they may still suffer from some limitations such as mislocated objects, incorrect number of objects. Therefore, the controllability of video editing remains a formidable challenge. In this paper, we aim to challenge the above limitations by proposing a Re-Attentional Controllable Video Diffusion Editing (ReAtCo) method. Specially, to align the spatial placement of the target objects with the edited text prompt in a training-free manner, we propose a Re-Attentional Diffusion (RAD) to refocus the cross-attention activation responses between the edited text prompt and the target video during the denoising stage, resulting in a spatially location-aligned and semantically high-fidelity manipulated video. In particular, to faithfully preserve the invariant region content with less border artifacts, we propose an Invariant Region-guided Joint Sampling (IRJS) strategy to mitigate the intrinsic sampling errors w.r.t the invariant regions at each denoising timestep and constrain the generated content to be harmonized with the invariant region content. Experimental results verify that ReAtCo consistently improves the controllability of video diffusion editing and achieves superior video editing performance.", "sections": [{"title": "Introduction", "content": "Text-guided video editing is a specialized facet of content creation, which can edit video content, including but not limited to manipulating objects, changing backgrounds, by manipulating the text prompt describing the source video. This task exemplifies the potential to augment and polish content within diverse domains, encompassing advertising design, marketing, and social media content. Recently, diffusion-based generative paradigm (Ho, Jain, and Abbeel 2020) has shown astonishing text-to-image (T2I) (Rombach et al. 2022; Saharia et al. 2022) and text-to-video (T2V) (Ho et al. 2022a; Blattmann et al. 2023) generation capabilities, which provides a great opportunity to manipulate video content via text guidance. To edit videos with low computational costs, some studies utilize large-scale pretrained T2I diffusion models, e.g., Stable Diffusion (Rombach et al. 2022) to develop various text-guided video editing methods (Wu et al. 2023a; Qi et al. 2023). The main idea of these methods is to flatten the temporal dimensionality of the source video and diffuse the flattened video into noise, and then the inverted noise is gradually denoised to the edited videos by the T2I-based video diffusion editing model under the condition of the edited text prompt. Moreover, due to the inherent absence of temporal awareness in T2I diffusion models, off-the-shelf methods tend to incorporate some additional modules or mechanisms to construct a well-designed video diffusion editing model, thus preserving the temporal consistency of edited videos. For example, Tune-A-Video (Wu et al. 2023a) incorporated the temporal attention modules and spatio-temporal attention modules into the T2I models for temporal coherence. FateZero (Qi et al. 2023) proposed a fusing attention mechanism to fuse the attention maps from the diffusion and generation process to facilitate motion consistency. Token-Flow (Geyer et al. 2024) designed a propagation mechanism to propagate a small set of edited features across frames.\nDespite the great success, the controllability of editing remains a formidable challenge when performing fine-grained manipulation of multiple foreground objects. As shown in Fig. 1, the results from the common method show mislocated objects (i.e., the jellyfish is above the goldfish which is not aligned with \"the jellyfish is to the left of the goldfish\") and incorrect number of objects (i.e., two goldfish and a jellyfish are generated which do not match \"A jellyfish and a goldfish\"). The essence behind this situation is the lack of spatial location awareness for the pretrained T2I models (Wu et al. 2023c,d). A question arises: Can we improve the controllability of video editing based on off-the-shelf methods?\nIn this paper, we aim to challenge the above limitations by proposing a Re-Attentional Controllable Video Diffusion Editing (ReAtCo) method. To efficiently control the spatial location of the edited objects aligned with the edited text prompts in a training-free manner, a Re-Attentional Diffusion (RAD) is proposed to refocus the cross-attention activation responses between the editied prompt and video content during the denoising stage, resulting in a spatially location-aligned and semantically high-fidelity target video. In addition, as each denoising timestep may lead to some sampling errors (Daras et al. 2024), the invariant region content that may exist during editing (e.g., the background in Fig. 1) is inevitably disrupted, ultimately resulting in a generated invariant region content that is far from the original ones. Therefore, we design an Invariant Region-guided Joint Sampling (IRJS) strategy to mitigate the sampling errors of the invariant region by injecting the original invariant region content into the denoising process, thus maintaining the invariant region information and constraining the generated content to be harmonized with the invariant region.\nIn contrast to prior works, our proposed ReAtCo could bring two benefits:1) ReAtCo can provide the ability for fine-grained manipulation of multiple foreground objects. As shown in Fig. 1, our ReAtCo can successfully edit \"two dolphins\" into \u201ca jellyfish and a goldfish\" while ensuring their spatial locations aligned with the target prompt (i.e., \"the jellyfish is to the left of the goldfish\u201d). 2) the invariant region content could be faithfully preserved and the generated content is harmonized with the invariant region. We can observe from Fig. 1 that the background region (i.e., the invariant region in this case) content is consistently preserved while editing the two foreground objects. In summary, the contributions of this work can be concluded as:\n\u2022 To improve the controllability of video editing, we propose a Re-Attentional Controllable Video Diffusion Editing (ReAtCo) method. ReAtCo can refocus the cross-attention activation responses by a well-designed RAD to control the spatial location of the edited objects aligned with the edited text prompt in a training-free manner.\n\u2022 To keep the consistency of the invariant region with less border artifacts maximally, we design an IRJS to mitigate the sampling errors of the invariant region at each denoising timestep and to constrain the generated content to be harmonized with the invariant region.\n\u2022 We perform extensive experiments and achieve superior or comparable results, demonstrating that our ReAtCo mitigates the limitations of existing state-of-the-arts, such as mislocated objects, incorrect number of objects."}, {"title": "Problem Description", "content": "Problem Let $V = (V_1, V_2, \\dots, V_m)$ denotes a source video that contains m video frames. P and P' denote the source prompt describing V and the edited prompt provided by the users, respectively. The goal of text-guided video editing is to generate a new video V' from source video V under the condition of the edited prompt P'. We illustrate an example:\n\u2022 Source: an initial video with a prompt \"Two dolphins are swimming in the blue ocean.\u201d\n\u2022 Target 1: output a video to change \"Two dolphins\" as \"Two goldfishes\".\nRecent state-of-the-art methods can excellently achieve the goal by modifying the prompt based on the pretrained text-to-image (T2I) diffusion models, such as \"Two goldfishes are swimming in the blue ocean.\" for Target 1. However, the fine-grained controllability of video editing remains a formidable challenge, e.g., to simply continue the above example (a failure for most existing methods):\n\u2022 Target 2: output a video to fine-grained manipulate \"Two dolphins\" by editing \"the left dolphin as a jellyfish\" and \"the right dolphin as a goldfish\u201d.\nThe reason behind this failure is that the employed base models (i.e., the pretrained T2I models) are typically trained on simple text descriptions, not including fine-grained spatial location descriptions between different objects (Wu et al. 2023c,d). In other words, these methods often lack spatial location awareness for controllable video editing. A question arises: Can we improve the fine-grained controllability of video editing with training-free mode? It is not necessary to rebuild a new training dataset with information-enriched long text descriptions and retrain a new model due to high resource requirements.\nIdea The edited video could be partitioned into two parts: changed parts and the remaining unchanged part (e.g., background, which we denote as the invariant region). For these changed parts, the users more focus on those objects of interest, which could be decided by the input prompts P' and P. Suppose n objects need to be manipulated, denoted"}, {"title": "Method", "content": "{$O_1, \\dots, O_n$}, the remaining part except objects is denoted $\\overline{O}$. To bridge the latent semantic information from new prompt P' to the video as well as keep spatial location awareness, we use text-video cross-attention maps (between text and denoised videos) to associate the objects of interest, denoted $A_{o_i}(V^{(t)}, P')$ for object $O_i$, where $V^{(t)}$ is a noisy video at the t-th sampling step of the denoising process. For the unchanged part such as the background region, we expect to perform a diffusion-identical transformation $D_1$ to prevent the disruption of the unchanged region. Formally, our video sampling process (t to t-1 timestep) is defined as:\n$V^{(t-1)} \\leftarrow F(D_R(V^{(t)}, \\{A_{o_i}(V^{(t)},P')\\}_{i=1}^n, P'), D_I(V_{\\overline{o}}^{(t)}, P'))$,\n(1)\nwhere $D_R$ is the diffusion editor w.r.t the changeable objects, F is an integration operation, and $V_{\\overline{o}}^{(t)}$ is the unchanged part of $V^{(t)}$. Accordingly, there are two questions that need to be solved:\n\u2022 Spatial-aware diffusion editor $D_R$: the spatial alignment problem between object prompts and intermediate sampled video in a training-free manner. We propose a Re-Attentional Diffusion (RAD).\n\u2022 Diffusion-identical transformation $D_I$: recovery unchanged region with less border artifacts when integrating with new-generated object regions. We propose an Invariant Region-guided Joint Sampling (IRJS)."}, {"title": "Overview Framework", "content": "The overview framework of ReAtCo is illustrated in Fig. 2. Given a source video, we first utilize DDIM Inversion (Song, Meng, and Ermon 2021) for the video-to-noise inversion. Then, the inverted noise is gradually denoised to the edited video by an off-the-shelf video diffusion editing model. In practice, we use the classic Tune-A-Video (Wu et al. 2023a) as the video diffusion editing model to conduct experiments. To achieve controllable video editing, the user needs to specify the region of interest according to their edited text prompt, e.g., the regions of two dolphins in the case of Fig. 2. Subsequently, the region of interest can be transformed into a set of binary masks, which are injected into the denoising stage to refocus the cross-attention activation responses by our proposed RAD, resulting in a spatially location-aligned and semantically high-fidelity edited video. In addition, to prevent the disruption of the invariant region with less border artifacts, we propose an IRJS strategy that mitigates the sampling errors of the invariant region to maintain the original invariant content and allows the generated content to be harmonized with the invariant region."}, {"title": "Re-Attentional Diffusion", "content": "Reviewing the mainstream video diffusion editing models (Wu et al. 2023a; Qi et al. 2023; Chai et al. 2023; Wang et al. 2024), where the interaction between the textual semantic space and the pixel space occurs in the cross-attention layers of the pretrained T2I model such as Stable Diffusion (Rombach et al. 2022). That means that each video frame is computed the cross-attention maps with the text embedding, thereby bridging the relationship between text and video. Reviewing the computation of cross-attention maps, taking the i-th video frame as an example, and assume that we obtain the noisy video frame feature $X_i^{(t)}$ at denoising timestep t. $X_i^{(t)}$ is multiplied by the learnable parameter $W_Q$ to obtain Query $Q_i^{(t)} = W_Q X_i^{(t)} \\in \\mathbb{R}^{H \\times W \\times C}$, where H, W, and C indicate the height, width, and the channel dimensionality. The input word embedding E is multiplied by the learnable parameter $W_K$ to generate Key $K = W_K E \\in \\mathbb{R}^{L \\times C}$, where L is the number of text tokens. With $Q_i^{(t)}$ and K, the cross-attention maps $A_i^{(t)}$ of the i-th frame at denoising timestep t can be computed as:\n$A_i^{(t)} = \\text{Softmax}(Q_i^{(t)}K^T/\\sqrt{d}) \\in \\mathbb{R}^{L \\times H \\times W}$.\n(2)\nFrom the above, $A_i^{(t)}$ is a tensor with the size of L\u00d7H\u00d7W, which means that each word is associated with a H\u00d7W pixel space cross-attention map, the values inside represent the relevance of the word to the pixel space. At a high level, the high response region in the cross-attention map associated with each word is equivalent to the region of generating word concept in the video frames, i.e., the higher the response, the more the word concept is being attended to in that region, and the content generated in that region is more aligned with word concept.\nInspired by the above phenomenon and facts, therefore, by modifying the pixel space cross-attention map corresponding to the word of interest in $A_i^{(t)}$, we could constrain the pixel region in which the word concept is generated. Taking Fig. 2 as an example, the user can first specify the two regions for the left and right dolphins from the source video (specify manually or automatically using the object detector), and then two regions can be transformed into two sets of binary masks $M_1 = \\{M_1^1, M_1^2, \\dots, M_1^m\\}$ and $M_2 = \\{M_2^1, M_2^2, \\dots, M_2^m\\}$. In this case, the ultimate goal is to edit the content of $M_1$ to a jellyfish and the content of $M_2$ to a goldfish. Thus, the words of interest are \"jellyfish\" and \"goldfish\u201d (the indexes of words are $I = \\{2,5\\}$), and we can modify the 2-nd and 5-th cross-attention maps along the L dimensionality of $A_i^{(t)}$ to maximize the attention response in the $M_1$ and $M_2$ regions, respectively. Once the cross-attention maps of all video frames are carefully modified, we can obtain a spatially location-aligned and semantically high-fidelity target video. For modifying cross-attention maps, a simple way is to modify all responses inside the object regions to 1 and outside the object regions to 0, but such a straightforward way may collapse the denoising process, potentially leading to a collapse of video fidelity.\nTherefore, we propose a Re-Attentional Diffusion (RAD) that contains an inner-region of object constraint and an outer-region of object constraint, over the target cross-attention maps to gradually update the noisy video sample at arbitrary denoising timestep t such that the spatial location of edited objects will be aligned with the target regions. Inner-Region of Object Constraint. To ensure the edited objects approach the user-specified regions, an intuitive objective is to ensure that high responses of cross-attention maps are in the target regions. Thus, we can build the inner-region of object constraint $L_{IR}^{(t)}$ at denoising timestep t:\n$L_{i_{IR}}^{(t)} = 1 - \\frac{1}{K \\times m} \\sum_{i=1}^m \\sum_{k=1}^K \\text{top}_k(A_i^{(t)} \\times M_i^j, K)$,\n(3)\n$L_{IR}^{(t)} = \\sum_{j \\in I} L_{i_{IR}}^{(t)}$,\n(4)\nwhere $L_{i_{IR}}^{(t)}$ denotes the constraint corresponding to word index $j \\in I$. $A_i^{(t)}$ denotes the a cross-attention map corresponding to word index j in the i-th video frame at denoising timestep t, where $A_i^{(t)} \\in A^{(t)}$ and $A^{(t)} = \\{A_1^{(t)}, A_2^{(t)}, \\dots, A_m^{(t)}\\}$ is a set of cross-attention maps"}, {"title": "Invariant Region-guided Joint Sampling", "content": "for word index j in m video frames. M denotes the target region mask of the word concept corresponding to word index j in i-th video frame. $top_k(\\cdot, K)$ represents that K elements with the highest response would be selected, which can reduce the sensitivity of the model to the masks (i.e., no precise masks are required). In the experiments, K is set as 20% of the number of the mask regions so that K is adaptively set according to the size of the mask.\nOuter-Region of Object Constraint. The Inner-region of object constraint can control the edited object to appear inside the mask region, but it cannot ensure that the edited object is not synthesized outside the mask region. To mitigate the above issue, we further build a outer-region of object constraint $L_{OR}^{(t)}$ at denoising timestep t:\n$L_{i_{OR}}^{(t)} = \\frac{1}{K \\times m} \\sum_{i=1}^m \\sum_{k=1}^K \\text{top}_k(A_i^{(t)}\\times(1-M_i^j), K)$,\n(5)\n$L_{OR}^{(t)} = \\sum_{j \\in I} L_{i_{OR}}^{(t)}$.\n(6)\nIntuitively, $L_{OR}^{(t)}$ aims to minimize the activation responses of cross-attention maps out of the mask region, so that $L_{IR}^{(t)}$ and $L_{OR}^{(t)}$ constrain the cross-attention maps in a complementary manner.\nObjective Optimization. We integrate the above constraints to reach the final RAD objective at denoising timestep t: $L^{(t)} = L_{IR}^{(t)} + L_{OR}^{(t)}$. Then, the noisy video sample $X^{(t)}$ could be updated with a step size of $\\alpha_t$ as:\n$X^{(t)} \\leftarrow X'^{(t)} = x^{(t)} - \\alpha_t \\nabla L^{(t)}$,\n(7)\nwhere $\\alpha_t$ decays linearly at each denoising timestep. With the above constraints, $X^{(t)}$ at each timestep gradually moves toward the direction of generating high response attention in the given mask regions, thereby editing the target objects in the user-specified regions.\nThe proposed RAD can refocus the cross-attention activation responses to control the editing region. However, we observe that when the user merely wants to edit foreground objects or edit partial foreground objects, e.g., in the case of Fig. 3, two dolphins need to be edited and the background region is the remaining invariant region, the generated invariant region content is often inconsistent with the original invariant region content. As shown in Fig. 3 (b), we can observe that although the edited frame is well-aligned with the edited prompt due to the nice property of RAD, the background region is inconsistent with the one of the source video frame (i.e., Fig. 3 (a)). This is because each denoising timestep leads to some sampling errors, and the accumulated errors from all timesteps eventually result in a generated background region that is far from the original background region. From the user's perspective, we would like to keep the original invariant background information when manipulating foreground objects. To preserve the content of the invariant region during the editing process, a straightforward idea is to copy the corresponding content from the source video directly into the target video, as shown in Fig. 3 (c). Intuitively, the object region is not harmonized with the background region, resulting in obvious border artifacts.\n        To mitigate the above issues, we propose an Invariant Region-guided Joint Sampling (IRJS) strategy to mitigate sampling errors of the invariant region by injecting the original invariant region content into the denoising stage and to constrain the generated content to be harmonized with the original invariant region content. The framework of IRJS is illustrated in Fig. 4, where we take the timestept to t-1 as an example. For the vanilla sampling strategy (Ho, Jain, and Abbeel 2020), the noisy video sample $X^{(t)}$ at timestep t could be denoised into a noisy sample $X^{(t-1)}$ at timestep t-1 by a video diffusion editing model, but it may disrupt the information of the invariant region. The goal of IRJS is to mitigate the sampling error at each timestep by injecting the invariant region of the diffused source video sample into $X^{(t-1)}$. Specifically, the source video V is first diffused into a noisy sample $V^{(t-1)}$ at timestep t-1 according to predefined diffusion noise scheduler (Ho, Jain, and Abbeel 2020). Then, we use the object masks M (containing all object regions) and invariant region masks 1-M to extract the object region of $X^{(t-1)}$ and invariant region of $V^{(t-1)}$, respectively. Finally, the extracted regions are added to obtain a noisy sample $X^{(t-1)}$:\n$X^{(t-1)} = x^{(t-1)} \\times M + V^{(t-1)} \\times (1-M)$,\n(8)\nwhere $X^{(t \u22121)} \\sim N(\\mu_{\\theta}(X^{(t)},t), \\Sigma_{\\theta}(X^{(t)}, t))$ and $V^{(t \u22121)} \\sim N(\\sqrt{\\bar{\\alpha}_t}v^{(0)}, (1 \u2013 \\bar{\\alpha}_t)I)$. Concretely, $\\mu_{\\theta}(X^{(t)},t)$ and $\\Sigma_{\\theta}(X^{(t)}, t)$ are the predicted parameters of Gaussian transition distribution in the sampling (i.e., denoising) process, and $\\bar{\\alpha}_t$ is the total noise variance in the diffusion process predefined by (Ho, Jain, and Abbeel 2020). Further, when the video diffusion editing model is well-trained, then"}, {"title": "Experiments", "content": "$\\{O_1, \\dots, O_n\\}$ for word index j in m video frames. typically, given a source prompt and an edited prompt such\nWe conduct experiments on the text-guided video editing dataset LOVEU-TGVE-2023 (Wu et al. 2023b), the video samples used in (Chai et al. 2023), and the video samples from (Videvo 2024). Each video has 4 different edited prompts for evaluation. For specifying the object regions, we consider enabling the user to provide it in the possibly simplest way, i.e., bounding boxes. We consider three standard evaluation metrics that are proposed in the (Wu et al. 2023b) to measure the quality of edited videos. Frame Consistency is to measure the temporal consistency in frames by computing CLIP image embeddings on all frames of output video and reporting the average cosine similarity between all pairs of video frames. Textual Alignment is to measure the textual faithfulness of the edited video by computing the average CLIP score between all frames of the output video and the corresponding edited prompt. PickScore (Kirstain et al. 2023) is to measure human preference for T2I models. We compute the average PickScore in all frames of the output video. Furthermore, to measure the spatial location relationships between objects, we introduce the VISOR (Gokhale et al. 2022) that evaluates the spatial relationships (including left, right, above, below) in T2I generation. We compute the average VISOR in all frames of the output videos.\nWe compare our ReAtCo with the current state-of-the-arts, including the pioneer in efficient T2I-based video diffusion editing Tune-A-Video (Wu et al. 2023a), the fusing attention mechanism-based method FateZero (Qi et al. 2023), the atlas model-based method StableVideo (Chai et al. 2023), the dual-Unet architecture-based method TCVE (Wang et al. 2024), and the propagation mechanism-based method TokenFlow (Geyer et al. 2024). Below, we analyze quantitative and qualitative experiments."}, {"title": "Ablation Studies", "content": "Quantitative analysis. We evaluate the effects of the key components in ReAtCo, including RAD and IRJS. The results are reported in Tab. 2, we conclude the conclusions as: 1) Editing videos with RAD is effective, this is because RAD can empower the video diffusion editing model to perceive the spatial location of the foreground objects, thus improving the controllability and performance of video editing. Further, IRJS can bring some performance improvement by maintaining information in the invariant region and constraining the generated content to be harmonized with the invariant region. 2) Combining RAD with IRJS brings further benefits, which proves that editing objects while maintaining invariant region content is feasible and effective.\nWe take the \"jellyfish\" in the first sample in Fig. 5 as an example to visualize the cross-attention maps during the denoising process. Fig. 6 shows the visualization of cross-attention maps associated with the word \"jellyfish\" from w/ RAD and w/o RAD, we can observe that the cross-attention responses in the initial denoising timestep (i.e., denoising timestep is 1000) are all in an irregular state. As the denoising timestep decreases, the cross-attention responses gradually focus on a region. In particular, for w/o RAD, the focused region of cross-attention responses gradually deviates from the user-specified region of the jellyfish. In contrast, cross-attention responses from w/ RAD gradually focus on the user-specified region of the jellyfish, which supports the effectiveness of RAD in refocusing cross-attention activation responses."}, {"title": "Conclusion", "content": "In this paper, we have proposed a Re-Attentional Controllable Video Diffusion Editing (ReAtCo) method for text-guided video editing. ReAtCo is inspired by the observation that the controllability of existing editing methods is not enough, especially in the controllability of spatial location. To efficiently improve the controllability of video editing, ReAtCo refocuses the cross-attention activation responses by the well-designed RAD to control the spatial location of the edited objects aligned with the edited text prompts in a training-free manner. In particular, we design an IRJS to preserve the invariant region information during editing and to constrain the generated content to be harmonized with the invariant region. Extensive experiments demonstrate the effectiveness of our ReAtCo."}, {"title": "Appendix", "content": "More Implementation Details\nReAtCo considers the classic publicly available Tune-A-Video (Wu et al. 2023a) as the pretrained video diffusion editing model that adopts the Stable Diffusion v1.4 as the base model, and the number of denoising steps is fixed as 50. $\\alpha_t$ decays linearly from 1 to 0.5 during the denoising process. We operate the RAD on the cross-attention maps with a resolution of $\\frac{W}{32} \\times \\frac{W}{32}$ (Hand W are the height and width of the source video) due to the sufficient semantic information (Hertz et al. 2023). Since the pretrained video diffusion editing model is based on the Latent Diffusion Model paradigm (Rombach et al. 2022), the proposed RAD and IRJS are performed in the latent space of an autoencoder in practice.\nWe now evaluate the effects of IRJS in the invariant region to prove the effectiveness of maintaining (i.e., reconstructing) invariant region content. To quantify the performance of IRJS for reconstructing invariant region, we consider two evaluation metrics: Peak Signal to Noise Ratio (PSNR) and Learned Perceptual Image Patch Similarity (LPIPS) (Zhang et al. 2018). Tab. 3 reports the PSNR and LPIPS of Ours and Ours w/o IRJS, we can observe that the fidelity of the invariant region shows a severe degradation when the IRJS is removed. These results support the fact that our proposed IRJS effectively maintains the invariant region content during video editing.\nSelecting words of interest is an important step in our RAD. Typically, given a source prompt and an edited prompt such as \"Two dolphins are swimming in the blue ocean.\" and \"A jellyfish and a goldfish are swimming in the blue ocean.\", the words of interest could be easily selected as \"jellyfish\" and \"goldfish\", which is enough to extract the corresponding cross-attention maps for RAD. However, in some cases, the user is interested in controlling the objects in the form of compound nouns. For example, given a source prompt and an edited prompt, i.e., \u201cA woman is playing with a cat on a bed.\" and \u201cA Wonder Woman is playing with a duck on a bed.\u201d, we can observe that the goal is to change \"woman\u201d to \u201cWonder Woman\" and \"cat\" to \"duck\". A question arises: how to perform RAD with two cross-attention maps for a single object? In the experiments, we found that a single word almost dominates the target semantic. As shown in Fig. 8, to control the synthesis of Wonder Woman, we only select \"Woman\" as a word of interest, which is enough for RAD to constrain the Wonder Woman within the object region.\nIn our ReAtCo, we need to track the gradient across the whole big model for attentional control, which inevitably increases GPU memory usage. For example, for a consumer-grade NVIDIA RTX 3090/4090 GPU, only 4 video frames can be processed simultaneously. A naive way is to edit a complete video clip independently every 4 frames, but this straightforward way would definitely disrupt the temporal consistency of the generated video. To address this issue, we introduce the long video generation technology (Wang et al. 2023) that can mitigate the temporal inconsistency between multiple generated video clips. The basic principle of this technique is to generate multiple short video clips in a sliding-window manner and to expect duplicate video frames at the time nodes, thereby enhancing the temporal consistency between the generated short videos (more details could be found in (Wang et al. 2023) and its publicly available codes). We integrate this technology into our ReAtCo to form a resource-friendly ReAtCo version.\nLimited by the generation capability of the backbone model we used (i.e., Tune-A-Video (Wu et al. 2023a)), some edited results may have some temporal unsmoothness. This limitation could be mitigated by replacing a more powerful backbone model (e.g., if the Sora\u00b9 could be open-sourced) due to the fact that our ReAtCo is a controllable video diffusion editing framework in which the base video model could be used in a plug-and-play manner.\nThe advancement of text-guided video editing will ease the creative efforts of artists and designers, while also causing a risk of misinformation, leading to permanent damage to the reliability of videos. However, it is possible to train a classifier to distinguish the real and ReAtCo-edited videos according to the texture features."}]}