{"title": "Robust Vehicle Localization and Tracking in Rain using Street Maps", "authors": ["Yu Xiang Tan", "Malika Meghjani"], "abstract": "Abstract-GPS-based vehicle localization and tracking suf-fers from unstable positional information commonly experi-enced in tunnel segments and in dense urban areas. Also, bothVisual Odometry (VO) and Visual Inertial Odometry (VIO) aresusceptible to adverse weather conditions that causes occlusionsor blur on the visual input. In this paper, we propose anovel approach for vehicle localization that uses street networkbased map information to correct drifting odometry estimatesand intermittent GPS measurements especially, in adversarialscenarios such as driving in rain and tunnels. Specifically,our approach is a flexible fusion algorithm that integratesintermittent GPS, drifting IMU and VO estimates togetherwith 2D map information for robust vehicle localization andtracking. We refer to our approach as Map-Fusion. We robustlyevaluate our proposed approach on four geographically datasets from different countries ranging across clear and rainweather conditions. These datasets also include challengingvisual segments in tunnels and underpasses. We show thatwith the integration of the map information, our Map-Fusionalgorithm reduces the error of the state-of-the-art VO and VIOapproaches across all datasets. We also validate our proposedalgorithm in a real-world environment and in real-time ona hardware constrained mobile robot. Map-Fusion achieved2.46m error in clear weather and 6.05m error in rain weatherfor a 150m route.", "sections": [{"title": "I. INTRODUCTION", "content": "Robust localization is required for real-time fleet monitor-ing or traffic monitoring to operate seamlessly in intermittentGPS and all weather conditions. It is important to haveaccurate and robust vehicle tracking system as missingtraffic information could negatively impact downstream taskssuch as traffic management in smart cities or travel timeprediction.\nConsider a scenario where a vehicle needs to be trackedin an adversarial setting of heavy rain and multiple tunnelsegments. The tunnel segments hinder GPS informationwhile the raindrops remain on the camera lenses (even undershelter) compromising the visual input for Visual Odometry(VO) and Visual Inertial Odometry (VIO). Specifically, theraindrops cause occlusions and lens flare creating additionalvisual artefacts [1]. Both of which causes VO and VIOalgorithms to drift or delocalize [2]. In such scenarios,only the sensors which are not affected by external factorscould be relied. These include, the Inertial MeasurementUnit (IMU). However, IMU drifts easily without correctionespecially in a long segment. This drift could be correctedif prior knowledge on the shape of the route is known. Inour proposed algorithm, the route information is used to help"}, {"title": "II. RELATED WORK", "content": "A. Map-based Localization\nMany existing work utilize map information to performlocalization [4]\u2013[7]. However, the map information usuallycomes from HD maps which could be expensive to obtain.Both of which could be used in conjunction with mapinformation.\nBrubaker et al. proposed a map-based global localizationapproach that utilizes OSM and Visual Odometry measure-ments [6]. They converted the OSM road network into agraph representation and used the VO measurements tosearch for the most likely lane the vehicle is on in thecity-wide network. Although it is able to localize with anaccuracy of up to 3m in the KITTI dataset, it is also heavilyreliant on the VO measurements. If the VO measurementsstarts to drift, the search for the most probable lane wouldbe affected. In our work, instead of inferring the lanefrom VO measurements, we use GPS during initializationto identify the road the vehicle starts from. Then, we usemap information to correct the VO measurements if it startsto drift. Ma et al. [4] and Guo et al. [5] both uses HD mapto perform localization. Their approaches involve identifyingsymantic objects in the scene such as road signs and trafficlights and matching with the position in the HD map. Thisrequires highly accurate positioning of semantic objects andcould be expensive to obtain. Similar to our previous work[8], which uses road contextual information to assist inperforming intent and driving style prediction, we propose to use road contextual information to improve the robustnessof localization estimates especially in rain scenarios."}, {"title": "B. Sensor Fusion Approaches", "content": "We use GTSAM [9] for sensor fusion in our localizationalgorithm. GTSAM is a factor graph based sensor fusionapproach where each sensor measurement has an associateduncertainty represented by a Gaussian distribution and thepose estimates are derived from maximizing the posteriorprobability of the unknown poses. The accuracy of eachsensor's measurement is determined by the uncertainty givenin the sensor's reading. A sensor fusion approach allowsfor flexibility in its usage as the user could always includeadditional sensors and the factor graph would still be able tosolve for the optimal pose estimates. Another sensor fusionapproach is [10] which proposed an Extended Kalman Filter(EKF) based sensor fusion between VO, IMU and GPS.Their approach was evaluated in GPS degraded urban areas,where GPS signals were lost for up to 4 minutes. This issimilar to the problem of intermittent GPS that we explore,however, we also investigate adverse weather conditions suchas rain. The difference in our approach is that we introducemap information to reduce drifts from compromised visual"}, {"title": "C. Localization in Adverse Weather", "content": "Other approaches of robust localization relies on robustsensors such as Radar and LiDAR sensors that are moreresistant to adverse weather conditions [14]\u2013[16]. Hong etal. proposed a full SLAM system using only 3D Radar andshowed that they were able to localize in both rain andfog [14]. Both Vizzo et al. and Tuna et al. uses LiDAR toperform robust localization [15], [16]. Vizzo et al. focused onimproving robustness across different platforms [15] whileTuna et al. improved robustness of localization in featurelessenvironments such as underground mines and construction sites [16]. A constraint with these approaches are that itrequires either Radar or LiDAR sensors which could beexpensive and bulky for tracking purposes. In our work, wefocus on VO estimates for a lighter and more affordablesolution to localization and tracking.\nVisual Odometry and Visual Inertial Odometry are widelyexplored in literature, ranging from classical VO [17]\u2013[19] tomore recent learning based VO [20]\u2013[22]. It uses a streamof image data as input to localize the pose of the cameraby matching visual features between frames. Image datahowever is susceptible to image artefacts such as blurring andocclusions. Tan et al. evaluated various VO approaches onrainy urban datasets and found that VO alone is insufficientto localize a vehicle accurately in rain [2]. In this paper,we propose the use of map information to correct existingVO algorithms when it drifts to more accurately localize avehicle in rainy urban scenarios."}, {"title": "III. APPROACH", "content": "Map-Fusion is a sensor fusion approach that uses GPSfor initialization, VO or VIO for odometry estimates andmap information for correcting drift in odometry. We usefactor graph [9] for combining various sensor informationand correcting the localization estimates using the mapinformation. A factor graph was chosen as it could easilyintegrate the new map information and align all previousodometry estimates with the current estimate. An overviewof our proposed Map-Fusion algorithm is shown in Fig. 2."}, {"title": "A. Processing Map Information", "content": "Map information is first obtained from OSM using a web-based data filtering tool [23]. Only lane information wasextracted where each lane was treated as a 2 way road andsimilar to [6], this map was converted into a graph. Each"}, {"title": "B. Matching Vehicle Pose Estimate to Map", "content": "At each input image frame, a visual odometry estimate canbe provided from any VO algorithm. This estimate $t^{-1}V_t \\in SE(3)$, is the relative transform from the previous pose at time $t-1$ to the current pose at time t. The current pose $P_t \\in SE(3)$ is parameterized as follows:\n$P_t = [x, y, z, \\alpha, \\beta, \\phi]$  (1)\nwhere x, y, z represents the vehicle's position and $\\alpha, \\beta, \\Phi$represents the Euler angle representation of the vehicle'sorientation. The global frame corresponds to the East, Northand up direction while the relative frame corresponds to theforward, left and up direction. The closest pose on the map$R_t \\in SE(3)$ to the current pose at time t is defined asfollows:\n$R_t = [x, y, P_{t,z}, P_{t,\\alpha}, P_{t,\\beta}, ]$\n$R_{t,\\theta} = arctan(\\frac{R_{t,y} - R_{t-1,y}}{R_{t,x} - R_{t-1,x}})$  (2)\nwhere $R_{t,x}$ refers to the x component of $R_t$ and $P_{t,z}$ refersto the z component of $P_t$. Given the global position ofthe vehicle at the previous timestep, applying the currentodometry measurement provides an estimate of the vehicle'scurrent position.\n$P_t \\sim P_{t-1}  t^{-1}V_t$ (3)"}, {"title": "", "content": "In order to determine the closest point on the map to thecurrent positional estimate, a distance metric, D, combin-ing both Euclidean distance, E, and angular distance, A,was used. This metric is specified in Eq. 4. Since thereare multiple points with similar angles in the map, cosinesimilarity was not used and the angle itself was used tobeter differentiate between these similar points. A higherweight was given to the angular distance such that eachdegree difference corresponds to 1m euclidean distance error.This was to ensure that the matching is based primarily ondirection, such that, the turns could be better matched evenif the vehicle is localized slightly off the center of the road.\n$D=E+A$\n$A = (arccos(2(q_{P_t}, q_R)^2 - 1)) \\times \\frac{180}{\\pi}$ (4)\nwhere $q_{P_t}$ represents the quaternion representation of theorientation of pose $P_t$ and $q_R$ represents the quaternionrepresentation of the orientation of a pose on the map.The lateral distance L between $P_t$ and the closest point onthe map $R_t$ is calculated using Eq. 5. $T_{Ft}$ represents therelative transform from $R_t$ to $P_t$ and L is the translationperpendicular to the direction of the road.\n$T_{Ft} = R_t^{-1}P_t$\n$L = |T_{Ft,y}|$ (5)\nIf the lateral distance exceeds the road width (approx-imated to be 3m per lane), a Gaussian prior is added tothe factor graph to align the odometry estimate to the road.Since the position and orientation of the closest point isobtained from a 2D map, the roll, pitch and altitude are allset to the odometry estimate. The corresponding covarianceof the Gaussian prior for the roll, pitch and altitude is setto infinity to represent uncertainty as the 2D map does notprovide this information. For the yaw parameter, the standarddeviation of the Gaussian prior is empirically set as 10degrees to account for differences in the angle of turn at theintersections. While the Easting and Northing parameters are"}, {"title": "C. GPS Initialization and Scaling", "content": "In order to align the odometry from local coordinateframe to the global frame, a series of GPS coordinates arerequired. This GPS sequence was used to estimate bothvehicle heading and scale of the localization estimates. Theheading is estimated by taking the arc-tangent of consecutiveGPS coordinates similar to estimating heading of map posesin Eq. 2. In the process of determining the scale using GPS,three thresholds were used. One threshold sets a minimumdistance travelled by the vehicle before estimating scalewhich is represented as $m_d$ and another threshold sets theminimum average speed for the vehicle in km/h, denotedas $m_s$. These two thresholds are used together in Eq. 7to determine the number of GPS measurements $C_{gps}$ andnumber of VO measurements $C_{vo}$ required to travel thethreshold distance at the threshold speed. $f_{vo}$ and $f_{gps}$ refersto the frequency of VO and GPS measurements.\n$C_{gps} = [\\frac{m_d \\times 3.6 \\times f_{gps}}{m_s}]$\n$C_{vo} = round(\\frac{(C_{gps} \\times f_{vo})}{f_{gps}})$ (7)\nAt every timestep, the distance travelled by GPS measure-ments and the distance travelled by VO measurements areaccumulated. When the GPS distance exceeds the minimumdistance threshold and the number of GPS and VO measure-ments do not exceed $C_{gps}$ and $C_{vo}$ respectively, then thescale S is estimated using Eq. 8.\n$S = \\frac{\\sum_{t_{gps}}||d_{gps}||^2}{\\sum_{t_{vo}}||d_{vo}||^2} \\frac{t_{vo}}{t_{gps}}$ (8)\nwhere $d_{gps}$ and $d_{vo}$ represents the distance accumulated byGPS and VO measurements. $t_{gps}$ and $t_{vo}$ represents theduration in seconds to accumulate $d_{gps}$ and $d_{vo}$ respectively.The last threshold sets the number of times scale estimationis repeated to average out the errors. Median was used insteadof mean to filter outliers as we found that it was easy for"}, {"title": "IV. EXPERIMENTS", "content": "Our proposed Map-Fusion algorithm was extensivelytested on 4 datasets across both in clear and rain weatherscenarios and across multiple countries. We experimentedon the datasets with and without Map-Fusion for both a VOapproach and a VIO approach and showed that on average itimproved both the VO and VIO results. The parameters aretuned empirically and the exact values alongside the imple-mentation of the Map-Fusion algorithm is open-sourced\u00b9."}, {"title": "A. Datasets and Evaluation", "content": "The 4 datasets used are: (a) KITTI Dataset (for clearweather) [24], (b) Oxford Robotcar Dataset (for rain weather)[25], [26], (c) 4Seasons Dataset (for rain + tunnel) [27]and (d) an internal dataset collected in Singapore (for heavyrain). Fig. 3 shows sample images from each of the datasets.Only the rain sequences and 1 clear sequence for each routefrom Oxford Robotcar and 4Seasons Datasets are used. Thesequences from Oxford Robotcar are also cut short to matchthe shortest sequence such that the errors could be fairlycompared against each other. In the Oxford Robotcar Dataset,VIO was not evaluated as there are no raw IMU readingsprovided. For the KITTI Dataset, there were multiple timegaps in the raw IMU readings. As such, the IMU reading wasextrapolated using a rolling average of the past 50 readingsfor such gaps. In the 4Seasons dataset, the tunnel sequencewas cut short to right after the vehicle passes the tunnel toanalyze the localization error within the tunnel."}, {"title": "B. Experimental Setup", "content": "We evaluated our proposed Map-Fusion algorithm usinga VO and a VIO approach. We used DROID-SLAM [20]for the VO approach and ORB-SLAM3 [17] for the VIOapproach. Both DROID-SLAM and ORB-SLAM3 were cho-sen as they provide a stereo camera mode and have goodscale consistency throughout the route. This is also to eval-uate Map-Fusion on different types of odometry approacheswhere DROID-SLAM is a learning based approach whileORB-SLAM3 is a classical approach. In both approaches,the stereo option was used. For DROID-SLAM, the GPUmemory usage was restricted to 12GB to prevent out ofmemory errors for long sequences. Also, post-processedglobal optimization was disabled to ensure localization re-sults were obtained in real-time. For ORB-SLAM3, sincethe results varies significantly each run, the evaluation wascarried out 3 times for each sequence and the average isreported. If it de-localizes any 1 out of the 3 runs, it isreported as de-localized. For all approaches, de-localizationis defined when ATE errors exceed 20m. This threshold waschosen as it was the widest road observed in all the datasetsevaluated. Specifically for ORB-SLAM3, a de-localizationis also defined as when ORB-SLAM3 resets its local map.The VO and VIO algorithms were executed independentlyand were used as baseline algorithms for comparison withour proposed Map-Fusion algorithm. The VO and VIOalgorithms without Map-Fusion were initialized with GPSbut they do not use map information. This is to evaluatethe effects of the map information separately from the GPSscaling and pose initialization. For the KITTI and OxfordRobotcar datasets, we use GPS only for initialization. Usingthe initialized segment, our proposed approach is able torobustly keep track of the vehicle in light rain scenarios.However, in challenging scenarios such as the tunnel segmentin the 4Seasons dataset and heavy rain in the Singaporedataset, the visual input is severely compromised. As such,we introduce intermittent GPS measurements (every 5 sec-onds) to reduce drift."}, {"title": "", "content": "We used GTSAM [9] as the sensor fusion approach.GTSAM is a factor graph based sensor fusion approachwhich assumes that every measurement taken is Gaussianwith its own covariance matrix to define the uncertainty ofthe measurement. Since the VO approaches do not comewith a predicted covariance alongside its estimated pose,two covariance matrices were set for the odometry mea-surements. One covariance matrix was set to be highlyuncertain before initialization when scaling is not completeand the other with higher certainty after initialization whenscaling is completed. Similarly for the GPS measurement,the covariance matrix was set to a predefined value as thedataset does not provide covariance values. Although theodometry approaches provide camera frame absolute pose,the relative odometry pose was used in GTSAM to model"}, {"title": "C. Quantitative Results", "content": "Table I reports the ATE for the KITTI Dataset recordedin clear weather. Localization estimates with larger than20m error is considered to be delocalized and are markedwith an x. The delocalized sequence is thus not included inthe average. We observe that ORB-SLAM3 VIO performedworse compared to DROID-SLAM VO. This is likely due tomissing IMU information for certain sequences. Comparingwith and without Map-Fusion for DROID-SLAM VO andORB-SLAM3 VIO without loop closure, Map-Fusion onaverage improves the ATE by 0.61m and 2.67m, respectively.Also, Map-Fusion improves the ATE for ORB-SLAM3 VIOwith loop closure by 2.53m. Specifically, our proposed Map-Fusion algorithm, was useful in recovering de-localized se-quences caused by lateral drifts. An example is sequence 00where ORB-SLAM3 VIO (no loop closure) was de-localizeddue to lateral drifts and we successfully corrected it usingthe map information. Our approach however, is limited bythe longitudinal drift correction capabilities of the underlyingalgorithm. For example in sequence 01, which has a lack oftemporal visual features to determine longitudinal motion ofthe vehicle, a pure VO algorithm such as DROID-SLAM willfail. In contrast, a VIO algorithm such as ORB-SLAM3 isable to track the longitudinal motion of the vehicle. Thus,"}, {"title": "D. Qualitative Results", "content": "The qualitative results are presented in Fig. 4 and Fig. 5for DROID-SLAM VO and ORB-SLAM3 VIO approachesrespectively. Fig. 4 shows qualitatively the comparison withand without Map-Fusion for the sequences from the Ox-ford Robotcar Dataset. The significant drift caused by therain is vastly reduced with the integration of Map-Fusion.Similarly in the 4Seasons Dataset, although the sequencecity_loop_1_train is marked as delocalized, Map-Fusion wasable to significantly correct odometry estimates in both rainconditions and tunnel segments as shown in Fig. 5."}, {"title": "E. Real-World Results", "content": "We also validate our proposed algorithm in a real-worldenvironment and in real-time on a Clearpath Jackal robot.The setup is shown in Fig. 6(a) and the traversed routeis shown in Fig. 6(b). Due to the limited CPU capacity,we reduced image size and frame rate to trade localizationaccuracy for real-time localization. The qualitative resultsfor both ORB-SLAM3 VIO and Map-Fusion are shown inFig. 6(c) and Fig. 6(d). The main source of error came froman inaccurate scale estimation likely caused by the reducedimage size. We also show the more qualitative results in thislink2. Map-Fusion achieved 2.46m error in clear weatherand 6.05m error in rain weather for a 150m route. Thisset of experiments show that Map-Fusion could run in real-time on a hardware constrained vehicle and achieve decentlocalization accuracy in both clear and rain weather."}, {"title": "V. CONCLUSION", "content": "In this paper, we introduce Map-Fusion, a sensor-fusionbased localization approach which uses map information forcorrecting odometry estimates in adverse scenarios such as"}]}