{"title": "Self-supervised Benchmark Lottery on ImageNet: Do Marginal Improvements Translate to Improvements on Similar Datasets?", "authors": ["Utku Ozbulak", "Esla Timothy Anzaku", "Solha Kang", "Wesley De Neve", "Joris Vankerschaver"], "abstract": "Machine learning (ML) research strongly relies on benchmarks in order to determine the relative effectiveness of newly proposed models. Recently, a number of prominent research effort argued that a number of models that improve the state-of-the-art by a small margin tend to do so by winning what they call a \"benchmark lottery\". An important benchmark in the field of machine learning and computer vision is the ImageNet where newly proposed models are often showcased based on their performance on this dataset. Given the large number of self-supervised learning (SSL) frameworks that has been proposed in the past couple of years each coming with marginal improvements on the ImageNet dataset, in this work, we evaluate whether those marginal improvements on ImageNet translate to improvements on similar datasets or not. To do so, we investigate twelve popular SSL frameworks on five ImageNet variants and discover that models that seem to perform well on ImageNet may experience significant performance declines on similar datasets. Specifically, state-of-the-art frameworks such as DINO and Swav, which are praised for their performance, exhibit substantial drops in performance while MoCo and Barlow Twins displays comparatively good results. As a result, we argue that otherwise good and desirable properties of models remain hidden when benchmarking is only performed on the ImageNet validation set, making us call for more adequate benchmarking. To avoid the \"benchmark lottery\" on ImageNet and to ensure a fair benchmarking process, we investigate the usage of a unified metric that takes into account the performance of models on other ImageNet variant datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "Given a large amount of labeled data, deep neural networks (DNNs) were shown to find highly accurate solutions to complex vision problems [1]-[3]. Unfortunately, for many domains, the availability of labeled data is a commodity in short supply due to (1) expert knowledge required to create annotations, (2) the cost associated with labeling, and (3) po- tential inconsistencies in the labeling process [4]. Traditionally, transfer and few-shot learning methods have been the go-to methods when faced with a shortage of labeled data [5], [6]. In recent years, transfer learning in particular has become so popular that studies often employ pretrained models to overcome data shortages. As a result of the straightforward ap- plicability of transfer learning, most deep learning frameworks now come with a large catalogue of models [7] pretrained on ImageNet [8]. A novel methodology that also allows mitigating label shortage is self-supervised learning, where feature extractors are trained in a self-supervised manner on all available data, and without labels [9], [10]. Although this line of work is still in its infancy for supervised computer vision tasks, the results obtained with recent SSL frameworks show promise, achieving comparable results to models trained with supervised learning."}, {"title": "II. RELATED WORK", "content": "On the other hand, while transfer learning is convenient and easy to utilize, self-supervised learning is a slow and expensive process that requires a large amount of compute with backbones requiring a great number of epochs to train, coupled with extensive parameter search requirements for linear probing [10], [11]. Due to these compute requirements, comparing a collection of SSL frameworks to one another means training multiple models for weeks, if not months, with a typical 8-GPU setup if the dataset under consideration is at the scale of ImageNet. To make matters even more com- plicated, within the span of two years, more than a hundred unique SSL frameworks were proposed, each claiming state- of-the-art results on ImageNet [12]-[16]. For an overview of SSL frameworks and results, see [17], [18] and the references therein. Given the SSL training limitations outlined above, what is the best approach to select a particular SSL framework? The most straightforward answer to this question is to select that SSL framework that achieves the highest top-1 accuracy on ImageNet, since ImageNet is the de facto standard for SSL evaluation in the domain of computer vision. However, most of the recently proposed SSL frameworks only improve the ImageNet validation accuracy marginally (approximately by 1%) compared to their predecessor. In this context, a fair question to pose is \"Are these improvements achieved by chance, or are they thanks to enhancements in the SSL method?\" Recent research efforts on the topic of benchmarking and marginal improvements argue that a significant portion of the marginal improvements in DNNs over certain benchmarks arises from improper experimental settings, such as training without a validation set (i.e., using the test set for feedback) and the random initialization of networks [19], [20]. Unfor- tunately, the cost to train neural networks, especially in self- supervised settings makes it unfeasible to reproduce majority of the research since training times are measured with years on a single GPU [21]. Given the aforementioned limitations, in this paper, we propose the usage of ImageNet variants along with the Im- ageNet validation dataset to benchmark SSL models for a fairer and more appropriate evaluation. To do so, we perform large-scale experiments using twelve SSL models on five ImageNet variants. Whereas some of these variant datasets contain similar images to that of ImageNet, others contain images with unique traits, thus enabling a variety of analyses of the generalization power of SSL models. Based on our experiments, we can briefly summarize our novel observations as follows:"}, {"title": "A. Self-supervised Learning", "content": "The idea of self-supervision dates back to the works of [22], [23] where the goal of those works was to achieve an agree- ment on representations coming from differently transformed views of the same image. After three decades of research efforts, self-supervised training methods are now considered go-to approaches for a large number of use-cases such as image colorization [24], [25], image super resolution [26], inpainting [27], and geometric transformations [28]. Although DNNs trained in self-supervised fashion with such methods were useful in solving these problems, their usage for down- stream supervised tasks were shown to be ineffective [15]. Nevertheless, research efforts on self-supervised training for supervised classification have shown gradual improvements over the course of the last decade [29], [30] and recently, the work of [31] demonstrated that it is possible to train feature extractors on ImageNet in a self-supervised fashion using clustering techniques and achieve comparable results to supervised models. This approach not only inspired future studies which improved SSL training that relies on clustering approaches [15], [32], it also paved the way for contrastive [9], [11], [16] and distillation-based [10], [33]\u2013[35] methods. Before a DNN can be utilized in a traditional way for down- stream predictive tasks, its backbone (i.e., feature extractor) goes through a self-supervised training stage where a neural network backbone is trained in a self-supervised fashion so that it learns good representations. We now briefly detail the various SSL frameworks used in this study. Clustering - Early works in this space such as DeepC [31] and SeLa [32] train backbones with the cross entropy loss (CE). However, instead of using actual labels for the loss cal- culation, they make use of pseudo-labels that are determined by cluster assignments [36]. Different from DeepC and SeLa, Swav makes use of the following loss\n$Lsw := l(zi, qj) + l(zj, qi),$\nwhere the aim is to predict the code $(q{i,j})$ of a view from the representation of another view $(z{j,i})$ with $l(\u00b7)$ measuring the fit between the two. Contrastive -Moving towards the contrastive-based meth- ods, SimCLR [9] and MoCo [21] both make use of InfoNCE loss where this loss tries to pull together representations of"}, {"title": "III. DATASETS AND MODELS", "content": "In Table I, we provide an overview of the datasets used to measure the properties of ImageNet-trained SSL models in this study, detailing the image count, number of classes as well as image per class. In what follows, we provide brief description of these datasets. ImageNet - One of the most-used datasets to evaluate SSL training, ImageNet [8] has played a crucial role in measuring progress in the field of computer vision [1], [2], [55], [56]. The ImageNet training set contains more than a million images spread across 1,000 classes and the validation set contains 50 images per class, totalling up to 50,000 images. ImageNet ReaL-To the dismay of the computer vision community, a number of studies indicate that ImageNet vali- dation set has some issues with the labels, whereby a portion of the images suffer from having a single label per image, restrictive label proposals, and arbitrary class distinctions [57], [58]. To overcome this problem, [59] came up with a new, more robust labeling strategy that alleviates the aforemen- tioned issues and named the new set of labels as ReaL, where this new set of labels can contain up to 10 labels for a single image. In Figure 2c we provide example ReaL labels instead of the original ImageNet validation labels. Note that Real labels converts the ImageNet validation set from a single-label dataset to a multi-label dataset where having a prediction as any of the given ReaL classes is considered a correct one. Furthermore, ReaL also removes a number of images from the ImageNet validation for being ambiguous and not falling into any of the categories employed in ImageNet-1k. ImageNet v2-In a quest to evaluate the correctness of using ImageNet validation as a single source of model per- formance, [49] replicated the image collection methodology of the ImageNet and assembled a new validation set called ImageNet v2. This dataset comes with 10 images per class and contains 10,000 images in total and can be seen as in-distribution dataset that similar to the ImageNet validation. ImageNet Rendition-In order to understand the out-of- distribution generalization of DNNs trained on ImageNet and to analyze the impact of real-world distribution shifts such as changes in image style, blurriness, geographic location, and camera view, [47] collected ImageNet Rendition dataset which contains 30,000 images for a subset of 200 classes of ImageNet (see Fig 2a for a set of example images). ImageNet Sketch-Local receptive fields in DNNs are known to be extremely influential when making predictions to the point where DNNs sometimes ignore global features such as the shape [60]. In order to evaluate the local biases of DNNs trained on ImageNet, [46] released a sketch variant of ImageNet that is void of color and texture. This dataset contains approximately 50 images per class, totalling up to 50,889 images (see Fig 2b for a set of example images). ImageNet Adversarial - A recently discovered flaw of DNNs is their vulnerability to adversarial examples [61],"}, {"title": "B. Models", "content": "The SSL frameworks described in Section II often come with repositories which contain feature extractors pretrained on the ImageNet training set. For a faithful analysis, we take the parameters of ResNet-50 backbone networks as-is, without making any changes from the respective repositories of the SSL frameworks. Unfortunately, the aforementioned SSL backbones often do not contain linear layers. In order to enable straightforward inference, we undertake the task of individually training each of these frameworks on the ImageNet training set. During this training process, we adhere to the SGD training routines outlined in the respective papers for each SSL framework. For additional information on this subject, we direct the reader to the section that elaborates on \"Linear Evaluation\" in the corresponding papers of the SSL frameworks."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "In Table II, we present the top-1 accuracy of each SSL backbone on the ImageNet validation set (refer to the second column). We have sorted the models based on their perfor- mance on the ImageNet validation set and maintain this order throughout the paper for consistency. For the majority of the models, we successfully capture the reported top-1 accuracy in their respective benchmarks, and for those where we do not, the models we obtain have only minor discrepancies. In Table II, we also include the top-1 accuracy performance of models on five ImageNet variant datasets. For ease of comparing model performances relative to each other, we introduce Table III. In this table, we rank the models based on their initial positions on the ImageNet validation set as well as the ImageNet variants. To enhance clarity, we use a blue font to highlight better rankings (compared to the initial"}, {"title": "A. Performance on ImageNet Variants", "content": "ImageNet ReaL \u2013 When evaluating the models using ReaL labels, the newly acquired results are presented in Table II. In contrast to certain supervised models assessed in the study by [59], we did not observe a decline in accuracy for any of the SSL models. This emphasizes the robustness of SSL models, especially for those with lower initial accuracy, when subjected to evaluations with ReaL labels. Additionally, we note that the linear accuracy gains for SSL models tend to be more pronounced for less accurate models compared to their more accurate counterparts. Consequently, when assessed with ReaL labels, the disparity between the best- and worst-performing SSL models decreases from 8.7% (on the ImageNet validation set) to 7.4% (on ImageNet ReaL). ImageNet v2-In the work of [49], supervised models are reported to have a reduction in top-1 accuracy within the range of 11% to 14% when evaluated on ImageNet-v2. Likewise in Table II, we observe that SSL models also show identical drops in accuracy. Similar to the evaluation with ReaL labels, we observe a few rank-order differences in v2, indicating once again, that marginal improvements on ImageNet validation may be misleading when interpreted as overall improvements on model performance. ImageNet Rendition \u2013 When the evaluation is performed on the rendition images, we observe significant declines in accuracy across all models, accompanied by substantial changes in their rank-ordering. Notably, the two initially top- performing models, DINO and Swav, are replaced by MoCo and BYOL, with subsequent positions taken by Barlow and VicReg. Particularly noteworthy is the performance of MoCo, which stands out as clearly better than any other model, with the closest competitor showing approximately 4% less top-1 accuracy. Furthermore, the drop in the performance of SimCLR is so pronounced that the difference between the best- and worst-performing models for ImageNet Rendition is larger than that observed in ImageNet validation. This observed shift in rankings highlights the dynamic nature of model performance on various out-of-distribution datasets, emphasizing the importance of understanding the nuances in model behavior and benchmarking. ImageNet Sketch - Similar to the results obtained with Rendition, our findings indicate that improvements on Im- ageNet validation do not seamlessly translate into improve- ments on the Sketch dataset. Specifically, DINO and Swav exhibit larger declines in accuracy compared to other models, highlighting a consistent trend. Once again, MoCo secures the top position, boasting a remarkable 4.5% top-1 linear accuracy difference compared to the next-most-accurate SSL model. This reinforces MoCo's resilience and effectiveness across different datasets. ImageNet Adversarial - As it can be seen, natural adver- sarial adversarial examples appear to reduce the accuracy of almost all models down to single digits no matter the training routine. This consistent reduction in accuracy highlights the robustness challenges that exist across different models, sug- gesting that the vulnerability to (natural) adversarial examples is a pervasive issue that transcends specific SSL training routines."}, {"title": "B. Correlation of Performance", "content": "In Figure 3, we present a comparison of the top-1 accuracy of models for various ImageNet variants plotted against the top-1 accuracy obtained on the standard ImageNet validation set. For ImageNet-Real and v2, we observe a robust correlation (r = 0.99) between ImageNet validation set with only minor differences in rank orders, as indicated in Table II. This high correlation implies that models performing well on ImageNet validation tend to exhibit similar success on the ReaL and v2 datasets. Conversely, when examining Rendition, Sketch, and Ad- versarial the correlation notably diminishes to approximately r = 0.6. This substantial drop suggests that the accuracy achieved on ImageNet validation is not a reliable indicator of a model's performance in out-of-distribution settings. The lower correlation highlights the unique challenges posed by these variant datasets, emphasizing the importance of evaluating model performance in diverse scenarios to ensure robust generalization across different data distributions."}, {"title": "C. Aggregate Measures of Accuracy", "content": "Building upon the insights gained from the earlier observa- tions, we now investigate aggregate measures of accuracy in order to conduct a comprehensive performance assessment on ImageNet variants. To do so, we employ two distinct aggregate measures of accuracy, namely the weighted average and the geometric mean. For a given model, the weighted average, W, is the average performance across all ImageNet variants, where the accuracy on each dataset is weighted by the number of images in that dataset:\n$W = \\frac{\\sum_{i=1}^{k} N_{i}A_{i}}{\\sum_{i=1}^{k} N_{i}}$\nwith $N_i$ the number of images and $A_i$ the accuracy on the ith ImageNet variant. This metric takes into account performance on all datasets, but due to the weight factor poor performance on small datasets is not penalized as harshly as on large datasets. The choice of the number of images as a weighing factor for the weighted average is reminiscent of the construction of the fixed-effect model in meta-analysis [63], where the precision (i.e., the inverse of the variance) is used as a weighing factor. In fact, as the precision is proportional to the number of points in the dataset, the fixed-effect estimator for the aggregate effect agrees with our weighted average up to a constant multiplicative factor. This unlocks certain attractive properties that the weighted average is conjectured to have, such as the fact that its precision will be proportional to the sum of the precisions on the individual datasets. To make this link more robust would require quantifying the uncertainty in the accuracy on individual datasets, which we do not attempt in this work. The second metric that we consider is the geometric mean, G, defined as\n$G = \\sqrt[k]{A_{1} \\times \\dots \\times A_{k}},$\nwith A\u2081 again the accuracy on the ith ImageNet variant. This is a \"pessimistic\" metric in the sense that small changes in accuracy on low-scoring datasets are more impactful than changes in accuracy of the same magnitude on high-scoring datasets. For example, consider a model with an accuracy of A1 = 0.2 and A2 = 0.8 on two datasets, so that the geometric mean is G = 0.4. A decrease in accuracy of 0.1 units on the first dataset will decrease the geometric mean to G' = 0.28, a 30% decrease, while the same decrease on the second dataset results in a geometric mean of G\" = 0.37, only a 7.5% decrease. While the two provided metrics penalize changes in accu- racy differently, they nevertheless convey a similar message (see Table IV and Figure 4), with MoCo taking the first (weighted average) or second (geometric mean) spot, while DINO and Swav see a drop in aggregate accuracy and hence in rank, compared to their performance on ImageNet validation. The model that scores best under the geometric mean is OBOW, due to its improved performance on ImageNet Adversarial, the dataset on which all models score poorly. This performance is not characteristic of the model as a whole, a fact that is reflected in its mediocre rank under the geometric mean."}, {"title": "V. DISCUSSION AND CONCLUSION", "content": "Ever since DNNs became mainstream solutions for com- puter vision problems, the selection of an appropriate model for the task at hand has become one of the important decisions to make. With the introduction of SSL, this problem becomes two-fold: (1) selection of the model and (2) use of the most appropriate SSL framework to train that model. Unfortunately, there are no shortcuts for both decisions. While the ImageNet validation accuracy is indeed helpful in making a decision, as we have shown, making a choice purely based on this criterion can miss certain useful aspects of SSL frameworks. However, this does not mean that the usage of any other Ima- geNet variant is more appropriate compared to the ImageNet validation set. On the contrary, we observe that each SSL framework comes with its own unique traits. In particular, we have identified:\nFailure of DINO and Swav in having their performance translate from ImageNet to its out-of-distribution variants. Absence of improved performance for VicReg over Barlow, even though the former has been proposed as an enhancement over the latter. Superb results obtained with MoCo over all other SSL models under dataset shifts.\nImprovements in ranking for BYOL and Barlow under dataset shifts relative to their initial standings. Noticeable resistance of OBOW against adversarial exam- pled compared to other models.\nNo correlation between performance (measured with the accuracy across ImageNet variants) and the core self-supervised training methodology: clustering (DeepC, SeLa, Swav), contrastive learning (SimCLR, MoCo, PCL), distillation (BYOL, SimSiam, OBOW, DINO), information-maximization (Barlow, VicReg). As a re- sult our experiments, we can conclude that claiming one methodology to be superior to others would be invalid. In essence, our experiments demonstrated that the complex- ity of SSL performance, when evaluated only with ImageNet, is not captured correctly. As such, for newly proposed SSL frameworks, we call for the use of a wide range of analysis techniques that involve evaluations on a variety of test sets to capture distinct characteristics of different SSL approaches."}]}