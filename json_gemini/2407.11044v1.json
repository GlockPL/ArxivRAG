{"title": "Generalizing soft actor-critic algorithms to discrete action spaces", "authors": ["Le Zhang", "Yong Gu", "Xin Zhao", "Yanshuo Zhang", "Shu Zhao", "Yifei Jin", "Xinxin Wu"], "abstract": "ATARI is a suite of video games used by reinforcement\nlearning (RL) researchers to test the effectiveness of the\nlearning algorithm. Receiving only the raw pixels and\nthe game score, the agent learns to develop sophisticated\nstrategies, even to the comparable level of a professional\nhuman games tester. Ideally, we also want an agent re-\nquiring very few interactions with the environment. Pre-\nvious competitive model-free algorithms for the task use the\nvalued-based Rainbow algorithm without any policy head.\nIn this paper, we change it by proposing a practical discrete\nvariant of the soft actor-critic (SAC) algorithm. The new\nvariant enables off-policy learning using policy heads for\ndiscrete domains. By incorporating it into the advanced\nRainbow variant, i.e., the \u201cbigger, better, faster\" (BBF),\nthe resulting SAC-BBF improves the previous state-of-the-\nart interquartile mean (IQM) from 1.045 to 1.088, and it\nachieves these results using only replay ratio (RR) 2. By\nusing lower RR 2, the training time of SAC-BBF is strictly\none-third of the time required for BBF to achieve an IQM\nof 1.045 using RR 8. As a value of IQM greater than one\nindicates super-human performance, SAC-BBF is also the\nonly model-free algorithm with a super-human level using\nonly RR 2. The code is publicly available on GitHub at\nhttps://github.com/lezhang-thu/bigger-\nbetter-faster-SAC.", "sections": [{"title": "1. Introduction", "content": "Back in 2015, DeepMind developed the deep Q-network\n(DQN) [19] to tackle the tasks in the challenging domain\nof classic ATARI 2600 games, which is a suite of video\ngames with a wide range of diverse environments. The al-\ngorithm uses Q-learning, with critical techniques like expe-\nrience replay and target networks only periodically updated.\nReceiving only the raw ATARI frames and the game score,\nDQN enables the agent to develop sophisticated human-\nlevel strategies. A series of extensions to DQN follows in\nthese years. A partial list includes: double DQN utilizing\nthe idea of double learning [27], prioritized experience re-\nplay [22], dueling network [30] splitting the Q-network into\nseparated representations of state values and action advan-\ntages, distributional Q-learning [2] which explicitly models\nthe distribution over the returns, NoisyNet [9] that substi-\ntutes the standard linear layers with noisy ones for efficient\nexploration, etc. With n-step learning as in [20], the Rain-\nbow [13] algorithm combines all the advances above, serv-\ning as a strong baseline for later algorithms. For distribu-\ntional Q-learning, we note a series of works of the theme,\ne.g., quantile regression (QR-DQN) [6], implicit quantile\nnetworks (IQN) [5], fully parameterized quantile function\n(FQF) [34] etc.\nAll the algorithms above are value-based and operate off-\npolicy, i.e., the agent can improve the existing policy by\nutilizing data whose distribution may not match the policy.\nThe off-policy characteristic enables repeated optimization\nusing the same data sampled from the replay buffer. By\ncontrast, in on-policy algorithms such as asynchronous ad-\nvantage actor-critic (A3C) [20] etc., ATARI frames are used\nonly once in training and then discarded. In the real world,\nwe want the agent to learn efficiently by requiring only\nlimited feedback from the environment. It is the task for\nsample-efficient RL. For the sample-efficient RL, a widely\nadopted benchmark is ATARI 100K, which limits the num-\nber of ATARI frames returned to 400K (frame-skipping in\nATARI introduces the extra factor of 4), corresponding to\napproximately two hours of real-time play. In contrast to\non-policy algorithms, off-policy algorithms like Rainbow fit\nthis task well."}, {"title": "2. Related work", "content": "We can broadly classify sample-efficient RL algorithms\ninto model-based approaches and model-free ones. The\nmodel-based algorithms hinge on learning a world model.\nFor the model-free ones, value-based Rainbow variants\nhave consistently been the dominant choice within this cat-\negory of algorithms. Now, we take a look at what Rainbow\ndoes.\nRainbow uses the Q-network with output dimension A\nto represent the policy, where A is the number of the dis-\ncrete actions. Given (s, a), the agent infers the Q-value\nQ(s, a) from the Q-network. Thus, Rainbow involves no\npolicy distribution like \u03c0(\u00b7|s) as in A3C.\nA natural question is whether one can incorporate a sep-\narate policy head representing \u03c0(\u00b7|s) into Rainbow. We\nnote, however, that algorithms with policy heads like A3C\nor proximal policy optimization (PPO) [24] are on-policy.\nFor these on-policy algorithms, reusing the same data to the\nsame extent as in Rainbow is not well-justified, empirically\noften leading to destructive policy updates. Thus, it appears\nthere are inherent conflicts between leveraging Rainbow's\noff-policy property to enhance sample efficiency and incor-\nporating a policy head representing \u03c0(\u00b7|s) into Rainbow to\nimprove the learning even better.\nNow, let us switch to the setting of continuous action\nspaces. Among the most widely employed algorithms in\nthis context, the SAC [12] algorithm successfully integrates\npolicy learning and off-policy learning of Q-values. In\nthis paper, we transfer this characteristic to discrete action\nspaces. We note, however, that it is not straightforward:\n1. SAC is designed for the maximum entropy RL rather than\nfor the standard maximum reward RL as Rainbow. 2. The\ntransferring seems unpromising, as previous works exist,\nsay SAC-Discrete, which falls far behind across almost all\nthe tested environments compared to data-efficient Rainbow\n(DER).\nIn this paper, we present a discrete variant of SAC for\nstandard maximum reward RL and prove its convergence\nfrom scratch. Integration of it into Rainbow is straightfor-\nward, as now both fit in the standard maximum reward RL\nand work for discrete action spaces. We test it on the most\nadvanced Rainbow variant for ATARI 100K, i.e., the BBF\nalgorithm. While with a 3x reduction of training time, the\nresulting algorithm SAC-BBF improves the previous state-\nof-the-art IQM from 1.045 to 1.088. Also, SAC-BBF is the\nonly model-free algorithm with a super-human level using\nonly RR 2. Further improvements of IQM using SAC-BBF\nare promising by using larger RRs, fostering the develop-\nment of even more competitive agents."}, {"title": "2.1. Competitive representatives in ATARI 100K", "content": "Kaiser et al. [38] introduced the ATARI 100K benchmark\nand proposed simulated policy learning (SimPLe), which\nutilizes video prediction models to train a policy within a\nlearned world model. Overtrained Rainbow (OTRainbow)\n14] and DER [28] can be seen as improved hyperparameter\nconfigurations of Rainbow [13], tailored for ATARI 100K.\nSrinivas et al. [16] presented contrastive unsupervised rep-\nresentations for RL (CURL), which employs contrastive\nlearning to enhance image representation quality. With sim-\nple image augmentations, data-regularized Q (DrQ) [35]\ndemonstrates superior performance compared to preced-\ning algorithms. Self-predictive representations (SPR) [25]\ntrains the agent to predict its latent state representations\nmultiple steps into the future, achieving a notable perfor-\nmance improvement over previous methods. Scaled-by-\nresetting SPR (SR-SPR) [7] significantly improves sam-\nple efficiency by utilizing a replay ratio (RR) as large as\n16, achievable by periodically fully or partially resetting\nthe agent's parameters [21]. EfficientZero [36], built upon\nMuZero [23], introduces the self-supervised consistency\nloss from SimSam [3] and utilizes other tricks of the predic-\ntion of value prefix instead of rewards, and dynamically ad-\njusting the step for computing the value targets. With these,\nit is the first algorithm to achieve super-human performance\non the ATARI 100K benchmark. Micheli et al. [18] pro-\nposed IRIS, where the agent learns within a world model\ncomposed of a discrete autoencoder and an autoregressive\nTransformer [29].\nWhile EfficientZero achieves human-level efficiency, it\nhinges on Monte Carlo tree search (MCTS) and learning a\nworld model. Super-human levels, therefore, seem elusive\nfor model-free RL agents. The breakthrough is the \"big-\nger, better, faster\u201d (BBF) agent, proposed by Schwarzer et\nal. [26]. The BBF algorithm is built upon SP-SPR and is the\nonly model-free RL agent capable of achieving human-level\nperformance (IQM \u2265 1.0). Compared to EfficientZero, it\nachieves slightly better IQM but exhibits significantly re-\nduced computational requirements, resulting in at least a 4x\nreduction in runtime.\nWe note, however, that all these representative model-\nfree sample-efficient RL algorithms use Rainbow variants\nwith no explicit policy head representing \u03c0(\u00b7|s) as the inter-\nnal backbone.\""}, {"title": "2.2. Previous results on discrete variants of SAC", "content": "A few results exist on applying SAC to discrete action\nspaces, although these algorithms work in maximum en-\ntropy RL framework as SAC. On ATRARI 100K, SAC-\nDiscrete is the first such attempt [4]. In the community,\nhowever, SAC-Discrete is observed not to work for the toy"}, {"title": "2.3. Previous algorithms combining Q-learning\nwith actor-critic", "content": "Researchers previously proposed actor-critic algorithms\nwith experience replay buffers. These algorithms also fit\nthe category of combining policy heads with Q-learning.\nWe only review two representatives, i.e., ACER by Wang\net al. [31] and Reactor by Gruslys et al. [10]. These two\nalgorithms are most closely based upon A3C [10].\nACER introduces importance sampling truncation with\nbias correction. Reactor introduces the B-LOO (i.e., leave-\none-out) policy gradient algorithm. These innovations en-\nable the reuse of the data in the replay buffer for pol-\nicy updates. The update forms of these algorithms, how-\never, do not have the theoretical elegance of SAC, involv-\ning importance weights, like $\\min(c, \\frac{\\pi(\\hat{a})}{\\mu(\\hat{a})})(R(a) -V)V\\log \\pi(a)$ for Reactor where u is the behavior pol-\nicy and $a \\sim \\mu$, and $P_tV_\\theta\\log \\pi_\\theta(a_t|x_t)[Q_{ret}(x_t, A_t) -V_\\theta(x_t)]$ for ACER, where $p_t = \\min{c, p_t}$ with $P_t = \\pi(a_t|x_t)/\\mu(a_t|x_t)$.\nExperimentally, the Reactor generally exceeds the per-\nformance of ACER. Results of the Reactor are reported on\n500M training frames and are comparable to Rainbow [10]."}, {"title": "3. Preliminaries", "content": "Consider a Markov decision process (MDP), defined as a\ntuple (S, A, p, r, \u03c1\u2080, \u03b3), where S and A represent the sets\nof possible states and actions, respectively. The transition\nfunction p : S \u00d7 A \u00d7 S \u2192 R represents $Pr(s_{t+1} = s' | S_t\ns, a_t = a)$. The reward function r : S \u00d7 A \u2192 [rmin, rmax]\nis the expected value of the scalar reward when action a is\ntaken in state s. The initial state distribution is denoted by\n\u03c1\u2080, and \u03b3\u2208 (0, 1) is a discount factor. We use \u03c1\u03c0(st, at)\nto denote the state-action marginals of the trajectory dis-\ntribution induced by a policy \u03c0(at|st). The objective for\noptimization is defined as follows:\n$J(\\pi) = \\sum_{t=0}^{\\infty} E_{(S_t,a_t)\\sim\\rho_\\pi}  E_{s_l~p}^{\\sum_{l=t}^{\\infty} \\gamma^{l-t} [r(s_i, a_i) | s_t, a_t]} \\text{(1)}$\nwhich aims to maximize the discounted expected reward\nfor future states, given every state-action tuple (st, at),\nweighted by its probability \u03c1\u03c0 under the current policy."}, {"title": "3.1. The SAC algorithm", "content": "SAC works for maximum entropy RL, so the objective for\nSAC is as follows:\n$J(\\pi) = \\sum_{t=0}^{\\infty} E_{(S_t,a_t)\\sim\\rho_\\pi} E_{s_l~p}^{\\sum_{l=t}^{\\infty} \\gamma^{l-t} [r(s_i, a_i) + \\alpha H(\\pi(:|S_t)) | S_t, a_t]} \\text{(2)}$\nwhere a determines the relative importance of the entropy\nterm.\nThe soft policy iteration of SAC alternates between soft\npolicy evaluation and soft policy improvement.\nSoft policy evaluation: For a fixed policy, the soft Q-value\ncan be computed by iteratively applying the following Bell-\nman backup operator\n$T^{Q}(s_t, a_t) \\triangleq r(s_t, a_t) + \\gamma E_{S_{t+1}~P} [V(S_{t+1})], \\text{(3)}$\nwhere $V (s_t) = E_{a_t~\\pi}[Q(S_t, a_t) - \\alpha \\log \\pi(a_t|s_t)]$.\nSoft policy improvement: SAC updates the policy accord-\ning to\n$\\pi_{new} = arg \\min_{\\pi' \\in \\Pi} D_{KL} (\\pi'(\\cdot|s_t)|| \\frac{exp(Q_{old}(s_t,\\cdot)/\\alpha)}{Z_{old} (S_t)} ) \\text{(4)}$\nwhere II is a parameterized family of distributions, DKL is\nKullback-Leibler divergence, and Zold(st) plays the role\nof a normalizing constant. SAC updates the policy towards\nthe exponential of the Q-function, and DKL(\u00b7||\u00b7) serves as\nthe projection so that the constraint \u03c0\u2208 II is satisfied.\nLet the parameters of the Q-network and the policy net-\nwork be \u03c6 and \u03b8 resp. SAC works for continuous do-\nmainsw, so the Q-network Q\u03c6(st, at) is of two inputs st and\nat. Thanks to it, Q\u03c6(st, at) in SAC is thus differentiable\nw.r.t. at. To utilize it for a lower variance estimator, SAC\napplies the re-parameterization [15, 32] trick of at as fol-\nlows:\n$a_t = f_\\theta(\\epsilon_t; S_t), \\text{(5)}$"}, {"title": "4. A discrete variant of SAC for standard max-imum reward RL", "content": "where \u03f5t is independent noise, which follows the distribu-\ntion, say, a spherical Gaussian. With this, the approximate\ngradient for the optimization in Eq. 4 is thus\n$\\triangledown_\\theta E_{\\epsilon\\sim\\pi_\\theta} [(\\triangledown_{a_t}log \\pi_\\theta(a_t|S_t) + (\\triangledown_{a_t}- \\triangledown_{a_t}Q(S_t, a_t))\\triangledown_{a_t}f_\\theta(\\epsilon_t; S_t)].  \\text{(6)}$\nIn this section, we extend SAC beyond the maximum en-\ntropy RL. In standard maximum reward RL, we set a\nas zero. In SAC, however, the presence of the term\nexp(Qold (St,)/a) in Eq. 4 (the objective of soft policy\nimprovement step of SAC) indicates setting a as zero makes\nno sense. So, it raises the concern whether the theory of\nSAC would break. To address this concern, we re-prove\nall the lemmas and the theorem. We rigorously follow the\noriginal proofs so one can easily verify the correctness."}, {"title": "4.1. Policy evaluation", "content": "Similar to Sarsa [27], the Q-value can be computed itera-\ntively for a fixed policy. We can start with any function\nQ:S\u00d7A\u2192 R and repeatedly apply the Bellman backup\noperator T\u03c0, defined as:\n$T^{\\pi}Q(s_t, a_t) = r(s_t, a_t) + \\gamma E_{S_{t+1}~P \\atop a_{t+1} ~\\pi} [Q(S_{t+1}, a_{t+1})].  \\text{(7)}$\nLemma 4.1 (Policy Evaluation). Let T\u03c0 be the Bellman\nbackup operator defined in Eq. 7, and let Q\u2070 : S \u00d7 A \u2192\nR be a mapping. We define Qk+1 = T\u03c0Qk. Then, as k\napproaches infinity, the sequence Qk converges to the Q-\nvalue of \u03c0.\nProof. The proof follows by applying standard convergence\nresults for policy evaluation [27]."}, {"title": "4.2. Policy improvement", "content": "In policy improvement, we depart from SAC's method\nof updating the policy towards the exponential of the Q-\nfunction. Besides, we eliminate the projection needed for\nsatisfying the constraint \u03c0\u2208 \u03a0. Concretely, for each state,\nour policy is updated based on the following equation:\n$\\pi_{new}(s_t) = arg \\max_{\\pi'\\in\\Pi} E_{a_{pi'} [Q_{old} (s_t, a_t)].  \\text{(8)}$\nWe now demonstrate that the policy update described in\nEq. 8 leads to an improved policy w.r.t. the objective stated\nin Eq. 1.\nLemma 4.2 (Policy Improvement). Let \u03c0old \u2208 II, and let\n\u03c0new be the optimizer of the maximization problem defined\nin Eq. 8. Then, for all (st, at) \u2208 S \u00d7 A, it holds that\n$Q^{new}(s_t, a_t) > Q^{old} (s_t, a_t)$.\nProof. See Appendix C.1 in the supplementary mate-\nrial.\nWith the two lemmas mentioned above, we can state\nthe following theorem on convergence to the optimal pol-\nicy among the policies in II."}, {"title": "4.3. A practial algorithm", "content": "Theorem 4.3 (Policy Iteration). The repeated applica-\ntion of policy evaluation and policy improvement from any\n\u03c0\u2208 \u03a0 converges to a policy \u03c0* such that $Q^{\\pi*}(s_t, a_t) >\nQ^{\\pi} (s_t, a_t)$ for all \u03c0\u2208 II and (st, at) \u2208 S \u00d7 A.\nProof. See Appendix C.2 in the supplementary mate-\nrial.\nAs a practical algorithm, we employ function approxima-\ntors, such as deep neural networks, to represent both the\nQ-function and the policy."}, {"title": "4.3.1 Variance reduction", "content": "Consider the parameterized Q-function Q\u03c6(st) (with pa-\nrameters \u03c6) and the parameterized policy \u03c0\u03b8 (with parame-\nters \u03b8). We have the following lemma on the optimization\nin Eq. 8.\nLemma 4.4. The objective in Eq. 8 can be optimized with\nstochastic gradients:\n$\\triangledown_\\theta E_{a_t~\\pi_{\\theta}} [Q_{old} (S_t, a_t)] =\nE_{a_t~\\pi_{\\theta}} [Q_{old} (S_t, a_t)\\triangledown_\\theta log \\pi_\\theta(a_t|S_t)].  \\text{(9)}$\nProof. See Appendix C.3 in the supplementary mate-\nrial.\nIn SAC, the Q-network is a neural network with two in-\nputs st and at. In contrast, for discrete action spaces, the\nQ-network only receives one input st, outputting a vector\nof the dimension |A|. For this case, we have the following\nlemma for the gradient estimator with reduced variance:\nLemma 4.5 (Variance reduction). The following two gradi-\nent estimators are equal:\n$E_{a_t~\\pi_{\\theta}} [\\triangledown_\\theta (Q_{gold} (S_t, a_t) - \\sum_{a'\\in A} \\pi_{\\theta_{old}} (a' |S_t)Q_{\\phi_{old}} (S_t, a'))\\triangledown_\\theta log \\pi_\\theta (a_t|S_t)]\\newline =\nE_{a_t~\\pi_{\\theta}}[Q_{\\phi_{old}} (S_t, a_t)\\triangledown_\\theta log \\pi_\\theta (a_t|S_t)]. \\text{(10)}$\nProof. See Appendix C.4 in the supplementary mate-\nrial."}, {"title": "4.3.2 An entropy bonus", "content": "We augment the objective by adding an entropy bonus to\nthe policy \u03c0\u03b8 to discourage premature convergence to sub-\noptimal deterministic policies. Additionally, we replace the"}, {"title": "5. Integrating SAC with BBF", "content": "expectations in Eq. 10 with sample averages. The final gra-\ndient estimator for policy parameters \u03b8 is given by:\n$Q_{gold} (S_t, a_t) \\triangledown_\\theta log \\pi_\\theta (a_t|S_t) + \\beta \\triangledown_\\theta H(\\pi(\\cdot|S_t)), \\text{(11)}$\nwhere the hyperparameter B controls the strength of explo-\nration encouragement. We note at ~ \u03c0\u03bf, so whenever st is\nused, it samples a new action $a_f ~ \\pi_{\\theta}$. It contrasts ACER\nor Reactor, where off-policy learning for policy updates al-\nways centers over the action in the replay buffer for st.\nWe linearly anneal \u03b2 from an initial constant value to\n0. We then keep \u1e9e = 0 till the training ends. The linear\nannealing scheme plays a role in better performance, for\nwhich we defer the details to the experiment section.\nIn this section, we integrate the SAC variant discussed in\nthe previous section with BBF. We depict the new archi-\ntecture in Fig. 1. Notably, all networks from BBF remain\nunaltered. SAC-BBF introduces three additional modules\n(excluding the target module counterparts): \u201conline projec-\ntion \u03b8,\u201d \u201cpolicy head,\u201d and \u201cpredictor \u03b8.\u201d We implement\nthese modules as simple linear layers."}, {"title": "5.1. Modifying target values for training the Q-network", "content": "In addition to the network modifications, SAC-BBF also al-\nters the target used in the n-step learning (Q-learning) of\nBBF. The target for BBF's n-step learning is defined as fol-\nlows:\n$\\sum_{k=0}^{n-1} \\gamma^k r_{t+k+1} + \\gamma^n Q_{\\phi_{targ}} (s_{t+n}, arg \\max_{a'} Q_{\\phi_{targ}}(s_{t+n}, a')), \\text{(12)}$\nwhere rt+k+1 represents the reward obtained from the state-\naction pair (st+k+1, at+k+1), and \u03c6targ denotes the corre-\nsponding set of parameters for the target Q-network.\nIn SAC-BBF, we substitute arg maxa' Q\u03c6(st+n, a') in\nEq. 12 with a' ~ \u03c0\u03bf(\u00b7|st+n). This replacement aligns with\nthe Bellman backup operator T\u03c0 defined in Eq. 7."}, {"title": "5.2. Incorporating additional terms in the predic-tion loss", "content": "SAC-BBF introduces a projection layer for the policy \u03c0\u03b8,\nwhich prompts an extension of the self-predictive represen-\ntations (SPR) loss to this layer. The modified SPR loss in\nSAC-BBF is defined as follows:\n$\\frac{1}{2K} \\sum_{0<j<k}^{\\nabla_{\\theta}} \\sum_{v\\in \\{x,y\\}}^{\\frac{||\\hat{U_{t+j}}^T U_{t+j}||}{(||\\hat{U_{t+j}}||)^2 }}, \\text{(13)}$\nwhere xt+j, \u02c6xt+j, yt+j, \u02c6yt+j correspond to the vectors de-\npicted in Fig. 1."}, {"title": "5.3. Implementing a new policy loss", "content": "In SAC-BBF, we use Eq. 11 for updating the policy head.\nWe note the learning process in SAC-BBF differs from soft\nQ-learning [11], where the policy network only acts as an\napproximate sampler from the soft Q-function."}, {"title": "6. Experiments", "content": "We build the implementation of SAC-BBF over that of\nBBF. To ensure a fair comparison, we maintain consistency\nwith BBF in all hyperparameters and training configurations\nwhenever applicable. We set F = 40K and the initial value\nof \u1e9e as 0.01.\nWe carry out a series of experiments focusing on the fol-\nlowing aspects:\n1. Investigating the role of variance reduction, as discussed\nin Sec. 4.3.1, in the effective functioning of an agent.\n2. Assessing the effectiveness of annealing \u03b2 and examin-\ning the impact of employing a sampling strategy during\nevaluation.\n3. Presenting the results of SAC-BBF, highlighting its abil-\nity to achieve new benchmark IQM results.\n4. Exploring miscellaneous factors such as training and in-\nference times compared to BBF.\nFor all variants of SAC-BBF, we obtain the results\nthrough 10 independent runs and evaluate them over 100\nepisodes upon completion of training."}, {"title": "6.1. Selecting subsets of environments for ablation\nstudies", "content": "For the ablation studies, we prioritize carbon reduction. We\nthus restrict the experiments to various randomly selected\nsubsets of 5 games from the complete suite of 26 games in\nthe Atari 100K benchmark. To ensure randomness in select-\ning these subsets, you can use the Python function provided\nin Algorithm 1. We state the seeds for each experiment at\nthe beginning of the following subsections."}, {"title": "6.2. The importance of variance reduction", "content": "We experiment with a seed value of 3 in Algorithm 1. The\nresults are listed in Table 1. It is worth emphasizing that\nSAC-BBF without variance reduction differs from SAC-\nBBF in just one line of code in the implementation.\nTable 1 demonstrates the impact of this single line of\ncode. Without variance reduction, SAC-BBF shows a neg-"}, {"title": "6.3. Annealing \u03b2 and using a sampling strategy dur-ing evaluation", "content": "We experiment with a seed value of 2 in Algorithm 1. The\nresults are listed in Table 2.\nAnnealing 8: The results in Table 2 indicate that annealing\n\u1e9e yields better results than using a constant value, as evi-\ndenced by the aggregate metrics such as IQM. In Seaquest\nand Alien, these variants produce comparable results.\nThe impact of using a sampling strategy for evaluation:\nAs indicated in Table 2, using sampling strategies gener-\nally leads to better results in terms of aggregate metrics, ex-\ncept for the \"Median\" metric. Regarding average scores,\nthese two strategies are comparable in Seaquest, Alien,\nand Pong. Sampling strategies outperform greedy ones in\nCrazyClimber but underperform in Kangaroo. After ob-\nserving the raw scores, we find that the higher average score\nof greedy strategies in Kangaroo is due to a single score of\n10400. The IQM metric is robust against outlier scores by\ndiscarding the bottom and top 25%. While these compar-\nisons do not provide a definitive conclusion, they do indi-\ncate that using sampling strategies does not negatively im-\npact performance.\nWe did not conduct experiments on using sampling or\ngreedy strategies during the evaluation for SAC-BBF with\nannealing B. We anticipate using sampling strategies or not\nfor this case would not significantly affect performance, as\nSAC-BBF runs with \u03b2 = 0 for the last F = 40K training\nupdates. So, the final strategy would not have very high\nentropy."}, {"title": "6.4. Scores and aggregate metrics for SAC-BBF across the 26 Atari 100K games", "content": "We present the results of SAC-BBF with RR 2 for the com-\nplete suite of Atari 100K benchmarks in Table 3. To provide"}, {"title": "6.5. Comparison results on inference and training\ntimes", "content": "We run the following experiment on a single RTX 4090\nGPU with 24GB of memory. The implementation of SAC-\nBBF follows the JAX implementation of BBF. Besides,\nwe modify JAX's default GPU memory allocation strat-\negy by os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"\n] = \"1.\".\nInference time: Despite incorporating additional modules,\nSAC-BBF demonstrates shorter inference time, as shown in\nTable 4. During inference, SAC-BBF agents rely solely on\nthe policy network. The \"policy head\" module depicted in\nFig. 1 consists of a linear layer with an output dimension of\nA. In contrast, BBF employs distributional RL, resulting\nin a \u201cQ-function head\u201d with an output dimension of N\u00d7|A|,\nwhere N represents the number of atoms.\nTraining time: For RR 2, the introduction of additional\nmodules in SAC-BBF only slightly increases the training\ntime for training one agent in the ChopperCommand envi-\nronment (the default environment in the official repository\nof BBF). The time difference is approximately 15 minutes.\nWith an increased RR from 2 to 4, the number of training\nupdates also doubles, resulting in a time difference of 30\nminutes for RR 4. We note SAC-BBF-RR2 requires less\nthan one-third of the training time compared to BBF-RR8."}, {"title": "7. Conclusion", "content": "In this paper, we have explored the application of SAC in\nthe context of discrete action spaces. By providing rigorous\ntheoretical proofs, we present a discrete variant of SAC that\nworks in standard maximum reward RL. It enables the in-\ntegration of SAC with the state-of-the-art sample-efficient\nmodel-free algorithm BBF. The resulting SAC-BBF is the\nonly model-free sample-efficient RL algorithm that intro-\nduces explicit policy heads into the Rainbow backbone. Ex-\nperimental results demonstrate the promising performance\nof the integration. With RR 2, the algorithm SAC-BBF\nachieves the highest IQM of 1.088. Additionally, SAC-BBF\nexhibits replay-ratio scaling capabilities, indicating the pos-\nsibility of even better results by increasing replay ratios as\nin BBF. We believe that SAC-BBF contributes to advanc-"}, {"title": "A. Action selection", "content": ""}, {"title": "A.1. Action selection in BBF", "content": "Utilizing target networks for action selection: BBF uti-\nlizes Rainbow as the underlying RL approach. However", "buffer": "Rainbow relies on the online parameters for\naction selection", "surprising importance.\"\nAction selection in training": "In Rainbow", "is": "How do the agents in BBF maintain ex-\nploration after 4K steps", "mechanisms": 1.0}]}