{"title": "Human-In-the-Loop Software Development Agents", "authors": ["Wannita Takerngsaksiri", "Jirat Pasuksmit", "Patanamamon Thongtanunam", "Chakkrit Tantithamthavorn", "Ruixiong Zhang", "Fan Jiang", "Jing Li", "Evan Cook", "Kun Chen", "Ming Wu"], "abstract": "Recently, Large Language Models (LLMs)-based multi-agent paradigms for software engineering are introduced to automatically resolve software development tasks (e.g., from a given issue to source code). However, existing work is evaluated based on historical benchmark datasets, does not consider human feedback at each stage of the automated software development process, and has not been deployed in practice. In this paper, we introduce a Human-in-the-loop LLM-based Agents framework (HULA) for software development that allows software engineers to refine and guide LLMs when generating coding plans and source code for a given task. We design, implement, and deploy the HULA framework into Atlassian JIRA for internal uses. Through a multi-stage evaluation of the HULA framework, Atlassian software engineers perceive that HULA can minimize the overall development time and effort, especially in initiating a coding plan and writing code for straightforward tasks. On the other hand, challenges around code quality are raised to be solved in some cases. We draw lessons learned and discuss opportunities for future work, which will pave the way for the advancement of LLM-based agents in software development.", "sections": [{"title": "I. INTRODUCTION", "content": "Multi-agent paradigm, powered by Large Language Models (LLMs), has demonstrate remarkable results in many research areas [1], including software engineering [2]\u2013[4]. Within the software engineering context, LLM-based multi-agent is defined as an autonomous agent (e.g., an LLM model) receiving inputs from surrounding environment (e.g., compilers, linter tools) and performing actions (e.g., writing code, fixing bugs) to achieve a specific objective (e.g., resolving a pull request, passing test cases). The agent will be assigned with a role (e.g., software engineer, software tester) as part of the software development process with access to external resources (e.g., compilers, linters). With this LLM-based multi-agent paradigm, they are able to solve complex software development tasks for a given input and a goal.\nRecently, various LLM-based software development agent frameworks are proposed to address complex software development tasks [5]\u2013[11]. However, existing LLM-based software development work is (1) evaluated based on historical benchmark datasets of open-source software projects, (2) does not consider human feedback at each stage of the automated software development process, and (3) has not been deployed in practice. These challenges raise important questions of whether the effectiveness of LLM-based software development agent can be generalised beyond the context of open-source software projects, how acceptable are the plans and code generated by LLM-based software development agent at each stage of the software development process, and how do practitioners perceive the benefits and challenges of using LLM-based software development agent.\nIn this paper, we introduce a Human-in-the-Loop LLM-based Software Development Agent Framework (HULA). Our framework consists of three agents: AI Planner Agent, AI Coding Agent, and Human Agent, working cooperatively to achieve the common goal of resolving a JIRA issue (i.e., a software development task). We design the user interface, implement, and deploy HULA into Atlassian JIRA for internal uses. To evaluate its effectiveness, our evaluation consists of three stages: (1) an offline evaluation of HULA without human feedback using SWE-Bench and our internal dataset of 369 JIRA issues; (2) an online evaluation of HULA augmented by human feedback using real-world 663 JIRA issues; and (3) an investigation of the practitioners' perceptions on the benefits and challenges of using HULA in practice. Through the multi-stage evaluation of our HULA through real-world deployment and user studies among Atlassian practitioners, we answer the following three research questions:"}, {"title": "II. SOFTWARE DEVELOPMENT WORKFLOW AT ATLASSIAN", "content": "Atlassian is an enterprise software company that develops products for software development, project management, team collaboration, and many more. Atlassian offers tools such as JIRA Software (for task management), Confluence (for collaboration and knowledge management), and Bitbucket (for source code management) to enhance software teams' productivity, streamline workflows, and facilitate Agile methodologies across various industries. These products serve over 300,000 customers globally, spanning various industries such as technology, finance, healthcare, transportation, and government.\nAt Atlassian, practitioners use JIRA as a central tool for managing software development tasks, tracking bugs, and facilitating collaboration across teams. First, they will create JIRA issues to define features, tasks, or bug fixes to work on. Then, JIRA issues will be assigned to individual practitioners or teams. Once they start working on the given JIRA issue, practitioners will create a coding plan (e.g., which files need to be modified or fixed?, where to fix a bug?), aiming to identify files that are relevant to the given JIRA issue (i.e., code or bug localization). Then, they will start the coding process. Once the coding process is finished, they will raise a pull request for code review and automated testing, ensuring that each pull request is of sufficient quality before integrating it into the main repository.\nSince these processes are generally done manually, the teams are prone to productivity challenges, e.g., an overwhelming backlog of JIRA issues and context switching between tools. These challenges can then result in delays and inefficiencies in software delivery, prompting Atlassian teams to seek automated solutions that can streamline workflows and improve their overall productivity."}, {"title": "III. RELATED WORKS AND LIMITATIONS", "content": "Automated software development tools and techniques, powered by deep learning (DL) or large language models (LLMs), are proposed to assist software engineers in various software development tasks (e.g., bug localization [12], code completion [13], code generation [8], [10], testing case generation [14], [15], code review automation [16], [17], vulnerability detection and repair [18], [19]). However, these automated software development tools and techniques are mainly designed to tackle individual SE tasks, leading to suboptimal performance when apply to real-world software development tasks.\nRecently, LLM-based multi-agent paradigm for software engineering has emerged [2]. The multi-agent paradigm for software engineering involves multiple autonomous agents, each with specific roles (e.g., software engineer, testers) and goals (e.g., write code, write test cases), working together to solve complex problems. Various LLM-based software development agent frameworks are proposed to resolve complex software development tasks [5]\u2013[11]. For example, Yang et al. [5] proposed SWE-agent, an LLM-based agent which build on top of Linux shell, enabling the agent to effectively search, navigate, edit, and execute code commands. Zhang et al. [6] proposed AutoCodeRover, an LLM-based agent with code search on AST (i.e., Abstract Syntax Tree) and spectrum-based fault localization using test suite. Ma et al. [11] proposed RepoUnderstander, an LLM-based agent with Monte Carlo tree search based exploration strategy on repository knowledge graph. Tao et al. [8] proposed Magis, a multi-agent framework with four customized roles: manager, repository custodian, developer, and quality assurance engineer. Chen et al. [7] proposed CodeR, a multi-agent framework with pre-defined task graphs. Arora et al. [10] proposed Masai, a modular framework divided the problem into multiple sub-problems for LLM-based agent.\nLimitations. However, existing LLM-based software development work is (1) evaluated based on historical benchmark datasets, limiting the understanding of its effectiveness in the context of resolving enterprise software development tasks; (2) does not consider human feedback at each stage of the automated software development process, limiting the"}, {"title": "IV. HULA: HUMAN-IN-THE-LOOP LLM-BASED AGENTS FRAMEWORK FOR SOFTWARE DEVELOPMENT", "content": "In this section, we present a human-in-the-loop LLM-based agent framework for software development.\nAt Atlassian, we highly value human intelligence (i.e., the expertise of practitioners) and human authority (i.e., the human abilities to rule AI agents). Therefore, rather than aiming to fully automate software development tasks, we designed an LLM-based software development agent to collaborate with practitioners, functioning as an assistant to help resolve software development tasks, promoting the Human-AI synergy [20], [21]. We envision that in the future of software development, practitioners and LLM-based software development agents should work collaboratively to complete software development tasks. Thus, we design our HULA framework to incorporate human feedback at each step of the software development tasks. This allow practitioners to provide feedback and guidance as needed, ensuring the software development process remains collaborative rather than entirely reliant on fully autonomous agents.\nBelow, we present the overview of our HULA framework and the user interface of HULA being seamlessly integrated into Atlassian JIRA.\nA. Framework\nThe ultimate goal of our human-in-the-loop LLM-based software development agent framework is to generate a pull request (i.e., a set of code changes) based on a given software development task (i.e., a JIRA issue). Since software development tasks for enterprise systems are complex, we exploit a multi-stage method to dissect the software development task into distinct stages, namely, file localization (i.e., which files should be changed?), plan generation (i.e., how each file should be changed), and coding (i.e, what are the suggested code changes?). Formally speaking, our framework consists of the following three agents:\nWith the Human-AI synergy in mind, we leverage a human-AI agent coordination paradigm, exploiting human intelligence in each stage to enhance the effectiveness of the multi-agent system for software development and facilitate better alignment with human preference. Different from the other multi-agent systems that leverage competitive, mixed, or hierarchical coordination style without human involvement, our framework exploits the cooperative communication style with the Decentralized Planning Decentralized Execution (DPDE) paradigm to ensure that each AI agent is independently responsible for its own objective and capabilities, while being able to work cooperatively with human agents to achieve the common goal. Although we exploit DPDE paradigm (i.e., no communication among agents), all Agents still have access to the shared memory of information (i.e, the JIRA issue and the source code repository). The merit of this approach lies in minimal communication overhead, reducing the computational resources on the central LLM, enhancing the Agents' adaptability, as each agent can quickly adjust its behavior based on local information and human feedback. This makes our framework better suited for dynamic and unpredictable environments, while promoting human-AI synergy.\nOur framework consists of the following four stages.\nStage 1 Setting up a task: A software engineer begins the workflow by selecting a development task (i.e., a JIRA issue) and an associated code repository. Generally, the issue description contains necessary documented information for humans to understand the work to be done, e.g., a user story, definition of done, acceptance criteria, or code examples [22].\nStage 2 Planning: AI Planner Agent (P) uses information in the JIRA issue (i.e., the issue summary and description) to understand the work to be done and the context and identify files that are relevant to the given JIRA issue. The software engineer can review, edit, and confirm the information before proceeding to the next step. Once the relevant files are determined, the AI Planner Agent generates a coding plan, outlining how the code in the identified files should be changed to resolve the given JIRA issue. For example, \"Add tests for the add_subfigure method\". After the coding plan is generated, the Human Agent (H) can review the plan, provide additional instructions, and re-generate the coding plan. Alternatively, the Human Agent (H) can directly modify the list of relevant files, and edit the change plan for each file.\nStage 3 Coding: Once the Human Agent (H) approves the coding plan, the LLM-based agent proceeds to the execution of the given coding plan. The AI Coding Agent (C) generates the code changes to each file according to the plan. The Human Agent (H) can then review the proposed code changes. If the code change does not meet the expectation, the Human Agent (H) can provide further instructions to change the code and then ask the AI Coding Agent (C) to re-generate the code changes. To ensure that the code being generated is syntactically correct with high code quality [13], the AI Coding Agent (C) undertakes self-refinement, improving the generated code based on feedback from code validation tools such as compilers and linters. The refinement is conducted"}, {"title": "V. THE MULTI-STAGE EVALUATION OF HULA IN REAL-WORLD DEPLOYMENT", "content": "in Real-World Deployment\nIn this section, we introduce the multi-stage evaluation framework for evaluating our HULA in real-world deployment. Our evaluation aims at answering the following three research questions:"}, {"title": "VI. RESULTS", "content": "In this section, we present the results according to our research questions."}, {"title": "VII. LESSONS LEARNED AND FUTURE DIRECTIONS", "content": "In this section, we discuss key lessons learned and offer suggestions for future work."}, {"title": "VIII. THREAT TO VALIDITY", "content": "In this section, we disclose the threats to the validity.\nThreats to construct validity. Measuring code similarity is complex and nuanced. Code similarity can be measured based on various dimensions, e.g., semantic similarity, syntactic similarity, functional similarity. To ensure that LLM-as-a-Judge for code similarity is highly similar to human judgement, we developed prompts that capture various concepts of code similarity. To validate if the code similarity score is aligned with human judgement, we randomly selected 203 Jira issues. We asked software engineers to provide a similarity score, ranging from 1 (no similarity), 2 (weak similarity), 3 (high similarity), to 4 (highly similarity). Then, we computed the correlation efficient between the human score and LLM-as-a-Judge score. We observed a high correlation coefficient of 0.7, demonstrating the high degree of alignment between human judgement and LLM-as-a-judge.\nThreats to external validity. Our HULA framework is generally applicable to other coding platforms and other LLMs. However, for this paper, the findings are limited to the context of Atlassian Jira that are internally used by Atlassian software engineers, which may not be generalized to other contexts. Therefore, the application of this framework in other coding platforms will prove fruitful.\nThreats to internal validity. We designed our HULA framework to be architecture-agnostic, meaning that other LLMs are applicable to this framework. However, the experiment and findings of this paper rely on the use of GPT-4 as a backbone LLM for our HULA framework. Based on the SWE-Bench Leaderboard, we find that our performance is comparable to the 6th-ranked agent, highlighting the competitive performance of our backbone LLM. It is possible that other LLMs that are larger with higher number of parameters may achieve higher performance. Nevertheless, the key goal of this paper is not to investigate which LLMs perform best, but instead to investigate the effectiveness of HULA framework in the real-world deployment. Therefore, the investigation of the best-performing LLMs is left for future work.\nLastly, although we may not be able to disclose all aspects of the implementation and experiment design due to the industrial confidentiality reasons, we are confident that the successful development and integration of HULA framework into Atlassian Jira is a great achievement, paving the way forward the future of AI-powered software development tools. In addition, our experience, findings, lessons learned, and future research directions will be beneficial for other practitioners and software engineering researchers."}, {"title": "IX. CONCLUSION", "content": "In this paper, we introduce a Human-in-the-Loop LLM-based Software Development Agents Framework (HULA). We design the user interface, implement, and deploy of LLM-based software development agents into Atlassian JIRA for internal uses. Through a multi-stage evaluation of HULA involving real-world deployment and user surveys with Atlassian practitioners, we conclude that (1) the detail of input can highly effect the performance of HULA; (2) incorporating human feedback into HULA can enhance the input context and be beneficial in practice; (3) practitioners perceive that HULA can help minimize the overall development time and effort but code quality concerns are raised in some cases."}, {"title": "X. DISCLAIMER", "content": "The perspectives and conclusions presented in this paper are solely the authors' and should not be interpreted as representing the official policies or endorsements of Atlassian or any of its subsidiaries and affiliates. Additionally, the outcomes of this paper are independent of, and should not be construed as an assessment of, the quality of products offered by Atlassian."}]}