{"title": "Multi-Sensor Fusion for UAV Classification Based\n on Feature Maps of Image and Radar Data", "authors": ["Nikos Sakellariou", "Antonios Lalas", "Dimitrios Tzovaras", "Konstantinos Votis"], "abstract": "Abstract-The unique cost, flexibility, speed, and\n efficiency of modern UAVs make them an attractive\n choice in many applications in contemporary society.\n This, however, causes an ever-increasing number of\n reported malicious or accidental incidents, rendering the\n need for the development of UAV detection and\n classification mechanisms essential. We propose a\n methodology for developing a system that fuses already\n processed multi-sensor data into a new Deep Neural\n Network to increase its classification accuracy towards\n UAV detection. The DNN model fuses high-level features\n extracted from individual object detection and\n classification models associated with thermal, optronic,\n and radar data. Additionally, emphasis is given to the\n model's Convolutional Neural Network (CNN) based\n architecture that combines the features of the three sensor\n modalities by stacking the extracted image features of the\n thermal and optronic sensor achieving higher\n classification accuracy than each sensor alone.", "sections": [{"title": "I. INTRODUCTION", "content": "Unmanned Aerial Vehicles (UAV) have successfully\n permeated modern society with various applications for civil\n and military purposes. Oil and gas, construction, metals and\n mining already incorporate UAVs in their processes.\n Furthermore, UAVs are employed for commercial purposes,\n such as the monitoring of public places, cartography, survey\n wildlife, search and rescue (SAR), first aid and delivery of\n goods. Big technological companies continuously challenge\n the status quote by announcing breakthrough services.\n Moreover, progress in UAV regulation has driven\n investments since 2019, to further increase the popularity and\n use of UAVs in sectors that present significant potential but\n still minimal use, such as agriculture, healthcare,\n infrastructure, property management and insurance. The\n global market value is estimated to reach 70$ billion by 2029\n with 9.6 percent compound annual growth. This growth\n however, causes an ever-increasing number of reported\n accidental or malicious incidents, rendering the need for the\n development of UAV detection and classification\n mechanisms essential.\n On the other hand, Airborne object detection and\n identification is a difficult task, particularly in the case of\n identifying a UAV against other airborne elements such as\n birds, clouds and airplanes. Traditional UAV detection\n methods employ systems with a single sensor modality\n including radar, camera and RF sensors or multiple\n modalities of the same type such as multiple acoustic arrays.\n However, unimodal sensors alone may result in pure\n detection range and classification accuracy. For that reason,\n multimodal sensor fusion systems can increase the system's\n performance by replenishing missing information with\n another sensor's perspective of the environment [1].\n Similarly, a one-eyed human can describe an observed object\n but needs a second eye to formulate a prediction about the\n object's range and by making use of her ears, she can more\n accurately determine the object's type and distance from her."}, {"title": "A. Single modality", "content": "In the case of a single modality radar-based sensor, the\n problem is divided into the detection, verification and\n classification task. For the detection task, methods employing\n Continuous Wave (CW) and Frequency Modulated\n Continuous Wave (FMCW) radars represent the most\n attractive and cost-efficient solutions [2]. While for the\n verification and classification task various methods exist in\n literature employing machine learning techniques such as\n SVM [3], Random Forests [4], Nearest Neighbor [5] and\n Deep Neural Networks [6][7][8]. More recent DNN\n approaches based on convolutional neural networks are\n introduced in Samaras et al. [9]. The authors presented a deep\n learning classification method based on data from an X-band\n FMCW surveillance 2D radar that is able to reach a\n classification accuracy of up to 95.0% utilizing a custom\n CNN based architecture. A similar approach is presented in\n 10] where the authors proposed Res-Net-SP, a compressed\n architecture of ResNet-18 that is based in convolutional\n neural networks. The model is trained on micro-Doppler\n features that can be extracted from spectrograms of a DMCW\n radar and it resulted in better accuracy (83.39%) than Res-\n Net-18 (79.88%) on the tested dataset discriminating between"}, {"title": "B. Multiple modalities", "content": "Multi sensor systems aim to tackle the UAV detection and\n classification problem by completing the limitations of one\n sensor type with another sensor's advantages. Wang et al.\n 27] proposed an object detection and tracking system based\n on fusion of visible and thermal camera data by utilizing a\n CNN architecture. Their special contribution is a Cycle-\n GAN-based model for data augmentation of thermal images\n containing drones. The system is trained offline by real and\n augmented data on the Fast R-CNN and MDNet models and\n its efficiency is tested on USC drone dataset with 43.8% AUC\n score on the test set. Another multi-fusion method is\n described in [28] where the authors employed an ADS-B\n receiver, standard video cameras (visible and thermal),\n microphone sensors and a fish-eye camera which directed the\n rest of the cameras to the drone achieving increased\n robustness with fusion of the sensory data through deep\n learning methods. Investigation of detection performance as\n a function of the sensor-to-target distance is also explored\n within their work.\n In [29] is presented a multi-frame deep learning drone\n detection technique that is based on videos recorded from a\n lower-angle rotating RGB camera and a static wide-angle\n RGB camera. The proposed method essentially fuses the\n static camera's frame with the zoomed frame of the rotating\n camera. The wide-angle camera detects the drone from afar\n and the detected drones that present specific motion\n characteristics are inspected from the lower-angle rotating\n camera. Both cameras use a lightweight version architecture\n of Yolo deep learning algorithm which is making use of the\n up-sampling concept in order to boost the performance for\n small objects detection. The classifier predicts four classes;\n namely drone, bird, airplane and background. The authors\n compared their solution with Cascaded Haar and GMM back.\n Sub algorithms that were trained on the same dataset. All the\n algorithms resulted on a good true positive rate (0.91, 0.95.\n 0.98 respectively) while the authors' lightweight Yolo\n implementation resulted in 0.0 false alarm rate against 0.42\n and 0.31 for Cascaded Haar and GMM respectively.\n Moreover, Nie et al. [30] present a UAV identification and"}, {"title": "C. Proposed Solution", "content": "In this work, the drone detection task was divided into two\n separate sub-problems. A flying object is detected and\n localized, and secondly, the object classification problem,\n where the flying object is classified into two classes: a UAV\n or a false alarm (birds, humans, other objects, noise). We\n focus on the second problem by proposing a deep learning\n multi-sensor fusion model for aerial object classification\n based on feature maps of the individual sensors' deep neural\n network (DNN) architectures. We proceed to an end-to-end\n description of the whole process which includes: the\n complete multi-module architecture of the system, a\n description of the sensor modules (thermal, optronic, and 2D\n radar) and their captured data formats, the data flow and\n preprocessing, as well as the final fusion algorithm.\n Additionally, this work introduces a novel method for the\n fusion of visible and thermal data by merging them into a new\n vector through feature stacking.\n The Inspiration to use feature maps (high level\n features) came from works such as [32] where the authors\n introduced a method to extract features from the OverFeat\n network [33] and use them as a generic image representation.\n Moreover, methods that fuse high level features in a late\n fusion approach so as to increase their models' classification\n accuracy are proposed in [34] and [35]. More specifically,\n Akilan et al. [34] explore the impact of fusing high level\n features from different DCNNs architectures. The authors\n fused feature maps of AlexNet [36], VGG-16 [37] and\n Inception-v3 [38] architectures trained on ImageNet and\n showed a 2% - 8% accuracy increase against the unimodal\n architectures.\n The rest of the paper is structured as follows: Section 2\n considers the materials and methods. In Section 3 are\n presented the experimental results and a discussion of the\n proposed methodology. Finally, Section 4 concludes the\n paper."}, {"title": "II. MATERIAL AND METHODS", "content": "A deep learning fusion method is introduced that combines\n data retrieved from three modalities (thermal, optronic and 2d\n radar) that incorporates different DNN architectures towards\n the classification of UAVs. The data are geospatially aligned\n and they are fed to a separate fusion neural network that aims\n to binary classify captured flying objects between two\n classes, \"UAV\" or \"No UAV\" and hence increase the\n performance of the combined system against the separate\n models alone.\n The data were collected by setting up the sensors in an\n open space and executing scenarios with flying UAVs. The\n development methodology begun by extracting the available\n data from independent deep learning models of each sensor\n that were trained to localize and classify flying objects. The\n data are feature maps that are extracted from intermediate\n layers of the DNN architectures of each modality (the\n individual architectures are described in more detail later in\n this section). A CNN based architecture for the thermal\n features was created as a base model. Two more models were\n created and optimized, a fusion of thermal and optronic data"}, {"title": "A. Data Acquisition and Processing", "content": "The data capturing sessions took place in a town of Greece\n named Markopoulo in an open area. Three main scenarios\n were employed during data recording. A UAV coming from\n afar, a UAV taking off from the nearby ground and heading\n to a protected area and a swarm of UAVs moving towards\n and away from the sensor area. The capturing sessions lasted\n for three days, recording UAV flights during different times\n off day and night in clear and cloudy sky. The sensors were\n set at the center of the area and the operation of the drones\n was performed from two points: near the sensor equipment\n and from a location 350m away from the sensors. In the data\n capturing sessions were used multiple DJI Mavick Pro, a\n single DJI Matrice 210 and a single DJI Phantom 4. In the\n swarm flight scenarios for the majority of the flight plans\n were utilized a DJI Phantom 4, one DJI Matrice 210 and two\n DJI Mavick Pro. In the capturing sessions the UAVs' max\n speed was up to 15m/s while their distance from the sensors\n reached up to 1300m with a maximum altitude of 100m from\n the sensor level. The labeling of the sensory data was held\n manually. During each capturing session, video files were\n recorded for the optical and thermal sensors that were latter\n processed. A frame per second was extracted containing\n UAVs or other background objects and manual annotation\n took place by drawing a bounding box for the aerial objects\n and noting down their coordinates in the image as well as\n their classification label. The radar data were compared with\n the UAV's GPS data in the time domain to formulate the\n ground truth. The complete system responsible for the UAV\n detection consists of a 2D radar, a thermal and an optronic\n camera sensor (Figure 1). Each sensor, individually captures\n data from its environment that are send for processing to a\n laptop connected to it. The raw data pass through deep\n learning algorithms that have been tuned to detect objects and\n classify them as UAVs or false alarms (e.g., birds, noise, etc.)\n (Figure 2).\n The feature maps [39] from the independent classifiers\n are extracted and they are sent to the main fusion algorithm\n as input data. The radar module receives as input a list of\n detections and radar features that are stemmed from radar\n signal processing algorithms. The annotation includes ground"}, {"title": "1) Radar Data", "content": "The radar sensor is an X-band LFMCW surveillance radar. It\n operates at 9.35GHz and has a transmission power of 4W\n with a Pulse Repetition Frequency (PRF) of 3.3 kHz. The\n sensor's detection range is up to 4km and during the data\n capturing sessions it was configured both for short (up to\n 400m) and long-range detection (up to 4km). The 2D radar\n produces raw data that are in the form of a matrix containing\n complex values that refer to Quadrature and In-phase receiver\n channels. The raw data are converted to Range Profile\n Matrices [40] with the application of calibration, radar\n equation correction and Fast Fourier Transform (FFT). These\n matrices are then divided in samples and are processed by\n FFT, resulting in a list of detections that essentially represent\n a Range Doppler Matrix. From each detection, the Range\n Profile Matrix signature can be extracted which is the first\n input to the sensor's DNN model that is consisted of two\n pipelines of convolutional networks. The second input to the\n model is a set of radar signature features that are extracted\n from the Range Doppler Matrix and represent the detection's\n amplitude, the ratio between the detection amplitude versus\n the mean amplitude and the radial velocity of the target. The\n classification and localization model are based on CNNs with\n two input streams. The Range Profile Matrix features are fed\n to the model's first input stream which is consisted of two\n convolutional layers with ReLU activation function and\n average pooling layers, while the second input stream is\n comprised by the RCS, SNR and Radial Velocity data that\n pass through a convolutional layer whose output is\n concatenated with the first stream's output. The two streams\n are eventually connected through a concatenation layer\n before they pass through a fully connected layer that\n classifies between a UAV or not. The output of the\n concatenation layer is a feature map with size of 1664\n elements that also comprises the radar input to the fusion\n model. The complete process of the radar sensor data\n processing and classification is described in detail in [9]."}, {"title": "2) Optronic Data", "content": "The optronic sensor is a PTZ camera that captures and\n transmits video with a resolution of 1920 x 1080 pixels at 25"}, {"title": "3) Thermal Data", "content": "The thermal sensor is a Spynel camera that continuously\n provides a 360o coverage of the area. The sensor captures\n video and images of size 12288 \u00d7 640 pixels and they are\n extracted at a rate of 2Hz to be fed to a Faster R-CNN [43]\n object detection and classification model that is running on a\n laptop connected to the sensor. The model may detect more\n than one occurrence of objects in a frame and it classifies\n them as UAVs or background. A modified version of ResNet-\n 50 is used as the base CNN of the Faster R-CNN architecture\n designed to handle large high resolution images rapidly. The\n feature maps that are used in the fusion model were extracted\n with the same method as in the case of the visible sensor and\n they are represented by a vector of size 7 \u00d7 7 \u00d7 1024."}, {"title": "B. Preprocessing and Data Flow", "content": "The fusion model incorporates data received from the three\n modalities occupying a total of 54.8 GB of hard disk storage.\n The data were stored in NumPy arrays (.npz files) that contain\n the feature maps and their annotation information (ground\n truth labels, timestamps and metadata). There is a total of 29\n recordings for each sensor. A python script loads and process\n them one by one aiming to register the data samples of the\n three sensors. This process breaks in two tasks: the feature\n matching between the thermal and optronic data and the\n registration between radar and the already matched two-\n modality data. The data shape of a single instance is (7, 7,\n 512) for the optronic modality and (7, 7, 1024) for the thermal\n modality. Each feature map corresponds to a detection from\n an extracted frame of the captured video. Both the optronic\n and infrared frames are extracted at the same time. Therefore,\n their feature maps were easily matched in the time domain\n with high alignment accuracy. The vectors from these two\n sensors are then merged into a new feature vector of shape (7,\n 7, 1536) by staking them across the last axis of the three-\n dimensional array.\n Once the match between the first two sensors is\n completed, the resulting samples are further compared with\n the corresponding radar recordings in the time domain with a\n threshold of one second. For each thermal-optronic instance,\n the closest radar data sample with the same classification\n label is matched with it in order to create a new set of\n temporally aligned features. The pair of the thermal-optronic"}, {"title": "C. Fusion Method", "content": "Three CNN based architectures were developed in total that\n utilized feature maps from the independent object detection\n and classification models of each sensor. An architecture\n relying purely on thermal data, an architecture based on\n fusion of thermal and optronic data and the proposed three\n modality architecture. The three models were compared to\n better measure the impact of the multimodal architecture. All\n the architectures were based on convolutional neural\n networks and were evaluated on the same testing set."}, {"title": "1) Convolutional Neural Networks", "content": "Artificial neural networks are vastly used for classification\n tasks of text, audio and image. In image classification are\n usually employed the Convolutional Neural Networks [12].\n A CNN at its most simple form contains three primary layers,\n a convolution, a pooling and a fully connected layer.\n Convolution, that is consisted of filters and features maps is\n a mathematical operation that is performed by sliding the\n filter along the input vector. At each place of the sliding, the\n element-wise multiplication of the matrices is performed and\n the result is summed. That way, important features are\n extracted from the input vectors through the trained filters.\n The output of the filter applied to a previous layer is called a\n feature map. Feature maps [39] are the resulting outputs of\n the convolution layers inner product of the linear filter and\n the underlying respective field followed by a non-linear\n activation function at every local portion of the input. The\n filters are usually two-dimensional vectors of odd size (1\u00d71,\n 3\u00d73, 5\u00d75).\n After each convolutional layer a non-linear activation\n function (ReLU, sigmoid, tanh, etc) is applied to convert the\n linear values obtained by the matrix multiplication to non-\n linear. A pooling layer may be employed into a CNN's\n architecture to reduce the image size and compress each\n feature map. The convolution, activation and pooling layers\n are followed by a fully connected layer. The convolution,\n activation and pooling layers are followed by a fully\n connected layer. Their output is flattened and is fed to the\n fully connected layer which in turn may feed another deep\n layer or the output layer that is responsible for the\n classification's outcome."}, {"title": "2) Fusion Architecture", "content": "The proposed model was developed in Keras, which is an\n easy-to-use deep learning library build on top of Tensorflow\n 2.0. Keras allows the rapid development of prototypes and\n enables scaling to large clusters of GPUs. The three-modality\n sensor fusion architecture is based on a deep learning binary\n classification model of two inputs. The stacked features from\n the thermal and optronic modalities comprise the first input\n vector of dimension (7,7,1536) in Keras notation, while the\n second input is consisted of the radar feature vectors with\n dimensions (,1664). The main input layer (thermal-optronic)\n is followed by a 2D convolutional layer of 512 filters with\n filter size of 3x3 and a ReLU activation function. Its output\n is flattened and Dropout with a rate of 0.5 is applied on it.\n The resulting features are concatenated with the radar input"}, {"title": "3) Training", "content": "The training was held in a cluster of two GPUs NVIDIA\n Tesla K40m with 12GB of GDDR5 memory each and with\n 100 GB of total available RAM. The data preprocessing\n utilized 18.0 GB of RAM and it was executed for 40 minutes.\n The preprocessed data that were saved into NumPy arrays\n were loaded from the hard drive during the model's training\n phase. Furthermore, the total execution time for the model's\n training utilizing both of the GPUs, consumed on average 17\n minutes with a 60 seconds average execution time of each\n iteration. The total memory utilization during training was\n 27GB. The available data consist of 29 recordings, 26 of them\n were used for the training and validation sets and three of\n them were left out to be used as the final testing set. The\n training and validation sets were split in 80% and 20%\n respectively counting a total number of 17633 samples.\n The network's hyper-parameters were tuned by\n fitting the network many times on different parameter\n combinations. The following parameters were tested with\n different values: learning rate, batch size, number of\n convolutional nodes, number of dense nodes, and activation\n function. The optimal hyper-parameters for the model\n applied on the dataset were found to be lr=0.0001,\n batch_size=12, convolutional_nodes=512, dense_nodes=512\n and activation_function=ReLU. The model was configured to\n run for 160 epochs. Early stopping was applied with the\n patience parameter being set to 10 epochs monitoring the\n validation loss. The training set was comprised of 13224\n samples and the validation set of 4409.\n Because of the early stopping parameter, the training\n stopped at epoch 12, which means the model is able to\n classify with ease between the two classes. Up to epoch 10,\n the validation accuracy (Figure 4) increases along with the\n training accuracy, indicating a strong model that does not\n overfit the data. It appears that there is no generalization\n benefit starting at epoch 11. The accuracy remains constant"}, {"title": "D. Evaluation Metrics", "content": "A system deployed in the field aiming to detect and\n categorize a possible threat needs to be fast toward the object\n detection and recognition task, it should detect all the\n available targets and it should have maximum classification\n accuracy regarding the targets. However, accuracy alone is\n not enough especially in cases where there are too many false\n alarms such as birds.\n In sensor fusion systems the considered metrics are\n confidence, accuracy, timeliness, throughput and cost\n 45][46]. However, this work focuses on the binary\n classification problem, that of fusing already processed data\n from multiple modalities \u201coffline\u201d rather than proposing a\n real time system for UAV detection. Therefore, well known\n metrics that are employed to evaluate classification problems\n in machine learning are adopted such as Precision, Recall,\n F1-score, Accuracy and ROC curve.\n Precision is the fraction of relevant samples among\n the retrieved instances, whilst Recall also called Sensitivity,\n is the fraction of relevant samples that have been retrieved\n over the total amount of relevant instances [47]. Recall can\n be also interpreted as the ratio of correct positive predictions\n to the total positive examples. F1-score, which is used as the\n basic metric for comparison between the algorithms of this\n work, is the harmonic mean of the precision and recall.\n $F1 = 2 * \\frac{Precision * Recall}{Precision + Recall}$\n For classification tasks, the number of correctly\n classified objects is called true positives (TPs) and true\n negatives (TNs). TPs represent the number of samples\n correctly classified in the positive class while TNs are the\n number of samples correctly classified in the negative class.\n The number of incorrect detection classifications is shared\n between the false positives (FPs) and false negatives (FNs).\n Again, FPs represent the number of detections falsely\n classified in the positive class while FNs are the number of\n detections falsely classified in the negative class.\n $Precision = \\frac{TP}{TP + FP}$"}, {"title": "III. RESULTS", "content": "The system was evaluated on three recordings that were\n preprocessed in the same way as the training set. The first\n recording was captured during the morning. The maximum\n range of the UAV from the sensors was 700 m while the\n UAV's maximum speed was 10 m/s. The second recording\n was captured during midday. The UAV's maximum range\n was 1 km and its maximum speed was 14 m/s. The third\n recording was captured in the morning and the UAV's\n maximum range from the sensor was 1km while its maximum\n speed was 11 m/s. In all three recording the maximum\n altitude of the UAV was 80 m from the sensors level.\n The recordings were mixed and shuffled (since the\n algorithm has no time dependencies) and produced a total\n number of 3209, 2662 and 1956 samples for the single\n modality (thermal), the two modalities (thermal-optronic)\n and the three modalities respectively. The reason for the\n different number of samples of each model, is the data\n registration procedure between the different modalities. The\n two modalities matched less data samples than the existed\n number of the thermal modality, while the three modalities\n matched even less. Because of the natural stochasticity of the\n deep neural networks (each execution may have a slightly\n different result), the models were trained five times and an\n average F1-score from each model was obtained. The results\n showed a 0.91 F1-score for the thermal modality alone, 0.93\n F1-score for the two modalities while in the three modalities\n case, the model achieved a 0.95 average F1-score (Table 1)."}, {"title": "V. FUNDING", "content": "This research was funded by the European Union's Horizon\n 2020 Research and Innovation Program Advanced holistic\n Adverse Drone Detection, Identification and Neutralization\n (ALADDIN) [Grant Agreement No. 740859]."}], "equations": ["F1 = 2 * \\frac{Precision * Recall}{Precision + Recall}", "Precision = \\frac{TP}{TP + FP}", "Recall = \\frac{TP}{TP + FN}"]}