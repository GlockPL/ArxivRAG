{"title": "Fairness at Every Intersection: Uncovering and Mitigating Intersectional Biases in Multimodal Clinical Predictions", "authors": ["Resmi Ramachandranpillai", "Kishore Sampath", "Ayaazuddin Mohammad", "Malihe Alikhani"], "abstract": "Biases in automated clinical decision-making using Electronic Healthcare Records (EHR) impose significant disparities in patient care and treatment outcomes. Conventional approaches have primarily focused on bias mitigation strategies stemming from single attributes, overlooking intersectional subgroups - groups formed across various demographic intersections (such as race, gender, ethnicity, etc.). Rendering single-attribute mitigation strategies to intersectional subgroups becomes statistically irrelevant due to the varying distribution and bias patterns across these subgroups. The multimodal nature of EHR - data from various sources such as combinations of text, time series, tabular, events, and images - adds another layer of complexity as the influence on minority groups may fluctuate across modalities. In this paper, we take the initial steps to uncover potential intersectional biases in predictions by sourcing extensive multimodal datasets, MIMIC-Eye and MIMIC-IV ED, and propose mitigation at the intersectional subgroup level. We perform and benchmark downstream tasks and bias evaluation on the datasets by learning a unified text representation from multimodal sources, harnessing the enormous capabilities of the pre-trained clinical Language Models (LM), MedBERT, Clinical BERT, and Clinical BioBERT. Our findings indicate that the proposed sub-group-specific bias mitigation is robust across different datasets, subgroups, and embeddings, demonstrating effectiveness in addressing intersectional biases in multimodal settings.", "sections": [{"title": "Introduction", "content": "Electronic Healthcare Records (EHR) have revolutionized the optimization and delivery of healthcare, providing a digital database of patient information that enhances the efficiency and effectiveness of clinical decision-making (Seymour, Frantsvog, and Graeber 2012). Automating clinical decision-making using EHR is not without significant challenges, particularly with the perpetuation and amplification of biases. These biases can adversely impact the quality and equity of care provided to diverse patient populations and their intersections (Pivovarov et al. 2014; Boyd et al. 2023; Rouzrokh et al. 2022; Banerjee et al. 2023).\nExisting measures for intersectional biases in healthcare (Ogungbe, Mitra, and Roberts 2019; Okoro, Hillman, and"}, {"title": "Background", "content": "We consider binary and multitask (binary) classifications. The data, D = {$X_n$}$_1$ of each instance contains information from clinical notes (EHR_notes$_n$), Xrays/radiology images (EHR_Xray$_n$), clinical events (EHR_events$_n$), laboratory test results (EHR_lab$_n$), and from the tabular structure (EHR_structured$_n$). The sensitive features S are from the EHR_structured$_n$. The data instance X encloses information from all the modalities and can be written as:\n$X_i = \\begin{pmatrix} EHR\\_notes_i, EHR\\_events_i,\\ EHR\\_lab_i, EHR\\_Xray_i,\\ EHR\\_structured_i \\end{pmatrix}$     (1)\nThe downstream prediction tasks on each X contain a ground truth label Y \u2208 {0,1}. Since we consider healthcare settings, we don't differentiate between favorable and unfavorable classes, as it depends on the underlying task at hand. We assume multiple sensitive settings, $ = {$s_1, s_2,....s_m$}, where m represents the total number of sensitive attributes in the dataset. Then we define subgroups, $SG_{g_{s1}}\\bigcap g_{s2} \\bigcap ... \\bigcap g_{sm}$ who belong to group $g_{s1}$ through $g_{sm}$ with marginal sensitive attributes $s_1, s_2,....s_m$ For example, if s = {gender, race}, gender \u2208 {male, female}, and race \u2208 {white, non - white}, then we will have 4 subgroups, $SG, (i) { female, white}, (ii) {female, non-white}, (iii) {male, white}, and (iv) {male, non-white}, |SG| = 4."}, {"title": "Algorithmic Fairness", "content": "Definition 1: Demographic Parity (DP) (Barocas and Selbst 2016) - Let f be a function f : X \u2192 Y, Y = {0,1} for binary classification, and let S, be a sensitive attribute, the function f satisfies DP if:\nP[f(x) = 1 | x \u2208 S] = P [f(x) = 1],\u2200s \u2208 S (2)\nwhere x denotes an instance of X and P[.] denotes the probability of an instance.\nDefinition 2: Equal Opportunity or True Positive Rate Parity (TPR) for a binary prediction Y and a member sub-group A, is satisfied if:\nP($\\hat{Y}$ = 1 | A \u2208 $SG_i$, Y = 1) =\nP($\\hat{Y}$ = 1 | A \u2208 $SG_j$, Y = 1)\n\u2200i, j\u2208 |SG|, i \u2260j (3)"}, {"title": "Intersectional Fairness", "content": "For defining intersectional fairness in this work, we follow (Ghosh, Genuit, and Reagan 2021).\nThe worst-case parity (WP) of measuring intersectional DP can be framed as follows (Ghosh, Genuit, and Reagan 2021):\nWP(DP) = $\\frac{min{P(\\hat{Y} | A \\in SG_i), \u2200i \\in |SG|}}{max{P(\\hat{Y} | A \\in SG_j),\u2200j \\in |SG|}}$ (4)"}, {"title": "Methodology", "content": "In this section, we describe the proposed workflow (Supplementary file, Section 1) and method in detail (Figure 2)."}, {"title": "Learning Unified Text Representations from Multimodal Data", "content": "The first step in our proposed work is to gather information from multimodal sources. To capture the information into a unified feature representation, we extend the approach described in (Lee et al. 2024), where textual representations of tabular EHR features have been learned. However, it overlooked the heterogeneous multimodal nature of the EHR data, which we address by extending the concept of textual representation learning to the remaining modalities.\nWe perform pre-processing to remove noises from the clinical texts to obtain EHR_notes. For generating radiology reports from the chest x-rays, we follow (Tanida et al. 2023). Given a chest X-ray, we run it through an object detection model to extract 29 regions of interest and run a binary image classifier to find any abnormalities. A Language model has been employed for creating the abnormality descriptions followed by a post-processing step to create EHR_Xray. For clinical events, we adopt a filtering mechanism to remove repeated events which are subsequently described in textual form, EHR_events. For laboratory results (with time stamps) we follow the box-plot outlier detection (Rousseeuw and Hubert 2018) to locate abnormal portions which are then textualized as EHR_lab. Finally, the tabular representations are textualized (EHR_structured) using the method described in (Lee et al. 2024).\nThe unified multimodal feature representation learning can be written as:\nEHR_unified($X_i$) = EHR_text($X_i$)\n\u2200$X_i$, i \u2208 {1, 2, ..., n} (6)\nHere, EHR_unified($X_i$) denotes the unified textual embedding for a patient $X_i$ from multimodal sources and EHR_text denotes the resultant textual representation."}, {"title": "Embedding and Prediction", "content": "One could argue that with the emergence of Large Language Models like GPT, training a model from scratch using embeddings is no longer necessary. However, this argument has been refuted in (Lee et al. 2024). To generate embeddings for the unified EHR concepts in the EHR_unified textual representation and enable the downstream models to learn interoperable and cross-modal features, we employ clinical LMs: Med-BERT, ClinicalBERT, and Bio-clinicalBERT, which centers on the BERT framework containing contextualized embeddings of the EHR dataset of 28, 490, 650 patients. Note that, the unified representation now contains information from all the sources unlike (Lee et al. 2024). During the training stage, tokenized data from EHR_unified is fed into the clinical LMs. These encoders produce embeddings that capture information from multimodal sources of a patient's EHR in rich and high-dimensional vector representations. We keep the encoders frozen at this stage and update the weights of the subsequent prediction models dedicated to tasks. The mathematical modeling can be represented as :\nE = Clinical LMs($c_i$), (7)\nwhere, \u2200$c_i$ \u2208 Tokenizer(EHR_unified). Once we generate the embeddings, the final output is sent to the subsequent classifiers for predictions. We train a self-attention neural network classifier to align the cross-modal features followed by fully connected layers as this combination will enhance the predictive capabilities of the model (Lee et al. 2024). This can be represented as :\n$E_{attention}$ = SelfAttention(E);\n(8)\n$E_{fc}$ = ReLU(FC($E_{attention}$))\n(9)\nThe classifier output can then be modeled:\n$\\hat{Y}$ = $\\underset{loss_{task}}{min}$ (Classifier($E_{fc}$)), (10)\nwhere loss_task is the loss tailored for the individual binary predictions. After obtaining the task predictions, an extensive bias analysis has been done to uncover potential biases."}, {"title": "Subgroup-specific Discrimination Aware Ensembling (SDAE)", "content": "The final step is to design bias mitigation strategies considering intersectionality and multimodality. In this work, we consider post-process bias mitigation for two main reasons:\n1. Data preprocessing approaches (Kamiran and Calders 2012) such as Reweighting and disparate impact remover (Feldman et al. 2015) will not be feasible in multimodality as data sources are dispersed, making the identification of data-label pairs impractical.\n2. In-processing methods such as adversarial debiasing (Zhang, Lemoine, and Mitchell 2018) often fall short of completely optimizing an adversary (Zhu et al. 2021) and their effect on intersectional subgroups is an ongoing research area, which we leave for future study.\nBased on the above observations we consider methods that enforce fairness through post-processing. Our goal is to learn"}, {"title": "Experiments", "content": "The MIMIC-Eye dataset (Hsieh et al. 2023) integrates various datasets associated with the Medical Information Mart for Intensive Care (MIMIC), offering a comprehensive collection of patient information. It includes medical images such as chest X-rays (available in both MIMIC CXR and MIMIC JPG formats), clinical data from the MIMIC IV Emergency Department (ED), detailed patient hospital journey records from MIMIC IV, and eye-tracking data capturing gaze information and pupil dilations. Detailed statistics of the dataset is given in the supplementary file, Section 2. We consider both MIMIC-Eye and MIMIC-IV ED for our experiments.\nBenchmarking tasks: In this study, we explore the downstream tasks to analyze the predictive performance. Specifically, we perform the binary classification and multitask classification (Lee et al. 2024).\n\u2022 ED Disposition - This involves predicting the post-visit destination of patients following their ED visit. This is a binary classification problem where label 1 denotes patients who were admitted to ED and 0 denotes patients who were discharged home.\n\u2022 ED Decompensation - It is a multitask binary classification and is used to predict three ED tasks simultaneously. The first task predicts the patient's discharge location (1:home, 0:not home). The second task predicts the need for an ICU. The third predicts patient mortality, specifically whether the patient will die during their hospital stay."}, {"title": "Downstream Tasks", "content": "where i \u2260 j. Similarly, the worst-case parity of intersectional TPR can be defined as:\nWP(TPR) = $\\frac{min{P(\\hat{Y} = 1 | A \\in SG_i, Y = 1 \\ } }{max{P(\\hat{Y} = 1 | A \\in SG_j, Y = 1)}}$ (5)\nwhere Vi \u2208 SG, \u2200j\u2208 SG, and i \u2260 j. For a definition of algorithmic fairness, U(S, \u0176) (DP or TPR) on the predictions \u0176, the WP can be calculated by taking the min-max ratio of values from the given set of subgroups. A value closer to 1 guarantees fair outcomes and we follow the 80% rule.\nRemark: We recommend choosing a fairness measure based on the context of the application in healthcare. The DP measure focuses solely on positive outcomes and does not consider ground truth values. However, in situations where outcomes are conditioned on the patient's acute index - for example, if all acute index values are similar - we suggest DP conditioned on acute index levels. Otherwise, we recommend using TPR as it helps in detecting underdiagnosed biases."}, {"title": "Multimodality selection", "content": "We analyze the model's performance change when supplemented with modality across embeddings. The underlying idea is that a well-designed model trained from multiple sources should not exhibit performance degradation (Chen et al. 2024) 5 when some information is missing at inference. Analysis of individual performances of modalities against multimodal models can better assist in developing more robust models (Chen et al. 2024). Following this observation, we train the downstream prediction models on an increasing subset of modalities using multiple clinical LMs. We initiate the process with a multimodal model trained only on categorical and numeric attributes, then incrementally add modalities from diagnoses, medications, orders, lab results, radiology reports, etc."}, {"title": "Intersectional Bias Analysis and Mitigation", "content": "Bias analysis: We perform a comprehensive analysis concerning multiple sensitive attributes, focusing on race, gender, and their intersectional subgroup pairs with algorithmic fairness metric, U(A, \u0176)\u2200A \u2208 SG, as detailed in Section 2.\nBias mitigation: Here, we compare our SDAE approach against state-of-the-art Reject Option Classification (ROC) (Kamiran, Karim, and Zhang 2012). Note that the ensemble, MAAT (Chen et al. 2022) relies on output probabilities which limits the applicability of post-processing methods. We compare SDAE against (i) the Base classifier with ROC (gender), (ii) the Base classifier with ROC (race), and (iii) the base classifier with ROC (gender+race)."}, {"title": "Related Works", "content": "There have been numerous pieces of research on biases in various clinical data sources and tasks (Pivovarov et al. 2014; Boyd et al. 2023; Banerjee et al. 2023; Tripathi et al. 2023; Arias-Garz\u00f3n et al. 2023). These studies have overlooked the biases in multimodal data sources with varying intensities and patterns. Recent works have shown efforts in identifying intersectional biases in clinical decision tasks (Ogungbe, Mitra, and Roberts 2019; Okoro, Hillman, and Cernasev 2022). However, to the best of our knowledge, there is no research in identifying and mitigating intersectional biases in tasks utilizing extensive multimodal data from all the sources, especially in the medical domain. Current intersectional bias mitigation strategies (Foulds et al. 2020; Chen et al. 2022) cannot be extended to multimodality as these are tailored for their specific in-process/pre-process algorithms, limiting their flexibility to multimodal models."}, {"title": "Conclusions and Future Works", "content": "Our work introduced a novel framework, SDAE, designed for bias mitigation involving intersectional subgroups, utilizing multimodal information from clinical databases. We have shown that learning a unified feature representation in the form of text from multimodal data sources improves the overall performance of the model. Additionally, the modality-wise performance metrics illustrate that multimodal models positively contribute to the overall prediction performance in binary and multitask classifications. The bias analysis focused on the intersectional subgroup level revealed the inability of traditional single-attribute-centric approaches to capture the biases at those intersections. To account for this, we proposed a Subgroup-specific Discrimination-Aware Ensemble (SDAE), a post-processing method customized at the intersection level. The extensive intersectional fairness analysis on binary and multitask learning illustrated that the proposed SDAE improves the Worst case Demographic Parity of minorities when evaluated on single attribute and intersectional settings, without reduction in group-wise DP. This is in contrast to conventional methods, which have shown improvement in single attribute scenarios while sacrificing overall fairness values.\nThe future directions for this study will be to extend our SDAE framework to encompass mitigation using the entire pipeline including pre-processing and in-processing methods and framing a metric for measuring the intersectional biases reflecting the overall fairness change across the sub-groups."}, {"title": "Limitations", "content": "Despite the numerous advantages outlined in this work in terms of intersectional fairness and multimodal learning without performance degradation, there are limitations to our work. Our Subgroup-specific Discrimination Aware Ensembling (SDAE) will be computationally expensive if the sensitive attributes have high cardinality, resulting in a high number of intersections (in our empirical analysis we have used subgroup pairs up to 9). Also, our SDAE is designed as a post-process mitigation, and its adaptability to in-process and pre-process strategies has not been explored here. It is worth mentioning that MIMIC-Eye being a combination of MIMIC-IV and MIMIC-CXR has only 15627 stay IDs. We have overcome the limitations of small-scale data learning using clinical Language Models and showed that the performance has been reasonably maintained."}, {"title": "Ethics Statement", "content": "The performance and bias metrics used in this work are taken from the literature and not from any clinical trials or real-world scenarios. Therefore before using our work in real-world settings, it is advisable to get recommendations from healthcare experts, ethicists, and policy makers. Another important point is that the major sources of biases in this work depend on the distribution of demographic data, model selection, and modality selection. We, therefore, recommend conducting an extensive bias analysis on any real-world dataset before generalizing our method. Finally, we advocate for analyzing group-wise performance drops in fair interventions despite satisfying the overall fairness criteria as interventions can be explicitly fair."}, {"title": "Reproducibility Checklist", "content": "This paper:\n1. Includes a conceptual outline and/or pseudocode description of AI methods introduced (yes)\n2. Clearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results (yes)\n3. Provides well marked pedagogical references for less-familiare readers to gain background necessary to replicate the paper (yes)\n4. Does this paper make theoretical contributions? (no)\n5. A motivation is given for why the experiments are conducted on the selected datasets (yes)\n6. All novel datasets introduced in this paper are included in a data appendix. (yes/links are given)\n7. All novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (NA)\n8. All datasets drawn from the existing literature (potentially including authors' own previously published work) are accompanied by appropriate citations. (yes)"}]}