{"title": "H-Net: A Multitask Architecture for Simultaneous\n3D Force Estimation and Stereo Semantic\nSegmentation in Intracardiac Catheters", "authors": ["Pedram Fekri", "Mehrdad Zadeh", "Javad Dargahi"], "abstract": "The success rate of catheterization procedures is\nclosely linked to the sensory data provided to the surgeon.\nVision-based deep learning models can deliver both tactile and\nvisual information in a sensor-free manner, while also being\ncost-effective to produce. Given the complexity of these models\nfor devices with limited computational resources, research has\nfocused on force estimation and catheter segmentation separately.\nHowever, there is a lack of a comprehensive architecture capable\nof simultaneously segmenting the catheter from two different\nangles and estimating the applied forces in 3D. To bridge this gap,\nthis work proposes a novel, lightweight, multi-input, multi-output\nencoder-decoder-based architecture. It is designed to segment\nthe catheter from two points of view and concurrently measure\nthe applied forces in the x, y, and z directions. This network\nprocesses two simultaneous X-Ray images, intended to be fed by\na biplane fluoroscopy system, showing a catheter's deflection from\ndifferent angles. It uses two parallel sub-networks with shared\nparameters to output two segmentation maps corresponding to\nthe inputs. Additionally, it leverages stereo vision to estimate\nthe applied forces at the catheter's tip in 3D. The architecture\nfeatures two input channels, two classification heads for segmen-\ntation, and a regression head for force estimation through a single\nend-to-end architecture. The output of all heads was assessed\nand compared with the literature, demonstrating state-of-the-art\nperformance in both segmentation and force estimation. To the\nbest of the authors' knowledge, this is the first time such a model\nhas been proposed.", "sections": [{"title": "I. INTRODUCTION", "content": "In a cardiac catheterization procedure, a surgeon explores\nthe cardiovascular system for various purposes, including\ndiagnostic testing, measuring pressure and oxygen levels,\nconducting biopsies, or delivering treatment. This is achieved\nby steering a long, flexible tube known as a catheter into the\npatient's vascular system, typically through the groin, neck,\nor shoulder, guided by an X-Ray fluoroscopy imaging system\n[1], [2]. While the efficacy of interventional catheterization\ntreatments is well-documented, the procedure is not without its\nimperfections and safety concerns. These issues primarily fall\ninto two categories: the lack of tactile feedback and challenges\nin catheter localization. Currently, conventional catheters do\nnot offer surgeons the ability to feel or sense of touch when the\ncatheter's tip interacts with anatomical structures such as heart\nor vessel tissues. For instance, during an ablation procedure,\nthe surgeon must apply a precise force between 0.1N and\n0.3N at the catheter's tip against the targeted defective spots.\nMoreover, inserting and maneuvering the catheter can result in\nunintended movements due to the complex vascular structures\nand flexibility of both the catheter and vessels. Without en-\nhanced visualization, the risk of causing tears or abrasions,\nleading to life-threatening bleeding, increases significantly.\nEnhanced visualization minimizes these risks and ensures\nprecise navigation and accurate tissue measurement, improving\nprocedural safety and outcomes. Having an accurate visualiza-\ntion of the catheter is also crucial for developing autonomous\nor semi-autonomous robotic systems for catheterization [3],\n[4], [5], [6], [7].\nRegarding the measurement of applied forces, sensor-free\napproaches mentioned in the literature aim to reduce the\nproduction costs of catheters with embedded sensors (e.g.,\nTactiCath\u2122\u2122 contact force ablation catheter by Abbott (Ab-\nbott Park, Illinois, United States)). For instance, model-based\nmethods extract mechanical features of catheters within the\nimages to estimate the forces [8]. Learning-based models\nemploy deep learning architectures to estimate forces from the\ndeflections. The input for these models can be either extracted\nfeatures from images of the catheter, simulations, or the images\nthemselves [9], [10], [11], [12]. However, these solutions face\nthree major challenges: 1) the hand-crafted features are not\nrobust [9]; 2) learning and model-based feature extractors\nrequire a segmented shape of the catheter [11]; and 3) the\nmodels are incapable of estimating the force in the z-direction\n[12]. These models are designed for deployment on monoplane\nfluoroscopy, where the catheter's deflection along the z-axis is\nnot trackable in a single 2D image. However, Y-Net has been\nproposed to measure forces in 3D by processing two images of\nthe catheter taken by a biplane fluoroscopy [13]. Although the\nY-Net addresses the previously mentioned issue, it still relies\non a catheter segmentor in the preprocessing phase to provide\na clear shape of the catheter as input, thereby preventing the\npotential impact of objects other than the distal shaft on the\nestimation.\nFurthermore, as a separate task, surgeons need a visualiza-"}, {"title": "II. MULTI-TASK ENCODER-DECODER NETWORK", "content": "The proposed architecture provides the following solutions\nfor the mentioned issues of learning-based models in catheter\napplied force estimators and segmentation: 1) It directly ap-\nproximates forces within the X-Ray images, eliminating the\nneed for a separate segmentation engine as a pre-processing\nstep. 2) It is capable of estimating forces in the x, y, and z\ndirections. 3) it provides a stereo catheter segmentation feature.\n4) Designed as an end-to-end system, it does not significantly\nincrease the computational complexity of the overall setup.\nInspired by both U-Net and Y-Net, this architecture processes\ntwo raw X-Ray images through two parallel encoders with\nshared parameters [13], [23]. The feature maps extracted\nfrom both inputs traverses through the decoder and then they\nare directed toward two segmentation heads and one force\nestimation head. This graph outputs the segmented shape of\nthe catheter from two perspectives, along with the predicted\nforces along the x, y, and z axes. This architecture consists of\ntwo parallel sub-networks, each with its own input and output\nhead, connected through a central force estimation head. This\nconfiguration resembles the letter 'H'. Hence, for simplicity,\nwe have named it \"H-Net\". Before delving into the details\nof H-Net, the following section will describe the process of\ncompiling the requisite dataset for training and evaluating this\nmodel."}, {"title": "A. Synthetic Data Generation and Data Preparation", "content": "As noted earlier, H-Net is fed by two images of a catheter\ncaptured from two angles. Similar to Y-Net, this stereo vision\napproach allows the network to understand the catheter's depth\nand 3D deflection variations, enabling force measurement\nalong the z axis. The necessary data embodying these charac-\nteristics was generated using a mechanical setup, as detailed\nin [13]. This setup, replicating biplane fluoroscopy, uses two\ncameras positioned to capture the distal shaft of the catheter\nfrom two perspectives. The catheter's tip is manipulated by\ntwo motorized linear actuators pressing it against a force\nsensor, allowing movement in various directions. Each dataset\nrecord from this setup comprises two images along with the\nmeasured forces in the x, y, and z directions. Since the Y-\nNet architecture receives segmented shape of the catheter, the\nbackground removes by a thresholding method. Thus, both\nsegmented and original RGB images are available from the\nprevious work.\nConsidering such a configuration, H-Net is trained and\nevaluated on two types of input images. The first type includes\nthe original unsegmented RGB images compiled in [13]. The\nsecond type comprises synthetic X-Ray images. Given that\nthe ultimate goal is to deploy such models in real operation\nrooms using X-Ray data, it is crucial for the H-Net model to\nbe exposed to data closely resembling real-world scenarios. In\nfact, this approach allows for a more effective comparison with\nRGB images, which typically have less complex backgrounds\nthan X-Ray images. To this end, we propose a synthetic data\ngenerator in which it utilizes the previously explained data\nand converts them into synthetic X-Ray images. First, a region\ncontaining the aorta, superior vena cava, and heart is cropped\nfrom a real chest X-Ray (refer to the background image under\nthe grid Fig. 1). This region is selected as the background\nof synthetic images. Typically, the pixels representing these\norgans exhibit higher intensity. This selected region may\ncontain pixels with consistently high intensity (e.g., white"}, {"title": "B. Methodology", "content": "The tip of a catheter generates forces when it is pressed against\nthe surface of lumens. These forces can be correlated with\nthe deflection shapes of the distal shaft, particularly when the\ncatheter's tendons are not engaged. In practice, the catheter\nis visualized using continuous X-Ray images provided by a\nfluoroscopy imaging system. Bearing this in mind, the goal is\nto segment the catheter and measure the forces generated at the\ntip directly from these X-Ray images. This problem presents\nthe following challenges: 1) The catheter's distal shaft must be\nconsidered in three dimensions, as it can be deflected along any\nof these axes. 2) Other objects present in the X-Ray images,\nsuch as organs, can interfere with the visualization of the\ncatheter's deflection shapes, potentially leading to inaccurate\nforce estimation. 3) Apart from force estimation, catheter\nvisualization can assist surgeons with accurate measurement\nand steering. As mentioned earlier, these three challenges are\ninterdependent but have been addressed independently in [13],\n[18], [15], [25]. However, there currently exists no single\nnetwork designed to tackle both challenges simultaneously.\nThe H-Net solves the above-mentioned challenges through\na novel end-to-end architecture. The proposed architecture\nfeatures two encoders and two decoders within two separate\nsub-networks. As illustrated in Fig. 2, H-Net is a multi-input\nnetwork capable of processing two X-Ray images simulta-\nneously, providing the network with stereo vision capabil-\nities. Additionally, it is a multi-output network comprising\nthree distinct heads. Two of these heads are dedicated to\nconcurrently segmenting the input X-Ray images, addressing\nrespective classification problems. Utilizing features from both\nthe bottlenecks and the decoders, the third head is designed\nto estimate forces along the x, y, and z axes. The parameters\nof the encoder and decoder are shared between the two sub-\nnetworks, with the goal of keeping the model's complexity and\nmemory utilization low. In essence, the lower sub-network is a\nmirrored version of the upper one. The inputs of the network\nare two X-Ray images of size $I \\in R^{(h \\times w \\times c)}$ showing the\ncatheter's from two viewing angles. The encoder of a sub-\nnetwork comprises 4 convolutional blocks (b). In each block\nthere are two consecutive 2D convolution layers with kernels\nof 3 \u00d7 3 and ReLu activation functions. The output of the\nsecond 2D convolution layer is down-scaled by a 2 \u00d7 2 max-\npooling layer. The encoder extracts higher-level features as\nthe input is directed to the depth of the network. The output\nof each block is denoted by $O_{sn, b}^{end}$, where b indicates the\nnumber of blocks in sub-network sn. These feature maps are\nthen sent to the subsequent encoder block, moving towards\nthe bottleneck. Moreover, $O_{sn, b}^{copy}$, which is a copy of each\nencoder block's output before downscaling (max-pooling), is\nforwarded to the corresponding decoder block within the same\nsub-network. It's evident that the dimensions $h_u \\times w_b$ of $O_{sn,b}^{copy}$\nmatch the input size of the decoder block. In each block\nof the encoder in both sub-networks, the inputs are down-\nscaled by a factor of 2, and their feature maps are extracted\nsimultaneously. The subsequent block after b = 4 in the\nencoder, is the bottleneck block. It comprises a stack of two\n2D convolution layers with ReLu activation functions. The\nbottleneck performs two tasks: 1) It supplies the input for the\nfirst block of the decoder in each sub-network. 2) It generates\nan embedding that forms a part of the input for the force\nestimation head as follows:\n$\\begin{equation}\nGAP(O_{btn}^{copy}) = \\frac{1}{h \\times w}  \\sum_{h}^{h} \\sum_{w}^{w}  [h, w]\n\\end{equation}$"}, {"content": "Where (1) is the global average pooling that generates\n$ v  [n]$ in which v is an embedding of length n obtained\nfrom the feature maps $ \\hat{O}$ with n channels in the bottleneck\n(btn) of sub-network sn. Furthermore, $O_{btn}^{sn}$ is input to the\nfirst block of the decoder. The decoder consists of four blocks,\neach upscaling the input by a factor of 2. In every block b of\nsub-network sn, a transposed convolution increases the input\nfeature maps' size, which are then concatenated with $O_{sn,b}^{copy}$\nreceived from the corresponding encoder block. This layer is\nfollowed by 2 successive 2D convolution layers with kernels\nof 3 \u00d7 3 and a ReLu activation function. $O_{b, sn}^{dec}$ is the output of\nblock b generated by the second convolution in sub-network\nsn. In both sub-network, the above-mentioned output feeds\nthe next block as well as supplies a part of input embedding\nfor the estimation head:\n$\\begin{equation}\nU_{dec}^{bsn} [n] = GAP(O_{b,sn}^{dec})\n\\end{equation}$"}, {"content": "where $U_{dec}^{bsn}$ is the embedding of decoder's block b in sub-network\nsn produced by the the global average pooling (1). The fourth\nblock of the decoder in each sub-network reconstructs the\nfeature maps to match the size of the input images. Afterwards,\nthe output is passed through a 2D convolution layer with a\nsingle 1 \u00d7 1 kernel and a sigmoid activation function. This\nlayer maps the feature maps onto the segmentation plane $ \\hat{O}^{sn}$.\nIn fact, the segmented catheter from the X-Ray images is\nachieved by solving a classification problem in the output layer\nof each sub-network through the following loss function:\n$\\begin{equation}\nL_{seg}^{Long} (O^{sn}, t^{sn}) = - \\frac{1}{N}  \\sum_{i}^{N} [t_i^{on} log(\\hat{O}_i^{sn}) + (1 - t_i^{on}) log(1 - \\hat{O}_i^{sn})]\n\\end{equation}$"}, {"content": "the equation above is a binary cross entropy and t denotes\nthe label map for the corresponding of sub-network sn and\nN is the total number pixels. As previously discussed, the re-\ngression head receives input embeddings from the bottlenecks\n$U_{btn}^{sn=1}$ and $U_{btn}^{sn=2}$, as well as feature vectors transferred by\nevery block of the decoder in each sub-network, denoted as\n$U_{b,sn}^{dec}$ (refer to equation (1) and (2)). To this end, a part of the\nfinal input can be created as follows:\n$\\begin{equation}\nV_{sn} = v_{btn}^{von}  [|| U_{b,sn}^{dec}\n\\end{equation}$"}, {"content": "In equation (4), '||' represents a series of concatenations of\nthe feature vectors generated by block b = 1 to b = 4 (as\nmentioned in equation (2) of the decoder in each sub-network,\ndenoted as sn. The resultant vector is then concatenated (+) with . To clarify, the feature vector from the bottlenecks\nreflects the global properties of the input image. Moreover,\nthe embeddings from the decoder infuse the input image's\nattributes, while simultaneously working to eliminate elements\nother than the catheter's shape. This enables the estimation\nhead to focus on variations in the catheter's shape, thereby\nmodeling the applied forces without being distracted by noise.\nThis process results in the creation of a general vector $V_{sn}$\nfor each sub-network. Given these vectors, the input to the\nregression head is formed as follows:\n$\\begin{equation}\nV_{reg} = V_{sn=1}  V_{sn=2}\n\\end{equation}$"}, {"content": "here $V_{reg}$ incorporates the aforementioned traits from two\ndistinct angles, thereby providing the regression head with\nstereo vision capabilities. This input vector traverses through\ntwo hidden dense layers with 64 and 32 units, respectively.\nThe activation function used is ReLu except for the output\nlayer. It generates $\\hat{O}_{reg}$ as a 3D force vector through 3 units\nwith a linear activation function. The force estimation head\nsolves a regression problem by optimizing a Mean Squared\nError (MSE) loss function as follows:\n$\\begin{equation}\nL_{reg}(\\hat{O}_{reg}, t_{reg}) = \\frac{1}{d}  \\sum_{k=1}^{d}  (\\hat{O}_{reg}^{k} \u2013 t_{reg}^{k})^2\n\\end{equation}$"}, {"content": "Here, $t_{reg}$ represents the target force vector, which consists of\nd = 3 elements corresponding to the forces along the x, y, and\nz directions. As explained, H-Net is an end-to-end architecture\ncomprising two sub-networks, each with a segmentation head,\nand includes a force estimation head for the entire network.\nConsequently, the total loss function for the network is defined\nas follows:\n$\\begin{equation}\nL_{total} = \\beta_1 L_{seg}^{sn=1} + \\beta_2 L_{seg}^{sn=2} + \\beta_3 L_{reg}\n\\end{equation}$"}, {"content": "in this context, $L_{seg}^{sn=1,2}$ is derived from equation (3), and\n$L_{reg}$ is obtained from equation (6). Additionally, $\u03b2_i$ adjusts\nthe contribution weight of each loss function within the\ntotal loss. The aforementioned total loss function, denoted\nby equation (7), is optimized using the Root Mean Squared\nPropagation (RMSprop) optimizer, which tunes the parameters\nof the model. It is worth mentioning that all layers and\ntheir parameters in both the encoder and decoder are shared\nbetween the two sub-networks, with the goal of maintaining\nthe model's computational complexity and memory utilization\nat manageable levels."}, {"title": "III. EVALUATION AND DISCUSSION", "content": "This section describes the data preparation and model config-\nuration for the training and validation phases. It also provides\na benchmark for evaluating the performance of H-Net in\ncomparison to other solutions found in the literature. The\ntopics mentioned above will be covered in two subsections.\nSubsection A will first review the data preparation process in\ndetail. This will be followed by a comprehensive explanation\nof the model's configuration for training and inference. Lastly,\nthe model's performance will be evaluated for both the seg-\nmentation and force estimation heads by comparing it with\nexisting solutions in the literature in Subsection B."}, {"title": "A. Dataset Preparation and Model Configuration", "content": "H-Net utilizes three types of data, each divided into train-\ning, testing, and validation sets. The first data type includes\nRGB images of a catheter, previously used in the Y-Net\nstudy [9], [11], [13]. The second and third types consist of\nsynthetic X-Ray images. As outlined in subsection II.A, the\ndifficulty level of synthetic X-Ray images for the model is\ndetermined by the background regions, which could affect\nthe model's ability to differentiate between the catheter and\nthe background. Examples of these datasets are illustrated\nin Fig. 3. The XRay-1 samples comprise synthetic images\nwith a lower difficulty level, characterized by more consistent\nintensity levels in the selected background regions. Conversely,\nthe XRay-2 samples includes synthetic images featuring a\nmix of dark and bright pixels, randomly distributed across\nthe background mesh. Each of the datasets described above\ncontains 19,500 samples. Every sample includes two images,\neach with dimensions of 224 x 224 x 3, which are normalized\nbefore being inputted into the network. The targets for the\nsegmentation head consist of two segmentation maps, also\nwith dimensions of 224 \u00d7 224 \u00d7 1, where each pixel indicates\neither the catheter or the background. For the force estimation\nhead, the target for each sample is a force vector along the\nx, y, and z axes. Furthermore, each dataset was shuffled and\ndivided into training, validation, and test sets. The training\nset, comprising 80% of the data or 13,650 samples, was the\nlargest portion. The remaining 20% of the data was equally\nsplit between the test and validation sets, with each containing\n2,925 samples.\nThe H-Net processes two input images at a time, which\nare each fed into their respective sub-network's encoders.\nThese encoders comprise four blocks. Each block consists\nof two successive 2D convolutional (conv) layers with ReLU\nactivation functions, followed by a max-pooling layer. The\n2D conv layers are equipped with 32 filters of size 3 \u00d73\nand have a stride of 1. The max-pooling layers feature a\nkernel size of 2 \u00d7 2, with a stride of 2. The feature maps\nproduced by these encoders are then processed by the two\nbottleneck blocks in both sub-networks. These bottleneck\nblocks are similar to the encoder blocks but are equipped\nwith 2D conv layers without the max-pooling layer. In each\nsub-network of H-Net, the decoder receives the output from\nthe encoder. It upscales the feature maps through four blocks\nto generate segmentation maps for each input image at the\nsegmentation (classification) heads. Each block within the\ndecoder includes a 2D convolution transpose layer, followed\nby two convolution layers. Both the 2D convolution and 2D\nconvolution transpose layers are outfitted with 32 filters, each\nsized 3 \u00d7 3. The convolution layers have a stride of 1, whereas\nthe convolution transpose layers use a stride of 2. The final\nblock of each decoder functions as the classification head\nand includes an additional 2D convolution layer. This layer,\nequipped with a single filter of size 1 \u00d7 1, is responsible\nfor producing the segmentation map. As mentioned in sub-\nsection II.B, the H-Net estimated the applied forces through a\nregression head in which it receives the input from both the\nbottlenecks as well as the decoders. This head processes the\ninput embeddings using three stacked dense layers, with 64,\n32, and 3 units, respectively. The first two layers employ a\nReLU activation function, while the final layer uses a linear\nactivation function to output the estimated forces. The weights\nof each head's loss (3) are equal. The configured H-Net\nwas trained on the compiled training set using batches of 32\nsamples, with a learning rate of 1\u00d710\u22124, over 100 epochs. To\nprevent overfitting, the model's performance was monitored by\nevaluating it on the validation set at the end of each epoch,\nfacilitating the implementation of an early-stopping method."}, {"title": "B. Results and Discussions", "content": "Given that H-Net produces two segmentation maps for a\ncatheter from two different angles, as well as the 3D forces, the\nperformance of these two tasks will be evaluated separately. In\nassessing H-Net's capability to estimate 3D forces, the model's\nperformance is evaluated using the test set as unseen data."}]}