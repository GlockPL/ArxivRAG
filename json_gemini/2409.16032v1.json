{"title": "Deep chroma compression of tone-mapped images", "authors": ["Xenios Milidonis", "Francesco Banterle", "Alessandro Artusi"], "abstract": "Acquisition of high dynamic range (HDR) images is thriving due to the increasing use of smart devices and the demand for high-quality output. Extensive research has focused on developing methods for reducing the luminance range in HDR images using conventional and deep learning-based tone mapping operators to enable accurate reproduction on conventional 8 and 10-bit digital displays. However, these methods often fail to account for pixels that may lie outside the target display's gamut, resulting in visible chromatic distortions or color clipping artifacts. Previous studies suggested that a gamut management step ensures that all pixels remain within the target gamut. However, such approaches are computationally expensive and cannot be deployed on devices with limited computational resources. We propose a generative adversarial network for fast and reliable chroma compression of HDR tone-mapped images. We design a loss function that considers the hue property of generated images to improve color accuracy, and train the model on an extensive image dataset. Quantitative experiments demonstrate that the proposed model outperforms state-of-the-art image generation and enhancement networks in color accuracy, while a subjective study suggests that the generated images are on par or superior to those produced by conventional chroma compression methods in terms of visual quality. Additionally, the model achieves real-time performance, showing promising results for deployment on devices with limited computational resources.", "sections": [{"title": "I. INTRODUCTION", "content": "HIGH dynamic range (HDR) imaging is an important technology targeting the acquisition, display, and storage of high-fidelity images that contain a considerably wider luminance spectrum than conventional images [1]. HDR cameras [2], modern setups [3], and smartphones [4] can acquire a scene with intensities approaching the range perceivable by human vision, reliably capturing shadows and sunlight in the same image. However, due to the limited capacity of traditional monitors to display HDR images, range reduction techniques are employed to generate standard dynamic range (SDR) images that can be accurately reproduced [5]. These techniques correct primarily the luminance, but subsequent color adjustment may be performed to remove any remaining chromatic distortions or color clipping artifacts (Figure 1). Most techniques do not consider color correction. The norm in HDR management on SDR display systems is the use of a tone-mapping operator (TMO) to adjust the luminance alone [6], [7], which can often generate pixel values lying outside the target display's gamut. When SDR images are displayed, such pixels are clipped to the gamut boundaries and appear over-saturated, affecting the overall conveyed color appearance of the original HDR image.\n\u0160ikudov\u00e1 et al. [8] were the first to develop a unified tone and chroma management framework that ensures pixels lie within the target gamut, while requiring no calibrated input or tunable parameters. This technique can be used in conjunction with any existing TMO and employs compression algorithms to correct both lightness and chroma channels. Despite its advantages, the technique is computationally expensive, hampering its deployment and use on devices with limited computational resources. Current trends in digital technology are pushing towards higher resolution displays with faster and more efficient processing capabilities, calling for a further step up in computational efficiency.\nDeep learning offers a unique opportunity to tackle this problem as it can replace complex conventional image processing pipelines with simple fine-tuned models that often match their accuracy while significantly decreasing computational costs. In this work, we propose the use of an image-to-image generative model tailored for fast and reliable gamut management of tone-mapped HDR images. Following extensive experiments with state-of-the-art (SOTA) image generation and enhancement models, our approach exploits the seminal generative adversarial networks (GANs) [9]. We correct undesirable chromatic effects produced by the prior step of tone mapping through careful network optimization, and design a custom loss function to reduce hue distortions. For training, we developed a large dataset augmented by multiple TMOs. The main contributions of this work are:\n\u2022 To the best of our knowledge, the proposed approach is the first deep learning model dedicated to chroma compression and is compatible with all common TMOs, in contrast with existing gamut mapping methods with generally reduced compatibility.\n\u2022 We use a novel loss function that combines the GAN and standard L1 losses with a hue-based loss to improve chromatic accuracy.\n\u2022 The model offers significant improvements in computational speed compared to the conventional framework, making it suitable for devices with limited computational resources.\n\u2022 The model is fully automatic and data-driven, requiring"}, {"title": "II. RELATED WORK", "content": "A display system's gamut can be considered as a volume containing the colors that can be reproduced by that display [5]. If compared with the standard xy-chromaticities diagram of the overall colors perceived by the human visual system, a display's gamut is only a subset of it. Tone-mapped images often contain colors lying outside of this gamut, which need to be processed with dedicated chroma correction techniques to be accurately reproduced on displays [6]. Chroma correction has been explored for over 2 decades. Initial work attempted to match the viewing conditions between a display and the real-world scene using complex models of the human vision that partially adapted the luminance in the images [10]. Aky\u00fcz and Reinhard [11] later utilized color appearance models for preserving chromatic appearance across various scenes and display environments, but models required manual tuning of multiple parameters which hampered automation and adaptability to variable lighting conditions. Other work tried to develop simple color correction formulas using fewer parameters, focusing either on reducing out-of-gamut colors or enhancing the details in the images but with an impact on the naturalness of the scenes [12]. Pouli et al. [13] presented a parameter-free method for chroma compression with improved visual quality and adaptability to various scenes and TMOs. The same group later extended the algorithm by adding a gamut correction step to further reduce hue shifts and luminance distortion [14].\nIn parallel, \u0160ikudov\u00e1 et al. [8] developed a unified framework combining color correction with a gamut management process that reliably handles out-of-gamut pixels. This method first compresses the luminance of an HDR image using any available TMO, corrects the chroma using a novel hue-specific approach that models the target gamut, and finally both luminance and chroma are processed to ensure all pixels remain within the gamut boundaries. The authors showed that this approach is superior to aforementioned methods in handling all pixels while avoiding desaturated or over-saturated results and preserving the details and naturalness of the scene. For these reasons, this method is widely considered among the SOTA in chroma compression, despite its development in the previous decade. However, its algorithmic complexity and reliance on both the HDR and tone-mapped SDR images for calculations lead to a substantial computational burden impacting its deployment potential."}, {"title": "B. Deep learning in HDR imaging", "content": "Deep learning methods for processing HDR content have gained considerable attention in recent years, due to the significant improvement in computational speed they offer. Various neural network architectures can learn contextual information in the images and produce realistic outputs in under- or over-saturated regions, something that conventional processing pipelines are lacking [15]. Nevertheless, the vast majority of methods target tone mapping or inverse tone mapping (the reconstruction of HDR from SDR data) [6], [15], and no published work has tackled the important problem of chroma compression for accurate image visualization.\nA considerable effort was put into the reconstruction of HDR images from multi-exposure SDR images [16] or even from a single-exposure image [17], [18] using convolutional neural networks (CNN). Alternatively, Liu et al. [19] modeled the conventional HDR-to-SDR pipeline and developed a sequence of CNNs to learn the inverse process of HDR reconstruction. One of the scarce models that targeted HDR color enhancement using CNNs is the popular HDRNet [20], a multi-scale encoder-decoder that learns local affine transformations from a low-resolution version of the input image without the need to perform full image generation.\nGANs have also been used for HDR image processing due to their ability to learn the true data distribution [21]. The highly successful implementations for paired and unpaired image-to-image translation, Pix2pix [9] and CycleGAN [22]"}, {"title": "III. METHOD", "content": "To train the proposed deep learning-based chroma compression method, we considered the unified gamut management framework by \u0160ikudov\u00e1 et al. [8] (Figure 2). In this framework, luminance and color are separated according to the standard CIELCh color space assuming a specific target gamut; here, we used the sRGB with D65 white point, the boundaries of which take the form of a triangular cusp for each hue value in CIELCh coordinates (Figure 1). The chroma is compressed by linearly aligning the input and target chroma cusps while maintaining lightness and hue. Fine details in the image are preserved by splitting each hue slice into base and details layers, of which only the base is compressed. Finally, the remaining pixels lying outside the target gamut are clipped along both luminance and chroma channels. The framework was implemented in MATLAB 2022b."}, {"title": "B. Dataset", "content": "We created a dataset covering a diverse range of scenes and lighting conditions. The dataset contains 1467 HDR images at a resolution of 512\u00d7512 pixels, captured by the authors or collected from different publicly available HDR repositories [30]\u2013[38]. We carefully inspected images to exclude identical scenes or images with very low native resolution. From HDR videos, we selected a frame every 3 seconds to avoid duplicating the scene. Furthermore, we removed black and white borders from panoramic or other images.\nRather than using a single TMO to generate the reference SDR images, we used a large number of existing TMOs to augment the dataset and generalize the model (see Table III for the full list). Tone mapping was performed using a MATLAB toolbox [1]. Images were then chroma-compressed"}, {"title": "C. Model implementation and training", "content": "We propose the use of a GAN architecture, which has proven effective for an extensive range of image-to-image translation applications [9]. We have extensively evaluated the performance of this and alternative base models (compared below) and found that it performs more accurately for this task following architecture and hyperparameter optimization. The model consists of a generator and a discriminator, which compete in an adversarial fashion to produce an image and then determine whether it is fake or real, respectively, driving the optimization of the model weights (Figure 2). In [9], authors used (i) a conditional GAN objective that the generator tries to minimize against an adversarial discriminator that tries to maximize it, and (ii) a generator L1 loss to ensure generated images are visually close to the reference images. In this study, we chose a least-squares GAN (LSGAN) objective due to its increased stability during the learning process, leading to the production of higher-quality images compared to standard GANs [40]. The GAN loss is the mean squared error (MSE). The model's generator uses a U-Net architecture and a pixel-based rather than a more common patch-based discriminator to reduce tiling artifacts while keeping the details unaffected. 128 generator and 64 discriminator filters are used in the last and first convolutional layers respectively.\nFor training, we use the Adam solver with a momentum term of 0.5 and an initial learning rate of $10^{-4}$, with a linear policy and 50 decay iterations. Instance normalization is performed to improve image quality and due to its compatibility with the U-Net architecture [41]. Orthogonal weight initialization is used to speed up the convergence. No data"}, {"title": "D. Hue-based loss function", "content": "L1 and L2 loss functions are based on the absolute difference between the reference and predicted image pixel values, and have proven critical in image generation and restoration tasks [15]. However, since these losses are traditionally used on non-perceptually uniform color spaces, they consider all pixels equally and do not give weight to regions that are perceptually more important. On the other hand, perceptual metrics alone or in combination with L1 and L2 were shown to improve the visual quality of images [42], but are less useful in tasks requiring chromatic consistency (i.e. constraining rather than expanding the images' gamut), as the one concerned in this study. Since the task of color correction focuses primarily on the image's hue, we propose the use of an improved loss function that shifts the attention to the hue channel to improve the chromatic accuracy of the result. In the context of conditional GANs, the proposed hue loss can be expressed as\n$L_H(G) = ||I_H - \\hat{I}_H||_2$,\nwhere $I_H$ is the hue channel of the true (reference) image, and $\\hat{I}_H$ is the hue channel of the generated image. Images are normalized between [0, 1] before being converted to the CIELCh color space, from which the hue channel is obtained. The loss is combined with the conditional GAN objective to capture global image statistics and perceptual realism to enhance the generalization capabilities of the network, as well as the standard L1 loss to improve image production accuracy at the local level. The final objective is given below:\n$G^* = \\arg \\min_G \\min_D \\max_{max} L_{CGAN}(G, D) + \\lambda_1 L_{L1}(G) + \\lambda_H L_H(G)$\nIn equation (1), the hue loss is measured in an L2 sense due to a small improvement in performance when used in this context, compared to the L1 norm. Weights $\u03bb_1$ and $\u03bb_H$ were fine-tuned independently and in combination; the optimal values were found to be 100 and 10 respectively, which encourage the model to favor pixel-wise accuracy over its generative ability."}, {"title": "IV. EVALUATION", "content": "In all experiments described below, unless otherwise stated, the same test dataset was used for an unbiased evaluation. Reference and predicted images were converted to double-precision floating-point format and normalized between [0,1]. The performance of the models was evaluated using standard metrics (peak signal-to-noise ratio (PSNR), and structural similarity index (SSIM)) [6], metrics that focus on color accuracy (mean absolute error of the hue channel based on the CIELCh color space (MAEH), and root mean squared error of the color channels in the CIELab color space (\u0394\u0395Lab)) [43], and a recent color-aware visual quality metric (ColorVideoVDP (CVVDP)) [44]. CVVDP has just-objectionable-difference (JOD) units with a maximum possible value of 10 indicating no difference. Model performance at the local level was evaluated using the CVVDP distortion map, which overlays a heatmap with pixel-wise JOD differences between 0 and 1 over a grayscale version of the image [44]. Models were trained and tested on a GPU cluster with 8 nodes, each with 8 NVIDIA RTX A5000 (24 GB on-board GPU memory), running Rocky Linux 8.5 and using Python 3.10 and PyTorch 1.13. Test metrics and processing times for the various models are reported using the mean and were compared using one-way analysis of variance. Statistical significance was set at p < 0.05."}, {"title": "A. Comparison against SOTA models", "content": "There are no published neural networks for chroma compression of SDR images. Therefore, the performance of the proposed model was compared against well-established image-to-image translation models, including three GANs and an HDR-focused transformation model: Pix2pix [9], CycleGAN [22], Pix2pixHD [27], and HDRNet [20]. All models were trained from scratch and tested using the same SDR images as our method, and using the parameters described in the corresponding papers. Training was performed for 200 epochs for all models; HDRNet converges faster than the GANs, but it was found that the same number of epochs provides a performance improvement over fewer epochs."}, {"title": "B. Component contribution", "content": "In this experiment, the proposed model was trained sequentially by replacing critical components with other common techniques to determine their impact on performance. A model trained without an adversarial loss (L1 only) to focus exclusively on pixel-wise accuracy was also considered."}, {"title": "C. Leave-one-out TMO evaluation", "content": "To evaluate the generalizability of the model, leave-one-out experiments were performed by training the model on SDR images tone-mapped with all except one TMO. The model was then tested on images tone-mapped with the excluded TMO. This was repeated for five randomly selected TMOs."}, {"title": "D. Subjective evaluation", "content": "To evaluate the perceived visual quality of the images generated by the proposed method compared to dedicated conventional chroma-compression methods [8], [12]\u2013[14], we designed a 2-alternative forced-choice experiment (2AFC). Ten images were drawn from the test subset, covering both indoor and outdoor scenes captured in either daylight or nighttime and encompassing a large dynamic range of luminance and chromaticity (Figure 3 (a)). The study compared the images produced by each method in pairs shown in successive trials (a total of 100 pairs covering all possible combinations between ours and four existing methods). Each pair of images was displayed side-by-side over a dark grey background"}, {"title": "E. Computational performance", "content": "The inference time during testing of the proposed model was measured to evaluate its computational performance in comparison to the reference conventional framework [8]. A batch size of 1 was used to process each image in series with a single GPU. For the conventional approach, both CPU and CUDA implementations of the chroma compression process were considered."}, {"title": "V. RESULTS", "content": "Figure 4 and Table I compare the performance of the proposed model against common image generation and dedicated HDR image enhancement models. Our model significantly outperforms considered methods based on all metrics in Table I (p < 0.001). Amongst the previous models, HDRNet was found to have the next best performance in chroma compression, followed by Pix2pix. It should be emphasized that efforts to improve the performance of these models suggested that an optimized GAN architecture leads to higher overall pixel-wise accuracy; this could be because GANs target both global patterns and fine-grained local features more effectively than traditional CNN-based methods, making them more compatible with global and local TMOs [20]. At the local level, CVVDP distortion maps in Figure 4 demonstrate the performance of all models in the production of images with accurate colors. The standard Pix2pix generally performs well, but often generates tiling artifacts that can be missed by qualitative evaluation (Figure 5). Despite being an unsupervised model, CycleGAN can preserve the clarity and colorfulness of the images. However, our results indicate that CycleGAN and Pix2pixHD lack consistency in the production of color-accurate results (Figure 4). Our simple GAN architecture significantly improves pixel-wise accuracy with considerably fewer chromatic distortions, while being effective in suppressing any artifacts generated by other models.\nVarious components contribute to the increase in model performance. As shown in Table II, the pixel-based discriminator increases PSNR by 1.7 dB compared to a 70 \u00d7 70 Patch-GAN discriminator, instance normalization increases SSIM by 0.005 compared to batch normalization, and orthogonal weight initialization improves MAEH by 0.024 compared to normal initialization. Additionally, the hue-based loss function improves MAE\u043d by 0.006 compared to the standard loss (GAN + L1). A vanilla loss function without an adversarial or hue-based component (L1) provides similar performance across all metrics, but is still inferior overall to our full hue-based loss.\nFigure 5 shows the impact that the size of the discriminator's receptive field has on the generated images. A standard patch-based discriminator [9] often generates tiling artifacts in the images, particularly in bright areas and usually small in size. These artifacts are removed by the use of a discriminator evaluating all individual pixels (i.e., 1 \u00d7 1 patches), without affecting the colorfulness or spatial sharpness of the results."}, {"title": "VI. CONCLUSIONS", "content": "In this work, we proposed a method for fast, accurate, and generalizable chroma compression of tone-mapped images. To the best of our knowledge, this is the first deep learning approach performing chroma compression, offering excellent performance in terms of pixel-wise accuracy, color accuracy, and imaging artifacts. Importantly, the model's performance based on PSNR and SSIM is superior to that reported for a variety of other tasks in HDR imaging [15], [20]. Despite being of secondary importance, the proposed method also matches [8], [13] or surpasses [12], [14] the visual quality of the results of conventional SOTA frameworks for chroma compression. The reference framework by \u0160ikudov\u00e1 et al. [8] was chosen as the foundation due to its superiority over competing methods, which can generate similarly visually pleasing results (e.g. [13]) but are more susceptible to hue distortion artifacts and out-of-gamut pixels, as previously demonstrated [8]. Therefore, our model, trained to be a significantly faster alternative to the conventional framework, shows great promise for applications where efficient and accurate color reproduction is of paramount importance. Our work offers a simple solution that could be implemented on devices with limited computational resources and software employing any TMO for handling HDR data, such as smartphones and tablets, reducing chromatic artifacts in captured images and videos. Nevertheless, the proposed method is limited to processing images for specific display properties (sRGB gamut), and further work is needed to develop a general-purpose chroma compression approach. Finally, while the proposed method is theoretically adaptable to video processing, future research could incorporate it into a pipeline for real-time HDR video management."}]}