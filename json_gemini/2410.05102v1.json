{"title": "SPARSEPO: CONTROLLING PREFERENCE ALIGNMENT OF LLMS VIA SPARSE TOKEN MASKS", "authors": ["Fenia Christopoulou", "Ronald Cardenas", "Gerasimos Lampouras", "Haitham Bou-Ammar", "Jun Wang"], "abstract": "Preference Optimization (PO) has proven an effective step for aligning language models to human-desired behaviors. Current variants, following the offline Direct Preference Optimization objective, have focused on a strict setting where all tokens are contributing signals of KL divergence and rewards to the loss function. However, human preference is not affected by each word in a sequence equally but is often dependent on specific words or phrases, e.g. existence of toxic terms leads to non-preferred responses. Based on this observation, we argue that not all tokens should be weighted equally during PO and propose a flexible objective termed SparsePO, that aims to automatically learn to weight the KL divergence and reward corresponding to each token during PO training. We propose two different variants of weight-masks that can either be derived from the reference model itself or learned on the fly. Notably, our method induces sparsity in the learned masks, allowing the model to learn how to best weight reward and KL divergence contributions at the token level, learning an optimal level of mask sparsity. Extensive experiments on multiple domains, including sentiment control, dialogue, text summarization and text-to-code generation, illustrate that our approach assigns meaningful weights to tokens according to the target task, generates more responses with the desired preference and improves reasoning tasks by up to 2 percentage points compared to other token- and response-level PO methods.", "sections": [{"title": "1 INTRODUCTION", "content": "The rise of employing Large Language Models (LLMs) as conversational agents has increased the importance of aligning them with human preferences. Preference Optimization (PO), i.e. the training paradigm that aims to steer models to a desired behavior (typically related to human perception), is considered the last and most important step in the pipeline of LLM training for producing accurate, harmless and controllable models. Reinforcement Learning from Human Feedback (RLHF; Christiano et al. (2017)) was the primary method for obtaining such a behavior. However, due to it's inherent complexity it has been overpowered by Direct Preference Optimization (DPO) (Rafailov et al., 2023), a simpler, offline approach that produces a policy model that fits the preference data without the need for reinforcement learning.\nDPO performs at the sequence level, optimizing rewards and measuring KL divergence for complete responses. However, various studies have shown that signals from specific tokens are primarily responsible for learning desired behaviors, both during pre-training (Lin et al., 2024) and preference optimization (Yang et al., 2024). In particular, in domains where the preference is determined by a specific aspect (e.g. sentiment, toxicity) or when the decision relies on certain subsequences (Pal et al., 2024), it is necessary to consider more fine-grained updates. To further illustrate this point,"}, {"title": "2 METHODOLOGY", "content": "The purpose of aligning models with human preferences is to steer model behavior to produce human-acceptable responses. To realize that, we assume training data in the form of static, paired preferences. A prompt x is associated with two responses, chosen $y_c$ and rejected $y_r$, so that $y_c$ is preferred over $y_r$ ($y_c \\succ y_r$), resulting in a dataset $\\mathcal{D} = \\{x^{(i)}, y_c^{(i)}, y_r^{(i)}\\}_{i=1}^N$. Such responses and their rankings are typically collected either by humans or automatically from other models (Xu et al., 2024). In PO, we aim to train a model to generate responses closer to $y_c$.\nIn the standard Reinforcement from Human Feedback (RLHF) pipeline (Ziegler et al., 2019) this is realized in a sequence of steps. Firstly, we perform supervised fine-tuning on the task for which we would like to learn preferences, to shift the distribution of the language model in-domain with the PO data. Then, a reward model is trained, responsible for assigning a higher score (reward) to chosen responses and lower scores to rejected ones. Given a policy network $\\pi$ (i.e., the model that we aim to optimize), responses are sampled and then scored by the reward model. The policy training aims to maximize the rewards associated with chosen responses and minimize those of rejected ones, subject to a KL constrain with a reference model $\\pi_{ref}$. The constrain prevents the policy $\\pi$ from deviating too much from the distribution that the reward model has learned, as well as avoids reward hacking. The above process is translated into the following objective."}, {"title": "2.1 PREFERENCE OPTIMIZATION", "content": "The purpose of aligning models with human preferences is to steer model behavior to produce human-acceptable responses. To realize that, we assume training data in the form of static, paired preferences. A prompt x is associated with two responses, chosen $y_c$ and rejected $y_r$, so that $y_c$ is preferred over $y_r$ ($y_c \\succ y_r$), resulting in a dataset $\\mathcal{D} = \\{x^{(i)}, y_c^{(i)}, y_r^{(i)}\\}_{i=1}^N$. Such responses and their rankings are typically collected either by humans or automatically from other models (Xu et al., 2024). In PO, we aim to train a model to generate responses closer to $y_c$.\nIn the standard Reinforcement from Human Feedback (RLHF) pipeline (Ziegler et al., 2019) this is realized in a sequence of steps. Firstly, we perform supervised fine-tuning on the task for which we would like to learn preferences, to shift the distribution of the language model in-domain with the PO data. Then, a reward model is trained, responsible for assigning a higher score (reward) to chosen responses and lower scores to rejected ones. Given a policy network $\\pi$ (i.e., the model that we aim to optimize), responses are sampled and then scored by the reward model. The policy training aims to maximize the rewards associated with chosen responses and minimize those of rejected ones, subject to a KL constrain with a reference model $\\pi_{ref}$. The constrain prevents the policy $\\pi$ from deviating too much from the distribution that the reward model has learned, as well as avoids reward hacking. The above process is translated into the following objective.\n$J_{\\pi} = \\max_{\\pi} \\mathbb{E}_{x,y \\sim \\mathcal{D}} [r(x, y)] \u2013 \\beta D_{KL} [\\pi(y|x)||\\pi_{ref}(y|x)],$           (1)"}, {"title": "2.2 SPARSE PREFERENCE OPTIMIZATION", "content": "Motivated by the fact that not all tokens are required to infer a preference, and in order to control token-level contributions, we start by converting the previous objective (Equation 1) that operates on the sequence-level to token-level. Based on the work of Zeng et al. (2024) (TDPO), this corresponds to maximizing the following equation:\n$J_{\\pi} = \\max_{\\pi} \\mathbb{E}_{x,y_{<t}\\sim D} [A_{\\pi_{ref}}(y_t|x, y_{<t}) \u2013 \\beta D_{KL}(\\pi(y_t|x,y_{<t})||\\pi_{ref}(y_t|x,y_{<t}))]$   (2)\nwith $A_{\\pi_{ref}}(y_t|x, y_{<t}) = Q_{\\pi_{ref}}(y_t|x,y_{<t}) \u2013 V_{\\pi_{ref}}(x, y_{<t})$ being the advantage function for the reference model as the difference between the state-action Q and the state-value function V, and $\\beta$ being a tunable parameter controlling the deviation from the reference model.\nWe argue that in order to control the contribution of each token, we can add a weight in front of the token-level KL divergence term, so that not all tokens are forced to stay close to the reference model. We speculate that this will lead to more diverse generation of responses, since only a handful of important tokens that indicate preference will have to be in-distribution.\nThus, we introduce a mask function $m(y_{<t}) \\in [0,1]$, $m(y_{<t}) > \\epsilon$ that produces a scalar for each token $y_t$ in a sequence y that measures the amount of token KL participation in the loss function.\n$J_{\\pi} = \\max_{\\pi} \\mathbb{E}_{x,y_{<t}\\sim D,y_t\\sim \\pi} [A_{\\pi_{ref}}(y_t|x, y_{<t}) \u2013 \\beta m(y_{<t}) D_{KL}(\\pi(y_t|x, y_{<t})||\\pi_{ref}(y_t|x,y_{<t}))]$   (3)\nDeriving Equation 3, in a similar manner as TDPO, and assuming that the mask is dependent on the reference model alone and on previously seen tokens, $m(y_{<t}) = f_{\\pi_{ref}}(x, y_{<t})$, we end up with the below optimal policy (refer to Appendix A.1 for a detailed solution),\n$\\pi^*(y_t|x, y_{<t}) = \\frac{1}{Z(x,y_{<t})} \\pi_{ref}(y_t|x, y_{<t}) \\exp \\bigg(\\frac{1}{\\beta m(y_{<t})} Q_{\\pi_{ref}}(y_t|x,y_{<t})\\bigg);$   (4)\nwhere $Z(x, y_{<t})$ is the partition function.\nThe Bradley-Terry model (Bradley & Terry, 1952) is a popular theoretical formula employed to model the human preference distribution. As it operates on the sequence-level, its equivalent to the token-level is the Regret Preference model as previously proven by Zeng et al. (2024).\n$P_{BT}(y_c \\succ y_r|x) = \\sigma \\bigg( \\sum_{t=1}^{T_1} A_{\\pi_{ref}}(y_t^c|x, y_{<t}^c) - \\sum_{t=1}^{T_2} A_{\\pi_{ref}}(y_t^r|x, y_{<t}^r) \\bigg).$   (5)\nSolving Eq. 4 for $Q_{ref}$, considering $A = Q \u2013 V$ and substituting to Eq. 5, we obtain the final objective, named SparsePO. Our primary difference is that m is dependent on each token effectively weighting both components of the objective (refer to Appendix A.2 for the detailed solution).\n$\\mathcal{L}_{SparsePO} = -\\mathbb{E}_{x,y_c,y_r\\sim D}[\\log \\sigma (u(x, y_c, y_r) \u2014 \\delta(x, y_c, y_r))]$                                             (6)\n$u(x, y_c, y_r) = \\beta \\sum_{t=1}^{T_1} m_u(y_{<t}^c) \\log \\frac{\\pi^*(y_t^c|x, y_{<t}^c)}{\\pi_{ref}(y_t^c|x, y_{<t}^c)} - \\beta \\sum_{t=1}^{T_2} m_u(y_{<t}^r) \\log \\frac{\\pi^*(y_t^r|x, y_{<t}^r)}{\\pi_{ref}(y_t^r|x, y_{<t}^r)}$                       (7)\n$\\delta(x, y_c, y_r) = \\beta D_{MaskKL}(x, y_c; \\pi^*||\\pi_{ref}) \u2013 \\beta D_{MaskKL}(x, y_r; \\pi^*||\\pi_{ref}),$                       (8)\nwhere $D_{MaskKL}(x, y; \\pi^* || \\pi_{ref}) = \\sum_{t=1}^T m_a(y_{<t}) D_{KL}(\\pi^*(y_t|x, y_{<t})||\\pi_{ref}(y_t|x, y_{<t}))$. The objective effectively adds token-level masks $m_u$ on rewards (Equation 7) and $m_a$ on the KL (Equation 8) for each response respectively. Naturally, these masks can either be shared or be independent. In the following sections we experiment with both $m_u = m_a$ and $m_u \\neq m_a$."}, {"title": "2.3 MASK COMPUTATION", "content": "In the previous section we showed how we can control the contribution of rewards and KL divergence of each token through the introduction of weights in the loss function. Next, we introduce two strategies to obtain these weights from the reference model, one that is derived directly from its internal activations and another that is learned in parallel during preference optimization."}, {"title": "Model Activation-based Mask (MAPO)", "content": "Inspired by mechanistic interpretability approaches (Huben et al., 2023), we leverage the rich information captured per token in the activations of the reference model and aggregate them into token-level weighting masks, as follows. Let $a \\in \\mathbb{R}^d$ be the output of activation function $g(*)$ in network $\\pi_{ref}$, and $\\bar{a}$ its average value across dimensions for token $y_t$. We obtain $[\bar{a}, .., \\bar{a}]$, where $\\tilde{a_g} = (a - mean(\\bar{a_g}))/std(\\bar{a_g})$ is the standardization of $\\bar{a}$ across sequence y. Then, we define activation-based mask $m(y_{<t}) = mean\\{\\tilde{a}^t | \\forall g \\in \\pi_{ref}\\}$, i.e. the average $\\tilde{a_g}$ for all activations in the reference model. In practice, we aggregate outputs from feed-forward layers, residual connections, and attention layers, across all layers in $\\pi_{ref}$. Finally, we set $m_u(y_{<t}) = m_a(y_{<t}) = m(y_{<t})$, i.e. a common mask for the rewards and KL terms given."}, {"title": "Learnable Sparse Mask (SPARSEPO)", "content": "In our second variant, mask $m(y_{<t})$ is computed using learnable parameters. Specifically, we learn one feed-forward network (FFN) with ReLU activation for each model layer, and aggregate representations from all layers with a linear layer. A single layer mask is computed as follows:\n$m^{(l)}(y_{<t}) = ReLU (H^{(l)} (y_{<t}) \\cdot w^{(l)} + b^{(l)}),$   where $H^{(l)} \\in \\mathbb{R}^{N\\times d}$ corresponds to the reference model hidden representation for layer $l$ for N tokens and $w^{(l)} \\in \\mathbb{R}^d, b^{(l)}$ are the l-layer learned parameters. Consequently, when learning multiple masks per layer, they are combined as\n$m(y_{<t}) = ReLU (Concat (m^{(1)}(y_{<t}), ..., m^{(L)}(y_{<t})) \\cdot w_o),$   with $w_o \\in \\mathbb{R}$ the output merging vector.\nThe ReLU activation function produces a sparsity in the masks, the degree of which is dependent on the target preference data and the reference model. The mask values (independent of strategy) are utilized solely during PO training and are ignored during model inference."}, {"title": "3 EXPERIMENTS", "content": "In this section, the effectiveness of SparsePO is investigated in both proxy-preference and human-preference setups. Proxy-preference setups are analyzed through sentiment control, summarization, and code generation, whereas human-preference setup is analyzed through single-turn dialogue tasks. We refer the reader to Appendix B for further details on experimental setup."}, {"title": "3.1 MODEL COMPARISON", "content": "We compare the performance of SparsePO against supervised fine-tuning over preferred responses (SFT, serving both as a baseline and the starting point of the PO variants) and performant PO strategies that model preference at the sequence and token levels. At the sequence level, we compare against DPO (Rafailov et al., 2023), which aims to mitigate KL divergence; SimPO (Meng et al., 2024), which aims to maximize the probability difference between chosen and rejected responses; and DPOP (Pal et al., 2024), which adds a penalty term to the DPO loss to encourage high probability scores of the preferred completions. At the token level, we compare against TDPO v1 and v2 (Zeng et al., 2024), which adds token-level KL divergence as a regularization term. Unless stated otherwise, we investigate SparsePO setups learning a common mask for reward and KL terms ($m_u = m_d$) as well as learning different ones ($m_u \\neq m_a$)."}, {"title": "3.2 SENTIMENT CONTROL", "content": "Following prior work (Rafailov et al., 2023; Amini et al., 2024; Zeng et al., 2024), we use sentiment as a proxy for preference and align models to generate positive movie reviews. For the SFT model, we use GPT2-LARGE (Radford et al., 2019) trained on the IMDB dataset (Maas et al., 2011).\nTo train PO, preference data is generated by sampling two completions per review prefix from the SFT model. Then, we use a sentiment classifier as a ground-truth reward model and set chosen ($y_c$) and rejected ($y_r$) responses such that $score(y_c) > score(y_r)$, where $score(y) = p(y|positive)$ or $1 - p(y|negative)$ if y is classified as positive or negative, respectively."}, {"title": "Reward and KL Divergence Trade-off.", "content": "We start our analysis by investigating the trade-off between ground-truth reward and response-level KL divergence by estimating their Pareto frontier. For all policies, we train using $\\beta = \\{0.01, 0.1, 0.2, ..., 1, 2, 3, 4, 5, 10, 20\\}$ and for SimPO, $\\gamma$ in $\\{0.3, 1\\}$. For each policy variation, we generate one response per prompt in the test set using multinomial sampling, every 100 training steps, and report the the ground-truth reward and the average response-level KL divergence, averaged over samples.\nThe following insights can be gathered from the frontier, showcased in Figure 2. We observe that DPOP greatly restricts KL divergence and maintains a lower reward level than other systems, whereas SimPO and TDPO v1 allows for larger KL divergence and improved reward. In contrast, TDPOv2 allows slightly larger KL divergence than v1 but it does not reach higher rewards. Among our proposed systems, MaPO notably dominates the frontier at moderate divergence levels, reaching higher rewards than DPO and comparable to SimPO and TDPOv1. On the other hand, SparsePO variants allow a much larger effective KL divergence range, with higher concentration of system points at high KL values than any baseline. Regarding rewards,"}, {"title": "Sparsity and Token-level KL divergence.", "content": "Next, we analyze the trade-off between mask sparsity and token-level KL divergence throughout training, in the independent mask setup of SparsePO. Figure 3 shows results for chosen responses from systems trained at different values of $\\beta$. Firstly, we note that sparsity in the reward mask ($m_u$) always starts high (80%), increasing slightly and then steadily decreasing until the end of training, reaching as down as 20%. Such decrease is controlled by increasing $\\beta$ until 0.8, after which the trend is inverted. We hypothesize that the reward mask first learns to identify the tokens most informative for sentiment control, and increasingly expands this token set as training proceeds at a rate controllable by $\\beta$. This insight adds to previous findings (Yang et al., 2024) stating that PO-trained models can learn to identify highly rewardable tokens.\nRegarding the divergence mask, we find that increasingly higher values of $\\beta$ induce higher levels of sparsity in $m_a$, restricting the amount of tokens allowed to diverge in a sequence, which translates to lower token-level KL divergence throughout training. However, for sufficiently low values of $\\beta$, sparsity can be kept below 20%.\nIn summary, we find that low values of $\\beta$ induce scenarios where reward sparsity is high and divergence sparsity is low, meaning that the loss is dominated by the masked divergence term, $\\delta(x, y_c, y_r)$. Conversely, high values of $\\beta$ induce high sparsity on both masks, hindering learning significantly."}, {"title": "Qualitative Analysis.", "content": "Finally, we perform qualitative analysis on the learned masks by observing their token-level values on example sentences. Similarly to Figure 1, we calculate token-level rewards as the log ratio of response probabilities between policy and reference models. Token-level KL divergence is calculated as the token-level KL between policy and reference. We show the values of reward and KL divergence after the mask application in a common mask setup($m_u = m_d \\rightarrow common$) and on independent setup ($m_u \\neq m_da \\rightarrow indp$). We also compare with the TDPO baseline as the closest method to ours. Technically, when $m_u = m_d = 1$ our objective becomes equivalent to TDPO, hence we can check the influence of the proposed masks on the target objective. Figure 4a illustrates that a common mask has less sparsity compared to independent, highlighting a larger set of tokens. Comparing directly reward maps with TDPO we see that that independent mask is weighting only subsequences that express a certain polarity (watch it again), while TDPO gives a weight to all tokens in the sequence. The same stands for common masks while being slightly noisier in the tokens they cover. Looking at KL divergence maps in Figure 4b, lower values indicate minor to no divergence from the reference model. TDPO is stricter in KL control, forcing the majority of tokens to be close to the reference model, while common and sparse masks allow more diversity with higher values on particular tokens, possibly easing diversity. Heatmaps for the rejected response can be found in Figure 9."}, {"title": "3.3 HELPFULNESS & HARMLESSNESS CONTROL", "content": "Here, we investigate the effectiveness of our approach in aligning models to generate helpful and harmless responses in dialogue. We employ the Anthropic HH dataset (Bai et al., 2022), consisting of open-ended multi-turn dialogues in which humans ask a chat assistant for help, advice, or to"}, {"title": "3.4 SUMMARY QUALITY CONTROL", "content": "In this task, we employ overall summary quality as proxy for human preference, which includes quality aspects such as information coverage, faithfulness, and coherence. We use the Reddit TL;DR dataset (V\u00f6lske et al., 2017) and its preference annotations (Stiennon et al., 2020) to fine-tune a GPTJ-6B (Wang & Komatsuzaki, 2021) SFT model using QLoRA (Dettmers et al., 2023). Here we only analyze representative baselines from sequence and token-level preference modeling (DPO, TDPO v1 and v2) against MaPO and SparsePO in common mask setup.\nFor evaluation, we take 120 prompts from the test set and sample 5 completions using nucleus sampling ($p = 0.95$) and temperatures $T = \\{0.25, 0.50, 0.75, 1.0\\}$. Regarding automatic metrics, we report ROUGE-L F1 (Lin & Hovy, 2003) and BERTScore F1 (Zhang et al., 2020) for lexical and semantic relevance, respectively; self-BLEU (Zhu et al., 2018) for lexical diversity; and EDNA (Narayan et al., 2022), a metric quantifying diversity and faithfulness by combining document-summary entailment (Laban et al., 2022) and self-entailment."}, {"title": "3.5 TEXT-TO-CODE GENERATION", "content": "Finally, we perform preference optimization for the task of text-to-code generation, using a simple preference dataset created from Python programming problems from Gee et al. (2024). In this experiment, we aim to optimize for correctness, i.e., a chosen program is an executionable one that passes all accompanied unit-tests and a rejected program is one with the opposite behavior.\nThe MBPP dataset (Austin et al., 2021) is employed, which consists of 384 train, 90 validation and 500 test programs. We use StarCoder-1B (Li et al., 2023) to sample 100 solutions for each problem in train and validation with multinomial sampling. After sampling and testing the generated programs, we end up with 183 prompts with at least two passing and one failed solution for the training set and 40 for the validation set. Using the resulting data, we train StarCoder-1B with different preference optimization algorithms. We select randomly different pass-fail solutions for each prompt at every epoch. The checkpoint that has the lowest validation set loss is chosen for testing. Performance is measured in terms of functional correctness on MBPP and HumanEval (Austin et al., 2021), sampling 100 solutions with temperature 0.6 and p = 0.95 in Table 2.\nOverall, DPO shows the strongest performance across the board on HumanEval for all pass@k setups, while all methods manage to improve over the baseline SFT model. Our proposed models tend to perform on par with other PO methods although worse on pass@100. On MBPP though, SparsePO shows gains over pass@100, offering a +2% improvement compared to DPO, with a slight decay in the remaining metrics. The discrepancy between HumanEval and MBPP could be attributed to the MBPP being the in-domain PO data. These results indicate that although SparsePO is weighting more tokens as important for preference, in the code domain and in particular code execution, this requirement cannot be easily satisfied. Intuitively, it is challenging to identify which particular tokens are more responsible for a program executing correctly, as indicated by low mask sparsity levels (shown below). Arguably, the entire structure of the code needs to adhere to certain rules for syntactic and semantic correctness. Despite the above, increased pass@100 performance could be an indication of more diverse generation."}, {"title": "Sparsity and Token-level KL divergence.", "content": "Similarly to sentiment control, we also report sparsity values as a function of training steps for models trained with different values of $\\beta$. Figure 6 shows sparsity and token-level KL divergence for chosen responses (Figure 8 in the Appendix for rejected responses). Higher values of $\\beta$ do offer significant KL control, resulting into lower KL. Sparsity is much lower for reward masks and higher for KL masks, with both being relatively stable within a small range of values ($\\pm$ 4-6 points). Intuitively, since code is stricter in syntax compared to natural language, several tokens should be considered important for correct execution of a program (e.g. correct signature, indentation, return values, etc), supporting our aforementioned argument. On the other hand, larger KL divergence can offer flexibility in the surface form of code variable names."}, {"title": "4 RELATED WORK", "content": "Since the introduction of DPO, several methods have been developed to mitigate the various shortcomings of the method, mostly by introducing further constrains to the loss function. Identity Preference Optimization (Gheshlaghi Azar et al., 2024, IPO) was proposed to primarily tackle overfitting, that does not rely on the Bradley-Terry modulation assumption. Ethayarajh et al. (2024) introduced KTO, that takes advantage of that Kahneman-Tversky model of human utility. The method drops the requirement for preference pairs and is dependent only on a binary signal of whether a response is acceptable or not. To control response length and dismiss the need for a reference model, SimPO (Meng et al., 2024) uses the average log probability of the sequence (instead of the sum) while also requiring the difference between responses to be at least equal to a margin. Another method that does not require a reference model or prior supervised fine-tuning, is ORPO (Hong et al., 2024), that optimizes the odds ratio together with cross-entropy. On a similar vein, Amini et al. (2024) argues that not all preference pairs are considered equal, requiring the preferred responses to have a likelihood larger than an offset value from the dispreferred ones, based on the score assigned to each response from an external reward model. Other methods that incorporate margins between probability differences include DPO-Positive (Pal et al., 2024), where the log probability of the preferred response for the policy needs to be higher than that of the reference model. The method is particularly effective when the edit distance between responses is low, e.g in math data. Wu et al. (2024) specifically aimed at a dynamic optimization of the $\\beta$ value for each batch, proposing $\\beta$-DPO.\nCloser to our approach, there is a family of methods that focus on token-level rather than sequence-level optimization. In TDPO (Zeng et al., 2024), the sequence-level DPO objective is converted into token-level, which results in the KL divergence to act as a regularization term, optimized together with the original objective. The new loss leads to more controllable KL values throughout the course of training. Inverse-Q*(Xia et al., 2024) optimizes the same objective as PPO assigning token-level reward feedback via an estimated policy. Similarly, Token-level Continuous Rewards (Yoon et al., 2024, TLCR) incorporate a discriminator trained to distinguish positive and negative tokens (obtained from GPT-4 judgments). The confidence of the discriminator is used to assign continuous rewards to each token considering the context. Similarly to our motivation, in Selective PO (Yang et al., 2024, SePO), not all tokens are considered equal. An oracle model is trained first to identify which tokens are important in chosen and rejected responses (based on their reward values). These"}, {"title": "5 DISCUSSION", "content": "Based on the controlled experiments we conducted in the previous section, here we briefly discuss our overall findings. Firstly, based on the sentiment control analysis, SparsePO allows larger KL divergence at little to no cost in expected ground-truth reward. The $\\beta$ value is able to control sparsity in both masks, across domains, with values between 0.6 to 4 leading to mid-range sparsity levels. Depending on the domain and target preference proxy, we found that higher sparsity was present in sentiment control, highlighting a certain triviality of the task as the SFT model seems able to already identify words that are important for the target preference. On the other end, for code generation and summarization, lower sparsity between 0.2 and 0.4 seemed best in terms of alignment accuracy as executability and summary correctness are less well-defined preference proxies. For helpfulness control, optimal sparsity was found instead between 0.6 and 0.8, possibly as existence of toxic terms immediately renders response dispreferred.\nFrom our analysis over DPO, TDPO and their variants, it is important to note that, although restricting divergence at the response or token-level proves effective at maintaining the model in-domain, this does not guarantee better ground-truth rewards or better downstream task performance. For cases in which the preference proxy is complex, such as 'helpfulness', 'summary quality' or 'executability', this plain control can even hinder performance. In contrast, we devise a training procedure in which a model can learn to enhance or suppress the reward and KL divergence for each token independently. Our qualitative analysis shows that indeed for trivial tasks tokens important towards the preference get high rewards and low KL divergence, meaning they need to be close to the reference predictions to maintain preference."}, {"title": "6 CONCLUSION", "content": "We introduced Sparse Token-level Preference Optimization (SparsePO), a novel LM alignment strategy that learns to weight the reward and KL divergence for each particular token in a response during PO training. We proposed two masking strategies, obtaining model activation-based masks from the reference model and learning mask representations either commonly for both reward and divergence terms or independently. By allowing masks to be learned along with preference, we observed that they converged to a non-trivial level of sparsity which can be controlled with well-studied hyperparameters in preference optimization, while being dependent on target preference proxy. Extensive experiments across several tasks and domains, reveal that our method consistently outperforms strong baselines that model preference at the response and token-level, while assigning higher rewards and lower KL values to tokens that are important for inferring target preference. SparsePO can be easily extended to use other masking strategies and can be combined with other PO variations."}, {"title": "A MATHEMATICAL DERIVATIONS", "content": "In order to get the optimal policy, we take advantage of A($y_t$|x, $y_{<t}$) = Q($y_t$|x, $y_{<t}$)-V($y_t$|x, $y_{<t}$) and solve the following objective that includes our introduced mask m($y_{<t}$). In the following equations sometimes we omit ($y_t$|x, $y_{<t}$) for simplicity and for saving space."}, {"title": "A.1 OBTAINING THE OPTIMAL POLICY", "content": "In order to get the optimal policy, we take advantage of A($y_t$|x, $y_{<t}$) = Q($y_t$|x, $y_{<t}$)-V($y_t$|x, $y_{<t}$) and solve the following objective that includes our introduced mask m($y_{<t}$). In the following equations sometimes we omit ($y_t$|x, $y_{<t}$) for simplicity and for saving space.\n$J_{\\pi} = \\max_{\\pi} \\mathbb{E}_{x,y_{<t}\\sim D,y_t\\sim \\pi} [A_{\\pi_{ref}}(y_t|x, y_{<t}) \u2013 \\beta m(y_{<t}) D_{\\kappa \\iota}(\\pi(y_t|x,y_{<t})||\\pi_{ref}(y_t|x,y_{<t}))", "bigg": "n$=\\max _{\\pi} \\mathbb{E}_{x, y<t \\sim D, y_t \\sim \\pi}\\bigg[Q_{\\pi_{ref}}(y_{t} | x, y_{<t}) \u2013 V_{\\pi_{ref}}(y_{t} | x"}]}