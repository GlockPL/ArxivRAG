{"title": "Intuitive physics understanding emerges from self-supervised pretraining on natural videos", "authors": ["Quentin Garrido", "Nicolas Ballas", "Mahmoud Assran", "Adrien Bardes", "Laurent Najman", "Michael Rabbat", "Emmanuel Dupoux", "Yann LeCun"], "abstract": "We investigate the emergence of intuitive physics understanding in general-purpose deep neural network models trained to predict masked regions in natural videos. Leveraging the violation-of-expectation framework, we find that video prediction models trained to predict outcomes in a learned representation space demonstrate an understanding of various intuitive physics properties, such as object permanence and shape consistency. In contrast, video prediction in pixel space and multimodal large language models, which reason through text, achieve performance closer to chance. Our comparisons of these architectures reveal that jointly learning an abstract representation space while predicting missing parts of sensory input, akin to predictive coding, is sufficient to acquire an understanding of intuitive physics, and that even models trained on one week of unique video achieve above chance performance. This challenges the idea that core knowledge a set of innate systems to help understand the world needs to be hardwired to develop an understanding of intuitive physics.", "sections": [{"title": "An intuitive understanding of physics is fundamental to human cognition", "content": "An intuitive understanding of physics is fundamental to human cognition: we expect objects to behave predictably, i.e., not to appear or disappear abruptly, move through obstacles, or change shape or color arbitrarily. This basic grasp of the physical world has been documented not only in human infants but also in primates, marine mammals corvids and chicks This has been taken as evidence for the core knowledge (or core systems) hypothesis, according to which humans are equipped with a set of innate or early developing evolutionary, ancient computational systems specialized to represent and reason about basic properties of the world: objects, space, numbers, geometry, agents, etc. In the pursuit of building machines with advanced human-level intelligence, rapid progress has produced artificial intelligence (AI) systems that often surpass human performance on high-level cognitive tasks like language, coding or mathematics but paradoxically struggle in common sense physical understanding illustrating Moravec's paradox namely, that tasks trivial for biological organisms can be remarkably difficult for artificial systems, and vice versa.\nPrevious work developing AI models with the aim of improving intuitive physics understanding can be sorted into two classes: structured models and pixel-based generative models. Structured models leverage hand-coded abstract representations of objects and their relationships in an Euclidean 3D space yielding a powerful mental \u201cgame engine\" able to capture human's physical intuitions This class of models can be seen as a possible computational implementation of the core knowledge hypothesis. Pixel-based generative models take a radically opposite view and deny the need for any hard-coded abstraction. Instead, they propose a general purpose learning mechanism consisting of reconstructing future sensory inputs (e.g., images) based on past ones"}, {"title": "Here, we explore a third class of models that occupies a middle ground between these opposing views", "content": "Here, we explore a third class of models that occupies a middle ground between these opposing views, integrating features from both: Joint Embedding Predictive Architectures (JEPAs) As structured models, JEPAs posit that prediction of future world states should be done in the model's learned abstract, internal representation, and not in terms of low-level, pixel-based prediction or generation.. However, unlike structured models, JEPAs leave it to the algorithm to learn its own representation rather than hand-coding it. The mechanism consisting of predicting in representation space is congruent with the predictive coding hypothesis of cognitive neuroscience. Here we study a video version of this architecture, V-JEPA which learns to represent video frames by reconstructing masked portions of the video in representation space. We rely on the violation-of-expectation framework to probe for intuitive physics understanding without requiring any task-specific training or adaptation By prompting the model to imagine the (representation of the) future of a video and comparing its predictions with the actual observed future of the video, we obtain a quantitative measure of surprise that can be used to detect violations of intuitive physics concepts.\nWe find that V-JEPA accurately and consistently distinguishes between videos that follow the laws of physics and those that violate them. Specifically, when tasked with classifying the physical plausibility of video pairs, where one video is plausible and the other is not, a V-JEPA model trained on natural videos achieves 98% zero-shot accuracy on the IntPhys benchmark and 62% zero-shot accuracy on the InfLevel benchmark."}, {"title": "et al., 2023) perform around chance", "content": "et al., 2023) perform around chance.\nTo better understand which design choices lead to the emergence of intuitive physics understanding in V-JEPA, we ablate the effect of the training data, the pretraining prediction objective (what to predict from what), and the model size. While we observe that varying each of these components influences performance, all V-JEPA models achieve performance significantly above chance, including a small 115 million parameter model, or a model trained on only one week of unique video, thereby suggesting that video prediction in a learned representation space is a robust objective for acquiring intuitive physics understanding."}, {"title": "Measuring intutive physics understanding", "content": "Measuring intutive physics understanding\nViolation of Expectation. The violation-of-expectation paradigm has its roots in developmental psychology Subjects, typically infants, are presented with two similar visual scenes, one of which contains a physical impossibility. A 'surprise' reaction to each scene is then obtained through various physiological measures, such as relative gaze time and is used to determine whether a concept violation has occurred in the subject This paradigm has been extended to evaluate the physical understanding of AI systems where, similarly to infant trials, pairs of scenes are presented to a model with all aspects (properties of objects, number of objects, occluders, etc.) kept identical across the two scenes, apart from a single aspect or event that violates a specific intuitive physics concept. For example, a ball may roll behind an occluder but never reappear in one of the paired videos, thereby testing for the concept of object permanence. A higher surprise response attributed by the model to the impossible scenario reflects a correct understanding of the violated concept.\nVideo Prediction for intuitive physics understanding. The V-JEPA architecture has been primarily developed to improve the capacity of a model to adapt to high-level downstream tasks, such as activity recognition and action classification directly from the input without hard-wiring a cascade of intermediate representations like object contours or pose estimation Here, we test the hypothesis that the reason this architecture is successful at high-level tasks is that it has learned a representation that implicitly captures the structure and dynamics of objects in the world without the need to represent them directly.\nAs illustrated in Figure 1.B, V-JEPA is instantiated with an encoder (a neural network) that extracts representations from a video, and a predictor (also a neural network) that predicts the representation of an artificially masked part of the video, such as a randomly masked spatiotemporal block, random pixels, or future frames. This joint training of the encoder and predictor enables the encoder to learn abstract representations that encode predictable information and discard low-level (typically less semantic) features. Refer to Section A.1 in the supplementary material for more details on architecture and training.\nAfter self-supervised training, we can use the encoder and predictor networks, without any additional adaptation, to probe the model's understanding of the world. Specifically, iterating through a stream of video, the model encodes the observed pixels and subsequently predicts the representation of the following frames in the video, as illustrated in Figure 1.C. By recording the prediction error the distance between the predicted video representations and the actual encoded video representations at each time-step, we obtain a temporally aligned quantitative measure of the model's surprise throughout the video. Varying how many past video frames (context) a model can use to predict the future allows us to control for memory, while varying the frame rate of the video allows us to control for the fineness of motions. Refer to Section A.7 in the supplementary material for more details."}, {"title": "between physically plausible and implausible videos, and that the model was not trained on data from any of the benchmarks", "content": "between physically plausible and implausible videos, and that the model was not trained on data from any of the benchmarks.\nNon-conceptual interpretations of gaze-times, e.g., based on low-level processes such as perceptual preferences, are typically mitigated to some degree in these experiments by conducting a series of habituation trials prior to the violation-of-expectation trials."}, {"title": "Representation prediction learns to detect violations of intuitive physics", "content": "Representation prediction learns to detect violations of intuitive physics\nWe evaluate intuitive physics understanding on three datasets: the dev set of IntPhys, GRASP and InfLevel-lab This mix of benchmarks provides diversity in the visual quality (synthetic/photorealistic), in the diversity of scenes considered, as well as in the intuitive physics properties that are probed. Specifically, the combination of these datasets allows us to probe the understanding of object permanence continuity, shape and color constancy gravity support solidity inertia and collision See Section A.5 in the supplementary material for exact definitions.\nWe compare V-JEPA to other video models to investigate how important to intuitive physics understanding is the video prediction objective, as well as the representation space where prediction is performed. We consider two other classes of models: video prediction models that predict directly in pixel space, and Multimodal Large Language Models (MLLMs). The former set of pre-training methods have a similar prediction objective as V-JEPA, but often learn representation spaces with poor semanticity. they are useful once fine-tuned for a specific task. As a representative method, we evaluate VideoMAEv2 While different prediction objectives and pretraining data are used, this allows a comparison to V-JEPA in terms of prediction space. Given its predictive nature, VideoMAEv2 can be evaluated in the same way as V-JEPA, by predicting the future and measuring surprise via prediction error.\nThe latter class of models, MLLMs, are trained to predict text and are only interleaved with video a posteriori, making them devoid of a video prediction objective. As exemplar methods, we study Qwen2-VL-7B a state-of-the-art, open-weights, video-language model, and Gemini 1.5 pro a closed commercial model. These models are both significantly larger than V-JEPA in terms of parameter count and the amount of data they were trained on, and they learn primarily from text data. Multimodal LLMs take videos and potentially a text prompt as input and learn to generate a corresponding textual output. Due to their text-only output, those models cannot use the same evaluation protocol based on a quantitative measure of surprise. Instead, we give the model a pair of videos, asking which one of the two is impossible. Section A.7 in the supplementary material describes the detailed protocol.\nFor every method considered, we evaluate the flagship models proposed in the original works. We further compare all models with untrained neural networks, testing the learnability of intuitive physics understanding. For each property and model, the context size is chosen as the one maximizing performance, allowing the models to adapt to the different evaluation setups. This process is done for all methods, and leads to results illustrating the best performance achievable by the model. We expand on this choice in section B in the supplementary material.\nWe summarize the performance of methods across datasets on pairwise classification (i.e., detecting the impossible video in a pair) in Figure 1.A. Refer to Section F in the supplementary material for detailed results, and Section A.8 for detailed parameters used.\nWe find that V-JEPA is the only method that achieves significantly higher performance than untrained networks across all datasets, achieving average accuracies of 98% 66% 62% respectively on IntPhys, GRASP, and InfLevel-lab. These results show that prediction in a learned representation space is sufficient to develop an understanding of intuitive physics. This is done without any predefined abstractions, and without knowledge of the benchmarks during pretraining or development of the method.\nBy comparison, we find that VideoMAEv2, Qwen2-VL-7B, and Gemini 1.5 pro achieve performance that is only marginally above that of randomly-initialized models. The low performance of pixel prediction and multimodal LLMs corroborates previous findings These comparisons further highlight the benefit of V-JEPA over the existing VideoMAEv2, Gemini 1.5 pro, and Qwen2-VL-72B models. These results, however, do not mean that LLMs or pixel prediction models cannot achieve intuitive physics understanding, but merely that this seemingly simple task remains difficult even for frontier models"}, {"title": "Per property analysis of V-JEPA", "content": "Per property analysis of V-JEPA\nWe now take a closer look at the per-property performance of V-JEPA on the previously used datasets in order to obtain a more precise understanding of its intuitive physics understanding. Here, the V-JEPA encoder and predictor are based on the Vision Transformer-Large (ViT-L, instead of ViT-H for the flagship model) architecture and are trained on the HowTo100M dataset We perform a two-sample one-tailed Welch's t-test to assess whether V-JEPA provides increased performance over randomly-initialized, untrained models The results are summarized in Figure 2.\nOn IntPhys, we find V-JEPA to significantly outperform untrained networks on multiple intuitive physics properties: Object Permanence: M=85.7, SD=7.6 vs. M=51.4, SD=1.0 (t(4.0) = -8.9, p = 4.19 \u00d7 10^{-4}), with an effect size g = 9.0 (95% CI [6.3,11.7]); Continuity: M=86.3, SD=6.2 vs. M=51.2, SD=1.2 (t(4.1) = -11.3, p = 1.61 \u00d7 10^{-4}), with an effect size g = 11.0 (95% CI [7.8,14.2]); Shape Constancy: M=83.7, SD=7.8 vs. M=51.7, SD=1.2 (t(4.0) = -8.1, p = 5.96 \u00d7 10^{-4}), with an effect size g = 8.1 (95% CI [5.7,10.6]). On GRASP, we find significantly higher accuracies for V-JEPA on: Object Permanence: M=70.7, SD=7.8 vs. M=54.1, SD=5.9 (t(5.0) = -4.0, p = 5.10 \u00d7 10^{-3}), with an effect size g = 2.4 (95% CI [1.2,3.6]); Continuity: M=65.0, SD=6.1 vs. M=55.0, SD=5.0 (t(5.2) = -3.0, p = 1.36 \u00d7 10^{-2}), with an effect size g = 1.8 (95% CI [0.7,2.9]); Support: M=98.1, SD=3.0 vs. M=58.4, SD=10.5 (t(21.4) = -14.0, p = 1.48 \u00d7 10^{-12}), with an effect size g = 3.9 (95% CI [2.4,5.3]); Gravity: M=74.9, SD=2.4 vs. M=55.3, SD=4.3 (t(10.3) = -12.6, p = 6.83 \u00d7 10^{-8}), with an effect size g = 4.5 (95% CI [2.9,6.1]); Inertia: M=62.0, SD=2.4 vs. M=54.3, SD=4.2 (t(10.1) = -5.1, p = 2.36 \u00d7 10^{-4}), with an effect size g = 1.8 (95% CI [0.7,2.9]). However, we do not find a significant gain on: Color Constancy, Solidity, or Collision (p > 0.05). On InfLevel, we find significantly higher accuracies for V-JEPA on: Object Permanence: M=72.1, SD=2.9 vs. M=52.5, SD=3.5 (t(6.8) = -11.9, p = 4.46 \u00d7 10^{-6}), with an effect size g = 5.4 (95% CI [3.6,7.1]). However, we do not find a significant gain on:"}, {"title": "Gravity or Solidity (p > 0.05)", "content": "Gravity or Solidity (p > 0.05).\nV-JEPA excels at properties related to the scene's content (e.g., object permanence), but struggles with categories that require knowledge of a contextualizing event (gravity and solidity in InfLevel-lab) or the modeling of precise object interactions such as collisions. We hypothesize that these limitations come mainly from the model's framerate constraints. Nevertheless, V-JEPA demonstrates an understanding of intuitive physics while learning the required abstractions from the raw perceptual signal and without relying on strong prior information. In contrast to previous work this suggests that core knowledge is not necessary for deep learning systems to understand intuitive physics concepts.\nWe further compare V-JEPA to human performance using the private test set from IntPhys The human data is taken from where it was obtained through Amazon Mechanical Turk. For this experiment, we focus on the flagship V-JEPA architecture, using a ViT-Huge with pretraining on VideoMix2M We find that V-JEPA achieves equal or higher performance for all intuitive physics properties, as illustrated in Figure 2.B. We find that using the maximum surprise in a video, rather than the average, leads to better performance on single videos. We discuss further this distinction in Section A.7 in the supplementary material. In general, we observe lower performance in both V-JEPA and humans for videos where the physics-breaking event happens behind an occluder. Additionally, performance is well-correlated between humans and V-JEPA for the occluded settings."}, {"title": "Keys to intuitive physics understanding", "content": "Keys to intuitive physics understanding\nWe now ablate V-JEPA design choices to better understand the conditions for intuitive physics understanding to emerge. We focus on three components that play a crucial role in the model's capabilities. First, we examine the impact of the training data. The choice of data defines the learning environment of the model, with different video sources providing variations in semantic diversity, movement patterns, and quantity. Second, we consider the effect of the model size. While conventional wisdom states that larger models perform better, we also ponder the minimum size required to achieve non-trivial performance. Third, we study the influence"}, {"title": "of the pretraining prediction task", "content": "of the pretraining prediction task. Does selecting what to predict from what observed context (pretraining masking strategy) affect the model's understanding of intuitive physics?\nImportance of the pretraining task. Recall that V-JEPA models are trained to predict representations of randomly masked portions of a video, but always perform causal prediction at inference time, where the context includes frames up to some time t and the model should predict representations of frames at times greater than t. Although we compute V-JEPA's surprise using causal prediction and have observed above that this is effective for intuitive physics understanding, V-JEPA is never trained using a causal prediction task. Rather, the pre-training task is referred to as Block Masking where a large spatial block is masked for the full duration of the video. V-JEPA's performance on action and activity recognition tasks has previously been observed to vary drastically depending on the exact strategy used\nTo understand the extent to which V-JEPA intuitive physics understanding emerges specifically from the Block Masking training task, we study the effect of changing this training task, and consider two possible alternatives. Causal Block Masking is similar to Block Masking, but also fully masks the last 25% of the video, thereby incorporating future prediction into the training procedure, and Random Masking which masks random pixels in the video. Contrary to classical video tasks , we find that the prediction task is not as important for intuitive physics understanding (see figure 3.B). Whereas Random Masking leads to a drop of 20 points on average on video classification tasks the drop on IntPhys is only around 5 points on average. Interestingly, Causal Block Masking seems to perform worse than its non-causal counterpart, despite being more closely aligned to the model's prediction setup at test time. The effective performance of Random Masking, perhaps the simplest strategy, suggests that the understanding of intuitive physics does not require a tailored objective, but that predicting in an abstract representation space is the key aspect."}, {"title": "Importance of pre-training data", "content": "Importance of pre-training data. Data is a key ingredient of deep learning models and video models are no exception. Video datasets can be described along several axes, such as the number of distinct videos, the (average) duration of videos, whether videos are captured from egocentric or exocentric views, whether that camera is static or moving, and so on. We thus investigate in more detail the influence of pretraining data on intuitive physics performance. V-JEPA has previously been trained on a mixture of three popular video datasets, referred to as VideoMix2M: Kinetics 710 Something-Something-v2 and HowTo100M. Each of these datasets focuses on a different slice of the distribution of natural videos, namely, activities in K710 (e.g., playing basketball), fine-grained motion in SSv2 (e.g., throwing something), and tutorials in HowTo100M (e.g., cooking). To study the influence of training data on learning intuitive physics, we re-train V-JEPA-L models separately using only one of the three component datasets.\nUnsurprisingly, we find a strong impact of data sources on performance. Training only with videos based on motion understanding (SSv2) leads to almost chance-level performance. While more action-focused data (K710) leads to an above-chance understanding of intuitive physics, we find that tutorial videos (HowTo) yield the best performance among individual component datasets. However, HowTo is also larger than SSv2 and K710 (15 years vs. 3 months combined). We thus further examine the evolution of performance with smaller datasets coming from the same distribution by subsampling HowTo100M. We hold the compute budget fixed across these experiments such that model training always processes the equivalent of 30 years of video (by revisiting videos from the training dataset multiple times) even when only using 0.1% of HowTo100M, which represents only 128 hours of unique video in total. We find in Figure 3.C that the size of the dataset does not meaningfully impact performance, and that the model can adequately distinguish violations of intuitive physics concepts even with 128h of unique videos, maintaining a pairwise accuracy of over 70% on all considered properties.\nImportance of the encoder size. Common wisdom in the deep learning literature is that larger models perform better Here, we are also interested in the minimal size at which we observe evidence of non-trivial intuitive physics understanding. We thus investigate what happens in both directions of the scaling, using smaller and larger encoders. In Figure 3.C, we find that larger models tend to perform better. However, a 115 million parameters model still achieves an accuracy of over 85%, demonstrating a robust understanding of intuitive physics."}, {"title": "Discussion", "content": "Discussion\nIn this work, we studied the emergence of intuitive physics understanding in state-of-the-art deep learning models. By pretraining on natural videos with a simple prediction task in a learned representation space, V-JEPA exhibits an understanding of intuitive physics on both synthetic and real videos without any task-specific adaptation. Our results show that intuitive physics understanding can be acquired using a general learning principle, and thus does not require hardwired core knowledge. Although we find that the size of the model, the choice of pretraining data, and the exact pretraining task influence this understanding, its emergence can be attributed to the general framework of representation space prediction rather than a precise design choice of V-JEPA. When studying other methods such as multimodal LLMs and pixel prediction methods, we find that current models perform around chance level. Higher-capacity generative video models could potentially benefit from a certain understanding of intuitive physics in order to produce realistic videos. Yet, current evidence points to an incomplete understanding of physics in existing video generative models.\nNonetheless, the demonstrated understanding of V-JEPA is not without limitations. Indeed, V-JEPA is not uniformly accurate under all conditions. Figure 2 shows that although the accuracies are high for physical violations that imply properties intrinsic to objects (except for the color property), violations implicating interactions between objects, like solidity or collision, are close to chance. This may be due to the fact that object interactions are not very frequent in the model training data, and are not learned as well as more frequent ones. Furthermore, current JEPA models have limited memory, and consequently process very short video clips at a time (typically 3-4 seconds). V-JEPA also lacks the ability to condition its predictions on additional context, such as an action taking place, and thus predicts the future only as an observer. Although this lends itself well to the tested properties, more complex interactions are out of reach at the moment. Indeed, it could be that interactions between objects require higher-order representations, and that a more powerful hierarchical version of JEPA is needed to capture these interactions. Finally, it is also possible that an agent has to be able to interact with objects themselves in order to learn about interactions, suggesting the need to add action channels to the learning system.\nFrom a data standpoint, it would also be interesting to study models trained on videos that mimic what infants see and whether an understanding of intuitive physics also emerges in models trained on such data.\nNonetheless, through the results reported here, we believe that the latent prediction framework is a path forward toward building neural networks that understand the physical world."}, {"title": "A.1 Unsupervised pretraining of V-JEPA", "content": "V-JEPA (Bardes et al., 2024) is composed of multiple components. First, a context encoder fe whose goal is to output abstract representations of a corrupted video. A target encoder fOEMA is used to encode the full video and produce targets for the predictor. The weights of the target encoder (EMA are an exponential moving average of the weights of the context encoder 0. For an exponential moving average parameter a \u2208 [0,1] and at iteration t during training, we get the update rule of \n  \n  \u03b8 E M A t = ( 1 \u2212 \u03b1 ) \u03b8 t + \u03b1 \u03b8 E M A . \n  \n\nFinally, the predictor Po is used to predict the uncorrupted representations from the corrupted ones. During training, we start from a video V, which is corrupted into Vc, by masking random blocks in the video. The target thus becomes the complementary Vc. The training objective is thus to predict the representations of Ve from Ve by minimizing the following objective:\n  \n  || Po (fo (Vc)) \u2013 \u03b8EMA (Vc)||1.  \n  \n\n(S1)\nWhile at training time the corruption used is the removal of spatio-temporal blocks, we can see that if we instead use the first C frames to predict the rest of the video, this objective turns into a measure of error for the prediction of the future."}, {"title": "A.2 Pretraining Data", "content": "A.2 Pretraining Data\nFor the pretraining of V-JEPA we rely on multiple data sources. The original data mix that was used is VideoMix2M which is the concatenation of three datasets: Kinetics710 SomethingSomething-v2 and HowTo100M. Kinetics710 consists of around 650k videos spanning 710 action classes (e.g. kayaking, ironing, etc.), each lasting around 10 seconds. SomethingSomething-v2 is focused more on motions, where we find classes such as \"Uncovering something\" or \"Throwing something\". It consists of around 200k clips that last a few seconds on average. HowTo100M is a much larger dataset, containing around 1.2M videos lasting 6m30s on average, for a total of around 15 years of unique video data. Here, the individuals are not curated as precisely as Kinetics or SomethingSomething, yielding a more \"in the wild\" data source.\nAs discussed in the main text, most of our experiments are done with only HowTo100m, which exhibits the highest performance and demonstrates how V-JEPA can leverage uncurated data sources."}, {"title": "A.3 V-JEPA pretraining hyperparameters", "content": "A.3 V-JEPA pretraining hyperparameters\nThroughout our experiments, we use the same set of hyperparameters across models, only varying the ablated component. We provide a summary of the hyperparameters in Table S1. We rely on the same training protocol as in the original V-JEPA paper but use ROPE to encode positional information instead of absolute positional embeddings. To use ROPE on 3D data (height\u00d7widthxtime) we split the feature dimensions in 3 and use each third for a spatiotemporal-dimension.\nHere, we expand on three core elements: architecture, optimization and masking.\nArchitecture. We use the Vision Transformer (ViT) for the context encoder and target encoder. All encoders are trained to take in a clip of maximum 3 seconds over 16 frames (5.33 fps), at a resolution of 224 \u00d7 224. The video clips are flattened in a sequence of non-overlapping patches of shape 16 \u00d7 16 \u00d7 2. The predictor also uses a ViT inspired architecture, consisting of 12 blocks with a smaller embedding dimension of 384.\nOptimization. We rely on the AdamW optimizer to train the context encoder and predictor. For all experiments, we use a batch size of 3072, where we train the models for 90000 iterations. This corresponds to a total of around 26 years of (non-unique) video. The learning rate is increased linearly from 2 \u00d7 10-4 to 6.25 \u00d7 10\u22124 over the first 12000 iterations. The learning rate is then decayed to 1 \u00d7 10-6"}, {"title": "following a cosine schedule during the remaining iterations", "content": "following a cosine schedule during the remaining iterations. We stretch the schedule by a factor of 1.25, meaning that the schedule lasts for 112500 iterations of which we only perform 90000. This avoids a decay of performance at the end of training when the learning rate becomes too small, which would lead the context and target encoder to collapse partially.\nMasking. In our experiments, we rely on a few masking strategies that we describe precisely here.\n\u2022 Block masking. We mask the union of 8 blocks with a spatial scale of 0.15, as well as 2 blocks with scale 0.7. For all blocks, we use an aspect ratio sampled uniformly between 0.75 and 1.5. This strategy is used unless specified otherwise.\n\u2022 Causal Block masking. This strategy is identical to Block masking, where we additionally fully mask the last 4 frames of the video clips.\n\u2022 Random masking. This strategy randomly masks 90% of all patches in the video clips, following a uniform distribution."}, {"title": "A.4 Evaluation Data", "content": "A.4 Evaluation Data\nTable S2 Summary of datasets used for evaluation. IntPhys, GRASP and InfLevel-lab provide qualitatively different data sources to perform a more holistic evaluation of models.\nTo provide a more general assessment of the models considered, we focus on multiple data sources for evaluation, for which we summarize the main characteristics in Table S2. IntPhys is the most carefully curated data source, with pairs of videos being aligned at the pixel level thanks to the use of a simulator and each frame being individually stored (avoiding compression artifacts). Due to its formulation as a challenge with a private test set, we rely on the smaller \"dev\" subset of the data, which has publicly available labels. Nevertheless, for each pair of videos, the number of objects, occluders, and texture/shape/color of objects are randomized, which ensures that the model performs well in diverse environments.\nGRASP  is similar to IntPhys in the sense that it also uses simulated data, but covers a wider range of properties (10 compared to 3) with more total videos. A caveat to its use for our study is that it was originally designed to evaluate models on a single video rather than a pair. As such, even if the videos are paired in practice, we found some issues regarding the performance of untrained networks where they could latch on spurious features and achieve high accuracy. Some videos in GRASP can be attributed to multiple properties, so in practice, we consider that they belong to all properties when presenting the results (e.g., a video belonging to gravity and support will be counted for both gravity and support separately).\nInfLevel-lab gives us a source of natural videos where the manipulations are remarkably paired. The only visual differences between videos inside a given pair are in the lighting of the scene, to which models should be robust. Here, three properties are tested; however, for two of them (solidity and gravity), the model needs to first see a contextualizing event where the objects used during the manipulation are shown. Without seeing this pretext video, the task becomes impossible. Inflevel-lab thus requires more memory and adaptability than the tested models have. We further rename the 'continuity' property as 'object permanence' for consistency with other datasets. The difference between the two is subtle, but the experimental setup in InfLevel-lab is closer to the one of object permanence in IntPhys and GRASP.\nWe emphasize that none of these datasets are seen during training and are only used for evaluation purposes where the networks are frozen, making all of these datasets out of distribution."}, {"title": "A.5 Properties", "content": "A.5 Properties\nWe provide brief descriptions of the intuitive physical properties considered in our work:"}, {"title": "Object permanence (Baillargeon and DeVos, 1991)", "content": "Object permanence (Baillargeon and DeVos, 1991). Objects do not spontaneously materialize or vanish out of thin air. Objects also keep existing when occluded.\nContinuity Objects follow a continuous path and do not teleport in space or time. This concept is closely linked to object permanence, but leads to more subtle differences in experimental setups.\nShape and color constancy Objects do not spontaneously change color or shape.\nGravity Objects fall down without support under them.\nSupport Objects are stable when positioned on a platform, but become unstable/fall when unsupported. This is closely related to gravity, where the main difference lies in the precise experimental setup. For example, an object can be pushed off of a platform to test support, and an object can just be dropped in the air to test gravity.\nSolidity Objects cannot overlap or pass through each other. When tested behind an occluder, this property shares similarities with continuity, where an object should also not teleport across another object.\nInertia Inanimate objects do not spontaneously alter their motion, such as a change in direction.\nCollision Objects do not stay still when hit by another similar moving object.\nFor exact experimental setups, we refer the reader to the original datasets"}, {"title": "A.6 Baselines", "content": "A.6 Baselines\nPixel prediction. For our pixel prediction baseline, we use VideoMAEv2 which is trained similarly to V-JEPA. However, instead of predicting missing parts of the video in latent space, this objective is solved in normalized pixel space. Each patch of 16x16x2 pixels is first normalized before being used as a target. This makes VideoMAEv2 a good comparison with V-JEPA due to the similarity in implementation details while being a fundamentally different framework.\nMultimodal Large Language Models. for this baseline, we choose to use Qwen2-VL , one of the best open source Multimodal LLM that can handle video inputs at the time of writing, as well as Gemini 1.5 pro, a proprietary Multimodal LLM which shines at video understanding. Using Qwen2-VL"}]}