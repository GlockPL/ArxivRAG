{"title": "LEARNING TO SOLVE ABSTRACT REASONING PROBLEMS WITH NEUROSYMBOLIC PROGRAM SYNTHESIS AND TASK GENERATION", "authors": ["Jakub Bednarek", "Krzysztof Krawiec"], "abstract": "The ability to think abstractly and reason by analogy is a prerequisite to rapidly adapt to new conditions, tackle newly encountered problems by decomposing them, and synthesize knowledge to solve problems comprehensively. We present TransCoder, a method for solving abstract problems based on neural program synthesis, and conduct a comprehensive analysis of decisions made by the generative module of the proposed architecture. At the core of TransCoder is a typed domain-specific language, designed to facilitate feature engineering and abstract reasoning. In training, we use the programs that failed to solve tasks to generate new tasks and gather them in a synthetic dataset. As each synthetic task created in this way has a known associated program (solution), the model is trained on them in supervised mode. Solutions are represented in a transparent programmatic form, which can be inspected and verified. We demonstrate TransCoder's performance using the Abstract Reasoning Corpus dataset, for which our framework generates tens of thousands of synthetic problems with corresponding solutions and facilitates systematic progress in learning.", "sections": [{"title": "Introduction", "content": "Abstract reasoning tasks have a long-standing tradition in AI (e.g. Bongard problems Bongard [1967], Hofstadter's analogies Hofstadter [1995]). In the past, they have been most often approached with algorithms relying exclusively on symbolic representations and typically involving some form of principled logic-based inference. While this can be successful for problems posed 'natively' in symbolic terms (e.g. Hofstadter [1995]), challenges start to mount up when a symbolic representation needs to be inferred from a low-level, e.g. visual, representation Bongard [1967]. The recent advances in deep learning and increasing possibilities of their hybridization with symbolic reasoning (see Sec. 4) opened the door to architectures that combine 'subsymbolic' processing required to perceive the task with sound symbolic inference.\nThis study introduces TransCoder, a neurosymbolic architecture that relies on programmatic representations to detect and capture relevant patterns in low-level representation of the task, infer higher-order structures from them, and encode the transformations required to solve the task. TransCoder is designed to handle the tasks from the Abstract Reasoning Corpus (ARC, Chollet [2019]), a popular benchmark that epitomizes the above-mentioned challenges. Our main contributions include (i) the original neural architecture that synthesizes programs that are syntactically correct by construction, (ii) the \u2018learning from mistakes' paradigm to provide itself with a learning gradient by synthesizing tasks of adequate difficulty, (iii) an advanced perception mechanism to reason about small-size rasters of variable size, and (iv) empirical assessment on the ARC suite."}, {"title": "Abstract Reasoning Corpus", "content": "Abstract Reasoning Corpus (ARC) Chollet [2019] is a collection of 800 visual tasks, partitioned into 400 training tasks and 400 testing tasks. Each task comprises a few (usually 3, maximally 6) demonstrations and a test (Fig. 1). A demonstration is a pair of raster images, an input image and an output image. Images are usually small (at most 30 by 30 pixels) and each pixel can assume one of 10 color values, represented as a categorical variable (there is no implicit ordering of colors). The test is also a raster image, meant to be interpreted as yet another input for which the corresponding output is unknown to the solver.\nFor each ARC task, there exists a unique processing rule (unknown to the solver) that maps the input raster of each demonstration to the corresponding output raster. The solver is expected to infer that rule from the demonstrations and apply it to the test raster to produce the corresponding output. The output is then submitted to the oracle which returns a binary response informing about the correctness/incorrectness of this solution.\nThe ARC collection is very heterogeneous in difficulty and nature, featuring tasks that range from simple pixel-wise image processing, re-coloring of objects, to mirroring of the parts of the image, to combinatorial aspects (e.g. counting objects), to intuitive physics (e.g. an input raster to be interpreted as a snapshot of moving objects and the corresponding output presenting the next state). In quite many tasks, the black color should be interpreted as the background on which objects are presented; however, there are also tasks with rasters filled with 'mosaics' of pixels, with no clear foreground-background separation (see e.g. the left example in Fig. 1). Raster sizes can vary between demonstrations, and between the inputs and outputs; in some tasks, it is the size of the output raster that conveys the response to the input. Because of these and other characteristics, ARC is widely considered extremely hard: in the Kaggle contest accompanying the publication of this benchmark, which closed on the 28th of May 2020, the best contestant entry algorithm achieved an error rate of 0.794, i.e. solved approximately 20% of the tasks from the (unpublished) evaluation set, and most entries relied on a computationally intensive search of possible input-output mappings."}, {"title": "The proposed approach", "content": "The broad scope of visual features, object properties, alternative interpretations of images, and inference mechanisms required to solve ARC tasks suggest that devising a successful solver requires at least some degree of symbolic processing. It is also clear that reasoning needs to be compositional; e.g. in some tasks objects must be first delineated from the background and then counted, while in others objects need to be first counted, and only then the foreground-"}, {"title": "The Perception Module", "content": "The perception module comprises the raster encoder and the demonstration encoder.\nThe raster encoder is based on an attention module that allows processing rasters of different sizes, which is required when solving ARC tasks (see, e.g., the task shown in Fig. 2). However, raster sizes need to be taken into account, as they often convey crucial information about the task. To convey it to the model, we tag each pixel with the color and its (x, y) coordinates, the latter acting as a positional embedding. The image is flattened to a tagged sequence of tokens representing pixels (see the left part of Fig. 3). The tensor resulting from the Reduce block forms the fixed-length representation of the raster, subsequently fed into the demonstration encoder.\nThe raster encoder is pre-trained within an autoencoder framework, where the raster encoder is combined with a compatible decoder that can reproduce varying-length sequences of pixels (and thus the input raster) from a fixed-"}, {"title": "Solver", "content": "The latent z produced by Perception forms a compressed representation of raster images in an ARC task. As it has been almost perfectly pre-trained via auto-association (see previous section), we expect it to contain the entirety of information about the content of the input raster. However, this does not mean that it conveys the knowledge sufficient for solving the task. The role of the Solver is to map z to a latent representation z' of the program to be generated. Technically, Solver is implemented as a two-layer MLP.\nHowever, as in most programming languages, the relationship between the programs written in our DSL and their input-output behaviors is many-to-one, i.e. the same mapping from the input to output raster can be implemented with more than one program. As a result, the relationship between DSL programs and ARC tasks is many-to-many, i.e. a given task can be solved with more than one DSL program, and this very program can be a solution to more than one ARC task.\nTo account for this absence of one-to-one correspondence, we make the Solver stochastic by following the blueprint of the Variational Autoencoder (VAE, Kingma and Welling [2022]): the last layer of the Solver does not directly produce z', but parameterizes the normal distribution with two vectors $\\tau_{\\mu}$ and $\\tau_{\\sigma}$ and then calculates $z' = \\tau_{\\mu} + \\tau_{\\sigma}N(0, 1)$ where N(0, 1) is generated by the random number generator. The intent is to allow $\\tau_{\\mu}$ to represent the centroid of the set of programs that solve a given task, while $\\tau_{\\sigma}$ to model the extent of that set in the latent space."}, {"title": "Workspace", "content": "As many ARC tasks involve qualitative and combinatorial concepts (e.g. counting objects), we supplement the information provided by Perception with selected symbolic percepts inferred from the task independently from Perception, via direct 'procedural parsing' of images. We provide two data structures for that purpose:\n\u2022 Workspace Template that contains the abstract placeholders for entities that appear in all demonstrations of a given task,\n\u2022 Workspaces that 'instantiate' that template with concrete values derived from particular demonstrations.\nThe entries in a Workspace Template act as keys that index specific values (realizations) in the Workspace of a specific demonstration. For instance, the Scene symbol in the Workspace Template may have a different value in each Workspace. For the task shown in Fig. 2, the Workspace Template contains the Scene symbol, and its values in the first two Workspaces are different:\nThe list of workspace keys is predefined and includes constants (universal symbols shared between all tasks), local invariants (values that repeat within a given task, e.g. in each input raster), and local symbols (information specific to a single demonstration pair, e.g. an input Region). For a complete list of available keys, see Appendix.\nThe workspaces require appropriate \u2018neural presentation' for the Generator of DSL programs. For a given task, all symbols available in the Workspace Template are first embedded in a Cartesian space, using a learnable embedding similar to those used in conventional DL. This representation is context-free, i.e. each symbol is processed independently. We then enrich this embedding with the information in the latent z' produced by the Solver (Sec. 3.2) by concatenating both vectors and processing them with a two-layer MLP, resulting in a contextual embedding of the symbol.\nMoreover, the DSL's operations are also included in the Workspace; each of them is also embedded in the same Cartesian space so that the Generator can choose from them alongside the symbols from the workspace. In this way, the elements of the DSL are presented to the Generator (on the neural level) in the context of the given task, and symbols (such as 'red') may have a different embedding depending on the perception result. This is expected to facilitate alternative interpretations of the roles of particular percepts; for instance, while the black pixels should often be interpreted as the background, some tasks are exceptions to this rule."}, {"title": "The Domain-Specific Language", "content": "The DSL we devised for TransCoder is a typed, functional programming language, with leaves of AST trees fetching input data and constants, and the root of the tree producing the return value. Each operation (an inner AST node) is a function with a typed signature and implementation. The DSL features concrete (e.g. Int, Bool, or Region) and generic (e.g. List[T]) data types.\nA complete DSL program has the signature Region \u2192 Region, i.e. it can be applied to the input of an ARC demonstration (or the query) and produce the corresponding output.\nThe current version of the DSL contains 40 operations, which can be divided into data-composing operations (form a more complex data structure from constituents, e.g. Pair, Rect), property-retrieving operations (fetch elements or extract simple characteristics from data structures, e.g. Width, Area or Length), data structure manipulations (e.g. Head of the list, First of a Pair, etc.), arithmetics (Add, Sub, etc.), and region-specific operations (high-level transformations of drawable objects, e.g. Shift, Paint, FloodFill). Our DSL features also higher-order functions known from functional programming, for example, Map and Filter which apply an argument in the form of a subprogram to elements of a compound data structure like a List. The complete definition of the DSL can be found in Appendix."}, {"title": "Program Generator", "content": "The Program Generator (Generator for short) is a bespoke architecture based on the blueprint of the doubly-recurrent neural network (see Alvarez-Melis and Jaakkola [2017] for a simple variant of DRNN). The latent z' obtained from the Solver becomes the initial state of this network, which then iterates over the nodes of the Abstract Syntax Tree (AST) of the program being generated in the breadth-first order. For the root node of the AST, the return type is Region for the root node; for other nodes, it is determined recursively by the types of arguments required by DSL functions picked in previous iterations.\nIn each iteration, the Generator receives the data on the current context of AST generation, including the current size (the number of already generated nodes in the AST) and depth of the node in the AST, the parent of the current node, and the return type of the node. It is also fed with the set of symbols available in the workspaces (including the elements of the DSL), via the embedding described in the previous section. From this set, the Generator selects the symbols that meet the requirements regarding the type and the maximum depth of the tree. Then, it applies an attention mechanism to the embedded representations of the selected symbols. The outcome of attention is the symbol to be 'plugged' into the AST at the current location.\nThe Generator also determines the hidden state of the DRNN to be passed to each of the child nodes. This is achieved by merging the current state with a learnable embedding indexed with children's indices, so that generation in deeper layers of the AST tree is informed about node's position in the sequence of parent's children. The generation process iterates recursively until the current node requests no children, which terminates the current branch of the AST (but not the others). It is also possible to enforce termination by narrowing down the set of available symbols."}, {"title": "Training", "content": "TransCoder can be trained with reinforcement learning (RL) or supervised learning (SL). The RL mode is most natural for handling ARC tasks: the program p synthesized by the Generator is applied to the query raster and returns an output raster, which is then sent to the oracle. The oracle deems it correct or not and that response determines the value of the reward (1 or 0, respectively), which is then used to update the Generator. In this mode, we rely on the REINFORCE algorithm Williams [1992], Sutton et al. [2000].\nUnfortunately, the a priori odds for a generated program to solve the given task are minuscule. As a result, training TransCoder only with RL is usually inefficient, especially in the early stages, when the generated programs are almost entirely random: most episodes lead to no reward and, consequently, no updates of TransCoder's parameters. This motivates considering the SL mode, in which we assume that the correct program (target) is known. This allows us to directly confront the actions of the Generator (i.e. the AST nodes it produces) with the target program node-by-node, and apply a loss function that rewards choosing the right symbols at individual nodes and penalizes the incorrect choices. In SL, every training episode produces non-zero updates for the model's parameters (unless the target program has been perfectly generated).\nIn general, the specific program used as the target in this scenario will be one of many programs that implement the input-output mapping required by the demonstrations of the presented task (see Sec. 2). Deterministic models are fundamentally incapable of realizing one-to-many mappings, and the variational layer described in Sec. 3.2 is meant to address this limitation. Upon the (unlikely in practice) perfect convergence of TransCoder's training, we expect the deterministic output of the Solver (corresponding to $\\tau_{\\mu}$) to abstractly represent the common semantic of all programs that solve the presented task, and the variational layer to sample the latents that cause the Generator to produce concrete programs with that very semantics.\nThe prerequisite for the SL mode is the availability of target programs; as those are not given in the ARC benchmark, we devise a method for producing them online during training, presented in the next section."}, {"title": "Learning from mistakes", "content": "The programs produced by the Generator are syntactically correct by construction. Barring occasional run-time errors (e.g., applying a function to an empty list), a generated program will thus always produce some output raster for a given input raster; we refer to it as response. By applying such a program p (and arguably any syntactically correct program with the Region \u2192 Region signature) to the list I of input rasters of some task T, we obtain the corresponding list of responses O. We observe that the resulting raster pairs made of the elements of I and O can be considered as another ARC task T', to which p is the solution (usually one of possible solutions, to be precise). The resulting pair (T', p) forms thus a complete example that can be used to train TransCoder in SL mode, as explained in the previous section, where T' is presented to TransCoder and p is the target program."}, {"title": "Related work", "content": "TransCoder engages programs to process and interpret the input data, and thus bears similarity to several past works on neurosymbolic systems, of which we review only the most prominent ones. In synthesizing programs in response to input (here: task), TransCoder resembles the Neuro-Symbolic Concept Learner (NSCL, Mao et al. [2019]). NSCL was designed to solve Visual Query Answering tasks Johnson et al. [2016] and learned to parameterize a semantic parser that translated a natural language query about scene content to a DSL program which, once executed, produced the answer"}, {"title": "Experimental evaluation", "content": "In the following experiment, we examine TransCoder's capacity to provide itself with a 'reflexive learning gradient', meant as continuous supply of synthetic tasks at the level of difficulty that facilitates further improvement. Therefore, we focus on the dynamics of the learning process.\nSetup. To ensure a sufficiently large pool of training examples, each Exploration phase lasts until the set S contains at least 8192 tasks and 32 unique solutions. For the Reduction phase, we set the number of categories to be drawn for L' to n = 64, the number k of tasks to be drawn from each category to 32, the solving threshold of 30%, and the number of stagnation iterations to 10. Moreover, generated programs are limited to a maximum number of nodes of 64, a maximum depth of 8 and at most of 2 nestings of higher-order functions (each nesting is considered as a separate program and is also required to meet the above limits).\nMetrics. The primary metric is the percentage of tasks solved from the testing subset of the original ARC (RateARC). However, because of the difficulty of this corpus, this metric is very coarse. To assess the progress in a more fine-grained way, we prepare a collection of 183,282 synthetic tasks by collecting them from several past runs of the method. This collection is fixed; the percentage of tasks solved from that collection will be referred to as RateSynth.\nResults. Table 1 presents the metrics at the completion of consecutive training cycles (cf. Fig. 4). RateSynth monotonously increases over time, indicating steady progress of TransCoder's capacity of solving tasks. This positively impacts the RateARC, which achieves the all-high value of 2% at the end of the run, suggesting that the skills learned from the synthetic, easier tasks translate into more effective solving of the harder original ARC tasks."}, {"title": "Conclusions and future work", "content": "This study summarized our preliminary findings on TransCoder and illustrated its overall capacity to provide itself with a learning gradient. Crucially, the generative aspect of this architecture, combined with expressing candidate solutions in a DSL, allows the method to obtain concrete target DSL programs and so gradually transform an unsupervised"}]}