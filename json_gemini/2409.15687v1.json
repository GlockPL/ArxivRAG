{"title": "A COMPREHENSIVE EVALUATION OF LARGE LANGUAGE MODELS ON MENTAL ILLNESSES", "authors": ["Abdelrahman Hanafi", "Mohammed Saad", "Noureldin Zahran", "Radwa J. Hanafy", "Mohammed E. Fouda"], "abstract": "Large Language Model (LLM)s have shown promise in various domains, including healthcare. In this study, we conducted a comprehensive evaluation of LLMs in the context of mental health tasks using social media data. We explored the zero-shot (ZS) and few-shot (FS) capabilities of various LLMs, including GPT-4, Llama 3, Claude, Gemma, Gemini, Phi-3, and others, on tasks such as binary disorder detection, disorder severity evaluation, and psychiatric knowledge assessment. Our evaluation involved 33 models ranging from 2 billion to 405+ billion parameters, and we utilized 9 main prompt templates across the tasks. Key findings revealed that models like GPT-4 and Llama 3 exhibited superior performance in binary disorder detection, with accuracies reaching up to 85% on certain datasets. Moreover, prompt engineering played a crucial role in enhancing model performance. For example, when transitioning from our first binary prompt to a more structured prompt, the accuracy of certain models increased significantly. Notably, the Mixtral 8x22b model showed an improvement of over 20%, while Gemma 7b experienced a similar boost in performance. In the task of disorder severity evaluation, we observed that FS learning significantly improved the model's accuracy, highlighting the importance of contextual examples in complex assessments. Notably, the Phi-3-mini model exhibited a substantial increase in performance, with balanced accuracy (BA) improving by over 6.80% and mean average error (MAE) dropping by nearly 1.3 when moving from ZS to FS learning. Additionally, in the psychiatric knowledge assessment task, recent models generally outperformed older, larger counterparts, with the Llama 3.1 405b achieving an accuracy of 91.2%. Despite promising results, our analysis identified several challenges, including variability in performance across datasets and the need for careful prompt engineering. There is also the high cost associated with using large models and the limitations imposed by the quality of social media data. Furthermore, the ethical guards imposed by many LLM providers hamper the ability to accurately evaluate their performance, due to tendency to not respond to potentially sensitive queries.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) is rapidly transforming the landscape of mental healthcare, offering innovative solutions to address the growing global burden of mental illness. According to the World Health Organization, mental disorders accounted for 5.1% of the global disease burden in 2019, with depressive disorders affecting 280 million people worldwide, and an estimated 703,000 people dead by suicide [1]. The economic consequences of mental health conditions are also significant. In the United States, the cumulative cost of mental health inequities is projected to reach a staggering US$14 trillion between 2024 and 2040. These costs include direct medical expenses, emergency department utilization, productivity losses, and premature deaths [2].\nThe integration of AI in psychiatry has enabled a wide array of applications, spanning from the crucial tasks of early detection and diagnosis to predicting treatment outcomes and providing therapeutic interventions. AI models have been employed to analyze diverse data modalities, including neuroimaging data such as functional magnetic"}, {"title": "1.1 Evaluating LLMs on Mental Health Tasks", "content": "One of the earliest works in this domain is the study by [12], which presents a simple evaluation of GPT-3.5-Turbo's ZS binary classification performance on stress, depression, and suicidality detection tasks using social media posts. The resulting F1 scores (73% for stress, 86% for depression, and 37% for suicide), were particularly bad in the suicidality severity prediction task. The authors of [13] extend this evaluation by assessing GPT-3.5-Turbo's text classification abilities on a broader range of affective computing problems, including big-five personality prediction and sentiment analysis, in addition to suicide tendency detection. Their findings reveal that while GPT-3.5-Turbo provided decent results, comparable to Word2Vec and bag-of-words baselines, it did not outperform the fine-tuned ROBERTa model on the given tasks. However, the authors did not prompt GPT-3.5-Turbo through an Application Programming Interface (API), they instead did it manually using the web interface, only resetting the context every 25 prompts, which might have affected their results."}, {"title": "1.2 Fine-tuning LLMs for mental health tasks", "content": "Building upon the evaluation studies, [14] and [15] explore fine-tuning LLMs for improved performance in mental health tasks. [14] introduce the MentaLlama model, which is fine-tuned on the IMHI dataset, a large dataset of social media posts labeled with mental health conditions and explanations. The authors created the IMHI dataset by taking multiple existing mental health datasets and generating additional explanatory labels using GPT-3.5-Turbo. The MentaLlama-chat-13B model surpasses or approaches state-of-the-art discriminative methods in prediction correctness on 7 out of 10 test sets and generates explanations on par with GPT-3.5-Turbo thus providing interpretability.\nThe authors of [15] conducted a comprehensive evaluation of multiple LLMs, including Alpaca, Alpaca-LoRA, FLAN-T5, Llama 2, GPT-3.5-Turbo, and GPT-4, on various mental health prediction tasks using online text data. Their work explored ZS prompting, FS prompting, and instruction fine-tuning techniques. The authors fine-tuned Alpaca and FLAN-T5 on six mental health prediction tasks across four datasets from Reddit. Results show that instruction"}, {"title": "1.3 LLMs for Data Augmentation and Chatbot Development", "content": "[16], [17], and [18] explore the application of LLMs beyond prediction tasks, focusing on data augmentation and chatbot development. [16] develop Chat-Diagnose, an explainable and interactive LLM-augmented system for depression detection in social media. Chat-Diagnose incorporates a tweet selector to filter excessive posts, an image descriptor to convert images into text, and professional diagnostic criteria (Diagnostic and Statistical Manual of Mental Disorders (DSM)-5) to guide the diagnostic process. The system utilizes the chain-of-thought (CoT) technique to provide explanations and diagnostic evidence. In the full-training setting, it leverages answer heuristics from a traditional depression detection model. Chat-Diagnose achieves state-of-the-art (SoTA) performance in various settings, including ZS, FS, and full-training scenarios.\n[17] investigates the use of GPT-3.5-Turbo for generating synthetic Reddit posts simulating depression symptoms from the Beck Depression Inventory (BDI)-II questionnaire, aiming to enhance semantic search capabilities. The study finds that using original BDI-II responses as queries is more effective, suggesting that the generated data might be too specific for this task.\n[18] focuses on developing chatbots that simulate psychiatrists and patients in clinical diagnosis scenarios, specifically for depressive disorders. The study follows a three-phase design, involving collaboration with psychiatrists, experimental studies, and evaluations with real psychiatrists and patients. The development process emphasizes an iterative refinement of prompt design and evaluation metrics based on feedback from both psychiatrists and patients. A key finding of the study is the comparison between real and simulated psychiatrists, revealing differences in questioning strategies and empathy behaviors."}, {"title": "1.4 Benchmarks for Evaluating LLMs in Psychiatry", "content": "In contrast to the aforementioned studies, the authors of [19] took a different approach. Instead of evaluating existing LLMs on established datasets, they sought to create a comprehensive benchmark specifically designed to assess the capabilities of LLMs in the mental health domain. PsyEval (the benchmark) includes tasks such as mental health question-answering, where LLMs are evaluated on their ability to provide accurate and informative responses to queries related to mental health conditions, symptoms, and treatments. The benchmark also features tasks focused on diagnosis prediction, both using data from social media or simulated dialouges between patients and clinicians. Additionally, PsyEval incorporates tasks that assess LLMs' ability to provide empathetic and safe psychological counseling in simulated conversations."}, {"title": "1.5 Literature Reviews", "content": "In addition to the contribution papers mentioned above, several surveys have been conducted to summarize and analyze the landscape of LLMs in mental health care. The authors of [11] provide a comprehensive overview of the opportunities and risks associated with LLMs in psychiatry, discussing their potential to enhance mental health care through improved diagnostic accuracy, personalized care, and streamlined administrative processes. The authors highlight the ability of LLMs to efficiently analyze patient data, summarize therapy sessions, and aid in complex diagnostic problem-solving. They also discuss the potential of LLMs to automate certain managerial decisions within hospital systems, such as personnel schedules and equipment needs. However, the authors also acknowledge the risks associated with LLMs, such as labor substitution, the potential for reduced human-to-human socialization, and the amplification of existing biases. They conclude by advocating for the development of pragmatic frameworks, including red-teaming and multi-stakeholder-oriented safety and ethical guidelines, to ensure the safe and responsible deployment of LLMs in psychiatry.\nThe authors of [20] conducted a systematic review of the applications of LLMs, such as GPT-3.5-Turbo, in the field of psychiatry. The review identified 16 studies that directly examined the use of LLMs in psychiatry, with applications ranging from clinical reasoning and social media analysis to education. The authors found that LLMs like GPT-3.5-Turbo and GPT-4 showed promise in assisting with tasks such as diagnosing mental health issues, managing depression, evaluating suicide risk, and supporting education in the field. However, the review also highlighted limitations, such as difficulties with complex cases and potential underestimation of suicide risks."}, {"title": "1.6 Research Gaps", "content": "While the potential of LLMs in mental health care is evident, several research gaps remain to be addressed. Firstly, most studies have employed relatively outdated models like GPT-3.5-Turbo, GPT-4, and Llama 2, with no investigations into newer models such as GPT-4-0, GPT-4-Turbo, Llama 3, Phi-3, Gemma 1 & 2, Claude models, and others. This limits our understanding of how the latest advancements in LLM technology can be leveraged for mental health applications. Secondly, there is a lack of focus on prompt engineering to investigate the effect of different prompts on disorder detection tasks. While a few studies have touched upon this aspect, a more systematic and comprehensive exploration of prompt engineering techniques is needed to optimize LLM performance in this domain. Thirdly, many studies have overlooked smaller yet important factors such as LLM variation across runs and the effect of small prompt modifications across models. Additionally, there is often a lack of full transparency in reporting methodologies, even when code is provided, hindering reproducibility and further research.\nThe goal of this work is to comprehensively evaluate the capabilities of LLMs on tasks relating to the detection of mental illness symptoms or mental illnesses from social media. We focus on several unique contributions:\n\u2022 We test models never used before in this context, including Gemini 1.0 Pro, GPT-4/GPT-4-Turbo/GPT-4o/GPT-4o mini, Mistral NeMo/Medium/Large/Large 2, Claude 3/3.5, Mixtral, Gemma 1/2, Phi-3, Llama 3/3.1, and Qwen 2.\n\u2022 We conduct an investigation of published human-annotated mental health datasets, both private and public.\n\u2022 We conduct extensive experiments with various prompting methods, including FS prompting and severity prompting.\n\u2022 We perform unique experiments such as temperature variation tests, filtering strategies, and chain-of-thought prompting to explore model performance and interpretability.\n\u2022 We provide comprehensive details about our methodology, including all prompts, to ensure full transparency and facilitate reproducibility.\n\u2022 We provide a detailed list of drawbacks, and challenges that currently exist when exploring the applications of LLMs in psychiatry, along with suggestions for future researchers.\nThe rest of this paper is organized as follows. Section 2 details our methodology implementation, including the experimental design, datasets, models, prompt templates, evaluation metrics, and parsing of model outputs. Section 3 presents and discusses the results of our experiments, analyzing the performance of various LLMs across the three tasks. Section 4 delves into additional investigations and experiments conducted alongside our main analysis. Finally, Section 5 concludes the paper by summarizing our findings, discussing the challenges encountered, and proposing potential directions for future research."}, {"title": "2 Methodology Implementation", "content": "To investigate the potential of LLMs in mental health tasks, we designed a series of experiments to evaluate their performance across various scenarios. This section details our experimental setup, including the chosen datasets, models, prompting strategies, and evaluation metrics."}, {"title": "2.1 Experimental Setup", "content": "Our experiments were designed to assess LLM capabilities across three primary tasks: binary disorder detection, disorder severity evaluation, and psychiatric knowledge assessment. We conducted ZS experiments for all three tasks,"}, {"title": "2.1.1 Task 1: ZS Binary Disorder Detection", "content": "ZS learning is a machine learning paradigm where the model makes predictions without having seen any labeled examples of the specific task during training. Instead, the model relies on its pre-existing knowledge and the task description to make inferences. This approach is particularly useful when labeled data is scarce or when deploying models to new tasks quickly.\nIn this analysis, ZS learning was employed to evaluate the LLM's inherent understanding of psychiatric disorders. Each LLM was tasked with determining whether a social media user exhibits a specific mental disorder based solely on their post and a brief task description. We focused on three prevalent disorders: depression, suicide risk, and stress. This setup allowed us to assess the model's ability to identify these disorders based on its pre-trained knowledge and general language understanding."}, {"title": "2.1.2 Task 2: ZS/FS Disorder Severity evaluation", "content": "FS learning, on the other hand, is a technique where the model is provided with a small number of labeled examples before being tasked with making predictions. This approach helps the model to better understand the context and complexities of the task.\nThis task involved evaluating the severity (e.g. from 0 to 4) of a specific mental disorder in a user solely from a social media post written by them. We evaluated LLMs on depression and suicide risk, employing both ZS and FS approaches. In the ZS scenario, the LLM received only the social media post and the task description. In the FS scenario, the LLM was provided with three examples of posts from other users with corresponding severity ratings before assessing the new post. By comparing the results of these two approaches, we aimed to determine how much additional context in the form of examples improved the model's ability to accurately gauge disorder severity.\nFS prompting was only employed for severity evaluation because it is a more complex and challenging task compared to binary disorder detection, thereby providing a better demonstration of the potential improvement gained through FS learning."}, {"title": "2.1.3 Task 3: ZS Psychiatric Knowledge Assessment", "content": "This task was designed to test the LLM's knowledge of basic psychiatric concepts. The LLM was presented with multiple-choice questions related to psychiatry. This task evaluates the LLM's foundational understanding of psychiatric concepts and its ability to provide accurate factual information typically found in textbooks."}, {"title": "2.2 Datasets", "content": "We deliberately avoided datasets with automated labeling methods, such as those collecting posts from specific mental health subreddits (e.g., r/Depression) and matching them to control posts from unrelated subreddits. Additionally, we steered clear of semi/weak-labeled datasets that include posts from users who self-reported a specific disorder using phrases like \"I am depressed.\" These approaches tend to introduce some level of unreliability, as they lack human verification to determine if a specific post or user is exhibiting mental health-related issues.\nInstead, we prioritized datasets labeled by experts or crowd-sourced workers/volunteers who were trained by experts. This ensures the quality and reliability of the data used for our experiments, providing a more accurate foundation for assessing the LLM's performance in detecting and evaluating mental health disorders.\nWe considered various mental disorders but ultimately focused on depression, suicide risk, and stress due to their prevalence and the relative availability of high-quality datasets. Other disorders were not included in this study due to challenges in data collection and annotation. The sensitivity of mental health data often limits the public availability of relevant social media posts, and stricter data usage agreements imposed by social media platforms\u00b9 have further"}, {"title": "2.3 Sampling", "content": "As all testing was conducted through APIs, and for the bigger models, such as GPT-4, the costs could escalate rapidly, we employed two strategies for cost saving. First, we utilized the test dataset only if it was pre-split. Second, if the dataset exceeded 1000 examples, we employed a fair random sampling of 1000 instances. This sample size was chosen based on Hoeffding's inequality (Equation 1) [51], which provides a general upper bound for the error difference between a sample and the entire population, given a random sample of that size.\nDatasets containing severity metrics like DepSeverity and SAD were binarized by assigning a \"False\" label (0) to posts/users with the minimum severity score and a \"True\" label (1) to those with any higher severity score.\n$P (\\\u03bd \u2013 \u03bc| > \u03b5) \u2264 2e^{-2\u03b5^{2} N}$\n* v is the empirical mean (sample average) * \u03bc is the true mean (population average) * & is the deviation from the mean * N is the number of samples\nPlugging in the values N = 1000 and \u025b = 0.05 into Equation 1 (representing a maximum discrepancy of 5%), we calculate that the probability of the in-sample and out-of-sample error deviating by more than 5% is merely 0.0135. This implies a greater than 98.5% probability that the error will remain within a 5% range of the true error."}, {"title": "2.4 Models", "content": "To ensure a comprehensive assessment of LLM capabilities across diverse architectures and sizes, we selected a wide range of models for our experiments. We included both large, state-of-the-art models known for their performance on various natural language processing tasks, as well as smaller, more accessible models that could potentially be deployed in resource-constrained environments.\nOur model selection encompassed both closed-source models, accessible through APIs, and open-source models, which offer greater transparency and flexibility for research purposes. We included models from various families, including OpenAI's GPT series, Anthropic's Claude models, Mistral AI's models, Google's models, Meta's Llama series, and finally, Phi-2 and MentaLlama, a model specifically fine-tuned on mental health-related data. Additionally, we attempted to include Gemini 1.5 Pro in our evaluation however due to rate limit constraints, and challenges mentioned in 5, we were not able to include it."}, {"title": "2.5 Prompt Templates", "content": "The prompt, or instruction, given to an LLM plays a great role in its performance on a given task. Prior research has shown that even slight variations in prompt wording can significantly impact the quality of LLM outputs [70]. Therefore, we dedicated considerable effort to crafting effective prompts tailored to each task in our study.\nTo optimize LLM performance on social media post analysis, we employed prompts that encouraged the models to adopt the role of a psychiatrist. Role-playing has proven effective in eliciting relevant and contextually appropriate responses from LLMs [71, 72]. We also utilized the concept of prompt templates, which consist of a fixed structure with variable elements that can be adapted for different tasks. This approach ensured consistency across experiments while minimizing prompt-based bias.\nWe developed multiple prompt templates for each task category: binary disorder detection, disorder severity evaluation, and psychiatric knowledge assessment. These templates varied in their structure and level of explicit instruction, aiming to accommodate the diverse capabilities of the LLMs included in our study."}, {"title": "2.5.1 Binary Disorder Detection Prompt Templates", "content": "In the ZS binary disorder detection task, we employed four distinct prompt templates, designated BIN-1 to BIN-4. By utilizing multiple prompt templates, we aimed to assess the impact of both open-ended and structured prompts on the performance of various tested LLMs. This approach allowed us to evaluate LLMs' sensitivity, variability, and adaptability to diverse prompting styles, highlighting their strengths and weaknesses in different scenarios.\nAs shown in 3, Prompts BIN-1 and BIN-2 utilized a straightforward structure, first instructing the LLM to assume the role of a psychiatrist and then directly asking whether the poster of a given social media post exhibited a specific disorder. We included a final instruction to elicit a \"yes\" or \"no\" response for easier parsing of the LLM output.\nPrompts BIN-3 and BIN-4 introduced a more structured format, explicitly separating the prompt into \"Task,\" \"Guidelines,\" and \"Post\" sections. The guidelines emphasized the desired output format (yes/no), discouraged explanations, and instructed the LLM to choose the most probable label in cases of ambiguity. This structure aimed to improve the LLMs' adherence to the task instructions and reduce the incidence of irrelevant or unparseable responses. The inspiration for this structure was from Gemini, as this is the structure used for its training [8]."}, {"title": "2.5.2 Disorder Severity Evaluation Prompt Templates", "content": "For the disorder severity evaluation task, we iteratively refined the prompt template based on initial observations from the binary task and the performance of various LLMs. We began with a baseline prompt, SEV-1, which was a modified version of the binary prompt template BIN-1. The key modification in SEV-1 was the addition of a severity scale, representing varying levels of disorder severity. \nDuring the design of this initial prompt, we observed that most LLMs performed better when provided with the full context of each severity level, rather than receiving no context or only the extreme values (0 and 4).\nSubsequent refinements aimed to address the tendency of some expensive models to provide verbose explanations despite being instructed to give concise answers. We systematically introduced modifications to the prompt, resulting in three additional versions: SEV-2, SEV-3, and SEV-4.\n1. SEV-2: Modified the presentation of the severity scale from free text to a numbered list to enhance clarity.\n2. SEV-3: Incorporated an ambiguity clause (similar to the one used in BIN-3 and BIN-4) to guide responses when the post was unclear.\n3. SEV-4:Reiterated the instruction to provide a concise answer without explanation. This simple repetition proved surprisingly effective, especially for Mistral models. The added line is henceforth referred to as repetition line\nTo evaluate the effectiveness of these modifications, we conducted experiments on a sample of 1000 posts from the DEPTWEET dataset, comparing the performance of different LLMs across the various prompt versions (P1-P4)."}, {"title": "2.5.3 Psychiatric Knowledge Assessment Prompt Template", "content": "For the psychiatric knowledge assessment task, we employed a simple prompt template (KNOW-1) that instructed the LLM to answer a multiple-choice question with only the corresponding letter (A, B, C, or D) and no explanation. There was no need for any iterative experimentation."}, {"title": "2.6 Evaluation Metrics", "content": "To assess the performance of the LLMs across our defined tasks, we primarily employed accuracy as our evaluation metric. For most tasks, we applied fair random sampling to the datasets, ensuring balanced class distributions. This balanced sampling naturally leads to the calculation of Balanced Accuracy (BA), which is the average of recall obtained on each class. BA is particularly well-suited for classification tasks where class imbalance may be present, as it provides a more equitable assessment of the model's ability to correctly identify both positive and negative instances compared to traditional accuracy.\nHowever, in cases where fair random sampling was not feasible due to limitations in the dataset, such as the SAD dataset with its sparse distribution of severity classes, we utilized regular random sampling with a random state of 42 and evaluated performance using traditional accuracy. Similarly, for the MedMCQA dataset, which involves multiple-choice question answering without distinct class labels, we also relied on accuracy as the primary evaluation metric.\nFor the disorder severity evaluation task, we employed two metrics to assess the performance of LLMs: BA and Mean Absolute Error (MAE). BA, in this context, refers to the proportion of posts for which the LLM predicted the exact severity level assigned by human annotators. It serves as a measure of how often the model gets the severity classification exactly right. On the other hand, MAE quantifies the average magnitude of the errors in the predicted severity ratings. It provides a complementary perspective on model performance by assessing how close, on average, the predicted severity levels are to the true levels, even if they are not exactly correct. A model could potentially improve in one metric while worsening in the other."}, {"title": "2.7 Parsing Model Outputs", "content": "Parsing, the process of interpreting and extracting meaningful information from LLM outputs, proved to be a non-trivial aspect of our methodology. While seemingly straightforward for tasks requiring simple responses like \"yes/no\" or numerical ratings, the reality was more complex. LLMs, particularly larger and more sophisticated models, often deviate from explicit instructions regarding output format, frequently providing explanations or incorporating the requested answer within a longer response.\nWe observed that model adherence to instructions varied widely and did not necessarily correlate with model size. For instance, smaller models like Llama 3 8b, as well as larger ones like Llama 3 70b and GPT-4, demonstrated a high degree of compliance with instructions to output only the answer. Conversely, other models, such as the 2 billion parameter Phi-2 and the seemingly large Mistral Large and Claude 3 Opus, frequently struggled to adhere to the desired format, often opting to provide reasons and explanations for their provided label.\nTo address these challenges, we developed custom parsers for each task:\n1. Binary Disorder Detection: Our parser searched for the text \"yes\" or \"no\" in the LLM output. If only one option was found, it was taken as the model's answer. If both were present, or neither was found, the output was considered to be an invalid response.\n2. Disorder Severity Evaluation: The parser searched for numerical values within the specified severity range. If a single valid number was found, it was accepted as the answer. If multiple numbers were found, the number was outside the valid range, or no number was found, then output was considered to be an invalid response.\n3. Psychiatric Knowledge Assessment: Parsing multiple-choice answers proved more challenging, particularly for the letter \"a,\" which could be either the answer or part of a word like \"an.\" We employed an iterative approach, refining the parser's regular expressions by examining invalid outputs. This iterative process revealed that LLMs tend to follow a limited set of patterns in their responses, making it feasible to achieve reliable parsing with relatively few rules. Interestingly, only eight rules were required to parse the majority of outputs across all LLMs."}, {"title": "2.8 Additional Considerations", "content": "In addition to the evaluation metrics and parsing strategies discussed previously, there are a few additional considerations worth noting.\nFirst, despite our efforts in prompt engineering, certain LLMs, such as Mistral Medium and Claude 3 Opus, consistently struggled to adhere to the instructions provided in the prompts across various tasks. To mitigate this issue, we employed modified prompts in some experiments, primarily by incorporating the \"repetition line\" from prompt SEV-4, which explicitly reiterated the instruction to provide a concise answer without explanation. This modification generally improved the adherence of these models to the desired output format.\nSecond, our analysis included several smaller, specialized experiments designed to evaluate specific aspects of LLM performance or explore particular research questions. Due to their focused nature, the details of these experiments, including their implementation, results, and discussion, are presented separately in Section 4 to maintain clarity and coherence in the main body of our analysis.\nThird, during evaluation, we compared two approaches: setting a maximum output token limit of 2 tokens versus allowing the models to respond freely without truncation. The rationale behind the former approach was that it would likely include the keyword for the diagnosis we were seeking. Conversely, for models that did not adhere well to prompt instructions, the diagnosis might appear later in their response. Our findings revealed that the longer a model's response, the more likely it was to become invalid due to the possibility of it using 'yes' or 'no' when providing explanations for its decision. Consequently, we decided to impose a response length limit of 2 tokens for all models tested."}, {"title": "3 Results & Discussion", "content": "In this section, we present the results of our experiments, analyzing the performance of various LLMs across the three tasks defined in Section 2. We begin by examining the performance variability between runs across different LLMs using consistent parameters for all experiments. This analysis establishes a foundation for assessing the reliability and reproducibility of the presented results, determining whether observed improvements between prompts are valid and significant or merely due to chance. Next, we delve into the results of the ZS binary disorder detection task (Section 3.3), followed by the ZS/FS disorder severity evaluation task (Section 3.4), and conclude with the ZS psychiatric knowledge assessment task (Section 3.5)."}, {"title": "3.1 Performance Variability Experiment", "content": "In the performance variability experiment, we evaluate the inherent performance variability of each LLM on the same task. The experiment involved repeatedly evaluating each model on the same dataset five times with the standard parameters we typically use (e.g., temperature 0). This approach allows us to approximate the natural performance fluctuations, enabling us to judge whether any observed performance enhancement brought about by changes to the prompt or other factors is a meaningful improvement or a random fluctuation due to the model's inherent variability. The experiment was conducted on 1000 fairly sampled instances from the DEPTWEET dataset."}, {"title": "3.2 Performance Variability Experiment", "content": "In the performance variability experiment, we evaluate the inherent performance variability of each LLM on the same task. The experiment involved repeatedly evaluating each model on the same dataset five times with the standard parameters we typically use (e.g., temperature 0). This approach allows us to approximate the natural performance fluctuations, enabling us to judge whether any observed performance enhancement brought about by changes to the prompt or other factors is a meaningful improvement or a random fluctuation due to the model's inherent variability. The experiment was conducted on 1000 fairly sampled instances from the DEPTWEET dataset."}, {"title": "3.3 Binary Disorder Classification (Task 1)", "content": "In the ZS binary disorder detection task, we evaluated the performance of various LLMs in identifying the presence or absence of depression, suicide risk, and stress in social media posts. We began by assessing their performance using the BIN-1 prompt template, followed by an analysis of the impact of modifications introduced in prompts BIN-2, BIN-3, and BIN-4."}, {"title": "3.3.1 Results of Prompt BIN-1", "content": "An analysis of Figure 3 reveals that the performance of each model varies across datasets. However, a general trend emerges: OpenAI stands out with three of the six best-performing models. Specifically, GPT-4 achieves top scores of 85.20% on the SAD dataset and 85.00% on the DEPTWEET dataset, while GPT-40 excels with 71.20% on the SDCNL dataset. In addition to OpenAI's success, Llama 2 70b performs best in the DepSeverity dataset with 73.58%, Mistral NeMo leads in both Dreaddit Test and RED SAM with 74.50, and 66.00% respectively.\nOpenAI models consistently demonstrate strong performance. GPT-4 achieves the highest accuracy among the tested models, reaching approximately 85% on both the DEPTWEET and SAD datasets. GPT-4-Turbo and GPT-40 generally follow closely behind GPT-4, though with some performance fluctuations. GPT-4-Turbo typically lags slightly, outperforming GPT-4 in two instances (63.7% in RED SAM vs. 62.1%, and 70.8% in SDCNL vs. 69.3%). GPT-40 exhibits more significant fluctuations, sometimes exceeding all other variants (e.g., 71.2% in SDCNL) but also deviating from GPT-4 by up to 9.8% in SDCNL. GPT-3.5-Turbo consistently follows closely behind GPT-4, deviating by approximately 5% at worst, which is notable considering its much smaller size. The newest and most cost-efficient model, GPT-40 mini, generally performs worse compared to GPT-3.5-Turbo.\nThe Llama 2 70b model demonstrates notable performance, achieving 73.58% in DepSeverity, unexpectedly outperforming the Llama 3 70b (70.5%) and even surpassing GPT-4, which achieves 72.6%. The newer Llama 3 models exhibit greater consistency, with deviations of only 3-10% between the 8b and 70b variants, compared to 10-20% deviations in the Llama 2 family. However, the Llama 3.1 family of models was a significant disappointment, with the Llama 3.1 8b model performing considerably worse compared to the earlier version, except in two datasets (SAD & DEPTWEET), where the performance is generally comparable. A similar observation applies to the Llama 3.1 70b model, which is generally closer in performance to the earlier version but actually outperforms the Llama 3 70b model in the SAD and DEPTWEET datasets. Lastly, the Llama 3.1 405b model was a major disappointment, performing even worse than the Llama 3.1 70b model across all six datasets.\nMistral 7b shows moderate performance, achieving an accuracy of 55% (\u00b12.8%) across datasets, barely surpassing the baseline accuracy of a random classifier. Although it outperforms Llama 2 13b, it is surpassed by Llama 3 8b. The Mistral Medium model demonstrates similar performance to Mistral 7b. Mistral Large initially encounters difficulties due to instruction-following issues but shows significant improvement with the BIN-1.1 prompt, achieving a 5-25% increase in accuracy depending on the dataset, with the exception of Dreaddit. The Mistral NeMo model stands out as the best-performing model on the Dreaddit Test dataset with 74.5%, outperforming GPT-4's 72%. Beyond Dreaddit, Mistral NeMo performs better than Mistral 7b, but is generally outpaced by Llama 3 8b. As for Mistral Large 2, its performance is much better than Mistral Large 1, even mostly outperforming Mistral Large 1 with the BIN-1.1 prompt. However, it performs worse overall compared to the much smaller Mistral NeMo model\nThe Claude family presents varied results. Claude 3 Haiku often performs similarly to or even surpasses Claude 3 Sonnet, particularly in Dreaddit (56.6% vs. 54.2%) and SAD (74.6% vs. 71.3%). Claude 3 Sonnet shows a significant advantage over the Haiku model only in DEPTWEET (81.5% vs. 75.5%). Claude 3 Opus, Anthropic's flagship model, also struggles initially (due to the same"}, {"title": "3.3.2 Results of Prompts BIN-2/3/4", "content": "Table 8 presents the changes in model performance when moving from prompt BIN-2 to BIN-3, and BIN-4 respectively. Comparing the improvement when going from prompt BIN-1 to prompt BIN-2, we can observe the effect of changing the main task from checking whether the poster suffers from a disorder (i.e., 'is the user stressed') to checking whether"}]}