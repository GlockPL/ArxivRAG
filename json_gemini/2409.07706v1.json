{"title": "Attack End-to-End Autonomous Driving through Module-Wise Noise", "authors": ["Lu Wang", "Tianyuan Zhang", "Yikai Han", "Muyang Fang", "Ting Jin", "Jiaqi Kang"], "abstract": "With recent breakthroughs in deep neural networks, numerous tasks within autonomous driving have exhibited remarkable performance. However, deep learning models are susceptible to adversarial attacks, presenting significant security risks to autonomous driving systems. Presently, end-to-end architectures have emerged as the predominant solution for autonomous driving, owing to their collaborative nature across different tasks. Yet, the implications of adversarial attacks on such models remain relatively unexplored. In this paper, we conduct comprehensive adversarial security research on the modular end-to-end autonomous driving model for the first time. We thoroughly consider the potential vulnerabilities in the model inference process and design a universal attack scheme through module-wise noise injection. We conduct large-scale experiments on the full-stack autonomous driving model and demonstrate that our attack method outperforms previous attack methods. We trust that our research will offer fresh insights into ensuring the safety and reliability of autonomous driving systems.", "sections": [{"title": "1. Introduction", "content": "With recent significant advancements in deep learning, autonomous driving technology plays an increasingly important role in today's society. End-to-end autonomous driving models map raw sensor data directly to driving decisions, becoming the predominant solution gradually. However, despite the excellent performance of end-to-end models, we must also realize the security challenges they face. A considerable amount of research has already been devoted to studying adversarial attack methods towards individual tasks within the field of autonomous driving, with particular emphasis on the perception layer. Nowadays, researchers have begun to conduct adversarial attacks on very simple end-to-end regression-based decision models directly from input images."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. End-to-End Autonomous Driving", "content": "The early end-to-end autonomous driving models combine relatively few tasks. [4] adopts a bounding box-based intermediate representation to construct the motion planner. Considering that Non-Maximum Suppression(NMS) in this method can lead to loss of perceptual information, P3 [5] innovatively designs an end-to-end network that utilizes maps, advanced control instructions, and LIDAR points to generate interpretable intermediate semantic occupancy representations, which facilitates safer trajectory planning. Following P3, MP3 [6] and ST-P3 [7] achieve new improvements. UniAD [8] is the first end-to-end network that integrates full-stack autonomous driving tasks. By thoroughly considering the contributions of each task to autonomous driving and mutual promotion among modules, UniAD significantly surpasses previous sota performance on each task."}, {"title": "2.2. Adversarial Attack", "content": "Adversarial noise refers to carefully crafted perturbations designed for neural network input data, which are typically small but can cause models to produce complete error outputs. [9] first introduces the concept of adversarial attack and utilizes L-BFGS approximation. Following that, a series of adversarial attack methods are proposed, such as gradient-based methods, and optimization-based methods. Although early adversarial attacks primarily target image classification models, they demonstrate the vulnerability of neural networks. Their attack principles have guided the implementation of various attack methods in different tasks, posing a serious threat to the practical application of models in the real world."}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Problem Definition", "content": "Let $y = F(x; \\theta)$ represent the end-to-end autonomous driving model with parameters $\\theta$, which includes $n$ sub modules ${M_i}_{i=1}^n$. The inference stage can be formulated as:\n$Q_i = M_i(Q_{i-1}; \\theta_i), i = 1, 2, ..., n,$\nwhere $Q_0 = x$ refers to the input images, $Q_n = y$ the planned results, and $Q_i$ the output intermediate queries of the i-th sub module.\nIn this research, we consider the interaction information among modules as a potential vulnerability. In addition to the input images, we inject adversarial noise $N_{i-1}$ into the input data $Q_{i-1}$ of each submodule $M_i$. That is,\n$Q_{i}^{adv} = Q_{i-1} + N_{i-1}, i = 1, 2, ..., n$.\nLet $N$ represents the final injected adversarial noise ${N_i}_{i=1}^n$, and $y^{adv} = F(x;\\theta, N)$ denotes the decision re-"}, {"title": "3.2. Module-Wise Adversarial Noise", "content": "Figure 2 presents an overall framework of the proposed method. A complete autonomous driving model comprises perception, prediction and the final planner. We aim to inject adversarial noise into the input of all sub modules in the end-to-end autonomous driving model so mainly focus on the current full-stack autonomous driving model UniAD [8]. Undoubtedly, our noise injection scheme is applicable to any modular end-to-end autonomous driving model.\nImages serve as the initial input for the model to perceive the environment, forming the foundation for all subsequent modules. Therefore, we design pixel-level noise specifically for input images, denoted as $N_I$. The track module accomplishes multi-object tracking, producing spatiotemporal information $Q_A$. Therefore, we design noise $N_A$ for the spatiotemporal features of agents. The mapping module models the features of road elements as $Q_M$ and we design adversarial noise $N_M$ for all map features. $Q_A$ and $Q_M$ respectively provide dynamic agent features and static scene information for downstream modules. The motion module, based on the aforementioned dynamic and static features, predicts the most likely future trajectory states of all agents, represented as $Q_T$. Similarly, we design noise $N_T$ for the state of agents. In addition, the motion module also expresses the future intentions of the ego vehicle, represented as $Q_E$. We separately design noise $N_E$ for $Q_E$.\nSince the occupancy map is ultimately used only in post-processing for collision optimization of the planned trajectory, and this optimization process is non-differentiable, we don't consider injecting noise into the occupancy map."}, {"title": "3.3. Attack Strategy", "content": "We achieve the attack by iteratively optimizing module-wise noise. To be more specific, during the model's inference stage for each batch of data, we first initialize corresponding adversarial noise randomly within the perturbation constraints for each module, following the forward propagation process of the model. At each iteration, noise injected into the corresponding module is propagated and stored until the final planning stage. We synchronize the update of all noise by the adversarial loss, using it to initialize the noise for the next iteration.\nAdversarial loss, essential for noise update, includes attack loss and noise loss. Attack loss seeks to maximize deviation between sub module predictions and actual results, mathematically represented as:\n$L_{att} = L_{track} + L_{map} + L_{motion} + L_{occ} + L_{plan}.$"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Setup", "content": "Dataset. Our robustness evaluation experiments are conducted on the validation split of the large-scale autonomous driving dataset nuScenes [21].\nModel. For the target model, we choose the current full-stack autonomous driving model UniAD, which includes complete sub tasks of perception, prediction, and decision. We conduct noise injection according to the five stages of the model as outlined in Section 3.2. For the specific implementation of the attack, we set the number of iterations $k = 10$, $\\sigma_I = 8 \\times 10^{-6}$, $\\sigma_A = \\sigma_M = \\sigma_T = \\sigma_E = 2 \\times 10^{-4}$."}, {"title": "4.2. Robust Evaluation", "content": "Baselines. As there are currently no adversarial attacks targeting complex end-to-end autonomous driving models composed of a series of sub modules, we choose attack methods targeting end-to-end regression-based decision models as baselines [3]. In our comparative experiments, we set the maximum allowable perturbation $\\epsilon$ for images to 8 and implement Image-specific Attack and Image-agnostic Attack tailored for UniAD.\nResults. We provide the robustness evaluation results compared with baselines for the five tasks, as shown in Table 1. Our attack can significantly reduce the performance of all tasks. We measure the main planning performance of the vehicle using the L2 error between the planned trajectory and the ground truth trajectory, as well as the collision rate with obstacles in the ego vehicle's driving environment. Results indicate that all three attacks lead to errors in the vehicle's planning, but the proposed attack method poses the greatest threat to the model as it interferes with both the perception and prediction processes during model inference, and the final prediction heavily relies on the inference results of upstream modules, resulting in severe planning errors due to error accumulation. Overall, the attack intensities of the three methods exhibit the same trend across the five tasks, with the Image-specific Attack being much stronger than the Image-agnostic Attack, and our module-wise attack surpassing the Image-specific Attack."}, {"title": "5. Conclusions", "content": "We delve into the robustness of complex end-to-end autonomous driving models with multi-modules by introducing a novel adversarial attack using module-wise noise injection. We strongly believe that it is imperative to consider the vulnerabilities in the interaction process between modules to enhance the security of autonomous driving systems. By conducting extensive experiments on the full-stack autonomous driving model, we demonstrate the profound impact of injecting noise into different modules on the planning performance of the model as well as other tasks."}]}