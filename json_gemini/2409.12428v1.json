{"title": "Is it Still Fair? A Comparative Evaluation of Fairness Algorithms through the Lens of Covariate Drift", "authors": ["Oscar Blessed Deho", "Michael Bewong", "Selasi Kwashie", "Jiuyong Li", "Jixue Liu", "Lin Liu", "Srecko Joksimovic"], "abstract": "Over the last few decades, machine learning (ML) applications have grown exponentially, yielding several benefits to society. However, these benefits are tempered with concerns of discriminatory behaviours exhibited by ML models. In this regard, fairness in machine learning has emerged as a priority research area. Consequently, several fairness metrics and algorithms have been developed to mitigate against discriminatory behaviours that ML models may possess. Yet still, very little attention has been paid to the problem of naturally occurring changes in data patterns (aka data distributional drift), and its impact on fairness algorithms and metrics. In this work, we study this problem comprehensively by analyzing 4 fairness-unaware baseline algorithms and 7 fairness-aware algorithms, carefully curated to cover the breadth of its typology, across 5 datasets including public and proprietary data, and evaluated them using 3 predictive performance and 10 fairness metrics. In doing so, we show that (1) data distributional drift is not a trivial occurrence, and in several cases can lead to serious deterioration of fairness in so-called fair models; (2) contrary to some existing literature, the size and direction of data distributional drift is not correlated to the resulting size and direction of unfairness; and (3) choice of, and training of fairness algorithms is impacted by the effect of data distributional drift which is largely ignored in the literature. Emanating from our findings, we synthesize several policy implications of data distributional drift on fairness algorithms that can be very relevant to stakeholders and practitioners.", "sections": [{"title": "1 Introduction", "content": "Nowadays, machine learning (ML) models are ubiquitous in all human settings. They have even been used in judicial systems to determine whether or not to parole convicted criminals (Angwin et al, 2016). While ML systems have undoubtedly improved human productivity (Furman and Seamans, 2019), they can also propagate biases in data, consequently discriminating against certain demographic groups. Notably, COMPAS, a software for parole sentencing, was found to label black defendants twice as likely as their white counterparts to recidivate over a two-year period (Angwin et al, 2016). In recent years, there has been significant research on the so-called fairness in machine learning which is primarily concerned with measuring and mitigating algorithmic biases.\u00b9 Several fairness algorithms constrained to satisfy certain fairness metrics have been designed (Mehrabi et al, 2021). Majority of existing fairness algorithms are designed on the assumption that the training and test datasets are independently and identically distributed (iid) (Li et al, 2024; Maity et al, 2021). In other words, it is assumed that ensuring fairness on the training dataset approximates fairness guarantees on the test dataset. However in reality, it is common for the distribution of the test dataset (target environment) to drift from that of the training dataset (source environment), thus no longer identically distributed. For example, a recent study demonstrated distributional drift of online learning behaviors within the educational setting at the height of COVID-19 (Impey and Formanek, 2021). In such circumstances, we posit that fairness algorithms cannot guarantee fairness. In recent years, only a handful of research works investigate the robustness of fairness algorithms amidst distributional drifts (Kamp et al, 2021). Nonetheless, there remain gaps in the literature. Firstly, the existing literature does not quantify the size of differential drift\u00b2 in data distribution across demographic groups. Therefore, it is difficult to characterize the relationship between differential distributional drift and the corresponding fairness. This characterization is needed in order to develop fairness algorithms that are robust against distributional drift. Secondly, there is no comprehensive study that benchmarks fairness algorithms vis-a-vis fairness metrics in the context of data distributional drift. Our study fills this gap by considering a large collection of metrics, algorithms and datasets and explores the conditions under which fairness algorithms may be considered fair amidst distributional drift. More formally, we explore the following research questions (RQs).\n\u00b9Throughout this work, we interchangeably use algorithmic bias, discrimination, or unfairness to refer to differential decisions made by an algorithm based on an individual's demographic attribute due to (or independent of) biases in the dataset.\n\u00b2We refer to differential distributional drift as the disparity in the magnitude of the drift in data distribution across different demographic groups."}, {"title": "\u2022 RQ1: What is the relationship between differential drift in data across demographic groups and algorithmic fairness?", "content": "\u2022 RQ2: Are existing fairness algorithms distributional drift aware?\nBy investigating these RQs, we can diagnose and reveal various ways that distributional drift impacts algorithmic fairness. To answer these questions, we perform an extensive experimental analysis involving 4 baseline models and 7 fairness algorithms using 3 predictive performance and 10 fairness metrics. We run our analysis on 5 real-world datasets across 4 domains: Education, Finance, Employment, and Criminal Justice. The main contributions of our work are as follows:\n\u2022 We reveal several interesting relationships between distributional drift- specifically covariate drift and algorithmic fairness. Especially, we show how covariate drift results in discrimination (or reverse discrimination).\n\u2022 We demonstrate the lack of robustness of existing algorithms in the face of covariate drift, while highlighting the need for the proper contextualization of what is fair.\n\u2022 We perform extensive experimental analysis that contributes critical empirical evidence on the impact of covariate drift on algorithmic fairness and recommend important policy implications of our findings for relevant stakeholders.\nThe rest of this paper is organized as follows. Section 2 discusses relevant related works. Section 3 provides preliminary information regarding notations, evaluation metrics, and fairness algorithms. Section 4 discusses our experimental setup. We discuss the results and their implications in Section 5. We conclude the paper in Section 6."}, {"title": "2 Related Work", "content": "Fairness algorithms either modify biased data (aka pre-processing algorithms), add a fairness constraint directly to a model's objective function (aka in-processing algorithms), or modify a biased model's outcomes (aka post-processing algorithms). In recent years, a number of studies have been done to assess the various fairness algorithms based on different evaluation criteria (Friedler et al, 2017; Roth, 2018; Deho et al, 2022). Existing works tend to compare fairness algorithms in terms of their predictive performance, fairness, and/or fairness-accuracy tradeoff. For instance, Hamilton (2017) compared four fairness algorithms using four fairness metrics across three different datasets. However, the author found that the fairness of the algorithms varied across datasets. In a similar study, Roth (2018) compared three fairness algorithms and found that in most cases, the in-processing approaches tend to achieve fairness better than the pre-processing approaches. Nonetheless, Roth found that the algorithms were inconsistent across datasets and remarked the need for further extensive experiments. Furthermore, studies by Friedler et al (2017) and Deho et al (2022) have compared fairness algorithms across several datasets, investigated the effect of hyperparameter variation on fairness algorithms, and evaluated the fairness-accuracy trade-offs of various fairness algorithms. Both studies also found that the performance of fairness algorithms tends to vary across datasets. In this regard, a new line of research that is focused on a comparative analysis of the utility of fairness algorithms due to drifts in data distribution has recently emerged."}, {"title": "3 Preliminaries and Background", "content": "Consistent with literature, we term attributes which are often not legally allowed to be used as the basis for decisions as protected attributes denoted A e.g., race and gender (Siegel, 2003). We represent non-protected attributes by X, actual outcomes by Y and predicted outcomes by \u0176. We denote the privileged group and favourable outcome by A = 1 and Y = 1 (or \u0176 = 1) respectively. Conversely, we represent unprivileged and unfavourable outcome by A = 0 and Y = 0 (or \u0176 = 0) respectively. As per convention in algorithmic fairness literature, we denote demographic groups that are historically \"disadvantaged\" e.g., racial minorities as unprivileged groups. These groups are protected by law or simply referred to as protected groups (Mehrabi et al, 2021). Non-protected/privileged groups are the demographic groups that are historically advantaged e.g., racial majorities."}, {"title": "3.1 Types of Distributional Drift and Drift Detection Metric", "content": "Given a non-protected input attribute X, and target outcome Y, at time t (e.g., training), and time t + k (e.g., testing), where k > 0, distributional drift can manifest in the following 3 ways (Moreno-Torres et al, 2012):\na) Covariate drift: This occurs when the input marginal probability P(x) changes but the conditional probability P(y|x) does not change. Covariate drift is expressed as:\nPt(x) \u2260 Pt+k(x) Pt(y|x) = Pt+k(y|x) \\newline\nb) Target drift: This occurs when the target marginal probability P(y) changes but the conditional probability P(x|y) does not change. Target drift is expressed as:\nPt(y) \u2260 Pt+k(y) Pt(x|y) = Pt+k(x|y) \\newline\nc) Concept drift: This occurs when the underlying relationship between the input and the target label changes but the input marginal probability P(x) does not change. Concept drift is expressed as:\nPt(y|x) \u2260 Pt+k(y|x) Pt(x) = Pt+k(x) \\newline\nConsistent with well-known related works (Maity et al, 2021; Xu and Wilson, 2021), we focus on the most prevalent drift, covariate drift. Further, we also make the assumption that the condition Pt(y|x) = Pt+k(y|x) in covariate drift is satisfied. Consistent with (Xu and Wilson, 2021), this assumption is reasonable due to data insufficiency.\nTo measure covariate drift (i.e., Pt(x) \u2260 Pt+k(x)) condition, we adopt the well- known statistical drift detection metric called Jensen-Shannon Distance (JSD) (Endres and Schindelin, 2003). JSD works well for both categorical and numerical variables and is based on the KL divergence. However, unlike KL divergence, JSD is symmetric and returns a finite score between 0 and 1. A drift value 0 signifies no drift and a drift value of 1 signifies maximum drift. More formally, given Pt and Pt+k:\nJSD(Pt, Pt+k) = \\sqrt{\\frac{D_{KL}(P_t||M) + D_{KL}(P_{t+k}||M)}{2}} \\newline where,\n\u2022 M = \\frac{P_t+P_{t+k}}{2}\n\u2022 We consider JSD \u2265 0.1 in this study as a significant drift.\u00b3\n\u00b3 There is no universally accepted threshold for significant drift. However, we chose 0.1 based on an extensive study by researchers at Evidently AI which is a drift monitoring company (https://www.evidentlyai.com/blog/data-drift-detection-large-datasets)"}, {"title": "3.2 Evaluation Metrics", "content": "In this section we present the various predictive performance and fairness metrics considered in this work."}, {"title": "3.2.1 Predictive Performance Metrics", "content": "To measure the predictive performance of our models, we consider 3 popularly used metrics namely accuracy for balanced data, and balanced accuracy and weighted F1- score for imbalanced data. All 3 metrics return a value of 0 (worst) to 1 (best)."}, {"title": "3.2.2 Fairness Metrics", "content": "To measure the fairness of our models, we carefully consider 10 fairness metrics such that each of them belongs to at least one of the 7 clusters discovered from 26 fairness metrics by Majumder et al (2021). The 7 clusters of metrics represent misclassification metrics (cluster 0,3), differential fairness metrics (cluster 1), individual fairness metrics (cluster 2), confusion matrix based group fairness metrics (cluster 4), between group individual fairness metrics (cluster 5) and intermediate metrics (cluster 6). Metrics in the same cluster were found to satisfy similar notions by the authors.\nGroup Fairness Metrics. Group fairness seeks to ensure that individuals belonging to different demographic groups are treated equally. The group fairness metrics that we considered in this work are statistical parity (SP) (Dwork et al, 2012), disparate impact (DI) (Feldman et al, 2015), error rate difference (ERD) (Berk et al, 2021), equalized odds (EO) (Hardt et al, 2016), equal opportunity(EOP) (Hardt et al, 2016), positive predictive value difference (PPV-DIFF) (Chouldechova, 2017), and negative predictive value difference (NPV-DIFF) (Berk et al, 2021).\nIndividual Fairness Metrics. Individual fairness is based on the idea that similar individuals should be treated similarly. There are fewer individual fairness metrics compared to group fairness metrics. The individual fairness metrics considered in this study are within group generalized entropy index (WGEI) (Speicher et al, 2018), within group theil index (WGTI) (Speicher et al, 2018), and consistency (Zemel et al, 2013)."}, {"title": "3.3 Fairness Algorithms", "content": "Fairness algorithms are often designed to facilitate ML models such that the ML models satisfy one or more fairness metrics. In such a case, the ML model is said to be \"fair\". Seven well known fairness algorithms covering pre-processing, in-processing and post-processing within an ML pipeline have been considered in this work."}, {"title": "3.3.1 Pre-processing Algorithms", "content": "Pre-processing algorithms tackle algorithmic unfairness by debiasing data. The fair data generated by the pre-processing algorithm can then be used to train any downstream ML model. The 3 pre-processing techniques we used in this study are Suppression (SUP) (Kamiran and Calders, 2012), Reweighing (RW) (Kamiran and Calders, 2012), and Disparate Impact Remover (DIR) (Feldman et al, 2015)."}, {"title": "3.3.2 In-processing Algorithms", "content": "In-processing algorithms achieve fairness by explicitly introducing fairness constraints in the ML algorithm. We consider 2 in-processing algorithms in this study namely Prejudice Remover (PR) (Kamishima et al, 2012) and Advesarial Debiasing (AdDeb) (Zhang et al, 2018)."}, {"title": "3.3.3 Post-Processing Algorithms", "content": "Post-processing methods involve altering the outcomes of a pre-trained model to attain specific fairness criteria across various groups. The 2 post-processing algorithms considered in this study are Equal Odds algorithm (EQ) (Hardt et al, 2016) and Calibrated Equal Odds (CEq) (Pleiss et al, 2017)."}, {"title": "4 Experimental Setup", "content": "We used 5 real-world datasets for our comparative analysis. Three of the datasets are commonly used publicly available datasets4,5 in the algorithmic fairness community while two are proprietary datasets. The two proprietary datasets are anonymized counts of first semester Moodle engagement records for 2 mandatory STEM courses-coded as NWF and ITF-in a large public Australian university.\nProprietary Datasets: NWF and ITF represent students' demographic and engagement records from 2015 to 2020 for two courses respectively. Public Datasets: BAF is synthetic data generated from anonymized real-world bank fraud detection dataset spanning 8 months (Jesus et al, 2022). The Adult dataset is a income census dataset detailing whether a person's income exceeds 50K US dollars (Becker and Kohavi, 1996). The COMPAS dataset contains criminal history and COMPAS risk scores for defendants from Broward County (Angwin et al, 2016)."}, {"title": "4.2 Analysis of Covariate Drift in Datasets", "content": "Given that the important covariates have significant influence on the prediction outcomes (Zien et al, 2009), we are interested in investigating the relationship between covariate importance, covariate drift, and algorithmic unfairness. To that end, we pursued two key objectives. Firstly, we are interested in knowing if a significant drift in the important covariates will correspond to significant levels of unfairness. Secondly, we are interested in knowing if unfairness flows in the direction of the drift of the important covariates. Specifically, we want to investigate whether, if the most important covariates drifts significantly for a particular demographic group, the measured bias will also flow in that same direction.\nRanking of Covariates. To determine the important covariates, for each dataset, we trained the 4 baseline models, i.e., RF, LR, XGB, and SVM and used the co-efficient weights (for LR and SVM), covariate importance scores (for XGB and RF), and the well-established SHapley Additive exPlanations (SHAP) values (for all 4 models). All the covariate importance scores were normalized to be between 0 and 1 and used to rank the covariates. Ranking results have been excluded due to space constraints.\nCovariate Drift: All datasets were partitioned into 3 seasons based on the timestamp of the records in each dataset. Thus simulating historical training dataset at time to denoted, and two test datasets at times t\u2081 and t2 respectively. For example, the NWF dataset is partitioned as (1) Historical data at time to (2015-2017), i.e., long before COVID-19 (LBC); (2) Pre-covid data at time t\u2081 (2018-2019), i.e., immediate pre-COVID-19 (PC); and (3) Peri-covid data at time t2 (2020), i.e., during or peri-COVID (PeC). We computed the size of covariate drift across the 3 seasons using Jensen-Shannon Distance (JSD) in the following fashion: JSDt01(Xto, Xt\u2081) and JSDt02(Xto, Xt2) (c.f. Equation 4)."}, {"title": "4.3 Model Training and Testing", "content": "We refer to the models without any fairness constraints as baseline models, and the models with fairness constraints as fairness-aware models. Overall, we used a total 4 baseline models and 227 fairness-aware models for all our experiments as shown in Table 3.\nTrain-Test Split: Recall that all datasets are partitioned into 3 seasons at times to, t1, and t2 which represent the historical training dataset, the test dataset1, and the test dataset2 respectively. To compare the impact of the implicit iid assumption often made in the literature, we used cross-validation and bootstrapping to simulate temporal impact of the iid assumption as follows. In the cross-validation, we performed 40 shuffled repeats of 5-fold cross validation, making a total of 200 train-test runs on the historical training dataset. The results from this cross-validation represents situation where the training and test datasets are drawn from the same distribution, i.e., the iid assumption. In the bootstrapping, we trained each model on 200 bootstrapped samples of the historical dataset and tested them on the test dataset\u2081 and test dataset2. The results from this bootstrapping experiment represent the scenario where the training and test datasets do not follow the iid assumption. All hyper-parameters where optimized accordingly."}, {"title": "5 Results and Discussion", "content": "5.1 Relationship Between Differential Covariate Drift and Fairness\nDifferential Covariate Drift (DCD), which is the difference between the covariate drift of a privileged group and an unprivileged group, is first calculated. We then computed the overall DCD by finding the signed and absolute average of the DCDs in the top-6 most important covariates. From hereon, we simply refer to the overall DCD as DCD. To establish the relationship between DCD and fairness, we computed the rate of the"}, {"title": "5.2 Robustness of Fairness Algorithms to Covariate Drift", "content": "This section focuses on the ability of fairness-aware models to maintain their predictive accuracy and fairness guarantees in the presence of covariate drift.  Negative fairness scores represent unfairness against the unprivileged group and positive scores indicate otherwise. Furthermore, the annotations on the bars are the average weighted F1-scores for each model on each test dataset. The bar heights are the average of the particular fairness metric and the error bars are the standard errors of the means. We made the following observations."}, {"title": "5.3 Impact of Design Choices on Fairness", "content": "We made other interesting observations based on design choices such as hyper- parameter optimization and baseline model selection across all datasets and fairness metrics. For example, consider  Furthermore, whereas Islam et al (2022) observed that the post-processing approaches tend to be generally less be impacted by the choice of ML model as compared to the"}, {"title": "5.4 Implications for practice", "content": "From our findings, we discuss some useful practical implications.\nThere is the need for continuous monitoring and evaluation of fairness algorithms. An implication of this is that, before classifying a fairness algorithm as fair and fit for deployment, or classifying an algorithm as unfair and should be discarded, fairness practitioners may have to continuously monitor the deployed algorithm as the tides of fairness can turn really fast. A few recent works have started to explore this line of research (Henzinger et al, 2023a,b). This is an emerging research area that requires significant attention.\nThe flexibility (e.g., in choice of base ML models) in pre- and post-processing approaches can be both strength and weakness. The ability of pre- and post process- ing approaches to pair up with any downstream models gives them the convenience of variety. Moreover, pre- and post-post processing approaches allow seamless integration with popular and powerful ML libraries such as sci-kit learn. However, the downstream models that are applied to the fair data are not designed with any fairness constraints. Therefore, the fairness of the downstream models are not guaranteed. Moreover, extra customization and hyper-parameter optimizations may undo whatever fairness was incorporated in the fair data. Additionally, pre-processing approaches mostly aim to correct the ground truth labels. Therefore, predictive error or accuracy-based fairness metrics cannot be catered for by pre-processing approaches (Islam et al, 2022). Fairness practitioners may address this weakness by ensembling pre- and in-processing approaches where the flexiblity of pre-processing is hybridized with the strictness of in-processing approaches.\nThe cause of algorithmic unfairness is a cocktail of latent variables. Differential covariate drift, is without a doubt, a source of algorithmic unfairness, thus should be addressed. However, wrongly attributing unfairness to differential covariate drift may cause the unfairness to persist even if the algorithm is monitored round-the- clock for any drift and appropriately handled. The source of algorithmic unfairness is multifaceted. Therefore, fairness researchers may have to identify the source of unfairness on a case by case basis and address it accordingly instead of proffering a one-size-fits-all solution."}, {"title": "6 Conclusion", "content": "In this work, we investigated the relationship between differential covariate drift and algorithmic unfairness, and we further analyzed the robustness of 7 existing fairness algorithms in the face of covariate drift. We found that significant drifts in important covariates in addition to higher differential covariate drifts often leads to unfairness. We also found that none of the existing fairness-aware algorithms that we evaluated are robust in the presence of covariate drift. Even more interestingly, in contrast to certain prior studies, we found that there is no correlation between the magnitude and direction of data distributional drift and the ensuing level and direction of unfairness. Based on these insights, the study offers policy implications related to the impact of data distributional drift on fairness algorithms. These implications are important for relevant stakeholders, offering valuable guidance on addressing and mitigating fairness issues in the presence of covariate drift.\nRecently, there are some algorithms that have been designed with distributional drift in mind (Chen et al, 2022; Du and Wu, 2021; Taskesen et al, 2020; Rezaei et al, 2021). In a future study, we intend to perform similar investigations on these algorithms to ascertain if they are indeed robust to distributional drift as claimed. Furthermore, we intend to investigate an interesting line of research which suggests that stability of fairness algorithms can be achieved via surrogate functions by reducing surrogate fairness-gaps and variance (Yao et al, 2024)."}]}