{"title": "Multi-path Exploration and Feedback Adjustment for Text-to-Image Person Retrieval", "authors": ["Bin Kang", "Bin Chen", "Junjie Wang", "Yong Xu"], "abstract": "Text-based person retrieval aims to identify matching individuals using textual descriptions as queries. Existing methods largely depend on vision-language pre-trained models to facilitate effective cross-modal alignment. However, the inherent constraints of these models, including a proclivity for global alignment and limited adaptability, impede optimal retrieval performance. In response, we propose a Multi-path Exploration and Feedback Adjustment (MeFa) network, it utilizes a cyclical pathway of exploration, feedback, and adjustment to perform triple-refined associations both within and across modalities, thereby achieving more precise person-text association. Specifically, we devised an intra-modal reasoning pathway that generates challenging negative samples for cross-modal data to optimize intra-modal inference, thereby boosting model sensitivity to subtle variances. Subsequently, we introduced a cross-modal refinement pathway that leverages intermodal feedback to refine local information, thus enhancing its global semantic representation. Finally, the discriminative clue correction pathway incorporates fine-grained features of secondary similarity as discriminative clues, substantially enhancing the model's capacity to mitigate retrieval failures caused by disparities in fine-grained features. Experimental outcomes on three public TBPS benchmarks demonstrate that MeFa achieves superior cross-modal pedestrian retrieval without necessitating additional data or complex structures. The code will be made available upon acceptance.", "sections": [{"title": "Introduction", "content": "Text-based person retrieval task extends the person re-identification task (Park and Ham 2020; Ye et al. 2022; Wang et al. 2022), aiming to accurately retrieve and identify specific individuals from large-scale image databases based on textual descriptions as queries. By integrating vision-language pre-training(VLP) models (Singh et al. 2022; Zhong et al. 2022; Li et al. 2022), this task surpasses traditional image-to-image retrieval methods, allowing users to locate individuals using only text, thereby significantly enhancing the efficiency of personnel retrieval in complex environments. However, this strategy, which relies on VLP models (Yu et al. 2019; Zhang and Lu 2018), also inherits the inherent limitations of these models, such as a preference for global information and poor self-regulation capabilities, making text-to-image person retrieval still challenging.\nIn the realm of text-based person retrieval, it is imperative that models exhibit a nuanced comprehension of both coarse and fine-grained semantic features encapsulated in textual descriptors and their corresponding visual representations, thereby forging precise matching relationships across multimodal data. A principal challenge in this task is is the effective alignment of fine-grained features across visual and textual modalities. To address this, researchers have devised various sophisticated alignment mechanisms that capture global correspondences between whole person and textual descriptions (Yu et al. 2019; Zhang and Lu 2018) or achieve localized alignment between specific regions of person and detailed textual annotations (Wang et al. 2020; Zhu et al. 2021). However, such complex alignment strategies risk pre-disposing models to overfitting, significantly constraining their generalization capabilities in scenarios characterized by limited data or notable deviations in data distribution (Yu et al. 2019; Zhang and Lu 2018; Wang et al. 2020; Zhu et al. 2021). Furthermore, some researchers have focused on extracting robust fine-grained feature sets, (Shao et al. 2022), while others have utilized auxiliary models to produce enriched textual descriptions that correspond closely to visual data (Li et al. 2022; Bai et al. 2023). While these methodologies bolster the model's ability to detect intricate details, they heavily depend on the quality of the generated or extracted fine-grained features and tend to neglect the broader semantic context (Shao et al. 2022; Li et al. 2022; Bai et al. 2023). This oversight can detrimentally affect the model's efficacy when dealing with complex or ambiguous descriptions.\nRecent research highlights the effectiveness of pretrained models such CLIP (Radford et al. 2021) and ALBEF (Li et al. 2021), which are trained on vast image-text datasets, in mastering cross-modal alignment and comprehending complex text and visual stimuli. Yan et al (Yan et al. 2023) advanced this field by proposing a multi-tiered alignment mechanism based on CLIP, significantly boosting retrieval performance. Further, the IRRA framework (Jiang and Ye 2023) capitalizes on these models to foster more effective visual-text alignment through implicit relational reasoning and cross-modal interactions. Despite these advantages, such models often falter in text-based person retrieval tasks that require discerning fine-grained distinctions, struggling to adapt to subtle image-text variations. This adaptation shortfall is twofold: 1) models such as CLIP and AL-BEF become fixated on specific image-text pair processing during training, acking dynamic optimization capabilities from real-time feedback. 2) their application to fine-grained tasks frequently results in an inability to sift through and exclude irrelevant data associations, thus preventing adaptation to nuanced descriptive styles. Therefore, while VLP models demonstrate formidable cross-modal task handling, their limitations in alignment preferences and feedback regulation for specialized text-based retrieval tasks are critical and demand prompt resolution. These deficiencies significantly impair the models' operational performance and reliability.\nIn this paper, we propose a Multi-path exploration and Feedback adjustment (MeFa) network which employs a cyclical exploration, feedback, and adjustment pathway to progressively achieve finer granular information associations from intra-modal to inter-modal levels. Specifically, to boost the representational capabilities and convergence of the VLP model, we initially employ the efficient Eva-CLIP(Sun et al. 2023) as the base model for extracting both image and text features. Building on this, we propose an intra-modal reasoning(IMR) pathway that utilizes feedback from negative samples highlighting subtle differences to adjust the model's internal representations, thereby improving its ability to discern fine distinctions. Subsequently, we designed a cross-modal refinement(CMR) pathway which intermodal interaction feedback refines global features into locally semantically-rich features, enhancing semantic consistency across scales. Lastly, we introduce a discriminative clue correction(DCC) pathway that utilizes fine-grained features of secondary similarity as discriminative cues to correct mismatches in person-text alignment caused by subtle differences. Contrasting with recent rigid alignment methods(Cao et al. 2024), MeFa depends on intrinsic feedback and adjustment across each pathway to ensure flexible and robust person-text matching. Our contributions are three-fold:\n\u2022\n\u2022\n\u2022 We propose MeFa network that employs an exploration, feedback and adjustment paradigm to mitigate global information biases and enhances self-regulatory capabilities in pre-aligned VLP models.\nLeveraging this paradigm, we meticulously associate fine-grained information across three pathways: intra-modal reasoning, cross-modal refinement, and discriminative cue correction.\nRigorous experiments on three benchmarks including CUHK-PEDES (Li et al. 2017), ICFG-PEDES (Ding et al. 2021), and RSTPReid (Zhu et al. 2021) show that MeFa outperformed prevailing advanced methods and achieved enhanced model convergence."}, {"title": "Related Work", "content": "Text-based Person Retrieval. Existing methods for Text-based Person Retrieval can be classified into two categories based on whether VLP models are utilized: the first either does not employ, or only uses language pre-training models in the text branch (Devlin et al. 2018). These methods rigorously examine multi-layered associations between images and text to explore diverse intra-modal and inter-modal alignment mechanisms, thereby achieving varied granularities of cross-modal alignment (global-global (Ding et al. 2021), local-local (Han et al. 2021), and global-local (Wang et al. 2020). These methods assume consistent semantic correspondence between modalities, the dynamic and variable relationships between text descriptions and images mean that static alignment mechanisms may fall short in diverse real-world scenarios. The second type relies on VLP models pre-trained on large-scale multimodal datasets (Radford et al. 2021; Li et al. 2021), utilizing their substantial cross-modal semantic capabilities to achieve accurate text-image matching, even with vague or unconventional text descriptions. Liu et al. (Liu et al. 2021) used a contrastive learning framework to transfer knowledge from large-scale generic image-text pairs to image search tasks, significantly surpassing previous methods. Subsequently, Wang et al.(Wang et al. 2020) introduced a dual pre-trained modality framework to transfer CLIP knowledge, yet its dependence on predefined prompt templates restricted flexibility in managing unconventional or novel attributes. Yu et al. (Yu et al. 2024) explored a text-free learning framework using CLIP for video-based person retrieval tasks. These methods inherit inherent limitations of VLP models, including a preference for global information and poor self-regulation, which restricts model performance. In this work, we explore alignment through multiple pathways and adaptively adjust inter-modal associations based on feedback signals, proposing the MeFa framework to achieve more flexible and robust cross-modal fine-grained alignment.\nVision-Language Pre-training models have demonstrated significant potential across a variety of vision-language tasks. These tasks include image-text retrieval (Diao et al. 2023) and image captioning (Yang et al. 2023). By pre-training on vast collections of image-text pairs, these models acquire expressive multimodal feature representations that yield remarkable performance enhancements across several downstream tasks such as open-vocabulary detection (Ma et al. 2023; Wang et al. 2024), zero-shot semantic segmentation (Jiao et al. 2023), and text-based pedestrian retrieval (Liu et al. 2024; Jiao et al. 2023; Li et al. 2024). A quintessential example of vision-language models is CLIP (Radford et al. 2021), which leverages 400 million internet-sourced image-text pairs with contrastive learning to align features in a unified space, extensively applied in real-world tasks (L\u00fcddecke and Ecker 2022; Subramanian et al. 2022; Sanghi et al. 2022), notably zero-shot and few-shot challenges. Following CLIP, EvaCLIP (Sun et al. 2023) introduces enhanced initialization and optimization mechanisms, boosting zero-shot transfer capabilities and demonstrating efficiency in various zero-shot and few-shot tasks.\nDespite these advancements, these models often prioritize global semantic alignment, neglecting the association of fine-grained local information crucial for tasks demanding detailed contextual awareness such as text-based pedestrian retrieval. Furthermore, these models typically lack the self-regulation required to adapt to the complexities and variations of real-world scenarios."}, {"title": "Method", "content": "In this section, we present MeFa, a framework tailored for text-based person retrieval that focuses on multi-granular and multi-level alignment exploration, augmented by adaptive feedback regulation to enhance performance. The overall structure of MeFa is illustrated in Figure 2.\nPreliminaries\nIn this study, we utilized a classic dual-encoder architecture to extract textual and image features separately. Text-based person retrieval models built on frameworks like CLIP primarily optimize distances between matched and unmatched image-text pairs through contrastive learning, so the model's performance is high sensitivity to the quantity and quality of samples within a batch. To boost the CLIP model's feature representation and hasten its convergence, we selected the superior performing EVA-CLIP as the initial model to improve cross-modal alignment potential. Specifically, for an input image $I \\in R^{H \\times W \\times C}$, I is divided into N fixed-size, non-overlapping patches. These patches are linearly mapped and then fed into an image encoder, producing local embedding representations ${V_1, V_2, ..., U_n}$ and a global embedding representation $v_{cls} \\in R^{D}$, where D is the dimension of the embeddings. Similarly, the input text T, represented as $T\\in R^{M \\times D}$ with M as the number of tokens and D as the embedding dimension, is encoded into a series of token features ${T_1, T_2, . . ., T_M }$ and a global feature $T_{cls} \\in R^{D}$.\nIntra-modal reasoning path\nIn the TBPS task, the primary objective is to accurately interpret and match the complex semantic content between images and texts, necessitating a model capable of discerning subtle semantic differences. To enhance this ability, we initially construct a series of challenging negative samples. For text, we develop three tiers of negative samples to test different aspects of semantic understanding: Tier-one uses the NLTK tool to swap key nouns, testing the model's sensitivity to subject-object relations; Tier-two randomly substitutes verbs or adjectives to evaluate the model's understanding of actions and states; and tier-three obscures key words and utilizes a pre-trained RoBERTa model for completion, thereby heightening uncertainty and probing the model's capacity to manage ambiguous contexts. For people images, given the unique challenges of the TBPS task, simple data augmentation techniques are insufficient to accurately mimic real person feature. Consequently, we utilize the target person's image as a query input, searching within a vectorized training database for visually similar pedestrians, and selecting the top-k similar pedestrians as negative samples.\nFurthermore, we have designed an intra-modal compound loss mechanism to enhance the model's ability to distinguish challenging negative samples which comprises two components: the intra-modal separation loss is designed to ensure substantial differentiation between positive and negative samples, expressed as follows:\n$L_{imr} = max(0, a + D(f_a, f_p) \u2013 D(f_a, f_n))$ (1)\nwhere $f_a$, $f_p$, and $f_n$ represent the feature embeddings of the target image, the matching text description, and the non-matching text description, respectively. D denotes the cosine similarity, and a is a margin ensuring adequate separation between positive and negative samples. The intra-modal contrastive loss is designed to enhance the model's sensitivity across a set of negative samples, defined as:\n$L_{imc} = \\frac{1}{N} \\sum_{i=1}^{N} log (1+exp(x(D(f_a, f_p)) - \\min_{f_n \\in N_i} D(f_a, f_n))))$ (2)\nwhere $\\gamma$ is a scaling factor adjusting the sensitivity of the loss, N represents the batch size, and $N_i$ indicates a set of challenging negative samples corresponding to the ith sample.\nCross-modal refinement path To further bridge the gap between task images and text descriptions, we have refined local features by exploring cross-modal interactions between global and local elements. Specifically, we first adjust the representation of local features through attention weights between cross modal local features, which can be formulated as follows:\n$\\v_i = \\sum_{t_i}^{N} \\frac{exp(s(v_i,t_i))}{\\sum_{t=1}^{M} exp(s(v_i, t))} v_i$ (3)\nwhere $v_i$ represents the i-th local feature of the image, and $t_j$ represents the j-th local feature of the text. $s(v_i, t_j)$ is the cosine similarity between $v_i$ and $t_j$. Although the interaction-enriched local representations can encode more detailed clues, the global features condense contextual information and high-level semantics. Thus, we employ weighted global features to refine each modality's local features, enhancing the semantic capability of local features. More specifically:\n$v_i^* = (\\hat{v_i} \\oplus g) \\odot \\sigma(W_f \\cdot (\\hat{v_i} \\oplus g) + b_f)$ (4)\n$t_i^* = (\\hat{t_i} \\oplus g) \\odot \\sigma(W_f \\cdot (\\hat{t_i} \\oplus g) + b_f)$ (5)\nwhere $\\oplus$ denotes feature concatenation fusion, $\\odot$ denotes element-wise multiplication, $\\sigma$ is the tanh activation function, and $W_f$ and $b_f$ are learnable parameters. These operations not only strengthen the semantic associations between cross-modal local features but also enhance the semantic representation of local features. After enhancing the interaction of local features, we introduce the NITC to refine the fine-grained information alignment of cross-modal features, further ensuring a high level of semantic consistency between the image and text descriptions at both global and local levels. Specifically:\n$L_{nitc} = \\frac{1}{2N} \\sum_{i=1}^{N} (\\sum_{j=1}^{M} p_{i,j} log Q_{i,j}) + \\sum_{j=1}^{M} (q_{i, j} log P_{i,j})$\n) (6)\nWhere $p_{i,j}$ and $p_{i,j}$ respectively represent the true association probability between instance i and text j and the model's predicted association probability, which helps the model capture rich local details and align cross-modal semantics at a more granular level.\nDistinct cues correction path\nAfter interaction alignment via the CMR pathway, the model effectively retrieves most prominent cross-modal associative features. At this juncture, resolving target ambiguities hinges on subtler discriminative cues. To more robust region-text matching, we initially compute intermodal similarities $s(v_i, t_i)$, selecting pairs with intermediate levels of similarity. These pairs are employed to formulate a corrective state $R_k$, encapsulating subtle yet pivotal semantic links between regions and texts. Specifically, $R_k$ is comprised of the top K words that are secondarily relevant to the overall sentence, focusing the model on subtle discriminative details. We then introduce a Discriminative Weighted Contrastive Loss (D-ITC):\n$L_{ditc} = \\sum_{i=1}^{N} log \\frac{exp((q_j R_k)/\\tau)}{\\sum_{j=1}^{M} exp((q_j.v_i)/\\tau)}$ (7)\nwhere $q_j$ represents textual features, $v_i$ denotes regional features, and $\\tau$ is a parameter tuning the softmax function's acuity. This methodology not only enhances prominent features but also refines the representation of more subtle attributes, thereby elevating region-text matching performance."}, {"title": "Experiments", "content": "Benchmark Setup\nDatasets. We evaluated the efficacy of mefa across three benchmarks. (1) CUHK-PEDES, the first large-scale text-based pedestrian recognition benchmark, encompasses 13,003 individuals and 40,206 images, each equipped with two manually crafted descriptions, totaling 80,412 textual descriptions. Descriptions average at least 23 words in length, encompassing rich visual details, and the dataset is divided into training, validation, and test sets. (2) ICFG-PEDES, sourced from the MSMT17 dataset, focuses on identity descriptions and details compared to CUHK-PEDES, comprising 54,522 images and corresponding text descriptions covering 4,102 individuals, with each description averaging 37 words. (3) RSTPReid, also based on MSMT17, aims to address real-world challenges and includes 20,505 images and 41,010 text descriptions involving 4,101 individuals. Each individual is captured in 5 images taken by 15 cameras, with each image accompanied by two descriptions, each no fewer than 23 words.\nEvaluation Metrics. In evaluating these datasets, Rank-K accuracy metrics, including Rank-1, Rank-5, and Rank-10, are primarily utilized. Rank-1 accuracy denotes the frequency with which the correct individual ranks first in retrieval results. Rank-5 and Rank-10 extend this criterion to measure appearances within the top five and ten results, respectively. Additionally, mean Average Precision (mAP) is another pivotal metric that quantifies the average precision of multiple queries, critically assessing performance in fine-grained retrieval tasks by reflecting the accuracy of ranked results.\nImplementation Details. Training is conducted on 8 RTX-4090 GPUs with a total batch size of 80. To ensure a fair comparison, we employed the ViT-B/32 CLIP as our backbone network, with all input images uniformly resized to 224 x 224 pixels and the maximum length of text token sequences set at 77 tokens. To accelerate convergence and extract richly representative features, we initialized parameters using the EvaCLIP pretrained model. Following EvaCLIP's guidelines, we adjusted weights using the LAMB optimizer. The model was trained over a total of 12 epochs, with the learning rate linearly increasing from $1 \\times 10^{-6}$ to $1 \\times 10^{-5}$.\nBenchmark Results\nIn this section, we compare our method with several state-of-the-art Methods on CUHK-PEDES, ICFG-PEDES and RST-PReid benchmark datasets."}, {"title": "Limitations and Conclusions", "content": "In this paper, we focus on exploring the impact of global information preferences and self-regulation limitations in pretrained models, introducing the MeFa network framework. By integrating modal internal reasoning, cross-modal refinement, and discriminative information correction, the MeFa network significantly enhances the model's accuracy in handling fine-grained information, achieving advanced results across various TBPR benchmarks. Moreover, our research identifies an interference effect when different alignment mechanisms are used together; the benefits brought by using alignment mechanisms A and B individually do not simply accumulate. This phenomenon suggests that the interactions between alignment mechanisms are complex and not yet fully understood. Consequently, we have deferred further"}]}