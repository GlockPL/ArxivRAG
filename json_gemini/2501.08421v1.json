{"title": "SEAL: Speaker Error Correction using Acoustic-conditioned Large Language Models", "authors": ["Anurag Kumar", "Rohit Paturi", "Amber Afshan", "Sundararajan Srinivasan"], "abstract": "Speaker Diarization (SD) is a crucial component of modern end-to-end ASR pipelines. Traditional SD systems, which are typically audio-based and operate independently of ASR, often introduce speaker errors, particularly during speaker transitions and overlapping speech. Recently, language models including fine-tuned large language models (LLMs) have shown to be effective as a second-pass speaker error corrector by leveraging lexical context in the transcribed output. In this work, we introduce a novel acoustic conditioning approach to provide more fine-grained information from the acoustic diarizer to the LLM. We also show that a simpler constrained decoding strategy reduces LLM hallucinations, while avoiding complicated post-processing. Our approach significantly reduces the speaker error rates by 24-43% across Fisher, Callhome, and RT03-CTS datasets, compared to the first-pass Acoustic SD.\nIndex Terms-Speaker Diarization, LLMs, Error Correction", "sections": [{"title": "I. INTRODUCTION", "content": "Speaker Diarization (SD) solves the problem of determining \"Who spoke when\" in an audio recording. SD systems can be broadly categorized into two types: 1) Modular systems [1], [2], which consist of components like a segmenter and an embedding model, and 2) End-to-end systems [3]-[5], which are designed to handle speech overlaps and are directly optimized for diarization by incorporating permutation invariant training loss, followed by a clustering phase. However, many real-world applications like meeting analytics, call-center analytics, and video captioning often require associating spoken words with the speaker labels, as opposed to just the speaker time ranges predicted by SD modules.\nConventional ASR [6]\u2013[9] systems are generally designed for speaker-agnostic scenarios and answer the question \"What was spoken\" without providing speaker labels. Multi-speaker transcription systems, however, address the question \"Who spoke what and when,\" which is essential for many practical applications. Various approaches have been developed to achieve this, primarily falling into three categories: 1) Speech/Speaker Separation followed by ASR [10]\u2013[12], 2) Speaker-attributed ASR [13]\u2013[16], and 3) Modular ASR and SD systems. Challenges with Speech Separation and SA-ASR systems include difficulties with a larger number of speakers [10], duplicated artifacts [11], lack of speaker timestamps [13], and handling long-form audio [11]. Independent ASR and SD systems, which operate separately and later reconcile their results, offer a more practical solution for multi-speaker transcription pipelines.\nIn modular ASR-SD systems, the ASR and SD components are trained independently and combined using their respective timestamps, a process we refer to as reconciliation. This rec-onciliation can lead to speaker attribution errors, compounded by inherent errors in the SD system, especially during speaker transitions or in overlapping speech regions [17], [18]. Although End-to-End Neural Diarization (EEND) systems can handle speech overlaps, since the speaker agnostic ASR system can only detect words from one speaker at a time, this can still produce speaker errors in overlap regions.\nLexical information, which complements acoustic data, can be crucial for accurately assigning speaker labels [17]-[20]. For example, by analyzing a transcript like \"how are you i am good\", one can infer a speaker change between \"how are you\" and \"i am good\". In this work, we propose SEAL, a Speaker Error Corrector using Acoustic-conditioned Large Language Models, which integrates the LLM's inherent lexical knowledge with acoustic information from the SD system. Additionally, we introduce a Constrained Decoding (CD) technique to prevent the LLM from hallucinating, ensuring that the generated word sequence strictly matches the original transcript without modifying any words."}, {"title": "II. RELATED WORK", "content": "Some earlier works [21], [22] have utilized lexical knowledge for speaker diarization, but they have been limited to speakers with specific roles. More recently, a Lexical Speaker Error Correction (SEC) framework [17] and its extension with acoustic score fusion [18] have been proposed to correct general speaker errors using a pre-trained Language Model (LM) encoder, such as RoBERTa, demonstrating good improvements. However, these methods only utilize a pre-trained LM encoder and do not take advantage of the capabilities of modern instruction-tuned large LLMs.\nA few works [23]-[25] have explored the use of LLMs for either performing SD or correcting speaker errors. For instance, [23] uses an LLM to predict speaker probabilities for the next word, incorporating these into beam search decoding along with the acoustic scores for speaker diarization. However, this approach will be computationally expensive since the LLM must be invoked for every word in the transcript, and the beam search adds additional complexity. Other works [24], [25]"}, {"title": "III. SPEAKER ERROR CORRECTION USING ACOUSTIC-CONDITIONED LLMS (SEAL)", "content": "An overview of the SEAL framework is shown in Fig 1.\n\nA. First pass Speaker Diarization Module\n\nEEND is a preferred choice as a first pass acoustic diarization module for our setup as it can efficiently handle overlaps and output soft speaker scores that can acoustically ground the LLM decisions for speaker error correction. Given the frame level acoustic features $X = {x_i}_{i=1}^t, X \\in R^{t \\times f}$, where $t$ and $f$ represent the number of frames and feature dimensions respectively, the speaker posteriors $P = {p_i}_{i=1}^t, P \\in R^{t \\times k}$, where $k$ represents the number of speakers, are obtained using an EEND network $f_{EEND}$\n\n${P_1, P_2, ..., P_t} = f_{EEND}(X_1,X_2, ..., X_t)$\n\nThese frame-level posterior scores are then median-filtered and mean-pooled at the word level to produce word-level aggregated posteriors {$s_i$}$_{i=1}^W$, where $W$ is the number of words in the transcript, with values ranging from [0, 1], following the approach in [18].\n\nB. Acoustic Conditioning\n\nTo account for the fact that LLMs are predominantly trained on text and often struggle with understanding numerical values without specialized mechanisms [29], we propose converting the soft scores from the EEND module into more easily interpretable labels. We classify the scores into three categories: \u201clow\u201d, \u201cmedium\u201d and \u201chigh\u201d. For each word-level score $s_i$, we assign a category $c_i$ based on the following formula:\n\n$C_i = \\begin{cases}\nhigh, & \\text{for } T_{hmed} < s_i \\leq 1 \\\\\nmed, & \\text{for } T_{hlow} < s_i \\leq T_{hmed} \\\\\nlow, & \\text{for } 0 < s_i \\leq T_{hlow}\n\\end{cases}$\n\nThe thresholds $T_{hlow}, T_{hmed}$ are tuned based on a dev set and we refer this component as the \u201cScore Mapper\" module in Figure 1. To assess the effectiveness of this approach, we also conducted an ablation study using the raw probabilities from the EEND module as well as a more granular categorization approach that divides the scores into 10 uniformly distributed integer classes between [0,1]. We refer to the model finteuned only on ASR transcripts without any acoustic conditioning as LLMAC-none, the model acoustically conditioned with raw speaker probabilities $s_i$ as LLMAC-prob, the model acoustically conditioned with speaker confidence labels $c_i$ as LLMAC-label and the LLM conditioned on the 10 integer classes as LLMAC-int. These comparisons are detailed in Section V.\nWith these different conditioning strategies, we experimented with two transcript formats: \"Spkturn\", which follows the same format from [24], [25] where speaker turns are indicated after the speaker label, and \"Spkword\" where each word is followed by its speaker label and corresponding acoustic label."}, {"title": "C. Constrained Decoding (CD)", "content": "Fine-tuned LLMs may still hallucinate and produce non-verbatim outputs [24], [25]. To address this, [24] introduces"}, {"title": "IV. EXPERIMENTAL SETUP", "content": "A. Dataset and Metrics\n\nWe use the Fisher dataset [30], [31] to finetune LLMs. The same pre-processing as [18], [24] was done to get a single channel Fisher audio inputs and respective ground-truth transcripts. We held out the same partition of the dataset as test set as mentioned in [18], [24], [32]. In addition to that, we also test our models on CALLHOME American English (CHAE) [33] and RT03-CTS [34].\nWe use the concatenated minimum permutation WER or cpWER [35] and $A_{cp}$ [23] as our main evaluation metrics"}, {"title": "V. RESULTS AND DISCUSSION", "content": "We evaluate different acoustic-conditioning variants of the SEAL models in Table I. As expected, it can be observed that LLMAC-prob struggles with raw probabilities, leading to degraded Acp compared to the non acoustic-conditioned model, LLMAC-none. Although LLMAC-int shows marginal improve-ments, LLMAC-label yields the best results by converting speaker probabilities into simple, parsable words.\nBased on these findings, Table II benchmarks the best-performing SEAL model, LLMAC-label, against other baselines. The first part of the table presents models without acoustic conditioning and the first-pass Acoustic SD baseline. All systems share the same SD and ASR components, resulting in identical WERs, as word hallucinations with the LLMs are eliminated using TPST or CD. The LLMAC-none with Spkturn format replicates the DiarizationLM framework [24] but with a different LLM (Mistral 7b Instruct v0.2) and baseline ASR, SD systems. While LLMAC-none improves error correction on the Fisher test set, it underperforms on other datasets, likely due to differing conversation types (informal vs. formal) and limited domain adaptability. LSEC [11] performs best among text-only models, underscoring the limitations of LLMs in handling natural conversations without further conditioning. The second part of Table II compares acoustic-conditioned models using CD or TPST for generating verbatim transcripts. AG-LSEC [18] consistently performs well across datasets, leading in two out of the five data splits. Acoustic conditioning significantly improves LLM performance across all datasets, as demonstrated by the LLMAC-label rows. CD further enhances performance significantly compared to TPST as demonstrated by the TPST and CD rows with Spkturn transcript format. Additionally, changing the transcript format from Spkturn to Spkword boosts performance in most data splits. Overall, non acoustic-conditioned LLMs lag behind specialized LSEC [17] and AG-LSEC [18] models, while SEAL models outperform AG-LSEC in most splits, achieving relative Acp improvements of 24-43% over first-pass Acoustic SD. Qualitative examples of the different correction strategies are shown in Figure 3."}, {"title": "VI. CONCLUSION", "content": "We introduce SEAL, a novel Speaker Error Corrector using Acoustic-conditioned LLMs that incorporates acoustic information from the SD system, coupled with a constrained decoding strategy to ensure that generated word sequences align precisely with the original transcripts. Our evaluations across three datasets shows that acoustic conditioning significantly enhances the LLM's Speaker Error Correction (SEC) capabili-ties and its ability to generalize to diverse conversation styles. The combination of our acoustic-conditioning and constrained decoding consistently surpasses both the first-pass acoustic SD baseline and the non acoustic-conditioned LLMs with TPST on the Fisher, Callhome, and RT03-CTS datasets. While SEAL models also achieve modest improvements over the specialized LSEC models, they offer the additional advantage of being versatile for tasks beyond SEC and will continue to improve as more powerful LLMs are developed. Future work will focus on expanding this framework to languages beyond English."}]}