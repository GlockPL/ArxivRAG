{"title": "ON STORAGE NEURAL NETWORK AUGMENTED\nAPPROXIMATE NEAREST NEIGHBOR SEARCH", "authors": ["Taiga Ikeda", "Daisuke Miyashita", "Jun Deguchi"], "abstract": "Large-scale approximate nearest neighbor search (ANN) has been gaining atten-\ntion along with the latest machine learning researches employing ANNs. If the\ndata is too large to fit in memory, it is necessary to search for the most similar\nvectors to a given query vector from the data stored in storage devices, not from\nthat in memory. The storage device such as NAND flash memory has larger ca-\npacity than the memory device such as DRAM, but they also have larger latency\nto read data. Therefore, ANN methods for storage require completely different\napproaches from conventional in-memory ANN methods. Since the approxima-\ntion that the time required for search is determined only by the amount of data\nfetched from storage holds under reasonable assumptions, our goal is to minimize\nit while maximizing recall. For partitioning-based ANNs, vectors are partitioned\ninto clusters in the index building phase. In the search phase, some of the clusters\nare chosen, the vectors in the chosen clusters are fetched from storage, and the\nnearest vector is retrieved from the fetched vectors. Thus, the key point is to accu-\nrately select the clusters containing the ground truth nearest neighbor vectors. We\naccomplish this by proposing a method to predict the correct clusters by means of\na neural network that is gradually refined by alternating supervised learning and\nduplicated cluster assignment. Compared to state-of-the-art SPANN and an ex-\nhaustive method using k-means clustering and linear search, the proposed method\nachieves 90% recall on SIFT1M with 80% and 58% less data fetched from storage,\nrespectively.", "sections": [{"title": "1 INTRODUCTION", "content": "Large-scale Approximate Nearest Neighbor searches (ANNs) for high-dimensional data are re-\nceiving growing attentions because of their appearance in emerging directions of deep learning\nresearch. For example, in natural language processing, methods leveraging relevant documents\nretrieval by similar dense vector search have significantly improved the scores of open-domain\nquestion-answering tasks (Karpukhin et al., 2020). Also for language modeling tasks, Borgeaud\net al. (2021) showed a model augmented by retrieval from 2 trillion tokens performs as well as 25\ntimes larger models. In computer vision, Nakata et al. (2022) showed that image classification using\nANNs has potential to alleviate catastrophic forgetting and improves accuracy in continual learning\nscenarios. In reinforcement learning, exploiting past experiences stored in external memory for an\nagent to make better decisions has been explored (Blundell et al., 2016; Pritzel et al., 2017). Re-\ncently, Goyal et al. (2022) and Humphreys et al. (2022) attempted to scale up the capacity of memory\nwith the help of ANN and showed promising results.\nANNs are algorithms to find one or k key vectors that are the nearest to a given query vector among\na large number of key vectors. Strict search is not required but higher recall with lower latency is\ndemanded. In order to achieve this, an index is generally build by data-dependent preprocessing.\nThus, an ANN method consists of the index building phase and the search phase. As the number of\nkey vectors increases, storing all of them in memory (e.g. DRAM) becomes very expensive, and it\nis forced to store the vectors in storage devices such as NAND flash memory. In general, a storage\ndevice has much larger capacity per cost, but also its latency is much larger than memory. When\nall the key vectors are stored in memory, as seen in the most papers regarding ANN, it is effective\nto reduce the number of calculating distance between vectors by employing graph-based (Malkov"}, {"title": "2 RELATED WORKS", "content": "ANN for storage. DiskANN (Jayaram Subramanya et al., 2019) is a graph-based ANN method\nfor storage. The information of connections defining graph structure and the full precision vectors\nare stored on storage and the vectors compressed by Product Quantization (J\u00e9gou et al., 2011a)\nare stored in memory. The algorithm traverses the graph by reading the connection information\nonly on the path from storage and computes distances between a query and the compressed vectors\nin memory. Although they compensate the deterioration of recall due to lossy compression by\ncombining reranking using full precision vector data, the recall-latency performance is inferior to\nSPANN (Chen et al., 2021). SPANN is another method dedicated to ANN for storage and exhibits\nstate-of-the-art performance. It employs a partitioning-based approach. By increasing the number of\nclusters as much as possible, it achieved to reduce the number of vectors fetched from storage under a\ngiven recall. In order to reduce the latency to choose clusters during search even when the number of\nclusters are large, they employ SPTAG algorithm that combines tree-based and graph-based ANNs.\nThey also proposed an efficient duplication method aiming at increasing probability that a chosen\ncluster contains the ground truth key vector. However, our investigation in Section 3.2.2 shows that\nits performance can be worse than a naive exhaustive method on some dataset.\nANN with GPU. FAISS (Johnson et al., 2019) supports a lot of ANN algorithms accelerated by\nusing GPU's massively parallel computing. On-storage search is also discussed in their project\npage. SONG (Zhao et al., 2020) optimized the graph-based ANN algorithm for GPU. They modified\nthe algorithm so that distance computations can be parallelized as much as possible, and showed\nsignificant speedup. However, they assume only all-in-memory scenarios.\nANN with neural networks. DSI (Tay et al., 2022) predicts the indices of the nearest neighbor key\nvectors directly from the query vectors with a neural network. We explore to use neural networks to\npredict the clusters containing the nearest neighbor vector rather than the vector indices themselves.\nDSI is also targeted for all-in-memory ANN. BLISS (Gupta et al., 2022) and NeuralLSH (Dong\net al., 2019) are methods to improve the partitioning rule using neural networks. They apply the\nsame rule to a query for choosing clusters as well. As depicted in Section 3.3, when the rule for\npartitioning keys is employed to choose clusters, often the chosen clusters don't contain the ground\ntruth key vector. Our method where a neural network is trained to predict the correct cluster for a\ngiven query is orthogonal and can be combined with these methods."}, {"title": "3 PRELIMINARIES", "content": "In this paper, we assume that the system on which our ANN algorithm runs has GPUs and storage\ndevices in addition to CPUs and memories. The GPU provides high-throughput computing through\nmassively parallel processing. The storage can store larger amount of data at lower cost, but has\nlarger read latency than memory devices. Data that are commonly used for all queries, e.g., all the\nrepresentative vectors of clusters, are loaded in advance on the memories from which CPU or GPU\ncan read data with low latency and is always there during the search. On the other hand, all the\nkey vectors are stored in the storage devices, and for simplicity, we assume that data fetched from\nstorage for computations for a query is not cached on memory to be reused for computations for\nanother query."}, {"title": "3.2 METRICS", "content": "Our goal is to minimize the average search time per query for nearest neighbor search, which we\nrefer to as mean latency, and simultaneously to maximize the recall. Without loss of generality, the"}, {"title": "3.2.1 THE NUMBER OF FETCHED VECTORS AS A PROXY METRICS OF LATENCY", "content": "Our goal is to minimize the average search time per query for nearest neighbor search, which we\nrefer to as mean latency, and simultaneously to maximize the recall. Without loss of generality, the"}, {"title": "3.2.2 MEMORY USAGE", "content": "Since memory usage greatly affects the latency of in-memory ANNs, VQ (Vector-Query), which is\na measure of throughput normalized by the memory usage, is introduced in GRIP (Zhang & He,\n2019) to compare algorithms with different memory usage as fairly as possible, and is also utilized\nin SPANN (Chen et al., 2021). SPANN claims superior capacity in large vector search scenarios\nbecause this VQ value is greater than that of other algorithms. Here, we consider whether VQ is\nreally fair metrics for comparing the methods with different memory usage. In a partitioning-based\nANN method for storage, memory capacity limits the number of clusters since the representative\nvectors of all the clusters must be kept in memory during search. Then, we investigate how the num-\nber of clusters affects the recall-latency and recall-VQ curves."}, {"title": "3.3 VISUALIZATION WITH 2-DIMENSIONAL \u03a4\u039f\u03a5 DATA", "content": "In this section, we explain the intuition behind our proposed method and visually demonstrate how\nit improves the accuracy to choose the correct cluster. One hundred key vectors are uniformly\nsampled from 2-dimensional space between -1 to 1. In the index building phase, we divide the\nkey vectors into four clusters. The representative vectors of the clusters are placed at (x, y) =\n(1/2,1/2), (-1/2, 1/2), (-1/2, -1/2), (1/2, -1/2). Then we assign one cluster to each key vector\naccording to the Euclidean distance, i.e., the distance of a key vector to the representative vector of\nthe assigned cluster is smaller than that of other clusters. As a result, the true border lines of the clusters for query are quite complex."}, {"title": "4 PROPOSED METHOD", "content": "Therefore, to improve the accuracy to choose correct cluster is the fundamental challenge. We\nattempt to accurately predict the complex border lines by using a neural network that is trained\nwith the objective to choose the correct cluster. We use simple three layer MLP. Input dimension\nis equal to the dimension of query and key vectors, and output dimension is equal to the number\nof clusters, and the dimension of hidden layer is set to 128 in this experiment. The query vectors\nfor training are sampled independently every epoch from the same distribution. The ground truth\ncluster is searched by exhaustive search for each training query. Using those pair samples of query\nand ground truth cluster as training data, we train the neural network with cross-entropy loss in\nsupervised manner. Note that this is a data-dependent method because we look into the clustered"}, {"title": "4.1 EXPERIMENT RESULTS", "content": "In this section, we describe the experiment using SIFT (J\u00e9gou et al., 2011a;b) and CLIP (Radford\net al., 2021) data for demonstrating that our proposed method is useful for realistic data.\nDataset. For SIFT1M, we use one million 128-dimensional SIFT1M base data as key vectors.\nAnother one million data are sampled from SIFT1B base data and used as query vectors for training.\nSIFT1B query data are used as query vectors for test. Euclidean distance is employed as metrics.\nFor CLIP, we extracted feature vectors from 1.28 million ImageNet (Deng et al., 2009) training data\nwith ViT B/16 model (Dosovitskiy et al., 2021). Although the dimension of the feature vector of the\nmodel is 512, we use the first 128 dimension for our experiment. We split it into 0.63 million, 0.64\nmillion, and 0.01 million for key vectors, training query vectors, and test query vectors, respectively.\nCosine similarity is employed as metrics.\nComparison with conventional methods. We compare our method with two conventional meth-\nads. The first one is the exhaustive method where key vectors are partitioned by k-means in the\nindex building phase, and the distances of a query to the representative vectors of all the clusters\npartitioned by k-means are calculated and the cluster corresponding to the closest representative\nvector is chosen in the search phase. The second one is SPANN (Chen et al., 2021). For SPANN, we\nbuild the index by the algorithm implemented in SPANN, which includes partitioning process. Since\nSPANN proposes multiple cluster assignment for improving recall, we set the ReplicaCount to"}, {"title": "4.2 ABLATION STUDY", "content": "Table 4 shows how much each ingredient of our proposed method improves the metrics. We report\nthe average and standard deviation values of the number of fetched vectors across 10 trials for\neach condition."}, {"title": "4.2.1 EFFECT OF EACH INGREDIENT", "content": "Table 4 shows how much each ingredient of our proposed method improves the metrics. We report\nthe average and standard deviation values of the number of fetched vectors across 10 trials for\neach condition. The both MCE and CE show\ngood performance but CE is better in R@1\u22640.95 and MCE is better in R@1=0.99. By comparing\n(a,d,e,f), the both neural network and duplication contribute to improving performance. The result is worse than that in the default setting where duplication is executed every 50 epochs.\nThis indicates that executing duplication process between training can relax the complexity of the\nborder lines of clusters and help the neural network to fit them. (i) shows the result when we"}, {"title": "4.2.2 BUILDING INDEX BY SPANN", "content": "In our experiment, we use k-means for partitioning, but it is an exhaustive method and can take\ntoo much time for larger dataset. In order to confirm that k-means is not a necessary component\nof our method, we apply the algorithm in SPANN to execute partitioning. As a result, the number\nof fetched vectors under 90% recall significantly improves from 30729\u00b11749 to 7372\u00b1162. This\nindicates that we may utilize a fast algorithm such as SPANN for clustering instead of exhaustive\nk-means when our proposed method is applied to larger dataset."}, {"title": "5 LIMITATIONS AND FUTURE WORKS", "content": "Since the discussion in this paper assumes that the condition that the mean latency for search is\ndetermined by storage access time holds true, the discussion in this paper may be invalid if this\ncondition is not satisfied.\nFor CLIP data, the improvement over the exhaustive method is steady but marginal as shown in\nFigure 4. The difference of the amount of improvement between SIFT and CLIP may come from\nthe difference of how well the training data reproduce the query distribution. This means the effec-\ntiveness of the proposed method could be limited under the condition where the query distribution\nis close to uniform and not predictable. Although this is a common issue in almost all of the ANN\nmethods, it remains future work to address such difficult use case.\nOur proposed method has a couple of hyperparameters. Although we show some of their effect\nin Section 4.2, thorough optimization is a future work. It may dependent on data distribution and\nrequired recall value. However, it is not difficult to find the acceptable values for hyperparameters\nthat provide at least better performance than the exhaustive method.\nAnother apparent remaining future work is to apply the proposed method to larger datasets such as\nbillion-scale or trillion-scale ones. However, we believe that foundings and direction we reveal in\nthis paper will be also useful for them."}, {"title": "6 CONCLUSION", "content": "We investigated the requirement to improve the recall and latency tradeoff of large scale approximate\nnearest neighbor search under the condition where the key vectors are stored in storage devices with\nlarge capacity and large read latency. We pointed out that in order to achieve it, we need to reduce the"}, {"title": "3.1 SYSTEM ENVIRONMENT", "content": "In this paper, we assume that the system on which our ANN algorithm runs has GPUs and storage\ndevices in addition to CPUs and memories. The GPU provides high-throughput computing through\nmassively parallel processing. The storage can store larger amount of data at lower cost, but has\nlarger read latency than memory devices. Data that are commonly used for all queries, e.g., all the\nrepresentative vectors of clusters, are loaded in advance on the memories from which CPU or GPU\ncan read data with low latency and is always there during the search. On the other hand, all the\nkey vectors are stored in the storage devices, and for simplicity, we assume that data fetched from\nstorage for computations for a query is not cached on memory to be reused for computations for\nanother query."}, {"title": "mean latency in the systems", "content": "mean latency T in the systems described in the previous subsection is expressed by the following\nequation,\n$T = T_a+T_b+T_c$\nwhere $T_a$ is the latency for computations using data that are always sit in memory, $T_b$ is the latency\nrequired for fetching data from storage, and $T_c$ is the latency for computations using data fetched\nfrom storage for each query. For example, in a partitioning-based ANN method such as SPANN,\n$T_a$ is the latency for the process to determine the clusters (called as the posting lists in SPANN)\nto be fetched from storage, $T_b$ is the latency for fetching the vectors in the chosen clusters from\nstorage, and $T_c$ is the latency for the computations to find the nearest neighbor vectors in the fetched\nvectors. In this paper, for simplicity, assuming that $T_a \\ll T_b$ and $T_c \\ll T_b$, we employ the following\napproximation,\n$T\\approx T_b$\nThen, since $T_b$ is roughly proportional to the number of fetched vectors, the number of fetched\nvectors is an effective metrics to evaluate the mean latency."}, {"title": "the above assumption", "content": "The above assumptions are reasonable in a realistic setting. For $T_c$, the computing performances\nof CPUs equipped with vector arithmetic units and GPUs capable of massively parallel operations\nrange from several hundred GFLOPS to more than TFLOPS. On the other hand, read bandwidth of\nstorage devices is at best a few GB/s even when high-speed NVMe is used. This means the fetched\ndata in $T_b$ can be processed in less than 1/100 of T. Note that if the most of the process for $T_c$ is\nexecuted in parallel with the process for $T_b$, for example, by performing distance calculation in the\nbackground of asynchronous storage access, the effective $T_b$ becomes almost zero. For $T_a$, when\nan exhaustive linear search is used to choose the clusters, i.e., calculating the distance between the\nquery and the representative vectors of all clusters in order to find the closest clusters, a 10-TFLOPS\nGPU can process 10 million representative vectors of 100 dimension each within a much shorter\ntime than $T_b$ = 1 ms. Also in the SPANN case without GPUs, since a fast algorithm that combines\ntree-based method and graph-based method is applied to choose the clusters, $T_a$ is quite short even\nwhen the number of posting lists is as large as a few hundred million."}]}