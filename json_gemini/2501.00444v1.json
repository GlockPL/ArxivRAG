{"title": "Knowledge-aware equation discovery with automated background knowledge extraction", "authors": ["Elizaveta Ivanchik", "Alexander Hvatov"], "abstract": "In differential equation discovery algorithms, a priori expert knowledge is mainly used implicitly to constrain the form of the expected equation, making it impossible for the algorithm to truly discover equations. Instead, most differential equation discovery algorithms try to recover the coefficients for a known structure. In this paper, we describe an algorithm that allows the discovery of unknown equations using automatically or manually extracted background knowledge. Instead of imposing rigid constraints, we modify the structure space so that certain terms are likely to appear within the crossover and mutation operators. In this way, we mimic expertly chosen terms while preserving the possibility of obtaining any equation form. The paper shows that the extraction and use of knowledge allows it to outperform the SINDy algorithm in terms of search stability and robustness. Synthetic examples are given for Burgers, wave, and Korteweg-De Vries equations.", "sections": [{"title": "1. Introduction", "content": "Knowledge extraction [1] and further use is a topic of many publications. Knowledge is extracted and used in the form of knowledge graphs [2] and other forms [3], which are further used to improve the quality of the predic- tions from machine learning [4, 5] and other models [6]. Differential equation discovery also may be considered a knowledge ex- traction tool in the form of a physical model, which in most cases is repre- sented by a differential equation. The discovery of equations has deep roots [7] with a significant breakthrough made by SINDy [8] and PDE-FIND [9]. The group of equation discovery methods allows one to extract interpretable models for physical data. In this pursuit, a variety of techniques have been developed, including SINDy add-ons such as weak [10] and ensemble meth- ods [11], as well as other frameworks such as PDE-Net [12], DLGA-PDE [13], SGA-PDE, PDE-READ, EPDE [14] for ordinary (ODE) and partial differ- ential equations (PDE). In addition, many methods of symbolic regression allow one to restore an expression without differentials [15]. Although we can extract knowledge in the form of an equation, there is a limited possibility to embed the background knowledge into the optimization algorithm to increase the quality of a resulting model. If we move to the algebraic (meaning that no differentials are considered within the expression), there is a lot of effort to embed the background knowledge in various forms. First attempts were made with expertly done grammatics [16, 17, 18]. The former also uses the first derivative as an expression language element for some cases. There is also a method for automated knowledge extraction using the Bayesian approach [19]. One could argue that for the differential equation it might be harder to extract grammatical rules due to the specificity of the model. That is, we cannot extract the function (a solution) directly from the equation without a solver. Moreover, we could not evaluate the importance of each term in the equation, since we also need to solve the reduced equations. Such arguments also apply to the Bayesian approach. The approach to extract the joint distribution of the differential equation terms with a Bayesian network and a differential equation solver is described in [20]. Moving to the differential equation, we are not just adding differentiation operations to the expression. The differential equation determines an implicit function that could only be extracted with a solver. Therefore, we could not compute the function directly at the given grid point. Classical differential equation discovery algorithms use LASSO regression based on gradient opti- mization. The only way to use background knowledge in the regression-based equation discovery algorithm (including neural networks) is to constrain the form of the equation [8]. Such an approach reduces the possibility of extract- ing new knowledge by reducing the problem to recovering the coefficients in a known equation. Evolutionary optimization methods have more parameters to control the discovery process.\nRecent advances in replacing symbolic regression with reinforcement learn- ing for differential equation discovery DISCOVER [21] and [22] could also benefit from reinforcement learning methods with automated symbloc gram- mar extraction [23] or with prior distribution [24]. We note that it also includes an automatically extracted distribution process described in the pa- per.\nPreviously, we introduced a version of the EPDE algorithm [25] that could be guided by the probability of selecting terms of the equation in the mutation and cross-over operators. We manually make the probability distribution in the reference based on the form of the equation.\nThe current paper is devoted to the following questions:\n\u2022 What is the best form of background knowledge representation for dif- ferential equation discovery?\n\u2022 Could we automatically extract background knowledge for differential equation discovery from data?\nAs a positive answer, the paper describes the complete process of back- ground knowledge extraction in the form of a term importance distribution using an initial guess obtained by a simpler algorithm. In terms of noise robustness, overall quality, and possible equation forms, the algorithm out- performs existing methods such as different versions of SINDy (pySINDy [26] is used for comparison). As mentioned above, the disadvantage of all evolu- tionary methods is the speed of optimization. However, the set of possible equations is much larger, which is important for the discovery of unknown equations.\nThe paper is organized as follows. Section 2 contains the statement of the equation discovery problem from a knowledge extraction point of view. Section 3 contains the main contribution of the paper, a knowledge-aware version of the EPDE algorithm, and a method for autonomous knowledge extraction. Section 4 contains an experimental comparison with pySINDy and the classical EPDE algorithm. Section 5 concludes the paper."}, {"title": "2. Equation discovery background", "content": "In what follows, we will discuss only differential equation discovery, and equation discovery means differential equation discovery for brevity.\nIn all cases for the equation discovery problem, it is assumed that the data is placed on a discrete grid $X = {x^{(i)} = (x_1^{(i)},...x_m^{(i)})}_{i=1}^N$, where $N$ is the number of observations and $dim$ is the dimensionality of the problem. We mention a particular case of time series, for which $dim = 1$ and $X = {t_i}_{i=1}^N$.\nIt is also assumed that for each point on the grid, there is an associated set of observations $U = {u^{(i)} = (u_1^{(i)}, ..., u_{L}^{(i)})}$ to define a grid map $u : X \\subset R^{dim} \\rightarrow U \\subset R^{L}$. It is assumed that $u$ is defined by the model $M$ which has the form:\n$M(S, P,x) \\rightarrow u(x) : M(S, P, x^{(i)}) \\rightarrow u(x^{(i)}) \\approx u_L^{(i)}$\nAs discussed before, the differntial equation defines implicit function. We show it in Eq. 1 with $\\rightarrow$ sign. In Eq.1, we define two parts of the model in the form of the equation: the structure $S$ and the parameters $P$. We note that we do not expect either interpolation (case $M(S, P, x^{(i)}) \\rightarrow u(x^{(i)}) = u_L^{(i)}$) or approximation case (case $M(S, P, x^{(i)}) \\rightarrow u(x^{(i)}) \\approx u_L^{(i)}$). It is assumed that the model $M(S, P, x)$ by itself may be interpreted by an expert and used, for example, to predict the behavior of the system in states that have not yet been observed $u_L^{(i)}$. In an ideal scenario, the discovery of differential equations enables the extraction of the complete set of underlying equations based on observational data. Unfortunately, in practical situations, we can only approximate the system and obtain a rough estimate.\nThe relation between grid $X$ and observation $U$ is defined by the problem statement. The structure $S$ is a computational graph of a model; it could be either a classical computational graph or a simple parametrized symbolic string-like expression. It is convenient to separate numerical characteristics such as the coefficient of the term, the power of the term, and the order of the derivative into a set of parameters $P$, i.e., make every node or element in the structure parametrized. The optimization process may be separated for structure $S$, and parameter set $P$.\nIn most cases, the search space of all possible structures $\\Sigma$ cannot be fully explored due to its size, and apart from the extraction of knowledge from the model, we have to use a priori background knowledge (application area, modeling expertise) to reduce the set of possible structures to $\\Sigma' \\subset \\Sigma$ and set of the possible parameters to $P' \\subset P$. As a result, all methods of equation discovery differ not only with the model type (ODE, PDE, other types of expressions) but also with how the background knowledge could be incorporated into the discovery process.\nThere are two limiting cases of differential equation discovery methods that use background knowledge:\n(I) The application of gradient optimization to a fixed number of terms in a fixed structure ($u_t = F(u, u_x, u_{xx}, ...)$). In this case, $\\Sigma'$ is a known fixed structure; it could be a weighted sum, pre-defined terms, or a neural network, as done in PDE-Net or NeuralODE. The parameter set $P'$ is reduced to the weights in a LASSO regression or a neural network. In this group of methods, the optimization is performed only in a parameter space $P'$.\n(II) The application of genetic programming to construct a computation graph using basic operations: differentiation, sum, multiplication, and power. In this case, $\\Sigma'$ is a subset of $\\Sigma$ of all models that could be expressed with a defined basic operation, and the space of parameters is empty, i.e., $P' = {\\emptyset}$.\nIn case (I), new information is generated in form of the coefficients $P'$, which could be the material or geometry parameters. Note that in this case we significantly reduce the ability to get new equations. In most applica- tions, it is a coefficient restoration rather than the equation discovery. As an advantage, the search space is numeric, and thus, the optimization process is faster than in any other type of equation discovery algorithm. To make the search faster and more stable, we add background knowledge to reduce the pre-defined structure $\\Sigma'$ and restrict the parameter values in $P'$ using expert knowledge of the process and presumably material or geometry parameters.\nWe note that adding expert background knowledge to the algorithm further reduces the possibility of discovering principally new equations; for example, other process scales are left out of the scope.\nWe note that an approach such as PDE-READ [27] extracts new infor- mation in form of the compact equation from the obtained within the sparse regression. It is a generalization of sequential reduction in the size of the term library.\nCase (II), on contrary, allows one to get as many equation structures as possible, making the optimization process a combinatorial search that significantly increases the optimization time. In its pure form, such methods are inapplicable to practical tasks. Thus, we must use background knowledge in the form of restrictions to a possible set of structures $\\Sigma'$ and, in some cases, make the nodes parametric to exchange part of the structural set $\\Sigma'$ to the"}, {"title": "2.1. Classical evolutionary equation discovery algorithm", "content": "As a ground, we select the EPDE differential discovery algorithm based on evolutionary optimization. As an optimization ground, it uses rather memetic algorithm. This section briefly describes the chromosome and op- erators used within the optimization.\nModel definition. Evolutionary algorithms use elementary operations to build a model structure. To reduce the amount of structural optimization, EPDE operates with building blocks - tokens - that are parametrized families of functions and operators. The token generally has the form shown in Eq. 2.\n$t = t(\\pi_1, ...\\pi_n)$\nIn Eq. 2 $\\pi_1, ...\\pi_n$ are the token parameters, explained below. In order to distinguish between a single token and a token product (term), we use the notation $T = t_1..... t_{T_{length}}$, where $0 < T_{length} < T_{max}$, and $T_{max}$ is considered an algorithm hyperparameter. However, it is essential to note that while $T_{max}$ affects the model's final form, a reasonable value of tokens in a term (usually 2 or 3) is sufficient to capture most of the actual differential equations.\nTokens $t_i$ are grouped into token families $\\Phi_j$ to aid in fine-tuning the model form. Tokens in each family have fixed parameters set $\\pi_1, ...\\pi_n$. For ex- ample, we could define the differential operators family $\\Phi_{der} = {\\frac{\\partial^{2n+1} u}{\\partial x_i \\partial x_j}}$ to find linear or nonlinear equations with constant coefficients. We could also consider the trigonometric token family $\\Phi_{trig} = {sin(\\pi_1x_1 + ... + \\pi_nx_n), cos(\\pi_1x_1 + ... + \\pi_nx_n)}$ to search for forcing functions or variable coefficients.\nThe parameters in tokens may be optimizable and non-optimizable. For example, it is convenient to fix the differential operator parameters every time they appear; therefore, they are considered one family but different tokens and may appear multiple times in the term to reflect nonlinearity. Trigonometric tokens are optimized and appear (if required) only once per term. The algorithm takes as input the set $\\Phi = \\bigcup \\Phi_j$ of chosen or user- defined token families.\nFor simplicity, we assume the tokens are pre-computed on a discrete grid, but the grid choice does not impact the algorithm's description. Therefore, the structure and parameters are only essential for the differential equation model learning, and the model has the form Eq. 3.\n$M(S, {C, P}) = \\sum_{j=1}^{j<Nterms} C_jT_j$\nIn Eq. 3 with the structure S we denote terms set ${T_j}_{j=1}^{N_{terms}}$ consisting of different tokens, the set of parameters is divided into terms coefficients $C = {C_j}_{j=1}^{N_{terms}}$ with $C_j$ and set of the optimizeable parameters $P = {\\pi_1, ...\\pi_j}$ without fixed length. Every model could have a different set of optimizeable parameters and the evolutionary operators could also change this number.\nThe maximum number of terms $N_{terms}$ is the hyperparameter of the al- gorithm. We note that the hyperparameter $N_{terms}$ also has not a directive but a restrictive function. The number of terms in the resulting model may be lower than $N_{terms}$ and is eventually reduced with the fitness calculation procedure described below.\nWe use the simplified individual to visualize the following evolutionary operator schemes, as illustrated in Fig. 1. Each individual corresponds to an instance of the model shown in Eq. 1.\nThe optimization is separated into two steps: structural and parametric. The population is initialized with the models with a separate structure. After the initialization, the parametric optimization step calculates each individ- ual's fitness.\nFitness evaluation. Fitness evaluation has two purposes. First, it allows de- termining the parameters ${C, P}$ for every model \u2013 individual. Second, it serves as a standard measure of individual fitness."}, {"title": "3. Directed evolutionary search. Usage and extraction of background knowledge.", "content": "In this section, we finalize the algorithm described first in [25]. It is an evolutionary differential equation discovery algorithm based on both EPDE and directed evolutionary serach. The algorithm is briefly outlined in Sec- tion. 3.1.\nThe main actor in the algorithm is the term preference distribution. It is the form in which background knowledge is expressed. Instead of parametric fixed equation form, we set the term preference that is likely to appear in equation, retaining the possibility to obtain all possible equations. Such preference distribution may be chosen expertly and automatically.\nIn Section 3.2 we describe the automated term preference distribution extraction process. It consists of initial guess generation and following dis- tribution forming algorithm.\nThe distribution of \u201cpreferred\u201d tokens obtained automatically or imposed manually is then used to generate new tokens for the model in mutation and cross-over operators.\nThe modified cross-over operator uses two model structures to generate new ones, for every model the distribution of \"preferred\u201d tokens is generated separately. After that, tokens of every model are sorted with respect to complement model preferences using the probabilities as weights. Terms with higher weight have a higher chance of participating in cross-over exchange, as shown in Fig. 4.\nThe mutation operator takes all terms in the model structure as subject to mutation uniformly, but for a replacement token choice (Fig.5 a)), its generated importance distribution is taken into account. Furthermore, for term mutation, a new term is generated using the importance distribution as shown in Fig. 5 b).\nIn summary, the classical and modified algorithms have nearly the same structure as shown in Fig. 6. The most substantial difference being that the modified algorithm has several extra steps for terms importance distribution calculation.\nWe note again that the main changes are done to avoid the rigid re- strictions on a structure optimization space \u03a3'. Instead, the applied expert"}, {"title": "3.1. Modified evolutionary operators", "content": "function, a term is randomly chosen as a \"target\" for a given model with the structure S. Namely, the model-individ before fitness computation trans- ferred to the form Eq. 4.\n$T_{target} = \\sum_{j=1,...target-1}^{}C_jT_j = \\sum_{j=target+1,..., N_{terms}}^{}C_jT_j $\nIn Eq. 4 target is a randomly chosen index. The randomly chosen target allows one to avoid a trivial solution $\\forall_j C_j = 0$. It is assumed that for the fitness computation, the terms $T_j$ are fixed and the coefficients $C = {C_1, ...C_{target}, ...C_{N_{terms}}}$ and optimizable parameters $P = {P_1, ...P_{target}, ...P_{N_{terms}}}$ (if any) should be determined. We note that $C_{target} = -1$ and the values in the set of term parameters $P_{target}$ are always fixed.\nThe term coefficients $C_{opt}$ and the parameter sets $P_{opt}$ are found using LASSO regression as shown in Eq. 5.\n$C_{opt}, P_{opt} = arg min \\| T_{target} - \\sum_{j=1,...target-1}^{}C_jT_j + \\lambda(\\parallel C \\parallel_1 + \\parallel P \\parallel_1) \\|_2$\nIn Eq. 5, with $\\parallel . \\parallel_p$ corresponding $l_p$ norm is designated. After apply- ing the LASSO regression operator, the coefficients are compared with the minimal coefficient value threshold of the term. If the absolute value of co- efficient $C_j$ is lower than the threshold, then the term is removed from the current model. Thus, the model is refined to reduce the excessive growth of unnecessary terms.\nAfter finding the final set of optimal coefficients for Eq. 5 is found, the fitness function $F$ is calculated as shown in Eq. 6.\n$F = \\frac{1}{\\|M(S, {P_{opt}, C_{opt}})|x\\|_2}$\nIn Eq. 6 denominator is basically an average discrepancy over a compu- tation grid X.\nEvolutionary operators. Population initialization, cross-over, and mutation operators use a set of expert rules for generation and exchange. The rules"}, {"title": "3.2. Probability distribution generation", "content": "We used a modified SymNet architecture as an educated initial guess of the structure of the equation. We describe the modifications in Appendix D. We note that the initial guess may be obtained by various means, the described approach works with an arbitrary initial guess in form of the dif- ferential equation. There are several discussion points on this choice. The first is if the bad initial guess makes the end result worse. Second, is there are any other choices available.\nThere are overall two incomes of bad choice \u2013 initial guess that contains wrong parts and initial guess that contains significant number of the possible terms within the guessing algorithm. As part of the answer to the first question, in the following, we show that SymNet does not always give a good guess (see Appendix F). However, the quality of the search (robustness and number of iterations) was still improved. Second, if the guessing algorithm is not able to find any structure, the answer tends to be the overfitted equation that contains excessive number of terms. In this case, the uniform term distribution will be extracted, and thus the algorithm is not affected, i.e. works in a classical uni-direction mode.\nSymnet was chosen since it is relatively fast and gives the initial guess and could generate a structure space \u03a3' larger than that of SINDy and other regression methods, without an explicit definition of the terms. However, any equation discovery method could be taken instead, the described method in general does not depend on a guessing algorithm choice. We note that the quality of the initial guess undoubtedly affects the end result. However, as we show below, we could achieve quality increase when only part of the equation is guessed correctly.\nThe first step is done within the initial population generation stage. Firstly, all possible terms with a given EDPE hyper-parameters are gen- erated. Secondly, if the term space of the initial guess does not coincide with the EPDE model term space, the mapping of the initial guess to the model term space is performed.\nDue to the rules that allow one to avoid equalities of type 0 = 0, every model has its own set of restricted terms. Therefore, the following steps are conducted separately within each individual's mutation and cross-over phases. The steps are illustrated in Fig. 7. First, depending on the structure of an individual undergoing a mutation process, the term space is adjusted so that the structural elements of an individual are excluded from the space. In"}, {"title": "4. Experimental results", "content": "The goal of the experiments is to determine whether the knowledge guid- ance mechanism in the form of the initial guess of term distribution can lead to better optimization in terms of stability, accuracy, and speed. The ex- periments are conducted for three classes of equations: Burgers', wave, and Korteweg-de Vries. The data for experiments was obtained either analyti- cally or numerically, the initial-boundary problem statements and solution methods are placed in Appendix E.\nIf possible, the results are compared with that of the PySINDY [26] frame- work (ver. 1.7.5); if not, only two algorithms are considered: classical and modified.\nFor every experiment, we run the classical and modified algorithms fifty times. We use the PySINDY package on the same data.\nThe performance of the algorithms is measured with different noise levels in the data. Equation 7 describes adding noise of a certain magnitude to the data. Magnitudes differ in scale depending on the input data. For this reason, every type of equation has its limit magnitude, under which the classical algorithm cannot discover the desired equation in any of fifty runs. Namely, as stated above, the observational data have the form $U = {u^{(i)} = (u_1^{(i)}, ..., u_{L}^{(i)})}_{i=1}^N$. The noise levels are relative to the limit case and therefore are equal to 0%, 25%, 50%, 75% and 100% of the limit noise level.\n$u_{noised}^{(i)} = u^{(i)} + \\epsilon, \\epsilon \\sim N(0, magnitude \\cdot |u^{(i)}|)$\n$\\epsilon \\in \\epsilon_{t=1}^E x \\in X ... x \\in E$\nThe quality metrics used to measure algorithm performance are mean absolute error (MAE) between coefficients of the obtained equations and 'ground truth' coefficients of the theoretical model and algorithms' conver- gence time. If several solutions are available, the structure of the equations is first checked. If the structure is correct, then MAE is computed. Among the computed MAEs, the minimum is selected as the final metric of the run.\nInitially, with the double precision, there were around 100 different MAE values; after grouping them with the precision of 3-5 digits past the decimal point, we are left with approximately 3-25 clusters, each represented with some shade of green on the plotted figures. Clusters are placed in ascending order on the vertical axis by the MAE value around which they are clustered.\nIn addition, hyper-parameters for all experiments are presented in sup- plementary material in Tab. C.9 and C.10."}, {"title": "4.1. Experimental setup", "content": "4.2. Initial guess: SymNet performance\nIn the proposed approach the SymNet architecture is used in order to provide an educated guess to EPDE. Although one might be tempted to use SymNet directly to discover the equations, in reality, some difficulties might be faced while doing so. This subsection is dedicated to these challenges and explains the reasoning behind SymNet' role in overall approach. The detailed equation and analysis placed in Appendix F. Integral results are places in Tab. 1 for MAE (coefficient error).\nIn Tab. 2 for structural Hamming distance (SHD) results are shown, i.e. how many terms we should add or remove to reach ground truth equations. Overall, the SymNet algorithm provided adequate results in cases of sim- ple equations, but was inaccurate for more complex cases due to low coeffi- cient error, but high SHD.\nThe experiment with wave equation deserves particular attention. We note that if the balancing term is represented by the second space derivative,"}, {"title": "4.3. Parameter sensitivity: mixing factor", "content": "In order to capture the dependency of the modified algorithm efficiency on the changes in term importance distribution, experiments with different mixing factors are performed. The results of these experiments are com- pared with the results of the algorithm with optimal distribution. Optimal distribution referrers to a distribution that is the most close to the ideal one, where the importance of the desired terms is 3.0 to 3.5 times higher than the other terms. Optimal distribution $Q(mf)$ is obtained by SymNet and then processed with a mixing factor, acquired by Eq. 8.\n$mf_{opt} = arg min |D_{KL}(P||Q(mf))|$\nFrom the MAE results depicted in the following sections we can conclude that depending on the value of a mixing factor, the modified algorithm may yield different results. Therefore, the purpose of this section is to study this dependency.\nThe modified algorithm was run with several mixing factors - 3.0, 3.6, 4.5 and 2.4 by default. The mixing factors defined by Kullback-Leibler generally slightly differ by the noise level in the data. In Appendix G detailed tables for mixing coefficient sensitivity analysis are provided. With the data on considered types of equations we can conclude, that the tuning of mixing factor may provide extra benefits, specifically, the mixing factors derived from proximity of ideal and found distributions, are mainly the most optimal."}, {"title": "4.4. SINDy comparison", "content": "We ran a classical algorithm for experiments and modified it with the setup described in Sec. 3. For these experiments PySINDy framework was used as well.\nIn Fig. 10 - inviscid Burgers equation case, it can be seen that the bars of the modified algorithm tend to have more runs with darker green colors. We can conclude that the modified algorithm performs better than the classical one regarding precision. Considering the number of runs with gray outcomes, we can conclude that the modified algorithm is comparatively more stable.\nWe note that although PySINDy framework was able to obtain a correct equation, the accuracy of the algorithm was relatively low considering the complexity of the Burger's equation."}, {"title": "4.5. Additional tests", "content": "Wave equation. Due to the hyperbolic nature of wave equations, it is rarely used to test equation discovery frameworks. However, the proposed approach can find time derivatives of any order. We note that this type of equation cannot be obtained with PySINDy because the algorithm restricts the equa- tion's form: $u_t = F(x, u_x, u_{xx}, ...)$.\nNevertheless, the running time (Fig. H.19) of the modified algorithm is significantly higher, apparently due to the reason that the SymNet module is run several times to compare losses of equations with $u_t$ and $u_{tt}$ terms, that are used to balance the right side of the equation.\nThe MAE distributions are shown on Fig. 13. Given the outcomes of the runs, it becomes clear that both classical and modified algorithms have simi- lar coefficient error levels, although the modified algorithms might be slightly more stable. We note that the modified algorithm with tuned mixing factor\nInhomogeneous Korteweg \u2013 de Vries equation. The proposed approach was also tested on a more complex case in the equation discovery area the inhomogenous Korteweg-de Vries equation.\nIn this experiment, two algorithms were compared: classical and modified. The results in the form of plots are shown in Fig. 14. Unlike in the experiment with the homogeneous Korteweg \u2013 de Vries equation, the modified algorithm was more precise, stable, and considerably faster than the classical one.\nThe benefits of tuning the mixing factor can also be seen on Fig. 14. For the Korteweg \u2013 de Vries equation, the tuning procedure yielded roughly 30% increase in discovery rate averaged on all noise levels compared to the modified algorithm with a mixing factor of 2.4. This fact encourages us to further develop this idea, as currently it only serves the purpose of showing the potential of the modified algorithm."}, {"title": "4.6. Algorithms time consumption", "content": "During all experiments execution time was recorded. Detailed reports may be found in Appendix H.\nTo sum up - the modified algorithm is generally slower than the classical one (reference time 5 s for classical EPDE and 15 s for modified algorithm). We note that the PySINDY algorithm's average running time is around 0.01 s, which outperforms both the classical and the modified algorithms. However, the equation structure's restrictions may outweigh its benefits."}, {"title": "5. Discussion and conclusion", "content": "The paper proposes a new way to introduce a priori applied area knowl- edge into the equation discovery algorithm without strict structure restric- tions. It is done by introducing the geometry change in the model structure space by changing the probability measure used in the classical cross-over and mutation operators of the EPDE algorithm. We improve the algorithm's ability to converge towards a given equation more frequently. The distribu- tion could be extracted in an automated manner using the proposed SymNet architecture training.\nThe main paper findings are:\n\u2022 Evolutionary algorithms could be used to incorporate knowledge more softly;\n\u2022 Knowledge incorporation in the form of a term preference distribution leads to a more stable convergence;\n\u2022 Knowledge in the form of distribution could be extracted manually;\n\u2022 Evolutionary algorithms work better for high noise magnitudes.\nCompared to the classical gradient-based sparse regression approach, our proposed evolutionary approach offers more flexibility and produces restored equation quality of at least the same level. In addition, it can restore an equation that gradient algorithms might have found incorrectly in complex cases. However, one downside of the evolutionary approach is that the opti- mization time is typically longer. As the main advantage of gradient-based algorithms, one may name optimization speed.\nDespite this, we observe that domain knowledge incorporation may in- crease the success rate of the algorithm by more than 40% (Tab. A.4) in the case of an unbiased solution in the task of complex equation discovery (in- homogeneous Korteweg-de Vries equation); on the other hand, in the task of simple equations discovery there was no room for improvement, as the success rate reached the score of 1.0 in the classical algorithm. The noise introduction shows that the difference is more apparent for complex cases of differential equations. The proposed algorithm allows one to restore known equations with higher noise levels, increasing robustness on the average of 12.5%, ranging from 2% up to 32%.\nThe proposed methodology may be further improved by tuning of mixing factor. In this case the increase in discovery rate (Tab. A.3) in complex differential equation types may be enlarged to as far as 70% for noise-free data and by 21.3% on average, ranging from 2% to 58%, when the noise is added to the data.\nThe accuracy of all algorithms is presented in Tabs. B.5, B.6, B.7, \u0392.8. As it may be noted the evolutionary approaches are generally more accurate than PySINDy framework. Whereas the accuracy of evolutionary algorithms is on par with one another with tuned modified algorithm having a slight advantage."}, {"title": "6. Code and data availability", "content": "The code and data to reproduce the experimental results are openly avail- able in the repository https://github.com/ITMO-NSS-team/EPDE_automated_ knowledge"}, {"title": "Appendix A. Success rates of equation discovery", "content": "The increase in success rate is calculated as follows: first, an average (on 50 runs) equation discovery rate is computed separately for each evolution- ary algorithm and noise levels, after which the efficiency of the proposed methodology can be evaluated as the difference in success rate between the modified and classical algorithms."}, {"title": "Appendix B. MAE relative rates", "content": "For each type of equation a maximum MAE of all algorithms - PySINDy framework, modified and classical algorithms, is obtained. Then a mean value of each algorithm' MAE is normalized by the maximum value. The"}, {"title": "Appendix C. Algorithms hyper-parameters", "content": "We note that both EPDE algorithms - classic and modified, share the same hyper-parameters in regards to the same type of input data and irre- spective of the noise levels."}, {"title": "Appendix D. SymNet modifications for initial guess generation", "content": "A knowledge extraction approach is inspired by PDE-Net 2.01 [32, 12]. In particular, it is also an equation discovery algorithm used to obtain the equations with a fixed structure, as shown in Eq. D.1. The authors proposed defining the structure \u03a3' of the equation as a symbolic neural network, which in [12] is called SymNet.\n$U_t = F(U,\\nabla U, \\nabla^2 U, ...), x \\in \\Omega \\subset R^d, t \\in [0, T]$.\nHowever, to retain possible structures \u03a3' variety of the evolutionary al- gorithm, the initial SymNet architecture requires significant changes. After that, the initial guess of the equation is converted into the term importance, which is, in turn, used to guide structural optimization.\nAs was mentioned, Eq. D.1 imposes certain restrictions onto the form of the equation. Therefore, the original algorithm was modified in such a way that the response function F is dependent on temporal and spatial grids, as well as the time derivative, which does not act as a balance term on the left side of the equation (Eq. D.2).\n$\\begin{cases}U_t = F(t, x,U,U_x, U_{xx}, U_{tt}, U_{ttt}, ...),&\\\\U_{tt} = F(t,x,U,U_x, U_t, U_{xx}, U_{ttt}, ...),&\\\\U_{ttt} = F(t, x,U,U_x, U_t, U_{xx}, U_{tt}, ...),&\\\\...&\\end{cases}$"}, {"title": "Appendix E. Initial-boundary value problem statements", "content": "Appendix E.1. Wave equation\nThe initial-boundary value problem for the wave equation is given in Eq. E.1.\n$\\begin{cases}\\frac{\\partial^2 u}{\\partial t^2} - \\frac{1 \\partial^2 u}{25 \\partial x^2} = 0&\\\\u(0,t) = u(1, t) = 0&\\\\u(x, 0) = 10^4 sin^2\\pi x(x - 1)&\\\\u'(x, 0) = 10^3 sin^2\\pi x(x - 1)&\\\\(x, t) \\in [0"}]}