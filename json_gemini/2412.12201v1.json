{"title": "Embracing Large Language Models in Traffic Flow Forecasting", "authors": ["Yusheng Zhao", "Xiao Luo", "Haomin Wen", "Zhiping Xiao", "Wei Ju", "Ming Zhang"], "abstract": "Traffic flow forecasting aims to predict future traffic flows based on the historical traffic conditions and the road network. It is an important problem in intelligent transportation systems, with a plethora of methods been proposed. Existing efforts mainly focus on capturing and utilizing spatio-temporal dependencies to predict future traffic flows. Though promising, they fall short in adapting to test-time environmental changes of traffic conditions. To tackle this challenge, we propose to introduce large language models (LLMs) to help traffic flow forecasting and design a novel method named Large Language Model Enhanced Traffic Flow Predictor (LEAF). LEAF adopts two branches, capturing different spatio-temporal relations using graph and hypergraph structures respectively. The two branches are first pre-trained individually, and during test-time, they yield different predictions. Based on these predictions, a large language model is used to select the most likely result. Then, a ranking loss is applied as the learning objective to enhance the prediction ability of the two branches. Extensive experiments on several datasets demonstrate the effectiveness of the proposed LEAF.", "sections": [{"title": "Introduction", "content": "Traffic flow forecasting is an integral part of intelligent transportation systems (Dimitrakopoulos and Demestichas, 2010; Zhang et al., 2011) and smart cities (Shahid et al., 2021; Dai et al., 2022). The goal of traffic flow forecasting is to predict future traffic flows using the historical data and the spatial information (i.e. the road network), which has a wide range of applications including traffic signal control (Jiang et al., 2021), route planning (Liebig et al., 2017), and congestion management (Fouladgar et al., 2017).\nDue to its value in real-world applications, great efforts have been made to resolve the problem of traffic flow forecasting (Smith and Demetsky, 1997; Sun et al., 2006; Guo et al., 2019; Li and Zhu, 2021). Early works mainly model the traffic systems using physical rules or shallow models (Ghosh et al., 2009; Tchrakian et al., 2011; Hong et al., 2011; Li et al., 2012). With the advent of deep learning, the main-stream of traffic flow forecasting methods utilizes graph neural networks (Kipf and Welling, 2016; Hamilton et al., 2017; Xu et al., 2018; Veli\u010dkovi\u0107 et al., 2018), recurrent neural networks (Hochreiter, 1997; Chung et al., 2014), and transformers (Vaswani, 2017; Jiang et al., 2023) to capture the rich spatio-temporal relations (Yu et al., 2018; Li et al., 2018; Guo et al., 2019; Li and Zhu, 2021; Zhang et al., 2021; Chen et al., 2022a; Liu et al., 2023; Ma et al., 2024).\nDespite their success, existing traffic flow forecasting methods have two limitations, which hinder their applications in the real world. (1) Unable to adapt to environmental changes of traffic conditions during test time. Most existing methods make the assumption that test data follow the same (or a very similar) distribution as the training data, which may fail to hold true in real-world scenarios (Kim et al., 2024; Chen et al., 2024c), especially for time series data (Lu et al., 2022; Gagnon-Audet et al., 2022; Jian et al., 2024). In the real world,\ntraffic condition changes over time due to a variety of factors like special events, the change of weather, or the shift of eras. While it is difficult for existing methods to adequately adapt to all these changes, the assistance of large language models (LLMs) can make a difference, as LLMs have the ability to understand these changes (Chang et al., 2024; Minaee et al., 2024; Beniwal et al., 2024). A naive solution is to utilize the generative ability of LLMs to make direct predictions (Li et al., 2024b; Ren et al., 2024; Liang et al., 2024), as shown in Figure 1. However, directly generating future traffic flows could be too challenging for language models, as accurate forecasting relies on both historical data and complex spatio-temporal relations. (2) Weak in capturing the rich structure of spatio-temporal relations in traffic data. The traffic network is complicated and the temporal dimension adds another layer of complexity. A large number of prior works focus on capturing the complex spatio-temporal relations, using graph structures (Song et al., 2020; Zheng et al., 2023a) or hypergraph structures (Wang et al., 2022; Wang and Zhu, 2022; Zhao et al., 2023). Graphs capture pair-wise relations, while hypergraphs model non-pair-wise relations. Adopting only one of them is not enough, as the spatio-temporal relations in traffic data is rich by nature. For example, traffic congestion at one vertex affects adjacent vertices (pair-wise relations), whereas road closures affect a large set of vertices (non-pair-wise relations). Modeling the rich structure of spatio-temporal relations is a challenging aspect of predicting future traffic flows.\nTowards this end, we propose a novel method termed Large Language Model Enhanced Traffic Flow Predictor (LEAF) for adaptive and structure-perspective traffic flow forecasting. LEAF consists of a predictor and a selector, where the predictor generates options of predictions and the selector chooses the most likely result. To enhance adaptability, we build an LLM-based selector that selects from a range of possible future traffic flows using the discriminative ability of a large language model, as shown in Figure 1. The selection results are used to guide the predictor with a ranking loss. The large language model is good at understanding the changing traffic conditions and is open to further information provided by humans, making it an adaptable predictor. To better capture the rich structures of spatio-temporal relations, we build a dual-branch predictor composed of a graph branch which captures pair-wise relations of spatio-temporal traffic data, and a hypergraph branch, which captures non-pair-wise relations. During test time, the dual-branch traffic flow predictor generates different forecasting results, and subsequently, a set of transformations is applied to obtain a wealth of choices of future traffic flows.\nOur contribution is summarized as follows:\n\u2022 We propose an LLM-enhanced traffic flow forecasting framework that introduces large language models in test time to enhance the adaptability of traffic flow forecasting models.\n\u2022 We propose a dual-branch predictor that captures both pair-wise and non-pair-wise relations of spatio-temporal traffic data, and an LLM-based selector that chooses from possible prediction results generated by the predictor. The selection results further guide the adaptation of the predictor with a ranking loss.\n\u2022 Extensive experiments on several benchmark datasets verify the effectiveness of the proposed method."}, {"title": "Related Works", "content": "Traffic Flow Forecasting\nTraffic flow forecasting is a topic that has been studied for several decades (Smith and Demetsky, 1997; Sun et al., 2006; Yang et al., 2016; Song et al., 2020; Li et al., 2024b). Early efforts mainly focus traditional models (Smith and Demetsky, 1997; Asadi et al., 2012). With the success of deep learning, deep neural networks have become the mainstream in this field. One line of research adopt recurrent neural networks (RNNs) (Hochreiter, 1997) and graph neural networks (GNNs) (Kipf and Welling, 2016), where the GNNs and RNNs capture the spatial and temporal relations respectively (Li et al., 2018; Wang et al., 2020; Chen et al., 2022b).\nTo jointly model spatial and temporal relations, another line of research utilize GNNs in both dimensions (Song et al., 2020; Li and Zhu, 2021; Lan et al., 2022; Chen et al., 2024a). As simple graphs only capture pair-wise relations, some works make a step further, introducing hypergraph neural networks (HGNNs) to capture non-pair-wise spatio-temporal relations (Luo et al., 2022; Wang and Zhu, 2022; Zhao et al., 2023; Wang et al., 2024). In this work, we take the benefits from both sides, with a dual-branch predictor, capturing rich structures of spatio-temporal relations.\nLarge Language Models\nIn recent years, large language models has drawn increased attention, both within the community of natural language processing (Chen et al., 2024b; Yin et al., 2024) and beyond. Efforts have been made to utilize the power of LLMs in other fields, including healthcare (Li\u00e9vin et al., 2024; Van Veen et al., 2024; Labrak et al., 2024), education (Milano et al., 2023), legal technology (Lai et al., 2024), economics (Li et al., 2024a), recommendation (Chen et al., 2024b; Bao et al., 2024), and transportation (Liu et al., 2024; Guo et al., 2024b; Ren et al., 2024).\nAmong these applications, traffic flow forecasting is an important one, being the foundation of smart cities (Boukerche et al., 2020). The success of LLMs in language has inspired a plethora of works in this task. Some works adopt the architecture of LLMs for building transformer-based traffic flow predictors (Cai et al., 2020; Xu et al., 2020; Chen et al., 2022a; Liu et al., 2023; Jiang et al., 2023; Zou et al., 2024). However, they typically require a large amount of data for training.\nAnother line of research attempts to equip LLMS with the ability to predict future traffic flows based on the history and specific situations (Zheng et al., 2023b; Guo et al., 2024a; Li et al., 2024b; Yuan et al., 2024; Han et al., 2024). Although LLMs have shown promising results in understanding time series (Yu et al., 2023; Koval et al., 2024; Gruver et al., 2024) or temporal events (Xia et al., 2024; Hu et al., 2024), the traffic data involves complex spatio-temporal relations challenging LLMs' generative ability. In this work, we show that one can instead utilize their discriminative ability to enhance existing traffic flow forecasting models."}, {"title": "Methodology", "content": "Problem Setup and Overview\nProblem Setup. We follow the standard setup for traffic flow forecasting (Song et al., 2020), where there is a road network denoted as $G = (V, E, A)$. $V$ is the set of $N$ vertices (i.e. the sensors in the city), $E$ is the set of edges, and $A \\in R^{N\\times N}$ is the adjacency matrix. In this road network, historical traffic flows can be represented as $X = (X_1, X_2,\\dots, X_T) \\in R^{T\\times N \\times F}$, where $T$ is the length of historical observation and $F$ is the dimension of input features. The goal is to predict the future of traffic flows with the length of $T'$, denoted as $X' = (X'_1, X'_2, \\dots, X'_{T'}) \\in R^{T'\\times N \\times F}$.\nFramework Overview. To solve the aforementioned problem with the help of large language models, we propose a novel framework termed Large Language Model Enhanced Traffic Flow Predictor (LEAF). The overview of the framework"}, {"title": "Dual-branch Traffic Flow Predictor", "content": "Previous works on traffic flow forecasting adopt the graph perspective (Song et al., 2020; Zheng et al., 2023a) or the hypergraph perspective (Wang et al., 2022; Zhao et al., 2023). The graph perspective propagates messages between pairs of nodes, which makes them adept in modeling pair-wise spatio-temporal relations (e.g. an accident affects adjacent locations). On the other hand, the hypergraph perspective propagates messages among groups of nodes, making them proficient in modeling non-pair-wise spatio-temporal relations (e.g. people move from the residential area to the business area). For LEAF, we aim to take the benefits from both sides by constructing a dual-branch predictor and letting the LLM to select.\nSpatio-temporal Graph Construction. To utilize graph neural networks and hypergraph neural networks, we first construct a spatio-temporal graph corresponding to the input tensor $X \\in R^{T\\times N \\times F}$. Particularly, the length of historical data $T$ yields a set of $TN$ spatio-temporal nodes, denoted as:\n$V^{ST} = \\{v^i_t | i = 1, . . ., N, t = 1, . . ., T\\}$, (1)\nand we add temporal edges in addition to spatial edges $E$ to obtain the edge set $E^{ST}$:\n$E^{ST} = \\{\\{v^{i}_{t_1}, v^{j}_{t_2}\\} | (|t_1 - t_2| = 1 \\land i = j)\n\\lor (t_1 = t_2 \\land (i, j) \\in E)\\}$. (2)\nThe spatio-temporal graph can then be represented as $G^{ST} = (V^{ST}, E^{ST}, A^{ST})$, where $A^{ST} \\in R^{TN\\times TN}$. The spatio-temporal features $X^{(0)} \\in$\n$R^{TN\\times d}$ is derived from $X \\in R^{T\\times N \\times F}$ with a linear mapping and a reshape operation, where $d$ is the dimension of the hidden space.\nThe Graph Branch. Based on the constructed spatio-temporal graph, we first adopt a graph neural network to model pair-wise spatio-temporal relations. Concretely, given the spatio-temporal feature inputs $X^{(0)} \\in R^{TN\\times d}$, we adopt convolution layers to process the features, which is\n$X^{(l)} = \\sigma (\\tilde{A}_{ST}X^{(l-1)}W^{(l)}_g )$, (3)\nwhere $\\sigma(\\cdot)$ is the activation layer and $W^{(l)}_g \\in R^{d\\times d}$ is the weight matrix. $\\tilde{A}_{ST} = D^{-1/2}A_{ST}D^{-1/2}$ is the normalized version of the adjacency matrix, where $D$ is the degree matrix (Kipf and Welling, 2016; Song et al., 2020). By adopting graph convolutions in Eq. 3, the information from one spatio-temporal vertex can be propagated to its neighbors as defined in $E^{ST}$ in Eq. 2, and thus this branch models pair-wise spatio-temporal relations.\nThe Hypergraph Branch. Although the graph branch is adept in capturing pair-wise relations, the complex traffic patterns contain non-pair-wise relations. For example, in the morning rush hours, people move from the residential area (which is a set of vertices in a hyperedge) to the business area (which is another hyperedge). The vertices in one hyperedge share common patterns and the hyperedges affect each other. To model non-pair-wise relations, hypergraphs are adopted. For a hypergraph, its incidence matrix $I_H \\in R^{NT\\times m}$ describes the assignment of $NT$ vertices to $m$ hyperedges. As the incidence matrix is not given as input, we resort to a learnable one with low-rank decomposition (Zhao et al., 2023):\n$I_H = softmax(X^{(l-1)}_H W_H)$, (4)\nwhere $X^{(l-1)}_H \\in R^{NT\\times d}$ is the hidden features, and $W_H \\in R^{d \\times m}$ is the weight matrix. $softmax(\\cdot)$ is applied for normalization. Subsequently, the output features can be computed as:\n$X^{(l)}_H = I_H (I_H^T X^{(l-1)}_H W_E) + (W_E (W_E^T I_H^T X^{(l-1)}_H))$, (5)\nwhere $W_E \\in R^{m\\times m}$ models the interactions of the hyperedges. In this way, the hypergraph branch considers both (a) the interactions within a set of vertices (within a hyperedge) through the first term of Eq. 5, and (b) the interactions among groups of vertices (among the hyperedges) through the second term of Eq. 5."}, {"title": "Large Language Model Based Selector", "content": "In the Section 3.2, we obtain different prediction results from different branches, denoted as $Y_G$ and $Y_H$. Most previous works (Li et al., 2024b; Ren et al., 2024; Liang et al., 2024) directly generate these predictions, which is challenging since the complex spatio-temporal relations are hard to express in texts. By comparison, the LLM-based selector aims to choose the best prediction, using the internal knowledge of a frozen LLM and the constructed prompts. As the traffic networks are usually large, we break the result $Y \\in \\{Y_G, Y_H \\} \\subset R^{T'\\times N}$ for individual vertices, i.e. $Y = [Y_1, Y_2, ..., Y_N]$, where $y_i \\in R^{T'}$.\nChoice Set Construction. In practice, we want to give the LLM-based selector more choices so that it has the potential to make better predictions. Therefore, we introduce several transformations: smoothing, upward trend, downward trend, overestimate, and underestimate. The set of transformations is denoted as $T$. For vertex $i \\in V$, the choice set is determined as follows:\n$C_i = \\{T(Y_i)|T \\in T, Y_i \\in \\{y^G_i, y^H_i\\}\\} \\cup \\{y^G_i, y^H_i\\}$. (6)\nBy adopting transformations, the choice set is expanded and the selector has the potential to deal with more complex situations. For instance, if it believes that both of the branches underestimate the traffic flow on a Monday morning, the selector can choose an option with an upward trend.\nPrompt Construction. When constructing the prompt, we consider the following aspects. (1)"}, {"title": "Large Language Model Enhanced Prediction", "content": "The LEAF framework consists of a predictor and a selector. The predictor (the graph branch and the hypergraph branch) is pre-trained on the training set. During test time, the predictor first predicts, and the selector then selects. The selection results are used to supervise the predictor, and thus the two modules benefit from each other, achieving large language model enhanced prediction. Concretely, given the input data $X \\in R^{T\\times N \\times F}$, the predictor generates two forecasts, i.e. $Y_G$ and $Y_H$. Subsequently, the selector constructs choice sets and use the LLM to find the best option for each individual vertex. The selection results is denoted as $\\hat{y}_i$, $i \\in V$, which are then used to supervise the predictor. Conceivably, $\\hat{y}_i$ may not be the optimal choice in the choice set, and therefore, we adopt a ranking loss described as follows:\n$\\mathcal{L}_G = [\\Delta(\\hat{y}_i, y^G_i) - \\underset{Y\\in C_i\\{\\hat{Y}\\} }{inf} \\Delta(\\hat{y}_i, Y) + \\epsilon]_+$, (7)\nwhere $[\\,]_+$ is the hinge function, $\\Delta(\\cdot,\\cdot)$ is a distance measure, and $\\epsilon$ is the margin. Similarly, we can define the loss function $\\mathcal{L}_H$ using $y^H_i$. The final objective is written as:\n$\\mathcal{L} = \\mathcal{L}_G + \\mathcal{L}_H$ (8)\nIn Eq. 7 and Eq. 8, we encourage the forecasts of the predictor (i.e. $y^G_i$ and $y^H_i$) to be closer to the selected prediction (i.e. $\\hat{y}_i$) than the closest one in suboptimal predictions (i.e. $C_i \\setminus \\{\\hat{y}_i\\}$). Since the ground truth may not be covered by the choice set, this objective is better than directly minimizing the distance between the predictions and the selected forecast, as it allows the model to learn from a better choice compared to suboptimal choices.\nBy supervising the predictor, the two modules (i.e. the predictor and the selector) benefit from"}, {"title": "Experiments", "content": "Experimental Setup\nDatasets. We adopt three widely used datasets in traffic flow forecasting, including PEMS03, PEMS04, and PEMS08. The datasets are publicly available and collected by California Transportation Agencies (CalTrans) Performance Measurement Systems (PEMS). 1. The traffic data come from sensors on the road of various places in California, and they are counted every five minutes.\nEvaluation Metrics. We follow standard setting (Li et al., 2018) that use one-hour historical data (i.e. $T = 12$ timesteps) to forecast one-hour future (i.e. $T' = 12$ timesteps). For evaluation the prediction results, we use three standard metrics: Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE) (Song et al., 2020).\nBaseline Methods. We compare LEAF with a variety of baseline methods, including DCRNN (Li et al., 2018), ASTGCN (Guo et al., 2019), STS-"}, {"title": "Main Results", "content": "The performance of LEAF in comparison with baselines are shown in Table 1. According to the results, we have several observations:\nFirstly, the proposed LEAF achieves a consistent lead in all three datasets, which demonstrates the effectiveness of the proposed framework that adopts a dual-branch traffic flow predictor and a LLM-based selector. Prior methods often fail to provide satisfactory predictions when there is test-time distribution changes, as they cannot learn adaptively, leaving the room for improvement.\nSecondly, the method that utilizes the generative ability of LLMs (i.e. LLM-MPE) does not perform well on all datasets. As we can see on PEMS03 and PEMS04 datasets, its predictions are generally worse or similar compared to simple methods using graph neural networks. Since LLMs are not very adept in capturing complex spatio-temporal"}, {"title": "Ablation Study", "content": "We also perform ablation studies on the PEMS08 dataset, and the results are shown in Table 2. Specifically, we perform the following experiments. E1 measures the performance of graph branch only without the LLM-based selector. E2 measures the performance of the hypergraph branch without the selector. The vanilla version (E1 and E2) of both branches performs much worse than LEAF. E3 uses the graph branch in conjunction with the selector, which leads to performance degradation compared to LEAF. This suggests that non-pair-wise relations are important in traffic flow forecasting. E4 only uses the hypergraph branch together with the"}, {"title": "Hyper-parameter Analysis", "content": "We perform experiments with respect to two hyperparameters: (a) $M$, i.e. the number of iterations when training with the ranking loss in Eq.7 and Eq.8, and (b) the number of $K$, i.e. the number of prediction-selection iterations in Algorithm 1. The results on the PEMS08 dataset are shown in Figure 4. As can be seen from the figure, when $M$ increases, both MAE and RMSE decreases, and then plateaus after around 5. This shows that the predictor converges and therefore, we set $M$ to 5. For another hyper-parameter $K$, the optimal performance is achieved when $K$ is set to 2, and with more iter-"}, {"title": "Visualization", "content": "Visualization of the Selection. We first provide visualizations of the historical traffic flow data, the choices in $C_i$, and the option selected by the LLM-based selector (marked with a green tick). The results on the PEMS03 dataset is shown in Figure 5. Moreover, we also provide the analysis of the LLM-based selector, which is shown in Figure 6. According to the results, we can see that the traffic flow is generally going downward, since it is the end of the rush hours. In Figure 5, we can see that the LLM-based selector chooses the option with the lowest traffic flows. From its analysis in Figure 6, we can see that it understands that the time period to forecast is around the end of the rush hours, which is the reason why it selects the lowest option. This suggests that the LLM-based selector is able to understand changing traffic conditions.\nVisualization of Errors in Different Timesteps. We then provide visualization of the mean absolute error (MAE) under different forecasting timesteps in Figure 7. The experiments are performed on the PEMS03 dataset, where we compare the MAE values of our framework compared to its two branches (i.e. the graph branch and the hypergraph branch). The results show that LEAF reduces forecasting errors generally. Although the errors are similar in the first few timesteps, LEAF quickly diverges from the two branches, resulting in significantly lower errors in the long term. This suggests that with the help of the discriminative ability of the LLM-based selector, our method can select predictions that are more accurate in the long run."}, {"title": "Conclusion", "content": "In this paper, we propose a novel framework named Large Language Model Enhanced Traffic Flow Predictor (LEAF), which consists of a dual-branch traffic flow predictor and an LLM-based selector. The predictor adopts two branches: the graph branch and the hypergraph branch, capturing the pair-wise and non-pair-wise spatio-temporal relations respectively. The selector uses the discriminative ability of the LLM to choose the best forecast of predictor. The selection results are then used to supervise the predictor with a ranking loss. We perform extensive experiments to demonstrate the effectiveness of the proposed LEAF framework."}, {"title": "Limitations", "content": "One limitation of this work is that we only focus on the traffic flow forecasting domain, due to the scope of this paper and the limited computational resources. In the future, we plan to extend this framework to more generalized spatio-temporal forecasting problem. Besides, we do not evaluate our methods on other traffic flow forecasting datasets due to limited resources. For future works, we plan to evaluate our framework using other datasets."}]}