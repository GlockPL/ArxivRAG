{"title": "M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language Models\nfor Chest X-ray Interpretation", "authors": ["Jonggwon Park", "Soobum Kim", "Byungmu Yoon", "Jihun Hyun", "Kyoyun Choi"], "abstract": "The rapid evolution of artificial intelligence, especially in\nlarge language models (LLMs), has significantly impacted\nvarious domains, including healthcare. In chest X-ray (CXR)\nanalysis, previous studies have employed LLMs, but with\nlimitations: either underutilizing the multi-tasking capabili-\nties of LLMs or lacking clinical accuracy. This paper presents\nM4CXR, a multi-modal LLM designed to enhance CXR in-\nterpretation. The model is trained on a visual instruction-\nfollowing dataset that integrates various task-specific datasets\nin a conversational format. As a result, the model supports\nmultiple tasks such as medical report generation (MRG),\nvisual grounding, and visual question answering (VQA).\nM4CXR achieves state-of-the-art clinical accuracy in MRG\nby employing a chain-of-thought prompting strategy, in\nwhich it identifies findings in CXR images and subsequently\ngenerates corresponding reports. The model is adaptable to\nvarious MRG scenarios depending on the available inputs,\nsuch as single-image, multi-image, and multi-study contexts.\nIn addition to MRG, M4CXR performs visual grounding at\na level comparable to specialized models and also demon-\nstrates outstanding performance in VQA. Both quantitative\nand qualitative assessments reveal M4CXR's versatility in\nMRG, visual grounding, and VQA, while consistently main-\ntaining clinical accuracy.", "sections": [{"title": "Introduction", "content": "Recent advancements in artificial intelligence, particularly\nlarge language models (LLMs), have led to their widespread\napplication across various fields, including healthcare. Nu-\nmerous studies are exploring diverse methods of applica-\ntion for improving healthcare outcomes, such as personal-\nized treatment plans, clinical decision support systems, and\nenhancing medical education (Zheng et al. 2024).\nWithin the medical domain, this paper focuses on chest\nX-ray (CXR) interpretation. Although many studies have in-\nvestigated LLMs for CXR, most encounter one of two is-\nsues: they either underutilize the capabilities of LLMs or\nstruggle with ensuring clinical accuracy. Despite LLMs'\nability to perform various tasks through conversational inter-\naction, many studies focus on a single task, typically medical\nreport generation (MRG) (Zhou et al. 2024; Hyland et al.\n2024; Chaves et al. 2024). While some studies effectively\nleverage LLMs to comprehend questions and generate ap-\npropriate responses (Thawkar et al. 2023; Pellegrini et al.\n2023; Lee et al. 2024; Chen et al. 2024), the clinical accu-\nracy of these conversational outcomes, including generated\nreports, falls short of expectations.\nIn this work, we propose M4CXR, an LLM for CXR\nadept at handling four \u201cMulti\u201d aspects: Multi-modal, Multi-\ntask, Multi-image input, and Multi-turn chatting. We train\nthe model on a dataset constructed by integrating various\ntask-specific datasets, enabling M4CXR to excel in MRG,\nimage understanding, and visual question answering (VQA)\ntasks, as depicted in Figure 1. In MRG, M4CXR improves\nclinical accuracy by leveraging LLMs' reasoning abilities\nthrough a chain-of-thought (CoT) (Wei et al. 2022) rea-\nsoning, where the model identifies findings in the image\nand generates reports based on these results. Additionally,\nwe conduct experiments across various scenarios with in-\nputs extending beyond a single image, including CXR im-\nages from different views and prior patient studies. Be-\nsides MRG, M4CXR demonstrates strong multi-tasking ca-\npability with successful application in visual grounding and\nVQA, showcasing its adaptability in diverse clinical con-\ntexts.\nIn summary, our main contributions are as follows:\n\u2022 We propose M4CXR, an MLLM designed for CXR in-\nterpretation, capable of handling multiple tasks. To en-\nable multi-tasking, we assemble a visual instruction-\nfollowing dataset from diverse CXR tasks.\n\u2022 By adopting a novel CoT reasoning process, M4CXR\nachieves state-of-the-art clinical accuracy in CXR report\ngeneration.\n\u2022 M4CXR effectively utilizes multiple images and reports,\nallowing for its applicability across different scenarios.\n\u2022 Beyond MRG, M4CXR demonstrates remarkable perfor-\nmance in visual grounding and VQA."}, {"title": "Related Works", "content": "Multi-modal Large Language Models\nAs research on LLMs progresses, their advantages continue\nto emerge. It is widely known that LLMs are capable of\nmulti-task learning (Raffel et al. 2020), and Wei et al. (2022)\nshowed that LLMs can reason through multi-turn chats using\nCoT prompting. The rise of multi-modal LLMs, which uti-\nlize the long context length of LLMs to process visual con-\ntent, further amplifies these advantages. MLLMs can handle\ntasks that require comprehension of spatial regions, and vi-\nsual instruction tuning (Liu et al. 2024) allows them to an-\nswer free-form questions about images.\nFor MLLMs, bridging the embedding spaces of vision and\nlanguage is crucial. A common approach is to freeze the pre-\ntrained vision model and LLM, and train only the bridging\nmodule (Alayrac et al. 2022; Zhu et al. 2023), also known as\nthe projector. The projector's structure varies, from a linear\nlayer to Q-Former (Li et al. 2023a), C-Abstractor (Cha et al.\n2024), and more. Drawing insights from these works, we\nemploy a two-phase training strategy: pre-training the pro-\njector first, and then fine-tuning the entire model with visual\ninstruction tuning.\nChest X-ray Report Generation\nCXR report generation via deep learning has been exten-\nsively studied over time (Shin et al. 2016; Jing, Xie, and\nXing 2018; Zhang et al. 2020; Tanida et al. 2023). Since\nthe advent of LLMs, training strategies such as freezing pre-\ntrained models (Thawkar et al. 2023) or visual instruction\ntuning (Lee et al. 2024) have been equivalently implemented\nin the CXR domain. However, a significant drawback of\nthese models is the lack of clinical accuracy in the gener-\nated reports. Efforts to address these shortcomings include\nadopting a separate disease classification module (Jin et al.\n2024; Zhou et al. 2024), increasing the visual encoder's res-\nolution (Chaves et al. 2024), or harnessing additional input\n(Bannur et al. 2024). In this work, we take a novel yet simple\napproach: leveraging the reasoning abilities of LLMs.\nMulti-Tasking in Chest X-ray Interpretation\nConfining the use of LLMs solely to report generation un-\nderutilizes their potential. MLLMs that capitalize on the\ncharacteristics of LLMs to enable conversations based on\nCXR images include XrayGPT (Thawkar et al. 2023), Ra-\nDialog (Pellegrini et al. 2023), and LLM-CXR (Lee et al.\n2024). CheXagent (Chen et al. 2024) is a multi-tasking CXR\nfoundation model trained on various tasks, similar to our ap-\nproach. Yet, the clinical accuracy of these models' responses\nremains suboptimal.\nMedical foundation models such as Med-Gemini (Yang\net al. 2024) and MedPaLM-M (Tu et al. 2024) support\nCXR interpretation with conversational capabilities, show-\ning satisfactory clinical accuracy. However, these models\nare not trained on spatial cognition tasks like visual ground-\ning, which involves identifying regions in an image corre-\nsponding to a phrase in the text. For CXR, Boecking et al.\n(2022a) released the benchmark dataset MS-CXR for phrase\ngrounding, and MedRPG (Chen et al. 2023b) specializes in\nidentifying bounding boxes for phrases in CXR images. The\nl latest concurrent work by Bannur et al. (2024) introduced\nMAIRA-2, an MRG model for CXR capable of ground-\ning but limited to grounded report generation. In contrast,\nM4CXR handles multiple tasks such as MRG, VQA, and\nvisual grounding, while maintaining clinical accuracy."}, {"title": "M4CXR", "content": "Methods\nArchitecture. Figure 2 (a) illustrates the overall archi-\ntecture of M4CXR. Following LLaVA (Liu et al. 2024),\nM4CXR includes a vision encoder, a projector, and an LLM,\ndenoted as MV, MP, and ML, respectively. Given n CXR\nimages x1,..., In, the vision encoder converts each image\nxi into visual feature f, which the projector then transforms\ninto a sequence of visual token embeddings X:\n$X = M_P(f) = M_P(M_V(x_i))$, for i = 1, ..., n\n(1)\nA text prompt is also mapped into the embedding space, re-\nsulting in a sequence of language token embeddings Xl. The\nLLM takes an input in which visual embeddings are inter-\nleaved with language embeddings to generate output Y:\nY = M_L(X_1^v, \u2026, X_n^v, X^l)\n(2)\nThe output of the MLLM Y = {yt}T\nt=1, consisting of T\nlanguage tokens, is generated auto-regressively:\np(YX_1^v, ..., X_n^v, X^l) =\n$\\prod_{t=1}^{T} p(y_t|X_1^v, ..., X_n^v, X^l, y_{<t})$.\n(3)\nBounding Box Representation. To enable visual ground-\ning, MLLMs need to represent spatial information as text\ntokens. Referring to You et al. (2024), we use bounding box\ncoordinates enclosed in square brackets without any addi-\ntional special tokens. The coordinates [x1, y1, x2, y2] repre-\nsent the top-left (x1, y1) and bottom-right (x2, y2) points of\nthe bounding box on the image. As input images are prepro-\ncessed to a uniform size by the vision encoder, the bounding\nbox coordinates are also normalized (Chen et al. 2023a) to\ninteger values between 0 and 100.\nMulti-turn Chain-of-Thought Prompting\nWe derive insights from PromptMRG (Jin et al. 2024) to en-\nhance the clinical accuracy of MRG. It classifies observable\nlesions from a CXR image and then uses the result as input\nprompts to generate reports. We follow a similar two-step\nprocess, first identifying key observations and then generat-\ning reports. The difference from PromptMRG is that, in our\nwork, a single MLLM performs both classification and re-\nport generation sequentially, enabling end-to-end learning.\nThe gist is to divide the input prompt into two questions,\ncreating multi-turn conversational data. In the first question,\nwe present a list of potential observations and prompt the\nmodel to identify those visible in the CXR image. Then, we\nask the model to generate a report based on the prior conver-\nsation. This approach resembles the reasoning process of a\nhuman radiologist, who first identifies lesions visible in the\nimage and then generates a diagnostic report. It can be seen\nas a variant of the commonly used CoT (Wei et al. 2022)\nprompting, hence we call it CoT MRG. Figure 3 shows the\nmulti-turn CoT prompting process. To validate the effective-\nness of CoT MRG, we conduct a comparative experiment\nwith classification and MRG as separate tasks.\nIntegrating Chest X-ray Interpretation Tasks\nWe focus on the following features of an MLLM: the ca-\npability for reasoning through multi-turn chat, the ability\nto utilize multiple images due to its long context length,\nand the flexibility for multi-task learning. To exploit these\nfeatures, we construct a CXR visual instruction-following\ndataset, as schematized in Figure 2 (b). We define a task\nset across three task types: MRG, image understanding, and\nVQA, and transform the corresponding datasets into con-\nversational data. Detailed task descriptions and conversation\ntemplates are provided in the Appendix (Table 8, 9).\nReport Generation: Various Scenarios. A single radio-\nlogic study can include multiple CXR images taken from\ndifferent views: posterior-anterior (PA), anterior-posterior\n(AP), or lateral. Additionally, when the same patient under-\ngoes follow-up studies, information from prior studies can\nalso be utilized in the radiologic interpretation.\nIn contrast to previous studies that generate reports from\na single image (Jin et al. 2024; Chaves et al. 2024; Hyland\net al. 2024; Tanida et al. 2023), we consider MRG tasks in\nthree different scenarios based on the available inputs. The\nsingle-image scenario incorporates one image from a single\nstudy. The multi-image scenario accesses multiple images\nfrom different views within a single study. The multi-study\nscenario accepts images and the corresponding report from\nprior study as inputs, along with follow-up study images.\nThese scenarios have different conversation templates. In\nthis work, since we distinguish tasks by templates, they are\ntreated as separate tasks. Through these different scenario\ntasks, we expect the model to understand and fully utilize\nthe given information, enabling it to generate more accurate\nreports when additional inputs are available.\nImage Understanding. Disease classification is one of the\ncore tasks in CXR image understanding. We use class label\ndata from various classification datasets. Since each dataset\nhas its own set of disease labels, we specify the relevant la-\nbels for each question in the prompt. The model is prompted\nto identify findings in the image from a list of diseases.\nOur task set also includes fine-grained tasks requiring de-\ntailed analysis of specific CXR regions. These tasks involve\ngrounding and identification of findings, phrases, organs,\nand anatomical structures, as well as abnormality detection.\nWe convert bounding box labels and clinical text reports into\ninstruction-following data to help the model link CXR im-\nages with spatial information for detailed analysis.\nVisual Question Answering. To train M4CXR, we con-\nvert all the data into an instruction-following format using\npredefined templates. However, training the LLM solely on\ntemplates presents a risk of overfitting: the model might lose\nits inherent conversational abilities and respond only in a\nfixed format. Therefore, we also capitalize on VQA datasets\nwhich are already in a conversational format, expecting the\nmodel to freely respond to a diverse range of questions."}, {"title": "Experiments", "content": "Training Datasets\nWe collect and integrate various datasets according to the\ntasks described in the Methods section.\nReport Generation. MIMIC-CXR (Johnson et al. 2019)\nencompasses a diverse collection of CXR images along with\ndetailed radiology reports. These reports are extensively an-\nnotated, facilitating advanced medical image analysis. For\nMRG tasks, our focus is on generating the FINDINGS sec-\ntion, which offers an in-depth description of significant ob-\nservations identified in the CXR images.\nWe employ CheXbert (Smit et al. 2020) to extract obser-\nvation labels from the FINDINGS section. CheXbert out-\nputs 4-class (positive, negative, uncertain, blank) classifica-\ntion results for 14 predefined observation labels. For binary\nclassification, all non-positive classes are treated as nega-\ntive. Its label set of observations is provided as the candidate\nfindings in the first question of CoT prompting.\nFor the three different MRG scenario tasks, image-report\npairs are organized as follows. For single-image, every im-\nage is used as a data instance. In multi-image, images from\ndifferent views that share the same study ID are gathered to\ncompose study-level data. Multi-study combines two con-\nsecutive studies of a patient, with the studies arranged in\nchronological order.\nImage Understanding. BRAX (Reis et al. 2022) and\nCheXpert (Irvin et al. 2019) datasets are used for disease\nclassification. We incorporate datasets that contain bounding\nboxes along with class labels (VinDr-CXR (Nguyen et al.\n2022), ChestX-ray14 (Wang et al. 2017), ChestX-Det10\n(Liu, Lian, and Yu 2020), JSRT (Shiraishi et al. 2000), SIIM\n(Zawacki et al. 2019), RSNA (Shih et al. 2019), COVID-19\nRadiography (Chowdhury et al. 2020), COVID-QU-Ex\n(Rahman et al. 2021; Tahir et al. 2021; Chowdhury et al.\n2020), QaTa-COV19 (Degerli et al. 2021)) for various fine-\ngrained image understanding tasks in addition to disease\nclassification. MS-CXR (Boecking et al. 2022b) is a dataset\nthat includes image-sentence pairs of bounding boxes and\ncorresponding phrases for images and reports from MIMIC-\nCXR. ImaGenome (Wu et al. 2021), which is also derived\nfrom MIMIC-CXR, annotates text in the report with bound-\ning boxes aligned with 29 anatomical regions.\nVisual Question Answering. MIMIC-CXR-VQA (Bae\net al. 2024) and MIMIC-Diff-VQA (Hu et al. 2023) datasets\nare designed to address a wide range of questions based\non MIMIC-CXR images. These datasets encompass diverse\nand comprehensive question-answer pairs, enabling models\nto effectively handle various CXR-related queries. RaDia-\nlog (Pellegrini et al. 2023) is a visual instruction-following\ndataset designed to facilitate tasks that require structured di-\nalogues. It guides models in understanding and responding\nto instructions based on CXR images.\nTest Datasets\nThe test set used for evaluating all MRG tasks is from\nMIMIC-CXR. The official test split includes 2,461 images in\nthe frontal (PA, AP) view and 1,397 in other views, totaling\n3,858 images. Each study corresponding to the 2,461 frontal\nimages is considered a separate data instance in multi-image\nand multi-study, with the prior study included for the latter\nif available. As a result, the test set size for multi-image and\nmulti-study is 2,461, while single-image considers two test\nset sizes: 2,461 and 3,858.\nFor phrase grounding evaluation, the test set of MS-CXR\nis used. Each query phrase in MS-CXR corresponds to a\nsingle bounding box. To ensure a rigorous evaluation of the\ntask, we exclude all MS-CXR images from the training set\nof other datasets across all tasks. Following the data split of\nChen et al. (2023b), the test set has 167 images.\nWe employ the test set of MIMIC-CXR-VQA for evaluat-\ning medical VQA. Of the 13,793 total samples, we exclude\n2,484 samples that lack answers, resulting in 11,309 samples\nfor assessment. Additionally, we incorporate SLAKE (Liu\net al. 2021), which is not included in the training data. From\nits test set of 2,094 samples, we use only CXR images with\nclose-ended questions in English, retaining 114 samples.\nEvaluation Metrics\nReport Generation. The generated reports are assessed\nusing natural language generation (NLG) and clinical met-"}, {"title": "Results", "content": "rics. We use the traditional BLEU (Papineni et al. 2002) and\nROUGE-L (Lin 2004) for NLG metrics.\nRegarding clinical accuracy, we calculate F1 scores from\nCheXbert classification results. Macro-averaged F1 (MF1)\nand micro-averaged F1 (mF1) as well as example-based av-\nerage F1 (eF1) scores are computed. F1-14 indicates the Fl\nscore calculated over all 14 CheXbert labels, while F1-5 is\ncomputed for only 5 of those labels (cardiomegaly, edema,\nconsolidation, atelectasis, pleural effusion).\nRadGraph-F1 (Yu et al. 2023) is another clinical accuracy\nmetric. RadGraph (Jain et al. 2021) extracts a graph com-\nposed of clinical entities and relations from a radiology re-\nport. RadGraph-F1 is the average of F1 scores calculated for\nboth the entities and relations of the graph.\nMedical Phrase Grounding. To evaluate the performance\nof phrase grounding, we calculate the intersection over\nunion (IoU) between the ground-truth and predicted bound-\ning boxes. We use the mean IoU (mIoU) averaged over all\ndata as the evaluation metric. Additionally, accuracy is de-\ntermined by considering predicted boxes with an IoU of 0.5\nor higher as correct predictions.\nVisual Question Answering. Accuracy, recall, and\nBLEU-1 are used to assess the VQA performance. Exact\nmatches between predictions and ground-truth are counted\nto calculate the accuracy. To consider partial matches for\nopen-ended responses, recall is calculated by measuring the\nproportion of ground-truth words present in the generated\nsequences. For MIMIC-CXR-VQA, which includes open-\nended questions, BLEU-1 scores are also calculated for\neach test sample and then averaged.\nModel Training\nWe use a randomly initialized C-Abstractor as the projec-\ntor to efficiently handle multiple radiology images and re-\nduce image tokens. The LLM and vision encoder are pre-\ntrained models: Mistral-7B-Instruct-v0.2 (Jiang et al. 2023)\nand Rad-DINO (P\u00e9rez-Garc\u00eda et al. 2024), respectively. The\ntraining involves two stages. First, we pre-train the projec-\ntor while keeping the LLM and vision encoder frozen, using\nonly CXR images and reports without instruction prompts.\nSubsequently, the vision encoder, projector, and LLM are\ntrained together for visual instruction tuning. We apply\nLORA (Hu et al. 2021) for LLM training to reduce compu-\ntational cost. Each instruction-following data input includes\nimages and instruction texts, and the model is trained to pre-\ndict the corresponding responses using cross-entropy loss.\nThe sampling ratio of datasets for each task significantly\nimpacts M4CXR's performance. The empirical ratio for\nmulti-tasking capabilities was determined to be approxi-\nmately 54%, 35%, and 11% for MRG, image understanding,\nand VQA, respectively. The detailed training hyperparame-\nters (Table 6), specific sampling ratios (Table 10), and the\nexploration of sampling ratios are provided in the Appendix.\nMedical Report Generation\nSingle-Image. To evaluate single-image MRG perfor-\nmance, we compared our model with state-of-the-art MRG\nmodels, including LLM-CXR, RaDialog, METransformer\n(Wang et al. 2023), DCL (Li et al. 2023b), PromptMRG,\nLM-RRG (Zhou et al. 2024), CheXagent, MAIRA-1 (Hy-\nland et al. 2024), Med-PaLM M, and LLaVA-Rad (Chaves\net al. 2024). The results are summarized in Table 1. For di-\nrect comparison with other models, we evaluated M4CXR\nusing two distinct test sets: one with 2,461 frontal images\nand another with 3,858 images of all views. Including lateral\nviews decreased clinical accuracy, suggesting the complex-\nity of recognizing observations from lateral images.\nAmong the models evaluated on only frontal images,\nM4CXR attained the highest CheXbert clinical accuracy,\nwith mF1-14 and MF1-14 scores of 60.6 and 40.0, re-\nspectively. In the evaluation that included all views, our\nmodel outperformed PromptMRG with an eF1-14 of 50.2.\nAlthough Med-PaLM M used a different all-view test set,\nwhich limits direct comparison, our model showed a signifi-\ncant advantage in mF1, with similar or slightly lower MF1s.\nHowever, M4CXR did not achieve the best scores in both\nRadGraph-F1 and NLG metrics. To improve clinical accu-\nracy, CoT prompting provides candidate findings in the first\nquestion before generating reports. This approach may have\nled the model to use terms from the findings list rather than\nwords that exactly match those in the ground-truth report,\nwhich could be a contributing factor to the observed lower\nNLG scores. RadGraph-F1, calculated by extracting entities\nand relations from the report, was likely reduced due to this\nmismatch.\nVarious Scenarios. Table 2 shows the evaluation results\nacross various input scenarios. Multi-image improved clin-\nical accuracy compared to single-image, with CheXbert\nmF1-14 and MF1-14 scores of 61.1 and 41.0, respectively.\nProviding the prior study as additional input improved MF1,\nthough mF1 slightly decreased. These results demonstrate\nthat our model can effectively utilize available inputs to gen-\nerate medical reports in various scenarios.\nAlthough test conditions differ, we compared our model's\nperformance with two baseline models. (Tu et al. 2024) re-\nported the zero-shot generalization results of Med-PaLM M\nin a two-view setting. Since it was trained only on single-\nimage, its effectiveness diminished, unlike M4CXR leverag-\ning additional images for enhanced outcomes. MAIRA-2 ac-\ncepts a prior study and multiple images as input, akin to our\nmulti-study scenario. Its performance matches ours closely:\nMAIRA-2 achieved a higher MF1-14 of 42.7, whereas\nM4CXR surpassed it with an mF1-14 of 60.7.\nMedical Phrase Grounding\nTable 3 compares the medical phrase grounding results of\nTransVG (Deng et al. 2022), MedRPG, MAIRA-2, and\nM4CXR, on the MS-CXR test set. While MedRPG achieved\nthe highest accuracy and mIoU, MedRPG and TransVG are\nspecialized for phrase grounding and cannot perform other\ntasks. Given this limitation, M4CXR, a multi-tasking model,\nshows competitive performance with an accuracy of 68.3\nand mIoU of 57.9. MAIRA-2, utilizing a private dataset for\ngrounded report generation, reported a similar mIoU to ours.\nVisual Question Answering\nTable 4 presents the results of medical VQA evaluation.\nWe conducted our own evaluations on open-source medi-\ncal MLLMs: LLaVA version of RaDialog (Pellegrini et al.\n2024), RadFM (Wu et al. 2023), and CheXagent. M4CXR\noutperformed the other models, except for the recall on\nMIMIC-CXR-VQA. CheXagent included the dataset in its\ntraining, likely explaining its high recall of 72.8. The lower\naccuracy on MIMIC-CXR-VQA, compared to SLAKE, is\ndue to its greater variety of questions. Moreover, the higher\nBLEU-1 score compared to accuracy suggests the presence\nof partially correct answers.\nAblation Study\nWe examined the effects of multi-turn CoT prompting and\nthe combination of task types through comparative exper-\niments (Table 5). Generating reports without CoT prompt-\ning in M4CXR led to a significant drop in clinical accuracy.\nExp1, trained on single-turn MRG without the first ques-\ntion of CoT prompts, yielded similar results. Exp2 involved\ntraining on single-turn MRG and disease classification as\nseparate tasks. Testing Exp2 with CoT prompts improved\nclinical accuracy, but NLG metrics fell significantly, with\nthe generated text being merely a list of identified observa-\ntions. The experiments reveal the effectiveness of multi-turn\nCoT prompting in MRG's training and testing phases.\nExp3 and Exp4 excluded image understanding and VQA\ntasks, respectively. In Exp3, while MRG performance was\non par and VQA performance improved due to the increased\nproportion of VQA data, the absence of image understand-\ning data precluded visual grounding. In Exp4, the BLEU-\n1 score for VQA dropped significantly from 66.4 to 36.1,\nsuggesting the necessity of VQA datasets in understanding\nand answering free-form questions. Moreover, the slight de-\ncrease in Exp4's clinical accuracy compared to M4CXR im-\nplies the contribution of VQA datasets to MRG performance\nimprovement. Detailed examples are in the Appendix.\nQualitative Analysis\nFigure 4 shows two examples of dialogs between a user and\nM4CXR, each for visual grounding and VQA. Additional"}, {"title": "Conclusion", "content": "examples can be found in the Appendix. In Figure 4 (a), af-\nter the multi-turn CoT prompting to generate a report, the\nuser asks a third question to locate the region of a sentence\nin the generated report. The model provides the coordinates\nof a bounding box, depicted as a red dashed box. The predic-\ntion covers all of the ground-truth, represented by the yellow\nsolid box, possessing sufficient explanatory power to indi-\ncate what the corresponding sentence represents.\nFigure 4 (b) illustrates M4CXR's ability to write a clear\nand concise summary from the generated report. On the left,\nthe ground-truth report is provided for comparison. For en-\nhanced visual clarity, findings and corresponding sentences\nare color-matched. M4CXR accurately identifies all the ob-\nservations in the first question, and subsequently generates\na comprehensive report without omitting any detail. In re-\nsponse to the third question which asks for a one-sentence\nsummary, the model offers a concise statement that includes\nevery noted finding. A notable flaw is that expressions such\nas \"increasing\u201d and \u201cworsening,\u201d which would be meaning-\nful only in a multi-study scenario, were generated despite\nthe single-image context. While it is understandable that this\nhallucination is due to the ground-truth reports containing\nsuch expressions during training, it is evident that there is\nroom for improvement.\nIn this study, we introduced M4CXR, a multi-modal LLM\naimed at enhancing CXR interpretation by leveraging the\nversatile advantages of LLMs. Trained on a CXR visual\ninstruction-following dataset constructed by appropriately\ncombining various datasets, M4CXR is capable of perform-\ning multiple tasks. Exploiting its reasoning capability, we\nproposed a novel chain-of-thought prompting strategy that\nsignificantly improved the clinical accuracy of report gener-\nation. By identifying observations from X-ray images and\ngenerating descriptions through multi-turn conversations,\nM4CXR achieved notable improvements in CheXbert F1\nscores compared to existing models. The model's ability to\nhandle multiple images and incorporate prior studies demon-\nstrated its adaptability in diverse clinical scenarios.\nBeyond report generation, M4CXR also proved to be\nhighly effective in visual grounding and VQA. M4CXR\ncompeted well with models specialized for visual ground-\ning, while also exhibiting outstanding performance in VQA."}, {"title": "Appendix", "content": "Implementation Details\nThe detailed hyperparameters for training are summarized\nin Table 6. C-Abstractor is configured with a depth of 3", "fol-\nlows": "rank is set to 8", "VQA": "to generate more natural responses, multinomial\nsampling is applied.\nDetails on Chest X-ray Interpretation Tasks\nDetailed descriptions for all the CXR interpretation tasks\nconsidered in this work are listed in Table 8. Table 9 pro-\nvides the conversation templates for each task. Although not\nshown in the table, a system prompt, which specifies that\nthe AI medical assistant should give helpful and detailed an-\nswers, is inserted before the first question in each template.\nTable 10 lists the datasets used for each task, along with the\nnumber of training instances and the sampling ratios applied\nduring training.\nDataset Preprocessing\nWe follow the official split for every dataset. The additional\npreprocessing steps, taken for each dataset as required, are\ndescribed in detail.\nMIMIC-CXR. We extract the"}]}