{"title": "M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language Models for Chest X-ray Interpretation", "authors": ["Jonggwon Park", "Soobum Kim", "Byungmu Yoon", "Jihun Hyun", "Kyoyun Choi"], "abstract": "The rapid evolution of artificial intelligence, especially in large language models (LLMs), has significantly impacted various domains, including healthcare. In chest X-ray (CXR) analysis, previous studies have employed LLMs, but with limitations: either underutilizing the multi-tasking capabilities of LLMs or lacking clinical accuracy. This paper presents M4CXR, a multi-modal LLM designed to enhance CXR interpretation. The model is trained on a visual instruction-following dataset that integrates various task-specific datasets in a conversational format. As a result, the model supports multiple tasks such as medical report generation (MRG), visual grounding, and visual question answering (VQA). M4CXR achieves state-of-the-art clinical accuracy in MRG by employing a chain-of-thought prompting strategy, in which it identifies findings in CXR images and subsequently generates corresponding reports. The model is adaptable to various MRG scenarios depending on the available inputs, such as single-image, multi-image, and multi-study contexts. In addition to MRG, M4CXR performs visual grounding at a level comparable to specialized models and also demonstrates outstanding performance in VQA. Both quantitative and qualitative assessments reveal M4CXR's versatility in MRG, visual grounding, and VQA, while consistently maintaining clinical accuracy.", "sections": [{"title": "Introduction", "content": "Recent advancements in artificial intelligence, particularly large language models (LLMs), have led to their widespread application across various fields, including healthcare. Numerous studies are exploring diverse methods of application for improving healthcare outcomes, such as personalized treatment plans, clinical decision support systems, and enhancing medical education (Zheng et al. 2024).\nWithin the medical domain, this paper focuses on chest X-ray (CXR) interpretation. Although many studies have investigated LLMs for CXR, most encounter one of two issues: they either underutilize the capabilities of LLMs or struggle with ensuring clinical accuracy. Despite LLMs' ability to perform various tasks through conversational interaction, many studies focus on a single task, typically medical report generation (MRG) (Zhou et al. 2024; Hyland et al. 2024; Chaves et al. 2024). While some studies effectively leverage LLMs to comprehend questions and generate appropriate responses (Thawkar et al. 2023; Pellegrini et al. 2023; Lee et al. 2024; Chen et al. 2024), the clinical accuracy of these conversational outcomes, including generated reports, falls short of expectations.\nIn this work, we propose M4CXR, an LLM for CXR adept at handling four \u201cMulti\u201d aspects: Multi-modal, Multi-task, Multi-image input, and Multi-turn chatting. We train the model on a dataset constructed by integrating various task-specific datasets, enabling M4CXR to excel in MRG, image understanding, and visual question answering (VQA) tasks, as depicted in Figure 1. In MRG, M4CXR improves clinical accuracy by leveraging LLMs' reasoning abilities through a chain-of-thought (CoT) (Wei et al. 2022) reasoning, where the model identifies findings in the image and generates reports based on these results. Additionally, we conduct experiments across various scenarios with inputs extending beyond a single image, including CXR images from different views and prior patient studies. Besides MRG, M4CXR demonstrates strong multi-tasking capability with successful application in visual grounding and VQA, showcasing its adaptability in diverse clinical contexts.\nIn summary, our main contributions are as follows:\n\u2022 We propose M4CXR, an MLLM designed for CXR interpretation, capable of handling multiple tasks. To enable multi-tasking, we assemble a visual instruction-following dataset from diverse CXR tasks.\n\u2022 By adopting a novel CoT reasoning process, M4CXR achieves state-of-the-art clinical accuracy in CXR report generation.\n\u2022 M4CXR effectively utilizes multiple images and reports, allowing for its applicability across different scenarios.\n\u2022 Beyond MRG, M4CXR demonstrates remarkable performance in visual grounding and VQA."}, {"title": "Related Works", "content": "Multi-modal Large Language Models\nAs research on LLMs progresses, their advantages continue to emerge. It is widely known that LLMs are capable of multi-task learning (Raffel et al. 2020), and Wei et al. (2022) showed that LLMs can reason through multi-turn chats using CoT prompting. The rise of multi-modal LLMs, which utilize the long context length of LLMs to process visual content, further amplifies these advantages. MLLMs can handle tasks that require comprehension of spatial regions, and visual instruction tuning (Liu et al. 2024) allows them to answer free-form questions about images.\nFor MLLMs, bridging the embedding spaces of vision and language is crucial. A common approach is to freeze the pre-trained vision model and LLM, and train only the bridging module (Alayrac et al. 2022; Zhu et al. 2023), also known as the projector. The projector's structure varies, from a linear layer to Q-Former (Li et al. 2023a), C-Abstractor (Cha et al. 2024), and more. Drawing insights from these works, we employ a two-phase training strategy: pre-training the projector first, and then fine-tuning the entire model with visual instruction tuning.\nChest X-ray Report Generation\nCXR report generation via deep learning has been extensively studied over time (Shin et al. 2016; Jing, Xie, and Xing 2018; Zhang et al. 2020; Tanida et al. 2023). Since the advent of LLMs, training strategies such as freezing pre-trained models (Thawkar et al. 2023) or visual instruction tuning (Lee et al. 2024) have been equivalently implemented in the CXR domain. However, a significant drawback of these models is the lack of clinical accuracy in the generated reports. Efforts to address these shortcomings include adopting a separate disease classification module (Jin et al. 2024; Zhou et al. 2024), increasing the visual encoder's resolution (Chaves et al. 2024), or harnessing additional input (Bannur et al. 2024). In this work, we take a novel yet simple approach: leveraging the reasoning abilities of LLMs.\nMulti-Tasking in Chest X-ray Interpretation\nConfining the use of LLMs solely to report generation underutilizes their potential. MLLMs that capitalize on the characteristics of LLMs to enable conversations based on CXR images include XrayGPT (Thawkar et al. 2023), Ra-Dialog (Pellegrini et al. 2023), and LLM-CXR (Lee et al. 2024). CheXagent (Chen et al. 2024) is a multi-tasking CXR foundation model trained on various tasks, similar to our approach. Yet, the clinical accuracy of these models' responses remains suboptimal.\nMedical foundation models such as Med-Gemini (Yang et al. 2024) and MedPaLM-M (Tu et al. 2024) support CXR interpretation with conversational capabilities, showing satisfactory clinical accuracy. However, these models are not trained on spatial cognition tasks like visual grounding, which involves identifying regions in an image corresponding to a phrase in the text. For CXR, Boecking et al. (2022a) released the benchmark dataset MS-CXR for phrase grounding, and MedRPG (Chen et al. 2023b) specializes in identifying bounding boxes for phrases in CXR images. The latest concurrent work by Bannur et al. (2024) introduced MAIRA-2, an MRG model for CXR capable of grounding but limited to grounded report generation. In contrast, M4CXR handles multiple tasks such as MRG, VQA, and visual grounding, while maintaining clinical accuracy."}, {"title": "Methods", "content": "M4CXR\nArchitecture. Figure 2 (a) illustrates the overall architecture of M4CXR. Following LLaVA (Liu et al. 2024), M4CXR includes a vision encoder, a projector, and an LLM, denoted as MV, MP, and ML, respectively. Given n CXR images x1,..., xn, the vision encoder converts each image xi into visual feature f, which the projector then transforms into a sequence of visual token embeddings X:\nX = Mp(f) = Mp(Mv(xi)), for i = 1, ..., n\n(1)\nA text prompt is also mapped into the embedding space, resulting in a sequence of language token embeddings Xl. The LLM takes an input in which visual embeddings are interleaved with language embeddings to generate output Y:\nY = ML(X1v, ..., Xnv, Xl)\n(2)\nThe output of the MLLM Y = {yt}t=1T, consisting of T language tokens, is generated auto-regressively:\np(Y|X1v, ..., Xnv, Xl) =\n\u03a0t=1Tp(yt|X1v, ..., Xnv, Xl, y<t).\n(3)\nBounding Box Representation. To enable visual grounding, MLLMs need to represent spatial information as text tokens. Referring to You et al. (2024), we use bounding box coordinates enclosed in square brackets without any additional special tokens. The coordinates [x1, y1, x2, y2] represent the top-left (x1, y1) and bottom-right (x2, y2) points of the bounding box on the image. As input images are preprocessed to a uniform size by the vision encoder, the bounding box coordinates are also normalized (Chen et al. 2023a) to integer values between 0 and 100.\nMulti-turn Chain-of-Thought Prompting\nWe derive insights from PromptMRG (Jin et al. 2024) to enhance the clinical accuracy of MRG. It classifies observable lesions from a CXR image and then uses the result as input prompts to generate reports. We follow a similar two-step process, first identifying key observations and then generating reports. The difference from PromptMRG is that, in our work, a single MLLM performs both classification and report generation sequentially, enabling end-to-end learning.\nThe gist is to divide the input prompt into two questions, creating multi-turn conversational data. In the first question, we present a list of potential observations and prompt the model to identify those visible in the CXR image. Then, we ask the model to generate a report based on the prior conversation. This approach resembles the reasoning process of a human radiologist, who first identifies lesions visible in the image and then generates a diagnostic report. It can be seen as a variant of the commonly used CoT (Wei et al. 2022) prompting, hence we call it CoT MRG. Figure 3 shows the multi-turn CoT prompting process. To validate the effectiveness of CoT MRG, we conduct a comparative experiment with classification and MRG as separate tasks.\nIntegrating Chest X-ray Interpretation Tasks\nWe focus on the following features of an MLLM: the capability for reasoning through multi-turn chat, the ability to utilize multiple images due to its long context length, and the flexibility for multi-task learning. To exploit these features, we construct a CXR visual instruction-following dataset, as schematized in Figure 2 (b). We define a task set across three task types: MRG, image understanding, and VQA, and transform the corresponding datasets into conversational data. Detailed task descriptions and conversation templates are provided in the Appendix (Table 8, 9).\nReport Generation: Various Scenarios. A single radiologic study can include multiple CXR images taken from different views: posterior-anterior (PA), anterior-posterior (AP), or lateral. Additionally, when the same patient undergoes follow-up studies, information from prior studies can also be utilized in the radiologic interpretation.\nIn contrast to previous studies that generate reports from a single image (Jin et al. 2024; Chaves et al. 2024; Hyland et al. 2024; Tanida et al. 2023), we consider MRG tasks in three different scenarios based on the available inputs. The single-image scenario incorporates one image from a single study. The multi-image scenario accesses multiple images from different views within a single study. The multi-study scenario accepts images and the corresponding report from prior study as inputs, along with follow-up study images. These scenarios have different conversation templates. In this work, since we distinguish tasks by templates, they are treated as separate tasks. Through these different scenario tasks, we expect the model to understand and fully utilize the given information, enabling it to generate more accurate reports when additional inputs are available.\nImage Understanding. Disease classification is one of the core tasks in CXR image understanding. We use class label data from various classification datasets. Since each dataset has its own set of disease labels, we specify the relevant labels for each question in the prompt. The model is prompted to identify findings in the image from a list of diseases.\nOur task set also includes fine-grained tasks requiring detailed analysis of specific CXR regions. These tasks involve grounding and identification of findings, phrases, organs, and anatomical structures, as well as abnormality detection. We convert bounding box labels and clinical text reports into instruction-following data to help the model link CXR images with spatial information for detailed analysis.\nVisual Question Answering. To train M4CXR, we convert all the data into an instruction-following format using predefined templates. However, training the LLM solely on templates presents a risk of overfitting: the model might lose its inherent conversational abilities and respond only in a fixed format. Therefore, we also capitalize on VQA datasets which are already in a conversational format, expecting the model to freely respond to a diverse range of questions.\nTraining Datasets\nWe collect and integrate various datasets according to the tasks described in the Methods section.\nReport Generation. MIMIC-CXR (Johnson et al. 2019) encompasses a diverse collection of CXR images along with detailed radiology reports. These reports are extensively annotated, facilitating advanced medical image analysis. For MRG tasks, our focus is on generating the FINDINGS section, which offers an in-depth description of significant observations identified in the CXR images.\nWe employ CheXbert (Smit et al. 2020) to extract observation labels from the FINDINGS section. CheXbert outputs 4-class (positive, negative, uncertain, blank) classification results for 14 predefined observation labels. For binary classification, all non-positive classes are treated as negative. Its label set of observations is provided as the candidate findings in the first question of CoT prompting.\nFor the three different MRG scenario tasks, image-report pairs are organized as follows. For single-image, every image is used as a data instance. In multi-image, images from different views that share the same study ID are gathered to compose study-level data. Multi-study combines two consecutive studies of a patient, with the studies arranged in chronological order.\nImage Understanding. BRAX (Reis et al. 2022) and CheXpert (Irvin et al. 2019) datasets are used for disease classification. We incorporate datasets that contain bounding boxes along with class labels (VinDr-CXR (Nguyen et al. 2022), ChestX-ray14 (Wang et al. 2017), ChestX-Det10 (Liu, Lian, and Yu 2020), JSRT (Shiraishi et al. 2000), SIIM (Zawacki et al. 2019), RSNA (Shih et al. 2019), COVID-19 Radiography (Chowdhury et al. 2020), COVID-QU-Ex (Rahman et al. 2021; Tahir et al. 2021; Chowdhury et al. 2020), QaTa-COV19 (Degerli et al. 2021)) for various fine-grained image understanding tasks in addition to disease classification. MS-CXR (Boecking et al. 2022b) is a dataset that includes image-sentence pairs of bounding boxes and corresponding phrases for images and reports from MIMIC-CXR. ImaGenome (Wu et al. 2021), which is also derived from MIMIC-CXR, annotates text in the report with bounding boxes aligned with 29 anatomical regions.\nVisual Question Answering. MIMIC-CXR-VQA (Bae et al. 2024) and MIMIC-Diff-VQA (Hu et al. 2023) datasets are designed to address a wide range of questions based on MIMIC-CXR images. These datasets encompass diverse and comprehensive question-answer pairs, enabling models to effectively handle various CXR-related queries. RaDialog (Pellegrini et al. 2023) is a visual instruction-following dataset designed to facilitate tasks that require structured dialogues. It guides models in understanding and responding to instructions based on CXR images.\nTest Datasets\nThe test set used for evaluating all MRG tasks is from MIMIC-CXR. The official test split includes 2,461 images in the frontal (PA, AP) view and 1,397 in other views, totaling 3,858 images. Each study corresponding to the 2,461 frontal images is considered a separate data instance in multi-image and multi-study, with the prior study included for the latter if available. As a result, the test set size for multi-image and multi-study is 2,461, while single-image considers two test set sizes: 2,461 and 3,858.\nFor phrase grounding evaluation, the test set of MS-CXR is used. Each query phrase in MS-CXR corresponds to a single bounding box. To ensure a rigorous evaluation of the task, we exclude all MS-CXR images from the training set of other datasets across all tasks. Following the data split of Chen et al. (2023b), the test set has 167 images.\nWe employ the test set of MIMIC-CXR-VQA for evaluating medical VQA. Of the 13,793 total samples, we exclude 2,484 samples that lack answers, resulting in 11,309 samples for assessment. Additionally, we incorporate SLAKE (Liu et al. 2021), which is not included in the training data. From its test set of 2,094 samples, we use only CXR images with close-ended questions in English, retaining 114 samples.\nEvaluation Metrics\nReport Generation. The generated reports are assessed using natural language generation (NLG) and clinical met-"}, {"title": "Results", "content": "Medical Report Generation\nSingle-Image. To evaluate single-image MRG performance, we compared our model with state-of-the-art MRG models, including LLM-CXR, RaDialog, METransformer (Wang et al. 2023), DCL (Li et al. 2023b), PromptMRG, LM-RRG (Zhou et al. 2024), CheXagent, MAIRA-1 (Hyland et al. 2024), Med-PaLM M, and LLaVA-Rad (Chaves et al. 2024). The results are summarized in Table 1. For direct comparison with other models, we evaluated M4CXR using two distinct test sets: one with 2,461 frontal images and another with 3,858 images of all views. Including lateral views decreased clinical accuracy, suggesting the complexity of recognizing observations from lateral images.\nAmong the models evaluated on only frontal images, M4CXR attained the highest CheXbert clinical accuracy, with mF1-14 and MF1-14 scores of 60.6 and 40.0, respectively. In the evaluation that included all views, our model outperformed PromptMRG with an eF1-14 of 50.2. Although Med-PaLM M used a different all-view test set, which limits direct comparison, our model showed a significant advantage in mF1, with similar or slightly lower MF1s.\nHowever, M4CXR did not achieve the best scores in both RadGraph-F1 and NLG metrics. To improve clinical accuracy, CoT prompting provides candidate findings in the first question before generating reports. This approach may have led the model to use terms from the findings list rather than words that exactly match those in the ground-truth report, which could be a contributing factor to the observed lower NLG scores. RadGraph-F1, calculated by extracting entities and relations from the report, was likely reduced due to this mismatch.\nVarious Scenarios. Table 2 shows the evaluation results across various input scenarios. Multi-image improved clinical accuracy compared to single-image, with CheXbert mF1-14 and MF1-14 scores of 61.1 and 41.0, respectively. Providing the prior study as additional input improved MF1, though mF1 slightly decreased. These results demonstrate that our model can effectively utilize available inputs to generate medical reports in various scenarios.\nAlthough test conditions differ, we compared our model's performance with two baseline models. (Tu et al. 2024) reported the zero-shot generalization results of Med-PaLM M in a two-view setting. Since it was trained only on single-image, its effectiveness diminished, unlike M4CXR leveraging additional images for enhanced outcomes. MAIRA-2 accepts a prior study and multiple images as input, akin to our multi-study scenario. Its performance matches ours closely: MAIRA-2 achieved a higher MF1-14 of 42.7, whereas M4CXR surpassed it with an mF1-14 of 60.7.\nMedical Phrase Grounding\nTable 3 compares the medical phrase grounding results of TransVG (Deng et al. 2022), MedRPG, MAIRA-2, and M4CXR, on the MS-CXR test set. While MedRPG achieved the highest accuracy and mIoU, MedRPG and TransVG are specialized for phrase grounding and cannot perform other tasks. Given this limitation, M4CXR, a multi-tasking model, shows competitive performance with an accuracy of 68.3 and mIoU of 57.9. MAIRA-2, utilizing a private dataset for grounded report generation, reported a similar mIoU to ours.\nVisual Question Answering\nTable 4 presents the results of medical VQA evaluation. We conducted our own evaluations on open-source medical MLLMs: LLaVA version of RaDialog (Pellegrini et al. 2024), RadFM (Wu et al. 2023), and CheXagent. M4CXR outperformed the other models, except for the recall on MIMIC-CXR-VQA. CheXagent included the dataset in its training, likely explaining its high recall of 72.8. The lower accuracy on MIMIC-CXR-VQA, compared to SLAKE, is due to its greater variety of questions. Moreover, the higher BLEU-1 score compared to accuracy suggests the presence of partially correct answers.\nAblation Study\nWe examined the effects of multi-turn CoT prompting and the combination of task types through comparative experiments (Table 5). Generating reports without CoT prompting in M4CXR led to a significant drop in clinical accuracy. Exp1, trained on single-turn MRG without the first question of CoT prompts, yielded similar results. Exp2 involved training on single-turn MRG and disease classification as separate tasks. Testing Exp2 with CoT prompts improved clinical accuracy, but NLG metrics fell significantly, with the generated text being merely a list of identified observations. The experiments reveal the effectiveness of multi-turn CoT prompting in MRG's training and testing phases.\nExp3 and Exp4 excluded image understanding and VQA tasks, respectively. In Exp3, while MRG performance was on par and VQA performance improved due to the increased proportion of VQA data, the absence of image understanding data precluded visual grounding. In Exp4, the BLEU-1 score for VQA dropped significantly from 66.4 to 36.1, suggesting the necessity of VQA datasets in understanding and answering free-form questions. Moreover, the slight decrease in Exp4's clinical accuracy compared to M4CXR implies the contribution of VQA datasets to MRG performance improvement. Detailed examples are in the Appendix.\nQualitative Analysis\nFigure 4 shows two examples of dialogs between a user and M4CXR, each for visual grounding and VQA. Additional"}, {"title": "Conclusion", "content": "In this study, we introduced M4CXR, a multi-modal LLM aimed at enhancing CXR interpretation by leveraging the versatile advantages of LLMs. Trained on a CXR visual instruction-following dataset constructed by appropriately combining various datasets, M4CXR is capable of performing multiple tasks. Exploiting its reasoning capability, we proposed a novel chain-of-thought prompting strategy that significantly improved the clinical accuracy of report generation. By identifying observations from X-ray images and generating descriptions through multi-turn conversations, M4CXR achieved notable improvements in CheXbert F1 scores compared to existing models. The model's ability to handle multiple images and incorporate prior studies demonstrated its adaptability in diverse clinical scenarios.\nBeyond report generation, M4CXR also proved to be highly effective in visual grounding and VQA. M4CXR competed well with models specialized for visual grounding, while also exhibiting outstanding performance in VQA.\nQualitative analysis highlights M4CXR's flexibility in answering free-form questions. With future work to address limitations such as low NLG metrics and the presence of hallucinations, we anticipate that further advancements in M4CXR will contribute to the development of a highly capable radiology assistant."}]}