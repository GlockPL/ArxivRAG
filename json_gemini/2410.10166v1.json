{"title": "AUTOMATED FILTERING OF HUMAN FEEDBACK DATA FOR ALIGNING TEXT-TO-IMAGE DIFFUSION MODELS", "authors": ["Yongjin Yang", "Sihyeon Kim", "Hojung Jung", "Sangmin Bae", "SangMook Kim", "Se-Young Yun", "Kimin Lee"], "abstract": "Fine-tuning text-to-image diffusion models with human feedback is an effective method for aligning model behavior with human intentions. However, this alignment process often suffers from slow convergence due to the large size and noise present in human feedback datasets. In this work, we propose FiFA, a novel automated data filtering algorithm designed to enhance the fine-tuning of diffusion models using human feedback datasets with direct preference optimization (DPO). Specifically, our approach selects data by solving an optimization problem to maximize three components: preference margin, text quality, and text diversity. The concept of preference margin is used to identify samples that contain high informational value to address the noisy nature of feedback dataset, which is calculated using a proxy reward model. Additionally, we incorporate text quality, assessed by large language models to prevent harmful contents, and consider text diversity through a k-nearest neighbor entropy estimator to improve generalization. Finally, we integrate all these components into an optimization process, with approximating the solution by assigning importance score to each data pair and selecting the most important ones. As a result, our method efficiently filters data automatically, without the need for manual intervention, and can be applied to any large-scale dataset. Experimental results show that FiFA significantly enhances training stability and achieves better performance, being preferred by humans 17% more, while using less than 0.5% of the full data and thus 1% of the GPU hours compared to utilizing full human feedback datasets. Warning: This paper contains offensive contents that may be upsetting.", "sections": [{"title": "1 INTRODUCTION", "content": "Large-scale models trained on extensive web-scale datasets using diffusion techniques (Ho et al., 2020; Song et al., 2020), such as Stable Diffusion (Rombach et al., 2022), Dall-E (Ramesh et al., 2022), and Imagen (Saharia et al., 2022), have enabled the generation of high-fidelity images from diverse text prompts. However, several failure cases remain, such as difficulties in illustrating text content, incorrect counting, or insufficient aesthetics for certain text prompts (Lee et al., 2023; Fan et al., 2024; Black et al., 2023). Fine-tuning text-to-image diffusion models using human feedback has recently emerged as a powerful approach to address this issue (Black et al., 2023; Fan et al., 2024; Prabhudesai et al., 2023; Clark et al., 2023). Unlike the conventional optimization strategy of likelihood maximization, this framework first trains reward models using human feedback (Kirstain et al., 2024; Wu et al., 2023; Xu et al., 2024) and then fine-tunes the diffusion models to maximize reward scores through policy gradient (Fan et al., 2024; Black et al., 2023) or reward-gradient based techniques (Prabhudesai et al., 2023; Clark et al., 2023). More recently, Diffusion-DPO (Wallace et al., 2023), which directly aligns the model using human feedback without the need for training reward models, has been proposed. This approach enables fine-tuning diffusion models at scale using human feedback, with the additional benefit of leveraging offline datasets more effectively.\nIn this paper, we propose a novel automated Filtering framework that selectively integrates human Feedback, designed for efficient Alignment of text-to-image diffusion models (FiFA). We frame the filtering task as an optimization problem, aiming to find a subset that maximizes the three components; (1) preference margin, (2) text quality, and (3) text diversity. A key component of our optimization is selecting data pairs that are more informative, as determined by their preference margins, which are calculated using a proxy reward model. Specifically, training pairs with a low preference margin can be considered noisy and ambiguous data, as their preferences may easily flip, thereby hindering the training process (Chowdhury et al., 2024; Yang et al., 2023b; Rosset et al., 2024). Furthermore, to address the concerns on harmfulness problems and coverage of selected subset induced by relying only on preference margin, we also consider the quality and diversity of the text prompts in the objective function. We assess text quality using a Large Language Model (LLM), following Sachdeva et al. (2024), and measure text diversity by calculating the entropy of embedded text prompts (Zheng et al., 2020) approximated using a k-nearest neighbor estimator (Singh et al., 2003). To integrate all these components, we define an objective function that combines the three metrics into a single optimization problem. Additionally, to improve efficiency, we approximate the solution by assigning a data importance score for each data pair, making FiFA efficient and applicable to large-scale datasets through an automated process."}, {"title": "2 PRELIMINARIES", "content": "Diffusion models (Ho et al., 2020) are probabilistic models that aim to learn a data distribution p(x) by performing multiple denoising steps starting from a Gaussian noise. The diffusion process consists of two parts, forward process and backward process.\nIn the forward process, noise is progressively injected at each timestamp t according to $q(x_t|x_0) \\sim N(\\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t)I)$, where the noise schedule \u03b1t is a monotonically decreasing function and $\\bar{\\alpha}_t := \\prod_{s=1}^t \\alpha_s$. The neural network $e_{\\theta}$ is trained to learn the denoising process with the following objective:\n$\\mathcal{L}_{DM}(\\theta) = \\mathbb{E}_{x_0, t}[\\lambda(t)||\\epsilon - \\epsilon_{\\theta}(x_t, t)||^2]$,\nwhere x(t) is determined by the noise schedule and \u03f5 is a Gaussian noise. During generation, the diffusion model takes reverse denoising steps starting from a random Gaussian noise.\nThe conditional diffusion model (Rombach et al., 2022), such as a text-to-image diffusion model, aims to learn the data distribution p(x|c), trained using the conditional error $\\epsilon(x, c)$ instead of the unconditional error \u03f5(x).\nReward Learning in Text-to-Image Domains Using human preference data, the goal of reward learning is to train a proxy function aligned with human preferences. In text-to-image domains, given textual condition c and the generated image x0 from that condition, we assume a ranked pair with $x_w$ as a \"winning\u201d sample and $x_l$ with a \"losing sample\u201d, that satisfy $x_w \\succ x_c$. Using the Bradley-Terry (BT) model, one can formulate maximum likelihood loss for binary classification to learn the reward model $r_{\\phi}$, parameterized by \u03c6, as follows:\n$\\mathcal{L}_{BT}(\\phi) = -\\mathbb{E}_{c, x_w, x_l} [\\log \\sigma(r_{\\phi}(c, x_w) - r_{\\phi}(c, x_l))]$,                                                                (1)\nwhere \u03c3 is a sigmoid function, c is a text prompt, and image pairs $x_w$ and $x_l$ labeled by humans.\nDirect Preference Optimization for Diffusion Models Direct Preference Optimization (DPO) (Rafailov et al., 2024) is an approach to align the model using human feedback without training a separate reward model. Directly applying DPO loss to diffusion models is not feasible, as an image is generated through the trajectory $x_{T:0}$ where T denotes the number of denoising steps, and obtaining the probability of this entire trajectory is generally intractable. Following Diffusion-DPO (Wallace et al., 2023), the DPO loss for diffusion models can be approximated as follows:\n$\\mathcal{L}_{DPO}(\\theta) = -\\mathbb{E}_{t,c,x_w,x_l} \\log \\sigma \\left( -\\beta \\tau_w(x_t) \\left[ \\frac{(||\\epsilon_w - \\epsilon_{\\theta}(x, t) ||^2 - ||\\epsilon_w - \\epsilon_{ref}(x, t)||^2)}{||\\epsilon_w - \\epsilon_{\\theta}(x, t)||^2 + ||\\epsilon_w - \\epsilon_{ref}(x, t)||^2} \\right] \\right),$                                                                                                                                                                                                                                                                                                                                                                                         (2)\nwhere w(At) is a weight function typically set to a constant, $x_w, x_l$ are the noised inputs of winning and losing images at timestep t respectively, $\\epsilon_w, \\epsilon_l \\sim N(0, I)$ represent the Gaussian noise for the winning and losing images respectively, and $E_{ref}$ is a pretrained diffusion model. Detailed derivation of DPO loss for diffusion is presented at Appendix B."}, {"title": "3 METHODS", "content": "Aligning text-to-image diffusion models with large-scale human feedback data requires significant computational resources and training time. Moreover, the noisy nature of feedback datasets complicates DPO optimization (Chowdhury et al., 2024), motivating us to identify a core subset to improve and accelerate alignment process.\nTo tackle this issue, we propose FiFA, which automatically filters the full human feedback data to obtain a subset for efficiently fine-tuning text-to-image models. Specifically, our method leverages preference margin as a key component to rapidly increase the reward value, while also considering the quality and diversity of the text prompts to mitigate harmfulness and ensure robustness. Additionally, we frame the task as an optimization problem to find the best subset that maximize these three components, resulting in an automated filtering framework applicable to any large-scale dataset."}, {"title": "3.1 PREFERENCE MARGIN", "content": "The noisy and ambiguous nature of human preference datasets has been well explored, where a labeled preference does not reflect the true preference and may contain spurious correlations (Yang et al., 2023b; Chowdhury et al., 2024). This can be especially problematic for the efficient fine-tuning of diffusion models, as such noisy data slow down training and reduce the generalization capability (Zhang et al., 2021). Inspired by recent papers that highlight the importance of clean preference pairs (Yang et al., 2023b; Rosset et al., 2024), we use the preference margin to enable more efficient and effective fine-tuning of diffusion models to alleviate this issue.\nTo estimate the preference margin between the winning and losing images, we utilize a proxy reward model r trained on the full feedback dataset using the BT modeling approach with Eq. (1). This process does not pose efficiency concerns for text-image domains, as training a reward model using CLIP (Radford et al., 2021) or BLIP architectures (Li et al., 2022) demands significantly less time than training large diffusion models, Additionally, open-sourced text-image reward models like PickScore (Kirstain et al., 2024) and HPSv2 (Wu et al., 2023), trained on human feedback datasets, can also be utilized."}, {"title": "3.2 TEXT QUALITY AND DIVERSITY", "content": "While reward margin is a critical component, relying solely on the reward margin may overlook two critical factors: the quality and diversity of text prompts.\nText Quality The text prompts in human feedback datasets created by real users tend to be of low quality due to unformatted structures, typos, and duplicated content. More importantly, these prompts may include harmful components, such as sexual content, bias, or violence. Figure 3a demonstrates the potential harm caused by naively using all open-sourced Pick-a-Pic v2 dataset for fine-tuning, motivating us to ensure that the system takes the quality of the text into consideration.\nTo estimate this text quality, inspired by ASK-LLM (Sachdeva et al., 2024), which was developed to assess the usefulness of pretraining data, we evaluate the text quality using a LLM. Specifically, we ask the LLM to evaluate whether the text prompt is clearly formatted, understandable, of appropriate difficulty, and free of harmful content by providing a score. We scale the scores from 0 to 10, denoted as the LLM score. In our experiments, we use OpenAI gpt-3.5-turbo-0125 model. A detailed explanation on LLM score is available in Appendix C.\nText Diversity The problem with our selection method is that image pairs with a high preference margin may be focused on certain prompts or families of prompts. This is supported by Figure 3b, as relying on high-margin prompts leads to a decrease in most diversity metrics, such as word entropy, compared to using the full dataset. The lack of diversity may limit generalization capability. Therefore, we also consider text diversity during data filtering.\nTo estimate text diversity, we employ the entropy of the embedded text prompts (Zheng et al., 2020). Specifically, let C denote a random variable with a probability density function p, representing a distribution of a selected subset of text prompts from the full dataset D, where each text prompt is embedded in Rd space. Text diversity is then estimated through H(C), where $H(C) = -\\mathbb{E}_{c\\sim p(c)} [\\log p(c)]$, with high H(C) indicating high text diversity.\nAdditionally, we set the hard constraint on the selected number of pairs for each text prompt, which is set to 5 and doubled if the number of desired data K is not met, to prevent the selection of a large number of duplicate prompts among the selected subset."}, {"title": "3.3 AUTOMATED DATA SELECTION WITH OBJECTIVE FUNCTION", "content": "Given the components for data importance, the remaining challenge is how to incorporate all components into an automated data filtering framework that could be applied to any dataset. To achieve this, we formulate data selection as an optimization problem to find the subset with high margin, text quality, and diversity. The pseudocode for our algorithm is presented in Algorithm 1."}, {"title": "Objective Function", "content": "Our objective function should consider the preference margin, text quality and text diversity. The first, preference margin $m^{reward}$, is calculated using the trained proxy reward model. Specifically, given each data pair {c, xw, xl}, we calculate the reward margin $m^{reward}$ as follows:\n$m^{reward}(c, x_w, x_l) = |r_{\\phi}(c, x_w) - r_{\\phi}(c, x_l)|,$\nwhere c is a text prompt. Then, we use the LLM score to evaluate the quality of the text prompts and text entropy H(C) to measure diversity, as explained in Section 3. Combining all of these components, our goal is to find the subset S that maximizes the following objective function f:\n$f(S) = \\sum_{c, x_w, x_l \\in S} [m^{reward}(c, x_w, x_l) + \\alpha * LLM\\_Score(c)] + \\gamma * H(C),$\n                                                                                                (3)\nwhere \u03b1 and \u03b3 are hyperparameters for balancing the three components. Unlike the other terms, calculating H(C) is infeasible. To address this issue, we estimate the entropy value using a k-nearest neighbor entropy estimator (Singh et al., 2003). Specifically, H(C) can be approximated as follows:\n$H(C) \\propto \\frac{1}{N_c}\\sum_{i=1}^{N_c} \\log ||c_i - c_{i}^{k-NN} ||^2,$\n                                                                                                                                                                         (4)\nwhere Nc is the number of text prompts, and $c_{i}^{k-NN}$ is the k-NN of $c_i$ within a prompt set ${c_i}_{i=1}^{Nc}$. Although Eq. 4 enables the calculation of H(C), finding an optimal set of C that maximizes this function is not feasible for large-scale datasets. Therefore, to efficiently select data, we approximate the function by calculating $\\log ||c_i - c_{i}^{k-NN} ||^2$ over the entire set of prompts and use this as an estimator of the diversity score for each data pair. The final objective function f that represents the data importance score for each data point is then formulated as follows:\n$f(c, x_w, x_l) = m^{reward}(c, x_w, x_l) + \\alpha * LLM\\_Score(c) + \\gamma * \\log ||c - c^{k-NN} ||^2.$\n                                                                                                                                    (5)\nUsing this objective function, we can select data by choosing the top K data that have high f value, with K determined based on the computational burden, as formulated below:\n$S = \\underset{X, |X|=K}{\\operatorname{argmax}} \\sum_{(c,x_w,x_l)\\in X}f(c,x_w,x_l).$"}, {"title": "4 EXPERIMENTS", "content": "EXPERIMENTAL SETTINGS\nDataset We use the Pick-a-Pic v2 dataset (Kirstain et al., 2024), which consists of 59k text prompts and 850k human preference pairs without tie, and the HPS v2 dataset (Wu et al., 2023), which consists of 104k text prompts and 645k human preference pairs, for fine-tuning in our main experiments. The pairs in these datasets contain images generated using SDXL-beta, Dreamlike, a fine-tuned version of SD1.5, etc. For ablation and further analysis, we mainly use models trained on the Pick-a-Pic v2 dataset. We primarily use the Pick-a-Pic test set for evaluation. To ensure safety, we manually filter out some harmful text prompts from these test prompts, resulting in 446 unique prompts. Moreover, to test the ability of the model to generalize across diverse prompts, we utilize text prompts from PartiPrompt (Yu et al., 2022), which consist of 1630 prompts, and the HPSv2 benchmark (Wu et al., 2023), which consists of 3200 prompts with diverse concepts.\nEvaluation We measure performance automatically using PickScore (Kirstain et al., 2024) and HPSv2 Reward (Wu et al., 2023), as our aim is to rapidly enhance the reward through DPO training. To also assess improvement on image-only quality, we additionally utilize the LAION Aesthetic"}, {"title": "4.2 MAIN RESULTS", "content": "Quantitative Results Table 1 demonstrates the performance of our methods compared to the baselines across three different reward models. Our method, requiring only 20% of the training time for SD1.5 and less than 1% of the training time for SDXL, consistently outperforms the trained models that use the full dataset for most metrics on all benchmarks, especially on SD1.5. The performance increase in both train sets indicate that FiFA is generalizable across different datasets. Notably, the increases in the Aesthetic Score, in addition to human preference rewards, indicate that the models trained using FiFA robustly enhance image quality.\nMoreover, FiFA achieves high scores on PartiPrompt, which tests various compositions, along with the HPSv2 benchmark, featuring diverse prompts from various concepts and domains. This demonstrates that our model, trained with the small dataset obtained from FiFA, can generalize well across a wide range of prompts from different domains and styles."}, {"title": "4.3 ADDITIONAL EXPERIMENTS", "content": "Comparison with Vanilla Pruning Methods In this section, we compare FiFA with multiple baselines, including traditional data pruning techniques of coreset selection (Mirzasoleiman et al., 2020) (CS) using CLIP embeddings, error score based selections (L) (Paul et al., 2021) using DPO loss, and random selection (R). We also compare it with the baselines of naively utilizing samples with high rewards (HR) of winning images. As shown in Figure 6a, FiFA outperforms all the baselines, demonstrating its effectiveness. Specifically, traditional baselines (CS and L) perform poorly as they are not designed for diffusion model alignment while requiring more filtering time. Random or absolute reward-based filtering also underperforms, showing that smaller datasets alone do not ensure efficient training, underscoring the effectiveness of our method."}, {"title": "4.4 ABLATION ON TEXT QUALITY AND DIVERSITY", "content": "Ablation on \u03b1 and \u03b3 Here, we explore how different \u03b1 and \u03b3 values, that control the effects of text quality and diversity, affect the performance of trained models. The results are illustrated in Figure 7a. Since a preference margin is used in all configurations, performance remains high compared to the model trained on the full dataset, demonstrating FiFA's robustness to hyperparameters. However, extremely high or low \u03b1 and \u03b3 values are ineffective, either reducing the total margin or compromising text quality and diversity. An \u03b1 range of 0.1-1.0 and a \u03b3 range of 0.5-1.5 ensure effective alignment, with the optimal configuration of (0.5, 0.5) marked in red that also works well for HPSv2 dataset.\nCan FiFA Reduce Harmful Contents? In this section, we evaluate whether FiFA can prevent models from generating harmful images by considering text quality. To estimate the harmfulness, we generated 200 images from three neutral prompts about \u201cwoman\u201d and \u201cgirl\u201d and manually labeled the harmfulness of each image (see Appendix D for more details). As shown in Figure 7b, when fine-tuned on the entire Pick-a-Pic v2 dataset, the harmfulness of images generated by the fine-tuned model increases significantly, showing at least a 30% increase for all three prompts compared to those produced by the pretrained model. This clearly demonstrates that RLHF on large-scale human datasets does not always align model's behaviors with human value. In contrast, images generated by the fine-tuned model using FiFA demonstrate reduced levels of harmfulness compared to the pretrained model, indicating that FiFA effectively enhances model safety.\nImpact of Text Diversity To demonstrate the importance of text diversity, we compare samples generated by models trained on subsets of the Pick-a-Pic v2 dataset that either consider only high margin with text quality or also include text diversity, using prompts including \u201craccoon\u201d with different artistic styles. As shown in Figure 7c, incorporating diversity leads to a better understanding of concepts like \u201cVincent van Gogh\u201d and \u201cabstract cubism\u201d compared to models trained without diversity. This demonstrates that adding text diversity improves the generalization of fine-tuned models."}, {"title": "5 CONCLUSION", "content": "In this paper, we propose FiFA, a new automated data filtering approach to efficiently and effectively fine-tune diffusion models using human feedback data with a DPO objective. Our approach involves selecting data by solving an optimization problem that maximize preference margin, which is calculated by a proxy reward model, text quality and text diversity. In our experiments, the model trained using FiFA, utilizing less than 1% of GPU hours, outperforms the model trained on the full dataset in both automatic and human evaluations across various models and datasets."}, {"title": "LIMITATIONS", "content": "Although our proposed FiFA demonstrates its effectiveness and efficiency, we validate our method primarily using the DPO objective among various alignment methods such as policy gradient approaches or those employing reward gradients. It would also be meaningful to extend our algorithm to fit diverse algorithms and other modalities."}, {"title": "ETHICS STATEMENT", "content": "Although text-to-image diffusion models are showing remarkable performance in creating high-fidelity images, they may generate harmful content, both intentionally and unintentionally, as the models do not always align well with the text prompts. Therefore, using text-to-image diffusion models requires extra caution.\nThis concern also applies to our approach. Despite the impressive performance of the model when using FiFA, text-to-image models can still generate harmful, hateful, or sexual images. Although we mitigate this risk by filtering for text quality, as evidenced by improvements over models trained on the full dataset, the inherent issues of pretrained models can still arise in our models. We strongly recommend that users exercise caution when using models trained with our methods, considering the potential risks involved. Moreover, we will open-source the model, along with the safety filtering tool and guidelines for using our model."}, {"title": "A IMPLEMENTATION DETAILS", "content": "Here, we explain the precise implementation details of FiFA. An overview of our hyperparameters is presented in Table 2. For the final models of the main experiments, we update the SD1.5 models for 1000 steps and the SDXL models for 100 steps. We apply warmup steps of 10 for SD1.5 and 5 for SDXL. We adopt the Adam optimizer for SD1.5 and Adafactor for SDXL to save memory consumption. We set the learning rate to le-7 for SD1.5 and 2e-8 for SDXL. We use an effective batch size of 128 for both models by differentiating the batch size and accumulation steps. A piecewise constant learning rate scheduler is applied, which reduces the learning rate at certain steps.\nFor the full training of SD1.5, we use warmup steps of 500 and then use a constant scheduler. To calculate the GPU hours for SDXL, we calculate the GPU hours for the first 20 steps, where the time spent for each step becomes constant, and then multiply that number to match the 1000 steps on which the released version of SDXL is trained."}, {"title": "B DPO FOR DIFFUSION MODELS", "content": "Direct Preference Optimization Given a reward model trained with BT modelling of Eq. (1), the typical approach to increasing the reward is to utilize reinforcement learning to maximize the reward. This often incorporates a KL regularization term with reference distribution Pref, which can be formulated as follows:\n$\\underset{P_\\theta}{\\operatorname{max}} \\mathbb{E}_{x_0 \\sim p_\\theta(x_0|c), c \\sim D} [r(x_0, c) - \\beta D_{KL}(p_\\theta((x_0|c) || p_{ref}(x_0|c)].$\n                                                                                                                                                                                                                                                                                                                                                                                                             (7)\nDirect Preference Optimization (DPO) (Rafailov et al., 2024) is a method that directly aligns the model without reward training by integrating the reward model training and RL training stages into one. This approach leverages the insight that the optimal policy of the model in Eq. (7) can be represented by a reward function. Hence, the optimal solution $p_\\theta$ can be written as:\n$p_\\theta(x_0|c) = p_{ref}(x_0|c) \\cdot \\exp(r(x_0, c)/\\beta)/Z(c),$\n                                                                                                                                                                                                                                                                                                                                                                                              (8)\nwhere Z(c) is a partition function. Since Z(c) is usually intractable, one cannot directly obtain the optimal policy from this closed-form solution. However, after reformulating the reward in Eq. (8) and incorporating it into the objective function of the BT model (Eq. (1)), intractable part cancel out and the result becomes a tractable function parameterized by the model. The resulting loss becomes:\n$\\mathcal{L}(\\theta) = -\\mathbb{E}_{c, x_w, x_l} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{p_\\theta(x_w|c)}{p_{ref}(x|c)} - \\beta \\log \\frac{p_\\theta(x_l|c)}{p_{ref}(x_l|c)} \\right) \\right].$\n(9)\nDiffusion DPO Objective For text-to-image generation, the diffusion model outputs $\\epsilon_{\\theta}(x_0, c)$, where c is a text prompt and x0 is a clean image. During inference, the DDIM sampler (Song et al.,"}, {"title": "C LLM SCORE FOR TEXT PROMPTS", "content": "To calculate the LLM score for each text prompt in the Pick-a-Pic v2 training dataset, we use the gpt-3.5-turbo-0125 model via the OpenAI API. Specifically, we instruct the LLM to assign"}, {"title": "Prompt Template for LLM Score", "content": "Please act as an impartial judge and assess the quality of the given text prompt for fine-tuning a text-to-image diffusion model. Use the following criteria to determine if a pair scores highly:\n1. The text prompt should demonstrate high learnability. In other words, by learning this prompt, the model should be able to understand various concepts.\n2. The prompt should contain no NSFW content; prompts with NSFW content should receive a score of 0.\n3. The prompt should have moderate difficulty, as prompts that are too difficult or too easy are not helpful.\n4. Prompts with excessive duplicate words, typos, or grammar errors should receive lower scores.\nBegin your evaluation by providing a short and brief explanation. Be as objective as pos-sible. After providing your explanation, please rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n[Text Prompt]\n{text prompt}"}, {"title": "D EVALUATING HARMFUL CONTENTS", "content": "To assess the harmfulness of the DPO-trained model on the Pick-a-Pic v2 dataset, we evaluate harmfulness using three neutral prompts: a simple prompt with an action \u201ca woman standing\", a simple prompt with an adjective \u201ca beautiful woman\", and a prompt that is not toxic but neutral, yet has a higher probability of creating sexual images, \u201ca hot girl\". The motivation for using \"woman\" and \"girl\" is because user-generated prompts in the Pick-a-Pic v2 dataset frequently contain these keywords.\nFor each prompt, we generate 200 images with different seeds from 0 to 199 with the model trained using FiFA and the model trained using the full dataset. For each generated image, we adopt human annotations, with three authors manually labeling each content with a binary label of harmful or not, considering the scale and safety issues when conducted on other humans. We label an image as harmful if it contains NSFW content such as nudity, vulgarity, or any other harmful elements. We employ majority voting to set the final label of each image. Then we calculate the harmfulness rate, the rate of images labeled as harmful out of all images for each prompt."}, {"title": "E RESULTS WITH DIFFERENT STATISTICAL MEASURES", "content": "In our experiments on the Pick-a-Pic test set (Kirstain et al., 2024), including Figure 1a, we generated four images for each prompt and then averaged the rewards across images and prompts. Here, we will show different statistical measures for aggregating multiple rewards for each prompt: the maximum"}, {"title": "F TRAINING LOSS AND IMPLICIT ACCURACY", "content": "shows the training loss comparing the training using FiFA and training using the full dataset. Consistent with the main result, FiFA enables much faster training with better convergence, as it decreases the loss rapidly. In contrast, due to the noisy nature, the loss of full training seems hard to converge and eventually increases at some points. Moreover, as shown in Figure 11b, the implicit reward model is much better trained when trained on the dataset pruned with FiFA. These results demonstrate that FiFA makes training more stable and achieves faster convergence."}, {"title": "G REWARD-WEIGHTED DPO LOSS", "content": "We interpreted labels with low reward margins as noisy preferences. To make the training robust in this situation, Mitchell (2023) proposes a conservative DPO loss (cDPO), with the assumption that"}, {"title": "H SAMPLE ANALYSIS OF SELECTED DATA USING FIFA", "content": "Here, we analyze some selected or filtered samples by FiFA on either HPSv2 or Pick-a-pic v2 dataset. As explained in Section 3.3, we select samples based on high margin, text quality, and text diversity. As shown in Figure 12, the selected samples contain images with clear distinctions, meaningful prompts of appropriate difficulty, and minimal overlap with other prompts. On the other hand, the filtered prompts include images with either high or low text-image alignment, meaningless or random prompts, or prompts highly similar to the selected ones. These examples demonstrate that our objective function in Eq. (5) effectively selects samples that consider a high preference margin and ensure high-quality, diverse text prompt sets, as intended."}, {"title": "I PROOFS OF THEORETICAL ANALYSIS", "content": "In this section, we formally state Theorem 1 with additional details. We first start with assuming linear model assumption for the reward feedback which is stated by following assumption.\nAssumption 1 (Linear model with noisy feedback). For any feature vector (x, c) \u2208 Rd of image x and text c, there exists unknown parameter for the reward model $\\theta_{*} \\in R^d$ such that a reward r(x, c( is given by following equation:\n$r(x, c) = \\phi(x, c)^{T} \\theta_{*} + \\eta.$\n                                                              (16)\nHere, \u03b7 is a 1-subgaussian random noise from the reward model, and $\\phi : I \\times C \\rightarrow R^d$ is the given feature map which sends text prompt c \u2208 C and generated image x \u2208 I into d-dimensional latent space.\nOLS estimator Suppose we have feature vectors $d_i(c) := \\phi(x_i, c)$ and corresponding observations ri(x, c) from Eq. 16 for i = 1, 2, . . ., n. Then, we can estimate the true parameter $\\theta_{*} $, by following estimator which is known as OLS estimator:\n$\\theta = V^{-1} \\sum_{i=1}^n r_i \\phi(x_i, c),$\n                                                                  (17)"}, {"title": "J HUMAN EVALUATION", "content": "In this section, we provide detailed information about our human evaluation, briefly explained in Section 4.1. We randomly select 100 prompts for each concept in the HPSv2 benchmark, where the concepts are photo, paintings, anime, and concept art. For the selected 100 prompts, we use 50 prompts to create one survey form and the remaining 50 prompts for another, totaling 8 survey forms with 400 prompts. We assign three different annotators to each survey form with a total of 24 annotators.\nIn the case of human evaluations, we take extreme care to ensure that no harmful content is included on the survey form. All the authors cross-check the content to prevent any harmful material from being exposed to others. Moreover, we do not collect any personal information from participants, including their name, email, or any other identifying details. Instead, all results are gathered from anonymous users. To ensure this, we cross-check all options, remove any questions related to identity, and select options that do not collect any data (e.g., email) from the survey form. Additionally, among the eight test sets, each participant is assigned only one, making it unnecessary to collect private information such as an ID.\nWe have informed the participants about the purpose of our research, potential risks, and their right to withdraw their data. For the benefit of the participants, we provided a payment of 15$ for each participant, where the evaluation took less than an hour. The instruction we"}]}