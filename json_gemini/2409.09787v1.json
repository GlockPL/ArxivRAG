{"title": "BEnDEM:A Boltzmann Sampler Based on\nBootstrapped Denoising Energy Matching", "authors": ["RuiKang Ou Yang", "Bo Qiang", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"], "abstract": "Developing an efficient sampler capable of generating independent and identically\ndistributed (IID) samples from a Boltzmann distribution is a crucial challenge in\nscientific research, e.g. molecular dynamics. In this work, we intend to learn neural\nsamplers given energy functions instead of data sampled from the Boltzmann dis-\ntribution. By learning the energies of the noised data, we propose a diffusion-based\nsampler, ENERGY-BASED DENOISING ENERGY MATCHING, which theoretically\nhas lower variance and more complexity compared to related works. Furthermore,\na novel bootstrapping technique is applied to EnDEM to balance between bias\nand variance. We evaluate EnDEM and BEnDEM on a 2-dimensional 40 Gaus-\nsian Mixture Model (GMM) and a 4-particle double-welling potential (DW-4).\nThe experimental results demonstrate that BEnDEM can achieve state-of-the-art\nperformance while being more robust.", "sections": [{"title": "1 Introduction", "content": "Given energy functions E(x), samplers for unnormalized target distributions i.e. Boltzmann distribu-\ntion target x exp(-E(x)) are fundamental in probabilistic modeling and physical systems simulation.\nFor example, predicting the folding of proteins could be formalized as sampling from a Boltzmann\ndistribution (\u015aled\u017a and Caflisch, 2018), where the energies are defined by inter-atomic forces (Case\net al., 2021). Efficient samplers for many-particle systems could have the potential to speed up drug\ndiscovery (Zheng et al., 2024) and material design (Komanduri et al., 2000).\nHowever, most related works face challenges with scalability to high dimensions and are time-\nconsuming. To solve these problems, a previous work(Akhound-Sadegh et al., 2024) proposes\nIterated Denoising Energy Matching (iDEM), which is not only computationally tractable but also\nguarantees good coverage of all modes. iDEM proposes a bi-level training scheme that iteratively\ngenerates samples from the learned sampler and performs score matching to the Monte Carlo\nestimated target. While Woo and Ahn (2024) proposes a variant, iEFM, that targets the MC estimated\nvector fields in a Flow Matching fashion, which we found can be linked through Tweedie's formula\n(Efron, 2011) shown in Appendix G. Nevertheless, both iDEM and iEFM require large numbers\nof MC samplings for score estimation to minimize the variance at large time steps, which holds\ndisadvantages for complicated energy functions.\nIn this work, we propose an energy-based variant of DEM, ENERGY-BASED DEM (EnDEM), which\ntargets the MC estimated noised energies instead of scores. Despite a need to differentiate the\nneural network when simulating the diffusion sampler, EnDEM is theoretically found to be targeting\nless noisy objectives compared with iDEM. Our method demonstrates better performance on a\n2-dimensional 40 Gaussian Mixture Model (GMM) and a 4-particle double-welling potential (DW-4).\nTo further reduce the variance, BOOTSTRAP ENDEM (BEnDEM) is proposed in our work, which"}, {"title": "2 Related works", "content": "Conventional methods for many-body system simulation are based on molecular dynamics\n(MD) Leimkuhler and Matthews (2012) techniques which require long simulation times. Others\nleverage Monte Carlo techniques e.g. Annealed Importance Sampling (AIS) (Lyman and Zuckerman,\n2007), which are computationally expensive. To speed up this process, a promising remedy is training\na deep generative model, i.e. a neural sampler. Unlike data-driven tasks, simply minimizing the\nreverse Kullback-Leibler (KL) divergence for samplers can lead to mode-seeking behavior. No\u00e9 et al.\n(2019) address this problem by using the Boltzmann Generator, which minimizes the combination of\nforward and reverse KL divergence.\nInspired by the rapid development of deep generative models, e.g. diffusion models (Song and Ermon,\n2019; Ho et al., 2020), pseudo samples could be generated from an arbitrary prior distribution. Then,\nwe can train the neural samplers by matching these sample trajectories, as in Path Integral Sampler\n(PIS)(Zhang and Chen, 2022) and Denoising Diffusion Sampler (DDS) (Vargas et al., 2023). Midgley\net al. (2023) further deploy a replay buffer for the trajectories while proposing an a-divergence as the\nobjective to avoid mode-seeking. However, these methods require simulation during training, which\nstill poses challenges for scaling up to higher dimensional tasks."}, {"title": "3 Methods", "content": null}, {"title": "3.1 Denoising diffusion based Boltzmann sampler", "content": "In this work, we consider training an energy-based diffusion sampler corresponding to a variance\nexploding (VE) noising process defined by dxt = g(t)dwt, where t \u2208 [0, 1], g(t) is a function of\ntime and wt is a Brownian motion. Then the corresponding reverse SDE with Brownian motion Wt is\ndxt = -g\u00b2(t) log pt(xt)dt+g(t)d\u016bt, where pt is the marginal distribution of the diffusion process\nthat starts at po = ptarget. Given access to the system energy E(x) and the perturbation kernel\nqt(xt|xo) = N(xt; xo, \u03c37), where exp(-E(x)) x po(x), one can obtain the marginal distribution pt\nPt(xt) x \\int exp(-E(x_0))N(x_t; x_0, \\sigma_t^2)dx_0 = E_{N(x_t; x_0, \\sigma_t^2)} [exp(-E(x))]\n(1)\niDEM is proposed to train a score network se to match an MC score estimator SK\nS_K (x_t, t) := \\nabla_{x_t} \\text{log} \\frac{1}{K}\\sum_{i=1}^K exp(-E(x_0^{(i)})), x_0^{(i)} \\sim N(x; x_t, \\sigma_t^2)\n(2)\nInstead of the score network, here we consider training an energy network, Ee(x, t), to approximate\nthe following noised energy, i.e. the energy of noised data\nE_t(x_t) := - \\text{log} E_{N(x;x_t,\\sigma_t^2)} [exp(-E(x))]\n(3)\nwhere exp(-Et(xt)) x pt(xt). Then the score of marginal distribution at t can be approximated by\ndifferentiating the energy network w.r.t its input x, i.e. se(x, t) = \u2212\u2207xEo(x,t). We train EnDEM\nwith a bi-level scheme by iterating: (a) an outer-loop that simulates the energy-based diffusion\nsampler and updates the replay buffer; (b) a simulation-free inner-loop that matches the noised\nenergies of samples in the replay buffer. Detailed implementation is discussed in Appendix A."}, {"title": "3.2 Energy-based Iterated Denoising Energy Matching", "content": "EnDEM targets a novel MC energy estimator that approximates the noised energies\nEK(xt,t) : = -log 1/K \\sum_{i=1}^K exp(-E(x_{0}^{(i)})), x_0^{(i)} \\sim N(x;x_t, \\sigma^2 I)\n(4)\nwhere Equation 4 can be implemented by the LogSumExp trick for stability. We characterize the bias\nof Ek with the following proposition:"}, {"title": "Proposition 1", "content": "If exp(-E(x)) is sub-Gaussian, then there exists a constant c(xt) such that with\n(i)\nprobability 1 \u2013 8 (over x ~ N(xt, \u03c3\u03c4)) we have\n||\u0395\u03ba (xt,t) \u2013 Et(xt) || \u2264 \\frac{c(x_t)\\sqrt{\\text{log} \\frac{1}{\\delta}}}{\\sqrt{K}}\n(5)\nwith c(xt)/c(xt) = 2(1 + ||\u2207Et(xt)||), where ||SK(xt,t) \u2013 St(xt)|| \u2264 \\frac{c(x_t)\\sqrt{\\text{log}(1/\\delta)}}{\\sqrt{K}}.\nProposition 1 shows that the training target of EnDEM has a smaller bound of bias compared with\nthe one of DEM, especially on regions with steep gradient, i.e. large ||\u2207Et(x))||. The complete proof\nis given in Appendix B. Besides, we characterize the variance of SK and Ek as follows:"}, {"title": "Proposition 2", "content": "If exp(-E(x_t^{(i)})) is sub-Gaussian and ||\u2207 exp(-E(x_t^{(i)}))|| is bounded, the total\nvariance of the MC score estimator SK is consistently larger than that of the MC energy estimator\nEk in low-energy regions, with\n\\frac{\\text{tr} (\\text{Cov}[S_K(x_t, t)])}{\\text{Var}[E_K (x_t, t)]} = 4(1 + ||\\nabla E_t(x_t)||)^2\n(6)\nUnder certain assumption, we can extend that Var[EK(xt,t)] < Var[SK(xt,t)] holds on high-energy\nregions.\nIt demonstrates that the MC energy estimator can provide a less noisy training signal than the score\none, showcasing the theoretical advantage of EnDEM compared with DEM. The complete proof is\nprovided in Appendix C."}, {"title": "3.3 Bootstrap EnDEM: an improvement with bootstrapped energy estimation", "content": "Bootstrap EnDEM, or BEnDEM, targets a novel MC energy estimator at a high noise level that\nis bootstrapped from the learned energies at a slightly lower noise level. Intuitively, the variance\nof Ek exploded at a high noise level as a result of the VE noising process; while we can estimate\nthese energies from the ones at a low noise level rather than the system energy to reduce variance.\nSuppose a low-level noise-convolved energy is well learned, say Es, we can construct a bootstrap\nenergy estimator at higher noise level t by\nE_K (x_t, t, s; \\phi) := -\\text{log} \\frac{1}{K} \\sum_{i=1}^K exp(-E_\\phi(x_{t,s}^{(i)})), x_{st}^{(i)} \\sim N (x; x_s, (\\sigma_t^2 - \\sigma_s^2)I)\n(7)\nwhere s < t and 4 a well learned neural network for any u \u2208 [0, s]. We show that this bootstrap\nenergy estimator is trading variance to bias, in terms of training target, characterized in the following\nproposition:"}, {"title": "Proposition 3", "content": "Given a bootstrap trajectory {si}=0 such that \\sigma_{s_i}^2 - \\sigma_{s_{i-1}}^2 \\leq \\kappa, where so = 0,\nSn = s and \u043a > 0 is a small number. Suppose E\u0219 is converged to the estimator in Eq. 4 at [0, s]. Let\nVy z(xz) = VarN(xz, (o2-04)1) [exp(-E(x))] and mz(xz) = exp(-Ez(xz)), where 0 < y < z < 1.\nThe variance of the bootstrap energy estimator is given by\n\\text{Var}[E_K (x_t, t, s; \\phi)] = \\frac{V_{st}(x_t)}{V_{ot}(x_t)}\\text{Var}[E_K (x_t,t)]\n(8)\nand the bias of learning Ex(xt, t, s; \u00a2) is given by\n\\frac{V_{ot}(x_t)}{2m^2(x_t)K^{n+1}} + \\sum_{j=1}^n\\frac{V_{os_j}(x_t)}{2m^2(x_t)K^j}\n(9)\nA detailed discussion and proof are given in Appendix D. Proposition 3 demonstrates that the\nbootstrap energy estimator, which estimates the noised energies by sampling in a smaller ball, can\nreduce the variance of the training target while this new target can introduce accumulated bias."}, {"title": "4 Experiments", "content": "We evaluate our methods and baseline models on 2 potentials, the Gaussian mixture model (GMM)\nand the 4-particle double-well (DW-4) potential. We provide the full description of the experimental\nsetting in Appendix H."}, {"title": "5 Conclusion", "content": "In this work, we introduce EnDEM and BEnDEM, neural samplers for Boltzmann distribution and\nequilibrium systems like many-body systems. EnDEM uses a novel Monte Carlo energy estimator\nwith reduced bias and variance. BEnDEM builds on EnDEM, employing an energy estimator boot-\nstrapped from lower noise-level data, theoretically trading bias for variance. Empirically, BEnDEM\nachieves state-of-the-art results on the GMM and DW4 benchmarks. Future work will focus on\nscaling these methods to higher-dimensional tasks like Lennard-Jones potential."}, {"title": "A Training Details", "content": "Algorithm 1 describes EnDEM in detail. The key difference in training the BEnDEM is only in the\ninner-loop.\nTo train the BEnDEM, it's crucial that the bootstrap energy estimation at t can be accurate only when\nthe noised energy at s is well-learned. Therefore, we need to sequentially learn energy at small t\nusing the original MC energy estimator then refines the estimator using bootstrap for large t, which\nis inefficient. To be more efficient, we adapt a rejection training scheme: (a) given s and t, we first\ncompute the loss of targeting the MC energy estimator (4), ls and lt; (b) These losses indicate how\ndoes the energy network fit the noised energies at different times, and therefore compute a = ls/lt as\nan indicator; (c) with probability a, we accept targeting a energy estimator bootstrapped from s and\notherwise we stick to target the original MC estimator. We provide a full description of the inner-loop\nof BEnDEM training in Algorithm 2."}, {"title": "B Proof of Proposition 1", "content": "Proposition 1 If exp(-E(x_t^{(i)})) is sub-Gaussian, then there exists a constant c(xt) such that with\n(i)\nprobability 1 \u2013 8 (over x ~ N(xt, \u03c3\u03c4)) we have\n||EK(xt,t) \u2013 Et (xt)|| \u2264 \\frac{\\tilde{c}(x_t)\\sqrt{\\text{log} \\frac{1}{\\delta}}}{\\sqrt{K}}\n(10)\nwith c(xt)/c(xt) = 2(1 + ||\u2207Et(xt)||), where ||SK(xt,t) \u2013 St(xt)|| \u2264 \\frac{c(x_t)\\sqrt{\\text{log} \\frac{1}{\\delta}}}{\\sqrt{K}}.\nProof. We first introduce the error bound of the MC score estimator SK, where SK = VEK,\nproposed by Akhound-Sadegh et al. (2024) as follows\n||SK(xt, t) - S(xt,t)|| \u2264 \\frac{2C_1\\sqrt{\\text{log}(\\frac{1}{\\delta})}(1 + ||\\nabla E_t(x_t)||) \\text{exp}(E_t(x_t))}{\\sqrt{K}}\n(11)\nwhich assumes that exp(-E(x)) is sub-Gaussian. Let's define the following variables\nmt(xt) = exp(-Et(xt))\n(12)\nUt(xt) = \\text{Var}_{N(x_t,(0.3-0.2)I)}[exp(-E(x))]\n(13)\nBy the sub-Gaussianess assumption, it's easy to show that the constant term C in Equation 11 is\nC = \u221a2vot(xt). Notice that Ek is a logarithm of an unbiased estimator . By the sub-Gaussian\nassumption, one can derive that Ek is also sub-Gaussian. Furthermore, it's mean and variance can\nbe derived by employing a first-order Taylor expansion:\nE[E_K(x_t,t) \\approx E_t(x_t) + \\frac{V_{ot}(x_t)}{2m^2(x_t)K}\n(14)\n\\text{Var} [E_K(x_t,t) \\approx \\frac{V_{ot}(x_t)}{2m^2(x_t) K}\n(15)\nAnd one can obtain its concentration inequality by incorporating the sub-Gaussianess\n||E_K(x_t,t) - E[E_K(x_t,t)] || \\leq \\sqrt{\\frac{V_{ot}(x_t)}{m_t^2(x_t) K}} \\sqrt{\\frac{2}{\\kappa}} \\text{log} \\frac{2}{\\delta}\n(16)"}, {"title": "Using the above Inequality", "content": "Using the above Inequality 16 and the triangle inequality\n||EK(xt, t) \u2013 Et(xt)|| \u2264 || log EK(xt,t) \u2013 E[Ex(xt,t)]|| + ||E[Ex(xt,t)] \u2013 Et(xt)||\n(17)\n= || log EK (xt, t) \u2013 E[EK(xt,t)]|| + \\frac{V_{ot}(x_t)}{2m^2(x_t) K}\n(18)\n\u2264 \\sqrt{\\frac{V_{ot}(x_t)}{m_t^2(x_t) K}} \\sqrt{\\frac{2}{\\kappa}} \\text{log} \\frac{2}{\\delta} + \\frac{V_{ot}(x_t)}{2m^2(x_t) K}\n(19)\n\u2264 C\\frac{\\sqrt{\\text{log} \\frac{2}{\\kappa}} \\text{exp}(E_t(x_t))}{\\sqrt{K}} + O(1/K)\n(20)\nc(xt) \\sqrt{\\text{log}}\\frac{1}{2(1+ ||\\nabla E_t(x_t)||) \\sqrt{K}}\n(21)\nTherefore, we have c(xt) = 2(1 + ||\u2207Et(xt)||)\u010d(xt). It demonstrates a less biased estimator, which,\nwhat's more, doesn't require a sub-Gaussianess assumption over ||\u2207E(xot)||."}, {"title": "CProof of Proposition 2", "content": "Proposition 2 If exp(-E(x_t^{(i)})) is sub-Gaussian and ||\u2207 exp(-E(x_t^{(i)}))|| is bounded, the total\nvariance of the MC score estimator SK is consistently larger than that of the MC energy estimator\nEk in low-energy regions, with\n\\frac{\\text{tr} (\\text{Cov}[S_K (x_t, t)])}{\\text{Var}[E_K (x_t, t)]} = 4(1 + ||\\nabla E_t(x_t)||)^2\n(22)\nUnder certain assumption, we can extend that Var[EK(xt,t)] < Var[SK(xt,t)] holds on high-energy\nregions.\nProof. We split the proof into two parts: low-energy region and high-energy one. The proof in\nthe low-energy region requires only the aforementioned sub-Gaussianess and bounded assumptions,\nwhile the one in the high-energy region requires an additional constraint which will be clarified later.\nReview that SK can be expressed as an importance-weighted estimator as follows:\nS_K (x_t,t) = \\frac{\\frac{1}{K}\\sum_{i=1}^K \\nabla exp(-E(x_t^{(i)}))}{\\frac{1}{K}\\sum_{i=1}^K exp(-E(x_t^{(i)}))}\n(23)\nLet || exp(-E(x_t)) || \u2264 M, where M > 0. Since a bounded variable is sub-Gaussian, this\nassumption resembles a sub-Gaussianess assumption of ||\u2207 exp(-E(x))||. x(i)))||. Then each element of\n(x(i)))[j], is bounded by M. And therefore \u2207 exp(-E(\n||\u2207exp(-E(x))||, i.e. \u2207 exp(-E(xt)\nexp(-E(x))[j]\nare sub-Gaussian.\nIn low-energy regions. exp(-E(x)) is concentrated away from 0 as E(x) is small. Then, there\nexists a constant c such that exp(-E(x)) > c > 0 and thus for each element j = 1, .., d:\n||S_K(x_t,t)[j]|| = \\frac{\\frac{1}{K}\\sum_{i=1}^K \\nabla exp(-E(x))[j]}{\\frac{1}{K}\\sum_{i=1}^K exp(-E(x))}\n(24)\n\\leq \\frac{||\\frac{1}{K}\\sum_{i=1}^K \\nabla exp(-E(x))[i]||}{Kc} \\leq M/c\n(25)\ntherefore, the jth element of Sk, i.e.SK [j], is bounded by M/c, suggesting it is sub-Gaussian. While\nInequality 11 can be expressed as\n\\sum_{j=1}^d (S_K(x_t,t) [j] - S_t(x_t)[j])^2 \\leq \\frac{2\\sqrt{2}V_{ot}(x_t)log(\\frac{3}{\\delta})(1 + ||\\nabla E_t(x_t)||)}{m_t(x_t)\\sqrt{K}}\n(26)"}, {"title": "We can roughly derive a bound elementwisely", "content": "We can roughly derive a bound elementwisely\n|SK(xt,t)[j] - St(xt)[j]| \u2264 \\frac{2\\sqrt{2}V_{ot}(x_t)log(\\frac{3}{\\delta})(1 + ||\\nabla E_t(x_t)||)}{m_t(x_t)\\sqrt{Kd}}\n(27)\nwhich suggests that we can approximate the variance of SK (xt, t)[j] by leveraging its sub-Gaussianess\n\\text{Var}(SK(xt,t)[j]) \\approx \\frac{4V_{ot}(x_t)(1 + ||\\nabla E_t(x_t)||)^2}{m_t^2(x_t)Kd}\n(28)\nTherefore, according to Equation 15 we can derive that\n\\text{tr}(\\text{Cov}[S_K (x,t)]) = \\sum_{j=1}^d \\text{Var}[S_K (x, t)[j]]\n(29)\n= \\sum_j \\frac{4V_{ot}(x_t)(1 + ||\\nabla E_t(x_t)||)^2}{m_t^2(x_t)K}\n(30)\n= 4(1 + ||\\nabla E_t(x_t)||)^2\\text{Var}[E_K(x_t, t)]\n(31)\nIn high-energy region. we assume that there exists a direction with a large norm pointing to low\nenergy regions, i.e. \u2203j such that E(x) are positively related to \u2207E(x)[j]. According to Section 9.2 in\nOwen (2023), the asymptotic variance of a self-normalized importance sampling estimator is given\nby:\n\u03bc = Eq[f(x)]\n(32)\n\\mu_q = \\frac{\\sum_{i=1}^K W_i f_i}{\\sum_{i=1}^K W_i}\n(33)\n\\text{Var}(\\mu_q) \\approx \\frac{1}{K}=E_q[w(X)]^{-2}E_q[w(X)^2(f(X) - \\mu)^2]\n(34)\nBy substituting \u00b5q = SK(xt,t)[j], f(X) = \u2212\u2207E(X)[j], w(X) = exp(\u2212E(X)), q = N(x; xt, \u03c3\uafbf),\nEq[w(X)] = mt(xt) and Eq[w\u00b2(X)] = vot(xt) + m\u00b2(xt), as well as using w(X) and f(X) are\npositive related, we have:\n\\text{Var}[S_K(x_t, t)[j]] \\geq \\frac{1}{K}E_q[w(X)]^{-2}E_q[w^2(X)]E_q[(f(X) - \\mu)^2]\n(35)\n= \\frac{V_{ot}(x_t) + m^2(x_t)}{m^2(x_t)K} \\text{Var}_q[\\nabla E(x)[j]]\n(36)\n\u2265 \\frac{V_{ot}(x_t)}{m^2(x_t)K} \\text{Var}_q[\\nabla E(x)[j]]\n(37)\nTherefore, if we further have a large variance over the system score at this region, i.e.\nVarq[\u2207E(x)[j]] > 1, then we have\n\\text{Var}[S_K (x_t,t)[j]] > \\frac{V_{ot}(x_t)}{m^2(x_t) K} = \\text{Var}[E_K (x_t, t)]\n(38)\nand thus tr(Cov[SK(xt,t])) > Var[EK(xt,t)] holds.\nD Proof of Proposition 3\nProposition 3 Given a bootstrap trajectory {si}=0 such that \\sigma_{s_i}^2 - \\sigma_{s_{i-1}}^2 \\leq \\kappa, where so = 0,\nSn = s and \u043a > 0 is a small number. Suppose E\u0219 is converged to the estimator in Eq. 4 at [0, s]. Let\nVy z(xz) = VarN(xz,(02-04)1) [exp(-E(x))] and mz(xz) = exp(-Ez(xz)), where 0 \u2264 y < z < 1.\nThe variance of the bootstrap energy estimator is given by\n\\text{Var}[E_K (x_t, t, s; \\phi)] = \\frac{V_{st}(x_t)}{V_{ot}(x_t)}\\text{Var}[E_K (x_t, t)]\n(39)\nand the bias of learning Ex(xt, t, s; \u00a2) is given by\n\\frac{V_{ot}(x_t)}{2m^2(x_t)K^{n+1}} + \\sum_{j=1}^n\\frac{V_{os_j}(x_t)}{2m^2(x_t)K^j}\n(40)"}, {"title": "Proof", "content": "Proof. The variance of EK(xt, t, s; $) can be simply derived by leveraging the variance of a sub-\nGaussian random variable similar to Equation 15. While the entire proof for bias of Ek (xt, t, s; \u00a2) is\norganized as follows:\n1. we first show the bias of Bootstrap(1) estimator, which is bootstrapped from the system\nenergy\n2. we then show the bias of Bootstrap(n) estimator, which is bootstrapped from a lower level\nnoise convolved energy recursively, by induction.\nD.1 Bootstrap(1) estimator\nThe Sequential estimator and Bootstrap(1) estimator are defined by:\nE_K^{Seq} (x_t, t): = -\\text{log} \\frac{1}{K} \\sum_{i=1}^K exp(-E_K (x_{s/t}^{(i)})), x_{s/t}^{(i)} \\sim N (x;x_t, (\\sigma_t^2 - \\sigma_s^2)I)\n(41)\nEK(xt, t): = -log 1/(K^2) \u03a3\u03a3 exp(-E(x)), x ~ N(x; xt, \u03c3I)\n(42)\nER(1) (xt, t, s; d) : = -log 1/K \u03a3 exp(-E(x)), x ~ N(x;xt, (o? \u2013 3)1)\n(43)\nThe mean and variance of a Sequential estimator can be derived by considering it as the MC estimator\nwith K\u00b2 samples:\nE[EK(xt,t)] = Et(xt) + \\frac{V_{ot}(x_t)}{2m_{ot}(x_t) K^2} and Var(Esed(x, t)) = \\frac{V_{ot}(x_t)}{m_{ot}(x_t)K^2}\n(44)\nWhile an optimal network obtained by targeting the original MC energy estimator 4 at s is 1 :\nEp* (xs, S) = E[EK(xs, s)] = logms(xs) + \\frac{V_{os} (X_s)}{2m^2(x_s) K}\n(45)\nThen the optimal Bootstrap(1) estimator can be expressed as:\nEB(1) (xt, t, s; $*) = -\\text{log} \\frac{1}{K} \\sum_{i=1}^K exp(-\\text{log }m_s(x_{st}^{(i)}) + \\frac{V_{os} (X_{st}^{(i)})}{2m^2(x_{st}^{(i)})K}\n(46)\nBefore linking the Bootstrap estimator and the Sequential one, we provide the following approxima-\ntion which is useful. Let a, b two random variables and {ai}1, {bi}1 are corresponding samples.\nAssume that {b}{K\u2081 are close to 0 and concentrated at m\u044c, while {ai}{K\u2081 are concentrated at ma,"}, {"title": "then", "content": "then\n\\frac{1"}, {"follows": "nEB(1) (xt", "44)": "nE[EB(1) (xt, t, s; $*)"}]}