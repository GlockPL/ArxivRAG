{"title": "Link-aware link prediction over temporal graph by pattern recognition", "authors": ["Bingqing Liu", "Xikun Huang"], "abstract": "A temporal graph can be considered as a stream of links, each of which represents an interaction between two nodes at a certain time. On temporal graphs, link prediction is a common task, which aims to answer whether the query link is true or not. To do this task, previous methods usually focus on the learning of representations of the two nodes in the query link. We point out that the learned representation by their models may encode too much information with side effects for link prediction because they have not utilized the information of the query link, i.e., they are link-unaware. Based on this observation, we propose a link-aware model: historical links and the query link are input together into the following model layers to distinguish whether this input implies a reasonable pattern that ends with the query link. During this process, we focus on the modeling of link evolution patterns rather than node representations. Experiments on six datasets show that our model achieves strong performances compared with state-of-the-art baselines, and the results of link prediction are interpretable. The code and datasets are available on the project website: https://github.com/lbq8942/TGACN.", "sections": [{"title": "Introduction", "content": "Temporal graphs are powerful mathematical abstractions to describe complex dynamic networks and have a wide range of applications in various areas of network science, such as citation networks, communication networks, social networks, biological networks, and the World Wide Web [4]. In temporal graph, there are many insightful patterns (usually refers to small and induced temporal subgraph, which is also called motif or graphlet [13]) which summarize the evolution laws of links. To evaluate whether our model captures these patterns, link prediction is a widely used task [9], which is defined in dynamic graphs as, given all historical links before t, determine whether link $e = (s, o)$ will happen at timestamp t, where s and o are the source and destination nodes respectively. In previous studies, a standard paradigm of link prediction is centered on"}, {"title": "Related Work", "content": "Prior work for link prediction over temporal graph mainly focuses on the learning of nodes' representation [12,1]. Early models are mainly snapshot-based methods, which learns temporal graphs in discrete time space. This kind of method divides the temporal graph into snapshots where links in one snapshot are assumed to take place at the same time and links in different snapshots still maintain the chronological order [11,8,5]. Within snapshots, they usually utilize static graph neural networks (GNN) [6,17,2] to encode structural features. Between snapshots, they use RNNs or Transformer [16] to model the temporal dynamics. The main drawback of these approaches is that they need to predetermine a time granularity for snapshot partition and thus hard to learn structural and temporal dynamics in different time scales.\nLearning on temporal graphs in continuous time space recently gained attention. These methods can be broadly classified into two categories, streaming methods and non-streaming methods. Streaming methods maintain a chronologically changing table for representations of all nodes and chronologically predict the links. Each time a new link appears, they use RNNs or Transformer to update that table, i.e., update representations of the source and destination node (and their neighbors) in that new link [7,15,10,18]. With the updated representations, they can predict the future link. Given a query link, streaming methods need to first digest all the previous historical links, while non-streaming methods do not [19,21,20]. It samples only a few historical links and uses GNN or Transformer to aggregate their information to obtain the representations of the two nodes in the query link. Commonly used sampling techniques include nearest sampling [10] and probabilistic sampling [19]."}, {"title": "THE TGACN MODEL", "content": "In this section, we introduce the proposed model, Temporal Graph Attention Convolution Network (TGACN). We first formally give the problem formulation and notations of the link prediction task over a temporal graph, then we introduce our model, which includes three parts: sampling, attention, and pattern recognition."}, {"title": "Problem Formulation and Notations", "content": "A temporal graph can be represented as a stream of links that come in over time, i.e., $E = {e_1, ..., e_i, ...}$, where link $e_i$ is defined as a triplet $(s_i, o_i, t_i)$. Link $e_i$ shows that source node $s_i$ interacted with destination node $o_i$ at timestamp $t_i$. Link prediction requires our model to distinguish the ground truth triplet $(s_g, o_g,t_g)$ and the corrupted triplet $(s_g, o_{neg},t_g)$, where $o_{neg}$ is sampled from the nodes set V. That is, link prediction can be described as: given historical links $e_h = (s_h, o_h,t_h)$, $t_h < t_q$, tell whether query link $e_q = (s_q, o_q, t_q)$ is a ground truth or not."}, {"title": "Method", "content": "Sampling. Given a query link, we first need sampling due to the large scale of historical links. Without domain knowledge, a widely used sampling technique is neighborhood sampling [17,21], i.e., links that are close to the query link should be sampled with higher priority. However, nearest sampling alone may sometimes fail, for example, in Figure 3, when we sample N = 3 nearest links, the target link (A,B,1) will be missed and it is not ideal to simply increase the sampling length because it will introduce more noisy links. Based on this consideration, for nearest sampling, we still keep N small and propose parametric sampling for those useful but more distant links. Parametric sampling locates valuable historical link $e_n$ by computing its \"closeness\" to the query link $e_q$ as follows:\n$closeness(e_q, e_n) = e_q \\circ e_n$                                   (1)\nIn which, $\\circ$ denotes the dot product between two vectors, $e_q$ and $e_n$ are computed as follows:\n$e_q = \\phi(t_q) + H(s_q) + H(o_q)$                                        (2)\n$e_n = \\phi(t_q-t_h) + H(s_h) + H(o_n)$                                        (3)\n$\\phi(t) = [cos(\\omega_1t + b_1), ..., cos(\\omega_at + b_a)]$                 (4)\nWhere $\\phi(\\cdot) \\in R^d$ is a time encoding proposed in TGAT [21] and $[\\omega_1,b_1, ..., \\omega_a, b_a] \\in R^{2d}$ are all trainable parameters. We calculate the \"closeness\" of M (usually much larger than N) nearest historical links, and select P (usually very small) links with the largest \"closeness\". Note that despite its potential of recalling useful links that are distant, parametric sampling practically consumes more time than nearest sampling, so we need a trade-off to achieve the best performance.\nBy nearest sampling and parametric sampling, we sampled a total of N + P historical links, which with the query event together form our input with l = (N + P + 1) links $ES = {e^1, ..., e^l}$.\nAttention. In order to determine whether there is a reasonable pattern that ends with the query link in the input links, we first need to encode these input links. During the encoding, we should provide enough convenience for the query link to directly check the target links while preserving as much of the original information as possible. In this paper, the attention mechanism is utilized, specifically includes transductive attention and inductive attention. Compared with traditional attention mechanism [16], we differ in two ways. First, the traditional attention mechanism is based on vector representation and dot-product, while inductive attention is not. Second, we use only the attention"}, {"title": "Transductive Attention", "content": "Transductive Attention. Transductive attention tries to extract the pattern information from the representation of input links. Since it is representation-based, the attention values carry rich information of nodes and edges, and thus can be used to capture fine-grained patterns. We first calculate the vector representations for each link in the input links by eq 2 eq 4. As a result, we get $ES(e) \\in R^{l\\times d}$. Then we compute the attention values between every two input links using dot product, i.e., we get\n$\\begin{bmatrix}\ne^1\\circ e^1 & ... & e^1\\circ e^l \\\\\n... & ... & ...\\\\\ne^l\\circ e^1 & ... & e^l \\circ e^l\n\\end{bmatrix}^{lxl}$\nWe denote the above attention result as $channel(e) \\in R^{lxl}$"}, {"title": "Inductive Attention", "content": "Inductive Attention. Inductive attention operates directly on node identities and link timestamps rather than vector representations. The goal of this attention is to remove node identities while still preserving the structure of the pattern so that they can be generalized to nodes and links that have not been seen before (see Figure 2(c)). In order to do this, new attention functions are required.\nThe timestamps of the input links ES(t) not only reflect the order of occurrence of the links, but the time interval itself carries a wealth of information. To encode this, we use the following attention functions:\n$attn_1(x, y) = exp(-a|x - y|)$                  (5)\nwhere time decaying coefficient $a > 0$ is a hyperparameter. We first pair the elements in ES(t) and then the above attention function is utilized, i.e., the attention result is:\n$\\begin{bmatrix}\nattn_1(t^1, t^1) & ... & attn_1(t^1, t^l) \\\\\n... & ... & ...\\\\\nattn_1(t^l, t^1) & ... & attn_1(t^l, t^l)\n\\end{bmatrix}^{lxl}$\nWe denote the above attention result as $channel(t) \\in R^{lxl}$.\nTo remove the node identities while still maintain the original pattern topology, we propose to use the following attention function:\n$attn_2(x, y) = \\begin{cases}\n1 & x = y \\\\\n0 & else\n\\end{cases}$                                      (6)\nLike handling ES(t) with eq 5, we handle ES(s) and ES(o) with eq 6. Similarly, we can get channel(s) $\\in R^{l\\times l}$ and channel(o) $\\in R^{l\\times l}$. Note that ES(s) and ES(o) may share the same nodes, while this kind of information is not yet reflected in the above two channels ES(s) and ES(o). To reserve this information,"}, {"title": "Pattern Recognition", "content": "Given the \"image\", we decide in this part whether it contains a reasonable pattern that ends with the query link. Like in computer vision, a convolutional neural network (CNN) is utilized to carry out this pattern recognition. Here, we directly use the existing CNN architecture, which has already achieved excellent performance in the image classification task. In this paper, EfficientNetV2-S [14] is selected. Since our \"image\"($5 \\times 1 \\times l$) (l is usually less than 20) is much smaller than images (3 \u00d7 224 \u00d7 224) in the real world, without harming the performance, we further simplify EfficientNetV2-S as EffNet by simply removing the last few stages and tune the number of layers in the left stages. The architecture of EffNet is shown in Table 1. Besides, to be explainable, a technique called Class Activation Mapping (CAM) [22] is utilized, which is able to identify the discriminative regions. For the architecture, CAM requests the utilization of the global average pooling layer before the last classification layer, which is already satisfied by EffNet.\nTo sum up, to be link-aware, TGACN makes use of the query link at two points. First and most importantly, the sampled historical links and the query link are put together as input. Under the instruction of the query link, it is possible for subsequent model layers to directly check the target links. Secondly, we use the vector representation of the query link to conduct parametric sampling, so as to recall historical links that may help to predict the query link."}, {"title": "Experiment", "content": "Datasets. We evaluate our method on six widely used datasets, UCI [19], Social Evolution [19,15], Enron [11,19], Wikipedia [7], Lastfm [7] and MOOC [7]. UCI is a network between online posts made by students, Social Evolution is a network recording the physical proximity between students. Enron is an email"}, {"title": "Results", "content": "In this part, we report the performance of link prediction, which can be seen in Table 3. By comparing with the memory-based approach Edgebank, we find that there are some models with worse performance, suggesting that these models are actually not learned well for link prediction (similar conclusion with [9]). Results show that we outperform all baselines consistently and significantly on six datasets. We attribute it to that the proposed model is link-aware, thus more"}, {"title": "Ablation Study", "content": "Impact of the sampling method. We explore this by using only the nearest sampling or parametric sampling. As shown in Table 6, generally speaking, these two kind of removals both see a decline in performance, which verifies the effectiveness of our proposed parametric sampling. Impact of two kinds of attention. Table 6 shows that remove transductive or inductive attention both brings about a decline in model performance, especially for MOOC, which demonstrates the importance of learning patterns of different granularity. Impact of the CNN architecture. We first replace EffNet with ResNet18 [3] and EfficientNetV2-S [14] respectively. Table 6 shows the robustness of the architecture of CNN, we can see that comparable results are obtained but our EffNet has much fewer parameters. Second, we further simplify EffNet by removing stage 1 and 2, with only a few convolutional layers left. The results show that performance drops but not much. A reasonable conjecture is that the attention values have already encoded high-level information and thus do not require a very deep neural network to extract. This validates the effectiveness of our proposed attention mechanism for link prediction."}, {"title": "Hyperparameter Investigation", "content": "In this part, we investigate how nearest sampling length N, parametric sampling length P and time decaying coefficient a in eq 5 affect the model performance. As shown in Table 7, generally speaking, model's performance increases with length N and then stabilizes, similar observation can be made for parametric sampling length P. Table 7 also shows that the proposed model is robust with respect to time decaying coefficient a for most of the datasets."}, {"title": "Interpretability", "content": "In this part, we take the model learned from dataset UCI as an example to conduct a case study about interpretability. Interpretability requires us to get the impact of historical links on the query link. However, CAM only outputs an \"image\" with shape of 1 \u00d7 1 \u00d7 1, which marks the importance of each pixel. To translate this importance of pixels to that of links, we sum the values of ith row and column in that \"image\" as the importance of the ith historical link to the query link. Figure 4 shows three examples for link prediction and illustrates our interpretation for the results of link prediction, i.e., showing the importance of the historical links to the query link. From the heatmap in this Figure, we can see that the query link can directly notice the target links, which verifies the TGACN's effectiveness of being link-aware."}, {"title": "Conclusion", "content": "In this paper, we propose a model named Temporal Graph Attention Convolution Neural Network (TGACN), which is specially designed for link prediction on temporal graphs. TGACN is as far as we know the first link-aware method and predict the query link from the perspective of pattern recognition rather than learning the node representations. The empirical results demonstrated that the proposed model has achieved the best performance. We believe that our work provides an alternative, effective way for link prediction. In the future, we will explore more opportunities for our proposed method. A promising direction is to apply our method to the link prediction of static graph, we leave this for future work."}]}