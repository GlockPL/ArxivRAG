{"title": "Are Anomaly Scores Telling the Whole Story? A Benchmark for Multilevel\nAnomaly Detection", "authors": ["Tri Cao", "Minh-Huy Trinh", "Ailin Deng", "Quoc-Nam Nguyen", "Khoa Duong", "Ngai-Man Cheung", "Bryan Hooi"], "abstract": "Anomaly detection (AD) is a machine learning task that\nidentifies anomalies by learning patterns from normal train-\ning data. In many real-world scenarios, anomalies vary in\nseverity, from minor anomalies with little risk to severe ab-\nnormalities requiring immediate attention. However, exist-\ning models primarily operate in a binary setting, and the\nanomaly scores they produce are usually based on the de-\nvation of data points from normal data, which may not ac-\ncurately reflect practical severity. In this paper, we address\nthis gap by making three key contributions. First, we pro-\npose a novel setting, Multilevel AD (MAD), in which the\nanomaly score represents the severity of anomalies in real-\nworld applications, and we highlight its diverse applica-\ntions across various domains. Second, we introduce a novel\nbenchmark, MAD-Bench, that evaluates models not only on\ntheir ability to detect anomalies, but also on how effectively\ntheir anomaly scores reflect severity. This benchmark in-\ncorporates multiple types of baselines and real-world ap-\nplications involving severity. Finally, we conduct a compre-\nhensive performance analysis on MAD-Bench. We evaluate\nmodels on their ability to assign severity-aligned scores, in-\nvestigate the correspondence between their performance on\nbinary and multilevel detection, and study their robustness.\nThis analysis offers key insights into improving AD mod-\nels for practical severity alignment. The code framework\nand datasets used for the benchmark will be made publicly\navailable.", "sections": [{"title": "1. Introduction", "content": "Anomaly detection (AD) is a fundamental machine learning\ntask that identifies deviations from normal patterns, with\ncritical applications in domains such as one-class novelty\ndetection [9, 40], industrial inspection [5, 69], and medical\ndiagnostics [43, 47]. However, in many real-world scenar-\nios, anomalies are not uniformly significant; they exist on\na spectrum of severity, ranging from minor anomalies that\npose little risk to severe abnormalities that demand imme-\ndiate attention. In a practical context, severity refers to the\ndegree of potential impact or risk an anomaly poses to the\nsystem. For instance, in industrial inspection, as shown in\nFigure 1b, anomalies range from minor surface contamina-\ntions to severe tears, requiring different levels of response.\nThe ability to differentiate between these levels of severity\nis important for effective decision-making, prioritizing crit-\nical issues, and resource allocation.\nWhile distinguishing between different levels of\nanomaly severity is crucial, it remains unclear whether\ncurrent anomaly detection methods are truly effective in\ncapturing this distinction. Existing approaches typically\nfocus on ensuring that the anomaly scores of anomalous\ndata are higher than those of normal samples, operating\nprimarily in a binary setting. Current techniques, such as\nreconstruction-based [36, 38, 57, 60, 63], one-class classi-\nfiers [9, 16, 41, 55, 58], and knowledge distillation-based\nmodels [12, 42, 49, 53], rely on metrics like reconstruction\nerrors, distance measures, or likelihood estimates to flag\ndeviations from normal patterns. However, the anomaly\nscores produced by these models are typically based on\nhow far a data point deviates from normal data, but it is\nuncertain how well these scores correlate with the practical\nseverity of the anomalies.\nThese observations raise a key question: How accurately\ncan the anomaly scores measure practical severity? To the\nbest of our knowledge, there are no works that directly ad-\ndress this question. While near-distribution novelty detec-\ntion [34] is related, it focuses on distinguishing between\nnormal samples and abnormal samples that closely resem-\nble normal ones rather than investigating the relationship\nbetween anomaly scores and severity. In this paper, we aim\nto study the relationship between anomaly score and sever-\nity of anomalies by presenting three main contributions:\n\u2022 First, we propose a novel setting, Multilevel Anomaly\nDetection (MAD), where the anomaly score should rep-\nresent the severity of anomalies in real-world appli-"}, {"title": "2. Related Work", "content": "Traditional Approaches. Many approaches have been pro-\nposed for anomaly detection. One-class classification meth-\nods have progressed from machine learning approaches\n[45, 48] to deep learning-based techniques [9, 16, 32, 40,\n41, 55, 58], which improve boundary refinement for dis-\ntinguishing normal and anomalous data. Reconstruction-\nbased models [30, 38, 57, 60, 62-64] identify anomalies\nby measuring deviations in reconstruction errors. Addition-\nally, self-supervised learning techniques [28, 44, 57, 67]\nintroduce synthetic anomalies to enhance model differ-\nentiation, improving detection accuracy in complex sce-\nnarios. Knowledge distillation-based methods employ a\nteacher-student framework [6, 12, 17, 42, 49, 53, 66], where\nanomalies are detected based on deviations between teacher\nand student representations. Distribution map-based meth-\nods [18, 27, 46] model the normal data distribution and\nidentify outliers as deviations. Memory-based approaches\n[4, 15, 17, 23, 36, 39, 53] detect anomalies by comparing in-\nput features to representative normal data stored in a mem-\nory bank.\nMLLM-Based Approaches. MLLMs have recently\nbeen leveraged for anomaly detection due to their abil-\nity to handle multimodal reasoning. These approaches of-\nten utilize zero-shot learning [13, 24] or few-shot learning\n[29, 68], allowing them to adapt to new datasets without\nextensive task-specific training.\nAD Benchmarks. Several benchmarks for anomaly de-\ntection have been introduced [3, 8, 19, 24, 52, 56, 65].\nHowever, these benchmarks primarily focus on traditional\nbinary anomaly detection rather than addressing more nu-\nanced multilevel anomaly detection settings.\nNear out-of-distribution (OOD) detection aims to\nidentify anomalies that are similar to normal data, show-\ning only slight deviations [34]. This differs from our work,"}, {"title": "3. Multilevel Anomaly Detection", "content": "which explores the relationship between anomaly scores\nand varying levels of severity.\nMulti-class AD identifies anomalies across various ob-\nject classes within a single, unified framework [21, 33, 59,\n65]. By training on multiple normal classes, it identifies\nanomalies as instances deviating from any learned normal\npatterns. In contrast, Multilevel AD focuses on represent-\ning multiple levels of anomalies during inference and en-\nsures that anomaly scores reflect these levels."}, {"title": "3.1. Problem Formulation", "content": "In anomaly detection, the goal is to define a function $f$ :\nX \u2192 R that assigns an anomaly score $f(x)$ to each data\npoint $x \\in X$, where higher scores indicate a greater likeli-\nhood of being an anomaly. Traditional approaches typically\ntreat anomalies in a binary manner, detecting whether a\npoint is normal or anomalous. However, real-world anoma-\nlies often vary in severity rather than fitting into binary cate-\ngories. For example, in medical diagnostics, doctors assess\nabnormalities on a spectrum, from mild issues needing ob-\nservation to critical conditions requiring urgent care.\nMotivated by this, we propose a Multilevel Anomaly\nDetection (MAD) setting, where data points are from $L_0$\nto $L_n$, with $L_0$ representing the set of normal data and\n$L_1, L_2,..., L_n$ representing sets of increasingly severe lev-\nels of anomaly.\nLet the training set be defined as $D_{\\text{train}} \\subseteq L_0$, consisting\nexclusively of normal data points from $L_0$. The testing set\nis defined as $D_{\\text{test}} \\coloneqq L_0 \\cup L_1 \\cup ... \\cup L_n$, which includes\ndata from all levels $L_0, L_1,..., L_n$.\nThe goal is to design an anomaly scoring function\n$f(x)$ such that higher severity levels correspond to higher\nanomaly scores. Specifically, normal data points in $L_0$\nshould be assigned the lowest anomaly scores $f(x_0)$, while\ndata points from higher severity levels $L_1, L_2,..., L_n$\nshould receive correspondingly higher scores. This ensures\na consistent and interpretable mapping between anomaly\nseverity and anomaly scores."}, {"title": "3.2. Evaluation Protocol", "content": "In this section, we outline the evaluation protocol used to as-\nsess the performance of models on the multi-level anomaly\ndetection task. Three key metrics will be employed: AU-\nROC [20], C-index [51], and Kendall's Tau-b [25].\nThe AUROC metric is used to evaluate the model's\nability to distinguish between normal data and anomalies,\nwhich is commonly used in binary AD settings [8, 9].\nThe C-index is a generalization of the AUROC that can\nevaluate how well anomaly scores align with the severity"}, {"title": "3.3. Applications of Multilevel Anomaly Detection", "content": "levels. In the context of MAD, C-index is defined as:\n$C = \\frac{\\sum_{a=1}^{n} \\sum_{x_i \\in L_a} \\sum_{x_j \\in L_0} 1(f(x_i) > f(x_j))}{\\sum_{a=1}^{n} |L_a| \\cdot |L_0|}$\nSimilar to the AUROC, a C-index of 1 corresponds to\nthe best model prediction, while a C-index of 0.5 indicates\na random prediction. A high C-index score indicates that\nthe model correctly ranks higher-severity anomalies with\nhigher anomaly scores.\nTo further assess the alignment between predicted\nanomaly scores and severity levels, we employ Kendall's\nTau-b. Similar to C-index, Kendall's Tau-b evaluates the\nconsistency between anomaly scores and severity levels but\nis stricter, as it requires samples within the same sever-\nity level to have identical anomaly scores to ensure perfect\nagreement. Kendall's Tau ranges from [-1,1], with \u22121 in-\ndicating a perfectly incorrect ranking, 0 signifying no cor-\nrelation, and 1 representing a perfect severity ranking. The\nformula is detailed in the Appendix.\nMultilevel anomaly detection has significant implications\nacross real-world domains where distinguishing anomalies\nbased on severity is essential for effective decision-making.\nOne-class Novelty Detection identifies samples that de-\nviate from a defined normal class. For instance, a model\ntrained on Golden Retriever (a dog breed) images can cate-\ngorize anomalies by their deviation: Level 1 includes other\ndog breeds (minor deviation); Level 2, cats (moderate sim-\nilarity); Level 3, birds (significant structural differences);\nand Level 4, flowers (inanimate objects). This frame-\nwork enables one-class novelty detection to assign anomaly\nscores based on the degree of relevance to the trained class.\nMedical Imaging often involves detecting and assessing\nanomalies that may significantly impact a patient's health.\nMultilevel anomaly detection is valuable as it allows mod-\nels to differentiate between conditions of varying severity.\nFor instance, in a model trained on images of healthy skin,\nanomalies can be categorized into levels: Level 1 includes\nbenign lesions, representing minor deviations with no im-\nmediate health risk; Level 2 encompasses precancerous le-\nsions, indicating a moderate risk that requires monitoring\nor intervention; and Level 3 consists of cancerous lesions,\nsevere anomalies demanding urgent medical attention. This\nstructured detection enables healthcare professionals to pri-\noritize high-risk cases, improving patient outcomes by ad-\ndressing critical conditions promptly.\nIn Industrial Inspection, the economic and operational\nimpact of anomalies can vary widely. Multilevel anomaly\ndetection helps by assessing anomalies not just based on\ntheir physical characteristics, but also their potential eco-\nnomic or operational consequences. For example, a small"}, {"title": "4. MAD-Bench: Multilevel AD Benchmark", "content": "cosmetic defect on a non-essential part may be a low-\nseverity anomaly, while a malfunction in a critical machine\ncomponent that leads to production downtime or safety haz-\nards would be classified as a high-severity anomaly. This\napproach ensures that high-risk anomalies that pose signifi-\ncant economic or safety threats are addressed first, optimiz-\ning quality control and manufacturing processes.\nBy applying multilevel anomaly detection, real-world\napplications across these domains benefit from more pre-\ncise anomaly classification, which enables better prioritiza-\ntion and resource allocation."}, {"title": "4.1. Datasets in MAD-Bench", "content": "In our proposed benchmark, we evaluate anomaly de-\ntection models across various applications. Among the\ndatasets used, two (DRD-MAD and Covid19-MAD) al-\nready include predefined severity labels. For the remaining\ndatasets, which contain class labels for each sample. (e.g.,\ndefect types, disease types, or class names), we manually\nassign severity levels based on these class labels. Classes\nwith ambiguity or samples that do not accurately reflect the\nassigned class name are excluded from the benchmark. De-\npending on the application, we categorize anomalies into\ndifferent numbers of severity levels. The statistics of all\ndatasets can be found in Table 1. Sample distribution across\nseverity levels is detailed in the appendix."}, {"title": "4.1.1. One-Class Novelty Detection Datasets", "content": "We introduce the MultiDogs-MAD dataset to evaluate\nmodels' ability to detect anomalies based on class simi-\nlarity. The training set consists of 500 samples from each\nof five dog breeds selected from the Stanford Dogs Dataset\n[26]: Bichon Frise, Chinese Rural Dog, Golden Retriever,\nLabrador Retriever, and Teddy. Each breed in Level 0 is se-\nquentially used as the normal class, with levels 1 to 4 as test-\ning datasets introducing anomalies of increasing severity.\nLevel 0 represents normal samples from the selected breed.\nLevel 1 includes near-distribution anomalies from other dog\nbreeds not in the training set, Level 2 introduces cat images\nas moderate anomalies [37], Level 3 includes bird images\nas high-severity anomalies [54], and Level 4 contains flower"}, {"title": "4.1.2. Industrial Inspection Datasets", "content": "images as the highest-severity anomalies [35], unrelated to\nanimals. Each testing level contains 500 samples.\nBuilding on the MVTec [5] and VisA [69] datasets, we cre-\nate two MAD datasets, MVTec-MAD and VisA-MAD, by\nassigning severity levels to anomalies based on the eco-\nnomic and operational impact of the class to which the\nanomalies belong. Classes that could not be confidently as-\nsigned a severity level were excluded. MVTec-MAD in-\ncludes over 5,073 high-resolution images across fourteen\nobject and texture categories, with defect-free images for\ntraining and diverse defects in the test sets. VisA-MAD\ncontains 7,874 images in nine subsets, with 6,405 for train-\ning and 1,469 for testing. Both datasets include four sever-\nity levels: Level 0 (non-defect), Level 1 (minor defects that\nare easily repairable), Level 2 (moderate defects with some\neconomic impact), and Level 3 (severe defects with high\neconomic impact)."}, {"title": "4.1.3. Medical Datasets", "content": "The DRD-MAD is derived from the Diabetic Retinopathy\nDetection dataset [14], contains high-resolution retina im-\nges labeled on a 0-4 severity scale, where 0 represents no\nDR and 4 indicates proliferative DR. The images vary in\nquality and orientation due to different imaging conditions\nand camera models. For training, we randomly select 1000\nnormal samples (severity 0). Testing includes 700 samples\nfor each severity level (0-4), ensuring balanced evaluation\nacross all stages of the disease.\nThe Covid19-MAD is sourced from COVID-19 Sever-\nity Scoring [11], contains 1,364 chest X-ray images: 580\nCOVID-19 positive cases and 784 normal cases, with 703\nimages allocated for training. Severity scores for positive\ncases range from 0 (no findings) to 6 (severe, above 85%\nlung involvement), sourced from four public datasets and\nannotated by two independent radiologists.\nThe SkinLesion-MAD is designed to evaluate models\nin detecting and classifying skin anomalies across varying\nseverity levels. It includes 500 training and 500 testing im-\nages of healthy skin sourced from Kaggle [1]. Anomalous\nimages from the ISIC Challenge 2018 dataset [10, 50], la-\nbeled by two doctors through consensus, are divided into\nBenign lesions (e.g., Melanocytic nevus, Vascular lesions,\n652 samples), Precancerous lesions (Actinic keratosis, 327\nsamples), and Cancerous lesions (Melanoma, 500 samples)."}, {"title": "4.2. Conventional Baselines", "content": "We include baselines of several types, including one-class\nclassification, knowledge distillation-based, reconstruction-\nbased, memory-bank, and distribution map-based methods.\nThe methods we evaluate are Skip-GAN [2], OCR-GAN\n[31], AE4AD [7], IGD [9], RD4AD [12], RRD [49], Patch-\nCore [39], PNI [4], CFLOW-AD [18] and SPR [46]. The"}, {"title": "4.3. MLLM-based Baselines", "content": "We acknowledge that assigning anomaly scores corre-\nsponding to severity levels requires domain knowledge,\nwhich conventional baselines lack. Researchers have re-\ncently explored MLLMs for zero-shot learning [13, 24] or\nfew-shot learning [29, 68], but these methods do not include\nthe specific context of multilevel anomaly detection. Thus,\nwe introduce MLLM-based baselines tailored to the MAD\nsetting. We adopt a few-shot setting to balance effectiveness\nand cost, by utilizing four images: three normal images and\none inference image. We prompt the model with the neces-"}, {"title": "5. Experiments and Analysis on MAD-Bench", "content": "sary application context, and instruct it to compare the infer-\nence image with the normal images. Based on this compar-\nison, the model generates an anomaly score ranging from 0\nto 100, accompanied by reasons, with higher scores indicat-\ning greater severity. Figure 2 illustrates the overall frame-\nwork. We evaluate the approach using four state-of-the-art\nMLLMs: GPT-40, GPT-40-mini, Claude-3.5-Sonnet, and\nClaude-3.0-Haiku. Additional details on the experimental\nsetup and prompts are provided in the appendix."}, {"title": "5.1. Research Questions", "content": "We conduct experiments to answer the research questions:\n\u2022 RQ1: (Benchmark and Model Types Analysis) How\naccurately can different types of anomaly detection mod-\nels assign anomaly scores that align with severity levels\nacross various applications?\n\u2022 RQ2: (Binary-Multilevel Performance Correlation)\nDoes a model that performs well in binary anomaly detec-\ntion also perform well in multilevel anomaly detection?\n\u2022 RQ3: (Anomalous Area Effect) How does the area of\nanomalies affect their anomaly score?\n\u2022 RQ4: (Detection Performance Across Severity) How\ndoes binary detection performance vary across different\nseverity levels of anomalies ?\n\u2022 RQ5: (Normal Class Expansion) How do the models\nperform when light anomalies are considered acceptable\nand included as part of the normal class?\n\u2022 RQ6: (Robustness Analysis) How robust are detection\nmodels in aligning anomaly scores with severity under\ndata corruption?"}, {"title": "5.2. RQ1: Benchmark and Model Type Analysis", "content": "We conducted a comprehensive benchmark across six\ndatasets: MultiDogs-MAD, MVTec-MAD, VisA-MAD,\nDRD-MAD, Covid19-MAD, and SkinLesion-MAD, eval-\nuating models on three metrics: AUROC (binary AD), C-\nindex, and Kendall's Tau-b. Table 3 shows results averaged\nacross dataset subsets. Detailed results are in the appendix.\nConventional models perform reasonably well in multi-\nlevel AD tasks on four datasets: MultiDogs-MAD, MVTec-\nMAD, Visa-MAD, and SkinLesion-MAD, with state-of-\nthe-art methods achieving C scores above 80%. How-\never, these models struggle on the two medical datasets,\nDRD-MAD and Covid19-MAD, where C scores are around\n65%. This suggests that multilevel AD for medical im-\nages, such as X-rays and retinal fundus images, presents\ngreater challenges for conventional models. Among con-\nventional approaches, knowledge distillation (e.g., RRD,\nRD4AD) and memory bank-based methods (e.g., PatchCore,\nPNI) demonstrated higher overall performance. Other ap-\nproaches, particularly reconstruction-based models, show"}, {"title": "5.3. RQ2: Binary-Multilevel Performance Correlation", "content": "Finding 1: Among conventional methods, knowl-\nedge distillation and memory bank-based methods\nbetter align anomaly scores with severity levels.\nHowever, MLLM-based models further outperform\nthese methods, highlighting the importance of prior\nknowledge in multilevel anomaly detection.\nSince most prior evaluations in anomaly detection focus on\nthe binary setting, we investigate how model performance\nunder the binary setting correlates with performance under\nour proposed multilevel setting. To this end, we compute"}, {"title": "5.4. RQ3: Anomalous Area Effect", "content": "the average scores for each metric across all datasets and\nrank the baselines accordingly, as shown in Table 3 (a lower\naverage rank indicates better performance). We then cal-\nculate Spearman correlation coefficients [61] of 0.973 be-\ntween AUC and C, and 0.916 between AUC and Ken, in-\ndicating a strong positive correlation between binary and\nmultilevel performance metrics.\nHowever, despite this overall strong correlation, certain\nmodels exhibit notable discrepancies. For most models, the\nrank differences between AUC and Ken or AUC and C re-\nmain within 2, but MMAD-Haiku shows a rank difference\nof 4, and PNI has a difference of 3. Besides, the binary\ndetection rankings of MLLM-based baselines are consis-\ntently higher (worse) than their multilevel detection rank-\nings. This suggests that while MLLM-based models excel\nin aligning anomaly scores with severity levels (multilevel\ndetection), their binary anomaly detection performance is\ncomparatively weaker. These observations show that binary\nevaluation does not fully reflect multilevel performance, un-\nderscoring the need for our benchmark.\nFinding 2: Binary and multilevel detection met-\nrics generally correlate, but some models, notably\nMLLM-based baselines, perform better in multi-\nlevel evaluation.\nWe hypothesize that the area of the anomalous region may\naffect the anomaly score generated by detection models, as\nthese models often rely on differences in spatial features\nto identify anomalies. To validate this hypothesis, we con-\nduct experiments using the MVTec-MAD and VisA-MAD\ndatasets, as both include ground truth masks for the anoma-\nlies. The masks are resized to a width of 256 pixels while\nmaintaining the aspect ratio. We then calculate the anomaly"}, {"title": "5.5. RQ4: Detection Performance Across Severity", "content": "area for each sample and use it as the basis for defining an\narea-based severity level.\nTable 4 presents the multilevel anomaly detection per-\nformance for both risk-based severity levels (as used in our\nbenchmark) and area-based severity levels. The experimen-\ntal results indicate a strong correlation between the area of\nanomalous regions and anomaly scores. This is evidenced\nby the C values for area-based severity levels, which consis-\ntently exceed 80% and are always higher than the C values\nfor risk-based severity levels across both MVTec-MAD and\nVisA-MAD, with the exception of the MLLM-based base-\nlines. Notably, models such as RD4AD, PatchCore, RRD, and\nPNI achieve exceptionally high C scores on VisA. These re-\nsults suggest a bias in conventional models toward the area\nof anomalous regions.\nThis bias can be advantageous in applications where\nseverity levels are strongly correlated with the area of\nanomalous regions, such as surface scratches, where a\nlarger area generally indicates greater severity. However, it\ncan underestimate the severity of anomalies with small ar-\neas but significant impact, such as a severed electrical wire.\nFinding 3: Conventional models are biased toward\nlarger spatial anomalies, assigning them higher\nscores, while MLLMs show less of this tendency.\nA more severe anomaly is intuitively more noticeable to hu-\nmans. We investigate whether models follow this intuition\nby examining their performance across increasing sever-\nity levels. Specifically, we calculate the AUC between the\nnormal class and each severity level. Results are reported\nonly for MultiDogs-MAD, VisA-MAD, DRD-MAD, and\nCovid19-MAD in Figure 3, as performance on MVTec-\nMAD and SkinLesion-MAD is nearly flawless across all\nseverity levels. Generally, most models exhibit an upward\ntrend in detection performance as severity levels increase,\nindicating that higher-severity anomalies are detected more\neffectively. However, this trend is inconsistent at adja-\ncent lower levels. Specifically, while the performance of\nmost models at the highest levels across all datasets con-\nsistently surpasses that at lower levels, models such as PNI\nand OCR-GAN show inconsistencies in achieving better per-\nformance as the severity level increases at adjacent lower\nlevels (e.g., Levels 1, 2, and 3)."}, {"title": "5.6. RQ5: Normal Class Expansion", "content": "Finding 4: Most models detect higher-severity\nanomalies more effectively. Yet, fluctuations are ob-\nserved at lower severity levels in certain cases.\nIn practice, light anomalies are often considered acceptable\nand may be treated as part of the normal class. For exam-\nple, a model trained on images of healthy skin without any\nabnormalities might encounter an inference image with a\nbenign mole, which users want to consider as normal. To\nreplicate this, during test time, we expand the definition of\nthe normal class by progressively including samples from\nabnormal classes as normal based on severity levels. It is\nimportant to note that during the training phase, we exclu-\nsively use normal samples (level 0).\nFigure 4 shows that all models experience a performance\ndecline as the normal class expands to include light anoma-\nlies, except for MMAD-40, which maintains stability on sev-\neral datasets. This decline occurs because many models\nassign similarly high anomaly scores to light and serious"}, {"title": "5.7. RQ6: Robustness Analysis", "content": "Finding 5: All models experience a drop in perfor-\nmance when light anomalies are considered as nor-\nmal samples, but MLLMs have a smaller drop.\nanomalies, hindering adaptation to the expanded normal\nclass. MMAD-40's robustness stems from its ability to dis-\ntinguish between light and severe anomalies. For DRD-\nMAD, the upward trend is due to the nature of light anoma-\nlies, which are difficult to distinguish from normal samples.\nModels often assign low anomaly scores to these anoma-\nlies, treating them as normal. This behavior allows models\nto adapt more effectively when light anomalies are included\nin the normal class, maintaining detection performance.\nFinding 6: All models are negatively affected by\ncorruption, particularly on datasets requiring fine-\ngrained feature analysis."}, {"title": "6. Future Direction", "content": "corruption having a particularly strong effect on MVTec-MAD\nand SkinLesion-MAD. This is because noise can be mis-\ninterpreted by the models as defects, leading to a signifi-\ncant drop in C-index. In contrast, models evaluated on the\nMultiDogs-MAD are less affected by both types of corrup-\ntion, due to the dataset's characteristic as a one-class nov-\nelty detection application, which encourages the models to\nrely on abstract features rather than fine-grained features.\nA promising direction for multilevel anomaly detection lies\nin integrating MLLMs with conventional approaches to\nleverage their complementary strengths. MLLMs excel in\naligning anomaly scores with severity levels through do-\nmain knowledge and contextual reasoning, while conven-\ntional models perform better in binary detection, particu-\nlarly for industrial inspection. A hybrid approach, such as\na two-stage framework where conventional models detect\nanomalies and MLLMs assign severity scores, offers a ro-\nbust and comprehensive solution by combining high accu-\nracy in detection with contextual understanding for severity\nassessment."}, {"title": "7. Conclusion", "content": "In this paper, we introduce Multilevel Anomaly Detection\n(MAD), a novel setting that emphasizes the alignment of\nanomaly scores with practical severity. To support this set-"}]}