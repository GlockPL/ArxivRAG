{"title": "When SparseMoE Meets Noisy Interactions: An Ensemble View on Denoising Recommendation", "authors": ["Weipu Chen", "Zhuangzhuang He", "Fei Liu"], "abstract": "Learning user preferences from implicit feedback is one of the core challenges in recommendation. The difficulty lies in the potential noise within implicit feedback. Therefore, various denoising recommendation methods have been proposed recently. However, most of them overly rely on the hyperparameter configurations, inevitably leading to inadequacies in model adaptability and generalization performance. In this study, we propose a novel Adaptive Ensemble Learning (AEL) for denoising recommendation, which employs a sparse gating network as a brain, selecting suitable experts to synthesize appropriate denoising capacities for different data samples. To address the ensemble learning shortcoming of model complexity and ensure sub-recommender diversity, we also proposed a novel method that stacks components to create sub-recommenders instead of directly constructing them. Extensive experiments across various datasets demonstrate that AEL outperforms others in kinds of popular metrics, even in the presence of substantial and dynamic noise.", "sections": [{"title": "I. INTRODUCTION", "content": "The rise of social media and global communication has led to an explosion of information, making personalized recommendation for users a key challenge. In recent years, learning user preferences from implicit feedback has become a mainstream strategy, as implicit signals like clicks are easier to collect than explicit ratings. However, as shown in Fig. 1, users might accidentally click on items that do not reflect their true preferences [1]\u2013[8]. Recent studies [9]-[11] refer to this type of interaction as noisy interaction. Moreover, treating noisy interactions as clean ones can negatively impact recommendation performance [9], [10], [12]. As a result, denoising recommendation methods [9]\u2013[11], [13]\u2013[17] have emerged as a popular research direction, aiming to learn users' true preferences from implicit feedback.\nMost recent efforts can be divided into reweight-based methods [10], [18], [19] and drop-based methods [10], [16], [20]\u2013[22]. However, these denoising methods still have non-neglectable limitations. 1) Reweight-based methods depend heavily on accurately identifying and assigning appropriate weights to noisy interactions. But, misclassifying numerous noisy interactions can result in suboptimal weight assignments, leading to poor performance. 2) Drop-based methods may drop valuable samples when denoising, leading to a loss of information that could contribute to a better understanding of user preference. Moreover, the performance of the both denoising methods overly relies on the hyperparameter configurations, demanding to adjust the denoising capacities case by case to gain notable results. Therefore, how to adaptively adjust the denoising capability without complex hyperparameter tuning also poses a key limitation for denoising recommendation.\nTo overcome the above limitations, we consider leveraging ensemble learning [23]\u2013[25], which is effective in adapting to different application scenarios. We expect this approach routes inputs to specialized sub-recommenders, enabling efficient noise filtering tailored to varying data patterns while minimizing the need for manual tuning. However, there are two challenges to achieving the above objectives. C1: Increasing model complexity due to multiple sub-recommenders. Directly creating multiple sub-recommenders increases the model complexity. This makes optimization difficult and raises computational costs, reducing the practicality for large-scale recommendation systems. C2: Overfitting caused by static weight distributions. General ensemble methods may fix weights for sub-recommenders based on training data causing overfitting. It may perform well on the training set but show poor results on out-of-distribution data, making it less effective in real-world scenarios where data varies.\nTherefore, we propose an Adaptive Ensemble Learning (AEL) for denoising recommendation. Inspired by SparseMoE (Sparse Mixture-of-Experts) [26], AEL consists of three experts and a sparse gating network. Specifically, we construct the experts of the denoising module in a novel approach and introduce an adaptive ensemble module to fundamentally address adaptability insufficiency. For C1, we propose a novel method that stacks components to construct sub-recommenders. It not only reduces the model complexity but also ensures sub-recommender diversity. For C2, we creatively introduce a sparse gating network, which can analyze the performance of each sub-recommender and generate suitable dynamic weight distributions for current input. In summary, our contributions are as follows:\n\u2022 We pioneer an effort to analyze the pitfalls of existing denoising recommendation methods that suffer from low adaptability due to an excessive number of hyperparameters.\n\u2022 We propose an Adaptive Ensemble Learning (AEL) method for denoising recommendation. Specifically, we design a novel process for constructing sub-recommenders to address the issue of model complexity. Additionally, we develop an adaptive ensemble module to tackle the overfitting problem caused by static weight distribution.\n\u2022 We conduct extensive experiments compared to cutting-edge methods and investigate various modules of our model, demonstrating the effectiveness and adaptability of AEL."}, {"title": "II. PROPOSED MODEL", "content": "AEL contains three modules: the denoising module, the corrupt module, and the adaptive ensemble module. In the denoising module, we first construct three sub-Autoencoders (sub-AEs) as components based on collaborative denoising autoencoder [27], [28]. Then, we vary the denoising capacities of three parent-AEs and reduce model size using a novel method, which first creates three sub-AEs as components, then stacks them to construct heterogeneous parent-Autoencoders (parent-AEs). We also introduce a corrupt module to improve robustness by partially corrupting initial input, preventing sub-AEs from simply learning the identity function. The adaptive ensemble module achieves denoising capacity adaptability. It contains an improved sparse gating network [26] as a brain, which can analyze the historical performance of parent-AEs, and automatically select the two most suitable parent-AEs to synthesize appropriate denoising capacity for current input data. \n\nA. The Components: Sub-Autoencoders\nTo save computing resources and reduce model size, we invent a novel method that allows the experts in SparseMoE to share parameters. We design and form three sub-AEs based on Collaborative Denoising Autoencoder (CADE), named Large, Medium, and Small, as components, then stack them to construct parent-AEs as experts. Specifically, each sub-AE consists of two functional modules: the encoder and the decoder. Their main difference is the hidden dimension. In the encoding process, sub-AEs encode additional latent vectors for user Vu to provide improved recommendation. Large also utilizes the corrupt module to corrupt the input xu to Zu using a mask-out technique. Its encoder and decoder functions can be described by the following formula:\nEncoder: $z_u^{(1)} = \\sigma (W_u + W_{item}x_u + b_{item}),$ (1)\nDecoder: $\\hat{x}_u = \\sigma(W^T \\cdot z_u^{(1)} + b_a),$ (2)\nwhere $x_u \\in \\mathbb{R}^{1\\times D}$ is the input, and $\\hat{x}_u$ is the output. $z_u \\in \\mathbb{R}^{1\\times K}$ is the latent feature representation of the input $\\hat{x}_u$. K is the hidden dimension and $K \\ll D$. W, $W_u$, $W_{item}$, and $W^T$ are the weight matrices, $b_{item}$ and $b_a$ are the bias vectors of the network, $\\sigma(\\cdot)$ is the activation function.\nThe other sub-AEs, Medium and Small, respectively take the hidden vector $z_u^{(1)}$ of the previous sub-AE as input. \nParameters of sub-AEs are learned by minimizing the average reconstruction error:\n$\\arg \\min \\sum_{u=1}^U \\rho(x_u, \\hat{x}_u) + \\mathcal{R}(W, W', V,b,b'),$ (3)\nwhere $\\mathcal{R}(\\cdot)$ is the regression term with the squared L2 Norm.\n$\\mathcal{R}(:) \\Rightarrow (||W|| + ||W'|| + ||V|| + ||b|| + ||b'||),$ (4)\nB. The Experts: Parent-Autoencoders\nWe hope sub-AEs, the three autoencoders with varying hidden dimensions, will have different denoising capacities so that we can take them as experts. However, the difference in hidden dimension merely illustrates that they have different numbers of parameters to capture the features. Considering that the features may be represented more compactly or more loosely, a causal link cannot be established between the hidden dimension and the denoising capacity of an autoencoder.\nTherefore, we stack sub-AEs to construct three parent-AEs, respectively named Mild Denoising, Moderate Denoising, and Strong Denoising. Since the hidden dimension is smaller than the input vector dimension, a sub-AE cannot fully recover its original input after encoding and decoding. This process performs denoising. Leveraging this characteristic, we ensure that parent-AEs have varying denoising capacities by stacking sub-AEs in different ways.\nC. The Brain: Sparse Gating Network\nWe construct three parent-AEs as experts using a novel method that significantly reduces the model size, as men-"}, {"title": "III. EXPERIMENT", "content": "A. Experimental Settings\nDataset. Our experiments are conducted on three popular recommendation system datasets: MovieLens\u00b9 [9], [30], Mod-cloth\u00b2, and Adressa\u00b3 [9], [10], [30]. Detailed statistics of these datasets are shown in Table III. We split each dataset into a training set and a test set in an 8:2 ratio. For ModCloth and Adressa, we follow [9]\u2013[11] to construct the clean test set.\nEvaluation Protocols. Following previous research on denoising recommendation [9], [10], [30], we used three popular metrics: Precision@N, Recall@N, and MRR@N. Higher scores indicate better performance. We conducted each experiment five times and presented the averaged results.\nCompared Models. We compare AEL with several popular top-N recommendation models based on implicit feedback, including GMF [29], NeuMF [29], CDAE [27], LightGCN [31], TCE and RCE [10], and DeCA [9].\nImplementation Details. The hidden dimensions of sub-AES are set to 12, 48, and 128, respectively. The number of experts k activated per input is set to 2. The coefficients of Limportance and LLoad, wi and wr, are set to le-2. For other baselines, we follow the hyperparameter settings suggested in [9], [10].\nB. Overall Performance Comparison\nIn this section, we compare AEL with several prominent top-N recommendation models. The experiment results show that our proposed model outperforms baselines across all datasets. AEL demonstrates good stability and performance across three different datasets, which other models do not possess. We also observe that our model maintains stable performance even on sparse datasets. AEL shows significant advantages in handling dynamic noise, while others' performance can fluctuate with great amplitude across datasets.\nC. Extensive Effectiveness Analysis\nAblation Study. To validate the contributions of parent-AEs, we gradually corrupted the training data by adding noise. To exclude the impact of the sparse gating network on results, we replaced it with averaging the output of all parent-AEs. The experimental results show that each parent-AE has its own \u201ccomfort zone\u201d: Mild Denoising outperforms the others when the number of masks added is small but is surpassed by Moderate Denoising as the number increases. Subsequently, Strong Denoising surpasses both of them. The ensemble model effectively leverages the advantages of all parent-AEs, achieving superior performance and stability.\nHyperparametric Sensitivity Analysis. The number of experts k selected is the most critical hyperparameter, as it controls how to synthesize the denoising capacity for current input. We experiment with all the feasible cases of k and show the results in the Table V. We observed that AEL performs well across three datasets in all cases of k. That reason is that for each interaction vector $x_u$, activating"}, {"title": "Aggregation Method Comparison", "content": "We conduct experiments to explore how the aggregation methods affect AEL performance. We compare other ensemble learning methods, including Bayesian Model Averaging and Averaging, to our spare gating network. According to the results shown in Fig. 4, our aggregation method outperforms others across all datasets, especially on the MovieLens dataset."}, {"title": "IV. CONCLUSION", "content": "In this paper, we proposed Adaptive Ensemble Learning (AEL), a novel ensemble method that can automatically synthesize appropriate denoising capacity for different implicit feedback. It contains three parent-AEs as experts and a sparse gating network as a brain. We creatively stack sub-AEs to construct parent-AEs instead of independently creating them, which can effectively reduce model size and ensure denoising capacity diversity. Experiments demonstrate the effectiveness and generalization of our proposed method."}]}