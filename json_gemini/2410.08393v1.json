{"title": "The Effects of Hallucinations in Synthetic Training Data for Relation Extraction", "authors": ["Steven Rogulsky", "Nicholas Popovic", "Michael F\u00e4rber"], "abstract": "Relation extraction is crucial for constructing knowledge graphs, with large high-quality datasets serving as the foundation for training, fine-tuning, and evaluating models. Generative data augmentation (GDA) is a common approach to expand such datasets. However, this approach often introduces hallucinations, such as spurious facts, whose impact on relation extraction remains underexplored. In this paper, we examine the effects of hallucinations on the performance of relation extraction on the document and sentence levels. Our empirical study reveals that hallucinations considerably compromise the ability of models to extract relations from text, with recall reductions between 19.1% and 39.2%. We identify that relevant hallucinations impair the model's performance, while irrelevant hallucinations have a minimal impact. Additionally, we develop methods for the detection of hallucinations to improve data quality and model performance. Our approaches successfully classify texts as either 'hallucinated' or 'clean,' achieving high F1-scores of 83.8% and 92.2%. These methods not only assist in removing hallucinations but also help in estimating their prevalence within datasets, which is crucial for selecting high-quality data. Overall, our work confirms the profound impact of relevant hallucinations on the effectiveness of relation extraction models.", "sections": [{"title": "1. Introduction", "content": "Relation extraction is an important step in extracting structured information from text documents, such as news articles, publications, patents, and websites, building the basis for knowledge graph construction. High-quality datasets play a crucial role in this process [1, 2, 3], as they form the basis for training, fine-tuning, and evaluating relation extraction models. Additionally, the amount of data they contain has a significant impact on the achieved results [4]. However, creating large datasets with high quality typically requires human annotation, which is expensive and slow. Although heuristic methods such as distant supervision can produce larger datasets, they often lack quality [5]. An alternative is Generative Data Augmentation (GDA), a technique for synthetically expanding datasets by generating new data samples (here: texts and extracted triples). It can generate datasets that are much larger, more diverse, and less expensive than traditional human annotations without directly collecting new data [6]. In the context of relation extraction, GDA has been widely used in combination with pre-trained language models such as BERT and GPT [7, 8, 9, 10, 11]."}, {"title": "2. Related Work", "content": "In this section, we first look at related work on creating synthetic training data. In the second part, we look at noisy data and hallucinations and how to recognize them.\nGenerating Synthetic Data. Several data augmentation approaches have been proposed. Feng et al. [15] differentiate between three main types: (1) Rule-based approaches use algorithms to modify existing real-world datasets. Techniques such as synonym replacement, random insertion, swapping and deletion are used to significantly increase the volume of training data [16, 3, 17]. (2) Sample interpolation, also known as Mixed Sample Data Augmentation, [18] interpolates data points to create more diverse and robust datasets for training language models [19, 20, 21, 22, 23]. Both approaches are limited by the fact that they are based on existing datasets. As a result, they are not able to introduce completely new features or vary the data types significantly, such as the relation types for relation extraction tasks. This can lead to the persistence of existing biases in the original datasets [15]. (3) Model-based approaches, referred to as Generative Data Augmentation (GDA), overcome these limitations. They are able to generate completely new and specific data points, independent of existing datasets. For example, the Control Prefixes model [24] is characterized by the generation of text data from structured knowledge graphs using the WebNLG dataset [25, 26]. Other notable implementations include the use of pretrained language models (PLMs), such as GPT-3.5, which have been successfully used to improve performance on relation extraction tasks [27, 2, 28, 6].\nJosifoski et al. [6] developed a large synthetic dataset named Wiki-cIE for closed information extraction, utilizing GPT-3.5 with prompt engineering. This dataset, containing 1.8 million data points, serves as a robust alternative to both distantly supervised and directly supervised datasets in terms of size and quality. It is positioned closely in scale to the largest distantly supervised dataset, REBEL [5]. Importantly, the Wiki-cIE dataset offers enhanced quality, especially in the distribution of relation types and the accuracy of text annotations. Josifoski et al. demonstrate that relation extraction models trained on Wiki-cIE significantly outperform those trained on REBEL, attributing this advantage to the superior quality of their synthetic dataset. However, they do not specify which particular attributes of the datasets contribute to these performance differences. A notable quality difference is in the accuracy of the text annotations, suggesting that this aspect may be a critical factor in the observed improvements in model performance.\nDetecting and Assessing Noisy Data. Corrupted or noisy data, characterized by issues such as incorrect labels, affects language model training [29, 30, 31, 32, 33, 34]. Several strategies have been developed to address noisy data in datasets. Techniques include resampling [35], loss reweighting [29], and label correction [36]. Additionally, some approaches advocate training models using noise-robust loss functions [30, 31], with a notable recent development being a noise-robust re-weighting framework [2]. While these methods effectively mitigate the impact of noisy data or reduce its presence, they do not specifically explore the influence of hallucinations within synthetic training data on the performance of relation extraction models.\nAnalyzing Hallucinations. Ji et al. [13] provide an overview on hallucinations, including relevant data-to-text use cases. The authors distinguish between two types of hallucinations: intrinsic and extrinsic. Intrinsic hallucinations are false information in texts that contradict the annotations, while extrinsic hallucinations consist of additional information in the texts"}, {"title": "3. Evaluation", "content": "The concept of hallucinations lacks a universally accepted definition [13, 32, 45, 12, 40]. Figure 1 provides an example of a hallucination. In this scenario, a GDA model, tasked with generating text from the input triple ('Ted', 'lives in', 'New York'), should ideally produce 'Ted lives in the city of New York.' Instead, the model might extend this to 'Ted lives in the city of New York, which has a population of 8.4 million inhabitants. This addition introduces an unsupported triple ('New York', 'has', '8.4 million inhabitants'), which is a hallucination.\nFormally, we define hallucinations H as the set difference $H = T^C \\setminus T$ between the set of triples T and the triples $T^C$ that are actually generated in the text S.\nWe differentiate between relevant and irrelevant hallucinations [24, 46] in relation extraction models, as illustrated in Figure 2. Relevant hallucinations occur when the text expresses triples with relation types that are relevant (i.e., included in the schema) but absent from the annotations. For example, if a model is trained exclusively to detect birth dates in texts, only triples related to birth dates are considered relevant. Conversely, irrelevant hallucinations involve relations that the model is designed to ignore, as they do not pertain to its trained focus. In the following, we first analyze the influence of hallucinations on synthetic training datasets for relation extraction. We then consider the automatic detection of hallucinations in relation extraction datasets."}, {"title": "3.1. Evaluating the Effects of Hallucinations", "content": null}, {"title": "3.1.1. Influence of Relevant Hallucinations on Document Level", "content": "In this subsection, we focus on evaluating the impact of relevant hallucinations on document-level relation extraction."}, {"title": "3.1.2. Influence of Relevant Hallucinations on Sentence Level", "content": "Datasets. We now require datasets on the sentence-level. We use the WebNLG [26] dataset for A, a widely used knowledge graph-to-text dataset [25, 24, 8, 51]. Based on an own analysis and to the best of our knowledge, the dataset is free of relevant hallucinations [52].\nThe first variant for B, $B_{MT}$, is created to ensure direct comparability to the document-level"}, {"title": "3.1.3. Differences Between Relevant and Irrelevant Hallucinations", "content": "In this subsection, we evaluate the influence of relevant and irrelevant hallucinations on the sentence level.\nDataset. We keep A the same as it can serve as the dataset with fewer hallucinations. On the other hand, B needs to contain irrelevant hallucinations instead of relevant ones. We also create another test dataset for testing whether the newly trained models only extract the first part of a text and ignore the rest. To that end, we use the chat version of LLAMA2 [54] to add irrelevant hallucinations to each of A's data points. The LLM takes text as input and returns the same text but with additional information. To create additional dataset variants, we adjust the prompt by adding or removing specific instructions. This allows us to partly control the amount and type of information added. In total, we produce five modified WebNLG datasets, with the only difference being the prompt we use for the creation process.\nNew Test Dataset. We modify the test dataset to assess if text length affects model performance. Each data point in $A^{test}$ is altered by fusing two data points (i.e., concatenating S and merging T), creating a test set with longer texts and more triples per data point without adding new hallucinations."}, {"title": "3.2. Evaluating Hallucination Detection", "content": "We consider two approaches of hallucination detection, as outlined in the following."}, {"title": "3.2.1. Named Entity Recognition-Based Hallucination Detection", "content": "A first approach for hallucination detection was suggested by Liu et al. [42] and involves named entity recognition (NER). This approach extracts entities from a text and compares them to the entities in the corresponding triples. Entities found in the text but absent from the triples are identified as hallucinations.\nDataset: The sentence-level WebNLG dataset version v3.0 [55] serves as the basis for this work. The dataset includes annotations with one to seven triples.\nModel: We decided to use the widely used SpaCy model given its wide usage and solid performance. Through preliminary tests, we can confirm the theory that the entities extracted by the model from S are often correct but not equivalent to those in the annotation T. This can result in cases where, for example, 'Alan Bean' is in T but only 'A. Bean' is extracted from S, which essentially means the same thing. To solve this problem, we use the sentence similarity model all-mpnet-base-v2 [56] to compare the extracted and annotated entities.\nEvaluation Setup: We use the precision, recall, and F1-score for the evaluation. We classify 'hallucination-free' texts that are correctly accepted as true positives.\nFor our experiments, we utilize 3,000 data points sampled from D. For each data point, we randomly select one correct text and one hallucination to maintain a balanced ratio between the two. The hallucination text can contain one to six hallucinated triples. Additionally, we conduct a hyperparameter sweep across all acceptance thresholds ranging from 0.05 to 0.95 (inclusive) in 0.05 increments to find the best-performing threshold for the sentence similarity model."}, {"title": "3.2.2. Textual Entailment Approach for Hallucination Detection", "content": "Another approach, inspired by Du\u0161ek and Kasner [43], uses an entailment model M to check if a sentence S contains the same information as a set of triples T. The triples $t \\in T$ are combined into a single sentence using conjunctions. If M classifies T as not entailed, it indicates hallucinations; if classified as entailed, S is considered hallucination-free.\nDataset: Since we evaluate a new approach on the same task as in the previous Section 3.2.1, we do not need to adjust the dataset and can continue to use D.\nModel: For this task, we focus on the roberta-large-mnli [57] and deberta-v2-xlarge-mnli [58] models. Both models perform well on SQuAD 1.1/2.0 and various GLUE benchmarks.\nClassifier: An entailment model can be used to test whether sentence S2 is part of sentence S1 or if the content of S1 implies the content of S2. We design the model to test for any hallucinations in S compared to T, from each data point of a dataset.\nThe initial step involves pre-processing the triples, typically formatted as 'Entity_1 | relation | Entity_2' or as a three-element list. Here, we receive the input in the former format and replace all occurrences of '_' and '|' with spaces. Next, the goal is to create a sentence, ST, that encompasses all triples from T and accurately conveys T's informative value. This is done by combining the pre-processed triples $t \\in T$ into a single sentence, linked by the conjunction 'and' Finally, M verifies whether S is entailed in ST. If the result is 'entailed,' S is deemed correct; otherwise, 'neutral' or 'contradictory' results signal the presence of hallucinations.\nEvaluation Setup: We test M on 4,000 sampled data points from D. Each sample comprises an annotation T and two texts, $S1 \\in h$ and $S2 \\in c$, while $h, c \\subset D$. That means that two tests have to be conducted for each data point, one for each text. This results in 8,000 classifications.\nThe evaluation is otherwise similar to the NER approach from the previous section."}, {"title": "4. Conclusion", "content": "In this paper, we analyzed the impact of hallucinations in synthetic training data on relation extraction tasks. Our evaluation revealed significant performance declines with recall reductions between 19.1% and 39.2%. This indicates that hallucinations notably compromise the ability of models to accurately extract relations from texts. We identified a distinction between relevant and irrelevant hallucinations, noting that the former significantly impairs performance, while the latter has a minimal impact. Additionally, we developed methods for the detection (and thus mitigation) of hallucinations to improve data quality and, thus, model performance. Our approaches, successfully classified texts as either 'hallucinated' or 'clean, with notable F1-scores of 83.8% and 92.2%. In the future, we will analyze the impact of hallucinations in datasets for other NLP tasks, such as entity and event extraction."}]}