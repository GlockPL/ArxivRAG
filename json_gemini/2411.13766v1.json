{"title": "Tiny-Align: Bridging Automatic Speech Recognition and Large Language Model on the Edge", "authors": ["Ruiyang Qin", "Dancheng Liu", "Gelei Xu", "Zheyu Yan", "Chenhui Xu", "Yuting Hu", "X. Sharon Hu", "Jinjun Xiong", "Yiyu Shi"], "abstract": "The combination of Large Language Models (LLM) and Automatic Speech Recognition (ASR), when deployed on edge devices (called edge ASR-LLM), can serve as a powerful personalized assistant to enable audio-based interaction for users. Compared to text-based interaction, edge ASR-LLM allows accessible and natural audio interactions. Unfortunately, existing ASR-LLM models are mainly trained in high-performance computing environments and produce substantial model weights, making them difficult to deploy on edge devices. More importantly, to better serve users' personalized needs, the ASR-LLM must be able to learn from each distinct user, given that audio input often contains highly personalized characteristics that necessitate personalized on-device training. Since individually fine-tuning the ASR or LLM often leads to suboptimal results due to modality-specific limitations, end-to-end training ensures seamless integration of audio features and language understanding (cross-modal alignment), ultimately enabling a more personalized and efficient adaptation on edge devices. However, due to the complex training requirements and substantial computational demands of existing approaches, cross-modal alignment between ASR audio and LLM can be challenging on edge devices. In this work, we propose a resource-efficient cross-modal alignment framework that bridges ASR and LLMs on edge devices to handle personalized audio input. Our framework enables efficient ASR-LLM alignment on resource-constrained devices like NVIDIA Jetson Orin (8GB RAM), achieving 50x training time speedup while improving the alignment quality by more than 50%. To the best of our knowledge, this is the first work to study efficient ASR-LLM alignment on resource-constrained edge devices.", "sections": [{"title": "I. INTRODUCTION", "content": "The Large Language Model (LLM) deployed on the edge device (edge LLM) can serve as a personalized assistant, guaranteeing data privacy [1] and continuous service without relying on stable internet connections [2], [3]. Edge LLMs normally interact with users based on text (text-based interaction). Yet, it is more natural and accessible for users to interact with edge LLMs, their personalized assistants, via audio [4]. To enable the audio-based interaction, the Automatic Speech Recognition (ASR) models can be employed [5] to simply take the audio as the input and send the converted text into the edge LLM [6]. Existing works, on the other hand, have shown that this simple combination of ASR and LLM can fail in cases where the audio has no corresponding text [7], or the mismatching pre-trained knowledge in the ASR and the edge LLM can compromise the performance of their combination.\nDifferent from simply combining the ASR and the edge LLM, past works [8] concatenate them at the feature level via the projector. In this concatenation, the ASR functions as the audio encoder and produces the audio embedding, which will then be sent to the edge LLM for LLM generation (edge ASR-LLM). Usually by default, the projector is a simple multi-layer perception (MLP) that matches the dimensionality of the audio embedding with the LLM. While the feature-level concatenation can overcome the problems in the simple combination of the ASR and the edge LLM, it often requires end-to-end training so the audio embeddings can be properly understood by the edge LLM (cross-modal alignment).\nFor cross-modal alignment, there are three main approaches to end-to-end training, as shown right part of Fig. 1. The first approach (Al) freezes both the ASR encoder and LLM while training only the projector [9]. This method updates the projector parameters by comparing LLM-generated outputs: one from projector-transformed audio embeddings and another from the corresponding text input. The second approach (A2) extends the first approach by jointly training [10] both the projector and LLM, creating a larger learning space. The third approach (A3) takes a different direction by training only the ASR encoder [11] while keeping the projector and LLM frozen.\nThe three approaches, while demonstrating decent performances in cross-modal alignment, can involve a huge size of parameter calculation upon the complex hyperparameters and the model tuning space. In this way, the huge resource budget can be prohibitive for cross-modal alignment on edge devices. Under the existing cross-modal alignment approaches, even if we choose the compressed model with smaller parameters, we can expect either significant performance downgrading due to the lowering of parameter size or the still prohibitively high resource usage due to the internal training process design. As shown left part of Fig. 1, we provide a pre-evaluation of the three alignment approaches on the concatenation of an ASR model-Whisper [12] and the edge LLM-Gemma-2-2B [13]. We also employ the compress Gemma-2-2B based on GPTQ [14] (G). The ASR and the edge LLM are connected by a two-layer MLP. In terms of training time (latency) and performance, we do not observe significant differences between the original model and its compressed version.\nFor approaches A1 and A2, the LLM generation involved can consume a huge amount of resources, leading to a longer convergence time. The approach of A2, which takes the projector and the LLM for training, outperforms the other two approaches but it also takes the most resources. The approach of Al is more lightweight, but it still uses the LLM, and its reduction in finetuning resources leads to significant performance degradation. Meanwhile, The Al's lower performance than the A2 can be due to the simple projector design. As shown in Fig. 1, while the A2 obtains decent performance, it"}, {"title": "II. BACKGROUND", "content": "A. Automatic Speech Recognition onto edge LLM\nExtending edge LLM's capability to process audio input can significantly enhance user accessibility. ASR models serve as the bridge between audio and text modalities, with several prominent approaches offering different trade-offs between performance and resource efficiency.\nWav2vec2 [15] pioneered an efficient approach to audio processing by directly learning representations from raw waveforms. Through a multi-layer convolutional network, it extracts context-dependent representations that effectively capture speech patterns. The model employs contrastive learning to identify relevant features and discern patterns in the audio signal. While its streamlined architecture makes it particularly efficient for edge deployment, wav2vec2 may struggle with highly noisy environments.\nWhisper [12] and Conformer [16] represent another advancement in ASR technology, leveraging transformer architectures for improved accuracy. These models process audio through self-attention mechanisms to capture long-range dependencies in speech signals, achieving superior performance in complex transcription tasks across various languages and acoustic conditions [17]. However, their so-sophisticated architecture requires more computational resources com-pared to wav2vec2.\nMore recent approaches like Diffsound [18], AudioLM [19], and Tango2 [20] adopt generative frameworks that extend beyond simple transcription. Based on diffusion models or autoregressive generation, these ASRs can not only recognize speech but also model the underlying audio distribution. While these models achieve state-of-the-art performance in complex scenarios, their resource-intensive nature makes them highly challenging to deploy in edge environments without significant optimization.\nB. Cross-modal Alignment on Edge\nOne key challenge in personalizing edge LLM is adapting to users' preferred input modality while operating within the constraints of limited computational resources [21]\u2013[23]. Cross-modal alignment enables the integration of non-textual inputs by combining specialized machine learning models for various modalities with the edge LLM which provides advanced reasoning capabilities for analyzing cross-modal data [24]. However, this alignment process demands sub-stantial resources, making deployment on edge devices particularly challenging [25], [26]. For example, on resource-constrained devices like NVIDIA Jetson Orin (275 TOPS, INT8), cross-modal alignment can quickly exhaust available memory and computational resources, with processing latency becoming prohibitive for real-time applications. These constraints highlight the need for optimizing cross-modal alignment specifically for edge deployment.\nC. Edge ASR-LLM Alignment for People with Speech Difficulties\nWhile edge ASR-LLM alignment enhances accessibility for gen-eral users, it holds particular promise for individuals with speech difficulties. In this work, we demonstrate the effectiveness of our alignment approach using datasets from individuals with dementia, aphasia, and specific language impairments (SLI), showcasing how edge ASR-LLM can adapt to and understand users despite significant speech challenges. This capability becomes increasingly crucial given the growing population affected by these conditions over 6.7 million people with dementia, 2 million with aphasia, and more than 180,000 new cases annually [27]. For these individuals, traditional text-based interaction is infeasible due to motor and cognitive challenges, making voice-based interaction not merely an alternative, but a necessity for healthcare delivery."}, {"title": "III. PROPOSED WORK", "content": "In this section, we first provide an overview of our Tiny-Align framework shown in Fig. 2. We then delve into the technical details, beginning with the pivotal component: the projector design. Following this, we explain how the projector collaborates with the LLM embedding layer. We then elaborate on the types of ASR models suitable for our edge ASR-LLM alignment framework. Finally, we introduce our instruction injection method, which enables projector-generated embeddings to better facilitate LLM generation.\nA. Framework Overview\nAs shown in Fig. 2, Tiny-Align operates in two modes. During the training mode (upper half in Fig. 2), our proposed BridgeFormer"}, {"title": "B. BridgeFormer: Transformer-based Projector", "content": "Motivated by our preliminary study in Fig. 1, where a simple MLP projector demonstrates promising results in edge ASR-LLM alignment, we design an enhanced projector architecture that offers a larger audio embedding space where the embeddings can also be understood by the LLM, while maintaining resource efficiency. This design is driven by two key requirements: the ability to support continuous learning during user interactions and architectural com-patibility with LLMs to ensure effective embedding transformation. As the sole trainable module in Tiny-Align, our projector aims to leverage the computational resources saved from avoiding LLM generation and training, allowing for a moderately larger architecture while remaining within edge device constraints.\nTo meet these design criteria, we introduce a transformer encoder architecture into the projector, named BridgeFormer to reflect its role in bridging ASR and LLM models. Compared to classical MLPs or deep neural networks, the transformer architecture offers several advantages: a larger embedding space with enhanced learning potential, natural alignment with LLM's architecture (as transformers were originally designed for natural language processing) [28], [29], and proven effectiveness in handling sequential data. Furthermore, since BridgeFormer processes ASR-extracted audio features where temporal positioning is already encoded, we remove the traditional positional encoding component, simplifying the architecture and improving its compatibility with both ASR and edge LLM.\nConsider audio features extracted from an ASR encoder with shape $[1, N, D_a]$, where N represents the audio sequence length and $D_a$ is the ASR embedding dimension. BridgeFormer first employs an input MLP to project these features into a higher-dimensional space, creating initial embeddings of shape $[1, N, H]$, where H is the hidden dimension. These embeddings then pass through the transformer encoder, where M attention heads across E encoder layers to process and refine the features while maintaining the sequence dimensions. To prevent overfitting and control sequence length, an adaptive pooling layer reshapes the sequence to a predefined token size T, producing outputs of shape $[1, T, H]$. Finally, an output MLP transforms these"}, {"title": "C. EmbedLink: Cross-modal Alignment via Embedding Layer", "content": "With BridgeFormer's architecture established, we now focus on its training methodology for creating a shared embedding space. We propose EmbedLink, a simple yet effective pipeline that avoids resource-intensive LLM generation during cross-modal alignment. Our approach leverages a key insight: LLM's text processing fun-damentally relies on embeddings from its embedding layer. There-fore, if BridgeFormer can transform audio features into embeddings that closely match those generated by the LLM's embedding layer from corresponding text input, we can achieve effective cross-modal alignment without the computational overhead of full LLM inference.\nA critical challenge in this alignment process is managing the dimensional disparities between audio features and LLM embed-dings. As illustrated in Fig. 4, audio embeddings have dimensions determined by audio length N and ASR encoding dimension $D_a$, while LLM embeddings are shaped by token size and embedding dimension $D_l$. The audio length N typically exceeds the token count significantly due to pauses, silence periods, and other audio-specific characteristics. Directly matching these dimensions could either result in learning failure or significant information loss."}, {"title": "D. Feature-based ASR encoder", "content": "After establishing BridgeFormer and EmbedLink, selecting an appropriate ASR encoder becomes crucial for our framework. While we adopt existing ASR models rather than designing new ones, the selection must balance hardware efficiency with feature extrac-tion capability. As discussed in Section II-A, we consider three mainstream ASR types: feature-based ASR (e.g., wav2vec2-base), transformer-based ASR (e.g., Whisper-base), and generative ASR (e.g., AudioLDM).\nOur analysis combines theoretical representation space study with empirical evaluation of resource usage and training efficiency. For theoretical analysis, we examine how these ASR models map an input audio sequence $x \\in X$ to their respective feature spaces:"}, {"title": "E. Instruction Injection", "content": "When the proper ASR encoder is chosen and EmbedLink has trained BridgeFormer to generate LLM-compatible embeddings, these embeddings purely represent the audio input information. However, to elicit high-quality LLM responses, instructions are often required [30], and these may vary across different use cases. Rather than incorporating instructions during BridgeFormer training, we design a flexible instruction injection module for inference time, allowing users to adapt instructions based on specific needs. As illustrated by the deep blue dashed lines in Fig. 2, this module implements a streamlined instruction injection process.\nGiven a task-specific instruction I, we first convert it into LLM-recognizable embeddings:"}, {"title": "IV. EXPERIMENTAL EVALUATION", "content": "A. Experimental Setup\n1) Datasets: To demonstrate our Tiny-Align framework, we em-ploy five diverse datasets from TalkBank [31], a comprehensive database for language research. These datasets include: (1) ADRESS-IS2020 [32], containing speech recordings of Alzheimer's patients describing the Cookie Theft picture; (2) Baycrest [33], featuring structured interviews with dementia patients; (3) EllisWeismer [34], focusing on children with specific language impairments; (4) ENNI [35], containing narrative samples from children with language dis-orders; and (5) NEURAL [36], comprising speech samples from in-dividuals with various neurological conditions. Each dataset contains recordings from over twenty patients, with paired audio samples and corresponding transcripts. For training and evaluation, we segment these recordings into sentence-level audio-transcript pairs, resulting in approximately X training pairs and Y validation pairs. Our usage of these privacy-concerned datasets is approved by IRB [37].\n2) Default Experimental Setting: We evaluate our framework across multiple state-of-the-art LLMs, including Llama-3.2-1B [38], Llama-3.2-3B [38], Phi-3.5-mini [39], Gemma-2-2B [13], and StableLM-2-1.6B [40]. These models represent different architectures and parameter scales, allowing a comprehensive evaluation of our alignment approach. For LLM configuration, we employ maximum length padding in the tokenizer and set the temperature to 0.1 with top-k sampling (k=50) to ensure consistent and high-quality outputs. For audio feature extraction, we utilize the wav2vec2-base-960h model [15] as our ASR model. It was pre-trained on 960 hours of Lib-riSpeech data. All audio inputs are preprocessed with a sampling rate of 16 kHz to match the model's training conditions. Our transformer-based projector employs a compact yet effective architecture with 4 attention heads, a hidden dimension of 256, and 4 transformer layers. We empirically set the casted token size to 30, which adequately captures typical sentence lengths while maintaining computational efficiency.\nThe training process uses the AdamW optimizer with an initial learning rate of le-3 and a linear decay schedule. This configuration balances training stability with convergence speed, particularly im-portant for edge deployment scenarios where training resources may be limited. The experiments are run on a single Nvidia P100 GPU, whose memory and computational power are close to edge devices like Jetson Orin AGX or Dell Precision 7560 with Laptop GPU.\n3) Evaluation Methods: We evaluate our framework from both effectiveness and efficiency perspectives. For effectiveness, we em-ploy a dual-path comparison approach: given an audio input, we compare the output from our ASR-LLM system (Output) with the output generated by directly feeding the ground truth transcript to the"}, {"title": "B. Results", "content": "In our experiments, we first analyze the training loss convergence of our method compared to three baselines. We evaluate an ASR-LLM system composed of wav2vec2 and Gemma-2-2B on two datasets: ADRESS-IS2020 and ENNI. Training loss is tracked over 1000 epochs. As shown in Fig. 6, our method achieves convergence within 400 epochs on ADRESS-IS2020 and demonstrates even faster convergence (within 100 epochs) on the ENNI dataset. This rapid convergence enables efficient ASR-LLM alignment while minimizing computational resource usage and training time. Additionally, we can observe the drastic fluctuations of the loss curve for the three baselines, indicating the training under these three methods can be less stable. One thing to notice is that, since the baselines are designed without considering the resource restrictions, we must limit their trainable parameters so they can run in resource-limited environments."}, {"title": "V. CONCLUSION", "content": "In this paper, we present the Tiny-Align framework, which enables edge ASR-LLM alignment. By properly designing a transformer-based projector-BridgeForme-and training it under EmbedLink, Tiny-Align can significantly lower the training cost. BridgeFormer can both effectively and efficiently build a shared embedding space, where the audio embeddings can be properly converted into LLM-compatible embeddings. With the proper ASR encoder chosen, and the boost from instruction injection, Tiny-Align results in significantly higher performance in edge ASR-LLM alignment."}]}