{"title": "Small Object Few-shot Segmentation for Vision-based\nIndustrial Inspection", "authors": ["Zilong Zhang", "Chang Niu", "Zhibin Zhao", "Xingwu Zhang", "Xuefeng Chen"], "abstract": "Vision-based industrial inspection (VII) aims to locate defects quickly and ac-\ncurately. Supervised learning under a close-set setting and industrial anomaly\ndetection, as two common paradigms in VII, face different problems in practical\napplications. The former is that various and sufficient defects are difficult to obtain,\nwhile the latter is that specific defects cannot be located. To solve these problems, in\nthis paper, we focus on the few-shot semantic segmentation (FSS) method, which\ncan locate unseen defects conditioned on a few annotations without retraining.\nCompared to common objects in natural images, the defects in VII are small. This\nbrings two problems to current FSS methods: \u2460 distortion of target semantics and\n\u2461 many false positives for backgrounds. To alleviate these problems, we propose a\nsmall object few-shot segmentation (SOFS) model. The key idea for alleviating \u2460\nis to avoid the resizing of the original image and correctly indicate the intensity\nof target semantics. SOFS achieves this idea via the non-resizing procedure and\nthe prototype intensity downsampling of support annotations. To alleviate \u2461, we\ndesign an abnormal prior map in SOFS to guide the model to reduce false positives\nand propose a mixed normal Dice loss to preferentially prevent the model from\npredicting false positives. SOFS can achieve FSS and few-shot anomaly detec-\ntion determined by support masks. Diverse experiments substantiate the superior\nperformance of SOFS. Code is available at https://github.com/zhangzilongc/SOFS.", "sections": [{"title": "1 Introduction", "content": "Vision-based industrial inspection (VII) automatically identifies defects in industrial processes and\nensures product quality. Compared to object recognition in natural images, defect recognition in VII\nis fine-grained. As shown in Fig.1, fine-grained defects appear as small objects, the area proportions\nof defects are mostly less than 0.3%, and the smallest is even 0.015%.\nThe common paradigm in VII is supervised learning under a close-set setting [31, 13, 63, 25, 55].\nThis paradigm requires researchers to collect a large number of various defects in specific scenarios,\nand then design specific models for fitting the distribution of small defects in training. Since industrial\nprocesses are generally optimized to minimize the production of defective products [52], collecting\nvarious and sufficient defects for training is often intractable, which makes it difficult for such methods\nto be widely applied. To tackle this problem, industrial anomaly detection (IAD) [34, 7, 33, 42, 58, 54]\nhas been widely adopted recently. IAD only requires normal samples to detect defects, while there\nare a large number of normal samples. Such a characteristic is conducive to the application of IAD in\nreal scenarios. However, due to limitations of the problem formulation of IAD, it regards various"}, {"title": "", "content": "defects as the same class, i.e., abnormal class, which prevents it from locating specific defects. In real\nindustrial processes, different defects have significantly different impacts on products, e.g., scratches\non the printed circuit board affect the appearance, while broken circuits affect the use. Ablation of\naircraft engine blades affects engine efficiency, and cracks may cause engine fire [59]. Thus, locating\nspecific defects has significant practical value.\nTo break through the above limitations, this paper focuses on the few-shot semantic segmentation\n(FSS) method. FSS divides the input into the query and support sets following the episode paradigm\n[44]. It segments the query targets conditioned on the semantic clues from a few support annotations\nand can quickly adapt to unseen classes without retraining. These properties make FSS an ideal\napproach for VII. The achievement of previous FSS methods [20, 41, 50, 30, 38, 48] is built upon\nthe accurate extraction of semantic features of the support set, determined by the feature intensity in\nthe downsampling of support annotations and the information in the resizing image. As shown in\nFig. 1, applying these methods to segment small defects brings two problems: failure to segment and\nmany false positives for backgrounds. The former is due to the distortion of target semantics, caused\nby the information loss during the resizing of the original image and the feature intensity bias in the\ndownsampling of support annotations. The latter is caused by the missing defect, e.g., the first and\nthird rows in Fig. 1, and the train-set overfitting.\nIn this paper, to alleviate the above problems, we propose a small object few-shot segmentation\n(SOFS) model for VII. SOFS alleviates the distortion of small objects by 1) avoiding the resizing and\n2) correctly indicating the intensity of target semantics. Specifically, 1) SOFS uses a non-resizing\nprocedure to crop small objects in training and adopt the sliding window mechanism in the test.\nSOFS ensures that the pixel area of small objects encoded by the model is consistent with that in\nthe original image. 2) We propose the prototype intensity downsampling of support annotations to\ncorrect the bias caused by the common interpolation. This downsampling is calculated by all pixels\nof corresponding regions of target semantics, providing a better estimate of target semantics.\nTo reduce false positives caused by the missing defect and the train-set overfitting, 1) We design an\nabnormal prior map in SOFS to guide the model to highlight the semantic while ignoring the normal\nbackground. This design also enables SOFS to have both FSS and few-shot anomaly detection (FAD)"}, {"title": "", "content": "abilities. 2) We propose a mixed normal Dice loss to impose a large penalty when the model predicts\nfalse positives, preferentially preventing the model from predicting false positives.\nContributions: 1) We recognize FSS as an ideal solution for addressing the difficulty of obtaining\ndiverse defects and the inability of anomaly detection to identify specific defects in VII. 2) We\npropose SOFS to alleviate the distortion of target semantics and many false positives. The key idea of\nthe former is to avoid the resizing of the original image and correctly indicate the intensity of target\nsemantics. The latter is to guide the model to focus more on backgrounds. SOFS can adapt to unseen\nclasses without retraining, achieving FSS and FAD. To the best of our knowledge, SOFS is the first\nmodel to realize both FSS and FAD in VII. 3) FSS experiments on VISION V1 show that SOFS\nachieves state-of-the-art results, achieving 12.2% mean mIoU improvements compared to SegGPT.\nFAD experiments also demonstrate its competitive performance."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Few-shot Semantic Segmentation", "content": "Few-shot semantic segmentation [57, 5, 20, 56] predicts dense masks for novel classes with only a\nfew annotations. According to the different fusion granularity of support features and query features,\nprevious methods can be divided into image-level feature fusion [20, 56, 41, 45, 50] or pixel-level\nfeature fusion [30, 38, 48]. The image-level feature fusion methods extract the semantic clue of\nsupport sets by a masked average pooling of a global image and then fuse this prototype with the\nquery features. Since a pooling prototype feature cannot cover all regions of an object, the pixel-level\nfeature fusion methods are proposed to mine the correspondence between the query pixel-level\nfeatures and the support semantic-related pixel-level features, where the residual connection in the\ncross attention plays the role of fusing query and support features.\nPrevious methods mostly focus on the segmentation of PASCAL-5i [36, 11] and COCO-20i [27, 24],\nwhere the segmented objects are mostly salient. In comparison, the segmented objects in VII are\ngenerally small. In VISION V1 [1], 84% of defect types have area proportions less than 0.3%,\nand 38% of defect types have pixel areas less than 900. These characteristics either cause previous\nmethods to predict many false positives or fail to segment."}, {"title": "2.2 Small Object Recognition", "content": "The term \"small object\" refers to objects that occupy a small fraction of the input image [32], e.g., in\nthe widely used MS COCO dataset [24], it defines objects whose bounding box is 32 \u00d7 32 pixels\nor less in a typical 480 \u00d7 640 image, while other datasets define the objects that occupy 10% of the\nimage. The main challenges for small object recognition include information loss, low tolerance for\nbounding box perturbation, etc [6]. Information loss refers to the fact that the feature information of\nsmall objects is almost wiped out during the downsampling of the feature extractor, it has the greatest\nimpact on performance. To alleviate this issue, there are mainly three kinds of methods. The first is to\nmaintain the high-resolution feature map while retaining the fast process [49, 51, 10]. The second is\na super-resolution-based method [15, 2, 21], which restores the distorted structures of small objects.\nThe third is to process small objects by multi-scale learning and hierarchical feature fusion [40, 61].\nPrevious methods mainly focus on small object recognition in a close-set setting. In this paper, we\nstudy this in an open-set setting, the target is to learn the meta-class-agnostic feature."}, {"title": "2.3 Few-shot Anomaly Detection (Abnormal Segmentation)", "content": "Recently, few-shot anomaly detection (FAD) [12, 17, 35, 62, 19] has been proposed to simultaneously\nsolve the problem of difficulty in collecting various and sufficient defects in VII and the problem of\nrequiring retraining for different objects. FAD can quickly adapt to unknown classes by a few normal\nsamples, i.e., support images. FAD methods detect anomalies based on the similarity of pixel-level\nfeatures between the query image and the support images. Furthermore, recent works [19, 22, 4] add\ntextual descriptions to supplement the missing details of the image modality.\nAnomaly detection regards various defects as the same class, i.e., abnormal class, which prevents\nit from identifying specific defects. In VII, different defects have significantly different impacts"}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Problem Formulation", "content": "Few-shot semantic segmentation trains model to capture the meta-knowledge of the training set,\nadapting to novel objects with only a few annotated support images. In definition, the model is trained\non the training set $D_{\\text{train}}$ and tested on the test set $D_{\\text{test}}$. Suppose the category sets in $D_{\\text{train}}$ and\n$D_{\\text{test}}$ are $C_{\\text{train}}$ and $C_{\\text{test}}$ respectively. In the few-shot setting, $C_{\\text{train}} \\cap C_{\\text{test}} = \\emptyset$. Episodes are\napplied to $D_{\\text{train}}$ and $D_{\\text{test}}$, each episode consists of a query set $Q = \\{(I^q, M^q)\\}$ and a support\nset $S = \\{(I^s,M^s)\\}_{i=1}^K$ with the same class, where $I^q, I^s \\in \\mathbb{R}^{H\\times W\\times 3}$ represent the RGB images\nand $M^q, M^s \\in \\{0,1\\}^{H\\times W}$ denote their binary masks, $K$ denotes the number of support images. In\ntraining, both $M^q$ and $M^s$ are used, while only $M^s$ are accessible in testing. During testing, the model\nrequires no optimization for the novel classes. In FAD, $M^q \\in \\{0\\}^{H\\times W}$."}, {"title": "3.2 Small Object Few-shot Segmentation", "content": ""}, {"title": "3.2.1 Overview", "content": "The proposed small object few-shot segmentation (SOFS) model consists of three major steps: 1)\nsmall object enhancement designs, 2) feature augmenter, 3) feature fusion and meta prediction, as\nillustrated in Fig. 2, where the small object enhancement designs alleviate the distortion of small\nobjects. In training, SOFS crops the semantic area on the support images and query images (Section\n3.2.2). The cropped query/support images are then encoded by a feature augmenter to generate the\nsemantic-related features (Section 3.2.3). Afterward, the encoded query and support features are"}, {"title": "3.2.2 Small Object Enhancement Designs", "content": "Non-resizing Procedure. The core idea is to ensure that the pixel area of small objects encoded by\nthe model is consistent with that in the original image. As shown in Fig. 2, the non-resizing procedure\nrandomly crops the small object on the original image in training and uses the sliding window\nmechanism to process all regions of the query image in the test. The reason why the non-resizing\nprocedure can be used is that the segmentation of small defects in VII has a strong correlation with the\nsurrounding areas, but a weak correlation with distant areas, which eliminates the need for the model\nto establish long-range dependencies. In addition to the above benefits, the non-resizing procedure\ncan produce normal samples (the non-defective areas are all normal areas). We show in Section 3.2.5\nhow these normal samples alleviate model overfitting.\nIn the following part, we refer to query set and support set after the non-resizing procedure as\n$Q = \\{(I^q, M^q)\\}$ and $S = \\{(I^{(1)}, M^{(1)})\\}$, where we show the case $K = 1$ without loss of generality.\nPrototype Intensity Downsampling. To extract the prototype feature on the support image, we need\nto downsample the support mask $M^s$ to ensure that it is consistent with the size of the support feature\nmap. As shown in the patch-wise original mask of Fig. 3, every patch represents a region that is highly\ncorrelated with corresponding features on the feature map, since the segmented object is small, some\ncorresponding features of the feature map may be weak, occupying only a small part of the original\nsegmentation area. However, if we use the common bilinear/bicubic interpolation to downsample\nthe original mask to indicate the feature intensity, since the bilinear/bicubic interpolation only uses\n4/16 points to get the result, the result may be overestimated or underestimated, as shown in Fig.\n3, which leads to the distortion of target semantics. To alleviate this issue, we propose a prototype\nintensity downsampling to replace the common bilinear/bicubic interpolation in the downsampling of\n$M^s$. Specifically, we employ a $l \\times l$ convolution layer to process $M^s$:"}, {"title": "", "content": "$$\nM^s = \\frac{\\text{Conv}(M^s)}{l\\times l},\n\\tag{1}\n$$"}, {"title": "", "content": "where $\\tilde{M}^s \\in \\mathbb{R}^{H\\times W}$ denotes the downsampling of the support mask, $l$ denotes the multiple of\ndownsampling of feature map, the parameters in the convolution kernel are all 1, the stride of\nconvolution is $l$. $\\tilde{M}^s$ is calculated by all pixels of corresponding regions of features. It provides a\nbetter estimate of the intensity of prototype features, avoiding the mismatch of semantic clues."}, {"title": "3.2.3 Feature Augmenter", "content": "Firstly, we encode $I^s$ and $I^q$ separately, resulting in feature maps $F^s, F^q \\in \\mathbb{R}^{H\\times W\\times C}$. Subsequently,\nwe use a prior generation module to generate the prototype feature $p \\in \\mathbb{R}^{C}$, a semantic prior map\n$M^s \\in \\mathbb{R}^{H\\times W}$ and an abnormal prior map $M^a \\in \\mathbb{R}^{H\\times W}$. $p$ is extracted by a MaskAvgPool($F^s,M^s$),"}, {"title": "", "content": "where MaskAvgPool($\\cdot$) denotes the masked average pooling (detailed descriptions are provided in\nAppendix). $M^s_i$ and $M^a_i$ are formulated as follows:"}, {"title": "", "content": "$$\nM^s_i = \\Phi(\\mathcal{F}^q)(\\Phi(\\mathcal{F}^s))^T \\odot \\Psi(M^s_i),\n\\tag{2}\n$$"}, {"title": "", "content": "$$\nM^a_i = 1 - \\Phi(\\mathcal{F}^q)(\\Phi(\\mathcal{F}^s))^T \\odot (1 - \\Psi(M^s_i)),\n\\tag{3}\n$$"}, {"title": "", "content": "where $\\Phi(\\cdot): \\mathbb{R}^{H\\times W\\times C} \\rightarrow \\mathbb{R}^{HW \\times C}$ refers to the reshape function and $L_2$ normalized on feature\ndimension, $\\odot: \\mathbb{R}^{H\\times W} \\rightarrow \\mathbb{R}^{1\\times HW} \\rightarrow \\mathbb{R}^{HW \\times HW}$ refers to the reshape and repeat along the row,\n$\\Psi(\\cdot): \\mathbb{R}^{HW \\times HW} \\rightarrow \\mathbb{R}^{HW \\times 1} \\rightarrow \\mathbb{R}^{H\\times W}$ refers to taking the maximum value along the column and\nthe reshape. $M^s_i$ denotes the maximum similarity between each pixel-level query feature and support\nsemantic features. The larger the value on $M^s_i$, the more relevant it is to the semantics in the support\nset. $1 - M^a_i$ denotes the maximum similarity between each pixel-level query feature and support\nnormal features. The larger the value on $M^a_i$, the smaller the similarity with normal features in the\nsupport set, and the more abnormal the corresponding region is.\nThe motivation for adding $M^a_i$ is that we hope that SOFS can reduce false positives for backgrounds\nin VII. Specifically, in VII, there is a unique type of defect: the missing defect, where the target area\nmisses some components, e.g., the defect shown in Fig. 2 is a missing defect, we can only observe the\nimage background. If we only use semantic matching for the missing defect, it may highlight many\nfalse backgrounds. $M^a_i$ matches every pixel-level query feature with the normal support features, if\nthere is a missing defect, it can be highlighted and the normal background can not be. In addition,\n$M^a_i$ enables SOFS to have FAD ability, we can input the normal support image.\nAfter obtaining the above features, we concat $F^s$ with $p$, concat $F^q$ with $p$, $M^a_i$ and $M^s_i$, then use a\n$1 \\times 1$ convolution to produce $\\mathcal{F}^s$, $\\mathcal{F}^q \\in \\mathbb{R}^{H\\times W\\times C_1}$. Subsequently, we use the self-attention [43] to\nfurther improve $\\mathcal{F}^s$,$\\mathcal{F}^q$."}, {"title": "3.2.4 Feature Fusion and Meta Prediction", "content": "$\\mathcal{F}^s$, $\\mathcal{F}^q$ are subsequently fused by a non-learnable feature fusion. The matching mechanism follows\n[30] to replace the dot produce with the cosine similarities, which is formulated as follows:"}, {"title": "", "content": "$$\npqs = \\text{softmax}' (\\frac{(\\Phi'(\\mathcal{F}^s))^T \\Phi'(\\mathcal{F}^q)}{\\tau}) \\odot \\Phi'(\\mathcal{F}^q \\odot \\upsilon(M^s_i)) + \\Phi'(\\mathcal{F}^q),\n\\tag{4}\n$$"}, {"title": "", "content": "where $\\Phi'(\\cdot): \\mathbb{R}^{H\\times W\\times C} \\rightarrow \\mathbb{R}^{HW \\times C}$ refers to the reshape function and non $L_2$ normalized, $\\tau$\ncontrols the distribution shape, softmax$' (\\cdot)$ refers to the normalization along the row, i.e., reverse\nsoftmax [37], $\\upsilon(\\cdot): \\mathbb{R}^{H\\times W} \\rightarrow \\mathbb{R}^{H\\times W\\times C}$ refers to first expanding the new dimension and then\nreplicating along the expanded dimension. Eq. (4) can be regarded as a type of cross-attention, where\nthe learnable parameters are discarded. We think that the recognition of small objects does not need\nlots of parameters, more parameters may cause the risk of overfitting the category-specific information.\nAfter the feature fusion, we extract the prototype feature $p \\in \\mathbb{R}^{C_1}$ by a MaskAvgPool($\\mathcal{F}^q, M^s_i$).\nThen the prediction result of SOFS model is formulated as follows:"}, {"title": "", "content": "$$\nM_{\\text{pred}}^q = \\mathbb{I}(\\text{sum}(M^s_i) > 0)\\text{Sigmoid}(\\phi(\\mathcal{F}^q\\odot p)) + (1 - \\mathbb{I}(\\text{sum}(M^s_i) > 0))M^a_i,\n\\tag{5}\n$$"}, {"title": "", "content": "where $\\mathbb{I}(\\cdot)$ refers to a indicator function, Sigmoid($\\cdot$) is a sigmoid function, $\\phi(\\cdot): \\mathbb{R}^{HW \\times 1} \\rightarrow \\mathbb{R}^{H\\times W}$\nrefers to the reshape function. Compared with the learnable prediction classifier (a multi-layer\nperceptron) used in [30, 41], our meta prediction can be learned dynamically, it is encouraged to learn\na class-agnostic meta feature. $M_{\\text{pred}}^q$ is a combination of a semantics prediction and an abnormal\nprior map, if the support images are all normal samples, SOFS produces a result of few-shot abnormal\nsegmentation. Otherwise, SOFS produces a result of few-shot semantic segmentation."}, {"title": "3.2.5 Loss Function", "content": "In our experiments, we find that training SOFS by Dice loss [26] and the cross entropy loss suffers\nfrom a severe overfitting problem, producing many false positives. To alleviate this issue, we propose\na mixed normal Dice loss as follows:"}, {"title": "", "content": "$$\\n\\text{DiceLoss}_{\\text{normal}} (X, Y) = t (1 - \\frac{2 |X \\cap Y|}{|X| + |Y|}) + (1-t)\\beta (1 - \\frac{1}{\\eta |X| + 1}),\n\\tag{6}\n$$"}, {"title": "", "content": "where $t = \\mathbb{I}(\\text{sum}(Y) > 0)$, $X$, $Y$ refers to the prediction and the ground truth, $\\eta$ is a large hyperpa-\nrameter, e.g., 1e4, $\\beta$ is a hyperparameter to control the weight, we set $\\beta = 1$ in the experiment. The\nnormal sample term (the second term of Eq. (6)) can be regarded as a regularization term of the\noriginal Dice loss. At the beginning of training, the model almost outputs 0 for all inputs. As the\ntraining evolves, once the model has a false prediction for the normal sample, the backpropagation of\nthis term produces a larger gradient compared to defective samples, which preferentially prevents\nthe model from predicting false positives. This loss facilitates that the model does not produce false\npositives while predicting the defects as much as possible. In our implementation, the normal query\nsamples are from the non-defective areas in the non-resizing procedure, the probability of sampling\nnormal query samples is $\\alpha$.\nOur final loss function is a linear combination of the cross entropy loss and the mixed normal Dice\nloss, where the weight of the cross entropy loss is $\\gamma$."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Implementation Details", "content": "Datasets. FSS: We conduct experiments on VISION V1 [1] and Defect Spectrum [53]. VISION\nV1 [1] released in CVPR 2023 Vision-based Industrial InspectiON workshop 2, is one of the largest\nand most diverse manufacturing datasets in terms of both image number and available annotations.\nVISION V1 consists of 14 objects, spanning 44 defect types. 84% of defect types have area\nproportions less than 0.3%, and 38% of defect types have pixel areas less than 900. Defect Spectrum\n[53] relabeled the data from MVTec AD [3], DAGM2007 [47], part of VISION V1 [1] and Cotton-\nFabric [18], including 97 defect types. Defect Spectrum is the dataset with the largest number of\ndefect types so far. 63% of defect types have area proportions less than 2%. For the above datasets,\ncross-validation is conducted by dividing all objects into 3 folds, we ensure that the distributions of\narea proportions of defects in each fold are roughly the same. In the experiments, two folds serve\nas training data, while the remaining one is used for testing. To ensure performance stability and\nfairness for comparison, the query/support pairs sampling by all methods are the same. FAD:We use\ntwo benchmark FAD datasets, i.e., MVTec AD [3] and VisA [64], including 27 objects in total. More\ndetails of datasets and experiments are shown in Appendix.\nMetrics. FSS: We adopt the mean intersection over union (mIoU) as the evaluation metric. We\ndenote $\\text{mIoU} = \\frac{1}{C}\\sum_{i=1}^C \\text{IoU}_i$, where $C$ is the number of defect classes in each fold, and IoU$_{\\text{i}}$\nindicates intersection-over-union for class $i$. FAD: For classification, we report Area Under the\nReceiver Operating Characteristic (AUROC). For segmentation, we report pixel-wise AUROC and\npixel-wise F\u2081-max.\nTraining and Test Details. FSS: We use ResNet-50 [16] and DINO v2 Base [28] as the encoder\nto verify the effectiveness of the proposed method and reimplementation methods on different\nbackbones. The training input sizes of ResNet-50 and DINO v2 Base are set to 512 \u00d7 512 and\n518 \u00d7 518 respectively. $l$ in the prototype intensity downsampling are set to 8 and 14 for ResNet-50\nand DINO v2 Base respectively. The probability $\\alpha$ is set to 0.3, $\\eta$ is set to 1e4, $\\gamma$ is set to 0.01, $\\tau$ is\nset to 0.1. In the non-resizing procedure, the stride of sliding window and the input size in test are the\nsame as the input size in training. All reimplementation methods and SOFS use the same training\naugmentations. FAD: We use DINO v2 Base [28] as the encoder. The input size is set to 518 \u00d7 518.\nWe only test SOFS. More details of SOFS and reimplementation methods are shown in Appendix."}, {"title": "4.2 Comparison with State-of-the-Art Methods", "content": "FSS: In Table 1, we report the comparison of SOFS with other state-of-the-art FSS approaches\non VISION V1. Compared with other methods, SOFS can gain passable improvement on ResNet-\n50. However, due to the insufficient discriminability of pre-trained features for small defects, the"}, {"title": "5 Conclusion", "content": "In this paper, we propose SOFS to tackle the problems existing in supervised learning under a\nclose-set setting and industrial anomaly detection in VII. SOFS significantly improves the small\nobject segmentation by avoiding resizing and a prototype feature intensity downsampling. Further,\nwe add an abnormal prior map in SOFS to guide the model to reduce false positives for backgrounds,\nwhile enabling SOFS to realize both FSS and FAD. In addition, we propose a mixed normal Dice\nloss, non-learnable feature fusion, and meta prediction to alleviate the overfitting problem. The"}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Supplemental Details of SOFS", "content": "Masked average pooling. The masked average pooling is formulated as follows:"}, {"title": "", "content": "$$\\n\\text{Mask Avg Pool}(F,M) =\\frac{\\text{sum}'(F \\odot \\upsilon(M))}{\\text{sum}(M)}\n\\tag{7}\n$$"}, {"title": "", "content": "where $F \\in [\\mathbb{R}^{H\\times W\\times C}, M \\in \\mathbb{R}^{H\\times W}, \\upsilon(\\cdot) : \\mathbb{R}^{H\\times W} \\rightarrow \\mathbb{R}^{H\\times W\\times C}$ refers to first expanding the new\ndimension and then replicating along the expanded dimension, $\\text{sum}'(\\cdot) : \\mathbb{R}^{H\\times W\\times C} \\rightarrow \\mathbb{R}^C$ refers to\nthe addition of spatial features, $\\text{sum}(\\cdot) : \\mathbb{R}^{H\\times W} \\rightarrow \\mathbb{R}$ refers to the addition along spatial dimensions.\nExtension to K-shot setting. In extension to K-shot ($K > 1$) setting, K support images with their\nannoted masks $S = \\{(I_i^s, M_i^s)\\}_{i=1}^K$ and the query set $Q = \\{(I^q, M^q)\\}$ are given. $\\tilde{M}_i^s$, $p_i$, $M^a_i$ and $M^s_i$\nare firstly calculated, where $p_i$ is extracted by a MaskAvgPool($F^s, M_i^s$), $M^s_i$ denote a semantic"}]}