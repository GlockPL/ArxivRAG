{"title": "Small Object Few-shot Segmentation for Vision-based Industrial Inspection", "authors": ["Zilong Zhang", "Chang Niu", "Zhibin Zhao", "Xingwu Zhang", "Xuefeng Chen"], "abstract": "Vision-based industrial inspection (VII) aims to locate defects quickly and ac- curately. Supervised learning under a close-set setting and industrial anomaly detection, as two common paradigms in VII, face different problems in practical applications. The former is that various and sufficient defects are difficult to obtain, while the latter is that specific defects cannot be located. To solve these problems, in this paper, we focus on the few-shot semantic segmentation (FSS) method, which can locate unseen defects conditioned on a few annotations without retraining. Compared to common objects in natural images, the defects in VII are small. This brings two problems to current FSS methods: \u2460 distortion of target semantics and \u2461 many false positives for backgrounds. To alleviate these problems, we propose a small object few-shot segmentation (SOFS) model. The key idea for alleviating \u2460 is to avoid the resizing of the original image and correctly indicate the intensity of target semantics. SOFS achieves this idea via the non-resizing procedure and the prototype intensity downsampling of support annotations. To alleviate \u2461, we design an abnormal prior map in SOFS to guide the model to reduce false positives and propose a mixed normal Dice loss to preferentially prevent the model from predicting false positives. SOFS can achieve FSS and few-shot anomaly detec- tion determined by support masks. Diverse experiments substantiate the superior performance of SOFS.", "sections": [{"title": "1 Introduction", "content": "Vision-based industrial inspection (VII) automatically identifies defects in industrial processes and ensures product quality. Compared to object recognition in natural images, defect recognition in VII is fine-grained. As shown in Fig.1, fine-grained defects appear as small objects, the area proportions of defects are mostly less than 0.3%, and the smallest is even 0.015%.\nThe common paradigm in VII is supervised learning under a close-set setting [31, 13, 63, 25, 55]. This paradigm requires researchers to collect a large number of various defects in specific scenarios, and then design specific models for fitting the distribution of small defects in training. Since industrial processes are generally optimized to minimize the production of defective products [52], collecting various and sufficient defects for training is often intractable, which makes it difficult for such methods to be widely applied. To tackle this problem, industrial anomaly detection (IAD) [34, 7, 33, 42, 58, 54] has been widely adopted recently. IAD only requires normal samples to detect defects, while there are a large number of normal samples. Such a characteristic is conducive to the application of IAD in real scenarios. However, due to limitations of the problem formulation of IAD, it regards various defects as the same class, i.e., abnormal class, which prevents it from locating specific defects. In real industrial processes, different defects have significantly different impacts on products, e.g., scratches on the printed circuit board affect the appearance, while broken circuits affect the use. Ablation of aircraft engine blades affects engine efficiency, and cracks may cause engine fire [59]. Thus, locating specific defects has significant practical value.\nTo break through the above limitations, this paper focuses on the few-shot semantic segmentation (FSS) method. FSS divides the input into the query and support sets following the episode paradigm [44]. It segments the query targets conditioned on the semantic clues from a few support annotations and can quickly adapt to unseen classes without retraining. These properties make FSS an ideal approach for VII. The achievement of previous FSS methods [20, 41, 50, 30, 38, 48] is built upon the accurate extraction of semantic features of the support set, determined by the feature intensity in the downsampling of support annotations and the information in the resizing image. As shown in Fig. 1, applying these methods to segment small defects brings two problems: failure to segment and many false positives for backgrounds. The former is due to the distortion of target semantics, caused by the information loss during the resizing of the original image and the feature intensity bias in the downsampling of support annotations. The latter is caused by the missing defect, e.g., the first and third rows in Fig. 1, and the train-set overfitting.\nIn this paper, to alleviate the above problems, we propose a small object few-shot segmentation (SOFS) model for VII. SOFS alleviates the distortion of small objects by 1) avoiding the resizing and 2) correctly indicating the intensity of target semantics. Specifically, 1) SOFS uses a non-resizing procedure to crop small objects in training and adopt the sliding window mechanism in the test. SOFS ensures that the pixel area of small objects encoded by the model is consistent with that in the original image. 2) We propose the prototype intensity downsampling of support annotations to correct the bias caused by the common interpolation. This downsampling is calculated by all pixels of corresponding regions of target semantics, providing a better estimate of target semantics.\nTo reduce false positives caused by the missing defect and the train-set overfitting, 1) We design an abnormal prior map in SOFS to guide the model to highlight the semantic while ignoring the normal background. This design also enables SOFS to have both FSS and few-shot anomaly detection (FAD)"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Few-shot Semantic Segmentation", "content": "Few-shot semantic segmentation [57, 5, 20, 56] predicts dense masks for novel classes with only a few annotations. According to the different fusion granularity of support features and query features, previous methods can be divided into image-level feature fusion [20, 56, 41, 45, 50] or pixel-level feature fusion [30, 38, 48]. The image-level feature fusion methods extract the semantic clue of support sets by a masked average pooling of a global image and then fuse this prototype with the query features. Since a pooling prototype feature cannot cover all regions of an object, the pixel-level feature fusion methods are proposed to mine the correspondence between the query pixel-level features and the support semantic-related pixel-level features, where the residual connection in the cross attention plays the role of fusing query and support features.\nPrevious methods mostly focus on the segmentation of PASCAL-5i [36, 11] and COCO-20i [27, 24], where the segmented objects are mostly salient. In comparison, the segmented objects in VII are generally small. In VISION V1 [1], 84% of defect types have area proportions less than 0.3%, and 38% of defect types have pixel areas less than 900. These characteristics either cause previous methods to predict many false positives or fail to segment."}, {"title": "2.2 Small Object Recognition", "content": "The term \"small object\" refers to objects that occupy a small fraction of the input image [32], e.g., in the widely used MS COCO dataset [24], it defines objects whose bounding box is 32 \u00d7 32 pixels or less in a typical 480 \u00d7 640 image, while other datasets define the objects that occupy 10% of the image. The main challenges for small object recognition include information loss, low tolerance for bounding box perturbation, etc [6]. Information loss refers to the fact that the feature information of small objects is almost wiped out during the downsampling of the feature extractor, it has the greatest impact on performance. To alleviate this issue, there are mainly three kinds of methods. The first is to maintain the high-resolution feature map while retaining the fast process [49, 51, 10]. The second is a super-resolution-based method [15, 2, 21], which restores the distorted structures of small objects. The third is to process small objects by multi-scale learning and hierarchical feature fusion [40, 61].\nPrevious methods mainly focus on small object recognition in a close-set setting. In this paper, we study this in an open-set setting, the target is to learn the meta-class-agnostic feature."}, {"title": "2.3 Few-shot Anomaly Detection (Abnormal Segmentation)", "content": "Recently, few-shot anomaly detection (FAD) [12, 17, 35, 62, 19] has been proposed to simultaneously solve the problem of difficulty in collecting various and sufficient defects in VII and the problem of requiring retraining for different objects. FAD can quickly adapt to unknown classes by a few normal samples, i.e., support images. FAD methods detect anomalies based on the similarity of pixel-level features between the query image and the support images. Furthermore, recent works [19, 22, 4] add textual descriptions to supplement the missing details of the image modality.\nAnomaly detection regards various defects as the same class, i.e., abnormal class, which prevents it from identifying specific defects. In VII, different defects have significantly different impacts"}, {"title": "3 Methods", "content": null}, {"title": "3.1 Problem Formulation", "content": "Few-shot semantic segmentation trains model to capture the meta-knowledge of the training set, adapting to novel objects with only a few annotated support images. In definition, the model is trained on the training set $D_{train}$ and tested on the test set $D_{test}$. Suppose the category sets in $D_{train}$ and $D_{test}$ are $C_{train}$ and $C_{test}$ respectively. In the few-shot setting, $C_{train} \\cap C_{test} = \\emptyset$. Episodes are applied to $D_{train}$ and $D_{test}$, each episode consists of a query set $Q = \\{(I^q, M^q)\\}$ and a support set $S = \\{(I^s,M^s)\\}_{i=1}^K$ with the same class, where $I^q, I^s \\in \\mathbb{R}^{H\\times W\\times 3}$ represent the RGB images and $M^q, M^s \\in \\{0,1\\}^{H\\times W}$ denote their binary masks, $K$ denotes the number of support images. In training, both $M^q$ and $M^s$ are used, while only $M^s$ are accessible in testing. During testing, the model requires no optimization for the novel classes. In FAD, $M^s \\in \\{0\\}^{H\\times W}$."}, {"title": "3.2 Small Object Few-shot Segmentation", "content": null}, {"title": "3.2.1 Overview", "content": "The proposed small object few-shot segmentation (SOFS) model consists of three major steps: 1) small object enhancement designs, 2) feature augmenter, 3) feature fusion and meta prediction, as illustrated in Fig. 2, where the small object enhancement designs alleviate the distortion of small objects. In training, SOFS crops the semantic area on the support images and query images (Section 3.2.2). The cropped query/support images are then encoded by a feature augmenter to generate the semantic-related features (Section 3.2.3). Afterward, the encoded query and support features are"}, {"title": "3.2.2 Small Object Enhancement Designs", "content": "Non-resizing Procedure. The core idea is to ensure that the pixel area of small objects encoded by the model is consistent with that in the original image. As shown in Fig. 2, the non-resizing procedure randomly crops the small object on the original image in training and uses the sliding window mechanism to process all regions of the query image in the test. The reason why the non-resizing procedure can be used is that the segmentation of small defects in VII has a strong correlation with the surrounding areas, but a weak correlation with distant areas, which eliminates the need for the model to establish long-range dependencies. In addition to the above benefits, the non-resizing procedure can produce normal samples (the non-defective areas are all normal areas). We show in Section 3.2.5 how these normal samples alleviate model overfitting.\nIn the following part, we refer to query set and support set after the non-resizing procedure as $Q = \\{(I^q, M^q)\\}$ and $S = \\{(I^{(1)}, M^{(1)})\\}$, where we show the case $K = 1$ without loss of generality.\nPrototype Intensity Downsampling. To extract the prototype feature on the support image, we need to downsample the support mask $M^s$ to ensure that it is consistent with the size of the support feature map. As shown in the patch-wise original mask of Fig. 3, every patch represents a region that is highly correlated with corresponding features on the feature map, since the segmented object is small, some corresponding features of the feature map may be weak, occupying only a small part of the original segmentation area. However, if we use the common bilinear/bicubic interpolation to downsample the original mask to indicate the feature intensity, since the bilinear/bicubic interpolation only uses 4/16 points to get the result, the result may be overestimated or underestimated, as shown in Fig. 3, which leads to the distortion of target semantics. To alleviate this issue, we propose a prototype intensity downsampling to replace the common bilinear/bicubic interpolation in the downsampling of $M^s$. Specifically, we employ a $l \\times l$ convolution layer to process $M^s$:\n$M^s = \\frac{Conv(M^s)}{l\\times l}, \\qquad(1)$"}, {"title": "3.2.3 Feature Augmenter", "content": "Firstly, we encode $I^s$ and $I^q$ separately, resulting in feature maps $F^s, F^q \\in \\mathbb{R}^{H\\times W\\times C}$. Subsequently, we use a prior generation module to generate the prototype feature $p \\in \\mathbb{R}^{C}$, a semantic prior map $M_s \\in \\mathbb{R}^{H\\times W}$ and an abnormal prior map $M_a \\in \\mathbb{R}^{H\\times W}$. $p$ is extracted by a $MaskAvgPool(F^s,M_s)$,"}, {"title": "3.2.4 Feature Fusion and Meta Prediction", "content": "$F^s, F^q$ are subsequently fused by a non-learnable feature fusion. The matching mechanism follows [30] to replace the dot produce with the cosine similarities, which is formulated as follows:\n$P^{qs} = softmax'(\\frac{\\Phi'(F^q) \\cdot \\Phi'(F^s)^T}{\\tau})\\Phi'(F^s \\odot \\psi(M^s)) + \\Phi'(F^q), \\qquad(4)$\nwhere $\\Phi'(.): \\mathbb{R}^{H\\times W\\times C} \\rightarrow \\mathbb{R}^{HW\\times C}$ refers to the reshape function and non $L2$ normalized, $\\tau$ controls the distribution shape, $softmax'(.)$ refers to the normalization along the row, i.e., reverse softmax [37], $\\psi(.): \\mathbb{R}^{H\\times W} \\rightarrow \\mathbb{R}^{H\\times W\\times C}$ refers to first expanding the new dimension and then replicating along the expanded dimension. Eq. (4) can be regarded as a type of cross-attention, where the learnable parameters are discarded. We think that the recognition of small objects does not need lots of parameters, more parameters may cause the risk of overfitting the category-specific information.\nAfter the feature fusion, we extract the prototype feature $p \\in \\mathbb{R}^{C'}$ by a $MaskAvgPool(F^q, M^s)$. Then the prediction result of SOFS model is formulated as follows:\n$M^{pred} = \\mathbb{I}(sum(M^s) > 0)Sigmoid(\\phi(F^q \\otimes p)) + (1 - \\mathbb{I}(sum(M^s) > 0))M_a, \\qquad(5)$"}, {"title": "3.2.5 Loss Function", "content": "In our experiments, we find that training SOFS by Dice loss [26] and the cross entropy loss suffers from a severe overfitting problem, producing many false positives. To alleviate this issue, we propose a mixed normal Dice loss as follows:\n$DiceLoss_{normal} (X, Y) = (1 - \\frac{2 \\cdot X \\bigcap Y}{X + Y}) + (1 - \\mathbb{I}(sum(Y) > 0))(\\beta (1 - \\frac{1}{\\eta X + 1})), \\qquad(6)$"}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Implementation Details", "content": "Datasets. FSS: We conduct experiments on VISION V1 [1] and Defect Spectrum [53]. VISION V1 [1] released in CVPR 2023 Vision-based Industrial InspectiON workshop 2, is one of the largest and most diverse manufacturing datasets in terms of both image number and available annotations. VISION V1 consists of 14 objects, spanning 44 defect types. 84% of defect types have area proportions less than 0.3%, and 38% of defect types have pixel areas less than 900. Defect Spectrum [53] relabeled the data from MVTec AD [3], DAGM2007 [47], part of VISION V1 [1] and Cotton-Fabric [18], including 97 defect types. Defect Spectrum is the dataset with the largest number of defect types so far. 63% of defect types have area proportions less than 2%. For the above datasets, cross-validation is conducted by dividing all objects into 3 folds, we ensure that the distributions of area proportions of defects in each fold are roughly the same. In the experiments, two folds serve as training data, while the remaining one is used for testing. To ensure performance stability and fairness for comparison, the query/support pairs sampling by all methods are the same. FAD:We use two benchmark FAD datasets, i.e., MVTec AD [3] and VisA [64], including 27 objects in total. More details of datasets and experiments are shown in Appendix.\nMetrics. FSS: We adopt the mean intersection over union (mIoU) as the evaluation metric. We denote $mIoU = \\frac{1}{C}\\sum_{i=1}^{C}IoU_i$, where $C$ is the number of defect classes in each fold, and $IoU_i$ indicates intersection-over-union for class $i$. FAD: For classification, we report Area Under the Receiver Operating Characteristic (AUROC). For segmentation, we report pixel-wise AUROC and pixel-wise $F_1$-max.\nTraining and Test Details. FSS: We use ResNet-50 [16] and DINO v2 Base [28] as the encoder to verify the effectiveness of the proposed method and reimplementation methods on different backbones. The training input sizes of ResNet-50 and DINO v2 Base are set to 512 \u00d7 512 and 518 \u00d7 518 respectively. $l$ in the prototype intensity downsampling are set to 8 and 14 for ResNet-50 and DINO v2 Base respectively. The probability $\\alpha$ is set to 0.3, $\\eta$ is set to $1e4$, $\\gamma$ is set to 0.01, $\\tau$ is set to 0.1. In the non-resizing procedure, the stride of sliding window and the input size in test are the same as the input size in training. All reimplementation methods and SOFS use the same training augmentations. FAD: We use DINO v2 Base [28] as the encoder. The input size is set to 518 \u00d7 518. We only test SOFS. More details of SOFS and reimplementation methods are shown in Appendix."}, {"title": "4.2 Comparison with State-of-the-Art Methods", "content": "FSS: In Table 1, we report the comparison of SOFS with other state-of-the-art FSS approaches on VISION V1. Compared with other methods, SOFS can gain passable improvement on ResNet-50. However, due to the insufficient discriminability of pre-trained features for small defects, the"}, {"title": "4.3 Ablation Study", "content": "We report the ablation studies in this section to investigate the effectiveness of each component. All ablation experiments are conducted under VISION V1 1-shot setting with DINO v2 Base backbone.\nSmall object enhancement designs ablation. Table 5 shows results regarding the effectiveness of small object enhancement designs. The baseline (line 1) uses the bilinear interpolation for $M_s$ and the standard resizing procedure, its performance is extremely low. Then we add the prototype feature intensity map (PFIM), resulting in improved results. Since many defects account for small area proportions in F0 and F1, the performances of F0 and F1 are still low. Due to \"Cylinder\" with large defects in F2, the improvement for F2 is significant. Subsequently, we only add the non-resizing procedure (NR). Compared with line 2, this brings a greater improvement. However, the improvement in F1 is still not enough. This is mainly because there are many defects with small pixel areas in F1. NR cannot deal with this kind of problem. Finally, we use PFIM and NR to simultaneously improve the performance for defects with small pixel areas and small area proportions, resulting in a significant improvement for F1.\nOther components ablation. Table 6 shows results regarding the effectiveness of other components in SOFS. The baseline (line 1) is built upon the matching mechanism in [30] (a type of cross attention), the learnable prediction classifier (a multi-layer perceptron) used in [30, 41] and trained by Dice loss [26]. We first replace Dice loss with the proposed mixed normal Dice loss (MNDL), resulting in a large improvement for F0 and F1. However, since the training on F2 does not include large defects (''Cylinder'' with large defects is in F2), the model with too many parameters causes overfitting to small defects. Then we replace the matching mechanism in [30] with a non-learnable feature fusion, the performances for F0, F1, and F2 improve. Subsequently, we add an abnormal prior map (APM). As shown in Fig. 8, APM can reduce the false positive regions of missing defects, resulting in further improvements. Finally, we replace the learnable prediction classifier with a meta prediction, bringing"}, {"title": "5 Conclusion", "content": "In this paper, we propose SOFS to tackle the problems existing in supervised learning under a close-set setting and industrial anomaly detection in VII. SOFS significantly improves the small object segmentation by avoiding resizing and a prototype feature intensity downsampling. Further, we add an abnormal prior map in SOFS to guide the model to reduce false positives for backgrounds, while enabling SOFS to realize both FSS and FAD. In addition, we propose a mixed normal Dice loss, non-learnable feature fusion, and meta prediction to alleviate the overfitting problem. The"}, {"title": "A Appendix", "content": null}, {"title": "A.1 Supplemental Details of SOFS", "content": "Masked average pooling. The masked average pooling is formulated as follows:\n$Mask Avg Pool(F,M) = \\frac{sum'(F\\odot v(M))}{sum(M)} \\qquad(7)$\nwhere $F \\in [\\mathbb{R}^{H\\times W\\times C}, M \\in \\mathbb{R}^{H\\times W}, v(.): \\mathbb{R}^{H\\times W} \\rightarrow \\mathbb{R}^{H\\times W\\times C}$ refers to first expanding the new dimension and then replicating along the expanded dimension, $sum'(.): \\mathbb{R}^{H\\times W\\times C} \\rightarrow \\mathbb{R}^{C}$ refers to the addition of spatial features, $sum(.): \\mathbb{R}^{H \\times W} \\rightarrow \\mathbb{R}$ refers to the addition along spatial dimensions.\nExtension to K-shot setting. In extension to K-shot (K > 1) setting, K support images with their annoted masks $S = \\{(I^i, M^i)\\}_{i=1}^K$ and the query set $Q = \\{(I^q, M^q)\\}$ are given. $M_s^i$, $p_i$, $M_{as}^i$ and $M_{ai}^q$ are firstly calculated, where $p_i$ is extracted by a $MaskAvgPool(F, M^i)$, $M_{asi}$ denote a semantic"}, {"title": "A.2 Supplemental Implementation Details", "content": "SOFS. SOFS is built upon the Pytorch [29] framework. All models are trained and tested on 8 NVIDIA GeForce RTX 2080 Ti GPUs. All reimplementation methods and SOFS use the same training augmentations, including rotation, blur, and flip. In the non-resizing procedure, we use the random crop, where the crop image includes the defect. We only use the non-resizing procedure if the area proportion of defect in the support set is less than 1%, otherwise, we use the common resizing. In addition, following the previous works, SOFS adopts features from different layers of backbones to further improve the useful information.\nSOFS is trained in an episode fashion for 50 epochs for VISION V1, the batch size is 4 for every GPU. During training, AdamW optimizer is adopted, and the learning rate is set to 1e-5, the weight decay is 0.01, and the \"poly\" strategy is used to adjust the learning rate. During testing, predictions are resized back to the original sizes of the input images, keeping the groundtruth labels intact.\nIn the few-shot anomaly detection, SOFS adopts the common resizing procedure.\nReimplementation of Comparison Methods We compare SOFS with the recent methods: PFENet [41], HDMNet [30], and SegGPT [46]. We reproduce them following the official codes and adjust the hyperparameters to maximize performance. For PFENet and HDMNet, we train the models by cross entropy loss and Dice loss [26]. Due to the small objects, we adjust the downsampling modules in PFENet and HDMNet, we do not downsample the feature map. The input sizes of models are the same as SOFS. In addition, we do not mask the support features in the feature extraction of the freezing encoder like previous methods, masking leads to poor performance. We only test the model for SegGPT, and the input size is set to 448 \u00d7 448, which is the same as the original setting. We do not observe the improvement if the input size is set to 512 \u00d7 512.\nWe use ResNet-50 [16] and DINO v2 Base [28] as the encoder to extract features with freezing parameters to verify the effectiveness of the proposed method and reimplementation methods on different backbones. For ResNet-50, we use the multi-scale features from layer 2 and layer 3, and the feature processing method in [33] is adopted to upsample the high-level features. We do not use PSPNet [60] to train the base learner, which is used in the previous methods [30, 20]. We find that this causes a severe overfitting problem to fit the category-specific distribution. Using the original pretrained model can alleviate this. The feature dimension C is set to 64 and C1 is set to 64. For DINO v2 Base, we use the features from layers 5, 6, 7, 8, 9, 10. The feature dimension C is set to 256 and C1 is set to 256. To ensure fairness, the features from the different layers of backbones for SOFS and reimplementation methods are the same. Since the backbone cannot be replaced, SegGPT is verified by ViT-Large [9].\nIn the experiments on VISION V1 and Defect Spectrum, we train the models with 3 runs for PFENet, HDMNet, and SOFS, and then test each model with 5 runs, for a total of 15 runs. For SegGPT, we only test it with 15 runs. In the experiments of domain shifts"}, {"title": "A.3 Supplemental Details for Dataset", "content": "VISION V1. VISION V1 [1] released in CVPR 2023 Vision-based Industrial InspectiON workshop, is one of the largest and most diverse manufacturing datasets in terms of both image number and available annotations. VISION V1 consists of 14 objects sourced from Roboflow and [8], 10k high-quality defect segmentation annotations on 4k images, spanning 44 defect types. Area proportions and pixel areas (number of pixels) for different defects are shown in Fig. 6. 84% of defect types have area proportions less than 0.3%, and 38% of defect types have pixel areas less than 900. These characteristics bring great challenges to defect segmentation. In the few-shot semantic segmentation, we divide all objects into 3 folds, we ensure that the distributions of area proportions of defects in each fold are roughly the same. The test objects for each fold are shown in Table 7.\nDefect Spectrum. Defect Spectrum [53] relabels the data from MVTec AD [3], DAGM2007 [47], part of VISION V1 [1] and Cotton-Fabric [18], ensuring that every image is accompanied by precise and diverse category annotations. It is the dataset with the largest number of defect categories so far, including in total 97 defect types. Area proportions and pixel areas (number of pixels) for different defects are shown in Fig. 7. 63% of defect types have area proportions less than 2%. In the few-shot semantic segmentation, we divide all objects into 3 folds, we ensure that the distributions of area proportions of defects in each fold are roughly the same. The test objects for each fold are shown in Table 8. Compared with the defects in VISION V1, the defects in Defect Spectrum are larger. The objects in this dataset are more under experimental conditions, i.e., placing a single object in a noise-free background, and there is a certain gap between the defects that need to be detected in real manufacturing. Thus, we choose VISION V1 as the main experimental dataset.\nAeBAD. AeBAD [59] is used to detect defects in aero-engine blades. The defects of blades are under different domains, including different illuminations, backgrounds, and views. The defects contain 4 types, involving some minor defects. This dataset is used to verify the robustness of different models for different domains."}, {"title": "A.4 Supplemental Experiments", "content": "The results (mean IoU) of different defect types of every object on VISION V1 are shown in Table 9 and 10. SOFS achieves the best in most cases."}, {"title": "A.5 Supplemental Ablation Study", "content": "The outputs of the model with and without the abnormal prior map are shown in Fig. 8. We can find that APM can reduce the false positive regions of missing defects."}, {"title": "A.6 More Qualitative Results", "content": "We show the visualizations for different defects of a query image conditioned on different support images in Fig. 9 and Fig. 10. This shows the segmentation results of SOFS for the specified defect. We show the qualitative results of every defect type on VISION V1 in"}]}