{"title": "Synth-SONAR: Sonar Image Synthesis with Enhanced Diversity and Realism via Dual Diffusion Models and GPT Prompting", "authors": ["Purushothaman Natarajan", "Kamal Basha", "Athira Nambiar"], "abstract": "Sonar image synthesis is crucial for advancing applications in underwater exploration, marine biology, and defence. Traditional methods often rely on extensive and costly data collection using sonar sensors, jeopardizing data quality and diversity. To overcome these limitations, this study proposes a new sonar image synthesis framework, \"Synth-SONAR\u201d leveraging diffusion models and GPT prompting. The key novelties of Synth-SONAR are threefold: First, by integrating Generative AI-based style injection techniques along with publicly available real/ simulated data, thereby producing one of the largest sonar data corpus for sonar research. Second, a dual text-conditioning sonar diffusion model hierarchy synthesizes coarse and fine-grained sonar images with enhanced quality and diversity. Third, high-level (coarse) and low-level (detailed) text-based sonar generation methods leverage advanced semantic information available in visual language models (VLMs) and GPT-prompting. During inference, the method generates diverse and realistic sonar images from textual prompts, bridging the gap between textual descriptions and sonar image generation. This marks the application of GPT-prompting in sonar imagery for the first time to the best of our knowledge. Synth-SONAR achieves state-of-the-art results in producing high-quality synthetic sonar datasets, significantly enhancing their diversity and realism.", "sections": [{"title": "1. Introduction", "content": "Sound Navigation and Ranging (SONAR) technology is an essential component of underwater exploration and object detection, with extensive applications in areas such as anti-submarine warfare, mine detection, submarine navigation, and torpedo guidance. It serves both civilian and military purposes, playing a crucial role in ensuring safety and operational efficiency in challenging underwater environments. Sonar operates by emitting sound waves that travel through water, reflect off objects, and are analyzed upon return to determine the location, size, and shape of underwater objects [46].\nSonar images are complex, consisting of the target, target shadow, and reverberation background regions [25]. Further, the underwater environment adds challenges like turbulence, noise, and low resolution, making underwater image analysis practically difficult [1]. Publicly available sonar datasets [16, 39, 53] often face challenges like low resolution, poor feature representation, and limited object diversity. Their scarcity is worsened by the need for expert labeling, security issues, and data sensitivity. To overcome these limitations, simulation-based studies [37, 40, 21] have been explored in the literature. However, these approaches still face limitations, including time-consuming manual modeling, the complexity of integrating various tools, and insufficient diversity in generated sonar data.\nTo this end, machine learning (ML)/ deep learning (DL) techniques have been employed for efficient sonar image synthesis [17, 47, 20, 18] in the recent years. Building on the success of generative AI (GenAI) in various fields e.g. medical imaging [41, 29], autonomous vehicles [51] etc. similar approaches such as Generative Adversarial Networks (GANs) [10], style transfer [9] and Denoising Diffusion Probabilistic Models (DDPMs) [13] have been adopted in sonar image synthesis to tackle data scarcity and improve model accuracy. Nonetheless, significant challenges remain, including insufficient object diversity, poor fine-grained feature preservation, complexities such as target shadows and reverberation effects, and high computational demands for real-time sonar image synthesis. Additionally, there exists a semantic gap between domain experts and machine learning models in interpreting and explaining the sonar characteristics in a human-compliant way.\nIn this paper, we propose a novel sonar image synthesis framework i.e. Synth-SONAR to overcome the aforemen-"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Underwater Sonar image analysis", "content": "Earlier sonar image analysis used traditional Machine learning (ML) techniques such as the Markov random field (MRF) model with the scale causal multigrid (SCM) algorithm [26] and undecimated discrete wavelet transform (UDWT) combined with PCA and k-means clustering [6]. Advancements in deep learning (DL) introduced methods like FS-UTNet [44], a framework for underwater target detection using few-shot learning. Further, techniques such as RotNet, Denoising Autoencoders, and Jigsaw [33] facilitated learning representations for sonar image classification without large labeled datasets. EsonarNet [12], a lightweight vision transformer network, is designed for efficient segmentation. The Global Context External-Attention Network (GCEANet) provides zero-shot classification in [3]. YOLOv7 improves high-precision object detection by integrating Swin-Transformer and Convolutional Block Attention Module (CBAM) [45]. EfficientNet is used as a backbone for feature extraction in [2], which uses dual-channel attention mechanisms (SE and ECA) and a modified BiFPN for multi-scale feature fusion. Additionally, DSA-Net for underwater object detection [23] used a dual spatial attention network (DSAM), and Generalized Focal Loss (GFL) for optimized object detection. To enhance interpretability, LIME and SP-LIME have been employed [27] to make sonar image classification more transparent and understandable."}, {"title": "2.2. Synthetic Data Generation via Generative AI", "content": "Synthetic data generation via GenAI addresses data scarcity using generative AI techniques like diffusion models [5], GANs [11], and VAEs [19]. CAD-based methods include using Unreal Engine (UE) [40] to create sonar images with diverse seabed conditions and objects, and the S3Simulator dataset [37], which leverages advanced simulation techniques, Segment Anything Model (SAM), and tools like SelfCAD and Gazebo for 3D modeling. Augmentation techniques feature the Seg2Sonar network [15], which uses spatially adaptive denormalization (SPADE), Skip-Layer channel-wise Excitation (SLE), and weight adjustment (WA) modules. GAN-based methods include an enhanced CycleGAN [54] model that improves underwater image contrast through a depth-oriented attention mechanism, the CBL-sinGAN network [32] which combines sinGAN with Convolutional Block Attention Module (CBAM) for target image augmentation. Further, SIGAN [31] used a multi-scale GAN for super-resolution of sonar images. Diffusion-based approaches are exemplified by a method that uses diffusion models for synthetic image generation and augmentation in [48, 49] and an enhanced YOLOv7 model that integrates a denoising-diffusion model [45], Vision Transformer (ViT) for high-precision object detection in side-scan sonar images."}, {"title": "2.3. Vision-Language Models (VLMs) and Their Application in the Sonar Domain", "content": "Vision-Language Models (VLMs) represent a significant advancement in the field of artificial intelligence by bridging the gap between visual and textual data [52]. Multimodels like CLIP [34], DALL-E [35], BLIP [22], FLAVA [42], and GIT [30] represent significant advancements by integrating visual and textual data, enhancing image generation and understanding through multimodal learning. While VLMs have made strides in various domains, their application to sonar data is still in its nascent stage, with only very few works in the sonar domain e.g. VALE [28], which combines VLM techniques with sonar data for improved underwater environment analysis."}, {"title": "3. Methodology", "content": "In this section, we provide a detailed description of the proposed Synth-SONAR framework for generating sonar images. The overall architecture of the model is depicted in Fig. 1, which consists of three key phases. Phase 1 is the Data Acquisition Phase, as explained in Section 3.1. It entails the collection of real-world, CAD-simulated, and Gen-AI-generated images. Phases 2 and 3 utilize text-conditioned dual diffusion models and GPT-based prompting to synthesize both \u201ccoarse\u201d and \u201cfine\" grained sonar images. In particular, Phase-2 customizes pre-trained diffusion models for generating sonar images as described in Section 3.2. Whereas, Phase-3 fine-tunes and generalizes the diffusion models for generating \"fine\u201d-grained sonar images as described in Section 3.3."}, {"title": "3.1. Phase-1: Data Acquisition", "content": "Underwater sonar imagery is a critical domain, wherein the data collection and processing of such a large training dataset is both expensive and challenging. Some of the common ways are to leverage the publicly available datasets e.g. Seabed Objects KLSG dataset, SCTD (see Section 4.1.1) or the CAD-based simulated dataset such as S3 simulator data (see Section 4.1.2). Further, we advance the sonar image synthesis via Generative AI techniques such as style injection (see Section 3.1.1), as explained in the forthcoming section."}, {"title": "3.1.1 Style Injection on Generated Images", "content": "Style injection is a technique in computer vision's image-to-image tasks, that combines content and style features to generate a new image, where the objective is to transform the content image (in our case, it is generated by the prompts from GPT) by infusing it with the stylistic elements of another image while preserving the original structure [7]. In this work, style injection is utilized to add more diversity to the real sonar data and to increase the availability of sonar data. Refering to Fig. 2, we leverage the generative capability of a pre-trained large-scale model to generate content images and transfer sonar style information to the generated content images using a pre-trained stable diffusion model, thereby resolving the issue of the traditional data-collection process."}, {"title": "Generic Attention Mechanism:", "content": "We employ Latent Diffusion Model (LDM) [36] to perform image synthesis by operating within a low-dimensional latent space. This approach significantly reduces computational costs while maintaining a focus on the semantic content of the data. For a given image $x \\in \\mathbb{R}^{H\\times W\\times 3}$, the encoder $E$ encodes the image into a latent representation $z \\in \\mathbb{R}^{h\\times w\\times c}$, and the decoder reconstructs the image from this latent representation. The diffusion model is trained on the latent space $z$, where the task is to predict noise $e$ from the noised latent representation $z_t$ at a given time step $t$. The corresponding training objective is:\n$L_{LDM} = \\mathbb{E}_{z,\\epsilon,t} [||\\epsilon - \\epsilon_{\\theta}(z_t, t, y)||^2]$ (1)\nwhere $\\epsilon \\sim N(0,1)$ is the noise, $t$ is uniformly sampled from $\\{1, . . ., T\\}$, $y$ is the conditioning variable (which could be a style reference or text prompt), and $\\epsilon_{\\theta}$ is a neural network predicting the noise added to $z$."}, {"title": "Style Injection in Image-to-Image Tasks:", "content": "In Latent Diffusion Models (LDM), the attention mechanism is fundamental to both style injection in image-to-image tasks and text-to-image generation. The attention mechanism can be generically expressed as:\n$Q = W_Q(Q), K = W_K(\\psi), V = W_V(\\psi)$ (2)\n$\\phi_{out} = Attn(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}})V$ (3)\nwhere, query $Q$, key $K$, and value $V$ are the result of learned linear transformations via the projection layers $W_Q(\u00b7)$, $W_K(\u00b7)$, and $W_V(\u00b7)$, respectively. The dimensionality of these projections is denoted by $d$, which is used in scaling the dot product between $Q$ and $K$ in the attention mechanism. Also, $\\phi$ represents the feature output from the residual block, while $\\psi$ is the conditioning variable, which can be the style features for image-to-image tasks or the text features for text-to-image tasks. For Style Injection, the conditioning variable $\\psi$ corresponds to the style features extracted from a reference image. The goal of style injection is to blend the style features with the content features of the original image, transferring the style while maintaining the content's structure. The content features are represented as:\n$Q_c = W_Q(\\phi)$ (4)\nwhere $\\phi$ denotes the content features extracted from the image. The style features are extracted from the style image and projected into key and value pairs:\n$K_s = W_K(\\phi_s), V_s = W_V(\\phi_s)$ (5)\nwhere $\\phi_s$ represents the style features from the reference image. The attention mechanism used for injecting the style features into the content is given by:\n$Q^{cs} = \\gamma \\times Q_c + (1 - \\gamma) \\times Q$ (6)\n$\\phi_{out}^{cs} = Attn(Q^{cs}, K_s, V_s)$ (7)\nwhere $\\gamma$ is a blending ratio that controls the amount of style injected. For sonar-specific style injection, referring to Fig. 2, the latent noise representations of the content and style images are denoted by $z^o$ and $z^s$, respectively. These representations are obtained through the DDIM inversion process, which reconstructs both the content and style images into Gaussian noise at time step $t = T$. During the DDIM inversion process, we collect the query features $Q_c$ from the content and the key-value pairs $K^s$, $V^s$ from the style at each time step $t$. Once the inversion is completed, we initialize the stylized latent noise $z^{cs}$ by copying the content latent noise $z^o$. To inject the style into the content, we blend the content and stylized queries using a blending ratio $\\gamma$ from equation 6. The blended query $Q^{cs}$ is then passed through the attention mechanism along with the style's key and value features. By adjusting $\\gamma$, we control the degree of style transfer, where a higher $\\gamma$ retains more of the original content features, and a lower $\\gamma$ strengthens the influence of the style features. This approach allows for precise control over the stylistic outcome, ensuring a smooth transition between content preservation and style injection."}, {"title": "3.2. Phase-2: Train and Generate sonar Images via DDPM and GPT-prompting", "content": "Our objective is to generate a sequence of sonar images that correspond to the given text conditioning. In the case of text-to-image generation, referring to the equation 2, the conditioning variable $\\psi$ corresponds to the text features. The cross-attention mechanism aligns the textual description with the visual features to generate an image that matches the text prompt.\n$Q_2$, $K_2$, and $V_2$ are analogous to the query, key, and value in the diffusion model explained earlier but are specific to Phase 2. In Phase 2, these components represent the following: $Q_2 = W_Q(\\phi)$ represents the image features, $K_2 = W_K(Text)$, and $V_2 = W_V(Text)$ represent the text features. The attention mechanism for text-to-image generation is given by:\n$\\phi_{out} = Attn(Q_2, K_2, V_2) = softmax(\\frac{Q_2K_2^T}{\\sqrt{d}})V_2$ (8)\nwhere $\\phi_{out}$ represents the output of the cross-attention layer, combining the image features with the text features. This interaction between the image and text features enables the generation of images conditioned on the provided text prompt."}, {"title": "3.3. Phase-3: Tune & Generalize DDPM for Fine-Grained sonar Image Generation", "content": "To train a diffusion model from scratch, enormous computation and a vast amount of training data are required. To mitigate the challenges of data scarcity and computation, we adopted LoRA [14] for fine-tuning. LoRA (Low-Rank Adaptation) applies to text-to-image tasks by efficiently fine-tuning models. Mathematically, LoRA updates the model's weight matrices by adding a low-rank decomposition $AW = A \u00b7 B$, where $A \\in \\mathbb{R}^{m\\times r}$ and $B \\in \\mathbb{R}^{r\\times n}$ are low-rank matrices, where $r$ is the rank.\nLORA updates the query, key, and value matrices in the following manner:\n$Q'_2 = Q_2 + \\alpha \u00b7 \\Delta W_{Q_2}$ (9)\n$K'_2 = K_2 + \\alpha \u00b7 \\Delta W_{K_2}$ (10)\n$V'_2 = V_2 + \\alpha \u00b7 \\Delta W_{V_2}$ (11)\nwhere, $Q'_2$, $K'_2$, $V'_2$ represent the LoRA-enhanced queries, keys, and values, where the original $Q_2$, $K_2$, and $V_2$ are adjusted by adding the LoRA updates $\\Delta W_{Q_2}$, $\\Delta W_{K_2}$, $\\Delta W_{V_2}$, scaled by a factor $\\alpha$. The enhanced attention mechanism with LORA is given by:\n$\\phi_{out} = Attn(Q'_2, K'_2, V'_2) = softmax(\\frac{Q'_2K'^T_2}{\\sqrt{d}})V'_2$ (12)\nThe attention mechanism, $Attn(Q'_2, K'_2, V'_2)$, now uses the LoRA-enhanced queries, keys, and values to generate the final output $\\phi_{out}$ in text-to-image tasks.\nThe fine-tuned model from Phase 2 can generate images only when the user provides a custom object tag given during training. If the user fails to provide this tag, the model generates regular images instead of SONAR-specific images. To mitigate this limitation and generalize the model for generating fine-grained sonar images, we further train the model using the existing corpus of sonar images.\nIn this phase, we generate images from the Phase 2 model using a series of prompts obtained from GPT. These generated images, combined with domain-specific language instructions, are processed through a Visual Language Model (VLM) to obtain low-level descriptions. These low-level descriptions are then fed back into GPT to generate high-level descriptions. The images and their corresponding low-level + high-level descriptions are subsequently used to fine-tune the stable diffusion model. The fine-tuning process involves optimizing the loss function for the diffusion model, where the loss is conditioned on both the image data and the text prompt:\n$L_{fine-tune} = \\mathbb{E}_{x,t,\\epsilon} [||\\epsilon - \\epsilon_{\\theta}(x_t, t | prompt)||^2]$ (13)\nHere, $x_t$ represents the generated images, $\\epsilon_{\\theta}$ is the noise predictor, and the fine-tuning process incorporates both the image data and sonar domain-specific textual instructions. This enhances the model's ability to generate fine-grained sonar images based on more generalized prompts."}, {"title": "4. Experimental Setup", "content": ""}, {"title": "4.1. Dataset", "content": ""}, {"title": "4.1.1 Publicly Available Sonar Images", "content": "In this work, publicly available datasets, specifically the Seabed Objects KLSG [16] and the sonar Common Target Detection Dataset (SCTD) [53], are utilized. The Seabed Objects KLSG dataset [16] includes 1,190 side-scan sonar images, featuring 385 shipwrecks, 36 drowning victims, 62 planes, 129 mines, and 578 seafloor images. Collected over ten years with the help of commercial sonar suppliers like Lcocean, Klein Martin, and EdgeTech, a subset of 1,171 images (excluding mines) is publicly available for academic research. The sonar Common Target Detection Dataset (SCTD) 1.0 [53], developed by multiple universities, contains 596 images across 3 classes."}, {"title": "4.1.2 S3 Simulator Images", "content": "The S3Simulator [37] dataset is a novel benchmark of simulated side-scan sonar images designed to overcome challenges in acquiring high-quality sonar data. Using advanced simulation techniques, the dataset accurately replicates underwater conditions and produces diverse synthetic sonar images. Tools like the Segment Anything Model (SAM) and Gazebo are utilized for optimal object segmentation and visualization, enhancing the quality of data for AI model training in underwater object classification."}, {"title": "4.2. Evaluation Metrics", "content": "We use Frechet Inception Distance (FID), Structural Similarity Index (SSIM), Peak Signal-to-Noise Ratio"}, {"title": "5. Experimental Results", "content": ""}, {"title": "5.1. Phase-1: Style-Injection Results", "content": ""}, {"title": "5.1.1 Style-Injection Quantitative Results", "content": "The objective of style injection is to improve the quality and diversity of generated sonar images through style injection, ensuring that the generated images better reflect the unique characteristics of the dataset. To achieve this, style images are selected not randomly but via K-Means clustering on the real sonar dataset. One to three images from each cluster are chosen for style injection, which enhances the relevance and consistency of the stylized outputs."}, {"title": "5.1.2 Style-Injection Qualitative Results", "content": "The images generated using the style injection are depicted in Fig. 5. The content image is generated from the prompt developed using GPT, the actual image is from a real sonar dataset, and the output is the stylized image from the proposed framework with the value of $\\gamma$ set to 0.5. The stylized output images retain the core structural elements necessary for accurate sonar image interpretation, such as the clear depiction of object boundaries and the representation of noise patterns that are characteristic of underwater acoustic imaging. This ensures that the model does not over-stylize the content, but instead enhances the aesthetic appeal while preserving the scientific accuracy of the generated outputs. Moreover, the generated images show clearer details and better feature separation, indicating that style injection with an attention mechanism helps the diffusion model distinguish foreground objects from background noise. This makes the images more similar to real sonar data while adding subtle artistic variations that enhance diversity."}, {"title": "5.1.3 Sonar Image Classification Model", "content": "To support the quantitative measure of sonar image synthesis, a classification task is carried out using the generated synthetic data. In particular, we developed classification models leveraging transfer learning on backbone models such as VGG16, ResNet50, DenseNet121, MobileNetV2, Xception, and InceptionResNetV2 With the real, synthetic, and real + synthetic dataset and the performance of the developed image classification model is assessed with real sonar dataset. Refer to Table 3 for the test accuracies of various models on real dataset. The model developed with fully synthetic dataset gave a maximum accuracy of 79%, using the MobileNetV2 as base model. When using only real data, the DenseNet121 model reached the highest accuracy of 96%, and when using the real + synthetic dataset, DenseNet121 again achieved the best performance with 97%. This demonstrates that our approach can generate high-quality synthetic sonar data, and the inclusion of synthetic data slightly improves model performance on real datasets."}, {"title": "5.2. Phase-2 & 3: Fine-Tuning Results", "content": "Training or fine-tuning stable diffusion models typically demands extensive computational resources and large amounts of annotated data. However, due to computational constraints and limited access to experts for annotating generated images, we evaluated our framework as a pilot study, utilizing a dataset of 30 images per label. In total, the dataset consists of six different objects with 18 object variations across six seafloor environments used for model training."}, {"title": "Generic Attention Mechanism:", "content": "We employ Latent Diffusion Model (LDM) [36] to perform image synthesis by operating within a low-dimensional latent space. This approach significantly reduces computational costs while maintaining a focus on the semantic content of the data. For a given image $x \\in \\mathbb{R}^{H\\times W\\times 3}$, the encoder $E$ encodes the image into a latent representation $z \\in \\mathbb{R}^{h\\times w\\times c}$, and the decoder reconstructs the image from this latent representation. The diffusion model is trained on the latent space $z$, where the task is to predict noise $e$ from the noised latent representation $z_t$ at a given time step $t$. The corresponding training objective is:\n$L_{LDM} = \\mathbb{E}_{z,\\epsilon,t} [||\\epsilon - \\epsilon_{\\theta}(z_t, t, y)||^2]$ (1)\nwhere $\\epsilon \\sim N(0,1)$ is the noise, $t$ is uniformly sampled from $\\{1, . . ., T\\}$, $y$ is the conditioning variable (which could be a style reference or text prompt), and $\\epsilon_{\\theta}$ is a neural network predicting the noise added to $z$."}, {"title": "Style Injection in Image-to-Image Tasks:", "content": "In Latent Diffusion Models (LDM), the attention mechanism is fundamental to both style injection in image-to-image tasks and text-to-image generation. The attention mechanism can be generically expressed as:\n$Q = W_Q(Q), K = W_K(\\psi), V = W_V(\\psi)$ (2)\n$\\phi_{out} = Attn(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}})V$ (3)\nwhere, query $Q$, key $K$, and value $V$ are the result of learned linear transformations via the projection layers $W_Q(\u00b7)$, $W_K(\u00b7)$, and $W_V(\u00b7)$, respectively. The dimensionality of these projections is denoted by $d$, which is used in scaling the dot product between $Q$ and $K$ in the attention mechanism. Also, $\\phi$ represents the feature output from the residual block, while $\\psi$ is the conditioning variable, which can be the style features for image-to-image tasks or the text features for text-to-image tasks. For Style Injection, the conditioning variable $\\psi$ corresponds to the style features extracted from a reference image. The goal of style injection is to blend the style features with the content features of the original image, transferring the style while maintaining the content's structure. The content features are represented as:\n$Q_c = W_Q(\\phi)$ (4)\nwhere $\\phi$ denotes the content features extracted from the image. The style features are extracted from the style image and projected into key and value pairs:\n$K_s = W_K(\\phi_s), V_s = W_V(\\phi_s)$ (5)\nwhere $\\phi_s$ represents the style features from the reference image. The attention mechanism used for injecting the style features into the content is given by:\n$Q^{cs} = \\gamma \\times Q_c + (1 - \\gamma) \\times Q$ (6)\n$\\phi_{out}^{cs} = Attn(Q^{cs}, K_s, V_s)$ (7)\nwhere $\\gamma$ is a blending ratio that controls the amount of style injected. For sonar-specific style injection, referring to Fig. 2, the latent noise representations of the content and style images are denoted by $z^o$ and $z^s$, respectively. These representations are obtained through the DDIM inversion process, which reconstructs both the content and style images into Gaussian noise at time step $t = T$. During the DDIM inversion process, we collect the query features $Q_c$ from the content and the key-value pairs $K^s$, $V^s$ from the style at each time step $t$. Once the inversion is completed, we initialize the stylized latent noise $z^{cs}$ by copying the content latent noise $z^o$. To inject the style into the content, we blend the content and stylized queries using a blending ratio $\\gamma$ from equation 6. The blended query $Q^{cs}$ is then passed through the attention mechanism along with the style's key and value features. By adjusting $\\gamma$, we control the degree of style transfer, where a higher $\\gamma$ retains more of the original content features, and a lower $\\gamma$ strengthens the influence of the style features. This approach allows for precise control over the stylistic outcome, ensuring a smooth transition between content preservation and style injection."}, {"title": "3.2. Phase-2: Train and Generate sonar Images via DDPM and GPT-prompting", "content": "Our objective is to generate a sequence of sonar images that correspond to the given text conditioning. In the case of text-to-image generation, referring to the equation 2, the conditioning variable $\\psi$ corresponds to the text features. The cross-attention mechanism aligns the textual description with the visual features to generate an image that matches the text prompt.\n$Q_2$, $K_2$, and $V_2$ are analogous to the query, key, and value in the diffusion model explained earlier but are specific to Phase 2. In Phase 2, these components represent the following: $Q_2 = W_Q(\\phi)$ represents the image features, $K_2 = W_K(Text)$, and $V_2 = W_V(Text)$ represent the text features. The attention mechanism for text-to-image generation is given by:\n$\\phi_{out} = Attn(Q_2, K_2, V_2) = softmax(\\frac{Q_2K_2^T}{\\sqrt{d}})V_2$ (8)\nwhere $\\phi_{out}$ represents the output of the cross-attention layer, combining the image features with the text features. This interaction between the image and text features enables the generation of images conditioned on the provided text prompt."}, {"title": "3.3. Phase-3: Tune & Generalize DDPM for Fine-Grained sonar Image Generation", "content": "To train a diffusion model from scratch, enormous computation and a vast amount of training data are required. To mitigate the challenges of data scarcity and computation, we adopted LoRA [14] for fine-tuning. LoRA (Low-Rank Adaptation) applies to text-to-image tasks by efficiently fine-tuning models. Mathematically, LoRA updates the model's weight matrices by adding a low-rank decomposition $AW = A \u00b7 B$, where $A \\in \\mathbb{R}^{m\\times r}$ and $B \\in \\mathbb{R}^{r\\times n}$ are low-rank matrices, where $r$ is the rank.\nLORA updates the query, key, and value matrices in the following manner:\n$Q'_2 = Q_2 + \\alpha \u00b7 \\Delta W_{Q_2}$ (9)\n$K'_2 = K_2 + \\alpha \u00b7 \\Delta W_{K_2}$ (10)\n$V'_2 = V_2 + \\alpha \u00b7 \\Delta W_{V_2}$ (11)\nwhere, $Q'_2$, $K'_2$, $V'_2$ represent the LoRA-enhanced queries, keys, and values, where the original $Q_2$, $K_2$, and $V_2$ are adjusted by adding the LoRA updates $\\Delta W_{Q_2}$, $\\Delta W_{K_2}$, $\\Delta W_{V_2}$, scaled by a factor $\\alpha$. The enhanced attention mechanism with LORA is given by:\n$\\phi_{out} = Attn(Q'_2, K'_2, V'_2) = softmax(\\frac{Q'_2K'^T_2}{\\sqrt{d}})V'_2$ (12)\nThe attention mechanism, $Attn(Q'_2, K'_2, V'_2)$, now uses the LoRA-enhanced queries, keys, and values to generate the final output $\\phi_{out}$ in text-to-image tasks.\nThe fine-tuned model from Phase 2 can generate images only when the user provides a custom object tag given during training. If the user fails to provide this tag, the model generates regular images instead of SONAR-specific images. To mitigate this limitation and generalize the model for generating fine-grained sonar images, we further train the model using the existing corpus of sonar images.\nIn this phase, we generate images from the Phase 2 model using a series of prompts obtained from GPT. These generated images, combined with domain-specific language instructions, are processed through a Visual Language Model (VLM) to obtain low-level descriptions. These low-level descriptions are then fed back into GPT to generate high-level descriptions. The images and their corresponding low-level + high-level descriptions are subsequently used to fine-tune the stable diffusion model. The fine-tuning process involves optimizing the loss function for the diffusion model, where the loss is conditioned on both the image data and the text prompt:\n$L_{fine-tune} = \\mathbb{E}_{x,t,\\epsilon} [||\\epsilon - \\epsilon_{\\theta}(x_t, t | prompt)||^2]$ (13)\nHere, $x_t$ represents the generated images, $\\epsilon_{\\theta}$ is the noise predictor, and the fine-tuning process incorporates both the image data and sonar domain-specific textual instructions. This enhances the model's ability to generate fine-grained sonar images based on more generalized prompts."}, {"title": "5.3. Ablation study", "content": "We conducted an in-depth analysis to regulate the results generated by the trained model. The style-injection ablation study is extensively discussed in Section 5.3.1, while the fine-tuning ablation study is detailed in Section 5.3.2."}, {"title": "5.3.1 Phase-1: Style-Injection (Image-to-Image)", "content": "The blending ratio of content and style is controlled by the parameter $\\gamma$. We conducted a comparative study to examine how it impacts the quality of the image produced through style injection, as shown in Fig. 8."}, {"title": "5.3.2 Phase-2 & 3: Fine-Tuning (Text-to-Image)", "content": "The number of training steps significantly impacts the quality of the generated images. To verify the same, a qualitative study on the outputs of the trained model (see Fig. 7) is carried out. It is observed that at lower step counts, the images tend to be coarse, with artifacts or incomplete object representations. Increasing the number of steps leads to sharper images with clearer object boundaries and better textures, allowing for more accurate and realistic depictions of underwater scenes."}, {"title": "5.4. State-of-the-art Comparison", "content": "To the best of our knowledge, our work is the first work in text-conditioned image generation for underwatersonarimagery, and it lacks benchmark datasets, thereby hindering our ability to fully assess the model's performance. However, to showcase the efficacy of the proposed architecture, we compared our work with existing similar image-to-image synthesis work and tabulated the results in the Table. 5. Our model outperforms the state-of-the-art models [50], [45],[48] in terms of image quality, with high SSIM & PSNR values and low FID value."}, {"title": "6. Conclusion", "content": "In this work, we present \u201cSynth-SONAR\u201d, a novel"}]}