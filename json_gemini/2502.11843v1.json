{"title": "Can LLM Agents Maintain a Persona in Discourse?", "authors": ["Pranav Bhandari", "Nicolas Fay", "Michael Wise", "Amitava Datta", "Stephanie Meek", "Usman Naseem", "Mehwish Nasim"], "abstract": "Large Language Models (LLMs) are widely used as conversational agents exploiting their capabilities in various sectors such as education, law, medicine, and more. However, LLMs are often subjected to context-shifting behaviour, resulting in a lack of consistent and interpretable personality-aligned interactions. Adherence to psychological traits lacks comprehensive analysis, especially in the case of dyadic (pairwise) conversations. We examine this challenge from two viewpoints, initially using two conversation agents to generate a discourse on a certain topic with an assigned personality from the OCEAN framework (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This is followed by using multiple judge agents to infer the original traits assigned to explore prediction consistency, inter-model agreement, and alignment with the assigned personality. Our findings indicate that while LLMs can be guided toward personality-driven dialogue, their ability to maintain personality traits varies significantly depending on the combination of models and discourse settings. These inconsistencies emphasise the challenges in achieving stable and interpretable personality-aligned interactions in LLMs.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have evolved from task solvers and general-purpose chatbots to sophisticated conversational agents capable of embodying distinct personas. This shift towards personalised agents, driven by LLMs' capacity for perception, planning, generalisation, and learning (Xi et al., 2025), has enabled context-sensitive discourse and opened up new possibilities across diverse domains. Persona, defined as conditioning AI models to adopt specific roles and characteristics (Li et al., 2024), is a key element in this evolution. Personalised agents show promise in areas such as emotional support, training, and social skills development (Dan et al., 2024), and are increasingly explored for applications ranging from social science research (Zhu et al., 2025) to mimicking human behaviour (Jiang et al., 2023). While various personalisation approaches exist, incorporating personas has proven particularly effective in generating contextually appropriate responses and enhancing overall performance (Tseng et al., 2024; Dan et al., 2024). \nUnderstanding how LLMs express and sustain personality traits in dynamic conversations is crucial, despite their tendency to generate neutral, balanced content. Existing work has explored personality in text using tools like the Big Five Inventory (BFI) (John et al., 1991) to infer and analyse personality profiles (Bhandari et al., 2025). However, two key gaps remain. First, it is unclear how consistently LLMs portray assigned personality traits during extended interactions, particularly in pairwise (dyadic) conversations where context shifts and adaptation are necessary. Second, robust methods are needed to evaluate the alignment between the expressed traits in the generated text and the intended psychological profile. We present an example in Figure 1. \nWhile previous studies (Jiang et al., 2023; Kim et al., 2025) have made progress in demonstrating that LLMs can reflect assigned personality traits (often through personality questionnaires), a critical gap remains in understanding how consistently these traits are maintained in generated content, particularly within dynamic conversational settings. Although assigning personality traits to conversational agents often yields positive results in controlled settings, this does not guarantee that the generated content effectively expresses those traits, nor does it quantify the degree of expression. Our work addresses this gap by focusing on the generation and evaluation of trait-adherent discourse, specifically within dyadic conversations involving frequent context shifts. We investigate whether and how LLMs maintain assigned personalities during these dynamic interactions, beyond simply demonstrating the potential for personality reflection to assessing its actual manifestation in conversation. \nThis work aims to investigate how effectively LLMs express assigned personality traits in generated dialogue. Specifically, we explore whether and how LLMs maintain Big Five Personality traits, which are represented as the OCEAN framework (Husain et al., 2025) (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism), during dyadic conversations. We employ a novel agent-based evaluation framework where two LLM agents, each assigned a distinct OCEAN personality profile, engage in a conversation on a given topic. Subsequently, independent LLM agents (judges) assess the generated dialogue to determine the consistency between expressed and assigned traits. This approach allows us to analyse not only whether LLMs reflect personality, but also the peculiarities in trait expression and the challenges of maintaining personality consistency within dynamic conversational contexts. \nThis work seeks to address the following research questions:\nRQ1: How accurately LLMs as a judge agent predict assigned traits from discourse?\nRQ2: How consistently do LLM agents express assigned personality traits in conversations?\nRQ3: Are all OCEAN traits equally prominent in generated conversations?"}, {"title": "Related Work", "content": "Personality traits matter since LLMs mimic humans, but their structured psychological evaluation remains an unexplored gap that needs further research (Zhu et al., 2025). The recent literature has looked at designing (Klinkert et al., 2024), improving(Huang et al., 2024), investigating(Frisch and Giulianelli, 2024; Zhu et al., 2025), customizing (Han et al., 2024; Dan et al., 2024; Zhang et al., 2018) and exploring (Zhu et al., 2025; Han et al., 2024) personality traits. The scope of our work lies both in generating and extracting personality traits embedded within discourse.\nHan et al. (2024) contribute towards the generation of synthetic dialogues through LLMs. A five-step generation process is used where personality is induced through personality character. Special consideration on prompts is made to infer Pre-trained Language Models (PLM) in generating dialogues. This is because dialogue generation is a challenging task, especially with many constraints and maintaining personality traits. Unlike traditional methods of curating datasets by humans, the authors leverage the capability of PLM to generate synthetic data that is easily scalable. The use of these synthetic datasets significantly improved the ability of LLMs to generate content that is more tailored towards personality traits. While the research is broad, its dataset is limited to Korean and focuses on a single personality trait, which may hinder balanced trait prediction.\nWhile designing and customising the personality traits for LLMs is an intriguing field of study, the focus of this work lies in inducing and investigating the personality traits through discourse generation (Yeo et al., 2025). Jiang et al. (2023) investigate the ability of LLMs to express personality traits through essay generation. Using both humans and LLMs as evaluators they explore the personality traits in the generated content. Evaluation through linguistic patterns (LIWC analysis) and human annotation is carried out for GPT models. They show a positive correlation between the generated content and personality traits. However, several gaps are identified such as focusing on closed models, limited data generation and conversations focused on single-ended generation(essays) which does not address the personality expression in scenarios consisting shift of context. Furthermore, the authors suggest models other than OpenAI's GPT models do not follow the instructions well, which results in discarding the content generated by these models for further evaluation. We aim to address this problem through systematic and structural prompting techniques which increases the scope of the analysis."}, {"title": "Methodology", "content": "We present the methodology of this work in Figure 2. In an agent-based setting the methodology is operationalised in 4 phases: Personifying agents, Generating discourse, Extracting personality within discourse, and Evaluation. A detailed explanation of the modular approach is presented in subsequent sections. In summary, the psychological personas are assigned to two agents and asked to converse on a topic. The discourse is evaluated using independent agents - judge agents through several evaluation metrics.\nWe adopted an iterative approach to refine the methodology. Various problems were encountered while producing the discourse between the models, starting with synchronization issues, over-generalisation, repeating the prompts, and explicitly mentioning the personality that the LLMs have assumed. Furthermore, in a dyadic conversation between two agents, the subsequent dialogues are highly dependent on the previous conversation, hence one unjustified/bad response can cause the whole conversation to deviate from its original objective. Hence, special consideration has been given to achieving complete and sensible conversations. To validate that LLMs are not generating the same dialogues as before, we perform a similarity check across all the dyadic conversations and validate them.\nWe selected GPT models from OpenAI(OpenAI, 2024) and LLaMA models from Meta(Patterson et al., 2022) due to their popularity and reach. As the landscape rapidly evolved, we expanded our scope to include DeepSeek\u00b9 to ensure broader coverage and comparison across architectures.\nSince the generation of essays on a particular topic has been explored in literature such as (Kim et al., 2025; Yeo et al., 2025), we wanted to explore the generation of discourses, particularly for two reasons 1) The complexity of the topic increases and maintaining a progressive discussion given the explicit persona is a difficult task. 2) It is also interesting to understand the consistency in the personality during a conversation.\nDataset: We have carefully selected 100 different topics that require, ethical, moral, social or political considerations 2 and 20 different combinations of random traits (more in Appendix)."}, {"title": "Prompt formation", "content": "There are two basic requirements to create the discourse between two agents. The first one is the assigned persona of the OCEAN model (the Big Five Inventory) (John et al., 1991) that is to be maintained at all times while producing an utterance and second is the consideration of the previous utterance in the dyadic conversation so that the current utterance reflects the understanding of the previous utterance and is not an independent reply. In addition, the context of the utterances must be lexically similar to the topic given.\nThe prompt formation is an essential part of our methodology. Since the discourse is analysed by other agents and we draw the results based on the discourse, it must be structured robustly to ensure reliability and objective evaluation.\nPrompting for LLMs is carried out through specific prompting methods where agents are assigned roles to convey requirements and expected outcomes. Usually, the system and user roles are passed as arguments (Yeo et al., 2025) in which the system role is responsible for defining the behaviour and limiting the scope of response and the user role is used for defining the input. Despite strict adherence to these techniques, agents may still be overwhelmed by excessive constraints.\nSystem Prompt: The system prompt in our case contains the rules for debates carried out on a specific topic. Structured prompts enhance clarity for agents, improve effectiveness, and help users create inclusive prompts despite multiple constraints. Although the formatting of the prompts varies according to the model specifications, they contain the following information.\n\u2022 The traits are assigned in two forms of extremities: High or Low.\n\u2022 You are a participant in a discourse in which the topic is topic and presented with the following traits traits.\n\u2022 Assigned personality traits must be maintained throughout the conversation but not explicitly mentioned in the utterances.\n\u2022 Each utterance must be under 50 words and the previous utterance needs to be addressed.\nUser Prompt: User prompt in this case contributes to an important role in shaping the conversation because the previous discussions are passed through the user prompt to generate the next utterance. During the experiments, we noted that GPT models followed instructions effectively in a zero-shot setting with minimal guidance, while models like Llama and DeepSeek required more detailed explanations and constraints. This suggests that GPT models are more adaptable to imperfect prompts compared to other state-of-the-art models."}, {"title": "Validation", "content": "Validation involves both human assessment and agent-based evaluation. Discourse quality and coherence are checked via: 1) A human observation of 10-15 discourses is made randomly for each of the categories for the length, content, coherence and quality of the discourse. 2) For each course of discourse, we analyse the similarity scores between all the utterances to make sure that the same arguments are not repeated. LLMs are used in the literature for personality trait extraction (Zhu et al., 2025; Sun et al., 2024). We employ PLMs to analyse dialogues to infer personality traits and then use pre-assigned personality traits as ground-truth data for evaluation in Section 4."}, {"title": "Evaluation", "content": "Once the discourses are generated, each of the discourses is evaluated by Judge agents. The judge agents return data in a json format with their prediction of each speaker's personality traits in the text. To reduce the bias of human vs agent-generated content, we provide the utterances to the Judge agents specifying that they are 'human-generated'. The following evaluations are made:"}, {"title": "Personality prediction consistency Across Models:", "content": "Personality prediction consistency Across Models: With access to both the assigned traits (Section 3.1) and inferred traits (Section 3.2) using different judge agents, we begin by calculating the accuracy of the models' predictions (a.k.a. inferred traits). We calculate the accuracy of prediction in two different ways: the accuracy of predicted High for each trait as High Trait Classification Accuracy(HTA) and finally accuracy of predicted Low for each trait as Low Trait Classification Accuracy(LTA). Recall, that we assign a high or a low value for each OCEAN trait while assigning personalities in Section 3.1. We create a confusion matrix for this labelling all the True and False predictions of High and Low values to compute the HTA and LTA values.\nHTA measures how well the models classify traits assigned as High originally. This is computed by creating a confusion matrix for correct and incorrect classifications. HTA is calculated by dividing the total correctly classified High by the total number of High cases.\nLTA on the other hand measures how well the models classify traits assigned as Low originally. It is calculated by dividing the total correctly classified Low by the total number of Low cases. An important aspect of this study is understanding potential bias in classification into High or Low traits. While overall accuracy may be high, we focus on whether both categories are proportionately represented."}, {"title": "Inter-rater reliability among the models:", "content": "Inter-rater reliability is the measure to understand the agreement between the models. Kappa statistics($K$) is a common method to assess the consistency of ratings among raters (Judge LLMs) (P\u00e9rez et al., 2020).\nWe computed Fleiss' Kappa by first gathering personality trait predictions from five different judge models. Each model analysed debates across multiple topics and rated Big Five personality traits for two participants (P1 & P2). We structured the data so that all model ratings for the same Topic-Trait pair were aligned, ensuring consistency in comparison. After validation, we reformatted the dataset into a matrix where each row represented a topic-trait combination. The matrix contained counts of how many models classified the trait as High or Low for both P1 and P2 separately. We calculated the inter-model agreement for each trait using Python's 'statsmodels'\u00b3 package, specifically the fleiss_kappa function to extract the consistency of various judge models across all topics.\nWhile the first measure explores the accuracy with which the models correctly identify High and Low, respective to the ground values, this method explores the agreement between the models for a particular trait at a time, irrespective of the base values."}, {"title": "Discourse alignment with Assigned Personality Traits:", "content": "The discourse alignment with assigned personality traits is an important part of this analysis as it depicts if the personality traits are reflected in the contents generated by the agents. We analyse if the discourses linguistically align with the assigned personality traits. Various factors like language, tone and argument structures contribute towards"}, {"title": "Results", "content": "The experiments are carried out in two phases: 1). Agents are personified and discourse is generated on a given topic; 2). Personality traits are extracted from the discourses and evaluation is performed.\nThis evaluation is critical for determining the controllability of personality traits in language models and validating their alignment with intended psychological characteristics.\nFour models are involved in the creation of discourse in different combinations (GPT-40 vs. GPT-40-mini, GPT-40 vs. Llama-3.3-70B-Instruct, GPT-40 vs. Deepseek-llm-67B-Chat). All of these models have been set up at higher temperatures (>0.8) to allow creativity during discourse generation. Limited by resources(NVIDIA A6000 GPU), the larger models such as Llama-3.3-70B-Instruct and Deepseek-llm-67B-Chat, were quantized to generate discourse. The max_tokens were limited to 150 to prevent the model from generating verbose utterances.\nFor the evaluations of the generated discourse, we used five different models: GPT-40, GPT-4o-mini, Llama-3.3-70B-Instruct, Qwen-2.5-14B-Instruct-1M, and Deepseek-llm-67B-Chat \u2014 the judge agents. The idea is to include a variety of models(both small and large) and understand the consistency in the results.\nUtterances from LLaMA-3.3-70B-Instruct and DeepSeek-LLM-67B-Chat required filtration due to prompt repetition and inline tags whereas GPT models adhered to instructions effectively."}, {"title": "Personality Prediction Consistency across models", "content": "Figures in Table 1 represent the result of personality prediction for each of the Judge models. We now describe various interesting patterns observed with different models as Judges.\nAnalysis across judge models: We note that for Agreeableness, Openness, and Conscientiousness, GPT-40, GPT-40-mini, and LLaMA-3.3-70B-Instruct achieve comparable and high-quality results for both Person 1 and Person 2, exceeding 90% accuracy. However, for the same categories of traits, Qwen-2.5-14B-1M produces significantly low numbers for Openness and Conscientiousness while the scores for Agreeableness are comparable. From the perspective of the size of the models, larger models (GPT-40 and Llama-3.3-70B-Instruct) have higher accuracy in predicting the High classification compared to smaller models (Qwen-2.5-14B-Instruct-1M). However, the accuracy of predicting the Low trait was significantly high for Openness and Conscientious with Qwen-2.5 as a Judge as compared to other models for both persons 1 and 2. Overall, for Agreeableness, Openness and Conscientiousness the ability of the (GPT-40, GPT-40-mini and Llama-3.3)models to predict their High values is significantly higher than predicting the Low values.\nJudgments for Neuroticism and Extraversion show a distinct pattern, with High values predicted less frequently across all discourses and participants. When observed with scrutiny, detecting High Neuroticism is particularly challenging, likely due to judge models failing to recognise it in text or conversational models avoiding highly neurotic responses. However, some divergent cases occur where GPT-40 detects neuroticism with 62% precision, significantly higher than in other models. Also, it is worth noticing that detecting High Neuroticism in discourse between the GPT-40 vs. Deepseek is more challenging than the other two combinations.\nWe used DeepSeek as a judge for pairwise conversation analysis. While LLaMA-3.3 and Qwen-2.5 required refinement, DeepSeek proved unreliable, with over 40% invalid responses, leading to its exclusion from Table 1.\nAnalysis Across Conversations: Compared to GPT-40 vs. GPT-40-mini and GPT-40 vs Llama-3.3, the accuracy of High trait prediction in Neuroticism and Extraversion was significantly lower for GPT-40 vs. Deepseek conversation for both participants 1 and 2. This suggests that while exploration of low Neuroticism and Extraversion is comparable to the other two conversations, the complexity increases when these domains are High in the GPT-40 vs. Deepseek conversations. While observing individual participants across all the conversations, the results tend to be constant among the judges meaning if GPT rates high Agreeableness to participant 1 in one conversation, other judge models are likely to present similar results.\nThis addresses RQ1 & RQ3. We observed a conditional capability of these agents as judges to accurately classify traits from the discourses. This is true within various traits and also for the High and Low classification of the traits. Also, this finding provides an impression of inconsistency and bias towards certain OCEAN traits more than others."}, {"title": "Inter Model Agreement", "content": "Table 2 presents the Fleiss' Kappa statistics, measuring inter-model agreement on personality trait judgments for Participants 1 and 2 across all dialogues.\nIn Discourse 1, Agreeableness showed moderate agreement (\u043a > 0.5) for both participants. Openness agreement was substantial for Participant 1 but moderate for Participant 2. Conscientiousness and Neuroticism exhibited fair to moderate agreement. Notably, Extraversion showed the lowest"}, {"title": "Discourse Alignment with assigned personality traits", "content": "Figure 3 presents the accuracy of personality trait depiction for Participants 1 and 2, measured using LIWC-22. GPT-40-mini achieved the highest accuracy for Agreeableness across all dialogues. However, GPT-40's Agreeableness accuracy decreased substantially (from 68% and 65% to 52%) when conversing with Deepseek than GPT-40-mini and Llama-3.3, suggesting a potential shift in personality expression depending on the interlocutor, similar to human behaviour (Atherton et al., 2022). \nOpenness was the trait least accurately represented in all dialogues, with a maximum accuracy of 51%. This suggests that expressing Openness is particularly challenging for these LLMs. Llama-3.3 exhibited the highest Conscientiousness, while GPT-40 showed the highest Extraversion. However, these differences were not statistically significant, and trait expression varied depending on the conversational partner. GPT-40's Neuroticism depiction was most accurate when interacting with Llama-3.3. This variability in traits and conversational settings directly addresses RQ3, confirming that all OCEAN traits are not equally prominent in generated conversations.\nWhen comparing pairwise dialogues, GPT-40 vs. GPT-40-mini and GPT-40 vs. Llama-3.3 showed similar performance. However, GPT-40 vs. Deepseek dialogues exhibited significantly different results. We observed that Deepseek struggled to consistently follow instructions from the prompts (even though the prompts were minimally adapted across models). Deepseek's generated text was also the most inconsistent in length compared to other models, which may have contributed to the observed differences."}, {"title": "Conclusion", "content": "This paper provides a comprehensive evaluation of trait adherence in LLM agents engaged in dyadic conversations. Our findings highlight the significant challenges in achieving consistent and interpretable personality-aligned interactions. While LLMs can be guided to exhibit certain personality traits, their ability to maintain these traits across dynamic conversations varies considerably. Future work should explore more sophisticated methods for instilling and evaluating personality, investigating the impact of dialogue context and developing metrics for assessing the nuances of personality expression in LLMs. Exploring fine-tuning strategies or reinforcement learning approaches for improving consistency would also be valuable."}, {"title": "Limitations", "content": "One of the key challenges in this study is the absence of a standardized benchmarking system that all evaluations adhere to, making direct comparisons across different approaches more difficult. While strict rules were enforced to structure the discourse, models did not always fully comply, occasionally deviating from expected dialogue patterns. Additionally, there is a risk of bias, as language models may incorporate their own implicit judgments into discussions, potentially influencing personality assessments. Another important consideration is the length of dyadic conversations, there is no widely accepted standard for how long a dialogue should be to ensure a reliable evaluation. This uncertainty raises questions about whether longer or shorter exchanges might yield different insights, adding a layer of complexity to the interpretation of results."}, {"title": "Ethical Considerations", "content": "We do not collect any personal information and views for the creation of the discourse dataset or refer to any kind of personal traits from any sources to judge the nature of conversations. All the discourses are created by LLM agents. Topics provided for discussion for the agents are debatable but do not involve or promote the thought of violence, hatred or extremism of any kind to anyone. \nWe use open and closed-source models that are available off the self and accessible to the general public. No changes in the model architecture have been made. Some hyperparameters have been adjusted to meet our expectations of the results, but they have been mentioned clearly in the paper. LLMs have the possibility of introducing bias in their results as per numerous studies. The dataset generated by the conversing agents has not been made public, but we do plan to publish it for further studies with careful ethical consideration and approvals. The results do present bias in predicting the BFI from the discourses but are solely limited to LLMs as judges. \nThe content of LLM agents is subject to change if they are altered, fine-tuned, and tempered in different ways, which is a potential risk."}, {"title": "Sample of Topics and Trait Combinations Used", "content": "Samples of topics used for debate:\n\"Is the concept of a universal language beneficial?\",\n\"Should the government regulate the pharmaceutical industry?\",\n\"Is the use of nuclear energy justified?\",\n\"Should the government provide free public transportation?\",\n\"Is the concept of a cashless society beneficial?\",\n\"Should the government regulate the gaming industry?\"\nTrait combinations samples to assign personas to Agents:\n{\"Agreeableness\": \"High\", \"Openness\":\n\"Low\", \"Conscientiousness\": \"High\",\n\"Extraversion\": \"Low\",\n\"Neuroticism\": \"High\"},\n{\"Agreeableness\": \"Low\", \"Openness\":\n\"High\", \"Conscientiousness\": \"Low\",\n\"Extraversion\": \"High\",\n\"Neuroticism\": \"Low\"},\n{\"Agreeableness\": \"High\",\n\"Openness\":\n\"High\", \"Conscientiousness\": \"Low\",\n\"Extraversion\": \"High\",\n\"Neuroticism\": \"High\"},\n{\"Agreeableness\": \"Low\", \"Openness\":\n\"Low\", \"Conscientiousness\": \"High\",\n\"Extraversion\": \"Low\",\n\"Neuroticism\": \"Low\"},\n{\"Agreeableness\": \"High\", \"Openness\":\n\"High\", \"Conscientiousness\":\n\"High\", \"Extraversion\": \"Low\",\n\"Neuroticism\": \"Low\"}"}, {"title": "System and User prompts", "content": "We use, different System and User prompts to extract the discourses and ratings from the conversing and judge agents."}, {"title": "Discourse Generation", "content": "The system prompt to generate the discourses:\nSYSTEM_PROMPT = f\"You are participating in a structured debate on: '{topic}'\\n\"Your responses should reflect these personality traits:\\n\" f\"- Agreeableness: {traits['Agreeableness']}\\n\" f\"- Openness: {traits ['Openness']}\\n\" f\" Conscientiousness: {traits['Conscientiousness']}\\n\" f\"- Extraversion: {traits['Extraversion']}\\n\" f\" Neuroticism: {traits['Neuroticism']}\\n\\n\"\n\"Rules: \"\n\"- Maintain these personality traits (DO NOT EXPLICITLY MENTION IN TEXT) at all times during your conversation\\n\"\n\"_ Keep responses under 50 words\\n\"\n\"_Maintain your personality consistently\\n\"\n\"- Address previous arguments directly but do not repeat what the other speaker said.\\n\"\n\"- End with proper punctuation \"\nThe user prompt carries the previous argument :\nUSER_PROMPT = \"\"\"Previous\nArgument:f\"{previous_arguement}\"\n\"\"\""}, {"title": "Extracting Personalities from the Judge Agents.", "content": "The system prompt to extract the personality traits:\nSYSTEM_PROMPT = \"\"\"Analyze text segments from two anonymous debaters (Person One and Person Two) for:\n1. Big Five Inventory (BFI) traits (High/Low for each dimension)\n2. Consistency with typical behavior for those traits (Yes/No)\nFor each person, return:\n{\n\"predicted_bfi\": {\n\"Agreeableness\": \"High/Low\",\n\"Openness\": \"High/Low\",\n\"Conscientiousness\": \"High/Low\",\n\"Extraversion\": \"High/Low\",\n\"Neuroticism\": \"High/Low\"\n}\n}\n}\n\"\"\"\nThe user prompt is:\nUSER_PROMPT= '''f\"Analyze {persona}'s text:\\n{text}'''\nwhere the persona contains Participant 1 and 2 and the text contains the discourses for each of the participants respectively."}, {"title": "Metadata of the Discourses.", "content": ""}]}