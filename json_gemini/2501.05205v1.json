{"title": "Discovering Hidden Visual Concepts Beyond Linguistic Input in Infant Learning", "authors": ["Xueyi Ke", "Satoshi Tsutsui", "Yayun Zhang", "Bihan Wen"], "abstract": "Infants develop complex visual understanding rapidly, even preceding of the acquisition of linguistic inputs. As computer vision seeks to replicate the human vision system, understanding infant visual development may offer valuable insights. In this paper, we present an interdisciplinary study exploring this question: can a computational model that imitates the infant learning process develop broader visual concepts that extend beyond the vocabulary it has heard, similar to how infants naturally learn? To investigate this, we analyze a recently published model in Science by Vong et al., which is trained on longitudinal, egocentric images of a single child paired with transcribed parental speech. We introduce a training-free framework that can discover visual concept neurons hidden in the model's internal representations. Our findings show that these neurons can classify objects outside its original vocabulary. Furthermore, we compare the visual representations in infant-like models with those in modern computer vision models, such as CLIP or ImageNet pre-trained model, highlighting key similarities and differences. Ultimately, our work bridges cognitive science and computer vision by analyzing the internal representations of a computational model trained on an infant's visual and linguistic inputs.", "sections": [{"title": "1. Introduction", "content": "Infants are remarkable learners, sparking interest across various academic disciplines to uncover their learning mechanisms. Computer vision is no exception, with researchers studying infant visual learning from various perspectives [1, 30, 31, 33]. A recent milestone is Vong et al. [38], who introduces a model trained on longitudinal egocentric videos of a single infant, captured from 6 to 25 months. This model, termed the Child's View for Contrastive Learning (CVCL), is trained to associate transcribed parental speech with corresponding video frames in a similar manner to CLIP [32]. Learning from this unique infant dataset, CVCL has demonstrated notable object recognition abilities.\nThese results are consistent with findings from developmental psychology, which directly studies infants through experimental and observational research. Infants learn their first object names through direct association, connecting words they hear with objects they see. They can associate words spoken by parents with their visual referents as early as 6 months of age [3, 16]. Their early object recognition develops through a prolonged, multi-stage learning process, which requires infants significant visual experience with specific objects in the complex real world. As infants accumulate visual experience with certain objects, they become better at recognizing and categorizing these objects in varied contexts. According to a headcam study done on 8.5 to 10.5 month-old infants during feeding [8], infants' first nouns were those objects that were most frequent in their visual fields. While infants may also be hearing the names of frequently seen objects, their early visual familiarity with these common objects might come first and even help support the process of learning their names. For example, infant has shown innate ability to recognize patterns [12]. Moreover, their development of visual concepts occurs before the emergence of verbal thought [26].\nBased on the infant studies shown above, we hypothesize that a model trained on infant-like data may similarly develop visual concepts that are not explicitly represented in its linguistic training data. Specifically, while the original study [38] performs classification with the vocabulary that limited to what the infant have heard from parents (in-vocabulary), the model could have developed visual concepts extending beyond the explicit linguistic input present during training phase (out-of-vocabulary).\nTo investigate this, we analyze the CVCL's internal representation, using network dissection [2, 29], or more intuitively, \u201cneuron labeling\". We then introduce NeuronClassifier, a neuron-based, training-free classification framework that leverages visual concepts identified from model's internal representations. Our approach not only achieves higher recognition accuracy than originally reported by CVCL, but also discovers meaningful out-of-vocabulary concepts embedded within the model, which supports our hypothesis. More interestingly, the out-of-vocabulary words found within the model tend to have a higher age of acquisition for infants (Figure 6), representing higher-level concepts. This indicates that the model has developed broader visual concepts ahead of linguistic understanding, aligning with cognitive studies showing that infants develop visual concepts as preverbal thought [26].\nNontheless, CVCL's recognition ability is still limited compared to models like CLIP or ImageNet-trained models, which are widely used in computer vision as generic visual representations. This raises the question: what distinguishes these higher-performing models from CVCL? To investigate this, we analyze the internal representations of CVCL in comparison with CLIP and ImageNet models. We find that while CVCL exhibits similar low-level features in its early layers, its high-level features in the final layer differ significantly. These differences extend to the visual concept neurons present in the model's deeper layers.\nContributions The key contributions of our work are:\n\u2022 We show that infant model have developed understanding beyond linguistic training inputs, by discovering visual concepts hidden in the model representations, aligning with cognitive naturalness.\n\u2022 We propose a training-free classification framework that leverages the model's internal representations through neuron labeling, which helps us understand the visual representations within the infant model.\n\u2022 We find that the infant model shares similar low-level representations with large-scale pre-trained models but diverges in deeper layers due to a lack of diverse high-level visual concepts."}, {"title": "2. Related Work", "content": "Learning from Children. Modeling how children learn has long been a strategy for advancing artificial intelligence. Instead of directly replicating adult intelligence, Alan Turing suggested, \u201cwhy not rather try to produce one which simulates the child's?\u201d [37] With this in mind, training models on infant's egocentric videos [1, 30, 31, 33, 36, 38] is a natural direction, because they are an approximation of real training data for infant vision system. This work builds upon CVCL model [38], which we explain in Sec. 2.1.\nInterpreting Vision Model Representations. Our goal is to understand the model trained on infant data. In this context, techniques for interpreting intermediate representations in deep neural networks are relevant. Beginning with Network Dissection [2], a method that quantifies alignment between hidden neurons and visual concepts, numerous studies have aimed to make black-box models more transparent. These methods enable compositional concept discovery [27], assignment of compositional concepts with statistical quantification [5], open-vocabulary neuron captioning [20], and the use of CLIP's rich embeddings for neuron-concept alignment [21, 29]. Beyond direct neuron dissection, other approaches analyze component functions by decomposing image representations to reveal the role of attention heads within multimodal embedding spaces [14] and identifying neurons with similar functions across a diverse model zoo [11]."}, {"title": "2.1. Preliminary: CVCL model", "content": "Training data. The CVCL model is trained on the SAYCam-S dataset [35], a longitudinal collection of egocentric recordings from a child aged 6 to 25 months, containing around 200 hours of video. To create meaningful image-text pairs for model training, transcripts were pre-processed to retain only child-directed utterances, excluding the child's own vocalizations. Frames were extracted to align with utterance timestamps. The resulting dataset comprises 600,285 frames paired with 37,500 transcribed utterances, forming a multimodal dataset that simulates the sparse and noisy real-world experiences from which children learn.\nModel architecture. Employing a self-supervised contrastive learning approach akin to CLIP [32], the CVCL model learns to align egocentric visual frames with transcribed parental speech. Co-occurring pairs are treated as positive examples, while non-co-occurring pairs serve as negatives. This method allows the model to develop multimodal representations without external labels, imitating a child's natural learning process.\nEvaluation. For evaluation, CVCL adopts a n-way classification task in which the model selects the most relevant visual reference from a set comprising one target image and n \u2212 1 foil images. This approach is inspired by the intermodal preferential looking paradigm (IPLP) [15, 16], used in infant recognition studies to measure language comprehension through differential visual fixation. By aligning its visual and text encoders, CVCL achieves comparable in-domain test accuracy to models like CLIP. However, for out-of-domain generalization, CVCL demonstrates relatively weak performance on the Konkle object dataset [22], which includes naturalistic object categories on a white background, using only classes available in the training data."}, {"title": "3. Method", "content": "In this section, we describe how to explore neuron-level concepts and leveraging them in n way classification tasks. We begin by using published neuron labeling techniques to discover visual concepts hidden within the CVCL model [38]. Then, we introduce our framework to utilize these labeled neuron concepts for n-way classification."}, {"title": "3.1. Neuron Labeling", "content": "We follow CLIP-Dissect [29] for internal representation analysis due to its flexibility in concept sets and probing images, which helps us easily identify visual concept neurons.\nPreliminary: CILP-Dissect. Given a neural network $f(x)$, where f takes a image x as input and $x \\in D_{probe}$ with $|D_{probe}| = N$, and a concept set S with $|S| = M$. The algorithm computes the concept-activation matrix $P\\in R^{N\\times M}$.\n$P_{i,j} = I_i. T_j$,\nwhere $I_i$ and $T_j$ are the embeddings of the images and concepts, respectively. For each neuron $k \\in K$, where K denotes the set of all neurons in the network, we summarize activations $A_k(x_i)$ with a scalar function g, producing an activation vector:\n$q^k = [g(A_k(x_1)),...,g(A_k(x_n))]^T \\in R^N$.\nThe neuron is labeled with the concept that maximizes similarity:\n$l_k = arg \\max_m sim(t_m, q^k; P)$.\nIn here $sim(\\cdot, \\cdot)$ represents the similarity function (e.g., cosine similarity, Soft-WPMI[29, 39]). Collecting the label $l_k$ for each neuron, we define the label vector $L = [l_1, l_2, ..., l_k]$ to represent the assigned concepts across the entire model.\nDuring this neuron labeling process, we aim to assign meaningful concepts to each neuron. The two main reasons for choosing CLIP-Dissect are:\n\u2022 Concept set S: Instead of allowing an infinite range of possible concepts for neuron labeling, using a fixed concept set narrows this process by constraining it to a limited selection of concepts. Including high-level concepts in this set enables us to identify neurons corresponding to these visual concepts. This setup ensures that each neuron is assigned a specific label, though some labels may be spurious, as illustrated in Figure 3.\n\u2022 Probing dataset $D_{probe}$: The probing dataset allows neurons to activate specifically in response to the dataset of interest. For instance, when analyzing representations from an infant model, we use images from the Konkle object dataset, which better aligns with infant recognition than the ImageNet [10] validation set. Additionally, for classification tasks, generalization can be achieved by adaptively selecting the probing dataset to focus on relevant concepts within a given dataset.\nExtending beyond vocabulary. The flexibility of the concept set S provides a spectrum of possibilities, allowing us to discover visual concepts that the infant model has never encountered in its training data. In this process, CLIP serves as a well-pretrained miner, utilizing its rich image-text embeddings to identify concepts hidden within the CVCL model. By aligning the activations of CVCL's neurons with CLIP's embeddings, we uncover meaningful hidden visual concepts within the infant model, even extending beyond the model's linguistic training data. This approach leverages the diverse vocabulary of the concept set and the rich embeddings of CLIP to reveal visual concepts embedded in the CVCL model's internal representations, each associated with corresponding neurons."}, {"title": "3.2. Neuron-Based Classification", "content": "How do we ensure that neurons representing concepts beyond the model's original vocabulary truly exist within the network? In this section, we propose NeuronClassifier, a training-free framework that leverages neuron activations to detect and validate such concepts. By identifying neurons with specific visual concept, we aim to confirm the presence of these latent, beyond-vocabulary neurons and use them to perform n-way classification. The framework, illustrated in Figure 4, involves three main steps.\nStep 1: neuron labeling with concept set. Given an image encoder f(x), we labeled each neuron in the network using a concept set S that contains (but is not limited to) all class labels relevant to the task. The dissection process can be expressed as a function:\n$N_s = NeuronLabeling(f, S)$,\nwhere $N_s$ is the set of neurons labeled with concepts from S. Each neuron $k \\in N$ is associated with a specific concept, such as \u201crug\u201d or \u201ccalculator\u201d, based on its alignment with concept embeddings obtained during neuron labeling (e.g. similarity in CLIP-Dissect [29]).\nStep 2: identifying visual concept neurons. Given a target label $l_k \\in L$, where L represents all neurons' label in the model, same as label vector in Section 3.1 (e.g., \"rug\"), we select the subset of neurons labeled with this concept, denoted as $N_{l_k} \\subset N_c$. These neurons are responsible for encoding the target concept.\nTo further refine the labeling and reduce spurious assignments, we select the most similar neuron from $N_{l_k}$ to represent the target concept. The similarity measure varies based on the dissection method used. For example, Network Dissection [2] employs Intersection over Union (IoU) for similarity, while CLIP-Dissect [29] supports multiple similarity metrics:\n$k^* = arg \\max_{k \\in N_{l_k}} sim(t_{l_k}, q^k; P)$,\nwhere $t_{l_k}$ is the embedding of the target label $l_k$, $q^k$ is the neuron activation value, same in Section 3.1. This step ensures that the neuron most aligned with the concept is selected, minimizing the possibility of spurious labeling.\nFor each selected neuron $k \\in N_{l_k}$, its activation value on an input image $x_i$ is computed as\n$q_k(x_i) = g (A_k(x_i))$,\nwhere $A_k (x_i)$ is the raw activation map, and g() is a summary function (e.g., spatial mean) that reduces it to a scalar representing the neuron's response strength.\nStep 3: selecting the most relevant image. Given n candidate images {$X_1,X_2, ..., X_n$} in an n-way trial, we compute the activation values $q_k (x_i)$ for neuron $k^*$ with highest similarity across all images. The image with the highest activation is selected as the most relevant to the target concept:\n$x^* = arg \\max_{X_i} q_{k^*} (X_i)$.\nIn the example shown in Figure 4, the target concept is \u201crug\u201d, and we select the image with the highest activation from the four candidates as the closest match to the concept."}, {"title": "4. Neuron-wise Representation Analysis", "content": "In this section, we conduct a neuronal analysis on infant models, focusing on the validation of beyond vocabulary neurons' visual concepts using the proposed NeuronClassifier framework. This analysis aims to demonstrate the infant model's strong generalization potential to unseen classes and, further, its capacity to capture higher-level visual concepts."}, {"title": "4.1. Setup", "content": "Datasets. We use the Konkle object dataset [22], as introduced in Section 2.1. This dataset consists of 3,406 images, each featuring a single object on a clean white background, including 406 test items across 200 classes. Each trial comprises n images: one target image and the remaining as foils, with foil images randomly sampled from classes other than the target class. For each class, we generate 5 trials, each containing n images. Following the previous work on CVCL, we use n = 4 in our main experiments, with one target and three foils per trial.\nNeuron labeling. We utilize CLIP-Dissect [29] for neuron labeling, which assigns visual concepts to each neuron in the network. As we aim to perform classification on the Konkle object dataset, we use the same dataset as a part of $D_{probe}$. Additionally, to avoid limiting the search space only around class names and ensure comprehensive neuron labeling, we employ a combined concept, consisting of three components:\n\u2022 SAYCam-S vocabulary: We clean the original vocabulary by removing noisy child speech and retaining meaningful words.\n\u2022 Common english words: We chose the top 30,000 most common English words based on a 1-gram frequency analysis by Peter Norvig [28, 29].\n\u2022 Class name in Konkle object dataset: All class labels from the Konkle object dataset are included.\nWe combine these three sources, ensuring no duplicates, resulting in a final concept set containing 30,427 words.\nModels. Our primary focus is the CVCL-ResNeXt50 [38], trained on SAY-Cam-S [35] dataset. For comprehensive analysis, we apply our framework to the following models:\n\u2022 Infant-like models: The CVCL-ResNeXt50 model is trained on infant data, incorporating both egocentric frames and transcribed parent speech. To further test our hypothesis on visual concept acquisition in infant models, we also use DINO-S-ResNeXt50 [30], trained with the DINO [6] self-supervised approach on same dataset as CVCL. However, unlike CVCL, DINO-S-ResNeXt50 is trained only using video frames to get visual embeddings, without transcribed parent speech.\n\u2022 Large-scale pre-trained models: To establish an upper bound, we include two well pre-trained models, CLIP [32] and ResNeXt [40]. Although CLIP uses a ResNet50-based vision encoder rather than ResNeXt, we select CLIP-ResNet50 due to the architectural similarity between ResNeXt [40] and ResNet [19].\n\u2022 Baseline (randomized) model: As a lower bound, we introduce a randomized version of the CVCL-ResNeXt50 model. In this setup, the convolution layer weights in the vision encoder are initialized using Kaiming Initialization [18], with batch normalization weights set to one and all biases initialized to zero."}, {"title": "4.2. Results", "content": "In this section, we present the results of our NeuronClassifier framework applied to different models, especially CVCL. Our results demonstrate that the proposed framework effectively uncovers meaningful neuron that go beyond the model's training vocabulary. Moreover, these findings, aligns with cogntitive perspective of vocabulary acquisition. We analyze the models' performance across different n-way settings. Our method results in strong out-of-vocabulary classification as well as improved in-vocabulary classification.\nClass coverage in visual concept neurons. How well do the class corresponding visual concept neurons identified through neuron labeling (e.g. \"rug\" as class name, rather than general discription as \"red\")? Figure 5 shows the percentage of number of classes in the Konkle object dataset that are discovered in visual concept neurons from each model during neuron labeling process. The results indicate that, well pre-trained models, such as CLIP-ResNet50 and ResNeXt50, demonstrate broader class name coverage. While CVCL perform slightly weaker, but still maintain around slightly less than half 50%. CVCL-Randomized is only around 28%. This class coverage reflects the models' capacity to directly class corresponding meaningful representations during the neuron labeling process.\nAge of acquisition ratings. Age of acquisition (AoA) is used to indicate when, and in what sequence, words are learned, and it is often assessed through ratings or observations reported by adults. This indirect method generally correlates well with other metrics indicating when children acquire vocabulary. Previous developmental work has shown that infants' early visual familiarity with common objects helps with object recognition, which subsequently helps support the process of learning the names of those objects. We next examined how early words in our models are learned and whether there is an AoA difference between in-vocab and out-of-vocab words.\nWe used AoA ratings from a dataset compiled by Kuperman, Stadthagen-Gonzalez, and Brysbaert [24], which includes norms for over 30,000 English words gathered via Amazon Mechanical Turk. Each participant estimated the age in years at which they believed they first understood each word, even if they didn't actively use them. This dataset is comparable to previously reported AoA norms gathered in laboratory settings.\nUsing this set of AoA norms, we compared mean AoA between in-vocab and out-of-vocab words discovered in CVCL's internal representations. As shown in Figure 6, we found a significant difference between in-vocab and out-of-vocab AoA rating (t(82) = 4.64, p < 0.0001), in-vocab words (mean AoA = 4.99) are learned earlier than out-of-vocab words (mean AoA = 6.82). This pattern suggests that: (1) both sets of words are learned quite early, around later preschool and school years, with or without supervised labeling; (2) the difference in AoA between in-vocabulary and out-of-vocabulary words indicates that the infant model has developed a basic understanding of higher-level concepts (reflected by larger AoA). This foundational knowledge may lay the groundwork for word learning once corresponding parental speech is introduced.\nNeuron-based classification performance. We evaluated the hidden potential of infant models' vision encoders by applying our NeuronClassifier framework. Despite being trained on limited infant egocentric data, CVCL-ResNeXt50 demonstrates the ability to recognize similarly as nature of infant learning, achieving strong out-of-vocabulary classification performance. This result suggests that this infant-like model developed broader visual concepts that extend beyond linguistic input, similar to how infants naturally learn.\nAdditionally, we applied our method to in-vocabulary classification, where it outperformed the vanilla method previously used in CVCL [38], as introduced in Figure 2. Here, \u201cin-vocab\u201d and \u201cout-of-vocab\u201d classes are defined based on the CVCL model's vocabulary, with \u201call\u201d representing the combined performance on both types.\nNotably, the DINO-S-ResNeXt50 [30], another infant model trained on the same dataset but without linguistic input, achieved similar performance using our method. This further proves that infant-like models have broader visual representations from egocentric visual inputs. Models such as ResNeXt50 and DINO-S-ResNeXt50 lack a text encoder and therefore cannot perform out-of-domain dataset classification without additional training; ours relies purely on their visual representations to perform classification.\nInterestingly, under the vanilla evaluation on the Konkle object dataset, DINO-S-ResNeXt50 required additional fine-tuning for this downstream task. However, when using our method, it performed comparably to CVCL, highlighting the effectiveness of our approach. On the other hand, applying our NeuronClassifier framework to the original CLIP's ResNet50 resulted in lower accuracy than using it directly. While the aim of this paper is not to improve the accuracy of neuron labeling methods, our goal is to discover the visual concepts learned by infants that extend beyond linguistic training input.\nAnalysis across n-way settings. We evaluate the models under various n-way classification setups (n = 2, 4, 8, 16, 32). Figure 7 illustrates the performance trends for out-of-vocabulary class classification accuracy using our proposed method, NeuronClassifier.\nOur method not only enables out-of-vocabulary classification but also significantly improves in-vocab performance compared to previous results [38], further supporting the presence of beyond-vocabulary potential in infant models. However, due to limited class coverage, the CVCL model with our method can classify a maximum of 31 classes, resulting in missing values at n = 32. These findings support our hypothesis that the infant model has acquired visual concepts beyond its initial vocabulary. These results show that leveraging the model's internal representations for classification that go beyond its vocabulary is not only feasible but also robust across different n-way settings. These findings support our hypothesis that the infant model has acquired visual concepts beyond its initial vocabulary.\nHowever, as n increases exponentially, performance gradually declines. This decline is reasonable, as the task becomes increasingly difficult by nature as n increases. It may also be attributed to dimensionality reduction in activation maps, leading to coarser classification. CLIP-RN50 and ResNeXt50 perform well with NeuronClassifier, though not as effectively as their direct or fine-tuned versions, since our method is designed to reveal latent concepts rather than to perform fully optimized classification. In in-vocab settings, the \u201cvanilla\" approach represents direct classification as shown in Figure 2."}, {"title": "5. Layer-wise Representation Analysis", "content": "While the previous section demonstrated that the model trained on infant data learned visual concepts beyond parental speech, its classification performance still lagged behind models trained on large-scale datasets like ImageNet. To explore how the representations captured by infant models compare to those of large-scale pre-trained models, we perform a layer-wise representation analysis using Centered Kernel Alignment (CKA) [23]. This method measures the similarity between the representations of the CVCL-ResNeXt50 model, an ImageNet-pretrained ResNeXt, and CLIP-ResNet50. Additionally, we apply neuron labeling techniques from a layer-wise perspective to identify unique visual concepts discovered at each layer. We found that the large-scale pre-trained models have much more diversity of high-level visual concepts in the last few layers, which aligns with the low similarity observed in these layers on CKA when compared to CVCL. This indicates that CVCL lacks the development of a certain level of complexity in visual concepts, providing insights into how the infant model and large-scale pre-trained models diverge or align at different levels of abstraction.\nCentered Kernel Alignment (CKA). CKA is an effective metric for comparing the similarity between the layer-wise representations of different neural networks. For two sets of activations, $X \\in R^{n \\times p}$ and $Y \\in R^{n \\times q}$, from corresponding layers of two models, where n is the number of examples and p and q are the feature dimensions (i.e., the number of neurons in each layer), the linear CKA is defined as:\n$CKA(X, Y) = \\frac{HSIC(X, Y)}{\\sqrt{HSIC(X, X) HSIC(Y, Y)}}$,\nwhere HSIC(,) is the Hilbert-Schmidt Independence Criterion [17], which measures the dependence between two datasets. A higher CKA score indicates more similar representations between two models at the given layer.\nLayer-wise similarity. To obtain activations X and Y, we used the ImageNet validation set [10] as input for the networks. We computed the CKA similarity between the infant model (CVCL), the ImageNet-pretrained ResNeXt50, and CLIP-ResNet50. The results are presented as matrices in Figure 8.\nOur layer-wise analysis using CKA highlights the differences in how CVCL captures representations compared to well-pretrained models. Layers closer to the input exhibit higher similarity across models, while deeper layers show greater divergence. This pattern aligns with findings in model training, where shallow layers tend to converge early on low-complexity features, and deeper layers progressively specialize to capture higher-level concepts [7, 13].\nDespite being trained on limited, noisy, and sparse data, CVCL successfully develops lower-level representations that are comparable to those in large-scale pre-trained models. However, the divergence in deeper layers reflects the constraints imposed by its dataset, indicating a lack of the more diverse high-level representations typically observed in models trained on large-scale datasets.\nUnique visual concept neurons identification. To further investigate the unique characteristics of each layer, we apply network dissection to identify neurons that are aligned with specific visual concepts and are unique. Essentially, this is an extension of the neuron labeling process, where the Broden[2] dataset provides category labels for each visual concept neuron. We counted the number of unique visual concepts discovered in each category and perform layer-wise comparisons to gain a broader view of the differences between models. In Figure 9, we visualize the number of unique detectors across layers for each model. The results show that early layers in all models predominantly detect low-level features like color, with minimal differences between models. However, as we move to deeper layers, high-level concepts such as objects and scenes become more prominent, and the disparities between models become clearer. The CVCL model exhibits fewer unique detectors in these high-level categories compared to ImageNet model, reflecting its limited exposure to diverse, large-scale datasets."}, {"title": "6. Conclusion", "content": "In this paper, we explored whether an infant-like model (CVCL), trained on infant egocentric video frames and linguistic inputs, can acquire broader visual concepts extending beyond its initial training vocabulary. By introducing NeuronClassifier, a training-free framework to discover and leverage visual concepts hidden in representations, we unlocked the CVCL visual encoder's ability to recognize out-of-vocabulary concepts, establishing its potential as a strong classifier. Our findings also reveal that while CVCL's representations capture low-level features similar to those in large-scale pre-trained models, they diverge significantly in higher-level representations, contributing to the observed performance differences.\nOverall, our approach bridges cognitive science and computer vision, providing insights into how infant-like models develop visual concepts that precede linguistic inputs, aligning with the natural way infants explore the world through sight."}, {"title": "Appendices", "content": "A. Neuron-wise Analysis\nWe present additional examples illustrating how the infant model classifies images using visual-concept neurons. Furthermore, we provide complete results of Age of Acquisition (AoA) to quantify the cognitive level of visual concepts for both in-vocabulary and out-of-vocabulary cases.\nA.1. Neuron-based Classification Examples\nFor this analysis, we conducted 4-way classification experiments on the Konkle object dataset to evaluate out-of-vocabulary classification. The examples are derived from neurons selected randomly under the specified experimental settings, with classification trial images and neuron activations presented, details in Figure 10 and Figure 11.\nA.2. Age-of-Acquisition Ratings\nA.2.1. Details\nAge-of-Acquisition (AoA) ratings, as defined by Kuperman et al. [25], estimate the age at which a person learns a word. These ratings were obtained via crowdsourcing using 30,121 English content words, organized into frequency-matched lists based on the SUBTLEX-US corpus [4]. Each list included calibrator and control words for validation.\nParticipants on Amazon Mechanical Turk rated the age they first understood each word on a numerical scale (in years). Words unfamiliar to participants could be marked with \"x\" to exclude outliers. Data cleaning removed non-numeric responses, ratings exceeding participant age, low-correlating responses (r < 0.4), and extremely high AoA ratings (> 25 years). This yielded 696,048 valid ratings.\nThe AoA ratings strongly correlated with prior norms (r = 0.93 with Cortese and Khanna [9], r = 0.86 with the Bristol norms [34]), confirming their reliability for studying vocabulary development.\nA.2.2. Detailed AoA Results\nHere we present visual concepts that originally from Konkle object dataset[22] class names, by applying neuron labeling techniques, we found many visual-concept neurons with corresponding class inside vision encoder's hidden representation. For founded classes, we investigate their AoA values to prove the alignment between computational model and infant cognition. The out-of-vocab class in Table 3 tend to have higher AoA values than in-vocab in Table 2, representing the early development of infant visual system that extend beyond linguistic inputs."}]}