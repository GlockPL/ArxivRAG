{"title": "SELF-[IN]CORRECT:\nLLMs Struggle with Refining Self-Generated Responses", "authors": ["Dongwei Jiang", "Jingyu Zhang", "Orion Weller", "Nathaniel Weir", "Benjamin Van Durme", "Daniel Khashabi"], "abstract": "Can LLMs continually improve their previous outputs for better results?\nAn affirmative answer would require LLMs to be better at discriminating\namong previously-generated alternatives, than generating initial responses.\nWe explore the validity of this hypothesis in practice.\nWe first introduce a unified framework that allows us to compare the\ngenerative and discriminative capability of any model on any task. Then,\nin our resulting experimental analysis of several LLMs, we do not observe\nthose models' performance on discrimination to be reliably better than\ngeneration. We hope these findings inform the growing literature on self-\nimprovement AI systems.", "sections": [{"title": "1 Introduction", "content": "The promise of Large Language Models (LLMs) that can self-improve has brought both\nexcitement and fear about the future impact of AI. On one hand, this has captured the\nattention of AI practitioners excited about Artificial General Intelligence (AGI). On the\nother hand, this has raised the alarms for those already anxious about the rapid pace of\ndevelopments in AI (Hunt, 2023; Hutson, 2023) since self-improving AI could evolve in\nunpredictable ways.\nIt remains a mystery what is needed for LLMs to continually self-improve. In human\nlearning, improvement involves learning from one's mistakes, a process often supported by\nfeedback from a coach or the environment. Needless to say, the feedback should accurately\nidentify the mistakes made by the individual without mistakenly criticizing what has\nbeen done correctly. This principle should be applicable to LLMs as well. For LLMs to\nreliably self-improve, i.e. generating their own feedback for self-improvement, the ability\nto discriminate whether their own generations are good or not should surpass the ability to\ngenerate good solutions directly. Given the importance of this capability, it is worth raising a\nquestion about the foundations of self-discrimination: Are LLMs really better at discrimination\nthan generation?\nThis paper seeks to answer this question by proposing the SELF-[IN]CORRECT hypothesis\n(\u00a73.2): LLMs are not better at discriminating among previously-generated alternatives than gen-\nerating initial responses. Determining the validity of this hypothesis is crucial, as existing\nstudies provide initial evidence suggesting that the capability to distinguish between LLM-\ngenerated options is both a sufficient (Tyen et al., 2023) and necessary (Huang et al., 2023)\ncondition for self-improvement.\nIt is non-trivial to compare LLMs' generative capability with their discriminative capability\non the same footing. Recent work, such as West et al. (2023), compares these capabilities by\ncontrasting model's generative ability v.s. their accuracy of choosing the ground-truth answer\namong answer options. However, this discriminative setting differs from the generative\nsetting (and crucially that of self-improvement settings), as the ground-truth answer is\nnot available during generation, potentially making the upper bound of discrimination\nhigher. To more clearly measure these abilities, we implement a two-phase methodology"}, {"title": "2 Related Work", "content": "Self-Improvement with LLMs. The idea of self-improvement predates the LLM era.\nEarlier efforts have utilized generative adversarial networks (GANs) (Subramanian et al.,\n2017; Yu et al., 2017) to enhance NLP systems through feedback mechanisms. Welleck et al.\n(2023) trains a separate corrector that learns to iteratively correct imperfect generations."}, {"title": "3 SELF-[IN]CORRECT", "content": "In this section, we clarify our definition, introduce our evaluation framework (Figure 1),\nand present our hypothesis."}, {"title": "3.1 Establishing an Evaluation Criteria to Compare Generation vs. Discrimination", "content": "Given a task T with an evaluation dataset $D = \\{(x_i, y_i)\\}_{i=1}^n$ and evaluation metric f, we use\nthe same LLM, denoted by PLM, for both generation and discrimination. For each evaluation"}, {"title": "3.2 Hypothesis Formulation", "content": "Given the above definitions, our main hypothesis becomes easy to formalize. For any\ngiven task, denote DG-DIFF as the difference between discrimination performance and\ngeneration performance,\n$DG-DIFF = S_{disc} - S_{gen}$.\nOur main hypothesis is:\nSELF-[IN]CORRECT. LLMs are not universally better at discriminating among previously\ngenerated alternatives than generating initial responses. Specifically, DG-DIFF < \u0454,\nwhere \u0454 is a small enough value that doesn't result in statistically significant superiority of\ndiscriminative performance ($S_{disc}$) over generative performance ($S_{gen}$) across the samples.\nNote the term \u201cdiscrimination\" is employed with precision in this context. It is selected\nover alternatives such as \u201cevaluation\u201d which may also imply the provision of qualitative or\nquantitative feedback on generated responses, or \u201cverification\", which typically results in a\nbinary judgment of truthfulness.\nAn important design choice in our framework is that the candidate generations $G(x_i)$ are\nshared across the generative and discriminative phases. This design choice allows us to\nformulate the generative phase as a random multiple choice among pre-generated candidates.\nAs a result, it allows apples-to-apples comparison with the discriminative phase, where the\ntask is using LLM for multiple choice among the same candidates."}, {"title": "4 Empirical Support for SELF-[IN]CORRECT", "content": "In this section, we describe our experimental setup (\u00a74.1) and lay out the main findings\n(\u00a74.2)."}, {"title": "4.1 Experimental Setup", "content": "Tasks. A summary of the tasks we evaluate on is provided in Table 1. We assess our\nhypothesis on a diverse set of tasks including GSM8K (Cobbe et al., 2021) for math, Trivi-\naQA (Joshi et al., 2017) for world knowledge, TruthfulQA (Lin et al., 2022) for truthfulness\nin question answering, and MT-Bench (Zheng et al., 2023b) for instruction following. These\nrepresent a diverse set of benchmarks used to evaluate LLMs across various domains. For\nTriviaQA, we use the rc.nocontext setup, which means the model relies solely on its\nparametric knowledge to answer the question correctly without accompanying context\nor documents. For TruthfulQA, we use the generation setup, where the model generates\nresponses to a set of questions. The metrics scale for MT-Bench is 0-10 Zheng et al. (2023b).\nTask metrics. The list of task-specific metrics f(.) is provided in Table 1. The evaluation\nfor GSM8K, TriviaQA and TruthfulQA is conducted using lm-evaluation-harness2, which\nprovides a standardized framework for assessing model performance across benchmarks.\nThe evaluation for MT-Bench is done with 11m_judge\u00b3, which use GPT-4 score (Zheng et al.,\n2023b) to score the generated answer from models. To evaluate TruthfulQA generations, we\nfollow Lin et al. (2022) and develop two \u201cGPT-judges\" by fine-tuning GPT-3 models with\nthe provided data . Specifically, we fine-tune one \"GPT-judge\u201d for truthfulness and another\nfor informativeness. Finally, we report the percentage of answers that are both truthful and\ninformative as the final metric for TruthfulQA.\nHandling failure modes during evaluation. While evaluating the discrimination phase,\nif the model output does not adhere to the expected format (i.e., integers indicating the"}, {"title": "4.2 Main Findings", "content": "DG-DIFF is generally small or negative. From extensive results in Table 2, it can be\nconcluded that DG-DIFF is generally small or negative, i.e., $S_{disc}$ is generally lower than\n$S_{gen}$. This conclusion also holds when we examine the average task performance for all\nmodels. An ablation experiment is conducted in Appendix D that implements several\ndifferent prompts and we find prompt variations do not significantly affect DG-DIFF.\nAlthough $S_{gen}$ sometimes exceed $S_{disc}$, in such cases DG-DIFF remains quite small. All these\nobservations lend support for SELF-[IN]CORRECT.\nDG-DIFF remains small or negative upon fine-tuning. LLaMA-2 Chat models are fine-\ntuned with both instruction-tuning and RLHF while LLaMA-2 Base models are only pre-\ntrained with autoregressive objective. It is reasonable to expect instruction-tuned models\nwould exhibit better performance in the discrimination phase because instruction-tuning is\nshown to make models better at solving a variety of tasks. Furthermore, classification tasks\n(that resemble our discrimination setup) are well-represented in most instruction-tuning\ndatasets (Wang et al., 2022b; Bach et al., 2022; Longpre et al., 2023). However, our empirical\nfindings do not support it. Notably, LLaMA-2 13B and LLaMA-2 70B models outperformed\ntheir fine-tuned counterparts in our evaluations."}, {"title": "5 Further Analysis of SELF-[IN]CORRECT", "content": "In this section, we outline experiments designed to provide further analysis of SELF-\n[IN]CORRECT."}, {"title": "5.1 Aiding Discrimination Phase via Prompt-Engineering", "content": "One might argue our current prompting setup doesn't fully capitalize on the model's\ncapacity for discrimination. To make sure SELF-[IN]CORRECT isn't an artifact of poor\nprompt engineering, we conduct additional experiments with LLaMA-2 Chat models on\nGSM8K, TriviaQA, and MT-Bench because their DG-DIFF on those tasks is mostly negative."}, {"title": "5.2 Is Autoregressive Pre-training Related to SELF-[IN]CORRECT?", "content": "The majority of modern LLMs are pre-trained with an autoregressive objective. Recent\nstudies suggest that autoregressive objectives used during pre-training may have unex-\npected impacts on LLM behavior (McCoy et al., 2023). Since the pre-training process of\nautoregressive models is more akin to generation than discrimination, we hypothesize\nSELF-[IN]CORRECT is also partially caused by the use of autoregressive pre-training objective.\nTo test this hypothesis, we evaluated Flan-T5-XXL (11B) and Flan-UL2 (20B) on the same\ntasks used in Table 2. Flan-T5-XXL is pre-trained using a span corruption objective, where\nthe loss is only calculated on the corrupted span (Raffel et al., 2020). Flan-UL2 (Chung\net al., 2022) is pre-trained using mixture-of-denoisers that combines multiple denoising\nobjective functions. Our findings, detailed in Table 4, reveal their DG-DIFF across all tasks\nare positive except for Flan-T5-XXL on MT-Bench. In fact, both models also demonstrate\nsignificantly higher DG-DIFF compared to the autoregressive models we tested in Table 2.\nThis outcome lends empirical support to the hypothesis that SELF-[IN]CORRECT could be\nrelated to autoregressive pre-training."}, {"title": "5.3 Controlled modification of experimental setting: DG-DIFF notably improves when the\nnegative candidates given to the discrimination phase are simplified", "content": "Here we consider the extent to which SELF-[IN]CORRECT may hold. For example, is\nSELF-[IN]CORRECT potentially a fundamental limitation of LLMs pre-trained with an\nautoregressive objective, or can a change in data distribution alter the outcome? To address\nthis question, we conduct experiments in an unconventional setting that simplifies the\ndiscrimination phase by substituting incorrect candidates with simpler ones.\nThe experiments are conducted on TriviaQA and GSM8K. TriviaQA contains a wide range\nof answer categories, including names, locations, historical events, etc. For this dataset, we\nsimplify the discrimination phase by substituting incorrect answer generations for question\nA with correct answer generations from another question B (left panel in Figure 2). This\napproach simplifies the discrimination phase as it replaces hard-to-discriminate distractors\nwith simpler ones. As for GSM8K, we create simplified distractors by randomly multiplying\nor dividing incorrect generated answers by 100 (right panel in Figure 2).\nFrom Figure 3, it is evident that simplifying the incorrect candidates improves DG-DIFF.\nFor TriviaQA, $S_{disc}$ exceeds $S_{gen}$ by a large margin. For GSM8K, all models tested also\ndemonstrate improved DG-DIFF in this setting."}, {"title": "6 Further Discussion", "content": "Does SELF-[IN]CORRECT contradict prior findings in self-refinement? The process of\nself-refine involves utilizing the same LLM to provide feedback for its own generation and\nusing the feedback to refine the generation. Both Huang et al. (2023) and Madaan et al.\n(2023) suggested LLMs can self-refine on tasks other than reasoning. Does this contradict\nour assertions?\nWe replicated the experiment outlined in Madaan et al. (2023) and observed the following\n(more detail in Appendix B):\n(1) For some evaluated tasks, certain aspects can be exploited for artificially amplifying\ntask performance without actually reasoning on the feedback. For example, with the task\nof constrained generation, where the objective is to generate sentences containing specific\nkeywords, self-refine with LLMs often leads to progressively longer sentences that build\non previous ones. Thus, even if the refined sentences do not incorporate new keywords"}, {"title": "7 Limitations", "content": "One limitation of our research is the inability to test the impact of pre-training data. The\nscale of pre-training data makes it difficult to assess its effect.\nAnother limitation of our study is the comprehensiveness of our experiments. Extending\nour experiments to more recent models and more diverse tasks can strengthen the validation\nof SELF-[IN] CORRECT.\nDetermining a model's preference over several candidate generations can be challenging due\nto various biases. Following the methodology used in other self-improvement studies (Yuan\net al., 2024), we employ LLM-as-a-Judge prompting (Zheng et al., 2023a) to elicit answer choice\nfrom the model. It is conceivable that the LLMs we examine can be biased toward certain\nanswer options or different answer formats (e.g., labels of A/B/C/D or [1]/[2]/[3]/[4]).\nAnother method is to rank candidate answers using the LLM's assigned probability over\neach answer text, which we did not explore. It is worth noting that this approach can be\nbiased too. For example, the LLM might be biased to evaluate text fluency instead of the\ncorrectness of the answers."}, {"title": "8 Conclusion", "content": "We focused on the question of whether language models are strictly better at discriminating\ntheir prior generations vs. generating responses directly. We proposed a metric for com-\nparing these capabilities and used it to evaluate several current LLMs. For those models\nand tasks, we do not observe that discrimination is reliably better than generation, in fact,\nwe often observed it was worse. These results raise concerns about the potential for LLM\nself-improvement on any task."}, {"title": "A Prompts for TriviaQA and MT-Bench", "content": "In this section, we provide the original and CoT prompts for GSM8k (Figure 4), TriviaQA\n(Figure 5), TruthfulQA (Figure 6) and MT-Bench (Figure 7)."}, {"title": "B Extra analysis with Self-Refine", "content": "In this section, we provide more explanations on the issues we discovered for tasks tested in\nMadaan et al. (2023). We also provide the percentage of times the model used for generation\nprefers previous generations over self-refined subsequent generations in Table 5. To better\nexplain our point, one example from the task of acronym generation is presented in Figure 8,\nand another example from the task of constraint generation is provided in Figure 9."}, {"title": "C Predicting exact answers instead of answer options", "content": "In this section, we explore whether having discriminators provide exact (verbatim) an-\nswers improves performance compared to selecting among predefined answer choices\n(1/2/3/4). For this experiment, the discrimination prompt and in-context-learning ex-\namples are changed accordingly (the discrimination prompt has changed from \"end your\ngeneration with 1, 2, 3, or 4\" to \"end your generation with one of the generated answers\").\nThis analysis focuses on instances where models exhibit a high percentage of invalid re-\nsponses. According to the results presented in Table 6, shifting to exact answer generation\ndoes not significantly reduce the rate of invalid responses. In fact, in several cases, it appears\nto increase the percentage of invalid answers."}, {"title": "DAblation of prompts used for discrimination", "content": "Given the sensitivity of model responses to prompt variations (i.e., modifications in wording\ncan impact outcomes), we implemented several prompts to examine if the observed SELF-\n[IN]CORRECT persists or if it's merely a byproduct of specific prompt constructions.\nThis evaluation was specifically carried out on the GSM8K dataset using the LLaMA-2 13B\nmodel. This model is selected because its DG-DIFF on GSM8K is a big negative number.\nAccording to the findings presented in Table 7, alterations in prompt wording do not\nsignificantly affect performance."}]}