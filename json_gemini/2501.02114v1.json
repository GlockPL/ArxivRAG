{"title": "Relaxation-assisted reverse annealing on nonnegative/binary matrix factorization", "authors": ["Renichiro Haba", "Masayuki Ohzeki", "Kazuyuki Tanaka"], "abstract": "Quantum annealing has garnered significant attention as meta-heuristics inspired by quantum physics for combinatorial optimization problems. Among its many applications, nonnegative/binary matrix factorization stands out for its complexity and relevance in unsupervised machine learning. The use of reverse annealing, a derivative procedure of quantum annealing to prioritize the search in a vicinity under a given initial state, helps improve its optimization performance in matrix factorization. This study proposes an improved strategy that integrates reverse annealing with a linear programming relaxation technique. Using relaxed solutions as the initial configuration for reverse annealing, we demonstrate improvements in optimization performance comparable to the exact optimization methods. Our experiments on facial image datasets show that our method provides better convergence than known reverse annealing methods. Furthermore, we investigate the effectiveness of relaxation-based initialization methods on randomized datasets, demonstrating a relationship between the relaxed solution and the optimal solution. This research underscores the potential of combining reverse annealing and classical optimization strategies to enhance optimization performance.", "sections": [{"title": "Introduction", "content": "In recent years, advancements in information and communication technologies have made it easier than ever to quantify and utilize vast amounts of data. This has led to a growing interest in addressing societal challenges through the effective use of such data. Optimization plays a crucial role in these efforts. It focuses on minimizing a given objective function within a set of constraints, often modeled mathematically.\nThe inherent complexity of optimization problems has led to the development of new computing architectures that can handle large-scale combinatorial optimization problems. Quantum annealing, a meta-heuristic inspired by quantum physics, has emerged as a promising approach to solving such problems [1]. Quantum annealing leverages quantum fluctuations to escape local minima and find the global minimum of a given objective function. The method is specialized in solving quadratic unconstrained binary optimization (QUBO) problems, and many well-known problems can be translated into [2]. In the ideal procedure, quantum annealing outputs the optimal solution by slowly decreasing the strength of the fluctuation. The quantum adiabatic theorem ensures that the ground state, which corresponds to the optimal solution, is obtained by evolving the system adiabatically [3-5]. The hardware implementing"}, {"title": "Methods", "content": "This section introduces NMF and NBMF and describes our method to integrate RA with linear programming relaxation."}, {"title": "Nonnegative Matrix Factorization", "content": "Nonnegative matrix factorization (NMF) is a method to factorize a given matrix V\u2208R+m\u00d7n_into two nonnegative matrices W\u2208R+m\u00d7k_and H\u2208R+k\u00d7n, where k is the rank of the factorization and R+ denotes the set of nonnegative real numbers. The matrix W represents the basis matrix and contains the potential features of the original matrix, while the matrix H represents the coefficient matrix and contains the contribution of each basis to the original matrix. In NMF, W and H are approximated by minimizing the Frobenius norm of the difference between the original matrix V and the product of W and H as follows:\n\nmin ||V - WH||\nW,H\nsubject to W\u2208R+mxk\nH\u2208R+kxn\n\n(1)\nwhere || ||F denotes the Frobenius norm. The problem is generally a non-convex and NP-hard problem [32]. This complexity means that finding an exact solution is computationally hard for large matrices, as the solution space is vast and contains multiple local minima. To tackle this challenge, NMF is typically solved using iterative algorithms that approximate a solution rather than solving it exactly. Among these methods, popular choices include alternating least squares (ALS) and multiplicative update rules [27,33].\nIn the ALS algorithm, W and H are approximated by iteratively updating W and H to decrease the squared error as depicted in Algorithm 1. In ALS, it is important to solve each subproblem efficiently. The method to solve the projected gradient descent (PGD) method is known to be effective for the nonnegative constraint [31]. In addition, PGD is commonly used for constrained optimization problems, where the solution is"}, {"title": "Nonnegative/Binary Matrix Factorization", "content": "Nonnegative/binary matrix factorization (NBMF) is a variant of NMF that constrains the coefficient matrix H to be binary and expressed as follows:\n\nmin ||V - WH||\nW,H\nsubject to W\u2208R+mxk\n\u0397 \u2208 {0,1}kxn.\n\n(6)\nTo approximate the solution of NBMF, we can similarly employ the ALS algorithm to iteratively update W and H as depicted in Algorithm 2. The PGD method, described in the previous section, can solve the optimization of W in each iteration. The optimization of H under the binary constraint becomes hard as the search space is discrete. In practice, the update in the H step is decomposed into n independent binary optimization problems as:\n\nH+1 := arg min ||V; \u2013 Wt+1h||2,\nh\u2208 {0,1}k\n\n(7)"}, {"title": "Quantum Annealing", "content": "Quantum annealing is a meta-heuristic inspired by quantum physics that leverages quantum fluctuations to escape local minima and find the global minimum of a given objective function. Quantum annealing is specialized in solving quadratic unconstrained binary optimization (QUBO) problems, which can be expressed as:\n\nminimize\nxQx\nsubject to x \u2208 {0,1},\n\n(8)\nwhere Q is a coefficient matrix, x represents a vector of a binary variable, and N is the number of variables. QUBO problems can be equivalently transformed into the problem of minimizing the energy of the Ising model, for which the Hamiltonian is expressed as:\n\n\u0397\u03bf(\u03c3) = \u2013 \u03a3 \u0399\u03af\u03c3\u03af\u03c3; \u2013 \u03a3\u03af\u03c3\u03b9,\ni,j\ni\n(9)\nwhere \u03c3\u2081 is an Ising spin variable that takes either 1 or -1 and Jij and hi are the coupling constant between Ising spins and the bias term, respectively.\nTo utilize quantum annealing, the objective function is encoded into an Ising Hamiltonian of a quantum system, and the ground state of the Hamiltonian corresponds to the optimal solution of the objective function. For efficient exploration of low-energy states, the quantum annealing uses quantum fluctuations to escape local minima. The quantum fluctuations are controlled by the transverse field, and the Hamiltonian of the quantum system is expressed as:\n\nH(s) = -A (s(t)) \u2211 + B (s(t)) Ho,\n\n(10)\nwhere represent the x-component of the Pauli matrices. The term Ho is the problem Hamiltonian, which encodes the Ising problem and is obtained by replacing the Ising spin variables with the z-component of the Pauli matrices, \u00f4i. The functions A (s(t)) and B (s(t)) are defined such that A(0) \u226b B(0) and A(1) \u226a B(1). The parameter 0 \u2264 s(t) \u2264 1 is used to control the time evolution of the quantum system, which is"}, {"title": "Reverse Annealing", "content": "RA is a technique focused on local search [20, 21]. When t = 0, a classical initial state is prepared with s(0) = 1, setting the transverse field term to zero. Then, the strength of the transverse field term gradually increases until it reaches a predetermined switching points(tinv) = Starget. We refer to the reversed switching point 1 - Starget as the reversal distance. Afterward, as in the standard quantum annealing (i.e., FA), the transverse field is weakened, allowing the system to find a lower-energy state close to the initial state. In this way, RA can refine the initial states and possibly find a better solution than FA alone.\nOne of the known limitations of D-Wave machines is that external influences can prevent ideal quantum annealing execution, resulting in suboptimal solutions with less-than-ideal probabilities. However, since D-Wave machines can output relatively low-energy solutions within microseconds, they are positioned similarly to heuristic methods like simulated annealing [37]. D-Wave machines offer a \"pause\" feature, which halts the annealing parameter at a certain magnitude s = s* over an extended period. During RA on D-Wave machines, the pause function can be used at the switching point s(tinv) = Starget to fix the transverse field, enabling solution search through thermal bath relaxation, a widely adopted technique.\nTo calibrate the annealing schedule, the annealing time 7 and the switching point Starget must be adjusted as they are dominant factors in the performance of RA. Especially, the annealing distance defined by the switching point is crucial for the performance of RA as it controls the strength of the quantum fluctuations and how far the system can traverse from the initial state. If we set the reversal distance to high, the system will completely forget the initial state and run as the FA equivalently. On the other hand, if the reversal distance is too short, the system will not have enough fluctuation to escape from the initial state, and the performance of RA will be degraded. The preceding study calibrated the reversal distance by evaluating the probabilities to escape from the initial state and reach a better solution [24]. The length of pausing"}, {"title": "Relaxation-Assisted Reverse Annealing", "content": "In RA, the performance is highly dependent on the initial states. In the preceding study, the states in each iteration in the ALS algorithm were utilized as the initial configuration for RA. This study proposes an improved strategy integrating RA with a classical linear programming relaxation technique. We apply the relaxation to the problem (7) and obtain the following relaxed problem:\n\nH+1 := arg min ||V; \u2013 Wt+1h||2,\nh\u2208 [0,1]k\n\n(12)\nwhere [0, 1] denotes the set of real numbers between 0 and 1. The relaxed problem (12) can be solved by the PGD method with brief modification. We only need to change the projection function (4) in the PGD method and simply set the upper bound parameter u as a vector of ones.\nThe relaxed solutions can be obtained by the PGD method and we use them for initial configuration for RA. As the relaxed solution is continuous, we need to map the relaxed solution to binary. There are several methods to map the continuous solution to binary, such as rounding or random sampling. In this study, we use the simple rounding method to map the relaxed solution to binary by the following rule:\n\nhi\n=\n{\n1 if hi\u2265 0.5\n0 otherwise.\n\n(13)\nAlthough the rounding method does not always yield the optimal solution, it approximates the original problem well. This rounded solution can then serve as the initial configuration for RA, which is expected to further refine the approximation, guiding it towards an optimal or near-optimal solution."}, {"title": "Results", "content": ""}, {"title": "Practical Performance Evaluation on Facial Image Datasets", "content": "We evaluate the performance of the proposed method on facial image datasets, which is used in the literature for NMF and NBMF [24,26,28]. We targeted 200 images, each with a resolution of 19 pixels, from the publicly available MIT face image dataset [38].\nTo convert the image dataset to a matrix V, we vectorized each image and stacked them into a matrix V\u2208 R+361\u00d7200. We set the rank of the factorization to k = 35 and initialized the basis matrix W and the coefficient matrix H randomly. We applied the ALS algorithm to factorize the matrix V into W and H. The optimization of W was solved by the PGD method, and the optimization of H was solved by the proposed relaxation-assisted RA method. To evaluate the performance of the proposed method, we compared the different methods used in the literature as shown in Table 1.\nAs devices to perform FA and RA, we used the D-Wave 2000Q, a machine publicly available from D-Wave. For the annealing schedule and execution frequency, we used previously demonstrated effective parameters in the preceding research [24]. The annealing time for FA was set to 20 microseconds. We implemented a total annealing time of 30 microseconds for RA, including a 10-microsecond pause. The number of executions for FA and RA were set to 1,000 and 240, respectively. This setting was determined to nearly equalize the total execution time of FA and RA. The problem"}, {"title": "Estimation Accuracy of Relaxation-Based Initialization", "content": "In this subsection, we assess the estimation performance of the relaxation-based initialization method. In facial image datasets, the relaxation-based initialization method was found to yield a better initial state for RA, offering closer proximity to the optimal solution. As shown in Fig. 4, the elements of the relaxed solution are concentrated around 0 and 1, a property that we believe facilitates a good approximation of the original problem. To identify the limitations of the relaxation-based initialization method, we examine how its performance generalizes to other NBMF problems. Specifically, we prepare randomly generated datasets and evaluate the performance in terms of the proximity depending on the rank of the"}, {"title": "Discussion", "content": "In this study, we proposed a hybrid approach combining relaxation-based initialization and RA in NBMF. For the relaxation, by extending the PGD method used in traditional NMF, we demonstrated that highly accurate estimated solutions could be obtained for original problems. The proposed method was evaluated on facial image datasets, and the results showed that relaxation-assisted RA outperformed conventional"}]}