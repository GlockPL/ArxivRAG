{"title": "Knowledge Generation for Zero-shot Knowledge-based VQA", "authors": ["Rui Cao", "Jing Jiang"], "abstract": "Previous solutions to knowledge-based visual question answering (K-VQA) retrieve knowledge from external knowledge bases and use supervised learning to train the K-VQA model. Recently pre-trained LLMs have been used as both a knowledge source and a zero-shot QA model for K-VQA and demonstrated promising results. However, these recent methods do not explicitly show the knowledge needed to answer the questions and thus lack interpretability. Inspired by recent work on knowledge generation from LLMs for text-based QA, in this work we propose and test a similar knowledge-generation-based K-VQA method, which first generates knowledge from an LLM and then incorporates the generated knowledge for K-VQA in a zero-shot manner. We evaluate our method on two K-VQA benchmarks and found that our method performs better than previous zero-shot K-VQA methods and our generated knowledge is generally relevant and helpful.", "sections": [{"title": "Introduction", "content": "Knowledge-based VQA (which we refer to as K-VQA in this paper) is a special visual question answering (VQA) task where, in addition to an image, external knowledge is needed to answer the given question. For instance, to answer the question in Figure 1, background knowledge about national parks in California is needed.\nEarly methods for K-VQA follow a retrieve and answer paradigm (Figure 1(a)), which first retrieves knowledge from external knowledge sources as additional input and then trains a VQA model through supervised learning (Wang et al., 2018; Narasimhan and Schwing, 2018; Narasimhan et al., 2018; Li et al., 2020). This paradigm requires both a suitable external knowledge base and a large amount of K-VQA training data, which may not be practical for real applications when either of these resources is not available. Recently, with the fast"}, {"title": "Related Work", "content": "K-VQA. Early K-VQA models were built through standard supervised training, with a large amount of (Image, Question, Answer) triplets as training data (Wang et al., 2018; Narasimhan and Schwing, 2018; Narasimhan et al., 2018; Li et al., 2020). Typically, these models retrieve knowledge from an external knowledge source such as ConceptNet or Wikipedia and use the retrieved knowledge to facilitate QA. In our work, we also use explicit knowledge to facilitate QA, but the knowledge is generated from an LLM instead.\nZero-shot K-VQA. Several recent studies utilized LLMs for zero-shot K-VQA (Yang et al., 2022; Hu et al., 2022; Guo et al., 2022; Li et al., 2023a; Alayrac et al., 2022). Generally, these methods first convert the given image into captions or embeddings compatible with a pre-trained language model. Then the captions or embeddings are combined with the question as input to the language model for zero-shot QA. We can categorize these methods into two types: those that need extra training using labeled data other than K-VQA data, and those that directly leverage existing pre-trained models without any further training or fine-tuning. Examples of the former category include Frozen (Tsimpoukelli et al., 2021) (which uses image-text pairs to train a projection module) and BLIP-2 (Li et al., 2023a) (which learns a Q-transformer module to model multimodal interactions). Examples of the latter category include PICa (Yang et al., 2022) and PNP-VQA (Tiong et al., 2022), which convert the images into captions with an off-the-shelf caption generator. However, to the best of our knowledge, none of the existing zero-shot K-VQA methods explicitly state the external knowledge used to answer the questions.\nKnowledge generation for QA. A few recent studies on text-based QA tested the idea of using LLMs to generate either short knowledge statements or long documents before combining them with the questions for zero-shot commonsense QA or open-domain QA (Liu et al., 2022; Sun et al., 2022; Yu et al., 2023). They found that by incorporating the generated knowledge in QA, performance can be significantly improved. Our work is inspired by these recent studies but we apply the idea to visual QA."}, {"title": "Method", "content": "The high-level idea of our KGENVQA method is to leverage an LLM to generate explicit knowledge statements given an image and a question. These knowledge statements can then be combined with"}, {"title": "Knowledge Generation", "content": "Our knowledge generation process consists of two steps: An initial knowledge generation step, in which we generate a single knowledge statement for each (image, question) pair in the K-VQA test dataset, and a subsequent self-supervised knowledge diversification step, in which we sample a diverse set of knowledge statements generated during the first step as in-context demonstrations to perform a second round of knowledge generation, in which we generate multiple knowledge statements per (image, question) pair. The motivation is that with a diverse set of in-context demonstrations, we expect the LLM to also generate knowledge statements covering different aspects of the same (image, question) pair, which may increase the chance of getting the correct answer.\nCaption generation. In both knowledge generation steps, we regard an LLM (GPT-3 in our experiments) as a knowledge base because the LLM has been trained on a large amount of text covering a wide range of topics. Previous work has shown that relevant knowledge statements can be generated from an LLM if appropriate text prompts including both the contexts and some demonstrations are used (Liu et al., 2022). However, different from text-based QA, for K-VQA, the context is an image, which cannot be directly used as input to an LLM. To address this issue, we adopt a simple solution that converts the image into one or more captions, using an off-the-shelf image captioning model. However, instead of using a general-purpose captioning model, we believe that question-aware captions, which focus on describing the parts of the image that are more relevant to the question, can provide better contexts for knowledge generation. Therefore, we adopt the question-aware caption generation mechanism by Tiong et al. (2022), which first highlights image regions that are more relevant to the question and then generates question-aware captions with the attention-weighted image. Following the practice of Tiong et al. (2022), we use multiple captions because this practice has been shown to be useful for subsequent question answering. We concatenate the multiple captions into a single sequence of tokens, which we denote as $C$.\nPrompt template for knowledge generation. In both the initial knowledge generation step and the knowledge diversification step, to generate a single piece of knowledge, we use the following prompt template: Please generate related background knowledge to the question; Context: [C]; Question: [Q]; Knowledge:. The LLM will complete the prompt above by generating a sentence, which we treat as a knowledge statement. In order to better generate the relevant knowledge, we leverage in-context learning by including a few demonstrations, i.e., a few examples each containing a context (which are also image captions), a question, and the expected knowledge statement to be generated. During the initial knowledge generation step and the knowledge diversification step, we use different kinds of demonstrations.\nInitial knowledge generation. During the initial knowledge generation step, we use six manually crafted in-context demonstrations for knowledge generation. They can be found in Appendix H. During this step, we generate a single knowledge statement for each (image, question) pair in a K-VQA test dataset.\nSelf-supervised knowledge diversification. Previous work showed that proper selection of demonstrations is of vital importance when prompting LLMs (Yang et al., 2022; Gonen et al., 2022). We suspect that the manually crafted demonstrations may not always be proper examples for all test instances. Besides, when answering knowledge-intensive questions, oftentimes more than one piece of knowledge may be needed. For instance, to answer the question in Figure 2, the knowledge 1) what national parks are in California; 2) among national parks in California, which is famous for black bears. To generate multiple knowledge statements per question, a straightforward solution is to ask the LLM to return multiple pieces of knowledge. However, beam search sampling, as mentioned in (Holtzman et al., 2020; Vijayakumar et al., 2018), tends to generate dull and repetitive outputs, and the improved top-k sampling (Fan et al., 2018) can only solve the issue to some extent. On the other hand, with different prompts, an LLM may generate diverse outputs (Li et al., 2023b).\nTherefore, we adopt a self-supervised knowl-"}, {"title": "Knowledge Integration for K-VQA", "content": "With the final set of $T$ knowledge statements generated for each (image, question) pair, we can combine them with the image captions and the question, and pass them to a pre-trained text-based QA model for answer generation. In our experiments, we use UnifiedQA (Khashabi et al., 2020), OPT (Zhang et al., 2022) and GPT-3 (Brown et al., 2020)."}, {"title": "Experiments", "content": "To validate our proposed method, we choose two commonly used K-VQA benchmark datasets, namely, OK-VQA (Marino et al., 2019) and A-OKVQA (Schwenk et al., 2022). Questions in OK-VQA need outside knowledge beyond the images to answer. A-OKVQA is an augmented version of OK-VQA that requires additional types of world knowledge. Because the ground-truth answers of the test-split of A-OKVQA are not available, we use its val-split for evaluation. In the end, the OK-VQA and A-OKVQA datasets we use contain 5, 046 and 1, 100 questions, respectively. We report the soft accuracy (Goyal et al., 2017) on both datasets as there are multiple ground-truth answers for a question. Due to the limit of space, implementation details are provided in Appendix B."}, {"title": "Zero-shot Methods for Comparison", "content": "In this work, we focus on zero-shot K-VQA. There are models that need extra training (with labeled data other than K-VQA data). There are also some few-shot K-VQA methods where the few shots are dynamically selected from a large pool of training examples, which means they still need much training data. For fair comparison, we do not include these methods because they are not strictly zero-shot.\nBelow we briefly review three existing zero-shot K-VQA methods that we compare with:\nPICa (Yang et al., 2022) converts images into captions with an off-the-shelf caption generator, CLIP-Cap (Mokady et al., 2021). The captions are regarded as contexts and fed to GPT-3 together with the question for answer prediction.\nPNP-VQA (Tiong et al., 2022) uses improved caption generation by exploiting an image-text matching model (Li et al., 2022) to highlight image regions related to the question. The attended images are then used for caption generation with BLIP (Li et al., 2022) so that the captions are question-aware. We adopt the same caption generation method in PNP-VQA in our method. PNP-VQA uses UnifiedQA (Khashabi et al., 2020), a pre-trained question answering model, in a fusion-in-decoder (FiD) manner (Izacard and Grave, 2021), for final answer prediction.\nImg2LLM (Guo et al., 2022) follows the caption generation process in PNP-VQA. Based on the captions, it generates synthetic QA pairs as demonstrating examples when prompting the LLM for final answers. OPT (Zhang et al., 2022) is used as the LLM for QA."}, {"title": "Main Results", "content": "In this section, we empirically evaluate our generate and answer approach in two ways: (1) We test the usefulness of the generated knowledge for K-VQA by systematically comparing our K-VQA system with and without knowledge generation. (2) We compare our generate and answer method with SOTA zero-shot K-VQA baselines, which do not explicitly generate knowledge.\nThe effect of knowledge generation. We first conduct systematic experiments to compare the generate and answer approach and the directly answer approach based on our own implementation. To see whether knowledge generation can consistently help K-VQA, we experiment with three dif-"}, {"title": "Ablation Studies", "content": "Knowledge generation method. We first compare our cluster-based knowledge diversification strategy with (1) using the manual prompt generated knowledge, i.e., a single piece of knowledge (Manual); (2) randomly sampling $K - 1$ single knowledge statement, instead of sampling from different clusters, from the initially generated knowledge statements, $K_{init}$ for knowledge diversification in the second stage (Random). Besides, we consider the idea of Chain-of-Thoughts (CoT) (Wei et al., 2022), which generates explanations before the answer generation. In K-VQA, the needed knowledge can also be regarded as a kind of explanations. Therefore, we test the widely used CoT for knowledge generation, which is an alternative to our cluster-based knowledge generation approach. We re-use the six manual demonstrations as mentioned in Section 3 and manually add answers to the questions (i.e., each demonstration consists of contexts of image descriptions, a question, a piece of related knowledge and an answer). Together with these demonstrations, we prompt GPT-3 (Brown et al., 2020) to first generate the relevant knowledge and then the answer (CoT). Due to the cost of calling GPT APIs, we only apply CoT to a subset questions on OK-VQA (200 questions). We show model performance, based on UnifiedQA3B, with different ways of knowledge generation and show results in Table 5. We have a few observations: (1) using initial generated knowledge with demonstrations offers improvements but no better than KGen. This may be that fixed manual demonstra-"}, {"title": "Evaluation of the Generated Knowledge", "content": "In this section, we conduct human evaluation to exam the quality of the generated knowledge. We follow Liu et al. (2022) and sample 40 cases from OK-VQA dataset where the correctness of the answers would be changed (i.e., either from correct to wrong or wrong to correct) after adding the generated knowledge. For each instance, we sample 5 knowledge statements for evaluation. We ask two annotators to check the quality of the generated knowledge in terms of the evaluation metrics below. To ensure objectiveness, annotators will not know whether the predictions are changed to become correct or wrong.\nEvaluation metrics. Following Liu et al. (2022); Shwartz et al. (2020), we take four metrics for evaluating generated knowledge: 1) Grammatically: whether it is grammatical 2) Relevance: whether it is related to answering the question and the image; 3) Factuality: whether it is factual; 4) Helpfulness: whether it is helpful so that it directly leads to the correct answers or provides indirect but supportive information of the correct answers. For helpfulness, we adopt three categories of evaluation: helpful (i.e., provides direct or indirect supportive information to correct answers), harmful (i.e., negates correct answers or support incorrect answers) or neutral (neither helpful or harmful). Besides the previously used metrics, we also consider Diversity as the fifth evaluation criteria, indicating the coverage of generated knowledge. Details about the definitions can be found in Appendix I and the examples we provide to annotators regarding the four evaluation metrics are included in the supplementary materials."}, {"title": "Case Study", "content": "To better understand the advantage of our method, we compare our method with the baseline, UnifiedQA3B (FiD), without generated knowledge. We analyze the first 20 cases, without cherry picking, where our method answers correctly while the baseline gives wrong predictions. Among the 20 error cases of the baseline, 85% are due to the lack of external knowledge, highlighting the advantage of our method. Due to the limitation of space, we provide the examples in Appendix G.\nBesides, we conduct error analysis to better understand the limitations of our method. We conduct an empirical analysis for the error cases by manual checking 40 error cases from UnifiedQA3B (FiD) after adding generated knowledge. Among all error cases, we observe 20% are due to the undesired knowledge. Due to limitation of space, we provide visualization of the error cases in Appendix 4.6. The main cause of generating misleading knowl-"}, {"title": "Conclusions", "content": "In this work, we propose to generate relevant knowledge from LLMs for zero-shot K-VQA. We evaluate the effectiveness of the generated knowledge by experimenting with different pre-trained QA models of varying model sizes on two K-VQA benchmarks. The experiment results show that the generated knowledge improves K-VQA performance, and our method can outperform SOTA zero-shot K-VQA methods. We further conduct human evaluation to validate the quality of the generated knowledge. The results demonstrate that the generated knowledge statements are relevant and helpful to questions in K-VQA."}, {"title": "Limitations", "content": "In this paper, we adopt GPT-3.5 as the LLM to generate several pieces of knowledge for one question. However, the generated knowledge may be redundant in some cases, which introduces noise to the final answer prediction process. Therefore, in the future, we need to investigate how to filter out redundant knowledge. Besides, in this work we only consider inserting the generated knowledge into a text-QA model when converting K-VQA into a text-based QA problem. A future direction is to design and insert generated knowledge into pre-trained vision-language models (PT-VLMs) (e.g., BLIP-2 (Li et al., 2023a)), because the conversion from images to texts may leave out crucial details, but PT-VLMs can take images as inputs without losing any potentially important visual information from the images."}]}