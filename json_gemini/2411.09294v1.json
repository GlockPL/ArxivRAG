{"title": "Learning Hand State Estimation for a Light Exoskeleton", "authors": ["Gabriele Abbate", "Alessandro Giusti", "Luca Randazzo", "Antonio Paolillo"], "abstract": "We propose a machine learning-based estimator of the hand\nstate for rehabilitation purposes, using light exoskeletons. These devices\nare easy to use and useful for delivering domestic and frequent therapies.\nWe build a supervised approach using information from the muscular\nactivity of the forearm and the motion of the exoskeleton to reconstruct\nthe hand's opening degree and compliance level. Such information can\nbe used to evaluate the therapy progress and develop adaptive control\nbehaviors. Our approach is validated with a real light exoskeleton. The\nexperiments demonstrate good predictive performance of our approach\nwhen trained on data coming from a single user and tested on the same\nuser, even across different sessions. This generalization capability makes\nour system promising for practical use in real rehabilitation.", "sections": [{"title": "Introduction", "content": "Stroke is one of the main causes of disability [6], resulting in severe hand function-\nalities limitation [13], or long-lasting hand impairments [14]. Given the impor-\ntance of manual operations in everyday life, rehabilitation procedures are given\na crucial role [10, 24]. Cutting-edge technology, like Virtual Reality (VR) [9, 16]\nand robotics [2], can assist standard rehabilitation to achieve better outcomes.\nIn this context, it is expected that Artificial Intelligence (AI) can bring great\nbenefits, especially in solving perception challenges that are otherwise difficult to\ntackle with standard devices. Indeed, Machine Learning (ML), and AI in general,\nhave been proven to be effective in tackling complex perception tasks in robotics,\ne.g. for complex localization problems [22, 23] or human-robot interaction pur-\nposes [1, 4]. Al finds application also in the domain of medical robotics [30].\nHand exoskeletons are very useful in rehabilitation (see e.g. [11, 27]). Equip-\nping them with an onboard and light perception module would enable even better\ntherapy outcomes for several reasons. First, a perception system that does not\nrequire complex infrastructure or additional heavy devices confers versatility and\nlightness on the exoskeleton. These aspects are particularly important for favor-\ning ease of use, and domestic and frequent utilization, a key factor for a successful\ntherapy [7, 12]. Secondly, an online and robust perception of the patients' state\nwould allow adaptive closed-loop exoskeleton control with a positive impact on\nthe therapy, as tuning the therapy to the patient's state positively affects mo-\ntor learning [21, 31]. Finally, and very importantly, a perception system would\npermit the measure of the patients' sensorimotor deficit, crucial for delivering\noptimal rehabilitation [15]. At the moment, the therapy verification is manually\nperformed by clinicians and suffer from low reliability and standardization, often\naffected by the therapist's perception of the patient's abilities [8].\nRobotics could offer accurate and objective assessments of function and im-\npairment [15, 19]. For example, robotic devices are deployed to evaluate propri-\noception and haptic perception [21], and finger Range of Motion (ROM) [26, 29].\nFingers' RoM are predictors of rehabilitation outcomes [20] and could be mea-\nsured using hand trackers. These systems need external infrastructures, be it\ncameras [18] or other specialized hardware, such as gloves [3]. However, vision-\nbased approaches have limited tracking area, as they are constrained by the\ncamera field of view, and are susceptible to external disturbances such as light\nchanging and occlusion. While gloves are robust in that sense, they may not\nbe suitable to be used together with exoskeletons. Also, an under-actuated ex-"}, {"title": "Problem formulation and proposed approach", "content": "We address the problem of building a perception module for light hand exoskele-\ntons with little sensory equipment. The aim is to reconstruct the state of the\npatient's hand, who wears a hand exoskeleton and an EMG sensor, in a rehabil-\nitation setup. In detail, we tackle the challenge of estimating the opening degree\nof the patient's hand and its level of compliance. The first can be used to esti-\nmate the patient's RoM, for the evaluation of the therapy progress; the second\ncan be used to adapt the action of the exoskeleton to the patient's current state.\nWe denote the opening degree with a continuous variable yo\u2208 [0, \u03c0/2], where\nyo = 0 means 'open hand' and yo = \u03c0/2 is for 'closed'. The hand's compliance\nlevel is defined with the dimensionless variable yc \u2208 [-1,1] ranging from yc = -1\n(for stiff hand) to yc = 1 (compliant hand), passing through yc = 0 (neutral).\nThe quantities yo and yc compose the target variable y to estimate:\ny = (Yo, Yc).\nWe pursue our perception objective without resorting to any external hand-\ntracking systems. Instead, we use the information coming from the motion of the\nexoskeleton (such as the position of its motors) collected in a feature set called\nfexo, and the corresponding muscular activity of the patient detected through the\nEMG sensor collected in another feature set, femg. Individually, these features"}, {"title": "Experimental setup", "content": "We use the hand exoskeleton built by Emovo Care\u00b3 for specific rehabilitation\npurposes. The mechanical structure of this exoskeleton consists of two soft, flexi-\nble tendons that are worn coupling the index with the middle finger, and the ring\nfinger with the pinkie (see Fig. 2). A single motor actuates both tendons simul-\ntaneously, assisting simple opening and closing hand motions. Such a light and\nsimple structure is easy to use for both patients and therapists. In particular, the\nsoft tendons allow passive compliance, so that the exoskeleton can simply adapt\nto the patient's hand posture. The exoskeleton is also simple in terms of sensory\nequipment: an encoder streams the motor position at 20 Hz; the current applied\nto the motor is also read at the same frequency. These measurements compose\nthe vector fexo defined in Sec. 2. The device is also equipped with Wi-Fi and\nserial USB communication interfaces to send commands to the motor and read\nits state. More details are in the description of an early device prototype [25].\nTo measure the muscular activity of the hand, we use the Myo armband, an\n8-channel dry surface EMG sensor widely used in combination with arm prosthe-\nses [28]. The device (visible in Fig. 2) consists of 8 pads that, in contact with the\nmuscles of the forearm, detect the activity of the user's hand opening motion.\nThe Myo system has a marker on one of the pads, allowing different users to\nconsistently wear the sensor at the same position (i.e. with the marker facing up"}, {"title": "Data collection", "content": "For collecting data, 5 users (3 M, 2 F, ages 25-64) are asked to wear the Myo\nsensor and the exoskeleton on their left forearm and hand, respectively (Fig. 2).\nParticipants gave their informed consent to participate and the procedure was\napproved by the local ethics committee of the University of Applied Sciences\nand Arts of Southern Switzerland.\nTo label our data, during the acquisition, we use an external hand-tracking\nsystem. We opt for the Meta Quest 3 Headset\u00b9 as its built-in capabilities allow\nprecise and high frequency (90 Hz) tracking of hand joints. We define the hand's\nopening degree as the average values of the pitch angles of each finger joint,\nexcluding the thumb as it is not actuated by the exoskeleton. This quantity,\nas provided by the hand tracker, represents the ground truth of the target yo\ndefined in Sec. 2. Also, we leverage the augmented reality features of the headset"}, {"title": "Model architectures and evaluation metrics", "content": "Data from the three devices (EMG sensor, exoskeleton, and hand tracker) are ac-\nquired in real time, stored, and synchronized on an external laptop. Such data,\ncollected together in the dataset D, are then used to train and evaluate our\nperception model. We compare different architectures: Linear Regressor (LR),\nMulti-Layer Perceptron (MLP), Support Vector Regression (SVR), and a state-\nful Long Short-Term Memory (LSTM)-based regressor. The stateless models\nare implemented with scikit-learn library using default parameters, except for\nthe MLP which consists of 2 hidden layers with 100 neurons each. The stateful\nLSTM-based model, instead, is implemented in PyTorch and is composed of\n2 LSTM cells with a 32-dimensional hidden state each. Both MLP and LSTM\nmodels have approximately 12000 trainable parameters. As a baseline, we ad-\nditionally report a Dummy regression model that always returns the average of\nthe ground truth from the training data.\nThe different models are evaluated using the Root Mean Squared Error\n(RMSE) and the coefficient of determination, denoted with R\u00b2. The former mea-\nsures the average difference between model prediction and ground truth values;\nan ideal regression provides RMSE equal to 0. The latter is a dimensionless value\nmeasuring the amount of variance in the target variable that is explained by the\nmodel. An ideal regression model yields R2 = 1, while a naive regression that\nestimates the mean of the target variable yields R2 = 0."}, {"title": "Prediction performance", "content": "We evaluate the prediction performance of the different regression architectures\n(introduced in Sec. 3.3) and the impact of different feature sets. To this extent,\nwe train each architecture with the full feature set defined in Sec. 2 (i.e., f) and\nwith the individual feature sets coming from the exoskeleton and the Myo sensor\n(fexo and femg). This comparison serves to confirm our claim that data of the\nexoskeleton and the EMG alone are not informative enough; instead, fusing them\nis beneficial. We also show that our approach is capable of learning to estimate"}, {"title": "Generalization capabilities", "content": "Generalization across different sessions We verify how well a model trained\non one user during a session, performs on data from the same user collected in a\ndifferent session. On a different day, users have to wear and position the sensors\nagain, potentially affecting the system's performance. Figs. 4-6 and Fig. 8 report\nexamples of this scenario. They demonstrate that models trained with just 9"}, {"title": "Online experiments", "content": "We execute an online experiment where we train an LSTM regressor based on f\nfeatures with data coming from a single user. The data is collected as explained\nin Sec. 3.2. Then, on a different day, the same user is asked to perform again the\ntask as during the data collection, but with a difference: every 30 s, as suggested\nby an audio clue, they are asked to switch to a different acquisition modality.\nIn this way, we record a 180 s long sequence during which the user loops back\nand forth between Helping, Passive, and Opposing acquisition modality. At the\nsame time, the model previously trained is deployed in real-time to predict both\ntarget variables. Figure 8 shows the prediction compared to the ground truth\nvalues. It qualitatively displays good performance, further confirmed by the R2\nand RMSE values (0.81 and 0.12 for yo, 0.55 and 0.45 for yc). The experiment"}, {"title": "Conclusions", "content": "We have presented a machine learning-based approach to estimate the hand\nstate of a user wearing a light exoskeleton for rehabilitation purposes. More in\ndetail, our objective is to predict the opening degree of the hand and its level\nof compliance. The former is an indicator of the user's RoM; the latter is a\nmeasure of the user's active participation in the therapy. Both are useful to\nobjectively asses a therapy outcome and continuously adapt it to the patient's\nstate. We have demonstrated that using the exoskeleton alone is not enough to\nachieve our goal. Indeed, the soft nature of the device introduces slacks between\nthe hand joints and the exoskeleton fingers, making the data coming from its\nmotor poorly informative of the actual hand state, in particular for low hand\ncompliance. We proposed to add an EMG sensor to the setup, which can enrich\nthe simple perception capabilities of the exoskeleton with more insightful sensing.\nOur experiments show that fusing the two data sources allows an LSTM-based\nregression to obtain a strong prediction performance even when we train with\nlittle data on a single user, generalizing well across different sessions of the same\nuser. Such capability makes the approach suitable for use in a real-world scenario,\neven if the model does not generalize well across different users. In fact, a patient\ncould easily and quickly calibrate the system at their very first use, using a hand-\ntracking device with the help of a therapist. Then, they could keep up with the\ntherapy on their own, thanks to the portable nature of the proposed system.\nThe main limitation of the current work is that experimentation is done with\nhealthy users. Our promising results suggest that the estimation capabilities of\nour system can potentially translate to patients with hand injuries; however, tests\nwith actual patients are needed. Future work will be devoted to the assessment of\nthe tool's usability and acceptability with stroke patients, which strongly impact\nthe success of the therapy. We have asked our participants to simulate different\nwell distinguishable levels of hand compliance. However, monitoring the hand\nbehavior of actual patients may be more difficult. Definitely, a study involving\npatients affected by hand injuries is a clear path for future research."}]}