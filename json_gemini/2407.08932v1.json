{"title": "Deep Attention Driven Reinforcement Learning (DAD-RL) for Autonomous Vehicle Decision-Making in Dynamic Environment", "authors": ["Jayabrata Chowdhury", "Venkataramanan Shivaraman", "Sumit Dangi", "Suresh Sundaram", "P B Sujit"], "abstract": "Autonomous Vehicle (AV) decision-making in urban environments is inherently challenging due to the dynamic interactions with surrounding vehicles. For safe planning, AV/ego must understand the weightage of various spatiotemporal interactions in a scene. Contemporary works use colossal transformer architectures to encode interactions mainly for trajectory prediction, resulting in increased computational complexity. To address this issue without compromising spatiotemporal understanding and performance, we propose the simple Deep Attention Driven Reinforcement Learning (DAD-RL) framework, which dynamically assigns and incorporates the significance of surrounding vehicles into the ego's RL-driven decision-making process. We introduce an AV-centric spatiotemporal attention encoding (STAE) mechanism for learning the dynamic interactions with different surrounding vehicles. To understand map and route context, we employ a context encoder to extract features from context maps. The spatiotemporal representations combined with contextual encoding provide a comprehensive state representation. The resulting model is trained using the Soft-Actor Critic (SAC) algorithm. We evaluate the proposed framework on the SMARTS urban benchmarking scenarios without traffic signals to demonstrate that DAD-RL outperforms recent state-of-the-art methods. Furthermore, an ablation study underscores the importance of the context-encoder and spatio-temporal attention encoder in achieving superior performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Navigating safely in a dynamic environment populated with other vehicles remains a significant hurdle for Autonomous Vehicles (AVs). Decisions made by the AV should not only be safe but also comply with human driving behavior. Fig. 1 illustrates a left-turn scenario without a traffic signal, emphasizing the AV's need to comprehend other road users' actions and decide the attention importance to ensure safe navigation. Previous methods have explored rule-based methods [1]. Rule-based methods excel in scenarios the rules were defined for but falter in new ones. An alternative approach [2], [3] involves explicit communication between the AV and other vehicles, enabling the AV to make informed decisions in collaboration with other vehicles. However, this method is limited because reliable communication channels can only be assured between vehicles from the same manufacturer. For effective decision-making, the AV must comprehend the dynamic driving context as it evolves implicitly. In a dynamic driving environment, AV should understand the temporal behaviors of other surrounding vehicles and learn to make safe decisions. Also, these behaviors can be influenced by spatial structures such as road geometry.\nImitation Learning approaches involve learning from an expert's actions. Several recent studies ([4], [5], [6], [7]) have utilized Imitation Learning (IL) methods to develop decision-making abilities that mimic an expert driver. However, expert bias and distribution shift challenges can considerably affect the efficacy of these IL-based approaches. Given the absence of near-collision situations in expert driving, it is difficult for IL-based techniques to recover from such scenarios. Recent advances in Reinforcement Learning (RL) based decision-making algorithms [8] show promising performance. RL's strength stems from its exploration capabilities; it can recover from near-collision scenarios better. However, these methods need an understanding of the state representations for RL. To better understand the socially related spatio-temporal interactive behaviors between AV and other vehicles, a method must be developed to encode spatio-temporal relationships and provide information for safe decision-making.\nIn the context of an AV navigating a roadway, the safety relevance of other vehicles varies. The work in [9] has identified essential vehicles using rule-based expertise. However, such expert knowledge can be intricate and may only sometimes scale to unfamiliar driving situations. In a dynamic real-world setting, the significance of each vehicle in the vicinity of the AV fluctuates with each passing moment. Therefore, a spatio-temporal state space representation is needed to encapsulate the evolving importance of the interactions between the AV and surrounding vehicles. This work introduces the Deep Attention Driven Reinforce-"}, {"title": "III. THE DAD-RL FRAMEWORK", "content": "This section explains the mechanics of the DAD-RL decision-making framework. The primary component of DAD-RL is the spatio-temporal deep attention state-encoding mechanism learned and wielded by RL-driven ego-vehicle. The framework also consists of a context-encoder to process route information. This RL driving task is formulated as a POMDP since our ego-vehicle can access limited knowledge ascribed to sensor range limitations. The following subsections explain the input and observation space design, the DAD-RL processing, and the RL framework with action space and reward structure."}, {"title": "A. Observation Space (Input) Preprocessing", "content": "The ego-vehicle can obtain historical information of its surrounding vehicles \\(H_t\\), segmented Bird-Eye-View (BEV) context \\(C_t\\), and an elaborate ego's odometric state history \\(E_t\\) as shown in Fig. 2. The whole observation space tensor is represented as \\(O_t = [H_t; E_t;C_t]\\). The subscript \\(t\\) attributes to the tensor's value at time \\(t\\). The BEV context is defined as \\(C_t\u00b0= {D_t;W_t}\\), where \\(D_t\\) is drivable area map and \\(W_t\\) waypoint map of size 128 \u00d7 128. The historical surrounding vehicle data is defined as \\(H_t = [H_{t-5k.dt}^{1su},..., H_{t-5k.dt}^{nsu}]\\) forn surrounding vehicles, with \\(H_{t-5k.dt}^{isu} = [x_{t-5k.dt}^{isu}, y_{t-5k.dt}^{isu}, p_{t-5k.dt}^{isu}, v_{t-5k.dt}^{isu}, l_{t-5k.dt}^{isu}]\\) of vehicle i's current and past timesteps spaced four simulation steps dt apart. Here, for vehicle i, \\(X^{isu} = (x^{isu}, y^{isu})\\), \\(p^{isu}\\), \\(v^{isu}\\), and \\(l^{isu}\\) are historical data of the relative position to the ego, the heading, speed of the vehicle and the lane it is in respectively. Observation \\(E_t = [(E_{t}^{(1)}; E_{t}^{(2)})]\\) consists of various historical odometric values split into two vectors based on utility in the DAD-RL framework. The vector \\(E_{t}^{(1)}\\) contains the same quantities as in \\(H^{isu}\\) and \\(E_{t}^{(2)} = [(w_{t\u22125k.5t}, \u03b8_{t\u22124k.st}, v_{t95k.st}, a_{t 5k.st}, l_{t\u22125k.5t})]_{i=0}^{4}\\), a tuple having steering, yaw rate, linear velocity, linear acceleration, and linear jerk of the ego at time t respectively. The following section explains how the spatiotemporal deep attention encoder processes this observation space. The encoder has two parts: a) Spatio-Temporal Attention Encoder (STAE) and b) Context Encoder (CE).\nSpatio-Temporal Attention Encoder (STAE): This encoder \\(\\phi_{\\eta}\\) takes in the historical kinematic states of the ego and surrounding vehicles \\(E_t\\) and \\(H_t\\) at timestep t as input. To encode the temporal kinematic relationships (past 2.1 seconds or 21 dt simulation steps) of a dynamic traffic scenario, the states of the surrounding vehicles \\(H_{ts}\\) and the ego vehicle \\(E_{t}^{(1)}\\) is passed through a shared Long Short-Term Memory (LSTM) network. Let's denote the aforementioned vectors of each vehicle \\(H^{isu}\\) and ego \\(E_{t}^{(1)}\\) as \\(I_t\\). Eq 1 represents the intermediate temporal encoding \\(P_t\\). \n\n\\(I = E_{t}^{(1)}\\) or \\(H^{isu}\\)\n\n\\(P_t = LSTM(I_t)\\)\n\n(1)"}, {"title": "B. RL Algorithm", "content": "The ego vehicle has a stochastic policy network \\(\u03c0\u03b8\\) with \\(\u03b8\\) as its parameters mapping state st to actions. As in any RL task, the objective is to learn \\(\u03c0\u03b8\\) along with our STAE (\\(\u03c6\u03b7\\)) that can maximize the cumulative reward obtained by the ego-vehicle. Soft Actor-Critic (SAC), a state-of-the-art RL algorithm, is used for training policy networks and STAE.\nReward Structure: The ego-vehicle is trained using a dense reward structure for safety and comfort (evaluated by 'Humanness error' metrics). The reward structure (Equation 5) is a linear combination of rewards and penalties:\n\n\\(R = \u03bb_1.r_{crash} + \u03bb_2.r_{Troad} + \u03bb_3.r_{vego} + \u03bb_4.r_{goal}\\)\n\n\\(+\u03bb_5.r_{prog} + \u03bb_6.r_{Toroute} + \u03bb_7.r_{ww} + \u03bb_8.r_{slow}\\)\n\n(5)\n\nThe positive reward terms are \\(r_{goal}\\) and \\(r_{prog}\\). \\(r_{goal}\\) = 1 if the agent reaches the goal; otherwise, \\(r_{goal}\\) = 0; \\(r_{prog}\\) represents distance travelled towards goal. To encourage the ego vehicle's momentum towards reaching the goal under"}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "The DAD-RL framework strongly emphasizes utilizing interaction encoding techniques to comprehend intricate and realistic traffic settings. SMARTS has been chosen as the simulation platform to assess the effectiveness of the DAD-RL framework in handling such complex scenarios. Within SMARTS, several demanding scenarios were constructed aimed at both training and testing the DAD-RL approach. These scenarios are designed to encompass diverse interactive and stochastic traffic dynamics. The following scenarios capture various aspects of realistic driving behaviors.\nLeft Turn-T: This is an urban T-junction with heavy traffic and no traffic signals. The goal of the ego-vehicle is to take an unprotected left turn.\nRoundabout: In this urban roundabout scenario, the ego transitions from a 2-lane bi-directional road to a 2-lane unidirectional roundabout with four exits\u2014three versions: Roundabout-A, B, and C, with increasing difficulties in that order. The ego vehicle is supposed to cover a quarter, half, and three-quarters in Roundabout-A, B, and C, respectively. The commute distance sets the difficulty level.\nDouble-Merge: In this scenario, an autonomous vehicle ('ego') starts from a single-lane road, navigates a two-lane one-way road with two entrances/exits, and exits on the opposite side. The 'ego' must effectively perform lane changes amidst traffic flows from all entrances to exits, honing its navigation and lane-changing skills.\nThe scenarios above incorporate heavy traffic flows randomly selected for each simulation episode. The agent en-"}, {"title": "V. CONCLUSIONS", "content": "This paper underscores the challenges recent approaches and RL-based decision-making algorithms encounter in dynamic driving environments. While emphasizing the significance of interaction modeling, we introduce a simple spatiotemporal attention encoder rather than a full transformer for secure decision-making in AV. A new approach to attention modeling for AVs is introduced, which encodes dynamic interactions with surrounding vehicles using the Deep Attention Driven Reinforcement Learning (DAD-RL) framework. The framework enhances performance compared to previous state-of-the-art RL-based decision-making for AVs, including the work that uses a transformer. Future research will concentrate on integrating the dynamics of AVs with safety layer design and interpretable decision-making, paving the way for more robust and reliable autonomous systems."}]}