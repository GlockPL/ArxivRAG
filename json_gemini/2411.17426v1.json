{"title": "CLOVER: Constrained Learning with Orthonormal Vectors for Eliminating Redundancy", "authors": ["Fanxu Meng", "Muhan Zhang"], "abstract": "To adapt a well-trained large model to downstream tasks, constraining learning within its original\nlatent space by leveraging linear combinations of its basis vectors ensures stable training without\ncompromising the model's capabilities. However, constructing orthonormal bases from a matrix\nrequires a transfer matrix, which significantly increases storage and computational overhead for both\nparameters and feature maps.\nIn this paper, we absorb W\u0119 and WK into WQK, and Wv and Wo into Wvo, where the rank of\nWQK and Wvo are bounded by the head dimension d. By decomposing WQK and Wvo, and\nremoving the singular vectors corresponding to zero singular values, we can orthogonalize WQ, WK,\nWv, and Wo without requiring transfer matrices. Furthermore, the absorb-decompose operation\neliminates redundant vectors, reducing the encoder attention parameters of Whisper-large-v3 by\n46.42% without requiring training.\nWe orthonormalized WQ, WK, Wv, and Wo, and fine-tuned only the singular values, enabling\nefficient adaptation constrained to the original latent space. We fine-tune LLaMA-2-7B on eight\ncommonsense reasoning datasets, and the results outperform LoRA by 5.4% and DoRA by 4.4%.", "sections": [{"title": "1 Introduction", "content": "Fine-tuning large language models (LLMs) is highly effective for enhancing downstream task performance. However,\nfine-tuning very large models is costly. For instance, 16-bit fine-tuning of GPT-3 175B consumes 1.2 TB of VRAM\n[1], while the LLaMA 65B model requires over 780 GB of GPU memory [2]. To address this, parameter-efficient\nfine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA) [1], have been developed to reduce memory usage\nand parameter requirements while maintaining performance without adding inference latency.\nHowever, LoRA is less stable than full-parameter fine-tuning. For example, Shuttleworth et al. [3] recently analyzed\nthe impact of fine-tuning on pre-trained models by examining the spectral properties of weight matrices. They found\nthat LoRA introduces high-ranking singular vectors, termed 'intruder dimensions', which are absent in full fine-tuning.\nModels fine-tuned with LoRA tend to forget more of the pre-training distribution and show less robust continual learning\ncompared to full fine-tuning.\nPiSSA [4] applies SVD to pre-trained matrices, fine-tuning the principal singular values and singular vectors. Following\nPISSA, SVFT [5] freezes the singular vectors and fine-tunes only the singular values. Both approaches help to stabilize\nthe training process by avoiding the introduction of 'intruder dimensions'. While SVFT reduces the number of\ntrainable parameters, it requires decomposing a matrix $W \\in \\mathbb{R}^{m \\times n}$ into orthonormal matrices $U \\in \\mathbb{R}^{m \\times \\min(m,n)}$,\n$V\\in \\mathbb{R}^{\\min(m,n) \\times n}$, and a diagonal matrix $S\\in \\mathbb{R}^{\\min(m,n) \\times \\min(m,n)}$. This process introduces significant additional\nparameters and computational overhead. In contrast, PiSSA focuses on fine-tuning only the principal singular values\nand singular vectors while keeping the remaining components frozen. This strategy avoids the substantial computational\ncost associated with SVFT but restricts adjustments to a limited portion of the orthonormal basis.\nA key question then arises: how can we efficient orthogonalize matrices and fine-tune within the latent space of\npre-trained models?\nIn this work, we observe that each attention layer naturally contains two pairs of matrices, W\u0119 and W\u2081\u2081, as well as Wv\nand Wo, which can be absorbed into head-wise low-rank matrices WQK \u2208 Rh\u00d7D\u00d7D and Wvo \u2208 Rh\u00d7D\u00d7D, where\nthe ranks satisfy rqk \u2264 d and rvo \u2264 d. By decomposing WQK and Wvo with SVD and removing singular vectors\ncorresponding to zero singular values, we obtain orthonormal bases without increasing and even reducing the number\nof frozen parameters. These frozen bases maintain stability during fine-tuning, while the corresponding singular values,\nwith minimal parameter overhead, enable efficient fine-tuning.\nSummary of Contributions:\n\u2022 Orthogonalization Vectors via Absorb-Decompose: We discovered that the WQ, WK and Wv, Wo matrices\nin attention mechanisms can be Orthogonalized through an Absorb-Decompose method. This transformation\npreserves the model's original capabilities.\n\u2022 Eliminating Redundant Vectors: Orthogonalizing attention heads effectively eliminates redundant parame-\nters. Applying Absorb-Decompose to the encoder of Whisper-large-v3 reduces 56.01% of the parameters in\nWQ and WK, and 36.82% in Wv and Wo, without any loss of performance, even without additional training.\n\u2022 Efficient and Stable Fine-Tuning: By freezing the orthonormal bases and fine-tuning only their linear\ncombinations, we introduce Constrained Learning with Orthonormal Vectors for Eliminating Redundancy\n(CLOVER). Applying CLOVER to fine-tune LLaMA-2-7B achieves an average improvement of 5.4% over\nLORA and 4.4% over DoRA across 8 commonsense reasoning fine-tuning tasks."}, {"title": "2 Related Works", "content": "LORA [1] integrates trainable adapters into linear layers, allowing these adaptations to be re-parameterized back into\nthe standard model structure after fine-tuning. This approach has gained widespread adoption for its ability to preserve\nthe model's original architecture while enabling efficient fine-tuning. Building on LoRA, AdaLoRA [6] dynamically\nlearns the rank size required for LoRA in each model layer, optimizing parameter efficiency. DeltaLoRA [7] enhances\nLoRA's representational capacity by directly updating the original weights of the model using parameters from adapter\nlayers. LoSparse [8] incorporates LoRA to mitigate the risk of pruning overly expressive neurons. DoRA [9] introduces\na magnitude component to learn the scale of AW, while using the original AB as the direction component of AW.\nPISSA [4] focuses on the impact of initialization on gradient directions, firstly applying singular value decomposition\n(SVD) to the original matrix and fine-tuning the principal singular values and corresponding singular vectors. This\ninitialization approach leads to faster convergence, improved performance, and reduced quantization error. Following\nPISSA, LORA-XS [10], LaMDA [11] and SVFT [5], perform singular value decomposition on the original matrix,\nfreezing the singular vectors while fine-tuning the singular values to reduces the trainable parameters. Among them,"}, {"title": "3 CLOVER: Constrained Learning with Orthonormal Vectors for Eliminating Redundancy", "content": ""}, {"title": "3.1 Orthonormal Vectors for Eliminating Redundancy", "content": "For Multi-Head Self-Attention, X \u2208 Rb\u00d7n\u00d7D, WQ \u2208 RD\u00d7h\u00d7d, WK \u2208RD\u00d7h\u00d7d, Wv \u2208 RD\u00d7h\u00d7d, Wo \u2208 Rh\u00d7d\u00d7D.\nWhere b, n, and D represent the batch size, the sequence length, and the dimension of X. h and d are the number of\nheads and the dimension of the head for WQ, WK and Wv, Wo.\nThe process of absorbing and decomposing WQ and WK can be represented as follows:\nattn(Q, K) = softmax($\\frac{QTKT}{\\sqrt{d}}$), Q = XWQ \u2208 Rbxhxnxd, K = XWK \u2208 Rbxhxnxd  (1)\n= softmax($\\frac{XWQW_kX^T}{\\sqrt{d}}$), WQWK = WQK \u2208 Rh\u00d7D\u00d7D. (2)\n= softmax($\\frac{XW_{QK}X^T}{\\sqrt{d}}$), WQK = USV = U[:,:,:rqk]S[:,:rqk,:rqk]V[:,:rqk,:] = UQKSQKVQK,rqk \u2264 d. (3)\n= softmax($\\frac{XU_{QK}S_{QK}V_{QK}X^T}{\\sqrt{d}}$), UQK \u2208 RD\u00d7hxrqk, SQK \u2208 RhxrqkXrqk, VQK \u2208 RhxrqkXD (4)\nThrough this series of transformations, WQ and WK can be equivalently replaced by orthogonal vectors UQK and VQK,\nalong with the diagonal matrix SQK.\nThe process of absorbing and decomposing Wv and Wo can be represented as follows:\nY = attn(Q, K)VWo, V = XWv \u2208 Rb\u00d7h\u00d7n\u00d7d, (5)\n= attn(Q, K)XWvWo, WvWo = Wvo \u2208 Rh\u00d7D\u00d7D (6)\n= attn(Q, K)XWvo, Wvo=USV = U[:,:,:rvo]S[:,:rvo,:rvo]U[:,:rvo,:] = UvoSvoVvo,rvo \u2264 d. (7)\n= attn(Q, K)XUvoSvoVvo, Uvo \u2208RDxhxrvo, Svo \u2208 RhxrvoXrvo, Vvo \u2208 Rhxrvo\u00d7D. (8)\nThrough this series of transformations, Wy and Wo can be equivalently replaced by orthogonal vectors Uvo and Vvo,\nalong with the diagonal matrix Svo."}, {"title": "4 Constrained Learning with Orthonormal Vectors", "content": "The proposed Absorb-Decompose operation effectively reduces linear dependencies in WQ, WK, WV, and Wo. During\nthe decomposition of WQK and Wvo, it generates vectors with dimensions smaller than the head dimension, enabling\na training-free pruning process.\nUnlike traditional pruning, which allows for lossy pruning followed by retraining to recover model accuracy, pruning\nlarge pre-trained models presents unique challenges. The pretraining of such models relies on vast, often inaccessible\ndatasets. As a result, while retraining may achieve strong performance on certain benchmarks, it can fail on other tasks\nor even introduce safety risks. This makes training-free pruning particularly critical.\nThanks to the Absorb-Decompose method, which orthogonalizes the original WQ-WK and Wv-Wo pairs in attention\nlayers, we obtain a moderate number of singular values. For comparison, SVFT [1] decomposes the entire matrix and\nproduces singular values S \u2208 Rmin(m,n)\u00d7min(m,n). One approach treats S as a vector, significantly reducing the number\nof trainable parameters but limiting expressive capacity since it can only scale singular vectors without learning their\nlinear combinations. Another approach considers S as a full matrix, which, for attention layers where m = n, results in\na parameter size equivalent to the original matrix, making it less practical. To balance these trade-offs, SVFT adopts a"}, {"title": "5 Experiment", "content": ""}, {"title": "5.1 Absorb-Decompose for Pruning", "content": "We selected the Whisper-large-v3 model [20], a Transformer-based encoder-decoder architecture with 1.55 billion\nparameters. It is trained simultaneously on multilingual speech recognition and speech translation tasks. We applied the\nAbsorb-Decompose method across all 32 attention layers in the encoder. Figure 2 illustrates the singular values SQK\nobtained through this method. For comparison, we also computed the Euclidean norm for each dimension and ranked\nthem in descending order within each attention head."}, {"title": "5.2 Absorb-Decompose for Fine-Tuning", "content": "In this section, we evaluate CLOVER against LoRA [1] and DoRA [9] on LLaMA-2-7B for commonsense reasoning\ntasks. We did not compare with SVFT [5] due to its significant additional overhead. Commonsense reasoning\ntasks are divided into eight sub-tasks, detailed in Table 1. Following the DoRA setup, we fine-tune the combined\nCommonsense-170k dataset and evaluate the individual test set for each sub-task."}, {"title": "6 Limitations", "content": "While Absorb-Decompose primarily supports Self-Attention, it also extends to Cross-Attention and cases where the\ninput dimensions of WQ and WK differ, or the input dimension of Wy differs from the output dimension of Wo.\nAdditionally, it supports Causal Mask, Sliding Window mechanisms, and Linear layers with bias. However, the method\ncurrently does not support scenarios where nonlinear operations, such as ROPE or QK norm, exist between WQ and\nWK. For these cases, we replace SVD with QR decomposition and directly decompose it by heads into an orthogonal\nmatrix Q and fine-tune the upper triangular matrix R."}, {"title": "7 Conclusion", "content": "In this paper, we highlight the importance of freezing the orthogonal basis and fine-tuning their linear combinations\nto improve the stability of pre-trained model fine-tuning. We analyze the trade-off between efficiency and stability\nin existing methods and propose the absorb-decompose operation to achieve a balance between the two. Which\nremoves linearly dependent bases in attention heads, enabling training-free pruning of 46.42% of the parameters in the\nWhisper-large-v3 encoder attention. By fine-tuning a subset of singular values with moderate parameter counts and\nstrong expressive power, our CLOVER method outperforms LoRA by 5.4% on eight commonsense reasoning tasks. We\nbelieve this approach is valuable for both PEFT and pruning, while also providing insights into the attention mechanism\nand the fine-tuning process of large models."}]}