{"title": "Zero-Shot Classification of Crisis Tweets Using Instruction-Finetuned Large Language Models*", "authors": ["Emma L. McDaniel*", "Samuel Scheele**", "Jeffrey Lius"], "abstract": "Social media posts are frequently identified as a valuable source of open-source intelligence for disaster response, and pre-LLM NLP techniques have been evaluated on datasets of crisis tweets. We assess three commercial large language models (OpenAI GPT-40, Gemini 1.5-flash-001 and Anthropic Claude-3-5 Sonnet) capabilities in zero-shot classification of short social media posts. In one prompt, the models are asked to perform two classification tasks: 1) identify if the post is informative in a humanitarian context; and 2) rank and provide probabilities for the post in relation to 16 possible humanitarian classes. The posts being classified are from the consolidated crisis tweet dataset, CrisisBench. Results are evaluated using macro, weighted, and binary F1-scores. The informative classification task, generally performed better without extra information, while for the humanitarian label classification providing the event that occurred during which the tweet was mined, resulted in better performance. Further, we found that the models have significantly varying performance by dataset, which raises questions about dataset quality.", "sections": [{"title": "I. INTRODUCTION", "content": "In crisis scenarios, such as natural hazard-induced disasters or humanitarian emergencies, timely and accurate information is crucial to decision makers. Social media posts can provide valuable information in real time; however, the sheer speed and quantity of data coming from social media can be overwhelming for human analysts to process. As such, Natural Language Processing (NLP) techniques have been used to automate the processing of social media data in order to classify and extract the most relevant information. CrisisBench [1] provides a benchmark dataset to evaluate the performance of NLP solutions for classifying crisis-related social media posts.\nRecently, Large Language Models (LLMs) and Large Multimodal Models (LMMs) have shown impressive performance on a wide range of NLP tasks without needing task-specific training or fine-tuning. Large Language Models, such as GPT [2], [3], are trained on massive text datasets to predict the next word in a sequence, and can be used to generate answers to questions. They can thus be used as zero-shot text classifiers by inputting the relevant text, followed by a question asking which of a set of given labels or classes apply to the text. Large Multimodal Models can be trained and utilized similarly, except they are configured to accept other data modalities, such as images, in addition to text.\nDue to the recent popularity of LLMs, we expect that humanitarian practitioners will try to use them to help automate the process of extracting relevant information from social media during crises. As a first step towards characterizing the performance of LLM/LMMs on such tasks and identifying which ones provide the best performance, we evaluate various open-access and commercial LLMs and LMMs on zero-shot classification of social media posts using the CrisisBench dataset. In addition, we compare the performance of the zero-shot classifiers to the existing benchmarks from purpose-built classifiers for crisis-related social media text classification."}, {"title": "A. Related Work", "content": "CrisisBench [1] combines a number of crisis datasets [4]\u2013[9] through cleaning and standardizing labels in order to create a benchmark for measuring performance of NLP classification of crisis-related social media posts. CrisisBench defines two tasks. The \u201cinformativeness\u201d task, a binary classification task that seeks to identify whether a provided tweet contains valuable information regarding a disaster or crisis event. The \u201chumanitarian information type,\u201d a multi-class classification task that seeks to categorize a tweet into one of 16 classes (e.g donation and volunteering, displaced and evacuations).\nPrevious work in classifying crisis-related social media posts has involved conventional machine learning method-"}, {"title": "II. METHODOLOGY", "content": "A. CrisisBench Task Descriptions\nIn this paper, we focus on the \"informativeness\" task from the CrisisBench consolidated dataset, and provide incidental analysis of the \u201chumanitarian information type\" task for those data points that also had \"humanitarian information\" labels. Roughly 5,000 of the examples in the informativeness test set are also in the humanitarian information type test set these examples are the only ones considered for the analysis of the humanitarian information type task.\nThe motivation for this is that the classes in the multi-class task are often amalgamations of classes from the constituent datasets, and most constituent datasets used only a few of the classes. In the Discussion, we will provide preliminary analysis of the multi-class task where semantic differences in definitions across constituent datasets substantially impacted performance. The \u201cinformativeness\u201d task does not suffer from the same ambiguity as the \u201chumanitarian information type\u201d task and results are therefore easier to obtain and interpret.\nA subset of tweets in the CrisisBench dataset are from CrisisMMD dataset, which contains only tweets which include images [5]. The CrisisBench authors also include an event type annotation that indicates the type of crisis event that was contemporaneous with the timestamp of the tweet. We evaluate each task both with/without event awareness, and with/without images for four configurations per task.\nB. Models Evaluated\nWe evaluate three commercial models: OpenAI's GPT-4o [29], Google's Gemini 1.5 Flash [30], and Anthropic's Claude Sonnet 3.5 [31], and accessed them through their respective APIs. The models were chosen based on several considerations, including prominence, performance on other benchmarks, and availability.\nC. Prompt Structure\nWe used the same base prompt for all models in the CrisisBench dataset, in which we requested that the model return a JSON string with a specified schema. We used Pydantic to validate the JSON. In the case where the model did not return valid JSON, we simply re-submitted the prompt and retried up to a set patience of three attempts. Responses that were not valid were omitted from analysis.\nWe asked the models to complete both the \"informativeness\" task as well as the multi-class \u201chumanitarian information type\" classification task in the same prompt. For the informativeness task the model provides a true or false The base prompt is provided below,\nProvide classifications of the following tweet\nbased on its relevance to a humanitarian\nevent and a classification of its content.\n\"\n\"{img_str}\"\n\"{event_str}\"\n\"The tweet follows:\\n{tweet_str} \\n\"\n\"{field_descriptions}\"\nare placeholders.\nwhere the fields {img_str}, {event_str},\n{tweet_str}, and {field_descriptions}\nThe placeholder {img_str} was filled in with the\ntext \"Use the images, if present, to help\nyou make a your determinations related to\nthe informativeness and category of the\ntweet.\" if an image was associated with the tweet;\notherwise, it was left blank. Images were resized to fit within\n768 x 768 pixels while maintaining aspect ratio, encoded in\nbase64, and appended to the prompt in accordance to the\nrespective LMM's specifications.\nThe placeholder {event_str} was filled in with\n\"While it may still be irrelevant or\nuninformative, this tweet was created\naround the time of a disaster with\ndescription: {event_type}.,\" where\n{event_type} corresponds to the event that was occurring\nduring the time of the tweet if we were evaluating the tweet in\nthe \u201cevent-aware\" configuration; otherwise, {event_str}\nwas left blank. The {tweet_str} placeholder contained\nthe actual text of the tweet.\nThe {field_descriptions} placeholder contained\ndescriptions of the classes as well as the desired\nJSON format for the output. We requested two fields:\nis_informative and humanitarian_label. For the\nis_informative field, the prompt was \"Does the\ntweet contain information pertinent to a\nhumanitarian event or natural disaster?\nRespond with a boolean true/false\". For the\nhumanitarian_label field, the prompt is provided\nbelow:\nFor a given tweet, determine which of the\nhumanitarian labels are most relevant:\nThe humanitarian labels and their descriptions\nare provided below:\n\"not_humanitarian\" the tweet is not\nhumanitarian in nature and does not fit\ninto any other class.\n\"donation_and_volunteering\" the tweet\nrelates to directing, accepting, or\ndistributing donations or volunteer\neffort."}, {"title": "D. Evaluation", "content": "For both the informativeness and humanitarian label tasks, we calculate F1 scores to facilitate comparison with existing evaluations of datasets within CrisisBench. For the informativeness classification, we calculate macro (unweighted), weighted, and binary (only the positive class) F1 scores; for the humanitarian classification, we calculate weighted and macro (unweighted) class-averaged F1 scores. Given that some datasets in CrisisBench may cover only a subset of the 16 labels specified in the prompt, we maintain consistency in the prompt by including all 16 labels in our evaluation framework. However, the performance assessment of each dataset is based exclusively on the rankings of the labels present within that dataset. Class-specific precision for class i is computed as\n$P_i = \\frac{TP}{TP+FP}$, where TP, FP; stand for the count of true positives and false positives for class i, respectively. Class-specific recall is defined as $R_i = \\frac{TP}{TP+FN}$, where $FN_i$ is the count of false negatives for class i. Class-specific F1 is defined as\n$F1_i = \\frac{2P_iR_i}{P_i + R_i}$\nIn the case where there is only one class, the class-specific F1 is equivalent to the binary F1."}, {"title": "III. RESULTS", "content": "In our experiment, the classification tasks for the crisis tweets on informativeness and type of humanitarian label are combined into one prompt. The model was prompted to: 1) determine if the social media post is informative in a humanitarian context, and 2) rank and assign probabilities to 16 potential humanitarian labels in how it fits the post. We report macro, weighted, and binary F1 scores for the informativeness task. For the humanitarian task, we use macro and weighted F1 scores.\nResults for the informativeness task across the three models for all tested crisis tweets are in Table I. Weighted, binary and macro F1 scores are reported with and without event awareness. \"Event-aware\" indicates whether the name of a disaster contemporaneous with the tweet is included in the prompt. For each version of the prompt, the highest F1 scores are bolded. Models performed slightly better without event-awareness. OpenAI's GPT-4o performed the best without event-awareness at a macro F1 score of 0.819, a binary F1 score at 0.860, and weighted F1 score at 0.828.\nThe results by dataset are in two tables, Table II contains all datasets not including CrisisMMD, and Table III contains the CrisisMMD results. These results are also separated by whether extra information (event and/or image) was included in the prompt. Across most datasets, OpenAI GPT-4o outperformed the other evaluated LLMs in informativeness classifications.\nBased on reported metrics, the best LLMs compares moderately lower to existing benchmarks on the consolidated dataset [1], with a weighted F1 of 0.828 (LLM: GPT-4o) vs. 0.883 (fine-tuned ROBERTa). When looking at specific datasets where benchmarks were available, the LLMs also underperform, sometimes by a large margin: on CrisisMMD [10], (weighted F1 of .638 vs .842), on CrisisLexT26 [11] (macro F1 of .492 vs .848), and CrisisLexT6 [11] (macro F1 of .882 vs .947).\nAs a note, we contend that binary F1 (class-wise F1 for single class) as the most appropriate metric for this task. Macro and weighted F1 are most appropriate metrics for multi-class classification, in which F1 scores for multiple classes are condensed into a single metric. For this binary classification problem, the \"informative\" class serves as a foreground and the \"not informative\" class as a background; binary F1 appropriately privileges the foreground class as being the one which is useful to distinguish. The use of macro and weighted F1 are significantly impacted by the number of negative examples in the dataset, which is not desirable when the task is trying to find positive instances in a haystack of negative examples. The LLMs obtain substantially better binary F1 scores than macro F1. Unfortunately, binary F1 is not reported for the informativeness task in any literature we found, making direct comparison on the metric difficult.\nFor the informativeness task, the inclusion of images (in CrisisMMD) and event context (all datasets) did not significantly affect F1 scores, with changes dependent on the dataset. For humanitarian classification, including event context slightly improved scores, while including images did not.\nFor the humanitarian classification task, we compute both weighted and macro F1 scores, and treat the highest-ranked label within the subset of labels from the constituent dataset as the predicted label for evaluation. These results are reported in Table IV. The highest F1 score for both macro and weighted"}, {"title": "IV. DISCUSSION", "content": "Overall, LLMs perform reasonably well on the informativeness task, achieving zero-shot performance on the consolidated dataset within 6% of that of pretrained classifiers in [1].\nHowever, performance was substantially worse on the multi-class humanitarian label task. The LLMs broadly underperformed the models trained in CrisisTransformers [20]. We note several limitations and challenges that may have contributed to this. The task performance of the LLMs tested may have been limited by a number of factors, including the absence of optimizations like prompt engineering or fine-tuning. Further, fine-tuned models may perform better because they were fine-tuned on each dataset individually and were able to fit the base rates at which various classes occur. More fundamentally, however, we note that methods in the construction of CrisisBench dataset had a substantial impact on the multi-class task performance for the LLMs.\nWhereas the zero-shot classification technique for LLMS uses natural language prompting to describe the criteria for each class based on the name and description of the label, traditional models are trained on the training dataset. This raises a potential issue if there is misalignment between the labeled examples and the semantic understanding of the label. We identified methods related to the construction of CrisisBench which may contribute to such misalignment.\nCrisisBench draws from a number of datasets, aggregating labels with potentially different definitions into single classes. For example, the infrastructure_and_utilities_damage class is defined in CrisisNLP-volunteers to be the destruction of houses, buildings, or roads, or the interruption of utilities, but CrisisNLP-CF defines it to include restoration of utilities as well [1]. Furthermore, the classes of constituent datasets are sometimes defined such that they cannot be mapped onto a single CrisisBench label. The CrisisLexT26 affected_individual class contributes all of the affected_individual examples in CrisisBench. But in CrisisLexT26, this class is defined to include personal updates, which is a separate CrisisBench class. The only way for an LLM to correctly categorize a personal update is to correctly guess whether it came from CrisisLexT26 or not. As CrisisLexT26 is one of the largest datasets labeled for humanitarian class, it is unsurprising that personal_update and affected_individual are the two lowest-performing classes for the evaluated LLMs.\nTo better understand the impact of ambiguous label mappings on performance, we performed manual binary annotation on two of the lowest-performing classes, affected_individual and caution_and_advice. We examined 75 randomly sampled tweets which were assigned to the two classes (for a total of 150 tweets) by either the ground truth label or the maximum likelihood estimate of the LLM, without knowledge of the ground-truth or predicted label of any particular tweet. We manually performed binary classification on each tweet as either matching or not matching the description of its reference class given in the prompt (for example, we might look at a tweet with the knowledge that at least one of the ground truth label or the predicted label was affected_individual, and assign a binary label based on whether we believed the tweet matched the definition we gave for that class). The results of this experiment are in Table V. This experiment suggests that semantic differences in the labels, which would not have affected models trained on the training data [20], had a substantial impact on the performance of the LLMs. It also showed generally low agreement between our manual labels and the ground truth, with our manual labels matching the LLM labels more often than the ground truth on the affected_individuals class. While the LLMs performed better on our manual labels than on the ground truth in general, the difference in performance is much larger in the cases where agreement between our labels and"}, {"title": "V. CONCLUSION", "content": "In this paper, we present the performance of commercial large language models on zero-shot classification for two tasks on short social media posts on CrisisBench. We find that overall performance on the binary informativeness task is strong, even relative to models fine-tuned on the evaluation datasets. Incorporating extra information in the form of possible event context and images did not substantially impact the model's performance on the task.\nFor the second task, humanitarian classification, an ambiguous multi-class task performance rapidly declined, emphasizing the need for careful deployment of these tools to the humanitarian space. Based on small-scale experiments with manual labeling, we attribute most of the LLMs' declining performance to semantic ambiguity in social media posts and their labels rather than a latent inability to parse and classify natural language.\nIn future work, we plan to include open-source models in our classification assessments. We will substantially reduce problems associated with the dataset aggregation performed by CrisisBench by changing the prompt and label definitions based on source dataset. This will also provide an avenue for further analysis of each dataset's quality. Further, we plan to analyze the results by language to better understand the multilingual components of the LLMs in relation to humanitarian classification tasks. Another avenue of future research is to assess the impact of prompt engineering more broadly. For example, we prompt for both classification tasks in the same prompt, but it would be of interest to look into the extent to which the dual classification task in one prompt impacts model performance."}]}