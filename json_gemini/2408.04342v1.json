{"title": "Towards Explainable Network Intrusion Detection using Large Language Models", "authors": ["Paul R. B. Houssel", "Siamak Layeghy", "Priyanka Singh", "Marius Portmann"], "abstract": "Large Language Models (LLMs) have revolutionised natural language processing tasks, particularly as chat agents. However, their applicability to threat detection problems remains unclear. This paper examines the feasibility of employing LLMs as a Network Intrusion Detection System (NIDS), despite their high computational requirements, primarily for the sake of explainability. Furthermore, considerable resources have been invested in developing LLMs, and they may offer utility for NIDS. Current state-of-the-art NIDS rely on artificial benchmarking datasets, resulting in skewed performance when applied to real-world networking environments. Therefore, we compare the GPT-4 and LLama3 models against traditional architectures and transformer-based models to assess their ability to detect malicious NetFlows without depending on artificially skewed datasets, but solely on their vast pre-trained acquired knowledge. Our results reveal that, although LLMs struggle with precise attack detection, they hold significant potential for a path towards explainable NIDS. Our preliminary exploration shows that LLMs are unfit for the detection of Malicious NetFlows. Most promisingly, however, these exhibit significant potential as complementary agents in NIDS, particularly in providing explanations and aiding in threat response when integrated with Retrieval Augmented Generation (RAG) and function calling capabilities.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have recently emerged as a transformative force in natural language processing, showcasing proficiency in handling unstructured data. With the ability to generate text and comprehend context, LLMs have found widespread application in various domains, ranging from conversational chat agents to code generation and translation tasks. Despite these advances, the application of LLMs to cybersecurity, particularly Network Intrusion Detection Systems (NIDS), remains largely unexplored.\nNIDS are critical for monitoring and analysing network traffic to detect malicious activities and security breaches. Existing NIDS employ a combination of signature-based and anomaly-based detection techniques. Signature-based methods rely on predefined patterns of known attacks, while anomaly-based methods identify deviations from established norms in network behaviour. Although deep learning-based NIDS have demonstrated near-perfect performance on benchmark datasets, their efficacy in real-world environments is often questioned due to the prevalence of synthetic and misrepresentative datasets used in academic research [1]. Additionally, these models lack explainability [2], making it difficult for security professionals to understand and trust their predictions and respond to the detected threat. It thus remains important to investigate if LLMs are a potential solution to these.\nSeveral studies have used transformers and LLMs to directly detect threats or as complementary solutions for NIDS. However, no previous work has thoroughly investigated LLMs' adaptivity to the domain of NetFlows. Previous studies replace LLM's sequence-to-sequence layers with a classification head, thus losing the potential for using the same model to provide explanations. Using LLMs and their natural language output, both detection by classification and explainability can be achieved. It remains to be seen if performance is degraded when using these text prediction models as threat classifiers, a use case very different from their original design purpose. LLMs have shown the ability to analyse long-term dependencies and be adaptive to the applied domains [3]. Furthermore, their pre-training on large amounts of natural language data might incorporate the knowledge required to detect malicious NetFlows without relying on skewed artificial datasets. This paper explores the capability of LLMs to detect network attacks based on network flows while contrasting their benefits and limitations with traditional Machine Learning (ML) solutions.\nWe empirically assess the pre-trained OpenAI'sGPT-4 and Meta'sLLama3 models, in the context of zero-shot learning"}, {"title": "2 RELATED WORK", "content": "In contrast to NIDS, Host-based Intrusion Detection Systems (HIDS) typically deal with textual data such as system calls, logs, or application and memory traces. Natural Language Processing (NLP) techniques are employed to examine the semantic interactions and sequences of system calls and contextual data, enabling the effective identification of suspicious activities. This facilitates precise, real-time intrusion detection by analysing text data streams. Many solutions relying on NLP have been developed even before the advent of LLMs [6]. Natural language is less present in the NIDS domain, but surprisingly more literature exists.\nFor detection based on network traffic, the application of LLMs has started to gain traction, although the solutions often modify the standard LLM architecture. Specifically, these models typically strip the sequence-to-sequence head and replace it with a classification head to obtain a deterministic output to evaluate the model. The initial textual output head, which predicts the next textual token, is replaced by a classification head vector which has a dimension equivalent to the number of prediction classes, e.g two for a binary classification. Lira et al. [7] investigated the capabilities of LLMs such as GPT-3.5, GPT-4, and ADA to detect Distributed Denial of Service (DDoS) attacks on the CICIDS2017 [8] and Urban IoT [9] datasets. Their findings indicate that LLMs, when fine-tuned or employed in few-shot learning scenarios, could detect DDoS attacks with significant accuracy proving their adaptivity trait. Manocchio et al. [10] explored various encoding and classification methods on different transformer architectures, including GPT's and BERT's to enhance network analysis. Ferrag et al. [11] used BERT for threat detection in Internet of Things (IoT)"}, {"title": "2.2 Explainable NIDS", "content": "Studies aiming to render NIDS explainable typically employ existing explainable AI methods. These methods assign an importance factor to each input feature, indicating their influence on the model's output label [2]. For instance, Mallampati et al. [14], Senevirathna et al. [15], and Zebin et al. [16] utilise the SHapley Additive explanation (SHAP) method across various network domains. This approach expresses explainability as the weight of each feature and its impact on the output class, enabling the development of algorithms to respond to threats in real time. Wei et al. [17] propose a framework that leverages this feature's importance to design rules for active intrusion responses. Their XNIDS framework identifies features critical to the prediction class and generates actionable defence rules, which aid in understanding DL-NIDS behaviours and troubleshooting detection errors.\nLLMs could potentially provide natural language explanations for human operators, not only limiting the explanations to features which statistically appear anomalous to the NIDS. This will allow to facilitate threat response and a better understanding of their NIDS functionality or potential malfunctioning. In this study, we further investigate the capability of LLMs to detect malicious NetFlows while explaining their reasoning. These could potentially be employed as a complementary solution, aiming to enhance the explainability of NIDS."}, {"title": "3 METHODOLOGY", "content": "This section outlines the methodology employed to investigate the potential of LLMs for NIDS. Our approach is based on the open-source LLama3-8B-Instruct and GPT-4 model, designed to better handle instruction-based tasks compared to the foundational model. In contrast with foundational model's difficulties in answering in the required format, as it was not trained to understand instructions but simply to predict sequences of words. We focus on the network traffic under the format of flows, which are currently the most prominent and efficient source of network telemetry for threat"}, {"title": "3.1 Domain Specific Performance", "content": "To measure the performance on specific network environments, we built two baseline models using the LLama3-8B-Instruct and GPT-4-0613 model by applying the zero-shot learning paradigm. Additionally, we will investigate two fine-tuning methods only on the LLama3 model due to the inherited cost associated with fine-tuning using the OpenAI API. For this series of experiments, all datasets are split into training and testing sets with a 95% to 5% ratio while stratifying the attack type. All evaluations are performed following a 10-fold cross-validation method. We only consider a small testing set due to the sheer computational cost of LLM inference. All prediction results from our experiments are evaluated using the macro average of Precision and Recall for both the benign and malicious classes. The macro average is the unweighted sum of the individual performance scores for each class, representing each class equally. We provide the pre-trained LLM with an instruction to detect network attacks based on NetFlows, requesting it to output either \"1\" or \"0\" to indicate whether the provided flow is malicious or benign. After ingesting the instruction prompt, the model will take as an input a tabular NetFlow entry encoded as text using key-value pairs separated by commas to represent the feature name and value pairs. For instance, a network flow originally represented as a row within the CSV table of the dataset is transformed into text as follows: \"L4_DST_PORT: 80\". To ensure consistency and reduce variability in outputs, we set the model's temperature parameter to $10^{-1}$. The temperature parameter, ranging from 0 to 1, influences the probabilities generated by the softmax output function. A low-temperature setting makes the model's outputs more deterministic by prioritising the most likely tokens, important for binary classification tasks [18]. Together with prompt engineering, it allows us to constrain the response of the model to a binary output without any exceptions.\nZero-shot Learning. Zero-shot learning consists of a pre-trained model tasked with making predictions on yet explicitly unseen data. In the context of a pre-trained LLM,"}, {"title": "3.2 Explainability", "content": "To investigate and grasp the LLM's ability to explain the reasons for predicting a network flow as benign or malicious, we manually analyse the models' responses to find patterns and inconsistencies. More specifically, we analysed the explanations provided by our baseline model for the cases of True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN) samples, as these will offer variability in answering. This analysis is not a quantitative evaluation of the quality of explanations but serves as an initial exploration to assess the importance of future research into making LLMs explainable for NIDS."}, {"title": "3.3 Computational Complexity", "content": "To assess the computational cost, we compared our baseline with the most common architectures with the default parameters, used in related work and industrial application of NIDS. All experiments were conducted on a machine equipped with an NVIDIA RTX3090 GPU and a 3.50 GHz CPU. The measured time for sequential inference corresponds to the average time taken to predict a single NetFlow"}, {"title": "4 EMPIRICAL RESULTS", "content": ""}, {"title": "4.1 Domain Specific Performance", "content": "Zero-shot Learning. The results of these empirical experiments under the zero-shot evaluation reveal several important insights. Unsurprisingly, the performance of the pre-trained LLama3 model is worse than random selection, which would have been, considering the macro average, 50% for all metrics, while GPT-4 is slightly better. This outcome is expected because pre-trained LLMs are primarily designed for natural language tasks and not for the specific task of network classification. In contrast, classical ML models demonstrate near-perfect performance.\nFine-tuning. In comparison to the zero-shot learning results, both fine-tuning methods showed slight improvements but still failed to significantly outperform random selection or approach the performance of related works Overall, KTO performed slightly better, indicating that the choice of fine-tuning method is an important parameter for future research. We see that fine-tuning can enhance performance with increased training, but the improvements observed were minimal. Fine-tuning tends to adjust the model's responses to fit a desired style but is insufficient to significantly alter the weights for the binary classification of network attacks. Pre-trained natural language models, which are trained on extensive natural language corpora, may not be capable of recognising the subtle patterns that indicate malicious NetFlows. They are more adept at generating appropriate responses to natural language queries without any further deep understanding.\nFine-tuning LLMs for detecting malicious NetFlows is unnecessary; instead, transformer-based models can be trained directly on entire datasets. Manocchio et al., through their proposed FlowTransformer framework [10], have shown that transformer architectures perform well on binary NetFlow classification tasks when trained on complete datasets. This performance is achieved without the need pre-training on natural language datasets. For instance, their model, using GPT's architecture achieves an 89.98% F1-score."}, {"title": "4.2 Explainability", "content": "The analysis of LLama3 generated explanations for various prediction cases reveal unique benefits and notable limitations to enrich future NIDS applications with explainability, a unique feature proper to LLMs. These sample prompts demonstrate that LLMs can provide detailed and contextually relevant explanations for their classifications of network flows but with notable limitations.\nWhen asked about a TN sample, the model correctly identifies the network flow as a DNS query. However, it inaccurately claims the source IP address is from China, whereas it is actually from Japan, and misidentifies the destination address as being in the USA, although it belongs to the University of New South Wales (UNSW), the dataset's creator. Despite these errors, the LLM correctly identifies the byte size range of all packets in the flow as typical for a DNS query. These inaccuracies highlight the LLM's tendency to hallucinate or generate incorrect information, particularly regarding geographical locations.\nOn a TP case, the LLM mistakenly states that protocol 139 corresponds to the NetBIOS protocol, whereas it corresponds to the Host Identity Protocol. Its confusion is evident as NetBIOS uses port 139. Most importantly, it operates at Layer 5, while the protocol feature identifies those of Layer 4. Additionally, the LLM incorrectly assumes that the use of ports with the value 0 indicates malicious activity. While unusual, ports set to 0 can occur in protocols not using port numbers, like ICMP. These errors show that the LLM has gaps in its understanding of protocol specifics and port usage, leading to incorrect conclusions about malicious behaviour.\nIn the FP scenario, unusual patterns, such as atypical ports and large file transfers are identified leading to the conclusion that these imply maliciousness. While the reasoning is plausible, it cannot correlate these features to specific attack signatures accurately.\nFor the FN sample, the model's explanation is mostly consistent. However, it incorrectly identifies the main Layer 7 protocol user and mistakenly states that Layer 7 protocol number 7 corresponds to HTTP when it indicates the Internet Printing Protocol (IPP). The model classifies the network flow as benign due to small packet lengths and a low number of retransmitted packets, correctly identifying the traffic as typical HTTP traffic. While the model can accurately explain benign traffic by recognising common patterns, it fails to identify the malicious aspects of the sample."}, {"title": "4.3 Computational Complexity", "content": "The feasibility of using LLMs for NIDS not only relies on detection performance but as importantly on inference time. Given that NIDS must handle hundreds, if not thousands, of NetFlows per second, these models must keep up with the network traffic and be able to report incidents promptly. Delays in inference can result in missed or late alerts, compromising network security."}, {"title": "5 DISCUSSION", "content": "Current fine-tuning approaches have not significantly improved performance, highlighting the need for more effective methods tailored to classification tasks. Parameters such as temperature, data encoding and prompt formulation can also affect performance and need to be studied comprehensively for their impact on classification. Furthermore, as demonstrated by the cross-domain performance limitations of existing NIDS models [21], there is a need for more robust and adaptable architectures. Comparing LLMs, known for their adaptability, with other ML architectures in terms of their performance on unseen network behaviours could offer insights for future development in this field.\nTo improve explainability, the integration of a Retrieval-Augmented Generation (RAG) system [24], linked with a Cyber Threat Intelligence (CTI) source, could significantly enhance the reliability of LLMs. RAG can ground LLMs in accurate, up-to-date information, potentially reducing or eliminating hallucinations. Expanding RAG beyond CTI to include specifications of observed systems, such as endpoints, operating systems, SBOM, and firewall rules, could enhance both detection rates and response effectiveness. On the other hand, to provide explainable threat response, deployed LLMs should be equipped with access to function calls. Function calling, also known as tool use or API calling, enables LLMs to interact with external systems [25]. By providing the LLM with a predefined set of functions or tools along with usage instructions, it can intelligently select and invoke appropriate functions to address a given threat scenario. For instance, it could propose specific functions with parameters to temporarily modify firewall rules. To evaluate the progress of future research in this area, it is essential to develop a methodology for quantitatively assessing the explainability of LLMs to reason on NIDS alerts. Currently, datasets necessary to fuel this methodology do not exist, highlighting future research.\nWhile LLMs show promise as assistants for providing explanations, they require further research and development before they can be considered reliable in this capacity. Future work shall also focus on LLMs' understanding of network traffic, as it is crucial for their potential role as a virtual system administrator, complementing NIDS. This research would build upon the work of Donadel et al. [26], evaluating their understanding of network topology."}, {"title": "6 CONCLUSION", "content": "Our investigation into the application of LLMs for NIDS has revealed notable limitations in their performance as primary solutions. If used as a solution to detect threats based on NetFlows, future research shall rather focus on transformer-based architectures without relying on pre-trained LLMs. LLMs currently struggle to effectively detect domain-specific malicious NetFlows and have high inference time complexity, making them impractical as standalone NIDS solutions. Given these findings, we propose that LLMs should be harnessed as complementary solutions to existing state-of-the-art NIDS. Specifically, their strengths in providing explainability make them valuable when integrated with traditional ML-based systems, particularly for generating detailed and contextually relevant explanations when alerts are raised. This approach leverages the potential of LLMs to enhance the interpretability and transparency of NIDS, thus aiding in threat response and decision-making processes. While challenges such as hallucination and logical reasoning limitations persist, the promising results in explainability underscore the importance of further exploring LLMs to finally render NIDS explainable."}]}