{"title": "Shavette: Low Power Neural Network Acceleration via Algorithm-level Error Detection and Undervolting", "authors": ["Mikael Rinkinen", "Lauri Koskinen", "Olli Silven", "Mehdi Safarpour"], "abstract": "Reduced voltage operation is an effective technique for substantial energy efficiency improvement in digital circuits. This brief introduces a simple approach for enabling reduced voltage operation of Deep Neural Network (DNN) accelerators by mere software modifications. Conventional approaches for enabling reduced voltage operation e.g., Timing Error Detection (TED) systems, incur significant development costs and overheads, while not being applicable to the off-the-shelf components. Contrary to those, the solution proposed in this paper relies on algorithm-based error detection, and hence, is implemented with low development costs, does not require any circuit modifications, and is even applicable to commodity devices. By showcasing the solution through experimenting on popular DNNs, i.e., LeNet and VGG16, on a GPU platform, we demonstrate 18% to 25% energy saving with no accuracy loss of the models and negligible throughput compromise (<3.9%), considering the overheads from integration of the error detection schemes into the DNN. The integration of presented algorithmic solution into the design is simpler when compared conventional TED based techniques that require extensive circuit-level modifications, cell library characterizations or special support from the design tools.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep Neural Networks (DNNs) and DNN based Large Language Models (LLMs) are gaining fast popularity for consumer and industrial application such as 6G telecommunications [1]. Current realization of DNNs mostly rely on Al accelerator in data centers. Hence, data centers are facing a growing challenge due to these ever-increasing demands that are sharply driving up power consumption, putting a strain on sustainability and presenting a significant financial burden. For example, consider LUMI (HPE Cray EX), at the time of writing, Europe's fastest supercomputer. It serves as an Al powerhouse equipped 11,912 AMD Radeon Instinct MI250X GPUs, packing a combined punch of 144 million cores with peak power consumption of 8.5 MW. A marginal power saving on those infrastructures would result in substantial savings of operational costs. This calls for innovative power management strategies to address these issues.\nOne such approach involves operating at reduce voltages [2]. Lowering voltage quadratically reduces power consumption, e.g., a mere 20% voltage reduction can cut the energy consumption to half. At the vicinity of the threshold voltages of the transistors, Vth, energy efficiency gains of 10x to 100x are provisioned [3].\nVoltage reduction usually leads to lower frequency and performance loss [2], [3]. However, manufacturers often include a safety margin, typically around 10% to 20%, in their recommended voltage, implying the device might function with the same performance at even lower voltages [4]. This margin accounts for potential variations in temperature, voltage, and process conditions. Studies have found different margin values: 10% in commercial CPUs [4], 20% in Field-Programmable Gate Arrays (FPGAs) [5], and around 10% in GPUs [6], [7]. If those voltage margins are eliminated safely, substantial power savings without throughput lost,i.e., frequency down-scaling, is achievable [6]. However, it is difficult to remove margins in a manner that no timing errors in logic circuit or bitflip in memory systems are induced. Furthermore, the margin size varies due to Process, Voltage and Temperature (PVT) variations from die to die and as a function of time for the same chip, i.e., due to aging. Hence, modeling the processor for optimal operating point, that is the lowest working voltage and highest clock frequency, is very challenging. GPUs in particular experience larger voltage variation, e.g., due to IR drop [8], which needs to be accounted for when the voltage is scaled down.\nTraditionally, voltage margin trimming techniques, e.g., Razor [3], rely on adding extra hardware into the digital circuit. Those are not conventionally supported by Computer Aided Design (CAD) tools and add substantial design complexity and incur significant cost increase of testing and verification stages while mostly applicable for designs that require very low clock rates, e.g., smart watches, sensors, etc. [10].\nWe propose to add relatively simple modifications on top of the computations of DNN models to enable reliable voltage margin elimination of their digital accelerator. This paper, demonstrates possibility of substantial energy savings by leveraging algorithmic fault detection [11] for guiding the Dynamic Voltage and Frequency Scaling (DVFS) unit on a commodity GPU, to eliminate the voltage margins and set the"}, {"title": "II. BACKGROUND", "content": "Two of the most investigated hardware-based techniques for enabling reduced voltage operation are briefly discussed here.\nA simple option is to use adaptive delay monitoring circuits, e.g., via ring oscillators or delay chains. This solution offers wider applicability but suffers from unreliability and does not ensure optimal voltage-frequency estimates [12].\nOther options are based on inserting Timing Error Detection (TED) based systems [3], like Error Detection Sequences (EDS), as is depicted in Fig. 1, into the critical paths of the circuit to monitor for any timing errors in-situ. As shown in Fig. 1, a set of extra secondary flip-flops clocked with a slight delay compared to the primary set of flip-flops are the end of combinational logic paths. By detecting a late arrival signal, possibly due to reduced voltage operation, the timing errors become detectable. The detections from TED system are fed back into the voltage regulator to adjust the voltage such that minimum voltage is selected while no errors are observed. Although TED based solutions, e.g., the well-known Razor [3], enable significant voltage reduction, they come with high development complexities and overheads as those techniques cause significant hiccups in semiconductor design and manufacturing flow [13].\nFurthermore, the hardware-based techniques are only implementable in pre-silicon stage [2], [14], whereas the proposed solution in this work can be applied even to commodity accelerators, as it is demonstrated in section IV."}, {"title": "B. Algorithm Based Error Detection", "content": "As an alternative to hardware-based error detection, the proposed solution in this paper relies on Algorithm Based Fault Tolerance (ABFT) error detection. ABFT was first introduced in 80s by Huang and Abraham [11] for detection and correction of errors in large matrix operations. Later on ABFT was extended for detection of errors in other computations such as Fourier transform [15] and convolutions [16], [17].\nIn the following, the ABFT schemes employed for the proposed solution are briefly described. While ABFT is not a general-purpose approach, it excels at detecting errors in computations that are dominantly found in DNN models.\n1) ABFT for matrix multiplications: The original ABFT scheme [11] shown in Fig. 2, detects errors in matrix operations by adding checksum property in input matrices and the verifies correctness of computations by inspecting the checksum property in output matrices. The overhead ratio from adding ABFT is O(1/N), with N being matrix size, hence the larger the input matrices are, the smaller the overhead becomes [14].\n2) ABFT for convolutions: We explored two approaches for ABFT in convolution operations. The method presented in [18] generates a single input checksum value for comparison against the sum of all convolution layer outputs. This involves calculating the sum of input values multiplied by specific weight matrix values, then multiplying this sum by corresponding weight sums. The results are summed to create the input checksum value. The method in [16] creates a weight sum matrix and applies it to the input. The resulting convolution output is compared against a sum matrix from the original convolution output, where the sum is taken from each channel. The first method requires fewer calculations for error detection but involves more complex checksum calculations, making optimization for multi-processor platforms difficult. We opt for the second method due to its superior GPU core utilization opportunities in practical implementations.\n3) Detecting errors in non-linear operations: The ABFT schemes only work for linear operations such as matrix, convolution and transformations. However, a DNN consists of non-linear layers, i.e., activation function [19] and different types of pooling. To detect fault in those operations Double Redundant Module (DMR) approach was employed. DMR incurs large overheads. Fortunately, those non-linear operations are lightweight and contribute to just of a few percent of total computations. Hence, error detection through DMR adds tolerable overheads, when DMR is restricted to only non-linear layers."}, {"title": "III. PROPOSED SOLUTION: ALGORITHMIC DETECTION", "content": "As mentioned earlier, for enabling error-free computations at reduced voltages an run-time error detection mechanism is essential. In our implementation inference is carried out in the GPU used as DNN accelerator, while its voltage is controlled by the CPU. The inference output, i.e., DNN's prediction, along with checksums and DMR results are sent back to the CPU. The CPU checks for computational errors by verifying the correctness of linear and non-linear operations within the model by using methods described in section II-B. If no errors are observed the inference results from the neural network model is accepted, otherwise, e.g., the voltage/frequency is adjusted and inference operation is repeated."}, {"title": "A. Error Detection in CNNs through ABFT and DMR", "content": "To support error detection in CNN models, ABFT [11], [20], [16] is integrated in the linear layer convolutional and matrix computations and DMR in the non-linear layers. All layers are executed within undervolted GPU. The redundant module must be implemented uncorrelated to the original one, e.g., with different instruction set. This is to avoid correlated errors between two modules. In experiments it was \u201cobserved\u201d that errors always appear in linear operations before non-linear operations possibly due to exercising longer delay paths. This may render the protection for non-linear layers unnecessary. Nonetheless DMR was added to cover all layers against errors.\nThe voltage of GPU processor is dynamically adjusted to minimum error free operating voltage, without incurring errors in computations, neither degrading network accuracy nor causing system crash by timely detection of the Point of First Failure (PoFF). No hardware is involved and hence the solution becomes even applicable to the commercial of-the shelf components [21].\nABFT overhead is sub-linear with respect to the size of the model. As analyses in [11] and [16] show, when matrix and convolution sizes increase, the overhead ratio from ABFT decreases. However, this is not the case for DMR based error detection in non-linear layers."}, {"title": "IV. IMPLEMENTATION", "content": "The experiments were carried out on desktop PC with an AMD Radeon RX 5600 XT GPU. The graphics cards support direct software based adjustment of the operating voltage of all subsystems together, with granularity of few millivolts. With other investigated platforms voltage control required access to APIs not available to the regular user. The CPU was assigned for checksum verification and operating point control.\nThe chosen GPU necessitated using the OpenCL framework. Due to the lack of support from commonly used DNN development tools for OpenCL, e.g., TensorFlow, we programmed the AI models from scratch in C++ and OpenCL. Two well-known CNN models each with different combinations of operation types, VGG16 [22] and LeNet [19], were selected for the implementations.\nTo detect errors in linear parts, i.e., the convolutional and fully-connected layers, the convolution and matrix operations kernels were slightly modified. For ABFT less than 100 lines of C++ code was needed to incorporate ABFT schemes introduced in [16]. With non-linear layers the DMR modules used different code to avoid the same faults appearing in both, resulting in incorrect voting results.\nABFT checksum for each layer was calculated on-thefly within the GPU. The checksums of weights were precomputed for each model to minimize overheads. For inference, pre- computing the input checksums for weights is possible, however, training obviously requires updating the weights and hence re-computing the wight checksums.\nIt is possible to use error correcting feature of ABFT, but adjustment of operating points and re-computations based on error detection capability of ABFT was preferred."}, {"title": "V. RESULTS", "content": "The OpenCL implemented models were executed on the GPU to investigate the potential of the proposed solution for enabling reduced voltage operation and investigating the energy efficiency gains and reliability of the method.\nThe scheme in our experimentation was to keep the clock frequency and reduce the operating voltage until errors start appearing. Once errors detected the voltage can be retracted to slightly higher voltage. However, we continued reducing voltage to detect both the PoFF and point of crash to assess the error detection capability. PoFF voltage turned out to be much higher than crash voltage, hence, the solution alarms voltage governor, way before approaching the crash point. Adding ABFT to DNNs models provides for very high reliability under voltage scaling, e.g., up to around 100% [16], [17]. In all experiments, the on-chip temperature sensor report was fluctuating around 55\u00b0C."}, {"title": "A. Power Saving", "content": "A particularly interesting issue is the dynamic power savings of the GPU. As the voltage scaling quadratically reduces power, we kept highest utilization possible and scaled down the voltage till an error is observed by ABFT, i.e., PoFF is detected. Once PoFF is determined one could stop reducing voltage, however, for characterization purposes we further reduced voltage down to the crash point. At the same time, the actual errors and impact of them as accuracy loss of the DNN one measured. The bar for reporting using ABFT was set high and hence the higher error rate reported by ABFT compared to actual error rate is from false positives, i.e., error report while there are no actual errors. One could set a more relaxed error threshold. Figure 2 depicts power savings trends while Fig. 3 plots the error rate and accuracy loss for different voltages. Notice, the PoFF and crash points changes for different clock frequencies. LeNet results were not plotted. The power drops from around 30W down to 25W at 812mV (PoFF for LeNet) and 22W at around 805mV (crash point at 1780MHz) for LeNet inference at 1780MHz. Measurements results for power are averaged out of 120 inference experiments per operating points taken within 0.5s intervals."}, {"title": "B. Error Detection", "content": "Figure 3, depicts the errors appearing in computations, i.e., actual error rate, error rate from ABFT detections and overall prediction errors (accuracy loss) of the model. Since error propagate from one layer to another, comparing ABFT error detection rate against actual errors per each operation is misleading. Experiments on single operation, shows very high (close to 100%) computational detection rate for ABFT, similiar to results on in [5]. The output error was reported if the difference between the GPU accelerated outputs and the reference was over 1e-13. The error threshold can be made slightly tighter to ensure more reliability, however, that would result in false positives being detected constantly, even at stock voltage and frequency settings. The DNN accuracy was compared against baseline (not undervolted). Notice, the accuracy of model does not degrade until the computational error rate grows substantially. Due to inherent fault-tolerance of DNNs a few hundred errors at vicinity of PoFF does not impact the accuracy. Nonetheless, since ABFT detects the computational errors early enough, we do not need to rely on fault-tolerance feature of DNNs for reliability.\nAs expected the errors appear in linear layers significantly before being detected in the non-linear ones. We speculate the possible explanation is that the circuit delay paths for nonlinear operations are much shorter than those of floating-point multipliers used for arithmetic computations."}, {"title": "C. Overheads", "content": "The power consumption at all voltages with ABFT enabled is slightly lower (around 1%) than when it is disabled, meaning ABFT overheads manifest in inference time rather than power. That is likely due to ABFT related calculations imposing more idle periods on processing units. As Table I shows, this translates into slightly longer inference time. Hence, we can get an overall energy overhead estimate by comparing the energy per inferences between ABFT enabled and disabled runs. Table II provides a summary of overheads in terms of inference time.\nNaive integration of ABFT into the DNN's model results in large overheads, however, with small tweaks and code optimization the overhead share becomes insignificant, e.g., < 3.9% for VGG-16.\nAs mentioned earlier, the overhead ratio of adding ABFT grows sub-linearly with respect to the model size, hence, for LeNet-5 we observe larger overheads compared to VGG-16 due to the smaller model size.\nHere, we mainly consider VGG-16 for analysis due to it larger model size compared to LeNet and it is more representative of models implemented in real world in data centers. Considering VGG-16 with default clock frequency and voltage (at 1780 MHz and 960 mV), the energy overhead of ABFT is as low as 1%. With same frequency but voltage set to 880 mV, the energy overhead is almost the same, i.e., 1.3%. Setting the frequency to the lowest, i.e., 1680 MHz, and at the lowest stable around 805 mV, the energy overhead of ABFT enabled is slightly higher, i.e., 1.9%. This is a small penalty for saving almost a quarter of the energy of each inference."}, {"title": "VII. DISCUSSION AND FUTURE WORKS", "content": "We showcased the solution for two DNN models running on commodity GPU used as an AI accelerator. Although, we implemented the models from scratch, the necessary modifications can be made to the DNN automatically using development tools such as TensorFlow or Keras [16] and inserting the ABFT schemes into the models.\nIn a data center the Power Usage Effectiveness (PUE) is always above 1.0 and typically around 1.3 or even 1.8, i.e., 30% to 80% more power dissipated to support computing equipment. Removing voltage margins not only provides for direct power saving in the accelerators, but also leads to indirect cost savings coming from lower costs of power delivery network, lower demands of cooling and ultimately higher life time.\nThis work demonstrated possibility of utilizing ABFT for reducing power consumption of DNN inference. However, this work can be extended for training as well [16]."}, {"title": "VIII. CONCLUSION", "content": "This study demonstrates the existence of a significant voltage margin in the default settings of a commercial GPU and possibility of reliably removing it. In computations of DNN models the margin can be safely optimized by incorporating ABFT schemes. For the used VGG-16 The time and energy overheads from ABFT are low, e.g., around 3.5% and 1.1% respectively. Nearly a quarter of energy could be saved without sacrificing accuracy of the neural network. One of the issues that was verified in the context of this paper is that when the voltage is reduced the control path fails long after the data path. Nonetheless, the control path can be aggressively pipelined or be operated at slightly higher voltage such that it is ensured that data path always fails before control path. We believe the results indicate a promising avenue for enhancing the energy efficiency of Al accelerators, without costly circuit techniques, cell characterization or having to hack CAD tools."}]}