{"title": "Enhancing Unimodal Latent Representations in Multimodal VAEs through Iterative Amortized Inference", "authors": ["Yuta Oshima", "Masahiro Suzuki", "Yutaka Matsuo"], "abstract": "In recent years, deep generative models for multimodal data have gained significant attention. Among these, multimodal variational autoencoders (VAEs) have emerged as a promising approach, aiming to capture a shared latent representation by integrating information across different modalities through their inference models. A primary challenge for multimodal VAEs is accurately inferring representations from arbitrary subsets of modalities after learning a multimodal inference model. Naively, this would require training 2M different inference networks (M is # of modalities) to handle every possible combination of modalities, which is infeasible for a large number of modalities. Mixture-based models address this challenge by requiring only as many inference models as there are modalities, aggregating unimodal inferences to perform multimodal inference. However, when modalities are missing, these models suffer from information loss, particularly of modality-specific information, leading to deteriorated inference performance. Alternatively, alignment-based multimodal VAEs aim to align unimodal inference models with a multimodal inference model by minimizing the Kullback-Leibler (KL) divergence between them. Yet, the multimodal amortized inference, which is alignment source in these models inherently suffers from amortization gaps, preventing it from perfectly approximating the true inference and compromising the accuracy of unimodal inference. To address both issues, we introduce an iterative amortized inference mechanism within the multimodal VAE framework, termed multimodal iterative amortized inference. By iteratively refining the multimodal inference using all modalities, this method overcomes the information loss due to missing modalities in mixture-based models and minimizes the amortization gap in alignment-based models. Furthermore, by aligning the unimodal inference to approximate this refined multimodal posterior, we obtain unimodal inferences that effectively incorporate multimodal information while requiring only unimodal inputs at inference time. Experimental results on two benchmark datasets demonstrate that the proposed method improves the performance of the inference itself, suggested by higher linear classification accuracy and cosine similarity, and that the learned representations effectively capture the distributions of other modalities, as indicated by lower Fr\u00e9chet Inception Distance (FID) scores in cross-modal generation. This indicates that the proposed approach significantly enhances the inferred representations from unimodal inputs.", "sections": [{"title": "1 Introduction", "content": "Humans acquire multimodal information from the world, deepening their understanding. This highlights the importance of multimodal data processing, a crucial aspect of artificial intelligence aimed at comprehending the complexities of our environment [1]. In self-supervised multimodal learning, variational autoencoders (VAEs) [2] have gained prominence, leading to the development of their multimodal variants, termed multimodal VAEs [3, 4]. VAEs are adept at encoding inputs into latent representations by learning the inference of latent variables through their encoder-decoder architecture. When provided with all modalities, they can infer latent representations from multimodal inputs.\nHowever, a key challenge in multimodal VAEs arises when a modality is missing; the inference collapses if we attempt to infer the latent representation from only a subset of modalities [3]. This collapse occurs because the inference network is approximated by a neural network that expects all modalities as input. To address this issue, one might consider designing separate inference networks for each possible subset of modalities to handle missing inputs. However, to infer from any combination of M modalities, a naive approach would require training 2M encoders, leading to an exponential increase in training cost with the number of modalities.\nRecent models, known as mixture-based models, such as MVAE [5], MMVAE [6], and MoPoE-VAE [7], allow for inference from any combination of modalities using only M inference models by aggregating unimodal inferences through mechanisms such as PoE [8], MoE, and MoPoE (Figure 8a). However, these models rely on sub-sampling modalities during training, which imposes a theoretical limitation on the performance of inference from subsets and leads to information loss due to missing modalities. Experimental results have demonstrated that cross-modal generation performance from these inferred representations deteriorates [9].\nSome studies called alignment-based multimodal VAEs address this by learning inferences from all modalities (i.e., multimodal inference) and optimizing unimodal inferences to minimize the Kullback-Leibler (KL) divergence between them and [3, 10] (Figure 8b). This approach avoids the theoretical limitations associated with subsampling in mixture-based models. Alignment-based"}, {"title": "2 Preliminary", "content": ""}, {"title": "2.1 Multimodal VAES", "content": "The purpose of multimodal VAEs is to maximize the likelihood \\(p_{\\theta}(X)\\) with respect to the parameter \\(\\theta\\) under the given M types of multimodal inputs \\(X = \\{x_m\\}_{m=0}^{M-1}\\), where the marginal log-likelihood is defined as \\(p_{\\theta}(X) = \\int p_{\\theta}(X|z)dz = \\int \\prod_{m:x_m \\in X} p_{\\theta}(x_m|z)dz\\) [4]. Here, z is the latent variable, the shared representation in"}, {"title": "2.2 Iterative Amortized Inference", "content": "Amortized variational inference refers to a method of performing variational inference by optimizing shared parameters across the entire dataset rather than optimizing the parameters of the approximate distribution for each data point. Amortized variational inference can significantly reduce computational costs when using approximate distributions that are costly to optimize, such as neural networks. However, it is known that this approach can lead to discrepancies with inferences optimized for individual data points, resulting in inferior inference accuracy. This discrepancy is referred to as the amortization gap [11]. VAEs are also subject to the negative effects of the amortization gap. This is because the inference model is approximated by a neural network \\(q_{\\phi}(z|x)\\), sharing parameters across the entire dataset, and employing amortized variational inference.\nIterative amortized inference is an algorithm that improves inference accuracy by iteratively updating the inference in amortized variational inference, thereby reducing the amortization gap. It is known that by applying iterative amortized inference to single-modality VAEs, the reduction in inference accuracy caused by the amortization gap can be improved. Iterative amortized inference is used in scenarios with challenging inference, such as object-centric representation learning [15].\nFor updating the inference, gradients of the latent variables obtained by backpropagating the error from the VAE's loss function and the differences between the input and reconstruction are used. This update is expressed as follows, where the input to the inferrer is x, the mean of the latent variables at iteration t is \\(\\mu_t\\), the standard deviation is \\(\\sigma_t\\), and the gradients of the mean and standard deviation of the latent variables by ELBO (\\(\\mathcal{L}\\)) are \\(\\nabla_{\\mu_t} \\mathcal{L}\\), \\(\\nabla_{\\sigma_t} \\mathcal{L}\\), respectively. A parameterized function \\(f_w\\) is used for the update:\n\\[\\mu_{t+1}, \\sigma_{t+1} = f_w(x, \\mu_t, \\sigma_t, \\nabla_{\\mu_t} \\mathcal{L}, \\nabla_{\\sigma_t} \\mathcal{L}).\\]"}, {"title": "3 Methods", "content": "To acquire a unimodal inference model \\(q_{\\lambda_m}(z|x_m)\\) that approximates the true multimodal inference \\(p_{\\theta}(z|X)\\), alignment-based multimodal VAEs perform a two-stage approximation. This approach avoids the limitations of mixture-based multimodal VAEs, which suffer from information loss due to missing modalities. However, in the first stage of approximation, the true multimodal inference \\(p_{\\theta}(z|X)\\) is approximated by an amortized variational inference model \\(q_{\\phi}(z|X)\\), which contains errors from the approximation gap of variational inference and the amortization gap inherent in amortized inference.\nTo acquire a unimodal inference model that closely approximates the true multimodal inference \\(p_{\\theta}(z|X)\\), it is necessary to reduce these gaps. Specifically, our study focuses on reducing the amortization gap, introducing iterative amortized inference in the first stage of approximation. By doing so, we aim to improve the approximation accuracy of the multimodal inference and, consequently, enhance the unimodal inference.\nHowever, iterative amortized inference alone cannot perform inference from truly unimodal inputs because the calculation of the multimodal ELBO requires information from all modalities. Our goal is to perform inference that incorporates multimodal information using only unimodal inputs. To achieve this, we align the unimodal inference \\(q_{\\lambda_m}(z|x_m)\\) with the multimodal iterative amortized inference \\(q_{\\phi_T}(z_T|x_m)\\). By minimizing the Kullback-Leibler (KL) divergence between them, we bring the unimodal inference closer to an inference process that overcomes both the information loss due to missing modalities and the amortization gap.\nThus, the proposed method is presented from two perspectives: (1) multimodal iterative amortized inference: improvement of multimodal inference by iterative amortized inference, which overcomes both the information loss due to missing modalities in"}, {"title": "3.1 Multimodal Iterative Amortized Inference", "content": "Applying iterative amortized inference to multimodal inference improves the amortization gap present in the approximation of the true multimodal inference \\(p_{\\theta}(z|X)\\) by \\(q_{\\phi}(z|x_m)\\). In our method, we explicitly input only a single modality \\(x_m\\), and then we perform iterative amortized inference using all modalities X during training. This approach leverages information from all modalities to refine the inference, even when only a single modality is available as explicit input.\nTo achieve this, we prepare a generator \\(p_{\\theta}(X|z)\\) and an iterative amortized inference function \\(f_w\\). We can improve the inference by utilizing information from all modalities (see the upper part of Figure 1) by performing iterative amortized inference using the gradient of the multimodal ELBO formulated as follows:\n\\[\\mathcal{L}(\\theta, \\phi; X) = E_{q_{\\phi}(z_t|x_m)}[\\log p_{\\theta}(X|z_t)] - D_{KL}[q_{\\phi_m}(z_t|x_m)||p(z)].\\]\nIn this formulation, even though the inference model \\(q_{\\phi_m}(z_t|x_m)\\) takes only \\(x_m\\) as input, the reconstruction term \\(\\log p_{\\theta}(X|z_t)\\) involves all modalities X. Therefore, the iterative updates incorporate information from all modalities, allowing us to recover information from missing modalities and reduce the missing modality gap. Therefore, we not only reduce the amortization gap in the multimodal inference approximated by a neural network but also overcome the information loss due to missing modalities. The update of multimodal iterative amoritzed inference is formulated as follows:\n\\[\\mu_{m_{t+1}}, \\sigma_{m_{t+1}} = f_w(x_m, \\mu_{m_t}, \\sigma_{m_t}, \\nabla_{\\mu_{m_t}}\\mathcal{L}, \\nabla_{\\sigma_{m_t}}\\mathcal{L}).\\]\nBy repeating this step T times, we obtain \\(\\mu_{m_T}, \\sigma_{m_T}\\), and sample latent variables \\(z_T\\) from \\(q(z_T|x_m) = \\mathcal{N}(\\mu_{m_T}, \\sigma_{m_T})\\), allowing the inference of a shared representation from modality m that incorporates multimodal information."}, {"title": "3.2 Alignment of Unimodal Inference with Iterative Amortized Inference", "content": "In multimodal iterative amortized inference, improving inference relies on the multimodal ELBO, which necessitates access to all modalities during computation. However, our goal is to perform inference that incorporates multimodal information using only unimodal inputs. To achieve this, we align the unimodal inference \\(q_{\\lambda_m}(z|x_m)\\) with the multimodal iterative amortized inference, thereby obtaining the desired unimodal inference (see the lower part of Figure 1). Our ultimate goal is to acquire a unimodal inference model that closely approximates the true multimodal inference \\(p_{\\theta}(z|X)\\)."}, {"title": "4 Related Works", "content": "Research on multimodal VAEs is being conducted in various ways beyond what was mentioned above [4]. Early multimodal VAEs did not devise Aggregation methods like those seen in Mixed-based. Hence, an exponential number of models had to be prepared for the number of modalities. JMVAE [3] prepares inferences from two modalities and a single modality and learns unimodal inference by bringing them closer together. This method is equivalent to MVTCAE when the number of modalities is two, but MVTCAE differs in that it uses the PoE of unimodal inference for multimodal inference. Furthermore, TELBO [16] is proposed as the sum of ELBO for all combinations of modalities, and as further studies, M\u00b2VAE [17] combining JMVAE and TELBO, and VAEVAE [18], which excludes the term of KL divergence between multimodal inference and prior distribution, have been proposed.\nIn addition, some studies introduce modality-specific latent variables [12, 19-23] or hierarchical latent variables [24-26]. Since our study aims to acquire shared representations of all modalities from a single modality, these studies complement ours.\nRecent models dealing with multimodal information include those that use large architectures (e.g., Transformer [27]) and a large amount of data. Conditionally generated models are actively being studied, and, for example, DALL-E2 [28] and Imagen [29] enable high-quality image generation conditioned on language. However, these models focus on generation rather than multimodal representation learning. Studies on representation learning from multimodal information include CLIP [30] and MultiMAE [31]. In CLIP, images and languages are encoded with their respective encoders and then learned to be close in the representation space through contrastive learning. MultiMAE is conducting representation learning and cross-modal generation against diverse image information with a large-scale model based on the Vision Transformer [32], which allows cross-modal generation. Still, it is not aimed at acquiring shared representations of modalities."}, {"title": "5 Experiments", "content": "In this study, we used the MoPoE-VAE [7] as the mixture-based model, employing MoPoE for combining the multimodal distributions. For the alignment-based model, based on prior work [10], we used a VAE that integrates multimodal information using the Product of Experts (PoE) as the alignment source model. Moreover, since we wanted to separately examine the properties of both the alignment source and the target unimodal inference in our proposed method, we conducted training in two stages: first learning the alignment source distribution, and then learning the acquisition of unimodal inference through distribution alignment.\nFor the network architecture, we followed [12] and [9], setting the dimension of the latent variable to 1024 based on the dimensions used for SVHN in the iterative amortized inference paper [33]. This architectural setting was shared across all experiments. Implementation details of multimodal iterative amoritzed inference are shown in Appendix A. For all models, we used the Adam optimizer [34] with a learning rate of 0.0002. Additionally, we employed a learning rate scheduler using ExponentialLR with a gamma of 0.98. The batch size was set to 256 for MNIST-SVHN-Text and 128 for CUB. Training was performed for 100 epochs on MNIST-SVHN-Text and 200 epochs on CUB.\nWe conducted experiments using the widely used multimodal datasets MNIST-SVHN-Text [12], composed of three modalities, and CUB [13], consisting of two modalities. MNIST-SVHN-Text is a dataset related to digits from 0 to 9, and the three constituting modalities are MNIST, a 28 \u00d7 28 grayscale image of digits; SVHN, a 3 x 32 x 32 RGB image of digits; and Text, the alphabetical notation of numbers randomly placed within an 8-word frame. CUB is a dataset with bird images and their captions. CUB images were compressed to 3 \u00d7 64 \u00d7 64. Each dataset was split into training, validation, and test sets. We used the validation set to monitor the training process and select models. For the second stage of training in alignment-based models and our proposed method, the number of epochs remained the same as in the first stage. However, we selected the model where the KL divergence between the alignment source and the target unimodal inference on the validation data was minimized."}, {"title": "5.1 Effectiveness of Multimodal Iterative Amortized Inference", "content": "To demonstrate that multimodal iterative amortized inference can compensate for missing modality information and reduce the amortization gap, we conducted experiments using the original undistilled model. For both datasets, the number of iteration T = 8 in training time of the proposed method.\nFirst, we present qualitative results of cross-modal generation to demonstrate how our proposed method addresses the information loss due to missing modalities inherent in mixture-based models. We apply multimodal iterative amortized inference to the unimodal inference model \\(q_{\\phi_m}(z|x_m)\\), where the input is unimodal. Figure 2 and Figure 3 shows generated images from text inputs with different numbers of iterations T = 1,2,4,8. When T = 1 (i.e., no iterative updates), the cross-modal generation may fail to capture detailed information from the missing image modality, resulting in poor-quality images. However, as we increase T, the quality of the generated"}, {"title": "5.2 Aligning Unimodal Inference with Iterative Amortized Inference", "content": "In multimodal iterative amortized inference, improving the inference relies on the multimodal ELBO, which requires access to all modalities during computation. However, our goal is to perform inference that incorporates multimodal information using only unimodal inputs. To achieve this, we align the unimodal inference \\(q_{\\lambda_m}(z|x_m)\\) with the multimodal iterative amortized inference, thereby obtaining a unimodal inference model that closely approximates the true multimodal inference. We set the number of iterations T = 8 for all settings and evaluate the effectiveness of our approach by"}, {"title": "6 Limitation and Conclusion", "content": "This study found that multimodal iterative amortized inference can improve not only the amortization gap but also the missing modality information loss caused by modality sub-sampling. By aligning multimodal iterative amortized inference with unimodal inference, we were able to obtain an improved unimodal inference model. The results demonstrated that our proposed method improves the performance of the inference itself, as evidenced by higher linear classification accuracy and greater cosine similarity of latent representations. Additionally, the representations learned by our method effectively capture the distributions of other modalities, which is reflected in favorable FID in cross-modal generation."}, {"title": "A Implementation Details of Multimodal Iterative Amortized Inference", "content": "In our experiments, we implemented multimodal iterative inference models following the settings of [33]. The model iteratively updates the mean \\(\\mu_m\\) and log-variance \\(\\log \\sigma_m\\) of the latent distribution by incorporating information from the input data \\(x_m\\) and the gradients of the multimodal ELBO with respect to these parameters.\nAt each iteration t, the mean \\(\\mu_{m_t}\\) and log-variance \\(\\log \\sigma_{m_t}\\) are updated based on the input data \\(x_m\\), as well as the gradients \\(\\nabla_{\\mu_{m_t}}\\mathcal{L}\\) and \\(\\nabla_{\\log \\sigma_{m_t}}\\mathcal{L}\\), using the following function:\n\\[\\mu_{m_{t+1}}, \\sigma_{m_{t+1}} = f_w(x_m, \\mu_{m_t}, \\log\\sigma_{m_t}, \\nabla_{\\mu_{m_t}}\\mathcal{L}, \\nabla_{\\log \\sigma_{m_t}}\\mathcal{L}),\\]\nwhere \\(f_w\\) represents the iterative amortized inference model.\nInitially, we extract features from the input data \\(x_m\\) by applying a linear transformation followed by the exponential linear unit (ELU) activation function [36]:\n\\[h_x = ELU(W_x x_m + b_x),\\]\nwhere \\(W_x\\) and \\(b_x\\) are learnable parameters.\nNext, the gradients of the multimodal ELBO with respect to the current mean and log-variance, \\(\\nabla_{\\mu_{m_t}}\\mathcal{L}\\) and \\(\\nabla_{\\log \\sigma_{m_t}}\\mathcal{L}\\), are processed using layer normalization [37] to stabilize the training:\n\\[\\nabla_{\\mu_{m_t}}^\\mathcal{L} = LayerNorm(\\nabla_{\\mu_{m_t}}\\mathcal{L}), \\quad \\nabla_{\\log \\sigma_{m_t}}^\\mathcal{L} = LayerNorm(\\nabla_{\\log \\sigma_{m_t}}\\mathcal{L}).\\]\nThese normalized gradients are concatenated with the current estimates of \\(\\mu_{m_t}\\) and \\(\\log \\sigma_{m_t}\\) to form a combined feature representation:\n\\[h_{grad} = ELU(W_{grad} [\\mu_{m_t}, \\log \\sigma_{m_t}, \\nabla_{\\mu_{m_t}}^\\mathcal{L}, \\nabla_{\\log \\sigma_{m_t}}^\\mathcal{L}] + b_{grad}),\\]\nwhere [.] denotes concatenation, and \\(W_{grad}\\) and \\(b_{grad}\\) are learnable parameters.\nThe data feature \\(h_x\\) and the gradient feature \\(h_{grad}\\) are concatenated to form a combined representation:\n\\[h = [h_x, h_{grad}].\\]\nThis combined feature vector is passed through additional neural network layers to compute candidate updates for the mean and log-variance:\n\\[\\mu^\\prime = \\tanh(W h + b_{\\mu}), \\quad \\log \\sigma^\\prime = \\tanh(W h + b_{\\sigma}),\\]\nwhere \\(\\tanh\\) ensures bounded outputs, preventing numerical instability.\nTo determine the influence of these candidate updates on the current estimates, we apply gating functions computed with a sigmoid activation:"}]}