{"title": "Enhance Learning Efficiency of Oblique Decision Tree via Feature Concatenation", "authors": ["Shen-Huan Lyu", "Yi-Xiao He", "Yanyan Wang", "Zhihao Qu", "Bin Tang", "Baoliu Ye"], "abstract": "Oblique Decision Tree (ODT) separates the feature space by linear projections, as opposed to the conventional Decision Tree (DT) that forces axis-parallel splits. ODT has been proven to have a stronger representation ability than DT, as it provides a way to create shallower tree structures while still approximating complex decision boundaries. However, its learning efficiency is still insufficient, since the linear projections cannot be transmitted to the child nodes, resulting in a waste of model parameters. In this work, we propose an enhanced ODT method with Feature Concatenation (FC-ODT), which enables in-model feature transformation to transmit the projections along the decision paths. Theoretically, we prove that our method enjoys a faster consistency rate w.r.t. the tree depth, indicating that our method possesses a significant advantage in generalization performance, especially for shallow trees. Experiments show that FC-ODT can outperform the other state-of-the-art decision trees with a limited tree depth.", "sections": [{"title": "1 Introduction", "content": "Tree-based ensemble methods, such as Random Forest (Breiman, 2001; Geurts et al., 2006) and Gradient Boosting Decision Tree (Friedman, 2001; Chen and Guestrin, 2016; Ke et al., 2017), have gained popularity in high-dimensional and ill-posed classification and regression tasks (Vershynin, 2018), for example on causal inference (Wager and Athey, 2018; Doubleday et al., 2022), time series (Kane et al., 2014), or signal processing (Pal, 2005), but also for inference in applications such as image segmentation or object recognition in computer vision (Payet and Todorovic, 2012; Kontschieder et al., 2014). These methods are usually collections of decision trees with axis-aligned splits, such as CART (Breiman et al., 1984, Section 2.2) or C4.5 (Quinlan, 1993), that is, the trees only split along feature dimensions, due to their computational efficiency and ease of tuning.\nHowever, the axis-parallel split methods often require very deep trees with complicated step-like decision boundaries when faced with high-dimensional data, leading to increased variance. Therefore, Olique Decision Tree (ODT) (Breiman et al., 1984, Section 5.2) is proposed to use oblique decision boundaries, potentially simplifying the boundary structure. And axis-parallel decision trees can be considered a special case of ODT when the oblique projection direction is only selected from the set of basis vectors. Theoretically, ODT has been proven to have stronger representation capabilities and the potential to achieve better learning properties (Cattaneo et al., 2024).\nThe major limitations of ODT are the excessive number of model parameters in decision paths and high overfitting risk in deep nodes (Cattaneo et al., 2024). In fact, the number of parameters required for the learning model to achieve a certain level of performance characterizes the learning efficiency of the algorithm. Although variant ODTs (Murthy et al., 1994; Brodley and Utgoff, 1995) use linear combinations of features in each node, they do not transmit projection information to the child nodes. The way of retraining in each node wastes the model parameters we invested in the projection selection, leading to the insufficient learning efficiency of ODT. On the other hand, as the tree grows, the number of samples in deep nodes rapidly decreases. ODT ignores the information obtained from the previous projection selection and retrains the linear model with limited samples, which can lead to severe overfitting risk (Shalev-Shwartz and Ben-David, 2014).\nPrevious studies often attempt to deal with these limitations via optimization but ignore the impact of wasted projection information. For example, Zhu et al. (2020) utilize the mixed-integer optimization (MIO) strategy to reduce the projection parameters by L\u00b9 regularization. L\u00f3pez-Chau et al. (2013) and Tomita et al. (2020) reduce the prediction variance of linear models by introducing L\u00b2 regularization terms or randomization, thus alleviating the overfitting risk. However, these methods can only deal with the learning process in each node separately and cannot consider the relationship between nodes in different layers. To overcome this challenge, we must note that, the transmission of projection information between nodes in different layers plays a critical role.\nIn this work, we propose an enhanced Oblique Decision Tree, FC-ODT, that leverages a Feature Concatenation mechanism to improve learning efficiency. This mechanism facilitates a layer-by-layer feature transformation during the tree's construction so that the optimized projection information in the node can be transmitted to its child nodes. Meanwhile, the inductive bias brought by the concatenated features combined with the ridge regression"}, {"title": "2 Related Work", "content": "2.1 Oblique Decision Tree\nODT alleviates the problem of high variance of DT in high-dimensional settings, but faces extremely high complexity at each node to find the optimal split and suffers the overfitting risk at the deep node. To deal with these challenges, Breiman et al. (1984) first use a fully deterministic hill-climbing algorithm to search for the best oblique split. Heath et al. (1993) and Murthy et al. (1994) propose combining random perturbations and hill-climbing algorithm to search for the best split, potentially avoiding getting stuck in local optima. Recently, Bertsimas and Dunn (2017) and Zhu et al. (2020) introduce the MIO strategy to further improve the efficiency of solving projection directions. Unlike these deterministic approximation algorithms, another more interesting and practical research direction is to generate candidate projections through data-driven methods. One possibility is to use dimensionality reduction techniques, such as PCA (Rodriguez et al., 2006; Menze et al., 2011) and LDA (Li et al., 2003; L\u00f3pez-Chau et al., 2013). Tomita et al. (2020) show that sparse random projections or random rotations can also be introduced by incorporating. Recently, some studies have extended ODTs to unsupervised learning frameworks such as clustering, demonstrating its advantages in representation ability (Stepi\u0161nik and Kocev, 2021; Ganaie et al., 2022). However, the explanation for their success is largely based on heuristics, until Cattaneo et al. (2024) demonstrate the consistency rate of excess risk for individual ODT."}, {"title": "2.2 Feature Concatenation", "content": "Deep Forest (Zhou and Feng, 2017) successfully constructs non-differentiable deep models by implementing feature concatenation mechanisms that enable in-model feature transformation based on decision trees. This mechanism has been theoretically proven to effectively improve the consistency rate of tree-based ensembles (Arnould et al., 2021; Lyu et al., 2022b). Chen et al. (2021) utilize the decision path of trees in the forest to generate oblique feature representations, which has been proven to effectively alleviate the risk of overfitting caused by feature redundancy (Lyu et al., 2022a). In addition, feature concatenation also has strong scalability and can adapt to different learning tasks by screening concatenated features. Recent research has expanded the tree-based deep models to some specific settings, such as multi-label learning (Yang et al., 2020) and semi-supervised learning (Wang et al., 2020). Although feature concatenation has been widely used in ensemble learning, this work is still the first to introduce it in tree construction."}, {"title": "3 Preliminary", "content": "In this work, we first describe the setting and notations related to tree-based estimators.\n3.1 Setting\nWe consider the regression setting, where the training set Sn consists of [0, 1]d \u00d7 R-valued independent random variables distributed as the prototype sample from a joint distribution P(x, y) = P(x|y)P(x) supported on X \u00d7 Y. This conditional distribution can be written as\n$y = f(x) + \\epsilon$,\nwhere f(x) = E[y|x] is the conditional expectation of y given \u00e6, and e is a noise satisfying E[e] = 0 and Var[e] < \u03c3\u00b2. The task considered in this work is to output a tree-based estimator hn(\u00b7, T, Sn): [0,1]d \u2192 R, where T is a tree structure dependent on the training set Sn. To simplify notation, we denote hr,n(x) = hn(x,T, Sn). The quality of a tree-based estimator ht,n is measured by its mean-of-squares error (MSE)\n$R(h_{t,n}) = E [(h_{t,n}(x) \u2013 f(x))^2]$,\nwhere the expectation is taken with respect to x, conditionally on Sn. As the training data size n increases, we get a sequence of estimators {hT,i}_{i=1}. A sequence of estimators {ht,n}_{n=1} is said to be consistent if R(hr,n) \u2192 0 as n \u2192 \u221e.\n3.2 Trees\nA decision tree is a data structure that is arranged and constructed in a top-down hierarchical manner using recursive binary partitioning in a greedy way. According to the CART methodology (Breiman et al., 1984), a parent node t (representing a region in X) within the tree is split into two child nodes, t\u2081 and tr, to maximize the impurity decrease measured in MSE\n$\\Delta(b, a, t) = \\mathbb{E}_{x \\in t}(Y_t - \\bar{Y}_t)^2 - \\mathbb{E}_{x \\in t} (Y_t - \\bar{Y}_{t_l} \\mathbb{1}(a^T x_i < b) - \\bar{Y}_{t_r} \\mathbb{1}(a^T x_i > b))^2$,"}, {"title": "4 The Proposed Approach", "content": "This section presents FC-ODT, whose key idea is to introduce a feature concatenation mechanism so that the layer-by-layer splitting process can achieve in-model feature transformation just like neural networks. FC-ODT can be not only practical but also a heuristic algorithm with provably better learning properties. It consists of three steps: feature concatenation, finding oblique splits, and tree construction, which are detailed in Algorithms 1-3.\n4.1 Feature Concatenation\nAs shown in Figure 1, when the parent node t of each level splits, it will learn the oblique decision rule a\u00afx < b. We will record the projected score \u1ef9t = a^T x as the augmented\n$X_t \\leftarrow [X, \\tilde{y}_t]$,\nFor FC-ODT, we rely on linear combinations of multiple features for binary splits at each node. For a sample x \u2208 Rd in a feature space, the decision function at the node t can be"}, {"title": "4.2 Finding Oblique Splits", "content": "formulated as:\n$a^T_t x < b_t$\nwith coefficients at denoting the projection direction for the split and splitting threshold bt. Determining the optimal value for at is proved to be more challenging than identifying a single optimal feature and setting a suitable threshold for a split in the univariate scenario (Murthy et al., 1994).\nConsidering that we have introduced a feature concatenation mechanism in the ODT generation process, this can lead to collinearity issues between the original and augmented features of the sample. Especially, the correlation between the augmented feature ax and the label y is high, which can cause overfitting problems during the training process as the augmented feature has a significant impact on the model's output. Therefore, we choose ridge regression to find the projection direction of the split\n$a_t(x) = arg max ||y \u2013 Xa_t||_2^2 + \\lambda ||a_t||_2^2$,\nusing regularization parameter \u03bb.\n4.3 Tree Construction\nWith these two steps at hand, we can iteratively perform node splitting and feature concatenation to construct an enhanced oblique decision tree with the ability of in-model feature transformation, named FC-ODT. The tree construction procedure is described in detail in Algorithm 3.\nTo prepare for analyzing the convergence rate of excess risk in Section 5, here we introduce the following definition.\nDefinition 1 (Orthonormal decision stumps). The orthonormal decision stumps is defined as\n$\\psi_t(x) = \\frac{C_{t_1}(X)n(t_R) - C_{t_R}(X)n(t_L)}{\\sqrt{w(t)n(t_L)n(t_R)}}$,\nwhere ct\u2081 (x) = x (XXtz + AI)\u00af\u00b9 X and Xt\u2081 = X01(x \u2208 t\u2081), where o is an elements-wise product, for internal nodes t \u2208 [T], where w(t) = n(t)/n denotes the proportion of observations that are in t.\nThe decision stump Vt in Definition 1 is produced from the Gram-Schmidt orthonormalization of the projection functions {ct(x), ct\u2081(x)} with respect to the empirical inner product space. Next, we use a lemma to demonstrate that tree estimation is equal to the empirical orthogonal projection of y onto the linear span of orthonormal decision stumps.\nLemma 2 (Orthogonal tree expansion). If T denotes a decision tree constructed by FC-ODT method, then its output (4) admits the following orthogonal expansion\n$h_{T,n}(x) = \\sum_{t \\in [T]} <y, \\psi_t>_n \\cdot \\psi_t(X)$,\nwhere \u03c8t = ($t(x1),..., \u03c8t(xn)) is defined in Definition 1.\nBy construction, ||4t|| = 1 and \u3008\u03c8t, \u03c8\u2081\u3009 = 0 are satisfied for distinct internal nodes t and t'in [T]. In other words, hyn is the empirical orthogonal projection of y onto the linear span of {Yt}t\u2208[T]. Furthermore, we have\n$|(y, \\psi_t)|^2 = \\Delta(\\hat{o}, \\hat{a}, t)$.\nRemark 3. Unlike the orthogonal decision stumps of conventional ODT, which iteratively projects the data onto the space of all constant predictors within a greedily obtained node (Cattaneo et al., 2024), Lemma 2 shows that FC-ODT uses a feature concatenation mechanism to make the prediction factors of its orthogonal decision stumps contain information"}, {"title": "5 Theoretical Analysis", "content": "In this section, we show that FC-ODT can achieve a faster convergence rate of consistency with respect to the tree depth K.\nWe consider an additive regression model to satisfy the following definition:\nDefinition 4 (Ridge expansions (Cattaneo et al., 2024)). Consider the family of functions consisting of finite linear combinations of ridge functions:\n$\\mathcal{G} = \\{g(x) = \\sum_{k=1}^K g_k(a_k^T x), a_k \\in \\mathbb{R}^d, g_k : \\mathbb{R} \\rightarrow \\mathbb{R}, k = 1,..., K, ||g||_{L_1} < \\infty\\}$,\nwhere ||9||L\u2081 is a total variation norm defined in Definition 5.\nDefinition 5 (Total variation norm in node t). Define the total variation of a ridge function h(ax) in the node t as V(h, a, t) = sup P|-1| \u2211e=0^P |h(ze+1) - h(ze)|, where the supremum is over all partitions P = {20,...,z|p|} of the interval I(a, t) = [min\u00e6et a a\u00afx, maxx\u2208tax].\nFor any f \u2208 F = cl(G), we define the L\u2081 total variation norm of f in the node t as\n$|| f ||_{L_1(t)} \\coloneqq inf_{\\epsilon>0} \\inf \\{ \\sum_{k=1}^K V (g_k, a_k,t) : g(x) = \\sum_{k=1}^K g_k(a_k^T x), ||f - g|| < \\epsilon \\}$.\nThe models that decompose the regression function into a sum of ridge functions have been widely recognized and promoted by Stone (1985), as well as Hastie and Tibshirani (1987). In particular, the consistency of ODT under this assumption has been proven by Cattaneo et al. (2024). On this basis, we study the impact of the feature concatenation in FC-ODT. Our results rely on the following assumption regarding the data-generating process.\nExponential tails (Cattaneo et al., 2024)] The conditional distribution of y given \u00e6 has exponentially decaying tails. That is, there exist positive constants C1, C2, and M, such that for all x \u2208 X,\n$P(|y| > B + M | x) \\leq c_1 \\exp(-c_2B^2), B \\geq 0$.\nTheorem 6 (Consistency rate for FC-ODT). Let the conditional expectation f(x) be from the ridge functions defined by Definition 4 and the conditional distribution P(y|x) satisfying Assumption 5. Consider a training set of n samples drawn from this distribution and a K-layer decision tree TK constructed by FC-ODT on the training set. Then, for any K \u2265 1 and n \u2265 1, we have\n$\\mathbb{E} [||h_{T_K,n}(x) - f(x)||^2] \\leq 2 \\inf_{f \\in \\mathcal{F}} ||g - f||_3^2 + C_1 \\frac{||g||_{L_1}}{K^2} + C_2 \\frac{2^K d \\log^2 n}{n}$ ,\nwhere C\u2081 = C1(B, M) and C2 = C2(C1.C2, B, M) are two positive constants."}, {"title": "6 Proofs", "content": "In this section, we provide the detailed proofs for the main theorem and lemma.\n6.1 Proof of Lemma 2\nProof Set Ut = {u(x)1(x \u2208 t\u2081) + v(x)1(x \u2208 tr) : u, v \u2208 span(H)} and consider the closed subspace Vt = {v(x)1(x \u2208 t) : v \u2208 span(H)}. By the orthogonal decomposition property of Hilbert spaces, we can express Ut as the direct sum V\u2081 \u2295V+, where V+ = {u \u2208 Ut : (u, v)n = 0,\u2200v \u2208 Vt}. Let It be any orthonormal basis for Vt that includes w\u22121/2(t)1(x \u2208 t), where w(t) = n(t)/n. Let + be any orthonormal basis for \ubc11 that includes the decision stump defined by Eqn. (9).\nConsider that \u1ef9t(x) = ax is the projection of y onto Vt, where at = (X^T Xt+I)^{-1}X^T y, we have\n$\\tilde{y_t}(x) = \\sum_{\\psi \\in \\Psi_t} <y, \\psi>_n \\psi(x)$,\nand\n$\\tilde{y_{t_l}}(x)\\mathbb{1}(x \\in t_l) + \\tilde{y_{t_r}}(x)\\mathbb{1}(x \\in t_R) = \\sum_{\\psi \\in \\Psi_t \\cup \\Psi^{\\perp}} <y, \\psi>_n \\psi(x)$.\nUsing the above expansions, observe that for each internal node t,\n$\\sum_{\\psi \\in \\Psi_t} <y, \\psi>_n \\psi(x) = (\\tilde{y_{t_l}} - \\tilde{y_t})\\mathbb{1}(x \\in t_l) + (\\tilde{y_{t_r}} - \\tilde{y_t})\\mathbb{1}(x \\in t_r)$ .\nFor each x \u2208 X, let to, t1,...,tK\u22121, tk = t be the unique path from the root node to to the terminal node t that contains \u00e6. Next, sum (18) over all internal nodes and telescope the successive internal node outputs to obtain\n$\\sum_{k=0}^{K-1} (\\tilde{y_{t_{k+1}}}(X) - \\tilde{y_{t_k}}(x)) = \\tilde{y_{t_K}}(X) - \\tilde{y_{t_0}} (X) = \\tilde{y_t}(x) \u2013 \\tilde{y}(x)$,\nwhere y is the linear estimation output by solving ridge regression in the root node:\n$\\min_a ||y \u2013 Xa||_2^2 + \\lambda ||a||_2^2$.\nCombining Eqns. (18) and (19), we have\n$\\sum_{t \\in [T]} \\tilde{y_t}(x)\\mathbb{1}(x \\in t) = \\tilde{y} + \\sum_{t \\in [T] \\setminus \\{t_0\\}} \\sum_{\\psi \\in \\Psi_t} <y, \\psi>_n \\psi(x) = \\sum_{t \\in [T]} \\sum_{\\psi \\in \\Psi_t} <y, \\psi>_n \\psi(x)$,\nwhere we recall that the root node to is an internal node of T.\nFinally, the decrease in impurity identity (3) satisfies that:\n$\\Delta(\\hat{o}, \\hat{a}, t) = \\frac{1}{n} \\sum_{x_i \\in t} (y_i - \\bar{Y_t(x)})^2 = \\frac{1}{n} \\sum_{x_i \\in t} (y_i \u2013 \\bar{Y_{t_l}} \\mathbb{1}(x_i \\in t_l) \u2013 \\bar{Y_{t_r}} \\mathbb{1}(x_i \\in t_r))^2 = \\frac{1}{n} \\sum_{x_i \\in t} y_i^2 - \\sum_{x_i \\in t} \\sum_{\\psi \\in \\Psi} |<y, \\psi>_n|^2 = \\sum_{\\psi \\in \\Psi_l \\cup \\Psi_R} |<y, \\psi>_n|^2 = \\sum_{\\psi \\in \\Psi} |<y, \\psi>_n|^2$.\n6.2 Proof of Theorem 6\nProof Following the proofs in (Cattaneo et al., 2024), we begin by splitting the MSE (averaging only with respect to the joint distribution of {At : t \u2208 [Tk]}) into two terms\n$\\mathbb{E}_{T_k} [|| h_{k,n}(x) - f(x)||^2] = E_1 + E_2$,\nwhere\n$E_1 =\\mathbb{E}_{T_k} [|| h_{T_k,n}(x) \u2013 f(x)||^2] - 2(\\mathbb{E}_{T_k} [|| h_{T_k,n}(x) - y||^2 - ||y - f(x)||^2]) - \\alpha(n,k) \u2013 \\beta(n)$,\n$E_2=2(\\mathbb{E}_{T_k} [|| h_{T_k,n}(x) \u2013 y||^2] - ||y - f(x)||^2) + \\alpha(n,k) + \\beta(n)$,\nand a(n,k) and \u1e9e(n) are positive sequences that will be specified later.\nTo bound E[E1], we split our analysis into two cases based on the observed data yi. Accordingly, we have\n$\\mathbb{E}[E_1] = \\mathbb{E}[E_1 \\mathbb{1}(\\forall i: |y_i| \\leq B)] + \\mathbb{E}[E_1 \\mathbb{1}(\\exists i: |y_i| > B)]$\nFirstly, we deal with the bounded term E[E11(\u2200i: |yi| \u2264 B)]. According to (Cattaneo et al., 2024, pages 24-25) and (Gy\u00f6rfi et al., 2002, Lemma 13.1, Theorem 11.4), let R = QB such that R > M > ||y||\u221e, we have\n$\\mathbb{P} (\\mathbb{E}_{T_K} [|| h_{k,n}(x) \u2013 f(x)||^2] \\geq E_2, \\forall i: |y_i| < B) \\\\ \\leq 14\\sup_{N \\in \\mathbb{N}} \\frac{\\mathcal{N}(E,\\frac{\\beta(n)}{30R}, \\mathcal{F}_{N,k}(R), \\mathbb{L}_1(\\mathbb{P}_{an}))}{\\exp(\\frac{\\beta(n)n}{2568R^4})}$,\nwhere $\\mathcal{N}(\\frac{\\beta(n)}{40R}, \\mathcal{F}_{N,k}(R), \\mathbb{L}_1 (\\mathbb{P}_{an}))$ denotes the covering number of $\\mathcal{F}_{n,k}(R)$ by balls of radius r> 0 in $\\mathbb{L}_1(\\mathbb{P}_{an})$ with respect to the empirical discrete measure $\\mathbb{P}_{an}$ on $\\mathbb{X}^{n}$ which satisfies that\n$\\mathcal{N}(\\frac{\\beta(n)}{40R}, \\mathcal{F}_{N,k}(R), \\mathbb{L}_1(\\mathbb{P}_{an})) < (3(\\frac{enp}{d})^k)^{\\mathcal{VC}(\\mathcal{H})^{2k+1}} (\\frac{240eR^2}{\\beta(n)})$ .\nCombining Eqns. (27) and (28), we have\n$\\mathbb{P}(E_1 \\geq 0, \\forall i: |y_i| \\leq B) \\leq 42 ((\\frac{enp}{d})^k)^{\\mathcal{VC}(\\mathcal{H})^{2k+1}} (\\frac{240eR^2}{\\beta(n)}) \\exp(-\\frac{\\beta(n)n}{e^{\\frac{a(n,k)}{2568R^4}}})$,\nso that E1 \u2265 0, \u2200i: |yi| \u2264 B \u2264 1/n\u00b2 and E\u20811(\u2200i: |yi| \u2264 B) \u2264 12R\u00b2.\nThen, by choosing\n$\\alpha(n,k) = \\frac{2568R^4 \\left(2^*d\\log(\\frac{enp}{d}) + 2^k \\log(3) + \\mathcal{VC}(\\mathcal{H})^{2k+1} \\log (\\frac{240eR^2}{\\beta(n)}) + \\log (14n^2)\\right)}{n}$,\n$\\beta(n) = \\frac{240eR^2}{n^2}$ we have\n$\\mathbb{E} [E_1 \\mathbb{1}(\\forall i: |y_i| < B)] < 12R^2\\mathbb{P}(E_1 \\geq 0, \\forall i: |y_i| < B) < \\frac{12R^2}{n^2} < \\frac{12Q^2B^2}{n^2}$ .\nSecondly, for the unbounded term $\\mathbb{E}[E_1 \\mathbb{1}(\\exists i: |y_i| > B)]$, by Cattaneo et al. (2024), we have\n$\\mathbb{E}[||h_{T_K,n}(x) - f(x)||^2 \\mathbb{1}(\\exists i: |y_i| > B)] < (Q+1)^2 \\sqrt{n(n+1)} \\mathbb{E}[y^4] \\sqrt{nc_1} \\exp(-\\frac{c_2(B \u2013 M)^2}{2})$ .\nand\n$\\mathbb{E}[E_2] = || f \u2013 g||^2 + \\mathbb{E}[||h_{T_k,n}(x) \u2013 y||^2 - ||y \u2013 g(x)||^2] + \\alpha(n,k) + \\beta(n)$ .\nSince the excess risk can be decomposed by the Ridge expansions g(x)\n$\\mathbb{E}[||h_{T_k,n}(x) - y||^2 - ||y - f(x)||^2] = ||f - g||^2 + \\mathbb{E}[||h_{T_k,n}(x) \u2013 y||^2 - ||y \u2013 g(x)||^2] ,$\nwe have\n$\\mathbb{E}_{T_k} [|| h_{T_k,n}(x) - f(x)||^2] < ||f \u2013 g||^2 + \\mathbb{E}[|| h_{T_k,n}(x) \u2013 y||^2 - ||y \u2013 g(x)||^2] + C_2\\frac{2^K (d+\\mathcal{VC}(\\mathcal{H})) \\log^2 n}{n}$ for some positive constant $C_2(\\mathcal{C_1}, \\mathcal{C_2}, B, M)$.\nDefine the squared node-wise norm and node-wise inner product as $|| f ||_t^2 = \\frac{1}{n(t)} \\sum_{x_i \\in t}(f(x_i))^2$ and $(f, g)_t = \\frac{1}{n(t)} f(x_i)g(x_i)$. We define the node-wise excess training error as\n$\\mathbb{R}_K(t) = ||h_{T_k,n}(x) \u2013 y||_t^2 - ||y \u2013 g||_t^2$.\nThen, we define the total excess training error as:\n$\\mathbb{R}_K = \\sum_{t \\in \\mathcal{T}_K} w(t)\\mathbb{R}_K(t), w(t) = n(t)/n$ ,\nwhere t \u2208 TK means t is a terminal node of TK.\nAccording to the orthogonal decomposition of the FC-ODT in Lemma 2, we have\n$\\mathbb{R}_K = \\mathbb{R}_{K-1} - \\sum_{t \\in \\mathcal{T}_{K-1}} \\sum_{\\psi \\in \\Psi} <y, \\psi>_n/2$.\nWe denote by ETK the expectation is taken with respect to the joint distribution of {At: t \u2208 [TK]}, conditional on the data. By the definition of RK, we have\n$\\mathbb{E}[\\mathbb{R}_K] = \\mathbb{E}[||h_{\\mathcal{T}_K,n}(x) \u2013 y||^2 - ||y \u2013 g||^2] \\geq 0$ .\nUsing the law of iterated expectations and the recursive relationship obtained in Eqn. (38), we have\n$\\mathbb{E}_{T_K} [\\mathbb{R}_K] = \\mathbb{E}_{T_K} [\\mathbb{E}_{T_K|T_{K-1}}[\\mathbb{R}_K]] = \\mathbb{E}_{T_{K-1}}[\\mathbb{R}_{K-1}] \u2013 \\mathbb{E}_{T_{K-1}} \\mathbb{E}_{T_K \\mid T_{K-1}} \\sum_{t \\in \\mathcal{T}_{K-1}} \\sum_{\\psi \\in \\Psi} <y, \\psi>_n/2]$.\nAccording to the sub-optimal probability defined by Cattaneo et al. (2024, Section 2.2) and the sum of the iterative equality (40), we have\n$\\mathbb{E}_{T_K} [\\mathbb{R}_K] = \\sum_{\\mathcal{A}_t} P_{\\mathcal{A}}(K) \\max_{(\\hat{b}, \\hat{a}) \\in \\mathbb{R}^{p+1}} \\Delta(\\hat{b}, \\hat{a}, t)$,\nwhere $P_{\\mathcal{A}_t}(K) = P_{\\mathcal{A}_t}(max_{(b,a) \\in \\mathbb{R}x\\mathcal{A}_t} \\Delta(b, a, t) \\geq max_{(b,a) \\in \\mathbb{R}^{1+d}} \\Delta(b, a, t))$ is defined to quantify the sub-optimality of the learning algorithm theoretically, and PAt denotes the probability w.r.t. the randomness in the learning algorithm At.\nBy (Cattaneo et al., 2024, Lemma 4 and Lemma 6), we have\n$\\mathbb{E}_{T_K} [\\mathbb{R}_K] \\leq \\mathbb{E}_{T_{K-1}} [\\mathbb{R}_{K-1}] - \\frac{\\mathbb{E} [||g_{L1}||/w(t)]}{\\mathbb{E}_{\\sum_{K=1}^{T_K} L1(t)}}$ .\nDue to the feature concatenation mechanism transmitting projection direction information within the model, the variation $||g_{L1}(t)||$ in the node t in the path decreases with increasing depth. This phenomenon is similar to the decay of residuals in boosting algorithms, and we have obtained a recursion for E[RK], by (Cattaneo et al., 2024, Lemma 5), we have\n$\\mathbb{E} [\\mathbb{R}_K] \\leq \\frac{1}{K \\sum_{k=1}^K 1/\\mathbb{E}[\\sum_{t \\in \\mathcal{T}_{K-1}} w(t)Q ||g||_{L1(t)}]}$"}, {"title": "7 Experiments", "content": "In this section, we verify our theoretical results on two simulated datasets that align with our assumptions. Moreover, we evaluate FC-ODT on eight real-world datasets and compare it to other state-of-the-art ODTs to demonstrate its superiority.\n7.1 Results on Simulated Datasets\n7.1.1 IMPLEMENTATION DETIALS\nDataset We generate two simulated datasets sim1 and sim2. They are fully aligned with the assumption of our theoretical analysis, as stated in Eqn. (12). All simulated datasets are generated as y = f(x) + \u20ac, where \u0454 ~ N (0, \u03c3\u00b2). For sim1 dataset,\n$f(x) = ReLU(x_1) + ReLU(\\frac{x_2 + x_3}{2}) + ReLU(\\frac{x_4 + x_5 + x_6}{3}) + ReLU(\\frac{x_7 + x_8 + x_9 + x_{10}}{4}) + ReLU(\\frac{x_1 + x_3 + x_5 + x_7 + x_9}{5})$\nFor sim2 dataset,\n$f(x) = exp(x_1) + exp (\\frac{x_2+x_3}{2}) + exp (\\frac{x_4 + x_5+ x_6}{3}) + exp (\\frac{x_7"}]}