{"title": "Treefix: Enabling Execution with a Tree of Prefixes", "authors": ["Beatriz Souza", "Michael Pradel"], "abstract": "The ability to execute code is a prerequisite for various dynamic program analyses. Learning-guided execution has been proposed as an approach to enable the execution of arbitrary code snippets by letting a neural model predict likely values for any missing variables. Although state-of-the-art learning-guided execution approaches, such as LExecutor, can enable the execution of a relative high amount of code, they are limited to predicting a restricted set of possible values and do not use any feedback from previous executions to execute even more code. This paper presents Treefix, a novel learning-guided execution approach that leverages LLMs to iteratively create code prefixes that enable the execution of a given code snippet. The approach addresses the problem in a multi-step fashion, where each step uses feedback about the code snippet and its execution to instruct an LLM to improve a previously generated prefix. This process iteratively creates a tree of prefixes, a subset of which is returned to the user as prefixes that maximize the number of executed lines in the code snippet. In our experiments with two datasets of Python code snippets, Treefix achieves 25% and 7% more coverage relative to the current state of the art in learning- guided execution, covering a total of 84% and 82% of all lines in the code snippets.", "sections": [{"title": "I. INTRODUCTION", "content": "Executing code is essential for reasoning about the runtime behavior of code, e.g., in a dynamic program analysis, when extracting runtime data to train a model, or when trying to understand code during manual debugging. However, getting code to actually run is challenging, both at the small and the large scale. At the small scale, individual code snippets, e.g., found in documentation or online forums, may contain undefined variables, functions, or classes, which prevent the code from executing. At the large scale, setting up a complex project is often difficult due to the diversity of build systems, missing dependencies, configuration issues, etc. Even when a project is perfectly set up, executing a specific code location will require a specific set of inputs, which may not be available.\nTo enable the execution of arbitrary code snippets, either stand-alone snippets that simply are not executable on their own or code snippets extracted from a larger project, re- searchers have proposed learning-guided execution [1]. The basic idea to predict likely values for any missing variables in a code snippet with a neural model, preventing the ex- ecution from getting stuck. Learning-guided execution has numerous applications because it enables dynamic analysis of code snippets in isolation. Since its introduction in 2023, the community has started to explore several applications, such as detecting runtime type errors [2], reproducing bugs [3], and checking whether a code change preserves the semantics [4]. Learning-guided execution may also serve as a mechanism for validating code or code changes produced by an auto- programming technique, e.g., LLM-based code completion [5], [6] and code editing [7]-[9]. The ability of learning-guided execution to execute arbitrary code without requiring the full setup of the project should make it relatively easily applicable to a wide range of codebases.\nWhile the current state of the art in learning-guided execu- tion, LExecutor [1], can enable the execution of a relative high amount of code, it suffers from two key limitations: (1) LExecutor predicts values sampled from a limited set of runtime values. Specifically, their neural model predicts one out of 23 abstract values, such as \u201cnon-empty string\u201d, which then are concretized into a hard-coded concrete value, such as \"a\". These values may not match realistic values, as they would occur in a real execution of the given code snippet, and they may not be diverse enough to reach all branches. (2) LExecutor is designed for and evaluated on the task of executing a code snippet once, which may not be sufficient to cover all branches. Furthermore, executing the snippet only once prevents the approach from leveraging feedback from previous executions to improve the environment in which the given code snippets gets executed. Overall, these two limitations curtail the effectiveness of the state-of-the-art approach at covering the lines in a given code snippet.\nThis paper presents Treefix, a novel learning-guided exe- cution approach that introduces several ideas. First, instead of training a custom model to predict abstract values, Treefix builds upon a large language model (LLM) to predict code that constructs concrete values. We refer to the code that constructs these values as prefixes, because the code gets prepended to the given code snippet before executing them together. By generating code prefixes, Treefix can create a much larger and more diverse set of values, including domain-specific strings, complex objects, and even values returned from third-party libraries. Second, Treefix reasons about the problem in a multi- step fashion, where each step uses feedback about the code snippet and its execution to instruct the LLM to improve a previously generated prefix. The prefixes generated by Treefix form a tree, where each node represents a prefix and an edge represents a refinement of a prefix in the next step. Finally, Treefix produces not only a single execution, but also returns a minimal set of prefixes that maximize the number of cumulatively executed lines in the code snippet.\nThe approach consists of three fully automated steps, which are designed to mimic the way a human may approach the problem of enabling the execution of a code snippet.\n\u2022 Step 1: Statically identify undefined references in the given code snippet and then query the LLM for prefixes that initialize them.\n\u2022 Step 2: Execute the code snippet with the prefixes, observe any runtime errors that may occur, and then query the LLM for refined prefixes that address these errors.\n\u2022 Step 3: Execute the code snippet with the refined prefixes, keep track of lines that are not yet covered, and then query the LLM for prefixes aimed at covering these lines.\nWe evaluate Treefix by applying it to two sets of code snippets from prior work [1]: functions extracted from popular open-source projects and code snippets extracted from Stack Overflow posts. As baselines, the evaluation compares Treefix with six alternative approaches for executing arbitrary code snippets: regular execution, a state-of-the-art type predic- tor [10], a state-of-the-art function-level test generator [11], LExecutor [1], Incompleter [3], and SelfPiCo [2]. We show that Treefix enables the execution of 84% and 82% of all lines in the open-source code and Stack Overflow snippets, respectively, which improves over the best baseline by 25% and 7%. Moreover, we show that each step of our approach contributes to its effectiveness and that Treefix produces, on the evaluated snippets, 16528 unique values, which is 16505 more values than LExecutor. In case studies we find that Treefix's ability to generate adequate imports, complex objects, and diverse primitive values contribute the most to Treefix's improvements over LExecutor.\nIn summary, our contributions are as follows:\n\u2022 We propose Treefix, a multi-step learning-guided execution approach leveraging LLMs for enabling code execution.\n\u2022 Through experimentation on two datasets, we show that Treefix can substantially improve code coverage and is superior to the state-of-the-art approaches.\nWe release our open-source implementation of Treefix upon acceptance of the paper."}, {"title": "II. MOTIVATING EXAMPLE", "content": "To motivate our work and illustrate the challenges of enabling code execution, consider the example in Figure 1a. The code could be part of a complex project or a code snippet extracted from an online resource. Our goal is to execute this code snippet while covering as many of its lines as possible. However, executing this code snippet is challenging due to various missing pieces of contextual information. At first, the code tries to read the attribute self.user_type, but self does not exist, which will cause the code to crash. In case self and self.user_type were defined, the code would try to call the function get_register_func, which is also undefined, as are the attributes self.name and self.alias. Beyond undefined variables and functions, covering all code lines in this snippet is even more challenging due to the multiple branches and the try and except blocks. To cover all lines, a single execution is not enough: A non-exceptional execution would miss out on the code in the except block, while an exceptional execution would miss out on any lines after the line that raises an exception in the try block.\nThe current state-of-the-art approach for executing arbitrary Python code snippets is LExecutor [1], which has introduced the concept of learning-guided execution. To enable the ex- ecution of arbitrary code snippets, LExecutor first trains a neural model that learns from real executions what values are typically used in a given code context. LExecutor then modifies the code snippet so that whenever it would usually refer to a non-existing value and crash, the code instead queries the model to predict the most likely kind of value. The approach then concretizes this kind of value, injects it into the running code, and continues the execution. For example, a predicted kind of value could be \u201cnon-empty string", "a\".\nWhile LExecutor enables the execution of a relative high amount of code, it suffers from two keys limitations": "n\u2022 Restricted set of runtime values. The kinds of values that LExecutor predicts are restricted to a fixed set of 23 ab- stractions, such as \u201cnon-empty string\u201d, \u201cnegative integer", "empty list\". For each of them, LExecutor has a single concrete value, such as \"a\", -1, or [], respectively. That is, the injected values may not match realistic values, as they would occur in a real execution of the given code snippet, and they may not be diverse enough to reach all branches. For the example in Figure 1a, in case LExecutor predicts that the value assigned to self.email is a \u201cnon- empty string": "the concrete value assigned to self.email would be \"a\". In this case, it will not be able to execute the code in the second if statement, which checks if \u201c@\" is in self.email. Moreover, \"@\" would not be present in any other concrete value used by LExecutor.\n\u2022 Single execution. LExecutor is designed for and evaluated on the task of executing a code snippet once, which may not be sufficient to cover all branches. In particular, any code snippet that contains mutually exclusive branches, such as two if-else branches without any loop or recursion around it, such as in Figure 1a, cannot be fully covered in a single execution.\nFigure 1b shows the values predicted by LExecutor for the code snippet in Figure 1a. Their approach predicts the return value of get_register_func to be a \"callable\", and hence, its concrete value is a DummyObject class, which is assigned to the register variable. As register is not None, the exe- cution proceeds to the assignment of self.email. LExecutor predicts self to be an \"object\", i.e., the concrete value is an instance of the DummyObject class. Moreover, LExecutor predicts self.user_type, self.name, and self.alias to be \"non-empty strings\", so they all get the concrete value \"a\". When register is called, a DummyObject instance is assigned to self.email. This way, when the code tries to check if \"a\" is in self.email, a \"TypeError: 'DummyOb- ject' object is not iterable\" exception is raised, crashing the code execution. As a result, only the first three lines in the try block are executed, but neither any of the remaining branches nor the except block, giving a line coverage of only 30%."}, {"title": "III. APPROACH", "content": "The following presents Treefix, a multi-step, LLM-based approach to enable the execution of arbitrary code snippets. We address the limitations of the state-of-the-art learning- guided execution approach, LExecutor, as follows. Instead of predicting a restricted set of possible values, we leverage LLMs to predict code prefixes that can produce arbitrary val- ues, such as domain-specific strings, complex objects, and even values returned from third-party libraries. By prepending such a code prefix to the given code snippet, Treefix significantly increases the likelihood of executing the code snippet with realistic values that reach branches guarded by non-trivial conditions. To address the limitation of using only a single execution, our approach creates a set of prefixes that iteratively increase the number of executed lines in the code snippet. Finally, the approach yields a set of prefixes that complement each other in terms of coverage, e.g., by executing different branches of the code snippet.\nBefore presenting Treefix, we start by defining the problem that we address. The input to our approach is a syntactically valid piece of code, which we refer to as a code snippet s. The goal is to execute the code snippet, one or more times, to maximize the number of successfully executed lines. To enable the execution of the code snippet, we generate one or more prefixes p, where a prefix is a syntactically valid piece of code that consists of two parts: a possibly empty list of import statements and a possibly empty list of assignment statements that initialize variables used in s.\nBecause a single prefix p may be insufficient to execute all lines in s, we aim to generate a set P of prefixes that maximizes the number of executed lines in s. We refer to those lines of s that are executed when running p + s as the coverage achieved by p, and to those lines of s that are executed when running p+s for each $p \\in P$ as the cumulative coverage. Using this terminology, the goal is to maximize the cumulative coverage of s by generating the prefixes P, where one $P_{best} \\in P$ will have the highest coverage achieved by any individual prefix. The set P and the single-best prefix $p_{best}$ are useful for different usage scenarios. For example, $P_{best}$ can be used to understand the execution of s, e.g., by inspecting the execution in an interactive debugger. Instead, P can be used to dynamically analyze different executions of s, e.g., to detect behavioral anomalies.\nWhile related, the problem addressed here differs from fuzzing [12]-[14] and test case generation [11], [15], [16]. One difference is the assumptions about the given code to execute. Both fuzzing and test case generation assume to have a complete and fully installed project, i.e., without any missing code and with all dependencies set up and ready to run."}, {"title": "A. Problem Statement", "content": null}, {"title": "B. Overview and Running Example", "content": "To address the problem of generating prefixes that maximize the cumulative coverage of a code snippet, Treefix reasons about the problem in a multi-step fashion. The basic idea is to iteratively generate and refine prefixes until reaching full coverage or exceeding a configurable budget. To generate and refine prefixes, Treefix queries an LLM with information obtained by statically and dynamically reasoning about the code snippet and already generated prefixes. The approach provides three kinds of information to the model: undefined- ness guidance, i.e., variables, attributes, and methods that are undefined in the code snippet; error guidance, i.e., runtime er- rors observed when executing the code snippet with a specific prefix; and coverage guidance, i.e., lines in the code snippet that are not yet covered by the currently known prefixes.\nThe prefixes generated by Treefix form a tree, as illustrated in Figure 2. The root is the given code snippet and each of the other nodes represents a prefix. An edge represents a refinement of a prefix in the next step. In addition to the root, the nodes in the tree are grouped into three levels, which correspond to the three kinds of guidance provided to the LLM. The approach iteratively generates the tree of prefixes in a breadth-first manner. That is, Treefix first generates prefixes based on undefinedness guidance, then refines them based on error guidance, and finally further refines the prefixes based on coverage guidance. During this process, the approach keeps track of the prefixes S that maximize cumulative coverage, highlighted in green in the figure, and of the single-best prefix $P_{best}$ that achieves the highest coverage, highlighted with a star.\nThe following illustrates the approach on the motivating example from Figure 1a.\nUndefinedness guidance: The first step is to statically identify any undefined values in the code and ask the LLM to predict code that initializes them. Figure 1c shows an example of prediction made in the first step of Treefix. The LLM predicts get_register_func to be the also predicted dummy_register_func function, which receives two arguments and returns a string. Moreover, the model predicts self to be a Mock object containing the attributes accessed in the snippet. While these values look legitimate at first sight, executing the prefix and the snippet shows that, when get_register_func is called on the first line of the try block, a \u201cTypeError: dummy_register_func() missing 1 required positional argument: 'alias\" exception is raised.\nError guidance: The second step of Treefix uses the feedback from the execution of the code snippet to refine the prefix, providing the invaluable information provided in the error message to the LLM. Figure 1d shows the refined prefix obtained in this step. Notice that only the value of dummy_register_func changed. Now, the LLM predict dummy_register_func to return another function, which receives two arguments and returns a string containing \"@\". Given this prefix, the code snippet runs without raising any exceptions. The execution covers the two if blocks, covering 60% of the lines.\nCoverage guidance: To further increase the coverage of the given code snippet, the third step of Treefix guides the LLM toward prefixes that execute those lines that were not executed in the previous steps. For our example, those lines are the except and else blocks. Figures 1e, 1f, and 1g show three prefixes obtained in the third step. The prefixes contain a value for self.email without \"@\", a register set to None, and a statement that will raise a SystemExit exception, respectively. The union of these prefixes successfully executes all lines in the code snippet, achieving 100% line coverage."}, {"title": "C. Main Algorithm", "content": "Algorithm 1 summarizes the main steps of the approach. In addition to the code snippet s, the algorithm takes two parameters as input: the number n of prefixes to generate per prompt with the LLM and the number k of coverage guidance attempts. The output of the algorithm is the set P of prefixes that maximize cumulative coverage and the single-best prefix $P_{best}$ that achieves the highest coverage. The remainder of this section describes the three steps of the approach in detail."}, {"title": "D. Step 1: Undefinedness Guidance", "content": "Given the code snippet s, the first step of Treefix aims to predict a prefix that initializes any undefined references in s. These steps are summarized in lines 2 to 6 of Algo- rithm 1. To identify all undefined references in the given code snippet, Treefix calls GetUndefineDREFS, which is based on static analysis. GETUNDEFINEDREFS first parses the source code of the code snippet into an AST. Then, within the AST, it identifies the scopes of all variables in the code. Finally, GETUNDEFINEDREFS iterates over each scope and then over each variable access within that scope. If a variable is being accessed but not defined in the current scope or any enclosing scopes, it is considered undefined. For example, in a code snippet a = b + foo() the vari- ables b and foo are undefined. Besides undefined variables, GETUNDEFINEDREFS also identifies undefined attributes and methods. For example, in y = d.year p.init(), the attribute d.year and the method p.init() are undefined, besides the undefined variables d and p. For any undefined variable, any of its attributes or methods are considered to be undefined as well. To determine the undefined attributes and methods, GETUNDEFINEDREFS identifies the locations of the nodes of the undefined variables and visits the attributes and methods being used in s. Whenever the base object of an undefined attribute or method matches with one of the undefined variables, the undefined attribute or method name with its base object, e.g., d. year, is added to a list, which is returned by GETUNDEFINEDREFS.\nBased on the statically determined set of undefined refer- ences, Treefix generates a prompt aimed at predicting code to define those references (GENPROMPT1). Figure 3 shows the prompt (slightly modified for readability), which has the following structure: 1) a request to provide the missing values; 2) the code snippet s with comments indicating its beginning and end; 3) the list of undefined references in s; 4) the list of undefined attributes and methods in s; 5) a specification of the expected response, as described in Figure 4. Next, Treefix calls QUERYLLM, which queries the model with the prompt to obtain n prefixes. The rationale for generating multiple prefixes for the prompt is to increase the diversity of the prefixes generated, and hence, the chance to find prefixes that successfully cover the code in s.\nGiven the prefixes returned by the LLM, which are stored in set P\u2081 in Algorithm 1, Treefix executes them and updates the coverage information. Because this part of the approach is used for all three of Treefix's steps, we describe it in more detail in Section III-G. In short, EXECUTE post-processes each prefix, automatically installs any third-party dependencies required to execute the prefix, prepends the prefixes to the code snippet"}, {"title": "E. Step 2: Error Guidance", "content": "The values predicted in step 1 of Treefix may be incom- plete or contain values that lead to execution errors, as in Figure 1c. To fix any errors, the second step of Treefix uses the error messages observed during the execution of the prefixes generated in step 1 as feedback to refine any erroneous prefixes (lines 7 to 12 in Algorithm 1). The helper function GETERRORPREFIXES checks the execution results of the prefixes in P\u2081 and returns those prefixes that raised an exception. Each exception contains the exception type, message, and line number where it happened.\nFor each prefix that raises an exception, Treefix formulates a prompt (GENPROMPT2) aimed at predicting a fixed version of the prefix. Figure 5 shows the prompt structure, which contains the following: 1) a description of the problem; 2) the exception type, message, and line number where it happened; 3) a statement of the task; 4) a response specification indicating the response content and its format, as described in Figure 4. Similar to step 1, Treefix queries the LLM with the prompt to obtain n refined prefixes, which are stored in set P2 in the algorithm. Because LLMs tend to be more effective when given meaningful context, Treefix keeps the conversation history that has led to the erroneous prefix in step 1 as part of the prompt for step 2. Finally, the approach executes the prefixes in P2 and updates the coverage information (again, Section III-G will provide the details)."}, {"title": "F. Step 3: Coverage Guidance", "content": "Steps 1 and 2 are often successfully at finding one or more prefixes that enable executing the given code snippet without errors. However, there may still be lines in the code snippet that are not executed by any of the prefixes generated in the previous steps, such as the multiple branches in Figure la.\nTo increase the coverage of the code snippet, the third step of Treefix aims to predict prefixes that exercise any not yet covered lines, as summarized in lines 13 to 21 in Algorithm 1. The basic idea of step 3 is to iteratively generate additional prefixes until either reaching full coverage or exhausting a budget of k attempts. In each iteration, Treefix calls ANNOTA- TEUNCOVEREDLINES, which identifies all the lines in s that have not been covered by the previous predictions and marks them using a special comment #uncovered. The approach then formulates a prompt aimed at predicting a prefix that exercises the uncovered lines. Figure 6 shows the prompt structure, which contains: 1) a description of the problem; 2) the annotated code; 3) a statement of the task; 4) the response specification. As in Steps 1 and 2, Treefix calls the LLM to generate n prefixes. For each prefix, the approach executes it and updates the coverage information."}, {"title": "G. Execution and Coverage Measurement", "content": "A key component of Treefix is to obtain feedback by executing the prefixes generated by the LLM. The following presents code execution and coverage measurement in more detail, which corresponds to the helper functions EXECUTE and UPDATEPREFIXES in Algorithm 1. These helper functions are used in all three steps of Treefix.\n1) Installing Third-Party Dependencies: The predicted pre- fixes may contain imports from dependencies that are not currently installed in the environment where Treefix is being executed. Our approach automatically identifies and installs any missing dependencies. To this end, we use pipreqs, a Python library that identifies the dependencies based on the imports in the code. For example, the predictions made by Treefix in Figure 8b start with two import statements. In this case, Treefix identifies that pandas and numpy are dependen- cies and installs them. Because installing dependencies is one of the most time-consuming processes of Treefix, we keep a shared environment between snippets. This environment con- tains all installed dependencies and avoids repeatedly installing the same libraries.\n2) Post-Processing of Prefixes: Some of the prefixes gen- erated by the LLM may contain errors, yet other parts of the same prefix are useful. Treefix post-processes the prefixes to heuristically remove any lines that raise an execution error. Specifically, the approach iteratively attempts to execute each prefix up to 10 times and removes any lines that raise an execution error. If this process results in a prefix that runs without errors, it will be concatenated with the code snippet s and executed to measure the coverage achieved, as described below. Otherwise, the prefix is discarded. Another problem is that the predicted prefixes may contain an infinity loop or take very long to run. To work around this problem, Treefix also removes prefixes that take more than 30 seconds to execute.\n3) Measure Coverage: For all prefixes that, when executed on their own, neither raise an error nor time out, Treefix measures the coverage achieved when prepending the prefix to the code snippet. To measure the coverage achieved by a prefix in s, Treefix uses the same strategy to measure coverage used by LExecutor. It instruments s by adding a call to a special function _1_ after every line in the code. _1_ receives a unique ID, which identifies the line above, as argument. Whenever _1_ is called, the previous line was successfully executed. Treefix combines, in this order, the predicted and post-processed prefix with the instrumented version of s into a program, and then executes it to record the executed line numbers. Unlike reports by popular coverage tools, this measurement considers a line \u201ccovered\" only if the entire line executes without crashing."}, {"title": "IV. EVALUATION", "content": "We structure our evaluation along five research questions.\n\u2022 RQ1: How much code does an execution guided by Treefix cover, and how does it compare to prior work?\n\u2022 RQ2: How do the three steps in Treefix contribute to its effectiveness?\n\u2022 RQ3: Qualitatively, why does Treefix achieve different coverage results than existing work?\n\u2022 RQ4: How diverse are the values predicted by Treefix?\n\u2022 RQ5: What are the costs of executing code with Treefix?"}, {"title": "A. Experimental Setup", "content": "1) Benchmark Datasets: We evaluate on two datasets, described in Table I, containing Python code snippets used in previous work [1]. The Open-source functions dataset contains 1,000 randomly selected functions from five large and diverse open-source Python projects. The Stack Overflow snippets dataset contains 462 code snippets from the answers to Python questions on Stack Overflow.\n2) Baselines: We compare Treefix with six alternative approaches for executing arbitrary code snippets: 1) LExecu- tor [1], for which we use the most effective variation, i.e., the fine-grained value abstraction, using top-1 predictions from the CodeT5 model. 2) SelfPiCo [2], an LLM-based approach developed concurrently with this work, which guides code execution in an interactive loop. We apply their approach to the datasets we use here, which is the same as in the LExecutor paper, but different from the dataset used in the SelfPiCo paper. As their fine-tuned Code Llama model is not available, we use their approach with GPT-3.5, which they report to achieve similar performance than the version with Code Llama. 3) Incompleter [3], a rule-based, feedback- driven approach. It measures coverage using coverage.py, which - unlike our coverage metric counts a line as covered even if that line crashes. For a fair comparison, we modify Incompleter to measure coverage as we do for Treefix and all the other baselines, i.e., considering a line as covered if it is successfully executed. We apply Incompleter to the datasets we use. In addition to these three state-of-the-art baselines, we also consider the baselines that LExecutor has been compared with: 4) \"As Is\", i.e., trying to execute a code snippet as it is without making any value predictions. 5) Pynguin, a function- level test generator for Python [11]. As in previous work [1], for a fair comparison, we run Pynguin on a single function at a time, which contains only the code to execute. 6) Type4Py, a neural model that predicts types for all local variables, parameters, and return values [10]. We concretize the predicted types as done for this baseline in previous work [1].\n3) Evaluated Models: We use OpenAI's latest flagship models: GPT-40, the largest available model, and GPT-40 mini, a more lightweight and cheaper model.\n4) Algorithm Parameters: We set the maximum number of completions to n = 10 for each query to the model. Moreover, in step 3, we set the maximum number of iterations k = 10."}, {"title": "B. RQ1: Effectiveness at Covering Code", "content": "Table II (left side) shows the line coverage achieved by Treefix and by the baselines on the two datasets. We use the Wilcoxon signed-rank test to compare the significance of coverage differences between techniques, at p = 0.05. For the open-source functions, on average, executing the code as it is and the type (Type4Py) predictor cover only 4.1% and 13.3% of the lines, respectively. LExecutor and Incompleter increase the mean coverage to 51.6%. Then, SelfPiCo further increases the coverage to 59%. Finally, Treefix covers 76% of"}, {"title": "C. RQ2: Design Choices", "content": "1) Effectiveness per Step: Treefix uses three steps to en- able code execution. We evaluate how each of these steps contributes to the improvements in effectiveness. To this end, we measure the line coverage and full execution rate achieved after each step, again for both datasets and models.\nTable III presents the results. On average, Treefix, with either of the considered models, in the first step already achieves comparable or higher effectiveness than the best baseline, SelfPiCo (Table II). Steps 2 and 3 consistently further increase effectiveness on both considered datasets, regardless of the model.\nTo understand when Treefix typically finds the single-best prefix, Table IV shows the number of snippets where $P_{best}$ is found in a specific step of the approach. We see that $p_{best}$ is most frequently found on step 1, yet steps 2 and 3 also contribute to finding the best prefix in many cases. Taken together, steps 2 and 3 contribute 21.7% and 17.0% of all $p_{best}$ prefixes for the open-source functions and Stack Overflow datasets, respectively.\nOverall, these results indicate that each step in Treefix consistently adds to its effectiveness, with step 1 being the most important. The low number of prefixes in P indicates that Treefix effectively determines a small number of prefixes that maximize coverage, which is useful to keep the number of executions performed in a any downstream applications of Treefix manageable.\n2) Trees of Explored Prefixes: Treefix generates the prefixes that form a tree. In total, every tree of prefixes could contain up to 210 nodes: 10 nodes from step 1, 100 nodes from step 2, and 100 nodes from step 3. We investigate how many of these nodes are visited, on average, before Treefix terminates, e.g., because it has already fully covered the given snippet. Figure 7 shows the distribution of the number of prefixes explored in total (\"all\"), and in each step of the approach. We also show the number of prefixes in the set P. On average, Treefix explores 70 prefixes for the open-source functions dataset, and only two of these prefixes end up in P, i.e., are required to achieve maximum coverage. The maximum observed size of P is twelve. Across the 1,000 snippets in the open-source functions dataset, Treefix achieves full coverage for 558 on step 1. Then, 442 go to step 2, and 392 go to step 3. Figure 7b shows the corresponding results on the Stack Overflow dataset. Here, Treefix explores 39 prefixes, and only one is added to P, on average. The maximum observed size of P is four. Out of the 462 snippets, Treefix achieves full coverage for 410 on step 1. Then, 52 go to step 2, and 166 go to step 3.\n3) Impact of Resolving Dependencies: As described in Sec- tion III-G1, Treefix uses pipreqs to identify and install missing dependencies. On the 1,462 snippets used in our evaluation, pipreqs is invoked 1,216 times and succeeds in 1,206 of these"}, {"title": "D. RQ3: Case Studies", "content": "We qualitatively analyze the strengths and weaknesses of Treefix by inspecting samples of examples.\n1) Reasons for Higher Coverage: Across all 1,462 analyzed code snippets, Treefix achieves higher coverage than LExecu- tor in 707 code snippets: 561 from the open-source functions dataset and 146 from the Stack Overflow dataset. We randomly inspect a sample of 20, 10 from each dataset, leading the the following observations.\nAdequate imports and usage of dependencies: For 15/20 code snippets, Treefix increases coverage by importing a dependency and using it to create adequate values. LExecutor does not add any imports but always injects a value from a fixed set.\nComplex objects: For 13/20 code snippets, Treefix predicts code that creates complex objects, e.g., with type(\"Mock\", bases, dict). These objects usually contain the attributes and methods used in the code snippet. Figure 8 shows an interesting combination of using imported dependencies and complex objects. Notice that the code snip- pet in Figure 8a tries to access a multi-index dataframe with random data. The prefix predicted by Treefix (Figure 8b) correctly imports pandas and numpy, and then instanti- ates a multi-index dataframe with random values. Moreover, Treefix assigns to tm a mock object, which contains an assert_almost_equal method receiving two arguments, as used in the last line of the given snippet.\nDiverse primitive values: For 11/20 code snippets, Treefix predicts domain-specific primitive values, e.g., mean- ingful string and integers. This differs from LExecutor, which predicts values from a fixed set only.\nMultiple paths covered: For 1/20 code snippets, the multi-step algorithm of Treefix yields multiple snippets, that cover different paths. Since LExecutor makes only one predic- tion, it fails to fully cover any examples with multiple paths.\n2) Reasons for Lower Coverage: Across all 1,462 code snippets Treefix has lower coverage than LExecutor in 58 code snippets: 47 from the open-source functions dataset and 11 from the Stack Overflow dataset. We randomly selected and inspect 10 snippets from each dataset, with the following observations."}, {"title": "E. RQ4: Diversity of Values", "content": "As observed in RQ3, one of the reasons for the effectiveness of Treefix is the diversity of values predicted by the LLM. We further study this diversity by analyzing how many unique types and values the approach predicts. To this end, we consider each of the prefixes predicted by Treefix"}]}