{"title": "Hyperbolic Contrastive Learning for Hierarchical 3D Point Cloud Embedding", "authors": ["Yingjie Liu", "Pengyu Zhang", "Ziyao He", "Mingsong Chen", "Xuan Tang", "Xian Wei"], "abstract": "Hyperbolic spaces allow for more efficient modeling of complex, hierarchical structures, which is particularly beneficial in tasks involving multi-modal data. Although hyperbolic geometries have been proven effective for language-image pre-training, their capabilities to unify language, image, and 3D Point Cloud modalities are under-explored. We extend the 3D Point Cloud modality in hyperbolic multi-modal contrastive pre-training. Additionally, we explore the entailment, modality gap, and alignment regularizers for learning hierarchical 3D embeddings and facilitating the transfer of knowledge from both Text and Image modalities. These regularizers enable the learning of intra-modal hierarchy within each modality and inter-modal hierarchy across text, 2D images, and 3D Point Clouds. Experimental results demonstrate that our proposed training strategy yields an outstanding 3D Point Cloud encoder, and the obtained 3D Point Cloud hierarchical embeddings significantly improve performance on various downstream tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, language models (LMs) [2], [22], [26], [48] have made great progress and shown remarkable capabilities in understanding and generating natural language. Meanwhile, to harness the advancements in language models, recent approaches [12], [13], [34] have evolved to combine visual processing with the reasoning and generalization capabilities of LMs by aligning vision and language embeddings in shared feature space and ensuring consistency. Even with the significant resource investment and progress in training schemes or prompt engineering, recalling the manifold learning hypothesis reveals that these models still face limitations, particularly due to the lack of consideration for the geometric priors of the feature space. The default Euclidean geometry used in these models for learning embeddings may not always be optimal, particularly in representing complex hierarchical structures and relationships in real-world data, whether in text modality or vision modality.\n\nHierarchical structure is a fundamental component of the natural world. Humans comprehend the world through the relationships and hierarchies described above [28], [38]. For example, all noun hierarchies lead to an entity, and verb synsets detail events, e.g., \u201ccommunicate\u201d to \"whisper\" [7]. Text-vision pairs also show hierarchy, while pixels form shapes, combining to create scenes, each layer building on the previous to abstract higher. Studies have shown that the text and vision data (including 2D images and 3D Point Clouds in this work) are part of the hierarchy [6], [23], [39]. Meanwhile, latent embeddings with an underlying tree-like and hierarchical structure learned by deep neural networks exhibit better performance [6], [19], [22]\u2013[24], [39]. Most recent research [48] investigates the non-Euclidean characteristics of LLMs on complex reasoning tasks, finding that token embeddings and hidden states exhibit a significant degree of hyperbolicity, indicating an underlying hyperbolic structure. We further hypothesize that incorporating hierarchical concepts in model feature space design can help models maintain stable, coherent perception when faced with complex visual inputs, which is beneficial for understanding the real world. Hyperbolic space with negative curvature is well-suited for modeling hierarchical data, yielding remarkable performance. Hierarchical embeddings in the hyperbolic space have been previously explored in single-modal and uni-modal settings, learning shared embeddings of different types of modalities, including Text and Images [10], [52]. MERU [6] is the first large-scale contrastive image-text models that yield hyperbolic embeddings. [37] further considers the modality-gap problem while preserving hierarchies.\n\nHowever, the aforementioned challenges remain when incorporating LLMs for 3D object understanding in hyperbolic space, especially embodied interaction that relies on precise geometry, which is currently under-explored. Hence, in this work, we further explore the hierarchical prior in 3D object understanding by bridging the hyperbolic language-image model and 3D Point Cloud representation learning. We train contrastive text-2D image-3D Point Cloud models that yield hyperbolic embeddings that capture the visual-semantic hierarchy. We summarize our contributions as follows:\n\n\u2022 We propose a regularizer for point cloud embedding reconstruction to promote intra-modal hierarchical knowledge capturing and implement a guidance contrastive learning process, aligning 3D Point Cloud embeddings with hyperbolic Text-Image embeddings.\n\n\u2022 We propose novel hierarchy-enhancing losses that promote inter-modal hierarchical concept relations during contrastive learning, achieving hierarchical embeddings for 3D Point Clouds, extending beyond common Text-Image modalities.\n\n\u2022 Experimental results show significant improvements in the performance of various point cloud tasks based on our hierarchical 3D Point Cloud embeddings."}, {"title": "II. RELATED WORK", "content": "Hyperbolic embedding learning has been explored in various fields [3], [9], [18], [21], [25]."}, {"title": "A. Hyperbolic Geometry for Point Clouds", "content": "Hyperbolic embeddings with InfoNCE loss for predicting hierarchical relations in the WordNet nouns hypernymy tree was first proposed in [25]. [9] suggested entailment loss as an alternative. In the vision area, [18] proposed Hyperbolic ProtoNet for few-shot classification. Point clouds of 3D objects also exhibit an inherent hierarchical compositional nature. HyCoRe [23] first proposes the explicit regularization to capture part-whole hierarchies and experimentally observed hierarchical structures, noting that hierarchical structures of embeddings naturally emerge within hyperbolic space but are crude without its proposed regularization [24]. PHGT [21] leverages an attention module based on the Poincar\u00e9 ball model to enhance 3D Point Cloud feature extraction and classification. HypLiLoc [42] fuses Euclidean and hyperbolic features for improved pose regression. HECPG [46] introduces hyperbolic attention with hyperbolic weight and Riemannian metric to fuse hyperbolic features, boosting point cloud matching accuracy adaptively. We further propose novel pre-training losses that enhance hyperbolicity, thereby preserving hyperbolic modeling capabilities during the multi-modal contrastive learning process."}, {"title": "B. Contrastive Pre-training for Hierarchy Multi-Modal Embeddings", "content": "ConVIRT [55] pioneered contrastive pre-training [16], [35] for zero-shot image classification, maintaining L2-normalization and cosine similarity. There are some other variant methods like CoCa [50] added captioning loss by a multi-modal text decoder, OTTER [44] considered intra-modal similarity, and SigLIP [53] applied logistic regression. Other than MERU [6], exponentially lifts the embeddings onto the Lorentz hyperboloid, combining entailment learning with the CLIP approach to learn embeddings in hyperbolic space capturing latent visual-semantic hierarchies. [37] further discusses the modality gap in hyperbolic space. The most recent [27] extends to include image patches and caption parts, enforcing an ordering that reflects the hierarchy shared by both modalities. EuCLIP [5] captures hierarchical relationships by using Euclidean geometry with negative squared distance softmax logits and removing final layer normalization.\n\nThe point cloud is a fundamental modal for understanding the three-dimensional (3D) world. Both inter and intra-modal contrastive learning strategies are extended to the point cloud area [1]. PointCLIP [54], [56] further aligns point clouds to 2D depth images and text in the context of CLIP. ReCon [33] utilizes contrast guided by reconstruction to address the pattern disparities between local masked data modeling and global cross-modal alignment. ShapeLLM [34] further scales up the parameters of ReCon and broadens the scale of the pretraining dataset for robust 3D embeddings. [4] pre-trains a 3D Point Cloud encoder and cross-modal interactor using phrase-level scene-text annotations, then tunes for multi-task instruction with referent tokens for flexible 3D scene understanding. However, it remains under-explored in the context of learning hyperbolic contrastive 3D Point Cloud embeddings. We further explore inferring concept hierarchies across multiple modalities, including text, 2D images, and 3D Point Clouds."}, {"title": "III. PRELIMINARY", "content": "In this section, we introduce relevant notations and briefly review the Lorentz space and related contrastive losses used in this work as preliminary knowledge. We di-vide a dataset of text-vision pairs into mini-batches $B = \\{(T_1, V_1, P_1), (T_2, I_2, P_2), . . . \\}$, where $T_i, I_i$ and $P_i$ denotes text, 2D images, and 3D Point Clouds, respectively. Let $B$ denote the batch size and $i \\in |B|$. Further, assume that we have a text encoder $f(\\cdot)$, an image encoder $g(\\cdot)$, and a 3D Point Cloud encoder $h(\\cdot)$. Let $x, y, z$ denote hyperbolic text embedding, hyperbolic image embedding and hyperbolic point cloud embeddings, respectively."}, {"title": "A. Hyperbolic Geometry & Lorentz. Embeddings", "content": "Hyperbolic space is a non-Euclidean space with constant negative curvature. Following [37], to better avoid numerical instabilities in the training process that comes from exponential volume growth, we adopt the Lorentzian hyperboloid rather than other popular models of hyperbolic geometry, e.g., the Poincar\u00e9 ball model, the Klein model, the Poincar\u00e9 half-space model, and the hemisphere model. The Poincar\u00e9 ball model and Beltrami-Klein model are the projections of the hyperboloid model onto the different dimensional space-like hyperplanes, as more details can referred to in [39]. Taking $x$ as an example, we provide a background discussion of the hyperbolic space but limit it to the Lorentz / hyperboloid model, denoted by\n\n$L^n = \\{x \\in R^{n+1} : (x, x)_L = - \\frac{1}{c}\\}, c > 0,$\n\nwhere every vector $x$ can be written as $[x_{space}, X_{time}]$, $X_{time} \\in R$ serves as the axis of symmetry [37]. Note that for $c > 0$, the curvature is -c. Since $x$ always lies on the hyperboloid, the time dimension can then be inferred as $X_{time} = \\sqrt{\\frac{1}{c}+ ||X_{space}||^2}$. $(\\cdot, \\cdot)_L$ denotes the Lorentzian inner product as\n\n$(x, y) = X_{space} \\cdot Y_{space} - X_{time} Y_{time}.$\n\nWe only consider these maps where m is the origin of the hyperboloid (O = $[0, \\frac{1}{c}]$) [37], i.e., simplifying the exponential map by using the tangent space of the origin, thus meanwhile minimizing potential numerical instability in the model's computation [19]. The exponential map provides a way to map vectors from tangent spaces onto the manifold. For the point O on the hyperboloid, it is defined as $exp_{mO} : T_OL^n \\rightarrow L^n$. Let $u = [u_{enc}, 0] \\in R^{n+1}$, thus $\\langle O, u \\rangle = 0$ and $u$ belong to the tangent space at the hyperboloid origin O. We have the space dimension of hyperbolic text embedding as $x_{space} = exp_{mO,space}(u)$, by lifting the embeddings to the Lorentz hyperboloid $L^n$ through the exponential map\n\n$exp_{mO,space} (u) = \\frac{sinh(\\sqrt{c} ||u_{enc}||)}{\\sqrt{c} || u_{enc}||} u_{enc} + O.$\n\nNote that $u_{enc}$ denote the scaled text embeddings,\n\n$u_{enc} = A_{txt}f(T)$.\n\nSpecifically, $f(T) \\in R^n$ would have an expected norm $\\sqrt{n}$ and the exponential map scales it to $e^{\\sqrt{n}}$, which can be numerically large [6]. Both $A_{txt}$ are initialized to $\\frac{1}{\\sqrt{n}}$ and learned"}, {"title": "B. Language-Vision Contrastive Learning Loss", "content": "Contrastive Loss for Hyperbolic Language-Image Embeddings Considering language-image contrastive learning loss $L_{cont}$, it can formulated by applying InfoNCE [40] with similarity function that measures the relationship within pairs. The similarity function is cosine similarity in CLIP, i.e., $sim(x, y) = \\frac{x^Ty}{||x||||y||}$, where $|| \\cdot ||$ is the L2 norm. For hyperbolic contrastive learning, we follow the formulation and the hyperboloid model in MERU [6] whose similarity function is parameterized by three trainable scalars: text embedding scale $\\lambda_{txt}$, image embedding scale $\\lambda_{img}$, and curvature parameter $c$. For a batch of size (B) containing text (T) $x$ and images (I) $y$, the contrastive loss is formulated by taking the negative Lorentzian distance as the similarity metric, as follows:\n\n$sim(f(T), g(I)) = -d_c(x, y)$\n\nReconstruction-guided Contrastive Learning for 3D Point Clouds Embeddings The training paradigm of the 3D Point Cloud encoder in this work is based on the reconstruction-guided contrastive learning framework ReCon [33]. It is consistently observed that contrastive models focus mainly on a global field, in contrast to generative models which exhibit a preference for focused local attention, leading to a task conflict in naive multi-task representation learning settings [33], [47]. Following ReCon, we consider the objective as ensemble representation distillation, encouraging the 3D Point Cloud encoder to learn disentangled knowledge representation. Both contrastive and generative methods are seen as student-teacher paradigms, unified as ensemble distillation from multiple teachers, where the generative model also acts as a \"teacher\" guiding the contrastive learning. Meanwhile, reconstruction guidance enhances the contrastive learning process by improving generalization, stability, and training efficiency. The ReCon loss $L_{ReCon} = L_{Rec} + L_{Con}$ ensembles the cross-modal contrastive learning loss using the positive-only representation learning with Smooth $l_1$ loss Smooth-$l_1(\\cdot, \\cdot)$, and the reconstruction guidance loss, constructed as the masked point modeling reconstruction following [29]. Specifically, the loss $L_{Con}$ can be written as:\n\n$L_{Con} = \\sum_{i=1}^{|B|} Smooth-l_1 (z, stopgrad(x)+\n\nSmooth-l_1 (z, stopgrad(y)],$\n(1)\n\nwhere $stopgrad(\\cdot)$ is the stop-gradient operation, which prevents gradients from back-propagating to the image or text teachers, i.e., keeping the text encoder and image encoder of MERU frozen in this work. Given the predicted point patches $P_{pre}$ and ground truth $P_{gt}$, we have\n\n$L^{Rec}_{pre} = \\frac{1}{|P_{pre}|} \\sum_{P' \\in P_{pre}} min_{P \\in P_{gt}} ||P' - P||^2 + \\frac{1}{|P_{gt}|} \\sum_{P' \\in P_{gt}} min_{P \\in P_{pre}} ||P' - P||^2.$\n(2)"}, {"title": "IV. APPROACH", "content": "In this section, we first introduce our basic model and then analyze hierarchy relations across text, image, and 3D Point Cloud. Finally, we present novel hyperbolicity-enhancing pre-training losses that promote preserving hyperbolic modeling capabilities."}, {"title": "A. Overall Architecture and Hierarchy Relations", "content": "Basic Model When addressing 3D Point Clouds, current hyperbolic contrastive learning methods have not been extended to the 3D Point Clouds modal, ignoring the importance of its underlying hierarchical prior. Therefore, we extend the reconstruction-guided contrastive learning framework in [33], aiming to transfer knowledge to the point cloud encoder from a pre-trained hyperbolic language-image model MERU [37], which has learned paired image-text embeddings in the hyperbolic space, facilitate the learning of hierarchical 3D Point Cloud embeddings. Note that we employ Point-MAE [29] as the point cloud encoder. Single-modal 3D Point Cloud inputs, and cross-modal inputs including rendered RGB images and text descriptions, are encoded as sequential tokens. During contrastive learning, the 3D token embeddings are masked for generative reconstruction, while the mask is disabled during inference. The obtained 3D Point Cloud embeddings and global queries are then fed to the decoder, which shares the same architecture as the encoder. The queries are learnable and supervised by contrastive learning. However, the objective of the 3D Point Cloud encoder in ReCon does not include a hierarchical representation-related loss function or target, meaning that the obtained 3D Point Cloud embeddings do not explicitly capture hierarchical knowledge. To address this, we further introduce appropriate regularizers to unify the objective for effective hierarchical 3D representation learning and understanding, while also addressing differences in data patterns and tasks across modalities.\n\nHierarchy Relation Analysis We aim to effectively and explicitly represent the underlying hierarchy structures, whether within a modality (intra-modal) or between different modalities (inter-modal), through positional relationships. Specifically, we consider how the relationships between point clouds, text, and images should be structured, focusing on the position of 3D modalities in the hierarchy.\n\nIn hyperbolic space, as the distance from the center increases, the available embedding area for features expands exponentially. Given that general concepts require less representational space than specific ones, simpler, more common concepts should be mapped closer to the center of the space than more detailed concepts. Thus,"}, {"title": "B. Hyperbolic Entailment Regularization", "content": "To better match the representation of the 3D object's point clouds, existing 3D cross-modal representation learning methods primarily average the image features extracted by single-view 2D foundation models from multi-view images, which are projections of the original 3D object. Recently, methods like DETR, BLIP2, and ShapeLLM [34] have further proposed to adaptively select and distill views to describe the 3D object better. However, while images can supplement information about material and color that 3D Point Clouds might not provide, they can only serve as approximations or augmentations for accurately representing the geometric structure of the 3D object. Therefore, to perform cross-modal learning better, we must acknowledge the differences between image embeddings and 3D Point Cloud embeddings and capture their relationship, which enhances comprehensive 3D shape information understanding.\n\nIn a nutshell, multi-view image embeddings are entailed within the hyperbolic cones of the 3D Point Cloud embeddings, and 3D Point Cloud embeddings are entailed within the hyperbolic cones of the text embeddings. This builds a hierarchy structure of the cross-modal feature space, establishing a partial order among Text-Image-point cloud pairs.\n\nWe extend the entailment loss [6], generalizing to capture the visual-semantic hierarchy across Text-Image-point cloud three modalities. Using the half-aperture of a text embedding and the exterior angle between a Text and Image embedding as examples, as\n\n$aper(x) = sin^{-1} (\\frac{2K}{\\sqrt{c} ||x_{space}||}), \\frac{2K}{\\sqrt{c} ||x_{space}||} \\geq \\frac{2K}{\\sqrt{c}},$\n\nwhere $\\frac{2K}{\\sqrt{c}} \\geq \\frac{2K}{\\sqrt{c}}$ will be clamped to 1\u20136, where e = 10-8 for training stability [6]. The exterior angle $ext(x, y) = \\pi-\\angle Oxy$ given by the origin O, x, and y is then\n\n$ext(x, y) = cos^{-1} ( \\frac{Y_{time} + X_{time} - c(x,y)_L}{||X_{space}|| \\sqrt{(c(x,y)_L)^2 - 1}} )$.\n\nFinally, the entailment loss mathcalLentail is formally written as:\n\n$L_{entail} (x, y, z) = max(0, ext(x, y) + ext(y, z)-\n\naper(x) - aper(y)).$\n\nThe entailment loss forces all image embeddings to match the point cloud embeddings within the cones that emanate from the point cloud embeddings and all point cloud embeddings to match the text embeddings within the cones that emanate from the text embeddings. Images still adhere to the principle that a corresponding text embedding represents a more abstract concept, and we build the inter-modal hierarchy between the embeddings of the new modality 3D Point Cloud and the existing Text-Image embeddings.\n\nTo encourage better distribution of embeddings on the hyperbolic space, we follow an objective that maintains a controllable or adaptive modality gap and further constructs the semantic hierarchy structure across text, 2D images, and 3D Point Clouds [37]. Specifically, we ensure that the centroid of text embeddings is closer to the origin than the centroid of visual embeddings, and the centroid of 3D Point Cloud embeddings should be closer to the origin than the centroid of the 2D image embeddings. Obtaining the centroid of a set of points in the hyperbolic space H is not as straightforward as the Euclidean setting. This is called the Einstein midpoint, and it is easier to obtain via converting to Klein coordinates K. A point on the hyperboloid model can be converted to Klein coordinates k and back via projections, and the the centroid takes the following form:\n\n$Centroid_K (x) = \\Pi_{K \\rightarrow H} (\\frac{\\sum_{j=1}^N \\frac{\\gamma_j \\Pi_{H \\rightarrow K}(x_j)}{\\sqrt{1 - c || \\Pi_{H \\rightarrow K}(x_j)||^2}}}{\\sum_{j=1}^N \\frac{\\gamma_j}{\\sqrt{1 - c || \\Pi_{H \\rightarrow K}(x_j)||^2}}} )$,\n\nwhere $x = \\{x_j\\}_{j=1}^N$, $\\gamma_j = \\frac{1}{\\sqrt{1 - c || \\Pi_{H \\rightarrow K}(x_j)||^2}}$ denotes the Lorentz factors. Formally, let $X_e, Y_e,$ and $Z_e$ be the Einstein midpoint of a set of text, 3D Point Cloud embeddings and image embeddings, respectively. Then, our regularization takes the following form:\n\n$\\rho = d_c(O, \\rho) = \\sqrt{1/c} \\cdot cosh^{-1}(-c \\langle O, \\rho \\rangle_L ) = \\sqrt{1/c} cosh^{-1} (1+ \\frac{||P_{space}||^2}{2}) ,$\n\n$q = d_{\\pounds}(O, q), r = d_{\\pounds}(O,r)$,\n\n$L_{cent} = ||Z_e - \\rho||^2 + ||Y_e \u2013 q||^2 + ||X_e \u2013 r||^2,$\n\nwhere is the Euclidean norm and $p > q > r > 0$ to ensure that the centroid relationships, avoiding the diversity collapse of visual embeddings."}, {"title": "C. Alignment Loss and Hyperbolicity Analysis", "content": "In addition to achieving the alignment of 3D Point Cloud embeddings to Text-Image embeddings, we refine the internal hierarchical structure of the point cloud embeddings by considering the hierarchical whole \u2192 part composition relation. Specifically, during training, the 3D Point Cloud encoder Point-MAE infers twice: once with full-size point clouds as input and once with a masked point cloud, resulting in a part embedding. This process helps capture the hierarchical relationships within the point cloud modal.\n\nNext, we introduce a quantitative analysis method to analyze the hierarchical structure of the obtained point cloud embeddings. Gromov \u03b4-hyperbolicity is a geometric metric that quantifies the deviation of a given metric space from an exact tree metric [11]. The simplest discrete metric space"}, {"title": "D. Contention Between Modal and Tasks", "content": "For the setting of visual scene understanding in computer vision, the down-streamed models must understand both the geometry and semantics of the scene by dealing with our learned visual embeddings simultaneously, leading to a multi-task representation learning problem. Considering that the different regularizers are independently propagated, the coverage speed of intra-modal learning and inter-modal learning was observed to be different during our pre-training [14]. Prior approaches to simultaneously learning multiple tasks use a naive weighted sum of losses, where the loss weights are uniform or manually tuned. Performance is highly dependent on an appropriate choice of weighting between each task's loss. Therefore, to ensure the effective use of the regularizers proposed in the above sections, this work automatically weighs multiple loss functions based on each task's homoscedastic uncertainty [17]. All objectives can be seen as regression-based reconstruction tasks [15]. Thus, we adopt this multi-task loss to balance these terms. The overall joint loss function is formulated as\n\n$L = \\sum(e^{-L_i} L_i + s), L_i \\in \\{L_{cent}, L_{entail}, L_{Rec}, L_{Con}\\}"}, {"title": "V. EXPERIMENTAL RESULTS", "content": "In this work, we apply the reconstruction-guided contrastive learning strategy [33] to train a 3D Point Cloud encoder, building on the hyperbolic Text-Image model MERU [6] and extending its multi-modal capabilities to obtain hierarchical 3D Point Cloud embeddings. Consequently, in this section, we mainly conducted experiments to evaluate the 3D Point Cloud encoder to answer the following two pivotal Research Questions (RQs): RQ1: What advantages does our method offer in achieving hierarchical 3D Point Cloud embeddings? RQ2: What improvements do our hierarchical 3D Point Cloud embeddings bring to the 3D Point Cloud downstream tasks? To answer RQ1, we analyze the hierarchical relationships among embeddings across language and vision modalities in Section V-A. To answer RQ2, we compare our method with state-of-the-art 3D Point Cloud methods on a variety of datasets and tasks in Section V-C, demonstrating the improvements brought by our hierarchical 3D point cloud embeddings.\n\nImplementation Details All the experiments are conducted on a GeForce RTX 4090 24 GB for all the experiments. We use CLIP and MERU as our teacher models: the CLIP-based approach uses the default CLIP-ViT-B/16, and the MERU-based method uses the base MERU model with ViT-B/16 as the image encoder and the same text encoder as CLIP [41]. We maintain consistent settings of the total train epoch, learning rate schedule, and optimizer configuration with maximum learning rate 5e-4, AdamW optimizer, cosine learning rate schedule with 10 warm-up steps, and batch size 128. To ensure curvature parameter c and text/image/point clouds embedding scales \\lambda_{txt}/\\lambda_{img}/\\lambda_{pts} stay positive, these scalar hyperparameters are parameterized on the logarithmic scale and are consistent with the MERU model: c is initialized with pre-trained MERU checkpoint and fixed. To maintain stability and avoid numerical issues, we employ trigonometric functions with values clamped to the range [1e - 8,1 \u2013 1e \u2013 8]."}, {"title": "A. Hierarchical Embedding Analysis", "content": "We calculate the \u03b4-hyperbolicity values according to Equation 3 and present in Table I and Figure 1. We measure the hyperbolicity of embeddings within individual randomly shuffled batches, each containing 128 samples.\n\nThe final \u03b4 result is the average and standard deviation across all 409 batches. The images are projected from the 3D"}, {"title": "B. Dictionary Learning Analysis", "content": "We trained a Sparse Autoencoder (SAE) [36], to obtain sparse codes of 3D Point Clouds features, while the decoder is a linear layer whose weights act as a traditional dictionary. Each column in this dictionary represents an atom. We set the dimension of each atom to 512 and the total number of atoms to five times the feature dimension, totaling 2560 atoms. Figure 3 shows the logarithmic frequency of sparse codes and the cosine similarities between all atoms, indicating the sparsity of the data structure. The atoms also exhibit low similarity, suggesting that our dictionary has learned highly independent base features. Notably, approximately 30% of the atoms show significant activation, as they are frequently activated when encoding the 3D Point Cloud samples. Furthermore, for samples of different classes, frequently activated atoms display distinct activation levels, indicating that the learned dictionary is discriminative. This also implies that the learned features can be further disentangled and semantically decomposed, which will be explored in future work."}, {"title": "C. Point Cloud Encoder Experiments", "content": "We conduct the evaluation on tasks including fine-tuning classification, few-shot learning, and part segmentation experiments. We conduct classification, and few-shot learning experiments on ModelNet40 and ModelNet10 datasets [45], which are all popular 3D object datasets. We use data augmentation operations during training, following ReCon [33], including standard random scaling and translation.\n\n1) Fine-tuned Classification Results.: We finetune on three model variants on ModelNet40, and ModelNet10. On ModelNet40, we report two different settings with 1,024 (1k) points and 8, 192 (8k) points respectively. Since the real-world dataset will inevitably be affected by noise or occlusions, it is a much more challenging dataset for point cloud analysis methods. The results are shown in Table III. In the comparison, our MERU (modified) significantly boosts the performance without voting, achieving higher accuracy scores than the other three baselines including ReCon, CLIP-based, and MERU-based approaches.\n\n2) Few-shot 3D object classification: We conduct few-shot learning experiments on ModelNet40 and zero-shot experiments on ModelNet40 and ModelNet10. In few-shot experiments, we use the 'K-way N-shot' setting, randomly selecting K classes and N instances per class to form a support set for training. We then evaluate the model using other N instances from the same K classes, referred to as the query set. The results with the setting of $K \\in \\{5,10\\}$ and $N \\in \\{10,20\\}$ are presented in Table IV. Note that additionally, we did not modify the task head to incorporate hyperbolic operations such as M\u00f6bius addition; instead, we continued using the original linear layers and activation functions. The necessity of such modifications can be explored in future work. Nonetheless, Table IV demonstrates that the proposed MERU (modified)-based approach achieves competitive performance.\n\n3) Part Segmentation: Object part segmentation is a challenging task with a high requirement of model representation capability. Table II reports the part segmentation results on the ShapeNetPart [49] dataset, providing the best average class mIOU mIoUc (%), the best average inctance mIOU, the mIOU\u2081 (%), as well as the IoU (%) for each categories. Recall that Table I shows that MERU-based 3D Point Cloud embeddings have lower hyperbolicity than CLIP-based embeddings. However, MERU-based embeddings are close to the origin, which violates the priors introduced in [24] that the embeddings of 3D models' components or parts should ideally sit at the border of hyperbolic geometry, where there is exponentially greater capacity. We suggest that the"}, {"title": "VI. CONCLUSION", "content": "In this work, we extended the application of hyperbolic geometry to multi-modal data, integrating 3D Point Cloud data with Text and Image modalities. We further propose hierarchy-enhancing regularizers to align 3D Point Cloud embeddings with hyperbolic Text-Image embeddings and effectively capture both intra-modal and inter-modal hierarchical knowledge. By establishing a partial order among Text-Image-3D Point Cloud pairs, we constructed hierarchical semantically similar relationships across different modalities, enhancing the interpretability of these embeddings. Experimental results demonstrate significant improvements in various point cloud tasks, the effectiveness of our approach in multi-modal embeddings and hierarchical 3D Point Cloud embeddings learning."}]}