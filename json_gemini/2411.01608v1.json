{"title": "GITSR: Graph Interaction Transformer-based Scene Representation for Multi Vehicle Collaborative Decision-making", "authors": ["Xingyu Hu", "Lijun Zhang", "Dejian Meng", "Ye Han", "Lisha Yuan"], "abstract": "In this study, we propose GITSR, an effective framework for Graph Interaction Transformer-based Scene Representation for multi-vehicle collaborative decision-making in intelligent transportation system. In the context of mixed traffic where Connected Automated Vehicles (CAVs) and Human Driving Vehicles (HDVs) coexist, in order to enhance the understanding of the environment by CAVs to improve decision-making capabilities, this framework focuses on efficient scene representation and the modeling of spatial interaction behaviors of traffic states. We first extract features of the driving environment based on the background of intelligent networking. Subsequently, the local scene representation, which is based on the agent-centric and dynamic occupation grid, is calculated by the Transformer module. Besides, feasible region of the map is captured through the multi-head attention mechanism to reduce the collision of vehicles. Notably, spatial interaction behaviors, based on motion information, are modeled as graph structures and extracted via Graph Neural Network (GNN). Ultimately, the collaborative decision-making among multiple vehicles is formulated as a Markov Decision Process (MDP), with driving actions output by Reinforcement Learning (RL) algorithms. Our algorithmic validation is executed within the extremely challenging scenario of highway off-ramp task, thereby substantiating the superiority of agent-centric approach to scene representation. Simulation results demonstrate that the GITSR method can not only effectively capture scene representation but also extract spatial interaction data, outperforming the baseline method across various comparative metrics.", "sections": [{"title": "Introduction", "content": "Autonomous vehicles have garnered significant research attention over the past two decades, driven by their substantial potential for societal and economical advancement. The efficient coordination of driving decisions among CAVs promises not only to enhance safety and operational efficiency but also to reduce energy consumption [1]. However, in dynamic traffic scenarios, the intricate interplay between scenarios and traffic participants presents formidable challenges for CAVs in making decisions that are safe, efficient, and comfortable [2]. The Internet of Vehicles (IoV) technology integrates Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) communications with artificial intelligence (AI) to offer innovative solutions for CAVs to process dynamic traffic scene information and perform collaborative driving decisions [3]. In this context, methods based on deep reinforcement learning (DRL) are becoming more and more popular because the intelligent agent can continuously learn through interaction with the driving environment, extract environmental information through deep learning, and make decisions through reinforcement learning algorithms [4]. However, modeling and representing scene information effectively, processing and calculating it to adapt to various complex traffic environments, while achieving high-quality collaborative decision-making in real-time dynamic settings, has emerged as a formidable research challenge. Concurrently, the research on autonomous vehicle decision-making is increasingly focusing on more complex scenarios. The crux of the challenge lies in the representation of the state, which must encompass the elements, characteristics, and interactions in the dynamic scene. Addressing this will become one of the key issues of the DRL methods [5].\nTo this end, we introduce GITSR, a novel graph interaction Transformer-based scene representation framework for multi-vehicle collaborative decision-making. This framework leverages the Transformer architecture to capture scene information and employs a graph structure to model spatial interaction, thereby enhancing the multi-vehicle collaborative decision-making ability of reinforcement learning. Firstly, we extract features from the dynamic driving environment within the context of intelligent networking, meticulously considering both the local interaction and global communication attributes of CAVs. We perform local reconstruction reasoning on scene input information, introduce the Transformer module to process information and enhance understanding of surrounding traffic scene for CAVs. We conduct local reconstruction reasoning on the input scene information and introduce the Transformer module to process this data, thereby enhancing the CAVs' comprehension of the surrounding traffic environment. Then, we represent the dynamic traffic scene as a graph, based on global communication attributes, and introduce GNN to extract spatial interaction features. This approach is advantageous as it optimally utilizes the information from all CAVs within dynamic traffic scenarios. It aids CAVs in scene comprehension and the transmission of upstream and downstream information. Moreover, it establishes the spatial interaction dynamics of the traffic environment, optimizing the collaborative driving decision-making capabilities. The main contributions of this article can be summarized as follows:\n1) A collaborative decision-making framework for intelligent connected vehicles that integrates Transformer and GNN is designed, which is tailored for scene extraction and interaction modeling from the perspective of state representation, thus significantly enhancing the state representation to improve the"}, {"title": "Background and related work", "content": null}, {"title": "DRL for Autonomous Driving Decision-making", "content": "There are primarily two approaches to decision-making for autonomous vehicles: rule-based and learning-based. The majority of decision modules in Baidu Apollo are rule-based, characterized by their simplicity of implementation and the clarity of their logic, which is derived from manually formulated rules [6]. However, as traffic scenarios grow increasingly complex, this approach becomes less efficient and challenging to apply. DRL amalgamates deep learning with reinforcement learning, enabling self-learning through environmental interactions without predefined complex rules. It can handle high-dimensional and complex decision-making problems and become one of the mainstream methods for autonomous vehicles behavior decision-making [7]. Especially in recent years, with the advancement of deep learning, a large number of cutting-edge algorithms that integrate reinforcement learning have yielded remarkable outcomes [8], [9], [10], [11]. Inspired by Natural Language Processing (NLP), autonomous vehicles can better select actions by remembering some history, and learn the long-term correlation between scenes and motion states through long short-term memory (LSTM) [12]. The attention mechanism enables neural networks to discover interdependencies in a variable number of inputs. Leurent et al. [13] designed an attention mechanism based on the scene of a non-signal intersection to successfully learn to identify and utilize the interaction mode of controlling nearby traffic, and realized the visualization of the attention matrix. Li et al. [14] have successfully integrated separable convolution with the Transformer architecture for vehicle lane-changing scenarios, resulting in a lightweight yet high-performing solution. Building on this, our research applies the DRL method to the behavioral decision-making process of autonomous vehicles and extends its application to multi-vehicle collaborative decision-making in higher-dimensional contexts."}, {"title": "Scene Representation in DRL", "content": "The realm of DRL-based behavior decision-making for autonomous vehicles continues to face numerous challenges. A key issue is the accurate representation of traffic scenes, including both static and dynamic road elements as well as the status of traffic participants [15]. Traffic participants are highly interactive in real time, which significantly influence the interpretation of the traffic scene and the output of decision-making behaviors of CAVs. Over the past few years, addressing the challenge of scene representation has emerged as a central focus in a multitude of studies, which can include vehicle status information, vehicle-observed environmental information, and the interactions among traffic participants. The feature list method represents the status of CAVs and surrounding vehicles observed by them, such as position, speed, and heading, in a matrix list. The encoding approach using motion information has been widely used in research [16], [17], [18]. However, a limitation of this method is that the number of selected surrounding vehicles and the ordering of the list directly impact the outcome, making it challenging to adapt to scenes that fluctuate dynamically. A common way to overcome this limitation is to employ a spatial grid representation, construct the scene into a grid, and no longer select surrounding vehicles for status representation, but instead cover them with occupied spatial grid. A pivotal aspect of the spatial grid method is the selection of the coordinate system, which typically falls into two categories: scene-centric and agent-centric models [19]. The scene-centric model depicts the status of traffic participants in a unified coordinate system after anchoring the scene, usually by discretizing the entire scene into a spatial grid akin to an aerial map. For example, Y. Zheng et al. [20] mapped the entire urban area into a spatial grid and developed a method to represent all vehicles within a coordinate system. Differently, the agent-centric model [1], [21] uses the CAV of interest as the central coordinate, and the surrounding traffic participants are represented by their states relative to the vehicle, which can be regarded as scene-centric reconstruction reasoning, as shown in Fig. 1. In our study, we evaluate the performance of these two models in decision-making tasks and employ a more powerful"}, {"title": "Methodology", "content": "In this research, our proposed GITSR framework for DRL is designed to focus on the effective scene representation and interaction modeling of CAVs within dynamic mixed traffic environments to improve collaborative decision-making capabilities. This section mainly introduces the overall framework of GITSR and its details, as shown in Fig. 2. After feature extraction of the driving environment, it is divided into three parts: scene representation, interactive behaviors modeling and mask matrix. The scene representation is input to the Transformer module for encoding and calculation to extract map information. The interactive behaviors are represented as a state space matrix and an adjacency matrix through graph neural network. Mask matrix is used to filter out non-autonomous vehicles information. Ultimately, the RL module synthesizes and processes the scene representation and interaction behaviors, then outputs the determination of driving actions. Once the CAVs execute these actions, the environment feedback rewards to facilitate the updating of the network."}, {"title": "Problem formulation", "content": "Based on the background of intelligent networking, we propose the problem of multi-vehicle cooperative decision-making in mixed traffic environments. The multi-vehicle cooperative decision-making problem based on RL can be formulated as a MDP, which can be represented as a tuple $(s_t, a_t, r_t, s_{t+1}, \\gamma)$. In the autonomous driving scenario, each CAV can only obtain environmental information within its perception range due to sensor constraints, and must communicate with each other to share information. Therefore, at each time step, CAVs construct a state representation $s_t \\in S$ of through information sharing, and each CAV executes an action $a_t \\in A$ that causes the environment to transfer to state $s_{t+1}$. Ultimately, all CAVs share a reward function $r_t$. The process is then repeated, with the goal of allowing CAVs to choose actions at each time step to maximize their expected future discounted reward $E[\\Sigma_\\tau \\gamma^\\tau r_t]$, where $r_t$ is the reward obtained at time $t$. The discount factor $\\gamma$ determines how much immediate rewards are favored over more distant rewards."}, {"title": "Scene representation with Transformer", "content": "We implement an agent-centric scene representation approach. Specifically, we first rasterize the input scene to extract scene features. We assume that each CAV can perceive the traffic environment within a 50-meter radius to the front and rear of the vehicle, and reconstruct the local traffic scene with the vehicle as the center coordinate of the grid to obtain the local map $map_i \\in \\mathbb{R}^{n_{lanes}\\times 51}$ of the i-th CAV. The ego vehicle occupies $map_i[j, centric]$ according to the lane $j$ it is in, and the other vehicles in the perception range occupy $map_i$ according to their relative positions and lanes. Notably, we employ a grid occupation method based on vehicle speed to more accurately represent the traffic scene. All unoccupied local grids are designated with a value of 0. Fig. 3. shows a schematic diagram of our scene representation method. In addition, all CAVs can engage in communication to share their individual local traffic scene information, thereby assembling a comprehensive multi-vehicle scene representation.\nAfter extracting the driving scene to generate the multi-vehicle scene representation, encoding it in the GITSR framework should have the following two properties: 1) The algorithm should be able to capture the interactive information between the vehicle and the surrounding traffic scene; 2) The algorithm should be able to effectively extract the shared information of multi-vehicle communication. Therefore, deploying the Transformer algorithm for information extraction in the GTISR framework is an effective method, as the Transformer can focus on key information among a large amount of input information and ignore unimportant information. In the scene representation, the Transformer can achieve: 1) Capturing the local dynamic changes of each CAV and analyzing the feasible domain; 2) Utilizing the shared information among all CAVs to effectively guide the formulation of collaborative decision-making and driving actions.\nSince Transformer was first proposed by Vaswani et al. [22] in 2017, it has been widely used in NLP and Computer Vision (CV) fields [23], [24]. We will first introduce the core Multi-Head Attention (MHA) mechanism of Transformer Block. In the self-attention mechanism of MHA, the input information is passed through to obtain the embedding vector $X$. Then, $X$ is multiplied with three different weight matrices $W_q$, $W_k$ and $W_v$ to obtain three different vectors $Q$, $K$ and $V$, which represent the query, key and value respectively. The computational formula is as follows:\n$Q = XW_q$\n$K = XW_k$\n$V = XW_v$ (1)\nwhere the dimensions of the three weight matrices are the same.\nThe attention matrix is obtained by scaling the dot product of the reciprocal square root of the number of columns $\\sqrt{d_k}$ of the $Q$ and $K$ matrices and normalizing it:\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$. (2)"}, {"title": "Spatial interactive behaviors with GNN", "content": "Dynamic traffic scenes also have spatial interaction characteristics, that is, the distribution and re-lationship of vehicles and their behaviors in space, which together affect the dynamic changes of traffic flow. In order to effectively represent this characteristic, after extracting the motion information of the vehicles, we construct the dynamic traffic scene as a graph. Specifically, the modeled spatial interaction behaviors is represented by $G = (\\mathcal{N}, \\mathcal{E})$. $\\mathcal{N} = {n_1, n_2, ..., n_{|\\mathcal{N}|}}$ represents the set of all vehicle features, and $\\mathcal{E} = {e_1, e_2, ..., e_{|\\mathcal{E}|}}$ represents the set of interaction relationships between them. $|\\mathcal{N}|$ constitutes the total number of vehicles in traffic, and $|\\mathcal{E}|$ represents the total number of vehicle interaction relationships.\nThe feature matrix $N$ represents the longitudinal position, speed, lane, vehicle category, lane head and tail ways of each vehicle in the scene. Therefore, the feature matrix can be expressed as:\n$N = \\begin{bmatrix}X_1 & V_1 & L_1 & I_1 & H_1 & T_1\\\\X_2 & V_2 & L_2 & I_2 & H_2 & T_2\\\\& ... \\\\X_i & V_i & L_i & I_i & H_i & T_i\\\\& ... \\\\X_n & V_n & L_n & I_n & H_n & T_n\\end{bmatrix}$ (5)\nThe motion information of the vehicle in the scene can be expressed as follows:\n$X_i = X_{i\\_position}/X_{road}$\n$V_i = V_{i\\_speed}/V_{max}$\n$L_i \\in {1,2,3}$\n$I_i \\in {1,2,3}$\n$H_i = [h_1, h_2,\\ldots,h_l]/X_{road}$\n$T_i = [t_1, t_2,\\ldots,t_l]/X_{road}$ (6)\nwhere $X_{i\\_position}$ is the current longitudinal position of the vehicle, $X_{road}$ is the total length of the road; $V_{i\\_speed}$ is the current speed of the vehicle, $V_{max}$ is the maximum speed limit of the road; $L_i$ is the lane that the vehicle is currently in; $I_i$ is the category of the vehicle ($V_i$=1 or 2 indicates a CAV with different driving task, otherwise it is a human-driven vehicle); $H_i$ includes the headways between the i-th vehicle and the vehicle immediately ahead of it in all lanes. $T_i$ includes the headways between the i-th vehicle and the vehicle immediately behind it in all lanes.\nIn the context of intelligent networking, we consider the interaction of multiple vehicles in a space, where each CAV is associated with other communicative vehicles in the scene. Specifically, we focus on the interaction between the i-th CAV and all vehicles j-th in the scene, denoted as $e_{ij} \\in {0,1}$, where $e_{ij}$ = 1 means that there is interaction between the i-th vehicle and the j-th vehicle, otherwise there is no interaction. In order to represent the spatial interaction behaviors, we make the following assumptions: 1) All CAVS can communicate and interact with each other; 2) CAVs can interact with HDVs within their surrounding perception range; 3) CAVs can interact with themselves. Based on the above assumptions, the adjacency matrix $E$ is obtained as:\n$E = \\begin{bmatrix}e_{11} & e_{12} & ... & e_{1n}\\\\e_{21} & e_{22} & ... & e_{2n}\\\\...\\\\e_{ij}\\\\...\\\\e_{n1} & e_{n2} & ... & e_{nn}\\end{bmatrix}$ (7)"}, {"title": "Decision-making with RL", "content": "As previously discussed, the state space consists of scene representation and motion information. Such a state representation has the following two advantages: 1) the traffic scene can be represented from multiple dimensions; 2) more effective information can be extracted by using the characteristics of different representations. We use Transformer to capture the interaction information between vehicles and scenes, and GNN to extract the spatial interaction behaviors of vehicles. This innovation fully utilizes the information sharing between CAVs by using the characteristics of different neural networks to process complex information in dynamic mixed traffic scenarios. Ultimately, the two information outputs are spliced and input into RL to improve the multi-vehicle collaborative decision-making training process. The formula for the $Q$ value of the driving actions of CAVs generated by RL is as follows:\n$Q(s, a) = \\phi^{RL}(Concat(X_L + H_l))$ (10)\nwhere $Q(s, a)$ represents the $Q$ value of the CAVs driving actions, $\\phi^{RL}$ represents the RL policy network, $X_L$ represents the output of the Transformer network, and $H_l$ represents the output of the GNN network.\nWe employ the classic reinforcement learning algorithm Deep Q-network (DQN) [26] to process the information output by the encoder to output multi-vehicle collaborative driving actions. DQN is a value-based reinforcement learning algorithm that introduces deep neural networks into Q-learning. By processing high-dimensional inputs, it achieves effective decision-making in complex environments. Q-learning is a model-free reinforcement learning algorithm that estimates the expected reward of taking an action in a given state by learning the state-action value function $Q(s, a)$. Q-learning needs to maintain a state-action value table and store a Q value for each possible state-action pair. When the number of states and actions increases, the required storage space and computing time will increase exponentially, limiting the application of Q-learning in high-dimensional environments. DQN approximates $Q(s, a)$ through the function $Q(s, a; \\theta)$, solving the problem that traditional Q-learning cannot effectively handle in high-dimensional state space. Specifically, the main idea of DQN is that Q value can be parameterized as $Q(s, a; \\theta)$ in the neural network, the state space is used as the input of the neural network, and the action that can obtain the maximum reward is selected as the output action in the action space according to the Q value. In addition, the parameters are updated by sampling from the replay pool and training another target network $Q(s, a; \\theta')$. The Q value calculation process is as follows:\n$Q^{DQN}_t = R_{t+1} + \\gamma \\arg \\max_a Q(s, a; \\theta')$. (11)\nIn our work, we utilize a single DQN to output the actions Q values of all CAVs at the same time, called MADQN [27]. Our objective is to maximize the cumulative reward of each episode, so the reward is the sum of the state values of all CAVs at the current moment."}, {"title": "Experiment", "content": null}, {"title": "Driving Environment", "content": "In order to evaluate the performance of GITSR in the driving environment, we build a challenging highway dual ramp exit scenario based on the FLOW [28] platform, as shown in Fig. 2. In a 400-meters-long highway, there are ramps exit at 250-meter and 370-meter respectively. There are two types of vehicles in the environment, the green cars represent HDVs, and the yellow and blue cars represent CAVs. Both types of vehicles enter from the left side of the highway, among which HDVs exit from the right side of the highway, and CAVs need to cooperate highly to complete the driving task of the yellow cars exiting from the first ramp and the blue cars exiting from the second ramp. The main road of the highway has 3 lanes and the ramp has 1 lane.\nWe established a simulation environment for multi-vehicle collaborative decision-making training based on the driving environment, and deployed HDVs and CAVs according to the parameters in Table I. In the simulation, the lateral and longitudinal control modules of the vehicle are included. The longitudinal acceleration action of HDVs is generated by the Intelligent Driver Model (IDM) [29], the lateral lane change model is LC2013 [30], and the driving action instructions of CAVs are generated by the Q value as mentioned before."}, {"title": "Multi-vehicle Decision-making Progress", "content": "As previously mentioned, the multi-vehicle collaborative decision-making process can be modeled as a MDP, which primarily consists of state representation, action space and reward function.\n1) State space s: At any time t, the state space $s_t = [SR; \\mathcal{N}]$ contains two parts of information. The scene information, $SR$ represents the local scene occupancy grid constructed by each CAV in an agent-centric manner. The motion information, $\\mathcal{N}$ contains the motion features of all vehicles within the scene.\n2) Action space a: In this study, we construct a discrete action space set with the aim that at each time step, all CAVs can learn lateral and longitudinal driving actions simultaneously. The lateral actions include changing lanes to the left, keeping lanes, and changing lanes to the right, and the longitudinal actions include accelerating, maintaining speed, and decelerating. Specifically, it can be expressed as follows:\n$\\mathcal{A} = {(a_{lc}, a_{acc})|a_{lc} \\in \\mathcal{A}_{lc}, a_{acc} \\in \\mathcal{A}_{acc}}$ (12)\nwhere $\\mathcal{A}_{lc} = {LC, LK, RC}$ and $\\mathcal{a}_{acc} = {AC, MS, DC}$.\n3) Reward function r: In decision-making models based on deep reinforcement learning, the ultimate model performance hinges on the reward function design and the weight distribution of the rewards [31]. The design of the reward function in this paper strikes a balance among traffic efficiency, driving tasks and safety.\nIn order to enhance the traffic efficiency of CAVs, a reward function is designed based on the overall average speed, aiming to encourage high-speed driving, which is specifically expressed as follow:\n$R_{speed} = \\frac{1}{m} \\sum_{i=1}^m \\frac{v_i}{V_{max}}$ (13)\nwhere $v_i$ is the speed of each CAV, $V_{max}$ is the maximum speed limit of the highway, and $m$ is the number of all CAVs in the scene.\nAnother component of the reward function is the intention reward. We modify the practice of some other works [32], [33], [34] that is, only reward when the vehicle reaches the target, because this may cause CAVs to change lanes arbitrarily on the highway to reach the target, which will disrupt the entire mixed traffic. Specifically, the intention reward is when a CAV is about to leave the next ramp. We design a reward area in the rightmost lane 50 meters before the exit. The specific schematic diagram is shown in Fig. 5. When the CAV is driving in the reward area (pink area), the closer it is to the ramp exit, the higher the reward it will receive. The formula is as follows:\n$R_{intention} = \\sum_{i=1}^m (1 - \\frac{I_i}{L_I})$ (14)\nwhere $I_i$ represents the distance from the exit ramp, and $L_I$ represents the length of the reward area, which is 50 meters.\nCollision penalty is the key to ensure that CAVs make safe decisions. We set $R_{collision} = -N_{collision}$, where $N_{collision}$ represents the number of collisions in each time step.\nFinally, the entire reward function is expressed as follows:\n$R = w_1 R_{speed} + w_2 R_{collision} + w_3 R_{intention}$ (15)\nwhere $w_1, w_2, w_3$ are weight coefficients."}, {"title": "Evaluating Indicator", "content": "In order to evaluate the performance of GITSR in a driving environment, we collect task success rate, number of collisions, average speed and return as evaluation indicators during training, which are described as follows:\n1) Task success rate:It quantifies the proportion of CAVs that successfully exit the designated ramp at the end of each training episode, reflecting the efficacy of the implemented training process strategy.\n2) Number of collisions: It tallies the total number of collisions among all CAVs at the end of each training episode, reflecting the safety of the driving strategy.\n3) Average speed: It calculates the average speed of all CAVs in the traffic flow at the end of each training episode, reflecting the efficiency of the driving strategy.\n4) Return: The cumulative return of each training episode reflects the comprehensive performance of traffic efficiency, safety, and effectiveness of all CAVs cooperative driving strategies."}, {"title": "Performance comparison and ablation experiments", "content": "In order to evaluate the performance of the GITSR algorithm and the significance of the framework design, we conduct the following performance comparison and ablation experiments.\nIn our work, we use Multi-Agent Deep Q-Networks (MADQN) as the baseline algorithm, and also compare it with the Transformer encoding only (MADQN_Transformer). In order to explore the role of agent-centric scene representation in multi-vehicle collaborative decision-making, we conduct the following two ablation experiments: 1) Comparing the results with and without scene representation in the MADQN algorithm; 2) Comparing the results of scene-centric direct input and agent-centric scene reconstruction in all algorithms."}, {"title": "Implementation details", "content": "The relevant parameters of our experiment are shown in Table II. The total number of training episodes is 3000. A warm-up phase of 20,000 steps is set before training. CAVs randomly execute actions and store them in the replay pool, which is defined as $\\pi(s) = random(a)$. During the training phase, CAVs make decisions based on Q values and the $\\epsilon$ exploration strategy. The $\\epsilon$ exploration strategy is that when CAVS make decisions at each step, there is an $\\epsilon$ probability to execute random actions, and a 1 - $\\epsilon$ probability to select a strategy based on the Q value. The specific formula is:\n$\\pi(s) \\begin{cases}random(a) & P = \\epsilon\\\\arg \\max Q(s, a) & P=1-\\epsilon\\end{cases}$. (16)\nWe use Adam optimizer in Pytorch to train the model with a learning rate of 1e-4. All methods are trained three times with random seeds, and each training takes about 6 hours on an Intel Core i9-10920 CPU and an NVIDIA GeForce RTX 3090 GPU."}, {"title": "Result and discussions", "content": "This section will present and analyze our experimental results, including comparisons with baseline methods and ablation experiments.\nFigure 6 shows the performance comparison between our method and the baseline methods. Overall performance from the reward return during the training process, it can be seen that the performance of GITSR is significantly better than MADQN_Transformer and MADQN, proving the effectiveness of the GITSR framework. The results of the task success rate show that it is necessary to model spatial interaction behaviors through GNN in the GITSR framework, which can improve the stability of multi-vehicle collaborative strategies. At the same time, we use the number of collisions in each episode to evaluate the safety of the algorithm, which shows that the self-attention mechanism of the Transformer encoder can help each CAV capture the traffic scene information around the vehicle and help CAVs make safe decisions. We hope that all CAVs can improve the overall efficiency of traffic flow while completing the driving task collaboratively. Fig. 7 shows the overall average speed of CAVs during the training process. GITSR achieves a good balance between safety and efficiency. The driving behavior of MADQN_Transformer is more conservative, resulting in slower speed, while MADQN sacrifices safety to maintain the highest speed, which is not conducive to autonomous driving decisions. In summary, GITSR shows better performance in many aspects compared with the baseline methods.\nWe are pleasantly surprised by the ablation experiment results comparing scene-centric direct input and agent-centric scene reconstruction among all algorithms, as shown in Fig. 8. The experimental results show that the agent-centric scene reconstruction method can help CAVs understand the surrounding traffic scenes more easily and effectively reduce the number of collisions. However, in terms of task success rate, scene-centric shows better performance. We speculate that because scene-centric does not need to re-infer the local traffic scenes of CAVs and uses a fixed coordinate system, it can directly obtain the target point of task completion from map features, which is interesting for downstream planning tasks. In addition, the scene-centric method has a smaller computational burden, while agent-centric needs to model all CAVs, which will be a computational bottleneck for large traffic scenes."}, {"title": "Conclusion", "content": "In this study, we introduce GITSR, an effective graph interaction Transformer-based scene representation reinforcement learning framework for improving collaborative decision-making of autonomous vehicles. The framework mainly includes: Transformer is used to encode agent-centric local scene input to capture interactive information of surrounding traffic scenes; GNN is used to refine the motion information of traffic participants to represent the spatial interaction characteristics of dynamic traffic scenes. Reinforcement learning algorithm MADQN splices the two parts of information as decision input and outputs collaborative driving behaviors. We verify it in a challenging interactive collaborative driving environment, and the results show that our method performs better than the baseline methods. We also study the performance and impact of different modules in GITSR and find that scene representation can help CAVs better understand the scene and effectively reduce the number of collisions. The scene-centric scene representation has a higher task success rate, while the agent-centric scene representation is better in terms of safety.\nIt is undeniable that although the current algorithm has achieved excellent performance in multi-vehicle collaborative decision-making, the increase in the number of CAVs will inevitably bring a higher secondary modeling burden in scene representation, which is not conducive to large-scale intelligent transportation. We believe that the efficient reasoning speed of scene-centric scene representation and its independence from the number of CAVs can enable better performance in large-scale scenes, especially in planning tasks. In future work, we will focus on exploring more effective scene representation framework in large-scale scenarios, improving the understanding of dynamic scenes and collaborative driving decision-making capabilities of CAVS."}]}