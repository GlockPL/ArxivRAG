{"title": "Action Quality Assessment via Hierarchical Pose-guided Multi-stage Contrastive Regression", "authors": ["Mengshi Qi", "Hao Ye", "Jiaxuan Peng", "Huadong Ma"], "abstract": "Action Quality Assessment (AQA), which aims at automatic and fair evaluation of athletic performance, has gained increasing attention in recent years. However, athletes are often in rapid movement and the corresponding visual appearance variances are subtle, making it challenging to capture fine-grained pose differences and leading to poor estimation performance. Furthermore, most common AQA tasks, such as diving in sports, are usually divided into multiple sub-actions, each of which contains different durations. However, existing methods focus on segmenting the video into fixed frames, which disrupts the temporal continuity of sub-actions resulting in unavoidable prediction errors. To address these challenges, we propose a novel action quality assessment method through hierarchically pose-guided multi-stage contrastive regression. Firstly, we introduce a multi-scale dynamic visual-skeleton encoder to capture fine-grained spatio-temporal visual and skeletal features. Then, a procedure segmentation network is introduced to separate different sub-actions and obtain segmented features. Afterwards, the segmented visual and skeletal features are both fed into a multi-modal fusion module as physics structural priors, to guide the model in learning refined activity similarities and variances. Finally, a multi-stage contrastive learning regression approach is employed to learn discriminative representations and output prediction results. In addition, we introduce a newly-annotated FineDiving-Pose Dataset to improve the current low-quality human pose labels. In experiments, the results on FineDiving and MTL-AQA datasets demonstrate the effectiveness and superiority of our proposed approach.", "sections": [{"title": "I. INTRODUCTION", "content": "ACTION quality assessment (AQA) [1]\u2013[7] has garnered significant attention within the computer vision com-munity, aiming to assess the quality of action execution. In practical applications, AQA plays a crucial role across various domains, such as automating evaluations in sports (e.g., gymnastics or diving) [2], [8]\u2013[10], providing corrective guidance in rehabilitation training [8], [11]\u2013[13], and obtaining performance feedback in skill learning. Unlike daily life videos, action videos in the AQA task are characterized by sequential processes and domain-specific execution standards, necessitating a deep understanding of actions and an accurate analysis of fine-grained sub-action features. These include subtle differences in limb and torso movements as well as variations in action durations. Additionally, performers in AQA tasks frequently exhibit rapid motion, which often leads to blurred frames, further complicating the assessment process.\nMost existing approaches [1]\u2013[5] treat the AQA task as either a regression problem or a pairwise comparison re-gression problem, typically focusing on coarse-grained action features and utilizing holistic video representations extracted from videos. However, as previously discussed, coarse-grained features are not adequately supportive of high-quality eval-uation, as holistic video representations struggle to capture the intricate variations between different sub-actions within a complete action sequence. Although impressive performance has been achieved by using powerful visual backbones to extract video features in recent works [14]\u2013[19], these meth-ods still face significant challenges. Specifically, performers, particularly professional athletes in sports videos, often exhibit rapid motion, accompanied by complex limb coordination and bending, resulting in frequently blurred video frames. This poses challenges for backbones to accurately capture fine-grained pose and physical structure differences among various athletes performing the same action.\nAdditionally, sports such as diving and gymnastics often comprise multiple discrete phases within a single performance [14], exemplified by the take-off, flight, and entry phases in a diving maneuver. Judges predominantly focus on the nuanced distinctions in athletes' executions across each sub-action, en-compassing the number of flips, twists, and the posture during flight. Existing methods [14], [20] typically segment input videos into fixed-frame clips and employ specific network structures to process these clips in a global manner. However, such approaches disrupt the temporal continuity of sub-actions, as the durations of phases often vary across different action categories. Identifying and segmenting each phase within an action sequence is believed to enable more precise analysis and conclusive evaluation [1], [2], [21]. Therefore, it is crucial to dynamically segment the videos. However, clips corresponding to different sub-actions often share similar visual backgrounds and features, which presents significant challenges for dynamic action segmentation.\nAs illustrated in Figure 1, we propose a hierarchically pose-guided multi-stage action quality assessment framework to address the aforementioned challenges. This framework processes action videos and corresponding skeleton sequences as inputs. Compared to blurred video frames, the skeleton sequences of performers offer precise spatial coordinates and motion trajectories, thereby providing more fine-grained sub-action features [22]\u2013[26]. The proposed framework primarily consists of two branches: a static branch and a dynamic branch. The static branch utilizes a static visual encoder to process video frames, aiming to retain more contextual details and enhance action representation. The proposed dynamic branch, comprising a dynamic visual encoder and a hierar-chical skeletal encoder, captures fine-grained spatiotemporal differences and physical priors to guide feature fusion through a multi-modal fusion module. During this process, the pro-cedure segmentation module distinguishes different stages of sub-actions and segments all features accordingly. Finally, the multi-stage features from both the static and dynamic branches are passed through the stage contrastive regression module to obtain discriminative features, leading to an understanding of differences in athlete actions across different videos. To optimize the model's understanding of different sub-actions, we design a stage contrastive loss to perform unsupervised training and learn the differences between sub-actions.\nIt should be noted that this paper is an extension of our con-ference paper [19]. Compared to the preliminary version, we leverage an additional skeletal modality to obtain hierarchical human pose features. Given the limitations in existing datasets characterized by the poor quality or absence of skeletal labels, we also present a newly-annotated FineDiving-Pose Dataset with refined pose labels, which are collected through a combi-nation of manual annotation and automatic generation to boost the related field further. Furthermore, we propose a multi-modal fusion module to integrate visual features and skeletal features and add a static branch to capture human static features. Additionally, we perform additional experiments on another public benchmark dataset (MTL-AQA [6]), and we conduct more detailed ablation experiments and then present more qualitative results to demonstrate the effectiveness of each component within the framework proposed in this paper. The contributions of this paper are summarized as follows:\n\u2022 We propose a hierarchical pose-guided AQA framework, which introduces the combination of static and dynamic visual branches to decompose pose information and guide the fusion of pose features with dynamic video features for capturing fine-grained spatiotemporal features.\n\u2022 We develop a procedure segmentation network, which divides the input sequences into multiple stages to learn broader contextual information, along with a stage contrastive learning regression module to enhance the model's ability to learn differences between sub-actions.\n\u2022 We introduce a newly annotated FineDiving-Pose Dataset to boost the AQA research further, which contains 12,722 human annotated pose labels and 288,000 automatically-annotated pose labels by our proposed annotated pipeline.\n\u2022 We conduct extensive experiments on two mainstream datasets, FineDiving [14] and MTL-AQA [6], to evaluate our method, demonstrating that our approach outperforms state-of-the-art methods."}, {"title": "II. RELATED WORK", "content": "AQA. Currently, the AQA task mainly follows two types of formulations: regression and pairwise ranking. Regression-based approaches [3], [4], [6], [14], [16], [27] are widely applied in sports such as diving, skiing, and synchronized swimming. Pirsiavash et al. [3] pioneered the using of regres-sion models for human action quality assessment, predicting action scores directly from video features. Parmar et al. [6] proposed a multi-task clip-level scoring method by utiliz-ing spatiotemporal action features. Tang et al. [4] proposed an uncertainty-aware score distribution learning framework, thereby addressing the uncertainties arising from subjective evaluations. Zeng et al. [27] proposed a hybrid method that integrates static and dynamic action features, accounting for the contributions of different temporal segments. Xu et al. [14] proposed a procedure-aware representation by designing a temporal segmentation attention module. Zhou et al. [16] proposed a hierarchical graph convolutional network (GCN) to refine semantic features, and aggregate dependencies for analyzing action procedures and motion units. While Pairwise ranking-based approaches [28]\u2013[33] tackle the challenge of distinguishing subtle differences between actions in similar contexts, by evaluating the score of a given video relative to other videos through pairwise comparisons. Yu et al. [2] were the first to propose a pairwise ranking-based approach for diving to learn relative scores. Li et al. [21] further enhanced the model's ability to capture subtle score differences by in-corporating Pairwise Contrastive Learning Network. Similarly, An et al. [19] proposed a multi-stage contrastive regression framework to efficiently extract spatiotemporal features for AQA. These methods primarily focus on evaluating entire video sequences either within the same video or across differ-ent videos. In contrast, our approach divides video sequences into different phases and then used the fused visual-skeletal representation, enabling fine-grained contrastive learning of motion differences at each stage.\nMultimodal Learning in AQA. Different modalities are lever-aged in AQA, which can be divided into vision-based methods and skeleton-based methods. Vision-based approaches typi-cally leverage powerful backbone architectures, such as C3D [34] and I3D [35]. Parmar et al. [36] were the first to utilize a segment-level C3D network [34] to extract features, with an LSTM network [37] to capture temporal relationships between video segments. Building on this, Li et al. [28] introduced a spatial attention network, which involved segmenting the video, performing random sparse sampling, and using RGB"}, {"title": "III. PROPOSED APPROACH", "content": "A. Overview\nOur proposed framework is illustrated in Figure 2. Firstly, a multi-scale visual-skeletal encoder is introduced to capture fine-grained spatiotemporal and static features from visual frames and skeletal pose. Next, the procedure segmentation module is designed to distinguish different temporal stages for both dynamic and static features. By comparing the action fea-tures across different stages, stage-level features are obtained. The segmented dynamic visual features and skeletal features are then processed through the multi-modal fusion module. Finally, a multi-stage contrastive learning regression method is utilized to learn discriminative representations to estimate the quality score.\nProblem Formulation. Given a pair of query video \\(V^q\\) and exemplar video \\(V^e\\), along with their respective skeletal sequences \\(P^q\\) and \\(P^e\\), the object is to predict quality score \\(\\hat{S}_q\\) of \\(V^q\\) based on the quality score \\(S_e\\) of \\(V^e\\), which is formulated as the following:\n\\[\\hat{S}_q = F(V^q, P^q, V^e, P^e | \\Theta) + S_e, \\tag{1}\\]\nwhere \\(F(\\cdot)\\) represents the overall network architecture, with \\(\\Theta\\) denoting the parameters of the model, \\(V \\in \\mathbb{R}^{T \\times H \\times W \\times C}\\)"}, {"title": "B. Multi-Scale Visual-Skeletal Encoder", "content": "As illustrated in Figure 2, we introduce the multi-scale visual-skeletal encoder that consists of three components: static visual encoder, dynamic visual encoder for extracting visual features, and the hierarchical skeletal encoder for cap-turing human pose features.\nStatic Visual Encoder: This component is designed to capture human static features and enhance the action repre-sentation at each temporal step. We adopt ResNet-50 [42] as the backbone, which places more emphasis on appearance information from individual RGB frames, compared to I3D [35]. The encoder consists of a ResNet-50 model denoted as \\(R\\) and a multi-branch downsampling module \\(Down_{mul}\\). Specifically, the ResNet model is primarily adopted to extract high-level global features from the input video, while the multi-branch downsampling module, consisting of a convolutional layer and three downsampling layers, captures local detailed information at different spatial scales. The static feature \\(F_{st}\\) can be obtained from this module as follows:\n\\[F_{st} = Down_{mul}(R(V)), \\tag{2}\\]\nwhere \\(V\\) represents query or exemplar video frames.\nDynamic Visual Encoder: This component is responsible for extracting spatiotemporal features and complex dynamic information from the video sequence, such as the sequence of an action from preparation to take-off. We adopt I3D [35] as the backbone network, denoted as \\(I\\), to encode the video input. To expand the temporal dimension's receptive field, mixed convolution layers are introduced before the max-pooling layer. Specifically, the query video \\(V^q\\) is fed into the backbone network \\(I\\), followed by convolution operations along the temporal axis to dilate the temporal dimension of \\(I(V^q)\\), and a max-pooling operation along the spatial axis, which is formulated as follows:\n\\[F_{dy} = maxpool(Conv_{mix}(I(V^q))), \\tag{3}\\]\nwhere \\(Conv_{mix}\\) represents the mixed convolution layers, consisting of three layers, and \\(F_{dy} \\in \\mathbb{R}^{T \\times D}\\) is the final visual spatiotemporal feature. Similarly, the exemplar video \\(V^e\\) also obtains the corresponding \\(F_{dy}^e\\).\nHierarchical Skeletal Encoder: Taking into account the kinematic principles and trajectory patterns, we observe that distant joints (e.g., hands and feet) execute both substantially and refined actions, while central joints (e.g., the torso) exhibit minimal actions yet are highly responsive to the overall body displacement. In light of these observations, we endeavor to decompose the full range of skeletal joints into three distinct semantic levels, differentiated by the amplitude and directionality of joint movements, as depicted in Figure 3.\nTo be specific, the lowest level \\(H_0\\) encapsulates the motion of the torso, which is indicative of the body's spatial dynamics within pose sequences. The highest level \\(H_2\\), characterized by the highest motion amplitude, captures the intricate and rich movements, encompassing joints that engage in agile and rapid sub-actions, such as the swift movements of the wrists and ankles observed in twists in straight, pike or tuck positions within diving scenario. The features derived from \\(H_2\\) are particularly fine-grained, as these motions are the focus of scrutiny during assessment. The intermediate level \\(H_1\\) serves as a bridge between \\(H_0\\) and \\(H_2\\), tasked with capturing the transitional aspects of sub-actions, exemplified by the changes in the angles of the elbow and knee from the take-off to the flight in diving scenario.\nAs illustrated in Figure 2 (c), the hierarchical skeletal encoder enhances feature capture capabilities at different levels through four parallel branches, including three graph convolu-tions. Each graph convolution corresponds to a specific skeletal level (e.g., \\(H_0\\)). For each level, we perform GCN operations to extract feature and concatenate the features of each level, which is formulated as\n\\[\\mathcal{H}^q = ||_{s \\in S} \\{\\mathcal{A} \\cdot MLP(P^q_s)\\}, \\tag{4}\\]\nwhere \\(\\mathcal{A} \\in \\mathbb{R}^{N_S \\times D \\times D}\\) denotes the skeletal node adjacency matrix for the \\(i\\)-th layer, and \\(S = \\{S_{id}, S_{cf}, S_{cp}\\}\\) denotes above-mentioned three pose skeleton subsets, and \\(S_{id}, S_{cf}, S_{cp}\\) indicate identity, centrifugal, and centripetal joint subsets, re-spectively. \\(MLP(\\cdot)\\) represents one layer MLP with parameters \\(W \\in \\mathbb{R}^{D \\times D}\\), \\(\\cdot\\) is the point-wise convolution operation, \\(||\\) denotes concatenation along with the channel dimension, and \\(\\mathcal{H}^q = \\{\\mathcal{H}_0, \\mathcal{H}_1, \\mathcal{H}_2\\}\\).\nThe three semantic levels defined above are introduced to model kinematic and motion information, but they exist as sparse graphs where the edges represent only physical connections. These edges are inadequate to capture implicit features that are embedded within the relationships among distant joint nodes. Therefore, we expand the human pose graph through connecting all nodes in adjacent levels, with"}, {"title": "C. Procedure Segmentation Module", "content": "During competition, judges typically focus on the perfor-mance of the athlete at different stages of the action [41], as-signing an overall score based on the effectiveness of each sub-action. For example, the athlete's movement can be divided into take-off, flight, and entry in the diving scenario. In our approach, we independently score the actions within each stage to achieve more accurate predictions. To enable the model to distinguish the significant variances between sub-actions, we propose a procedure segmentation network. We assume that the action video can be divided into \\(K\\) stages, and the goal of the procedure segmentation network is to predict the \\(K-1\\) moments where stage transitions occur. To capture long-term temporal dependencies, we propose a procedure segmentation network \\(P\\) based on the Bidirectional Gated Recurrent Unit (Bi-GRU) [44] followed by a fully connected layer and a softmax operation. Specifically, the dynamic visual feature \\(F_{dy}\\) is input to \\(P\\) to predict the stage transition labels, which can be expressed as follows:\n\\[P(F_{dy}) = [\\hat{a}_1, \\hat{a}_2, \\dots, \\dots, \\hat{a}_T]. \\tag{6}\\]\nHere, \\(\\hat{a}_i\\) denotes the predicted label of the \\(i\\)-th frame. If the predicted labels of the \\(i\\)-th frame and the \\((i - 1)\\)-th frame are identical, it indicates that both frames belong to the same sub-action stage, and we set \\(\\hat{a}_i = 0\\). Conversely, if the predicted labels differ, it signifies that the \\(i\\)-th frame marks the transition to the next sub-action stage, and set \\(\\hat{a}_i = 1\\).\nNext, we identify the frames with the highest probability as the prediction of stage transitions. The per-frame classifi-cation task is optimized using the cross-entropy loss. The loss function \\(\\mathcal{L}_{ce}\\) calculates the difference between the predicted stage transition moments and the ground truth, which can be formulated as follows:\n\\[\\mathcal{L}_{ce} = - \\sum_{i=1}^{K} CE(a_i, \\hat{a}_i), \\tag{7}\\]\nwhere \\(a_i\\) represents the ground truth of the stage transition label."}, {"title": "D. Multi-Modal Fusion Module", "content": "To integrate the visual spatiotemporal feature \\(F_{dy}\\) and the hierarchical skeletal feature \\(F_{sk}\\) together, we propose a novel multi-modal fusion module, as illustrated in Figure 4. The attention map from \\(F_{sk}\\) is added to the map of \\(F_{dy}\\) to enhance the model's ability to capture and understand human poses, ensuring accurate comprehension of target features across different perspectives or pose variations.\nSpecifically, the details of the fusion module are formulated in Equations 8 and 9. First, the \\(i\\)-th stage skeletal features \\(F_{sk}^i\\) and visual features \\(F_{dy}^i\\) within the \\(K\\) stages are fed into individual linear layers to yield the queries \\(Q^i\\), keys \\(K^i\\), and values \\(V^i\\) with the same dimensions. The attention maps for the visual and skeletal features are then calculated separately and element-wise added to form a pose-guided attention map \\(\\alpha^i\\). The pose-guided attention map is then multiplied by the value \\(V^i\\) to obtain the attention output \\(Atten^i\\):\n\\[\\alpha^i = Softmax \\left( \\frac{K_{dy}^i Q_{dy}^i}{\\sqrt{D_{dy}}} + p \\frac{K_{sk}^i Q_{sk}^i}{\\sqrt{D_{pose}}} \\right), \\tag{8}\\]\n\\[Atten^i = Multihead(V_{dy}^i \\cdot \\alpha^i). \\tag{9}\\]\nwhere \\(p\\) represents a learnable weight parameter, \\(D_{dy}\\) and \\(D_{pose}\\) are the dimension values of the corresponding features, \\(i \\in [1, K]\\). The whole formulation of the multi-modal fusion can be expressed as follows:\n\\[H_{dy} = Atten^i + LN(F_{dy}^i), \\tag{10}\\]\n\\[F_{fu} = FFN(LN(H_{dy})) + H_{dy}, \\tag{11}\\]\nwhere \\(LN\\) and \\(FFN\\) denote Layer Normalization and Feed Forward Network, respectively."}, {"title": "E. Multi-stage Contrastive Regression Module", "content": "Stage-wise Contrastive Loss: After obtaining the segmented fused features \\(F_{fu}\\) and static features \\(F_{st}\\), we introduce a stage-wise contrastive loss, denoted as \\(\\mathcal{L}_{cont}\\), to further enhance the differentiation between sub-actions at various stages. We apply the stage-wise contrastive loss to \\(F_{fu}\\) and \\(F_{st}\\) separately, denoted as \\(F\\) in the following for simplification.\nFormally, we define a critic function \\(sim (F(q,k), F(e,k))\\), where \\(cos\\) denotes cosine similarity, and \\(norm\\) denotes a nor-malization function designed to enhance the critic's expressive capacity, as follows:\n\\[\\delta (F^{(q,k)}, F^{(e,k)}) = cos (norm(F^{(q,k)}), norm(F^{(e,k)})). \\tag{12}\\]\nDuring the calculation of the contrastive loss, we treat the features of the same stage in both the query video and the sam-ple video as positive pairs, denoted as \\(\\epsilon(F^{(q,k)}, F^{(e,k)})\\). While negative pairs \\((F^{(q,k)}, F^{(e,k)})\\) can be divided into inter-video or intra-video pairs. Inter-video negative pair \\((F^{(q,k)}, F^{(e,l)})\\) consists of features from different stages in different videos, while intra-video pair \\((F^{(q,k)}, F^{(q,l)})\\) refers to features from different stages within the same video. The positive pairs \\(\\epsilon(F^{(q,k)}, F^{(e,k)})\\) and negative pairs \\((F^{(q,k)}, F^{(e,k)})\\) can be formulated as follows:\n\\[\\epsilon(F^{(q,k)}, F^{(e,k)}) = e^{\\delta(F^{(q,k)}, F^{(e,k)})/T}, \\tag{13}\\]\n\\[(F^{(q,k)}, F^{(e,k)}) = \\frac{\\sum_{k=1;k \\neq l}^{K} e^{\\delta(F^{(q,k)}, F^{(e,l)})} + \\sum_{k=1;k \\neq l}^{K} e^{\\delta(F^{(q,k)}, F^{(q,l)})}}{\\text{inter negative pairs} \\quad \\text{intra negative pairs}}, \\tag{14}\\]\nwhere \\(T\\) denotes the temperature parameter and we set as 0.5 in practice. We define the pairwise objective for pair \\((F^{(q,k)}, F^{(e,k)})\\) considering the positive and negative terms as:\n\\[l(F^{(q,k)}, F^{(e,k)}) = log \\frac{\\epsilon(F^{(q,k)}, F^{(e,k)})}{\\epsilon(F^{(q,k)}, F^{(e,k)}) + (F^{q,k)}, F^{(q,k)})}. \\tag{15}\\]\nThe stage contrastive loss is defined as the average over all given video pairs, denoted as \\(\\mathcal{L}_{cont}\\), formulated as follows:\n\\[\\mathcal{L}_{cont} = \\frac{1}{2K} \\sum_{i=1}^{K} [l(F^{(q,k)}, F^{(e,k)}) + l(F^{(e,k)}, F^{(q,k)})]. \\tag{16}\\]\nScore Regression: We leverage the powerful representation capability of the Transformer to capture differences between the query pairs and the exemplar pairs across different stages. Specifically, we utilize a Transformer decoder \\(D\\) to calculate the differences between the query and exemplar features for the \\(k\\)-th stage:\n\\[f_{fu}^k = D \\left( F_{fu}^{(q,k)}, F_{fu}^{(e,k)} \\right), \\tag{17}\\]\n\\[f_{st}^k = D \\left( F_{st}^{(q,k)}, F_{st}^{(e,k)} \\right), \\tag{18}\\]\nwhere \\(D\\) in both branches shares the same structure but different parameters, \\(f_{fu}\\) and \\(f_{st}\\) represent fusion and static difference features, respectively. In each block of \\(D\\), cross-attention is adopted to calculate the differences between query \\(F^{(q,k)}\\) and exemplar \\(F^{(e,k)}\\). \\(F^{(q,k)}\\) refers to the fusion feature of the \\(k\\)-th stage of the query video.\nFinally, based on the generated difference fusion and static features, the stage contrastive regression network quantifies the action score difference between the query and exemplar videos by learning relative scores. The predicted score \\(\\hat{S}_q\\) for the query video \\(V^q\\) is calculated based on the exemplar video \\(V^e\\) as follows:\n\\[\\hat{S}_q = S_e + CR(F_{fu}, F_{st}), \\tag{19}\\]\n\\[CR(F_{fu}, F_{st}) = \\sum_{k=1}^{K} (M_{fu}(f_{fu}^k) + M_{st}(f_{st}^k)). \\tag{20}\\]\nwhere \\(F_{fu} = [f_{fu}^1, \\dots, f_{fu}^K]\\) and \\(F_{st} = [f_{st}^1, \\dots, f_{st}^K]\\), \\(CR\\) denotes the stage-wise contrastive regression, which calculates the relative score difference between query video and exemplar video. This module incorporates two multi-layer perceptrons (MLPs) with ReLU activation, denoted as \\(M_{fu}\\) and \\(M_{st}\\) respectively. \\(\\lambda_k\\) is a learnable weight parameter for different stages, which is normalized to ensure the contributions of the various stages are appropriately balanced. \\(S_e\\) is the actual score of the exemplar video, and \\(K\\) is the number of stages.\nTo evaluate the accuracy of score prediction in the AQA task, we utilize the Mean Squared Error (MSE) as a metric. The MSE calculates the squared difference between the pre-dicted scores and the ground truth values as the following:\n\\[\\mathcal{L}_{aqa} = MSE(S^q, \\hat{S}^q), \\tag{21}\\]\n\\[MSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2, \\tag{22}\\]\nwhere \\(N\\) is the number of samples, \\(y_i\\) is the true value of the \\(i\\)-th sample, and \\(\\hat{y}_i\\) is the predicted value."}, {"title": "F. Optimization and Inference", "content": "Optimization. For each video pair in the training data with pose sequences and score label \\(S^q\\), the overall objective function for our task can be written as:\n\\[\\mathcal{L} = \\mathcal{L}_{aqa} + \\mathcal{L}_{ce} + \\mathcal{L}_{cont}. \\tag{23}\\]\nInference. For the query video \\(V^q\\) and the corresponding skeletal pose \\(P^q\\) in the test set, we adopt a multi-sample voting strategy to select \\(N\\) samples from the training set, obtaining the corresponding exemplar videos \\(\\{V_i^e\\}_{i=1}^N\\) and exemplar skeletal pose \\(\\{P_i^e\\}_{i=1}^N\\). We take \\(\\{(V^q, P^q, V_i^e, P_i^e)\\}_{i=1}^N\\) as inputs, with their corresponding scores \\(\\{S_i^e\\}_{i=1}^N\\). The inference process can be formulated as follows:\n\\[\\hat{S}^q = \\frac{1}{N} \\sum_{i=1}^{N} (F(V^q, P^q, V_i^e, P_i^e) + S_i^e). \\tag{24}\\]"}, {"title": "IV. FINEDIVING-POSE DATASET", "content": "To enable studies on fine-grained articulated action, we construct a new dataset termed FINEDING-POSE, which includes fully-annotated human skeletal pose labels."}, {"title": "A. Data Source", "content": "The widely-adopted FineDiving Dataset [14] and MTL-AQA Dataset [6] both lack human pose labels. The Fine-Diving Dataset contains 52 different diving categories, which substantially overlap with the categories present in the MTL-AQA Dataset. Therefore, we source data from the FineDiving dataset [14] and annotate all samples in FineDiving. A total of 367 videos and 12,722 frames are manually labeled to extract paired 2D human pose keypoints and bounding box labels according to the raw resolution, without any cropping or scaling. Additionally, using an automated annotation method, we label 3,000 videos and 288,000 frames, each accompanied by annotated 2D keypoints and bounding box labels, with 96 frames per video."}, {"title": "B. Annotation Pipeline", "content": "Most of the related work only employs HRNet [45] to estimate the human pose keypoints and present suboptimal performance, which are attributed to the following limitations: 1) The videos depict complex diving actions of the athletes, and HRNet struggles to estimate the precise human pose, due to the poor image quality resulting from high-speed motions and extreme body distortions. 2) The presence of both athletes and spectators within the frame often results in the model incorrectly estimating the pose of the audience members. To address these limitations, we implement a new annotation pipeline that integrates both manual and automatic annotation methodologies. The details are presented as follows:\nCollection of manual annotated labels. To address the issue of blurred RGB images, we initially implement a series of data preprocessing steps to enhance the quality of the data and then manually annotate the skeletal keypoints. Specifically, we perform the cleaning of the dataset by removing synchronized diving videos and retaining 2,303 individual diving videos. We extract up to 10 videos per action number, based on 48 distinct diving action numbers, resulting in a total of 367 videos. The specific distribution of diving action numbers is provided in the supplementary material. We then extract the airborne movements of the divers across these videos, manually annotating each frame to construct the annotated dataset, which consists of 12,722 manually labeled images. These images encompass various poses, such as pike, tuck, and twist, with a resolution of 455 \u00d7 256 pixels for each image.\nWe utilize the LabelMe [46] tool for annotation and verifica-tion. The annotations adhere to the MPII data standard [47], encompassing the bounding box of the athlete and 16 pose keypoints. The bounding box is rectangular, with the top-left and bottom-right coordinates clearly annotated. The keypoints are represented as points, with the coordinates of their centers marked from 0 to 15. After the first round of annotation, the labels from one annotator are reviewed and refined by another annotator to ensure that each image contains annotations for both the bounding box and all 16 keypoints. Additional details about the keypoints and manual annotation process are provided in the supplementary material.\nCollection of automatic annotated labels. The 12,722 annotated images serve as the training set to finetune the HRNet. This process builds an athlete pose estimation network that achieves multi-scale fusion by exchanging information across parallel multi-resolution sub-networks. The network is responsible for resolving blurry images and extreme body dis-tortions, thereby enabling more accurate estimation of highly contorted human poses. Prior to performing pose recognition, we propose to detect human targets in the image, locating all bounding boxes, and then using the pose estimator to detect keypoints within those bounding boxes. For target tracking, we opt for a simple yet effective method: the nearest neighbor algorithm, rather than using deep learning-based approaches. The primary rationale behind this choice is that in extreme pose scenarios, such as diving, the variation in human pose distribution is significant. Additionally, interference from spec-tators, referees, and other normal human poses complicates the task of tracking divers effectively using deep learning methods.\nSpecifically, given the athlete coordinates in the \\(k\\)-th frame are \\(p_k = (x_1, y_1, x_2, y_2)_k\\), and the potential human coordinates in the next frame are \\((x_1, y_1, x_2, y_2)_{k+1}^i\\), where \\(i\\) represents the potential human target, then we calculate the distance between the \\(i\\)-th detected bounding box in the \\(k + 1\\)-th frame and other confirmed bounding boxes as follows:\n\\[d_{i,k+1} = \\sum_{j=1}^2 \\left((x_j^i - x_j)^2 + (y_j^i - y_j)^2\\right). \\tag{25}\\]\nAfterwards, we determine the final subject based on the confidence score and distance among human bounding boxes. Then, we adopt HRNet to obtain the more accurate athlete poses estimation based on the subject's coordinates, as the following:\n\\[P^q = \\frac{1}{T} \\sum_{i=1}^{T} HRNet(\\{V(x_1, y_1, x_2, y_2)_k\\}_{k=1}^{T}), \\tag{26}\\]\nwhere \\(\\{V_k\\}_{k=1}^T\\) denotes the cropped images from frame 1 to frame \\(T\\). Finally, for skeletal frames with incomplete joint sequences, joint interpolation is applied to reconstruct missing information. The interpolation formula is listed as follows:\n\\[P_k = P_i \\frac{k-i}{j-i} + P_j \\frac{j-k}{j-i}, \\tag{27}\\]\nwhere \\(p_i\\) and \\(p_j\\) represent the coordinates of the corresponding skeletal nodes, and \\(i, j\\) are the \\(i\\)-th and \\(j\\)-th frames respec-tively."}, {"title": "C. Visualization Analysis of Annotated Data", "content": "Figure 5 shows the visualized differences between our introduced data annotations and the skeletal poses annotated by HRNet. We show various actions in the visualizations to demonstrate the effectiveness of our approach"}]}