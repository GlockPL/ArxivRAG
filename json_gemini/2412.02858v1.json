{"title": "Unpaired Modality Translation for Pseudo Labeling of Histology Images", "authors": ["Arthur Boschet", "Armand Collin", "Nishka Katoch", "Julien Cohen-Adad"], "abstract": "The segmentation of histological images is critical for various biomedical applications, yet the lack of annotated data presents a significant challenge. We propose a microscopy pseudo labeling pipeline utilizing unsupervised image translation to address this issue. Our method generates pseudo labels by translating between labeled and unlabeled domains without requiring prior annotation in the target domain. We evaluate two pseudo labeling strategies across three image domains increasingly dissimilar from the labeled data, demonstrating their effectiveness. Notably, our method achieves a mean Dice score of 0.736 \u00b1 0.005 on a SEM dataset using the tutoring path, which involves training a segmentation model on synthetic data created by translating the labeled dataset (TEM) to the target modality (SEM). This approach aims to accelerate the annotation process by providing high-quality pseudo labels as a starting point for manual refinement.", "sections": [{"title": "Introduction", "content": "Histology images play a crucial role in the study of neurodegenerative disorders, such as Parkinson's [13] or Alzheimer's disease [9]. High resolution histology imaging of neurological tissues can be obtained using scanning (SEM) or transmission (TEM) electron microscopy, bright field imaging (BF) or Coherent anti-Stokes Raman spectroscopy (CARS), each producing images of different resolution and contrasts, making it difficult to train a segmentation model that works across a variety of imaging technologies and acquisition parameters. Recent advances in deep learning have largely improved models to segment microscopy images [19, 8]. For instance, the U-Net architecture [11], a CNN-based model has become a standard in biomedical image segmentation due to its ability to produce precise segmentation with the help of annotated data [4]. Unfortunately, one challenge in developing effective models of biomedical segmentation is the low amount of labels, which are modality specific. Annotating histology images is a labor and time-intensive process that requires expert knowledge. A new annotation strategy is to use image translation techniques to augment existing"}, {"title": "Methods", "content": "We have two similar image domains: L (labeled) and U (unlabeled). The task we perform on these two domains is shared in our case, segmentation of the axon and myelin tissues. Images X\u2081 sampled from L are annotated with segmentation labels YL, whereas images Xu sampled from U are unlabeled. Typically, we have a model ML pre-trained on {XL, YL} but it doesn't generalize well to U. Our goal is ultimately to generate pseudo labels Yu to bootstrap and accelerate the annotation of images Xu.\nAs shown in Fig. 1b, our approach is to perform image translation between domains L and U, resulting in synthetic images XL U and XU\u2192L. We denote the translation directions L \u2192 U as tutoring path and U \u2192 Las adaptive path. Using the tutoring path, we obtain a synthetic dataset {XL\u2192U, YL}, effectively recycling the previously available annotations. A segmentation model is trained on this synthetic dataset and subsequently applied to Xu to generate pseudo labels Yu,t. We denote this process pseudo labeling by tutorship, because we use the annotations of data from L to guide prediction on images from U, as illustrated in Fig. 1c.\nXL and Xu are unpaired, so we use an unsupervised image translation method based on cycle-consistency. Thus, although we are interested in the tutoring path yielding XL U, we also get Xu\u2192L from the adaptive path at no additional cost. We apply the pre-trained model ML on XU\u2192L to generate"}, {"title": "Data", "content": "The 3 unlabeled datasets used in this project are described in Table 1, along with the reference annotated TEM dataset. Note the diversity in input modality (TEM/SEM/BF), species, pathology, body part, spatial resolution and median size. This diversity is illustrated in Fig. S1 in the Supplementary Material for a dataset preview. The TEM dataset represents the XL annotated images across all experiments. We evaluate our pipeline in 3 different settings, using in turn the TEM-MACAQUE, SEM and BF datasets to simulate the unannotated data pool Xu. In reality, these datasets are annotated so the pseudo labels can be evaluated quantitatively. Conveniently, the XL data pool has the smallest pixel size (highest magnification). As such, X\u2081 is resized to match the Xu pixel size in all 3 experiments without significant loss of details."}, {"title": "Models", "content": "To perform unpaired modality translation, we utilized the SynDiff framework, which leverages conditional adversarial diffusion models for translation [10]. Training on unpaired data is enabled by the use of a cycle consistency loss on a pair of non-diffusive GANs which generate initial unrefined estimates of the target translations. These GANs employ a 3-layer discriminator and a 6-block ResNet generator, both utilizing instance normalization, adapting the architecture from the pix2pix repository [7]. Subsequently, these preliminary translations are passed as conditioning inputs to the denoising diffusion GANs, which are thus trained to produce translations between two modalities in an unpaired manner. At inference time, the non-diffusive modules are ignored, and the source image is directly used as the conditioning input for the denoising diffusion GANS.\nThe use of denoising diffusion GANs is motivated by resolving the trade-offs between the sampling efficiency of GANs and the training stability and mode coverage of diffusion models [17]. It has been demonstrated that with infinitesimally small time-steps, the reverse diffusion process mirrors the Gaussian nature of the forward diffusion process. Thus, if we model the forward diffusion process with a Gaussian distribution, we can assume the reverse process will also follow a Gaussian distribution [2, 14, 17]. As such, one major limitation of standard diffusion models is their requirement for a large number of timesteps with minimal variance steps for this assumption to hold, significantly increasing sampling time. This is suboptimal for tasks such as translating whole slide images. In contrast, GANs generate high-quality samples with low sampling times but often suffer from reduced sample diversity due to issues like mode collapse [12, 20].\nDenoising diffusion GANs combine the advantages of both approaches\u2014high quality and sampling speed with extensive mode coverage-by bypassing the assumption of normally distributed data at each diffusion step and using fewer"}, {"title": "Experiments", "content": "The following 3 experiments are increasing challenging, as the domains L and U are progressively more dissimilar. In each case, we evaluate pseudo labeling strategies using the tutoring and adaptive paths. Both methods are illustrated in Fig. 2.\nIn this experiment, both XL and Xu were acquired using transmission electron microscopy at similar magnifications. The main difference between these datasets is the provenance of the samples in terms of species, and a slightly different intensity profile (most likely due to sample preparation and acquisition parameters). This experiment acts as a baseline and shows how our pseudo labeling method performs on a small domain shift.\nHere, XL and Xu were acquired with different modalities. TEM and SEM have opposite contrasts (i.e. myelin is dark in TEM, white in SEM. See Supplemental Materials, Fig. S1). The species, body part and pixel size are also different, resulting in an intermediate domain shift.\nThis final experiment investigates a larger domain shift between XL and XU. The BF dataset exhibits a wide diversity across 4 organs, including 3 different species, pathology and pixel size."}, {"title": "Results and Discussion", "content": "Learning curves featuring the peak signal-to-noise ratio (PSNR), the structural similarity index measure (SSIM), and the L1-loss of the image translation models after an L \u2192 U \u2192 L translation cycle are reported in Fig. 3. These curves, along with visual assessment of the generation process, guided the selection of the best checkpoints. Specifically, checkpoints for TEM \u2194 TEM-MACAQUE, TEM \u2194 SEM, and TEM \u2192 BF were selected at epochs 100, 50, and 40, respectively.\nAs illustrated in Fig. 4, our pseudo labeling strategy is effective, especially using the tutoring path. The Dice score, sensitivity, and specificity computed on the pseudo labels, along with the standard deviations of each metric across all five folds of the nnU-Net segmentations, are reported in Table 2. For the TEM \u2192 TEM-MACAQUE experiment, using the pre-trained model provides a similar performance to our pseudo labeling strategies. This highlights the fact that when XL and Xu are sufficiently similar, using a model pre-trained on XL is more than enough to produce good pseudo labels. However, this is not the case for the two remaining experiments, where the pre-trained TEM model performance drops to zero, as expected due to larger domain shifts. Both the TEM SEM and TEM \u2192 BF experiments demonstrate the benefit of our pseudo labeling strategies. In the TEM \u2192 SEM case, the tutoring path provides a better prediction than the adapting path, the latter having a much lower sensitivity. Using the tutorship method, the mean Dice score averages 0.736\u00b10.005 for axons and 0.652\u00b10.005 for myelin, a remarkable starting point in the context of manual annotation. In the TEM \u2192 BF scenario, the adapting path marginally surpasses the tutoring"}, {"title": "Conclusion", "content": "Our results provide a better understanding of the optimal scenarios where leveraging image translation is advantageous for pseudo-labeling. When the target unlabeled dataset is sufficiently similar to the available labeled data, our pseudo labeling strategies do not provide a significant advantage over applying a pre-trained model, as demonstrated in the TEM \u2194 \u0422\u0415\u041c-\u041c\u0410CAQUE experiment. However, in both the TEM \u2192 SEM and TEM \u2192 BF scenarios, the target dataset is dissimilar enough that applying a pre-trained model is ineffective. In these cases, our pseudo labeling strategies are effective and provide useful initial masks to be manually corrected instead of annotated from scratch, saving precious time in the process. Realistically, a Dice score superior to 0.5 goes a long way, reducing the annotation time by 25%-50% (although this should be further quantified in future works). We recommend first using the adaptive path which does not require training an intermediate segmentation model. The tutoring path can provide a complementary set of pseudo labels, at the additional cost of training a proxy segmentation model. We hope this works inspires other groups to recycle available datasets when applicable to acquire more labeled data."}, {"title": "Appendix", "content": "{\\tilde{x}}_{A} = G_{A \\leftarrow B}(x_{B}), {\\tilde{x}}_{B} = G_{B \\leftarrow A}(x_{A}), x_{\\tilde{A}} = G_{\\oslash A}(x_{A}, y = {\\tilde{y}}_{B}, t), x_{\\tilde{B}} = G_{\\oslash B}(x_{B}, y = {\\tilde{y}}_{A}, t)  \nL_{G_{\\A}} = E_{\\tilde{y} \\sim p_{G_{\\A}}(y|x_{B})} [-\\log D_{\\oslash A} (\\tilde{y})]  \nL_{D_{\\A}} = E_{y \\sim p_{A}} [-\\log D_{\\oslash A} (y_{A})] + E_{\\tilde{y} \\sim p_{G_{\\A}}(y|x_{B})} [-\\log (1 - D_{\\oslash A} (\\tilde{y}))]  \nL_{G_{\\A}} = -E_{x_{t-1} \\sim q(x_{t-1}|x_{0}=G_{\\oslash A} (x_{A}, y = {\\tilde{y}}_{B}, t))} [-\\log D_{\\oslash A} (x_{t-1})]  \nL_{D_{\\A}} = E_{x_{t-1} \\sim q(x_{t-1}|x_{0}=x_{A})} [-\\log D_{\\oslash A} (x_{t-1})] + E_{x_{t-1} \\sim q(x_{t-1}|x_{0}=G_{\\oslash A} (x_{A}, y = {\\tilde{y}}_{B}, t))} [-\\log (1 - D_{\\oslash A} (x_{t-1}))]   \nL_{cycle} = E_{q(x_{A}, x_{B})} (\\lambda_{\\phi} (\\|x_{A} - x^{1}\\|_{1} + \\|x_{B} - x^{2}\\|_{1})  \nL_{G} = L_{cycle} + \\lambda_{\\phi} (L_{G_{\\A}} + L_{G_{\\B}}) + \\lambda_{\\oslash} (L_{G_{\\oslash A}} + L_{G_{\\oslash B}})  \nL_{D} = \\lambda_{\\phi} (L_{D_{\\A}} + L_{D_{\\B}}) + \\lambda_{\\oslash} (L_{D_{\\oslash A}} + L_{D_{\\oslash B}})"}]}