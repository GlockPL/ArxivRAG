{"title": "Disjoint Processing Mechanisms of Hierarchical and Linear Grammars in Large Language Models", "authors": ["Aruna Sankaranarayanan", "Dylan Hadfield-Menell", "Aaron Mueller"], "abstract": "All natural languages are structured hierarchically. In humans, this structural restriction is neurologically coded: when two grammars are presented with identical vocabularies, brain areas responsible for language processing are only sensitive to hierarchical grammars. Using large language models (LLMs), we investigate whether such functionally distinct hierarchical processing regions can arise solely from exposure to large-scale language distributions. We generate inputs using English, Italian, Japanese, or nonce words, varying the underlying grammars to conform to either hierarchical or linear/positional rules. Using these grammars, we first observe that language models show distinct behaviors on hierarchical versus linearly structured inputs. Then, we find that the components responsible for processing hierarchical grammars are distinct from those that process linear grammars; we causally verify this in ablation experiments. Finally, we observe that hierarchy-selective components are also active on nonce grammars; this suggests that hierarchy sensitivity is not tied to meaning, nor in-distribution inputs.", "sections": [{"title": "1 Introduction", "content": "In 1861, Broca found evidence that language processing functions are localized in specific brain regions. Since then, our mapping of the brain has advanced tremendously; we now know that functional specialization can arise not only from biologically coded mechanisms, but also from experience (Baker et al., 2007). More recently, there has been significant interest in understanding the mechanisms of language processing in large language models (Olsson et al., 2022; Hanna et al., 2023; Yu et al., 2023; Todd et al., 2024), whose inductive biases are more general than those of humans.\nSensitivity and functional selectivity toward the hierarchical structure of language is a hallmark of human language processing (Chomsky, 1957,"}, {"title": "2 Methods", "content": "We use Mistral-v0.3 (7B; Jiang et al., 2023), QWen 2 (0.5B and 1.5B; Yang et al., 2024), Llama 2 (7B; Touvron et al., 2023), and Llama 3.1 (8B and 70B; Grattafiori et al., 2024). We select these models because they are open-weights, relatively commonly used, and are currently among the best-performing open models. In all experiments, we use nucleus sampling (temperature= 0.1, p = 0.9) to reduce variance. We run all experiments on a GPU cluster containing 4 A100s (80G each). We used approxi-"}, {"title": "2.2 Data", "content": "We define 3 classes of hierarchical and linear grammars respectively in English, Italian, and Japanese, yielding 18 grammars total. Each sentence is generated using templates inspired by the constructs defined in Musso et al. (2003). For each structure, we generate positive and negative examples. Each grammar, its underlying rule, and examples of corresponding positive and negative examples are available in Tables 1 and 2. Our dataset consists of 7 verbs with at least 5 subjects and objects each; fully enumerated, we have 1106 positive-negative example pairs for each grammar. We use a 50/50 train/test split (for 553 pairs for each grammar).\nThe difference between hierarchical and linear grammars lies in whether their latent structure is explained via hierarchical or positional rules. Hierarchical grammars contain rules that conform to the hierarchical structure of natural language (Chomsky, 1957; Everaert et al., 2015). Linear grammars contain rules that are defined by word positions or relative word orderings\u2014e.g., insert a word at position 4. Such rules are argued to be impossible in human language (Chomsky, 1957, 1965).\nFor each grammar, a positive example is one that conforms to the rule, and a negative example is one that does not. For all hierarchical grammars, we form the negative example by swapping the final two words of a positive example. For all linear grammars that entail inserting a word at a specific position, we form the negative example by inserting the word at the final position. For linear grammars that entail reversing the word order, we form the negative example by swapping the final two words after reversing the input. For Italian last-noun agreement, the positive example is created by agreeing the gender of the determiner of the first noun with the gender of the last noun; we form negative examples by using a gender determiner that agrees with the first noun."}, {"title": "3 Experiments", "content": "We run 4 experiments to evaluate the behaviors and mechanisms of 6 LLMs in processing hierarchical and linear grammars. We use an in-context learning setup. All models are pre-trained on datasets primarily consisting of English sentences,"}, {"title": "3.1 Experiment 1: Are language models significantly more accurate at classifying the grammaticality of sentences from hierarchical grammars?", "content": "We first evaluate the accuracy of LLMs on grammaticality judgments given examples from each grammar. Musso et al. (2003) found that humans were more accurate at classifying examples of hierarchical grammars, even when they had no prior fluency in the test languages; we therefore hypothesize that a similar phenomenon would arise in LLMs if they contain functionally specialized regions for processing hierarchical structure.\nAs described in \u00a72, for each of the 18 grammars, we generate 1106 examples. We perform a uniform 50/50 split to obtain our train/test split. Given a grammar, we first prompt an LLM with an instruction describing the nature of the in-context task (see Appendix B.1.1). This is followed by 10 demonstrations that are uniformly sampled from the training split. For each demonstration, we use the format \u201cQ: {sentence}\nA: {answer}\", where sentence is the generated sentence, and answer is Yes if the sentence is a positive example, or No if it is a negative example. We ensure that each prompt contains exactly 5 positive and 5 negative examples; these can appear in any order. The model is then given the metalinguistic judgment task of generating \u201cYes\u201d or \u201cNo\u201d when given an example from the test split. We extract the probabilities of the \"Yes\" and \"No\" tokens to determine if the model made the correct prediction. We report the accuracy of all the models in Figure 1.\""}, {"title": "3.2 Experiment 2: Are the model components implicated in processing hierarchical structures disjoint from those implicated in processing linear structures?", "content": "Our behavioral evaluations suggest that LLMs are more accurate on grammaticality judgment tasks with hierarchical inputs, but this does not disambiguate whether models have separate mechanisms for processing hierarchical and linear grammars. If a model has specialized mechanisms for processing hierarchical and linear grammars, we hypothesize that the set of model components causally responsible for correct grammaticality predictions on hierarchical inputs should be different from those responsible for correct predictions on linear inputs.\nTo test this, we locate neurons in the model that are most sensitive towards processing hierarchical syntax. Specifically, we investigate dimensions of the output vector of the MLP and attention sub-modules in each layer. We test whether there is significant overlap between the neurons responsible for processing hierarchical and linear structures.\nRecall that we prompt the model with a task instruction followed by 10 uniformly sampled demonstrations of positive and negative examples. Given this prompt, we quantify the importance of each neuron z in increasing the logit difference m between the correct and incorrect answer tokens y and y' for a test sentence t. In other words, given a language model M, $m = M(t)_{y'} \u2013 M(t)_{y}$; $M(t)_{y}$ and $M(t)_{y'}$ are the logits corresponding to the correct and incorrect answer tokens. We compute the component $z$'s indirect effect (IE; Pearl, 2001; Robins and Greenland, 1992) on m given the test sentence t, and a minimally different sentence $t'$ that flips the correct answer from y to y'. Acti-"}, {"title": "3.3 Experiment 3: Does ablating hierarchy-sensitive components affect performance on linear grammars, and vice versa?", "content": "We have located neurons responsible for processing hierarchical and linear grammars. If these neurons are selective for only hierarchical or linear structure, then ablating them should selectively impact the model's performance on the grammaticality judgment tasks from \u00a73.1. We now perform an ablation experiment to causally verify this prediction.\nLet $a_i$ be the mean activation of neuron a at token position i across training examples. We first cache $a_i$ for each MLP and attention output dimension. We then run three additional iterations of the grammaticality judgment task from \u00a73.1, each while ablating a different set of components. (i) We ablate the union of the top 1% of neurons by IE across hierarchical grammars. (ii) We take the union of the top 1% of neurons across linear grammars, subsample to the same number of neurons as in the hierarchical union (subsampling procedure described below), and ablate this set. Finally, (iii) we ablate a random uniform subsample of neurons, where the number of ablated neurons is the same as in (i) and (ii). Sets (i) and (ii) are derived from \u00a73.3. We call the hierarchy-sensitive neuron set H and the linearity-sensitive neuron set L.\nDue to the strong overlaps between components responsible for processing hierarchical syntax and only minimal overlaps between components responsible for processing linear syntax, we observe"}, {"title": "3.4 Experiment 4: Are these neurons sensitive to hierarchical structure or in-distribution language?", "content": "Thus far, our results have been confounded by the fact that hierarchical sentences are commonly attested in the natural language corpora that LLMs are trained on, whereas linear sentences would be unlikely to appear. Thus, it is unclear if we have observed functional selectivity toward hierarchical language, or merely toward in-distribution language. To address this confound, we propose additional experiments using sentences constructed from nonce words what Fedorenko et al. (2016) call Jabberwocky sentences (abbreviated ZZ).\nWe define a bijective mapping from all words in the English grammars to nonce words; see Table 3 for examples. Then, we replicate our previous experiments on this set of out-of-distribution Jabberwocky sentences. By preserving the distinction between hierarchical and linear grammars and using a meaningless lexicon, we can disentangle hierarchy-sensitive mechanisms from mechanisms that are merely sensitive to natural language distributions resembling those in the training corpus.\nHypotheses. In humans, Jabberwocky sentences cause a smaller increase in neural activity as compared to natural sentences (Fedorenko et al., 2016), implying that the language processing regions of the brain are not sensitive to Jabberwocky sentences. If language models are similarly selective for meaningful inputs\u2014and therefore, if the H neurons from previous experiments are actually in-distribution-language neurons, and if the L neurons are actually out-of-distribution language neurons-then we expect the following three trends."}, {"title": "4 Discussion and Related Work", "content": "Acquiring syntax-selective subnetworks. We find behavioral and causal evidence supporting the hypothesis that hierarchical and linear grammars are processed using largely disjoint mechanisms in large language models. Thus, as in humans (Baker et al., 2007), general-purpose learners such as language models can acquire functionally specific regions. To some extent, linguistic functional selectivity in LLMs is surprising: humans process many more modalities and signal types than language alone, so functional specialization toward linguistic signals may be sensible as one among many modal specializations (Kanwisher, 2010). However, unimodal language models like those we test are exposed only to text. While not all of this text is natural language, one might expect a larger portion of the model to be responsible for processing hierarchical structure. These as well as our behavioral results extend prior evidence that pretraining induces preferential reliance on syntactic features over positional features (Mueller et al., 2022; Murty et al., 2023; Ahuja et al., 2024), and supports prior findings that there exist syntax-selective\u2014and more broadly, language-selective\u2014subnetworks in LLMs (AlKhamissi et al., 2024; Sun et al., 2024).\nOn human-likeness and learnability. Note that hierarchical functional specialization is not evidence that humans and LLMs process language in the same manner. Fedorenko et al. (2016) find that language processing circuits in the brain activate significantly less on Jabberwocky sentences, whereas we observe significant overlaps (albeit not complete) in these circuits in LLMs. This suggests some degree of selectivity for natural in-distribution language, as in humans, but the hierarchy-sensitive mechanisms are also more abstract and not tied to meaning as in humans.\nThere is evidence that hierarchical grammars are easier to learn than grammars that do not occur in human languages (Kallini et al., 2024; Ahuja et al., 2024). This could provide an explanation for why language models are so attuned to this structure and learn to explicitly represent it: it is easier\nto learn a hierarchical organization than flat organization of a vocabulary, and it may simply be a more efficient explanation of the distribution. That said, randomly shuffling input data does not seem to destroy downstream performance (Sinha et al., 2021), despite destroying performance on structural probing tasks (Hewitt and Manning, 2019). Future work should investigate the relationship between the syntax-sensitive components we discover and performance on downstream NLP tasks.\nMechanistic interpretability. Using causal localizations to investigate the mechanisms underlying model behaviors has recently become more popular (e.g., Wang et al., 2023; Hanna et al., 2023; Prakash et al., 2024; Merullo et al., 2024; Bayazit et al., 2024). While localization is not equivalent to explanation, it can reveal distinctions in where and how certain phenomena are encoded in activation space. Future work could employ techniques from the training dynamics and mechanistic interpretability literature to better understand how and when these components arise during pretraining, as well as the (presumably numerous) functional sub-roles of these distinct component sets.\nMore broadly, this work suggests a less-explored direction in interpretability based on high-level coarse-grained abstractions. Much recent work has aimed to discover more fine-grained and single-purpose units of causal analysis (e.g., sparse autoencoder features; Bricken et al., 2023; Cunningham et al., 2024; Marks et al., 2024); we believe that a parallel direction based in functionally coherent sets (or subgraphs) of components would yield equally interesting insights. For example, effective representations of syntax are a necessary condition for robust language understanding and generation; thus, we would expect the hierarchy-sensitive components we discover to be implicated in any NLP task if the model were robustly understanding the inputs. Therefore, not relying on these components could be a signal that models have learned to rely on some mixture of heuristics."}, {"title": "5 Conclusion", "content": "We have investigated whether there exist localizable and functionally distinct sets of components for processing hierarchically versus linearly structured language inputs. We find behavioral and causal evidence that these component sets are distinct, both in location and in their functional role in the network."}, {"title": "Limitations", "content": "We acknowledge that our work could be improved in several respects. First, neurons and attention outputs are problematic units of analysis due to polysemanticity (Elhage et al., 2022); i.e., observing the activations of a component is often not informative, as they are sensitive to many features simultaneously. Further, the component sets we analyze are unordered sets, which means that we do not yet understand how many distinct mechanisms are responsible for the behaviors we observe, nor what these mechanisms qualitatively represent. We have also not evaluated the effect of these components on tasks outside of grammaticality judgments; thus, we do not yet understand how selective nor how robust these behaviors or localizations are under different settings.\nSecond, the grammaticality judgment task may prime the model to be sensitive to valid linguistic structures more generally, rather than the structures that we present to the models. Therefore, we cannot confidently conclude that the significant accuracy differences we observe will generalize to other task settings or prompt formats given the same grammars."}, {"title": "A Methods", "content": "We define a series of hierarchical sentences in English, Japanese, and Italian."}, {"title": "A.1 Grammar rule descriptions", "content": "We define a series of hierarchical sentences in English, Japanese, and Italian.\n*   Declarative sentence: For English sentences, subjects and objects can be singular or plural nouns. Verbs agree with their subjects. IT sentences are Italian translations of the English sentences. Unlike Italian and English which have SVO word order, Japanese translations (JP sentences) have SOV word order.\n*   Subordinate sentence: In each language, matrix subjects, subordinate subjects, matrix objects, and subordinate objects can be singular or plural nouns. In English and Italian, verbs of the subordinate subject and the subject agree with their respective subjects in number. We generate subordinate clauses by using verbs which take complementizer phrases as objects (e.g., \"Tom sees that the dog carries the fish\"). English and Italian both place the main clause's verb before the start of the subordinate clause, whereas Japanese places the main verb after the end of the clause.\n*   Passive sentence: Subjects and objects can be singular or plural nouns. Verbs are always in the passive form. Like in (Musso et al., 2003), in the passive construction, we include the agent of a transitive verb in a prepositional phrase.\n*   Null subject sentence: This structure is restricted to Italian. We use the verb and object without the subject, since the use of the subject is not a strict requirement in Italian."}, {"title": "Linear Grammars", "content": "Similar to Musso et al. (2003), the linear sentences we test are constructed by breaking the hierarchical order between the subject and the nominal words. While our linear sentences use English, Italian, and Japanese lexicons, they break the hierarchical relationship between"}, {"title": "A.2 Dataset Description and Examples", "content": "Examples of all the grammars we construct in English, Italian, and Japanese may be found in Table 2. Examples of Jabberwocky sentences may be found in Table 3."}, {"title": "B Experiments", "content": ""}, {"title": "B.1 Experiment 1: Few-shot learning accuracy", "content": "Experiment 1 (\u00a73.1) assesses the model's performance on grammaticality judgments of hierarchical and linear structures. Here we share statistical comparisons of the accuracy distributions (Table 4), accuracy values by language (Figure 5 and Table 5) and grammar-wise accuracy values (Table 5)."}, {"title": "B.1.1 Example Prompts", "content": "We present example prompts from one of the hierarchical structures for each language. The prompt skeleton is in English, regardless of the language used for the examples. We intentionally strip the final whitespace after A:, as the model expects a leading space within the answer token (and thus, it should not be present in the prompt)."}, {"title": "B.2 Experiment 2: Identify MLP and Attention Components with the highest IE", "content": "Experiment 2 locates MLP and Attention neurons that are implicated in processing hierarchical and linear structures, and investigates if these components are disjoint. The language-level pairwise overlaps for all models across English, Italian and Japanese hierarchical and linear inputs, as well as Jabberwocky hierarchical and linear inputs is given in Figures 7. Grammar wise overlaps for MLP and attention components for English, Italian, and Japanese grammars are shown in Figures 8 and 11, respectively. Grammar wise overlaps for MLP and attention components for Jabberwocky grammars are shown in Figures 8 and 11, respectively. We also share results testing whether the pairwise mean overlaps of H, L, and HxL structures are significantly different among grammars using English, Italian, and Japanese versus Jabberwocky tokens in Tables 8 and 7."}, {"title": "B.3 Experiment 3: Ablations of top 1% of Attention and MLP Components", "content": "Experiment 3 considers selective ablations of hierarchy and linearity sensitive components, and evaluates how these ablations impact the accuracy of the model on the in-context learning task. We share ablation results by model in Figure 12a for English, Italian, and Japanese grammars. Through model-wise comparisons, we find that relative accuracy decreases on hierarchical grammars are significantly different for English, Italian and Japanese grammars depending on whether hierarchical, linear, or uniformly sampled components are ablated (see Table 9). However, the same is not true for linear grammars\u2014relative accuracy decreases are not significantly different between ablations of hierarchical/linear components. In the case of Italian structures, ablating hierarchy-sensitive components appears akin to ablating uniformly sampled components.\nAdditionally, we also run ablation experiments on jabberwocky grammars using components that are sensitive to hierarchical or linear jabberwocky grammars. We share ablation results by model for"}, {"title": "B.4 Experiment 4: Are neurons identified in experiment 3 sensitive to hierarchical structure or in-distribution lexical tokens?", "content": "Experiment 4 considers selective ablations of the top 1% of hierarchy and linearity sensitive components, and evaluates how these ablations impact the accuracy of the model on the in-context learning task, when processing Jabberwocky grammars. If the neurons discovered in Experiment 3 are not sensitive to hierarchical structure and instead sensitive to in-distribution tokens, these ablations should not cause a decrease in model performance on Jabberwocky grammars which are composed of meaningless words. Alternatively, any decreases in model performance on Jabberwocky grammars should be caused by neurons in the L set which are, say, sensitive to out of distribution inputs. Ablation results by model are in Figure 13. We also present grammar-wise overlaps of the top 1% of attention and MLP neurons for hierarchical and linear English and Jabberwocky grammars in Figures 15 and 14 respectively, and show that the difference in overlaps between these grammars is statistically significant in Table 11. Then, we test the relative change in accuracy in Jabberwocky grammars after ablating components sensitive to hierarchical and linear English structures as well as uniformly sampled components. Ablating hierarchy vs. linearity sensitive components that are sensitive to the English task, causes a significantly different decrease in model performance on Jabberwocky hierarchical grammars. However, the same is not true for linear Jabberwocky grammars where ablating hierarchy sensitive components is no different from ablating linearity-sensitive or uniformly sampled components (see Table 12. This suggests that the components identified in Experiment 3 are at least partially sensitive to the structure of the inputs."}]}