{"title": "KEDformer:Knowledge Extraction Seasonal Trend Decomposition for Long-term Sequence Prediction", "authors": ["Zhenkai Qin", "Baozhong Wei", "Caifeng Gao", "Jianyuan Ni"], "abstract": "Time series forecasting is a critical task in domains such as energy, finance, and meteorology, where accurate long-term predictions are essential. While Transformer-based models have shown promise in capturing temporal dependencies, their application to extended sequences is limited by computational inefficiencies and limited generalization. In this study, we propose KEDformer, a knowledge extraction-driven framework that integrates seasonal-trend decomposition to address these challenges. KEDformer leverages knowledge extraction methods that focus on the most informative weights within the self-attention mechanism to reduce computational overhead. Additionally, the proposed KEDformer framework decouples time series into seasonal and trend components. This decomposition enhances the model's ability to capture both short-term fluctuations and long-term patterns. Extensive experiments on five public datasets from energy, transportation, and weather domains demonstrate the effectiveness and competitiveness of KEDformer, providing an efficient solution for long-term time series forecasting.", "sections": [{"title": "INTRODUCTION", "content": "Long-term forecasting plays a critical role in decision-making domains such as transportation logistics [1], health-care monitoring [2], utility management [3], and energy optimization [4]. However, as the forecasting horizon increases, computational demands and challenges in modeling complex temporal dependencies grow substantially. Traditional time series decomposition methods, although useful, often rely on linear assumptions, making them less effective in handling complex multivariate scenarios or un-predictable, non-stationary data patterns. These limitations limit their ability to capture the interplay between components such as trends, seasonality, and irregularities [5].\nRecent advancements have integrated deep learning approaches into the decomposition process to improve forecasting accuracy [6], [7]. For instance, by leveraging rep-resentation learning and nonlinear transformations, these methods aim to better capture dynamic dependencies and multi-scale interactions within time series data [8], [9]. More recently, transformers have excelled in various tasks, such as computer vision (CV) [10], [11], natural language processing (NLP) [12], and time series forecasting, due to their powerful modeling capabilities and flexibility. However, in long-term forecasting tasks, transformers still face significant challenges. For example, the computational complexity of the traditional self-attention mechanism is $\\mathcal{O}(L^2)$, where $L$ represents the sequence length. This quadratic growth leads to increasing demands on memory and computational resources, limiting the applicability of transformers in resource-constrained or real-time analysis scenarios [13], [14], [15], [16]. In addition, transformers often struggle to model long-term dependencies effectively due to noise interference, where irrelevant information weakens the attention distribution and degrades overall performance [16], [17]. Consequently, capturing long-term dependencies in time series data while ensuring computational efficiency over extended prediction horizons remains a significant challenge.\nTo address these challenges, we propose an end-to-end Knowledge Extraction Decomposition (KEDformer) framework for long-term time series prediction. First, we integrate sparse attention mechanisms with autocorrelation techniques within the model to reduce computational overhead and mitigate interference from irrelevant features. This integration reduces computational complexity from $\\mathcal{O}(L^2)$ to $\\mathcal{O}(L \\log L)$, significantly lowering memory usage and enhancing the model's ability to process long sequences. Moreover, the autocorrelation mechanism decomposes the time series data into seasonal and trend components, further improving prediction accuracy. This approach captures both short-term fluctuations and long-term patterns, making the predictions more consistent with real-world temporal dynamics. As a result, the proposed KEDformer framework not only addresses the computational bottleneck of traditional Transformers in long-term forecasting, but also enhances their performance and robustness in complex sequence tasks. In summary, the contributions of this study are as follows:\n\u2022 We introduce a knowledge extraction mechanism that combines sparse attention and autocorrelation to reduce the computational cost of the self-attention layer. This mechanism reduces the computational"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Transformer-based Long-term Time Series Forecasting", "content": "Transformer-based models have demonstrated exceptional performance in time series forecasting due to their powerful self-attention mechanism and parallel processing capabil-ities, excelling at capturing long-term dependencies and handling long-sequence data [18], [19]. However, traditional Transformer models still face several challenges in time series forecasting, such as high computational complexity and difficulty addressing noise issues in long-term dependencies [12], [15]. For instance, the core self-attention mechanism exhibits quadratic computational complexity with respect to sequence length, which limits its efficiency in long-sequence tasks [20]."}, {"title": "2.2 Decomposition of Time Series", "content": "Time series decomposition is a traditional approach that breaks down time series data into components such as trend, seasonality, and residuals, revealing the intrinsic patterns within the data [29], [30]. Among traditional methods, ARIMA [31] uses differencing and parameterized model-ing to decompose and forecast non-stationary time series effectively. In contrast, the Prophet model combines trend and seasonal components while accommodating external covariates [29], making it suitable for modeling complex time series patterns. Matrix decomposition-based methods, such as DeepGLO, extract global and local features through low-rank matrix decomposition, while N-BEATS employs a hierarchical structure to dissect trends and periodicity [30]. However, these approaches primarily focus on the static decomposition of historical sequences and often fall short in capturing dynamic interactions for future forecasting.\nMore recently, deep learning models have increasingly incorporated time series decomposition to enhance predic-tive power. For example, Autoformer introduces an em-bedded decomposition module, treating trend and seasonal components as core building blocks to achieve progressive decomposition and forecasting [32]. FEDformer combines Fourier and wavelet transforms to decompose time series into components of varying frequencies, capturing global characteristics and local structures while significantly re-ducing computational complexity and improving the accu-racy of long-sequence predictions. Similarly, ETSformer [33] adopts a hierarchical decomposition framework inspired by exponential smoothing, segmenting time series into level, growth, and seasonality components [34]. By integrating exponential smoothing attention and frequency-domain at-tention mechanisms, ETSformer effectively extracts key fea-tures, demonstrating superior performance across multiple datasets [35]. Inspired by these studies, our proposed KED-former approach integrates decomposition modules dynam-ically with a progressive decomposition strategy. This not"}, {"title": "3 METHODOLOGY", "content": ""}, {"title": "3.1 Background", "content": "The Long Sequence Time Forecasting (LSTF) problem is defined within a rolling forecasting setup, where predictions over an extended future horizon are made based on past observations within a fixed-size window [20]. At each time point t, the input sequence $X_t = \\{x_1,...,x_I\\}$ consists of observations with multiple feature dimensions, and the output sequence $Y_t = \\{y_1,...,y_{L_y}\\}$ predicts multiple fu-ture values. The output length $L_y$ is intentionally set to be relatively long to capture complex dependencies over time. This setup enables the model to predict multiple attributes, making it well-suited for time series applications."}, {"title": "3.2 Data Decomposition", "content": "In this section, we will cover the following aspects of KED-former: (1) the decomposition process designed to capture seasonal and trend components in time series data; and (2) the architecture of the KEDformer encoder and decoder.\nTime Series Decomposition To capture complex tem-poral patterns in long-term predictions, we utilize a de-composition approach that separates sequences into trend, cyclical, and seasonal components. These components cor-respond to the long-term progression and seasonal vari-ations inherent in the data. However, directly decompos-ing future sequences is impractical due to the uncertainty of future data. To address this challenge, we introduce a novel internal operation within the sequence decomposition block, referred to as the autocoupling mechanism in KEDformer, as shown in Figure 1. This mechanism enables the progressive extraction of long-term stationary trends from predicted intermediate hidden states. Specifically, we adjust the moving average to smooth periodic fluctuations and emphasize the long-term trends. For the length-L input sequence $X \\in \\mathbb{R}^{L \\times d}$, the procedure is as follows:\n$x_t = \\text{AvgPool}(\\text{Padding}(X))$\n$X_s = X - X_t$\n(1)\nwhere $x_s, x_t \\in \\mathbb{R}^{L \\times d}$ represent the seasonal part and the extracted trend component, respectively. We use AvgPool(\u00b7) for moving average and filling operations to maintain a constant sequence length. We summarize the above process as $X_s, X_t = \\text{MSTWDecomp}(x)$, which is a within-model block.\nModel input In Figure 1, the encoder's input consists of the past $I$ time steps, denoted as $X_{en} \\in \\mathbb{R}^{L \\times d}$. In the decomposition architecture, the input to the decoder is composed of both a seasonal component, $X_{des} \\in \\mathbb{R}^{(I/2 + O) \\times d}$, and a trend-cyclical component, $X_{det} \\in \\mathbb{R}^{(I/2 + O) \\times d}$, both of which are subject to further refinement. Each initialization consists of two elements: (1) the decomposed component derived from the latter half of the encoder's input, $X_{en}$, of length $I/2$, which provides recent information, and (2) placeholders of length $O$, filled with scalar values. The formulation is as follows:\n$X_{ens}, X_{ent} = \\text{MSTWDecomp}(X_{en:1})$\n(3)\n$X_{des} = \\text{Concat} (X_{ens}, X_O)$\n(4)\n$X_{det} = \\text{Concat} (X_{ent}, X_{Mean})$\n(5)\nwhere $X_{ens}, X_{ent} \\in \\mathbb{R}^{(I/2) \\times d}$ denote the seasonal and trend-cyclical components of $X_{en}$, respectively. The placeholders, labeled as $X_O, X_{Mean} \\in \\mathbb{R}^{O \\times d}$, are populated with zeros and the mean values of $X_{en}$, respectively.\nEncoder In Figure 1, the encoder follows a multilayer architecture, defined as\n$X_{en}^l = \\text{Encoder}(X_{en}^{l-1}),$\nwhere $l \\in \\{1,..., N\\}$ represents the output of the l-th encoder layer. The initial input, $X_{en}^0 \\in \\mathbb{R}^{L \\times d}$, corresponds to the embedded historical time series. The Encoder function, Encoder(\u00b7), is formally expressed as:\n$S_{en}^{l,1} = \\text{MSTWDecomp} (\\text{KEDA} (X_{en}^{l-1} + X_{en}^{l-1}))$ (6)\n$S_{en}^{l,2} = \\text{MSWTDecomp} (\\text{FeedForward} (S_{en}^{l,1}) + S_{en}^{l-1})$ (7)\n$X_{en}^l = S_{en}^{l,2}$\n(8)\nwhere $S_{en}^{l, i} \\in \\{1, 2\\}$ represents the seasonal component after the i-th decomposition block in the l-th layer.\nDecoder In Figure 1, the decoder has two roles: the accumulation of the trend time series part and the knowl-edge extraction stacking of the seasonal time series part. For example,\n$X_{de}^{l}, T_{de}^l = \\text{Decoder} (X_{de}^{l-1}, T_{de}^{l-1}),$\nwhere $l \\in \\{1,..., M\\}$ represents the output of the l-th decoder layer. The decoder is formalized as:\n$S_{de}^{l,1} = \\text{MSWTDecomp} (\\text{KEDA} (X_{de}^{l-1} + T_{de}^{l-1}))$ (9)\n$S_{de}^{l,2} = \\text{MSWTDecomp} (\\text{KEDA} (S_{de}^{l,1} + S_{de}^{l-1}))$\n(10)\n$S_{de}^{l,3} = \\text{MSWTDecomp} (\\text{FeedForward} (S_{de}^{l,2}) + S_{de}^{l-1})$\n(11)\n$X_{de}^l = S_{de}^{l,3}$\n(12)\n$T_{de} = T_{ae} + W_{1,1} T_{de} + W_{1,2} T_{de}^2 + W_{1,3} T_{de}^3$\n(13)\nIn this context, $S_{de}$ and $T_{de}$, where $i \\in \\{1,2,3\\}$, represent the seasonal and trend components, respectively, after the i-th decomposition block within the l-th layer. The matrix $W_{i,L}$, where $i \\in \\{1, 2, 3\\}$, serves as the projection matrix for the i-th extracted trend component, $T_{de}$."}, {"title": "3.3 Knowledge Extraction Process", "content": ""}, {"title": "3.3.1 Self-attention Mechanism", "content": "The canonical self-attention mechanism is defined by the tuple inputs Q, K, and V, which correspond to the query, key, and value matrices, respectively. This mechanism performs scaled dot-product attention, computed as:\n$A(Q, K, V) = \\text{Softmax} (\\frac{QK^T}{\\sqrt{d}}) V$ (14)\nwhere $Q \\in \\mathbb{R}^{L_Q \\times d}, K \\in \\mathbb{R}^{L_K \\times d}$, and $V \\in \\mathbb{R}^{L_V \\times d}$, with $d$ representing the input dimension. To further analyze the self-attention mechanism, we focus on the attention distribution of the i-th query, denoted as $q_i$, which is based on an asymmetric kernel smoother. The attention for the i-th query is formulated in probabilistic terms:\n$A(q_i, K, V) = \\sum_j \\frac{k(q_i, k_j)}{\\sum_j k(q_i, k_j)} v_j = \\mathbb{E}_{p(k_j|q_i)} [v_j]$ (15)\nwhere $p(k_j | q_i) = \\frac{k(q_i, k_j)}{\\sum_j k(q_i, k_j)}$ and $k(q_i, k_j)$ represents the asymmetric exponential kernel $\\exp {\\frac{q_i k_j^T}{\\sqrt{d}}}$. This self-attention mechanism combines the values and produces outputs by computing the probability $p(k_j | q_i)$. However, this process involves quadratic dot-product computations, resulting in a complexity of $\\mathcal{O}(L_Q L_K)$, which poses a significant limitation in memory usage, particularly for models designed to enhance predictive capacity."}, {"title": "3.3.2 Knowledge Selection", "content": "From Eq. (15), the attention of the i-th query across all keys is represented as a probability distribution $p(k_j | q_i)$, where the output is computed by aggregating the values $v_j$ weighted by this probability. High dot-product values between query-key pairs lead to a non-uniform attention"}, {"title": "3.3.3 Decoupled Knowledge Extraction", "content": "Period-based dependencies The period-based dependen-cies are quantified using the autocorrelation function, which measures the similarity between different time points in a time series, revealing its underlying periodic characteristics.\nFor a discrete time series $\\{X_t\\}$, the autocorrelation function is defined as:\n$R_{xx}(\\tau) = \\lim_{L \\to \\infty} \\frac{1}{L} \\sum_{t=1}^L X_t X_{t-\\tau},$ (18)\nWhere $\\tau$ represents the time lag, $L$ is the total length of the series, and $\\{X_t\\}$ and $\\{X_{t-\\tau}\\}$ are the values at the current and lagged time points, respectively. The autocor-relation function computes the cumulative similarity over lagged time intervals, reflecting the degree of self-similarity within the series for various time delays. Peaks in the au-tocorrelation values indicate potential periodicity and help identify the likely period lengths.\nBy identifying the peaks of the autocorrelation func-tion, the most probable period lengths $(\\tau_1, \\tau_2, ..., \\tau_k)$ can be determined. These period lengths not only capture the dominant periodic patterns in the series but also serve as weighted features, enhancing interpretability and predictive capabilities.\nTime-delay Aggregation The time-delay aggregation method for knowledge acquisition focuses on estimating the correlation of sub-sequences within a specific period. Therefore, we propose an innovative time-delay aggregation module that can perform hierarchical convolution opera-tions on sub-sequences based on the selected time delays $\\tau_1, ..., ..., \\tau_k$, thereby narrowing down the key knowledge weight matrix. This process captures sub-sequences from the same location and similar positions within the period, extracting the potential key-weight aggregation matrix. Finally, we apply the Softmax function to normalize the weights, enhancing the accuracy of sub-sequence aggregation.\nFor a time series $x$ of length $L$, after projection and filtering of the weight matrix, we obtain the query Q, key K, and value v. The knowledge extraction attention mechanism is then as follows:\n$\\mathcal{T}_1,..., \\mathcal{T}_k = \\text{arg Topk}_{\\tau \\in \\{1,...,L\\}} (\\mathcal{R}_{Q,K}(\\tau))$ (19)\n$\\mathcal{R}_{\\hat{Q},K}(\\tau) = \\text{Topu} (M(Q, K)) \\cdot \\mathcal{R}_{Q,K}(\\tau)$ (20)\n$\\mathcal{R}_{\\hat{Q},K}(\\tau_1),..., \\mathcal{R}_{\\hat{Q},K}(\\tau_k) = \\text{SoftMax} (\\mathcal{R}_{\\hat{Q},K}(\\tau_1),..., \\mathcal{R}_{\\hat{Q},K}(\\tau_k))$ (21)\n$\\text{KEDattention}(Q, K, V) = \\sum_{i=1}^k \\text{Roll}(V, \\tau_i) \\mathcal{R}_{\\hat{Q},K} (\\tau_i)$ (22)\nWhere $\\text{arg Topk}(\\cdot)$ is used to obtain the top k parameters of self-attention, and let $k = [c \\times \\log L]$, where $c$ is a hyperparameter. $\\mathcal{R}_{Q,K}$ represents the self-attention matrix between sequences Q and K. $\\text{Topu}$ selects the most important u queries in the weight matrix. $\\mathcal{R}_{\\hat{Q},K}$ represents the self-attention matrix after filtering between sequences"}, {"title": "4 EXPERIMENT", "content": ""}, {"title": "4.1 Datasets", "content": "Five public datasets across multiple tasks were used to eval-uate the effectiveness of the proposed KEDformer model as follows: (1) ETT [42]: This dataset comprises four sub-datasets-ETTh1, ETTH2, ETTm1, and ETTm2. The data in ETTh1 and ETTh2 were sampled every hour, while the data in ETTm1 and ETTm2 were sampled every 15 minutes. These datasets include load and oil temperature measure-ments collected from power transformers between July 2016 and July 2018. (2) Electricity [44]: This dataset contains hourly electricity consumption data from 321 customers, spanning from 2012 to 2014. (3) Exchange Rates [40]: This dataset records daily exchange rates across eight different countries from 1990 to 2016. (4) Traffic [41]: This dataset consists of hourly traffic data from the California Depart-ment of Transportation, capturing road occupancy through various sensors on the Bay Area Highway. (5) Weather [43]: This dataset includes meteorological data recorded every 10 minutes throughout the year 2020, with 21 indicators such as temperature and humidity. In accordance with standard protocols, all datasets were chronologically split into training, validation, and test sets. The ETT dataset was partitioned using a 6:2:2 ratio [42], while the other datasets followed a 7:1:2 split [40], [41], [43], [44]."}, {"title": "4.2 Implementation Details", "content": "For the Transformer model, due to its approach to han-dling time series data, residual connections are embedded within the decomposition blocks [27]. These blocks include processes such as moving averages to smooth out periodic fluctuations and highlight long-term trends in the data. By incorporating residual connections in this manner, the model is better equipped to learn and leverage the complex patterns within time series, thereby improving performance in long-term time series forecasting. Our method is trained using L2 loss with the ADAM [50] optimizer. The entire training process is initialized with a fixed random seed. The initial learning rate is set to $10^{-4}$, with a batch size of 32. The attention factor is set to 3, and the weight decay is set to 0.1. Training is stopped early after 10 epochs. All experiments are repeated three times and implemented in PyTorch [?], running on a single NVIDIA Tesla V100 32GB GPU [?].\nWe evaluated nine baseline methods for comparison. For the multivariate setting, we selected three Transformer-based models: Autoformer [27], which introduces decompo-sition blocks for trend-seasonality extraction and employs auto-correlation mechanisms to effectively capture long-range dependencies; Informer [14], which enhances perfor-mance in processing long-sequence data through its prob-abilistic sparse self-attention mechanism and self-attention distillation; and Reformer [36], which optimizes computa-tional efficiency and memory usage using Locality-Sensitive Hashing (LSH) and Reversible Layers. Additionally, we included two RNN-based models: LSTNet [37], which lever-ages adaptive feature selection and multi-scale forecasting for improved long-term time series prediction, and LSTM [38], which captures long-term dependencies using gated mechanisms. For CNN-based models, we selected TCN [39], designed to capture local patterns and long-range dependencies in time series data through causal and dilated convolutions.\nIn the univariate setting, we incorporated several com-petitive baselines: LogTrans [51], which improves the ef-ficiency and accuracy of Transformers in time series fore-casting with convolutional self-attention and sparse biases; DeepAR [47], which enhances forecasting accuracy by learn-ing complex patterns, including seasonality and trends, through deep learning techniques; Prophet [48], a model that combines statistical methods and machine learning to effectively handle strong seasonal patterns and multiple seasonal cycles; and ARIMA [49], which integrates autore-gressive (AR), integrated (I), and moving average (MA) components to effectively capture and forecast trends and seasonal patterns in time series data."}, {"title": "4.2.1 Performance Comparison", "content": "Multivariate results: In multivariate settings, KEDformer consistently demonstrated superior performance across all benchmarks, as shown in Table 1. Notably, under the input-96-predict-336 configuration, KEDformer achieved significant improvements across five real-world datasets, while its predictive performance on the weather dataset remained unchanged. The mean squared error (MSE) was notably reduced by 0.8% (0.339 \u2192 0.336) in the ETT dataset, by 0.8% (0.231 \u2192 0.229) in the Electricity dataset, by 10.4% (0.509 \u2192 0.456) in the Exchange dataset, and by 0.4% (0.622 \u2192 0.619) in the Traffic dataset. On average, KEDformer reduced MSE by 2.48% across these datasets. In particular, KEDformer demonstrated substantial improvements on the Exchange dataset, which is characterized by a lack of apparent period-icity. Moreover, the model's performance remained stable as the prediction length increased, indicating its robustness in long-term forecasting. This robustness is especially benefi-cial for practical applications, such as early weather warning systems and long-term energy consumption planning.\nUnivariate results: We present the univariate results for two representative datasets, as shown in Table 2. Com-pared to various baseline models, KEDformer has largely achieved state-of-the-art performance in long-term predic-tion tasks. Specifically, in the input-96-predict-336 configu-ration, our model reduces the Mean Absolute Error (MAE) on the ETTm2 dataset by 2.6% (0.305 \u2192 0.297). In the power dataset, which exhibits significant periodicity, KEDformer demonstrates its effectiveness. For the Exchange dataset, which lacks significant periodicity, KEDformer out-performed other baselines by 7.7% (0.539 \u2192 0.497), thereby demonstrating excellent long-term predictive power. Additionally, ARIMA achieved the best performance in the input-96-predict-96 configuration on the Exchange dataset; however, its performance declined in long-term forecasting settings. This decline can be attributed to ARIMA's strong ability to capture global features in time series data dur-ing processing. However, it is constrained by the complex temporal patterns in real-world time series, which pose challenges for long-term predictions."}, {"title": "4.3 Ablation research", "content": "This study evaluates the impact of the Knowledge Ex-traction Attention (KEDA) module on model performance through ablation experiments, testing three KEDformer variants: KEDformer, which completely replaces both the self-attention and cross-attention mechanisms with KEDA; KEDformer V1, which replaces only the self-attention mech-anism with KEDA while retaining traditional attention for cross-attention; and KEDformer V2, which uses traditional attention for both mechanisms. Experiments were con-ducted on the Exchange and Weather datasets, as shown in Table 3, where KEDformer achieved performance improve-ments in 14 out of 16 test cases, whereas KEDformer V1 showed enhancements in only 2 cases. Notably, KEDformer with the KEDA module demonstrated consistent improve-ments across all cases, confirming the effectiveness of KEDA in replacing attention mechanisms and significantly enhanc-ing model performance."}, {"title": "4.3.1 Time series decomposition effects on the mode", "content": "The integration of time series decomposition into the KED-former model substantially improves predictive accuracy by isolating seasonal patterns from trend components. This decomposition allows the model to focus on short-term fluctuations while preserving an understanding of long-term trends, which is crucial for accurate forecasting. As demonstrated in Figure 2, this enhancement can be at-tributed to several factors. First, by explicitly modeling seasonal variations, the model can adapt more effectively to recurring patterns, thus improving its ability to project future values based on historical data. Second, decompo-sition helps identify significant features within the data, enabling the model to prioritize relevant information during the forecasting process."}, {"title": "4.3.2 Effect of KEDformer number on Encoder and De-coder", "content": "In this study, we conducted comparative experiments using the Exchange dataset, varying the number of KEDformer mechanisms. The results, illustrated in Figure 3, demon-strate that the model achieves superior performance when the number of KEDformer mechanisms in the decoding phase exceeds that in the encoding phase. This improve-ment can be attributed to the model's enhanced ability to"}, {"title": "4.3.3 Effect of KEDformer on computational efficiency", "content": "We conducted experiments to evaluate the impact of increas-ing the number of KEDformer mechanisms on the compu-tational efficiency of the model, as shown in Figure 4. The results demonstrate that the model achieves improved effi-ciency as the number of KEDformer mechanisms increases across various datasets (ETTm1, ETTm2, and Weather). Notably, the time required for each epoch decreases sig-nificantly with the increase in the number of KEDformer mechanisms, with the most pronounced improvement ob-served in the ETTm1 dataset, where computation time drops from 794.0 seconds to 467.2 seconds. This enhancement can be attributed to the model's improved ability to capture temporal dependencies and optimize resource utilization, enabling parallel processing and more effective distribution of the computational load."}, {"title": "4.3.4 Efficiency analysis and performance analysis", "content": "In this study, we conducted an efficiency and performance analysis of models utilizing different self-attention mecha-nisms, as illustrated in Figure 5. On the Exchange dataset, the KEDformer model ranked third in terms of running time but achieved the highest prediction accuracy, thanks to its optimized knowledge extraction mechanism and seasonal trend decomposition approach. These features enhance the model's ability to capture key patterns in the time series. However, the model's performance may degrade when handling time series data without clear periodicity, as the seasonal trend decomposition may fail to effectively extract relevant information. Additionally, an inappropriate con-figuration of the number of KEDformer mechanisms can reduce efficiency and negatively impact the final prediction results."}, {"title": "5 CONCLUSION", "content": "In this study, we introduce KEDformer, a novel framework designed to address the computational inefficiencies inher-ent in the self-attention mechanism for long-term time se-ries forecasting. By leveraging sparse attention, KEDformer reduces computational complexity from quadratic to linear time, significantly improving processing speed. The frame-work integrates seasonal-trend decomposition and autocor-relation mechanisms, which greatly enhance the model's ability to capture both short-term fluctuations and long-term patterns in time series data. This dual approach minimizes information loss during prediction, enabling the model to more effectively align with real-world time dynamics. This alignment is particularly critical for accurate forecasting in domains such as energy, finance, and meteorology. Exper-imental results across several benchmark datasets demon-strate that KEDformer consistently outperforms existing Transformer-based models, highlighting its robustness and adaptability. These findings underscore the potential of KEDformer as a valuable tool for long-term time series forecasting.\nDespite these advancements, we acknowledge several limitations. First, when applied to non-periodic datasets, the effectiveness of KEDformer may decrease, as seasonal-trend decomposition and autocorrelation mechanisms may not provide significant benefits in such cases. Moreover, optimizing the weight balancing within the self-attention mechanism remains a challenging issue that requires further investigation. Future research will focus on enhancing KED-former's adaptability to a wider range of datasets, including those with irregular patterns. We also aim to refine the knowledge extraction process to extend the framework's applicability to other sequence-based tasks, broadening its practical utility beyond time series forecasting. In conclu-sion, KEDformer represents a significant advancement in addressing the challenges of long-term time series forecast-ing, offering a solution that balances both efficiency and accuracy."}]}