{"title": "FreeAugment: Data Augmentation Search Across All Degrees of Freedom", "authors": ["Tom Bekor", "Niv Nayman", "Lihi Zelnik-Manor"], "abstract": "Data augmentation has become an integral part of deep learning, as it is known to improve the generalization capabilities of neural networks. Since the most effective set of image transformations differs between tasks and domains, automatic data augmentation search aims to alleviate the extreme burden of manually finding the optimal image transformations. However, current methods are not able to jointly optimize all degrees of freedom: (1) the number of transformations to be applied, their (2) types, (3) order, and (4) magnitudes. Many existing methods risk picking the same transformation more than once, limit the search to two transformations only, or search for the number of transformations exhaustively or iteratively in a myopic manner. Our approach, FreeAugment, is the first to achieve global optimization of all four degrees of freedom simultaneously, using a fully differentiable method. It efficiently learns the number of transformations and a probability distribution over their permutations, inherently refraining from redundant repetition while sampling. Our experiments demonstrate that this joint learning of all degrees of freedom significantly improves performance, achieving state-of-the-art results on various natural image benchmarks and beyond across other domains.", "sections": [{"title": "1 Introduction", "content": "Data augmentations expand the training set by generating virtual samples through random transformations to the original ones. This mitigates the overfitting [41] problem when training large deep neural networks. However, designing a data augmentation pipeline for a given task requires domain knowledge, as different domains benefit from different types of transformations of different strengths. This requirement has been alleviated by Data Augmentation Search (DAS), which enables the automatic customization of data augmentation for different tasks and domains. DAS methods sample from an optimized data augmentation policy, that consists of the probability distribution of choosing transformations"}, {"title": "2 Related Work", "content": "To alleviate the domain-knowledge and manual labor required for choosing the right data augmentation policy for any given domain, AutoAugment (AA) [5] was the pioneering work to automate the search of data augmentations. This was done by modeling the policy search problem as a sequence prediction problem, and using a recurrent neural network controller in a Reinforcement Learning (RL) framework to predict the type and magnitude of at most two sequential transformations. Since RL tends to suffer from high variance, this method is sample inefficient and hence requires a large amount of data and compute. A following evolutionary approach, PBA [14], is based on sampling population of policies and hence still expensive. A Bayesian optimization [45] based method, FastAA [24] does not leverage gradient information, and thus in practice those all are not learned in an end to end manner and do not perform better than random search, e.g., RandAugment [6] and TrivialAugment [32]. Inspired by differentiable neural architecture search [28, 34\u201336], differentiable DAS formulate the search space of augmentations so that it can be optimized using gradient descent. FasterAA [10] minimizes the distance between the original and augmented image distributions, AdversarialAA [57] and TeachAugment [44] maximize the training loss with respect to the augmentation policy under some regularizations, and MADAO [11] minimizes the validation loss directly with Neumann"}, {"title": "3 Method", "content": "Data augmentation can be viewed as a sequential application of image transformations over an input image $X$, where an image transformation with an application magnitude $m$ transforms the input image into another augmented image $\\tau(X; m)$ of the same size. This magnitude defines how strongly to transform the input image, e.g., the rotation angle for a rotation transformation. Image transformations are selected from a predefined set {T1,...,TN} of N candidate elementary transformations.\nFollowing previous work [5, 10, 23, 24, 29], we define the augmentation policy as a probabilistic model $P$ that generates a sequence of elementary transformations with corresponding magnitudes, governed by parameters $\\varphi$ as outlined in Algorithm 1. Those parameters are divided into three groups, $\\delta$, $\\Pi$, and $\\mu$, associated with the different degrees of freedom."}, {"title": "3.1 1st Degree of Freedom: Depth", "content": "The policy depth refers to the number of sequential transformations applied to a single image. To dynamically determine this depth, inspired by [15,33], we induce a learnable Gumbel-Softmax distribution [18] over categorical depths. It is parameterized by a vector of logits $d$, where each element $d_k$ for $k \\in \\{0,..., K\\}$ represents the unnormalized log-probability of selecting an augmentation policy with depth $k$, as illustrated in Fig. 2 (Left). This results in a probability distribution over the depths that is smooth and differentiable:\n$d \\sim \\text{Gumbel-Softmax}(d; t)$, such that $P(d_k = 1 \\vert t) = \\frac{e^{(d_k+g_k)/t}}{\\Sigma_{k=0}^{K} e^{(d_k+g_k)/t}}$\nwhere $g_i$ are i.i.d samples from a standard Gumbel distribution, and $t$ is the temperature parameter. Effectively, the straight-through (ST) gradient estimator [2] is utilized to to efficiently learn the logits $d$ by backpropagating through the discrete choice of augmentation depth of maximal value in $d$, as demonstrated in Fig. 2 (Right)."}, {"title": "3.2 2nd and 3rd Degrees of Freedom: Types & Order", "content": "Learning the distribution over types and order of transformations, requires sampling a transformation for every augmentation layer. Doing so for each layer independently using the Gumbel-Softmax distribution, as in [54], permits the repetition of the same transformations in several layers.\nTo mitigate that, we view the search for the types and order of transformations as a matching problem, where we seek to match between N elementary transformations to K augmentation layers. Thus, we couple the different layers and learn a probability distribution for sampling an entire permutation matrix P of size N\u00d7K, where not only its columns are normalized but also its rows. Its kth column $P_{1:N,k}$ represents the distribution over transformations at augmentation layer k, as illustrated the middle of Fig. 1 and the top middle of Fig. 3.\nTo this end, we utilize the Gumbel-Sinkhorn operator [30] for differentiating through permutation sampling as an analogy of the Gumbel-Softmax for differentiating through categorical sampling."}, {"title": "3.3 4th Degree of Freedom: Magnitudes", "content": "We follow previous studies [29], which found uniform sampling of transformation magnitudes to work on par with more elaborate sampling strategies, while their range has more impact on the results. Thus, for the elementary transformation Ti of the kth augmentation layer we set $M_{ik} \\sim \\text{Uniform}(\\mu_{ik})$ with the ranges $\\mu_{ik} = (l_{ik}, U_{ik})$ consisting of the learnable lower and upper bounds $l_{ik}, U_{ik} \\in \\mathbb{R}$ respectively for $i \\in \\{1, ..., N\\}$ and $k \\in \\{1, . . ., K\\}$.\nIn order to learn those magnitude ranges via backpropagation, as demonstrated in Fig. 4, we utilize a differentiable implementation of the elementary transformations [40] and the reparameterization trick [20] to backpropagate"}, {"title": "3.4 Policy Search as a Bilevel Optimization", "content": "Once we outlined a fully differentiable way to sample from the introduced search space of data augmentation policies via Algorithm 1, we follow a large portion of differentiable DAS methods [23, 25, 27, 29, 31, 54] and learn the underlying parameters by solving a bilevel optimization problem,\n$\\min_{\\Phi} L_{val} (\\theta^* (\\phi))$\ns.t. $\\theta^*(\\phi) = \\text{argmin}_{\\theta} L_{train} (\\theta, \\phi)$\nwhere,\n$L_{val}(\\theta) = E_{(x,y) \\sim D_{val}} L_{CE} (M_{\\theta} (x), y)$\n$L_{train}(\\theta, \\phi) = E_{(x,y) \\sim D_{train}} E_{T \\sim P_{\\phi}} L_{CE} (M_{\\theta}(T(x)), y)$\nand $D_{train}, D_{val}$ are the train and validation splits respectively, $L_{CE}$ is the cross entropy loss, $M_{\\theta}$ is the model with parameters $\\theta$, and T is an augmentation pipeline sampled from the augmentation policy $P_{\\phi}$ parameterized by $\\phi$ according to Algorithm 1.\nDirectly solving the upper problem in Eq. (5) would require solving the lower problem in Eq. (6) for every optimization step over the upper problem. To avoid that, we revert to the single step approximation and optimize $\\theta$ and $\\phi$ alternately through stochastic gradient descent, with the update terms,\n$\\Delta_{\\theta_{i}} (\\phi) = \\nabla_{\\theta} L_{train} (\\theta_{i}, \\phi)$\n$\\Delta_{\\phi_{i}} = \\nabla_{\\phi} L_{val} (\\theta_{i} - \\eta \\Delta_{\\theta_{i}} (\\phi_{i})) = -\\eta \\nabla_{\\phi,\\theta} L_{train} (\\theta_{i}, \\phi_{i}) \\nabla_{\\theta} L_{val}(\\theta_{i})$\nThus $\\theta_{i+1}$ and $\\phi_{i+1}$ are updated by an optimizer of choice with $\\Delta_{\\theta_{i}}(\\phi_{i})$ and $\\Delta_{\\phi_{i}}$ respectively. In practice, we replace all expectations by estimates on a batch of data samples and sampled augmentations. The variance of those estimates with respect to the sampled augmentations is significantly reduced by sampling a distinct augmentation policy for each image in the batch rather than sampling once for the entire batch. This is enabled by an efficient implementation of Algorithm 1, inspired by [33] as detailed in the supplementary material. Furthermore, the computation of second order derivatives in Eq. (10) is reasonable since the size of $\\phi$ is much smaller than the size of $\\theta$, and thus it is performed efficiently using automatic differentiation software [37,39]."}, {"title": "4 Experiments", "content": "Next we describe our experimental setup and evaluation process in Sec. 4.1. In Sec. 4.2 our method is compared to previous work over a variety of common natural images benchmarks, as well as a variety of several other domains in Sec. 4.2. Additionally, in Sec. 4.3 we delve into an empirical analysis of insightful aspects of our approach including ablation studies of the main components of our method. This comprehensive evaluation showcases the effectiveness of FreeAugment and provides insights into its properties."}, {"title": "4.1 Experimental Setup", "content": "Benchmarks We evaluate the performance of FreeAugment on three standard benchmarks; CIFAR10, CIFAR100 [22], and ImageNet-100 [47], which are all natural images. We also evaluate on the DomainNet [38] dataset, which consists of six distinct domains; real, quickdraw, infograph, sketch, painting, and clipart images, with a total of approximately 0.6 million images of 345 categories. Following [29], we use a reduced train set of 50k images for the two largest domains, real and quickdraw, and leave the rest of the samples for testing, according to the published file names of train and test images for each domain by [29]. In alignment with previous work [5, 24, 27, 29, 32,58], Wide-ResNet40-2 and Wide-ResNet28-10 [56] architectures are used for CIFAR10/100 datasets and ResNet18 for ImageNet-100 and DomainNet [12].\nBaselines Following previous works [5,14,58], we compare FreeAugment to a baseline based on standard augmentations: random horizontal flipping and random pad-and-crop for CIFAR-10/100, an Inception-style preprocessing [53] for ImageNet-100, and a set of augmentations used by DomainBed [9] for domain generalization on DomainNet datasets, including random horizontal flipping, color jitter (which is a mix of brightness, contrast, and saturation) and random gray-scale. Additionally, we compare FreeAugment to many other existing methods, including: AA [5], PBA [14], FastAA [24], FasterAA [10], DADA [23], DABO [25], RandAugment (RA) [6], UniformAugment (UA) [26], TrivialAugment (TA) [32], TeachAugment [44] MADAO [11] DeepAA [58], SLACK [29], DRA [54], and DDAS [27].\nSearch Space FreeAugment's search space is designed to enable the joint optimization of augmentation policies across all 4 DoF depicted in Fig. 1. In all of our experiments we search across augmentation policies with the maximal depth of K = 7. We adopt AutoAugment's space of possible transformation types, except for SamplePairing [17] and Identity. These include: ShearX, ShearY, TranslateX, TranslateY, Rotate, Solarize, Posterize, Contrast, Color, Brightness, Sharpness, AutoContrast, Invert and Equalize. All of which but the last three are associated with learnable magnitudes, whose ranges are detailed in the supplementary materials. Following conventional training practices and previous work [5,14,32], cropping, horizontal flipping, and cutout were applied by default."}, {"title": "4.2 Comparison with Previous Approaches", "content": "CIFAR10 and CIFAR100 Table 1 provides performance evaluation of augmentation policies generated by FreeAugment on CIFAR10 and CIFAR100 for Wide-ResNet40-2 and Wide-ResNet28-10 compared to the results reported by other state-of-the-art methods. FreeAugment outperforms the sota on CIFAR10 for both architectures and on CIFAR100 for Wide-ResNet40-2, with comparable performance to many other DAS methods. Specifically, FreeAugment improves over the baseline of basic augmentations by +1.05%, +1.18%, +3.3%, +2.39% for Wide-ResNet40-2 and Wide-ResNet28-10 on CIFAR10 and CIFAR 100 respectively.\nImageNet-100 Table 2 presents a comparison to previous work on ImageNet-100. All methods were reproduced with the exact training configurations of TA for ResNet-18. Since SLACK officially published four generated augmentation policies, we average the performance of all methods over four random seeds. FreeAugment outperforms the Inception-style baseline of manual choice of augmentations by +1.78%, and surpasses the leading DAS methods compared to by up to +0.94%. Notably, our approach maintained this high level of accuracy despite a reduction in batch size from 128 to 64 images in a batch during the search phase, demonstrating its effectiveness in variance reduction of gradients with respect to the expectation over the augmentation policy, see Sec. 3.4.\nBeyond Natural Images For showcasing the robustness of FreeAugment to different domains beyond natural images, we experiment with the DomainNet"}, {"title": "4.3 Ablation Study and Empirical Analysis", "content": "In this section, we analyze the impact of each component of FreeAugment. We aim to demonstrate the superiority of jointly learning all four degrees of freedom, and compare the effects of employing a fixed-size augmentation policy against a soft distribution over policy depth. Our analysis also includes a comparison of two augmentation types learning methods, Gumbel-Sinkhorn and Gumbel-Softmax, to illustrate their impact on repetitive sampling of transformations, and later on the final model's performance.\nJoint Learning of All Degrees of Freedom We evaluate the benefits of jointly optimizing all four DoFs by freezing each one's distribution as a uniform distribution while learning the other three. Table 4 shows the results, indicating that the joint learning of all four degrees yields superior results compared to any variant where one is held frozen.\nFixed vs. Variable Policy Depth Free Augment samples from a learnable probability distribution over the depth of the data augmentation pipeline. Figure 5 shows the benefit of this approach compared to fixing the depth to each"}, {"title": "5 Conclusion", "content": "In this paper, we propose a novel formulation of the data augmentation search space that allows for the first time to jointly optimize all degrees of freedom of a data augmentation pipeline in an end-to-end manner. This is enabled by utilizing fully differentiable tools for learning the probability distributions over discrete options for the number of transformations in the pipeline, their continuous magnitudes, and their discrete types and order. We jointly learn (1) the categorical distribution over the first by utilizing the Gumbel-Softmax reparameterization trick, (2) the parameterized distribution over magnitudes by applying differentiable implementations of the transformation, and (3) the distribution over their order and types using the Gumbel-Sinkhorn operator. All are learned in an end-to-end manner by alternating gradient decent steps to solve a bilevel optimization problem, whose objective is that the augmentations make a model trained on the train set generalize better to a held-out validation set. Extensive experiments show that this effective way of optimizing the joint probability distribution over all degrees of freedom yields data augmentation policies that achieve state-of-the-art results across various benchmarks."}, {"title": "A Policy Search Cost", "content": "Tab. 5 summarizes the policy search cost for different DAS methods. While achieving better or comparable performance with all of those, FreeAugment finds augmentation policies with a relatively small cost, especially considering that the cost is reported for a single search, while some other methods have to repeatedly apply it for several policy depth values. Some other methods are limited to searching merely two consecutive transformations due to the exponential growth of the cost with the policy depth. FreeAugment is the only one to eliminate that entirely, requiring a single search to obtain all degrees of freedom, including policy depth, without resulting in iterative solutions."}, {"title": "B Found Augmentation Policies", "content": "Fig. 7 visualizes the data augmentation policies found by FreeAugment."}, {"title": "C Hyper-Parameters and Training Configurations", "content": "The configurations for policy search and evaluation of each dataset can be found in Tab. 6 and Tab. 7, respectively. Following previous work, both in search and evaluation, the classifier's learning rate is decayed according to a cosine annealing scheduler. Following TA [32], for CIFAR10/100 images we apply pad-and-crop to a resolution of 32 \u00d7 32 with a reflection mode and a pad length of 4, random horizontal flipping with a probability of 0.5, then our generated policy is applied, followed by cutout [8] with a length of 16. For ImageNet-100 and DomainNet we use random resized crops and scales between 0.08 and 1.0 to a resolution of 224 \u00d7 244 using bicubic interpolation, random horizontal flipping with a probability of 0.5, then our generated policy is applied, followed by cutout with a length of 75, which is about 1/3 of the image size, as done in [53]."}, {"title": "G Learning Magnitudes of Non-Differentiable Transformations", "content": "It is important to note that some transformations are associated with a learn-able magnitude but are not differentiable with respect to it (e.g., posterize and solarize). To get gradients for such transformations' magnitudes, we use the straight-trough estimator as:\n$\\frac{\\partial X^{k+1}}{\\partial M_{ik}} = \\frac{\\partial\\tau_{i}(X^k)}{\\partial M_{ik}} = 1$\nwhere $X^{k}$ is the input image of the kth policy layer, $X^{k+1}$ is the output image of the same layer, $\\tau_i$ is a non-differentiable elementary transformation, $M_{ik}$ is the sampled magnitude at the kth policy layer for $\\tau_i$, and 1 is a matrix from the same size as $X^k$ filled with ones. Namely, Eq. (11) means that the gradient of each pixel in the output image $X^{k+1}$ w.r.t. the sampled magnitude $X^{k+1}$ equals 1.\nIn practice, this trick can be easily implemented via:\n$\\hat{X}^{k+1} = X^{k+1} + M_{ik} - StopGrad(M_{ik})$,\nwhere StopGrad(\u00b7) is the analog to PyTorch's detach function, which returns the input value without passing its gradient. Note that $\\frac{\\partial X^{k+1}}{\\partial M_{ik}} = 0$, where 0 is a matrix with the same size of $X^{k+1}$ filled with zeros. Thus, the use of the trick in Eq. (12) eventually makes Eq. (11) hold."}]}