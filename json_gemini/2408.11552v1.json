{"title": "Explainable Deep Learning Framework for Human Activity Recognition", "authors": ["Yiran Huang", "Yexu Zhou", "Haibin Zhao", "Till Riedel", "Michael Beigl"], "abstract": "In the realm of human activity recognition (HAR), the inte-\ngration of explainable Artificial Intelligence (XAI) emerges as a critical\nnecessity to elucidate the decision-making processes of complex models,\nfostering transparency and trust. Traditional explanatory methods like\nClass Activation Mapping (CAM) and attention mechanisms, although\neffective in highlighting regions vital for decisions in various contexts,\nprove inadequate for HAR. This inadequacy stems from the inherently\nabstract nature of HAR data, rendering these explanations obscure. In\ncontrast, state-of-th-art post-hoc interpretation techniques for time se-\nries can explain the model from other perspectives. However, this requires\nextra effort. It usually takes 10 to 20 seconds to generate an explana-\ntion. To overcome these challenges, we proposes a novel, model-agnostic\nframework that enhances both the interpretability and efficacy of HAR\nmodels through the strategic use of competitive data augmentation. This\ninnovative approach does not rely on any particular model architecture,\nthereby broadening its applicability across various HAR models. By im-\nplementing competitive data augmentation, our framework provides intu-\nitive and accessible explanations of model decisions, thereby significantly\nadvancing the interpretability of HAR systems without compromising on\nperformance.", "sections": [{"title": "Introduction", "content": "The field of Human Activity Recognition (HAR) has seen significant advance-\nments in recent years, driven by the proliferation of wearable devices and the\ndevelopment of deep learning technique. HAR systems, which recognize com-\nplex human behaviors from sensor data, have a wide array of applications, from\nhealthcare monitoring to smart home systems. However, as these systems become\nmore integrated into daily life, the demand for explainable AI (XAI) within the\nHAR domain intensifies. The necessity for XAI stems from a growing need to\nmake the decision-making processes of AI systems transparent, ensuring their\nreliability and fostering trust among users.\nCurrent methods for providing explanations in AI, such as Grad-Class Acti-\nvation Mapping (CAM) and attention mechanisms, face significant challenges in\nthe HAR context. These techniques, while effective in visualizing influential data\nregions for decision-making in image-based models, struggle with the abstract\nnature of data in HAR. Such data often lack a visual component, making the ex-\nplanations generated by these methods less intuitive and harder to comprehend.\nIn addition to this, there are some time series interpretation methods such as\nMCXAI and SBXAI. although they explain decisions through the structural re-\nlationships of different cognitive blocks, they are still limited in their exploration\nof the data, e.g., the effect of data frequency on decision making.\nIn response to these challenges, we introduces a simple, novel, model-agnostic\nframework designed to enhance both the interpretability and performance of\nHAR models. By employing competitive data augmentation, our approach not\nonly aids in the generation of more intuitive explanations but also does so with-\nout the constraints of model-specific architectures. This method allows for a\nbroader application across various HAR models, addressing the limitations of\ncurrent explanatory techniques.\nOur contributions can be summarized as follows:\nwe propose a unique framework that not only addresses these gaps by offer-\ning a model-agnostic solution but also demonstrates how competitive data\naugmentation can simultaneously improve model interpretability and per-\nformance.\nExtensive experiments on five benchmark datasets with three state-of-the-art\nbase models validate the effectiveness of the proposed framework.\nThe source code of OptiHAR has been released http://www.github.com/...\n, enabling seamless integration into any given HAR models for boosting\ntheir interpretability and performance."}, {"title": "Related Work", "content": "Within the study of analyzing sequential data, methods for explaining mod-\nels without depending on their internal mechanisms can essentially be grouped\ninto three categories, reflecting the core elements they focus on for explanation:\ninstance-based (or feature-based) explanations, subsequence-based, and time-\npoint-based. These categories come with their own unique strategies and chal-\nlenges.\nInstance-based approaches utilize statistical methods to derive features, bas-\ning their explanations on how understandable these extracted features are.\nTS-MULE [15] evaluates the relevance of segments within the data by forming\nlocalized linear models, with techniques like Symbolic Aggregate approximation\n(SAX) used for segmenting the data into what can be described as cognitive\nblocks. SAX-VSM [16] segments the time-series data and constructs a vocab-\nulary of segments, which then helps in explaining the model's input through\nthese defined 'words'. Techniques like MCXAI [5] and SBXAI [4] delve into\nunderstanding the connection between these segments, with MCXAI analyzing\nspatial connections and SBXAI looking at the temporal flow through different\nsegments. These methods, while insightful, often overlook the deeper temporal\naspects and might combine various factors into their explanations, making them\ncomplex to grasp.\nSoundLime [11] introduces slight changes to the original sound files to create\nnew instances. The significance of each moment is evaluated by observing the\nchanges in the model's output for these modified instances. Tsinsight [18] uses a\nreconstruction method where an auto-encoder, trained on the dataset, attempts\nto replicate the input, aiding in the explanation process. Salience cam [21] creates\na visual map highlighting important parts of the input by examining how the\nmodel's output changes with respect to the input, aiming to pinpoint critical\nmoments. Nevertheless, this method does not adequately address how certain\ntemporal characteristics, like patterns or trends, influence the model, as it mainly\nfocuses on singular points in time."}, {"title": "Methodology", "content": "In this section, we delineate the proposed methodology in detail. Initially,\nwe elucidate the central concept underlying our explanatory framework. Sub-\nsequently, we introduce the data augmentation technique employed. Lastly, we\ndetail the comprehensive algorithmic process."}, {"title": "Competitive Data Augmentation and explanation", "content": "The central element of our proposed framework is the incorporation of data aug-\nmentation processes into both the model training and prediction phases. Figure 2\nillustrates the operational mechanics of this strategy. Data Augmentation (DA)\nemploys predefined transformations to create new data instances that retain the\noriginal data's semantic integrity, as outlined by Rumelhart et al. (1986). Dur-\ning the training phase, these transformations are applied to original samples\nwithin the dataset to produce variant samples. These variants are then utilized\nto enhance the model's training, bolstering its robustness and generalizability.\nAs depicted in Figure 2-b, this method refines the model's decision boundaries.\nIn this way, during the prediction phase, samples sharing similar distributions\nwith the augmented variants are classified into the same category as the original\nsamples. However, it is critical to note the scenario depicted in Figure 2-c, where\nvariants distribute near samples from different categories. Although transfor-\nmations influence the decision boundary, the prediction for a variant may shift\nduring prediction due to a higher sample density from other categories.\nThe preservation or alteration of model predictions for augmented variants\nduring the prediction phase facilitates the explanation of model decisions. For\ninstance, considering the 'segmentout' transformation depicted in Figure 3, if a\nmodel's prediction alters post-masking a data segment, that segment is deemed\ncritical for the model's decision. Conversely, if the prediction remains unchanged\npost-transformation, the segment is considered irrelevant for decision-making.\nThis principle, foundational to counterfactual-based explanation approaches, is\ninnovatively integrated into our data augmentation strategy during prediction.\nSimilarly, changes in model predictions following the addition of high-frequency\nnoise to data imply the significance of high-frequency elements in the original\ndata for predictions."}, {"title": "Data Augmentation", "content": "Figure 3 delineates the data augmentation strategies employed in our study, se-\nlected based on two pivotal criteria: realism and comprehensibility. Firstly, we\nprioritize augmentations that mimic perturbations plausible within real-world\nscenarios, aiming to ensure that our research aligns closely with practical appli-\ncations. This not only maintains the augmented data within the model's overall\ndistribution, mitigating additional computational strain, but also potentially el-\nevates the model's performance in realistic settings. Secondly, we emphasize the\nimportance of human interpretability. Given our objective to elucidate model be-\nhavior, the value of an explanation diminishes if it is not readily understandable\nby humans.\nJitter generates new data samples by adding random Gaussian noise. This\nprocess simulates the noise in the sensor and the real environment. Recognizing\nthat signals from different sensors possess unique value ranges, we modulate the\nnoise intensity based on the data's variance, formalized as:\n$X = x + normal(0, \\alpha\\cdot \\sigma^2)$,\nwhere a adjusts the noise magnitude and $\\sigma^2$ is the variance of the sensor's\nsignal. This method allows us to discern the influence of high-frequency and\nlow-frequency signals on model predictions.\nAdditionally, we simulate data loss scenarios that may occur during data\ncollection or transmission. The Clip operation truncates a time segment from\nthe signal, simulating temporal data loss, and compensates for the alteration\nby interpolating the truncated segment based on the linear model, ensuring\nconsistency with the Human Activity Recognition (HAR) models' fixed input\ndimensions. Similarly, the SegmentOut technique nullifies signals from selected\nsensors or signal segments, offering insights into the significance of specific nu-\nmerical segments in driving model decisions. These methodical manipulations\nfacilitate a deeper understanding of data segment relevance in model outcomes."}, {"title": "Framework", "content": "Figure 1 depicts the architecture of the proposed framework, which is bifurcated\ninto training and prediction phases. The framework incorporates a list of trans-\nformations for data augmentation. During each iteration of training, a subset of\nthese transformations is randomly chosen and applied to the training dataset,\nthereby augmenting its size and enhancing the informational depth of the data.\nThis augmented data serves as the foundation for model training. In the pre-\ndiction phase, a similar approach is employed wherein samples are modified\nusing randomly selected transformations. The model evaluates both the original\nand the transformed samples. Final predictions are aggregated through a voting\nmechanism based on the outcomes across these samples. Concurrently, the ra-\ntionale behind the model's decisions is elucidated by examining the distribution\nof these votes, providing insight into the model's predictive behavior."}, {"title": "Explanation", "content": "In this section, illustrated in Fig. 4, we demonstrate how the proposed approach\ninterprets the model's decisions using an example. The figure includes both solid\nand dashed lines representing the raw input data, with each line corresponding\nto the acceleration in the x, y, and z directions. The input data are regularized,\nand their values correspond to the vertical coordinates on the left side of the\nfigure.\nThe interpretation provided by the proposed method is determined by the\nchosen data augmentation techniques. In our experiments, we employed three\nmethods: jitter, clip, and segment Out. Each method provides a unique perspec-\ntive on the model's decision-making process.\nFirst, the jitter method adds noise of a specific frequency to the input signal.\nThis allows us to explore the frequency range to which the model is most sen-\nsitive. Information within this sensitive frequency domain is considered crucial\nfor the model's decision-making, as the introduction of noise in this domain can\nalter the model's decisions. In the figure, the blue shaded area represents the\nsensitive region, with its values corresponding to the vertical coordinates on the\nright side of the figure.\nThe clip method involves selecting a portion of the data as input to the\nmodel, while the segmentOut method replaces a portion of the data with zeros.\nBoth methods investigate the impact of specific data segments on the model's\ndecisions. In the figure, these effects are illustrated in two ways: Line Type: The\ndashed lines represent data segments deemed non-essential for decision-making,\nas the model can make the same decision even without this data. Red Region:\nThe red shaded area indicates the data segments crucial for the model's decisions.\nRetaining this information alone is sufficient for the model to determine its\ndecisions.\nBy applying these three data enhancement techniques, we have explored the\nsignificance of both regional and frequency domain information in the model's\ndecision-making process. Further insights can be gained by incorporating addi-\ntional data augmentation methods."}, {"title": "Evaluation", "content": "In the last section, we have demonstrate the explaination of the prediction, in this\nsection, we design two different experiments to answer the following questions\nabout the performance improvement: (i) How well does the proposed method\nperform compared to the state-of-the-art (SOTA) model-agnostic method? (ii)\nHow much does each component of the framework contribute to performance?\nIn the first experiment, we compare the performance of the given model in\nthe following three scenarios: (i) without using the proposed framework (denoted\nby Base); (ii) using the entire SOTA model-agnostic method ActivityGAN [10]\n(denoted by activityGAN); (iii) using the proposed framework (denoted by Op-\ntiHAR).\nTo detect the contribution of each component, in the second experiment, we\ncompare the model performance in four scenarios: (i) without using the pro-\nposed framework (denoted by Base), (ii) using only DA in the training process\n(denoted by DAug), (iii) using only CAWR (denoted by CAWR), and (v) using\nthe entire proposed framework (denoted by Opti)."}, {"title": "Benchmark Models", "content": "The Convolution Neural Network (CNN) architecture [9], Long Short-Term Mem-\nory (LSTM) architecture [3], and Transformer architecture [20] are the most\nwidely-used structures in the field of deep learning. Currently, these structures\nare also being applied in the context of Human Activity Recognition (HAR).\nIn order to validate the universality of the proposed framework, we constructed\nthree deep models using these three structures, based on the research by Ito et\nal. [6], Vaswani et al. [20], and Zhou et al. [22]. The architecture of each model\nis presented in Figure 5."}, {"title": "Benchmark HAR Datasets", "content": "To test OptiHAR in various scenarios and to keep consistency of the experiments\nwith other works, we employ five widely used benchmark datasets in HAR,\nnamely, DSADS [1], HAPT [14], OPPO [2], PAMAP2 [13], and RW [19].\nDSADS [1]. DSADS is a dataset that focuses on recognizing daily and\nsports activities. It includes sensor data from body-worn devices placed at spe-\ncific locations, such as the wrist or waist capturing movement and orientation\nduring various activities. The sensors are securely fastened to ensure accurate\nreadings while minimizing interference with the participants' movements.\nHAPT [14]. The HAPT dataset utilizes the embedded sensors in smart-\nphones, which are typically carried by participants in their pockets or attached\nto belts. The smartphones are equipped with accelerometers and gyroscopes to\ncapture the movements and orientations of the users' bodies during various activ-\nities. It is designed for addressing issues regarding the occurrence of transitions\nbetween activities and unknown activities to the learning algorithm.\nOPPO [2]. This dataset is aimed at recognizing activities of daily living\n(ADL) with inertial measurement unit worn at multiple locations on the partici-\npant body such as wrists, ankles, and chest using straps or adhesive patches. This\nsetup enables the collection of multi-modal sensor data to capture the body's\nmovements and orientations during activities of daily living.\n\u0420\u0410\u041c\u0410\u04202 [13]. This dataset includes sensor data from inertial measure-\nment units (IMUs) and heart rate monitors. The IMUs are typically worn on the\nparticipant's dominant wrists using straps or bands. The heart rate monitors are\nworn on the chest, typically utilizing chest straps. This configuration allows for\nsimultaneous measurement of body movement and heart rate during different\nphysical activities.\nRW [19]. This dataset focuses on recognizing real-world activities using\nwearable sensors. It captures sensor data from accelerometers and gyroscopes\nembedded in smartphones and smartwatches.\nThese datasets exhibit significant differences in sensor types, mounting loca-\ntions, sampling rates, and classification activities. To initially prepare the data,\nwe split the data using sliding windows. For the training and validation sets, we\nemploy a 50% overlap between adjacent windows, whereas for the test set, 90%\noverlap is adopted to better realistically represent the data slices in the actual\nexecution [7]. A summary of the main information regarding these datasets is\npresented in Table 1."}, {"title": "Experiment Setup", "content": "During the experiment, the parameters $N_1$ and $N_2$ of the proposed framework\nare set to 20 and 10, respectively. DA parameter a is set to 0.1. Clip ratio is set\nto 0.2. SensorOut & Segment Out ratio is set to 0.1. The initial repeat period of\nCAWR is set to 50. 50 transformation are generted with random parameter to\nform the transformation set.\nTraining. For all the experiment scenarios, we use an Adam optimizer [8]\nwith default parameterization and an initial learning rate of $10^{-3}$. Moreover, we\nemploy batch-training with batch size of 256. As the objective function, cross-\nentropy loss [17] is utilized for increasing the classification accuracy. We apply\nearly-stopping based on the validation loss and set its patience to be equal to\nthe CAWR repeat period. The maximal epoch for model training is set to 500.\nEvaluation. To examine the generalizability across different subjects, Leave-\nOne-Subject-Out (LOSO) cross-validation (CV) is utilized to evaluate the per-\nformance of the trained models. In addition, we employ the macro-averaged F1\nscore as the evaluation metric 1, which reflects the model's ability to identify each\nactivity without considering the unbalanced distribution of scoring categories.\n$F_1 = (S_p* S_n)/(S_p+S_n)$\n$S_n = TP/(TP + FN)$\n$S_p = TP/(TP + FP)$\nwhere TP represents the number of positive instances that were classified as\npositive, TN represents the number of negative instances that were classified as\nnegative, FP represents the number of negative instances that were classified as\npositive, FN represents the number of positive instances that were classified as\nnegative.\nWe repeat the experiment five times using different random seeds (ranging\nfrom 1 to 5) and report the averaged results w.r.t. the random runs in Figure 6\nand Table 2."}, {"title": "Discussion", "content": "The result of the first experiment is summarized in Figure 6, each row in the fig-\nure corresponds to a dataset, and the columns from left to right in the figure cor-\nrespond to the performance of the specific models (MCNN, DCL, Transformer)\nunder scene basis, activityGAN and OptiHAR. The colours of each row are in-\ndependent of each other. In each row, darker colors indicate better performance.\nIt is evident that, the proposed framework leads to improved performance across\nall datasets and models except one (The performance of Transformer model on\nPAMAP2 dataset). The performance of OptiHAR exhibits an average of 4% im-\nprovement on the DSADS, HAPT, PAMAP2 and RW datasets when contrasted\nwith ActivityGAN. Further, on the OPPO dataset, the performance demon-\nstrates a significant improvement of 13%.\nTable 2 shows the result of the second experiment. It can be observed that\nsolely employing DA (which is corresponding to the scenario DAug) did not\nresult in a significant performance gain. CAWR also results in performance en-\nhancement in approximately half of the instances.\nIn addition, compared with other scenarios, the utilization of the proposed\nframework yields superior outcomes. This could be attributed to the incorpora-\ntion of other training techniques that enhance the generalizability of the target\nmodels.\nFrom the hardware aspect, it is also notable that, this framework does not\nnecessitate stronger GPUs for training, as it doesn't impact the density of the\nGPU workload, but rather only prolongs its working time due to the slower\nconvergence caused by data augmentation and CAWR. In execution, e.g., the\ntransformer model on the HAPT dataset takes 0.296 seconds to predict 6629\nsamples without the framework, while thanks to the parallelization function in\nPyTorch [12], the execution time is only increased by 9.8% even using the Opti\nframework."}, {"title": "Conclusion and future work", "content": "In this study, we present a novel framework designed to enhance both the per-\nformance and explainability of models in the field of Human Activity Recogni-\ntion (HAR). This framework is model-agnostic, allowing it to be applied across\nvarious HAR models. At its core, the framework utilizes a competitive data aug-\nmentation process that boosts model performance and predictive interpretability\nthrough dual-phase data enhancements during training and prediction. Addi-\ntionally, it integrates several techniques, including Domain Adaptation (DA),\nbagging, and Class-Aware Weight Regularization (CAWR), in a meticulously\ncrafted manner. This combination improves the overall performance of HAR\nmodels without significantly increasing resource requirements. Extensive experi-\nments demonstrate that OptiHAR is versatile and capable of delivering substan-\ntial performance improvements across different HAR models."}]}