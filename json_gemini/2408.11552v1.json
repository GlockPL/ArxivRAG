{"title": "Explainable Deep Learning Framework for Human Activity Recognition", "authors": ["Yiran Huang", "Yexu Zhou", "Haibin Zhao", "Till Riedel", "Michael Beigl"], "abstract": "In the realm of human activity recognition (HAR), the integration of explainable Artificial Intelligence (XAI) emerges as a critical necessity to elucidate the decision-making processes of complex models, fostering transparency and trust. Traditional explanatory methods like Class Activation Mapping (CAM) and attention mechanisms, although effective in highlighting regions vital for decisions in various contexts, prove inadequate for HAR. This inadequacy stems from the inherently abstract nature of HAR data, rendering these explanations obscure. In contrast, state-of-th-art post-hoc interpretation techniques for time series can explain the model from other perspectives. However, this requires extra effort. It usually takes 10 to 20 seconds to generate an explanation. To overcome these challenges, we proposes a novel, model-agnostic framework that enhances both the interpretability and efficacy of HAR models through the strategic use of competitive data augmentation. This innovative approach does not rely on any particular model architecture, thereby broadening its applicability across various HAR models. By implementing competitive data augmentation, our framework provides intuitive and accessible explanations of model decisions, thereby significantly advancing the interpretability of HAR systems without compromising on performance.", "sections": [{"title": "Introduction", "content": "The field of Human Activity Recognition (HAR) has seen significant advancements in recent years, driven by the proliferation of wearable devices and the development of deep learning technique. HAR systems, which recognize complex human behaviors from sensor data, have a wide array of applications, from healthcare monitoring to smart home systems. However, as these systems become more integrated into daily life, the demand for explainable AI (XAI) within the HAR domain intensifies. The necessity for XAI stems from a growing need to"}, {"title": "Related Work", "content": "Within the study of analyzing sequential data, methods for explaining models without depending on their internal mechanisms can essentially be grouped into three categories, reflecting the core elements they focus on for explanation: instance-based (or feature-based) explanations, subsequence-based, and time-point-based. These categories come with their own unique strategies and challenges.\nInstance-based approaches utilize statistical methods to derive features, basing their explanations on how understandable these extracted features are.\nTS-MULE [15] evaluates the relevance of segments within the data by forming localized linear models, with techniques like Symbolic Aggregate approximation (SAX) used for segmenting the data into what can be described as cognitive blocks. SAX-VSM [16] segments the time-series data and constructs a vocabulary of segments, which then helps in explaining the model's input through"}, {"title": "Methodology", "content": "In this section, we delineate the proposed methodology in detail. Initially, we elucidate the central concept underlying our explanatory framework. Subsequently, we introduce the data augmentation technique employed. Lastly, we detail the comprehensive algorithmic process."}, {"title": "Competitive Data Augmentation and explanation", "content": "The central element of our proposed framework is the incorporation of data augmentation processes into both the model training and prediction phases. Figure 2 illustrates the operational mechanics of this strategy. Data Augmentation (DA) employs predefined transformations to create new data instances that retain the original data's semantic integrity, as outlined by Rumelhart et al. (1986). During the training phase, these transformations are applied to original samples within the dataset to produce variant samples. These variants are then utilized to enhance the model's training, bolstering its robustness and generalizability. As depicted in Figure 2-b, this method refines the model's decision boundaries. In this way, during the prediction phase, samples sharing similar distributions with the augmented variants are classified into the same category as the original samples. However, it is critical to note the scenario depicted in Figure 2-c, where variants distribute near samples from different categories. Although transformations influence the decision boundary, the prediction for a variant may shift during prediction due to a higher sample density from other categories.\nThe preservation or alteration of model predictions for augmented variants during the prediction phase facilitates the explanation of model decisions. For instance, considering the 'segmentout' transformation depicted in Figure 3, if a model's prediction alters post-masking a data segment, that segment is deemed critical for the model's decision. Conversely, if the prediction remains unchanged post-transformation, the segment is considered irrelevant for decision-making. This principle, foundational to counterfactual-based explanation approaches, is innovatively integrated into our data augmentation strategy during prediction. Similarly, changes in model predictions following the addition of high-frequency noise to data imply the significance of high-frequency elements in the original data for predictions."}, {"title": "Data Augmentation", "content": "Figure 3 delineates the data augmentation strategies employed in our study, selected based on two pivotal criteria: realism and comprehensibility. Firstly, we prioritize augmentations that mimic perturbations plausible within real-world scenarios, aiming to ensure that our research aligns closely with practical applications. This not only maintains the augmented data within the model's overall distribution, mitigating additional computational strain, but also potentially elevates the model's performance in realistic settings. Secondly, we emphasize the importance of human interpretability. Given our objective to elucidate model behavior, the value of an explanation diminishes if it is not readily understandable by humans.\nJitter generates new data samples by adding random Gaussian noise. This process simulates the noise in the sensor and the real environment. Recognizing that signals from different sensors possess unique value ranges, we modulate the noise intensity based on the data's variance, formalized as:\n$X = x + normal(0, \\alpha \\cdot \\sigma^2)$,\nwhere $\\alpha$ adjusts the noise magnitude and $\\sigma^2$ is the variance of the sensor's signal. This method allows us to discern the influence of high-frequency and low-frequency signals on model predictions.\nAdditionally, we simulate data loss scenarios that may occur during data collection or transmission. The Clip operation truncates a time segment from the signal, simulating temporal data loss, and compensates for the alteration by interpolating the truncated segment based on the linear model, ensuring consistency with the Human Activity Recognition (HAR) models' fixed input dimensions. Similarly, the SegmentOut technique nullifies signals from selected sensors or signal segments, offering insights into the significance of specific numerical segments in driving model decisions. These methodical manipulations facilitate a deeper understanding of data segment relevance in model outcomes."}, {"title": "Framework", "content": "Figure 1 depicts the architecture of the proposed framework, which is bifurcated into training and prediction phases. The framework incorporates a list of transformations for data augmentation. During each iteration of training, a subset of these transformations is randomly chosen and applied to the training dataset, thereby augmenting its size and enhancing the informational depth of the data. This augmented data serves as the foundation for model training. In the prediction phase, a similar approach is employed wherein samples are modified using randomly selected transformations. The model evaluates both the original and the transformed samples. Final predictions are aggregated through a voting mechanism based on the outcomes across these samples. Concurrently, the rationale behind the model's decisions is elucidated by examining the distribution of these votes, providing insight into the model's predictive behavior."}, {"title": "Explanation", "content": "In this section, illustrated in Fig. 4, we demonstrate how the proposed approach interprets the model's decisions using an example. The figure includes both solid and dashed lines representing the raw input data, with each line corresponding to the acceleration in the x, y, and z directions. The input data are regularized, and their values correspond to the vertical coordinates on the left side of the figure.\nThe interpretation provided by the proposed method is determined by the chosen data augmentation techniques. In our experiments, we employed three"}, {"title": "Evaluation", "content": "In the last section, we have demonstrate the explaination of the prediction, in this section, we design two different experiments to answer the following questions about the performance improvement: (i) How well does the proposed method perform compared to the state-of-the-art (SOTA) model-agnostic method? (ii) How much does each component of the framework contribute to performance?\nIn the first experiment, we compare the performance of the given model in the following three scenarios: (i) without using the proposed framework (denoted by Base); (ii) using the entire SOTA model-agnostic method ActivityGAN [10] (denoted by activityGAN); (iii) using the proposed framework (denoted by OptiHAR).\nTo detect the contribution of each component, in the second experiment, we compare the model performance in four scenarios: (i) without using the proposed framework (denoted by Base), (ii) using only DA in the training process (denoted by DAug), (iii) using only CAWR (denoted by CAWR), and (v) using the entire proposed framework (denoted by Opti)."}, {"title": "Benchmark Models", "content": "The Convolution Neural Network (CNN) architecture [9], Long Short-Term Memory (LSTM) architecture [3], and Transformer architecture [20] are the most widely-used structures in the field of deep learning. Currently, these structures are also being applied in the context of Human Activity Recognition (HAR). In order to validate the universality of the proposed framework, we constructed three deep models using these three structures, based on the research by Ito et al. [6], Vaswani et al. [20], and Zhou et al. [22]. The architecture of each model is presented in Figure 5."}, {"title": "Benchmark HAR Datasets", "content": "To test OptiHAR in various scenarios and to keep consistency of the experiments with other works, we employ five widely used benchmark datasets in HAR, namely, DSADS [1], HAPT [14], OPPO [2], PAMAP2 [13], and RW [19].\nDSADS [1]. DSADS is a dataset that focuses on recognizing daily and sports activities. It includes sensor data from body-worn devices placed at specific locations, such as the wrist or waist capturing movement and orientation"}, {"title": "Experiment Setup", "content": "During the experiment, the parameters $N_1$ and $N_2$ of the proposed framework are set to 20 and 10, respectively. DA parameter $\\alpha$ is set to 0.1. Clip ratio is set to 0.2. SensorOut & Segment Out ratio is set to 0.1. The initial repeat period of CAWR is set to 50. 50 transformation are generted with random parameter to form the transformation set.\nTraining. For all the experiment scenarios, we use an Adam optimizer [8] with default parameterization and an initial learning rate of $10^{-3}$. Moreover, we employ batch-training with batch size of 256. As the objective function, cross-entropy loss [17] is utilized for increasing the classification accuracy. We apply early-stopping based on the validation loss and set its patience to be equal to the CAWR repeat period. The maximal epoch for model training is set to 500.\nEvaluation. To examine the generalizability across different subjects, Leave-One-Subject-Out (LOSO) cross-validation (CV) is utilized to evaluate the performance of the trained models. In addition, we employ the macro-averaged F1"}, {"title": "Discussion", "content": "The result of the first experiment is summarized in Figure 6, each row in the figure corresponds to a dataset, and the columns from left to right in the figure correspond to the performance of the specific models (MCNN, DCL, Transformer)"}, {"title": "Conclusion and future work", "content": "In this study, we present a novel framework designed to enhance both the performance and explainability of models in the field of Human Activity Recognition (HAR). This framework is model-agnostic, allowing it to be applied across various HAR models. At its core, the framework utilizes a competitive data augmentation process that boosts model performance and predictive interpretability through dual-phase data enhancements during training and prediction. Additionally, it integrates several techniques, including Domain Adaptation (DA), bagging, and Class-Aware Weight Regularization (CAWR), in a meticulously crafted manner. This combination improves the overall performance of HAR models without significantly increasing resource requirements. Extensive experiments demonstrate that OptiHAR is versatile and capable of delivering substantial performance improvements across different HAR models."}]}