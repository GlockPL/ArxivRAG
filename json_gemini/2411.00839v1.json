{"title": "CausAdv: A Causal-based Framework for Detecting Adversarial Examples", "authors": ["Hichem Debbi"], "abstract": "Deep learning has led to tremendous success in many real-world applications of computer vision, thanks to sophisticated architectures such as Convolutional neural networks (CNNs). However, CNNs have been shown to be vulnerable to crafted adversarial perturbations in inputs. These inputs appear almost indistinguishable from natural images, yet they are incorrectly classified by CNN architectures. This vulnerability of adversarial examples has led researchers to focus on enhancing the robustness of deep learning models in general, and CNNs in particular, by creating defense and detection methods to distinguish adversarials inputs from natural ones. In this paper, we address the adversarial robustness of CNNs through causal reasoning.\nWe propose CausAdv: a causal framework for detecting adversarial examples based on counterfactual reasoning. CausAdv learns causal and non-causal features of every input, and quantifies the counterfactual information (CI) of every filter of the last convolutional layer. Then we perform statistical analysis on the filters' CI of every sample, whether clan or adversarials, to demonstrate how adversarial examples indeed exhibit different CI distributions compared to clean samples. Our results show that causal reasoning enhances the process of adversarials detection without the need to train a separate detector. In addition, we illustrate the efficiency of causal explanations as a helpful detection technique through visualizing the causal features. The results can be reproduced using the code available in the repository: https://github.com/HichemDebbi/CausAdv", "sections": [{"title": "1 Introduction", "content": "Deep neural networks (DNNs), which represent an important class of ML models have shown a great success in many applications such as image classification [1], natural language processing [2], and two-player games [3]. DNNs could achieve good levels of precision, but they have a black box nature. As a result, extending their application to other fields such as: autonomous driving and avionics [4] and healthcare [5], the safety and trustworthiness of DNNs have been raised [6]. Compared to other ML models such as decision trees and Bayesian networks, diagnosing the inference process in DNNs is more challenging, due to the large number of learnable parameters, which increases as much as we go deeper in the network. In addition, they have various components such as activation functions, connectivity patterns and properties of the learning procedure, such as regularization and employed cost functions. From what precedes, it is evident that the ability of human users to understand and trust the decisions of DNNs is a very challenging task.\nThis blackbox nature of DNNs leads to discovering one of their most important vulnerabilities, which is the adversarial examples. [7] and [8] show that every DNN model is vulnerable to adversarial examples, which refer to malicious perturbations on the input image. Although these perturbations are imperceptible to humans, yet they succeed to fool the state-of-the-art models [9-12]. It has been found that adversarial examples can be applied across different models [12-14] and be harmful even in the real world [15], which raises many safety concerns especially in autonomous driving [16] and healthcare [17].\nThe act of generating adversarial examples to threat the security of DNNs systems is called adversarial attacks. Adversarial attacks range in different categories. Regarding adversary's knowledge, we have white and black box attacks. For white box attacks, the adversary has a full knowledge about the trained classification model including model architecture, hyperparameters, activation functions, and model weights. These attacks usually take the gradient of the predictions probabilities with respect to particular pixels. Some famous white box attacks are FGSM [18], PGD [19], BIM [20], and C&W [21].\nMost adversarial attacks are white-box attacks, however, due to the transferability of adversarial examples[22], they can be transferred to blackbox attack. In contrast to white box attacks, black box attacks such as Zoo[23] do not need to have access to the internal architecture of the pretarined models, and have knowledge only of the model's output, which reflects the real world scenario. Chen et al. [23] proposed Zeroth-Order Optimization (ZOO), a black-box attack that does not require gradients, and can be applied to both targeted and untargeted attacks. Adversarial training which represents a famous defense technique could remain actually vulnerable to black-box attacks.\nFrom the target label's perspective, we have targeted and untargeted attacks. Consider an input image x that is fed into a classifier Me(.), where e refers to the model's parameters. An adversarial example against Mo(.) is defined then as another image 2 such that || 1 - x || is small, but the classifier prediction is no longer the same, i.e Mo(x) \u2260 Mo(x). While untargeted attacks aim to generate any \u00ee that fools the model, targeted attacks in contrast generate \u00ee given a specific prediction label \u0177 such that Mo(x) = \u0177 and \u0177 \u2260 Mo(x). The small difference between the original image x and"}, {"title": "1 Introduction", "content": "the adversarial one \u00ee is subjected to a perceptibility threshold \u0454 : || x \u2212 x ||\u2264 6, where \u20ac > 0. Here the perceptible difference || || is usually obtained using the Euclidean norm || ||2 or the max-norm || ||\u221e, which is reduced to an optimization problem. While lo measures the maximum change that can be made for all the pixels in the adversarial examples, 12 norm measures the Euclidean distance between x and \u00ee.\nMany defense methods have been proposed in order to make CNNs more secure and robust against adversarial attacks. According to Papernot et al. [22], the works on defending against adversarial examples can be grouped into tow main categories: adversarial training and robust network architectures.\nAdversarial training (AT-based) techniques rely on injecting adversarial generated data in the training phase, and it is treated as a minimax optimization problem. Recently many other variants of AT have been proposed [24-26]. [27] proposed the Misclassification-Aware-AdveRsarial-Training (MART) process that supports adversarial data for model training by simultaneously applying the misclassified natural data. As AT leads to robustneess of CNNs, in the same time it could decrease the natural accuracy, in this regard, [28] proposed TRADES to achieve a better trade-off of adversarial robustness and accuracy. Moreover, different regularization techniques such as layer-wise feature matching [29, 30] and Lipschitz regularity ([31]) have been proposed to support AT. Although adversarial training represents the current state-of-the-art robustness method against white-box attacks [32], it could remain actually vulnerable to black-box attacks.\nAlthough AT has been shown to be the most effective strategy, from the activation perspective, the behavior of DNNs is still not understood how given the small perturbations accumulating across intermediate layers fool the network, and as a result, how adversarial training can deal with these activations. In this regard, many papers tried to tackle this aspect by proposing robust architectures designs [33-35], which represent the second category of defense methods.\nWe have also detection-based methods, which are about testing whether an input image meets some criteria, which are satisfied by natural images in general, even those which are incorrectly classified, and they are not satisfied by adversarially perturbed images. The general assumption is that input images that are subjected to perturbations force the network architectures to behave slightly, at least to natural images. These works tackle this issue from many perspectives such as statistics [36-39], detector training [40, 41] and prediction inconsistency[42, 43] where the two later can be combined. Detector training follows similar approach to adversarial training, where adversarial examples are used to train the detector.\nBased on the assumption that causal reasoning could help to bring robustness, and thus it could cope with adversarial attacks, many works tried recently to investigate this direction [44, 45].\nThis paper proposes CausAdv: a causal-based framework for detecting adversarial examples. Our approach is based on two main steps, in the first step we perform a causal learning process to identify causal robust features, and then, in the second step, we perform statistical analysis based on the outcome of the first step. In CNNs, filters are the main elements of the network that contribute to a decision, and thus, in our setting, filters are considered as actual causes. Based on the causality abstraction"}, {"title": "1 Introduction", "content": "principle [46], we consider only the filters of the last convolutional layer. We perform a causal learning process for each prediction, in which, filters can be either causal, or non causal for such a classification. For each filter, we assign contribution score in term of the difference in prediction probability when this filter is removed. Such a value would help to measure the importance of this filter with respect to the predicted class. We denote these values by Counterfactual information (CI), and they represent the output result of the first step. In the second step we perform many detection strategies based on statistical analysis of the filters' CI in order to distinguish clean samples from adversarial ones. Finally, we show how visualizing the causal features as possible explanations would serve as well as a helpful detection technique.\nThe main contributions of this paper are:\n\u2022 We propose CausAdv, a detection framework based on causality that is able to detect adversarial examples.\n\u2022 CausAdv employs statistical analysis on the filters' counterfactual information (CI) of each sample in order to distinguish adversarial examples from natural ones.\n\u2022 CausAdv can be performed on the top of any CNN architecture.\n\u2022 In contrast to existing defense methods that add noise, CausAdv does not need any modification of the input image. In addition, no modification of the architecture is needed, and no training is needed in contrast to adversarial training approaches.\n\u2022 CausAdv was able to achieve 100% detection with BIM attack by only basic analysis of the CI.\n\u2022 CausAdv has a powerful explanation and interpretation ability that helps to distinguish natural samples from adversarial ones."}, {"title": "2 Related work", "content": "Robust Design\nOne of the most used methods in this category is gradient masking, which aims to reduce the sensitivity of DNNs models to small perturbations that can be exploited by attacks. Papernot et al. [33] introduced the \"defensive distillation\u201d strategy to defend DNN models against adversarial attacks. A defensive distillation strategy consists of two models, a large model called the teacher is trained to maximize the likelihood of the classes, and then it transfers its knowledge to a smaller model called the student. The second model is then anticipated to predict the same probability distribution over classes. So, relying on a hard softmax function could provide smooth decision boundaries, which makes it very hard to find adversarial examples without losing imperceptibility. The method has shown promising results on MNIST and CIFAR datasets against JSMA attack, however it has limitations against black box attacks due to the transferability of adversarial examples.\nGu and Rigazio [34] have also proposed a gradient regularization strategy by training a contractive autoencoder on adversarial examples prior to the classification, which adds a layer-wise contractive penalty. Unfortunately, the combination of the classifier and the autoencoder may fail depending on the class of adversarial examples on which autoencoder has been trained."}, {"title": "2 Related work", "content": "Other works tried to build robust architectures through different techniques such as batch normalization [47] and skip connection [48]. Other recent works addressing defense against adversarial attacks from the activations perspective include: different activation functions [49] and new activation operations [50]. Xu et al. [35] explored through interpretation the promotion, suppression and balance effects adversarial perturbations on neurons' activations, and recently Bai et al. (2021) [51] by proposing Channel-wise Activation Suppressing (CAS) that tries to suppress redundant activations, which can be exploited by adversarial attacks.\nWhile these works aim directly to modify the activation functions, there have been recent works on addressing the activation outputs to reinforce the robustness of DNNs. Stochastic Activation Pruning (SAP) [52] takes each activation with a probability proportional to its absolute value. Xiao et al. [53] proposed a new activation function k-Winner-Takes-All (kWTA) that takes only top-K outputs in every activation layer. Adversarial Neural Pruning (ANP). [54] tries to remove activation outputs that are vulnerable to adversarial examples using Bayesian method. Feature Denoising (FD) [55] alters the CNN architecture by adding new blocks, with the aim of denoising the feature maps. However the modified architecture needs to be retrained on adversarially generated samples. Previous similar denoising approach has been proposed to work on the level of pixels [30].\nAnother emerging strategy is to transform input images such that we reduce the model's sensitivity to small perturbations. These defense methods lead to degradation on the quality of non adversarial inputs, thus impacting the accuracy of the model. Similarly, [56] tries to minimize the impact on the accuracy of non-adversarial images by proposing a combination of methods such as robust activation map and adaptive soft-thresholding for pixel deflection.\nPrediction inconsistency\nThe general assumption made by [42], is that feature input spaces are very large, which gives the opportunity for adversary construction based on a large feature space. As a result, they think of the feature Squeezing strategy, which aims to squeeze unnecessary input feature. They first apply different transformation techniques such as non-local mean and median smoothing, thus yielding different possible outputs, which are supposed not to affect the image content; and thus the model is expected to give similar prediction probabilities after applying these squeezers. Then it compares the prediction probability of the model on the original input image to those after applying squezeers, if the prediction probability on the squeezed inputs substantially differ (given a selected threshold value) from the original one, it will be detected as an adversarial example. Additional techniques applied includes squeezing color bit depth from 8-bit (original) to 1-bit, where the squeezed input is still nearly identical to the original input.\nYu et al. [43] suggest that by inheritance, there exist valid adversarial perturbations around a natural image, and use them as signature for the input image being not perpetuated. This leads to investigating main properties of natural images which are: first, robustness to random noise, and this can be achieved by adding Gaussian noise and estimating the prediction probability, and second, a natural image is close to the decision boundary of its class, this can be achieved by observing the number of gradient"}, {"title": "2 Related work", "content": "steps required to change the class. These two properties are used to build a detection strategy to distinguish the distribution of adversarial perturbations always given some thresholds. They show that with enough gradient queries, adversarial examples can be optimized through simple techniques to appear more natural and thus bypass the detection methods. However, comparing to the previous detection methods, they report promising results against white-box attacks, but the false positive rate is high, in addition, the second property to be verified requires a high computation cost, which makes its application in the real-world very challenging. This method is similar to [42], since it performs modifications on the input image. In contrast, our method does not need to modify neither the input image, nor the architecture and the training process, and no retraining is needed, we only perform causal learning process as a first step.\nStatistical-based Techniques:\nRoth et al. [36] proposed a statistical-based strategy for detecting adversarial examples. Given an input image x, the method tries to compare its logit or log-odds vector fz(x) to the noisy logit vector fz(x+n) where \u03b7 ~ N is a noise added to the input from some distribution N such as Bernoulli and Gaussian noise. The idea is that the difference of the two vectors should be small when x is a natural sample fz(x) \u2248 fz(x+n), otherwise the input is declared to be adversary. The decision whether the input is declared to be adversary or not is subjected to a threshold, which is determined by trying to achieve the highest true positive detection rate. Their idea is purely statistical with more focus on gradients info (log-odds of weights vectors). Our idea is similar, however, it is based on causal reasoning and employs statistics as a tool on causal features, which we believe is more robust than gradient information. Their approach just like the others needs to modify the input image by adding noise, where a threshold is set through trying to maximize the true positive rate.\nLi and Li. [37] introduced a fully-statistical approach for detecting adversarial examples. They employ a statistical technique called PCA (Principal Component Analysis) on the features extracted from every convolutional layer. At every layer we will have as a result a K_vector of normalized PCA, where K refers to the number of filters at each layer. As the CNN consists of many convolutional layers, it would be challenging on which layers we should focus the most. Statistically, they have shown that given the VGG architecture as an example, more than 80% of natural samples can be determined from just the first convolutional layer. However, to do so, they need to build and train a cascade classifier, which acts on the features extracted from every conv layer, and then concatenate and pass to the next convolutional layer. Before all of this, both the input image, and the convolved images (i.e by applying each filter) are treated as distributions of pixels. They build their idea on the same assumption of previous papers which state that the projections of natural samples are very similar to random samples under a Gaussian distribution. However, adversarial examples could have projections with a large deviation. The difference between natural samples and adversary ones lies in the statistical difference on PCAs, which is determined through the cascade classifier after being trained on many natural and adversary samples.\nCompared to our work, they consider all the intermediate layers, which are traited through their cascade classifier, while ours focuses only on the last convolutional layer, which has been justified from the causal abstraction perspective. In addition, we do not"}, {"title": "2 Related work", "content": "need to train a detector, CausAdv acts only on top of existing architectures in a plug- and-play manner. We need only natural samples for performing the causal learning process. Besides, they consider only one attack based on the L-BFGS algorithm [7] for generating the adversarial examples without any experiments on the famous white box attacks such as PGD, FGSM and BIM. As it relies on transformation techniques, it shares with the transformation techniques presented before the idea of input images modification.\nSimilarly, Feinman et al. [39] use a logistic regression model for detecting adversarial examples. The model is based on two main estimates that detect adversarial inputs from natural ones, the first is empirical density estimated based on the feature vector of the last hidden layer, and the Bayesian uncertainty, which is estimated based on the variance in the output through different dropout masks. This work is also considered as a statistical-based one since it proves that uncertainty distributions of adversarial inputs are statistically distinct from natural inputs.\nCausal-based Techniques\nWhile causal reasoning has been widely adopted recently into deep learning for different tasks such as explanation [57-59], in adversarial attacks it starts to gain attention recently. Zhang et al. [44] start from the assumption that adversarial attacks represent some specific type of distribution change on natural data. Therefore, they proposed an adversarial distribution alignment method to eliminate the difference between adversarial distribution and natural distribution. Their approach relies on building a causal graph for the adversarial data generation process, which is represented through many random variables including the natural as well as the perturbed data, the causes, which are grouped into two categories (content information (C), and style information (content-independent)(S)) the label (Y), the intervention (E), which can be either soft or hard, and finally the network's parameters (0).\nBy applying every intervention E we get a distribution, which could be either natural or adversarial, where the causal model without any intervention refers to a natural perturbation process. By analyzing the causal graph, it is found that spurious correlation between Y and S plays a crucial role in adversarial vulnerability. As a result, their adversarial distribution alignment method is proposed to detect such a spurious correlation as an indication for adversarial distribution compared to natural one. They use for representing style information a Gaussian distribution similar to previous works [60, 61].\nTheir work is meant mainly to analyze the adversarial data generation process and how spurious correlation plays a major role in the alignment method, this without addressing how to eliminate such a vulnerability, this is similar to our objective in this work, which is about detection. By analysing the distributions of filters Counterfactual Information (CI), we would be able to spot the difference between adversarial samples and natural ones. From causal a point of view, they do not consider counterfactual reasoning, while we do in computing the CI. Besides, their framework needs training, and thus it can be considered as an adversarial training variant. In addition, they aim to look at spurious correlation between labels and the style information (S), which could fail if S is not sufficiently available, especially in datasets like MINIST. They also build their idea on causal graph, which makes its application very challenging for"}, {"title": "2 Related work", "content": "high-dimensional data. Concerning the experiments, they only consider CIFAR, and always experiments on large datasets such as ImageNet are absent in causal-based approaches.\nBased on the same assumption that the threat of adversarial attacks emerges from the spurious correlation, or the confounding effect, Tang et al. [45] tried to use causality as well. While [44] let the question of eliminating spurious correlation not resolved, [45] aims to suppress the learning of these confounders through a causal regularization technique called CiiV, by forcing the model to learn causal features instead. They refer to those confounders as possible causes and then build a causal graph. As any causal framework requires intervention and estimating the causal effect, they rely on the instrumental variable estimation as a mean of intervention. Such variables are incorporated into the causal graph. Both confounders C and instrumental variable R are both added to the input image. The Ciiv needs to be trained and its loss is combined with the conventional cross-entropy loss, which brings notable robustness as tested on CIFAR-10.\nIn general, all previous works rely on modifying the input image, whereas CausAdv does not affect neither the image, nor the architecture and no training or retraining is needed. All causal-based approaches rely on do-calculus or intervention, while ours relies on counterfactuals."}, {"title": "3 Causality for detecting adversarial Examples", "content": "3.1 Introducing causality into CNNs\nIt is well known that the most important layers of CNNs are the convolution layers, which include filters. Filters represent the basic elements of the network, which are crucial for generating activations in response to different regions of input images. Researchers have explored various aspects of CNNs to understand the effects of filters. In our framework we also investigate filters by considering them as the actual causes.\nHalpern and Pearl have extended the definition of counterfactuals to build a rigorous model of causation, which is called a structural equations [62]. This definition of causality can be adapted to any CNN architecture by considering filters as actual causes for a predicted 4. However, since a class is predicted with a probability P, should not be just a Boolean formula that refers to the class predicted. Therefore, we should define countefactuals in a probabilistic setting. To do so, we should ask the following question: what would be the prediction probability, when one of the filters f is removed at a specific layer l, while keeping all the filters intact ?\nNow we can introduce the definition of an actual cause in CNNs.\nActual cause. Let F be the set of all filters, and fi \u2208 F be a filter at a specific layer l. fi can be considered causal for a prediction, if its own removal decreases the prediction probability P to P', while the rest of filters are not modified. We consider the difference P - P' as the counterfactual information, and it is denoted by CI. \u0410 filter is causal if its CI > 0 and it is not causal otherwise\nAccording to the definition of causality [62], the set of filters F can be partitioned into two sets Fw and Fr, where the set Fw refers to filters causing the prediction 4, i.e. their removal will decrease the prediction probability P, these refer to causal"}, {"title": "3 Causality for detecting adversarial Examples", "content": "features, whereas Fr are not causal, in way that the removal of a filter in F\u2082 does not affect P, or otherwise increases it, and these refer to non-causal features. When the removal of a filter leads to increasing P, this means that this filter has a negative effect on the decision, which means that it is responsible for increasing the prediction probability of another class, not the current class. In the following, we will refer to F and F\u2764 as the causal and non-causal features of an input image x predicted as \u03c6.\nIn CNN architectures, the last layers always learn complex features, since their filters are applied after many convolutions steps on the input image. In other words, the last convolution layer outputs the high level features, since it lays in the top hierarchy of the causal network. According to Beckers and Halpern [46] causal models have abstraction form, where micro variables have causal effect on macro variables. Actually this represents an essential property of our framework of causality. In our framework,"}, {"title": "3.2 Detection Strategies", "content": "many works such that[63] stated that classifiers try to use any possible activations in order to maximize (distributional) accuracy, even those activations related to incomprehensible features to humans. As a result, the adversarial attacks try to exploit the existence of the non-robust features learned by different architectures. As a result, many works rely on the input images themselves in order to identify and remove those non-robust features, to mitigate any adversarial attack.\nCausAdv tries to solve such an issue by performing causal learning process, which shows that the causal robust features learned play a crucial role for distinguishing natural samples from adversarial ones. In addition, we will show that statistical analysis of the CI distributions of adversarial samples and clean samples is very useful. Thus, we rely on different analysis strategies on the CI vectors to detect adversarial from clean samples, without any modification of the input images.\nAfter defining causal and non-causal filters, which represent the main concepts of our causal view on robustness, we define in the following the main strategies for detecting if an input image x is an adversarial or it is natural, based on analyzing its causal (FW) and and non-causal filters (FZ\u2194\u00ba).\nStrategy 1: Causal Features Existence: We consider an input image x predicted as o adversarial if Fw = 0, or instead: |Fw\u2192| <= n, where n is a small natural number, which means that x has no causal features, or at most has a very few number of causal features, or in other words all the CI are equal or less than zero.\nThe idea here is that it is not possible to find a sample without any causal features. This is has been proved experimentally by computing the CI vectors of every class of the 1k prototype (clean) classes of the ImageNet dataset, and the 10 classes of CIFAR-10, and we find that there are always causal features (whose filters have CI > 0). This applies even for classes with small number of features, which could happen in some basic objects having a low number of features, or objects having low CI such as the ones having fine-grained features such as flowers. Such a detection strategy would be very efficient as we will see later in the experiments section particularly with the BIM attack.\nStrategy 2: Correlation analysis (Pearson's coefficient):\nLet us denote by xprot the prototype image of an input image x predicted as p. We consider the input image x adversarial, if its CI's vector is not well correlated with the CI's vector of \u00e6prot, given a threshold \u03c4. We say that x is natural if the correlation coefficient Px,xprot > T, otherwise it is adversarial.\nxprot is identified as prototype for the class 4 from the dataset itself, among their instances, as the one having the high prediction confidence by the same CNN architecture. After the causal learning process, will have all causal and non-causal filters of each class, and these refer to related and non-related features to this class respectively. We employ the CI vectors for measuring similar features between the prototype class xprot and their instances by evaluating the differences of their CI vectors. To do"}, {"title": "3.2 Detection Strategies", "content": "so, we apply on them a correlation method such as the Pearson coefficient method, to measure to which extent the vectors of features are highly correlated.\nThis idea is similar to statistical approaches such as Grosse et al. [64], who proposed a statistical distance measure between large sets of adversarial and legitimate inputs in order to detect individual adversarial. In contrast to this, especially on the legitimate part, we consider for this strategy only one legitimate prototype image by class, besides, our statistics are based on causal features, which are high abstract robust features, and not on gradient information, which makes CausAdv more robust and has superior detection performance.\nStrategy 3: Zero effect: Let FH4 FZ be the set of zero filters of an input image x (filters whose CI=0) and a natural threshold n. x is decided to be adversarial if its number of its zero filters |FZ| < n.\nThis strategy states that an input image it might be adversarial, if we find that it has no zero filters, which means that the attacker is trying to exploit those non-causal features. These results have been found in similar works, where they showed that some channels are found to be over-activated by adversarials.\nWe decide on the threshold n by observing the behavior of the prototype image xprot. So if the number of zero filters is dramatically decreased compared to its prototype, that might be a sign of an adversary, because in a normal distribution no class has all its CI positives, or all its CI negatives, at least we have an accepted number of n of zero filters, those that do not affect the prediction probability of the class at any way, positively or negatively. The results are confirmed later on an experiment on CIFAR-10, because with the ImageNet, since we deal with high level features, we still always have an acceptable number of zero filters.\nStrategy 4: Common Robust Causal Features: Let x be an input image and its set of causal features, such that o its predicted label. We denote by Fu F the top n causal features of x, and Fw F the top m causal features of the y's prototype xprot. x is said to be adversarial example if FuF \u2260 0, such that m >= n.\nThat is, an input image x is said to be adversarial if it has at least top n causal filters present in top m causal filtrs of the prototype image xprot, where m >= n. In our experiments, we give several values for n and m such as 3/10, 5/20 and 10/30 respectively, all of them show the difference between original samples and adversarial ones. So the idea here is that we consider the top causal features as the robust features that should be present in any instance of the class, when robust features are not found it might be a sign of an adversary trying to maximize the prediction probability toward another class depending on other features.\nOur approach for analyzing causal/non-causal features strategies might be similar in its principle to [51], which tried to identify anomalous activations of certain channels from the entire architecture. Then they will lead to more robustness of the CNN architecture via removing or promoting certain vulnerable or reliable channels respectively. In contrast to this work, which considers also intermediate layers, our approach is more sample by focusing only on the behavior of the last channels of the last convolutional layer based on the causality abstraction principle."}, {"title": "4 Experiments", "content": "4.1 Experiment Setup\nData and Attacks: We conduct our experiments on ImageNet[1] and CIFAR-10 [65]. ImageNet consists of 1000 diverse classes. We sample 6 random images from 50 different classes within the ImageNet validation dataset, resulting in 300 samples. For CIFAR, We sample 10 images from each of the 10 classes, yielding 100 samples.\nArchitecture We made our experiments on ImageNet by considering the VGG16 architecture. We consider the last convolution layer, denoted conv13 to compute the counterfactual information (CI) of filters. For CIFAR, we use a customized version of the pre-trained VGG16 architecture, achieving a commendable accuracy of 93.15% on the CIFAR test set. For computing the CI vectors of the CIFAR samples, clean and adversarial, we consider always the last conv layer. The CI vectors are computed for all the 1k prototypes of ImageNet and the 10 prototypes of CIFAR, in addition to all the 300 samples for ImageNet, and the 100 samples for CIFAR. For the adversarial samples, the CI vectors are computed for every type of attack.\nAttacks Setting:\nWe evaluate CausAdv on the two sets against targeted and untargeted variants of the three prevalent attacks: Fast Gradient Sign Method (FGSM), Projected Gradient Descent (PGD), and Basic Iterative Method (BIM) with lo\u221e. In targeted attacks for ImageNet, we randomly designate a set of 50 target labels for every class, ensuring the target label is distinct from the 50 attacked classes. We test this set against the same attacks. The same methodology applies to CIFAR, where a unique target label is assigned for each sample from the 10 classes.\nFor both sets we tried to perform attacks that truly change the truth label for every sample. So, we needed to determine a suitable perturbation budget e that effectively triggers attacks while maintaining sample recognizability. For ImageNeta perturbation budget of e = 8 is chosen, as lower values like e = 4 did not give efficient rate across all the attacks. With e = 8, we achieved an attack success rate of 100% for all targeted and untargeted attacks with ImageNet.\nFor CIFAR, since its images are more robust against adversarial attack compared to ImageNet images[66], we choose a larger perturbation budget e = 24 in order to reinforce the architecture not to predict the truth label. Nevertheless, we did not achieve 100% attack success.\nAll the experiments have been implemented within the Keras framework. Regarding attacks on both ImageNet and CIFAR-10, we rely on the Adversarial Robustness Toolbox (ART) library [67]."}, {"title": "4 Experiments", "content": "4.2 Experimental results\nWe perform the causal learning process and compute the CI vectors for all prototypes, clean samples, as well as adversarial samples. The plot results of some samples in form of histograms are depicted in Figure 2. The x-axis refers to the number of filters, while the y-axis represents CI values of filters."}, {"title": "Strategy 1: Causal Features Existence:", "content": "For this strategy, we observe that it is accepted for all the generated adversarial samples, for all attacks except the BIM attack, where all the samples have no causal features. Which means that the detection rate for both BIM attack variants, targeted and untargeted on ImageNet reaches 100%. As a result, since BIM attack does not meet this condition, it would fail automatically in the subsequent detection strategies. By decreasing the value of \u20ac = 4 we still have the same results, since with this perturbation budget, BIM is still efficient as an adversarial attack with 100% success rate.\nWe tried to increase e = 8 to \u20ac = 16, the results for targeted and untargeted PGD attacks are similar to BIM, where all filters have nearly Zero CI values. Given this strategy, the detection rate will be also 100% for PGD attack. The best results for the PGD attack on ImageNet given max perturbation \u20ac = 16 is achieved by the ResNet-152 Denoise model [55] with accuracy 42.80.\nAccording to [68]: PGD and BIM are quite similar, often exhibiting similar performance. Our results in this detection strategy agree with this idea, since PGD and BIM are found to be more efficient than FGSM, though both are easily detected through causal feature analysis. While BIM is detected with even small perturbation budget, PGD remains robust against our detection strategy, and it is only detected similarly to BIM attack when e is increased from 8 to 16."}, {"title": "Strategy 2: Correlation analysis (Pearson coefficient)", "content": "We test first the efficiency of this technique for natural images as a similarity technique for both the CIFAR-10 and ImageNet datasets. For ImageNet, as we are dealing with 1000 classes rather than 10 classes, we measure correlation with the top-5 classes.\nIn the case of ImageNet, we compute the correlation coefficient p between the CI vectors of the clean images and their prototypes from a side, and the CI vectors of the adversarial samples with their prototypes from the other side. The results are reported in table 1. We report in the second column the number of samples, whose top 5 correlated classes include the predicted label, which means that a sample x is counted suspicious if its predicted label is not included in the top 5"}]}