{"title": "GIRAFFE: Design Choices for Extending the Context Length of Visual Language Models", "authors": ["Mukai Li", "Lei Li", "Shansan Gong", "Qi Liu"], "abstract": "Visual Language Models (VLMs) demonstrate impressive capabilities in processing multi-modal inputs, yet applications such as visual agents, which require handling multiple images and high-resolution videos, demand enhanced long-range modeling. Moreover, existing open-source VLMs lack systematic exploration into extending their context length, and commercial models often provide limited details. To tackle this, we aim to establish an effective solution that enhances long context performance of VLMs while preserving their capacities in short context scenarios. Towards this goal, we make the best design choice through extensive experiment settings from data curation to context window extending and utilizing: (1) we analyze data sources and length distributions to construct ETVLM - a data recipe to balance the performance across scenarios; (2) we examine existing position extending methods, identify their limitations and propose M-RoPE++ as an enhanced approach; we also choose to solely instruction-tune the backbone with mixed-source data; (3) we discuss how to better utilize extended context windows and propose hybrid-resolution training. Built on the Qwen-VL series model, we propose GIRAFFE, which is effectively extended to 128K lengths. Evaluated on extensive long context VLM benchmarks such as VideoMME and Viusal Haystacks, our GIRAFFE achieves state-of-the-art performance among similarly sized open-source long VLMs and is competitive with commercial model GPT-4V.", "sections": [{"title": "1 Introduction", "content": "Visual Language Models (VLMs) integrate visual and textual information, which are pivotal in understanding the multimodal world and excel in various applications, such as visual question answering and video understanding. However, more advanced scenarios involve multi-image and long video comprehension, which challenge the long-range modeling capabilities of VLMs. For instance, a 2K context length can only digest less than a few frames, limiting the upper bound of long video understanding. Consequently, there is a pressing need for methods to extend the context window of VLMs and improve their performance in long context scenarios. This would benefit next-generation VLMs in performing long history visual agents or serving as world models.\nRecent efforts for longer context VLMs focus on extending base Large Language Models (LLMs), along with visual alignment or efficient architectures. LongVA seeks to transfer long context ability from language models to vision by modifying position embeddings in the LLM backbone (PI, Chen et al. 2023b; NTK, LocalLLaMA 2023). LongVILA and LongLLaVA accommodate longer sequences using multi-stage alignment and instruction tuning with additional infrastructure and architecture. Despite these initial explorations, they have not investigated the feasibility of directly extending the context window of existing VLMs or systematically explored the design space in the extending pipeline. To bridge this gap, we decompose the challenge of extending context windows of existing VLMs into three fundamental research questions: (1) How to effectively organize and curate training data? (2) How to efficiently train longer VLMs? (3) How to leverage the extended context window?\nIn our work, our goal is to answer the three research questions and find a solution in practice. To validate our design choices, we implement thorough experiments based on Qwen-VL series model and conduct comprehensive evaluations on single image understanding, image interleave, and video tasks (\u00a72.1). For data curation, we prepare a diverse dataset comprising long context instruction data, multimodal instruction data, multimodal interleave data, and video instruction data (\u00a72.2). We analyze the impact of different data compositions, ratios, and lengths on model performance (\u00a72.3) and find that (1) short multimodal instruction data is crucial for both extending long context capability and retaining short context performance; (2) a balanced data ratio contributes to balanced performance on downstream tasks. For the second research question on extending training, we examine the effective context length of previous position embedding extending alternatives such as PI and NTK, discovering that, akin to LLM studies , the effective length is shorter than the training length (\u00a73.1). We propose M-ROPE++ (\u00a73.2) to extend position embedding on spatial and temporal dimensions. Validation experiments reveal that our method achieves better downstream task performance and longer effective length under the same training length (\u00a73.2). Different from LongVA that first extend LLM base or LongLLaVA and LongVILA that adopt multi-stage training with visual alignment and instruction tuning, we find that directly training VLMs by only updating LLM backbone's parameters achieves optimal results (\u00a73.3). To figure out how to use long context well in VLM, the third research question, we examine the trade-off between single-frame resolution and frame numbers regarding task performance (\u00a73.4). We consequently propose hybrid-resolution training, which further improves the utilization of a fixed context length (\u00a73.5).\nBased on our findings from the three research questions, we carefully select data recipes and training methods to extend Qwen-VL and Qwen2-VL to GIRAFFE-QwenVL and GIRAFFE with 128K length. Our final models are evaluated on both short context tasks such as single image understanding and long context tasks with multi-image and long videos. Experimental results demonstrate that our GIRAFFE achieves state-of-the-art performance among long VLMs and there is a significant improvement for our GIRAFFE-QwenVL compared with Qwen-VL base (\u00a74.3). Summarized contributions:"}, {"title": "2 How to Curate Extending Data", "content": "Developing an effective recipe for extending the context window of VLMs is crucial. To systematically evaluate such recipes, we construct a comprehensive metric suite encompassing single-image, multi-image, and video tasks (\u00a72.1), enabling a thorough assessment of model performance across diverse scenarios. This section focuses on the selection and preprocessing of training data (\u00a72.2), with an emphasis on understanding how data compositions, ratios, and lengths influence the model's capabilities (\u00a72.3).\nWe evaluate both long and short-context multi-modal tasks, as it is essential for VLMs to sustain performance on short-context tasks after extended training. For short-context evaluation, we utilize widely adopted benchmarks such as single-image MME and MMBench , which capture the diverse capabilities of VLMs. For multi-image tasks, we incorporate Mantis-Eval, QBench , and BLINK , in line with LLaVA-Interleave. Given the temporal nature of videos, which naturally represent long-context multimodal tasks, we evaluate on LongVideoBench and VideoMME. Additionally, we include the Visual Haystack Single Needle Challenge , which requires locating specific visual information within a long sequence of images, providing a robust measure of the model's effective context length.\nTo construct our extended dataset, ETVLM, we incorporate four primary types of data with varying lengths: (i) Long-context instruction data, sourced primarily from"}, {"title": "2.3 Data Recipe Exploration", "content": "We investigate the impact of different data ratio and data length on downstream task performance and provide recommendations for optimal data recipes. Using the same number of training tokens across all datasets, we conduct experiments with Qwen-VL as the base model.\nTo further investigate the impact of data composition on model performance, we conduct experiments by varying the proportion of a single data type from 10% to 90% while keeping the total training volume consistent. The results presented in Figure 2 reveal that increasing the proportion of long video data improves long video comprehension but compromises performance on other tasks. Similarly, increasing the ratio of any specific data type predominantly enhances its associated downstream task performance. Based on these findings, we determine the final data composition strategy, as shown in Table 1, which modestly increases the proportion of video data while reducing the share of pure text data. This adjusted recipe achieves a well-balanced performance across diverse task types."}, {"title": "Findings 1", "content": "Short multimodal instruction data is crucial for both extending long context capability and retaining short context performance. A balanced data ratio contributes to balanced performance on downstream tasks."}, {"title": "3 How to Extend Context Length", "content": "In this section, we test the effective length of existing length-extending methods, address their limitations (\u00a73.1), and introduce our position embedding technique M-ROPE++ (\u00a73.2). We find that for extending VLMs, it is sufficient to tune the LLM base of VLMs without requiring multi-stage training (\u00a73.3). We propose hybrid-resolution training to further leverage the fixed context length (\u00a73.5)."}, {"title": "3.1 Effective Length of VLMS", "content": "To evaluate the effective context length of VLMs, we draw inspiration from recent studies on LLMs, which suggest that their effective lengths are often only about half of their training lengths. We adopt the single needle setting from Visual Haystack , where models process varying numbers of input images and are tasked with identifying specific images and answering questions such as, \"For the image with the anchor object, is there a target object?\" This setup enables the assessment of performance across different context lengths, with random guessing yielding a 50% success rate. All tests are conducted using native image resolutions consistent with the original configuration.\nAs shown in Figure 4, retrieval success rates decrease as the number of input images grows. We define an accuracy threshold of 60% to determine the effective length. The base Qwen2-VL model achieves effectiveness up to 15 images, corresponding to an effective length to approximately 10K tokens. After extending the training length to 128K tokens using existing length-extending methods like PI and NTK, the effective length increases to around 50 images, equivalent to approximately 40K tokens still less than one-third of the training length. These findings highlight that the extended VLMs, similar to LLMs, exhibit the falls short phenomenon, where effective length falls short of the training length. These findings highlight the need for a novel position-extending method to enhance the effective length of models."}, {"title": "Findings2", "content": "The effective length in VLMs, including models that utilize existing position-extending methods, is smaller than the training length."}, {"title": "3.2 Position Extending on VLM", "content": "In this subsection, we briefly introduce M-ROPE, discuss potential issues associated with existing position extending methods, and then present our proposed M-ROPE++ along with experimental results validating its effectiveness.\nM-ROPE Multimodal Rotary Position Embedding (M-ROPE) proposed in Qwen2-VL extends the ROPE (Su et al., 2024) to effectively model positional information with multi-dimensions. M-RoPE deconstructs the original rotary embedding into three components: temporal, height, and width. The formal definition of M-ROPE and RoPE can be found in Appendix A. For a 16x-dimensional M-ROPE matrix, the dimensions are allocated in a 2:3:3 ratio for temporal, height, and width components respectively. This can be represented as:\n$RM (0, i_t, i_h, i_w) =\\begin{bmatrix}\nA_1 & 0 & 0 \\\\ 0 & A_{2x} & 0 \\\\ 0 & 0 & A_{8x}\\end{bmatrix}$,\nwhere each $A_i \\in \\mathbb{R}^{2\\times2}$ is a rotary block and $i_t, i_w, i_h$ are position indices. $\\Theta$ represents the rotary base. The blocks are allocated as follows:\nPosition extending on M-ROPE In M-ROPE, the temporal index are allocated to the lower dimensions of the rotary embedding, which correspond to high-frequency information. Preserving this information is crucial for maintaining the model's ability to discern temporal order. Position extending methods such as position interpolation (PI; Chen et al. 2023b) or modifying the ROPE base (NTK; LocalLLaMA 2023) tend to compress high-frequency signals indiscriminately, potentially confusing the model's perception of order of close-by frames. Conversely, the height and width dimensions occupy higher-dimensional spaces in the rotary embedding, indicating that they may not have fully covered the rotational domain during pre-training. This necessitates the application of interpolation to these dimensions. To address this, we propose M-ROPE++ that applies extrapolation exclusively to the temporal index and apply interpolation on height and weight index.\nM-ROPE++ We begin by defining key parameters following YaRN: \n$s = \\frac{L'}{L_v}$            (3)\nwhere s is the ratio between the extended context length $L'$ and the original visual context length $L_v$. We define $\u03bb_d$ as the wavelength of the RoPE embedding at the d-th hidden dimension:\n$\u03bb_d = \\frac{2\u03c0}{\u03b8_d}$       (4)\nand introduce the ratio r:\n$r = \\frac{L'}{\\lambda}$           (5)\nFor M-ROPE, the index range is divided into three segments: temporal (t), height (h), and width (w). Temporal information is predominantly in high-frequency, which has been covered during pre-training stage. Therefore, we maintain extrapolation for this segment. For the height and width segments, where $\u03bb > L'$, indicating insufficient rotational domain training, we employ interpolation to preserve their performance. This design is illustrated in Figure 1 right part.\nWe propose the following piecewise function to obtain the updated $\u03b8'_d$ for M-RoPE++:\n$\\theta'_d =\\begin{cases}\n\\theta_d & \\text{if } 0 < d \\leq 2x, \\\\ (\n\\theta_d + (1 - \\frac{r}{s}) \\theta_d & \\text{if } 2x < d \\leq 5x, \\\\\n\\theta_d & \\text{if } 5x < d < 8x.\n\\end{cases}$      (6)\nExperiment Validation We conduct a comparative analysis of various methods for extending the"}, {"title": "Findings 3", "content": "The effective lengths achieved by existing position-extending methods remain insufficiently long. M-ROPE++ achieves better downstream task performance and longer effective length in the same training length."}, {"title": "3.3 Multi-Stage Training", "content": "We investigate whether multi-stage training strategies commonly used in VLM training are necessary for extending context length. Previous works on long-context VLMs, typically training from an LLM base, often employ multiple stages, including extending the text-based model's context length, multimodal alignment, and multimodal instruction tuning. For extending existing VLMs like Qwen2-VL, we explore two approaches: (1) extending the LLM base with additional pure text data (Wiki-103) followed by multimodal instruction data, like LongVA , and (2) multimodal alignment using image-text pairs (Sampled from LAION-5B) followed by instruction tuning . As shown in Table 3, our experiments indicate that pre-extending the text-based model with pure text data provides no significant advantage. This is likely because training with long-context multimodal data already addresses diverse length distributions, rendering pure text extension redundant. Moreover, performing multimodal alignment before instruction tuning degrades performance on short-context tasks. This could be attributed to Qwen2-VL already undergoing instruction tuning during pre-training; further tuning of MLP and ViT layers with alignment objectives may disrupt the model's learned distributions. With fixed training steps, this disruption negatively impacts short-context performance without yielding improvements for long-context multimodal tasks. These findings suggest that directly fine-tuning with long-context instruction data is the most effective approach for extending existing VLMs."}, {"title": "Findings 4", "content": "Directly train VLM with mixed instruction data while only updating LLM backbone's parameters achieves optimal results."}, {"title": "3.4 Trade-off in Fixed Context Length", "content": "We explore the trade-off between the resolution of individual visual tokens for single images and the number of frames used, proposing a hybrid-resolution approach to improve long-context performance under a fixed context length. When encoding videos with a fixed total number of visual tokens, there exists an inherent balance between the"}, {"title": "Findings 5", "content": "Hybrid-resolution training can further improve the utilization of a fixed context length."}, {"title": "4 Extended VLMs", "content": "In this section, we first present the experimental setup and the relevant models, followed by an analysis of their performance across various downstream tasks."}, {"title": "4.1 Infrastructure and Engineering", "content": "We select Qwen2-VL, Qwen-VL as the base model for further training because of their strong performance at the 7B parameter scale and support for interleaved inputs. We employ the NTK method for Qwen-VL and M-RoPE++ for GIRAFFE to extend the model's window length. Training long VLMs results in substantial memory demands, thus we employ several optimization strategies to perform training on such long sequences. These include FlashAttention-2, Ring Attention, ZERO (including activation checkpointing, and parameter offload). To balance the load across 8 80G H100 GPUs, we shard the sequence in a zigzag way. We use LoRA to reduce the GPU memory usage to train longer VLMs. We train the model for an average of 80 H100 hours."}, {"title": "4.2 Models", "content": "We assess the following models: Qwen-VL-Chat-7B A visual language model based on the Qwen language model, incorporating visual capabilities through cross-attention and learnable query embeddings. VideoLLaVA-7B A video-language model that extends LLaVA to handle video inputs, capable of processing up to 8 frames. VideoChat2-Mistral-7B An advanced VLM built on the Mistral-7B, designed to process up to 16 frames. LongVA-7B A long context VLM based on Qwen-2 language model, utilizing a two-stage alignment process to handle up to 128 frames. LongVILA-8B A long context VLM based on VILA language model, capable of processing up to 256 frames. Qwen2-VL A foundational VLM that employs dynamic image tokenization and M-RoPE, with pre-trained 16K context length. We train and evaluate our GIRAFFE-QwenVL and GIRAFFE in this section with the best setting shown in \u00a72 and \u00a73."}, {"title": "4.3 Video Task Results", "content": "Our extended models, GIRAFFE-QwenVL and GIRAFFE, demonstrate substantial improvements in video understanding across various temporal scales while specifically maintaining competitive performance on short videos. Table 6 shows that GIRAFFE-QwenVL significantly outperforms its base model Qwen-VL-Chat, enabling better understanding of video content. Notably, GIRAFFE, based on an improved base model and capable of processing 1024 frames, achieves state-of-the-art performance among open-source models in both VideoMME and LongVideoBench, even surpassing GPT-4V in several categories. These results provide compelling evidence that our approach successfully extends the context window of VLMs, particularly benefiting long context video understanding tasks while reserving original short context capacities."}, {"title": "4.4 Multi Image Task Results", "content": "In the multi-image evaluation presented in Table 7, GIRAFFE-QwenVL exhibits substantial improvements, whereas GIRAFFE also demonstrates enhancements, validating the efficacy of our pipeline. In multi-image scenarios, context length is less critical than in long video tasks. Qwen-VL's superior performance stems from capacities trained on the ETVLM dataset, compared to its initial 2K context length. In contrast, Qwen2-VL has already undergone substantial pre-training in 16K contexts. Additionally, Qwen2-VL benefits from a broader range of training data compared to Qwen-VL, rendering the incremental advantages from ETVLM data relatively modest."}, {"title": "4.5 Image Task Results", "content": "The results from Table 8 demonstrate that our GIRAFFE maintains competitive performance on short-form multimodal tasks. This balanced capability can be attributed to our training strategy, which incorporates a mix of short instruction data alongside long context video inputs. Incorporating LLaVA-Instruct and M3IT in our training process ensures the model retains its capacity in single-image understanding."}, {"title": "5 Related Work", "content": "We list the related work for extending the existing LLMs and VLMs."}, {"title": "5.1 Long Context Language Models", "content": "With the rise of LLMs, research has focused on extending their capacity for longer contexts. The main solution involves addressing the out-of-distribution issue with position-embedding and enhancing model extrapolation capabilities. Training-free methods like streamingLLM , InfLLM and ChunkLLaMA offer cost-effective ways to scale context window size. Additionally, further training using modified ROPE base frequency is introduced in NTK, PI and YaRN, a effective practice adopted by models such as CodeLlama and LLaMA 3. Moreover, efforts have also been made on data curation for long context training . However, corresponding comprehensive studies on extending context for open-source VLMs remain limited."}, {"title": "5.2 Visual Language Models", "content": "Advancements in LLMs are driving the evolution of VLMs. MiniGPT-4 and LLaVA-series integrate visual modalities into the LLM architecture using visual encoders. The recent Qwen-VL series significantly enhances multimodal training pipeline, distinguishing itself with strong performance among open-source VLMs. VideoLLaVa and VideoChat focus on video scenarios requiring long context windows, yet their context windows are still limited. For long context VLMs, recent LongVA are first extending an LLM base model to 128K token lengths and then developing it into a VLM. Concurrent work LongVILA also involves multi-stage training starting from an LLM backbone and employs an improved sequence parallel technique"}, {"title": "6 Conclusion and Future Work", "content": "We develop an effective solution to extend the context length of VLMs while preserving their performance on shorter contexts. Our comprehensive experiments led to the introduction of the ETVLM dataset for extended training and M-RoPE++ for improved position embedding learning. We use Hybrid-res training to better use long context window. Our extended model, GIRAFFE, achieves state-of-the-art performance for long context tasks. In the future, we aim to apply GIRAFFE to more complex scenarios, such as long-term history multi-modal chats and visual agents in real-world applications."}, {"title": "A ROPE and M-ROPE", "content": "Attention is defined over $C$ embeddings $X = [x_1,x_2,...,x_C]^T \\in \\mathbb{R}^{C\\times d}$ where $d$ is the model dimension. Learned weight matrices $W_v \\in \\mathbb{R}^{d\\times d_k}$, $W_q \\in \\mathbb{R}^{d\\times d_k}$, and $W_k \\in \\mathbb{R}^{d\\times d_k}$ are used to transform these inputs where $d_k$ is the projected hidden dimension. The attention mechanism itself computes the attention matrix and applies it to produce a weighted sum of the value vectors:\n$Attention(Q, K, V) = AV = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$. (7)\nBasic attention was originally defined with: $Q = XW_q, K = XW_k, V = XW_v$. However, this approach does not directly encode the relative position of keys and values.\nRotary Position Embeddings (RoPE) encode positional information by applying a phase rotation to each element of the embedding vectors. Formally, we define a transformation $f$:\n$fw(xi, \\theta) = R(\\theta, i)W^Txi$ (8)\nHere $xi \\in \\mathbb{R}^{d_k}$ is an embedding for position $i$, $W$ is a projection matrix, and $\\theta \\in \\mathbb{R}^{dk/2}$ is a frequency basis. The function is defined based on the rotary position matrix:\n$R(\\theta, i) =\\begin{bmatrix}\ncos i\\theta_1 & -sin i\\theta_1\\\\ sin i\\theta_1 & cos i\\theta_1\\\\\n\\\\ ... & ...\\\\\ncos i\\theta_{dk/2} & -sin i\\theta_{dk/2}\\\\ sin i\\theta_{dk/2} & cos i\\theta_{dk/2}\n\\end{bmatrix}$(9)\nDue to the arrangement of frequencies, this matrix has the property that $R(\\theta,n \u2013 m) = R(\\theta, m)^TR(\\theta, n)$ by Ptolemy's identity. We redefine the query-key product between two positions m and n as,\n$A_{mkn} = fwq(xm, \\theta)^TfWk (xn, \\theta)$ (10)\nMultimodal Rotary Position Embedding (M-ROPE) extends the concept of RoPE to effectively model positional information of multimodal inputs. M-ROPE deconstructs the original rotary embedding into three components: temporal, height, and width. For text inputs, these components utilize identical position IDs, making M-ROPE functionally equivalent to 1D-ROPE. For image inputs, the temporal IDs remain constant, while distinct IDs are assigned to the height and width components based on the token's position in the image. For video inputs, the temporal ID increments for each frame, while the height and width components follow the same ID assignment pattern as images. Formally, we define the M-RoPE transformation function $f\u2122$ as:\n$fM(xi, \u03b8\u03c4, \u03b8\u03c9, \u03b8\u03b7) = [Rt(\u03b8t, it)Wxit; Rw(\u03b8\u03c9, iw)Wxiw; Rh(Oh, in)Wxih]$ (11)\nwhere xi is the embedding vector, \u03b8\u03c4, \u03b8\u03c9, \u03b8\u03b7 are frequency bases, it, iw, ih are position indices, and Wt, Ww, Wh are projection matrices for temporal, width, and height dimensions respectively.\nThe query-key product for M-ROPE is then redefined as:\n$a_{mkn} = f(xm, \\theta_t, \\theta_w, \\theta_h)^T f(x\u03b7, \u03b8\u03c4, \u03b8\u03c9, \u03b8\u03b7)$ (12)\nFor a 16x-dimensional M-ROPE matrix, the dimensions are allocated in a 2:3:3 ratio for temporal, height, and width components respectively. This can be represented as:\n$RM (\\theta, i_t, i_h, i_w) =\\begin{bmatrix}\nA_1 & 0 & 0 \\\\ 0 & A_{2x} & 0 \\\\ 0 & 0 & A_{8x}\\end{bmatrix}$(13)\nwhere each $A_i \\in \\mathbb{R}^{2\\times2}$ is a rotary block. The blocks are allocated as follows:\nEach rotary block $A_i$ is defined as:\n$A\u2081 = \\begin{bmatrix} cos(ix\\theta_d) & -sin(ix\\theta_d)\\\\ sin(ix\\theta_d) & cos(ix\\theta_d)\\end{bmatrix}$(14)\nwhere ix represents it, ih, or iw depending on which dimension the block belongs to. The frequency basis @ is shared across all dimensions.\nThis formulation allows M-ROPE to effectively model multimodal inputs while maintaining the rotary structure for each dimension."}, {"title": "B Impact of ROPE Base", "content": "We investigated the effect of different RoPE bases on the performance of Qwen-VL. Our findings indicate that the optimal performance was achieved by following the recommendations from Su's blog, specifically using a RoPE base of 500,000 for a context length of 128k. Increasing the base beyond this point did not yield significant improvements while keeping the default base of 10,000 resulted in a notable performance drop. Table 9 summarizes our results.\nThese results underscore the significance of meticulously adjusting the RoPE base when expanding the context window of visual language models. Our findings corroborate the conclusions presented in Su's blog which posits that for models with a context length of 128k, an optimal RoPE base of 4.9 \u00d7 106 is recommended. This value closely approximates our selected base of 5 \u00d7 105, which consistently demonstrates superior performance compared to the default configuration across all evaluated metrics.\nInterestingly, further increasing the base beyond this point does not yield significant performance improvements. This observation is consistent with the approaches taken by models like LLaMA 2 and Qwen, which have opted for even larger base values. Such choices may provide additional flexibility for future extensions of model context lengths.\nThe effectiveness of the optimized RoPE base in capturing long-range dependencies in multimodal data underscores the critical role of position embedding strategies in enhancing the performance of extended visual language models."}, {"title": "C Progressive Extending", "content": "To ensure more stable training, we adopted a progressive extending strategy. For GIRAFFE-QwenVL, we set multiple incrementally increasing context lengths: 8K, 32K, 64K, and 128K. We concatenate and chunk ETVLM data according to these different context lengths. For GIRAFFE-QwenVL, we investigate the optimal RoPE base setting, as detailed in Appendix B. Following Su , we experiment with bases of 5\u00d7104, 1\u00d7106, 2.5\u00d7106, and 5 \u00d7 106. For GIRAFFE, we employ M-RoPE++, training up to 64K before extending to 128K. This approach allows the model to gradually adapt to longer sequences while maintaining performance on shorter contexts.\nAblation of progressive extending We conduct comparative experiments on Qwen-VL to evaluate two methods for extending the model's context length: a single-stage approach and a progressive multi-stage approach. Both methods are using the same number of training steps. The results are summarized in Table 10. Our experiments demonstrate that the progressive extending approach consistently outperforms the single-stage method across different evaluated tasks. This suggests that gradually increasing the context length during training allows the model to better adapt to longer sequences, resulting in improved performance on various tasks."}]}