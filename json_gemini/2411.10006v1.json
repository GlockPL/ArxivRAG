{"title": "ORCA: ENHANCING ROLE-PLAYING ABILITIES OF LARGE LANGUAGE MODELS BY INTEGRATING PERSONALITY TRAITS", "authors": ["Yuxuan Huang"], "abstract": "Large language models has catalyzed the development of personalized dialogue systems, numerous role-playing conversational agents have emerged. While previous research predominantly focused on enhancing the model's capability to follow instructions by designing character profiles, neglecting the psychological factors that drive human conversations. In this paper, we propose Orca, a framework for data processing and training LLMs of custom characters by integrating personality traits. Orca comprises four stages: (1) Personality traits inferring, leverage LLMs to infer user's BigFive personality trait reports and scores. (2) Data Augment, simulate user's profile, background story, and psychological activities. (3) Dataset construction, personality-conditioned instruction prompting (PCIP) to stimulate LLMs. (4) Modeling and Training, personality-conditioned instruction tuning (PTIT and PSIT), using the generated data to enhance existing open-source LLMs. We introduce OrcaBench, the first benchmark for evaluating the quality of content generated by LLMs on social platforms across multiple scales. Our experiments demonstrate that our proposed model achieves superior performance on this benchmark, demonstrating its excellence and effectiveness in perceiving personality traits that significantly improve role-playing abilities.", "sections": [{"title": "1 Introduction", "content": "Building human-like conversation agents is a long-term challenge for AI researchers. The emergence of groundbreaking language models such as ChatGPT and GPT-4 [OpenAI, 2023], coupled with their intrinsic capacity for emergent in-context learning (ICL) [Brown et al., 2020] abilities and a three-stage reinforcement learning from human feedback (RLHF) [Ouyang et al., 2022] algorithm which have largely raised the capacity bar of existing AI systems.\nLLMs have acquired a wealth of knowledge during their pre-training stage. ICL utilizes LLMs in a few-shot or zero-shot way that can instruct LLMs to understand the tasks in the form of natural language text. Therefore, personality-based responses have gained significant attention. Despite GPT-4 exhibit advanced role-playing capabilities because human-generated conversations are combined with the Instruct tuning dataset in a dialogue format for training, it is widely recognized that LLMs, suffer from a lack of consistent personality traits often failing to be engaging. This is the result of the existing LLMs are predominantly trained on general domains and lack specific optimization for personalized LLMs.\nRecently, human social behavior is being changed by role-playing applications, such as Character.AI which has attracted a growing number of researchers to bridge the gap between the text and behavior of dialogue agents and humans [Team, 2023, Wang et al., 2024]. Personality-based dialogue systems can be broadly categorized into two types. (1) Persona-based Dialogue, represented by the work in Zhang et al. [2018] where the manipulation of profile information is employed to enhance the appeal of chit chat. These ideas are also used in the latest character LLMs such as CharacterGLM [Zhou et al., 2023], Ditto [Lu et al., 2024], and ChatHaruhi [Li et al., 2023], aiming to improve the humanity of customized characters. However, these approaches primarily create profile settings as prompts for model"}, {"title": "2 Related Works", "content": "2.1 Persona-based Dialogue\nIn open-domain dialogue systems, one big issue is that the responses are entirely learned from training data [Ni et al., 2022]. The inconsistent response may be received when asking the system about some personal facts (e.g., age, interestings). If the dataset contains multiple utterance pairs about the query of age, then the response generated tends to be shifting, which is unacceptable because personal facts are usually not random. Thus, for a data-driven agent, it is necessary to be aware of its role and respond based on a fixed persona. Explicitly modeling the persona is the main strategy in recent works. Responding with personas needs to condition on some persona descriptions. For example, to build a outgoing agent, descriptions like \u201cI am an outgoing person\" are needed as a part of the model input. Here are some related works that make chat more engaging by conditioning on profile information. The work presented in Wakaki et al. [2024] introduces a novel benchmark for evaluating open-domain dialogue systems, emphasizing the importance of diverse and robust evaluation metrics. This dataset, ComperDial, provides human-scored responses and facilitates the training of metrics that assess dialogue quality over multiple turns, offering a more holistic view of conversational performance. Similarly, the paper Cheng et al. [2024] explores the concept of in-dialogue learning (IDL), which allows for the dynamic acquisition of persona information during the conversation. This approach stands out as it does not rely on predefined profiles, thus providing greater flexibility and reducing the labor-intensive process of profile creation. The creation of large-scale datasets with persona information is addressed in Cho et al. [2023]. This work focuses on constructing a dataset that captures the nuances of persona in open-domain conversations, ensuring a safe and engaging conversational experience. Enhancing personalized dialogue generation is the focus of Tang et al. [2023] (CLV). This study innovatively combines sparse and dense persona descriptions to generate more accurate and rich persona representations, improving the personalization of dialogue agents. In the vein of long-term memory in dialogues, Xu et al. [2022] presents a dataset and framework that enable dialogue systems to maintain persona consistency over extended interactions, thus fostering more intimate and engaging long-term relationships with users. The FoCus [Jang et al., 2022] dataset aims to provide customized and knowledgeable responses by grounding dialogue in both persona and external knowledge sources, such as Wikipedia. Pchatbot [Qian et al., 2021] offers a substantial contribution to the field by providing a large-scale dialogue dataset that includes anonymized user IDs and timestamps, allowing for the development of personalized dialogue models that can learn implicit user personality from dialogue history. Improving persona consistency through pragmatic self-consciousness is the central theme of Kim et al. [2020]. This work introduces a novel approach to endowing dialogue agents with an awareness of their public self, thereby improving their consistency in dialogues. Lastly, the seminal work PersonaChat [Zhang et al., 2018] laid the groundwork for the field of persona-based dialogue systems, introducing a dataset that has significantly influenced subsequent research and development in the area. These works collectively represent the cutting edge of research in persona-based dialogue systems, each contributing unique insights and methodologies to the goal of creating more natural, engaging, and personalized conversational agents."}, {"title": "2.2 Personality-aware Dialogue", "content": "The field of dialogue systems has seen significant advancements with the integration of personality-aware models, aiming to enhance user engagement and interaction authenticity. A parallel stream of research has focused on developing mechanisms to tailor the personality traits of language models, enabling them to simulate a range of human-like behaviors and characteristics. The work most closely related to our approach is the P-Tailor system introduced by Dan et al. [2024], which customizes personality traits in large language models (LLMs) using a mixture of specialized LoRA experts. This method allows for fine-grained control over the Big Five personality traits, thereby enabling more nuanced and personalized interactions. Similarly, Li et al. [2024] propose UBPL, a method for tailoring personality traits in LLMs through unsupervised learning from personalized lexicons. Both approaches underscore the importance of leveraging psychological theories to ground the personality modeling in a theoretical framework. In the realm of social support conversation systems, the CharacterChat framework by Tu et al. [2023] stands out for its innovative use of interpersonal matching mechanisms to link individuals with compatible virtual supporters, based on MBTI personality types. This work highlights the significance of persona compatibility in delivering effective social support through conversational AI. The potential of LLMs to not only exhibit but also assess human personalities is explored in the work by Rao et al. [2023], who present a general evaluation framework for assessing human personalities using the Myers-Briggs Type Indicator (MBTI). This work opens up new avenues for understanding and evaluating the psychological capabilities of AI systems. The concept of controlling personality style in dialogue with zero-shot prompt-based learning is addressed by Ramirez et al. [2023], who experiment with different prompt classes to generate text that is both semantically accurate and stylistically consistent with specified personality types. This work contributes to the understanding of prompt-based learning for stylistic control in NLG tasks. Lastly, the CPED dataset by Chen et al. [2022] provides a rich resource for research in conversational AI, offering a large-scale collection of Chinese dialogues annotated with personalized and emotional information. The multimodal context provided by this dataset facilitates the development of dialogue systems that can better understand and exhibit human-like personalities and emotions. In summary, these works collectively advance the state of the art in personality-aware dialogue systems, emphasizing the importance of psychological grounding, stylistic control, and personalized interactions in AI-driven conversational agents."}, {"title": "2.3 Character-based Dialogue", "content": "The burgeoning field of character-based dialogue has seen significant advancements with the advent of large language models (LLMs). These models have demonstrated remarkable proficiency in simulating conversations that are indicative of specific characters, thereby enriching the interaction experience for users. Notably, the work LLM-Werewolf by Xu et al. [2024] in explores the integration of LLMs into communication games that hinge on natural language processing, showcasing the models' ability to engage in strategic behaviors such as trust and confrontation without the need for parameter tuning. Parallel to this, the study ChatHaruhi by Li et al. [2023], presents an algorithm that harnesses improved prompts and character memories to control language models, thereby mimicking the behavior of specific fictional characters. This work constructs a dataset that encapsulates a diverse range of characters and demonstrates the potential of LLMs in role-playing applications. Furthering the discourse on role-playing abilities, Wang et al. [2024] introduce RoleLLM, a comprehensive framework that benchmarks, elicits, and enhances the role-playing capabilities of LLMs. This framework includes a novel dataset, RoleBench, which provides a systematic and fine-grained evaluation of character-level role-playing. In the Chinese context, Zhou et al. [2023] present CharacterGLM, a series of models that facilitate the customization of AI characters for character-based dialogues. This work underscores the importance of character attributes and behaviors in creating consistent, human-like, and engaging conversations. Lastly, the concept of self-alignment in role-play is introduced by Lu et al. [2024]. This study posits that LLMs, by virtue of their training, are inherently capable of role-play, and through a method named DITTO, they can be aligned to simulate dialogues reflective of a multitude of characters. These works collectively contribute to the evolving landscape of character-based dialogue, each bringing forth innovative approaches and insights that pave the way for more nuanced and interactive AI systems."}, {"title": "3 Methods", "content": "In this section, we introduce the overall framework of Orca as illustrated in Figure 1. We first introduce the design principles of inferring personality traits. Then, we illustrate data augmentation mechanisms associated with based character customization procedure. At the third part, we present personality traits instruction tuning (PTIT) and personality scores instruction tuning (PSIT). Finally, we introduce the details of OrcaBench, which can be used to assess and enhance personality-integrating capabilities."}, {"title": "3.1 Personality traits inferring", "content": "In this paper we adapt Digman [1990] as psychometrics to capture the Big Five personality traits of Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. Peters & Matz [2024a] mentioned that LLMs like ChatGPT can accurately infer the psychological dispositions of social media users and whether their ability to do so varies across socio-demographic groups. The ability of LLMs to infer psychological dispositions from user generated text has the potential to democratize access to cheap and scalable psychometric assessments for both researchers and practitioners.\nThe aim of our approach is to incorporate personality traits into LLMs. However, people's personality traits in the real world are generally obtained from questionnaires, as discussed in Pan & Zeng [2023], and it is difficult to construct a large scale dataset in a conversational format for LLMs' training due to privacy reasons. Therefore, we use X as the data acquisition platform. X and other social media platform protocols specify that the content of public tweets can be used for scientific purposes. Different personality traits have different frequencies of certain keywords corresponding to them in the corpus [Yang et al., 2023]. We prepared different keywords to ensure the diversity of personality traits of the sampled users, and then retrieved Lists based on the keywords because there is a high probability that people interested in the same keywords will be listed in the same List. For example, people who like \"dancing\" and are extroverted, BigFive personality traits are generally high in openness. The opposite is \"hidden thoughts\", where introverts tend to be more introspcetive. We filtered out sensitive, harmful and inappropriate content. After desensitizing the dataset, we obtained 500 users with 200 tweets each, without the user's privacy.\nWe derive the Big Five personality traits from users' posts in a zero-shot learning scenario. To supplement the multi-modal information, we use visual LLMs to caption all media sources of posts. Based on the previous work [Peters & Matz, 2024a], we modified the prompt and added six sub-dimensions to each personality trait using the inference prompt: \"Please play as an expert in impartial assessment of personality traits in the field of psychology. In this assessment, when I give you a some user's recently published content and replies, score the user's personality traits according to the sub-dimention features of bigfive scoring criteria\". For scoring criteria, if a certain personality trait is exhibited, score one point; otherwise, score zero (Please see AppendixA for detailed prompts). In order to avoid exceeding the LLMs max new tokens limit, 200 posts were processed in chunks of 10 conversations, and the inferred personality scores were then averaged to derive overall scores. Since each chunk request receives an explanation, we design a summary prompt to summarize the user's personality trait report A.1. The summary of user's personality report as shown in A.1."}, {"title": "3.2 Data Augment", "content": "Recall from above that ChatGPT can be customized to play specific roles using prompt engineering such as zero-shot customization commands and few-shot prompts [Dong et al., 2024]. Previous work has also demonstrated the importance of predefined profile data for training personalized dialogue systems. To take advantage of these benefits, we enhance the data in three steps: (1) Due to the limited access to user information, we simulate a profile for each user using the LLMs, the profile simulation instructions are shown in A.1. (2) Users tend to have certain motivations for posting, in order to fill in the motivations behind the content posted by the personalized model, we simulate this part of the knowledge called Potential Knowledge. For each post, the potential knowledge simulation instructions are shown in A.1. (3) To filter high-quality data, we utilize LLMs to determine whether posts were relevant to profiles and potential knowledge, as well as to simulate a brief related psychological activities at the time of generating the post."}, {"title": "3.3 Dataset Construction", "content": "Personality-conditioned instruction prompting, we called PCIP. The final input contains instruction, profile, personality and potential knowledge, ordered and described by a four-tuple $I = (i, r, p, k)$. Note that for explicit modeling, personality is the user's personality trait report $p_r$, and for implicit modeling, personality is the explanation of user's personality scores $p_e$. The final output contains psychological activities, post content and media, ordered and described by a three-tuple $O = (a, t, m)$. If there is no correlation between $O$ and $r, p$, and $k$, we leave the corresponding slots empty in train dataset, allowing the model to learn these differences during training. The detail prompt as shown in A.1.\nThe figure 3.3 illustrate the workflow of assistant follow the character and PCIP to generate psychological activities and response content. We finally release the OrcaData dataset."}, {"title": "3.4 Model", "content": "For explicit modeling also called PTIT that can use any open source LLMs."}, {"title": "3.5 OrcaBench", "content": "In this section, we introduce the details of OrcaBench, which can be utilized to assess and enhance role-playing capabilities and personality consistency for personalized agents. We selected 25 users that different from the training data to construct the evaluation data according to the above method 3.2. Table 2 provides basic statistics for OrcaBench.\nThe assess pipeline is as follows:\n\u2022 1. LLMs are asked to generate content based on the prompt.\n\u2022 2. After collecting the responses from the LLMs, we evaluate the performance of the model according to the following criteria:\n1. Overlap.\n* 1. BLEU.\n* 2. ROUGE.\n2. Related Judge 3.2.\n* 1. Profile Related (+1).\n* 2. Personality Trait Related (+1).\n* 3. Potential Knowledge Related (+1).\n3. Personality Consistency.\n* 1. Personality Score Inferring, evaluate the personality trait scores of the character based on the n contents generated by LLMs 3.1.\n* 2. Distance Measure, compare the similarity between the character's personality trait scores and the ground truth personality trait scores."}, {"title": "4 Experiment", "content": "We test the effectiveness of training methods and models to generate personalized content on social platforms by integrating personality traits. We conduct ablation studies to verify the effects of various components in our model. Our model achieves the best results on the OrcaBench evaluation benchmark compared to general open-source models."}, {"title": "4.1 Implement Details", "content": "To facilitate reproducibility and save experimental costs we deployed Llama3.1-70B [Touvron et al., 2023] for data construction and Llama3.1-8B for model training. We use cogvlm2-llama3-chat-19B-tgi for image caption [Wang et al., 2023]. For more information about hyperparameters is available in the appendix A.2."}, {"title": "4.2 Baselines", "content": "LLaMA3.1-8b-Instruct, LLaMA3.1-70b-Instruct and DeepSeek-v2\u00b9 are foundation models. Personality-conditioned instruction prompting (PCIP) to stimulate these foundation models are strong baselines. We consider both direct tasks, where the model is expected to directly map from input to output, and combined tasks, where we instruct the model to also output intermediate steps for the content generation task. This is similar in spirit to chain of thought prompting (COT) [Wei et al., 2024]. We also use models trained on OrcaData in PTIT and PSIT modes as additional baselines."}, {"title": "4.3 Evaluation Metrics", "content": "\u2022 Overlap. We use BLEU and ROUGE-1 scores. A higher overlap score means better humanity.\n\u2022 Relevance. We used the LLMs to automatically assess the relevance of the model outputs to our given roles in terms of each of the three dimensions - character profile relevance (CPR), personality trait relevance (PTR), and potential knowledge relevance (PKR). The automated assessment still had a high level of confidence due to the simplicity of the task.\n\u2022 Personality Score Similarity (PSS). We use the cosine similarity to calculate the character's personality trait scores and ground-truth scores thereby measuring personality trait similarity."}, {"title": "4.4 Result", "content": "4.4.1 Personality Conditioned Instruction Prompting (PCIP)\nTo determine the role of the various modules of PCIP, we constructed these ablation experiments as depicted in Table 3, the following conclusions can be drawn from the data analysis: (1) A comparison between PCIP and PCIP-70b underscores the dependence on the performance of the foundation models. (2) PTIT-CPA means character profile ablation, CPR decreased to 7.60 indicating that profiles play an important role in maintaining consistency of profile about characters. (3) Within the personality traits and potential knowledge, PTIT exhibits superior performance. This adequacy is apparent in the performance of PTIT-PTA (personality traits ablation) and PTIT-PKA (potential knowledge ablation). The PTR score for PTIT-PTA decreased to 18.09 and the PSS score decreased by 3.06, suggesting that LLMs are able to perceive explicit personality traits. The BLEU and Rouge-l scores on the PCIP-PKA decreased to 18.46 and 8.07, respectively, indicating that potential knowledge is a key factor in guiding conversation topics. This is in some sense not surprising as the most of the character's personality is revealed by having been involved in certain events. (4) Psychological activities and images descriptions impairs the consistency of the model's PSS scores compared to output of final content directly, as can be seen from the experimental results of PTIT-WPM (without psychological activities and media): PSS score improved by 1.42. (5) The evaluation results can be slightly different using different foundation models, as illustrated by the PCIP-DSC using deepseek-chat as a critic.\nThe above findings play a critical role in helping us determine the final instructions to balance the evaluation metrics.\n4.4.2 Personality Conditioned Instruction tuning (PTIT)\nPTIT result as shown in Table 4. We observe that PTIT shows a considerable enhancement in role-playing performance compared to PCIP baselines in terms of BLEU, ROUGE-1, and personality score similarity (PSS). In contrast to the findings of Result 3 - PCIP-WPM, the addition of psychology activities did not result in a significant decrease in PSS scores compared between PTIT and PTIT-WPM, with a difference of only 0.04 percentage points, this is because the model has learned to correlate the output of psychological activities with personality traits during the training process, enriching the information prior to the final output of content in a similar way to COT, as shown by the bold blue text in the bubble in Figure 3.3.\nHaving psychological activities and media resources is more in line with human habits, firstly, the psychological activity information facilitates us to directly observe the inner activities of the model and enhances the interpretability of the LLMs, because the psychological activities will establish explicit connections with the personality traits. The results shown based on the current training method and model structure support us to build more complex and controllable and personalized LLMs.\nWe observe a significant decrease in CPR and PTR scores compared between PCIP, PCIT and PSIT, which is due to the fact that in real scenarios each generated content is alternately related to profiles and personality traits, but it is closely"}, {"title": "5 Conclusion", "content": "In this study, we introduce Orca, an approach to integrate psychological theories BigFive personality trait into existing role-playing methods. We constructed the OrcaData dataset using prompt engineering and state-of-the-art open-souce LLMs. We designed two approaches, PTIT and PSIT, aim to enhance LLMs perceiving personality traits. Through these methods, we developed OrcaBench, a benchmark for assessing the performance of personality-infused role-playing models. The experimental results demonstrate the effectiveness of our approach and strong role-playing capabilities."}, {"title": "Limitations", "content": "\u2022 Limitations of the benchmark. Neuroticism is not usually manifested in social media, and the differences are difficult to distinguish from the questionnaire format and personality traits are very often expressed in behavior;\n\u2022 Limitations of the implicit modeling. How to fuse personality trait score vectors is a challenge, this paper only presents a feasible idea, more appropriate methods are yet to be proposed."}, {"title": "Ethics Statement", "content": "Since role-playing can lead to LLMs of Jailbreaking [Fu et al., 2024]. it is recommended to employ moderation and filtering mechanisms to curb adverse content dissemination [Wang et al., 2024]. Staab et al. [2024] mentions that current LLMs may violate personal privacy by inferring personal attributes from text during inference. The assets in our work are strictly for research purposes, and we oppose the use of the framework proposed here to extract personal information in any aspect of life. It is the responsibility of researchers and users to ensure the ethical use of Orca."}, {"title": "A Appendix", "content": "A.1 Prompts"}, {"title": "A.2 Hyperparameters", "content": null}]}