{"title": "RFPPO : Motion Dynamic RRT based Fluid Field - PPO for Dynamic TF/TA Routing Planning", "authors": ["Rongkun Xue", "Jing Yang", "Yuyang Jiang", "Yiming Feng", "Zi Yang"], "abstract": "Existing local dynamic route planning algorithms, when directly applied to terrain following/terrain avoidance, or dynamic obstacle avoidance for large and medium-sized fixed-wing aircraft, fail to simultaneously meet the requirements of real-time performance, long-distance planning, and the dynamic constraints of large and medium-sized aircraft. To deal with this issue, this paper proposes the Motion Dynamic RRT based Fluid Field - PPO for Dynamic TF/TA Routing Planning. Firstly, the action and state spaces of the proximal policy gradient algorithm are redesigned using disturbance flow fields and artificial potential field algorithms, establishing an aircraft dynamics model, and designing a state transition process based on this model. Additionally, a reward function is designed to encourage strategies for obstacle avoidance, terrain following, terrain avoidance, and safe flight. Experimental results on real DEM data demonstrate that our algorithm can complete long-distance flight tasks through collision-free trajectory planning that complies with dynamic constraints, without the need for prior global planning.", "sections": [{"title": "I. INTRODUCTION", "content": "In modern autonomous systems such as self-driving vehicles, and mobile robots, route planning algorithms are crucial, especially for drones or fighter jets in terms of terrain following and avoidance. From a decision-making perspective, route planning can be divided into two types: global static path planning and local dynamic path planning. Global static path planning aims to construct feasible routes on high-precision maps, while local dynamic path planning allows the system to perceive and adapt to real-time environmental changes. This includes responding to moving and static obstacles, and, integrating high-precision terrain maps, to generate or update optimal and safe paths in real time.\nFor large and medium-sized fixed-wing aircraft, due to the complexity of dynamic constraints and the vast scope of the maps, route planning algorithms typically employ both global static planning and local dynamic planning to complete flight missions. This approach requires prior offline global static planning to provide a global trajectory from the starting point to the destination, followed by local dynamic planning during flight based on environmental information to return to the global trajectory. However, when the prior map information is not accurate enough, or the actual flight environment is complex, the trajectory optimization performance of this method is limited. Furthermore, as large, and medium-sized fixed-wing aircraft have relatively weaker maneuverability while performing autonomous TF/TA, existing works are more focused on global static algorithms.\nGlobal path planning primarily relies on designing heuristic and incremental functions. Koenig S et al. [1] proposed the LPA* algorithm, an incremental heuristic search variant of the A* algorithm. This algorithm categorizes nodes into three states: locally consistent, locally over-consistent, and locally under-consistent. It addresses the issue of changing edge costs on a finite grid map over time, as caused by changes in obstacles and grid points. This solves the efficiency problem of repeatedly using A* for research under these conditions. Moreover, in its planning process, the start and goal points are fixed, further limiting the power requirements of large and medium-sized fixed-wing aircraft. The Rapidly exploring Random Tree (RRT) algorithm aims to effectively search non-convex, high-dimensional spaces by randomly building a space-filling tree. This tree is incrementally constructed from randomly sampled points in the search space, with a bias towards exploring large unsearched areas, and can easily handle obstacles and differential constraints. Jiaming Fan and Xia Chen [2] proposed a drone trajectory planning based on a bidirectional APF-RRT* algorithm with target-biasing. This approach guides the generation of random sample points with a target-biasing strategy, uses a bidirectional RRT* algorithm to establish two alternating random search trees, and introduces an improved artificial potential field method in the bidirectional growth trees, significantly reducing the number of iterations.\nLocal dynamic programming algorithms focus more on real-time performance. Dave Ferguson [3] proposed the Field D* algorithm, which uses linear interpolation on grids to allow path points not to be limited to endpoints. This approach allows for smoother planned curves as changes in planning direction are no longer restricted to \u03c0/4 increments. However, this method does not consider dynamic factors, making it challenging to plan paths for large and medium-sized aircraft effectively. The DWA (Dynamic Window Approach) algorithm [4] samples multiple speed sets in the velocity space (v,w) and simulates their trajectories over a unit of time. It selects the optimal trajectory and corresponding (v,w) to drive the robot's movement. Unlike D*, DWA fully considers the robot's dynamics but only simulates and evaluates the next step, making it inefficient for avoiding obstacles like 'C' shaped ones. When extended to three-dimensional space, its computational load increases drastically, unsuitable for real-time planning. The Artificial Potential Field (APF) algorithm [5-7], known for its simpler theory, often experiences significant path"}, {"title": "II. METHOD", "content": "For the task of planning for large and medium-sized fixed-wing aircraft in complex terrain, we propose a framework named Motion Dynamic RRT based Fluid Field - PPO (RF-PPO). This framework can achieve collision-free trajectory planning that adheres to dynamic constraints without relying on prior global planning, thereby effectively facilitating long-distance flight planning.\nOur framework's input state space includes information such as threats and the aircraft's own observations, while the action space covers the strength and direction for constructing flow fluid. By integrating field velocity with aircraft dynamics, we achieve more trackable trajectories during state transitions. In the Agent section, we introduce the state space and the RFPPO neural network. We then discuss the RFPPO action space based on field velocity and dynamic constraints, along with its state transition process. Additionally, this study combines the concept of safe reinforcement learning with the characteristics of TF/TA tasks, proposing a method for key point safety decision-making based on Motion Dynamics Rapidly exploring Random Trees (RRT). This not only"}, {"title": "A. Agent", "content": "The state space and neural networks together constitute the intelligent agent we have defined. The task of this agent is to model the fluid field based on information from the state space, thereby deriving an efficient trajectory planning algorithm. The efficiency and simplicity of the fluid field ensure that it can accomplish flight missions within a smaller state space and a simple neural network, without the need for extensive environmental information.\n1) State space\nOur state space is composed of multiple positional vectors and informational scalars, which comprehensively detail the aircraft's current flight state and its obstacle avoidance challenges. It encompasses the vector distances from the aircraft's current position to both the start and target points, the vector length to the nearest point of any observable obstacles, the current altitude above ground, and flight state parameters, such as the heading and climb angles.\n2) RFPPO Network\nWe employ the Proximal Policy Optimization algorithm for learning and decision-making, utilizing a parameterized probability distribution $\u03c0\u03c1(\u03b1|s) = P(a | s; 0)$ to replace the deterministic policy of the value function \u03c0:\u03c2 \u2192 a.Compared to other reinforcement learning algorithms, this class of algorithms holds unique advantages in handling continuous spaces.\nWe developed two fully connected neural network models, each with four identical layers (N = 4). ReLU (Rectified Linear Unit) activation functions are applied between these layers, while the final layer uses the 'tau' activation function for output range control. The actor network, aligning its input layer with the state space dimension, selects actions based on current state. Its output layer matches the action space dimension. And, the critic network assesses state values, concentrating on the long-term effects of the agent's present state on policy decisions.\nSpecifically, we make policy parameters 00, and make clipping threshold e, and for k=0,1, 2,..., we collect set of partial trajectories Dk on policy $\u03c0\u03ba = \u03c0(\u03b8\u03ba)$. So, estimate advantages Atk using any advantage estimation algorithm and we can take K steps of minibatch SGD. Subsequently, we update the actor network using $LCLIP (0) = \u0395[\u03a3=0 [min (re(0), clip (re(0),1 \u2212 6,1 + \u2208)\u00c2*)]]$ and train the critic network by calculating the difference between the discounted return $Gt = rt+1 + Yrt+2 + \u00b7\u00b7\u00b7 + y\u00b2-tr1+1 + \u03b3T+1-tv(ST+1))$ and the network output Criticstate using the MSE loss function. The reward at time T, which is derived from the designed reward function based on the state space, will be discussed in section E. Additionally, y represents the discount factor for rewards, used in reinforcement learning to balance the importance of immediate versus future rewards, thus guiding the agent to make rational decisions between long-term goals and short-term actions."}, {"title": "B. RFPPO Action Space", "content": "If the coordinates of the aircraft are directly defined as the action space in reinforcement learning, it necessitates the design of complex reward functions and neural network architectures to ensure the feasibility of the task and the compliance of the flight trajectory. Therefore, drawing inspiration from the concepts of Interfered Fluid Dynamical Systems (IFDS) and Artificial Potential Fields, we use four positive constants as parameters, transforming the action space from directly deciding the next coordinate point to responding to the perceived field of the current environment. Under this approach, flight path planning is conducted under the influence of this field.\n\u2022 Ground Disturbance Intensity (\u03b2): The influence exerted by the ground on the agent is abstracted as an upward repulsive potential field emanating from the ground. When this value is large, the agent is more inclined to perform terrain-following tasks, thereby maintaining a greater distance from the ground. Conversely, when the value is small, the agent tends to engage in terrain avoidance, reducing its proximity to the ground.\n\u2022 Repulsive Field Strength (p): This parameter represents the intensity of the repulsion exerted by an obstacle on the agent in that state. A higher value of p generates a stronger reaction force, pushing the agent away from the obstacle; conversely, a lower value results in a weaker force, potentially allowing the agent to approach the obstacle.\n\u2022 Tangential Field Strength (\u03c3): This parameter is used to quantify the strength of the flow field created by obstacles on the tangential plane. It not only generates a repulsive effect but also forms a guiding effect on the tangential plane. When this value is high, the agent's trajectory converges more rapidly towards the target point; conversely, when the value is low, the agent's trajectory becomes smoother, enhancing the transitional and continuous nature of the flight path.\n\u2022 Tangential Field Angle (0): This parameter defines the direction of the tangential field, endowing the agent with the ability to find and follow flow field trajectories that conform to kinematic constraints within the flow field."}, {"title": "C. State Transition", "content": "Through the values in the action space, we can abstract the obstacles and environment into a fluid field, ultimately represented as the velocity of the field\u016b. By using $Pt+1 = Pt+\u016b\u00b7AT$, we obtain the coordinates for the desired trajectory planning at the next moment, Pt+1(unrestricted) \u00b7 Simultaneously, it's necessary to integrate adjustments based on the aircraft's dynamics. This correction ensures that while we follow the direction of the fluid field for state transitions, we also strictly adhere to the characteristics of the aircraft's dynamics. This results in the trajectory planning coordinates for the next moment, Pt+1(restricted)\u00b7\n1) Flow Field Velocity\nIn this section, we elaborate on how to combine the Interfered Fluid Dynamical System and Artificial Potential Field to construct the fluid field, and ultimately derive the flow field velocity from the fluid field.\nFirst, we quantify the threat of obstacles to the agent by (1)."}, {"title": "B. Ground Disturbance Field", "content": "To achieve TF/TA in complex terrains, we incorporate the concept of artificial potential fields. Through Eq 3, we define the ground disturbance field, which is always perpendicular to the ground surface."}, {"title": "a) Repulsive Field", "content": "Eq 2 quantifies the repulsion effect and direction of obstacles on the agent. When a large or medium-sized fixed-wing aircraft approaches an obstacle, the agent experiences repulsion in the direction of the normal vector to the surface of the obstacle."}, {"title": "b) Tangential Field", "content": "Eq 4 quantifies the strength and direction of the tangential guidance exerted by obstacles on the fluid.\nEq 5 quantifies the influence of the kth obstacle on the field. In practical engineering applications, a constant Rconf is established, meaning only disturbances from obstacles within a distance less than Rconf from the agent are considered, thus reducing the data volume in the algorithm's iterative process."}, {"title": "D. Key Decision Points in Motion Dynamics RRT", "content": "a) Motion Dynamics RRT\nThe Motion Dynamics RRT [18] algorithm largely adopts the framework of the RRT* algorithm. However, to address"}, {"title": "b) Ground Disturbance Field", "content": "In the RFPPO training process, key decision points, keylist, are determined based on distance. Particularly, more decision points are set when the distance to the endpoint increases, to ensure effective exploration. At these points, we employ Motion Dynamics RRT sampling with the same time sampling rate as PPO for global planning. The RRT quickly ascertains whether the current state can reach the endpoint or identifies the time step T\u2081 when it cannot. If it's the latter, we terminate the training and return Rrrt = T1 After implementing this algorithm, training on a relatively complex map only requires about 54 hours on a 3080, enabling our model to achieve real-time path planning within a 200KM range. This approach nearly saves 30% of the time compared to methods not using it, leading us to integrate this improvement into our framework."}, {"title": "E. Reward Function", "content": "The purpose of the reward function is to motivate the agent to achieve its objectives. Based on the characteristics of the TF/TA tasks, we designed the reward function according to Eq 10. The ratiown/w\u3002 influences the large aircraft's propensity for terrain avoidance and following, while wp and wr affect the aircraft's flight posture.\nFor the first part rh, we use Eq 13 to ensure that large aircraft can follow the terrain while maintaining the safest possible distance close to the ground.\nFor the second part, robs, we use the method of Eq 12, employing a gradient-based reward approach to ensure that large aircraft stay as far away as possible from dynamic obstacles and choose safer penetration routes.\nThe third part, rp, is usually 0 and only imposes limits through Eq 15 when the deviation angle and climb angle exceed @good. This ensures that the paths planned by the controller are easy to follow. The purpose of this sparse reward is to encourage the agent to focus more on TF/TA.\nIn the fourth part, rrrt is typically 0, only changing when the agent is at a key point. Here, we use Motion Dynamics RRT to assess if the agent can reach its destination while meeting dynamic constraints. If deemed unreachable, rrrt is set to -T, where T represents the predicted number of future failed steps."}, {"title": "III. EXPERIMENTS", "content": "To validate the effectiveness and robustness of our algorithm, we select a diverse Digital Elevation Model (DEM) map as the basis for training and testing. This map is approximately 100KM*100KM in size, encompassing terrains such as canyons, forests, and plains. We randomly position 10 groups of dynamic threats across the map, featuring movement patterns including sine, circular, tangent, and straight lines. In the experiments, the takeoff point is randomly located within a 20km area around 34\u00b023\u203255\u2033N 107\u00b025\u203231\u2033E, and the destination is within a 20km area around 37\u00b042\u203204\u2033N 113\u00b059\u203258\u2033E. We randomly select 20 points, forming 10 sets of start and target points, and conduct multiple flight path plannings for each set to ensure the stability of the results. The simulations are run on a computer equipped with a CPU Intel Core i3-8100 3.60 GHz.\nIn the specific experiment, we set the number of training rounds to 10,000, with a batch size of 512. The learning rates for the critic and actor networks are both 0.001. The reward decay rate y' is 0.98. To ensure our simulation scenario is realistic, the agent can only obtain dynamic threat information when it is within 10KM of the threat. There is also a 0.05 probability of not receiving threat information."}, {"title": "IV. CONCLUSION", "content": "For large and medium-sized fixed-wing aircraft operating in complex environments, this study proposes the RFPPO flight path planning algorithm based on the aircraft motion dynamics model. Tailoring to mission characteristics, we restructured the state and action spaces of reinforcement learning through APF and IFDS, while strictly adhering to aircraft dynamics in state transition. Additionally, we introduced a Key Point training strategy based on Motion Dynamics RRT, significantly reducing the computational resources required for training. Extensive simulations on DEM maps were conducted, developing an effective reward function that converges efficiently. The results demonstrate that the algorithm can effectively perform TF/TA in highly complex environments, achieving dynamic obstacle avoidance within 25 milliseconds. In the future, we look forward to further exploring the potential of Transformer networks in the flight state recognition of large and medium-sized aircraft and their application in disturbance flow field control for multi-aircraft collaborative avoidance."}]}