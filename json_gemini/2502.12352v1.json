{"title": "TOWARDS MECHANISTIC INTERPRETABILITY OF\nGRAPH TRANSFORMERS VIA ATTENTION GRAPHS", "authors": ["Batu El", "Deepro Choudhury", "Pietro Li\u00f2", "Chaitanya K. Joshi"], "abstract": "We introduce Attention Graphs, a new tool for mechanistic interpretability of\nGraph Neural Networks (GNNs) and Graph Transformers based on the mathe-\nmatical equivalence between message passing in GNNs and the self-attention\nmechanism in Transformers. Attention Graphs aggregate attention matrices across\nTransformer layers and heads to describe how information flows among input\nnodes. Through experiments on homophilous and heterophilous node classification\ntasks, we analyze Attention Graphs from a network science perspective and find\nthat: (1) When Graph Transformers are allowed to learn the optimal graph structure\nusing all-to-all attention among input nodes, the Attention Graphs learned by the\nmodel do not tend to correlate with the input/original graph structure; and (2) For\nheterophilous graphs, different Graph Transformer variants can achieve similar\nperformance while utilising distinct information flow patterns. Open source code:\ngithub.com/batu-el/understanding-inductive-biases-of-gnns", "sections": [{"title": "INTRODUCTION", "content": "Graph Neural Networks (GNNs) and Graph Transformers (GTs) have emerged as core deep learning\narchitectures behind several recent breakthroughs in physical and life sciences (Zhang et al., 2023),\nwith AlphaFold (Jumper et al., 2021) being the most prominent example. Despite their remarkable\nsuccess, these models remain largely \u2018engineering artifacts', as noted by Demis Hassabis (Hassabis,\n2024), requiring dedicated tools to interpret their underlying mechanisms and advance our scientific\nunderstanding (Lawrence et al., 2024). While significant progress has been made in mechanistic\ninterpretability of regular transformers in natural language processing (Elhage et al., 2021; Bricken\net al., 2023), similar tools for GNNs and GTs for scientific applications are currently lacking.\nIn this paper, we leverage the mathematical equivalence between message passing in GNNs and\nthe self-attention mechanism in Transformers (Joshi, 2020; Vaswani et al., 2017) to investigate\ninformation flow patterns in these architectures. The core insight is that Graph Transformers produce\ntwo types of attention matrices: (1) matrices from different heads that capture distinct relationships\nbetween nodes, analogous to heterogeneous graphs; and (2) attention matrices across layers that\nrepresent how information flows through a dynamically evolving network over time. We introduce\nAttention Graphs, a principled framework that aggregates these attention matrices into a unified\nrepresentation of information flow among input nodes, as illustrated in Figure 1. Through Attention\nGraphs, we utilize techniques from network science (Rathkopf, 2018; Krickel et al., 2023) to analyze\nhow learned information flow patterns in GTs relate to the underlying graph structure. Based on the\nfindings, we explore how different GT variants, which enforce graph structure to varying degrees,\nlearn distinct information flow patterns despite performing similarly.\nOur experiments across multiple architectures and node classification tasks reveal two key findings.\nFirst, when architectures do not explicitly constrain attention to the underlying graph structure"}, {"title": "PRELIMINARIES", "content": "Graph Neural Networks. A graph, G = (V, E), is a mathematical structure that consists of a set\nof vertices V representing entities and a set of edges E \u2286 V \u00d7 V representing pairwise relationships\nbetween entities (Bronstein et al., 2021). A Graph Neural Network (GNN) for node classification\nlearns a function f that maps nodes V to an associated set of class labels Y per node. Each node i\nis represented by an input feature vector xi \u2208 Rd, where d is the feature dimension. We focus on\ngraphs with binary relationships between nodes, characterized by an adjacency matrix A \u2208 Rn\u00d7n,\nwhere aij = 1 if nodes i and j are connected, and 0 otherwise. Thus, each graph is described by two\nmatrices: the adjacency matrix A and the node feature matrix X \u2208 Rn\u00d7d, where n is the number of\nnodes. The 1-hop neighbourhood of node i is defined as N = {j | aij = 1}. More generally, the\nk-hop neighborhood is Nk = {j | (Ak)ij \u2260 0}, where Ak denotes the k-th power of A.\nGNNs are the standard toolkit for deep learning on graph-structured data, using graph topology to\npropagate and aggregate information between connected nodes (Veli\u010dkovi\u0107, 2023). Unlike feed-\nforward networks (standard multi-layer perceptrons) which process each node independently via\nMLP(xi) = Yi, GNNs use both the node features X and the graph structure A simultaneously\nwhen predicting node classes. Formally, a GNN implements a permutation-equivariant function\nf(X, A) = y such that f(PX, PAPT) = Py for any permutation matrix P. This permutation\nequivariance ensures the predictions for each node are invariant to node ordering."}, {"title": "Message Passing", "content": "GNN layers are implemented via message-passing (Battaglia et al., 2018)\nwhere nodes iteratively update their representations by aggregating information from their neighbors.\nFormally, a node's representation hli \u2208 Rdmodel at layer l is updated to hl+1i via:\nhl+1i = \u03c6(hli, \u2295j\u2208Ni\u03c8(hli, hlj, cij)).\nHere, \u03c8 is a message function that determines how information flows between nodes i and j based on\ntheir representations and edge features cij, \u2295 is a permutation-invariant aggregation function (like\nsum or mean), and \u03c6 is a node-wise update function that combines the aggregated messages with each\nnode's current representation. \u03c6 and \u03c8 are typically implemented as multi-layer perceptrons (MLPs)\nwith learnable weights shared across all nodes. For the binary graphs in our experiments, edge\nfeatures cij simply equal the corresponding entries aij in the adjacency matrix, though in general\nthey can encode richer edge attributes.\nEquation 1 provides a general formulation that encompasses most widely-used GNN architectures,\nincluding Graph Convolutional Networks (GCNs) (Kipf & Welling, 2017), Graph Isomorphism\nNetworks (GINs) (Xu et al., 2019), and Message Passing Neural Networks (MPNNs) (Gilmer\net al., 2017). We are particularly interested in attentional GNNs, where the message function \u03c8 is\nimplemented via the self-attention operation (Veli\u010dkovi\u0107 et al., 2018; Brody et al., 2022). In this case,\nthe message function \u03c8 is a weighted sum of the representations of neighboring nodes, where the\nweights are computed using an attention mechanism.\n\u03c8(hli, hlj, eij) = LocalAttention (WQhli, WKhli, WVhlj) = exp(QKi)\u03a3k\u2208Ni exp(QKk)Vj"}, {"title": "Graph Transformers", "content": "GNNs and Transformers have deep mathematical connections (Joshi, 2020).\nTransformers are attentional GNNs operating on fully-connected graphs, where self-attention models\nrelationships between all pairs of input tokens (Vaswani et al., 2017), i.e. graph nodes:\nhl+1i = \u03c6(hli, \u2295j\u2208V\u03c8(hli, hlj, eij)) = FFN(hli + \u03a3j\u2208V\u03c8(hli, hlj, eij)),\n\u03c8(hli, hlj, eij) = GlobalAttention (WQhli, WKhli, WVhlj) = exp(QKi)\u03a3k\u2208V exp(QKk)Vj.\nThis ability to attend to and gather information from all nodes allows Transformers to learn complex\nlong-range dependencies without being constrained by a pre-defined graph structure or suffering\nfrom oversquashing bottlenecks (Di Giovanni et al., 2023).\nConversely, GNNs are Transformers where self-attention is restricted to local neighborhoods (Buterez\net al., 2024), as formalized in equation 2, which can be realized in equation 3 by setting the mes-\nsage/attention weights to zero for any nodes i and j that are not connected in the graph, i.e. masking\nthe attention. These insights have given rise to Graph Transformers (GTs) that generalize Transform-\ners for graph-structured data (Dwivedi & Bresson, 2021; M\u00fcller et al., 2023). GTs aim to overcome\noversquashing in GNNs by allowing global attention while still leveraging graph structure as an\ninductive bias (Ramp\u00e1\u0161ek et al., 2022)."}, {"title": "Attention Matrices in Graph Transformers", "content": "Each GT layer performs a message passing operation\nas in equation 3, where the message function \u03c8 is implemented via the global self-attention operation\nin equation 4. We can thus define an attention matrix Al \u2208 Rn\u00d7n at each layer as a function of Hl and\nA, where Hl \u2208 Rn\u00d7dmodel contains the representation of a node from the graph in each of its rows and\nA is the adjacency matrix. A GT with NL layers and NH attention heads per layers will result in\nNL \u00d7 NH attention matrices every time the model is run on an input."}, {"title": "ATTENTION GRAPHS FRAMEWORK", "content": "Processing graph-structured data through GNNs and GTs results in multiple attention matrices - one\nfor each attention head in each layer. To understand these models, we need a principled approach\nto aggregate these matrices into a single Attention Graph that captures the overall information flow\namong nodes. In this section, we first formalize a unified design space of GNNs and GTs based\non their attention mechanisms, and then introduce our framework for constructing and analyzing\nAttention Graphs to reveal the algorithmic patterns learned by different architectures."}, {"title": "DESIGN SPACE OF GRAPH TRANSFORMERS", "content": "Having established the connection between GTs and GNNs through the attention mechanism and re-\nsulting attention matrices, we can formalise a spectrum of architectures based on two key dimensions:\nthe parametrization and sparsity of the attention matrix A. The parametrization dimension refers\nto whether the attention matrix is fixed or learned, while the sparsity dimension refers to whether\nthe attention matrix is restricted to the neighborhood of the node or is allowed to be dense/global.\nFigure 2 visualizes these dimensions."}, {"title": "DESIGN SPACE OF GRAPH TRANSFORMERS", "content": "Having established the connection between GTs and GNNs through the attention mechanism and re-\nsulting attention matrices, we can formalise a spectrum of architectures based on two key dimensions:\nthe parametrization and sparsity of the attention matrix A. The parametrization dimension refers\nto whether the attention matrix is fixed or learned, while the sparsity dimension refers to whether\nthe attention matrix is restricted to the neighborhood of the node or is allowed to be dense/global.\nFigure 2 visualizes these dimensions."}, {"title": "Sparse and Constant (SC)", "content": "The attention matrix is sparse and fixed, with attention coefficients\ninversely proportional to the square root of the degree of the node being attended to, recovering GCNs\n(Kipf & Welling, 2017). The layer-wise update equation is:\nhl+1i = FFN(hli + \u03a3j\u2208Ni\u221a(d(i)d(j))hlj)"}, {"title": "Sparse and Learned (SL)", "content": "The attention matrix is sparse but learned, with attention coefficients\nlearned as a function of the node features, recovering GATs (Veli\u010dkovi\u0107 et al., 2018; Brody et al.,\n2022). The layer-wise update equation is:\nhl+1i = FFN(hli + \u03a3j\u2208Ni exp(QKi)\u03a3k\u2208Ni exp(QKk)Vj)"}, {"title": "Dense and Learned but Biased (DLB)", "content": "The attention matrix is dense and learned, but biased to\nbe inversely proportional to the shortest path length between the nodes, recovering the Graphormer\n(Ying et al., 2021). The layer-wise update equation is:\nhl+1i = FFN(hli + \u03a3j\u2208V exp(QKi + bij)\u03a3k\u2208V exp(QKk + bik)Vj),\nwhere the bias term bij = 1/shortest path length between nodes i and j encourages the model to\nattend to nodes that are closer in the graph without enforcing a strict neighborhood structure."}, {"title": "Dense and Learned (DL)", "content": "The attention matrix is dense and learned in an unbiased manner, akin\nto the Transformer attention mechanism (Vaswani et al., 2017). This is the most flexible model that"}, {"title": "AGGREGATING ATTENTION ACROSS HEADS AND LAYERS", "content": "To understand how information flows within GNNs and Graph Transformers, we need to combine\nmultiple attention matrices: (1) attention matrices from different heads, which can be viewed as\ncapturing different types of relationships between nodes, similar to heterogeneous graphs; and (2)\nattention matrices across layers, which represent how information flows through a dynamically\nevolving network over time. This section presents our framework for aggregating these matrices into\na single Attention Graph that reveals the overall information flow patterns in the model."}, {"title": "Aggregating attention across heads", "content": "Attention matrices across heads can be viewed as representing\ndifferent types of relationships between nodes, analogous to edge types in heterogeneous graphs.\nTo enable systematic analysis, we need to combine these relationships into a single homogeneous\ngraph whose edges capture the aggregate information flow from all attention heads at a given layer.\nWe first examine whether different heads learn similar or distinct attention patterns in Appendix\nFigure 6, which shows pairwise correlations between attention values learned by different heads in\n1-layer 2-head variants of our models defined in Section 3.1. For homophilous datasets, SL models\nexhibit strong positive correlations between heads, indicating they converge on similar attention\npatterns. This correlation remains positive but weaker for heterophilous datasets in both SL (row 1)\nand DLB models (row 2). DL models (row 3) show the weakest correlations, though still consistently\npositive. Crucially, we never observe negative correlations between heads, suggesting they learn\ncomplementary rather than competing patterns.\nThese intuitive observations motivate aggregatingacross heads through simple averaging: AAvg. = 1NH\u03a3NHi=1 AHi, where NH is the number of heads and AHi is the attention matrix for head i. While\nstraightforward, this averaging approach effectively captures the overall information flow patterns\nlearned by multi-head attention without losing important signals from any individual head."}, {"title": "Aggregating attention across layers", "content": "Attention matrices across layers capture how information\nflows through the network over multiple message passing steps, forming dynamic graphs that evolve\ntemporally. Our goal is to aggregate these matrices into a single Attention Graph that captures the\noverall flow of information across all layers of the model. We analyzed the correlation between\nattention patterns learned in different layers in Appendix Figure 7, which shows pairwise correlations\nbetween attention values learned in first and second layers of 2-layer 1-head models. We observed\npositive but weaker correlations compared to across-head patterns. This suggests that simple averaging"}, {"title": "AGGREGATING ATTENTION ACROSS HEADS AND LAYERS", "content": "may not be sufficient for combining attention across layers, necessitating a more nuanced approach\nthat captures how information propagates through the network.\nWe propose using matrix multiplication of attention matrices from successive layers to model the\ninformation flow from successive layers (illustrated in Figure 3):\nAAgg. = AL2 AL1,"}, {"title": "AGGREGATING ATTENTION ACROSS HEADS AND LAYERS", "content": "This formulation elegantly captures indirect attention patterns: if node i attends to node j in layer\n2, it indirectly attends to all nodes that j attended to in layer 1. Mathematically, row i of AAgg.\nrepresents a linear combination of rows in AL1, weighted by attention coefficients in row i of AL2.\nThis multiplication operation naturally models multi-hop information flow - for example, if node j\nattends to k in layer 1 and node i attends to j in layer 2, the matrix product captures the indirect flow\nof information from k to i through the intermediate node j."}, {"title": "Constructing the Attention Graph", "content": "After performing a forward pass through the model, we\nobtain attention matrices Aelh for each layer l and head h. To construct the Attention Graph, we\nfirst aggregate attention across heads at each layer to obtain layer-wise attention matrices Al, then\nmultiply these matrices sequentially across layers to construct the final aggregate attention matrix\nAAgg. This process captures the overall information flow patterns learned by the model, revealing\nhow information propagates through the network over time."}, {"title": "EXPERIMENTS", "content": null}, {"title": "EXPERIMENTAL SETUP", "content": null}, {"title": "Datasets", "content": "We evaluate our framework on 7 node classification datasets with varying levels of\nhomophily, with further details in Appendix A. The datasets include: (1) Citation Networks (Ho-\nmophilous): Cora and Citeseer (Yang et al., 2016), where nodes represent scientific papers (with\nbag-of-words features from abstracts), edges represent citations, and classes are research topics. (2)\nWikipedia Networks (Heterophilous): Chameleon and Squirrel (Rozemberczki et al., 2019), where\nnodes are Wikipedia articles (with noun presence features), edges are hyperlinks, and classes are\nbased on monthly traffic. (3) University Webpages (Heterophilous): Cornell, Texas, and Wisconsin\nfrom WebKB (Pei et al., 2020), where nodes are university webpages (with bag-of-words features),\nedges are hyperlinks, and classes are webpage categories."}, {"title": "Models", "content": "We experiment with four variants of Graph Transformers defined in Section 3.1: SC, SL,\nDLB, and DL. These models span a spectrum of attention mechanisms, from sparse and fixed to\ndense and learned, allowing us to systematically analyze the impact of different inductive biases on\nthe information flow patterns learned by the model. We use the Transformer Encoder module in\nPyTorch to implement all models, with the only difference being the attention mask and/or bias used.\nWe experiment with number of layers NL \u2208 {1,2} and number of heads NH \u2208 {1,2}, resulting in 4\nmodel variants for each dataset (described subsequently). We set the hidden dimension dmodel = 128\nfor all models, resulting in node representations HL \u2208 Rn\u00d7128."}, {"title": "NODE CLASSIFICATION PERFORMANCE", "content": "Before analyzing the mechanistic interpretability of our models,\nwe first evaluate their performance on node classification tasks. While achieving state-of-the-art\nperformance is not our primary goal, we verify that our models are competitive with existing\napproaches. As shown in Appendix Table 3, we observe distinct performance patterns across different\ngraph types: (1) On homophilous graphs (Cora, Citeseer), models that restrict attention to local\nneighborhoods (SC, SL) achieve the highest accuracy. (2) On heterophilous graphs (Cornell, Texas,\nWisconsin), removing neighborhood constraints (DL) leads to better performance, suggesting the\nimportance of long-range interactions. (3) On moderately homophilous graphs (Chameleon, Squirrel),\nbiasing attention towards neighbors without strict restrictions (DLB) achieves optimal results. Notably,\nthese performance patterns remain consistent across different model configurations, with minimal\nvariation when increasing the number of attention heads or layers."}, {"title": "HOW DO GRAPH TRANSFORMERS DISTRIBUTE ATTENTION?", "content": "Figure 4 visualizes how different GT variants distribute attention between neighboring and non-\nneighboring nodes. In single-layer SL models, nodes can only attend to their immediate neighborhood"}, {"title": "DO GRAPH TRANSFORMERS RECOVER INPUT GRAPH STRUCTURE?", "content": "Next we check whether the learned attention in our dense models matches the underlying graph\nstructure. We construct quasi-adjacency matrices by thresholding Attention Graphs to recover binary\nconnectivity patterns, as described in Appendix B. Table 1 compares the quasi-adjacency matrices\nagainst original adjacency matrices using F1-scores. Low F1-scores (<4%) for DL models across\nall datasets indicate their attention patterns do not reflect the input graph structure. In contrast,\nDLB models show moderate to high F1-scores (28-86%), suggesting they partially recover graph\nconnectivity through biased attention. Multi-head models generally achieve higher F1-scores, likely\nbecause averaging across heads reduces noise in learned attention patterns. However, for both\narchitectures, F1-scores decrease with model depth, indicating that deeper layers develop more\ncomplex information flow patterns beyond the original graph structure."}, {"title": "A CLOSER LOOK AT QUASI-ADJACENCY MATRICES", "content": "Finally, we analyze the quasi-adjacency matrices to understand information flow patterns across\ndifferent architectures. Figure 9 reveals three key findings:"}, {"title": "RELATED WORK", "content": "Our work bridges GNN explainability and mechanistic interpretability of Transformers, aiming to\nunderstand how information flows through these architectures during inference from the perspective\nof graph theory and network science (Rathkopf, 2018; Krickel et al., 2023).\nGNN Explainability. Early work on explaining GNNs focused on identifying influential subgraphs\nfor specific predictions (Ying et al., 2019). This spawned several approaches including concept-based\nmethods (Magister et al., 2021), counterfactual explanations (Lucic et al., 2022), and generative\nexplanations (Yuan et al., 2020). While valuable, these methods primarily analyze input-output\nrelationships rather than internal model dynamics. Other work has investigated physical laws\nlearned by GNNs using symbolic regression (Cranmer et al., 2020), but a systematic framework for\nunderstanding information flow in GNNs remains lacking.\nMechanistic Interpretability of Transformers. Recent advances in mechanistic interpretability aim\nto reverse-engineer neural networks into human-understandable components (Olah, 2022; Elhage\net al., 2021). This has led to breakthroughs in understanding model features (Olah et al., 2017; Elhage\net al., 2022), identifying computational circuits (Nanda et al., 2023; Cammarata et al., 2020), and\nexplaining emergent behaviors (Barak et al., 2023; Wei et al., 2022). These insights have practical\nbenefits - enabling better out-of-distribution generalization (Mu & Andreas, 2021), error correction\n(Hernandez et al., 2022), and prediction of model behavior (Meng et al., 2022). Our work aims"}, {"title": "DISCUSSION", "content": "In this paper, we developed a framework to mechanistically interpret GNNs and Graph Transformers\nby analyzing their attention patterns from the perspective of network science. While previous work\non Transformer circuits focused on discrete feature interactions (Bricken et al., 2023; Meng et al.,\n2022), our approach captures continuous information flow patterns between nodes through Attention\nGraphs. Through this lens, we demonstrated that architectures with different graph inductive biases\ncan achieve similar performance while implementing distinct algorithmic strategies. This observation\nchallenges the common practice of evaluating models solely based on accuracy metrics and highlights\nthe need for more holistic evaluation approaches.\nOur framework has important limitations to address in future work. First, while matrix multiplication\neffectively models indirect attention flow, it may not capture non-linear interactions from activation\nfunctions between layers. Second, aggregating heterogeneous attention patterns across heads and\ntemporal patterns across layers into a single matrix may oversimplify complex model dynamics.\nAdditionally, our preliminary experiments are currently limited to small models (up to 2 layers,\n2 heads) and node classification tasks where homophily analysis provides clear insights into the\nimportance of graph structure. Future work should extend the Attention Graph framework to\nmore complex graph architectures and inductive tasks to develop a more complete understanding\nof information flow in Graph Transformers. Attention Graphs opens up numerous directions for\napplying network science and graph theory to analyze Transformers, including spectral analysis,\ncommunity detection, and information flow dynamics. These tools could reveal deeper insights into\nthe emergent computational strategies learned by these models."}]}