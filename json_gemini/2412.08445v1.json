{"title": "TapeAgents: a Holistic Framework for Agent Development and Optimization", "authors": ["Dzmitry Bahdanau", "Nicolas Gontier", "Gabriel Huang", "Ehsan Kamalloo", "Rafael Pardinas", "Alex Pich\u00e9", "Torsten Scholak", "Oleh Shliazhko", "Jordan Prince Tremblay", "Karam Ghanem", "Soham Parikh", "Mitul Tiwari", "Quaizar Vohra"], "abstract": "We present TapeAgents, an agent framework built around a granular, structured log (tape) of the agent session that also plays the role of the session's resumable state. In TapeAgents we leverage tapes to facilitate all stages of the LLM Agent development lifecycle. The agent reasons by processing the tape and the LLM output to produce new thought and action steps and append them to the tape. The environment then reacts to the agent's actions by likewise appending observation steps to the tape. By virtue of this tape-centred design, TapeAgents can provide AI practitioners with holistic end-to-end support. At the development stage, tapes facilitate session persistence, agent auditing, and step-by-step debugging. Post-deployment, one can reuse tapes for evaluation, fine-tuning, and prompt-tuning; crucially, one can adapt tapes from other agents or use revised historical tapes. In this report, we explain the TapeAgents design in detail. We demonstrate possible applications of TapeAgents with several concrete examples of building monolithic agents and multi-agent teams, of optimizing agent prompts and finetuning the agent's LLM. We present tooling prototypes and report a case study where we use TapeAgents to finetune a Llama-3.1-8B form-filling assistant to perform as well as GPT-40 while being orders of magnitude cheaper. Lastly, our comparative analysis shows that TapeAgents's advantages over prior frameworks stem from our novel design of the LLM agent as a resumable, modular state machine with a structured configuration, that generates granular, structured logs and that can transform these logs into training text a unique combination of features absent in previous work.", "sections": [{"title": "1 Introduction", "content": "In the coming years, we will likely witness widespread deployments of Large Language Model (LLM) Agents: complex user-facing and background workflows that interleave traditional programming with LLM-based intelligence. This big paradigm shift in software architecture will greatly challenge AI practitioners who put LLM agents to work. The agent developers and applied scientists will have to troubleshoot and improve systems that operate in non-stationary environments and deal with non-deterministic LLM behavior and the LLM's often fragile instruction following. For the LLM agent adoption to go smoothly and lead to good outcomes, it is crucial that agent developers and applied scientists operate in appropriate frameworks that enable effective tooling. Developers and researchers have recently proposed many agentic frameworks that support practitioners at different stages of the agent development lifecycle. Several frameworks, like LangChain (Chase, 2022), CrewAI and AutoGen (Wu et al., 2024a), help developers quickly build an agent using low-code paradigms, such as prompt-chaining or multi-agent teams. Others, like LangGraph (Chase, 2023), offer low-level support in achieving resumability, asynchronous execution, concurrency and instrumentation. At the other end of the spectrum are frameworks built by researchers like DSPy (Khattab et al., 2023a), TextGrad (Yuksekgonul et al., 2024) and Agents (Zhou et al., 2023a), that usually focus on data-driven optimization of the agent performance with model finetuning and prompt-tuning algorithms, while putting less emphasis on the needs of the agent developers.\nIn this technical report, we present TapeAgents a new holistic agent framework that supports practitioners at both the agent development and data-driven agent optimization stages. We achieve both objectives by building the framework around a comprehensive, structured, granular, semantic-level log of the agent session that we call a tape, a term that also gives the framework its name (see Figure 1 for an illustration). The agents in TapeAgents read the tape to make the LLM prompt and then process the LLM output to append new steps to the tape: thought steps to express reasoning and action steps to request external inputs. The environment responds to the action steps at the end of the tape with observation steps that it likewise appends to the tape. The orchestrator invokes the agent and the environment in an alternate fashion and maintains full control over their interactions. By design, the orchestrator can resume from any intermediate tape, which enables session persistence and step-by-step debugging, both key developer requirements for an agent framework. For data-driven algorithms, tapes record the attribution of each step to the respective part of the agent configuration, which facilitates training, data generation and automatic prompt-tuning. Crucially, for both manual debugging and algorithms, agents can reuse lightly adapted tapes from other agents and revise their own tapes. This allows practitioners to maximally benefit from imperfect historical tapes by earlier versions of the agent, both for evaluating the newer versions and for improving them algorithmically. Last but not least, agents stream their intermediate events to the orchestrator to enable delightful interactive experiences.\nWe invite the reader to start their TapeAgents journey with the technical presentation of the framework in Section 2. There, we cover the details of agent architecture, agent-environment orchestration, tape content and structure. Section 3 describes three low-code agent-building framework prototypes on top of TapeAgents: one for monolithic agents, another for multi-agent teams and the third one with easy-to-tune function-like prompts. The same section also covers early versions of our Studio toolsuite for development and debugging and our Optimize toolsuite for agent optimization. In Section 4, we present diverse examples of building and optimizing agents using TapeAgents framework and tooling. Section 5 presents a deeper case study of a key practical TapeAgents use case: optimizing the quality of a cost-effective conversational assistant using tapes from an expensive multi-node Teacher agent. After presenting the framework and the examples we offer the reader a detailed comparison of TapeAgents with prior work in Section 6. Lastly, Section 7 discusses possible extensions and applications of TapeAgents."}, {"title": "2 TapeAgents: foundations", "content": "Our TapeAgents framework proposes an agent-building paradigm that facilitates all stages of the AI Agent development lifecycle. This section presents the framework in a detailed bottom-up approach. First, we introduce the building blocks: the nodes, the agents, and the environment. Then, we explore how these parts can be composed and orchestrated to build a tape-centered system. In this section, we also describe the tape structure and metadata."}, {"title": "2.1 Nodes and Steps", "content": "As outlined in Figure 1, in TapeAgents, one builds the agent from nodes: the basic atoms of intelligence. A node describes one LLM call and the classical symbolic processing of the call's output. The agent will dynamically determine which node to run next based on the tape. Nodes generate new tape entries that we call steps: basic atoms of the agent's memory. Examples of what an agent can do in a step include making a long-term plan, reasoning about how to fulfill the plan or how to use a tool, requesting a tool call. Among these examples, the last one is an action step as it requests interaction with or has an impact on the agent's environment. The first three examples are thoughts: the agent's inner reasoning steps. The remaining step type in TapeAgents is the observations that the agent receives from its environment in response to the agent's actions. The reader can find an example tape with color-coded actions, thoughts and observations in Figure 3. In TapeAgents we often define a tape type by specifying what specific actions, thoughts, and observations classes it can contain, though all such tapes are currently merely aliases for the one and only tapeagents.core.Tape class.\nA typical node uses an LLM to generate tape steps. One defines this process with two node methods: make_prompt and generate_steps. First, the node constructs the LLM prompt through its make_prompt method that has the following Python signature:\n\ndef make_prompt(self, agent, tape) -> Prompt                                                                  (1)\n\nSome nodes perform only the conventional non-neural computation, like taking a branching decision. These nodes can use the default make_prompt implementation that produces a null prompt. Note that the node does not call the LLM directly but only makes a prompt. This is a deliberate design decision to keep all node methods pure functions, i.e. deterministic functions with no side effects.\nSecond, the node generates steps based on the stream of tokens that it receives from the last LLM call. One defines the step-generating behavior of a new node class in its generate_steps method:\n\ndef generate_steps(self, agent, tape, llm_stream) -> Generator [Step | PartialStep]                                 (2)\n\nIf a node produced a null prompt in its make_prompt method above, the 1lm_stream will also be null. All nodes must generate Step objects; some can also parse the LLM token stream incrementally to produce partial steps which the agent will pass through to the application without adding them to the tape. Figure 2 shows how the agent runs one node and adds the resulting steps to the tape, along with the relationship between make_prompt and generate_steps.\nBy default, the agent calls its nodes sequentially and appends the steps they create to the tape that the agent is asked to continue. A code example of a node implementing these two methods can be seen in Appendix A under class SearchAgentMainNode."}, {"title": "2.1.1 Nodes That Can Make Training Data", "content": "Some nodes also implement the reverse direction make the LLM output that would be required to produce the steps at a given index in the tape. The respective node method is\n\ndef make_llm_output(self, agent, tape, index) -> LLMOutput                                          (3)\n\nThis method is crucial for making fine-tuning data."}, {"title": "2.2 Agents", "content": "Like nodes, a TapeAgent agent generates steps and makes a new tape by appending the generated steps to the input tape. Specifically, agent.run(tape) runs an iterative reasoning loop that, at every iteration, selects a node, lets it make the prompt and generates the next steps (see Figure 2). By default, the agent will run its nodes sequentially, unless the previous node produced a special SetNextNode step that explicitly the determines the next that will run next. The loop continues as long as the nodes only generate thoughts. When a node produces an action, the agent stops and returns a new tape with the generated steps from all iterations appended to it. More precisely, agent.run(tape) returns an AgentStream object for streaming events like partial tapes and steps, but the final new agent tape is easy to extract from the stream object using AgentStream.get_final_tape() method.\nAn agent may have subagents for whom this agent is the manager. The subagents can have further subagents, which gives rise to a hierarchical agent organization with a single manager-free root agent on top. Given an input tape, the root agent determines the next active organization member to which it will delegate the generation of next steps. By default, the root agent makes the delegation decision by looking at the special Call and Respond thoughts. When an agent A wants the root to delegate to an agent B, A will append Call(agent_name=\"B\", content=...) thought to the tape with an optional free-form message in the content field. When B responds by appending Respond(content=...), A becomes active again. Note that both Call and Respond will affect the delegation logic at the next agent iteration. To sum up the delegation description, the root delegates to the agent that was called last and has not responded yet. See Figure 3 for an example of communication between a financial analyst agent and its web search helper. See Appendix A for a listing of the complete code for this example."}, {"title": "2.2.1 Tape Views", "content": "In a multi-agent system different agents usually are expected to maintain their respective different states. In TapeAgents the tape combines the states of all agents in the team. To determine what they should base their acting and reasoning on, most agents use tape view stack. For each agent that has not responded yet, the view contains the steps that this agent can see. Specifically, for an agent A the view contains the tape's steps starting from the Call step that initiated A's activity and excluding the inner steps of the subagents that A called (see Figure 3). Note that the default tape view stack is only one possible way to determine what parts of the tape each agent should see. For example, one can let an agent see all its prior Call steps to enable an inter-agent conversation history.\nA reader familiar with how Python interpreter works can find agents similar to Python functions, node similar to lines of Python code, steps similar to Python bytecode instructions, the tape view stack similar to the Python call stack and tape views similar to Python frames."}, {"title": "2.2.2 Optimizable Agents", "content": "Agent optimization algorithms tune agent prompts (Khattab et al., 2023a; Pryzant et al., 2023; Zhou et al., 2023b) or alter agent structure (Hu et al., 2024) in order to maximize the agent's performance. To make such algorithms applicable to as many agents as possible, we standardize the structure of the agent configuration. We achieve this by making tapeagents.agent.Agent a Pydantic model with the following mandatory fields: 1lms for the LLM configurations, templates for the prompt templates, nodes for the nodes, and subagents for the subagents.\nAgents can also make training data for the LLM that they use. An agent's agent.make_training_text(tape) method reconstructs the LLM calls from a given tape, validates the reconstruction by replaying the step generation and returns training text characters. Internally, agent.make_training_text uses node.make_llm_output method introduced in Section 2.1.1; hence all nodes must implement this method for the agent to be trainable."}, {"title": "2.3 Environment", "content": "Just like nodes and agents, the environment in TapeAgents makes a new tape by adding steps to an existing tape. The main method of an environment object is:\n\ndef react(self, tape) -> Tape.                                                                                     (4)\n\nThe environment.react searches for the unfulfilled actions in the tape and adds the corresponding observation steps to the tape. Unlike nodes and agents, the environment may be non-deterministic and have side effects. We encourage agent developers to put all the deterministic and pure-function aspects of the system in the agent part, isolating only non-deterministic, computationally heavy or transactional aspects in the environment part."}, {"title": "2.4 Orchestration", "content": "To run a TapeAgent-based agentic application, one must alternate between running the root agent (which handles the delegation internally) and calling the environment to react to the agent's actions (see Figure 1). While we provide a default tapeagents.orchestrator.main_loop orchestrator for this purpose, we expect many application developers to build their custom orchestrators to closely control the agent-environment communication and ensure safety or enhance iteration logic."}, {"title": "2.4.1 Resumption and Replay", "content": "We designed TapeAgents with resumption and replay as key priorities. To resume, one can just restart the orchestration from an intermediate tape. For testing purposes, one can run an agent with replayed observations and LLM outputs and verify that this process leads to the same tape or print the diff otherwise. We found the replay tests to be incredibly helpful in our development work. When applicable, one can also replay the tape's observations (or even some of the agent's steps) in a new session to evaluate a new agent, though the old observations can be implausible if the new tape deviates too much from the old one."}, {"title": "2.5 Tape Metadata and LLM Call Database", "content": "Regardless of the orchestration method, the implementations of agent.run() and environment.react() ensure that the tape and its steps contain rich metadata, including these fields:\n\u2022 tape.metadata.author: which agent or environment made this tape; either by authoring it, or by adding steps to it, or by making a revision of another tape.\n\u2022 tape.metadata.parent_id: the ID of the parent tape of which the current tape is a continuation (when applicable).\n\u2022 step.metadata.agent: the hierarchical name of the agent that generated the step.\n\u2022 step.metadata.node: the name of the node that generated the step.\n\u2022 step.metadata.prompt_id: the identifier (id) of the prompt that led to the generation of this step, see the explanation below.\nWhen an agent runs a node, the node generates a unique ID for the prompt that it builds at this iteration. The prompt ID thus serves as the unique identifier of a node execution, i.e., of a specific iteration when the node was active. The ID also links the step to the LLM call from the node run so we can trace the origin of each step down to the specific prompt and LLM output. We store the prompt and the output for all LLM calls in an SQLite database. One can view LLM calls as an effective part of the tape in that they are always easily accessible; we don't include them in the tape to keep the latter lightweight.\nThe metadata is crucial for building the tooling and the algorithms that empower the agent developer. Figure 3 shows a visualization of some metadata fields."}, {"title": "3 TapeAgents: tooling", "content": "The TapeAgents foundation that we covered in Section 2 allows the creation of a wide range of reusable agent components, tooling and learning algorithms. What the right building blocks and tooling are often depends on the application area. In our initial release, we provide several prototypes to jump-start future open-source collaborations."}, {"title": "3.1 Low-code Mini-Frameworks", "content": "Building agents requires implementing many similar template rendering (node.make_prompt) and text parsing (node.generate_steps) routines. As a part of TapeAgents, we provide three examples of low-code mini-frameworks for building agents by composing and configuring off-the-shelf components:\n1. MonoAgent exemplifies the most straightforward way to implement a monolithic agent: make a comprehensive prompt from all the data from the tape and the possible step schemas, then parse the LLM output using the schemas. One creates a MonoAgent from MonoNode nodes whose prompts are the same except for the final user message instruction. A MonoAgent also requires the agent developer to provide Pydantic models for all possible steps that the agent can generate.\n2. TeamAgent shows how an AutoGen-style agent team can work in one tape. One can create three different kinds of team agents: (a) an initiator that send the first Call message, (b) a manager that chooses the next active agent, (c) a worker agent that responds using its system prompt.\n3. LLMFunction demonstrates how one can build agents using function-style prompt templates, akin to DSPy signatures. These prompt templates are particularly easy to optimize by adding demonstrations."}, {"title": "3.2 Tooling", "content": "In TapeAgents, the agent configuration and the tape are highly structured and linked with metadata. This allows us to offer developer tooling for a broad range of possible TapeAgents. In the initial release, we include several app prototypes. We offer TapeAgents Studio (see Figure 8 in Appendix D), an app to interact with the agent and its tape, Tape Browser (Figure 9 in Appendix D), an app to inspect a batch of tapes, and Tape Diff (Figure 10 in Appendix D), which compares two batches of tape. Furthermore, for agent optimization, we provide algorithms for auto-prompting, LLM fine-tuning, and a modular Reinforcement Learning orchestrator. The finetuning component uses Accelerate (Gugger et al., 2022) and DeepSpeed (Rasley et al., 2020) libraries and supports tuning resumption, experiment tracking, reproducibility, LoRA tuning, and distributed training. The above apps and algorithms represent the first steps towards the fully fledged Studio and Optimize modules that we envision in Figure 1."}, {"title": "4 Examples", "content": "In an initial set of examples, we demonstrate agents that represent different agent-building paradigms, as well case-studies of using different agent optimization methods."}, {"title": "4.1 Financial Analyst and Their Web Search Helper", "content": "To offer an example with the maximal educational value, we have implemented a user-facing financial analyst agent that can delegate searching the web to its subagent. We show the structure of the analyst agent and an example tape in Figure 3. Our introductory hands-on notebook takes the reader through a journey from TapeAgents basic concepts to building this agent.\nFor illustrative purposes, we implemented the nodes in this example from scratch, without using mini-frameworks from Section 3.1. We offered the analyst agent an environment with several tools: one to get the company ticker, another to download stock data, as well as several tools to search and browse the web. We inform the analyst and their web search helper of the tools that they can use by including their tools' schemas in the prompts that the respective agent's nodes make. The agent operates on a tape type called DialogTape, which can only contain two kinds of actions: ToolCalls to call one or more tools and AssistantStep to respond to the user. The agent uses the same ToolCalls step with different content to call different tools. This is the use we intend for DialogTape: quick agent prototyping without declaring usecase-specific step schemas, though we believe most TapeAgents users will find it useful to declare their own action and thought types."}, {"title": "4.2 Open-domain Question Answering and Web Browsing With Monolithic Agents", "content": "To validate TapeAgents quantitatively, we build two agents that target existing benchmarks. The first one is a question-answering (QA) agent that targets the GAIA benchmark (Mialon et al., 2024). The QA agent can search the web, run Python code, read multiple file types. To meet the GAIA evaluation requirements, we prompt the agent to output the precise short answer only. We build the QA agent from MonoNode nodes, with two planning nodes and one acting node in which the agent loops (see Figure 4). Table 1 shows that the agent performs quite well for such a simple agent. Our agent beats the more complicated Magentic-1 agent (Fourney et al., 2024) which incorporates Orchestrator and multiple executor agents and also progress ledger. For comparison implementing our agent only requires gathering the tools, declaring corresponding action steps (like ReadDocumentAction and UseCalculatorAction) and declaring usecase-specific thoughts for reasoning (like ListOfFactsThought and NewFactThought). There is a lot of room for further improvement of the agent, starting from the obviously beneficial majority voting ensembling of runs and up to the multiagents, self-critic techniques, search over the tree of thoughts, etc.\nWe used a similar approach to build a web-browsing agent that targets the WorkArena benchmark (Drouin et al., 2024). The BrowserGym environment has been used for the evaluation, with a convenient interface to the real browser. We declared action classes for different browser actions like HoverAction and PressAction and so on. Figure 4 illustrates the exact agent structure, which is following ReAct (Yao et al., 2023b)+Planning approach, with goal-setting, reflection and acting nodes. We benchmark our web agent and find that it performs competitively (see Table 2)."}, {"title": "4.3 Data Science With a Team of Agents", "content": "To demonstrate that TapeAgents natively supports the multi-agent paradigm, we implement a \"data science\" agent team that consists of the Requestor, Manager, Software Engineer, Code Executor and Asset Reviewer agent. Figure 7 (in Appendix C) shows the team in action as it builds a stock price comparison plot. We drew inspiration from the popular AutoGen framework for the multi-agent communication pattern in this example. Benefits of the TapeAgents implementation of this agent team include that one can easily resume the team from an intermediate tape or use tapes to optimize the entire agent organization algorithmically."}, {"title": "4.4 Finetuning a Cheap Math Agent", "content": "We test-drive TapeAgents fine-tuning component with a distillation example. We train a LLAMA3.1-8B-based math agent using tapes by its teacher counterpart with LLAMA3.1-70B under the hood. We equip each agent with a reasoning node and run the environment with a calculator tool. After finetuning on 3,000 samples from 1,000 teacher tapes, the student performance rises significantly from 66.2% to 77.5%, though the teacher's performance, at 93.1%, remains much higher still."}, {"title": "4.5 Prompt-Tuning for Agentic RAG", "content": "In our last example, we show how the tape, the agent configuration and the metadata linking them, can serve as a medium to implement data-driven agent optimization algorithms. In this example, we use LLMFunction prompt templates (see examples in Appendix B) that describe the intended behavior of a transformation that the LLM should perform, including the instruction, the input/output format, and optionally, a few demonstrations. We designed LLMFunction to make it possible to implement DSPy-like algorithms in TapeAgents. Below we describe how we used TapeAgents components to closely reimplement the DSPy introductory notebook.\nWe compose a Retrieval-Augmented Generation (RAG) agent. Figure 4 illustrates the structure where the agent performs two rounds of query generation and Wikipedia retrieval and then produces a short factual answer. We build this agent mostly from LLMFunctionNode nodes that describe how the input fields in their respective LLMFunction templates should be filled with the steps from the tape. The only different kind of a node is a null-prompt node that deduplicates the retrieved paragraphs. We tune the prompts of the resulting 5-node agent by adding demonstrations to the function prompt templates. We obtain demonstrations by running the agent on 50 HotpotQA training examples and filtering the tapes with the wrong answer or duplicate queries. We try 10 different combinations of 4 randomly selected good tapes, extract demonstrations from them, add them to the agent's prompt template and measure the validation set performance, namely the sum of retrieval and answer accuracies. We select the best agent and evaluate on a set of 300 examples. Results in Table 3 show that prompt-tuning leads to significant gains in both retrieval and answer accuracy. The optimized agent is still a TapeAgent that can be resumed from any intermediate tape, unlike a free-form Python program that uses DSPy. Notably, the implementation of the actual demonstration selection algorithm took just 12 lines of code (see Appendix B), highlighting how the TapeAgent structures and metadata facilitate algorithm implementation."}, {"title": "5 Case Study: Building a Cost-Effective Enterprise Form-Filling Assistant", "content": "A key use case of TapeAgents is optimizing LLM Agents to offer great quality services at a fraction of the cost. In this section, we present a fleshed-out example of how these goals can be achieved for a conversational assistant that can help fill a request form and submit the request."}, {"title": "5.1 Problem Setting", "content": "Employees in large enterprises often fill forms to request resources, assistance or access. A conversational assistant can make the form-filling experience smoother by guiding its user to the right form, by accepting the user's free-form inputs, and by answering the questions that the user may have in the process. For a great experience, the assistant must also gracefully handle the \"unhappy-path\" situations, such as when the user's ask is impossible to fulfill or when the assistant cannot answer the user's question. In this case study, we show one can use TapeAgents to train a cost-effective assistant that scores high according to a formal metric of user experience that we call the GREADTH score. GREADTH stands for Grounded, REsponsive, Accurate, Disciplined, Transparent, and Helpful. We will explain these metrics in Section 5.2.\nFor simplicity, we consider building a restricted assistant:\n\u2022 The assistant should answer questions solely based on the form documentation; it does not have to retrieve any additional documents.\n\u2022 The assistant can only help with one form at a time.\n\u2022 At the start of the conversation the assistant converses with the user to guide them to the correct form.\n\u2022 After the form is chosen, the assistant will help the user fill out the form correctly. The agent will not allow the user to switch to a different form after this point.\n\u2022 During the form-filling process, the assistant maintains the field values that the user has provided so far.\n\u2022 The interaction ends with either the form submission or the agent exiting the conversation after the user's confirmation.\nThe resulting form-filling setup is reminiscent of the Task-Oriented Dialogue setting that has been widely discussed in the literature (Rastogi et al., 2020; Budzianowski et al., 2018). Following this body of work, we will refer to form fields as slots."}, {"title": "5.2 Evaluation Criteria: GREADTH Experience", "content": "Despite the apparent simplicity of the form-filling setup, it can be non-trivial to develop a form-filler assistant that balances an excellent conversational experience with low hallucination rate and reasonable cost. To balance these desiderata, one must first define them in a measurable way. In our case study, we design our assistant to have maximum GREADTH: Grounded, REsponsive, Accurate, Disciplined, Transparent, and Helpful. We define these aspects as follows:\n\u2022 Everything a Grounded assistant says must be fully supported by the form documentation, the conversation history and the grounding statement. The latter defines the assistant's identity and purpose and constrains the assistant to form-filling. Small talk is considered ungrounded.\n\u2022 A Helpful assistant must actively take the conversation forward by asking for the user's intent, requesting the next slot to fill, or asking for confirmation before making the request once all slots have been filled. It should also (a) provide all relevant information regarding a slot when asking for it (default value, allowed values, optionality), (b) answer any user question if the form documentation provides the relevant information, (c) exit the conversation at any time if the user desires so.\n\u2022 An Accurate assistant must correctly identify the user's intent and import the relevant form documentation, fill the slots correctly based on user messages, update the slots if the user changes their mind, or skip the slots if relevant.\n\u2022 A Transparent assistant acknowledges all changes made to the partially filled form. This includes slot-filling or skipping slots that are optional or have a default value. The summary of slots changes can be concise, yet the user must be able to understand how the slots were affected. While in a mixed modality interaction the user may visually see the form changes, in a purely voice interaction such as talking over the phone the transparent behavior is essential.\n\u2022 A Disciplined assistant must follow its planning thoughts, such as requesting a slot, asking for confirmation, answering a question, rejecting incorrect slot values (as defined by the form documentation), or rejecting an invalid ask.\n\u2022 We require the assistant to be REsponsive to address a common experience issue with AI assistants: the robotic and opaque behavior when the user goes off the expected conversational path. We wish that AI assistant infers and acknowledges what the user had in mind, while explaining that their request or question is not possible. In particular, we want to cover the following scenarios:\nif the user tries to fill a slot with an invalid value, the assistant should acknowledge the value and respond that it is invalid;\nif the user offers information that looks like a value for a nonexistent but plausible slot, the assistant should acknowledge the value and the inferred name of the slot and respond that such slots is not available in this form;\nif at the form-filling stage the user's ask looks like a request for another form, the agent should acknowledge their ask and say that it can not fulfill it right now (note that in our setup the user must either finish filling the current form or exit);\nif the user asks a plausible question that the form documentation does not answer, the agent should acknowledge the question and say that it can not be answered.\nThe user may ask other requests that have nothing to do with form-filling (i.e. weather requests). In that case, the agent must politely decline the request and keep moving the conversation towards either submitting or aborting the request. To align our definition of responsiveness with the common sense meaning of this word, we also require a REsponsive assistant to acknowledge all valid slot values and valid questions. Thus we often deem a response that is not Transparent, or not Helpful also not REsponsive.\nThe GREADTH criteria above are binary, a conscious choice that we made to simplify the evaluation and the analysis. We acknowledge that this makes them somewhat crude, as e.g. two assistant answers can be both technically correct but can widely differ in readability and in the choice of the information to present. One can complement these criteria with a preference-based experience evaluation that implicitly covers fluency, verbosity, and other aspects of the assistant's response."}, {"title": "5.3 Design of a Form Filling TapeAgent", "content": "Tape Structure To build an agent that provides a GREADTH experience, we decompose the conversational form-filling task into smaller reasoning steps before each agent message. Having the GREADTH metrics in mind, we define the agent's thoughts to help it plan its response. The thoughts are used to represent a chain-of-thought of the agent, which includes (a) analyzing the user's intent (e.g. the form that is requested, the provided slot values, a question being asked); (b) updating the internal state of the conversation (e.g. slot-filling); and (c) planning the next actions (e.g. requesting a specific slot value or requesting a user confirmation). In particular, after each user message or observation, the agent must return:\n1. A list of thoughts: these include slot-filling related thoughts such as UpdateFunctionParametersThought; and message planning thoughts specifying the next slot to request (RequestFunctionParametersThought), the need to ask for confirmation before submitting the request (RequestFunctionCallConfirmationThought), planning to answer a question (AnswerFromFunctionSchemaThought), planning to inform the user that their question cannot be answered (NoAnswerInFunctionSchemaThought), refusing unsupported request/behavior/slot values (RefuseInexistentFunctionThought / RefuseToEngageThought / RefuseInvalidFunctionParameterValueThought), etc. The full list of thoughts is described in Appendix E.2.\n2. A single action: the agent returns a single action, such as searching available forms (ResolveFunctionAction), retrieving form documentation (InspectFunctionAction), replying to the user (PromptUserForTextMessageAction), submitting the request (CallFunctionAction), or exiting the conversation (ExitAction). Each action results in new observations (available forms, retrieved documentation, or user input), and ends the current agent turn.\nMulti-node Teacher Agent We experimented with various combinations of prompting techniques and LLMs to obtain the best performance (see GREADTH metrics in Sections 5.2 and human evaluation pro-"}, {"title": "6 Related Work", "content": "Autonomous agents aim to tackle complex tasks via self-guided planning and actions in often dynamic environments (Wang et al., 2024). Several strategies have been introduced to streamline agent's abilities to accomplish their objectives. One tried-and-true strategy to enhance the reasoning capabilities involves meta-reasoning where the agent is instructed to generate reasoning traces and plans prior to arriving at a final decision (Yao et al., 2023c). Another prominent strategy hinges on improving the agent's ability to conduct self-diagnosis (Xie et al., 2023; Kim et al., 2023; Shinn et al., 2024; Chen et al., 2024). Combining both strategies to find a success path from different reasoning paths and self-evaluating choices has also proven effective (Yao et al., 2023a). TapeAgents offers a holistic solution for seamlessly integrating these paradigms into agent behavior. In particular, Steps in TapeAgents represents planning and reasoning traces that guide the agent's actions and Nodes can be used for self-evaluation. Finally, Tapes serves as an abstraction for memory that logs agent's internal thought process and interactions with the environment. In this section, we provide a detailed overview of existing agentic"}]}