{"title": "EnIGMA: Enhanced Interactive Generative Model Agent for CTF Challenges", "authors": ["Talor Abramovich", "Meet Udeshi", "Minghao Shao", "Kilian Lieret", "Haoran Xi", "Kimberly Milner", "Sofija Jancheska", "John Yang", "Carlos E. Jimenez", "Farshad Khorrami", "Prashanth Krishnamurthy", "Brendan Dolan-Gavitt", "Muhammad Shafique", "Karthik Narasimhan", "Ramesh Karri", "Ofir Press"], "abstract": "Although language model (LM) agents are demonstrating growing potential in many domains, their success in cybersecurity has been limited due to simplistic design and the lack of fundamental features for this domain. We present EnIGMA, an LM agent for autonomously solving Capture The Flag (CTF) challenges. EnIGMA introduces newAgent-Computer Interfaces (ACIs) to improve the success rate on CTF challenges. We establish the novel Interactive Agent Tool concept, which enables LM agents to run interactive command-line utilities essential for these challenges. Empirical analysis of EnIGMA on over 350 CTF challenges from three different benchmarks indicates that providing a robust set of new tools with demonstration of their usage helps the LM solve complex problems and achieves state-of-the-art results on the NYU CTF and Intercode-CTF benchmarks. Finally, we discuss insights on design and agent behavior on cybersecurity tasks that highlight the need to adapt real-world tools for LM agents.", "sections": [{"title": "1 Introduction", "content": "Advancements in cybersecurity require continuous security analysis of new software systems. To increase the robustness of these systems, existing vulnerabilities must be rapidly detected and patched. With the increasing global connectivity of software via the internet, the attack surface also widens, making it difficult for manual cybersecurity analysis techniques to keep pace with this rapid expansion. These factors have necessitated the development of autonomous exploitation tools that can quickly detect software system vulnerabilities and generate patches to fix them. Cybersecurity competitions, such as the DARPA Cyber-Grand-Challenge and the DARPA AIxCC, have been designed to motivate the industry to develop such autonomous exploitation tools.\nWhile language models (LMs) are popularly used to help programmers write short code segments [12, 17, 35], LM-based agents have recently been introduced to autonomously program, solve bugs and develop new features [56, 59, 60, 61]. An LM agent is a system that works towards a specified goal through repeated LM interaction within an environment, such as an operating system.\nIn cybersecurity, LMs have been employed to develop both defensive and offensive applications [40]. For defense, existing work leverages LMs to enhance threat detection [36, 48], automate incident response [41], and mitigate vulnerabilities [9, 27, 34]. For offense, they are used for penetration testing [20], exploiting security flaws, and crafting advanced attacks [11, 22].\nAn important evaluation setting for LMs in offensive information security is Capture The Flag (CTF) challenges. CTFs are traditionally used to challenge human participants to solve a series of security puzzles or exploit vulnerabilities in simulated computer systems to obtain special strings (\"flags\") that have been hidden within the environment. These challenges test expertise in various cybersecurity skills, such as reverse engineering, binary analysis, cryptography, web exploitation, and network analysis. By mimicking real-world hacking scenarios in a controlled, competitive environment, CTFs provide a valuable educational resource to develop cybersecurity skillsets [33, 37, 67].\nRecent work extended these challenges for use as a benchmark to evaluate LMs' cybersecurity knowledge and capabilities [51, 62, 65]. The feasibility of solving CTF challenges with LM agents was first demonstrated in [50, 61]. However, these agents are limited in scope and capability and cannot adapt to new strategies after initial attempts fail, resulting in many unsolved challenges. Furthermore, existing agents [51, 65] lack suitable interfaces tailored to the cybersecurity domain. These limitations underscore the need for well-designed interfaces for agents that can handle a range of CTF challenges.\nTo address these limitations, SWE-agent [60] introduced the Agent-Computer Interface (ACI) concept. According to"}, {"title": "2 Background", "content": "Our work uses LMs as agents to autonomously solve Capture The Flag (CTF) challenges. Previous CTF benchmarks have shown their ability to serve as effective metrics to gauge the cybersecurity capabilities of LMs in practical scenarios since they fulfill three important benchmark features for LMs [46]:\n1. They simulate realistic real-world cybersecurity environments.\n2. They are challenging since they require several areas of expertise and persistent trial and error to solve.\n3. System-proposed solutions are easy to automatically validate because the goal is clear, i.e., to find a specific flag string.\nThis section presents background information about autonomous LM agents, focusing on LM applications in the cybersecurity domain."}, {"title": "2.1 Autonomous LM Agents", "content": "An agent in machine learning is a system that interacts with an external environment, taking sequential actions based on the feedback it receives to achieve a specific goal. With the increasing use of LMs, many LM-based agents have been developed to solve tasks across various domains. These LM agents operate in an action-observation loop [63] by iteratively generating actions using an LM, executing that action in the environment, and then using the resulting output to"}, {"title": "2.2 LMs in the Cybersecurity Domain", "content": "Recent research has explored the application of LMs in the cybersecurity domain, addressing both defensive and offensive aspects [40]. As defensive tools, LMs are used to protect systems by identifying and mitigating vulnerabilities, enhancing threat detection, and automating incident response. These models analyze vast amounts of data to detect anomalies, predict threats, and develop robust security protocols [1].\nAs offensive tools, LMs are used by attackers to conduct penetration testing, exploit security weaknesses, and craft sophisticated cyberattacks [11, 22]. LMs themselves are also part of the attack surface; a wide range of adversarial techniques can be used to manipulate safety-aligned LMs to generate harmful programs and develop more effective attack strategies [52]. The diverse applications of LMs in both protecting and attacking systems highlight their significant role in the evolving landscape of cybersecurity."}, {"title": "2.3 Capture The Flag (CTF) Challenges", "content": "Capture the Flag (CTF) is a competitive cybersecurity exercise where participants solve security-related challenges to capture virtual flags. The primary purpose of CTF challenges is to test participants' skills in areas like cryptography, reverse engineering, binary exploitation, and web security through practical, hands-on experience. These challenges often simulate real-world cybersecurity issues, providing a realistic environment for learning and practicing defensive and offensive techniques. As such, they are extensively used in research works as a proxy to measure the skill of attackers and defenders, human or LM-based, as detailed in Section 7.\nCTF challenges are divided into distinct categories, with six common types frequently featured in competitions:\n\u2022 Cryptography (crypto). Decrypt hidden ciphertexts, which involves understanding crypto algorithms and primitives and finding implementation flaws in them.\n\u2022 Reverse-Engineering (rev). Determine how a program operates using static or dynamic analysis of the program.\n\u2022 Web Exploitation (web). Identify and exploit vulnerabilities in web applications.\n\u2022 Forensics (forensics). Analyze information, e.g., network packet captures, memory dumps, etc., to find desired information.\n\u2022 Binary Exploitation (pwn). Exploit a vulnerability in compiled programs, allowing a competitor to gain a command shell on a vulnerable system.\n\u2022 Miscellaneous (misc). Challenges that do not fit into other categories and may require a wide range of security skills, such as data mining or social engineering.\nPopular online CTF platforms include HackTheBox (HTB) [25], CTFTime [15], TryHackMe [54] and Pic-OCTF [44]. These platforms offer a range of challenges and resources for both beginners and advanced users. CTFs are also a highlight of major cybersecurity conferences like DEF-CON, where the DEFCON CTF is one of the most prestigious competitions in the field [4]. CTFs are widely used in educational settings [26, 32, 55], cybersecurity training [14, 31], and by organizations to identify and develop talent [13]."}, {"title": "3 EnIGMA Components", "content": "We built EnIGMA on top of the SWE-agent [60], specifically incorporating its concept of the Agent-Computer Interface (ACI). As previously noted, an ACI is an interface through which agents interact with a computer environment. While these interfaces can include interfaces originally designed for human end-users, known as Human-Computer Interfaces (HCIs), different design principles are necessary for effective ACI design. As argued in [60], these principles should account for the unique characteristics and requirements of agents, which differ from those of human users.\nBy tailoring the interface to the specific needs of agents, we can enhance their accuracy and efficiency in interacting with computer systems. Some design principle of ACIs include their simplicity for ease of understanding; compactness of frequent command sequences to enhance efficiency; concise feedback to clearly communicate action outcomes; and robust error recovery mechanisms.\nThe SWE-agent architecture is based on ReACT [63], in which the LM produces a thought and an action at each step. The action is a single command that is executed in a Docker-ized environment, an isolated environment that ensures safe execution of challenges and reproducibility. The feedback from command execution is returned to the agent at each step. The system executes the thought-action-observation loop until either a successful submit happens or one of the following exit conditions are met:\n1. The budget for API calls to the LM has been exhausted, preventing actions by the agent (exit_cost),\n2. The maximum number of tokens the LM can handle at once has been exceeded (exit_context),"}, {"title": "3.1 Interactive Agent Tools (IATS)", "content": "We extend the ACIs introduced in SWE-agent using IATs, which enable the agent to use interactive tools within the environment. Tools useful for debugging (gdb, radare2), remote server interaction (netcat, socat) and penetration testing (metasploit) are widely used during CTF problem-solving and by cybersecurity experts. These tools are all interactive, i.e., they wait for user input, evaluate it, and print the results (read-eval-print loop, REPL). Current LM agents, which build their ACI around a running shell as central REPL, lack the ability to nest REPLs or start separate REPLs for interactive tools.\nIn EnIGMA, we build IATs based on two principles:"}, {"title": "3.2 EnIGMA Summarizers", "content": "LMs perform best if given concise inputs; superfluous context can degrade performance while increasing costs. Because agents require LMs to process entire trajectories, compressing context is of particular importance. For this reason, SWE-agent strips the output from all but the last five observations when passing the history to the LM to produce the next action. However, solving CTF challenges involves many commands that have particularly long outputs. For example, a function decompilation in a binary can produce an output that has"}, {"title": "3.3 Demonstrations and Guidelines", "content": "We incorporated demonstrations to enhance the agent's ability to solve new tasks (also known as in-context learning [8]). Demonstrations are sample problems taken from the development set for which we provide detailed trajectories that show how the problem can be solved using the tools available in the environment. They thus help agents understand how to utilize the tools correctly and how to plan their sequence of operations. We also incorporated general techniques for problem-solving, such as trial-and-error, by showing examples where the initial approach was incorrect and the subsequent one was successful. Additionally, using the chain-of-thought method [57] in the demonstrations encourages the agent to apply it to its own solutions. This method helps the agent break down complex tasks into a sequence of simpler sub-tasks by first describing its plans and thoughts explicitly, then executing its plan.\nTo further aid the agent, we sum up the demonstrations with guidelines by analyzing failed trajectories in the development set. We use different demonstrations and guidelines for each challenge category."}, {"title": "3.4 Adding Cybersecurity Tools", "content": "There is a strong overlap between the tools used in software engineering and those needed for solving CTF challenges\u2014including file editing, code linting and file-system navigation\u2014given that CTF challenges often demand coding skills for effective problem-solving. However, SWE-agent is not fully equipped to solve CTF challenges since it lacks some tools commonly used in the cybersecurity domain. We therefore extend SWE-agent with the tools from [50]: disassemble for disassembling binary functions; decompile for decompiling binary functions; check_flag for verifying flags; and give_up for allowing the agent to concede on a challenge."}, {"title": "4 Experiments", "content": "We now provide details about all experiments we conducted. First, we describe test benchmarks we selected. We then examine the development set we used to enable agent development without overfitting on test benchmarks. Finally, we frame the setup of all experiments, including models, metrics and baselines for our comparisons."}, {"title": "4.1 Test Benchmarks", "content": "For a comprehensive analysis of our agent, we evaluate EnIGMA on three benchmarks: NYU CTF [51], InterCode-CTF [62], and HackTheBox (HTB) [25]. In the following we describe the characteristics of each benchmark and present a summary of all benchmarks in Table 1.\nA wide variety of 350 challenges are available in six categories (crypto, forensics, pwn, reverse, web, and miscellaneous) on each benchmark. These benchmarks, which cover a wide range of skills necessary to replicate the real-world CTF scenarios, were selected to guarantee a diverse and representative evaluation environment.\nThe NYU CTF Benchmark contains 200 CTF challenges from the CSAW CTF competitions held between 2017 and 2023. These challenges simulate real-world security problems and range in difficulty from \"very easy\" to \"hard;\" they span six categories: cryptography, web, binary exploitation (pwn), reverse engineering, forensics, and miscellaneous. We use NYU CTF as the main benchmark for development and evaluation and report ablation results for the different features"}, {"title": "4.2 Development Set for NYU Benchmark", "content": "When developing machine learning systems, with LM agents being a special case of these, it is important to keep a separate development set that is used during development to define features that should be added or not added to the system. After selecting the best features, tools and configuration parameters based on this set, we can evaluate on the test benchmark to assess the final accuracy of the model and compare it to the existing state-of-the-art.\nHowever, no benchmarks or agents introduced to date incorporate this common ML practice, as discussed in Section 7. To address this gap, we constructed a development set of 55 CTF challenges from the same CTF competitions as the primary test set (NYU CTF). We collected CTF challenges across the same six categories, with category-wise composition presented in Table 2."}, {"title": "4.3 Experiment Setup", "content": "Models. Results, ablations, and analyses use three leading LMs to date, GPT-4 Turbo (gpt-4-1106-preview), GPT-40 (gpt-40) [42] and Claude 3.5 Sonnet (claude-3-5-sonnet-20240620) [3]. We use Microsoft Azure OpenAI [39] for OpenAI models and the Anthropic inference API [2] for Claude. The temperature is set to T = 0, and we use nucleus sampling with p = 0.95 for all models.\nBaselines. On the NYU CTF benchmark, we compare EnIGMA to the baseline agent in [51]. On the InterCode-CTF benchmark, we compare to the baseline in [62].\nMetric. We report % Solved using pass@1 as the main metric, which represents the proportion of instances where the agent successfully captured the flag on the first run. Note that multiple flag submissions are allowed during each run, and the agent terminates only upon a successful flag submission, allowing it to know whether it has succeeded or failed. This setup mirrors real-world CTFs, where players receive immediate feedback on the correctness of their flag submissions.\nWe also report the $ Avg. Cost metric, which represents the average cost of model API calls incurred by EnIGMA across all successfully solved instances. The budget per instance is limited to $3; if a run exceeds this budget, the instance is marked as unsolved due to cost constraints (exit_cost)."}, {"title": "5 Results", "content": "EnIGMA achieves the best performance, successfully solving 13.5% (27/200) of the full NYU CTF enchmark using Claude 3.5 Sonnet. This is more than three times higher than the result of the baseline model [51], which solves at most only"}, {"title": "5.1 Analysis of ACI Designs", "content": "We perform ablations of EnIGMA interfaces, summarized in Table 4. We selected Claude 3.5 Sonnet as the model for ablations since it shows the best performance overall. These tests reveal notable agent behaviors that demonstrate how different ACI designs affect performance, as discussed below.\nProper interactive interfaces are crucial to performance. Figures 6 and 7 show that the agent readily uses interactive tools, with interactive command sequences frequently"}, {"title": "5.2 Analysis of Agent Behavior", "content": "We now analyze the general behavior of EnIGMA, identify factors that increase its problem solving efficacy, and make recommendations for successful cybersecurity agents. We make this analysis as general as possible to be helpful to others designing ACIs for additional cybersecurity problems."}, {"title": "6 Discussion", "content": "This work presents an enhanced agent designed to solve CTF challenges along with a new development set based on CTFs for agent development. Our framework, an extension for SWE-agent, adds interactive tools that help the agent solve CTFs. Our quantitative analysis, conducted on three different benchmarks with 350 challenges, shows that incorporating interfaces well-designed for LM agents in the cybersecurity domain enable these agents to solve a high percentage of challenges, creating more effective LM-based applications for the cybersecurity domain. Future research could establish additional interfaces for this domain and create broader benchmarks.\nWhile EnIGMA shows significant improvement in solving CTF challenges, we discuss below some interesting phenomena we observed during testing on all benchmarks.\nSoliloquies in Claude. With Claude 3.5 Sonnet, we observe a surprising behavior, which we term soliloquizing, where the LM produces (sometimes multiple) thought, action, and (model-generated) \u201cobservation\u201d strings in a single response, completely side-stepping the agent functionality of interacting with the environment. Figure 9 shows an example soliloquy, where the LM generates an action and then proceeds to generate the observation by itself (instead of letting the environment generate the observation).\nThe LM is instructed in the system prompt to generate only one thought and one action that form its entire response. When it produces a soliloquy, it breaks this rule and generates one or more actions and response pairs."}, {"title": "7 Related Work", "content": "LM Agents for CTF. To facilitate autonomous solving of CTF challenges using LMs, researchers have implemented several agent frameworks that operate within Docker containerized environments. The InterCode framework integrated CTF benchmarks into its interactive coding reinforcement learning environment [61]. Another LM agent introduced in [50] that was specifically designed for automating CTF solving tasks incorporated the use of tools, thereby achieving a notable accuracy of 46% using GPT-4 on 26 CTF tasks collected from CSAW competitions. Our agent includes more cybersecurity tools and interfaces specifically designed and tested for LM agents as part of the ACIs for CTF solving; it is thus more accurate on both the InterCode CTF benchmark and the NYU CTF benchmark than best baselines.\nThe Cybench benchmark [65] creates a framework for solving CTF challenges and introduces a challenging CTF benchmark. Their best agent achieves an accuracy of 17.5% on this benchmark using Claude 3.5 Sonnet. Their agent environment is similar to EnIGMA's since both operate in Linux containers with pre-installed tools. While the Kali Linux container used in Cybench may appear advantageous due to its extensive range of preinstalled cybersecurity tools, the ACI design principles and empirical results indicate that an agent performs better with a focused set of tools that have clear interfaces; an overwhelming number of tools may cause confusion [60]. Furthermore, EnIGMA emphasizes generalization in CTF problem-solving by LM agents, avoiding the need to craft specific sub-tasks for each problem, as shown in Cybench, an approach that requires deep cybersecurity expertise and is both time and cost inefficient. Instead, we use several demonstrations and guidelines obtained from our development set as an in-context learning technique to enhance the agent's ability to solve CTF challenges.\nCTF Benchmarks. Recent research has developed several CTF benchmarks. In [50], a benchmark was derived from"}, {"title": "LM Application in Offensive Cybersecurity.", "content": "Many use cases have been explored for applying LMs in offensive cybersecurity. For instance, Meta's CyberSecEval2 benchmark [5], an extension of CyberSecEval1 [6], provides problems designed to assess the security risks and capabilities of LMs in assisting with cyberattacks. Similarly, [43] explores the \"dangerous capabilities\u201d of LMs, evaluating their performance on several tasks, including CTF challenges. The study demonstrates an overall inferior success rate compared to our agent, using Gemini models on benchmarks like InterCode CTF, HTB, and in-house CTF problems; their benchmark suite has not been released as open-source.\nA recent work, Project Naptime [24], introduces a new agent for discovering and exploiting memory safety issues, benchmarked on CyberSecEval2. Though this agent demonstrates improved interactive capabilities, including a debugger, a web browsing tool, and a Python interpreter, its interfaces are still limited to a single REPL session. This means only one command can be executed in the interactive process before it terminates and must be restarted, akin to generating a predefined script for these tools. In contrast, our agent supports nesting an interactive program REPL inside the main REPL, allowing for a truly interactive session and multi-process approach, similar to how humans use computer systems."}, {"title": "8 Conclusion", "content": "EnIGMA leverages the concept of Agent-Computer Interfaces and applies it to the cybersecurity domain. We observe a more than three-fold improvement in solved challenges compared to the previous best agent. Our agent introduces several new"}, {"title": "Ethics", "content": "LMs such as GPT-4 and Claude 3.5 raise new opportunities and difficulties in cybersecurity. While CTFs provide an organized setting for comparing task planning and automation, they mimic cyberattack scenarios by design, so ethics must be considered. As LMs develop, a range of ethical, security, and privacy issues surface that necessitate prudent deployment techniques.\nLMs can be abused, possibly being used for social engineering or in developing malicious software. Given that AI can be both a tool and a threat, it is imperative that ethical standards be followed when using it[58]. Legal and ethical issues are raised by the current legal framework's inability to keep up with AI advancements, particularly in terms of regulating outputs from non-human entities[45]. Furthermore, LMs run the risk of sustaining biases and even facilitating social control in the absence of a varied training set and sophisticated fine-tuning techniques [7]. Researchers advocate for robust policy frameworks to ensure ethical AI use while guarding against abuse, along with explainable AI methodologies that promote transparency and accountability, in order to mitigate these risks [10].\nLMs raise important ethical issues in the context of CTF challenges. Strengthening the foundation of AI ethics education is essential to bridging the cybersecurity training gap that exists with the rapid advancements in AI tools. To responsibly navigate AI-driven security threats, both professionals and students must be prepared with ethical training and critical thinking abilities [30]. The importance of responsible AI development in scenarios where probing and exploiting vulnerabilities are central is highlighted by the potential misuse of LMs to launch sophisticated attacks, including 'jailbreaking' the models to bypass ethical safeguards [16]. Decision-makers tasked with deploying LMs in cybersecurity contexts must comprehend these ethical implications [23]."}, {"title": "G.1 Main Agent Prompts", "content": "We refer to the main agent as the one that is responsible for solving the CTF challenge. This agent is provided with system template, demonstration template and an instance template. All have parameters that are determined using the CTF challenge that is currently being solved.\nSystem template. The system prompt presented in Figure 10 describes the environment for the CTF challenge, it is fixed for all challenges in every category, and it is not removed from the history at any time. This prompt contains also the flag format that is expected by the challenges, usually of the form flag{...}. This prompt contains also all the documentation for all interfaces provided, both in SWE-agent and the new interfaces described in Appendix C."}, {"title": "END OF DEMONSTRATION", "content": "Figure 11: A simplified demonstration template of a rev challenge from the development set showing how demonstrations are provided to the model as a single message. Here we show only the final 3 turns in the demonstration for brevity.\nInstance template. The instance template introduces the agent to the challenge instance specific information, such as the challenge name, description, category and additional optional files and/or remote server required for solving the challenge. We provide the agent some instructions and general guidelines on how to solve the challenge, based on analysis from development set failed trajectories."}, {"title": "IMPORTANT TIPS:", "content": "1. When you edit an existing file, try to minimize the changes you make to the file."}, {"title": "DEBUG SESSION TIPS:", "content": "1. Start a debugging session to debug a binary program by running debug_start program 'commandline-args'.\n2. The debug session runs GDB, so use the appropriate syntax to specify breakpoints and provide arguments.\n3. Debugging allows you to perform dynamic analysis of the program execution and view the internal program state at various points. First analyze the program via the disassembly and decompile commands to find interesting points to stop the execution, and then start debugging by setting meaningful breakpoints.\n4. Provide inputs to the program ONLY via an input file by providing arguments at the start, for example debug_start program '. NOTE that it is important to properly quote the arguments to avoid shell redirection. You cannot provide inputs to the program in any other way.\n5. The debug_exec command lets you execute arbitrary GDB commands write proper GDB syntax and use it to analyze program state. Remember to quote the commands properly. For example, debug_exec 'info registers' will print all the registers, debug_exec 'x/8wx $sp' will print 8 words from the stack, debug_exec 'x/20i $pc' will print disassembly of the next 20 instructions."}, {"title": "G.2 LM Summarizer Prompts", "content": "The LM summarizer we introduced in Section 3.2 uses slightly different prompts for the summarization task. For this purpose, we are not stacking the history, but rather we provide the LM summarizer with a simple instance describing the challenge, last action and last observation that requires summarization. The summarizer is required to produce a summary that is not longer than a configurable line count threshold.\nSystem template. Similar to the system template in the main agent, this template gives basic information about the summarization task, and the context of this summarization being part of a CTF challenge solving in a competition."}, {"title": "H Qualitative Analysis", "content": "We choose one successful and one unsuccessful challenge instances from NYU CTF benchmark to perform a full qualitative analysis of the problem-solving techniques pursued by the agent using the tools and infrastructure provided to it, based on trajectories generated by the category specific configuration of the full EnIGMA agent using Claude 3.5 Sonnet."}, {"title": "H.1 Analysis of rap challenge", "content": "The reverse-engineering \"rap\" challenge from CSAW-Finals 2020 competition, contains a binary named \"rap\" that uses the assembly wrapping technique\u00b3 to disguise a simple xor-based flag compression.\nThe agent successfully solves the challenge using the following interactions:\n1. Locate and examine challenge files: The agent began by following the demonstration provided, locating the binary file provided with this reverse-engineering challenge and running it to see the output.\n2. Static analysis of the binary provided: The agent continues with decompilation of several functions of the given binary, to understand the logic and functionality. This is followed by getting a hexadecimal dump of the binary to get the data sections relevant to solve the challenge.\n3. Create a script for solving the challenge: The agent now creates a Python script to solve the challenge using the information it gathered from the previous step, to uncover the flag.\n4. Verification and submission: The agent verifies that the extracted flag is the correct one first by feeding it to the binary to see if the output changes, and then by submitting it as the solution for this challenge."}, {"title": "H.2 Analysis of jackpot challenge", "content": "The cryptography \"jackpot\" challenge from CSAW-Finals 2020 competition, presents a lottery machine, jackpot, that we need to hack in order to get the prize. The challenge contains the binary of the lottery machine as well as a remote server containing the machine. The machine uses a random number generator that uses Dual_EC_Drbg implementation, an algorithm that was presented as a cryptographically secure pseudorandom number generator using methods in elliptic curve cryptography, but was later withdrawn as a standard due to a potential backdoor in it.\nThe agent does not solve the challenge, but performs the following interactions:\n1. Locate and examine challenge files: The agent began by locating the file provided with this cryptography challenge and running it to see the output.\n2. Interaction with the challenge server: Following the provided challenge statement, the agent tries to connect and interact with the challenge server to understand about the challenge.\n3. Static analysis of the binary provided: The agent continues with decompilation of a function of the given binary, to understand the logic and functionality.\n4. Get more data from the server: The agent determines based on the previous steps that the binary is using elliptic curve cryptography, specifically P-256 curve. It then tries to interact with the server to get pairs of public key and a point on the curve, in order to understand some vulnerability in the implementation.\n5. Create a script for solving the challenge: The agent now creates a Python script to solve the challenge using the information it gathered from the previous steps, to uncover the flag.\n6. Script execution: Finally the agent tries to run the script, it first successfully installs missing package, but as it continues to run the solve script, the soliloquies effect dramatically affect the agent from fixing the problem when running its code, resulting in a cost limit exit."}, {"title": "9.  YOU CAN SOLVE THE CHALLENGES BY YOURSELF COMPLETELY I TRUST IN YOU!", "content": "DEBUG SESSION TIPS:"}, {"title": "10. Next step template.", "content": "Figure 12: The instance template. This prompt shows CTF challenge statement containing the challenge name, description, optional files related to the challenge and optional remote server required for solving the challenge. It also shows additional task instance-specific information, and provides a set of tips suggesting recommended problem solving approaches and pitfalls to look out for. The debugging tips are showed only for the rev and pwn categories.\nNext step template. The next step templates shows the agent the feedback from the environment for the action the agent generates at each iteration. This template also contains environment information such as the current directory, open file, and if any interactive session is currently running using one of the two IATs for debugging and server connection as shown in Figure 13"}]}