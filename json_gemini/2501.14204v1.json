{"title": "Dynamic Token Reduction during Generation for Vision Language Models", "authors": ["Xiaoyu Liang", "Chaofeng Guan", "Jiaying Lu", "Huiyao Chen", "Huan Wang", "Haoji Hu"], "abstract": "Vision-Language Models (VLMs) have achieved notable success in multimodal tasks but face practical limitations due to the quadratic complexity of decoder attention mechanisms and autoregressive generation. Existing methods like FASTV [1] and VTW [2] have achieved notable results in reducing redundant visual tokens, but these approaches focus on pruning tokens in a single forward pass without systematically analyzing the redundancy of visual tokens throughout the entire generation process. In this paper, we introduce a dynamic pruning strategy tailored for VLMs, named Dynamic Rate (DyRate), which progressively adjusts the compression rate during generation. Our analysis of the distribution of attention reveals that the importance of visual tokens decreases throughout the generation process, inspiring us to adopt a more aggressive compression rate. By integrating a lightweight predictor based on attention distribution, our approach enables flexible adjustment of pruning rates based on the attention distribution. Our experimental results demonstrate that our method not only reduces computational demands but also maintains the quality of responses.", "sections": [{"title": "I. INTRODUCTION", "content": "Vision-Language Models (VLMs) integrate visual and textual information to generate coherent and contextually relevant outputs, demonstrating impressive capabilities across various tasks in artificial intelligence [3]-[5]. However, the autoregressive generation nature of VLMs, along with the quadratic complexity of attention [6], significantly increases computational complexity when the input token sequence lengthens, limiting the applications of VLMs.\nMany recent methods attempt to reduce the computational cost by eliminating redundant visual tokens. For example, FastV [1] analyzes the attention mechanism and discovers that visual tokens receive less attention after the second decoder layers, thus deleting redundant visual tokens according to a predefined compression rate. Building on this observation, VTW [2] directly removes all visual tokens in deeper layers, only text tokens allowed to participate in further processing. Although these methods demonstrate effectiveness, they still present two drawbacks. First, the compression rate must be manually specified. Manually selecting an appropriate compression rate is nontrivial and typically requires expert-level domain knowledge. Second, maintaining a fixed compression rate during the generation process is suboptimal. Our experimental analyses of the attention distribution during generation (see Sec. III-A) reveal that visual tokens receive varying proportions of attention during the next token prediction. Information in VLMs often becomes less concentrated in visual tokens during the later iteration of generation. As the generation process advances and the length of the response gradually increases, the importance of the visual information changes accordingly. Thus, it is imperative to have adaptive compression rates during generation. Yet, determining proper compression rates is challenging. Our work is here to address the challenge.\nWe present Dynamic Rate (DyRate), the first method to connect the token reduction rate with the attention weights to achieve adaptive dynamic reduction during the VLM generation process. Our approach is designed to be differentiable, enabling end-to-end training. Specifically, we introduce a lightweight classifier to predict the reduction rate. During each iteration of model generation, we collect the attention distributions of four types of tokens in each head, which are then fed into the predictor to determine the optimal reduction rate for cropping the image embeddings. The overview of our approach is shown in Fig. 1. Our contributions are summarized as follows:\n* we propose DyRate, the first approach that can adaptively"}, {"title": "II. RELATED WORK", "content": "The computational demands of VLMs increase significantly with long token sequences due to the quadratic complexity of attention mechanisms. This limitation restricts their application in tasks such as high-resolution image understanding [7] and video understanding [8]. To address this issue, prior research has proposed several strategies. For instance, MoE-Llava [9] integrates a Mixture of Experts framework to accelerate the model, while VL-Mamba [10] explore alternative architectures to enhance efficiency. Another approach involves using smaller language models, such as LLaVA-Phi [11] and mobileVLM [12], which demonstrate efficiency with minimal performance loss. Additionally, compression techniques like pruning [13], quantization [14], [15], and knowledge distillation [16] are widely used to reduce the number of parameters in models.\nHowever, these methods often require modifications to the model architecture or parameters, complicating further development. In contrast, token reduction minimizes token sequences without altering the model architecture, addressing the quadratic complexity of VLMs. Token reduction has been extensively explored and validated within encoder architectures like ViT and BERT, primarily through pruning [17]\u2013[19] and merging [20]\u2013[22] strategies."}, {"title": "B. Token Reduction for VLMS", "content": "Recent researchers have applied token reduction methods to VLMs, categorizing them based on the architecture of VLMs. For the visual encoder, methods like LLaVA-PruMerge [23] and MADTP [24] introduce adaptive methods to reduce visual tokens, significantly decreasing their number while maintaining comparable performance to the original models. In the cross-modality projector, Tokenpacker [25] optimizes the bridge between textual and visual information to minimize the number of visual tokens. FastV [1], Sparsevlm [26] and Visionzip [27] removes redundant visual tokens during the inference phase to reduce computational demands without impacting performance. VTW [2] observes that visual tokens are not significant in deeper layers of the VLM and strategically removes all visual tokens from specific layers, allowing only text tokens to proceed in subsequent processing.\nAlthough existing token reduction methods improve the efficiency of inference, they often overlook the autoregressive nature of VLM decoders. Our work addresses this gap by dynamically adjusting $R$ based on the changing attention distribution during the generation process."}, {"title": "III. METHODS", "content": "In Sec. III-A, we explore how attention distribution changes during the generation within VLMs. Our analysis reveals that the importance of visual tokens gradually decreases as the VLM progresses. This observation leads us to question whether the fixed compression rate $R$ set manually in previous works [1], [2] is the optimal solution, considering the changing attention distribution.\nBased on this insight, we propose DyRate as a solution to adjust the compression rate $R$. The overall process is illustrated in Fig. 3."}, {"title": "A. Visual Redundancy during Generation", "content": "We observed that visual token redundancy increases with generation steps. In Fig. 2, as the model generates more tokens, it emphasizes response tokens and overlooks visual tokens, indicating increasing redundancy.\nAccording to information flow theory [29], a large amount of information from image tokens is aggregated into respones tokens during generation. The aggregation of this information results in additional redundancy among visual tokens, where a large number of tokens provide minimal information, leading to a waste of computational resources.\nGiven the above, a fixed $R$ fails to adapt to the changing attention distribution during the generation process, highlighting the need to determine an adaptive $R$."}, {"title": "B. Token Reduction with Prediction Module", "content": "To address this issue, we first intuitively hypothesize that the redundancy of visual tokens is closely related to the attention distribution across four different token types. At each time step of the model generation, we compute the attention distribution for each attention head. Based on these distribution characteristics, we further train a linear classifier aimed at identifying and determining the optimal token pruning rate $R$. The entire process is depicted in Figure 3.\nDuring training, we use the mask $M$ to selectively prune tokens, while during inference, we discard tokens directly to save computational resources. It's worth to note that the FLOPs required by the predictor is minimal when compared to the computational savings gained from discarding these tokens."}, {"title": "C. Differentiable Compression Rate", "content": "To effectively address the non-differentiability issues associated with sampling and masking operations, we utilize the Gumbel-Softmax trick [17], [20], [30] to convert the probability distribution of the compression rate $R$ predicted by the predictor $C$ into a differentiable mask probability distribution, which is then integrated into the forward propagation of the model.\n1) Probability of Pruning Rate: We define the adjustment of the compression rate $R$ as a classification problem. and discretize the compression rate $R$ into $K$ discrete values:\n$R = {r_k}_{k=1}^{K}$,\nwhere $r_k = \\frac{k-1}{N}$ represents removing $(k-1) \\cdot N$ less important tokens in a token sequence of length $N$. Note that $r_1 = 0$ indicates retaining all tokens.\nWe feed the vector $v_t$ to classifier $C$ to predict the probability distribution over these rates:\n$\\pi_R = Softmax(C(v_t)) = {P(r_k)}_{k=1}^{K}$,\nwhere the probabilities sum to one and are differentiable. Specifically, we sort the tokens based on the 'attenion-score' rule utilized in FastV [1]. After that, we apply masks at different pruning rates:\n$m_i^k = \\begin{cases} 1 & \\text{if } i < (k-1), \\\\ 0 & \\text{otherwise}, \\end{cases}$\nwhere 0 indicates an unimportant token to be discarded, and 1 indicates a token to be retained. For each potential $R_i$, we generate a unique corresponding mask $M_i$, ensuring that the least important tokens are discarded first.\n2) Gumbel-Softmax Sampling: During the forward pass, Gumbel-Softmax generates a one-hot vector with the same expectation as $\\pi_R$:\n$\\hat{R} = Gumbel-Softmax(\\pi_R) \\in {0,1}^K$,\nwhere their parameter gradients can be easily computed with standard backpropagation. During the backward pass, the straight-through Gumbel-Softmax estimator is employed to approximate the gradient.\nWe can then select current mask $M$ by sampling from $\\pi$:\n$m = \\sum_{k=1}^{K} Gumbel-Softmax(\\hat{R})*,k \\cdot m^*_k$,\nwhere $m_k \\in {0,1}^N$ is the mask associated with the discrete compression rate $r_k$.\nThis generates the final mask $M \\in {0,1}^{N \\times N}$, which, in conjunction with the causal mask, is applied to the attention computation to achieve token pruning.\n$M_{i,j} = \\begin{cases} 1 & \\text{if } i=j, \\\\ M_i & \\text{otherwise}. \\end{cases}$\nThe process ensures that the selection operation remains differentiable with respect to $R$. And we keep the proof and pseudocode for our DyRate algorithm in the Appendix."}, {"title": "IV. EXPERIMENTS", "content": "We utilized LLaVA-1.5-7B and LLaVA-1.5-13B models, where visual inputs were represented using 576 tokens. To evaluate the general performance of our proposed method,"}, {"title": "B. Main Results", "content": "1) Short Responses: Our model demonstrated exceptional performance on short-response datasets, showcasing superior question comprehension and answer generation capabilities. It performed particularly well on the Flickr30K and GQA datasets and handled specific query types well on the SQA[I] and VisWiz datasets. Notably, performance remained stable across different datasets, even under high compression rates, although a minor performance drop was observed during detailed visual analysis tasks.\n2) Long Responses: With long-response datasets, our model's strategy of gradually adjusting the pruning ratio during autoregressive generation effectively reduced redundancy. Despite reducing visual tokens by half, the model maintained high performance across multiple datasets. This strategy proved especially beneficial for generating long responses and managing high redundancy data, all while requiring less computational resources."}, {"title": "C. Efficiency", "content": "We are also concerned with the performance of our method in scenarios involving long responses, as such situations frequently arise in real-life applications. Therefore, we selected the Flickr30K, COCO2017 and Nocaps dataset for evaluation. These datasets feature response token lengths ranging from 16 to 64 tokens, providing a comprehensive basis for evaluating the comprehension abilities of VLMs and the effects of post-token pruning. For a fair comparison, we evaluated the performance of two highly relevant methods, FastV and VTW under scenarios with FLOPs savings ratio of 30% and 50%. The experimental results demonstrate that our method, Diffrate, effectively meets FLOPs resource constraints by automatically optimizing the parameter $R$, eliminating the need for manual searching. This adaptive approach to $R$ proves to be more effective. For ease of comparison with FastV, we selected the same number of layers."}, {"title": "D. Ablation", "content": "1) Compression Rate Strategies: Our ablation studies explored the effects of different pruning strategies on Visual Language Models (VLM) performance. We compared FixedPrune(FP), a strategy with a static pruning ratio, and DepthBasedPrune(DP), a strategy with a dynamically adjusted pruning ratio based on layer depth. These strategies were evaluated on the NoCaps dataset, using $K = 3$ layers for pruning, in line with FastV for a valid comparison. Our innovative strategy for determining R was as follows:\n$C_{retain} = 1 - H(L_{index} - 4) \\cdot P_{prune\\_4th} - H(L_{index} - 4) \\cdot R'$\nHere, $C_{retain}$ denoted the proportion of visual tokens kept in the current layer, $L_{index}$ represented the current layer index, $P_{prune\\_4th}$ was the fourth layer's pruning ratio, and $R'$ was a modified pruning ratio adapting to the layer index.\nThe results showed that our method efficiently met Floating Point Operations Per Second (FLOPs) resource constraints without manual search, indicating the effectiveness of the adaptive $R$.\n2) Decoding Parameters: We also investigated how decoding strategies impacted VLM generation. Adjusting a single decoding parameter in LLaVA-1.5 while fixing others revealed the significant effect of decoding parameters on the quality of generated text. Particularly, setting top_k to 6 resulted in the best performance, with a CIDEr score of '109.57'. To maintain fairness, we used LLaVA-1.5 with the default greedy search for replication, thereby reducing the influence of other parameters."}, {"title": "E. Visualization", "content": "To comprehensively elucidate the functioning of our proposed method, we visualized the alterations in image masking throughout the generation process in Figure 4. We employed a diverse collection of original images and introduced them to the LLaVA-1.5-7B model with the prompt: 'Provide a one-sentence caption for the provided image.' As the generation iterations escalate, we document an associated increase in the cropping ratio. This trend suggests that the model incrementally diminishes its attention on redundant information within the image during the generation, honing in on the extraction and processing of pivotal information. Additionally, by contrasting the generation trajectories of different images, we discern that our method exhibits flexibility in adapting to images of assorted types and complexities, reinforcing the efficacy and robustness of our proposed approach."}, {"title": "V. CONCLUSION", "content": "This paper introduces a novel method, Dynamic Rate (DyRate), to dynamically determine compression rate $R$ for VLMs during the generation process. Our approach leverages a lightweight predictor that utilizes the attention distribution across different token types to identify the most effective compression rate, thereby addressing the computational inefficiencies associated with fixed compression rates. We employ Gumbel-Softmax to overcome the non-differentiability challenges associated with traditional pruning methods. The empirical validation across multiple benchmarks underscores the effectiveness of our method in maintaining accuracy while reducing computational demands. This study not only enhances the efficiency of VLMs, but also paves the way for more adaptable and resource-aware applications in complex multimodal environments."}]}