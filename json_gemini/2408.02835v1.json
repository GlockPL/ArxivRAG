{"title": "Training a multilayer dynamical spintronic network with standard machine learning tools to perform time series classification", "authors": ["Erwan Plouet", "D\u00e9dalo Sanz-Hern\u00e1ndez", "Aymeric Vecchiola", "Julie Grollier", "Frank Mizrahi"], "abstract": "The ability to process time-series at low energy cost is critical for many applications. Recurrent neural network, which can perform such tasks, are computationally expensive when implementing in software on conventional computers. Here we propose to implement a recurrent neural network in hardware using spintronic oscillators as dynamical neurons. Using numerical simulations, we build a multi-layer network and demonstrate that we can use backpropagation through time (BPTT) and standard machine learning tools to train this network. Leveraging the transient dynamics of the spintronic oscillators, we solve the sequential digits classification task with 89.83\u00b12.91% accuracy, as good as the equivalent software network. We devise guidelines on how to choose the time constant of the oscillators as well as hyper-parameters of the network to adapt to different input time scales.", "sections": [{"title": "I. INTRODUCTION", "content": "The ability to process time-series (classification, pre-diction, generation etc.) is important for many appli-cations from smart sensors in industrial maintenance to personal assistants and medical devices. Using the dy-namics of a physical system, leveraging its non-linearity and memory for such processing has been widely explored with the development of recurrent neural networks, both from a purely mathematical perspective [1-4] as well as from a brain-inspired perspective with spiking recurrent neural networks [5-7]. Chen et al. have shown that neural networks based on ordinary differential equations (Neural-ODEs) can be seen as residual neural networks where the time dimension acts as depth thus providing computing power [8]. These findings highlight the poten-tial of dynamical systems to implement deep neural net-works. While implementing a recurrent neural network in software on a conventional computer requires comput-ing the evolution of each neuron step by step, a physical network would naturally perform this computation. As there is an increasing demand for learning and processing on the edge, with strong footprint and energy cost con-straints, building novel hardware that directly implement dynamic recurrent neural networks is an attractive path. This motivation has led to the realisation and training of recurrent neural networks with a wide variety of dy-namic systems: analogue CMOS [9], photonic systems [10], acoustic resonators [11], mechanical oscillators [12] and wave systems [13] to cite a few.\nSpintronic oscillators are promising building blocks for the hardware implementation of neural networks, due to their non-linear high-speed dynamics as well as poten-tial for miniaturization and CMOS integration [14, 15]. Time-series processing using the dynamics of spintronic oscillators has been experimentally demonstrated in the context of reservoir computing [16-20], where there is no training of the dynamics, or in single layer networks [21, 22]. Ross et al. have experimentally demonstrated a multilayer network of spintronic oscillators, but with a feedforward architecture dedicated to static tasks [23]. Rodrigues et al. have shown by numerical simulations how to train the transient dynamics of a single layer net-work of oscillators with optimal control theory, on a static task [24].\nHere we simulate and train a multi-layer network of spintronic oscillators as neurons, using standard machine learning tools. We leverage the transient dynamics of the oscillators to perform time-series classification of the se-quential digits dataset. First, we describe the dynamic spintronic neuron model, the architecture of the network and how to cascade neuron layers. Second, we describe how to train the network with PyTorch and backprop-agation through time and demonstrate 89.83 \u00b1 2.91% accuracy on sequential digits, as good as the equivalent software network. Third, we show that the spintronic network can be trained on input timescales over a 5-fold range, centered around a value depending on device parameters. Finally, we derive guidelines to find these device parameters. In particular, the neuron relaxation time must be larger than the input time scale and the cumulative drive must be around one."}, {"title": "II. MULTILAYER NETWORK OF SPINTRONIC DYNAMICAL NEURONS", "content": "Spintronic oscillators exhibit magnetization self-sustained oscillations when a dc current drive overcomes the magnetic damping. Magnetoresistive effects trans-late the magnetization oscillations into voltage oscilla-tions that can be fed as input to other devices. In this work, we use the amplitude of these oscillations as inter-nal variable of the spintronic neuron. When submitted to a drive, the oscillation amplitude undergoes a transient regime which provides memory to the device. We use the auto-oscillator model [25], verified experimentally, to describe the dynamics of the dimensionless normalized amplitude x:\n$\\frac{dx}{dt} = \u22122(\u03b3(1 + Qx) \u2013 \u03c3I(t)(1 \u2212 x))x$ (1)\nWhere \u03b3 = \u03b1xw is a damping term with alpha the magnetic Gilbert damping and w the frequency of the device.Q is the non-linear damping, which we set to zero. \u03c3is a material parameter which we set to one. In this work, we simplify the magnetization dynamics into:\n$\\frac{dx}{dt} = -\u03b3x + I(t)x(1 - x)$ (2)\nI(t) is the time dependent drive, a dimensionless vari-able proportional to the dc current injected into the de-vice. The damping \u03b3 pulls the output to zero and the x(x - 1) terms acts as natural bounding of the output between 0 and 1. The oscillator exhibits non-linearity as well as memory, making it suitable for processing time series.\nWe consider a network, depicted in Figure 1, composed of successive layers of dynamical neurons, represented by blue circles. The neurons are connected by inter-layer connections (Wext, in purple) and intra-layer connections (Wint, in green). In the scope of this paper, we consider standard linear connections, performing weighted sums of the RF power outputs of the neurons.\nIn order to exploit the whole dynamical range of the neurons, we seek to keep their output above zero (no output) and below one (saturation). We apply a fixed bias on top the trainable biases to each neuron. The value of the fixed bias is twice the damping bfixed = 2\u03b3. This set the zero-drive output of the neuron at half the max power. Furthermore, we use a high-pass filter as well as a tunable amplification factor set to S = 0.5 between each layer to keep the drives centered around zero.\nThe dynamics of the power $x^{n+1}_{i}$ of neuron i from layer n + 1 is given by:\n$\\frac{dx^{n+1}_{i}}{dt} = -\u03b3x^{n+1}_{i} + I(t)x^{n+1}_{i}(1 - x^{n+1}_{i})$ (3)\nWith the drive:\n$I(t) = S \u00d7 (W^{next}_{i}y^{n}_{i}+ W^{n+1,int}_{i} y^{n}_{i} +b^{n+1}_{i}) +b_{fixed}$ (4)\nWhere $y^{n}_{i}$ is the output of neuron i from layer n after application of the high pass filter and follows:\n$\\frac{dy^{n}_{i}}{dt} = -2 f_{cut} \u00d7 y^{n}_{i} + \\frac{dx^{n}_{i}}{dt}$ (5)\nFigures 1(b-c-d) illustrate the dynamics of the neurons for layers 0, 1 and 2 respectively. Each color is one time-varying input applied to the neuron. We observe varied dynamics around 0.5 (i.e. half of the maximum ampli-tude of the oscillations). Each layer acts an integrator, slowing down information as it flows into the network."}, {"title": "III. PROCESSING OF A TIME SERIES TASK", "content": "The time-series classification task we chose to evalu-ate the spintronic network is sequential digits [26]. The dataset is composed of 1797 grayscale 8-by-8-pixel im-ages of handwritten digits and the goal is to identify the digit (labels 0 to 9). We split the dataset 50/50 between train and test. The images are presented pixel by pixel to the network, as time series of 64 input points each, as depicted in Figure 2(a). Here the time interval between input points is 1 ns. In consequence, the input of the net-work is of size 1 and the output of size 10. The network is simulated using the PyTorch library as well as the differ-ential equation (2). After the final time step of the series, we compute the log-likelihood loss of the output. We re-set the internal state of the neurons after processing each image as there is no time-correlation between successive images. We update the trainable parameters (weights and biases) using backpropagation through time (BPTT) and batches of 120 images [27-29]. We clip the amplitude of each gradient element to 1 to prevent gradient explo-sion. We set the connectivity density at 0.5 both for the interlayer and intralayer connections (dinter = 0.5 and dintra = 0.5). The effect of connection density is stud-ied in Section IV. We use a hyperparameter optimization using the library Optuna [30], in order to maximize the accuracy, and obtain a relaxation time of \u03c4 = 14.12 ns for the neurons. We use a learning rate decay method $Ir = \\frac{lro}{\\frac{nepoch}{Irdecay} +1}$ with lro = 0.149 and Irdecay = 8.077, as well as the Adam optimizer [31]. We achieve an accuracy of 89.83 \u00b1 2.91 %.\nWe benchmark this result by comparing the simulated spintronic network to a standard software-based standard continuous-time recurrent neural network (CTRNN) [32]. We consider a CTRNN with the same architecture (3 layers of 32 neurons, inter and intra-layer connections with density of 0.5 each).\nThe hidden state $x^{n+1}_{i}$ of the i-th neuron of layer n+1 is driven by:\n$\\frac{dx^{n+1}_{i}}{dt} = -\u03b3x^{n+1}_{i} + I(t)$ (6)\nWith the drive:\n$I(t) = S \u00d7 (W^{next}_{i}y^{n}_{i}+ W^{n+1,int}_{i} y^{n}_{i} +b^{n+1}_{i})$ (7)\nWhere $y^{n}_{i}$ is the output of the j-th neuron of layer n with application of a non-linear activation function:\n$y^{n}_{i} = tanh(x^{n}_{i})$ (8)\nWe see here that the spintronic network is a natural implementation of a CTRNN by a physical system. The main difference is that the internal state of the spintronic neuron is intrinsically bounded, while it is not for the software neurons, which rely on their activation function to bound their outputs. This constraint is common to all physical implementations of neurons. We observe that the CTRNN achieves a top accuracy of 89.00 \u00b1 3.48%. The uncertainty corresponds to the standard deviation over ten runs, each with a different random initialisa-tion of the trainable parameters. We conclude that the spintronic network performs as well as a standard CTRNN, despite its strong constraint of having neurons with bound internal state."}, {"title": "IV. ADAPTATION OF THE NETWORK TO DIFFERENT TIME SCALES", "content": "When processing time series, a critical feature is the time scale of the input. Because the memory of the net-work comes from the dynamics of the physical system, the system needs to be designed to match the time scales relevant to the task. We derive design rules by studying how the network behaves at different input time scales.\nWe set the hyper-parameters of the network to the val-ues found by optimization for 1 ns between input points, then train the network on series with different input time scales. Figure 2(b) shows how the accuracy depends on the time interval between two input points. We observe that the maximum accuracy is for 1 ns, which is expected as this is the optimization point. This maximal accuracy is maintained over a wide window of time scales, about a 5-fold factor, from 0.4 to 2.5 ns. When the time scale is well below or well above 1 ns, the accuracy is significantly degraded. Here again, the spintronic network performs as well as the CTRNN. Panels (c-d-e) of Figure 2 show the evolution of a neuron from the third layer for differ-ent inputs (colors), in three regimes. In panel (d), the dynamics of the network is optimized for the input time scale. We observe that the different inputs produce rich varied dynamics, enabling separation of the inputs into classes. In panel (c), the input is faster and the neuron is too slow to follow its changes. We observe that the dynamics of the input is completely lost in the smoothed trajectories of the neuron, preventing accurate classifica-tion. In panel (e), the input is slower and the neuron is too fast. The memory of the network is too short to remember enough input points to achieve accurate clas-sification.\nBuilding on these results, we devise guidelines to de-sign networks adapted to learn a task with a given time scale. The dynamics of the spintronic neuron relies on a competition between the current drive and the damping. The damping pulls the output amplitude to zero while the current drive pushes the neuron away from this rest position.\nFigure 3(a) shows the accuracy versus the time be-tween two input points, for three different relaxation times. We observe that the optimal time scale shifts with the relaxation time. The dependence of the accu-racy is always a bell-shaped curve, centered around the optimal time. In particular, slower inputs require longer relaxation times so that the network has a long enough memory to remember the input points necessary for clas-sification.\nFigure 3b shows the accuracy versus the time between two input points for three different learning rates. We observe that here as well, learning requires a matching between learning rate and input time scale. Further-more, we observe that faster inputs require higher learn-ing rates. Indeed, the neuron receives a cumulative drive I \u00d7 \u0394t for each input point. The strength of this drive depends on the weights and the scaling factors:\n$I = S \u00d7 (W) \u00d7 \\sqrt{N_{neurons}}$ (9)\nWhere here $N_{neurons}$ = dinter \u00d7 32+dintra \u00d7 32 = 32 is the fan-in for each neuron and (W) is the mean of the absolute value of the weight elements. When the input is fast (low \u0394t), the network needs to reach higher weights to provide a cumulative drive high enough to counter-balance the damping. Higher learning rates allow the weights to reach higher values.\nWe identify two conditions. First, the relaxation time of the neurons must be larger than the time between two input points \u0394t < \u03c4. Second, the cumulative drive divided by the relaxation time must be roughly equal to the damping:\n$\\frac{I \u00d7 \u0394t}{\u03c4} \u2248 \u03b3$ (10)\nAs the damping is the inverse of the relaxation time, this simplifies as:\n$I x \u0394t \u2248 1$ (11)\nFigure 3c shows the accuracy versus the cumulative drive for a broad range of conditions. We observe that the best accuracies are indeed obtained when the cumulative drive is around 1, provided that the input scale is larger than the relaxation time (green dots). Furthermore, we recommend to set the learning rate in the range of one hundredth of the optimal weights values $\\frac{1}{\u0394txSxN_{neurons}}$.\nWe observe that the design rule regarding the cumu-lative drive does not require fine tuning. In Figure 2b as well as Figure 3, we observe that the accuracy is not significantly degraded when the input time scale varies within a wide window, typically 5-fold centered around the optimal value, which is promising for practical real-izations."}, {"title": "V. ARCHITECTURE DESIGN", "content": "We evaluate the impact of the connection density. Figure 4 shows the accuracy of the task versus the density of connections. Here the network is randomly sparsified before training. We observe that the intra-layer connec-tivity (red curve) can be reduced to 10% without los-ing accuracy. When removing the intra-layer connec-tions altogether, the accuracy is degraded but remains high. The inter-layer connectivity (green curve) can be reduced to 20% without losing accuracy. We also observe that the performance exhibits higher run-to-run variabil-ity at higher densities of connection. This is due to the fact that higher connectivity increases the risk of neu-ron saturation and thus failure of learning. The global connectivity (both inter-layer and intral-ayer) is optimal around 50%.\nTo build such a network, we need physical implemen-tations of the synaptic layer. These synapses need to re-spond fast enough to preserve the neuron dynamics. One possibility is to use spintronic resonators as described in [23]. In the network described here, there are 96 neu-rons and 2896 synapses. If the image is presented for about 100 ns, according to the estimations in [23](Ross et al), each neuron and each synapse would consume 100 fJ and 10 fJ respectively, leading to a total consumption of about 40 pJ. Here the training was performed with backpropagation through time, supposing off-line train-ing or chip-in-the-loop training [33], which is satisfactory for a wide range of applications. Online training lever-aging the physics of the network would make it possible for the network to keep learning new tasks during its use, but is out of the scope of this paper."}, {"title": "VI. CONCLUSION", "content": "We have simulated a multi-layer neural network where the neurons are spintronic oscillators. Leveraging the transient dynamics of the neurons for memory and non-linearity, we have performed a time series classification task, sequential digits. We demonstrate that this multi-layer dynamical spintronic network can be trained with backpropagation through time (BPTT) and standard machine learning tools, which is key for scaling to more complex tasks. Despite having a bounded internal vari-able, the spintronic network achieves 89.83% accuracy, as good as a standard recurrent neural network. These results demonstrate the potential of spintronic oscillators for time-series processing. We have devised guidelines for the design of such networks and their training: the relax-ation time must be longer than the time-scale of the in-put and the cumulative drive must be close to one. Once built, a dynamical spintronic network can be trained to learn tasks over a 5-fold range of input time-scales. We have shown that we can sparsify the network down to 50% connectivity before training, further reducing the footprint and energy consumption of the system. For the sequential digits tasks and the considered architecture, we estimate that the network would consume about 40 pJ per image. These results open the path to the scaling up of spintronic neural networks for time-series processing at low energy cost."}]}