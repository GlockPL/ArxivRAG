{"title": "NEUTRAL RESIDUES: REVISITING ADAPTERS FOR MODEL EXTENSION", "authors": ["Franck Signe Talla", "Herv\u00e9 J\u00e9gou", "\u00c9douard Grave"], "abstract": "We address the problem of extending a pretrained large language model to a new domain that was not seen at training time, like adding a language for which the original model has seen no or little training data. Popular solutions like fine-tuning or low-rank adaptation are successful at domain adaptation, but formally they do not add any extra capacity and degrade the performance in the original domain. Our paper analyzes this extension problem under three angles: data, architecture and training procedure, which are advantageously considered jointly. In particular, we improve adapters and make it possible to learn an entire new language while ensuring that the output of the neural network is almost unchanged in the original domain. For this purpose, we modify the new residual blocks in a way that leads each new residual block to output near-zeros in the original domain. This solution of neutral residues, which borrows architectural components from mixture of experts, is effective: with only 20% extra learnable weights compared to an original model trained on English, we get results that are significantly better than concurrent approaches (fine-tuning, low-rank or vanilla adapters) in terms of the trade-off between learning a new language and not forgetting English.", "sections": [{"title": "INTRODUCTION", "content": "The dominating strategy for producing foundation models involves training from scratch on a large collection of data, typically trillions of tokens, covering multiple domains. Training such models is extremely costly in terms of computing resources. In view of the explosion of the number of gigantic models produced in the last years (the HuggingFace model repository claims to host about one million models), this training paradigm raises the problem of the economical and ecological sustainability of the model production pipelines. As an example, training the Llama 3 model is estimated by multiple sources to cost hundreds of millions dollars, mostly due to the training on 24,000 flagship GPUs H100 for months\u00b9, not counting the hidden costs of exploration.\nIn this paper, we consider the question of extending an existing model, in order to add some new capabilities or knowledge without retraining it from scratch and therefore in a much more resource-"}, {"title": "RELATED WORK", "content": "The success of deep learning is often associated to the outstanding performance obtained when training convolutional neural networks on large datasets such as Imagenet (Deng et al., 2009). Yet another key benefit is their amenability to transfer knowledge between tasks with a limited amount of training data (Oquab et al., 2014). This transfer is usually performed with a so-called finetuning stage, where the original backbone is trained in a data-efficient manner to solve a distinct target task than the one employed at training time. It often requires to modify the architecture to adapt the output layers to the new tasks. Several self-supervised pretraining methods are noticeably interested in improving ways of pretraining models on proxy tasks (Devlin et al., 2018; Caron et al., 2021), such that networks finetuned from these backbones, or used in a 0-shot manner, perform well on multiple tasks. In most of these settings, the backbone only requires some form of domain adaptation.\nFinetuning is however not sufficient when a large body of knowledge must be incorporated to the network. In this section we review a body of literature related to this problem and to our work. This includes solutions that add new knowledge into a pretrained network, as well as methods that incorporate gating mechanisms such as mixtures of experts (MoE).\nEnhancing language model capabilities using module based training has gained interest in recent years due to the high cost of full finetuning. Those methods are known as parameter efficient finetuning methods (PEFT; Houlsby et al. (2019); Hu et al. (2022); Li & Liang (2021)). Unlike full finetuning, they only require a limited amount of memory. Houlsby et al. (2019) proposed to insert small bottleneck layers, known as Adapters, within each transformer layer, allowing models to be finetuned by training only a small fraction of the parameters, while keeping the original model frozen. There is a large body a literature on adapters, see the overview by Pfeiffer et al. (2023). Low-Rank Adaptation (LoRA) by Hu et al. (2022) adds trainable low-rank matrices to transformer layers while keeping the original weights frozen.\nContinual Learning is a widely studied concept (De Lange et al., 2021; Wang et al., 2024) as it allows the addition of new knowledge to LLM after their initial pretraining. It is usually done through full finetuning in order to learn domain specific knowledge (Roziere et al., 2023), to enhance instruction-following abilities (Wang et al., 2023) or to align LLM with human preferences (Ouyang et al., 2022). One of the biggest challenges of continual learning and the main drawback of doing it through full finetuning is catastrophic forgetting (Kirkpatrick et al., 2017). This is partially alleviated by using the initial training data (Robins, 1995), when available, or as a proxy data from a similar distribution.\nSeveral studies have investigated more advanced methods to address forgetting in continual learning (Biderman et al. (2024); Wu et al. (2024); Li et al. (2024); Zhong et al. (2024); Riemer et al. (2019); Li & Hoiem (2017)). Biderman et al. (2024) show that using LoRA helps reducing forgetting but is less efficient in learning than full finetuning especially for distributions far from the LLM pretraining distribution. Wu et al. (2024) proposed a continual learning method for LLMs, which amounts to adding new intermediate transformer blocks in an interleaved manner. This enables the injection of new knowledge while preserving the initial capabilities.\nMoEs have a long history in neural networks (Yuksel et al., 2012), for instance with set of classifiers where each individual classifier was specialized in a different task or aspects of data. Residual architectures (He et al., 2016) and in particular transformers (Vaswani et al., 2017) have renewed the interest in this strategy. In this context, they amount to replacing the standard feed-forward networks (FFN) of the transformer blocks by a collection of FFNs, referred to as experts. A gating mechanism selectively activates a subset of relevant experts for a given input. This technique has been widely used to reduce the computational cost of inference in large language models (Jiang et al., 2024; Xue et al., 2024) with a focus on the design of an expert-balancing loss to promote equitable token distribution across experts (Shazeer et al., 2017; Pfeiffer et al., 2023).\nRecent works have explored utilizing MoE's gating mechanism to mitigate catastrophic forgetting during the continual learning phase of language models. LorRAMOE (Dou et al., 2024) employs LoRAs as experts, integrating them with a gating mechanism to improve performance on specific downstream tasks through Supervised finetuning (SFT). LoRAMOE mitigates catastrophic forgetting in the case of SFT, but to our knowledge its effectiveness has not been studied in a broader context. In contrast, two recent parallel concurrent works are closer to our proposal: Zhong et al. (2024) augment Mixture-of-Experts LLMs with new modalities by addition of new experts to a pretrained mixture of experts. Similarly, Li et al. (2024) add an MoE layer in parallel to the FFN of the transformer block to learn multilingual capabilities and mitigate catastrophic forgetting during Continual Learning. Our solution shares some similarities with this work by employing mixed data training. Yet Li et al. (2024) do not integrate an explicit sparsity criterion, and need to weight differently the blocks of the pretrained model with those of the gated adapters. They also consider multiple elements in the mixture and apply a softmax for the rooting mechanism. Since at least one expert is activated in their mixture, this selection mechanism also prevents the capacity of the model to produce 0-valued outputs. Our solution of neutral residues with local loss is more effective than simply gating adapters, as shown in our experimental Section 4."}, {"title": "NEUTRAL RESIDUES WITH GATED ADAPTERS", "content": "The first adapters by Rebuffi et al. (2017) were initially introduced in the context of convolutional residual networks. Their motivation was to adapt a backbone to multiple tasks by finetuning on a limited amount of data, rather than adding a large body of knowledge. Therefore they consider a limited number of trainable parameters, typically less than 10% additional weights compared to the original backbone. In the context of language models, Houlsby et al. (2019) add residual feed-forward block sequentially after each multi-head attention and the original FFN. In our case and as shown in Figure 2, we add more capacity (typically 20%) and adopt parallel adapters as we do not observe a noticeable difference compared to serial adapters. This concurs with observations by Touvron et al. (2022) that blocks can be parallelized pairwise if vision transformers are large enough.\nOne key ingredient pointed out by Houlsby et al. (2019) is that it is best to start training adapters such that the initial network is near-identical to the pretrained one, thereby not modifying the initial function. This is typically done by initializing the output adapter matrix to 0s, as also advocated by Zhang et al. (2019). We are even more drastic in that respect: in addition to setting the output matrix with 0s, we depart from the usual He's initialization for both the input and gating matrix and initialize them with a much lower variance, such that the model remains longer close to the original one. More specifically, we reduce the variance employed in He's initialization: instead of initializing weights with variance $2/d$, where $d$ in the input dimension of the matrix, we use a variance of $1/(d \\cdot L)$, where $L$ is the number of transformer layers.\nIn our experiments carried out for adding a new language, it is more effective to add extra weights in the form of FFN than in MHA blocks, for a given number of additional parameters. This is consistent with how extra capacity is added in MoE. Similarly Li et al. (2024) present an ablation that shows that adding FFN blocks is better than adding attention blocks. Note, this is in contrast with what is observed by Hu et al. (2022) with the LoRA method, for which the best choice to adapt to different downstream tasks with few parameters is to update the MHA matrix, in particular they choose to update the keys and values matrices. In our preliminary experiments, we re-evaluate LoRA in our context of knowledge addition, and observe that LoRA is significantly more effective when updating the FFN blocks than the MHA blocks. We hypothesize that this apparent discrepancy is due to the fact that task adaptation is different from the question of adding a large body of knowledge to a pretrained network."}, {"title": "EXPERIMENTS", "content": "In this section, we report experimental results to validate the performance of neutral residues to adapt a neural network to a new domain. We consider the case of adding or improving the multilingual ability of a large language model. More precisely, we start from an English-only model (or a model that has only seen a small amount of non-English data) and finetune it on French data."}, {"title": "DATASETS AND EVALUATION PROTOCOLS", "content": "For the French finetuning dataset, we use a dataset extracted from Common-Crawl, which was pre-processed with the following steps. First, the text content was extracted from HTML using the resiliparse package. Then, we performed language identification with fastText to keep only French data. Next, we performed deduplication at the paragraph level, by computing a hash of each paragraph. Finally, we filtered document using a fastText linear classifier, which was trained to discriminate random pages from CommonCrawl vs. high-quality documents such as Wikipedia articles, textbooks, news articles or scientific articles.\nFor the English domain, we restricted ourselves to text from Wikipedia. The motivation for this is to use a finetuning dataset that is close, but yet different, from the training dataset of the backbone model. Indeed, as discussed earlier, in many cases one does not have access to the original dataset that was used to train the backbone model, and we want to demonstrate that our method is robust even in the case where exact data from the original domain is not available.\nWe consider two main ways to evaluate the performance of finetuning. First, we evaluate the perplexity of the model on held-out sets, both in English and in French, to measure forgetting and learning. For English, we consider text from a domain that does not correspond to the finetuning data, and use the PubMed subset from ThePile (Gao et al., 2020). The goal here is to evaluate how robust our method is to forgetting, especially in the case where we do not have access to the same training distribution as the original model. For French and German, we consider text from the same distribution as the finetuning dataset.\nSecond, we consider standard academic benchmarks used to evaluate large language models, such as question answering or Cloze-style problems. We use the following datasets: ARC challenge (Clark et al., 2018), HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2020), CommonSense QA (Talmor et al., 2018) and Belebele (Bandarkar et al., 2023). For French, we use the translated datasets from Dac Lai et al. (2023) and Sakai et al. (2024). For all datasets, we score each answer independently as the continuation of the question, and predict the most probable one. We use normalization, following the recommendation from Gu et al. (2024). We perform 5-shot evaluation for ARC challenge, CSQA and MMLU, and 0-shot evaluation for HellaSwag and Belebele.\nWe consider three models that we use as backbone. The two first models were trained internally on 2.4T tokens, and have 1B parameters. The main difference between these two models is that the first model, denoted Transf-EN, was trained on English data only. The second model Trans-ML is MultiLingual: it was trained on a mix of English, French, German, Spanish, Italian and Portuguese data. The non-English data represents roughly 12.5% of the tokens. The last model is Gemma-2B (Gemma et al., 2024). It also includes a small amount of multi-lingual data during pre-training. We do not have access to the training dataset of Gemma, making it a realistic test case for our approach. Appendix A gives more details about these models.\nFor finetuning with new knowledge, we train during 100000 steps with a batch size of 64 for Trans-ML and Trans-EN, and we train Gemma-2B during 150000 steps with a batch size of 8.\nIn our experiments, we use the following method as baselines:\n\\bullet Finetuning: we train all the weights from the backbone. We re-initialize the optimizer state.\n\\bullet LoRA: unlike Hu et al. (2022), we add all the additional parameters in the FFN layers, as in our preliminary experiments this choice was significantly better for knowledge extension.\n\\bullet Adapters: we use the vanilla adapters as Houlsby et al. (2019) with near-identical initialization. We use parallel instead of serial blocks to have a more directly comparable baseline."}, {"title": "PRELIMINARY ANALYSIS", "content": "Table 1 analyzes the impact of the proportion $p$ of data from the pretraining distribution used to extend the model, measured for vanilla adapters. As one can see, $p = 0.1$ offers a good trade-off: the forgetting is not significantly higher than with $p = 0.5$, while the model is almost as good in French as when we train with English only ($p = 0$). Therefore, when training with mixed data, we set $p = 0.1$ in all subsequent experiments.\nTable 2 gives the trade-off between learning and not forgetting for all baselines and our method, when training either without or with mixed training distributions. As one can see, the mixed training significantly reduces the forgetting for all the methods without hindering too much the learning."}, {"title": "MAIN RESULTS", "content": "Table 4 reports our main detailed results when we want to improve the performance in French of a pretrained model. Table 5 provides the counterpart when the new language to be added to the pretrained model is German, which overall leads to the same conclusions. As one can see, our proposal is overall offering the best trade-off between learning and not forgetting compared to all other techniques: It is first in terms of the sum of both metrics (English+French or English+German).\nWe observe that the results are consistent for all models. This indicates that even if we do not know exactly the data employed to produce the pretrained model, our neutral residues is beneficial. Note, this is also evidenced by Table 2, where neutral residues perform significantly better than other techniques even when we do not use any pretraining data. If we compare Transf-EN with Trans-ML, as expected the pretrained model Trans-ML obtains much better performance on French (resp. German). However, even the Trans-ML model, which has seen some multilingual at training time, significantly benefits from our posttraining method."}, {"title": "ABLATIONS", "content": "The low-variance initialization proposed in Section 3 improves the results in the context of gated adapters. Compared to He's initialization, it favors training the gating faster that the other weights. This hypothesis is compatible with the observation that this initialization is detrimental without gating (\u00d8).\nTable 8 reports the trade-offs achievable by different techniques when varying the finetuning learning rates, see also Figure 1. The results of our method are remarkably stable across a large range of learning rate (one order of magnitude from $5\\cdot10^{-5}$ and $5 \\cdot 10^{-4}$), in sharp contrast with Fine-tuning and LoRA, which are highly sensitive to this parameter.\nThe perplexity results in Table 7 show that this weight gives a trade-off between learning and not forgetting, which is best optimized when selected $\\alpha = 100$. We retain this value in all our experiments. This choice does not translate to downstream tasks in this specific case, yet Table 6 shows that using the $l_1$ is preferable in other contexts (with no gating or sigmoid)."}, {"title": "CONCLUSION", "content": "This paper has explored the feasibility and effectiveness of extending existing foundation models to incorporate new capabilities or knowledge without the need for resource-intensive retraining from scratch. By building upon the concept of adapters, which add new parameters to a pretrained model, we have demonstrated that it is possible to extend a model without compromising its original knowledge, thereby offering a more sustainable approach than retraining from scratch. Our study focused on the use-case of adding a new language to a pretrained model and evaluated the performance of the extended model on both training criteria and downstream tasks. The findings highlight several critical factors that contribute to the successful extension of a model while mitigating the issue of catastrophic forgetting. These factors include the strategic use of mixed training data, an adapter gating mechanism coupled with a local loss, and the importance of near-identical initialization."}]}