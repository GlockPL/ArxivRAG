{"title": "A PROBABILISTIC PERSPECTIVE ON UNLEARNING\nAND ALIGNMENT FOR LARGE LANGUAGE MODELS", "authors": ["Yan Scholten", "Stephan G\u00fcnnemann", "Leo Schwinn"], "abstract": "Comprehensive evaluation of Large Language Models (LLMs) is an open research\nproblem. Existing evaluations rely on deterministic point estimates generated via\ngreedy decoding. However, we find that deterministic evaluations fail to capture\nthe whole output distribution of a model, yielding inaccurate estimations of model\ncapabilities. This is particularly problematic in critical contexts such as unlearning\nand alignment, where precise model evaluations are crucial. To remedy this, we\nintroduce the first formal probabilistic evaluation framework in LLMs. Namely,\nwe derive novel metrics with high-probability guarantees concerning the output\ndistribution of a model. Our metrics are application-independent and allow prac-\ntitioners to make more reliable estimates about model capabilities before deploy-\nment. Through a case study focused on unlearning, we reveal that deterministic\nevaluations falsely indicate successful unlearning, whereas our probabilistic eval-\nuations demonstrate that most if not all of the supposedly unlearned information\nremains accessible in these models. Additionally, we propose a novel unlearning\nloss based on entropy optimization and adaptive temperature scaling, which sig-\nnificantly improves unlearning in probabilistic settings on recent benchmarks. Our\nproposed shift from point estimates to probabilistic evaluations of output distribu-\ntions represents an important step toward comprehensive evaluations of LLMs.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) are widely employed across various applications, from chatbots\nto code generation, relying on outputs generated through probabilistic decoding methods such as\nbeam-search and multinominal sampling. Despite their probabilistic deployment, performance eval-\nuations in LLMs predominately rely on deterministic point estimates, where outputs are generated\nthrough greedy decoding. This raises a critical research question:\nAre deterministic evaluations adequate for assessing sensitive applications\nor do they fall short in capturing the risks associated with probabilistic outputs?\nCurrent deterministic evaluation might result in a potential misalignment between evaluation and\npractical usage overlooking the inherent variability in model outputs. As a result, they could fail\nto account for both utility and potential risks associated with the model's entire output distribution.\nYet, use cases like model alignment and unlearning demand precise model evaluations to mitigate\nthe risk of harmful usage or privacy non-compliance during deployment. As illustrated in Figure 1,\nan unlearning algorithm may appear to successfully delete information in a deterministic setting yet\nstill leak that information with a certain probability when outputs are sampled. In many scenarios,\nleakage in even a small fraction of samples \u2013 such as revealing a social security number, user pass-\nwords, or copyrighted information \u2013 can be as problematic as leakage in every response, making\ndeterministic evaluations insufficient to capture practical risks.\nTo address this, we evaluate the sufficiency of deterministic methods in an unlearning case study,\nfocusing on whether they accurately reflect risks of information leakage in real-world probabilis-\ntic settings. We find that deterministic evaluations are insufficient, introduce a probabilistic view on\nunlearning and propose to evaluate the LLM's entire output distribution instead of point estimates."}, {"title": "RELATED WORK", "content": "Machine Unlearning. Machine unlearning aims to remove specific information from a model's\nweights while preserving its overall capabilities (Cao & Yang, 2015). Early works focus on classifi-\ncation tasks (Guo et al., 2020; Golatkar et al., 2020; Tanno et al., 2022; Wang et al., 2023; Pawelczyk\net al., 2023). Later works consider more complex scenarios, such as unlearning in autoregressive\nLLMs for text generation (Jang et al., 2022; Chen & Yang, 2023; Eldan & Russinovich, 2023; Kim\net al., 2024; Maini et al., 2024; Sheshadri et al., 2024; Li et al., 2024), which we will focus on.\nMaini et al. (2024) introduced a synthetic benchmark dataset that allows for controlled learning and\nunlearning of fictional information. Other works explored broader unlearning contexts, such as re-\nmoving knowledge about specific pop culture topics like Harry Potter (Eldan & Russinovich, 2023).\nPrevious unlearning algorithms introduced considerable trade-offs between model capabilities and\nthe effectiveness of unlearning, this includes Gradient Ascent (GA), Gradient Difference (GD) (Liu\net al., 2022), Kullback-Leibler minization (KL), or preference optimization (PO) (Rafailov et al.,\n2024). Zhang et al. (2024) address this by proposing Negative Preference Optimization (NPO),\nwhich shows notable improvements in balancing model capability and unlearning quality.\nAttacks against unlearning. Towards more accurate evaluations of unlearning recent studies have\nexplored whether information supposedly removed by unlearning algorithms can be retrieved using\nextraction attacks. Patil et al. (2023) utilized a logit lens (Geva et al., 2020) approach to analyze hid-\nden states of LLMs to extract unlearned information. Recently, adversarial attacks in the embedding\nspace of LLMs have been proposed to retrieve harmful (Schwinn et al., 2023) and unlearned infor-\nmation (Schwinn et al., 2024). Subsequent work demonstrated that continuous attacks can be used to\ndefend models against such threats (Sheshadri et al., 2024; Xhonneux et al., 2024). Moreover, Lynch\net al. (2024) proposed a diverse set of methods to robustly evaluate unlearning in LLMs. Beyond\nextraction attacks, recent studies aimed to quantify the degree of memorization in LLMs. Carlini\net al. (2022) estimated that these models memorize at least 1% of their training dataset."}, {"title": "PRELIMINARIES", "content": "Language models. We model language models as parameterized functions $\\pi_{\\theta} : V^{\\infty} \\rightarrow \\Delta^{|V|^{m-1}}$\nmapping an input sequence of arbitrary length to a distribution over output sequences of length m,\nwhere $\\theta$ are the model parameters, V denotes a vocabulary, and $\\Delta^{|V|^{m-1}}$ is the probability sim-\nplex in $\\mathbb{R}^{|V|^m}$. In other words, for a fixed input sequence $x \\in V^{\\infty}, \\pi_{\\theta}(x)$ spans a probability\ndistribution over all possible output sequences $V^m$ of length m. While we are generally inter-\nested in the output distribution $\\pi_{\\theta}(x)$, in practice we cannot directly access this distribution since\nthe number of possible output sequences $|V|^m$ quickly outgrows the number of atoms in the ob-\nservable universe. Instead, we can only access and evaluate the language model autoregressively\n$\\pi_{\\theta}(y_1,\\ldots,y_m|x) = \\prod_{t=1}^{m}\\pi_{\\theta}(y_t|y_{t-1},\\cdots, y_1,x)$, where $\\pi_{\\theta}(y_t|\\cdot)$ corresponds to the distribution\nover the possibilities for the next token $y_t$ at time step t. This represents a challenge: Without any\nfurther knowledge about the underlying distribution $\\pi_{\\theta}(x)$, practically we can only learn about it via\nsampling the model's responses for a given input sequence $x, Y \\sim \\pi_{\\theta}(x)$.\nMachine unlearning. The goal of machine unlearning is to remove knowledge from a model while\npreserving its overall performance. That is, given a model $\\pi_{\\theta}$, a forget set $D_{FG}$, and a retain set\n$D_{RT}$, we seek an algorithm to transform the model's parameters $\\theta$ such that the response y of the\nupdated model $\\pi_{\\tilde{\\theta}}$ does not answer the queries x for all $(x, y) \\in D_{FG}$ of the forget set. The challenge\nis that the model's utility should remain high for queries from the retain set $D_{RT}$ at the same time.\nUnlearning metrics. Assume we have a perfect oracle to decide if a generated text leaked informa-\ntion. We model the oracle as a function $h : V^m \\rightarrow [0, 1]$ that quantifies how much information got\nleaked, where $h(s) = 0$ means s does not leak information, and $h(s) = 1$ means complete leakage.\nFor example, h can be a binary and indicate if specific data from the forget set got leaked, or the\nROUGE score which measures similarity between the model's response and a ground truth."}, {"title": "A COMPREHENSIVE EVALUATION FRAMEWORK FOR LLMS", "content": "Current evaluation schemes are insufficient to evaluate LLMs in sensitive applications since they are\nbased on point estimates. To remedy this, we propose a probabilistic evaluation framework. For\nthe sake of clarity, we introduce our framework using the application case of machine unlearning,\nalthough our framework generalizes beyond unlearning to other domains as well. First, we properly\ndefine four desiderata for machine unlearning that comprehensive evaluations must fulfil:\nDesiderata for comprehensive machine unlearning evaluations\nI: Must quantify the extend of unlearning.\nII: Must be efficient to ensure feasibility in practical deployments.\nIII: Must accurately reflect practical leakage risks (e.g., when sampling from the model) and\nmust detect residual information contained in the unlearned model.\nIV: Must offer guarantees on leakage risks to satisfy real-world use cases.\nDesiderata I ensures that metrics quantify unlearning and not other unrelated factors. II addresses\nthe practicality of implementing evaluations in real-world scenarios. III and IV focus on minimiz-"}, {"title": "METRICS FOR COMPREHENSIVE EVALUATIONS OF OUTPUT DISTRIBUTIONS", "content": "Computing metrics with guarantees on distributions is challenging especially for language mod-\nels since their output distributions are complex and we cannot make any assumptions about them.\nWe propose to overcome this challenge through (1) Monte Carlo sampling to estimate properties\nof their output distribution and by (2) introducing novel metrics with formal guarantees based\non distribution-free, non-parametric bounds. Specifically, our metrics are based on concentration\nbounds that are widely used in the machine learning literature, for example in the context of proba-\nbilistic certifiable robustness (expectation-bounds (L\u00e9cuyer et al., 2019; Cohen et al., 2019), CDF-\nbounds (Kumar et al., 2020), and variance-bounds (Schuchardt et al., 2023)). Therefore, the follow-\ning metrics require that one specifies an (arbitrarily high) confidence level of $1 - \\alpha >$.\nLet q denote an input prompt and $Y = (y_1,\\ldots,\\ldots, Y_m) \\sim \\pi_{\\theta}(q)$ is a sequence sampled from the com-\nplex distribution that LLMs span over output sequences given input sequence q. We are interested\nin the random variable $X = h(Y)$ where h quantifies information leakage for a single $Y \\sim \\pi_{\\theta}(q)$.\nAssume $X_1, ..., X_n$ are n independent realizations of X obtained through Monte-Carlo sampling.\nBinary case. First we consider the special case of a binary metric $h : V^m \\rightarrow \\{0, 1\\}$, where\n$h(Y) = 1$ means the information got leaked. Then X is a Bernoulli random variable where the\nsuccess probability p corresponds to the probability of leaking the information when sampling from\nthe model's output distribution. We can upper bound the probability p with a Binomial proportion\nconfidence bound: Let $S_n = \\Sigma_{1}^{n}X_i$ denote the number of times information got leaked when\nsampling from the LLM, where n represents the number of Monte Carlo samples. We propose the\nconservative Clopper-Pearson upper confidence bound as unlearning metric for LLM distributions:\nMetric 1 (Binary leakage bound). We define the unlearning metric $\\mathcal{M}_1 \\triangleq B(1-\\frac{\\alpha}{2}; S_n+1,n-S_n)$\nwhere $B(\\hat{q}; a, b)$ is the $\\hat{q}$th-quantile of the beta distribution with shape parameters a and b.\nProposition 1. With high probability of at least $1 - \\alpha$, metric $\\mathcal{M}_1$ represents an upper bound on the\nprobability that the next sample leaks information, $p < \\mathcal{M}_1$.\nGeneral case. Most applications will require more fine-grained metrics to bound the probability\nof leaking a certain percentage of the information. We therefore consider the general case of an\narbitrary unlearning metric $h : V^m \\rightarrow [0, 1]$ and bound the probability that a model leaks more than\na certain threshold x, Pr[h(Y) > x]. In particular, we propose to estimate the CDF of X = h(Y)\nusing Monte Carlo sampling and to derive an upper bound using the Dvoretzky-Kiefer-Wolfowitz\ninequality. Let $F_n(x) = \\frac{1}{n} \\sum_{i=1}^{n}1\\{X_i < x\\}$ denote the empirical CDF counting how many times\nat most x% of the secret got leaked for n Monte-Carlo samples. We introduce the following metric:\nMetric 2 (General leakage bound). Given a specified percentage $x \\in [0,1]$ of the information the\nmodel should not leak, we define the metric $M_2(x) \\triangleq 1 - F_n(x) + \\epsilon$ with $\\epsilon = \\sqrt{\\frac{ln(1/\\alpha)}{2n}}$.\nWe provide the following high-probability guarantee for our second metric $M_2$:\nProposition 2. With high probability of at least $1 - \\alpha$, metric $\\mathcal{M}_2(x)$ upper-bounds the probability\nthat the next sample leaks more than x% of the information, $Pr(X > x) < \\mathcal{M}_2(x)$ for all $x \\in [0, 1]$."}, {"title": "QUANTIFYING OUTPUT DISTRIBUTIONS WITH MOMENT BOUNDS", "content": "Besides bounding the probability of leaking information, we can also quantify the quality of unlearn-\ning by bounding moments of the output distribution of LLMs. In particular, we propose metrics by\nbounding moments of the random variable X = h(Y) with high probability using CDF bounds.\nExpectation bounds. First we propose to bound the expected secret leakage E[X] with high prob-\nability. Let the points $(T_0, ..., T_M)$ partition the interval [0, 1] into M disjoint intervals, meaning\n$0 = T_0 \\leq T_1 < ... < T_M = 1$. Our metrics are based on the following result (Anderson, 1969):"}, {"title": "DISTRIBUTION UNLEARNING USING ENTROPY OPTIMIZATION AND\nADAPTIVE TEMPERATURE SCALING", "content": "Existing unlearning methods typically focus on the greedy point estimate of a language model's\noutput distribution, $\\pi_{\\theta}(x)$, overlooking that the unlearned data may still be embedded in the broader\ndistribution. This presents a significant vulnerability, as unlearning methods can be circumvented\nby simply sampling from the model's full output distribution. In addition to introducing improved\nmetrics for evaluating unlearning success from a probabilistic perspective, we propose a novel ap-\nproach that accounts for output distributions during machine unlearning itself. Our method utilizes\nentropy optimization and adaptive temperature scaling, which we describe in the following:\nEntropy optimization. First, our goal is to minimize the entropy of the model's output distribution\nfor forget samples $D_{FG}$. To this end, we define the following loss function that corresponds to the"}, {"title": "EXPERIMENTAL EVALUATION", "content": "In the following, we present results on two recent unlearning datasets, demonstrating that existing\ndeterministic evaluations are insufficient. We show that by using our probabilistic evaluation\nframework (see \u00a74), we can measure the residual information contained in a model more accurately\nand that previous unlearning methods are prone to significant leakage. We address the problem of\ninformation leakage by proposing entropy optimization with adaptive temperature scaling, which\nsubstantially enhances unlearning performance from a distributional perspective while maintaining\ndiversity of the output distribution and the utility of the model. A detailed description of hyperpa-\nrameters for all methods is provided in Appendix B.\nDatasets and models. We use two recent unlearning benchmarks for our evaluations. We conduct\nexperiments on TOFU, which consists of 200 fictitious author profiles (Maini et al., 2024). These\nprofiles are split into a retain and forget set, where the retain set is used to maintain model capabilities\nand the forget set is used for unlearning. Additionally, each profile is divided into multiple question-\nanswer pairs. TOFU provides three different unlearning splits where 99, 95, or 90 percent of the data\nis used as retain set and the remainder as forget set. For measuring model utility after unlearning,\nTOFU additionally provides the Real Authors and World Facts datasets. All TOFU experiments are\nperformed with the Phi-1.5 model (Li et al., 2023).\nIn addition to TOFU, we conduct experiments on the Llama-2-Who-is-Harry-Potter model, which\nwas unlearned to remove any Harry Potter-related knowledge (Eldan & Russinovich, 2023). We use\nthe recently proposed Harry Potter Q&A for evaluation Schwinn et al. (2024). This dataset con-\nsists of pairs of questions and relevant keywords, allowing for the detection of information leakage\nthrough keyword matching.\nBaseline metrics. For all experiments, we use ROUGE-L as a deterministic metric to measure infor-\nmation contained in the model after unlearning. ROUGE-L computes a statistic based on the longest\ncommon subsequence between two strings (Lin, 2004). Additionally, we use the ROUGE-L score\nobtained from multiple sampled responses to compute probabilistic metrics, such as bounds, mean,\nstandard deviation, and the expectation-deviation (ED) score. Note that our framework (\u00a74) can be\napplied to all deterministic metrics, such as perplexity or forget quality. We chose ROUGE-L as it\ndirectly measures information leakage with respect to a ground truth reference and is widely used\nin the unlearning domain. Throughout the manuscript, we use information leakage to refer to the\nmagnitude of the ROUGE-L score, where a high score indicates high information leakage. We use\nthe model utility score as described in TOFU to measure generation quality of a given model Maini\net al. (2024). We additionally employ the self-BLEU score (Zhu et al., 2018), which computes\nBLEU scores (Papineni et al., 2002) between generated samples and allows us to investigate the\ninfluence of our proposed unlearning algorithm on generation diversity.\nUnlearning methods. We use Gradient Ascent (GA), Gradient Difference (GD) (Liu et al., 2022),\nRMU (Li et al., 2024), and NPO (Zhang et al., 2024) for a diverse selection of unlearning baselines\nand combine NPO with entropy optimization and adaptive temperature scaling for our approach\nsince it is the current state-of-the-art unlearning method."}, {"title": "IMPROVING LLM EVALUATIONS WITH PROBABILITY BOUNDS", "content": "Most existing metrics used to measure unlearning quality in LLMs already fulfill desiderata I and II,\ni.e., they quantify the extent of unlearning and are efficient to compute. In the following, we discuss\nhow deterministic evaluations do not satisfy the remaining desiderata and are thus insufficient. To\naddress these limitations and satisfy the desiderata outlined earlier, we use the metrics introduced in\nour probabilistic evaluation framework ( \u00a74). These metrics address desiderata III and IV, particu-\nlarly focusing on the risk of information leakage during sampling."}, {"title": "EFFECT OF ENTROPY REGULARIZATION", "content": "To mitigate the risk of information leakage during sampling, we introduce entropy optimization to\nselectively decrease the model's entropy on the forget set. This approach aims to decrease the vari-\nance of the sampling distribution, as illustrated in Figure 3 (c). Figure 4 (a) demonstrates the effects\nof the forget entropy regularization parameter $A_f$ on two TOFU dataset splits (90/10 and 95/5). As\nwe increase the regularization strength, the diversity for unlearning-related queries approaches zero,\neliminating the risk of information leakage during sampling.\nAn alternative approach to reduce output diversity could consist in lowering the model's softmax\ntemperature $\\tau$. As $\\tau$ approaches 0, sampling converges to greedy generation. Figure 4 (b) illus-\ntrates the impact of temperature scaling across various forget regularization weights $A_f$. Lowering\nthe temperature $\\tau$ consistently reduces the standard deviation of the ROUGE-L score, indicating de-\ncreased output diversity. However, temperature scaling affects both unlearning-related and unrelated\ntasks indiscriminately. This creates a trade-off between robust unlearning and maintaining output\ndiversity on general tasks. We show how output diversity can be maintained within the entropy\noptimization approach in the next section."}, {"title": "MAINTAINING OUTPUT DIVERSITY AND MODEL UTILITY", "content": "Entropy optimization effectively reduces information leakage in our experiments. At the same time,\nunlearning methods should not negatively affect other properties of the model, such as output diver-\nsity, model confidence, and overall utility. We investigate these metrics using the Real Authors and\nWorld Facts dataset, which were not used during training. Results are summarized in Figure 5."}, {"title": "CONCLUSION", "content": "We introduce a probabilistic perspective on LLM evaluation and propose a novel framework to di-\nrectly assess the output distribution of a model. Our proposed perspective shift from single point\nestimates towards evaluating entire output distributions offers significant potential for the field of\nunlearning and can be directly used for evaluating a variety of sensitive applications beyond un-\nlearning, such as measuring toxicity and mitigating undesired biases in model outputs. Furthermore,\nour framework lays the groundwork for developing metrics for accurate evaluations in distribu-\ntions beyond text, extending to generative models in image, audio, and other modalities. Overall,\nour work represents an important contribution towards comprehensive evaluations of machine un-\nlearning methods and provides a foundation for future research in this area, such as investigating\nalignment or model utility from a probabilistic perspective."}, {"title": "BROADER IMPACT", "content": "Our work highlights the limitations of current LLM evaluations being conducted in a deterministic\nmanner. By introducing a probabilistic evaluation framework, we enable more accurate assessments\nof model behavior and potential risks. This approach could lead to improved safety and reliability in\nAI systems, more effective unlearning techniques enhancing privacy protection, and better alignment\nof AI models. Additionally, our methods could reveal previously unknown vulnerabilities in existing\nmodels. Overall, this research contributes to more accurate evaluations of generative models."}, {"title": "HYPERPARAMETERS", "content": "For all unlearning algorithms we use a learning rate of 1e \u2013 5 with a cosine learning rate schedule\nwith warmup ratio of 0.1, batch size of 32, and weight decay of 0.01. For NPO we set $\\beta_{NPO} =$\n0.05. We use 10 training epochs for all experiments as in (Maini et al., 2024). For probabilistic\nevaluations we sample n = 1024 model generations for every experiment if not stated otherwise.\nProbabilistic guarantees are calculated with a high-probability guarantee of $\\alpha = 0.01$. We set the\nadaptive temperature scaling threshold $c_T = 0.9$ for all experiments. This was done as the average\nconfidence of all models remained considerably below 0.9 during training. In our experiments,\nadaptive temperature thresholding has a negligible effect on the diversity of the model outputs using\nthis threshold (see Section 6.3)."}, {"title": "METRIC GUARANTEE PROOFS", "content": "Note that confidence intervals have two bounds that share a significance level of $\\alpha$, meaning each\nbound uses a significance level of $\\alpha/2$. Consequently, since we propose metrics based on one bound\nonly, our bounds can make use of the full significance level $\\alpha$.\nMetric 1 (Binary leakage bound). We define the unlearning metric $\\mathcal{M}_1 \\triangleq B(1-\\frac{\\alpha}{2}; S_n+1,n-S_n)$\nwhere $B(\\hat{q}; a, b)$ is the $\\hat{q}$th-quantile of the beta distribution with shape parameters a and b.\nProposition 1. With high probability of at least $1 - \\alpha$, metric $\\mathcal{M}_1$ represents an upper bound on the\nprobability that the next sample leaks information, $p < \\mathcal{M}_1$.\nMetric 2 (General leakage bound). Given a specified percentage $x \\in [0,1]$ of the information the\nmodel should not leak, we define the metric $M_2(x) \\triangleq 1 - F_n(x) + \\epsilon$ with $\\epsilon = \\sqrt{\\frac{ln(1/\\alpha)}{2n}}$.\nProposition 2. With high probability of at least $1 - \\alpha$, metric $\\mathcal{M}_2(x)$ upper-bounds the probability\nthat the next sample leaks more than x% of the secret, $Pr(X > x) < \\mathcal{M}_2(x)$ for all $x \\in [0, 1]$."}]}