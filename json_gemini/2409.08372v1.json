{"title": "FEDPROPHET: Memory-Efficient Federated Adversarial Training via Theoretic-Robustness and Low-Inconsistency Cascade Learning", "authors": ["Minxue Tang", "Yitu Wang", "Jingyang Zhang", "Louis DiValentin", "Aolin Ding", "Amin Hass", "Yiran Chen", "Hai \"Helen\" Li"], "abstract": "Federated Learning (FL) provides a strong privacy guarantee by enabling local training across edge devices without training data sharing, and Federated Adversarial Training (FAT) further enhances the robustness against adversarial examples, promoting a step toward trustworthy artificial intelligence. However, FAT requires a large model to preserve high accuracy while achieving strong robustness, and it is impractically slow when directly training with memory-constrained edge devices due to the memory-swapping latency. Moreover, existing memory-efficient FL methods suffer from poor accuracy and weak robustness in FAT because of inconsistent local and global models, i.e., objective inconsistency.\nIn this paper, we propose FEDPROPHET, a novel FAT framework that can achieve memory efficiency, adversarial robustness, and objective consistency simultaneously. FEDPROPHET partitions the large model into small cascaded modules such that the memory-constrained devices can conduct adversarial training module-by-module. A strong convexity regularization is derived to theoretically guarantee the robustness of the whole model, and we show that the strong robustness implies low objective inconsistency in FEDPROPHET. We also develop a training coordinator on the server of FL, with Adaptive Perturbation Adjustment for utility-robustness balance and Differentiated Module Assignment for objective inconsistency mitigation. FEDPROPHET empirically shows a significant improvement in both accuracy and robustness compared to previous memory-efficient methods, achieving almost the same performance of end-to-end FAT with 80% memory reduction and up to 10.8\u00d7 speedup in training time.", "sections": [{"title": "1 Introduction", "content": "With the rapid development of modern data-gluttonous artificial intelligence (AI), concerns about data privacy also arise. As a distributed machine learning paradigm, Federated Learning (FL) is proposed to provide a strong privacy guarantee when training AI models [15, 16]. FL pushes model training to local edge devices (denoted as clients in FL) and only aggregates locally trained models, which avoids privacy leakage in data gathering and transmission.\nWhile FL can offer privacy benefits, it cannot guarantee robustness against adversarial examples that are also significant threats to AI systems. Previous studies have shown that AI models are usually sensitive to small input perturbations, and a manipulated imperceptible noise can cause catastrophic errors in the outputs [9]. To tackle the adversarial examples, adversarial training is proposed to train the model with adversarially perturbed training data [21], and federated adversarial training is also explored to conduct adversarial training in the federated learning context [39].\nHowever, adversarial training will sacrifice the model performance as a trade-off between the utility (accuracy) and the robustness [33]. Therefore, a larger model with higher capacity is required to achieve high accuracy and strong robustness simultaneously. In the cross-device FL where the majority of clients are resource-constrained edge devices such as IOT devices and mobile phones [13], not all the clients have sufficient memory to train a large model demanded by federated adversarial training.\nIt is still possible to train a model that exceeds the memory capacity of a device by swapping the model parameters and intermediate features between the internal memory (e.g., CPU RAM or GPU memory) and the external storage (e.g., SSD) [25, 31]. When the internal memory is significantly smaller than the memory requirement of training the whole model, frequent memory swapping during each forward and backward propagation can incur high latency. The latency introduced by memory swapping becomes more significant in adversarial training, where additional backward and forward propagations are usually required for adversarially perturbing training data in each SGD iteration [21].\nSome previous studies explored memory-efficient federated learning frameworks that allow resource-constrained clients to train a smaller local model or a sub-model of the large global model, and aggregate the heterogeneous models with knowledge distillation [7, 20] or partial average [3, 8]."}, {"title": "2 Related Works and Preliminaries", "content": ""}, {"title": "2.1 Federated Learning", "content": "Federated Learning (FL) is a distributed learning framework, where different devices (i.e., clients) collaboratively train a model w that can minimize the empirical task loss L [15, 23]:\n$\\min_{w} L(w) = \\sum_{k=1}^{N} q_k L_k(w)$,\nwhere $L_k(w) = \\frac{1}{|D_k|} \\sum_{(x,y) \\in D_k} l(x,y;w)$.\n$D_k$ with size $|D_k| = q_k \\sum_i |D_i|$ is the local dataset of client k. The local dataset is never shared with others such that privacy is preserved in FL. To achieve this collaborative goal without data sharing, clients conduct local training on the model independently, and a central server aggregates the locally trained model periodically. The model aggregation is usually an average over the local model parameters [23].\nOne challenge in FL is the heterogeneous clients [13, 18]. Some previous studies try to improve the convergence and performance of FL with statistical heterogeneity (non-I.I.D. and unbalanced local data) [14, 29, 32, 37]. The others accelerate and stabilize FL under systematic heterogeneity (various computational resources, e.g., computing power and memory capacity, among clients) [19, 28, 30]. We mainly focus on the systematic heterogeneity among clients in this paper.\nSome recent studies try to deal with insufficient memory and poor performance of clients in FL. Knowledge-distillation FL allows different clients to train a model with different sizes and architectures, which enables resource-constrained devices to train a smaller model than the other clients [7, 20]."}, {"title": "2.2 Adversarial Training", "content": "It is well known that the performance of deep neural networks can be dramatically undermined by adversarial examples, which are generated by adding small perturbations to the clean data [9]. To confer robustness to the deep neural networks, Adversarial Training (AT) is proposed. In contrast to standard training (ST) that simply minimizes the empirical task loss, AT solves a min-max problem:\n$\\min_{w} \\max_{\\delta: ||\\delta||_p \\le \\epsilon} l(x + \\delta, y;w)$.\nAT alternatively solves the inner maximization and the outer minimization such that the model becomes insensitive to the small perturbation $\\delta$ in the input $x$ [21, 36]. Specifically, in each iteration of AT, PGD-n Attack [21] conducts n-steps projected gradient accent to find the optimal perturbation $\\delta$, and then the perturbed inputs are used for one-step gradient descent over the model parameter $w$. It is shown that AT with a larger n can confer stronger robustness to the model, but also with more forward and backward propagations [36].\nFor the analysis purpose, we define $(\\epsilon, c)$-robustness:\nDefinition 1. A model w is $(\\epsilon, c)$-robust in a loss function l at input x if $\\forall \\delta \\in {\\delta : ||\\delta||_p \\le \\epsilon}$,\nl(x + \\delta, y;w) \u2013 l(x, y;w) \u2264 c."}, {"title": "2.3 Cascade Learning", "content": "Cascade Learning (also known as Decoupled Learning) is proposed to reduce the memory requirement for training a large model [4, 12, 22]. As illustrated in Figure 2, the key idea of Cascade Learning is to partition a large neural network into cascaded small modules and train each module $w_m$ with an early exit loss $l_m$ provided by a small auxiliary output model $O_m$, instead of the joint loss $l$ of the whole model:\n$\\min_{w_m, O_m} L_m(w_m, O_m) = \\sum_{(z_{m-1},y)} l_m(z_{m-1}, y; w_m, O_m)$,\nEach module is fixed after training, and the output features $z_m(z_{m-1}; w_m)$ are used to train the next module $(w_{m+1}, O_{m+1})$.\nHowever, cascade learning is shown to have inferior performance compared to joint training because of the objective inconsistency, i.e., each module is independently trained with the early exit loss $l_m$ that is different from the joint loss $l$ of the whole model. Since $\\nabla l_m \\ne \\nabla l$, the independent training of each module converges to suboptimal parameters in the whole model [34]. In addition, the robustness of each module in the early exit loss does not sufficiently lead to the robustness of the whole model in the joint loss."}, {"title": "3 Motivation", "content": "The majority of devices in federated learning (FL) are memory-constrained but federated adversarial training requires larger models. The common edge devices in FL scenarios are IOT devices, mobile phones, and laptops, which usually have relatively small memory capacity and low performance [13]. Furthermore, considering other real-time co-running applications on the same device, such as video playing and website searching, the available memory reserved for training could be very limited [30]. On the one hand, although some small models like a three/four-layer CNN (CNN3/4) can be fully fed into the limited memory to be trained, the clean and adversarial accuracy of federated adversarial training could be severely degraded in comparison to training larger models as illustrated in Table 1. On the other hand, training a large model can exceed the available memory of a large proportion of edge devices (see Figure 7), which makes them the bottleneck of FL.\nMemory swapping can introduce high data access overhead in federated adversarial training. One common solution to training a large model on a memory-constrained device is partitioning the model into several smaller modules, adopting \"memory swapping\" to offload/fetch the parameters, optimizer states and activations to/from the external storage when necessary [31]. However, when the available memory is significantly less than the requirement, memory swapping leads to high data access latency that can dominate the federated adversarial training workload, as shown by the comparison between \u201cSuff. Mem\u201d and \u201cLim. w/ Swap\" in Figure 3. There are three reasons behind the high data access latency. First, memory swapping involves context switching of data access between the internal memory and the external storage. Offloading/fetching data to/from the external storage induces the software driver management overhead on the CPU. Second, the storage I/O bandwidth could be only 10% of memory bandwidth, leading to long data transfer time. Third, federated adversarial training requires more forward and backward propagations to solve the min-max problem as described in Equation (2), which makes the frequency of memory swapping even higher.\nPrior memory-efficient methods get poor accuracy and robustness. Some prior works propose memory-efficient FL [3, 7, 8, 20] to avoid memory swapping when training a large model, but they show low clean and adversarial accuracy (an example is given as \u201cLarge-PT\u201d in Table 1). The poor performance of these memory-efficient FL frameworks is attributed to objective inconsistency, i.e., memory-constrained clients may train small models or sub-models that are different from the large global model. The objective inconsistency hinders the convergence with suboptimal local model updates, and introduces a gap between the local model robustness and the global model robustness.\nOur goal. Based on the observations above, we have the insight that memory swapping is the bottleneck of the convergence time in large-scale federated adversarial training with resource-constrained edge devices, but existing techniques for avoiding memory swapping induce issues of low clean accuracy and weak robustness. Hence, our goal is to develop a memory-efficient federated adversarial training framework that can avoid memory swapping while maintaining utility and robustness when training a large model. As far as we know, FEDPROPHET is the first federated adversarial training framework that achieves memory efficiency, high utility, and theoretic robustness simultaneously."}, {"title": "4 Overview", "content": "FEDPROPHET is a federated adversarial training framework wherein each client conducts theoretic-robustness and low-inconsistency cascade learning under the coordination of a central server. FEDPROPHET consists of local trainers on the client side and the model partitioner, the training coordinator, and the model aggregator on the server side, as shown in Figure 4. At the beginning of the FL process, the model partitioner partitions a predefined large global model into small modules that satisfy the memory constraints on clients. After determining the model partition, we have four steps in each communication round of FL: \u2460 Clients upload the validation (clean and adversarial) accuracy of the current model and their available hardware resources (memory and performance) to the server; \u2461 The training coordinator on the server adjusts the perturbation magnitude $\\epsilon_{m-1}$ based on the validation accuracy (Adaptive Perturbation Adjustment), and determines which modules should be trained by each client based on their available hardware resources (Differentiated Module Assignment); \u2462 Each client conducts adversarial training on the assigned modules with the strong-convexity regularized loss, and uploads the trained modules to the server; \u2463 The server conducts partial average to aggregate the trained modules, and broadcasts the aggregated global model back to the clients.\nIn the following sections, we first introduce the local trainer on the client side in Section 5, about how each client conducts local adversarial training on a module to achieve theoretic robustness and low inconsistency. Then in Section 6, we discuss how the central server further enhances robustness and eliminates inconsistency by coordinating the local training of clients based on the training and hardware status. We empirically evaluate FEDPROPHET in Section 7 and we conclude this paper in Section 8."}, {"title": "5 Local Client Design", "content": "In this section, we assume that the whole model (also denoted as the backbone model) has been partitioned into M modules for cascade learning, while we leave the discussion on how the backbone model is partitioned to Section 6. We will introduce how a client adversarially trains a module with strong convexity regularization to ensure the robustness of the backbone model in Section 5.1. In Section 5.2, we will uncover the relationship between objective inconsistency and robustness, which demonstrates that the adversarial cascade learning we propose can also achieve low objective inconsistency in addition to the robustness guarantee."}, {"title": "5.1 Adversarial Cascade Learning with Strong Convexity Relularization", "content": "Sufficient Condition for Backbone Robustness. A client can conduct adversarial training on module m by adding adversarial perturbation to its input $z_{m-1}$, as shown in Figure 5. However, since the module is trained with the early exit loss $l_m$ that is not equivalent to the joint loss $l$ of the backbone model, the robustness of the module in the early exit loss does not sufficiently lead to the robustness of the backbone model in the joint loss. The following proposition gives a sufficient condition for the robustness of the backbone model that consists of M cascaded modules:\nProposition 1. The backbone model $(1 \\to 2 \\to ... \\to m \\to ... \\to M)$ can have $(\\epsilon_0, c_M)$-robustness in the joint loss l if for every module $m < M$, we have a finite upper bound $\\epsilon_m$ such that\n$\\max_{\\delta_{m-1}: || \\delta_{m-1} || \\le \\epsilon_{m-1}} ||z_m(z_{m-1} + \\delta_{m-1}) \u2013 z_m(z_{m-1})|| \\le \\epsilon_m$,\nand the last module M has $(\\epsilon_{M-1}, c_M)$-robustness in $l_M = l$.\nSimply speaking, the sufficient condition is that the perturbation magnitude on the output $z_m$ of each module m should be upper bounded by $\\epsilon_m$, given the adversarial perturbation $\\delta_{m-1}$ on the input $z_{m-1}$ upper bounded by $\\epsilon_{m-1}$. To upper bound the perturbation on $z_m$, a straightforward method is conducting adversarial training with the perturbation magnitude on $z_m$ as the loss function, namely,\n$\\min_{w_m} \\max_{|| \\delta_{m-1} || \\le \\epsilon_{m-1}} ||z_m(z_{m-1}+\\delta_{m-1}; w_m) - z_m(z_{m-1}; w_m)||$.\nHowever, calculating the gradient of $w_m$ with this loss function by backward propagation requires memory to store the intermediate results of $z_{m-1} + \\delta_{m-1}$ in addition to the intermediate results of $z_{m-1}$. Therefore, this method is infeasible in the memory-constrained scenarios we are discussing."}, {"title": "Strong Convexity Regularization", "content": "The following lemma provides an alternative method to upper bound the perturbation on $z_m$, by making the early exit loss function strongly convex on $z_m$ and conducting adversarial training with it:\nLemma 1. If $l_m$ is $\\mu_m$-strongly convex on $z_m$ and module m is $(\\epsilon_{m-1}, c_m)$-robust in $l_m$, we have\n$\\max_{|| \\delta_{m-1} || \\le \\epsilon_{m-1}} ||z_m(z_{m-1}+\\delta_{m-1}) - z_m(z_{m-1}) ||^2 \\le \\frac{||\\nabla_{z_m}l_m(z_m, y)||^2}{\\mu_m^2} + \\frac{2c_m}{\\mu_m} + \\frac{||\\nabla_{z_m}l_m(z_m, y)||}{\\mu_m}$.\nProof. Let $r = z_m(z_{m-1}+\\delta_{m-1}) \u2013 z_m(z_{m-1})$, with the strong convexity and the robustness of $l_m$, we have\n$(\\nabla_{z_m}l_m(z_m, y))^T r + \\frac{\\mu_m}{2} ||r||^2 \\le l_m(z_m+r, y) - l_m(z_m, y) \\le c_m$\n$\\Rightarrow r + \\frac{\\nabla_{z_m}l_m(z_m, y)}{\\mu_m}||^2 \\le \\frac{c_m}{\\mu_m}$\n$\\Rightarrow ||r||^2 \\le \\frac{||\\nabla_{z_m}l_m(z_m, y)||^2}{\\mu_m^2} + \\frac{2c_m}{\\mu_m} + \\frac{||\\nabla_{z_m}l_m(z_m, y)||}{\\mu_m}$.\nAccording to Proposition 1 and Lemma 1, we propose the following two designs for each module m to attain robustness of the backbone model in the joint loss l:"}, {"title": "5.2 Robustness-Consistency Relationship in Adversarial Cascade Learning", "content": "While we have guaranteed the robustness of the whole model in adversarial cascade learning, the poor performance caused by objective inconsistency is not addressed. Since the early exit loss is not equivalent to the joint loss, the gradient $\\nabla_{w_m} l_m$ provided by the early exit loss is not the same as the gradient $\\nabla_{w_m} l$ provided by the joint loss either, leading to suboptimal model updates. To improve the performance, we need to reduce the gradient difference $| | \\nabla_{w_m} l - \\nabla_{w_m} l_m | |$.\nThere is usually no upper bound on the gradient difference since the auxiliary model can be arbitrarily different from the backbone model. However, under the adversarial cascade learning with both module robustness and backbone model robustness, we can derive an upper bound on the gradient difference with the following lemma:\nLemma 2. If the early exit loss $l_m$ has $\\beta_m$-smoothness and $(\\epsilon_m, c_m)$-robustness on $z_m$, the joint loss l has $\\beta'_m$-smoothness and $(\\epsilon_m, c_M)$-robustness on $z_m$, we have\n$||\\nabla_{w_m} l - \\nabla_{w_m} l_m||_2 \\le \\sqrt{2 (c_M + c_m) (\\beta_m + \\beta'_m)}$.\nProof. We define $h_m(z_m) = l(z_m) - l_m(z_m)$, which has $(\\beta_m + \\beta'_m)$-smoothness and $(\\epsilon_m, c_m + c_M)$-robustness on $z_m$. Thus for $\\delta_m$ with $| | \\delta_m | | \\le \\epsilon_m$, we have\n$\\frac{(\\nabla_{z_m} h_m(z_m))^T \\delta_m}{\\epsilon_m} \\le \\frac{(\\beta_m + \\beta'_m) ||\\delta_m||^2}{2} \\le h_m(z_m + \\delta_m) - h_m(z_m) \\le c_m + c_M$.\nWe take the maximum of the left hand side with $\\delta_m = \\frac{\\nabla_{z_m} h_m(z_m)}{\\beta_m + \\beta'_m} \\epsilon_m$, and we can get\n$\\frac{||\\nabla_{z_m} h_m(z_m)||^2}{2 (\\beta_m + \\beta'_m)} \\le c_m + c_M$\n$\\Rightarrow ||\\nabla_{z_m} h_m(z_m)||^2 \\le \\sqrt{2 (c_M + c_m) (\\beta_m + \\beta'_m)}$.\nWith the chain rule and $||\\nabla_{w_m} l - \\nabla_{w_m} l_m|| \\le ||\\frac{dz_m}{dw_m}|| ||\\nabla_{z_m} l - \\nabla_{z_m} l_m||$, we prove the lemma.\nAn intuitive understanding of Lemma 2 is that if both $l_m$ and l are not sensitive to the perturbation on $z_m$ (i.e., smooth and robust), their gradients on $z_m$ are close to 0 simultaneously and thus the gradient difference is small. A previous study has shown that the robustness of a deep neural network also implies smoothness [24]. Thus, we achieve small $\\beta_m$ and $c_m$ with the adversarial training on module m (Equation (7)), and we achieve small $\\beta\u2019_m$ and $c_M$ with the robustness guarantee of the whole model (Proposition 1). In other words, the adversarial cascaded learning discussed above can also mitigate the objective inconsistency simultaneously."}, {"title": "6 Central Server Design", "content": "In this section, we introduce how the server coordinates the training in FEDPROPHET with three components: model partitioner (Section 6.1), training coordinator (Section 6.2 and Section 6.3), and partial-average model aggregator (Section 6.4).\nThe model partitioner partitions the backbone model into cascaded modules while ensuring that each module can be trained within the smallest memory among all the clients. The training scheduler consists of two sub-modules: (a) Adaptive Perturbation Adjustment adjusts the perturbation magnitude $\\epsilon_{m-1}$ for each module during training to achieve the optimal accuracy-robustness balance; (b) Differentiated Module Assignment determines which modules to be trained by each client, such that \u201cprophet\u201d clients with sufficient resources can train more modules to reduce the objective inconsistency. The partial-average model aggregator conducts a partial average for each module only among clients who trained the corresponding module in the current communication round, instead of averaging among all the active clients as the standard federated learning does."}, {"title": "6.1 Memory-constrained Model Partition", "content": "The model partitioner partitions the backbone model into cascaded modules and each module can be independently trained with an auxiliary model. To formalize the model partitioning, we first define the \u201catom\u201d which cannot be further partitioned, and the backbone model should be able to be constructed as a sequence of \u201catoms\u201d ($a_1 \\to ... \\to a_L$) connected from end to end. For example, the \"atom\" can be defined as a single layer in a plain neural network like VGG [27], or a residual block in ResNet [11]. With the definition of the \"atom\", a module is defined as a combination of several connected \"atoms\u201d, with an auxiliary model at the end.\nThe key point of the model partitioner is to ensure that the memory requirement for training each module does not exceed the minimal reserved memory $R_{min}$ among all the clients, such that the clients can always train at least one module without memory swapping in any communication round. We adopt a greedy model partitioning method as given in Algorithm 1, which can achieve the least number of modules under the memory constraint. Specifically, we traverse each \"atom\" in the model and append it into one module until it reaches the memory constraint. The $MemReq(m)$ in Algorithm 1 is a function that returns the memory requirement for training the module m, which could be measured or estimated by the server. We adopt the same methodology as [25] to estimate the memory requirement for training a model, considering the memory consumption of parameters, gradients, optimizer states and intermediate features."}, {"title": "6.2 Adaptive Perturbation Adjustment", "content": "When conducting adversarial training on module m, though Proposition 1 requires to set $\\epsilon_{m-1}$ in Equation (7) as the upper bound of the perturbation on $z_{m-1}$ to sufficiently guarantee the backbone robustness, we find that it is not necessary to use such a large perturbation magnitude in practice. On the one hand, a too-large perturbation magnitude on the intermediate features $z_{m-1}$ can cause a significant performance drop and even divergence of the backbone model. On the other hand, a too-small perturbation magnitude cannot confer strong robustness to the backbone model. Therefore, it is essential to find an appropriate perturbation magnitude for each module to achieve the best utility-robustness trade-off.\nSince different modules may have different optimal perturbation magnitudes, we propose an adaptive mechanism to automatically adjust the perturbation magnitudes during training. When we complete training the previous module m \u2013 1 and fix it, we request all clients to measure the largest perturbation magnitude $d_{m-1}$ on the output $z_{m-1}$ given the perturbation on its input $z_{m-2}$. The perturbation magnitude $\\epsilon_{m-1}^{(t)}$ for the adversarial training of module m is based on the average of $d_{m-1}$ over all local training data, namely,\n$\\epsilon_{m-1}^{(t)} = \\alpha_{m-1}^{(t)} = E_{z_{m-2}}[d_{m-1}]$,\n$d_{m-1} = \\max_{|| \\delta_{m-2}|| \\le \\epsilon_{m-2}} ||z_{m-1}(z_{m-2}+ \\delta_{m-2}) \u2013 z_{m-1}(z_{m-2})||$.\nThe Adaptive Perturbation Adjustment mechanism needs to tune the scaling factor $\\alpha_{m-1}^{(t)}$ at each communication round t to balance the utility and robustness. The foundation of this mechanism is that the ratio between the clean accuracy (accuracy on clean examples) and the adversarial accuracy (accuracy on adversarial examples) reveals the balance between utility and robustness, and this ratio should not change significantly when cascading one more module. Therefore, we monitor the ratio between the clean accuracy and the adversarial accuracy during training, and we adjust $\\alpha_{m-1}^{(t)}$ by comparing the accuracy ratio of this module m and the accuracy ratio of the previous module m \u2013 1:\n$\\alpha_{m-1}^{(t)} = {\\alpha_{m-1}^{(t-1)} + 0.1,  if \\frac{C_m^{(t)}}{A_m^{(t)}} > (1+\\Delta) \\frac{C_{m-1}}{A_{m-1}};\\\\ \\alpha_{m-1}^{(t-1)} - 0.1, if \\frac{C_m^{(t)}}{A_m^{(t)}} < (1-\\Delta) \\frac{C_{m-1}}{A_{m-1}};\\\\  \\alpha_{m-1}^{(t-1)}, elsewhere.}$\n$C_m^{(t)}$ and $A_m^{(t)}$ are the validation clean accuracy and the adversarial accuracy of the cascaded modules (1 \u2192 2 \u2192 ... \u2192 m) at communication round t. $C_{m-1}$ and $A_{m-1}$ denote the final clean and adversarial accuracy when completing training module m \u2013 1 and fixing it. \u0394 is a small threshold constant, e.g., 0.05 in our experiments. When the ratio is large, which means that the clean accuracy is too high and the adversarial accuracy is too low, we increase the scaling factor $\\alpha_{m-1}^{(t)}$ to enhance the robustness, and vice versa.\nIt is noteworthy that we do not adjust the perturbation magnitude for the first module, whose input is the original data with pre-defined adversarial tolerance $\\epsilon_0$ (e.g., $l_\\infty$ norm with $\\epsilon_0 = 8/255$ is commonly used for CIFAR-10 and Caltech-256 [38]). Thus, the ratio of the first module can be used as a good beginning for the adaptive perturbation adjustment in the following modules m > 1."}, {"title": "6.3 Differentiated Module Assignment", "content": "Since different clients in federated learning may have different available hardware resources, it is possible that some of them can train multiple modules or even the whole backbone model with sufficient resources. Training more modules jointly can reduce objective inconsistency since the loss of more cascaded modules is closer to the joint loss of the whole model. In the extreme case where the client trains the whole backbone model, the objective inconsistency can be completely eliminated. Thus, it is beneficial to appoint the resource-sufficient clients as \"prophets\", who train more modules than the other resource-constrained clients.\nThe server assigns different numbers of modules to clients in each communication round according to their real-time available resources. A client k who is assigned multiple modules (m \u2192 ... \u2192 M) in round t trains the cascaded modules jointly with the following loss:\n$l_{m \u2192 M}^{(t)} (z_{m-1}, y; w_m \u2192 w_M, \u2299_M)$ = $l_{ICE}^{M \u2192 M} (z_{m-1}, y)$ + $\\frac{\\theta}{2} ||z_{M}^{(t)} (z_{m-1}; w_m \u2192 w_M)||^2$.\nSimilar to the definition of the early exit loss in Equation (7), $z_{M}^{(t)}(z_{m-1})$ calculates the feature $z_{M}^{(t)}$ by forward propagation through the joint modules (m \u2192 m + 1 \u2192 ... \u2192 M). Then the early exit loss provided by the auxiliary model of the last assigned module $M^{(t)}$, together with the strong convexity regularization, is used to train the joint modules.\nResource-constrained Module Assignment. We now discuss how to assign modules to maximize the utilization of the resources of each client. When determining the module assignment for a client k, we choose the largest $M^{(t)}$ that satisfies both of the following two constraints:\n(1) The total memory requirement of the assigned modules should not exceed the available memory $R^{(t)}$:\n$MemReq(m \\to m+1 \\to ... \u2192 M^{(t)}) \\le R^{(t)}$.\n(2) The time for training the assigned modules by client k should not exceed the time for training the single module m by the slowest client:\n$FLOPs(m \\to m+1 \\to ... \u2192 M^{(t)}) \\le FLOPs(m)_{min}$."}, {"title": "6.4 Partial-Average Model Aggregator", "content": "Because of the Differentiated Module Assignment mechanism, the server needs to aggregate the updated local models with different numbers of modules from different clients. Similar to previous partial training federated learning algorithms [3, 6, 8], we adopt partial average to aggregate local models with different numbers of modules. For each module m \u2264 n \u2264 M, the module parameter $w_n$ is aggregated as:\n$w_n^{(t+1)} = \\frac{\\sum_{k \\in S_n^{(t)}} q_k w_{n,k}^{(t, E)}}{\\sum_{k \\in S_n^{(t)}} q_k}$, where $S_n^{(t)} = {k: M_k^{(t)} \\ge n}$,"}, {"title": "7 Empirical Evaluation", "content": ""}, {"title": "7.1 Experiment Setup", "content": "Datasets and Data Heterogeneity. We adopt two popular image classification datasets, CIFAR-10 with 10 classes of 3 \u00d7 32 \u00d7 32 images [17] and Caltech-256 with 256 classes of 3 \u00d7 224 \u00d7 224 images [10], for empirical evaluation. For both datasets, we partition the whole training set onto N = 100 clients. Following previous literature [26], we adopt a 20 - 80 data distribution on each client to simulate the data heterogeneity in federated learning: 80% training data belongs to around 20% classes (i.e., 2 classes in CIFAR-10 and 46 classes in Caltech-256), and 20% data belongs to the other classes. We use the same random seed to partition the dataset in all experiments such that the data of each client keeps the same.\nDevices and Systematic Heterogeneity. We collect a device pool consisting of common edge devices like Jetson TX2 [1], Quadro M2200 [2], etc. Considering the different memory requirements for training on CIFAR-10 (small images) and Caltech-256 (large images), we divide the whole device pool into two groups for CIFAR-10 and Caltech-256 respectively. When sampling the devices, we emulate two resource-constrained scenarios with different systematic heterogeneity: We sample devices with different resources uniformly in the balanced setting, while we give a higher sampling probability for devices with smaller memory and lower performance in the unbalanced setting. Meanwhile, we abstract degrading factors for both memory and performance in each communication round to emulate the real-time available memory and performance when co-running different runtime applications, such as 4k-video playing and object detection. Figure 7 shows the distribution of memory and performance in the real-time device samplings.\nEvaluation Metrics. We conduct PGD-10 adversarial training [21] with VGG16 [27] on CIFAR-10, and ResNet34 [11] on Caltech-256. We report the accuracy on the clean test data (Clean Accuracy) as the utility metric, and the accuracy on the adversarial test data generated with PGD-20 attack (Adversarial Accuracy) as the robustness metric. Following the popular adversarial training setting [21, 26, 38], the perturbations on both training data and test data are bounded by $l_\u221e$ norm with $\\epsilon_0$ = 8/255, and the step size of PGD is set to be \u03b1 = 2/255. We report the training time (including computation time and data access time) as the efficiency metric."}, {"title": "7.2 Performance of FEDPROPHET", "content": "Baselines. We compare FEDPROPHET with joint federated adversarial learning (jFAT) [39", "20": "FedET-AT [7"}]}