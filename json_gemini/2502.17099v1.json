{"title": "IMPROVED DIFFUSION-BASED GENERATIVE MODEL\nWITH BETTER ADVERSARIAL ROBUSTNESS", "authors": ["Zekun Wang", "Mingyang Yi", "Shuchen Xue", "Zhenguo Li", "Ming Liu", "Bing Qin", "Zhi-Ming Ma"], "abstract": "Diffusion Probabilistic Models (DPMs) have achieved significant success in gener-\native tasks. However, their training and sampling processes suffer from the issue of\ndistribution mismatch. During the denoising process, the input data distributions\ndiffer between the training and inference stages, potentially leading to inaccurate\ndata generation. To obviate this, we analyze the training objective of DPMs and the-\noretically demonstrate that this mismatch can be alleviated through Distributionally\nRobust Optimization (DRO), which is equivalent to performing robustness-driven\nAdversarial Training (AT) on DPMs. Furthermore, for the recently proposed Con-\nsistency Model (CM), which distills the inference process of the DPM, we prove\nthat its training objective also encounters the mismatch issue. Fortunately, this\nissue can be mitigated by AT as well. Based on these insights, we propose to\nconduct efficient AT on both DPM and CM. Finally, extensive empirical studies\nvalidate the effectiveness of AT in diffusion-based models. The code is available at\nhttps://github.com/kugwzk/AT_Diff.", "sections": [{"title": "1 INTRODUCTION", "content": "Diffusion Probabilistic Models (DPMs) (Ho et al., 2020; Song et al., 2020; Yi et al., 2024) have\nachieved remarkable success across a wide range of generative tasks such as image synthesis (Dhari-\nwal & Nichol, 2021; Rombach et al., 2022; Ho et al., 2022a), video generation (Ho et al., 2022b;\nBlattmann et al., 2023), text-to-image generation (Nichol et al.; Ramesh et al., 2022; Saharia et al.,\n2022), etc. The core mechanism of DPMs involves a forward diffusion process that progressively\ninjects noise into the data, followed by a reverse process that learns to generate data by denoising the\nnoise. Unlike traditional generative models such as GANs(Goodfellow et al., 2014) or VAEs (Kingma\n& Welling, 2013), which directly map an easily sampled latent variable (e.g., Gaussian noise) to the\ntarget data through a single network function evaluation (NFE), DPMs adopt a gradual denoising\napproach that requires multiple NFEs (Song et al., 2022; Salimans & Ho, 2022; Lu et al., 2022b;\nMa et al., 2024). However, this noising-then-denoising process introduces a distribution mismatch\nbetween the training and sampling stages, potentially leading to inaccuracies in the generated outputs.\nConcretely, during the training stage, the model is learned to predict the noise in ground-truth noisy\ndata derived from the training set. In contrast, during the inference stage, the input distribution is\nobtained from the output generated by the DPM in the previous step, which differs from the training\nphase, caused by the inaccurate estimation of the score function due to training (Song et al., 2021;\nYi et al., 2023a) and the discretization error (Chen et al., 2022; Li et al., 2023; Xue et al., 2024b;a)\nbrought by sampling. Such distribution mismatches are referred to as Exposure Bias, which has been\ndiscussed in auto-regressive language models (Bengio et al., 2015; Ranzato et al., 2016).\nRecently, the aforementioned distribution mismatch problem in diffusion has been also recognized\nby (Ning et al., 2023; Li & van der Schaar, 2024; Ren et al., 2024; Ning et al., 2024; Li et al.,"}, {"title": "2 RELATED WORK", "content": "Distribution Mismatch in DPM. The problem is analogous to the exposure bias in auto-regressive\nlanguage models (Bengio et al., 2015; Ranzato et al., 2016; Shen et al., 2016; Rennie et al., 2017;\nZhang et al., 2019c), whereas the next word prediction (Radford et al., 2019) relies on tokens predicted\nby the model in the inference stage, which may be mismatched with the ground-truth one taken in the\ntraining stage. The similarity to DPMs becomes evident due to their gradual denoising generation\nprocess. Ning et al. (2023) and Ning et al. (2024) propose adding extra Gaussian perturbation\nduring the training stage or data-dependent perturbation during the inference stage, to mitigate this\nissue. Following this line of work, several methods are further proposed. For instance, to reduce\nthe accumulated discrepancy between the intermediate noisy data in the training and inference\nstages, Li et al. (2024) search for a suboptimal mismatched input time step of the model to conduct\ninference. Similarly, Li & van der Schaar (2024) and Ren et al. (2024) directly minimize the difference\nbetween the generated intermediate noisy data and the ground-truth data. However, these methods\neither rely on strong assumptions (Ning et al., 2023; 2024; Li et al., 2024; Ren et al., 2024) or are\ncomputationally expensive (Li & van der Schaar, 2024). In contrast, we are the first to explore the\ndistribution mismatch problem from the perspective of DRO. Meanwhile, our proposed AT with\nstrong theoretical foundations is both simple and efficient, compared with the existing methods.\nAdversarial Training and DRO. In this paper, we leverage the Distributionally Robust Opti-\nmization (DRO) (Shapiro, 2017; Namkoong, 2019; Yi et al., 2021; Sinha et al., 2018; Wang et al.,\n2022; Yi et al., 2023b) to improve the distributional robustness of DPM and CM, thereby mitigating"}, {"title": "3 PRELIMINARY", "content": "Diffusion Probabilistic Models. DPM (Sohl-Dickstein et al., 2015; Ho et al., 2020) constructs the\nMarkov chain $x_{t}$ by transition kernel $q(x_{t+1} | x_{t}) = \\mathcal{N}(\\sqrt{\\alpha_{t+1}}x_{t}, (1-\\alpha_{t+1})I)$, where $\\alpha_{1},\\ldots, \\alpha_{T}$\nare in $[0, 1]$. Let $\\bar{\\alpha}_{t} := \\prod_{i=1}^{t} \\alpha_{i}$, and $x_{0} \\sim q$ be ground-truth data. Then, for $x_{t}$, it holds\n$x_{t} = \\sqrt{\\bar{\\alpha}_{t}}x_{0} + \\sqrt{1 - \\bar{\\alpha}_{t}}\\epsilon_{t} \\ \\ \\ t = 1,\\ldots, T,$\\documentclass{article}\n\\usepackage{amsmath}\n\\begin{document}\n\nwith $\\epsilon_{t} \\sim \\mathcal{N}(0, I)$. The reverse process $p_{\\theta}(x_{t} | x_{t+1})$ is parameterized as\n$p_{\\theta}(x_{t} | x_{t+1}) = \\mathcal{N}(\\mu_{\\theta}(x_{t+1}, t + 1), \\sigma_{t+1}^{2}I),$\nwhere $\\sigma_{t+1}^{2} = 1 - \\alpha_{t+1}$. To learn $p_{\\theta}(x_{t} | x_{t+1})$, a standard method is to minimize the following\nevidence lower bound of negative log-likelihood (NLL) (Ho et al., 2020),\n$-\\mathbb{E}_{q} [\\log p_{\\theta}(x_{0})] \\leq \\mathbb{E}_{q} \\Bigg[-\\log \\frac{p_{\\theta}(x_{0:T})}{q(x_{1:T} | x_{0})}\\Bigg]$\nHere, minimizing the ELBO in the r.h.s. of above inequality links to $p_{\\theta}(x_{t} | x_{t+1})$ since it is\nequivalent to minimizing the following rewritten objective\n$\\min_{\\theta} \\Bigg\\{D_{KL}(q(x_{T}) || p_{\\theta}(x_{T})) + \\sum_{t=0}^{T-1} D_{KL}(q(x_{t} | x_{t+1}) || p_{\\theta}(x_{t} | x_{t+1}))\\Bigg\\}$\nas in (Ho et al., 2020; Bao et al., 2022; Yi et al., 2023a). Here, the conditional Kullback\u2013Leibler (KL)\ndivergence $D_{KL}(q(x_{t} | x_{t+1}) || p_{\\theta}(x_{t} | x_{t+1})) = \\int q(x_{t} | x_{t+1}) \\log \\frac{q(x_{t} | x_{t+1})}{p_{\\theta}(x_{t} | x_{t+1})} dx_{t}dx_{t+1}$ (Duchi,\n2016), and minimizing $L_{t}$ is equivalent to solve the following noise prediction problem\n$\\mathbb{E}_{t} \\Big[||\\epsilon - \\epsilon_{\\theta}(\\sqrt{\\bar{\\alpha}_{t}}x_{0} + \\sqrt{1 - \\bar{\\alpha}_{t}}\\epsilon, t)||^{2}\\Big].$\nWe use $\\| \\cdot \\|_{p}$ to denote $\\ell_{p}$-norm. Unless specified, the norm $\\| \\cdot \\|$ refers to the $\\ell_{2}$-norm $\\| \\cdot \\|_{2}$. Since\n$\\bar{\\alpha}_{t} \\rightarrow 0$ for $t \\rightarrow T$, $x_{0}$ is obtained by conducting the reverse diffusion process $p_{\\theta}(x_{t} | x_{t+1})$ starting\nfrom $x_{T} \\sim \\mathcal{N}(0, I)$ and $\\epsilon \\sim \\mathcal{N}(0, I)$, under the learned model $e_{\\theta}$ with\n$x_{t} = \\frac{1}{\\sqrt{\\alpha_{t+1}}} \\Bigg(x_{t+1} - \\frac{1-\\alpha_{t+1}}{\\sqrt{1-\\bar{\\alpha}_{t+1}}}\\epsilon_{\\theta}(x_{t+1}, t + 1)\\Bigg) + \\sqrt{1 - \\alpha_{t+1}}\\epsilon.$\nWasserstein Distance. For integer $p > 0$, $\\Gamma(\\mu, \\nu)$ as the set of union distributions with marginal\n$\\mu$ and $\\nu$, the Wasserstein $p$-distance (Villani et al., 2009) between distributions $\\mu$ and $\\nu$ with finite\n$p$-moments is\n$W_{p}(\\mu, \\nu) = \\inf_{\\gamma \\in \\Gamma(\\mu, \\nu)} \\mathbb{E}_{{(x, y) \\sim \\gamma}}[\\|x - y\\|^{p}].$\n\\end{document}"}, {"title": "4 ROBUSTNESS-DRIVEN ADVERSARIAL TRAINING OF DIFFUSION MODELS", "content": "In this section, we formally show that the success of DPM relies on specific conditions, i.e., $x_{t}$ is\nclose to $x_{t+1}$. Next, to mitigate the drawbacks brought by the restriction, we propose to consider\nthe distribution mismatch problem as discussed in Section 1, and connect the problem to a rewritten\nELBO. Finally, we apply DRO for this ELBO to mitigate the distribution mismatch problem and\nfinally link it to AT to be implemented in practice."}, {"title": "4.1 HOW DOES DPM WORKS IN PRACTICE?", "content": "Notably, minimizing (4) potentially obtains a sharp NLL under target distribution $q(x_{0})$. However,\nin the following proposition, we show that (4) also implicitly minimizes the NLL of each $x_{t}$.\nProposition 1. The minimization problem (4) is equivalent to minimizing an upper bound of\n$\\mathbb{E}_{q}[-\\log p_{\\theta}(x_{t})]$ for any $0 \\leq t \\leq T$.\nThe proof is provided in Appendix A. It shows that though (4) is proposed to generate $x_{0} \\sim q(x_{0})$,\nit also guides the model to generate $x_{t}$ such that $p_{\\theta}(x_{t})$ approximates the ground-truth distribution\n$q(x_{t})$. The conclusion is nontrivial as minimizing the ELBO of NLL $\\mathbb{E}_{q}[-\\log p_{\\theta}(x_{0})]$ does not\nnecessarily impose any restrictions on $x_{t}$ for $t > 1$.\nNext, we will further explain why (4) leads to a small NLL of $x_{t}$. In $L_{t}$ of (4), $p_{\\theta}(x_{t} | x_{t+1})$\napproximates $q(x_{t} | x_{t+1})$ with $x_{t+1} \\sim q(x_{t+1})$ representing ground-truth data. Consequently,\n$p_{\\theta}(x_{t})$ approximates $q(x_{t})$ by recursively applying such a relationship as in the following proposition.\nProposition 2. Suppose $p_{\\theta}(x_{t} | x_{t+1})$ matches $q(x_{t} | x_{t+1})$ well such that\n$L_{t} = D_{KL}(q(x_{t} | x_{t+1}) || p_{\\theta}(x_{t} | x_{t+1})) \\leq \\frac{\\gamma_{0}}{T},$\nand the discrepancy satisfies $D_{KL}(q(x_{T}) || p_{\\theta}(x_{T})) \\leq \\gamma_{0}$, then for any $0 \\leq t \\leq T$, we have\n$D_{KL}(q(x_{t}) || p_{\\theta}(x_{t})) \\leq D_{KL}(q(x_{T}) || p_{\\theta}(x_{T})) + L_{t} \\leq \\gamma_{0} + \\frac{(T - t)}{T}\\gamma_{0}.$\nThe results is similarly obtained in (Chen et al., 2023), while their result is applied for $D_{KL}(q(x_{0}) ||\np_{\\theta})$, which is narrowed compared with Proposition 2. The proof is provided in Appendix A, which\nformally explains why (4) results in $p_{\\theta}(x_{t})$ approximating $q(x_{t})$. However, this proposition is built\nupon small $L_{t}$, and notably, the error introduced by $L_{t}$ will be accumulated on the r.h.s. of (9), as\nit increases w.r.t. $t$. This phenomenon is caused by the distribution mismatch problem discussed in\nSection 1. Concretely, in (4), minimizing $L_{t}$ learns the transition probability $p_{\\theta}(x_{t} | x_{t+1})$ based on\n$x_{t+1} \\sim q(x_{t+1})$, while in practice, $x_{t}$ in (6) is generated from $x_{t+1} \\sim p_{\\theta}(x_{t+1})$. The error between\n$p_{\\theta}(x_{t+1})$ and $q(x_{t+1})$ will propagates into the error between $p_{\\theta}(x_{t})$ and $q(x_{t})$ as in (9).\nTherefore, owing to the existence of distribution mismatch, only if $L_{t}$ is minimized, the gap between\n$p_{\\theta}(x_{t})$ and $q(x_{t})$ can be guaranteed. However, the following proposition proved in Appendix A\nindicates that $L_{t}$ is theoretically minimized with restrictions.\nProposition 3. $L_{t}$ in (4) is well minimized, only if $q(x_{t+1})$ is Gaussian or $\\|x_{t+1} - x_{t} \\|\\rightarrow 0$.\nIn practice, the $q(x_{t+1})$ is usually non-Gaussian. Besides, the gap $\\|x_{t+1} - x_{t}\\|$ is not necessarily\nsmall, especially for samplers with few sampling steps, e.g., DDIM (Song et al., 2022), DPM-Solver\n(Lu et al., 2022a). Therefore, in practice, the accumulated error in (9) caused by the distribution\nmismatch problem may become large, and degenerate the quality of $x_{0}$."}, {"title": "4.2 DISTRIBUTIONAL ROBUSTNESS IN DPM", "content": "Inspired by the discussion above, we propose a new training objective as the sum of NLLs under $x_{t}$,\n$\\min_{\\theta} C(\\theta) = \\sum_{t=0}^{T} \\mathbb{E}_{q}[-\\log p_{\\theta}(x_{t})].$\nThen the following proposition constructs ELBOs for each of $\\mathbb{E}_{q}[-\\log p_{\\theta}(x_{t})]$.\nProposition 4. For any distribution $\\tilde{q}$ satisfies $q(x_{t}) = \\tilde{q}(x_{t})$ for specific $t$, we have\n$\\mathbb{E}_{\\tilde{q}} [-\\log p_{\\theta}(x_{t})] \\leq D_{KL}(q(x_{t} | x_{t+1}) || p_{\\theta}(x_{t} | x_{t+1})) + C,$\nfor a constant $C$ independent of $\\theta$.\nThe proof is in Appendix A.2. This proposition generalizes the results in Proposition 1 since $\\tilde{q}$ can be\ntaken as $q$ in Proposition 1. During minimizing $\\tilde{L}_{t}$, the transition probability $p_{\\theta}(x_{t} | x_{t+1})$ matches\n$q(x_{t} | x_{t+1})$, while $x_{t+1} \\sim q(x_{t+1})$ in the training stage has no restriction. Thus, one may take\n$q(x_{t+1}) \\approx p_{\\theta}(x_{t+1})$, then in $\\tilde{L}_{t}$, $p_{\\theta}(x_{t} | x_{t+1})$ matches $\\tilde{q}(x_{t} | x_{t+1})$ leads $p_{\\theta}(x_{t}) \\approx q(x_{t}) =$\n$\\tilde{q}(x_{t})$, which mitigates the distribution mismatch problem, when minimizing such $\\tilde{L}_{t}$.\nUnfortunately, for each $t$, obtaining such specific $\\tilde{q}_{t}(x_{t+1}) = p_{\\theta}(x_{t+1})$ is computationally expensive\n(Li & van der Schaar, 2024), which prevents us using desired $\\tilde{q}_{t}(x_{t+1})$. However, we know $p_{\\theta}(x_{t+1})$\nis around $q(x_{t+1})$. Therefore, by borrowing the idea from DRO (Shapiro, 2017), for each $t$, we\npropose to minimize the maximal value of $\\tilde{L}_{t}$ over all possible $\\tilde{q}_{t}(x_{t+1})$ around $q(x_{t+1})$. This leads\nto a small $L_{t}$, as $p_{\\theta}(x_{t+1})$ locates around $q(x_{t+1})$, so that $\\tilde{q}$ is included in the \u201cmaximal range\u201d.\nTechnically, the DRO-based EBLO of (11) is formulated as follows. Here $p_{\\theta}(x_{t+1})$ is supposed in\n$\\mathcal{B}_{D_{KL}}(q(x_{t+1}), \\eta_{0})$, and it capatures the distributional robustness of $p_{\\theta}(x_{t} | x_{t+1})$ w.r.t. input $x_{t+1}$.\n$\\min_{\\theta} EL^{PRO}(\\theta) = \\min_{\\theta} \\sum_{t=0}^{T-1} \\sup_{\\tilde{q}_{t}(x_{t+1}) \\in \\mathcal{B}_{D_{KL}}(q(x_{t+1}), \\eta_{0})} D_{KL}(q_{t}(x_{t} | x_{t+1}) || p_{\\theta}(x_{t} | x_{t+1}));$\ns.t. $\\tilde{q}_{t}(x_{t}) = q(x_{t}).$\nHere $q_{t}(x_{t+1}) \\in \\mathcal{B}_{D_{KL}}(q(x_{t+1}), \\eta_{0})$ means $D_{KL}(q(x_{t+1}) || \\tilde{q}_{t}(x_{t+1})) \\leq \\eta_{0}$. By solving problem\n(12), if the desired $\\tilde{q}_{t}(x_{t+1}) = p_{\\theta}(x_{t+1})$ is in $\\mathcal{B}_{D_{KL}}(q(x_{t+1}), \\eta_{0})$, then the conditional probability\nin (12) transfers $x_{t+1} \\sim p_{\\theta}(x_{t+1})$ to target $x_{t} \\sim q(x_{t})$ is learned, which mitigates the distribution\nmismatch problem. The theoretical clarification is in the following Proposition proved in Appendix\nA.2, which indicates that small DRO loss (12) guarantees the quality of generated $x_{0}$.\nProposition 5. If $L^{PRO}(\\theta) \\leq \\gamma_{0}$ in (12) for all $t$, and $D_{KL}(q(x_{T}) || p_{\\theta}(x_{T})) \\leq \\eta_{0}$, then\n$D_{KL}(q(x_{0}) || p_{\\theta}(x_{0})) \\leq \\eta_{0}$.\nUp to now, we do not know how to compute the DRO-based training objective (12) we derived.\nFortunately, the following theorem corresponds (12) to a \"perturbed\" noise prediction problem similar\nto (5). The theorem is proved in Appendix A.2.\nTheorem 1. There exists $\\delta_{t}$ depends on $x_{0}$ and $\\epsilon_{t}$ makes (13) equivalent to problem (12).\n$\\min_{\\theta} \\sum_{t=0}^{T-1} \\mathbb{E}_{q(x_{0}),\\epsilon_{t}} ||\\epsilon_{\\theta}(\\sqrt{\\bar{\\alpha}_{t}}x_{0} + \\sqrt{1 - \\bar{\\alpha}_{t}}\\epsilon_{t} + \\delta_{t}, t) - \\epsilon_{t}||^{2},$\nThis theorem connects the proposed DRO problem (12) with noise prediction problem (13). Naturally,\nwe can solve (13), if we know the exact $\\delta_{t}$. Fortunately, we have the following proposition to\ncharacterize the range of $\\delta_{t}$, and it is proved in Appendix A.2.\nProposition 6. For $\\eta > 0$ and $\\delta_{t}$ in (13), $\\|\\delta_{t}\\|_{1} \\leq \\eta$ holds with probability at least $1-\\sqrt{2(1 - \\bar{\\alpha}_{t})/\\eta}$.\nThe proposition indicates that for any $\\delta_{t}$ depends on $x_{0}$, $\\epsilon_{t}$ in (13), it is likely in a small range\n(measured under any $\\ell_{p}$-norm, since they can bound each other in Euclidean space). Thus, to resolve\n(13) (so that (12)), we propose to directly consider the following adversarial training (Madry et al.,"}, {"title": "5 ADVERSARIAL TRAINING UNDER CONSISTENCY MODEL", "content": "Although the DPM generates high-quality target data $x_{0}$, the multi-step denoising process (6) requires\nnumerous model evaluations, which can be computationally expensive. To resolve this, the diffusion-\nbased consistency model (CM) is proposed in (Song et al., 2023). Consistency model $f_{\\theta}(x_{t}, t)$\ntransfers $x_{t} \\sim q(x_{t})$ into a distribution that approximates the target $q(x_{0})$. $f_{\\theta}$ is optimized by the\nfollowing consistency distillation (CD) loss\n$\\min_{\\theta} L^{CD}(\\theta) = \\sum_{t=0}^{T-1} \\mathbb{E}_{x_{t+1}\\sim q(x_{t+1})} [d (f_{\\theta}(\\Phi_{t}(x_{t+1}), t), f_{\\theta}(x_{t+1}, t + 1))],$\nwhere $\\Phi_{t}(x_{t+1})$ is a solution of a specific ordinary differential equation (ODE) ((37) in Appendix\nB) which is a deterministic function transfers $x_{t+1}$ to $x_{t}$, i.e., $\\Phi_{t}(x_{t+1}) \\sim q(x_{t})$, and $d(x, y)$ is a\ndistance between $x$ and $y$ e.g., $\\ell_{1}, \\ell_{2}$ distance.\nNext, we use the following theorem to illustrate that solving problem (15) indeed creates $f_{\\theta}(x_{t}, t)$\nwith distribution close target $q(x_{0})$. The theorem is proved in Appendix B.\nTheorem 2. For $L^{CD}(\\theta)$ in (15) with $d(\\cdot, \\cdot)$ is $\\ell_{2}$ distance, then $W_{1}(f_{\\theta}(x_{t}, t), x_{0}) \\leq \\sqrt{tL^{CD}(\\theta)}$.\nThough solving problem (15) creates the desired CM $f_{\\theta}$, computing the exact $\\Phi_{t}(x_{t+1})$ involves\nsolving an ODE as pointed out in Appendix B. Thus, in practice (Song et al., 2023; Luo et al., 2023),\nthe $\\Phi_{t}(x_{t+1})$ is approximated by a computable numerical estimation $\\hat{\\Phi}_{t}(x_{t+1}, \\epsilon_{\\phi})$ of it, e.g., Euler\n((42) in Appendix B.1) or DDIM (Song et al., 2023), where $\\epsilon_{\\phi}$ is a pretrained noise prediction model\nas in (5). Therefore, the practical training objective of (15) becomes\n$\\min_{\\theta} L^{CD}(\\theta) = \\mathbb{E}_{x_{t+1}\\sim q(z_{t})} [d (f_{\\theta} (\\hat{\\Phi}_{t}(x_{t+1}, \\epsilon_{\\phi}), t), f_{\\theta} (x_{t+1}, t + 1))].$\nIn (16), $\\hat{\\Phi}_{t}(x_{t+1}, \\epsilon_{\\phi})$ is an estimation to $\\Phi_{t}(x_{t+1})$, which causes an inaccurate training objective\n$L^{CD}$ in (16), compared with target $L^{CD}$ (15). Thus, this results in the distribution mismatch problem\nin CM, as in DPM of Section 4. However, similar to Section 4.2, if we train $f_{\\theta}$ with robustness to the\ngap between $\\hat{\\Phi}_{t}(x_{t+1}, \\epsilon_{\\phi})$ and $\\Phi_{t}(x_{t+1})$, the distribution mismatch problem in CM is mitigated.\nTechnically, suppose $\\Phi_{t}(x_{t+1}) = \\hat{\\Phi}_{t}(x_{t+1}, \\epsilon_{\\phi}) + \\delta_{t}(x_{t+1})$, we can consider minimizing the follow-\ning adversarial training objective of CM, if $\\|\\delta_{t}(x_{t+1})\\|\\leq \\eta$ uniformly over $t$, for some constant $\\eta$,"}, {"title": "6 EXPERIMENTS", "content": "In the standard adversarial training method like Projected Gradient Descent (PGD) (Madry et al.,\n2018), the perturbation $\\delta$ is constructed by implementing numbers (3-8) of gradient ascents to $\\delta"}]}