{"title": "Graph-Augmented Relation Extraction Model with LLMs-Generated Support Document", "authors": ["Vicky Dong", "Hao Yu", "Yao Chen"], "abstract": "This study introduces a novel approach to sentence-level relation extraction (RE) that integrates Graph Neural Networks (GNNs) with Large Language Models (LLMs) to generate contextually enriched support documents. By harnessing the power of LLMs to generate auxiliary information, our approach crafts an intricate graph representation of textual data. This graph is subsequently processed through a Graph Neural Network (GNN) to refine and enrich the embeddings associated with each entity ensuring a more nuanced and interconnected understanding of the data. This methodology addresses the limitations of traditional sentence-level RE models by incorporating broader contexts and leveraging inter-entity interactions, thereby improving the model's ability to capture complex relationships across sentences. Our experiments, conducted on the CrossRE dataset, demonstrate the effectiveness of our approach, with notable improvements in performance across various domains. The results underscore the potential of combining GNNs with LLM-generated context to advance the field of relation extraction.", "sections": [{"title": "1 Introduction", "content": "Among the fundamental tasks of NLP, relation extraction between entities from text holds paramount importance and plays an important role in diverse knowledge-dependent applications, such as machine reading comprehension (Li et al., 2022) and molecular property prediction (Zeng et al.). Identifying connections between entities within a single sentence poses several limitations:\nSentence-level (SL) models often miss broader contexts and struggle with relationships that extend beyond single sentences. They are less effective at identifying long-range dependencies, which is crucial for understanding complex sentence structures and distant entity relationships.\nIn response to these challenges, drawing inspiration from the Graph Aggregation-and-Inference Network (GAIN) model proposed by (Zeng et al., 2020a), we propose a graph-augmented relation model. The model aims to enrich word embeddings of entities by connecting their context and leveraging their interactions using an LLM-generated supplemental dataset via GCN.\nTo validate the performance and robustness of relation extraction models, particularly in capturing intricate relationships across multiple sentences, we will develop and evaluate our model using the cross-domain benchmark introduced by (Bassignana and Plank, 2022). Furthermore, an ablation study will be conducted to assess the impact of each component. We will measure the model's effectiveness using the Macro F1-Score and test its resilience against adversarial examples to ensure robust performance across diverse contexts and domains. The visualized pipeline is displayed in Figure 1.\nOur work includes the following contributions:\n\u2022 Using LLMs to generate supplement dataset for sentence-level relation extraction.\n\u2022 Integrating embedding via the GNN module that takes an input of entity relation graph.\n\u2022 Developing an evaluation framework that systematically assesses the performance of GNN-aided models across various embeddings and base models.\n\u2022 Initiating domain-specific analysis for models, pinpointing unique challenges across diverse domains and outlining future directions to refine model training and evaluation."}, {"title": "2 Relative Work", "content": "Current Sentence-level Relation Extraction (SLRE) methods, utilizing neural networks with architectures like CNNS, RNNs, and attention mechanisms(Zeng et al., 2014; Shen and Huang, 2016; Alt et al., 2019), have been significantly enhanced by the adoption of pre-trained models like BERT for their ability to leverage contextual information from extensive corpora. The introduction of a cross-domain benchmark for SLRE by (Bassignana and Plank, 2022) marks an effort to address the gap in RE evaluation, which has traditionally been confined to in-domain scenarios. However, these SLRE methods often fail to capture relationships that span across multiple sentences due to loss of critical contextual information. In contrast, Document-level Relation Extraction (DocRE) focuses on identifying relations between entities across entire documents (Huang et al., 2021; Zhang et al., 2021; Xu et al., 2022; Ru et al., 2021). Most existing DocRE approaches utilize dependency graphs to capture document-specific features, subsequently employing hierarchical inference networks (Tang et al., 2020) or graph neural networks (GNN) (Zeng et al., 2020b) for relational inference. However, these existing DocRE techniques could be improved by incorporating data augmentation strategies and enhancing the graph model structure.\nTo tackle sentence-level relation extraction problems, we have taken inspiration from document-level relation extraction and use graphs to establish relationships. In the following sections, we will delve deeper into these improvements."}, {"title": "3 Dataset", "content": "Our training, validating, and testing data are obtained from CrossRE (Bassignana and Plank, 2022), a sentence-level cross-domain relation extraction dataset introduced by Bassignana et al. This dataset is manually curated with hand-annotated relations spanning 17 relation types: PART-OF, PHYSICAL, USAGE, ROLE, SOCIAL, GENERAL-AFFILIATION, COMPARE, TEMPORAL, ARTIFACT, ORIGIN, TOPIC, OPPOSITE, CAUSE-EFFECT, WIN-DEFEAT, TYPE-OF, NAMED, and RELATED-TO. Some annotations are enriched with meta-data information, such as the explanation (Exp) for the choice of the assigned label, identification of syntax ambiguity (SA), and uncertainty of the annotator (UN)."}, {"title": "3.1 Supplemental Data", "content": "To provide context for the sentences and entities in the sentences, ChatGPT 3.5 is leveraged to generate supplemental information. Without modifying the JSON format of the existing data, newly generated sentences are appended at the end of each data element. The prompt, \"Generate some context for the given sentence: Original sentence while including the sentence in the paragraph generated. Keep the paragraph around 4 sentences.\", created a paragraph of 4 - 6 sentences."}, {"title": "3.2 Examples", "content": "Here is a sample data point from the CrossRE dataset:\nDomain: Artificial Intelligence\nSentence: For many years starting from\n\u2190\n\u2190\n1986, Miller directed the\ndevelopment of WordNet, a large\ncomputer-readable electronic\nreference usable in applications\nsuch as search engines.\nEntities:\n\u2190\n\u2190\n1. Miller (el.researcher)\n2. WordNet (e2.product)\n3. search engines (e3.product)"}, {"title": "3.3 Cross Relation Entity (CrossRE)", "content": "This dataset is derived from the CrossNER dataset (Liu et al., 2020), and contains six diverse text domains: news, politics, natural science, music, literature, and artificial intelligence, each with distinctive vocabularies, entity types, and relation distributions. The news domain is sourced from CONLL-2003 (Sang and Meulder, 2003), while the other five domains are collected from Wikipedia. The statistics of CrossRE are summarized in Table 1. The domains vary considerably in the amount and type of relations present in the data, posing a challenge for cross-domain generalization."}, {"title": "4 Methodology", "content": "Our project introduces a novel approach to enhance sentence-level RE by incorporating document-level insights. Our method seamlessly integrates the Graph Neural Network (GNN) model, proficient at capturing complex inter-sentence relations and synthesizing information across an entire document, with the sentence-level model proposed in CrossRE. The objective is to establish a robust RE system capable of effectively handling both in-domain and out-of-distribution evaluation setups, while simultaneously improving the accuracy and context-awareness of relation extraction through the synergistic integration of these two approaches."}, {"title": "4.1 LLMs for Support Document", "content": "Research has illuminated the capacity of LLMs to be finely tuned through prompts, enabling them to generate text that is both contextually rich and highly relevant, drawing from initial sentences (Raffel et al., 2020; Brown et al., 2020). Inspired by the work of (Meng et al., 2023), our approach leverages the remarkable adaptability and capabilities of LLMs to craft tailored support documents for sentences within the CrossRE dataset."}, {"title": "4.2 Graph Neural Network Augmented", "content": "GNNs are neural networks that operate on graph data. They have been developed for over a decade and recent developments have increased their capabilities and expressive power. (Sanchez-Lengeling et al., 2021) GNNs are used to process and understand graph-structured data, or information represented within a graph data structure. They provide a convenient way for node-level, edge-level, and graph-level prediction tasks.\nTo construct a comprehensive graph representation of these mini-documents, we iteratively identified words within each entity and any repeats in the supplemental document.\n\u2022 A document D made up of N sentences:\nD = {$i\\}_{i=1}^{N}\n\u2022 Each sentence $s_i$ consists of M words:\n$s_i = {w_j}\\_{j=1}^{M}$\n\u2022 A set of entities E in the document:\n$E = {e_i}\\_{i=1}^{P}$,\neach entity can be composed of more than one word.\n\u2022 Each entity $e_i$ has Q mentions:\n$e_i = {m_j}\\_{j=1}^{Q}$,\nwhere $m_j$ is any word within part of the entity to the document.\n\u2022 Nodes within the same sentence or entity will be connected."}, {"title": "4.3 Overall Architecture", "content": "The proposed architecture involves several steps. First, the sentences are tokenized and fed into a BERT model, which generates a contextualized embedding. Second, a graph representation is constructed using the word embeddings as node features and the adjacency matrix obtained from the previous steps. Finally, a Graph Convolutional Network (GCN) is used to aggregate the features of node neighbours.\n$\u03c5_v^{(l+1)} = \u03c3(\\frac{1}{\\sum_{u \\in N(v)} C_{vu}} \\sum_{u \\in N(v)} W^{(l)}h_u^{(l)})$\nwhere $h_v^{(l+1)}$ is the updated feature of node v at layer l+1, N(v) is the set of neighbors of v, $C_{vu}$ is a normalization constant, $W^{(l)}$ is a weight matrix for layer l, and \u03c3 is a non-linear activation function.\nThe graph structure represents the intricate interplay of entities and their relationships within each sentence. By extracting and embedding these sentence-level graphs, we create a rich tapestry of interconnected embeddings that encapsulate both the depth of individual sentences and the broader narrative flow of the document.\nAs introduced in the Figure 1. The overall architecture would be that first the input document is vectorized and is consumed by an encoder to generate a contextualized embedding. Second, construct the graph representation using the word embeddings following the steps mentioned above. Third, apply Graph Attention Network (GAT) (Brody et al., 2021) to aggregate mentions of each entity into a comprehensive graph embedding. Lastly, a classifier will perform link prediction between selected entities."}, {"title": "5 Experiment Setup", "content": ""}, {"title": "5.1 Environment Setup", "content": "All experiments were conducted on a single A100 80GB GPU on the conda created environment. The details of the environment can be seen in the code repository later."}, {"title": "5.2 Base Model", "content": "BERT (Bidirectional Encoder Representations from Transformers) bert-base-cased: Used in the CrossRE (Bassignana and Plank, 2022).\nROBERTa (Robustly Optimized BERT Approach) roberta-base: RoBERTa is a variant of the BERT model. During training, the model was trained on a larger dataset and with a more effective training procedure that utilizes dynamic masking. This helps the model learn robust and generalizable representations of words. (He et al., 2020)\nDeBERTa-v3 deberta-v3-base: DeBERTa-v3 improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. In DeBERTa V3, the efficiency of DeBERTa was further improved using ELECTRA-Style (Chi et al., 2021) pre-training with Gradient Disentangled Embedding Sharing. (He et al., 2020)"}, {"title": "6 Experiment Result", "content": "Our experimental results, as depicted in Table 2, offer a comprehensive overview of how different GNN-aided models, coupled with various embedding methods, perform across multiple domains. These results are pivotal in understanding the impact of integrating GNNs with different base models (bert-base-cased, roberta-base, and deberta-v3-base) and employing distinct embedding techniques (mean, max, tanh, and times) on the Macro F1 score, a critical metric for evaluating the performance of relation extraction models.\nThe bert-base-cased model, when augmented with the tanh embedding method, achieved a notable improvement, highlighting the efficacy of this particular combination. Conversely, the roberta-base model saw a general improvement across all embedding methods, suggesting robust compatibility with the GNN module. The deberta-v3-base model, however, presented a mixed outcome; while it showed an improvement with the times embedding method, it also indicated potential areas for optimization in other embedding techniques. Overall, the stronger the base model, the worse its performance in this task, as indicated by the average score of deberta-v3-base and roberta-base.\nA key insight from these results is the domain-specific performance variability. For instance, the News domain posed significant challenges, likely due to its imbalanced label distribution and the succinct nature of its content. This underscores the necessity for domain-specific strategies in model training and evaluation to address unique challenges and capitalize on domain-specific characteristics.\nMoreover, the results underscore the critical role of the embedding method in optimizing model outcomes. The variance in performance across different base models and embedding techniques suggests that a more tailored approach to embedding selection could further enhance model performance. This observation opens up avenues for future research to explore the synergy between base models and embedding methods to optimize relation extraction tasks."}, {"title": "7 Discussion", "content": "The methodology explored in this study demonstrates considerable potential across various domains of the CrossRE dataset. The enhancement of performance across all foundational models, driven by the integration of the GNN module, underscores the effectiveness and value of this method. our findings confirm the pivotal importance of the embedding method in enhancing the efficacy of model outcomes, marking a significant area where we have achieved advancements. The observation that various embedding techniques achieve optimal results with different foundational models indicates that adopting a more customized strategy for selecting embeddings could significantly boost model performance\nThe challenge presented by the News domain, characterized by its imbalanced label distribution and the concise nature of headlines and short reports, underscores the need for domain-specific considerations in both model training and evaluation. Furthermore, our study encountered challenges related to the use of GNNs, particularly the tendency for node representations to become overly similar (a phenomenon known as over-smoothing), and the static nature of GCNs, which are not well-suited to dynamic graphs that evolve over time.\nTo address these challenges and further advance the field, we propose several avenues for future research:\n\u2022 Exploring a variety of GNN architectures that can mitigate the issue of over-smoothing and are better equipped to handle dynamically changing graphs.\n\u2022 Fine-tuning the application of adversarial training to better match the unique characteristics of different base models and the specific requirements of various data domains.\n\u2022 Developing strategies to address the imbalance in label distribution, possibly through the use of specialized models that are tailored to the content and structural peculiarities of specific domains."}, {"title": "8 Conclusion", "content": "In conclusion, this study presents an innovative approach to relation extraction that leverages the strengths of Graph Neural Networks (GNNs) and Large Language Models (LLMs) to enhance sentence-level analysis. Our experimental evaluations have demonstrated the promise of this model, showcasing its ability to improve performance across a variety of domains. As we continue to refine our methodology and explore new avenues for research, we are optimistic about the potential of our model to contribute significantly to the field of relation extraction."}]}