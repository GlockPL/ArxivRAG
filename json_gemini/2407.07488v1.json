{"title": "FUNAvg: Federated Uncertainty Weighted Averaging for Datasets with Diverse Labels", "authors": ["Malte T\u00f6lle", "Fernando Navarro", "Sebastian Eble", "Ivo Wolf", "Bjoern Menze", "Sandy Engelhardt"], "abstract": "Federated learning is one popular paradigm to train a joint model in a distributed, privacy-preserving environment. But partial annotations pose an obstacle meaning that categories of labels are heterogeneous over clients. We propose to learn a joint backbone in a federated manner, while each site receives its own multi-label segmentation head. By using Bayesian techniques we observe that the different segmentation heads although only trained on the individual client's labels also learn information about the other labels not present at the respective site. This information is encoded in their predictive uncertainty. To obtain a final prediction we leverage this uncertainty and perform a weighted averaging of the ensemble of distributed segmentation heads, which allows us to segment \"locally unknown\" structures. With our method, which we refer to as FUNAvg, we are even on-par with the models trained and tested on the same dataset on average. The code is publicly available.", "sections": [{"title": "1 Introduction", "content": "The ability of deep neural networks to accurately segment anatomical structures promises precise quantitative analysis and clinical diagnosis, and has the potential to significantly improve medical decision making [24]. However, due to the required effort for accurately labelling medical images every institution only creates annotations for their particular research endeavour [22]. Despite being from\nthe same modality and capturing a joint field of view, the cumulative information is hardly exploited. This leads to partially annotated, distributed datasets in a sense that some structures might be segmented in one dataset (e.g. liver, kidney) and other structures in the other dataset (e.g., spleen, spine) that can not be gathered on a central server because of privacy constraints.\nThe standard workflow includes training a model on dataset A that has the structure of interest annotated and applying it to dataset B, which due to domain gaps leads to subpar performance. Recently, self-supervised methods have gained attention which use an unsupervised pre-training on a large corpus of data similar to dataset B, where the model is usually trained to minimize a reconstruction loss [7]. Federated Learning (FL) is one renowned method that reverts the common paradigm of central data storage to circumvent privacy constraints [20]. In FL the model is sent to each data holding institution and subsequently averaged on a central server during model training. Our method combines both approaches by jointly learning all annotated structures across a multitude of datasets while showing that not-annotated information in the data is learned as well and therefore can be explored for improving predictions during inference similar to self-supervised methods. The approach most related to ours, MultiTalent [24], also employs training multiple segmentation heads across different datasets but discards the possible information learned from not-annotated structures, partly because they do not employ a Bayesian approach [24]."}, {"title": "2 Related Work", "content": "Learning from Partially Annotated Datasets. Lately, there has been an increasing amount of studies focusing on multi-organ segmentation using data that is partially labeled. One line of research leverages the inherent homogeneous anatomy of the human body in terms of shape, size, and locations of anatomical struc-"}, {"title": "Bayesian Federated Learning", "content": "In contrast to the point estimate used in frequentist deep learning, the Bayesian approach models the parameters with a distribution. Mathematically a prior distribution \\(p(\\theta|\\alpha)\\) is placed over the weights \\(0\\) of a neural network, governed by a hyperparameter \\(a\\). Our interest lies in the posterior after observing some data \\(D\\) \\(p(0|D,a) = p(D|\\theta,\\alpha)p(\\theta|\\alpha)/p(D)\\). Unfortunately, this distribution is not tractable, but can be approximated with Bayesian inference techniques such as variational inference (VI).\nIn federated learning, numerous clients collaboratively train a unified global model, while adhering to data privacy as no data is shared with a central server managing the training process called federated averaging (FedAvg) [20]. For a given set of clients, let \\(D_i = \\{(x_i, y_i)\\}\\), represent the data of client \\(i\\) and \\(\\theta\\) the weights of the global model. At the beginning of each training round the model"}, {"title": "3 Methods", "content": "Uncertainty. To reduce computational load we opt for the practical Bayesian technique of Monte Carlo (MC) dropout [5]. Using dropout during training and testing has been shown to approximate the true posterior, while not introducing more computational load to capture uncertainty. Uncertainty is divided into aleatoric (data dependent) and epistemic (model dependent) uncertainty. We follow [14] and define uncertainty as\n\\(U = \\frac{1}{T} \\sum_{t=1}^{T} [diag(p_t) - p_t^2] + \\frac{1}{T} \\sum_{t=1}^{T} (p_t - \\bar{p})^2\\) , (1)\nwhere T is the number of MC sampling steps, \\(\\odot\\) denotes the outer product, f the network with parameters \\(\\theta_t\\) in step t for input \\(x^*, \\bar{p} = 1/T \\sum_{t=1}^T p_t\\) and \\(p_t = p(\\theta_t) = Softmax\\{f_{\\theta_t} (x^*)\\}\\). The first part captures the inherent variability in the data that cannot be reduced even if more data were to be collected, while the second term captures the variance in the model's output that can potentially be reduced with more data.\nNeural networks tend to produce overconfident predictions, meaning the predictive uncertainty is smaller than the predictive error, this is also known as miscalibration [16] and can be expressed with\n\\(E_p [|P (y = y|p = q)-q|], \\forall q \\in [0,1]\\), (2)\nwhich quantifys the expectation of the difference between predicted softmax likelihood \\(\\hat{p}\\) and accuracy and can be approximated by the Expected Calibration"}, {"title": "Uncertainty Weighted Averaging", "content": "For training a model across partially labelled clients each client's model gains a separate segmentation head i.e. the last layer. The backbone of the model until the last layer is trained in a federated fashion. This enables us to learn each client's labels while simultaneously perform feature extraction across all tasks similar to [24]. To obtain the final prediction we average the predictions from all classifiers per channel and divide each channel by the number of clients possessing annotations of that particular label \\(p = \\frac{1}{K} \\sum_{k=1}^K p_k\\) with \\(k\\) \\([k_1,k_2, ...] \\in R^{k_i}\\) the number of clients that particular label was present and \\(p_k \\in R^{k_i\\times H\\times W}\\) the predicted per-channel softmax probability of each segmentation head.\nSurprisingly, we noticed that although all classifiers are not aware of at least two labels present only at other clients, these unannotated structures in their local dataset were visible in the classifier's predictive uncertainty (see Fig. 5). The segmentation heads seem to have learned that there might be a structure but did not have the ability to assign a specific label to it due to predefined channels. We interpret the uncertainty as the probability of \"something\" being present, while the background represents the probability for \"nothing\". Consequently, we reduce the likelihood for a pixel being background by the probability of the presence of a structure encoded in the uncertainty. In areas where u is large, the probability for a structure being present is high. We thus multiply the background channel (the first channel 0) with the inverse of the uncertainty \\(p_0 = (1 - u)p_0\\). Fig. 2 shows a graphical explanation of the proposed averaging procedure for one specific pixel and for the whole image. Our approach represents a self-supervised method for reweighting the predictive probability by the self-supervised learned un-annotated structures for each segmentation head, while still obtaining an unambiguous prediction due to the preservation of probabilities. To ensure the probabilities for each pixel sum to one we must adjust the MultiTalent pipeline to be trained with cross entropy followed by a softmax. This enables us for each pixel to have the softmax probability of \"something\" compared to the probability to \"nothing\" that sums to one. We control for calibration of our probabilities in terms of ECE."}, {"title": "Data and Model Architecture", "content": "In total we used 8 datasets for training comprising a total of 2413 3D images across 12 unique classes and 2 datasets for testing with 240 samples. The training datasets are further split 80/20% for training"}, {"title": "4 Results", "content": "We performed an 80-20% train-test split at each client. For a baseline, we trained models on single clients (no FL) and evaluated them in the intra-client scenario on the same client (row 1 in Tab. 1) and inter-client scenario on the test split of all other clients where the target label was available (row 2 in Tab. 1). We then performed training similar to MultiTalent [24], where each dataset obtained a separate segmentation head. We performed vanilla averaging and uncertainty weighted averaging for the centralized (Cen Avg and CUNAvg, rows 3 and 4) and federated setting (Fed Avg and FUNAvg, rows 5 and 6).\nDespite the challenges of different quantities of annotated labels, CT scans, and fields of view, federated training was successful for all structures present at any client. Our method (DICE=84.77) is on par with the models trained and evaluated on the same dataset (84.79) and outperforms the centralized setting (81.74) when uncertainty is utilized to enhance predictions. This improvement is especially high in scenarios with limited data, such as Learn2Reg [27],"}, {"title": "5 Discussion", "content": "The information for segmenting all structures across the datasets is encoded in the (federated trained) backbone. Unable to assign a previously unseen label in its fixed number of channels, the segmentation head only learns the presence of some structures encoded in the uncertainty, without being explicitly trained for it. For example, Fig.5 illustrates how a network with a head trained on the BCV Cervix[15] dataset can segment organs well in the uncertainty when applied to\nthe AMOS [9] dataset, which contains none of the labels used for training. In federated learning, the segmentation heads with non-overlapping labels may use the same feature maps from the backbone, whereas in the centralized case, the heads tend to use different feature maps. The improvement of our proposed FUNAvg is larger for underrepresented labels (see Fig. 4 and Tab. 2 in the Appendix). We believe that predictions for these structures are encoded in the uncertainties of the other clients, whereas for labels present at many sites, predictions are already encoded in the output. For these labels, our proposed method may lead to slightly worse results. Future work could explore whether uncertainty can be used more targeted during averaging in such cases."}]}