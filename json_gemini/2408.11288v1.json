{"title": "Applying and Evaluating Large Language Models in Mental Health Care: A Scoping Review of Human-Assessed Generative Tasks", "authors": ["Yining Hua", "Hongbin Na", "Zehan Li", "Fenglin Liu", "Xiao Fang", "David Clifton", "John Torous"], "abstract": "Large language models (LLMs) are emerging as promising tools for mental health care, offering scalable support through their ability to generate human-like responses. However, the effectiveness of these models in clinical settings remains unclear. This scoping review aimed to assess the current generative applications of LLMs in mental health care, focusing on studies where these models were tested with human participants in real-world scenarios. A systematic search across APA PsycNet, Scopus, PubMed, and Web of Science identified 726 unique articles, of which 17 met the inclusion criteria. These studies encompassed applications such as clinical assistance, counseling, therapy, and emotional support. However, the evaluation methods were often non-standardized, with most studies relying on ad-hoc scales that limit comparability and robustness. Privacy, safety, and fairness were also frequently underexplored. Moreover, a reliance on proprietary models, such as OpenAI's GPT series, raises concerns about transparency and reproducibility. While LLMs show potential in expanding mental health care access, especially in underserved areas, the current evidence does not fully support their use as standalone interventions. More rigorous, standardized evaluations and ethical oversight are needed to ensure these tools can be safely and effectively integrated into clinical practice.", "sections": [{"title": "1 INTRODUCTION", "content": "Mental health issues have been a concern of global health ever since they recognized the profound impact on individuals and societies, and the urgency has only grown in recent years. Nearly 1% of all global deaths annually are now due to suicide, with approximately 800,000 people dying by suicide each year\u00b9. In the United States alone, the annual public mental health expenditure exceeded $16.1 billion, including a $2.21 billion budget for the National Institute of Mental Health (NIMH) and $13.9 billion on mental healthcare\u00b2. Still, even in the United States, the psychiatry workforce is projected to face a pressing shortage through 2024, with a potential shortfall of 14,280 to 31,091 psychiatrists3,4. And in low-and-middle income countries, the situation is even worse with up to 85% of people there still receive no treatment for their mental health5.\nIn response to the growing mental health crisis and the projected shortage of mental health professionals, artificial intelligence (AI)-driven mental health applications like chatbots are emerging as vital tools to bridge the treatment gap. These technologies offer scalable, accessible, and cost-effective support, particularly in areas where traditional mental health services, including psychiatric care, are insufficient"}, {"title": "2 BACKGROUND", "content": "or unavailable. As of 2023, the global market for mental health apps has grown rapidly, with over 10,000 apps collectively serving millions of users. AI-driven platforms are increasingly incorporating psychiatric assessments, medication management reminders, and monitoring tools that assist in the management of conditions such as depression, anxiety, and bipolar disorder. Studies suggest these tools can help reduce symptoms and improve patient outcomes, making them a promising avenue for addressing mental health challenges, especially in regions with limited access to psychiatric professionals, and they are increasingly being integrated into broader mental health care strategies to help meet the growing demand7,8.\nThe introduction of large language models (LLMs) like OpenAI's ChatGPT, Google's Bard10, and Anthropic's Claude\u00b9\u00b9 marks a transformative advancement in AI-driven mental health care, offering capabilities far beyond those of earlier AI tools. Unlike previous models, which were limited to scripted interactions and specific tasks, LLMs can engage in dynamic, context-aware conversations that feel more natural and personalized via generating human-like conversations. This allows them to provide tailored emotional support, detect subtle cues indicating changes in mental health, and adjust their guidance to meet individual user needs in generative tasks. Increasingly, research is exploring anthropomorphic features such as empathy, politeness, and other human-like traits in these models to enhance their effectiveness in delivering more realistic and supportive mental health care12.\nDespite the promising potential, these tools are still in the early stages of development and evaluation. Users often do not understand the models they are interacting with, including the limitations and biases inherent in the Al's design. Unfortunately, there is currently no standardized framework for evaluating the effectiveness and safety of these models in mental health applications. Many studies, including those focused on evaluating LLMs, often develop their own metrics and methods, leading to inconsistent and sometimes unreliable results. The lack of standardized evaluation hinders the comparison of models or assess their true impact on mental health outcomes. Concerns about data privacy, the potential for misuse, and the ethical implications of relying on Al for sensitive mental health care decisions further underscore the need for rigorous oversight. Considering these promises and challenges, a scoping review of the current applications of LLMs in mental health care is essential from the perspective of psychiatrists and clinical informaticians. Our review aims to synthesize existing research with a focus on clinical relevance, identify gaps in understanding from a mental health practice standpoint, and provide clear guidelines for future development and evaluation of these technologies in real-world settings."}, {"title": "2.1 Subfields of Mental health care and the potential of generative AI", "content": "The potential of generative Al in mental health care is broad given the many different treatment approaches employed today for care delivery. These approaches generally fall into three main categories: psychotherapy, psychiatry, and general mental health support. Psychotherapy is one of the most common forms of mental health care. However, access to psychotherapy is often limited by factors like a shortage of therapists, long wait times, and high costs. Generative AI could help address these issues by offering on-demand support, providing education about mental health, and guiding people through therapeutic exercises when they can't see a therapist in person. Psychiatry focuses on the medical side of mental health care, including diagnosing, treating, and preventing mental disorders. But like psychotherapy, psychiatry also faces challenges, particularly a shortage of psychiatrists. Generative AI could support psychiatrists by helping monitor patients' symptoms, reminding them to take their medication, and providing initial assessments, which could reduce the strain on the healthcare system and improve patient outcomes. General mental health support includes a wide range of services designed to promote mental well-being and prevent mental health problems. This might include community programs, self-help resources, peer support networks, and public health initiatives. These services are important for early intervention, managing stress, and preventing more serious mental health issues from developing. However, many people don't take advantage of these resources, often because of stigma, lack of awareness, or insufficient availability. Generative AI could help make these resources more accessible by providing anonymous, personalized support through chatbots and apps"}, {"title": "2.2 Large language models (LLMs)", "content": "Although LLMs gained widespread attention with the release of OpenAI's ChatGPT-4, the concept has existed for some time, though there is no single unified definition. In the natural language processing (NLP) community, LLMs are generally understood as large generative AI models capable of producing text by predicting the next word or phrase based on vast amounts of training data. NLP has evolved drastically over time, with early models being task-specific and limited in their ability to understand context and nuance. The introduction of advanced deep learning frameworks marked a major improvement, as these models are designed to better capture contextual language meaning. However, they still struggled with generating coherent, contextually appropriate text over longer conversations, which is crucial for mental health applications. LLMs have advanced this further by leveraging large datasets and transformer architectures to predict and generate highly coherent and context-aware text. This enables them to mimic human conversation, making them valuable for creating therapeutic content, offering psychoeducation, and simulating therapy sessions\u2014important tools for expanding access to mental health care. For clinicians, LLMs offer promising tools to support mental health services by providing personalized, scalable interactions. For example, it's important to recognize that most current LLMs are general models and do not perform as well as specialized pre-trained models for domain- specific tasks such as prediction and classification. For example, Bidirectional Encoder Representations from Transformers (BERT) models, which model word segments (tokens) using both the segments before and after them, are more accurate and efficient for these purposes."}, {"title": "3 METHODS", "content": "We adhered to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) 2020 guidelines13 to ensure a transparent and reproducible search process (Figure 1). Our search included four databases: APA PsycNet, Scopus, PubMed, and Web of Science. To ensure comprehensiveness, we employed a combination of generative Al keywords and LLM keywords, and used shortest matching string to capture all lexical variations. Our search query was as follows, with different variations used across database platforms (detailed in Appendix A):\n(\"generative artificial intelligence\" OR \"large language models\" OR \"generative model\" OR \"chatbot\") AND (\"mental\" OR \"psychiatr\" OR \"psycho\" OR \"emotional support\")\nWe conducted the search in the title or abstract of articles, covering the period from January 1, 2020, to July 19, 2024, without language restrictions. The search results included 259 articles from PubMed, 444 articles from Scopus, 1 article from APA PsycNet (PsychInfo and PsycArticles), and 500 articles from Web of Science. The initial search yielded 1,204 articles, with 14 additional articles identified from sources such as Google Scholar, the ACM Digital Library, and reverse referencing. After removing 492 duplicates, we were left with a total of 726 unique articles.\nWe applied the following inclusion criteria to select studies for our review: first, the study must involve using an LLM to generate responses (generative task); second, the study must focus specifically on mental health care, distinguishing it from studies in related fields like psycholinguistics; third, the study must involve human participants prospectively testing the LLM, ensuring the assessment of real-world applicability and therapeutic effectiveness. An LLM is defined as \u201ctransformer-based models with more than ten billion parameters, which are trained on massive text data and excel at a variety of complex generation tasks.\" in this study following a highly cited review from the NLP community14. We excluded reviews, meta-analyses, and clinical trials from our selection. Then we future removed seven studies not meeting our inclusion criteria upon full-text review. The result analysis review include 17 articles, with 16 full-text-length papers and one brief communication paper. Screening, data extraction and synthesis details are detailed in Appendix B."}, {"title": "4 RESULTS", "content": "The standards used to target mental disorders within these studies vary widely. Some studies focus on clinically confirmed diagnoses, relying on established criteria like those found in the DSM-5. Others define mental health conditions more arbitrarily, using constructs identified through keywords or self- reported measures. Therefore, we categorized the targeted mental health disorders into two groups: 1) established constructs, which are based on standard diagnostic criteria and validated clinical knowledge; and 2) custom constructs, which lack a clear definition or a standard, validated method for assessment or diagnosis.\nAs shown in Table 1, ten studies out of the 17 reviewed included established constructs 15\u201324, while eight involved custom constructs25\u201332 . Depression and suicidality were the most explored mental health constructs. Two studies adopted the Patient Health Questionnaire-9 (PHQ-9)21 and the Center for Epidemiologic Studies Depression Scale for Children (CES-DC)33 as inclusion criteria and outcome measures 18,23, while another study used PHQ-9 as an exclusion criterion 30. Studies assessing suicidality also adopted the PHQ-9, either as an inclusion21 or exclusion criterion30. Other clinically valid disorders include anxiety16,18,22, Attention-Deficit/Hyperactivity Disorder (ADHD)17,24, bipolar disorder23, cognitive distortion19,20, loneliness21, and stress18 One study evaluated GPT's performance on 100 clinical case vignettes of different disorders, comparing GPT against psychiatrists across different evaluation constructs15, covering a range of disorders."}, {"title": "4.2 Applications and Model Information", "content": "Existing generative applications of LLMs in mental health care can be categorized into six main types based on model functionalities: Clinical Assistant23,31, Counselling25,26,34, Therapy22,22,24, Emotional Support18,21,27,28, Positive Psychology Intervention16,19,20, and Education 17,35. Among them, the Clinical Assistant application includes attempts to develop and evaluate LLMs for supporting mental health professionals by generating management strategies and diagnoses for psychiatric conditions. In the Counselling category, LLMs are used to interact with participants, such as engaging Spanish teenagers in discussions about mental health disorders17 and providing relationship advice in single-session"}, {"title": "4.3 Evaluation methods, scales, and constructs", "content": "Constructs and scales are essential in systematically measuring mental health interventions, particularly when evaluating new technologies. Constructs refer to specific concepts or characteristics being measured, such as privacy, safety, or user experience. They provide a clear focus for what is being assessed in a study, which is crucial for ensuring that the evaluation is meaningful and relevant. Scales, in turn, offer a structured and standardized approach to quantify these constructs. This standardization is necessary for consistency across different studies, allowing researchers to compare results and draw more robust conclusions.\nGiven the diversity in how constructs are defined and measured across studies, it is important to use a framework that can harmonize these variations. Therefore, we employ a hierarchical pyramid framework that categorizes constructs into three levels: (1) Safety, Privacy, and Fairness; (2) Trustworthiness and Usefulness; and (3) Design and Operational Effectiveness. The pyramid framework ensures that each level of evaluation builds on the previous one. For example, without ensuring that an intervention is safe, it would be premature to evaluate its usability or cost- effectiveness.\nAmong the studies reviewed, those that involved direct participant feedback (n=5)16,19\u201322 generally focused on user-centric constructs. These studies typically involved larger sample sizes ranging from 28 to over 15,000 participants, assessed constructs such as accessibility, ease of use, personalized engagement, user experience, and cost-effectiveness. They provide direct insights into how users experience of LLMs are in real-world settings. On the other hand, studies that focused on evaluating LLM performance\u2014typically involving expert assessments\u2014concentrated more on foundational and core efficacy constructs. These studies often used smaller sample sizes, ranging from 12 to 100 cases, focusing on technical or functional aspects of the LLMs. Additionally, one study20 designed and incorporated automated metrics for Rationality, Positivity, and Empathy, using NLP models to evaluate LLM outputs. These automated evaluations offer a more detailed, algorithmic perspective on the LLM's performance, complementing human judgments.\nThe use of scales remains a problem in the mental health field. We observe that 12 studies developed their own scales15,17,18,20,22\u201324,30,31,34,36 or adapted existing ones for their evaluations. Most of the studies using established scales were those directly measuring patient outcomes, such as anxiety, where the General Anxiety Disorder-7 (GAD-7) was employed16,18. However, many articles that created their own"}, {"title": "5 DISCUSSION", "content": "Our review suggests that there is great enthusiasm for LLM-based mental health interventions and that many teams are creating interesting and unique applications. We found these chatbots already developed to serve as clinical assistants, counselors, emotional support vehicles, and positive psychology interventions. However, the evaluation of LLM-based mental health interventions is hindered by the lack of unified guidelines for scale development and reporting. While this is appropriate for feasibility testing, it belies the ability to understand the actual clinical potential of these new chatbots. With the majority of studies using non-validated, ad-hoc scales without addressing their validity and reliability, there is the opportunity for the next wave of research to better support the credibility and the need for guidelines to standardize reporting and scales used in this field.\nWhile effective evaluation is still nascent, results, as shown in the table highlight that the current focus ignores foundational privacy and safety concerns. LLM-based mental health chatbots are multifaceted with privacy, technical, engagement, legal, and clinical considerations. Our team recently introduced a simplified framework to unify these many evaluations, suggesting that safety and privacy should be the foundation of any evaluation41. This is not to minimize the value of evaluation of design and effectiveness (level 3) and usefulness and trustworthiness (level 2), but rather that such should not be at the expense or priority over safety, privacy, and fairness (level 1). Without these level 1 considerations, LLM-based mental health interventions may be impressive but unfit for healthcare or clinical use.\nOur results also show that the focus of current LLMs today is directed more at patients and less at clinicians. This approach is logical as direct to consumer/patient approaches often avoid complex healthcare regulations and clinical workflow barriers. However, this approach also risks fragmenting the potential of LLM-based mental health interventions to influence care as there is strong evidence that clinician engagement is required for more sustained and impactful patient use with any digital technology12. There is strong data that clinicians are interested in using LLMs in care, but first require and are asking for more training and support on how to use these in care.\nThe LLMs reviewed in this paper target a wide variety of disorders. Over half of the studies reviewed included clinically valid disorders, with other studies targeted general mental health constructs. Overall most studies did not offer sufficient details on the target population, for example one study specified a population of children and adolescents, ages between 12 and 18 years old\u00b97 and the difference between mental health risk factors versus mental health conditions was also poorly delineated. Given that only one study emphasized data security, with conversations proceeding through a HIPAA-compliant environment22, the lack of more clinical use cases is perhaps appropriate.\nAnother issue is the dependence on proprietary models, such as OpenAI's GPT-3.5 and GPT-4, in many mental health applications. This reliance raises concerns about transparency and customization, as the use of closed-source models limits external validation of reliability and safety, crucial in mental health research. Promoting the use of open-source models and improving transparency can enhance the scientific and ethical standards of these applications.\nTo advance the scalability and scientific rigor of LLM-based mental health interventions, the research community must also adopt more controlled methodologies. Some studies, particularly those utilizing ChatGPT, rely on the website interface for research purposes. While this approach is convenient, it should be discouraged for rigorous scientific investigations. Research should be conducted using the API, where hyperparameters such as the \u201ctemperature\" can be controlled, ensuring replicability of the results. The website interface should primarily be used for testing third-level constructs such as Design and Operational Effectiveness and potentially assessing the safety and transparency of the user-facing system. For studies focusing on constructs like Beneficence and Validity, using APIs that allow control over the model's reproducibility is crucial. This approach ensures that findings are consistent and can be reliably replicated, which is essential for advancing the field of mental healthcare applications of LLMs.\nFinally, the global applicability of LLM-based mental health tools warrants careful consideration. Public health, especially mental health care, is a global issue, and it's crucial to develop and deploy\""}, {"title": "6 FUTURE RESEARCH", "content": "Future directions for LLMs in mental health care should prioritize expanding their applications beyond narrow prediction tasks, especially given that only 17 studies over the past five years have explored generative tasks prospectively involving human participants for evaluation. Human-centered studies provide critical insights into how LLMs interact with individuals, particularly in sensitive contexts like mental health care, where nuances in communication and emotional understanding are vital. To improve the rigor and credibility of LLM-based mental health interventions, studies should prioritize the development of standardized evaluation guidelines. These guidelines should include the creation of validated and reliable scales that can be universally applied across studies, ensuring consistent and accurate assessments of clinical potential. To enhance transparency and overcome the limitations of proprietary models, researchers should move away from using web interfaces like ChatGPT for rigorous scientific studies, as these platforms lack the necessary controls for reproducibility. Instead, APIs and locally deployable models that allow for control over hyperparameters should be used to ensure the replicability of the results. Finally, studies focused on critical constructs such as beneficence, validity, and reproducibility should adopt rigorous evaluation methods and widely validated scales, moving beyond metrics like recall and F1 scores, to establish a more comprehensive understanding of model accuracy and clinical relevance."}, {"title": "7 CONCLUSION", "content": "While LLMs show considerable promise for enhancing mental health care accessibility, particularly in underserved areas, there is currently insufficient evidence to fully support their use as standalone interventions. The field faces substantial challenges, including a lack of standardized evaluation methods, potential risks related to privacy and safety, and ethical concerns that must be addressed. Without rigorous validation and closer integration with established clinical practices, there is a risk that these tools could fall short of their potential, diverting users from proven, evidence-based treatments. Moving forward, the focus must be on developing robust, ethically sound frameworks to ensure that LLMs contribute meaningfully and safely to mental health care."}]}