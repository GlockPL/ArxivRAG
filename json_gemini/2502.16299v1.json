{"title": "A CALIBRATION TEST FOR EVALUATING SET-BASED EPISTEMIC UNCERTAINTY REPRESENTATIONS", "authors": ["Mira J\u00fcrgens", "Thomas Mortier", "Viktor Bengs", "Eyke H\u00fcllermeier", "Willem Waegeman"], "abstract": "The accurate representation of epistemic uncertainty is a challenging yet essential task in machine learning. A widely used representation corresponds to convex sets of probabilistic predictors, also known as credal sets. One popular way of constructing these credal sets is via ensembling or specialized supervised learning methods, where the epistemic uncertainty can be quantified through measures such as the set size or the disagreement among members. In principle, these sets should contain the true data-generating distribution. As a necessary condition for this validity, we adopt the strongest notion of calibration as a proxy. Concretely, we propose a novel statistical test to determine whether there is a convex combination of the set's predictions that is calibrated in distribution. In contrast to previous methods, our framework allows the convex combination to be instance dependent, recognizing that different ensemble members may be better calibrated in different regions of the input space. Moreover, we learn this combination via proper scoring rules, which inherently optimize for calibration. Building on differentiable, kernel-based estimators of calibration errors, we introduce a nonparametric testing procedure and demonstrate the benefits of capturing instance-level variability on of synthetic and real-world experiments.", "sections": [{"title": "1 Introduction", "content": "In supervised machine learning, it has become more and more important to not only have accurate predictors, but also a reliable quantification of predictive uncertainty, i.e., the learner's uncertainty in the outcome $y \\in \\mathcal{Y}$ given a query instance $x \\in \\mathcal{X}$ for which a prediction is sought. Predictive uncertainty is often divided into aleatoric and epistemic uncertainty [40, 17, 21, 12], where the former corresponds to uncertainty that cannot be reduced by further information (e.g. more training data), as it originates from inherent randomness in the relationship between features $X$ and labels $Y$. Therefore, we assume that the ground truth is a conditional probability distribution $P_{Y|X}$ on $\\mathcal{Y}$, i.e. given an input sample $x \\in \\mathcal{X}$, each outcome $y$ has a certain probability $P(y|x)$ to occur. Even with perfect knowledge about the underlying data-generating process, the outcome cannot be predicted with certainty. However, in a typical machine learning scenario, the learner does not know $P_{Y|x}$. Having a space of possible hypotheses, an estimator of the underlying probability distribution $f$ within this space typically consists of a mapping $f : \\mathcal{X} \\rightarrow \\mathcal{P}(\\mathcal{Y})$, where"}, {"title": "2 Calibration of first-order predictors", "content": "This section reviews common calibration metrics that have been introduced for evaluating distribution calibration in multi-class classification. We also discuss differentiable estimators of these metrics, and statistical tests that assess calibration for a single probabilistic model. The extension to sets of probabilistic models will be discussed in Section 3."}, {"title": "2.1 General setting", "content": "We assume a multi-class classification setting with feature space $\\mathcal{X} \\subseteq \\mathbb{R}^d$ and label space $\\mathcal{Y} = \\{1, ..., K\\}$, consisting of $K$ classes. Let $X$ and $Y$ be random variables that are distributed according to a joint distribution $P_{X,Y}$ on $\\mathcal{X} \\times \\mathcal{Y}$. A probabilistic model can be represented as $f : \\mathcal{X} \\rightarrow \\mathcal{P}(\\mathcal{Y})$, which is a map from the feature space to the space of probability distributions over the output space $\\mathcal{Y}$. For multi-class classification problems with $K$ classes, $\\mathcal{P}(\\mathcal{Y}) = \\Delta_K$ (the $(K - 1)$-dimensional probability simplex. Roughly speaking, a classifier is calibrated if its outputs coincide with probability distributions that match the empirical frequencies observed from realized outcomes. In multi-class classification, one distinguishes between different types of calibration. Confidence calibration [34, 14, 24] only analyses the confidence score, i.e., the probability of the predicted class being calibrated. It is therefore the weakest notion of calibration. Distribution calibration [5, 42], which will be the focus in this paper, analyses whether all class probabilities are calibrated. Instead of only requiring calibrated marginal probabilities as in the definition of classwise calibration [48], it is defined via conditioning on the full probability vector; hence, it is the strongest notion of calibration.\nDefinition 1. A probabilistic multi-class classifier with output vector $f(X) \\in \\Delta_K$ is calibrated in distribution if for all $k \\in \\mathcal{Y} = \\{1, . . ., K\\}$ it holds that\n$\\mathbb{P}(Y = k | f(X) = s) = s_k,$\nwith $s_k$ being the $k$-th component of the probability vector $s \\in \\Delta_K$.\nTo evaluate calibration, one typically makes use of calibration errors. In general, these errors measure the (average) discrepancy between the conditional probability distribution $P_{Y|f(X)}$ and the predictions $f(X)$. Different calibration errors have been introduced to measure distribution calibration. One can in general differentiate between calibration errors defined as an expectation over a divergence $d : \\Delta_K \\times \\Delta_K \\rightarrow [0, \\infty)$ between $f(X)$ and $P_{Y|f(X)}$ [36, 37]:\n$CE_d(f) = \\mathbb{E}[d(f(X), P_{Y|f(X)})],$  (1)\nand calibration errors defined via integral probability metrics [30], also called kernel calibration errors [24, 45, 27]:\n$CE_\\mathcal{H}(f) = \\sup_{\\Phi \\in \\mathcal{H}} |\\mathbb{E}[\\Phi(f(X), Y)] - \\mathbb{E}[\\Phi(f(X), Z)]|,$  (2)\nwhere $\\mathcal{H}$ is typically chosen to be the unit ball of a reproducing kernel Hilbert space (RKHS) $\\mathcal{H}$, and $Z$ is a conditioning variable that is assumed to be calibrated."}, {"title": "2.2 Overview of calibration errors and estimators", "content": "We now introduce the calibration estimators that we later use for our test. For a given dataset $\\mathcal{D} = \\{(x_i, Y_i)\\}_{i=1}^N$, we denote $\\widehat{CE}(f) := \\widehat{CE}(f, \\mathcal{D})$ as an estimator of $CE$ based on the dataset $\\mathcal{D}$. A very common way to estimate the calibration error is to use binning [47, 32, 14], i.e., partitioning the domain of the predictor variable $f(X)$ into a finite number of discrete intervals or bins, and estimating the calibration error by aggregating data within each bin. However, the piecewise nature of binning functions makes binned estimators non-differentiable and unsuitable for optimization tasks requiring gradient information, a property that we will need in Section 3. Binning also introduces a bias in estimating the conditional expectation $\\mathbb{E}[Y|f(X)]$, because it replaces continuous variations with average values within bins. Furthermore, binned estimators usually measure a weaker form of calibration than distribution calibration as defined in Definition 1, namely classwise calibration, which only demands calibrated marginal probabilities. For these reasons, binning is not discussed in the following overview of existing estimators. Due to the nature of our problem, we make use of calibration estimators that are consistent, (asymptotically) unbiased and differentiable. One of the first and still widely used metrics [15, 31, 38] that intrinsically also assesses calibration is the (expected) Brier score [4]. It is defined as\n$BS(f) = \\mathbb{E} [||f(X) - e_Y ||^2],$  (3)"}, {"title": "2.3 Statistical tests for calibration", "content": "As the proposed estimators $\\widehat{CE}(g, \\mathcal{D})$ are based on finite samples, comparing them in terms of the realized values does not take their randomness into account. In Appendix A we explore this further and show their distribution under the null hypothesis. In dependence of the sample size $N$, the realized values lie closer or further away of the true calibration error, which is zero. In practice, one needs to perform a statistical test to evaluate the calibration of $f$ for its predictions on a finite dataset $\\mathcal{D} = \\{(f(x_i), Y_i)\\}_{i=1}^N$. Different tests have been introduced in the past to evaluate model calibration. For a classifier $f : X \\rightarrow \\Delta_K$, they commonly define the null and alternative hypothesis as\n$H_0:$ $f$ is calibrated      $H_1:$ $\\neg H_0$\nThe Hosmer-Lemeshow test [16], initially developed as a goodness-of-fit test for the logistic regression model, can be used to test for confidence calibration and considers a chi-squared test statistic based on observed and expected frequencies of events. Vaicenavicius et al. [42] developed a test based on consistency sampling [6], a method that uses bootstrapping the data to estimate the distribution of the calibration estimator's values under the null hypothesis that $f$ is calibrated. This is done by sampling new labels in each bootstrap iteration, based on the distribution that $f$ predicts for the respective bootstrap sample. Given the (empirical) distribution function of the calibration estimator under the null hypothesis, one can then check how likely the given calibration estimate is under the assumption that $f$ is calibrated. Being a non-parametric test, the latter is more flexible and can therefore be used to test for distribution calibration in combination with any of the calibration estimators mentioned above. Our proposed test will build upon this work. Widmann et al. [45] also introduced a more specific calibration test for the kernel calibration estimator."}, {"title": "3 Calibration of sets of probabilistic predictors", "content": "We will now come to the main part of this paper, namely the evaluation of calibration of not only one, but a set of classifier models. To this end, after introducing the necessary methodology, we introduce our proposed test in a step-wise manner."}, {"title": "3.1 Credal sets", "content": "We use the same setting as described in the previous section, where we have given a dataset of $N$ realizations, $\\mathcal{D}_N = \\{(x_i, Y_i)\\}_{i=1}^N$, generated according to the joint distribution $P$. Credal sets of probability distributions can be created in various ways [44, 7]. One direct way of obtaining a credal set is via sets of probabilistic classifiers. In the following,"}, {"title": "3.2 Validity and calibration for sets of predictions", "content": "In the following, we formally define our notion of validity for set representations of epistemic uncertainty. It implies that for each $x \\in \\mathcal{X}$, the true underlying probability distribution $P_{Y|X=x}$ is contained in the (credal) set of all possible convex combinations $S(F, x)$.\nDefinition 3 (Validity of credal sets). Let $F = \\{f^{(1)},..., f^{(M)}\\}$ be a set of predictors, with $f^{(i)} : \\mathcal{X} \\rightarrow \\mathcal{P}(\\mathcal{Y})$, and for each $x \\in \\mathcal{X}$, define the induced credal set $F|x$ as in Definition 2. Then $F$ is valid if $P_{Y|X=x} \\in S(F,x)$ for all $x \\in \\mathcal{X}$.\nIn practice, testing validity requires having either access to the ground truth conditional distribution $P_{Y|X}$ or a sufficient number of samples thereof, as assumed in [8]. Hence, in our setting, we make use of the weaker notion of calibration which results from conditioning on the predictions $f(X)$ instead of $X$. This way, we analyse whether there is a $f_\\lambda(x) \\in S(F, x)$ such that $P_{Y|f_\\lambda(x)} \\in S(F, x)$. Similar as in [29], we now define a set of classifiers, or equivalently a credal classifier, as calibrated if there exists (at least) one calibrated convex combination of predictors. Definition 4 forms a generalization of Definition 4 of [29], where the coefficients of the convex combination do not form functions on the instance space, but are constants, i.e., $\\lambda \\in \\Delta_M$.\nDefinition 4. Let $F = \\{f^{(1)},..., f^{(M)}\\}$ be a set of predictors, $f^{(i)} : \\mathcal{X} \\rightarrow \\Delta_K$. We say that $F$ is calibrated in distribution if there exists $\\lambda \\in \\Delta_{M,x}$ such that the combined predictor $f_\\lambda : \\mathcal{X} \\rightarrow S(F, X)$ defined as\n$f_\\lambda(x) = \\sum_{i=1}^M \\lambda_i (x) f^{(i)} (x)$\nis calibrated in distribution (Definition 1).\nNote that calibration provides a necessary, but not sufficient condition for validity: A valid predictor is always cal-ibrated, yet the reverse does not have to hold. In fact, there are (possibly) many calibrated predictors, as shown in [42]. In Appendix B, we show that this is also the case for convex combinations of probabilistic models. However, if a predictor is not calibrated, one knows that it is also not valid."}, {"title": "3.3 A novel calibration test for sets of predictors", "content": "Our null and alternative hypothesis for testing the calibration of a set of predictors $F$ can now be formulated as\n$H_0: \\exists \\lambda \\in \\Delta_{M,x}$ s.t. $f_\\lambda$ is calibrated,    $H_1: \\neg H_0.$  (12)\nIn order to analyse whether there is a calibrated convex combination, a natural approach is to analyse the one with the lowest calibration error. Hence, we define the minimum calibration error as follows:\n$\\min_{\\lambda \\in \\Delta_{M, X}} g(f_\\lambda),$  (13)\nwhere $g$ is a calibration error.\nFor the experiments, we choose $g$ to be equal to the errors proposed in Section 2, i.e., $g \\in \\{\\widehat{CE}_P, \\widehat{CE}_{KL}, \\widehat{CE}_{MMD}, \\widehat{CE}_k\\}$. These have under suitable conditions the desirable property that $f$ is calibrated if and only if $CE = 0$, making them suitable for optimization. Furthermore, the respective estimators described in Section 2.2 are both consistent and at least asymptotically unbiased. Having an optimisation dataset $\\mathcal{D}_{opt} = \\{(x_i, Y_i)\\}_{i=1}^{\\tilde{N}}$, one can represent the evaluations of the weight function $\\Lambda : \\mathcal{X} \\rightarrow \\Delta_{M,x}$ by an $\\tilde{N} \\times M$ matrix:\n$\\Lambda = \\begin{pmatrix}\\lambda_1 (x_1) &...& \\lambda_M(x_1) \\\\:...&...&...\\\\\\lambda_1 (x_{\\tilde{N}}) &...& \\lambda_M(x_{\\tilde{N}})\\end{pmatrix}$  (14)\nHence, for $\\hat{g}$ being the (data-dependent) estimator of $g$, finding the minimum empirical calibration error can be formulated as follows:\n$\\min_{\\Lambda_{i,j}, i \\in \\{1,...,M\\}, j \\in \\{1,...,\\tilde{N}\\}} \\hat{g}(f_{\\Lambda}, D_{val})$\nwith $f(x_i) = \\sum_{m=1}^M \\lambda_m (x_i) f^{(m)} (x_i) \\in [0, 1]^K$, having the constraint $\\sum_{i=1}^M \\Lambda_{i,j} = 1$ for all $j \\in \\{1, ..., \\tilde{N}\\}$.  (15)\nAs already described in Section 3.2, there are in general many calibrated functions $f$, implying the absence of a unique global minimum for the optimization problem (16). Therefore, in the optimisation, we make use of a combined loss function consisting of a proper scoring rule and the respective calibration estimator, weighted by a constant. Proper scoring rules intrinsically optimise for both a calibration error and an accuracy term and provide a stable optimisation. The specific optimisation method is described in Section 3.5."}, {"title": "3.4 Algorithmic procedure", "content": "We now explain in detail our new, adapted version of the statistical test proposed in Mortier at al. [29], which in turn is based on the bootstrap test introduced by Vaicenavicius et al. [42]. Algorithm 1 shows the pseudo code of the test, which consists of the following steps:\n\u2022 First, using an appropriate optimization algorithm, the per-instance convex combination $\\Lambda^*$ with minimal calibration error is found (line 3).\n\u2022 Using $\\Lambda^*$, the predictions of the combined classifier $f_{\\Lambda^*}$ are computed (line 4).\n\u2022 Third, the calibration test of Vaicenavicius et al. [42] is employed to assess whether the predictions of $f_{\\Lambda^*}$ are calibrated (line 5-19). This statistical test is based on bootstrapping the data and consistency resampling, that is, resampling the labels from the distribution induced by $f_{\\Lambda^*}$. This way, one is able to estimate the distribution of the calibration error estimator $\\hat{g}$ under the null hypothesis that $f_{\\Lambda^*}$ is calibrated (see also Figure 5 for an example).\nFor a given significance level $\\alpha$, the test rejects the null hypothesis if the value of the calibration error estimator is higher than the $(1 - \\alpha)$-quantile of the bootstrapped empirical distribution. In the other case, it cannot reject it. Algorithm 1 differs from the proposed algorithm in Mortier at al. [29] in two important aspects:\n1. It allows for the case where the weights for the most calibrated convex combination depend on the instance space, i.e., for different regions in it, it accounts for the fact that some predictors might be better calibrated than others and vice versa."}, {"title": "3.5 A robust way of finding the most calibrated convex combination", "content": "We will now analyse further how optimization problem (15) can be solved in an efficient and robust manner. Specifically, we try to avoid the problem of overfitting on the empirical calibration error on the respective dataset: Since the latter is finite, there might be a mismatch between the estimated and the population-level calibration error. Classical gradient-based solvers, which purely optimise the calibration error on the same dataset that is used for the test, might therefore run into the problem of underestimating the true calibration error, thereby making the test more conservative. This can also be seen in the empirical results of Mortier et al. [29]. Therefore, we suggest an alternative approach to optimize (15), which incorporates two important aspects:\n\u2022 We use a neural network for learning the weight function $\\Lambda : \\mathcal{X} \\rightarrow \\Delta_{M,x}$, exemplary visualized in Figure 1. This approach is similar to stacking [46], where a second meta-learner is trained to find the optimal combination of predictions. It is trained on a separate optimization dataset, and then used to predict the optimal convex combination for our test.\n\u2022 As a loss function, we use a proper scoring rule $L \\in \\{BS, \\ell_\\ell\\}$, combined with a term controlling the calibration error:\n$\\Lambda^* = \\arg \\min_{\\lambda \\in \\Delta_{M, X}} L(f_\\lambda, D_{val}) + \\gamma \\cdot \\hat{g}(f_\\lambda, D_{val}),$  (16)\nwhere $\\hat{g}(f_x, y) \\in \\{\\widehat{CE}_{KL}, \\widehat{CE}_{2}, \\widehat{CE}_k, \\widehat{CE}_{MMD}\\}$ is the respective calibration error estimator of interest, and $\\gamma \\geq 0$ denotes a weight factor. This approach takes recent insights [36, 27] that adding a calibrating penalty to a proper scoring rule can help ensuring calibration, into account, while avoiding trivial or degenerate solutions. In particular, many convex combinations can be calibrated (Appendix B), hence, the proper scoring rule serves as a regularizer, guiding the optimizer toward solutions that not only reduce calibration error but also maintain high predictive accuracy. Furthermore, we learn $\\Lambda^*$ on a validation set but evaluate calibration on a separate test set. This split reduces the risk of overfitting to our chosen calibration metric, ensuring the resulting test does not become overly conservative."}, {"title": "4 Experimental results", "content": "We evaluate our test both on synthetic and real-world data. A more detailed description of the experimental setup can also be found in Appendix D and the code for reproducing the experiments is available on GitHub.\u00b9 For the experiments on synthetic data, a simple MLP architecture with 3 hidden layers is used. As a loss function, we use the combined loss as in (16), with Brier score or log loss as a proper scoring rule."}, {"title": "4.1 Binary classification", "content": "For illustration purposes, we first examine the case of binary classification by simulating $M = 2$ probabilistic predictors, each outputting a probability for the positive class. For each input $x$, the predictor's probabilities $f^{(1)}(x)$ and $f^{(2)}(x)$ are sampled from a Gaussian process (GP) constrained to [0, 1]. We then define a true calibrated predictor $f^*$ that lies inside (under $H_0$) or outside (under $H_1$) the convex hull of the two predictors. Figure 2 shows the exemplary setting for this experiment. Under $H_0$, we consider two cases:\n1. Non-instance dependency ($H_{0,1}$): The calibrated predictor $f^*$ is a constant convex combination of $f^{(1)}$ and $f^{(2)}$.\n2. Instance dependency ($H_{0,2}$): $\\lambda^*(x)$ is a randomly generated polynomial function and $f^*(x) = \\lambda^*(x) f^{(1)}(x) + (1 - \\lambda^*(x)) f^{(2)}(x)$.\nUnder $H_1$, $f^*(x)$ lies strictly outside or near the boundary of the credal set. We generate 3 scenarios of increasing distance from it:\n1. $H_{1,1}$: $f^*(x)$ is set at a small $\\epsilon$-distance to one boundary.\n2. $H_{1,2}$: is sampled from a GP that remains close but outside the credal set.\n3. $H_{1,3}$: similar to $H_{1,2}$, but allowing a larger maximum distance.\nFigure 3 shows the average Type 1 and Type 2 error for 100 different runs of the experiment, in comparison with the results given by the previously proposed algorithm [29]. The average Type 1 error of our newly proposed test lies much closer to the chosen significance level, while the baseline test is in general too conservative. In some cases, the average Type 1 error of our test lies above the significance level. This is due to the generalization error on the unseen data: Since we learn the optimally calibrated convex combination on a separate dataset, the learned convex combination on the test set might be not the one with lowest calibration error. This depends on the test set sample size, but underestimating the calibration error can lead to more false rejections. Both the previous test as well as our new test show a high power for this setting, with zero Type 2 error also in the case where the true distribution lies in very close distance to the polytope, except for the kernel calibration error, which shows a slightly higher Type 2 error."}, {"title": "4.2 Multi-class classification", "content": "We consider $K > 2$ classes, and follow a Dirichlet-based scheme to generate an ensemble of $M$ probabilistic predictors. Specifically, we generate a prior $p \\sim Dir(1)$ and predictions $f^{(1)}(x), ..., f^{(M)}(x) \\sim Dir(p_K^u)$, where $u : X \\rightarrow \\mathbb{R}^{\\gt 0}$ is a function defining the epistemic uncertainty (i.e. the spread between the predictions within the set) over the instance space. For the experiments, we use a constant $u(x) = 0.5$. Under $H_0$, we again distinguish between\n1. Non-instance dependency ($H_{0,1}$): $\\lambda^*(x) = c \\in [0,1]$ where $c \\sim Dir(1,...,1) \\in \\Delta_M$ ($\\lambda^*$ is constant across the instance space).\n2. Instance dependency ($H_{0,2}$): $\\lambda^*(x) = (\\lambda_1(x),..., \\lambda_M(x)) \\in \\Delta_M$ with $\\lambda_m(x) = \\sum_{i=0}^{D} \\beta_i x^i$ for $m = 1,..., M$ and $\\sum_m \\lambda_m (x) = 1$ (the components of $\\lambda^*$ form scaled polynomials of a certain degree).\nFor the alternative hypothesis, we select a random corner $f_c \\in F$ of the simplex and then set the true underlying conditional distribution $f^*$ as a point on the connecting line between the corner and the boundary point $f_b \\in F$ of the credal set: $f^*(x) = \\nu \\cdot f_c(x) + (1 - \\nu) \\cdot f_b(x)$. We define three cases with increasing distance to the credal set by varying the mixing coefficient $\\nu$: $H_{1,1}$: $\\nu = 0.01$, $H_{1,2}$: $\\nu = 0.1$ and $H_{1,3}$: $\\nu = 0.2$.\nThe resulting Type 1 and Type 2 errors of this experiment are shown in Figure 4. We see that the proposed test yields more reliable decisions, not exceeding the given significance level while following it closely. The test of Mortier et al. seems to be too conservative for all estimators, except for the kernel calibration error estimator $CE_k$, which in general leads to unreliable results and low power. Note here that we use the same datasets for both tests, but we do the optimisation for our proposed test on a separate dataset of the same size. The Type 2 error analysis shows that, except for the kernel calibration error, our proposed test has a lower average Type 2 error. Furthermore, aligns better with the chosen significance level."}, {"title": "4.3 Experiments on real-world data", "content": "Since the true data-generating distribution is unknown in real-world datasets, we cannot directly quantify Type 1 or Type 2 errors. Instead, we demonstrate the practical usefulness of our test by applying it to well-known image classification tasks (CIFAR-10, CIFAR-100). More specifically, we apply our test on the predictions of three ensemble methods, combined with two different architectures that are trained on the two datasets, respectively. We train a deep ensemble (DE), a dropout network with dropout rate 0.5 (DN(0.5) and a dropout network with dropout rate 0.2 (DN(0.2)). For the deep ensemble, we train 10 different models using different weight initializations, while in the dropout networks, dropout is applied after specific layers. After training, we obtain a set of 10 different predictions from these models. As model architectures, the ResNet-18 (11.6\u00d7106 parameters) and VGG-19 (138\u00d7106 parameters) are employed. We leverage the pre-trained ResNet-18 or VGG-19 to embed images and attach a fully-connected layer to predict the weight function $\\Lambda$. Section D describes the full experimental setup. The results are shown in Table 1. For each model and calibration estimator, we run the respective bootstrapping test (line 5-19 of Algorithm 1), with the optimal weight function $\\Lambda^*$ learned by the neural network. For comparison, we also perform the bootstrapping part of the test with the mean predictor (Algorithm 1 from line 5 with $f_\\Lambda = \\frac{1}{M}\\sum_{i=1}^{M} f^{(i)}$). The results differ significantly, depending on the used calibration error. In general, using the calibration error $\\widehat{CE}_{KL}$ led to the fewest rejections (i.e. the highest p-values), potentially because the models were trained with a log-loss objective, which aligns more closely with KL-based calibration measures. In general, for the CIFAR-100 dataset with more classes, the"}, {"title": "5 Discussion", "content": "We developed a novel statistical test for the calibration of epistemic uncertainty representations based on sets of probability distributions. It is defined on an instance-based level, thereby making the test more flexible. As calibration forms a necessary condition for validity, we claim that by testing for it, one can safely detect scenarios where the credal set is not valid. For this case, the next step to include would be actionability, i.e., ways to increase the size of the credal set to include at least one calibrated convex combination. Here, we model calibration as a property that applies across the entire instance space. Alternatively, calibration could be viewed as an instance-specific concept, allowing analysis in different regions of the instance space. However, there has been limited research on this form of calibration, often referred to as conditional or local calibration [26], and the challenge of partitioning the instance space in a way that enables the computation of expectations remains unresolved. Future work includes also the analysis of the regression case."}, {"title": "Appendix A Estimator analysis", "content": "In this section we empirically analyse the distribution of the chosen calibration estimators and their behaviour with respect to the distance from the underlying ground truth probability distribution. Figure 5 shows the distribution of the estimators under the null hypothesis. In the figure, we see that the mean of the estimators $\\widehat{CE}_2$ and $\\widehat{CE}_{KL}$ is slightly above zero - since they are only asymptotically unbiased - while $\\widehat{CE}_{MMD}$ and $\\widehat{CE}_k$ also empirically show their unbiasedness. The kernel calibration estimator $\\widehat{CE}_k$ is symmetrically distributed around zero - its asymptotic normality was also shown by Widmann et al. [45]. As the estimators $\\widehat{CE}_k$ and $\\widehat{CE}_{MMD}$ can obtain negative values, we use their squared versions for the optimization in Eqn. (16).\nFigure 6 shows the values of different calibration estimators in dependence of the position in the simplex, where the true underlying calibrated convex combination $f_{\\lambda^*}$ is set with $\\lambda^* = (0.1, 0.1, 0.8)$, and the predictions are sampled from a Dirichlet distribution. We see that by optimizing over $\\Lambda$, the true calibrated convex combination is learned differently \"well enough\". However, $\\lambda^*$ still lies within a certain region of low estimator values. For $\\widehat{CE}_k$, we see in general more noisy behavior in the region around the ground truth distribution (red dot).\nWe also do a similar analysis in $\\Lambda$-space: In Figure 7, we show the value of the calibration estimators not in distance to the ground truth probability distribution, but the ground truth weight vector $\\Lambda \\in \\Delta_3$, given the predictions of two predictors $\\{f^{(1)}(x), f^{(2)} (x), f^{(3)} (x)\\}$ and a ground truth probability distribution $f_{\\lambda^*}(x) = 0.1 \\cdot f^{(1)}(x) + 0.1 \\cdot f^{(2)}(x) + 0.8 \\cdot f^{(3)}(x)$, that is $\\lambda^* = (0.1, 0.1, 0.8)^T$. We see that here the heatmaps vary more significantly for the different calibration estimators."}, {"title": "Appendix B Non-uniqueness of calibrated models", "content": "Using simple examples, we will show the non-uniqueness of calibrated convex combinations, that is, that there are often many different (possibly instance-dependent) convex combinations leading to a calibrated predictor. Using a proposition proven by Vaicenavicius et al. [42], which states that conditioning $Y$ on any measurable function $h$ yields a calibrated predictor, we show this with a concrete example.\nExample 1 (Many calibrated classifiers). For the following, assume the case of having 3 classes, i.e. $Y = \\{1,2,3\\}$. We have ([42], Proposition 1) for any measurable function $h : X \\rightarrow Z$ with $Z$ being some measurable space, that the function\n$f(X) := \\mathbb{P}[Y \\in h(X)]$\nis a calibrated classifier. As an example, taking a constant $h(x) = c \\in \\mathbb{R} \\forall x \\in X$, the classifier that simply predicts the marginal class probabilities\n$f = \\begin{pmatrix} \\mathbb{P}(Y = 1) \\\\ \\mathbb{P}(Y = 2) \\\\ \\mathbb{P}(Y = 3) \\end{pmatrix}$\nis calibrated for $Y = \\{1,2,3\\}$. Further, if we take $h : X \\rightarrow X$ with $h(x) = x$, then\n$f' = \\begin{pmatrix} \\mathbb{P}(Y = 1|X) \\\\ \\mathbb{P}(Y = 2|X) \\\\ \\mathbb{P}(Y = 3|X) \\end{pmatrix}$\nis also a calibrated classifier. To make this example more concrete, let $X = \\{1,2,3\\}$ with $\\mathbb{P}(X = x) = \\frac{1}{3} \\forall x \\in X$. Further let again $Y = \\{1,2,3\\}$ and\n$\\mathbb{P}(Y = i|X = j) = \\begin{cases} 1, i = j \\\\ 0, else \\end{cases}$"}, {"title": "Appendix C Uniform sampling in the credal set", "content": "In the work of Mortier et al. [29], a sampling within the polytope of convex combinations of predictions is done by sampling weights $\\lambda \\in \\Delta_M \\sim Dir(1, . . ., 1)$. This however does in general not lead to a uniform sampling within the"}, {"title": "Appendix D Experimental setup", "content": "In this section we explain the experimental setup for the experiments conducted in Section 4.\nD.1 Synthetic data\nFor optimising the weights $\\Lambda$ in the synthetic experiments, we use an MLP with 3 hidden layers each consisting of 16 neurons that is trained on an optimization dataset of size $N_{opt} = 400$. Both tests (the previous one proposed by Mortier et al. and ours) are performed on a test set of size $N_{test} = 400$.\nFor the case of binary classification, we draw \\{x_i\\}_{i=1}^N \\sim U([0,5", "1": "."}]}