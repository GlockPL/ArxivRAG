{"title": "Fusing Pretrained ViTs with TCNet for Enhanced EEG Regression", "authors": ["Eric Modesitt", "Haicheng Yin", "Williams Huang Wang", "Brian Lu"], "abstract": "The task of Electroencephalogram (EEG) analysis is paramount to the development of Brain-Computer Interfaces (BCIs). However, to reach the goal of developing robust, useful BCIs depends heavily on the speed and the accuracy at which BCIs can understand neural dynamics. In response to that goal, this paper details the integration of pre-trained Vision Transformers (ViTs) with Temporal Convolutional Networks (TCNet) to enhance the precision of EEG regression. The core of this approach lies in harnessing the sequential data processing strengths of ViTs along with the superior feature extraction capabilities of TCNet, to significantly improve EEG analysis accuracy. In addition, we analyze the importance of how to construct optimal patches for the attention mechanism to analyze, balancing both speed and accuracy tradeoffs. Our results showcase a substantial improvement in regression accuracy, as evidenced by the reduction of Root Mean Square Error (RMSE) from 55.4 to 51.8 on EEGEyeNet's Absolute Position Task, outperforming existing state-of-the-art models. Without sacrificing performance, we increase the speed of this model by an order of magnitude (up to 4.32x faster). This breakthrough not only sets a new benchmark in EEG regression analysis but also opens new avenues for future research in the integration of transformer architectures with specialized feature extraction methods for diverse EEG datasets.", "sections": [{"title": "Introduction", "content": "The analysis of Electroencephalogram (EEG) signals is a cornerstone in the advancement of Brain-Computer Interfaces (BCIs), offering profound insights into the intricate neural processes of the human brain. EEG regression, in particular, stands as a pivotal tool in both neuroscience and medical diagnostics, gaining prominence for its ability to decode complex neural dynamics. This technique plays a crucial role in a myriad of applications, ranging from pinpointing brain damage locations to monitoring cognitive activities and deciphering the neural basis of seizures (Subasi and Ercelebi, 2005; Sabbagh et al., 2020; Teplan, 2002). The essence of EEG regression lies in its capacity to transform raw EEG data into interpretable and meaningful information, thus providing an invaluable perspective into the brain's operations."}, {"title": "Related Work", "content": ""}, {"title": "Deep Learning in EEG", "content": "The evolution of EEG signal processing has been significantly influenced by the emergence of deep learning techniques. Traditional machine learning methods, while effective, often fall short in capturing the high-dimensional and complex nature of EEG data. The introduction of deep learning models, particularly convolutional neural networks (CNNs) and recurrent neural networks (RNNs), revolutionized this field. These models brought enhanced capabilities in handling large datasets, extracting relevant features, and recognizing intricate patterns in EEG signals (Subasi and Ercelebi, 2005; Sabbagh et al., 2020; Teplan, 2002). This shift not only improved the accuracy of EEG analyses but also expanded the potential applications in neurological research and clinical diagnostics.\nDeep learning's impact on EEG signal processing is profound, offering new perspectives in understanding brain activity. The ability of these models to learn from data autonomously, without the need for extensive feature engineering, has opened avenues for more nuanced and detailed analyses of neural signals. This advancement is crucial in fields where EEG data plays a pivotal role, such as in the study of cognitive processes, sleep patterns, and brain-computer interfaces. The integration of advanced deep learning architectures in EEG analysis heralds a new era of innovation and discovery in neuroscience."}, {"title": "ViTs in Non-Image Data Analysis", "content": "Vision Transformers (ViTs), originally designed for image recognition, have demonstrated remarkable versatility by extending their application to various other domains, including EEG data analysis (Dosovitskiy et al., 2020; Wu et al., 2020; Han et al., 2022). The cornerstone of their success, the self-attention mechanism, enables ViTs to efficiently manage sequential data, a feature crucial in interpreting EEG signals Vaswani et al. (2017). This characteristic of ViTs facilitates an understanding of the complex, temporal relationships inherent in EEG data, making them an ideal choice for this type of analysis.\nThe adaptation of ViTs to non-image data, such as EEG signals, signifies a major shift in the approach to data analysis across disciplines. It underscores the potential of transformer models to handle diverse types of data beyond their initial scope. This cross-domain applicability of ViTs not only enriches the toolkit available for EEG analysis but also inspires innovative approaches to data interpretation. The flexibility and effectiveness of ViTs in handling sequential data pave the way for their broader adoption in various scientific and analytical fields."}, {"title": "Temporal Convolutional Networks (TCNet)", "content": "Temporal Convolutional Networks (TCNet) have gained significant attention for their ability to process time-series data, particularly in EEG signal analysis. The architecture of TCNet, with its focus on capturing temporal dependencies through convolutional layers, makes it exceptionally suited for extracting detailed features from EEG data (Farha and Gall, 2019; Hewage et al., 2020). The efficacy of TCNet in identifying subtle patterns and temporal features in complex datasets has established it as a leading tool in the field of neural signal processing.\nThe role of TCNet in EEG data interpretation extends beyond mere feature extraction. It involves a deeper understanding of the temporal dynamics and inherent structures within the EEG signals. This understanding is vital in applications where precise timing and sequence of neural events are critical, such as in epilepsy research or brain-computer interface development. The combination of TCNet with other advanced models like ViTs presents a promising avenue for enhancing EEG analysis, potentially leading to more accurate and insightful interpretations of neural data."}, {"title": "Methods", "content": "Our study employs an innovative approach by integrating pre-trained Vision Transformers (ViTs) with Temporal Convolutional Networks (TCNet) to enhance EEG regression analysis. This section outlines the dataset utilized, the specifics of the proposed model, and the methodology for evaluating its effectiveness."}, {"title": "EEGEyeNet Dataset", "content": "The data presented here is derived from the EEGEyeNet dataset Kastrati et al. (2021). The EEGEyeNet dataset encompasses recordings from 356 healthy adults, including 190 females and 166 males, aged 18 to 80 years. All individuals in this study provided written informed consent, compliant with the Declaration of Helsinki, and were compensated monetarily.\nThe EEG recordings in the EEGEyeNet dataset were obtained using a high-density 128-channel EEG Geodesic Hydrocel system, operating at a sampling rate of 500 Hz with a central recording reference. Eye positions were concurrently recorded using an EyeLink 1000 Plus system at the same sampling rate. This setup maintained electrode impedances below 40 kOhm and ensured accurate eye tracker calibration. Participants were positioned 68 cm from a 24-inch monitor, with their head stabilized using a chin rest.\nEEG data, as recorded in the EEGEyeNet dataset, are prone to various artifacts, including environmental noise and physiological interferences such as eye movements and blinks. To address this, the dataset underwent rigorous pre-processing with two levels: minimal and maximal. The minimal preprocessing involved identifying and interpolating faulty electrodes, along with applying a high-pass filter at 40 Hz and a low-pass filter at 0.5 Hz. The maximal pre-processing, aimed at neuroscientific analyses, further incorporated Independent Component Analysis (ICA) and IClabel for artifact component removal.\nThe EEGEyeNet dataset also includes synchronized EEG and eye-tracking data, facilitating time-locked analyses relative to event onsets. This synchronization was stringently verified to ensure a maximum error margin of 2 ms."}, {"title": "EEGVIT-TCNet Model Architecture", "content": "Intending to advance EEG signal analysis, we developed the EEGVIT-TCNet model, a novel architecture that combines Temporal Convolutional Networks (TCNet) with a pre-trained Vision Transformer (ViT). This model was meticulously designed to decipher the temporal dynamics and spatial characteristics embedded within EEG signals.\nTemporal Convolutional Network (TCNet) Component: The EEGVIT-TCNet model begins with the TCNet component, tailored to embrace the complexities of EEG data. This component is characterized by:\nInput Layer: Accepting EEG signals, the TCNet is prepared to handle an input dimensionality of 129, corresponding to the number of recorded EEG channels +1 for including grounding information (as done in the original EEGEyeNet paper).\nSequential TCNet Layers: The architecture encompasses three layers, with the number of channel dimensions expanding progressively to 64, 128, and 256. This hierarchy is instrumental in capturing a comprehensive spectrum of temporal dependencies inherent in the EEG signals.\n\u2022 Kernel Size: A kernel size of 3 is uniformly applied across the TCNet layers.\n\u2022 Dropout: To counteract the potential for overfitting, a dropout rate of 0.75 is employed.\n\u2022 Causality and Normalization: The layers incorporate weight normalization alongside the ReLU activation function to enhance the model's stability and performance."}, {"title": "Training and Evaluation Procedure", "content": "For training, we employed a supervised learning approach. The model was trained on a split of the EEG dataset, with 70% used for training and 30% for validation. During training, we employed a mean squared error loss function, optimized using the Adam optimizer with a learning rate of 1e-4. To prevent overfitting, we implemented early stopping based on the validation loss, with a patience of 10 epochs.\nThe primary metric for evaluating our model's performance was the Root Mean Square Error (RMSE). This metric provides a clear indication of the model's accuracy in predicting the gaze coordinates. A lower RMSE value indicates a closer approximation to the actual gaze positions. To ensure the robustness of our findings, we conducted five independent runs for each model configuration and reported the mean and standard deviation of these runs."}, {"title": "Results", "content": "Through rigorous testing and comparison, our model has demonstrated its capability to predict gaze positions with state-of-the-art precision, outperforming a spectrum of both conventional and advanced methodologies."}, {"title": "Performance Benchmarking", "content": "Our EEGVIT-TCNet model achieved a Root Mean Square Error (RMSE) of 51.8mm, marking a significant advancement over existing models. This performance showcases a 6.5% enhancement in precision over standalone ViT models Yang and Modesitt (2023). The stark contrast is further accentuated when juxtaposed with traditional approaches such as Linear Regression and Random Forest, where the RMSE figures exceed 115mm.\nOur model's evaluation, focusing on a single data partition divided by subject as done in Kastrati et al. (2021), demonstrates its ability to generalize effectively. This approach, where the testing data consisted of completely unseen,"}, {"title": "Computational Efficiency and Speed Enhancement", "content": "One of the features of our EEGVIT-TCNet model is its computational efficiency. In comparison to the original EEGViT architecture, our enhanced model achieves a substantial speed-up, being 4.32 times faster during inference. This improvement is not merely a testament to the model's optimized architecture but also underscores its practicality for real-time gaze position prediction applications.\nThe speed enhancement is achieved through key optimizations in the model's architecture, mainly through an optimization of the patch size accuracy-efficiency tradeoff. These optimizations ensure that our model not only maintains high accuracy but also operates with greater efficiency, making it well-suited for deployment in computationally expensive environments."}, {"title": "Ablation Studies", "content": "To assess the individual contributions of various components within our EEGVIT-TCNet model, we conducted a series of ablation studies. These studies aimed to isolate the effects of specific elements of the model, such as the convolutional"}, {"title": "Impact of Convolutional Layers", "content": "We first examined the impact of the additional convolutional layers that bridge the gap between the TCNet and the ViT on the model's performance. In particular, we analyze the results after removing all possible combinations of 1 convolutional layer (spatial, temporal, and pointwise convolution).\nBy removing the pointwise layer, we observed a slight increase in the Root Mean Square Error (RMSE) from 51.8 \u00b1 0.6mm to 52.5 \u00b1 0.8mm. This suggests that the pointwise convolutional layer plays a modest role in feature extraction and spatial representation, contributing to the model's overall accuracy.\nFor both the spatial and temporal layers we observed a significant increase in the Root Mean Square Error (RMSE) from 51.8 \u00b1 0.6mm to 55.1 \u00b1 0.6mm and 55.0 \u00b1 0.5mm respecitvely. This suggests that both sptial and temproal layers plays a major role in feature extraction and representation, contributing to the model's overall accuracy."}, {"title": "Influence of Dropout Rates in TCNet", "content": "The role of dropout rates in TCNet was another focus of our study. By varying the dropout rates, we investigated their effect on the model's capability to generalize and prevent overfitting. The original model with a 0.75 dropout rate achieved an RMSE of 51.8 \u00b1 0.6mm.\nReducing the dropout rate to 0 increased the RMSE to 54.1 \u00b1 0.6mm, indicating a higher propensity for overfitting. Conversely, lower dropout rates of 0.25 and 0.5 yielded RMSEs of 52.5 \u00b1 0.4mm and 52.1 \u00b1 0.4mm, respectively. These findings illustrate a nuanced balance between dropout rate and model performance, with moderate dropout rates contributing positively to the model's accuracy and generalizability."}, {"title": "Contribution of Pretrained ViT", "content": "Finally, we evaluated the contribution of using a pretrained ViT in our model. By replacing the pretrained ViT with a non-pretrained counterpart, the RMSE increased to 53.2 \u00b1 0.5mm. This increase underscores the significance of pretraining in enhancing the model's feature recognition capabilities, particularly in the context of EEG data analysis.\nThese ablation studies reveal the delicate interplay of different architectural components in optimizing the EEGVIT-TCNet model for EEG regression analysis. The presence of the second convolutional layer, the calibration of dropout rates in TCNet, and the incorporation of a pretrained ViT each contribute uniquely to the model's performance. Our findings highlight the importance of these components in achieving high precision in EEG regression tasks, providing valuable insights for future enhancements and applications of the model."}, {"title": "Discussion", "content": "Our novel approach has not only showcased a marked improvement in regression accuracy but also set new benchmarks in processing speed and efficiency. The results obtained from this study reflect the significant potential of leveraging the strengths of both ViTs and TCNet, underscoring the profound impact that such hybrid models can have on understanding and interpreting complex neural dynamics.\nThe success of the EEGVIT-TCNet model in reducing the Root Mean Square Error (RMSE) to its current levels emphasizes the model's capability to provide a more accurate interpretation of EEG data. This breakthrough is particularly relevant in the development of BCIs, where the precision of signal interpretation directly correlates to the effectiveness and user-friendliness of the interface. In clinical settings, the enhanced accuracy and speed of EEG analysis facilitated by the EEGVIT-TCNet model could lead to more timely and accurate diagnoses of neurological conditions, potentially transforming patient care.\nThroughout the research, adapting ViT to the unique nature of EEG data highlighted the complexities inherent in neural signal processing. The preprocessing of EEG signals, essential for maintaining the integrity of temporal features, posed significant challenges. This process is critical in ensuring the model's adaptability and generalizability across different subjects and experimental conditions, a vital aspect for the practical application of such technologies.\nLooking forward, the field beckons for further exploration into the scalability of hybrid models like EEGViT-TCNet, particularly in handling larger datasets and assessing performance in diverse real-world scenarios. A key area of interest lies in enhancing the interpretability of these deep learning models. Improved interpretability is crucial for clinical acceptance and can lead to advancements in personalized medicine, where EEG analysis can be tailored to individual patients for monitoring or therapeutic purposes."}, {"title": "Conclusion", "content": "This study represents a significant advancement in EEG regression analysis, underscoring the role of meticulous feature extraction in the efficacy of sophisticated computational models like Vision Transformers (ViT). The integration of Temporal Convolutional Networks (TCNet) with pretrained ViTs has unveiled the vast potential of harmonizing specialized feature extraction techniques with advanced deep learning frameworks. This synergy not only elevates the accuracy of EEG analysis but also establishes a new benchmark in the field, showcasing the profound benefits of refined feature representation.\nOur findings highlight the criticality of nuanced feature extraction in interpreting complex EEG data, with the EEGVIT-TCNet model demonstrating notable performance enhancements. This indicates that features, often overlooked by traditional models, can be captured and leveraged for more accurate regression analysis, suggesting a broad array of applications, from clinical diagnostics to enhanced brain-computer interfaces.\nFor future research, the horizon of EEG analysis and deep learning promises continued expansion and innovation. The development of increasingly sophisticated models capable of navigating the complexities inherent in EEG data is anticipated. A pivotal challenge will be enhancing the interpretability of these models, ensuring they not only perform optimally but also offer actionable insights for practitioners. For example, the performance of nearly all effective models in EEG analysis currently relies heavily on the subject-dependent nature on which they are trained, leading to the possibility of poor generalization in some instances. Moreover, integrating these advanced models into real-world applications will be crucial, extending the benefits of this research to society at large.\nIn sum, the fusion of TCNet and pretrained ViTs within the EEG regression domain exemplifies the transformative power of targeted feature extraction and advanced data processing. This study not only redefines the standards for EEG analysis but also lights the way for future endeavors in the realms of deep learning and neural data interpretation. As we delve deeper into the complexities of the human brain, the significance of innovative computational models grows ever more evident, harboring the potential for groundbreaking discoveries in neuroscience and artificial intelligence."}]}