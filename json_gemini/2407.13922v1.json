{"title": "SYNTHETIC COUNTERFACTUAL FACES", "authors": ["Guruprasad V Ramesh", "Harrison Rosenberg", "Ashish Hoooda", "Kassem Fawaz"], "abstract": "Computer vision systems have been deployed in various applications involving biometrics like\nhuman faces. These systems can identify social media users, search for missing persons, and\nverify identity of individuals. While computer vision models are often evaluated for accuracy on\navailable benchmarks, more annotated data is necessary to learn about their robustness and fairness\nagainst semantic distributional shifts in input data, especially in face data. Among annotated data,\ncounterfactual examples grant strong explainability characteristics. Because collecting natural face\ndata is prohibitively expensive, we put forth a generative AI-based framework to construct targeted,\ncounterfactual, high-quality synthetic face data. Our synthetic data pipeline has many use cases,\nincluding face recognition systems sensitivity evaluations and image understanding system probes.\nThe pipeline is validated with multiple user studies. We showcase the efficacy of our face generation\npipeline on a leading commercial vision model. We identify facial attributes that cause vision systems\nto fail.", "sections": [{"title": "Introduction", "content": "The growing availability of visual data, in combination with powerful compute resources, has led to the prevalence of\nmachine learning-based computer vision technologies across many industries, from healthcare to autonomous vehicles.\nVision models also are being used in many security-critical applications. An example of this is understanding human\nfaces. Facial recognition and understanding systems are deployed in identifying and analyzing faces on social media,\nlocating missing persons, and authenticating identities [21, 58]. However, this widespread deployment also highlights\nthe critical need to evaluate these models for fairness and reliability. Ensuring that these systems operate equitably\nacross diverse populations and are robust to real-world scenarios is essential to prevent biases and inaccuracies that\ncould have significant social and ethical implications.\nA standard way to evaluate these systems is to use a benchmark dataset. A benchmark dataset needs to be balanced\nacross demographics such as gender and race to test for performance disparities, and to allow evaluating sensitivity to\ntargeted attributes such as changes in facial hair or makeup. The greatest hurdle in generating sequences of realistic\ncounterfactual faces is covering a range of demographic and semantic attributes. Collecting similarly diverse natural\nimages at scale is nearly impossible. The combinations of skin tones, lighting conditions, hairstyles, and accessories is\ntoo numerous to efficiently collect naturally, even with an army of human subjects, makeup artists, costume makers,\nand set designers.\nState-of-the-art in image generation models, Diffusion has been used to address the data collection problem by\ngenerating synthetic datasets, including faces [36, 26]. Diffusion can be guided with textual prompts to generate faces\nwith both specified demographics and facial attributes. However, Diffusion does not always adhere to the textual\nprompt [54, 31]. Images generated with Diffusion may contain unnatural artifacts [75], and existing editing methods\ndon't always produce instructed semantic changes [25]. Further, recent studies have shown faces generated from\nDiffusion can be biased to certain demographics [61, 43, 22, 57].\nWe address these limitations by utilizing recent innovations in image understanding models such as GPT-4 [48] and\ndevise a filtering procedure to improve the quality of the synthetic dataset. In particular, we propose a novel end-to-end\npipeline to generate high-quality counterfactuals for facial attributes. The pipeline first uses Stable Diffusion to generate"}, {"title": "Background and Related Work", "content": "In this section, we discuss prior research in synthesizing faces, its biases, generating counterfactual examples, and\ncharacterizing face recognition performance."}, {"title": "Synthetic Face Generation Architectures", "content": "GANS (Generative Adversarial Networks), such as StyleGAN [34, 35], and Diffusion Models such as Stable Diffu-\nsion [56] are the two most popular architectures for synthesizing images, including faces. GANs, especially StyleGAN,\nhave seen extensive use in synthetic face data generation [62, 63, 52, 32]. In particular, latent space manipulation\nis effective at inducing facial semantics like pose [64, 39], age [6, 65] and facial expressions [74]. However, GAN-\ngenerated faces exhibit low variability [17], limiting their ability to mitigate diversity concerns we seek to address.\nAlso, diffusion can generate better quality images compared to GANs [19]. Diffusion models [24, 56, 76, 57] are the\nstart-of-the-art image generation architecture. They rely on internal Gaussian randomness and the construction of\nMarkov Chains to construct images. Many diffusion models utilize highly-functional text (prompt) guidance. Recent\nworks utilized diffusion models to generate faces with different styles (DCFace [36]), poses, accessories, and expressions\n(GANDiffFace [45]), and age [10]. Diffusion editing techniques [13, 7, 14] allow the application of semantic changes\nto faces. However, these editing techniques don't always apply the edits according to instructions [25]. Thus, we\nvalidate our generated edits by employing different semantic detectors, as described in sections 3 and 4."}, {"title": "Generated Face Biases", "content": "Many diffusion models are trained, either in whole or in part, on LAION-5B [59]. LAION-5B is a dataset scraped from\nthe internet. Accordingly, the diffusion models have biases: FAIR-Diffusion [22] shows that the building blocks of\na Stable diffusion model, i.e., the training data, LAION-5B, and the text encoder CLIP [53] are biased. The authors\npropose an approach to mitigate social biases in the generated data. Seshadri et al. [61] reveals that discrepancy between\nthe training data caption distribution and prompt distribution can lead to amplification of biases related to gender\nand occupation. Other works have studied the different gender, age and racial biases present in data synthesized by\nunconditioned diffusion models [46, 50] and text conditioned diffusion models [12, 43]. We take steps to mitigate\ndemographic biases within our generated face dataset. Our methodology is described in sections 3 and 4."}, {"title": "Face Recognition Evaluation", "content": "Race and gender bias in facial recognition has been extensively documented [51, 20, 1, 37, 15, 38], with performance\noften biased in favor of male and light-skinned faces [4, 15, 37]. Additionally, face recognition systems show disparities\nalong other semantic lines such as age [3, 37, 42], pose [60, 38], and hair [42, 11]. Prominent natural datasets\nused to train these models, such as CASIA WebFace [73] and VGGFace [49, 16], are often biased towards certain\ndemographics due to their internet-sourced and celebrity-dominated content [44]. While many balanced datasets have\nbeen proposed [28, 33, 55], they can still exhibit performance disparities due to factors like lighting, pose, and image\nquality [72].\nExisting natural face datasets are limited as they contain finitely many samples. Curating large natural datasets\nwith targeted attributes can be expensive. Synthetic data allows for targeted face recognition evaluation. Recent\nworks [70, 66] have explored diffusion models for generating counterfactual examples, aiming to identify spurious\ncorrelation and failure modes in vision classifiers trained on ImageNet. DiME [30] leverages gradient of a target\nattribute classifier to guide Diffusion models in generating counterfactual examples for the attributes 'smile' and\n'young'. Other studies employing GANs also perform counterfactual explanation (CE) on faces for 'young' and 'smile'"}, {"title": "Our Contributions", "content": "Our main contribution is a pipeline to synthesize a realistic, high-quality counterfactual face dataset from text prompts.\nThese generated faces are annotated by identity and attribute, allowing us to assess face recognition system performance\nconditionally by demographic and/or semantic. We apply filtering techniques to remove synthetic faces from our\nevaluation which are visually distorted or fail to match semantic intent. We validate the pipeline by multiple user studies.\nWe demonstrate the utility of our pipeline on a leading commercially deployed face understanding system."}, {"title": "Framework for Counterfactual Examples", "content": "We first introduce the notation used in this paper. Next, we list the requirements for producing counterfactual faces.\nFinally, we outline the steps of our pipeline for generating a large dataset of synthetic counterfactual faces."}, {"title": "Notation", "content": "Let $x \\in \\mathbb{R}^{H\\times W\\times 3}$ denote an RGB image. Typically, $x$ depicts a face containing identity $y$ within the set of identities\n$Y$. We refer to different variations $(x_1,...,x_m)$ of an identity $y$ as Source Faces. Each face image is associated with\nsemantic attributes $a$ within the space of semantic attributes $A$. For the scope of this paper, we regard all attributes to be\nbinary and discrete. For example, \"blue hair\u201d and \u201cred hair\u201d may be separate, binary entries within attribute vector $a$.\nAttribute vector $a_x \\in \\{0,1\\}^A$ denotes the presence or absence of attributes $a_1, . . ., a_{|A|}$ in image $x$. We use attribute\ntransformation function $g_{a_i} : \\mathbb{R}^{H\\times W\\times 3} \\rightarrow \\mathbb{R}^{H\\times W\\times 3}$ to apply attribute $a_i$ on a source face $x$. $g_{a_i}(x)$ is referred to as\nthe Transformed Face and $a_{g_{a_i}(x)}[i] = 1$.\nLastly, we employ binary attribute detection function $h_{a_i} : \\mathbb{R}^{H\\times W\\times 3} \\rightarrow \\{0, 1\\}$ which returns 1 if attribute $a_i$ is present\nin the input image, otherwise returns 0. In other words, $h_{a_i}(x) = a_x[i]$. We apply the attribute detector to verify the\ncorrectness of transformed examples."}, {"title": "Requirements for Counterfactual Examples", "content": "Prior work has established a set of requirements for counterfactual examples [67, 2]. We describe them below along\nwith the corresponding challenges for faces.\nValidity: The counterfactual example needs to be valid by satisfying real-world constraints. In the case of faces,\napplying the changes should keep the face semantically correct and retain the identity of the face. If image $x$ depicts\nidentity $y$, then $g_a(x)$ should also depict identity $y$.\nCorrectness: A counterfactual example is correct if it satisfies an intended result. In our case, the counterfactual face\nshould correctly reflect the applied semantic attribute, which is to say $g_a(x)$ should depict an image with attribute $a$\npresent in it: $h_{a_i}(g_{a_i}(x)) = 1$.\nSpecificity The counterfactual example should have only the intended change being applied. That is to say $g_a(x)$ should\nonly induce attribute $a$, but no other enumerated semantic attributes i.e., $h_{a_j}(g_{a_i}(x)) = h_{a_j}(x) \\forall j \\neq i$."}, {"title": "Generating Counterfactual Faces", "content": "We propose a pipeline (fig. 1) for generating a fully synthetic counterfactual dataset for faces, which involves two steps:\n(1) Candidate Counterfactual Generation: Obtaining source and transformed faces for different facial attributes. (2)\nCandidate Filtering: Filtering candidate counterfactuals ensuring they adhere to requirements mentioned in section 3.2.\nCandidate Counterfactual Generation: This step of the pipeline involves obtaining pairs of source faces and\ntransformed faces. The candidate counterfactuals (transformed faces) correspond to applying semantic attributes to the"}, {"title": "Dataset Generation and Filtering Pipeline", "content": "We instantiate our pipeline of candidate generation and filtering steps with different components. The detailed\ndescription of some of the components can be found in the Appendix."}, {"title": "Candidate Counterfactual Generation", "content": "Our method to generate source and transformed faces is based on Rosenberg et al. [57]. Similar to them, we obtain\na list of celebrity names for four ethnicities: East Asian, Indian, White and Black, and two sexes: Male, Female.\nHereafter, each ethnicity-sex combination is referred to as a demographic. The list of demographics is provided in\ntable 1. We obtain 100 celebrity names for each demographic. The names are used in the prompt 'A photo of the face\nof <Name>', which is fed to a Text-to-Image (TTI) Diffusion Model. We use the Stable diffusion fine-tuned model,\nRealistic Vision\u00b9, hereafter referred to as Realism. We used Realism to generate six variations for each of the 100\nidentities per demographic group. Variations were manually validated to ensure each set of six variations depicts the\nsame identity.\nWe employ a latent manipulator to create the transformed faces. That manipulator induces desired semantic transforma-\ntions on the source faces. Our choice of manipulator is the semantic-guidance image generation technique SEGA [13].\nSEGA steers the TTI model to generate images that incorporate specific semantic intent from user-provided text."}, {"title": "Candidate Filtering", "content": "Candidate Filtering comprised of a distortion detector and mul-\ntiple attribute detectors helps in keeping transformed images\nthat meet our requirements stated in section 3.2. Images found\nto be distorted, or not matching our intent, are removed from\nthe dataset. Our data validation step is important to decouple\npotential failures in the generative pipeline from the assessment\nof the face model.\nDistortion Detector: We use a Linear SVM trained on em-\nbeddings obtained from a ViT-L-14 CLIP model [53]. To train\nthis detector, we created a set of clean and distorted faces. We\ncurated 10 non-celebrity names for each demographic. We\nobtained clean faces by generating source faces for three vari-\nations of the curated identities. The distorted faces were obtained for the same names using SEGA-based attribute\ntransformations by explicitly setting the hyperparameters of SEGA to larger values. Larger values lead to distortion of\nfacia"}, {"title": "Dataset Composition", "content": "Our candidate counterfactual dataset consists of 4800 source faces and 91200 transformed faces. The source faces\ninclude 800 identities (100 per demographic) and 6 variations for each identity. The transformed faces were generated\nfor 19 attributes. A total of 15542 transformed faces survived our pipeline, i.e., were deemed as counterfactual according\nto requirements in section 3.2. This amounts to an average of 102 images per demographic-attribute combination.\n135 out of 152 attribute-demographic combinations had at least 25 transformed faces that survived our strict filtering\nscheme. Only 3 demographic-attribute combinations resulted in less than 5 transformed faces. We use these 15542\ntransformed faces and corresponding source faces for validating the Instagram model."}, {"title": "Human Validation of Pipeline", "content": "To understand the efficacy of our pipeline, we use a subset of 751 image pairs from the faces that survived our pipeline.\nWe perform human validation for the correctness of this subset, which involves distortion and attribute filtering (to\nsatisfy the requirements in section 3.2).\nFirst, we manually perform the distortion filtering step and remove the distorted faces. Then, we applied the attribute\nfiltering procedure via the annotations received in the Attribute Survey. The ratio of images that survive both these steps\nrepresents the overall efficacy of our pipeline.\nSpecifically, we removed 11 images because they were distorted. Out of the 740 non-distorted faces, annotations of 540\nfaces from at least one annotator satisfied attribute requirements of our filtering step. This amounts to an efficacy of\n71.9% (540/751). On a closer look, we observed that human annotators could overlook some of the attributes. Thus, we\nconducted a second round of the Attribute survey on the transformed faces that didn't survive the attribute filtering step.\nThe second round of responses gave 43 more faces that satisfied the attribute requirements. Finally, we also remove 19\nfaces which failed the identity retention question in the survey.\nOverall, our pipeline achieves an efficacy of 75.09%, i.e., a total of 564 transformed faces out of 751 did not have\nany distortion and were approved by at least one annotator in the Attribute survey. table 2 contains a breakdown of\nthe number of annotated faces validated per attribute-demographic combination. We find that 24 out of 152 attribute-\ndemographic combinations have an individual efficacy (computed per attribute-demographic combination) of less than\n50%. We do not consider images corresponding to these combinations in assessing the Instagram model section 5.3.\nThe efficacy of our pipeline without considering these attribute-demographic combinations is 84.6% (536/633). Note\nthe filtering scheme which achieves this 84.6% efficacy number is exceptionally strict."}, {"title": "Assessing Commercial Image Understanding Models", "content": "We utilize our pipeline to validate Instagram's Android Image Understanding Model. We use our annotated dataset to\ngain a fine-grained understanding of model successes and vulnerabilities.\nInstagram's Android image understanding model is a more general purpose semantic understanding model tuned to\nextract hundreds of concepts from images including faces. West et al. recently discovered the existence of this model\nby reverse engineering the Instagram app on Android [69]. We use their pipeline to capture the outputs of Instagram's\nmodel when presented with pictures from the dataset. Our work is among the first to perform counterfactual evaluation\nof a model used in production.\nInstagram's internal image understanding system aims to capture semantics of any image that could be viewed by the\nInstagram App. It provides similarity scores between the input image and hundreds of text-specified concepts. Many\nconcepts, such as \u201cblurry\u201d and \u201ccrustacean\u201d are not specific to human faces. There are also many concepts tuned for\nfaces, including \u201ceyeglasses\u201d, \u201cblond\u201d and \u201cface\u201d. Results of this evaluation, shown with 99.9% confidence interval per\ncell, are depicted in table 3. Each cell depicts the average of differences in concept score between transformed faces\nwith the indicated attribute $g_a(x)$ and their associated source faces $x$. In the table, we showcase several interesting\nchanges in Instagram's conceptual understanding once face attributes are induced. For example, we see that adding a\nfacemask greatly reduces the Instagram model's ability to resolve a face, with some disparity in that reduction White\n(-0.11) and Black (-0.091) males and the remaining six demographics (all at or below -0.16). We also see that\nadding sunglasses to a face, unsurprisingly, increases similarity with the sunglasses concept. Somewhat surprisingly,\nwe see disparities in the similarity, with Asian males receiving the largest bump (0.68) and Black Females receiving\nthe smallest (0.39). From this evaluation, we see that our face generation pipeline allows us to deeply probe general\npurpose vision systems."}, {"title": "Future Work", "content": "Instruction-guided editing [14] diffusion models provide a more friendly way for editing images with desired instructions.\nDatasets like MagicBrush [77], HIVE [78], HQ-Edit [27], SEED-Data-Edit [23] have helped in raising the quality of\nthe outputs from these models. However, none of these datasets are face-centric. Our pipeline and dataset can be used\nfor improving instruction-guided editing models for targeted editing of human faces. Additional data can be used to\nbring more confidence to our evaluations. The data also has annotations for 17 facial attributes that can be helped in"}, {"title": "Conclusion", "content": "We present a pipeline to generate a fully synthetic face counterfactual dataset. The pipeline depends on text-to-image\ngenerative models for generating counterfactual faces and vision-language models for validating them. Our pipeline is\nvalidated using human surveys. We then use our dataset to demonstrate that a commercial face understanding system\nperforms poorly on out-of-distribution examples."}, {"title": "Pipeline Implementation Design Choices", "content": "Rosenberg et al. [57] studies the quality of generated faces and edited faces with two stable diffusion models, Realism\nand Stable Diffusion v2.1. Their human evaluation reports higher quality scores for Realism across all demographics\nand edit attributes. Also, both Rosenberg et al. [57] and SEGA[13] show that SEGA is a promising technique for\nediting faces while ensuring the attribute intended is applied with high frequency."}, {"title": "Candidate Filtering: Attribute Transition Matrix", "content": "Our candidate filtering step ensures that only the attribute applied and its 'coinciding' or 'contradicting' attributes\nchange in the transformed face (section 4.2). We define this in a transition matrix shown in fig. 3. Each row in the\nmatrix corresponds to attribute $a_i$ being applied on a source face $x$. The vector describes the requirements of $a_i$ in\n$g_{a_i}(x)$, i.e., transformed face. The values in the row vector can be \u20181', '-1', '-2', and '0'. \u20181' indicates that an attribute\n$a_j$ should be present in the transformed face. \u2018-1' indicates that an attribute $a_j$ should be present in the transformed face\nonly if it is present in the source face. '-2' indicates that $a_j$ is not considered in the filtering step. \u20180' indicates that $a_j$\nshould not be present in the transformed face."}, {"title": "GPT-40 Prompting", "content": "For attributes except 'old' and 'young', we use GPT-40 as the attribute detector in our filtering step. The GPT-40 prompt\ncontains: (i) Image: Concatenated transformed face and corresponding source face (ii) Text: Contains list of attributes\nand asks model to return a JSON containing a 'Yes' or 'No' for each attribute for both source and transformed face. We"}, {"title": "Improving Human Alignment of GPT-40: An Additional Use-case of our Dataset", "content": "Our attribute survey indicates 75% of images survived our pipeline if human responses were considered for filtering.\nThis shows the GPT-40 responses don't always align with humans. One way to improve human-alignment is few-shot\nprompting, which we plan to fully explore in future work. Here, we show a brief and preliminary example of few-shot\nprompting of GPT-40 that improves human-alignment.\nOur preliminary analysis of few-shot prompting considers the dataset used in the Attribute Survey for the attributes\n'shoulder length hair', 'pigtails', and 'buzz cut'. One can realize from our filtering step requirements (section 4.2) that\ntransformed faces that survived our pipeline contain an attribute applied according to GPT-4o. Similarly, corresponding\nsource faces do not contain the same attribute according to GPT-40. Thus, for this experiment we consider just pairs of\ntransformed faces and source faces where human annotators felt otherwise. Images that were used as examples in the\nprompt, distorted or not validated for identity in the Attribute Survey were also excluded.\nUnlike our filtering step where we prompt the model for identifying all attributes at the same time, here, we just prompt\nthe model to identify one attribute (applied attribute) for the concatenated source and transformed image. The two-shot\nand four-shot results can be found in table 4. While these preliminary results show that few-shot prompting is promising,\nmore work is needed to design prompts that apply to different attributes and evaluate them."}, {"title": "Training Distortion Detector", "content": "As described in section 4.2, to train the distortion detector, we curated a training dataset consisting of clean faces and\ndistorted faces. We didn't want an overlap between the candidate transformed faces and this training set. So we obtained\na set of 10 non-celebrity names for each demographic. We used the source face prompt template (refer section 4.2) and\nobtained clean faces for 25 variations of these 80 names. The seeds were randomly generated for this process as the\nclean faces were only needed for training this detector and not generating counterfactuals. An observation we made was\nas the hyperparameters of SEGA were increased to larger values, the obtained transformed faces contained artifacts that\nwould occlude the identity andor semantics of a face. Using this observation, we generated a set of distorted faces for 3\nvariations of the non-celebrity names from 19 attributes.\nWe trained a Linear SVM with the training set consisting of CLIP embeddings of these clean and distorted faces. A\nsubset of the distorted faces used in the training set can be found in fig. 4. As the amount of distortion in these images\ncan be higher compared to the candidate transformed faces (see fig. 5), we tuned the distortion detector using survey\nannotated data to have a recall of 0.97 for detecting distortion. We used the tuned distortion detector in the filtering step\nto remove distorted faces. The overall performance of this tuned detector is high as only 11 out of the 751 transformed\nfaces that survived the filtering pipeline had any distortion (see section 5.2)"}, {"title": "User-Survey Details", "content": "We conducted two user-surveys to aid the filtering step in our pipeline (section 4.3). The Distortion Survey was used\nin tuning the threshold of the distortion detector. The Attribute Survey was used to get human annotations for the\ncandidates selected by our pipeline, and use them to measure the efficacy of the pipeline. Both the Distortion Survey\nand Attribute Survey were designed on Qualtrics and hosted on the Prolific platform. Both were approved by our\nInstitutional Research Board, had a median survey time of 10 minutes and the participants were paid $2.5 for their\nresponses. In this section, we discuss the details of each survey."}, {"title": "Distortion Survey", "content": "The purpose of this survey was to assess distortion in the transformed faces and use the labels to tune our distortion\ndetector. We showed the participants just the transformed faces. We instructed them that our AI tool generated the faces\nand asked them to assess each face for distortion. Each face contained the question \"Do you think the facial features are\ndistorted?\" and had two options (Yes & No). The instructions provided to the participants can be found below:\nThe survey consisted of three stages: in the first stage, we presented participants with five examples of both distorted\nand non-distorted faces, along with justifications for each; in the second stage, participants evaluated three faces,\nwhere we flagged them for choosing an incorrect option; in the third stage, we collected the main data, showing each\nparticipant 30 faces, including 2 attention-check questions. The first two stages helped participants understand the task\nrequirements and were the same for everyone. We did not use the attention-check questions in the analysis. None of the\nparticipants failed both attention checks.\nFor this survey, we randomly sampled 9 transformed faces per demographic for all 19 attributes, totaling 1368 faces. A\ntotal of 150 participants took part in and each participant annotated 28 faces, and each face received a minimum of 3\nresponses until we reached a majority. The final label of each face was the majority vote of the annotations received\nfrom different participants of the survey. A total of 131 transformed faces out of 1368 were labelled as distorted.\nWe used these labels to tune the distortion detector's threshold for each attribute-demographic combination. The\ndistortion detector had a minimum TPR of 0.97 for detecting distortion for each attribute-demographic combination. In"}, {"title": "Attribute Survey", "content": "The purpose of this survey was to obtain human annotations for a subset of the faces that survived our candidate\nfiltering pipeline. This allowed us to estimate our pipeline's efficacy. 478 participants took part in this survey and each\nparticipant had to label 5 pairs of transformed and corresponding source faces by answering three questions. Similar to\nthe Distortion Survey, we didn't explicitly mention that these faces were edited and instead mentioned that the faces\nwere both generated using our AI tool according to specified facial features. The source and the transformed faces were\nreferred to as the left and right faces, respectively.\nIn the first question, the participants selected the attributes present on the face using radio buttons for the options 'Yes'\nand 'No'. This consisted of all the attributes except 'old' and 'young'. We also received the responses for the sex of both\nthe faces in this question. We used the response for the source face (left face in the survey) as an attention-check since\nthe ground truth was known to us. Responses that failed this check was not used for the final analysis. No participant\nfailed more than 2 out of 5 attention checks; only one participant failed two attention checks.\nIn the second question, the participants answered the question \u2018Which of the two faces look younger?\u201d. The options were\n'Source Face by 10 or more years', \u2018Source Face by about 5 years', \u2018Equal age or insignificant difference', 'Transformed\nFace by about 5 years' and 'Transformed Face by 10 or more years'. We used this response for transformed faces as\ndescribed in section 4.2.\nIn the third question, the participants answered the question \u2018Do these images depict the same person?' among the\noptions 'Yes', 'No', \u2018Not sure'. 19 transformed faces were removed from the calculation of efficacy as they were\ndeemed by more than one participant as not the same person as the source face. The full set of instructions provided to\nthe participants can be found below and screenshots of the different questions can be found in fig. 7."}]}