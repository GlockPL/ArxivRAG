{"title": "SYNTHETIC COUNTERFACTUAL FACES", "authors": ["Guruprasad V Ramesh", "Harrison Rosenberg", "Ashish Hoooda", "Kassem Fawaz"], "abstract": "Computer vision systems have been deployed in various applications involving biometrics like human faces. These systems can identify social media users, search for missing persons, and verify identity of individuals. While computer vision models are often evaluated for accuracy on available benchmarks, more annotated data is necessary to learn about their robustness and fairness against semantic distributional shifts in input data, especially in face data. Among annotated data, counterfactual examples grant strong explainability characteristics. Because collecting natural face data is prohibitively expensive, we put forth a generative AI-based framework to construct targeted, counterfactual, high-quality synthetic face data. Our synthetic data pipeline has many use cases, including face recognition systems sensitivity evaluations and image understanding system probes. The pipeline is validated with multiple user studies. We showcase the efficacy of our face generation pipeline on a leading commercial vision model. We identify facial attributes that cause vision systems to fail.", "sections": [{"title": "1 Introduction", "content": "The growing availability of visual data, in combination with powerful compute resources, has led to the prevalence of machine learning-based computer vision technologies across many industries, from healthcare to autonomous vehicles. Vision models also are being used in many security-critical applications. An example of this is understanding human faces. Facial recognition and understanding systems are deployed in identifying and analyzing faces on social media, locating missing persons, and authenticating identities [21, 58]. However, this widespread deployment also highlights the critical need to evaluate these models for fairness and reliability. Ensuring that these systems operate equitably across diverse populations and are robust to real-world scenarios is essential to prevent biases and inaccuracies that could have significant social and ethical implications.\nA standard way to evaluate these systems is to use a benchmark dataset. A benchmark dataset needs to be balanced across demographics such as gender and race to test for performance disparities, and to allow evaluating sensitivity to targeted attributes such as changes in facial hair or makeup. The greatest hurdle in generating sequences of realistic counterfactual faces is covering a range of demographic and semantic attributes. Collecting similarly diverse natural images at scale is nearly impossible. The combinations of skin tones, lighting conditions, hairstyles, and accessories is too numerous to efficiently collect naturally, even with an army of human subjects, makeup artists, costume makers, and set designers.\nState-of-the-art in image generation models, Diffusion has been used to address the data collection problem by generating synthetic datasets, including faces [36, 26]. Diffusion can be guided with textual prompts to generate faces with both specified demographics and facial attributes. However, Diffusion does not always adhere to the textual prompt [54, 31]. Images generated with Diffusion may contain unnatural artifacts [75], and existing editing methods don't always produce instructed semantic changes [25]. Further, recent studies have shown faces generated from Diffusion can be biased to certain demographics [61, 43, 22, 57].\nWe address these limitations by utilizing recent innovations in image understanding models such as GPT-4 [48] and devise a filtering procedure to improve the quality of the synthetic dataset. In particular, we propose a novel end-to-end pipeline to generate high-quality counterfactuals for facial attributes. The pipeline first uses Stable Diffusion to generate"}, {"title": "2 Background and Related Work", "content": "In this section, we discuss prior research in synthesizing faces, its biases, generating counterfactual examples, and characterizing face recognition performance."}, {"title": "2.1 Synthetic Face Generation Architectures", "content": "GANS (Generative Adversarial Networks), such as StyleGAN [34, 35], and Diffusion Models such as Stable Diffusion [56] are the two most popular architectures for synthesizing images, including faces. GANs, especially StyleGAN, have seen extensive use in synthetic face data generation [62, 63, 52, 32]. In particular, latent space manipulation is effective at inducing facial semantics like pose [64, 39], age [6, 65] and facial expressions [74]. However, GAN-generated faces exhibit low variability [17], limiting their ability to mitigate diversity concerns we seek to address. Also, diffusion can generate better quality images compared to GANs [19]. Diffusion models [24, 56, 76, 57] are the start-of-the-art image generation architecture. They rely on internal Gaussian randomness and the construction of Markov Chains to construct images. Many diffusion models utilize highly-functional text (prompt) guidance. Recent works utilized diffusion models to generate faces with different styles (DCFace [36]), poses, accessories, and expressions (GANDiffFace [45]), and age [10]. Diffusion editing techniques [13, 7, 14] allow the application of semantic changes to faces. However, these editing techniques don't always apply the edits according to instructions [25]. Thus, we validate our generated edits by employing different semantic detectors, as described in sections 3 and 4."}, {"title": "2.2 Generated Face Biases", "content": "Many diffusion models are trained, either in whole or in part, on LAION-5B [59]. LAION-5B is a dataset scraped from the internet. Accordingly, the diffusion models have biases: FAIR-Diffusion [22] shows that the building blocks of a Stable diffusion model, i.e., the training data, LAION-5B, and the text encoder CLIP [53] are biased. The authors propose an approach to mitigate social biases in the generated data. Seshadri et al. [61] reveals that discrepancy between the training data caption distribution and prompt distribution can lead to amplification of biases related to gender and occupation. Other works have studied the different gender, age and racial biases present in data synthesized by unconditioned diffusion models [46, 50] and text conditioned diffusion models [12, 43]. We take steps to mitigate demographic biases within our generated face dataset. Our methodology is described in sections 3 and 4."}, {"title": "2.3 Face Recognition Evaluation", "content": "Race and gender bias in facial recognition has been extensively documented [51, 20, 1, 37, 15, 38], with performance often biased in favor of male and light-skinned faces [4, 15, 37]. Additionally, face recognition systems show disparities along other semantic lines such as age [3, 37, 42], pose [60, 38], and hair [42, 11]. Prominent natural datasets used to train these models, such as CASIA WebFace [73] and VGGFace [49, 16], are often biased towards certain demographics due to their internet-sourced and celebrity-dominated content [44]. While many balanced datasets have been proposed [28, 33, 55], they can still exhibit performance disparities due to factors like lighting, pose, and image quality [72].\nExisting natural face datasets are limited as they contain finitely many samples. Curating large natural datasets with targeted attributes can be expensive. Synthetic data allows for targeted face recognition evaluation. Recent works [70, 66] have explored diffusion models for generating counterfactual examples, aiming to identify spurious correlation and failure modes in vision classifiers trained on ImageNet. DiME [30] leverages gradient of a target attribute classifier to guide Diffusion models in generating counterfactual examples for the attributes 'smile' and 'young'. Other studies employing GANs also perform counterfactual explanation (CE) on faces for 'young' and 'smile'"}, {"title": "2.4 Our Contributions", "content": "Our main contribution is a pipeline to synthesize a realistic, high-quality counterfactual face dataset from text prompts. These generated faces are annotated by identity and attribute, allowing us to assess face recognition system performance conditionally by demographic and/or semantic. We apply filtering techniques to remove synthetic faces from our evaluation which are visually distorted or fail to match semantic intent. We validate the pipeline by multiple user studies. We demonstrate the utility of our pipeline on a leading commercially deployed face understanding system."}, {"title": "3 Framework for Counterfactual Examples", "content": "We first introduce the notation used in this paper. Next, we list the requirements for producing counterfactual faces. Finally, we outline the steps of our pipeline for generating a large dataset of synthetic counterfactual faces."}, {"title": "3.1 Notation", "content": "Let $x \\in \\mathbb{R}^{H\\times W\\times 3}$ denote an RGB image. Typically, $x$ depicts a face containing identity $y$ within the set of identities $Y$. We refer to different variations $(x_1,...,x_m)$ of an identity $y$ as Source Faces. Each face image is associated with semantic attributes $a$ within the space of semantic attributes $A$. For the scope of this paper, we regard all attributes to be binary and discrete. For example, \"blue hair\u201d and \u201cred hair\u201d may be separate, binary entries within attribute vector $a$.\nAttribute vector $a_i \\in \\{0,1\\}^A$ denotes the presence or absence of attributes $a_1, ..., a_{|A|}$ in image $x$. We use attribute transformation function $g_{a_i} : \\mathbb{R}^{H\\times W\\times 3} \\rightarrow ]\\mathbb{R}^{H\\times W\\times 3}$ to apply attribute $a_i$ on a source face $x$. $g_{a_i}(x)$ is referred to as the Transformed Face and $a_{g_{a_i}(x)}[i] = 1$.\nLastly, we employ binary attribute detection function $h_{a_i} : \\mathbb{R}^{H\\times W\\times 3} \\rightarrow \\{0, 1\\}$ which returns 1 if attribute $a_i$ is present in the input image, otherwise returns 0. In other words, $h_{a_i}(x) = a_x[i]$. We apply the attribute detector to verify the correctness of transformed examples."}, {"title": "3.2 Requirements for Counterfactual Examples", "content": "Prior work has established a set of requirements for counterfactual examples [67, 2]. We describe them below along with the corresponding challenges for faces.\nValidity: The counterfactual example needs to be valid by satisfying real-world constraints. In the case of faces, applying the changes should keep the face semantically correct and retain the identity of the face. If image $x$ depicts identity $y$, then $g_{a_i}(x)$ should also depict identity $y$.\nCorrectness: A counterfactual example is correct if it satisfies an intended result. In our case, the counterfactual face should correctly reflect the applied semantic attribute, which is to say $g_{a_i}(x)$ should depict an image with attribute $a_i$ present in it: $h_{a_i}(g_{a_i}(x)) = 1$.\nSpecificity The counterfactual example should have only the intended change being applied. That is to say $g_{a_i}(x)$ should only induce attribute $a_i$, but no other enumerated semantic attributes i.e., $h_{a_j}(g_{a_i}(x)) = h_{a_j}(x) \\forall j \\neq i$."}, {"title": "3.3 Generating Counterfactual Faces", "content": "We propose a pipeline (fig. 1) for generating a fully synthetic counterfactual dataset for faces, which involves two steps: (1) Candidate Counterfactual Generation: Obtaining source and transformed faces for different facial attributes. (2) Candidate Filtering: Filtering candidate counterfactuals ensuring they adhere to requirements mentioned in section 3.2.\nCandidate Counterfactual Generation: This step of the pipeline involves obtaining pairs of source faces and transformed faces. The candidate counterfactuals (transformed faces) correspond to applying semantic attributes to the"}, {"title": "4 Dataset Generation and Filtering Pipeline", "content": "We instantiate our pipeline of candidate generation and filtering steps with different components. The detailed description of some of the components can be found in the Appendix."}, {"title": "4.1 Candidate Counterfactual Generation", "content": "Our method to generate source and transformed faces is based on Rosenberg et al. [57]. Similar to them, we obtain a list of celebrity names for four ethnicities: East Asian, Indian, White and Black, and two sexes: Male, Female. Hereafter, each ethnicity-sex combination is referred to as a demographic. The list of demographics is provided in table 1. We obtain 100 celebrity names for each demographic. The names are used in the prompt 'A photo of the face of <Name>', which is fed to a Text-to-Image (TTI) Diffusion Model. We use the Stable diffusion fine-tuned model, Realistic Vision, hereafter referred to as Realism. We used Realism to generate six variations for each of the 100 identities per demographic group. Variations were manually validated to ensure each set of six variations depicts the same identity.\nWe employ a latent manipulator to create the transformed faces. That manipulator induces desired semantic transformations on the source faces. Our choice of manipulator is the semantic-guidance image generation technique SEGA [13]. SEGA steers the TTI model to generate images that incorporate specific semantic intent from user-provided text."}, {"title": "4.2 Candidate Filtering", "content": "Candidate Filtering comprised of a distortion detector and mul-\ntiple attribute detectors helps in keeping transformed images\nthat meet our requirements stated in section 3.2. Images found\nto be distorted, or not matching our intent, are removed from\nthe dataset. Our data validation step is important to decouple\npotential failures in the generative pipeline from the assessment\nof the face model.\nDistortion Detector: We use a Linear SVM trained on em-\nbeddings obtained from a ViT-L-14 CLIP model [53]. To train\nthis detector, we created a set of clean and distorted faces. We\ncurated 10 non-celebrity names for each demographic. We\nobtained clean faces by generating source faces for three vari-\nations of the curated identities. The distorted faces were obtained for the same names using SEGA-based attribute\ntransformations by explicitly setting the hyperparameters of SEGA to larger values. Larger values lead to distortion of\nfacial attributes and unnatural looking transformed faces. We tuned the distortion detector using the human annotated\ndataset collected from our survey (section 4.3) to achieve a recall of 0.97 or more for detecting distortion for each\ndemographic-attribute pair.\nWe use the tuned distortion detector to label our transformed faces as distorted and non-distorted. The non-distorted\nimages then pass to the attribute detectors.\nAttribute Detectors: We use two types of attribute detectors. For the age related attributes 'old' and 'young', we use\nan open-sourced age estimator by InsightFace\u00b3. For all other attributes, we use the most recent GPT-4 (GPT-40) [48].\nAll non-distorted faces are passed through these attribute detectors.\nThe age detector gives a positive integer as the predicted age of a face. GPT-40 is used to validate the remaining\nattributes. The specificity criterion requires that only the target attribute is changed, however, many semantic attributes\nnaturally either coincide or contradict: strict specificity is difficult to achieve in practice. For example, 'facemask'\ncontradicts with 'mustache' because a facemask would almost certainly visually block a mustache. Likewise, we\nconsider \"heavy makeup\" to coincide with \u201cred lipstick\" because red lipstick is likely to be considered a form of heavy\nmakeup.\nOne solution would be to restrict attributes in the pipeline to those whose presence is mutually agnostic to the remaining\nset of considered attributes, but this would severly restrict the number of attributes we include in the pipeline. To address\nthis challenge, for a handful of the attributes, we relax the requirement that only one semantic attribute be changed, and\nallow for both coinciding and contradicting to change. This allows us to simultaneously include \u2018facemask' and facial\nhair attributes. The exact details can be found in Appendix.\nFiltering Transformed Faces: We summarize our filtering procedure in two steps. First, we pass transformed faces\nthrough a distortion detector, rejecting any distorted faces. Second, we pass non-distorted transformed faces and their\ncorresponding source faces to attribute detectors, selecting candidates as counterfactuals based on these rules: (i) Reject\ntransformed faces when the corresponding source face already contains the attribute to be added. (ii) For non-age-related\nattributes, only the attribute and any coinciding/contradicting attribute(s) may change, with an age difference of less\nthan 10 years between source and transformed faces. (iii) For age-related attributes ('old' and 'young'), the transformed\nface must be appropriately older or younger by 10 years, without other attribute changes."}, {"title": "4.3 Human Validation of Detectors", "content": "We conducted two user-surveys in this work, both approved by our Institutional Research Board. The surveys were set up on Qualtrics and hosted on the Prolific platform. The median time of completion for both surveys was 10 minutes and we paid $2.5 to each participant. We incorporated attention-checks consistent with Prolific standards to monitor the quality of the user responses. Responses that failed the attention checks were not considered for the final validation. Due to space constraints, we provide an overview of the two surveys in this section and discuss the exact details in Appendix.\nThe first survey, hereafter referred to as Distortion Survey, helped to tune the the threshold of the distortion detector for different attribute-demographic combinations. We randomly sampled 9 transformed faces per attribute-demographic combination and the participants labeled each image as distorted or not-distorted. Each transformed face was labeled by at least three participants. We took the majority response as the final label for the annotated faces.\nWe validated the efficacy of our filtering step with the second survey, hereafter referred to as Attribute Survey. We randomly sampled 5 pairs of source and transformed faces filtered by our pipeline for each attribute-demographic combination. The participants labeled each pair, by answering three questions. In the first question, they labeled if each attribute is present or not for both the source and the transformed image. They picked which of the source or transformed face was looking younger in the second question. They verified if identity of the source and transformed faces match in the third question.\nWe intended for the Attribute Survey to mimic the attribute detectors in the filtering step. It gauges if the faces approved by our pipeline would also be approved by a human. Although we did not employ a detector to verify identity in the pipeline, we use the human annotations to understand the efficacy of SEGA in retaining identity."}, {"title": "5 Results", "content": "We demonstrate the efficacy and utility of our counterfactual face image generation pipeline. To evaluate efficacy, we measure human perception of our synthetic image generation pipeline. Using two user surveys (section 4.3), we verify that our generated faces match user intent. To demonstrate utility, we use the images generated from our framework to characterize the performance of Instagram's Android Image/Face Understanding model. In particular, we analyze the performance of the system for the 8 demographics and a subset of the 19 attributes listed in table 1."}, {"title": "5.1 Dataset Composition", "content": "Our candidate counterfactual dataset consists of 4800 source faces and 91200 transformed faces. The source faces include 800 identities (100 per demographic) and 6 variations for each identity. The transformed faces were generated for 19 attributes. A total of 15542 transformed faces survived our pipeline, i.e., were deemed as counterfactual according to requirements in section 3.2. This amounts to an average of 102 images per demographic-attribute combination. 135 out of 152 attribute-demographic combinations had at least 25 transformed faces that survived our strict filtering scheme. Only 3 demographic-attribute combinations resulted in less than 5 transformed faces. We use these 15542 transformed faces and corresponding source faces for validating the Instagram model."}, {"title": "5.2 Human Validation of Pipeline", "content": "To understand the efficacy of our pipeline, we use a subset of 751 image pairs from the faces that survived our pipeline. We perform human validation for the correctness of this subset, which involves distortion and attribute filtering (to satisfy the requirements in section 3.2).\nFirst, we manually perform the distortion filtering step and remove the distorted faces. Then, we applied the attribute filtering procedure via the annotations received in the Attribute Survey. The ratio of images that survive both these steps represents the overall efficacy of our pipeline.\nSpecifically, we removed 11 images because they were distorted. Out of the 740 non-distorted faces, annotations of 540 faces from at least one annotator satisfied attribute requirements of our filtering step. This amounts to an efficacy of 71.9% (540/751). On a closer look, we observed that human annotators could overlook some of the attributes. Thus, we conducted a second round of the Attribute survey on the transformed faces that didn't survive the attribute filtering step. The second round of responses gave 43 more faces that satisfied the attribute requirements. Finally, we also remove 19 faces which failed the identity retention question in the survey.\nOverall, our pipeline achieves an efficacy of 75.09%, i.e., a total of 564 transformed faces out of 751 did not have any distortion and were approved by at least one annotator in the Attribute survey. table 2 contains a breakdown of the number of annotated faces validated per attribute-demographic combination. We find that 24 out of 152 attribute-demographic combinations have an individual efficacy (computed per attribute-demographic combination) of less than 50%. We do not consider images corresponding to these combinations in assessing the Instagram model section 5.3. The efficacy of our pipeline without considering these attribute-demographic combinations is 84.6% (536/633). Note the filtering scheme which achieves this 84.6% efficacy number is exceptionally strict."}, {"title": "5.3 Assessing Commercial Image Understanding Models", "content": "We utilize our pipeline to validate Instagram's Android Image Understanding Model. We use our annotated dataset to gain a fine-grained understanding of model successes and vulnerabilities.\nInstagram's Android image understanding model is a more general purpose semantic understanding model tuned to extract hundreds of concepts from images including faces. West et al. recently discovered the existence of this model by reverse engineering the Instagram app on Android [69]. We use their pipeline to capture the outputs of Instagram's model when presented with pictures from the dataset. Our work is among the first to perform counterfactual evaluation of a model used in production.\nInstagram's internal image understanding system aims to capture semantics of any image that could be viewed by the Instagram App. It provides similarity scores between the input image and hundreds of text-specified concepts. Many concepts, such as \u201cblurry\u201d and \u201ccrustacean\u201d are not specific to human faces. There are also many concepts tuned for faces, including \u201ceyeglasses\u201d, \u201cblond\u201d and \u201cface\u201d. Results of this evaluation, shown with 99.9% confidence interval per cell, are depicted in table 3. Each cell depicts the average of differences in concept score between transformed faces with the indicated attribute $g_{a_i}(x)$ and their associated source faces $x$. In the table, we showcase several interesting changes in Instagram's conceptual understanding once face attributes are induced. For example, we see that adding a facemask greatly reduces the Instagram model's ability to resolve a face, with some disparity in that reduction White (-0.11) and Black (-0.091) males and the remaining six demographics (all at or below -0.16). We also see that adding sunglasses to a face, unsurprisingly, increases similarity with the sunglasses concept. Somewhat surprisingly, we see disparities in the similarity, with Asian males receiving the largest bump (0.68) and Black Females receiving the smallest (0.39). From this evaluation, we see that our face generation pipeline allows us to deeply probe general purpose vision systems."}, {"title": "6 Future Work", "content": "Instruction-guided editing [14] diffusion models provide a more friendly way for editing images with desired instructions. Datasets like MagicBrush [77], HIVE [78], HQ-Edit [27], SEED-Data-Edit [23] have helped in raising the quality of the outputs from these models. However, none of these datasets are face-centric. Our pipeline and dataset can be used for improving instruction-guided editing models for targeted editing of human faces. Additional data can be used to bring more confidence to our evaluations. The data also has annotations for 17 facial attributes that can be helped in"}, {"title": "7 Conclusion", "content": "We present a pipeline to generate a fully synthetic face counterfactual dataset. The pipeline depends on text-to-image generative models for generating counterfactual faces and vision-language models for validating them. Our pipeline is validated using human surveys. We then use our dataset to demonstrate that a commercial face understanding system performs poorly on out-of-distribution examples."}, {"title": "A Pipeline Implementation Design Choices", "content": "Rosenberg et al. [57] studies the quality of generated faces and edited faces with two stable diffusion models, Realism and Stable Diffusion v2.1. Their human evaluation reports higher quality scores for Realism across all demographics and edit attributes. Also, both Rosenberg et al. [57] and SEGA[13] show that SEGA is a promising technique for editing faces while ensuring the attribute intended is applied with high frequency."}, {"title": "B Candidate Filtering: Attribute Transition Matrix", "content": "Our candidate filtering step ensures that only the attribute applied and its 'coinciding' or 'contradicting' attributes change in the transformed face (section 4.2). We define this in a transition matrix shown in fig. 3. Each row in the matrix corresponds to attribute $a_i$ being applied on a source face $x$. The vector describes the requirements of $a_i$ in $g_{a_i}(x)$, i.e., transformed face. The values in the row vector can be \u20181', '-1', '-2', and '0'. '1' indicates that an attribute $a_j$ should be present in the transformed face. \u2018-1' indicates that an attribute $a_j$ should be present in the transformed face only if it is present in the source face. '-2' indicates that $a_j$ is not considered in the filtering step. '0' indicates that $a_j$ should not be present in the transformed face."}, {"title": "CGPT-40 Prompting", "content": ""}, {"title": "C.1 Prompting the model", "content": "For attributes except 'old' and 'young', we use GPT-40 as the attribute detector in our filtering step. The GPT-40 prompt contains: (i) Image: Concatenated transformed face and corresponding source face (ii) Text: Contains list of attributes and asks model to return a JSON containing a 'Yes' or 'No' for each attribute for both source and transformed face. We"}, {"title": "C.2 Improving Human Alignment of GPT-40: An Additional Use-case of our Dataset", "content": "Our attribute survey indicates 75% of images survived our pipeline if human responses were considered for filtering. This shows the GPT-40 responses don't always align with humans. One way to improve human-alignment is few-shot prompting, which we plan to fully explore in future work. Here, we show a brief and preliminary example of few-shot prompting of GPT-40 that improves human-alignment.\nOur preliminary analysis of few-shot prompting considers the dataset used in the Attribute Survey for the attributes 'shoulder length hair', 'pigtails', and 'buzz cut'. One can realize from our filtering step requirements (section 4.2) that transformed faces that survived our pipeline contain an attribute applied according to GPT-4o. Similarly, corresponding source faces do not contain the same attribute according to GPT-40. Thus, for this experiment we consider just pairs of transformed faces and source faces where human annotators felt otherwise. Images that were used as examples in the prompt, distorted or not validated for identity in the Attribute Survey were also excluded.\nUnlike our filtering step where we prompt the model for identifying all attributes at the same time, here, we just prompt the model to identify one attribute (applied attribute) for the concatenated source and transformed image. The two-shot and four-shot results can be found in table 4. While these preliminary results show that few-shot prompting is promising, more work is needed to design prompts that apply to different attributes and evaluate them."}, {"title": "D Training Distortion Detector", "content": "As described in section 4.2, to train the distortion detector, we curated a training dataset consisting of clean faces and distorted faces. We didn't want an overlap between the candidate transformed faces and this training set. So we obtained a set of 10 non-celebrity names for each demographic. We used the source face prompt template (refer section 4.2) and obtained clean faces for 25 variations of these 80 names. The seeds were randomly generated for this process as the clean faces were only needed for training this detector and not generating counterfactuals. An observation we made was as the hyperparameters of SEGA were increased to larger values, the obtained transformed faces contained artifacts that would occlude the identity andor semantics of a face. Using this observation, we generated a set of distorted faces for 3 variations of the non-celebrity names from 19 attributes.\nWe trained a Linear SVM with the training set consisting of CLIP embeddings of these clean and distorted faces. A subset of the distorted faces used in the training set can be found in fig. 4. As the amount of distortion in these images can be higher compared to the candidate transformed faces (see fig. 5), we tuned the distortion detector using survey annotated data to have a recall of 0.97 for detecting distortion. We used the tuned distortion detector in the filtering step to remove distorted faces. The overall performance of this tuned detector is high as only 11 out of the 751 transformed faces that survived the filtering pipeline had any distortion (see section 5.2)"}, {"title": "E User-Survey Details", "content": "We conducted two user-surveys to aid the filtering step in our pipeline (section 4.3). The Distortion Survey was used in tuning the threshold of the distortion detector. The Attribute Survey was used to get human annotations for the candidates selected by our pipeline, and use them to measure the efficacy of the pipeline. Both the Distortion Survey and Attribute Survey were designed on Qualtrics and hosted on the Prolific platform. Both were approved by our Institutional Research Board, had a median survey time of 10 minutes and the participants were paid $2.5 for their responses. In this section, we discuss the details of each survey."}]}