{"title": "KASA: KNOWLEDGE-AWARE SINGULAR-VALUE ADAPTATION OF LARGE LANGUAGE MODELS", "authors": ["Fan Wang", "Juyong Jiang", "Chansung Park", "Sunghun Kim", "Jing Tang"], "abstract": "The increasing sizes of large language models (LLMs) result in significant computational overhead and memory usage when adapting these models to specific tasks or domains. Various parameter-efficient fine-tuning (PEFT) methods have been devised to mitigate these challenges by training a small set of parameters for the task-specific updates of the model weights. Among PEFT methods, LORA stands out for its simplicity and efficiency, inspiring the development of a series of variants. However, LoRA and its successors disregard the knowledge that is noisy or irrelevant to the targeted task, detrimentally impacting model performance and leading to suboptimality. To address this limitation, we introduce Knowledge-aware Singular-value Adaptation (KaSA), a PEFT method that leverages singular value decomposition (SVD) with knowledge-aware singular values to dynamically activate knowledge based on its relevance to the task at hand. We conduct extensive experiments across a range of LLMs on tasks spanning natural language understanding (NLU), generation (NLG), instruction following, and commonsense reasoning. The experimental results demonstrate that KaSA consistently outperforms FFT and 14 popular PEFT baselines across 16 benchmarks and 4 synthetic datasets, underscoring our method's efficacy and adaptability. The source code of our method is available at https://github.com/juyongjiang/KaSA.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) pretrained on massive general domain data have shown remarkable generalization ability, facilitating their application across diverse tasks (Zhao et al., 2023; Touvron et al., 2023b; OpenAI, 2023; Yoo et al., 2024; Jiang et al., 2024). The adaptation of these pretrained language models (PLMs) to specific downstream tasks generally involves full fine-tuning (FFT), where all model parameters are updated and distinct replicas of model parameters are saved for each task (Guo et al., 2021; Mao et al., 2022; Gao et al., 2024). However, the increasing size of LLMs significantly raises the computational and memory costs associated with FFT, making FFT impractical in resource-constrained environments (Lester et al., 2021; Cai et al., 2024; Meng et al., 2024). Consequently, a surge of parameter-efficient fine-tuning (PEFT) methods (Zaken et al., 2021; Li & Liang, 2021; Hu et al., 2021; Liu et al., 2023; Pfeiffer et al., 2021; Houlsby et al., 2019; Liu et al., 2024) have emerged, aiming to reduce the computational and memory costs by only updating a small set of parameters while fixing the base model (Mao et al., 2022; Lialin et al., 2023).\nNotably, LoRA (Hu et al., 2021) is popular for its simplicity and effectiveness (Wang et al., 2024a; Liu et al., 2024; Gao et al., 2024). It reparameterizes the task-specific update $\\triangle W \\in \\mathbb{R}^{n \\times m}$ with a couple of low-rank matrices, A and B, while keeping the base model $W^{(0)} \\in \\mathbb{R}^{n \\times m}$ unchanged during fine-tuning. Without loss of generality, we suppose $n \\geq m$ to simplify the notation. The fine-tuning process of LoRA can be formally expressed as $W^{(0)} + \\triangle W = W^{(0)} + BA^T$, where"}, {"title": "2 RELATED WORK", "content": null}, {"title": "2.1 PARAMETER-EFFICIENT FINE-TUNING", "content": "The increasing LLM scale presents significant challenges to efficiently adapting these models to specific tasks (Lialin et al., 2023; Zhao et al., 2023). In response, a surge of PEFT methods has emerged, reducing the computation burden by updating a minimal set of parameters during fine-tuning (Mao et al., 2022; Karimi Mahabadi et al., 2021; Han et al., 2024). PEFT methods can be generally categorized into selective, additive, and re-parameterized methods (Ding et al., 2022; Lialin et al., 2023; Xu et al., 2023). Selective methods (Zaken et al., 2021; Sung et al., 2021; Guo et al., 2021; He et al., 2023) train a predetermined set of the model's existing parameters while keeping the rest of the model intact. Additive methods (Houlsby et al., 2019; He et al., 2022a; Li & Liang, 2021; Liu et al., 2023; Lester et al., 2021) introduce extra modules or parameters to fine-tune and maintain the original base model frozen. Reparametrized methods (Hu et al., 2021; Dettmers et al., 2023; Zhang et al., 2022; Valipour et al., 2023; Liu et al., 2024) reparameterize the model's weight updates into an equivalent low-rank form for fine-tuning. Among reparameterized approaches, LoRA stands out for its simple yet efficient mechanism of employing two low-rank matrices to approximate task-specific updates. The fine-tuned LoRA matrices can be integrated with the base model, ensuring no inference latency. LoRA has inspired a series of variants, each targeting specific improvements. For instance, DyLoRA (Valipour et al., 2023) trains the low-rank matrices across a spectrum of ranks by sorting the representation learned at different ranks during training, shortening the training time. QLoRA (Dettmers et al., 2023) combines 4-bit quantization with LoRA for enhanced resource efficiency. DORA (Liu et al., 2024) decomposes the base model into magnitude and direction components for fine-tuning, reducing the number of trainable parameters and improving performance over LoRA. Our method, KaSA, diverges from these reparametrized methods by employing a knowledge-aware SVD structure, enhancing the fine-tuning efficacy further."}, {"title": "2.2 SINGULAR VALUE DECOMPOSITION IN NATURAL LANGUAGE PROCESSING", "content": "SVD plays a crucial role in Natural Language Processing (NLP) domain for various applications, such as model compression (Yuan et al., 2023; Wang et al., 2024b; Hsu et al., 2021; Chen et al., 2021), dimensionality reduction of word embeddings (Tanwar et al., 2018; Shyamasundar & Rani, 2016), and latent semantic structure analysis (Deerwester et al., 1990; Kou & Peng, 2015; Horasan et al., 2019). In the rapidly growing realm of LLMs, SVD emerges as a promising, yet relatively underexplored, technique for PEFT. A series of SVD-based PEFT methods exploit the relationship between SVD and matrix rank to ascertain optimal ranks for specific downstream tasks. For example, AdaLoRA (Zhang et al., 2022) employs SVD to reparameterize task-specific updates and adaptively determines the suitable rank through importance scoring, thus improving the model performance and parameter efficiency. SARA (Gu et al., 2024) conducts SVD at the initialization phase to identify the appropriate rank for each layer, thereby maintaining the benefits of LORA and boosting performance. PiSSA (Meng et al., 2024) and MiLoRA (Wang et al., 2024a), as mentioned in Section 1, utilize SVD to optimize LoRA's initialization. Specifically, PiSSA (Meng et al., 2024) only fine-tunes the low-rank matrices initialized with the principal components associated with a few largest singular values, while preserving the residual frozen. This initialization strategy facilitates faster convergence and enhanced performance. Conversely, MiLoRA (Wang et al., 2024a) fine-tunes the minor components associated with minimal singular values, enhancing model"}, {"title": "3 METHODOLOGY", "content": "In this section, we commence with modeling the general PEFT process and training objective in Section 3.1. We subsequently provide a detailed introduction of KaSA in Section 3.2, followed by the description of its training objective in Section 3.3."}, {"title": "3.1 PROBLEM STATEMENT", "content": "Before introducing KaSA, it is essential to delineate and model the process and objective of PEFT for LLMs based on the Transformer architecture (Vaswani, 2017). Fundamentally, PEFT is the process of training a pretrained model to a targeted task using a task-specific dataset. It aims to minimize the divergence between the predicted probability distribution of the fine-tuned model and the actual distribution of the training data, while only modifying a small set of parameters.\nConsider a pretrained model $W^{(0)}$, initially parameterized by $\\Theta_0$. To adapt this model to a particular task, we employ PEFT using a dataset $\\mathcal{D} = \\{(x_t, y_t)\\}_{t=1}^Q$ comprising Q input-output instances. The PEFT process utilizes a limited set of parameters, denoted as $\\Psi$, to learn the task-specific update $\\triangle \\Theta$, ensuring that $|\\Psi| \\ll |\\Theta_0|$. This results in a fine-tuned model W, parameterized by $\\Theta_0 + \\triangle \\Theta(\\Psi)$. The objective is to align the predicted probability distribution of W with the actual distribution of training data, thereby enhancing the fine-tuned model's task performance. The primary objective of PEFT is thus centered on the optimization of $\\Psi$:\n$\\mathcal{L}_1(\\Psi) = \\sum_{(x,y)\\in \\mathcal{D}} \\sum_{t=1}^{y} -log(P_{\\Theta_0 + \\triangle \\Theta(\\Psi)}(y_t|x, y_{<t}))$\n(1)"}, {"title": "3.2 KNOWLEDGE-AWARE SINGULAR-VALUE ADAPTATION", "content": "As depicted in Fig.1, KaSA encompasses two primary stages: 1) the knowledge-based SVD truncation, which removes the noisy knowledge from the base model; and 2) knowledge-aware singular-value adaptation, which involves adjustment of singular values that dynamically activates parametric knowledge based on its relevance to the targeted task."}, {"title": "3.3 TRAINING OBJECTIVE", "content": "FFT typically serves as a comparative performance upper bound for PEFT methods (Valipour et al., 2023). Consequently, we expect that the performance of the model fine-tuned with KaSA will closely approximate that of FFT. We denote the FFT model as $W_{fft} = W^{(0)} + \\triangle W$. We impose a regularization $||W_{fft} - W_{world}||_F$, represented by the Frobenius norm, to constrain the task-specific updates. Based on the properties of Frobenius norms, we can further explore the boundary of the task-specific updates:\n$||W_{fft}||_F + ||W_{world}||_F \\geq ||W_{fft} - W_{world}||_F \\geq ||\\triangle U \\triangle \\Sigma \\triangle V^T||_F = ||\\sum_{j=1}^r \\triangle u_j (\\triangle \\sigma_j) \\triangle v_j ||_F$\n(8)\nTo stabilize the model training and extend the searching space, we introduce $\\mathcal{L}_2$ to minimize the lower boundary of $||W_{fft} - W_{world}||_F$:\n$\\mathcal{L}_2(\\triangle \\Sigma) = ||\\triangle U \\triangle \\Sigma \\triangle V^T||$\n(9)\nAccording to the Eckart-Young-Mirsky theorem (Eckart & Young, 1936), $\\mathcal{L}_2$ is reformulated as:\n$\\mathcal{L}_2(\\triangle \\Sigma) = ||\\triangle U \\triangle \\Sigma \\triangle V^T|| = || \\sum_{j=1}^r \\triangle u_j (\\triangle \\sigma_j) \\triangle v_j || = \\sum_{j=1}^r (\\triangle \\sigma_j)^2$\n(10)"}, {"title": "4 EXPERIMENTS", "content": "In this section, we evaluate KaSA's efficacy across different downstream tasks, including natural language understanding (NLU), natural language generation (NLG), instruction following, and commonsense reasoning. For NLU tasks, we evaluate KaSA with RoBERTa (Liu et al., 2021) and DeBERTaV3 (He et al., 2022b) on the GLUE (Wang et al., 2018) benchmark. For NLG tasks, we assess our method with GPT-2 (Radford et al., 2019) on the E2E NLG Challenge (Novikova et al., 2017) benchmark. We further assess instruction following performance with popular LLMs, including LLaMA3 8B (Meta, 2024), Mistal 7B (Jiang et al., 2023), Gemma 7B (Gemma Team, 2024), and LLaMA2 13B (Touvron et al., 2023b). These models are fine-tuned with different PEFT methods using four synthetic datasets generated by GPT4o, each tailored to summarization, classification, coding, and closed QA. GPT4o is then employed as a judge to evaluate the fine-tuned models' performance, assigning scores on a scale of 10. We also follow (Kopiczko et al., 2023) and (Gao et al., 2024) to fine-tune the four models on the Alpaca dataset (Taori et al., 2023b) and report evaluation results on MT-Bench, with GPT4 serving as the judge, yielding scores within 10. Additionally, we substantiate KaSA's generality by fine-tuning LLaMA2 7B and LLaMA3 8B models on the Commonsense170K dataset (Hu et al., 2023), which includes training sets from eight commonsense reasoning datasets, and evaluating them on individual test sets of these constituent datasets. Finally, we conduct ablation studies to investigate the impacts of different components, budget parameter scalability, and the distribution of knowledge-aware singular values across various layers. All experiments are conducted on NVIDIA A100-SXM4 (80GB) GPUs, except for the NLU experiments, which are conducted on NVIDIA GeForce RTX 3090 (24GB) GPUs."}, {"title": "4.1 BASELINES", "content": "We compare KaSA with FFT and 14 PEFT baselines to substantiate its efficacy and robustness:\n\u2022 Adapter-based methods We consider four representative Adapter tuning methods as baselines: 1) Adapter (Houlsby et al., 2019); 2) AdapterD (R\u00fcckl\u00e9 et al., 2021); 3) Adapter (Lin et al., 2020); and 4) Adapter (Pfeiffer et al., 2021).\n\u2022 LoRA-based methods We select LoRA and its variants: 1) LoRA (Hu et al., 2021); 2) DyLoRA (Valipour et al., 2023); 3) VeRA (Kopiczko et al., 2023); and 4) DORA (Liu et al., 2024).\n\u2022 SVD-based methods Considering that our method is associated with SVD, we chose other SVD-based PEFT baselines: 1) AdaLoRA (Zhang et al., 2022); 2) PiSSA (Meng et al., 2024); 3) MiLoRA (Wang et al., 2024a); 4) SARA (Gu et al., 2024); and 5) CorDA (Yang et al., 2024).\n\u2022 Other methods Apart from the aforementioned baselines, we also consider other important fine-tuning methods: 1) FFT; and 2) BitFit (Zaken et al., 2021).\nTo ensure a fair comparison with these baselines, we meticulously replicate the experimental configurations as described in previous studies (Hu et al., 2021; Zhang et al., 2022; Gu et al., 2024). Introductions of the baselines and comprehensive details of the experimental setup are provided in Appendix B and Appendix E, respectively."}, {"title": "4.2 NATURAL LANGUAGE UNDERSTANDING", "content": "Models and Datasets. For NLU tasks, our method involves fine-tuning foundation models such as ROBERTa-base (125M), ROBERTa-large (355M) (Liu et al., 2021), and DeBERTaV3-base (He"}, {"title": "4.3 NATURAL LANGUAGE GENERATION", "content": "Models and Datasets. For NLG tasks, we employ KaSA and other PEFT baselines to fine-tune both GPT-2 Medium (355M) and GPT-2 Large (774M) models (Radford et al., 2019) on the well-established E2E (End-to-End) NLG Challenge benchmark (Novikova et al., 2017), which focuses on restaurant domain information. The statistics of the E2E NLG Challenge benchmark and the evaluation metrics applied are detailed in Appendix C.2."}, {"title": "4.4 INSTRUCTION FOLLOWING", "content": "Models and Datasets. To validate KaSA's adaptability and versatility, we extend our experiments to include instruction tuning of LLaMA3 8B (Meta, 2024), Mistral 7B (Jiang et al., 2023), Gemma 7B (Gemma Team, 2024), and LLaMA2 13B (Touvron et al., 2023b). We fine-tune the models using four synthetic instruction-following datasets produced by GPT4o, each containing 128K samples, covering tasks such as summarization, classification, coding, and closed QA. Additionally, we fine-tune using the Alpaca dataset (Taori et al., 2023b) and report the evaluation results on MT-Bench"}, {"title": "4.5 \u0421\u043e\u043cMONSENSE REASONING", "content": "Models and Datasets. Following (Wang et al., 2024a), we fine-tune the LLaMA2 7B (Touvron et al., 2023a) and the LLaMA3 8B (Meta, 2024) models using the Commonsense170K dataset, aiming to conduct a comprehensive evaluation across eight well-known commonsense reasoning tasks: BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC-e, ARC-c (Clark et al., 2018), and OBQA (Mihaylov et al., 2018)."}, {"title": "4.6 IN-DEPTH ANALYSIS", "content": "Components Ablation Study. Our method encompasses four principle components: knowledge-based SVD truncation, knowledge-aware singular value adaptation, singular value regularization $\\mathcal{L}_2$, and orthogonal regularization $\\mathcal{L}_3$. To examine the collective contributions of these components, we conduct ablation experiments on MRPC, CoLA, and RTE datasets from GLUE using the RoBERTa-base. Specifically, we compare KaSA with the following variants: (1) standard LORA (as the base); (2) SVD truncation + LoRA; (3) SVD truncation + knowledge-aware singular-value adaptation; (4) SVD truncation + knowledge-aware singular-value adaptation + $\\mathcal{L}_2$; (5) SVD truncation + knowledge-aware singular-value adaptation + $\\mathcal{L}_2$ + $\\mathcal{L}_3$. From the results in Figure 2, we observe that the model performances continually increase as more components are involved in the fine-tuning. The fifth bar in Figure 2 shows that variant (5), the full implementation of KaSA, achieves significant performance improvements across all three datasets. Conversely, excluding any of these components results in performance declines ranging from 2.05% to 3.25%, underscoring their collective importance in enhancing KaSA's effectiveness. Additional results of the components ablation study on SST-2, QNLI, and STS-B datasets are detailed in Appendix F.1."}, {"title": "5 CONCLUSION", "content": "In this paper, we introduce a PEFT method, KaSA, which incorporates SVD with knowledge-aware singular values for dynamic knowledge activation of parametric knowledge according to their relevance to a given task. KaSA commences knowledge-based SVD truncation of minor singular value components to remove noisy knowledge within the base model. Subsequently, it reparameterizes task-specific updates in the SVD form, leveraging knowledge-aware singular values for dynamic knowledge activation according to relevance. Our extensive experiments on various LLMs across tasks in NLU, NLG, instruction following, and commonsense reasoning reveal that KaSA consistently surpasses FFT and a variety of popular PEFT baselines across well-known benchmarks and our synthetic datasets, highlighting the superiority of our method."}, {"title": "A PSEUDOCODE FOR KASA", "content": null}, {"title": "B BASELINES", "content": "To demonstrate its efficacy and robustness, we evaluate KaSA against FFT and multiple well-regarded PEFT baselines. The descriptions of our selective baselines are as follows:\n\u2022 Full fine-tuning (FFT) initializes the base model with pre-trained weights and biases, up-dating all parameters during fine-tuning. Full fine-tuning typically serves as a comparative performance upper bound for PEFT methods (Valipour et al., 2023).\n\u2022 Bitfit (Zaken et al., 2021) fine-tunes the bias vectors, leaving other model parameters un-changed."}, {"title": "C DETAILS OF BENCHMARK DATASETS", "content": null}, {"title": "C.1 GLUE BENCHMARK", "content": "For natural language understanding (NLU), we employ the GLUE benchmark (Wang et al., 2018), which is a widely used benchmark containing a collection of 8 NLU datasets, including CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, and RTE. We present the statistical information of the GLUE benchmark in the table below."}, {"title": "C.2 E2E NLG CHALLENGE", "content": "For natural language generation (NLG), we utilize the E2E (End-to-End) NLG Challenge dataset (Novikova et al., 2017), which is commonly used for the evaluation of natural language generation models. This dataset includes approximately 42k training samples, 4.6k validation samples, and 4.6k test samples from the restaurant domain. The E2E dataset involves evaluations across five metrics: BLEU, NIST, METEOR, ROUGE-L, and CIDEr. Detailed explanations of these metrics are as follows:\n\u2022 BLEU (Bilingual Evaluation Understudy) evaluates the quality of machine-generated text by comparing it to one or more human-generated reference translations."}, {"title": "C.3 SYNTHETIC DATASET", "content": "For instruction following tasks, we employ synthetic datasets generated using GPT4o, based on the foundational \u201cNo Robots\u201d seed dataset (Rajani et al., 2023). Task-specific subsets, including summarization, classification, coding, and closed QA, serve as seeds for generating synthetic data through the framework proposed by (Park et al., 2024). Table 7 presents the volume of data samples and token-level statistical information for these task-specific synthetic subsets."}, {"title": "C.4 ALPACA AND MT-BENCH", "content": "Alpaca (Taori et al., 2023a) is a well-known instruction dataset that contains 51k instruction-following demonstrations generated by text-davinci-003. These data are synthesized using an im-proved self-instruct method (Wang et al., 2023). The dataset is designed for instruction-tuning LLMs"}, {"title": "C.5 \u0421\u043e\u043cMONSENSE REASONING", "content": "The Commonsense170K dataset (Hu et al., 2023) contains data samples from eight well-known commonsense reasoning tasks:\n\u2022 BoolQ (Clark et al., 2019) dataset comprises 15,942 naturally occurring yes/no questions, generated in unprompted and unconstrained settings.\n\u2022 PIQA (Bisk et al., 2020) dataset consists of samples structured as multiple-choice questions, each presenting a question with two possible solutions that require physical com-monsense to answer.\n\u2022 SIQA (Sap et al., 2019) dataset contains multiple-choice questions regarding the prag-matic implications of social events, which can measure LLMs' abilities to address social commonsense reasoning.\n\u2022 HellaSwag (Zellers et al., 2019) dataset includes commonsense natural language inference questions, offering a context and multiple endings to complete it."}, {"title": "D PROMPT TEMPLATES", "content": "Following the typical practices of (Wang et al., 2023) and (Zheng et al., 2023), we leverage two specialized prompt templates: 1) one for generating synthetic datasets and 2) another for evaluating the outputs of fine-tuned LLMs. To be specific, Figure 5 presents the prompt template crafted for generating synthetic data aimed at the summarization task, whereas Figure 6 shows the prompt template for other tasks. We guide GPT4o in generating analogous data samples by using a reference example pair consisting of a prompt $instruction and its corresponding response $response from the training subset of the seed dataset. In addition, the template is designed to request multiple synthetic data samples in a single query, thus maximizing the efficiency of API use. On the other hand, Figure 7 shows the prompt template used for assessing the precision and similarity between the response $lm_response and $human_response given the same $instruction from the test subset of the seed dataset, where the $ symbol indicates a placeholder, designed to be substituted with actual data during the runtime. We only report the precision results in our experiments for the sake of brevity. Given the unique features of different downstream tasks, there is no optimal prompt template that universally applies. Therefore, the actual content of the prompt template is adjusted to align with the specific requirements of the task for which the synthetic dataset is being generated."}, {"title": "E TRAINING DETAILS", "content": null}, {"title": "E.1 NATURAL LANGUAGE UNDERSTANDING", "content": "For NLU tasks, we align with the experimental setup detailed in (Hu et al., 2021; Zhang et al., 2022) for a fair comparison. The detailed configurations of KaSA for ROBERTa-base, RoBERTa-large, and DeBERTaV3-base on the GLUE benchmark are depicted in Table 8 and Table 9, respectively. It is important to note that our adaptation process for the MRPC, RTE, and STS-B tasks begins with the pre-trained RoBERTa model, rather than a model that has already been adapted to MNLI. As a result, we fine-tune the models on all datasets starting from their original pre-trained weights. The results we present are the median results from 5 runs, each conducted with a distinct random seed."}, {"title": "E.2 NATURAL LANGUAGE GENERATION", "content": "For NLG tasks, our KaSA adheres to the experimental setup outlined in (Hu et al., 2021; Gu et al., 2024) to ensure a fair comparison. The comprehensive configurations of KaSA for GPT-2 Medium and GPT-2 Large models on the E2E NLG Challenge benchmark are depicted in Table 10."}, {"title": "E.3 INSTRUCTION FOLLOWING", "content": "For instruction following tasks, we adopt the framework proposed by (Park et al., 2024) to streamline the processes of data synthesis, fine-tuning, and evaluation. We fine-tune several of the most popular LLMs, including LLaMA3 8B, Mistal 7B, Gemma 7B, and LLaMA2 13B, using KaSA and different PEFT baselines to facilitate comparative analysis. Detailed hyperparameter configurations are provided in Table 11."}, {"title": "\u0415.4 \u0421\u043e\u043cMONSENSE REASONING", "content": "We adhere strictly to the hyperparameter configurations for training and evaluation as specified by (Wang et al., 2024a) and (Hu et al., 2023), without any tuning. The specific hyperparameter configurations used are shown in Table 12."}, {"title": "F ADDITIONAL EXPERIMENTAL RESULTS", "content": null}, {"title": "F.1 COMPONENTS ABLATION STUDY ON SST-2, QNLI, AND STS-B", "content": "Figure 8 shows the results of ablation studies conducted on the SST-2, QNLI, and STS-B datasets. From the results, we observe that: 1) the model's performance consistently improves with the inclusion of additional components during fine-tuning; 2) excluding any of these components leads to a decline in performance. These findings align with that observed in Section 4.6, emphasizing the effectiveness of each designed principal component of KaSA in enhancing model performance."}, {"title": "F.2 RANK k OF KNOWLEDGE-BASED SVD TRUNCATION", "content": "As depicted in Section 1, components of the original base model weight matrix $W^{(0)}$ associated with smaller singular values are identified to contain noise or less relevant information (Sharma et al., 2023; Wang et al., 2024a). This presence can adversely affect the convergence of model training"}, {"title": "G INITIALIZATION AND SINGULAR-VALUE ADAPTATION ANALYSIS", "content": "In this section, we conduct a detailed analysis of initialization dilemmas associated with PiSSA and MiLoRA, and subsequently explore the core advantages of KaSA, aiming to provide a comprehensive understanding of the foundational principles governing these PEFT methods. Before embarking on a detailed examination of each method, we summarize the general mechanism underpinning PEFT. Considering a base model characterized by a weight matrix $W^{(0)} \\in \\mathbb{R}^{n \\times m}$, PEFT aims to efficiently fine-tune $W^{(0)}$ by learning a task-specific update $\\triangle W$ with as few trainable parameters as possible, such that the updated weights $W^{(0)} + \\triangle W$ are better aligned with the requirements of downstream tasks. PEFT approaches generally involve keeping the base model $W^{(0)}$ frozen during training, while exclusively updating the parameters of $\\triangle W$."}, {"title": "G.1 INITIALIZATION DILEMMAS OF $\\triangle W$ IN PISSA AND MILORA", "content": "PiSSA employs SVD on the base model weight matrix $W^{(0)} \\in \\mathbb{R}^{n \\times m}$, decomposing it as:\n$W^{(0)} = U \\Sigma V^T$\n(13)"}]}