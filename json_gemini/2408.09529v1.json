{"title": "Revisiting the Graph Reasoning Ability of Large Language Models: Case Studies in Translation, Connectivity and Shortest Path", "authors": ["Xinnan Dai", "Qihao Wen", "Yifei Shen", "Hongzhi Wen", "Dongsheng Li", "Jiliang Tang", "Caihua Shan"], "abstract": "Large Language Models (LLMs) have achieved great success in various reasoning tasks. In this work, we focus on the graph reasoning ability of LLMs. Although theoretical studies proved that LLMs are capable of handling graph reasoning tasks, empirical evaluations reveal numerous failures. To deepen our understanding on this discrepancy, we revisit the ability of LLMs on three fundamental graph tasks: graph description translation, graph connectivity, and the shortest-path problem. Our findings suggest that LLMs can fail to understand graph structures through text descriptions and exhibit varying performance for all these three fundamental tasks. Meanwhile, we perform a real-world investigation on knowledge graphs and make consistent observations with our findings. The codes and datasets are available.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have shown remarkable achievements in a multitude of reasoning tasks, ranging from mathematical, commonsense and symbolic problem-solving (Luo et al., 2023; Creswell et al., 2023), to more specialized applications like dialogue systems (Ouyang et al., 2022), program debugging (Surameery and Shakor, 2023) and scientific discovery (Boiko et al., 2023). In this work, we focus on graph reasoning capability, where LLMs employ an explicit graph, sourced either from the input data or external resources, to infer the outcome. This reasoning ability is crucial and can be applied across various domains, such as improving question-answering system by a domain-specific knowledge graph (Huang et al., 2022), facilitating planning in autonomous agents through the tool relation graph (Liu et al., 2024), and enhancing robot navigation via physical maps (Creswell et al., 2022). \nThere are recent studies initially exploring the LLM's graph reasoning capability. On the one hand, the theoretical work (Feng et al., 2024) proved that LLMs have the ability to mimic a powerful decision-making framework (i.e., dynamic programming), to solve the complex tasks. This suggests that LLMs are capable of handling certain graph reasoning tasks that can be formulated as decision-making problems, including breadth-first search for graph connectivity, and the Dijkstra for shortest path problem. On the other hand, recent empirical studies, such as GPT4Graph (Guo et al., 2023) and NLGraph (Wang et al., 2024), found that LLMs could fail in these graph tasks. This discrepancy between theoretical expectations and practical observations indicates a critical gap in our comprehension of LLMs' graph reasoning abilities. In light of this, we aim to delve deeper into fundamental graph tasks to uncover the limitations inherent in LLMs, assess the impact of these limitations in real-world graphs, and propose possible explanations to understand the discrepancy.\nIn this work, we re-evaluate three fundamental graph reasoning tasks: graph description translation, graph connectivity, and the shortest path problem. First, we check whether LLMs can comprehend graph structures through the translation of varied graph descriptions (See Section 3.1). We summarize the three most popular graph description methods and evaluate the translation tasks among them. Despite it is a simple reasoning task and LLMs could achieve high performance, LLMs are not entirely error-free. Then, we explore graph connectivity and examine LLMs systematically by considering varying connectivity lengths between nodes, diverse types of disconnections and different graph descriptions (See Section 3.2). Existing works(Wang et al., 2024; Luo et al., 2024) primarily focuses on the influence of graph size while considering only a limited range of connectivity types, leading to biased evaluations in connectivity tasks, as demonstrated in Figure 1. To address this, we constructed a balanced and comprehensive dataset. Our investigations on this dataset indicate that in addition to graph size, connection types and graph descriptions also play significant roles. Moreover, we conduct experiments to a more challenging problem, i.e., the shortest path problem (Section 3.3), and investigate real-world graphs, i.e., entity connections in the knowledge graphs (Section 4). The consistent observations are made, suggesting the same underlying mechanism employed by LLMs in graph reasoning tasks."}, {"title": "2 Related work and Background", "content": "2.1 Evaluation on graph reasoning tasks\nRecent efforts have been made on graph reasoning evaluations (Guo et al., 2023; Fatemi et al., 2023; McLeish et al., 2024). NLGraph (Wang et al., 2024) evaluates LLMs across the 8 fundamental graph reasoning tasks, suggesting that LLMs have preliminary graph reasoning abilities. GraphInstruct (Luo et al., 2024) extends the graph reasoning benchmark to 21 classical graph tasks and introduces a step masking method to enhance the graph reasoning abilities of LLMs. Additionally, VisionGraph (Li et al., 2024) provides a multimodal version of the graph reasoning task benchmark, extending its applicability beyond text.\n2.2 Graph connectivity in theory\nLLMs, through their transformer architecture, have demonstrated essential capabilities for reasoning tasks (Giannou et al., 2023; Yang et al., 2023; Sanford et al., 2024b). Specifically, for the graph reasoning tasks, de Luca and Fountoulakis (2024) suggest that looped transformers are able to simulate every step in a graph algorithm. Sanford et al. (2024a) reveal that a single-layer transformer is sufficient for a naive graph connectivity task.\n2.3 LLMs for graphs in the applications\nDespite LLMs having capabilities in graph reasoning tasks in theory, there remains a gap between text understanding and graph reasoning (Chai et al., 2023; Zhao et al., 2023). Therefore, some recent work approves the use of additional tools to help LLMs understand graphs. Recent studies have validated the use of extra tools to enhance LLMs' comprehension of graphs. GraphEmb (Perozzi et al., 2024) employs an encoding function to augment prompts with explicit structured information. Additionally, GraphWiz (Chen et al., 2024) fine-tunes LLMs using graph reasoning datasets to achieve higher performance in graph tasks. However, when LLMs are pretrained using text data, their limitations in graph reasoning tasks remain unclear. In this work, we do a comprehensive study on the failures of LLMs in graph reasoning tasks. We summarize and analyze the potential reasons why LLMs fail in graph reasoning only using text prompts.\n2.4 Theoretical support for graph reasoning tasks\nFeng et al. (2024) prove that if a task can be deconstructed into subtasks, it can be solved by LLMs. Based on this, Wu et al. (2024) offer insights into transforming message-passing processes among graphs into subtasks of message-passing among nodes using transition functions, suggesting that LLMs are capable of handling graph decision tasks. Specifically, it can be theoretically proven that graph connectivity and shortest-path tasks are two examples of problems solvable by LLMs.\nSuppose that the structure of a graph can be represented as \\(G = (X,E,E)\\), where \\(X\\) is the set of nodes, E is the edge set, and \\(& is the feature set of the edges. For the graph connectivity task, we start from node \\(n_i\\) and end at node \\(n_j\\). The transition function F(i, j) for the graph"}, {"title": "3 Limitations of LLMs in graph reasoning", "content": "In this section, we empirically revisit the graph reasoning ability via case studies. In particular, we introduce three fundamental graph tasks: graph description translations in Section 3.1, graph connectivity in Section 3.2, and the shortest path task in Section 3.3. Finally, we summarize and analyze our findings in Section 3.4."}, {"title": "3.1 Graph description translation", "content": "3.1.1 Graph Descriptions\nTo begin with, we first describe the graph properties denoted as: G describes a [properties] graph among \\(x \u2208 X\\), where [properties] define the graph types, such as undirected, directed, or knowledge graphs. Then, we use different graph descriptions to introduce their structures.\nWe summarize three types of graph structure description methods that have been widely used by the previous works (Fatemi et al., 2023; McLeish et al., 2024) as shown in Figure 2. They are (1) Adjacency Matrix: describing the adjacency matrix of a graph; (2) Node List: referring to the neighbors of a central node on a graph, and (3) Edges List: listing every edge of a graph. Adjacency Matrix is denoted as \\(A \u2208 R^{N\u00d7N}\\), where N is the number of nodes. In the text description, it encodes a paragraph by \\(N \u00d7 N\\) binary tokens.\nNode List uses the neighbors of a central node to describe a graph. For instance, consider the set of sentences \\(S_N = {s_1,s_2,...,s_N}\\), which describes the graph via the neighbors [u] of node vi with the edge feature \u20ac. A single sentence is as follows:\nsi = Node vi [relation] Nodes {[u, \u20ac]u\u2208Nv;,EEE(vi,u)}.\nNote that the [relation] varies across different types of graphs. In undirected graphs, we use the relation \"is connected to,\" whereas in directed graphs,"}, {"title": "3.1.2 Translations on graph descriptions", "content": "If LLMs can comprehend the structures of a graph, such understanding should be independent of the methods used to describe the graph. Therefore, to verify the ability of LLMs to understand the structural information of a graph, we design a graph translation description task. This task requires LLMs to use the input graph description to generate various descriptions. After that, we will compare these descriptions to determine if they represent the same graph structure.\nNote that the number of tokens in the Adjacency Matrix depends on the number of nodes. This suggests that the Adjacency Matrix may require more tokens in dense graphs than Node or Edge Descriptions, limiting its applicability in the real world when the graph size is large. Therefore, we only apply the Adjacency Matrix as the target format in the graph description translation task while employing Node List and Edge List as both source and target descriptions. It is the same reason that we use Node List and Edge List for graph connectivity"}, {"title": "3.2 Revisit graph connectivity task", "content": "3.2.1 Connectivity types\nPrevious studies suggest that large language models (LLMs) possess essential capabilities for graph connectivity tasks (Wang et al., 2024; Luo et al., 2024), yet they still fail in some instances. To further investigate the graph connectivity task, we begin by analyzing the samples where failures occurred based on those two baseline datasets.\nWe first categorize the types of connectivity samples. For the samples of connected nodes, we classify them according to the path length, which is denoted as K-hops. Besides, for the samples of unconnected nodes, we label them into three categories: Singleton, Isolated Components (IC), and Asymmetric, as shown in Figure 3. Singleton denotes that one node is isolated. Isolated Components indicate that these two nodes belong to separate components in the graph. Note that a Singleton is a special case of Isolated Components. The distinction lies in the representations using Node List and Edge List, where the isolated node is not included in the descriptions of the graph structure, such as Node 3 in Figure 2. Asymmetric is designated for directed graphs, highlighting situations where a path exists from one node to another, but the reverse path does not exist, indicating a one-way connectivity.\nWe calculate the distribution of connectivity types in the baseline datasets, as shown in Appendix C.1, Table 13, and subsequently conduct an experiment on them. The results, presented in Appendix C.2 Table 8, indicate that the baseline datasets lack a balanced distribution across different connectivity types. More importantly, LLMs exhibit varying performances across these types. Thus, it is crucial to establish a balanced dataset to better evaluate graph connectivity.\n3.2.2 Dataset Construction\nIn previous work, NLGraph (Wang et al., 2024) included only an undirected graph dataset for the connectivity task, and GraphInstruct (Luo et al.,"}, {"title": "3.2.3 Evaluation Metrics", "content": "Instead of only evaluating the accuracy of graph connectivity, we also want to check if the reasoning path to make the prediction can support the prediction. Thus, the prompt is defined as follows: \"If a path exists, present the path formatted as \"Node #1 -> Node #2.\"; If no path is found, state \"No path.\". Therefore, to evaluate the reliability of such paths, we design two novel metrics, FidelityAcc (Facc) and Path Consistency Ratio (PCR), which are used to analyze the correctness of reasoning paths. Facc evaluates whether the reasoning path to infer the answer is correct or not. The formulation is denoted as:\n\\(Facc = \\frac{1}{M}\\sum_{i=1}^{M} \\left(\\mathbb{I}(\\hat{y_i} = y_i) \\land (p_i \\in P)\\right)\\), where\n\\(\u0177_i\\) denotes the predicted answer, \\(y_i\\) the ground truth answer, \\(p_i\\) the predicted path, and P the set of reachable paths. M is the number of data samples. Facc correctly identifies the answer only when both the connective prediction and the path prediction are accurate. The range of Facc is [0, 1], where a higher score indicates greater consistency with the ground truth. A high accuracy with a low Facc score suggests that the reasoning paths cannot well support connectivity predictions, which could indicate that LLMs are hallucinating.\nMultiple reachable paths exist within a graph. LLMs demonstrate superior reasoning abilities if they can identify a shorter path. To assess the paths LLMs select for reasoning, we introduce the Path Consistency Ratio (PCR):\nPCR = \\(\\frac{\\sum_{1}^{M} \\frac{|p_i|}{|p^*_i|}}{M}\\), where\n\\(|p_i|\\) represents the number of nodes in the path, while \\(|p^*_i|\\) denotes the number of nodes in the shortest path. We evaluate PCR only when the LLMs give the correct path. A higher score indicates that the LLMs are more adept at selecting the shortest path between two nodes."}, {"title": "3.2.4 Results", "content": "We select tree representative large language models, GPT-3 (GPT-3.5-turbo-0301), GPT-4 (GPT-4-0125-preview) and LLAMA 3 (LLAMA3.0-70B) with the temperature equal to 0.\nUndirected Graph Results We start with the undirected graph datasets and show the results in Table 2. First of all, GPT-4 has better reasoning ability compared with GPT-3 and LLAMA 3 across all cases, regardless of the graph difficulty, graph description or the categories of connectivity.\nSecondly, we have following observations by comparing different connectivity situations: (1) The difficulty of reasoning increases as the path"}, {"title": "3.3 The shortest-path problem", "content": "The shortest path problem is another essential task theoretically proven to be achievable by LLMs, yet it fails in practice. Compared to the graph connectivity task, it is more challenging because it requires not only determining whether nodes are connected but also calculating edge weights to identify the shortest path among multiple potential solutions. Next, we explore if the varied performance of LLMs across different connectivity types is also applicable to the shortest-path problem.\nExperimental setup We study the shortest-path problem using the Easy datasets from the unweighted graphs as mentioned in Section 3.2. For the weighted graphs, we applied similar strategies that were used in undirected graph generations to generate the directed and undirected graph datasets. The directed graph datasets include two types, whether there are negative edges in the graphs. Appendix C.1 Table 13 shows the details. The graph structure descriptions are shown in Appendix A\nResults We use GPT-4 to illustrate an example of the shortest-path problem. Table 4 displays the results of LLMs' performance. The findings for"}, {"title": "3.4 Analysis of other factors", "content": "3.4.1 Impact of the algorithm prompts\nIn-context learning approaches, including Chain-of-Thought (CoT) (Wei et al., 2022) and zero-Chain-of-Thought (0-CoT) (Kojima et al., 2022), have been widely utilized in LLMs to enhance their reasoning capabilities. Meanwhile, specifically in graph-related tasks, previous works combined the prompts with the graph algorithms. However, they do not demonstrate consistent improvement (Wang et al., 2024). In this subsection, we revisit these approaches in detail.\nWe consider several graph algorithms in the experiments. For the graph connectivity task, we focus on the Breadth-First Search (BFS) and we employ the Dijkstra algorithms to soleve the shortest path problem. We utilize Node descriptions to search the connectivity and shortest pathes in Easy setting by GPT-4. The prompts examples are shown in Appendix B. The results are detailed in Table 5.\nThe observations can be summarized as follows: (1) In the connectivity task, few-shot examples help LLMs recognize isolated components. This is because few-shot examples enable the LLMs to correctly output 'No connection' when they do not find a connected path. (2) In the shortest path cases, few-shot examples do not consistently lead to better performance. However, performance improves when the Dijkstra-CoT method is applied. This suggests that while LLMs may use multiple strategies to make decisions, but a specific algorithm can guide them toward a unique solution.\n3.4.2 The influence of node names\nFatemi et al. (2023) suggest that different naming methods for graphs can yield varied results. This variation is attributed to the graph node IDs occupying the same space as the pre-trained data of LLMs. Thus, we further evaluated the impact of naming conventions on nodes for the connectivity task. Table 6 summarizes the results for GPT-4"}, {"title": "4 A case study on knowledge graphs", "content": "To determine if our findings from previous sections are applicable to real-world graphs, we conducted the entity connections on knowledge graphs.\nDataset We used WN18RR (Shang et al., 2019) as the base dataset, which provides both ID names and Entity names. The ID names consist of strings of random numbers, and Entity names are used as specific and meaningful identifiers. From its training set, we randomly selected 150 subgraphs based on ego graphs with a depth of 3. Within each subgraph, we identified two nodes with the longest paths and segmented the paths into k'-hops. This strategy allowed us to generate k' question-answer pairs, ranging from 1-hop to k'-hop. Table 12 shows the details of our sampled dataset. We take both Node List and Edge List in the experiment."}, {"title": "5 Conclusion", "content": "We focus on the graph reasoning ability of LLMs in this paper. Recently, there exists a discrepancy between theoretical understanding and empirical experiments, where LLMs can handle complex decision-making tasks in theory, yet empirical findings often show poor performance. To bridge this gap, we have revisited fundamental graph-related tasks, including translation, graph connectivity and shortest path tasks. We construct a balanced and comprehensive dataset to involve various situations and obtain extensive observations. Our results show the failure cases of LLMs and reveal that LLMs may utilize different algorithms to solve the complex graph-related task, depending on the input"}, {"title": "FK-hops influence on the connectivity task", "content": "In Section 3.2, we have demonstrated that performance in the graph connectivity task is closely related to the number of nodes and k-hops in a graph. However, it is important to note that smaller graphs inherently support shorter paths. To fairly assess the impact of k-hops on different graph sizes, we further evaluate the relations between k-hop and graph density.\nWe create a subset with 100 undirected graphs where the graph node number is 16 - 36 and the density is in the range of (0.2,0.4) and evaluate them by Node and Edge List descriptions. The results are shown in Figure 4.\nThe results indicate that 1-hop cases maintain a very high accuracy regardless of graph density, while 2-hop and 3-hop cases show a slight accuracy decrease. In contrast, 4-hop and 5-hop cases exhibit high accuracy only in sparse graphs but significantly decline when graph density approaches 0.38. This suggests that LLMs become confused as the graph complexity increases.\nComparing the Node List and Edge List descriptions, it is observed that the Node List exhibits a smaller reduction in performance compared to the Edge List. This suggests that the Node List may be more effective in describing complex graphs."}, {"title": "G Failed cases", "content": "In this section, we will list some failed cases. We mark the added edges in Red and ignored edges in Green.\nG.1 Translation for Edge List to Node List\nQuestion: Your task is giving the neighbors of each node.G describes an undirected graph among node 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, and 12.\nNode 0 is connected to Node 1. Node 0 is connected to Node 5. Node 0 is connected to Node 9. Node 0 is connected to Node 12. Node 0 is connected to Node 3. Node 0 is connected to Node 10. Node 0 is connected to Node 8. Node 0 is connected to Node 11. Node 0 is connected to Node 7.\nNode 1 is connected to Node 2. Node 1 is connected to Node 4. Node 1 is connected to Node 3. Node 1 is connected to Node 12. Node 1 is connected to Node 9. Node 1 is connected to Node 11. Node 1 is connected to Node 10. Node 1 is connected to Node 5. Node 1 is connected to Node 6.\nNode 2 is connected to Node 3. Node 2 is connected to Node 4. Node 2 is connected to Node 6. Node 2 is connected to Node 10. Node 2 is connected to Node 9. Node 2 is connected to Node"}]}