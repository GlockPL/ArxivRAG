{"title": "KG-TRICK 7: Unifying Textual and Relational Information Completion of Knowledge for Multilingual Knowledge Graphs", "authors": ["Zelin Zhou", "Simone Conia", "Daniel Lee", "Min Li", "Shenglei Huang", "Umar Farooq Minhas", "Saloni Potdar", "Henry Xiao", "Yunyao Li"], "abstract": "Multilingual knowledge graphs (KGs) provide high-quality relational and textual information for various NLP applications, but they are often incomplete, especially in non-English languages. Previous research has shown that combining information from KGs in different languages aids either Knowledge Graph Completion (KGC), the task of predicting missing relations between entities, or Knowledge Graph Enhancement (KGE), the task of predicting missing textual information for entities. Although previous efforts have considered KGC and KGE as independent tasks, we hypothesize that they are interdependent and mutually beneficial. To this end, we introduce KG-TRICK, a novel sequence-to-sequence framework that unifies the tasks of textual and relational information completion for multilingual KGs. KG-TRICK demonstrates that: i) it is possible to unify the tasks of KGC and KGE into a single framework, and ii) combining textual information from multiple languages is beneficial to improve the completeness of a KG. As part of our contributions, we also introduce WikiKGE-10++, the largest manually-curated benchmark for textual information completion of KGs, which features over 25,000 entities across 10 diverse languages.", "sections": [{"title": "1 Introduction", "content": "Knowledge graphs (KGs) aim to encode structured information about the world in a machine-readable format (Hogan et al., 2021), providing high-quality relational and textual information for various NLP applications, such as question answering (Mckenna and Sen, 2023), information retrieval (Reinanda et al., 2020), entity linking (Hu et al., 2023), and machine translation (Modrzejewski et al., 2020; Conia et al., 2024), among others. Although large language models (LLMs) are increasingly retrieving information from KGs to improve their factuality and performance in many NLP tasks (Wang et al., 2023), their effectiveness in multilingual applications is limited due to the important gap between the completeness of English and non-English information in KGs (Peng et al., 2023). In fact, KGs are not complete: a non-negligible quantity of information about entities (e.g., entity names, aliases, and descriptions) and relations (e.g., the connections between entities) is missing in non-English languages (Conia et al., 2023). Therefore, improving the completeness of KGs has attracted significant attention over the years.\nTo address this issue, the research community has worked on two main tasks: Knowledge Graph Completion (KGC) and Knowledge Graph Enhancement (KGE). KGC is the task of predicting missing relations between entities already defined in a KG (Bordes et al., 2013), while KGE is the task of predicting missing textual information for entities in a KG (Conia et al., 2023). More formally, KGC \u2013 also known as link prediction \u2013 is often defined as follows: given a KG G, the task is to predict the missing tail entity t given the head entity h and the relation r in a triplet (h, r, ?). For example, given the triplet (Joe Biden, occupation, ?), a possible answer could be politician or, more specifically, the ID Q82955 of the politician entity. On the other hand, KGE is defined as follows: given an entity e in a KG G, the task is to predict missing"}, {"title": "2 Related Work", "content": "In this section, we briefly review the literature on Knowledge Graph Completion (KGC) and Knowledge Graph Enhancement (KGE) and discuss the challenges of completing textual and relational information in multilingual knowledge graphs.\nMultilingual Knowledge Graphs. As mentioned above, KGs aim to encode information about our world knowledge in a structured, machine-readable format (Hogan et al., 2021). This information also includes lexicalizations, such as entity names, aliases, and descriptions; when these are available in multiple languages, the KG is called a multilingual KG. There are different ways to construct and organize multilingual KGs. For example, in DBPedia (Lehmann et al., 2015), an entity is language-dependent and is represented in different languages using different entity IDs. Instead, in Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014), an entity is language-independent and is represented by the same entity ID to which different language-specific labels are attached. The construction of multilingual KGs is an active area of research, and there are several challenges to be addressed, such as the alignment of entities across languages (Chakrabarti et al., 2022), the completion of missing relational information (Chen et al., 2020b), and the addition of textual information, especially in non-English languages (Conia et al., 2023).\nKnowledge Graph Completion. The task of KGC is to predict missing relations between entities already defined in a KG (Bordes et al., 2013). This task has been studied extensively in the literature, and there are several categories of methods, including embedding-based methods (Lin et al., 2015b), path-based methods (Lin et al., 2015a), and rule-based methods (Chen et al., 2020a). More recently, sequence-to-sequence models have been proposed to solve KGC by treating it as a text-to-text generation task, where the input is a partial triplet and the output is the missing entity (Saxena et al., 2022). However, these approaches have been designed for monolingual KGs, as multilinguality adds a layer of complexity to the task. More specifically, the completion of missing relations in a multilingual KG requires the ability to process and generate text in multiple languages. Chakrabarti et al. (2022) have taken a step in this direction by including an auxiliary task to translate entity names, but they neither consider completing triples across languages nor the completion of more complex textual information, such as entity descriptions.\nKnowledge Graph Enhancement. The task of KGE is to predict missing textual information for entities in a KG. This task is more recent in the literature, but there are several approaches to tackle it, such as machine translation, Web search, and language model-based methods (Conia et al., 2023). However, Conia et al. (2023) have mainly focused on i) combining answers from multiple KGE systems to improve coverage and precision, and ii) evaluating the quality of the textual information generated by KGE systems for popular entities only, while iii) ignoring the connection between KGE and KGC, especially in the multilingual setting."}, {"title": "3 Unifying Textual and Relational Information Completion", "content": "In this section, we introduce KG-TRICK, or how we unify the tasks of KGC and KGE into a single framework, and how we leverage the complementary textual information from multiple languages to improve the completeness of a multilingual KG."}, {"title": "3.1 Task Reformulation", "content": "Given the similarities between the two tasks of KGC and KGE and the interdependence between them (see Section 1, in which we provide a high-level intuition), we reformulate both tasks as a single multilingual text-to-text generation task as shown in Figure 1. KG-TRICK consists of three main components, namely the verbalization, the fine-tuned multilingual sequence-to-sequence model and the ensemble module to obtain the predicted entities for KGC task. This unified framework allows us to i) treat KGC and KGE as a single task, and ii) leverage the complementary textual information from multiple languages to better complete factual information and reversely to improve the latent, dense representation of the fine-tuned sequence-to-sequence model, leading to improved KGE performance. Thanks to our reformulation, KG-TRICK sees both tasks as the task of predicting the tail entity t given the head entity h and the relation r in a triplet (h, r, ?), as we will detail in the following sections.\n3.1.1 KGC as Text-to-Text Generation\nIn KGC, the task is to predict the missing tail entity t given the head entity h and the relation r in a triplet (h, r,?). We first reformulate this task as a text-to-text generation task, where the input is a partial triplet composed of the primary name and short description of the head entity h and the relation r. The model is then asked to generate the missing tail, or, more precisely, the primary name and short description of the tail entity t.\nOne important drawback of this reformulation is that it does not take into account the input and output languages, which is crucial for multilingual KGs. We overcome this limitation by extending the triplet to a tuple of five elements, which include the source and target languages. More specifically, the input to the model is now a tuple (ls, lt, h, r, ?), where l, is the source language of the input, lt is the target language of the output, h = primary name + short description, and r is the relation. The model then predicts t, i.e., the primary name and short description of the tail entity in lt. For example, given the input (en, es, h, r, ?), the model generates the primary name and short description of the entity pol\u00edtico | persona involucrada en la pol\u00edtica; while given the input (en, zh, h, r), the model generates the primary name and short description of the entity \u653f"}, {"title": "3.1.2 KGE as Text-to-Text Generation", "content": "In KGE, the task is to predict missing textual information for entities in a KG. This task can also be reformulated as a text-to-text generation task, similar to KGC. We can immediately see that the formulation outlined above for KGC can be directly applied to KGE, with the only difference being that the head entity h may be represented only by its primary name in case we want to generate a short description for h itself. Moreover, we also allow the head entity h and the tail entity t to be the same entity, which allows us to generate aliases for an entity in a specific language. For example, given the partial triplet Joe Biden: President of the US | has name I, the model predicts the primary name of the entity Joe Biden in the target language but it can also generate one or more aliases, such as Joseph R. Biden Jr. or Joseph Robinette Biden Jr. in English, or\u4e54\u00b7\u62dc\u767b or \u4e54\u00b7\u7f57\u5bbe\u5167\u7279\u00b7\u62dc\u767b in Chinese. Interestingly, when this reformulation is used in its most simple form, i.e., by using only the primary name of the head entity, it becomes equivalent to translation into the target language. This is a powerful feature, as it allows us to generate missing textual information in any language in KG."}, {"title": "3.2 The KG-TRICK Model", "content": "Unifying KGE and KGC, we implement KG-TRICK as a general sequence-to-sequence model, which learns to generate both missing relational and textual missing information in a KG. More formally, given a tuple (ls, lt, h, r, ?), the model is asked to generate t from the source language ls to the target language lt conditioned on h and r as following:\n$$0 = KG-TRICK(ls, lt, h, r,?)$$\nwhere o is the output generated by the model. In other words, o is the primary name and short description of the tail entity in the target language, and KG-TRICK learns to estimate the probability of generating o given the input (ls, lt, h, r, ?).\nKG-TRICK can be implemented using any sequence-to-sequence architecture, such as transformer (Vaswani et al., 2017) or recurrent neural network (Rumelhart et al., 1985). In practice, we conducted our experiments with one main architecture, i.e., multilingual BART, which is a transformer-based encoder-decoder model, and we found that it performs well on the task, as shown in Section 5. The model can be trained using a standard maximum likelihood estimation (MLE) objective, and it can be evaluated using standard metrics for text generation, such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and COMET (Rei et al., 2020). In this work, we study three main variants of KG-TRICK, which differ in the training data they use: i) TRICKKGC, which uses only the relational information of the KG, ii) TRICKKGE, which uses only the textual information of the KG, and iii) TRICKKGC+KGE, which uses both the relational and textual information of the KG."}, {"title": "3.3 Inference for KGC and KGE", "content": "Once the output is generated, it can be used to complete the KG in two ways. The application to KGE is straightforward, as the output is the missing textual information for an entity in the target language. The application to KGC is slightly more complex, as the output is the textual representation (i.e., the primary name and short description) of the missing tail entity in the target language. Relying only on an exact match between primary names may not be sufficient to determine the correct entity, especially in the case of ambiguous entities, such as Paris the city and Paris the prince of Troy. While previous work (Saxena et al., 2022) enumer-ates the entities with the same name (e.g., Paris1, Paris2, etc.), we incorporate entity descriptions as additional information from KG-TRICK to help disambiguate entities with the same primary name resulting in higher entity linking accuracy.\nEnsemble across languages. Since KG-TRICK can generate text in any target language for which it has been (pre-)trained, we leverage this capability to complete the missing information from multiple languages. When corresponding entity IDs are linked from generated text by different languages, we then ensemble and choose the most common predictions as is showed in Figure 1."}, {"title": "4 WikiKGE-10++", "content": "In this section, we introduce WikiKGE-10++, the largest human-curated benchmark through expert crowdsourcing for textual information completion of KGs, which features over 25,000 entities and their corresponding aliases and descriptions across 10 languages. Having realized the importance of evaluating systems on textual information completion of multilingual KGs, Conia et al. created WikiKGE-10, a benchmark for evaluating KGE systems on the completion of entity names and aliases in 10 languages. However, WikiKGE-10 is limited in two dimensions: i) it only allows for the evaluation of entity names and aliases, and ii) the entities included in the benchmark are popular entities only (i.e., they belong to the top-10% most popular entities in Wikidata according to number of page views of their corresponding Wikipedia pages). A core contribution of our work is the creation of WikiKGE-10++, which extends WikiKGE-10 in two above-mentioned dimensions, hence the two \"+\" signs in the name of our benchmark. We include the number of entities across different languages and popularity tiers in The human annotation process, details and resources can be found in Appendix A.\nIncluding entity descriptions. WikiKGE-10++ includes not only entity names and aliases, but also entity descriptions, which are crucial for many downstream tasks to create better entity representations (Ri et al., 2022). The inclusion of entity descriptions in WikiKGE-10++ is important for evaluating KGE systems on the completion of textual information that is usually longer and more complex than entity names and aliases. However, evaluating KGE systems on the entity descriptions already available in Wikidata is not ideal, as those are not always manually curated and often under-specific, e.g., the description of many cities is simply city in country. Therefore, we asked a team of annotators to produce high-quality descriptions for each language.\nIncluding torso and tail entities. WikiKGE-10++ includes a much larger set of entities, which belong to the torso of the popularity distribution of Wikidata (i.e., between the top-10% and top-50% most popular entities) and also the tail of the popularity distribution (i.e., below the top-50% most popular entities). This is important as the majority of entities in a KG are not popular, and different conclusions can be drawn when evaluating systems on different popularity tiers. The inclusion of torso and tail entities in WikiKGE-10++ is important to assess the robustness of KGE systems on the completion of textual information for entities where the amount of information available inside (and also outside) the KG is limited.\nOverall, WikiKGE-10++ is around 2.5 times larger than WikiKGE-10 in terms of number of entities, while also including entity descriptions in addition to the entity names."}, {"title": "5 Experiments and Results", "content": "5.1 Datasets and Benchmarks\nKGC. We carry out our KGC experiments on Wikidata5M (Wang et al., 2021) transductive split. We stress that, although entities in Wikidata are language-agnostic, the original Wikidata5M dataset is English-only, i.e., the textual information (names and descriptions) for the 5 million entities is only in English. Therefore, this English-only setting may not be ideal to evaluate our multilingual KGC system; however, as is illustrated in Table 4, our task reformulation could further enhance KGC performance with multilinguality.\nKGE. We carry out our KGE experiments on our newly annotated WikiKGE-10++ dataset, which features 10 languages and over 25,000 entities as introduced in Section 4. We use this dataset to evaluate KGE models on the completion of entity names, aliases, and descriptions in 10 languages.\n5.2 Comparison Systems\nKGC. Baseline approaches for KGC can be divided into two categories: embedding-based and text-based. Embedding-based methods derive an embedding for each entity and relation from the graph structure of the KG, and rank the most probable tail entity via a vector similarity function, e.g., L2 distance (Bordes et al., 2013), complex space distance (Trouillon et al., 2016) or other distance measures. Text-based methods use encoder-only language models (Wang et al., 2022, SimKGC) to encode both the head entity and the relation using their textual information, or encoder-decoder language models (Saxena et al., 2022, KG-T5) to generate the missing tail entity. Recent work (Kochsiek et al., 2023, KGT5-context) integrates extra subgraph-structure information into sequence-to-sequence models. Since we build upon text-based methods with multilinguality, we compare our KG-TRICK models with SimKGC, KG-T5 and SKG-KGC (Shan et al., 2024), which are the most relevant baselines for a fair comparison.\nKGE. While KGE is a relatively recent task, previous work has already indicated several strong baselines, including i) using NLLB-200\u00b9 (Costa-juss\u00e0 et al., 2022) to translate entity names and descriptions from a language, and ii) prompting LLMs (e.g. GPT-3.5 or Llama), to generate textual information for an entity.\n5.3 Experimental Setup\nWe use the entities within Wikidata5M which contains a set of around 5 million entities and a collection of around 20 million triplets and collect their available textual information (entity names, aliases, and descriptions) for English and 9 other languages from a Wikidata dump\u00b2 to create a silver training set & for KGE in multiple EN\u2192XX directions containing around 16 million records. For KGC, the original Wikidata5M triplets are expanded with our downloaded Wikidata dump to form over 150 million triplets T multilingually (EN\u2192XX) in 9 languages pairs. We train three variants of KG-TRICK: one on E (denoted as TRICKKGE), one on T (denoted as TRICKKGC), and one on the mixture of both (denoted as TRICKKGC+KGE).3 We took care to prevent any test-set contamination by excluding test entity IDs from training within the WikiKGE10++ dataset. To verify the multilinguality of TRICKKGE, we also include a bilingual version trained with single language pair (e.g. EN\u2192IT) on KGE task, denoted as TRICKKGE (bilingual) in Table 4. While the number of training samples are disproportional for KGC and KGE, to trade off the performance between the two tasks, as is shown in Table 6, we find a sweet spot of combining 50% of KGC data into the joint training. We denote this derivative as TRICK50%KGC+Kge in Table 3 and Table 4. We provide more details balancing the training data of the two tasks in Appendix D.\nEvaluation. For KGC, we evaluate the systems using standard ranking-based metrics, namely, hit@1, hit@3, hit@10, and Mean Reciprocal Rank (MRR). Hit@k measures the proportion of correct answers in the top-k predictions, while MRR measures the average rank of the correct answer. For KGE, we follow the evaluation protocol proposed"}, {"title": "5.4 Results on KGC", "content": "Table 2 shows the results obtained by our KG-TRICK models compared to other KGC-only models on the transductive test set of Wikidata5M. In general, we can observe that KG-TRICK outperforms strong baselines on MRR, hit@1, and hit@3, and it is the third best model for hit@10. More specifically, TRICKKGC (trained only on KGC data but in multiple languages) already outperforms both SimKGC and KG-T5, on almost all the metrics. Notably, this first result demonstrates that our model is able to outperform strong baselines that are tailored for KGC on a dataset Wikidata5M that is designed for KGC (and that is biased in its creation towards entities that have English lexicalizations). Moreover, we can observe that TRICKKGC+KGE achieves scores that are even higher than TRICKKGC, which demonstrates that unifying KGC and KGE leads to additional improvements on the KGC task. This second result empirically shows that the two tasks are indeed interdependent and mutually beneficial, and that the combination of the two tasks can lead to better results than the two tasks individually.\nOn the other hand, we can observe that the performance of TRICKKGC and TRICKKGC+KGE is not as good as the performance of SimKGC or SKG-KGC on hit@10. We hypothesize that this is likely due to the fact that the sampling capacity of sequence-to-sequence models is limited and constrains their performance with higher values of k. Indeed, for SimKGC, TransE and ComplEx, due to their closed-world assumption, the candidates for the tail entities are known during inference for similarity search. However, for text generation models, such as KG-T5 and KG-TRICK, which operate under a more challenging open-world assumption, the diversity of generated candidates may be limited by the sampling strategy used for decoding. We also include the results from KGT5-context paper which"}, {"title": "5.5 Results on KGE", "content": "Table 3 shows the KGE results on our new WikiKGE-10++ benchmark split by entity popularity, while Table 4 shows the results by language.\nCoverage. Overall, we could observe that TRICK50%KGC+KGE outperforms strong baselines on average and across most languages. Interestingly in Table 3, TRICKKGE and TRICKKGC+KGE perform worse than GPT-3.5 on Coverage of head entities. This is likely due to the fact that GPT-3.5 has seen substantially more popular entity names during its pre-training and is equipped with considerably more parameters to store such information. However, TRICK series quickly catch up with GPT-3.5 on Coverage of torso entities, and significantly outperform GPT-3.5 on Coverage of tail entities. It shows that GPT-3.5 quickly loses its advantage when the entities are less popular, and that KG-TRICK models feature a more balanced and consistent performance across different popularity tiers.\nPrecision. Overall, we can observe that TRICK significantly outperforms strong baselines on precision on head, torso, and tail entities, i.e., it is a more reliable system in identifying incorrect entity names and aliases in a given target language in a multilingual knowledge graph. In fact, TRICK is particularly effective on torso and tail entities, where it improves over NLLB-200 and GPT-3.5 by around 10% points in F1 score. This is important as completing missing knowledge is not only about providing the correct information but also about avoiding incorrect information.\nEntity descriptions. Finally, we also report the COMET score for the completion of entity descriptions, borrowing a metric for open-ended text generation from MT. In this task, we can observe that TRICKKGE is comparable with NLLB-200 and GPT-3.5, while TRICKKGC+KGE is slightly worse on average than TRICKKGE. These results open the door to future work: indeed, very different methods achieve comparable results on entity description completion, meaning that there is still a wide room for improvement in this task or COMET is not a good metric for comparing descriptions. We note that BLEU is not appropriate either, as its score is not defined for short texts, e.g., one word.\nMultilinguality and Multi-tasking. As is illustrated in Table 4, TRICKKGE outperforms TRICKKGE (bilingual) in almost all languages both on Precision and Coverage, indicating that jointly training a unified model for all languages can inherently benefit its multilingual capabilities. On the multi-task side, combining KGC and KGE tasks requires caution, as is demonstrated by TRICKKGC+KGE and TRICK50%KGC+KGE. KGC and KGE are mutually beneficial when training data is balanced, otherwise one task dominates the distribution and causes a regression on the other. A more in-depth analysis focused on data balancing is discussed in Appendix D. We can also observe that the multilingual capability of Llama3 is far from ideal in KGE."}, {"title": "6 Downstream Application: Results on Multilingual Question Answering", "content": "In addition to the KGC and KGE tasks, we also evaluate our KG-TRICK on a downstream application: our intuition is that (post-)pretraining on KGC and KGE tasks can allow a model to store more factoid knowledge, which can be useful for multilingual question answering. Therefore, we evaluate the performance of our TRICKKGC+KGE when fine-tuned on answering the questions in the Mintaka dataset (Sen et al., 2022), which is a multilingual question answering dataset that contains knowledge-seeking questions, and compare its results in the same setting with directly fine-tuning on Mintaka using mBART-large-50 that has not been (post-)pretrained on KGC and KGE tasks. Our experiments show that our model outperforms mBART-large-50 by 3.1% (29.2% vs. 26.1%) on average in terms of EM (Exact Match), which demonstrates that the knowledge embedded in KGC and KGE training data could be easily transferred to the QA task in cross-lingual settings."}, {"title": "7 Conclusion", "content": "The contributions of this paper are threefold. First, we propose a novel multilingual KGC and KGE system, TRICK, which is able to complete relational and textual information in and across 10 languages. Second, we introduce a largest human-curated dataset, WikiKGE-10++, which contains 10 languages and over 25,000 entities for KGE evaluation. Third, we demonstrate that our TRICK system outperforms strong baselines on both KGC and KGE tasks, and that the combination of the two tasks can lead to better results than addressing them individually. In addition, we also show that the knowledge embedded in KGC and KGE training data could be easily transferred to cross-lingual QA. We hope our work and our WikiKGE-10++ can inspire future research on multilingual KGC and KGE, and further contribute to both KG and LLM communities on evaluating the factuality of Language Models across languages."}, {"title": "Limitations", "content": "Closed world assumption of KGC. In this paper, we assume that the entities within KGC tasks already exist in the KG. If the model predicts some entities that do not exist, we simply ignore the inference. This assumption limits the model's capability to explore the encoded knowledge within pre-trained multilingual LMs. Although our model can be extended to predict and extract entities outside of a KG, our experiments in Section 5 demonstrate that there is still a large headroom to complete the relational information in a multilingual setting, since we combine knowledge across languages that are not considered in a monolingual setting. We leave the exploration of how to leverage KG-TRICK to work in an open-world setting for future work.\nLimited exploration of pre-trained multilingual LMs. Our KGE task pays great attention to the enrichment of entity names and entity descriptions with limited attention to other textual information such as mottos, quotes, and others. Given that pretrained LMs have been trained on massive amounts of data, there is great potential in the extraction of information that has been seen by the pre-trained LMs and that does not exist in the KG yet. Although KG-TRICK can be extended to infer other entity facts, our analysis shows that entity names and descriptions are still the most essential pieces of information to enrich and disambiguate entities, especially the entity descriptions, as they are a summary in free-form text that is highly representative of a particular entity.\nSupport unified multilingual KGs. We focus on the multilingual KGs in which entities are represented by entity IDs, and language-dependent textual information is structured as attributes associated with the corresponding entities. The benefit of this setting is that we enriched a KG that has been unified across languages at the very beginning of its construction process, and add new knowledge to the KG itself by inferring information that can be derived from the multilingual KG itself. Therefore, our system does not suffer from the error propagation introduced by entity and relation alignment between different KGs. Nonetheless, our system is limited to the setting of unified multilingual KGs, such as Wikidata. KG-TRICK is complementary to other related work on multilingual KG completion, which calls for integration of different KGs. Our system can be applied to further improve the completeness after the KGs are unified since such techniques focus on fusing different KGs but not inferring knowledge from the unified KG itself.\nWikiKGE-10++. While WikiKGE-10++ significantly extends WikiKGE-10 by adding a significant number of entities sampled from torso and tail entities, it contains only two types of facts, i.e., entities names (and aliases) and descriptions. Future work may extend WikiKGE-10++ to cover more types of facts that are usually associated with entities, so that the research community will be able to get a more accurate and thorough picture on how to evaluate novel approaches in this area.\nPotential risks for generative textual information completion. As we employ a text-to-text framework to complete the information in a multilingual KG, it may generate biased or inaccurate text that could be misleading for downstream tasks. If this work is considered for production use, human annotators should be added in the loop to reduce the risks of harmful text generation."}, {"title": "A Creating WikiKGE-10++", "content": "In this section, we describe the in-depth details on the creation of WikiKGE-10++, our novel human-curated dataset for the evaluation of automatic approaches on KGE of Wikidata entity names and descriptiptions."}, {"title": "A.1 Choice of Languages", "content": "Aligned with the previous work completed in Conia et al., the benchmark, we select 9 languages from a set of typologically diverse linguistic families, while replacing the Russian (Slavic) language for the Thai (Kra-Dai) language:\n\u2022 West Germanic: English, German;\n\u2022 Romance: Spanish, French, Italian;\n\u2022 Semitic: Arabic;\n\u2022 Sino-Tibetan: Chinese (simplified);\n\u2022 Kra-Dai: Thai;\n\u2022 Koreanic: Korean;\n\u2022 Japonic: Japanese.\nThe Russian language was interchanged for the Thai language due to export and import restrictions placed on Russia, thereby, restricting access to Russia-based human annotators."}, {"title": "A.2 Human annotation process", "content": "The objective of the annotation process was to (i) rate and suggest entity names in the target language, (ii), verify the suggest entity names in the target languages, (iii) curate description for the entity in the target language, (iv) validate the provided descriptions quality."}, {"title": "A.2.1 Rate and suggest entity names.", "content": "The objective of this annotation step was to rate entity names in a target language. Detailed information on the annotation process and UI design can be found in Conia et al.."}, {"title": "A.2.2 Verify suggested entity names.", "content": "The objective of this annotation step was to verify the suggested entity names in a target language provided by the human annotators. Detailed information on the annotation process and UI design can be found in Conia et al.."}, {"title": "A.2.3 Curate entity descriptions.", "content": "The objective of this annotation step was to curate descriptions for a given entity in the target language.\nGiven an entity name in a target language, annotations were required to familiarize themselves with its information: the user interface provided the entity names, as well as a built-in panel that directly displayed Wikipedia articles for the corresponding entity in English and the target language, if available. In addition, annotators were recommended to further familiarize themselves with the entity outside of the provided information.\nNext, the annotators were tasked with learning about the required format of the requested description with detailed instructions. This was facilitated by providing: (i) examples of correctly curated description given an example entity, and (ii) strict rules that the descriptions had to comply by.\nAfter learning about the entity and the required description format, the human annotator was requested to manually curate the description for the corresponding entity in the target language. During this task, the human annotator was provided with descriptions from other sources (such as Wikidata) in English and the target language. Human annotators were instructed that they could leverage the extraneous descriptions, but not to copy and paste unless satisfactory."}, {"title": "A.2.4 Validate entity descriptions.", "content": "The objective of this annotation step was to validate the quality of the descriptions in the target language provided by the human annotators.\nFirst, given an entity, the human annotator was provided with corresponding information (i.e., entity names/aliases, Wikipedia pages etc.) as done in the previous task. In addition, the description requirements were detailed (in-depth guidelines provided in a different document).\nThen, they were prompted to analyze the corresponding description for the entity in the target language with a series of questions. The questions were reformulated from description requirements, to verify the presented description in the target language followed the requested format. If the annotator negatively responded to any of the presented questions, they were prompted to edit the description to satisfy the requirements. If the initial description meets the requirements, the originally provided description was sustained."}, {"title": "A.3 Quality assurance and inter-annotator agreement.", "content": "We follow the annotation guidelines by Conia et al. to get high quality annotation results for the datasets. As the entire annotation procedure was done in a crowd-source platform outside of our organization, we will only disclose necessary information to protect annotators' privacy and meanwhile ensure the quality of WikiKGE-10++. Each item in WikiKGE-10++ is annotated by 3 annotators. All annotators are native speakers of the language they annotate, and they are fluent in English too. We follow the evaluation protocol proposed by Conia et al.. Inter-annotator agreement on head, torso and tail entity names resembles what is reported in Conia"}]}