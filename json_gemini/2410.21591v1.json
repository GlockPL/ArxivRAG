{"title": "Can Large Language Models Replace Data Scientists in Clinical Research?", "authors": ["Zifeng Wang", "Benjamin Danek", "Ziwei Yang", "Zheng Chen", "Jimeng Sun"], "abstract": "Data science plays a critical role in clinical research, but it requires professionals with expertise in coding and medical data analysis. Large language models (LLMs) have shown great potential in supporting medical tasks and performing well in general coding tests. However, these tests do not assess LLMs' ability to handle data science tasks in medicine, nor do they explore their practical utility in clinical research. To address this, we developed a dataset consisting of 293 real-world data science coding tasks, based on 39 published clinical studies, covering 128 tasks in Python and 165 tasks in R. This dataset simulates realistic clinical research scenarios using patient data. Our findings reveal that cutting-edge LLMs struggle to generate perfect solutions, frequently failing to follow input instructions, understand target data, and adhere to standard analysis practices. Consequently, LLMs are not yet ready to fully automate data science tasks. We benchmarked advanced adaptation methods and found two to be particularly effective: chain-of-thought prompting, which provides a step-by-step plan for data analysis, which led to a 60% improvement in code accuracy; and self-reflection, enabling LLMs to iteratively refine their code, yielding a 38% accuracy improvement. Building on these insights, we developed a platform that integrates LLMs into the data science workflow for medical professionals. In a user study with five medical doctors, we found that while LLMs cannot fully automate coding tasks, they significantly streamline the programming process. We found that 80% of their submitted code solutions were incorporated from LLM-generated code, with up to 96% reuse in some cases. Our analysis highlights the potential of LLMs, when integrated into expert workflows, to enhance data science efficiency in clinical research.", "sections": [{"title": "Introduction", "content": "In clinical research, data science plays a pivotal role in analyzing complex datasets, such as clinical trial data and real-world data (RWD), which are critical for improving patient care and advancing evidence-based medicine.\u00b9 For example, it was reported that for a pharmaceutical company, the insights brought from RWD analysis could unlock up to $300M values annually by optimizing clinical trial design and execution.2 Data scientists, at the core of this process, require years of coding expertise, alongside a deep understanding of diverse medical data types, including patient clinical data and omics data, while also collaborating closely with medical professionals. However, the growing demand for data science skills and the limited availability of experienced data scientists have become bottlenecks in the clinical research process.4 Coding is central to the work of data scientists, underpinning essential tasks such as statistical modeling, data cleaning, and visualization using Python and R. Given this, exploring methods for streamlining the coding process in clinical research data science is crucial to accelerate drug development and improve patient outcomes.\nCode generation has been extensively explored with the advent of large language models (LLMs), which have demonstrated strong capabilities in tasks like code completion. 5 Continuous efforts have been made in developing more powerful code-specific LLMs, 6-8 refining prompting strategies,\u00ba integrating external knowledge through retrieval-augmented generation (RAG),10,11 and enabling LLMs' self-reflection. 12 These advancements further lead to the LLM-based platform for software development 13 and data analysis. 14 While LLMs have been evaluated for general programming tests, 5, 15-17 software engineering, 18 and data analysis, 19,20 assessments specifically targeting clinical research data science remain scarce. In medicine, recent works have introduced LLMs to automate machine learning modeling21 and support bioinformatics tool development, 22 but they do not cover broad data science tasks. Therefore, this paper seeks to build a comprehensive code generation dataset to assess to which extent the cutting-edge LLMs can automate clinical research data analysis, modeling, and visualization.\nOur objective was to evaluate the practical utility of LLMs in handling complex clinical research data and performing the associated data science tasks. To this end, we identified 39 clinical studies published in medical journals that were linked to patient-level datasets (Fig. 1a). We started by extracting and summarizing the analyses performed in these studies, such as patient characteristic exploration and Kaplan-Meier curves. We then developed the code necessary to reproduce these analyses and the reported results in these studies. These coding tasks, along with their reference solutions, were all manually crafted and cross-verified to ensure accuracy. The result was a collection of 293 diverse, high-quality data science tasks, covering primary tools used in Python and R, e.g., lifelines for survival analysis in Python and Bioconductor for biomedical data analysis in R. Additionally, we categorized the difficulty of these tasks into Easy, Medium, and Hard, by the number of \"semantic lines\" of code in the reference solutions (Fig 1d). The semantic lines metric aggregates lines of code that serve the same operation into a single unit, providing a clear measure of task complexity. A detailed overview of the dataset and its characteristics is provided in Fig. 1.\nHere, we rigorously evaluated the extent to which clinical research data science tasks can be automated by LLMs. We benchmarked six state-of-the-art LLMs using various methods, including chain-of-thought prompting, few-shot prompting, automatic prompting, self-reflection, and retrieval-augmented generation (Fig. 1f). Our analysis focused on both the accuracy and quality of the code generated by these models. While we found that current LLMs are not yet capable of fully automating complex clinical data science tasks, they do generate code that is highly similar to the correct final solutions. Building on this insight, we investigated the development of a platform designed to facilitate collaboration between human experts and artificial intelligence (AI) to streamline coding tasks. This platform aims to enhance the productivity of clinical researchers by integrating LLMs into established data science workflows, with a focus on user-friendliness and the reliability of the outputs. Our results demonstrated that the platform significantly improved human experts' efficiency in executing data science tasks, highlighting the promising future of human-AI collaboration in clinical research data science."}, {"title": "Results", "content": null}, {"title": "Creating data science tasks from clinical studies", "content": "We curated our testing dataset, CliniDSBench, to reflect the real-world challenges that data scientists face in clinical research. The dataset is grounded in published medical studies and linked to patient-level datasets from cBioPortal. 23 These patient datasets are diverse, each containing data from hundreds to thousands of patients, including clinical information such as demographics and survival data, lab results, and omics data like gene expression, mutations, structural variants, and copy number alterations. Unlike prior studies that mostly focus on single spreadsheets, each study in our dataset is linked to multiple spreadsheets, up to five, providing a more complex and realistic basis for evaluating data science workflows. This setup mirrors the multifaceted nature of real-world clinical research, where data scientists must integrate and analyze information from various sources to generate insights. The dataset building and evaluation framework is illustrated in Fig. 1a.\nWe obtained the publications associated with these datasets from PubMed and reviewed them to extract the types of analyses conducted. Through this process, we identified common analyses frequently performed in clinical research, such as summarizing patient baseline characteristics, plotting Kaplan-Meier curves to assess treatment effects across groups, and creating mutational OncoPrints to highlight significant gene mutations in specific patients. This extraction process allowed us to filter and refine the initial set of studies, ensuring both diversity and comprehensiveness in the analyses covered. After this refinement, we retained 39 studies, which were used to create the final testing dataset.\nWe designed a series of coding questions in a step-by-step format, mirroring the logical progression of analyses in the original studies, ultimately leading to the main findings. For example, a study may include exploratory data analysis, gene mutation analysis to detect abnormal mutation patterns, survival analysis to visualize patient outcomes, and statistical tests to verify significance. Correspondingly, we developed coding questions for each step, ensuring that earlier steps provide the necessary groundwork for subsequent analyses. As such, each coding task consists of five components: (1) the input question, (2) patient dataset schema description, (3) prerequisite code (called \"prefix\"), (4) reference solutions, and (5) test cases. This design reflects the practical setup data scientists encounter in real-world projects. In total, we manually curated 128 analysis tasks in Python and 165 in R based on the extracted analyses from 39 studies. As shown in Fig. 1c, the input questions typically consist of 50-100 words describing the task and output requirements, while the reference code solutions span more than 20 lines, sometimes exceeding 50 lines, reflecting the complexity of the tasks.\nWe quantitatively assessed the difficulty of each coding task by calculating the number of semantic lines in the reference solutions. A semantic line aggregates multiple lines of code that contribute to the same operation, as illustrated in Fig. 1d. This approach prevents the difficulty assessment from being skewed by repetitive or tedious tasks that are fundamentally simple. The statistics for semantic lines and difficulty levels are presented in Fig. 1e. Our analysis shows that Python solutions tend to be more complex than R solutions, particularly for Medium and Hard tasks. This is largely due to R's rich ecosystem of medical-specific packages, which often allow for more direct solutions. In contrast, Python frequently requires additional customization and manual coding to achieve similar outcomes, contributing to higher complexity in Python coding tasks. The pie charts in Fig. 1b show the libraries frequently used in the reference answers."}, {"title": "LLMs are not yet ready for fully automated data science", "content": "As illustrated in Fig. 1f, our evaluation framework is composed of three key components: models, methods, and tasks. For the first component, we selected six cutting-edge LLMs: GPT-40,24 GPT-40-mini, 25 Sonnet, 26 Opus, 27 Gemini-pro, 28 and Gemini-flash.28\nThese models represent a diverse set of advanced generalist LLMs capable of performing code generation based on input instructions. To explore their effectiveness and potential room for improvement in clinical research data science tasks, we applied several adaptation methods: chain-of-thought, 29 few-shot prompting, 30 automatic prompting, 31 self-reflection, 12 and retrieval-augmented generation (RAG).32\nFor evaluation, we assessed the combinations of models and adaptation methods across three tasks: code generation, code debugging, and human-AI collaboration. The first two tasks were used to measure the models' accuracy in automating the coding process. We employed Pass@k as the primary metric, where k represents the number of attempts the model is allowed to make to solve a coding task. This metric behaves like the probability that at least one out of the k attempts is correct. Specifically, we selected k = 1 as a strict benchmark to evaluate how well LLMs can automate tasks on the first attempt, providing insight into their immediate accuracy. Additionally, we used k = 5 as a more relaxed metric to explore the model's potential to improve given multiple attempts, offering a broader assessment of its ability to generate correct solutions when allowed more tries.\nWe first evaluated the immediate accuracy of LLMs in generating code solutions on their initial attempt. For each task, the LLM is provided with a raw question that describes the target task, as well as a dataset description (Fig. 2a). The dataset description includes details such as spreadsheet names, column names, and common cell values, which guide the LLM in identifying the correct spreadsheet, column, and values to work with. Additionally, the instruction section offers supplementary guidance for the LLM during code generation. We used three types of instructions: Vanilla, Manual, and Automatic. The Vanilla instruction provides minimal guidance, merely instructing the LLM to solve the task, while Manual and Automatic instructions are more detailed, either manually crafted or optimized through automatic prompt generation. 31 The generated code solutions must pass all the testing cases to be judged right.\nFrom our experiments, we found that current LLMs cannot consistently produce perfect code for clinical research data science tasks across all difficulty levels. As shown in Fig. 2d, for Python tasks, the Pass@1 scores vary significantly based on task difficulty. For Easy tasks, most LLMs achieve Pass@1 rates in the range of 0.40-0.80. However, for Medium tasks, the Pass@1 rates drop to 0.15-0.40, and for Hard tasks, they range from 0.05 to 0.15. Performance differences also exist between different LLMs, particularly within the same series. For instance, the lightweight variant GPT-40-mini generally underperforms compared to its larger counterpart, GPT-40, with differences in performance of up to twofold in many cases. This highlights the limitations of current LLMs, especially as task complexity increases. The trend is similar in R, where performance declines with increased task difficulty, though there is a significant difference in performance between Python and R tasks (Fig 2e).\nDiving deeper into the variations across instruction types, we observed that (1) in Python tasks (Fig. 2d), neither automatically generated prompts nor manually crafted prompts consistently outperformed the Vanilla prompts. For example, Vanilla performed better than AutoPrompt in 4 out of 6 LLMs for Easy tasks, 2 out of 6 for Medium tasks, and 4 out of 6 for Hard tasks. A similar trend was observed for R tasks (Fig. 2e). (2) More powerful models, such as GPT-40 and Gemini-Pro, showed greater benefits from carefully crafted instructions, particularly in Easy and Medium Python tasks. In contrast, lighter models like GPT-40-mini and Gemini-Flash did not exhibit such improvements, and in some cases, complex instructions even seemed to hinder performance. This suggests that lightweight models may struggle to fully interpret and utilize complex instructions, which can reduce their effectiveness in data science coding tasks.\nWe adjusted the temperature settings to sample multiple solutions from LLMs and calculated Pass@5 scores for Python tasks (Fig. 2b). In most cases, increasing the temperature allows LLMs to generate more creative and diverse solutions, resulting in higher probabilities of producing a correct solution. This trend was consistent across all models, suggesting the potential benefit of having LLMs brainstorm multiple solutions to reach better outcomes. On average, LLMs solved more tasks when given five attempts compared to just one, as measured by Pass@1. However, despite this improvement, the overall performance remains far from perfect."}, {"title": "Unlocking the power of LLMs through strategic adaptations", "content": "Motivated by the varied performances of LLMs with different instruction levels, we hypothesized that tailored adaptations for LLMs in clinical research data science could lead to greater improvements. To test this, we introduced two key dimensions of adaptation: (1) enhancing LLM inference and reasoning by incorporating advanced instructions or external knowledge, and (2) employing multiple rounds of trial-and-error, allowing LLMs to iteratively correct their errors. The results of these adaptations are shown in Fig. 3.\nFor the first dimension of adaptation, in addition to Manual (ManualPrompt) and Automatic prompt optimization (AutoPrompt), we introduced three additional strategies: chain-of-thought (CoT), few-shot prompting (Few-shot), and retrieval-augmented generation (RAG). ManualPrompt incorporates human knowledge into the instructions, offering additional hints such as common error cases, key columns like unique patient identifiers, and specific guidance for certain analyses. AutoPrompt utilizes the dspy prompt optimizer, 31 which generates prompts via an LLM and selects the best one. We optimized prompts using three studies from the training set, keeping 11 studies as the test set. RAG equips LLMs with a Google search engine, enabling them to look up package documentation, StackOverflow discussions, and clinical knowledge before generating code solutions. Few-shot prompting adds several example question-and-answer pairs from the training set to guide the model. For CoT, we enriched the instructions with step-by-step guidance, asking the LLM to follow concrete steps toward the final solution. These instructions were manually created to ensure accuracy, mimicking scenarios where proficient data scientists provide more detailed input.\nThe comparison of adaptation strategies based on GPT-40 is illustrated in Fig. 3b. Each data point represents the average Pass@1 score achieved for coding tasks in a given study. A point on the diagonal line indicates equivalent performance between the adaptation and the vanilla method. The results can be categorized into three patterns:\n\u2022 AutoPrompt overfitted on the training tasks and struggled to generalize effectively on the testing tasks. The diversity of analyses in our dataset led to substantial differences between the training and testing tasks, which AutoPrompt failed to navigate. This limitation is further verified by the results from Few-shot, which also did not show improvements when incorporating examples from the training tasks.\n\u2022 RAG performed similarly to Vanilla, despite incorporating external knowledge into the inputs. We hypothesize this is because GPT-40 was likely trained on a wide range of public sources, including package documentation, webpages, medical articles, and online guidelines. As a result, the additional information retrieved by RAG offered minimal benefit, as much of it was already within the model's pre-existing knowledge. Furthermore, the retrieval process can sometimes introduce noise, embedding irrelevant or distracting context into the prompt, which negatively affects performance.\n\u2022 ManualPrompt provided a modest improvement, boosting Pass@1 by an average of 10% across studies and outperforming Vanilla in 7 out of 11 cases. This demonstrates the effectiveness of incorporating expert knowledge to better adapt LLMs to specific tasks. However, the benefit remains limited, as LLMs often struggle to process nuanced hints and apply them accurately to the tasks at hand. In contrast, CoT led to substantial improvements, outperforming Vanilla in 8 out of 11 studies, with improvements ranging from double to triple the Pass@1 scores. These results highlight the potential of human-AI collaboration. When LLMs are guided with more structured, step-by-step instructions from human experts, they can perform significantly better than when generating solutions independently.\nWe conducted further experiments to evaluate whether LLMs can solve more problems through self-reflection. The results are shown in Fig. 3c for Python tasks and Fig. 3d for R tasks. To enable self-reflection, we provided LLMs with three types of logs captured from"}, {"title": "Human-AI collaboration boosts productivity for data science in clinical research", "content": "By far, the two critical findings from our experiments are: (1) When human experts provide more detailed, step-by-step instructions, the quality of LLM-generated code significantly improves, as demonstrated by the superior performance of Chain-of-Thought (CoT) prompting (Fig. 3b). (2) Although LLM-generated code is often imperfect, it serves as a strong starting point for human experts to refine. Evidence from Fig. 2c shows that LLM-generated code is close to the correct solution, and Fig. 3e and Fig. 3f indicate that most code executes successfully but fails only at the final testing stages. Additionally, LLM self-reflection can resolve most of the bugs. These findings highlight the potential of LLMs to assist data scientists in streamlining the coding process in clinical research.\nTo bridge the gap in utilizing LLMs for clinical research and leveraging the insights from our experiments, we developed a platform that integrates LLMs into data science projects for clinical research. The architecture of this platform is shown in Extended Fig. 1. The platform is designed to offer an integrated interface for users to:\n\u2022 Chat with LLMs to brainstorm and plan analyses, with the ability to query external knowledge bases, including webpages, research papers, and other resources.\n\u2022 Generate code for data science tasks through interactions with LLMs, allowing users to streamline code writing for complex analyses.\n\u2022 Identify and debug errors in the user-provided code, with LLMs proposing solutions to improve the code.\nThe interface supports real-time interactions, allowing users to generate and execute code in a sandbox environment with instant visualizations. This removes the need for users to handle complex prompt crafting or manually switch between chat sessions and coding platforms like Jupyter Notebook. By simplifying the data science workflow, the platform empowers users with minimal coding expertise to perform complex data science tasks with ease.\nIn our user study, we involved five medical doctors with varying levels of coding expertise. Each participant was assigned three studies, 33-35 with approximately 10 coding tasks per study (Fig. 4d). Users worked with LLMs on our platform to complete these tasks and submitted their solutions once their code passed all the test cases. The difficulty levels of the tasks were quantified, with the distribution shown in Fig. 4b. During the study, we tracked two core actions: code generation and code improvement (debugging) requests. The statistics of these user behaviors are depicted in Fig. 4b, where most users completed the first two studies, and a few tackled the third. After the study, we analyzed the logs to compare the LLM-generated code with the final code solutions submitted by the users (Fig. 4a). Additionally, we conducted a survey to gather their feedback on the platform and their experience working with LLMs. The survey questions were built based on the Health Information Technology Usability Evaluation Scale (Health-ITUES). 36\nThe results of the code comparison analysis are presented in Fig. 4c, showing the distribution of the proportion of user-submitted code derived from LLM-generated solutions. We found that a significant portion of the user-submitted code was drawn from AI-generated code. For Easy tasks, the median proportions were 0.88, 0.87, and 0.84 across the three studies, indicating that users heavily relied on LLM-provided solutions when crafting their final submissions. For Medium and Hard tasks, the ratios were generally lower: in Study 1, the proportions were 0.44 for Medium tasks and 0.96 for Hard tasks, while in Study 2, the proportions were 0.75 for Medium and 0.28 for Hard tasks. These findings demonstrate the potential of LLMs to streamline the data science process, even for users without advanced coding expertise, with greater reliance on LLMs for easier tasks and more mixed results for more complex ones.\nThe quantitative results from the user survey are summarized in Fig. 4e, where we grouped the questions into four main categories. The average user ratings for each category are: Output Quality (3.4), Support & Integration (3.0), System Complexity (3.5), and System Usability (4.0). These ratings suggest that, overall, users had a positive experience using the platform. Additionally, we collected qualitative feedback (Fig. 4f), where one user expressed a strong interest in continuing to use AI for research on their own data, highlighting the platform's practical utility. Another user acknowledged the platform's value in helping them learn programming and data analysis, underscoring its potential as an educational tool for those with limited coding experience. These insights reinforce the platform's ability to enhance both productivity and learning in data science workflows."}, {"title": "Discussion", "content": "In collaboration with medical experts, data scientists play a pivotal role in analyzing complex datasets, such as real-world patient data, to derive insights that improve patient care and inform evidence-based medicine. However, the rising demand for data science expertise, combined with the limited availability of skilled professionals, has created a bottleneck, slowing progress and hindering the full potential of data-driven clinical research. This shortage is restricting the ability to fully harness the vast amount of data available for advancing clinical research.\nLarge language models (LLMs) have emerged as powerful generalist artificial intelligence (AI) capable of following human instructions to perform a wide range of tasks in medicine. 37-39 In parallel, LLMs have demonstrated strong capabilities in solving coding challenges,5 completing software engineering tasks,13 and performing basic data analysis. 20 These advancements suggest that LLMs hold great promise for streamlining data science projects in clinical research, a potential that has not yet been fully explored.\nThe primary goal of this study is to thoroughly evaluate the performance of cutting-edge LLMs in handling complex clinical research data and performing data science programming tasks. To achieve this, we developed a comprehensive coding benchmark, CliniDSBench, comprising 39 published clinical studies and 293 diverse, practical, and high-quality data analysis tasks in both Python and R. Based on these benchmarks, we found that current LLMs are not yet capable of fully automating data science tasks. At their first attempts, LLMs successfully solved only 40%-80% of Easy tasks, 15%-40% of Medium tasks, and 5%-15% of Hard tasks. This highlights the necessity of human oversight and post-processing to prevent misinformation and incorrect results when relying on LLMs for clinical data analysis.\nThough imperfect, we found that much of the LLM-generated code was quite close to the correct solution. This observation motivated us to explore advanced adaptation methods to improve LLM performance further. On the one hand, we found that LLMs could self-correct a significant portion of erroneous code, leading to substantial improvements over their initial attempts. On the other hand, involving human experts more directly in the process, such as by providing concrete, step-by-step plans for data analysis tasks, resulting in the best performance across all adaptation strategies.\nBeyond automatic testing, we conducted a user study using our developed interface, which integrates LLMs into the data science workflow. The study revealed that users heavily relied on LLM-generated code when crafting their final solutions, validating the effectiveness of LLMs in streamlining the coding process. This workflow typically followed a pattern where LLMs provided an initial solution, users collaborated with the LLMs for debugging, and then the users refined the final solution. The platform was highly appreciated by users, not only for its practical utility in accelerating data science tasks but also for its educational value in helping them improve their programming and data analysis skills.\nThis study has several limitations. First, to ensure the quality of the benchmark, we manually created all the questions and solutions for the analysis tasks, which restricted our ability to scale the benchmark to cover more studies. A larger dataset with more coding tasks would not only enhance evaluation but could also be used for training LLMS specifically for data science tasks. Second, the user study results may be biased, as the participants were primarily medical doctors who, while knowledgeable in their domain, had varying levels of coding proficiency. The usage patterns might differ significantly if data scientists were the users, as they possess more advanced coding skills but know less about medicine. Third, the patient-level data in our testing set are publicly available, but privacy risks must be carefully considered when deploying LLMs for real-world clinical data analysis. A recommended approach would be to separate the environment running code on sensitive patient data from the environment where LLMs are used. LLMs should only access the dataset schema or global statistics without access to individual patient data. Finally, the patient data used in our testing set were relatively clean, standardized, and semantically meaningful. In real-world scenarios, data can be messier and more varied, which could affect LLM performance. Future work should explore strategies to handle less structured, real-world data effectively.\nThe findings from our study show that while LLMs are not yet capable of fully automating clinical research data science tasks, they can be valuable tools when used in collaboration with human experts. This human-AI partnership can lead to the creation of effective coding solutions, boost productivity, potentially accelerate drug development, and improve patient outcomes. However, testing this hypothesis requires future prospective studies in clinical research to confirm the practical impact of such collaborations."}, {"title": "Methods", "content": null}, {"title": "Dataset curation", "content": "We created the testing dataset, referred to as CliniDSBench, based on published clinical studies and their associated patient-level datasets from cBioPortal. 23 cBioPortal is a comprehensive database for cancer and genomics research, providing access to hundreds of studies with linked patient data. These datasets encompass various modalities, including clinical data, clinical sample data, mutations, copy number alterations, structural variants, RNA, mRNA, and tumor miRNA, among others. This setup ensures that the coding tasks in CliniDSBench are closely aligned with the real-world challenges faced in clinical research data science, using authentic data and analysis tasks.\nWe began by reviewing the studies listed on cBioPortal's dataset page. For each study, we labeled the types of analyses performed. These labels were then aggregated to identify the most common analyses, ensuring the selected studies covered a comprehensive range of tasks for the testing dataset. For each selected study, we manually created coding tasks based on the extracted analyses, mirroring the sequence of data analysis steps that led to the findings in the original studies. Each coding task represents one step in this process. The tasks are structured with five key components: the input question, a description of the patient dataset schema, prefix code, reference solutions, and test cases. An example of the input coding task is shown in Extended Fig. 2.\nTo ensure the feasibility of automatic testing, it is crucial to maintain consistency between the input question and the testing cases, particularly regarding the output name and format. For instance, a simple question might be: \"tell me the number of patients in the dataset\". This question is inherently open-ended, allowing for a variety of answers. The most straightforward approach is to calculate the unique number of patient IDs in the dataset, such as num = df[\"patient_id\"].nunique(). However, for the testing cases to work, it is essential that the variable num represents this number in the code. Since the variable name can be arbitrary (e.g., n, num_patient, or number_of_patients), a testing case inspecting the variable num will fail if the name differs. To avoid this issue, each question is divided into two parts: the task description and the output format requirement, ensuring a constrained answer. For example, the full question would also specify the output requirement: \"make sure the output number of patients is an integer assigned to a variable named \"num\"\". Correspondingly, testing cases like assert num == 20 are attached to the LLM-generated code to verify its correctness.\nThe prefix code refers to the prerequisite code necessary to run before addressing a specific question. This approach mirrors the workflow of data scientists working in computational notebook environments like Jupyter Notebook, 40 where certain data processing steps are required for multiple analyses. However, it would be redundant and inefficient to repeat these steps for every coding task. For example, in one step, a data scientist might merge the patient clinical data table with the mutation table to link patient outcomes with gene mutation information. This merged dataset is then used in subsequent analyses, such as survival analysis grouped by gene mutations. For these follow-up tasks, the LLMs are not required to repeat the data merging process. Instead, the merging code is provided as prefix code, allowing the LLMs to build on the processed data and focus on the specific task at hand. This structure ensures efficiency and mimics how data scientists typically manage code dependencies across related tasks.\nTo protect the privacy of patient records, it is crucial to handle how patient data is"}, {"title": "Large language models and adaptation methods", "content": "We investigated a diverse range of transformer-based large language models (LLMs) for data science code generation tasks, focusing on cutting-edge proprietary models. These include OpenAI's GPT-4024 and GPT-40-mini,25 Google's Gemini-Pro and Gemini-Flash, 28 as well as Anthropic's Opus-327 and Sonnet-3.5.26 Each of these models is a flagship proprietary LLM known for its strong performance in medical and biomedical tasks. Additionally, all these models feature long context windows, enabling them to handle large inputs efficiently: GPT-40 and GPT-40-mini support up to 128K tokens, Gemini-Pro up to 2M tokens, Gemini-Flash up to 1M tokens, and both Opus-3 and Sonnet-3.5 support up to 200K tokens. This extended context capacity is essential for processing complex datasets and tasks typical in clinical research and data science.\nNo open-source code LLMs were included in this study for several key reasons. Firstly, most open-source code LLMs have limited context lengths, typically ranging from 2K to 8K tokens, which is insufficient for many of the data science tasks in our dataset. These tasks not only require input questions but also detailed dataset schema descriptions, sometimes spanning multiple tables with hundreds of columns. Secondly, previous studies have shown that open-source code LLMs significantly underperform compared to proprietary models, even on simpler tasks. For instance, in DS-1000,20 proprietary models like Codex41 outperformed open-source models such as CodeGen42 and InCoder43 by four to five times. Similarly, in BioCoder, 22 GPT-4 achieved a Pass@1 rate of approximately 40%, while open-source models like StarCoder, even at 15.5 billion parameters, 8 scored below 10%, despite fine-tuning. Given these findings, the proprietary LLMs used in our study can be considered to represent the upper bound of current LLM performance.\nIn this study, we explored adaptation methods to guide pre-trained generalist LLMS for specific tasks without fine-tuning the models. The primary reason for this approach"}, {"title": "Evaluation metrics and statistical analysis", "content": "The Pass@k metric was used to evaluate the performance of code generation in our study. Here, n represents the total number of code solutions generated, and c is the number of correct solutions, where c <n. Correct samples are those that pass all unit tests. The unbiased estimator for Pass@k is given by:\nPass@k = Eproblems [1 - \\frac{\\binom{n-c}{k}}{\\binom{n}{k}}]  (1)\nPass@k ranges from 0 to 1 and estimates the probability that at least one of the k generated code samples for a given task passes all the unit tests. In our study, we used two metrics: Pass@1 and Pass@5. Pass@1 is a stricter metric, evaluating whether the LLM can solve the task on the first attempt. To ensure reproducibility, we set the LLMs' temperature to zero for this evaluation. For Pass@5, we allowed the LLMs to generate 10 solutions for each question to estimate the likelihood of producing a correct answer within five attempts.\nTo compare the LLM-generated code with user-submitted or reference code solutions, we first parse the code string using abstract syntax trees (AST) to extract operators and variables, which allows for a structural analysis of the code. We then tokenize the code based on this parsing result. Using Python's difflib library, we compare the differences between two text sequences.\nLet s\u2081 represent the tokenized LLM-generated code and s2 represent the tokenized user-submitted code. We compute the length of overlapping tokens between the two sequences, denoted as s. The ratio of user-submitted code copied from LLM-generated code can then be calculated using the following formula:\nCopy Ratio = \\frac{length(s)}{length(s2)} (2)\nThis method quantifies the extent to which the user-submitted code overlaps with the LLM-generated code, providing insight into the level of influence the LLM had on the final solution."}]}