{"title": "Beyond Self-Consistency: Loss-Balanced Perturbation-Based Regularization Improves Industrial-Scale Ads Ranking", "authors": ["Ilqar Ramazanli", "Hamid Eghbalzadeh", "Xiaoyi Liu", "Yang Wang", "Jiaxiang Fu", "Kaushik Rangadurai", "Sem Park", "Bo Long", "Xue Feng"], "abstract": "Perturbation-based regularization techniques address many challenges in industrial-scale large models, particularly with sparse labels, and emphasize consistency and invariance for perturbation in model predictions. One of the popular regularization techniques has been various forms of self-consistency, which involve making small modifications to input data while preserving contextual information and enforcing similar predictions through auxiliary loss functions. In this work, we explore the first successful application of perturbation-based regularization algorithms in large-scale ads ranking models, and further propose a novel regularization algorithm, namely, Loss-Balanced Small Perturbation Regularization (LSPR) that can be used in potentially any deep learning model. We have successfully demonstrate that both Self-Consistency Regularization approaches (SCR) and LSPR are scalable and can improve ads delivery systems. By conducting industrial-scale experiments, and numerical analysis, we additionally show that our proposed LSPR, performs consistently better compared to SCR, across various groups and signal availability setups. Finally, we report a successful application of the proposed LSPR in a billion-scale industrial ranking system, which to the best of our knowledge, is the first of its kind, and it is specially designed to address the various scalability challenges (e.g, various surfaces, geological locations, clients and so on) as we will mention in this paper.", "sections": [{"title": "1 Introduction", "content": "In the fast-paced and dynamic world of online advertising, the task of advertisements (ads) ranking helps businesses with their target audiences. The primary goal of ads ranking is to determine which ads are displayed to users via machine learning techniques, ensuring that the most relevant ones appears prominently. This process directly influences user engagement and click-through rates Anil et al. (2022); Gu et al. (2021).\nAds ranking at an industry scale is often achieved through a multi-stage approach, encompassing retrieval, pre-ranking (or early-stage ranking), and final-stage ranking, which nowadays are mostly powered by large-scale neural networks Covington et al. (2016); Gallagher et al. (2019). This efficient multi-stage system strikes a balance between computational costs and recommendation quality Guo et al. (2017); Zhang et al. (2021);\nNaumov et al. (2019).\nIn recent years, the impact of deep learning, and notably its success in domains such as computer vision and natural language processing Y et al. (2015 May); Young et al. (2018), has been extended to recommendation systems. Part of this success is due to the use of optimization objectives that can model user engagement via leveraging for deep neural networks, which as a result has motivated the migration of many significant industrial recommendation models to deep neural network architectures Zhang et al. (2021); Wang et al.\n(2015), illustrating its profound role in shaping the future of recommendation systems.\nSelf-supervised learning (SSL) stands out as a powerful technique with significant benefits for various facets of deep learning model development. At its essence, SSL is crafted to aid models in capturing intricate information that may prove challenging to extract directly from raw data, due to its reliance not only on"}, {"title": "2 Related Work", "content": "Various studies in the literature have shown the benefit for the use of simple input perturbations in regularizing model's generalization and robustness, which is dubbed in the literature as perturbation-based regularization (see Figure 1). For instance, it has been shown that perturbing inputs with noise, regularizes the models towards more robustness and better generalization capabilities Dhifallah and Lu (2021); Orvieto et al. (2023);\nHua et al. (2021); Wager et al. (2013). More concretely, two kinds of input perturbations have been identified to be effective in terms of model's generalization: 1) noise injection Dhifallah and Lu (2021); Orvieto et al.\n(2023); Hua et al. (2021), and 2) feature dropout Tamkin et al. (2022); Wager et al. (2013); Srivastava et al.\n(2014a). Such regularizations have been proven to play an important role in preventing learning suppression, for instance, via leveraging techniques such as Self-Consistency Regularization e.g, in Sinha and Dieng\n(2021), which evidently reports the distance between semantically similar points has undergone a significant reduction, showcasing the substantial impact of this regularization technique which leads to its popularity in\nthe literature Ko et al. (2022); Tan et al. (2022); Sinha and Dieng (2021); Wang et al. (2021); Englesson and\nAzizpour (2021); Kim et al. (2021, 2022).\nIn this work, we take a broader look at perturbation based regularization approaches for industrial-scale applications in ads ranking, and share our findings on achieving better generalization via self-supervised learning, applicability for industrial usecases, and integration into complex industrial systems. More concretely, we present the first instance of its kind for a successful integration of perturbation based regularization into industrial-scale recommendation systems. Additionally, we present Loss-Balanced Small- Perturbation Regularization (LSPR), a novel perturbation-based regularization method that as we show, can improve the performance of industrial-scale ads ranking systems, while being simpler than its counterparts, hence, assist in scaling. In summary, the main contributions of our work are as follows:\nRegularization Techniques for Ads Ranking at Scale: We share our findings on regularization techniques that are applicable in industrial settings for ads ranking. These encompass improvements in offline metrics, and as we report, in several experiments we have obtained 0.1% - 0.3% relative Normalized Entropy (NE) offline gains by applying perturbation based regularizations.\nLoss-Balanced Small Perturbation Regularization (LSPR): We propose LSPR: instead of adding an additional auxiliary loss function (e.g, often an MSE term) to alleviate the difference in predictions (e.g, as in Self-\nConsistency Regularization (SCR) ), we create new samples by perturbing datapoints with noise that are\nscaled by a small weight, and include them in the training data, but additionally weight them down in the the loss term calculation (see Figure 3). Our numerical analysis (see Section 5.1) shows LSPR achieves a"}, {"title": "2.1 Perturbation based self-Supervised learning", "content": "Perturbation based self-supervised learning has showcased its effectiveness in numerous applications. For instance, Chen et, al. Chen et al. (2020) introduced SimCLR, a Contrastive Learning approach, demonstrating that after representation learning with SimCLR, only a minimal 1% of labeled data suffices to attain the same top-5 accuracy as AlexNet. Building on top of this work, Zbontar et, al. Zbontar et al. (2021) introduced Barlow Twins, which through the correlation of augmented and original data representations, achieved significant performance gains in computer vision problems. SSL has also made substantial contributions to the field of Natural Language Processing (NLP). For instance, Gao et, al. Gao et al. (2021) introduced SimCSE, and Chuang et, al. Chuang et al. (2022) introduced DiffCSE, both of which leveraged contrastive learning methods on improving sentence embeddings."}, {"title": "2.2 Self-Supervised Learning for Recommendation Systems", "content": "With the substantial influence of perturbation based self supervised learning in the fields such as natural language processing and computer vision, researchers have extended their exploration to recommendation systems. One example of such kind of efforts is Wang et, al. Fet al. (2023) which focuses on enhancing Click- Through Rate (CTR) and Conversion Rate (CVR) estimation by applying Contrastive Learning techniques at the embedding level. This approach emphasizes the importance of post-embedding level operations and highlights the potential of self-supervised techniques for advancing ad ranking, offering valuable insights for large-scale ad recommendation systems.\nIn Yao et al. (2021), researchers have made substantial contributions to the field of large-scale recommendation systems with a focus on perturbation based self-supervised learning. Their work introduces a two-stage perturbation approach at the embedding level, complemented by the application of contrastive learning to the predictions generated in each of these stages. Moreover, the paper introduces an inventive feature masking technique named Correlated Feature Masking. The combination of Correlated Feature Masking and Contrastive Learning yields exceptional performance in the desired metrics. These innovations, including the two-stage perturbation approach and Correlated Feature Masking, mark significant advancements in the domain of self-supervised learning for recommendation systems.\nGu et al. (2021) has harnessed the power of Self-Supervised Learning techniques in daily user interactions."}, {"title": "2.3 Self-Consistency Regularization (SCR)", "content": "Self-Consistency Regularization, engineered to ensure semantic similarity within the latent space for objects that share common semantics, as detailed in the research by Sinha et al. Sinha and Dieng (2021), has a well-documented track record of efficacy. Previous studies consistently attest to the capability of this technique in fostering proximity of representations for semantically related objects in the latent space. In the literature, one of the aspects that has been attributed to the success of consistency regularization and contrastive learning Zhang and Ma (2022) has been identified as the use of Data Augmentation."}, {"title": "2.4 Data Augmentation", "content": "Data augmentation stands as a fundamental component in many self-supervised learning algorithms. While deep neural networks excel in various challenges in learning from data, they are particularly sensitive to data volume Shorten and Khoshgoftaar (2007) and often struggle to grasp the underlying data distribution. Given the scale of these models, insufficient data can lead to highly variable predictions in diverse settings. Data augmentation can incorporate strong priors from data or domain knowledge into models Eghbalzadeh et al.\n(2024), and further be used to regularize models towards better robustness and generalization Zhang et al.\n(2017); Yun et al. (2019). However, most of the focus in such approaches have been on structured data such as images, audio, etc; and it has been shown that such domain-specific augmentations should be used in new domains with caution Eghbalzadeh et al. (2024)."}, {"title": "3 Preliminaries", "content": "Click-Through Rate (CTR) prediction aims to estimate the probability of the user clicking a candidate ad after having an impression in the ranking stage. Similarly, Conversion Rate (CVR) prediction estimates how likely the user will convert the candidate ad after having a click. Our perturbation-based regularization techniques can be applied to both CTR and CVR predictions with similar set-ups, therefore, we use the CTR prediction as an example to introduce the basic preliminaries, and the intrinsic differences between CTR and CVR modeling (e.g., delayed feedback for ad conversions) is beyond the scope of the discussions in this paper.\nFor the CTR prediction task, let a training dataset with N examples be defined as {xn, Yn}Nn=1, where a random variable \u00e6n represents the feature space of the n-th training example, and a random variable Yn \u2208 {0,1} represents the binary label indicating whether the user has clicked the candidate ad or not. The feature space can consist of the following types:\n\u2022 Dense features are single-digit float values (e.g., counts and stats (mean, percentiles, variances) of user/ad behaviors and profiles), and the total number of such features could be in the scale of thousands. We initially apply a pre-processing procedure on each of them, and then concatenate them together to form a single high-dimensional float vector, so as to interact with other features later;\n\u2022 Embedding features are high-dimensional float vectors which are usually generated from pre-trained manners (e.g., user and ad embeddings from graph learning algorithms). We will denote embedding features as ei"}, {"title": "4 Methodology", "content": "\u2022 Sparse features are high-dimensional integer vectors, that represent concepts such as user-item interactions, where both number of items and users are large. Sparse features can often be represented via a much lower dimensional vector si via various techniques, e.g, the use of an embedding matrix, or affinity scores for reweighing.\nAfter processing each feature to generate the corresponding representation, the feature interaction layer is applied on top to learn their interactions with arbitrary orders (e.g., DHEN Zhang et al. (2022), DCN Wang\net al. (2017), Transformer Vaswani et al. (2017)), and generate a final representation rn. Afterwards, the prediction layer produces the prediction probability \u0177n \u2208 [0,1] based on rn, and the commonly adopted loss function is calculated as\n$L_{supervised} (Yi, \\hat{y_i}) = \\frac{1}{N} \\Sigma Y_n log(y_n) + (1 - Y_n) log(1 - \\hat{y_n}).$ (1)\nIn this section, we delve into the core regularization techniques we have applied to an industrial-scale ads recommendation system. We start by discussing our data augmentation strategies which are an important part of Perturbation-Based Regularization, and further detail Self-Consistency regularization methods. We then discuss regularization techniques that promote perturbation invariance beyond Self-Consistency Regularization. Finally, we discuss the integration of these techniques into different phases of industrial-scale models - such as Retrieval, Early and Final Stage Ranking."}, {"title": "4.1 Data Augmentation", "content": "Data augmentation has played a role in our perturbation-based regularization algorithms. It's essential to underline that recommender systems, as mentioned in Guo et al. (2017) and Yao et al. (2021), are significantly influenced by both sparse and dense features. Therefore, a robust augmentation strategy that caters to both types of features, improving the effectiveness of our regularization methods in various recommendation scenarios.\nDropout was initially proposed as a regularization method that enabled deep learning models to generalize, and is known as one of the stepping stones of deep learning Srivastava et al. (2014b). It further emerged as a vital data augmentation strategy tailored for sparse features, leveraging insights from its prior applications in natural language processing Gao et al. (2021) and recommender systems Yao et al. (2021). In this context, the core concept involves creating a subset of existing sparse features as augmented copies of the original sparse feature set. For instance, consider a datapoint with embedding features e and sparse features s:\n$(S_1, S_2, S_3, S_4, S_5, S_6) \\rightarrow (e_1, e_2, e_3), (S_1, S_2, 0, 0, e_5, e_6)$\nThe extent of dropout perturbation varies depending on the problem setting, with the option to employ either a strong or weak dropout. When integrated correctly with Self-Supervised Learning (SSL) techniques, dropout has exhibited substantial performance improvements in large-scale item recommendations Yao et al. (2021), emphasizing its pivotal role in enhancing recommendation systems.\nGaussian Noise Injection serves as a technique for augmenting dense features within our framework. The concept is elegantly simple, involving the generation of a random vector i from a Gaussian distribution, denoted as $i \\sim \u039d(\\mu, \\sigma)$. For instance, in a 3-dimensional float vector, represented as $x = (X_1,X_2,X_3)$, augmentation with Gaussian Noise can be described as follows:\n$(e_1, e_2, e_3) \\rightarrow (e_1 + i_1, e_2 + i_2, e_3 + i_3)$\nThis augmentation introduces controlled randomness to the features, contributing to the model's robustness and diversity of the data."}, {"title": "4.2 Self-Consistency Regularization (SCR)", "content": "Self-Consistency Regularization is an algorithm that enforces small modifications in the data still preserve the similar prediction value. The algorithm, is especially important when the model is too large, and data-set is not large enough to serve to the model's capacity. The algorithm introduces an auxiliary loss term, that penalizes the disparity between the outcomes of the perturbed and the original data point, effectively promoting consistency in the latent space representation (see Figure 2).\nThe concept underlying SCR is as straightforward as depicted in Figure 2. As can be seen, perturbed data along with the original data is fed to the model, and an additional regularization loss is used to minimize the model's output differences between original and perturbed data. In this approach, we incorporate Mean Squared Error (MSE) loss term as the regularizer alongside the supervised loss term.\n$L_{consistency} (Yi, \\hat{y_i}, Pi, \\hat{p_i}) = L_{supervised}(Yi, \\hat{y_i}) + \\lambda L_{MSE} (Pi, \\hat{p_i})$\nwhere $L_{supervised}$ is as defined in Eq. 1 and $L_{MSE}$ is defined as\n$L_{MSE} (P_i, \\hat{p_i}) = \\frac{1}{N} \\Sigma (P_i - \\hat{P_i})^2$\nand pi represent some hidden representation of the deep neural network for some input xi, while p'; denotes this representation for the perturbed input x'i."}, {"title": "4.3 Loss-Balanced Small Perturbation Regularization (LSPR)", "content": "Despite its simplicity and generality, training models with noise has been known to improve generalization of models Bishop (1995). In this section, we study a variation of Perturbation-Based Regularization, namely, Loss-Balanced Small Perturbation Regularization (LSPR). In this approach, perturbed points are treated as original points but with smaller weights in the loss. Moreover, we expect these datapoints that contain small perturbations to have the same label as the original data points. Therefore, we name this algorithm Loss-Balanced Small Perturbation Regularization (LSPR). In contrast to data augmentation that treats both augmented (or perturbed) data and original data equal in the loss calculation, LSPR reduces the weights of perturbed data in the calculation of the loss, hence, is less disruptive to learning dynamics. Furthermore,"}, {"title": "4.4 LSPR's Hyperparameters", "content": "LSPR is designed to be simple yet effective, with only three major hyperparameters, while providing significant values in performance and optimization. Here are its hyperparameters and our approach in hyperparameter tuning: dense feature perturb: We enforce perturbation to be of the same distribution as our dense features.\nsparse feature dropout: we apply a relatively small dropout rate to sparse features. - loss weight: We start our hyperparameter search for the loss weight from a smaller scale relative to main objectives, and then to\na more fine grained search This simplicity allows for easier tuning and deployment in large-scale industrial settings while still delivering significant performance improvements. We will add these details to the final version of our paper."}, {"title": "5 Analysis and Experimentation", "content": "In this section, we leverage a well-known theoretical framework proposed in Werfel et al. (2003) to demonstrate how LSPR results in a better alignment of weights in the model optimization to portray a clear picture of the construct of a optimization problem in ranking, and how LSPR affects it. We start by formalization of our framework, as well as the integration of Perturbation-Based Regularizations, namely SCR and LSPR. In Section 5.1 we analyze how LSPR compares to SCR via controlled experimentations and analysis on linear models, investigating the learning dynamics with these regularization applied. Further, in Section 5.2, we report our empirical results on an in-house dataset that was used to evaluate the methodologies applied here. We tracked model accuracy using Normalized Entropy (NE) in offline experiments He et al. (2014). In experiments with real data, each datapoint exhibits a substantial volume of features, comprising thousands of dense features and hundreds of sparse features and we employ the Adagrad optimizer for optimization. Ranking has been done through multiple stages during learning, which are described below in more details. In this section, we report performance improvements via the presented regularization techniques on a multi-stage ranking system with 3 stages of retrieval, early stage ranking, and final-stage ranker."}, {"title": "5.1 Numerical Analysis", "content": "In this section, we provide a numerical analysis for the linear models trained with SGD 1) with Self-consistency Regularization (SCR), and 2) with Loss-Balanced Small Perturbation Regularization (LSPR). We analyse the gradient update directions and the alignment with the optimal weight (See Section. 3) by calculating the cosine similarity in the model's weight space, comparing weights of different iterations to the optimal weight. Our numerical analysis (see Figure 4) shows that:\n1. compared to SCR, LSPR finds a better alignment with the optimal weight, while converging faster and achieving a lower error in the weight space.\n2. we also show that balancing both amount of noise w and loss A is crucial to the success of LSPR and SCR. As we show, smaller values for these weights are recommended for better convergence and performance."}, {"title": "5.1.1 Setup", "content": "The goal in this section is to analyse how different perturbation-based regularizations, namely SCR and LSPR, impact learning and performance. To this end, we simplify both LSPR and SCR frameworks to their core, and furthermore using linear models study their effects in learning dynamics and performance. We use 2-layer linear models which strike a good balance between model expressiveness and simplicity Werfel et al. (2003).\nTo this end, we define a ground-truth function with the weight W* that maps input data to their labels as"}, {"title": "5.1.2 Results", "content": "follows:\n$y = W*x$ (3)\nwhere x denotes an input feature and y denotes the ground-truth output, and W* is a Ly \u00d7 Lx matrix where\nLy and Lx are input and output dimensionalities.\nWe now define the following linear model that we use to learn the input-output relationship by:\n$y = W_2W_1x$ (4)\nwhere x denotes an input feature and y denotes the ground-truth output, W\u2081 is a matrix of size Lh \u00d7 Lx\nand and W2 is a matrix of size Ly \u00d7 Lh and Lh is the dimensionality of the intermediate representations. To\nsimulate the effect of regularization, we use Stochastic Gradient Descent (SGD) with an MSE error as follows:\n$\\frac{1}{2} ||y - \\hat{y}||^2$(5)\nand y is the output of the linear model. In order to study the learning dynamics, we denote the weight error\nas:\n$E = W_2W_1 \u2013 W*$(6)\nand further introduce:\n$\\epsilon = \\frac{1}{L_xL_y}-tr[ETE], \\gamma = \\frac{W_2W_1 W*}{\\|W_2W_1\\\\ \\|W*\\|}$(7)\nwhere e represents the error in the weight space to the optimal weight, while y demonstrates weight alignment with the optimal weight W*. The SGD weight updates are as follows:\n$\\delta W_1^{SGD} = -\\eta \\frac{\\delta L(x, y)}{\\delta W_1} = -\\eta (W_2^{T}(W_2W_1x - y)) \\otimes x$(8)\n$\\delta W_1^{SGD} = -\\eta \\frac{\\delta L(x, y)}{\\delta W_2} = -\\eta (W_2W_1x - y) \\otimes W_1x$(9)\nwith $ \\otimes $ denoting the outer product.\nIn order to simulate LSPR, we sample noise $z \\sim N(0, I)$ and additionally add $L(x + wz, y)$ where wisa small weight for the perturbation z. The final loss will be a balance of $L(x, y) + \\lambda L(wz + x, y)$. The LSPR weight updates are then defined as:\n$\\delta W_1^{LSPR} = -\\eta (W_2^{T}(W_2W_1x - y)) \\otimes x \\ -\n\\lambda \\eta (W_2^{T}(W_2W_1 (\\omega z + x) \u2013 y)) \\otimes (\\omega z + x)$ (12)\n$\\delta W_2^{LSPR} = -\\eta (W_2W_1x - y)) \\otimes W_1x \\ -\n\\lambda \\eta (W_2^{T}(W_2W_1 (\\omega z + x) \u2013 y)) \\otimes W_1(\\omega z + x)$ (13)\nTo analyse the SCR method, we rely on the additional learning signal that pushes the output of a model on clean and noisy inputs closer together, namely, $L(x + wz, \\hat{y})$ where $ \\hat{y} = W_2W_1x$. Consequently, the SCP weight updates are as follows:\n$\\delta W_1^{SCR} = -\\eta (W_2^{T}(W_2W_1x - y)) \\otimes x \\ -\n\\lambda \\eta (W_2^{T}(W_2W_1 (\\omega z + x) \u2013 W_2W_1x)) \\otimes (\\omega z + x)$ (14)\n$\\delta W_2^{SCR} = -\\eta (W_2W_1x - y)) \\otimes W_1x \\ -\n\\lambda \\eta (W_2^{T}(W_2W_1 (\\omega z + x) \u2013 W_2W_1x)) \\otimes W_1(\\omega z + x)$ (15)"}, {"title": "5.2 Experiments on Real Data", "content": "As can be seen in Figure 4, we can observe that for small perturbation weights wand loss weights A, LSPR tends to better find the optimal weight as can be seen by looking at the two presented plots.\nWe experimented with perturbation-based consistency regularization on different stages of various prediction problems. We present these results in Table 1. We observed a relative NE gain of approximately 0.1%-0.3%, depending on the prediction model tested in various offline experiments. We will first present the results for the Retrieval stage from the offline experiments, followed by the experimental results for the Early and Final Stage ranker.\nOffline Retrieval Stage: We have experimented with consistency regularization in two different models that predict conversion rate and click-through rate, respectively. We obtained the best results when regularizing both logit and representative of embedding with 0.15%-0.2% relative NE improvements.\nOffline Early Stage Ranking: models are generally simpler ranking models compared to final stage models. Therefore, we applied regularization to the entire object and user embedding, resulting in a 0.3% relative NE gain in various offline experiments.\nOffline Final Ranking Stage: Models in this stage are generally much larger and complex compared to previous stages, as we are looking for more precise ranking of ads. We obtained the best results by regularizing both the logit and output of the embedding together. Results from several experiments suggest an average 0.1% relative NE gain, which has been further validated with online testing."}, {"title": "5.2.1 Self-Consistency Regularization (SCR)", "content": "We have explored LSPR primarily in the offline Final Ranker Stage, under various signal availability setups. We have observed that the technique has performed promisingly in various setups, ultimately leading to"}, {"title": "5.2.2 Loss-Balanced Small Perturbation Regularization (LSPR)", "content": "As can be seen in Figure 4, we can observe that for small perturbation weights wand loss weights A, LSPR tends to better find the optimal weight as can be seen by looking at the two presented plots.\nWe experimented with perturbation-based consistency regularization on different stages of various prediction problems. We present these results in Table 1. We observed a relative NE gain of approximately 0.1%-0.3%, depending on the prediction model tested in various offline experiments. We will first present the results for the Retrieval stage from the offline experiments, followed by the experimental results for the Early and Final Stage ranker.\nOffline Retrieval Stage: We have experimented with consistency regularization in two different models that predict conversion rate and click-through rate, respectively. We obtained the best results when regularizing both logit and representative of embedding with 0.15%-0.2% relative NE improvements.\nOffline Early Stage Ranking: models are generally simpler ranking models compared to final stage models. Therefore, we applied regularization to the entire object and user embedding, resulting in a 0.3% relative NE gain in various offline experiments.\nOffline Final Ranking Stage: Models in this stage are generally much larger and complex compared to previous stages, as we are looking for more precise ranking of ads. We obtained the best results by regularizing both the logit and output of the embedding together. Results from several experiments suggest an average 0.1% relative NE gain, which has been further validated with online testing."}, {"title": "5.2.3 Online Experiments", "content": "We additionally have conducted online experimentation for a prediction model after testing it in offline setup. The online experimentation is different than offline one in the nature that, it runs in continuous training and inferring routine, compared to full training and inferring mode. These online experiments on various data from different parts of the data stream, using both noisy and clean labels, have demonstrated a similar trend to the offline experiments we reported in the previous sections. Our results indicate that LSPR has achieved a 0.1% to 0.2% relative improvement in online top-line metrics, consistently across multiple launches. Note that the magnitude of the impact is significant at the level of a billion-scale industrial production ads ranking system, which serves billions of users across various surfaces, across global geological locations, and across various clients."}, {"title": "5.3 Baselines", "content": "The experiment comparisons in this manuscript are all compared against the latest production models in a multi-billion-scale industrial ads ranking system, prior to the adoption of LSPR. Our criteria for selecting baselines was to identify models that 1) have been proven to operate effectively at the industry scale; 2) represent the state-of-the-art ads ranking product models in the industry. We consider these production-level recommendation models to be among the state-of-the-art baselines that meet the above criterion."}, {"title": "6 Conclusion and Future Work", "content": "Our study has explored the application of perturbation based regularization algorithms in an Industrial-Scale Recommendation Systems. To this end, we have made significant contributions: firstly, to the best of our knowledge, we showed for the first time that Perturbation Based Regularization techniques can bring meaningful improvements to Industrial-Scale Recommendation Systems. Secondly, we introduced a novel regularization technique - LSPR, a general method that is applicable in many Deep Learning setups. In summary, LSPR has been launched to major industrial-scale ads recommendation models across different ranking stages and traffic. This indicates that it can be generalized to diverse user demographics and content types, considering the scale and reach of the deployed ads platform. Our future research endeavors are poised to focus on other variations of the use of unlabeled data, tailored for Large Scale Recommendation Systems,"}]}