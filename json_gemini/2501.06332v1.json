{"title": "Aggregating Low Rank Adapters in Federated Fine-tuning", "authors": ["Evelyn Trautmann", "Ian Hales", "Martin F. Volk"], "abstract": "Fine-tuning large language models requires high computational and memory resources, and is therefore associated with significant costs. When training on federated datasets, an increased communication effort is also needed. For this reason, parameter-efficient methods (PEFT) are becoming increasingly important. In this context, very good results have already been achieved by fine-tuning with low-rank adaptation methods (LoRA). The application of LoRA methods in Federated Learning, and especially the aggregation of adaptation matrices, is a current research field. In this article, we propose a novel aggregation method and compare it with different existing aggregation methods of low rank adapters trained in a federated fine-tuning of large machine learning models and evaluate their performance with respect to selected GLUE benchmark datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "With the increasing importance of large language models (LLMs), even fine-tuning tasks have become more and more challenging. Computational cost has not only financial but also environmental and legal impact. For instance the EU AI Act clearly states energy sustainability as a requirement. To save machine capacity and cost, parameter efficient fine-tuning (PEFT) has gained popularity, especially as results from PEFT methods have been shown to be reasonably performant keeping a high model quality while at the same time they achieve a considerable reduction of training parameters [8].\nLow Rank Adaptation (LoRA [13]) is a popular PEFT method that aims to train only a low-dimensional approximation of the gradient in each fine-tuning iteration instead of the weight matrices. For this purpose, the actual weight matrices are frozen, and only two low-rank adapter matrices are trained, whose product approximates the gradient.\nIf the training data is not present at a single central place but distributed over several clients, it can still be used for fine-tuning. Federated Learning [21] is a machine learning ap- proach where a central server coordinates training across mul- tiple decentralized devices or servers holding local datasets, without exchanging the data itself. This method enhances data privacy and security, as raw data remains on local devices, while only model updates are shared and aggregated. This setup alone does not guarantee privacy, but it allows for employment of privacy enhancement technologies as stated in subsection II-B1.\nPEFT methods like LoRA can be combined with Federated Learning. In an immediate approach all trained parameters are aggregated with Federated Averaging. We will show in this article the shortcomings of the naive approach and present an improved aggregation method."}, {"title": "A. Outline", "content": "In the following, we will outline our method and conduct various experiments, in which we apply LoRA in a federated way where the data is distributed over several clients. The trained parameters must be aggregated at regular intervals. We show empirical evidence that our approach provides improved convergence in the early stages of model training, where the contribution of the A matrix is stronger [9, 10]. Finally, we discuss possible combinations of the individual methods."}, {"title": "II. BACKGROUND", "content": "Fine-tuning LLMs comes with several challenges. As mod- els grow, the number of training parameters increases, leading to high training costs and deployment challenges.\nAgainst this backdrop, a dedicated research area focusing on parameter-efficient fine-tuning has emerged, encompassing methods such as adapter layers and prompt optimization. The LORA method proposed in [13] represents a good trade-off between efficiency and model quality.\nWhereas previous PEFT methods [12, 22], merely focused on adding adapter layers to the model, LORA modifies existing layers using low-rank approximations of weight increments without adding new layers. Each frozen layer is modified as illustrated in Fig. 1.\nAt its core, this method involves training two low-rank adapters, A and B, of size $(m \\times r)$ and $(r \\times n)$, instead of the full $(m \\times n)$ parameter matrix W for each layer. The product of these adapters approximates the gradient of the parameter matrix. This reduces the number of parameters to be trained from $m \\cdot n$ to $(m+n) \\cdot r$, which leads to a significant reduction in the number of trainable parameters when $r < \\text{min}(n, m)$."}, {"title": "A. Low Rank Decomposition", "content": null}, {"title": "B. Federated Learning and Privacy", "content": "Federated Learning was originally introduced by Google [18] as a decentralized training method for highly distributed (and private) data on mobile devices. However, it has quickly been adopted in other fields, particularly the healthcare sector [5], to make highly protected (patient) data from various sources available for collaborative training.\nThe method consists of two steps: a computation step, where clients locally train a model on their respective data, and an aggregation step, where the model parameters of the involved clients are aggregated on a central server. Thus, only the model parameters, not the data itself, leave the client. There are also decentralized FL architectures that do not require a central server, but in this article, we focus exclusively on aggregation using a central orchestrator as illustrated in Fig. 2. A promising approach for federated PEFT methods in decentralized environments can be found in the following publication [29].\nThe computation step and aggregation step together is called a communication round. The distribution of the data can vary. In the simplest case, the data is independently and identically distributed (i.i.d.), but Federated Learning can also be successfully applied in the non-i.i.d. case [30]. Especially in fine-tuning task federation can lead to a better generalization of the model and even be used for representation learning [4].\n1) Privacy Enhancing Technologies: Even if the data does not leave its location, it is possible that information could be revealed through the updates of the model parameters. To address this risk of disclosure, there are various privacy- enhancing technologies. In differential privacy [6, 7], noise is added to each parameter update, the amount of which is calculated based on a privacy budget and the sensitivity of the respective update function."}, {"title": "III. RELATED WORK - EXISTING FEDERATION APPROACHES", "content": "Federated Learning is applied in various scenarios. One example is a training scenario on mobile devices, where many devices contribute to the training, but not all are reliably available. Challenges like the high number of clients and their heterogeneity are addressed in these articles [2, 3]. Another scenario is the healthcare [11] or financial [16] sector, where a smaller number of institutions agree on joint training on particularly protected data. In the following, we focus on the second scenario and assume a federated setup consisting of two clients, each with a private dataset, which is not accessible from outside the client. Both train the parameters $A_1, B_1$ and $A_2, B_2$ respectively. After each communication round, these parameters are aggregated and redistributed to the clients."}, {"title": "A. Vanilla Federated Averaging", "content": "The conventional Federated Averaging approach averages over the client adapters $A_1, A_2$ and $B_1, B_2$ by computing the (weighted) mean of the parameters. However, by definition, the LoRA parameters are a decomposition of the actual up- date parameter $\\Delta W$. Averaging on each parameter separately introduces an error:\n$A = 0.5 \\cdot (A_1 + A_2)$\n$B = 0.5 \\cdot (B_1+B_2)$\n$B A \\neq 0.5 \\cdot (B_1A_1 + B_2A_2)$\n(1)\nThe precise aggregation would be the average of the prod- ucts of the adaptors (right side of inequality 1) and not the product of the averaged adaptors as in the vanilla approach (left side of inequality 1). Despite from this additional in- troduced error we show below (in Eq. 2) that the vanilla approach is also not compatible with additive noise, which is a requirement for PET methods such as Differential Privacy."}, {"title": "B. FFA-LORA", "content": "One way to overcome the above described error was pro- posed in Federated Freeze A LORA (FFA-LoRA [24]). In this approach, only one of the adaptation matrices is trained, the other is frozen as well. This way, not only the aggregation error is eliminated, it turns out to be also compatible with differential privacy."}, {"title": "a) Differential Privacy:", "content": "Adding differential privacy to A and B leads to a quadratic noise term. To overcome this problem and the inequality of (1), the authors of [24] propose to freeze A and train only B. This ensures that noise is added solely to B, thereby avoiding the non-linear effects that arise when two Gaussian noise terms are multiplied.\n$A = A + \\eta \\epsilon$\n$B = B + \\eta \\epsilon$\n$\\epsilon \\sim N(0, \\epsilon)$\n$\\epsilon \\sim N(0, \\epsilon)$\n$B A = B A + (A + B) \\cdot \\eta \\epsilon + \\eta^2$\n(2)\nWe can see immediately that the error in equation 2 is no longer normally distributed.\n$B \\cdot A + (A + B) \\cdot \\eta \\epsilon + \\eta \\sim N(0, \\epsilon)$\n(3)"}, {"title": "IV. FULL RANK AGGREGATION (FRA) OF LORA ADAPTERS", "content": "The direct averaging of LoRA adapters introduces errors, as indicated in (1). We hypothesize that an alternative approach based on directly averaging the weight increments will reduce these errors. This method, termed Full Rank Aggregation LORA (FRA-LORA), is expected to improve the accuracy and effectiveness in the early training rounds of Federated Learning aggregation.\n$\\Delta W_1 = B_1A_1$\n$\\Delta W_2 = B_2A_2$\n$A = 0.5 \\cdot concat(A_1, A_2)$\n$B = 0.5 \\cdot concat(B_1, B_2)$\n$B A = 0.5 \\cdot (\\Delta W_1 + \\Delta W_2) = \\Delta W$\n(4)\nThis approach is also compatible with Differential Privacy as the noise can be added directly on the high-dimensional aggregation (eq. 4), such that non-linear effects that come with the product of the low rank adapters are avoided.\nIn a setup with a small number of clients the concatenation of low rank matrices has still a rather low rank and thus the matrix multiplication stays $O(r \\times (n + m))$, where r is the rank and m\u00d7r is the dimensionality of B and r\u00d7n the dimensionality of A. However, the return value of the federated aggregation must have the same dimensionality as the unaggregated contributors, which is obviously not the case for the concatenations. Thus, we need an additional step to reduce the dimensionality of aggregated adapters back to the original. The main requirement we want to achieve with the aggregation of the federated parameters is a close approximation of the aggregated weight increment $\\Delta W$. This is the product of the concatenated matrices (4) but can be as well decomposed again in low rank matrices of rank r like the original federated LoRA adapters. For this purpose we perform a singular value decomposition (SVD) of $\\Delta W$ and keep only the eigenvectors that belong to the r highest singular values."}, {"title": "V. EXPERIMENTS", "content": "In the following, we examine the presented aggregation methods using the General Language Understanding Evalu- ation (GLUE) benchmark datasets [25] as examples. These are common benchmarks for LM tasks. The datasets selected for this article were:\n1) Multi-Genre Natural Language Inference (MNLI) [27]: this dataset contains a collection of sentence pairs (a premise and a hypothesis) and a label that indicates whether the hypothesis is entailed by the premise, con- tradicted or neutral. The training dataset has 392702 data points. For MNLI are two validation sets available, we chose the validation-matched, which was generated from the same source as the training set and contains 9815 data points.\n2) Stanford Sentiment Treebank (SST2) [23]: This dataset contains movie reviews that are labeled with human annotations of their sentiment. The training dataset has 67349 data points, the validation set 872."}, {"title": "A. Federated Training on balanced Datasets", "content": "First, we test all three methods in a federated setting by dividing the datasets into two arbitrary i.i.d. sets to simulate federated training. The parameters are aggregated after each epoch following the FedAvg training regime. The hyperpa- rameters for the experiments below were chosen as follows: weight decay 0.005, learning rate 1e-4, and dropout 0.2. The training was conducted over 30 communication rounds. For MNLI we specified a maximum number of steps per epoch of 5000, for the SST2 dataset, which is smaller, we trained over the entire datasets in each epoch."}, {"title": "B. Federated Training on imbalanced Datasets", "content": "Experiments on imbalanced datasets further confirm the findings from the i.i.d. setting. The results in terms of accuracy are listed in table III. The datasets were distributed according to their labels as given in table II.\na) SST2: The evolution of evaluation set accuracy over the communication rounds (Fig. 5) show, that FRA-LORA is performing well in the first 15-20 rounds but then accuracy drops and the model starts to overfit. In all experiments,"}, {"title": "c) Error Analysis:", "content": "All approaches contain a deviation from the exact gradient in the aggregation which is introduced by the low dimesional projection of A and B introduced by LORA. In case of FedAvg we get an additional error that arises from inequality (1),\n$err_{FedAvg} = 0.25 \\cdot (A_1 - A_2) \\cdot (B_2 - B_1)$"}, {"title": "The singular value decomposition of the weight increment has the following structure:", "content": null}, {"title": "a) Differential Privacy:", "content": null}, {"title": "VI. CONCLUSION AND OUTLOOK", "content": "Based on the preceding experiments, we believe that FRA- LORA can contribute to the federated fine-tuning of LLMs,"}, {"title": "especially under the following conditions:", "content": "1) The number of clients is small and the rank of the adap- tors was chosen significantly smaller than the dimension of the original weight matrix,\n2) If the fine-tuning data is large and the risk of overfitting is small, it can speed up the training by reaching a higher accuracy level faster. FRA-LORA achieves a good accuracy level faster, but in some cases, we observed that it is more prone to overfitting compared to FFA- LoRA. This is easily understandable since FRA-LORA trains twice the number of parameters. Preliminary in- vestigations suggest that the overfitting tendencies can be mitigated by adjustments to the learning rate sched- uler, weight decay as well as improved hyper-parameter search methods. In future work, it would certainly be worthwhile to explore a combination of both methods. Use FRA in the early rounds where the model is less stable and accuracy of gradients is more important, then simplify to FFA once the training is stable and the A matrix is sufficiently trained so as not to impact training performance. These findings align with recent work on LORA convergence [9], showing that as the training stabilises the A matrix has lower impact on training. Therefore it naturally follows that using FLA-LORA should improve model convergence in the early stages of training.\n3) Fine-tuning data is sensitive and privacy enhancing methods are required. Since in Federated Learning the training data is often sensitive, privacy preserving train- ing methods are of high importance and the compati- bility of FRA-LORA with Differential Privacy is a clear advantage over the vanilla Federated Averaging.\n4) A very recent promising improvement of LoRA itself is proposed here [28]. The authors propose to initialize A and B in the training rounds by an SVD of the weight update. This proceeding fits very well with our approach of aggregation.\nOur aggregation method is currently based on Federated Averaging. In future work, it should be investigated how the approach performs with other aggregation techniques, such as Federated Optimization [19], Scaffolding [17] and variants of Federated Averaging, as used, for example, in [14]."}]}