{"title": "TRACKING OBJECTS THAT CHANGE IN APPEARANCE WITH PHASE SYNCHRONY", "authors": ["Sabine Muzellec", "Drew Linsley", "Alekh K. Ashok", "Ennio Mingolla", "Girik Malik", "Rufin VanRullen", "Thomas Serre"], "abstract": "Objects we encounter often change appearance as we interact with them. Changes in illumination (shadows), object pose, or movement of nonrigid objects can drastically alter available image features. How do biological visual systems track objects as they change? It may involve specific attentional mechanisms for reasoning about the locations of objects independently of their appearances a capability that prominent neuroscientific theories have associated with computing through neural synchrony. We computationally test the hypothesis that the implementation of visual attention through neural synchrony underlies the ability of biological visual systems to track objects that change in appearance over time. We first introduce a novel deep learning circuit that can learn to precisely control attention to features separately from their location in the world through neural synchrony: the complex-valued recurrent neural network (CV-RNN). Next, we compare object tracking in humans, the CV-RNN, and other deep neural networks (DNNs), using FeatureTracker: a large-scale challenge that asks observers to track objects as their locations and appearances change in precisely controlled ways. While humans effortlessly solved FeatureTracker, state-of-the-art DNNs did not. In contrast, our CV-RNN behaved similarly to humans on the challenge, providing a computational proof-of-concept for the role of phase synchronization as a neural substrate for tracking appearance-morphing objects as they move about.", "sections": [{"title": "INTRODUCTION", "content": "Think back to the last time you prepared a meal or built something. You could keep track of the objects around you even as they changed in shape, size, texture, and location. Higher biological visual systems have evolved to track objects using multiple visual strategies that enable object tracking under different visual conditions. For instance, when objects have distinct and consistent appearances over time, humans can solve the temporal correspondence problem of object tracking by \u201cre-recognizing\" them (Fig. 1a, (Pylyshyn & Storm, 1988; Pylyshyn, 2006)). When two or more objects in the world look similar to each other, and re-recognition becomes challenging, a complementary strategy is to\ntrack one of them by integrating their motion over time (Fig. 1b, (Lettvin et al., 1959; Takemura et al., 2013; Kim et al., 2014; Adelson & Bergen, 1985; Frye, 2015; Linsley et al., 2021)).\nThe neural substrates for tracking objects by re-recognition or motion integration have been the focus of extensive studies over the past half-century, and the current consensus is that distinct neural circuits are responsible for each strategy (Lettvin et al., 1959; Takemura et al., 2013; Kim et al., 2014; Adelson & Bergen, 1985; Frye, 2015; Pylyshyn, 2006). Much less progress has been made in characterizing how visual systems track objects as their appearances change (Fig. 1c,d), although visual attention likely plays a critical role in tracking (Blaser et al., 2000). Indeed, visual attention is considered essential for solving many visual challenges that occur during object tracking, such as maintaining the location of an object even as it is occluded from view (Koch & Ullman, 1987; Roelfsema et al., 1998; Busch & VanRullen, 2010; Herrmann & Knight, 2001; Pylyshyn & Storm, 1988; Pylyshyn, 2006). We hypothesize that visual attention similarly helps when tracking objects that change appearance by maintaining information about their location in the world independently from their appearances.\nHow is this type of visual attention implemented in the brain? Prominent neuroscience theories have proposed that the synchronized firing of neurons reflects the allocation of visual attention. Specifically, neural synchrony enables populations of neurons to multiplex the appearance of objects with more complex visual routines controlled by attention (McLelland & VanRullen, 2016; Wutz et al., 2020; Frey et al., 2015). Neural synchrony could, therefore, help keep track of objects regardless of their exact appearance at any point in time.\nRecent work proposed using complex-valued representations to implement neural synchrony in artificial models (Reichert & Serre, 2013; L\u00f6we et al., 2022; Stani\u0107 et al., 2023). According to the framework proposed by Reichert & Serre (2013), each neuron in an artificial neural network"}, {"title": "BACKGROUND AND RELATED WORK", "content": "Visual routines Ullman (1984) theorized that humans can compose atomic attentional operations, like those for segmenting or comparing objects, into rich \"visual routines\u201d that support reasoning. He further proposed that the core set of computations that comprise visual routines can be flexibly reused and applied to objects regardless of their appearance, making them a strong candidate for explaining how humans can track objects that change in appearance. Visual routines are likely implemented in brains through feedback circuits that control attention (Roelfsema et al., 2000), and potentially through neural synchrony (McLelland & VanRullen, 2016). Developing a computational understanding of how visual routines contribute to object tracking and how they might be implemented in brains would significantly advance the current state of cognitive neuroscience.\nComputing through neural synchrony The empirical finding that alpha/beta (12\u201330Hz) and gamma (>30Hz) oscillations tend to be anti-correlated in primate cortex has motivated the development of theories on how the temporal synchronization of different groups of neurons may reflect an overarching computational strategy of brains. In the communication-through-coherence (CTC) theory, Fries (2015) proposed that alpha/beta activity carries top-down attentional signals, which reflect information about the current context and goals. Others have expanded on this theory to propose that these top-down signals can be spatially localized in the cortex to multiplex attentional computations independently of the features encoded by neurons (Miller et al., 2024). While there have been many different theories proposed on how computing through oscillations works (McLelland & VanRullen, 2016; Lisman & Jensen, 2013; Grossberg, 1976; Milner, 1974; Mioche & Singer, 1989), here we assume an induced oscillation and study synchrony as a mechanism for visual routines and its potential for implementing object tracking in brains."}, {"title": "MOTIVATION", "content": "How do biological visual systems track objects while they move through the world and change in appearance? Given that this problem has received little attention until now, we began addressing it through a toy experiment. We developed a simple shell game where observers had to describe how the colors, locations, and shapes of two objects changed from one point in time to the next (Fig. 2a; see SI A.5.1 for additional details). We then created a highly simplified model of a hierarchical and recurrent biological visual system to identify any challenges it may face with this game. The model was composed of an initial convolutional layer with high-resolution spatial feature maps, followed by a global average pooling layer (to approximate the coarser representations found in inferotemporal cortex, and a layer of recurrent neurons with more features than the first layer but no spatial map (McLelland & VanRullen (2016), Fig.2b). The convolutional layer was implemented using a standard PyTorch Conv2D layer, whereas the recurrent layer was implemented with the recently developed Index-and-Track (InT) recurrent neural network (RNN), which includes an abstraction of biological circuits for object tracking (Linsley et al., 2021). The combined model was trained on a balanced dataset of 10,000 samples from the shell game using a Cross-Entropy loss and the Adam optimizer (Kingma & Ba, 2014). In conditions of the game when the objects changed positions, this model performed close to chance (45% accuracy). The loss of spatial resolution between the model's early and deeper layers caused its representations of each object's appearance and location to interfere with each other (Figs. A.1 and A.2; see SI A.5 for more details).\nNeural synchrony can implement visual routines for object tracking\nThe retinotopic organization of hierarchical visual systems provides an\nimportant constraint for developing in models of object tracking: the spatial resolution of representations de- creases as they move through the hi- erarchy. We need a mechanism that can resolve the interference that this loss of spatial resolution causes to ob- ject representations without expand- ing the capacity of the model. One potential solution to this problem is neural synchrony, which can multi- plex different sources of information within the same neuronal population with minimal interference (Sternshein et al., 2011; Drew et al., 2009). Sim- ilarly, synchrony has been proposed to implement object-based attention to form perceptual groups based on their gestalt (Woelbern et al., 2002; El- liott & M\u00fcller, 2001). We, therefore, hypothesized that neural synchrony could rescue model performance in the shell game (Fig.2c)."}, {"title": "THE FEATURETRACKER CHALLENGE", "content": "Overview Our shell game was a highly simplified test of object tracking. In the real world, objects and their appearance and spatial features evolve smoothly and predictably over time. To better understand the capabilities of our CV-RNN \u2014 and neural synchrony - to track objects under such conditions, we next developed the FeatureTracker challenge. In FeatureTracker, observers watch a video and decide if a target object, which begins in a red square, travels to a blue square by the end of the video as opposed to a distractor (Fig. 4). This task is challenging for two reasons: (i) each video contains distractor objects that sometimes pass by and occlude the target object, and (ii) the color, shape, or color and shape of all objects in each video morph over time in precisely controlled ways.\nDesign Videos in the FeatureTracker challenge consists of 32 frames that are 32 \u00d7 32 pixels. Every frame shows a red start square, a blue goal square, a target object (defined as the object inside the red square at the start of the video), and 10 other \"distractor\" objects. As the objects move, their shapes, colors, or shapes and colors change in smooth and predictable ways (Fig.A.3; SI A.7.1 for details). Positive samples are when the target object ends in the blue square by the end of the video. In negative samples, a non-target object winds up in the blue square by the end of the video."}, {"title": "DISCUSSION", "content": "Humans can track objects through the world even as they change in appearance, state, or visibility through occlusion. This ability underlies many everyday behaviors, from cooking to building, and as we have demonstrated, it remains an outstanding challenge for today's machine vision systems. However, by taking inspiration from neuroscience, we have developed CV-RNN, a novel recurrent network architecture that implements attention through neural synchrony, which can do significantly better. Our CV-RNN performs significantly better than any other DNN tested on FeatureTracker, and rivaled or approached human accuracy and decision-making on all versions of the challenge.\nOur findings are strongly related to Neuroscience work that suggests that phase synchronization is a key component of perceptual grouping. The binding-by-synchrony theory (Singer, 2007) suggests"}, {"title": "CV-RNN", "content": "The InT circuit. Our CV-RNN was derived from the InT circuit (Linsley et al., 2021), which was inspired by neuroscience and designed for tracking objects based on their motion. Our goal was to enhance this circuit to become tolerant to changes in object features over time.\nThe full circuit represents two interacting inhibitory and excitatory units defined by (but see Linsley et al. (2021) for more details):\n$i[t] = gi[t - 1] + (1 \u2212 g)[z[t] \u2013 (yi[t]a[t] + \u03b2)m[t] \u2013 i[t \u2013 1]]+$ (5)\n$e[t] = he[t - 1] + (1 \u2212 h) [i[t] + (vi[t] + \u00b5)n[t] \u2013 e[t - 1]]+$ (6)\nand a mechanism for selective \"attention\":\n$a[t] = \u03c3(W_a* e[t - 1] + W_z * z[t])$ (7)\nwhere\n$m[t] = W_{e,i} * (e[t-1]a[t])$ and $n[t] = W_{i,e} * i[t]$ (8)\nand\n$g[t] = \u03c3(W_g * i[t -1] + U_g *z[t])$ and $h[t] = (W_h* e[t \u2212 1] + U_h * i[t]$ (9)\nHere, z[t] denotes the input at frame t, which is subsequently forwarded to the inhibitory unit i interacting with the excitatory unit e. Both units possess persistent states preserving memories facilitated by gates g and h. Additionally, the inhibitory unit is modulated by another inhibitory unit, a, which operates as a non-linear function of e capable of modulating the inhibitory drive either downwards or upwards (i.e., through disinhibition). In essence, the sigmoidal nonlinearity of a enables position-selective modulation, which we refer to as \"attention\". Furthermore, as a is contingent on e, lagging temporally behind z[t], its activity reflects the displacement (or motion) of an object in z[t] versus the current memory of e. Fundamentally, this attention mechanism aims to relocate and enhance the target object in each successive frame."}, {"title": "COMPLEX OPERATIONS", "content": "Before delving into our methodology for developing the CV-RNN, we first examined various operations achievable with complex numbers, including weights and operations. Given the vast array of potential operations, we will only elaborate on those that will be utilized throughout the remainder of this article.\nConsidering a complex number $z \u2208 C$, z can be written as:\n$z= Real(z)+ j.Imag(z)$ or $z=||z||.e^{j.\u03b8_z}$ (10)\nWhere Real and Imag respectively denote the real and imaginary parts of the complex number, with $j^2 = \u22121$. Similarly, $||.||$ and \u03b8 respectively stand for the magnitude and the phase of the complex number.\nApplying complex weights on real-valued activation. Given a real-valued activation x and a set of complex weights $W_C$, the resulting activity z1 is:\n$Z_1 = W_C * x = Real(W_C) *x+j.Imag(W_C) *x = (||W_C|| *x).e^{j.\u03b8_{wc}}$ (11)\nThe intuition behind the use of this operation is to learn an appropriate phase distribution ($\u03b8_{wc}$) from the real-valued input.\nApplying real-valued weights on complex activation. Given a complex-valued activation Zin and a set of real weights W, the resulting activity z2 is:\n$22= W *Z_{in} = Real(Z_{in}) * W + j.Imag(Z_{in}) * W = (||Z_{in}|| * W).e^{j.\u03b8_{zin}}$ (12)\nContrary to Eq. 11, the assumption here is that the phase of zin remains unchanged, but the amplitude is updated by the weights."}, {"title": "COMPLEX ATTENTION MECHANISM.", "content": "To induce neural synchrony through complex-valued units, in the attention mechanism of the RNN, we began from Eq. 7 and proceeded in three steps:\n1. Convert e and z \u2208 R to complex numbers: we apply complex weights $W_e^C$ and $W_z^C$ to e and z respectively following the operation described in Eq. 11. We obtain ec and ze \u2208 C.\n2. Update with the current feed-forward drive: we apply real weights $W_a$ to $(z_c + \u03c6)$ with Eq. 12. The addition operation activates the pixels corresponding to the new position of the target within the complex hidden state. Subsequently, applying the real weights deactivates the pixels where the target is no longer present. The initialization of at t = 0 is detailed below.\n3. Compute the new complex attention map: by combining the information for the feed- forward drive, the excitation unit, and the complex hidden state $a = z_c+e_c+ \u03c6$.\n4. Transfering the activity back to the real domain and applying the sigmoid operation: because, by definition, the amplitude of a complex number is positive and we want to apply a sigmoid on the attention map, we first normalize a using the InstanceNorm (In) operator (Ulyanov et al., 2016). The final attention map is obtained with a = \u03c3(In(a)).\n5. Obtaining a 2D phase-map of (2D case only). We additionally apply a real convolution (Eq. 12) on o to reduce the channel dimension and obtain a phase map of the complex hidden state: $0 arg(W_p *)$"}, {"title": "GENERATING OBJECT TRAJECTORIES.", "content": "To generate objects with smoothly changing appearances, we employed three distinct rules for generating trajectories within each dimension of the feature space:\nPosition: Following the approach in Linsley et al. (2021), spatial trajectories are randomly generated, commencing from a random position within the first input frame. To maintain trajectory smoothness, an angle is randomly selected. If this angle falls within a predefined range ensuring trajectory smoothness, the object advances in that direction; otherwise, it remains stationary.\nColor: Colors are generated using the HSV colorspace, with Saturation and Value fixed. Initialization begins with a random Hue value. Subsequently, at each frame, the Hue is updated at a constant speed and in a consistent direction across all objects.\nShape: Objects are represented by 5x5 squares. They begin in a random state, where a random number of pixels within this grid are active. Over time, they evolve according to the rules of the Game of Life (GoL) (Gardner, 1970):\nIf the pixel is active (value 1), and the number of active neighbors is less than 2 or more than 3: the pixel becomes inactive (value 0).\nIf the pixel is inactive, and the number of active neighbors is equal to 3: the pixel gets active.\nWe add a third rule to avoid making the objects disappear: if no pixel is active, the center pixel is activated."}, {"title": "GENERATING SEVERAL CONDITIONS TO EVALUATE GENERALIZATION.", "content": "We divided the challenge into 10 different conditions. The training condition was generated with (i) half the HSV spectrum (and a fixed Saturation and Value) for the colors, (ii) the first (and last) rule of the GoL to generate the shapes \u2013 making the objects grow over time (see Fig. A.3 \u2013 top row for illustrations).\nNext, we introduced testing conditions where features are out-of-distribution (OOD), meaning colors and/or shapes were not encountered during training. These conditions are depicted in the second row of Fig. A.3, and are as follows:\nOOD colors: Shapes and positions are sampled identically to the training distribution. However, colors are drawn from the unobserved portion of the colorspace.\nOOD shapes: Colors and positions evolve in a manner similar to the training distributions. Shapes, however, evolve according to the second and last rules of the Game of Life (GoL), resulting in their sizes diminishing over the duration of the video.\nOOD colors and shapes: Both the shapes and the colors are out-of-distribution (following the two rules described above). The position is the sole common feature retained from the training videos.\nThirdly, videos are generated where the trajectory of shapes and/or colors lies within the training feature space but remains fixed over time (refer to Fig.A.3, third row, and Fig.A.4 for visual representations):\nFixed trajectory colors: All objects maintain a consistent green color throughout the video, with no changes over time. Meanwhile, their shapes evolve according to the rules utilized to generate the training videos.\nFixed shape trajectory: All objects are defined by 3x3 squares, and their shapes remain constant throughout the video without any alterations over time. Meanwhile, their colors are generated in a manner similar to the training data.\nFixed colors and shapes: All the objects are 3x3 green squares (akin to the PathTracker challenge (Linsley et al., 2021))."}]}